# Arxiv Papers in cs.CV on 2019-01-14
### Fast and Robust Multi-Person 3D Pose Estimation from Multiple Views
- **Arxiv ID**: http://arxiv.org/abs/1901.04111v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.04111v1)
- **Published**: 2019-01-14 03:27:05+00:00
- **Updated**: 2019-01-14 03:27:05+00:00
- **Authors**: Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, Xiaowei Zhou
- **Comment**: Project page: https://zju-3dv.github.io/mvpose/
- **Journal**: None
- **Summary**: This paper addresses the problem of 3D pose estimation for multiple people in a few calibrated camera views. The main challenge of this problem is to find the cross-view correspondences among noisy and incomplete 2D pose predictions. Most previous methods address this challenge by directly reasoning in 3D using a pictorial structure model, which is inefficient due to the huge state space. We propose a fast and robust approach to solve this problem. Our key idea is to use a multi-way matching algorithm to cluster the detected 2D poses in all views. Each resulting cluster encodes 2D poses of the same person across different views and consistent correspondences across the keypoints, from which the 3D pose of each person can be effectively inferred. The proposed convex optimization based multi-way matching algorithm is efficient and robust against missing and false detections, without knowing the number of people in the scene. Moreover, we propose to combine geometric and appearance cues for cross-view matching. The proposed approach achieves significant performance gains from the state-of-the-art (96.3% vs. 90.6% and 96.9% vs. 88% on the Campus and Shelf datasets, respectively), while being efficient for real-time applications.



### Image Based Review Text Generation with Emotional Guidance
- **Arxiv ID**: http://arxiv.org/abs/1901.04140v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1901.04140v1)
- **Published**: 2019-01-14 05:42:51+00:00
- **Updated**: 2019-01-14 05:42:51+00:00
- **Authors**: Xuehui Sun, Zihan Zhou, Yuda Fan
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: In the current field of computer vision, automatically generating texts from given images has been a fully worked technique. Up till now, most works of this area focus on image content describing, namely image-captioning. However, rare researches focus on generating product review texts, which is ubiquitous in the online shopping malls and is crucial for online shopping selection and evaluation. Different from content describing, review texts include more subjective information of customers, which may bring difference to the results. Therefore, we aimed at a new field concerning generating review text from customers based on images together with the ratings of online shopping products, which appear as non-image attributes. We made several adjustments to the existing image-captioning model to fit our task, in which we should also take non-image features into consideration. We also did experiments based on our model and get effective primary results.



### Electrical Impedance Tomography based on Genetic Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1901.04872v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1901.04872v1)
- **Published**: 2019-01-14 07:01:36+00:00
- **Updated**: 2019-01-14 07:01:36+00:00
- **Authors**: Mingyong Zhou
- **Comment**: Full paper was accepted into proceedings of 4th International
  Conference on Innovation in Computing System & Engineering Technology (
  ICICSET 2018) in Zurich, Switzerland on August 14- 15, 2018
- **Journal**: None
- **Summary**: In this paper, we applies GA algorithm into Electrical Impedance Tomography (EIT) application. We first outline the EIT problem as an optimization problem and define a target optimization function. Then we show how the GA algorithm as an alternative searching algorithm can be used for solving EIT inverse problem. In this paper, we explore evolutionary methods such as GA algorithms combined with various regularization operators to solve EIT inverse computing problem.   Key words: Electrical Impedance Tomography (EIT), GA, Tikhonov operator , Mumford-Shah operator, Particle Swarm Optimization(PSO), Back Propagation(BP).



### Multi-band Weighted $l_p$ Norm Minimization for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1901.04206v5
- **DOI**: 10.1016/j.ins.2020.05.049
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.04206v5)
- **Published**: 2019-01-14 09:32:09+00:00
- **Updated**: 2020-06-23 05:16:00+00:00
- **Authors**: Yanchi Su, Zhanshan Li, Haihong Yu, Zeyu Wang
- **Comment**: accepted by Information Sciences
- **Journal**: None
- **Summary**: Low rank matrix approximation (LRMA) has drawn increasing attention in recent years, due to its wide range of applications in computer vision and machine learning. However, LRMA, achieved by nuclear norm minimization (NNM), tends to over-shrink the rank components with the same threshold and ignore the differences between rank components. To address this problem, we propose a flexible and precise model named multi-band weighted $l_p$ norm minimization (MBWPNM). The proposed MBWPNM not only gives more accurate approximation with a Schatten $p$-norm, but also considers the prior knowledge where different rank components have different importance. We analyze the solution of MBWPNM and prove that MBWPNM is equivalent to a non-convex $l_p$ norm subproblems under certain weight condition, whose global optimum can be solved by a generalized soft-thresholding algorithm. We then adopt the MBWPNM algorithm to color and multispectral image denoising. Extensive experiments on additive white Gaussian noise removal and realistic noise removal demonstrate that the proposed MBWPNM achieves a better performance than several state-of-art algorithms.



### Edge SLAM: Edge Points Based Monocular Visual SLAM
- **Arxiv ID**: http://arxiv.org/abs/1901.04210v1
- **DOI**: 10.1109/ICCVW.2017.284
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.04210v1)
- **Published**: 2019-01-14 09:40:45+00:00
- **Updated**: 2019-01-14 09:40:45+00:00
- **Authors**: Soumyadip Maity, Arindam Saha, Brojeshwar Bhowmick
- **Comment**: ICCV Workshops 2017, Venice, Italy, October 22-29, 2017
- **Journal**: International Conference on Computer Vision Workshops, 2017,
  2408--2417
- **Summary**: Visual SLAM shows significant progress in recent years due to high attention from vision community but still, challenges remain for low-textured environments. Feature based visual SLAMs do not produce reliable camera and structure estimates due to insufficient features in a low-textured environment. Moreover, existing visual SLAMs produce partial reconstruction when the number of 3D-2D correspondences is insufficient for incremental camera estimation using bundle adjustment. This paper presents Edge SLAM, a feature based monocular visual SLAM which mitigates the above mentioned problems. Our proposed Edge SLAM pipeline detects edge points from images and tracks those using optical flow for point correspondence. We further refine these point correspondences using geometrical relationship among three views. Owing to our edge-point tracking, we use a robust method for two-view initialization for bundle adjustment. Our proposed SLAM also identifies the potential situations where estimating a new camera into the existing reconstruction is becoming unreliable and we adopt a novel method to estimate the new camera reliably using a local optimization technique. We present an extensive evaluation of our proposed SLAM pipeline with most popular open datasets and compare with the state-of-the art. Experimental result indicates that our Edge SLAM is robust and works reliably well for both textured and less-textured environment in comparison to existing state-of-the-art SLAMs.



### Semi-supervised Learning with Graphs: Covariance Based Superpixels For Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1901.04240v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.04240v4)
- **Published**: 2019-01-14 11:18:27+00:00
- **Updated**: 2019-05-14 18:23:41+00:00
- **Authors**: Philip Sellars, Angelica Aviles-Rivero, Nicolas Papadakis, David Coomes, Anita Faul, Carola-Bibane Sch√∂nlieb
- **Comment**: Four pages with two figures
- **Journal**: None
- **Summary**: In this paper, we present a graph-based semi-supervised framework for hyperspectral image classification. We first introduce a novel superpixel algorithm based on the spectral covariance matrix representation of pixels to provide a better representation of our data. We then construct a superpixel graph, based on carefully considered feature vectors, before performing classification. We demonstrate, through a set of experimental results using two benchmarking datasets, that our approach outperforms three state-of-the-art classification frameworks, especially when an extremely small amount of labelled data is used.



### NEARBY Platform for Detecting Asteroids in Astronomical Images Using Cloud-based Containerized Applications
- **Arxiv ID**: http://arxiv.org/abs/1901.04248v1
- **DOI**: 10.1109/ICCP.2018.8516578
- **Categories**: **astro-ph.IM**, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1901.04248v1)
- **Published**: 2019-01-14 11:49:06+00:00
- **Updated**: 2019-01-14 11:49:06+00:00
- **Authors**: V. Bacu, A. Sabou, T. Stefanut, D. Gorgan, O. Vaduvescu
- **Comment**: IEEE 14th International Conference on Intelligent Computer
  Communication and Processing (ICCP), Cluj-Napoca, Romania
- **Journal**: IEEE, 2018
- **Summary**: The continuing monitoring and surveying of the nearby space to detect Near Earth Objects (NEOs) and Near Earth Asteroids (NEAs) are essential because of the threats that this kind of objects impose on the future of our planet. We need more computational resources and advanced algorithms to deal with the exponential growth of the digital cameras' performances and to be able to process (in near real-time) data coming from large surveys. This paper presents a software platform called NEARBY that supports automated detection of moving sources (asteroids) among stars from astronomical images. The detection procedure is based on the classic "blink" detection and, after that, the system supports visual analysis techniques to validate the moving sources, assisted by static and dynamical presentations.



### DeepFlash: Turning a Flash Selfie into a Studio Portrait
- **Arxiv ID**: http://arxiv.org/abs/1901.04252v2
- **DOI**: 10.1016/j.image.2019.05.013
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.04252v2)
- **Published**: 2019-01-14 11:57:05+00:00
- **Updated**: 2019-06-05 16:04:51+00:00
- **Authors**: Nicola Capece, Francesco Banterle, Paolo Cignoni, Fabio Ganovelli, Roberto Scopigno, Ugo Erra
- **Comment**: None
- **Journal**: Elsevier Image Communication, Volume 77, September 2019, Pages
  28-39
- **Summary**: We present a method for turning a flash selfie taken with a smartphone into a photograph as if it was taken in a studio setting with uniform lighting. Our method uses a convolutional neural network trained on a set of pairs of photographs acquired in an ad-hoc acquisition campaign. Each pair consists of one photograph of a subject's face taken with the camera flash enabled and another one of the same subject in the same pose illuminated using a photographic studio-lighting setup. We show how our method can amend defects introduced by a close-up camera flash, such as specular highlights, shadows, skin shine, and flattened images.



### Learning Shared Semantic Space with Correlation Alignment for Cross-modal Event Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1901.04268v3
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1901.04268v3)
- **Published**: 2019-01-14 12:48:53+00:00
- **Updated**: 2019-05-22 00:31:51+00:00
- **Authors**: Zhenguo Yang, Zehang Lin, Peipei Kang, Jianming Lv, Qing Li, Wenyin Liu
- **Comment**: 22 pages, submitted to ACM Transactions on Multimedia Computing
  Communications and Applications(ACM TOMM)
- **Journal**: None
- **Summary**: In this paper, we propose to learn shared semantic space with correlation alignment (${S}^{3}CA$) for multimodal data representations, which aligns nonlinear correlations of multimodal data distributions in deep neural networks designed for heterogeneous data. In the context of cross-modal (event) retrieval, we design a neural network with convolutional layers and fully-connected layers to extract features for images, including images on Flickr-like social media. Simultaneously, we exploit a fully-connected neural network to extract semantic features for texts, including news articles from news media. In particular, nonlinear correlations of layer activations in the two neural networks are aligned with correlation alignment during the joint training of the networks. Furthermore, we project the multimodal data into a shared semantic space for cross-modal (event) retrieval, where the distances between heterogeneous data samples can be measured directly. In addition, we contribute a Wiki-Flickr Event dataset, where the multimodal data samples are not describing each other in pairs like the existing paired datasets, but all of them are describing semantic events. Extensive experiments conducted on both paired and unpaired datasets manifest the effectiveness of ${S}^{3}CA$, outperforming the state-of-the-art methods.



### Iterative Deep Learning Based Unbiased Stereology With Human-in-the-Loop
- **Arxiv ID**: http://arxiv.org/abs/1901.04355v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.04355v1)
- **Published**: 2019-01-14 15:01:49+00:00
- **Updated**: 2019-01-14 15:01:49+00:00
- **Authors**: Saeed S. Alahmari, Dmitry Goldgof, Lawrence O. Hall, Palak Dave, Hady Ahmady Phoulady, Peter R. Mouton
- **Comment**: Accepted by ICMLA18 conference
- **Journal**: None
- **Summary**: Lack of enough labeled data is a major problem in building machine learning based models when the manual annotation (labeling) is error-prone, expensive, tedious, and time-consuming. In this paper, we introduce an iterative deep learning based method to improve segmentation and counting of cells based on unbiased stereology applied to regions of interest of extended depth of field (EDF) images. This method uses an existing machine learning algorithm called the adaptive segmentation algorithm (ASA) to generate masks (verified by a user) for EDF images to train deep learning models. Then an iterative deep learning approach is used to feed newly predicted and accepted deep learning masks/images (verified by a user) to the training set of the deep learning model. The error rate in unbiased stereology count of cells on an unseen test set reduced from about 3 % to less than 1 % after 5 iterations of the iterative deep learning based unbiased stereology process.



### iPhys: An Open Non-Contact Imaging-Based Physiological Measurement Toolbox
- **Arxiv ID**: http://arxiv.org/abs/1901.04366v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.04366v1)
- **Published**: 2019-01-14 15:48:31+00:00
- **Updated**: 2019-01-14 15:48:31+00:00
- **Authors**: Daniel McDuff, Ethan Blackford
- **Comment**: None
- **Journal**: None
- **Summary**: Imaging-based, non-contact measurement of physiology (including imaging photoplethysmography and imaging ballistocardiography) is a growing field of research. There are several strengths of imaging methods that make them attractive. They remove the need for uncomfortable contact sensors and can enable spatial and concomitant measurement from a single sensor. Furthermore, cameras are ubiquitous and often low-cost solutions for sensing. Open source toolboxes help accelerate the progress of research by providing a means to compare new approaches against standard implementations of the state-of-the-art. We present an open source imaging-based physiological measurement toolbox with implementations of many of the most frequently employed computational methods. We hope that this toolbox will contribute to the advancement of non-contact physiological sensing methods.



### Seizure Detection using Least EEG Channels by Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1901.05305v1
- **DOI**: 10.1109/ICASSP.2019.8683229
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.05305v1)
- **Published**: 2019-01-14 16:31:52+00:00
- **Updated**: 2019-01-14 16:31:52+00:00
- **Authors**: Mustafa Talha Avcu, Zhuo Zhang, Derrick Wei Shih Chan
- **Comment**: None
- **Journal**: None
- **Summary**: This work aims to develop an end-to-end solution for seizure onset detection. We design the SeizNet, a Convolutional Neural Network for seizure detection. To compare SeizNet with traditional machine learning approach, a baseline classifier is implemented using spectrum band power features with Support Vector Machines (BPsvm). We explore the possibility to use the least number of channels for accurate seizure detection by evaluating SeizNet and BPsvm approaches using all channels and two channels settings respectively. EEG Data is acquired from 29 pediatric patients admitted to KK Woman's and Children's Hospital who were diagnosed as typical absence seizures. We conduct leave-one-out cross validation for all subjects. Using full channel data, BPsvm yields a sensitivity of 86.6\% and 0.84 false alarm (per hour) while SeizNet yields overall sensitivity of 95.8 \% with 0.17 false alarm. More interestingly, two channels seizNet outperforms full channel BPsvm with a sensitivity of 93.3\% and 0.58 false alarm. We further investigate interpretability of SeizNet by decoding the filters learned along convolutional layers. Seizure-like characteristics can be clearly observed in the filters from third and forth convolutional layers.



### Unsupervised Visual Feature Learning with Spike-timing-dependent Plasticity: How Far are we from Traditional Feature Learning Approaches?
- **Arxiv ID**: http://arxiv.org/abs/1901.04392v2
- **DOI**: 10.1016/j.patcog.2019.04.016
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1901.04392v2)
- **Published**: 2019-01-14 16:42:30+00:00
- **Updated**: 2019-04-05 11:04:24+00:00
- **Authors**: Pierre Falez, Pierre Tirilly, Ioan Marius Bilasco, Philippe Devienne, Pierre Boulet
- **Comment**: None
- **Journal**: None
- **Summary**: Spiking neural networks (SNNs) equipped with latency coding and spike-timing dependent plasticity rules offer an alternative to solve the data and energy bottlenecks of standard computer vision approaches: they can learn visual features without supervision and can be implemented by ultra-low power hardware architectures. However, their performance in image classification has never been evaluated on recent image datasets. In this paper, we compare SNNs to auto-encoders on three visual recognition datasets, and extend the use of SNNs to color images. The analysis of the results helps us identify some bottlenecks of SNNs: the limits of on-center/off-center coding, especially for color images, and the ineffectiveness of current inhibition mechanisms. These issues should be addressed to build effective SNNs for image recognition.



### Quadratization in discrete optimization and quantum mechanics
- **Arxiv ID**: http://arxiv.org/abs/1901.04405v2
- **DOI**: None
- **Categories**: **quant-ph**, cs.CV, cs.DM, math.OC, physics.chem-ph, 05C50, 11A41, 11A51, 11N35, 11N36, 11N80, 11Y05, 65K10, 65P10,
  65Y20, 68Q12, 81P68, 81P94, 94A60, 81-08, B.2.4; B.8.2; C.1.3; C.1.m; F.2.1; F.2.3; F.4.1; G.1.0; G.1.3;
  G.1.5; G.1.6; G.2.0; G.2.1; I.1.2; I.6.4; C.4; E.3; G.0; J.2; K.2
- **Links**: [PDF](http://arxiv.org/pdf/1901.04405v2)
- **Published**: 2019-01-14 17:06:40+00:00
- **Updated**: 2019-09-23 04:47:42+00:00
- **Authors**: Nike Dattani
- **Comment**: Contributors (in order of lines contributed on GitHub): Nike Dattani,
  Richard Tanburn, Andreas Soteriou, Nicholas Chancellor, Szilard Szalay,
  Elisabeth Rodriguez-Heck, Hou Tin Chau, Ka Wa Yip, Yudong Cao
- **Journal**: None
- **Summary**: A book about turning high-degree optimization problems into quadratic optimization problems that maintain the same global minimum (ground state). This book explores quadratizations for pseudo-Boolean optimization, perturbative gadgets used in QMA completeness theorems, and also non-perturbative k-local to 2-local transformations used for quantum mechanics, quantum annealing and universal adiabatic quantum computing. The book contains ~70 different Hamiltonian transformations, each of them on a separate page, where the cost (in number of auxiliary binary variables or auxiliary qubits, or number of sub-modular terms, or in graph connectivity, etc.), pros, cons, examples, and references are given. One can therefore look up a quadratization appropriate for the specific term(s) that need to be quadratized, much like using an integral table to look up the integral that needs to be done. This book is therefore useful for writing compilers to transform general optimization problems, into a form that quantum annealing or universal adiabatic quantum computing hardware requires; or for transforming quantum chemistry problems written in the Jordan-Wigner or Bravyi-Kitaev form, into a form where all multi-qubit interactions become 2-qubit pairwise interactions, without changing the desired ground state. Applications cited include computer vision problems (e.g. image de-noising, un-blurring, etc.), number theory (e.g. integer factoring), graph theory (e.g. Ramsey number determination), and quantum chemistry. The book is open source, and anyone can make modifications here: https://github.com/HPQC-LABS/Book_About_Quadratization.



### River Ice Segmentation with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1901.04412v2
- **DOI**: 10.1109/TGRS.2020.2981082
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.04412v2)
- **Published**: 2019-01-14 17:16:14+00:00
- **Updated**: 2019-06-12 19:19:33+00:00
- **Authors**: Abhineet Singh, Hayden Kalke, Mark Loewen, Nilanjan Ray
- **Comment**: supplementary:
  http://webdocs.cs.ualberta.ca/~vis/asingh1/docs/river_ice_segm_supp.pdf
- **Journal**: None
- **Summary**: This paper deals with the problem of computing surface ice concentration for two different types of ice from digital images of river surface. It presents the results of attempting to solve this problem using several state of the art semantic segmentation methods based on deep convolutional neural networks (CNNs). This task presents two main challenges - very limited availability of labeled training data and presence of noisy labels due to the great difficulty of visually distinguishing between the two types of ice, even for human experts. The results are used to analyze the extent to which some of the best deep learning methods currently in existence can handle these challenges. The code and data used in the experiments are made publicly available to facilitate further work in this domain.



### CrossNet: Latent Cross-Consistency for Unpaired Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1901.04530v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.04530v2)
- **Published**: 2019-01-14 19:31:58+00:00
- **Updated**: 2019-05-26 19:47:34+00:00
- **Authors**: Omry Sendik, Dani Lischinski, Daniel Cohen-Or
- **Comment**: None
- **Journal**: None
- **Summary**: Recent GAN-based architectures have been able to deliver impressive performance on the general task of image-to-image translation. In particular, it was shown that a wide variety of image translation operators may be learned from two image sets, containing images from two different domains, without establishing an explicit pairing between the images. This was made possible by introducing clever regularizers to overcome the under-constrained nature of the unpaired translation problem. In this work, we introduce a novel architecture for unpaired image translation, and explore several new regularizers enabled by it. Specifically, our architecture comprises a pair of GANs, as well as a pair of translators between their respective latent spaces. These cross-translators enable us to impose several regularizing constraints on the learnt image translation operator, collectively referred to as latent cross-consistency. Our results show that our proposed architecture and latent cross-consistency constraints are able to outperform the existing state-of-the-art on a variety of image translation tasks.



### Assessment of central serous chorioretinopathy (CSC) depicted on color fundus photographs using deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1901.04540v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.04540v1)
- **Published**: 2019-01-14 19:56:22+00:00
- **Updated**: 2019-01-14 19:56:22+00:00
- **Authors**: Yi Zhen, Hang Chen, Xu Zhang, Meng Liu, Xin Meng, Jian Zhang, Jiantao Pu
- **Comment**: 4 figure
- **Journal**: None
- **Summary**: To investigate whether and to what extent central serous chorioretinopathy (CSC) depicted on color fundus photographs can be assessed using deep learning technology. We collected a total of 2,504 fundus images acquired on different subjects. We verified the CSC status of these images using their corresponding optical coherence tomography (OCT) images. A total of 1,329 images depicted CSC. These images were preprocessed and normalized. This resulting dataset was randomly split into three parts in the ratio of 8:1:1 respectively for training, validation, and testing purposes. We used the deep learning architecture termed InceptionV3 to train the classifier. We performed nonparametric receiver operating characteristic (ROC) analyses to assess the capability of the developed algorithm to identify CSC. The Kappa coefficient between the two raters was 0.48 (p < 0.001), while the Kappa coefficients between the computer and the two raters were 0.59 (p < 0.001) and 0.33 (p < 0.05).Our experiments showed that the computer algorithm based on deep learning can assess CSC depicted on color fundus photographs in a relatively reliable and consistent way.



### Self-Supervised Deep Active Accelerated MRI
- **Arxiv ID**: http://arxiv.org/abs/1901.04547v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.04547v1)
- **Published**: 2019-01-14 20:13:00+00:00
- **Updated**: 2019-01-14 20:13:00+00:00
- **Authors**: Kyong Hwan Jin, Michael Unser, Kwang Moo Yi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose to simultaneously learn to sample and reconstruct magnetic resonance images (MRI) to maximize the reconstruction quality given a limited sample budget, in a self-supervised setup. Unlike existing deep methods that focus only on reconstructing given data, thus being passive, we go beyond the current state of the art by considering both the data acquisition and the reconstruction process within a single deep-learning framework. As our network learns to acquire data, the network is active in nature. In order to do so, we simultaneously train two neural networks, one dedicated to reconstruction and the other to progressive sampling, each with an automatically generated supervision signal that links them together. The two supervision signals are created through Monte Carlo tree search (MCTS). MCTS returns a better sampling pattern than what the current sampling network can give and, thus, a better final reconstruction. The sampling network is trained to mimic the MCTS results using the previous sampling network, thus being enhanced. The reconstruction network is trained to give the highest reconstruction quality, given the MCTS sampling pattern. Through this framework, we are able to train the two networks without providing any direct supervision on sampling.



### AET vs. AED: Unsupervised Representation Learning by Auto-Encoding Transformations rather than Data
- **Arxiv ID**: http://arxiv.org/abs/1901.04596v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.04596v2)
- **Published**: 2019-01-14 22:45:21+00:00
- **Updated**: 2019-03-23 22:10:45+00:00
- **Authors**: Liheng Zhang, Guo-Jun Qi, Liqiang Wang, Jiebo Luo
- **Comment**: in Proceedings of IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR 2019), Long Beach, CA, June 16th - June 20th, 2019
- **Journal**: None
- **Summary**: The success of deep neural networks often relies on a large amount of labeled examples, which can be difficult to obtain in many real scenarios. To address this challenge, unsupervised methods are strongly preferred for training neural networks without using any labeled data. In this paper, we present a novel paradigm of unsupervised representation learning by Auto-Encoding Transformation (AET) in contrast to the conventional Auto-Encoding Data (AED) approach. Given a randomly sampled transformation, AET seeks to predict it merely from the encoded features as accurately as possible at the output end. The idea is the following: as long as the unsupervised features successfully encode the essential information about the visual structures of original and transformed images, the transformation can be well predicted. We will show that this AET paradigm allows us to instantiate a large variety of transformations, from parameterized, to non-parameterized and GAN-induced ones. Our experiments show that AET greatly improves over existing unsupervised approaches, setting new state-of-the-art performances being greatly closer to the upper bounds by their fully supervised counterparts on CIFAR-10, ImageNet and Places datasets.



### Dual Generator Generative Adversarial Networks for Multi-Domain Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1901.04604v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.04604v1)
- **Published**: 2019-01-14 23:25:41+00:00
- **Updated**: 2019-01-14 23:25:41+00:00
- **Authors**: Hao Tang, Dan Xu, Wei Wang, Yan Yan, Nicu Sebe
- **Comment**: 16 pages, 7 figures, accepted to ACCV 2018
- **Journal**: None
- **Summary**: State-of-the-art methods for image-to-image translation with Generative Adversarial Networks (GANs) can learn a mapping from one domain to another domain using unpaired image data. However, these methods require the training of one specific model for every pair of image domains, which limits the scalability in dealing with more than two image domains. In addition, the training stage of these methods has the common problem of model collapse that degrades the quality of the generated images. To tackle these issues, we propose a Dual Generator Generative Adversarial Network (G$^2$GAN), which is a robust and scalable approach allowing to perform unpaired image-to-image translation for multiple domains using only dual generators within a single model. Moreover, we explore different optimization losses for better training of G$^2$GAN, and thus make unpaired image-to-image translation with higher consistency and better stability. Extensive experiments on six publicly available datasets with different scenarios, i.e., architectural buildings, seasons, landscape and human faces, demonstrate that the proposed G$^2$GAN achieves superior model capacity and better generation performance comparing with existing image-to-image translation GAN models.



