# Arxiv Papers in cs.CV on 2019-01-11
### Mono3D++: Monocular 3D Vehicle Detection with Two-Scale 3D Hypotheses and Task Priors
- **Arxiv ID**: http://arxiv.org/abs/1901.03446v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03446v1)
- **Published**: 2019-01-11 01:21:10+00:00
- **Updated**: 2019-01-11 01:21:10+00:00
- **Authors**: Tong He, Stefano Soatto
- **Comment**: Proc. of the AAAI, September 2018
- **Journal**: None
- **Summary**: We present a method to infer 3D pose and shape of vehicles from a single image. To tackle this ill-posed problem, we optimize two-scale projection consistency between the generated 3D hypotheses and their 2D pseudo-measurements. Specifically, we use a morphable wireframe model to generate a fine-scaled representation of vehicle shape and pose. To reduce its sensitivity to 2D landmarks, we jointly model the 3D bounding box as a coarse representation which improves robustness. We also integrate three task priors, including unsupervised monocular depth, a ground plane constraint as well as vehicle shape priors, with forward projection errors into an overall energy function.



### Texture Mixer: A Network for Controllable Synthesis and Interpolation of Texture
- **Arxiv ID**: http://arxiv.org/abs/1901.03447v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03447v2)
- **Published**: 2019-01-11 01:21:12+00:00
- **Updated**: 2019-04-16 09:22:11+00:00
- **Authors**: Ning Yu, Connelly Barnes, Eli Shechtman, Sohrab Amirghodsi, Michal Lukac
- **Comment**: Accepted to CVPR'19
- **Journal**: None
- **Summary**: This paper addresses the problem of interpolating visual textures. We formulate this problem by requiring (1) by-example controllability and (2) realistic and smooth interpolation among an arbitrary number of texture samples. To solve it we propose a neural network trained simultaneously on a reconstruction task and a generation task, which can project texture examples onto a latent space where they can be linearly interpolated and projected back onto the image domain, thus ensuring both intuitive control and realistic results. We show our method outperforms a number of baselines according to a comprehensive suite of metrics as well as a user study. We further show several applications based on our technique, which include texture brush, texture dissolve, and animal hybridization.



### DMC-Net: Generating Discriminative Motion Cues for Fast Compressed Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1901.03460v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03460v3)
- **Published**: 2019-01-11 02:39:41+00:00
- **Updated**: 2019-05-07 20:51:27+00:00
- **Authors**: Zheng Shou, Xudong Lin, Yannis Kalantidis, Laura Sevilla-Lara, Marcus Rohrbach, Shih-Fu Chang, Zhicheng Yan
- **Comment**: Accepted by CVPR'19
- **Journal**: None
- **Summary**: Motion has shown to be useful for video understanding, where motion is typically represented by optical flow. However, computing flow from video frames is very time-consuming. Recent works directly leverage the motion vectors and residuals readily available in the compressed video to represent motion at no cost. While this avoids flow computation, it also hurts accuracy since the motion vector is noisy and has substantially reduced resolution, which makes it a less discriminative motion representation. To remedy these issues, we propose a lightweight generator network, which reduces noises in motion vectors and captures fine motion details, achieving a more Discriminative Motion Cue (DMC) representation. Since optical flow is a more accurate motion representation, we train the DMC generator to approximate flow using a reconstruction loss and a generative adversarial loss, jointly with the downstream action classification task. Extensive evaluations on three action recognition benchmarks (HMDB-51, UCF-101, and a subset of Kinetics) confirm the effectiveness of our method. Our full system, consisting of the generator and the classifier, is coined as DMC-Net which obtains high accuracy close to that of using flow and runs two orders of magnitude faster than using optical flow at inference time.



### Analyzing Periodicity and Saliency for Adult Video Detection
- **Arxiv ID**: http://arxiv.org/abs/1901.03462v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03462v1)
- **Published**: 2019-01-11 02:50:29+00:00
- **Updated**: 2019-01-11 02:50:29+00:00
- **Authors**: Yizhi Liu, Xiaoyan Gu, Lei Huang, Junlin Ouyang, Miao Liao, Liangran Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Content-based adult video detection plays an important role in preventing pornography. However, existing methods usually rely on single modality and seldom focus on multi-modality semantics representation. Addressing at this problem, we put forward an approach of analyzing periodicity and saliency for adult video detection. At first, periodic patterns and salient regions are respective-ly analyzed in audio-frames and visual-frames. Next, the multi-modal co-occurrence semantics is described by combining audio periodicity with visual saliency. Moreover, the performance of our approach is evaluated step by step. Experimental results show that our approach obviously outper-forms some state-of-the-art methods.



### Hand Segmentation and Fingertip Tracking from Depth Camera Images Using Deep Convolutional Neural Network and Multi-task SegNet
- **Arxiv ID**: http://arxiv.org/abs/1901.03465v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03465v3)
- **Published**: 2019-01-11 03:06:19+00:00
- **Updated**: 2020-03-11 15:17:14+00:00
- **Authors**: Duong Hai Nguyen, Tai Nhu Do, In-Seop Na, Soo-Hyung Kim
- **Comment**: 7 pages, 10 figures
- **Journal**: None
- **Summary**: Hand segmentation and fingertip detection play an indispensable role in hand gesture-based human-machine interaction systems. In this study, we propose a method to discriminate hand components and to locate fingertips in RGB-D images. The system consists of three main steps: hand detection using RGB images providing regions which are considered as promising areas for further processing, hand segmentation, and fingertip detection using depth image and our modified SegNet, a single lightweight architecture that can process two independent tasks at the same time. The experimental results show that our system is a promising method for hand segmentation and fingertip detection which achieves a comparable performance while model complexity is suitable for real-time applications.



### Color Recognition for Rubik's Cube Robot
- **Arxiv ID**: http://arxiv.org/abs/1901.03470v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03470v1)
- **Published**: 2019-01-11 04:11:45+00:00
- **Updated**: 2019-01-11 04:11:45+00:00
- **Authors**: Shenglan Liu, Dong Jiang, Lin Feng, Feilong Wang, Zhanbo Feng, Xiang Liu, Shuai Guo, Bingjun Li, Yuchen Cong
- **Comment**: 6 pages, 6 figures, uses spconf.sty
- **Journal**: None
- **Summary**: In this paper, we proposed three methods to solve color recognition of Rubik's cube, which includes one offline method and two online methods. Scatter balance \& extreme learning machine (SB-ELM), a offline method, is proposed to illustrate the efficiency of training based method. We also point out the conception of color drifting which indicates offline methods are always ineffectiveness and can not work well in continuous change circumstance. By contrast, dynamic weight label propagation is proposed for labeling blocks color by known center blocks color of Rubik's cube. Furthermore, weak label hierarchic propagation, another online method, is also proposed for unknown all color information but only utilizes weak label of center block in color recognition. We finally design a Rubik's cube robot and construct a dataset to illustrate the efficiency and effectiveness of our online methods and to indicate the ineffectiveness of offline method by color drifting in our dataset.



### Segmentation of Levator Hiatus Using Multi-Scale Local Region Active contours and Boundary Shape Similarity Constraint
- **Arxiv ID**: http://arxiv.org/abs/1901.03472v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03472v1)
- **Published**: 2019-01-11 04:15:09+00:00
- **Updated**: 2019-01-11 04:15:09+00:00
- **Authors**: Xinling Zhang, Xu Li, Ying Chen, Yixin Gan, Dexing Kong, Rongqin Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a multi-scale framework with local region based active contour and boundary shape similarity constraint is proposed for the segmentation of levator hiatus in ultrasound images. In this paper, we proposed a multiscale active contour framework to segment levator hiatus ultrasound images by combining the local region information and boundary shape similarity constraint. In order to get more precisely initializations and reduce the computational cost, Gaussian pyramid method is used to decompose the image into coarse-to-fine scales. A localized region active contour model is firstly performed on the coarsest scale image to get a rough contour of the levator hiatus, then the segmentation result on the coarse scale is interpolated into the finer scale image as the initialization. The boundary shape similarity between different scales is incorporate into the local region based active contour model so that the result from coarse scale can guide the contour evolution at finer scale. By incorporating the multi-scale and boundary shape similarity, the proposed method can precisely locate the levator hiatus boundaries despite various ultrasound image artifacts. With a data set of 90 levator hiatus ultrasound images, the efficiency and accuracy of the proposed method are validated by quantitative and qualitative evaluations (TP, FP, Js) and comparison with other two state-of-art active contour segmentation methods (C-V model, DRLSE model).



### LGAN: Lung Segmentation in CT Scans Using Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1901.03473v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03473v1)
- **Published**: 2019-01-11 04:26:44+00:00
- **Updated**: 2019-01-11 04:26:44+00:00
- **Authors**: Jiaxing Tan, Longlong Jing, Yumei Huo, Yingli Tian, Oguz Akin
- **Comment**: None
- **Journal**: None
- **Summary**: Lung segmentation in computerized tomography (CT) images is an important procedure in various lung disease diagnosis. Most of the current lung segmentation approaches are performed through a series of procedures with manually empirical parameter adjustments in each step. Pursuing an automatic segmentation method with fewer steps, in this paper, we propose a novel deep learning Generative Adversarial Network (GAN) based lung segmentation schema, which we denote as LGAN. Our proposed schema can be generalized to different kinds of neural networks for lung segmentation in CT images and is evaluated on a dataset containing 220 individual CT scans with two metrics: segmentation quality and shape similarity. Also, we compared our work with current state of the art methods. The results obtained with this study demonstrate that the proposed LGAN schema can be used as a promising tool for automatic lung segmentation due to its simplified procedure as well as its good performance.



### FishNet: A Versatile Backbone for Image, Region, and Pixel Level Prediction
- **Arxiv ID**: http://arxiv.org/abs/1901.03495v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03495v1)
- **Published**: 2019-01-11 06:43:56+00:00
- **Updated**: 2019-01-11 06:43:56+00:00
- **Authors**: Shuyang Sun, Jiangmiao Pang, Jianping Shi, Shuai Yi, Wanli Ouyang
- **Comment**: NeurIPS 2018. Code available at https://github.com/kevin-ssy/FishNet
- **Journal**: None
- **Summary**: The basic principles in designing convolutional neural network (CNN) structures for predicting objects on different levels, e.g., image-level, region-level, and pixel-level are diverging. Generally, network structures designed specifically for image classification are directly used as default backbone structure for other tasks including detection and segmentation, but there is seldom backbone structure designed under the consideration of unifying the advantages of networks designed for pixel-level or region-level predicting tasks, which may require very deep features with high resolution. Towards this goal, we design a fish-like network, called FishNet. In FishNet, the information of all resolutions is preserved and refined for the final task. Besides, we observe that existing works still cannot \emph{directly} propagate the gradient information from deep layers to shallow layers. Our design can better handle this problem. Extensive experiments have been conducted to demonstrate the remarkable performance of the FishNet. In particular, on ImageNet-1k, the accuracy of FishNet is able to surpass the performance of DenseNet and ResNet with fewer parameters. FishNet was applied as one of the modules in the winning entry of the COCO Detection 2018 challenge. The code is available at https://github.com/kevin-ssy/FishNet.



### Disease Knowledge Transfer across Neurodegenerative Diseases
- **Arxiv ID**: http://arxiv.org/abs/1901.03517v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, q-bio.QM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.03517v2)
- **Published**: 2019-01-11 09:11:27+00:00
- **Updated**: 2019-07-29 14:25:07+00:00
- **Authors**: Razvan V. Marinescu, Marco Lorenzi, Stefano B. Blumberg, Alexandra L. Young, Pere P. Morell, Neil P. Oxtoby, Arman Eshaghi, Keir X. Yong, Sebastian J. Crutch, Polina Golland, Daniel C. Alexander
- **Comment**: accepted at MICCAI 2019, 13 pages, 5 figures, 2 tables
- **Journal**: Medical Image Computing and Computer Assisted Intervention 2019
- **Summary**: We introduce Disease Knowledge Transfer (DKT), a novel technique for transferring biomarker information between related neurodegenerative diseases. DKT infers robust multimodal biomarker trajectories in rare neurodegenerative diseases even when only limited, unimodal data is available, by transferring information from larger multimodal datasets from common neurodegenerative diseases. DKT is a joint-disease generative model of biomarker progressions, which exploits biomarker relationships that are shared across diseases. Our proposed method allows, for the first time, the estimation of plausible, multimodal biomarker trajectories in Posterior Cortical Atrophy (PCA), a rare neurodegenerative disease where only unimodal MRI data is available. For this we train DKT on a combined dataset containing subjects with two distinct diseases and sizes of data available: 1) a larger, multimodal typical AD (tAD) dataset from the TADPOLE Challenge, and 2) a smaller unimodal Posterior Cortical Atrophy (PCA) dataset from the Dementia Research Centre (DRC), for which only a limited number of Magnetic Resonance Imaging (MRI) scans are available. Although validation is challenging due to lack of data in PCA, we validate DKT on synthetic data and two patient datasets (TADPOLE and PCA cohorts), showing it can estimate the ground truth parameters in the simulation and predict unseen biomarkers on the two patient datasets. While we demonstrated DKT on Alzheimer's variants, we note DKT is generalisable to other forms of related neurodegenerative diseases. Source code for DKT is available online: https://github.com/mrazvan22/dkt.



### Retrieving Similar E-Commerce Images Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1901.03546v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03546v1)
- **Published**: 2019-01-11 10:48:48+00:00
- **Updated**: 2019-01-11 10:48:48+00:00
- **Authors**: Rishab Sharma, Anirudha Vishvakarma
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: In this paper, we propose a deep convolutional neural network for learning the embeddings of images in order to capture the notion of visual similarity. We present a deep siamese architecture that when trained on positive and negative pairs of images learn an embedding that accurately approximates the ranking of images in order of visual similarity notion. We also implement a novel loss calculation method using an angular loss metrics based on the problems requirement. The final embedding of the image is combined representation of the lower and top-level embeddings. We used fractional distance matrix to calculate the distance between the learned embeddings in n-dimensional space. In the end, we compare our architecture with other existing deep architecture and go on to demonstrate the superiority of our solution in terms of image retrieval by testing the architecture on four datasets. We also show how our suggested network is better than the other traditional deep CNNs used for capturing fine-grained image similarities by learning an optimum embedding.



### Feature Fusion for Robust Patch Matching With Compact Binary Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1901.03547v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.03547v1)
- **Published**: 2019-01-11 10:52:54+00:00
- **Updated**: 2019-01-11 10:52:54+00:00
- **Authors**: Andrea Migliorati, Attilio Fiandrotti, Gianluca Francini, Skjalg Lepsoy, Riccardo Leonardi
- **Comment**: MMSP 2018 - IEEE 20th International Workshop on Multimedia Signal
  Processing - August 29-31 2018, Vancouver, Canada
- **Journal**: None
- **Summary**: This work addresses the problem of learning compact yet discriminative patch descriptors within a deep learning framework. We observe that features extracted by convolutional layers in the pixel domain are largely complementary to features extracted in a transformed domain. We propose a convolutional network framework for learning binary patch descriptors where pixel domain features are fused with features extracted from the transformed domain. In our framework, while convolutional and transformed features are distinctly extracted, they are fused and provided to a single classifier which thus jointly operates on convolutional and transformed features. We experiment at matching patches from three different datasets, showing that our feature fusion approach outperforms multiple state-of-the-art approaches in terms of accuracy, rate, and complexity.



### DIVE: A spatiotemporal progression model of brain pathology in neurodegenerative disorders
- **Arxiv ID**: http://arxiv.org/abs/1901.03553v1
- **DOI**: 10.1016/j.neuroimage.2019.02.053
- **Categories**: **cs.CV**, cs.LG, q-bio.NC, q-bio.QM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.03553v1)
- **Published**: 2019-01-11 11:13:44+00:00
- **Updated**: 2019-01-11 11:13:44+00:00
- **Authors**: Razvan V. Marinescu, Arman Eshaghi, Marco Lorenzi, Alexandra L. Young, Neil P. Oxtoby, Sara Garbarino, Sebastian J. Crutch, Daniel C. Alexander
- **Comment**: 24 pages, 5 figures, 2 tables, 1 algorithm
- **Journal**: NeuroImage, Volume 192, 15 May 2019, Pages 166-177
- **Summary**: Here we present DIVE: Data-driven Inference of Vertexwise Evolution. DIVE is an image-based disease progression model with single-vertex resolution, designed to reconstruct long-term patterns of brain pathology from short-term longitudinal data sets. DIVE clusters vertex-wise biomarker measurements on the cortical surface that have similar temporal dynamics across a patient population, and concurrently estimates an average trajectory of vertex measurements in each cluster. DIVE uniquely outputs a parcellation of the cortex into areas with common progression patterns, leading to a new signature for individual diseases. DIVE further estimates the disease stage and progression speed for every visit of every subject, potentially enhancing stratification for clinical trials or management. On simulated data, DIVE can recover ground truth clusters and their underlying trajectory, provided the average trajectories are sufficiently different between clusters. We demonstrate DIVE on data from two cohorts: the Alzheimer's Disease Neuroimaging Initiative (ADNI) and the Dementia Research Centre (DRC), UK, containing patients with Posterior Cortical Atrophy (PCA) as well as typical Alzheimer's disease (tAD). DIVE finds similar spatial patterns of atrophy for tAD subjects in the two independent datasets (ADNI and DRC), and further reveals distinct patterns of pathology in different diseases (tAD vs PCA) and for distinct types of biomarker data: cortical thickness from Magnetic Resonance Imaging (MRI) vs amyloid load from Positron Emission Tomography (PET). Finally, DIVE can be used to estimate a fine-grained spatial distribution of pathology in the brain using any kind of voxelwise or vertexwise measures including Jacobian compression maps, fractional anisotropy (FA) maps from diffusion imaging or other PET measures. DIVE source code is available online: https://github.com/mrazvan22/dive



### CSGAN: Cyclic-Synthesized Generative Adversarial Networks for Image-to-Image Transformation
- **Arxiv ID**: http://arxiv.org/abs/1901.03554v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03554v1)
- **Published**: 2019-01-11 11:17:34+00:00
- **Updated**: 2019-01-11 11:17:34+00:00
- **Authors**: Kishan Babu Kancharagunta, Shiv Ram Dubey
- **Comment**: None
- **Journal**: None
- **Summary**: The primary motivation of Image-to-Image Transformation is to convert an image of one domain to another domain. Most of the research has been focused on the task of image transformation for a set of pre-defined domains. Very few works are reported that actually developed a common framework for image-to-image transformation for different domains. With the introduction of Generative Adversarial Networks (GANs) as a general framework for the image generation problem, there is a tremendous growth in the area of image-to-image transformation. Most of the research focuses over the suitable objective function for image-to-image transformation. In this paper, we propose a new Cyclic-Synthesized Generative Adversarial Networks (CSGAN) for image-to-image transformation. The proposed CSGAN uses a new objective function (loss) called Cyclic-Synthesized Loss (CS) between the synthesized image of one domain and cycled image of another domain. The performance of the proposed CSGAN is evaluated on two benchmark image-to-image transformation datasets, including CUHK Face dataset and CMP Facades dataset. The results are computed using the widely used evaluation metrics such as MSE, SSIM, PSNR, and LPIPS. The experimental results of the proposed CSGAN approach are compared with the latest state-of-the-art approaches such as GAN, Pix2Pix, DualGAN, CycleGAN and PS2GAN. The proposed CSGAN technique outperforms all the methods over CUHK dataset and exhibits the promising and comparable performance over Facades dataset in terms of both qualitative and quantitative measures. The code is available at https://github.com/KishanKancharagunta/CSGAN.



### An Underwater Image Enhancement Benchmark Dataset and Beyond
- **Arxiv ID**: http://arxiv.org/abs/1901.05495v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.05495v2)
- **Published**: 2019-01-11 12:19:25+00:00
- **Updated**: 2019-11-26 10:51:11+00:00
- **Authors**: Chongyi Li, Chunle Guo, Wenqi Ren, Runmin Cong, Junhui Hou, Sam Kwong, Dacheng Tao
- **Comment**: 14 pages
- **Journal**: IEEE TRANSACTIONS ON IMAGE PROCESSING 2019
- **Summary**: Underwater image enhancement has been attracting much attention due to its significance in marine engineering and aquatic robotics. Numerous underwater image enhancement algorithms have been proposed in the last few years. However, these algorithms are mainly evaluated using either synthetic datasets or few selected real-world images. It is thus unclear how these algorithms would perform on images acquired in the wild and how we could gauge the progress in the field. To bridge this gap, we present the first comprehensive perceptual study and analysis of underwater image enhancement using large-scale real-world images. In this paper, we construct an Underwater Image Enhancement Benchmark (UIEB) including 950 real-world underwater images, 890 of which have the corresponding reference images. We treat the rest 60 underwater images which cannot obtain satisfactory reference images as challenging data. Using this dataset, we conduct a comprehensive study of the state-of-the-art underwater image enhancement algorithms qualitatively and quantitatively. In addition, we propose an underwater image enhancement network (called Water-Net) trained on this benchmark as a baseline, which indicates the generalization of the proposed UIEB for training Convolutional Neural Networks (CNNs). The benchmark evaluations and the proposed Water-Net demonstrate the performance and limitations of state-of-the-art algorithms, which shed light on future research in underwater image enhancement. The dataset and code are available at https://li-chongyi.github.io/proj_benchmark.html.



### Background Subtraction in Real Applications: Challenges, Current Models and Future Directions
- **Arxiv ID**: http://arxiv.org/abs/1901.03577v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03577v1)
- **Published**: 2019-01-11 12:55:48+00:00
- **Updated**: 2019-01-11 12:55:48+00:00
- **Authors**: T. Bouwmans, B. Garcia-Garcia
- **Comment**: Submitted to Computer Science Review
- **Journal**: None
- **Summary**: Computer vision applications based on videos often require the detection of moving objects in their first step. Background subtraction is then applied in order to separate the background and the foreground. In literature, background subtraction is surely among the most investigated field in computer vision providing a big amount of publications. Most of them concern the application of mathematical and machine learning models to be more robust to the challenges met in videos. However, the ultimate goal is that the background subtraction methods developed in research could be employed in real applications like traffic surveillance. But looking at the literature, we can remark that there is often a gap between the current methods used in real applications and the current methods in fundamental research. In addition, the videos evaluated in large-scale datasets are not exhaustive in the way that they only covered a part of the complete spectrum of the challenges met in real applications. In this context, we attempt to provide the most exhaustive survey as possible on real applications that used background subtraction in order to identify the real challenges met in practice, the current used background models and to provide future directions. Thus, challenges are investigated in terms of camera, foreground objects and environments. In addition, we identify the background models that are effectively used in these applications in order to find potential usable recent background models in terms of robustness, time and memory requirements.



### CT-GAN: Malicious Tampering of 3D Medical Imagery using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1901.03597v3
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.03597v3)
- **Published**: 2019-01-11 14:41:31+00:00
- **Updated**: 2019-06-06 10:15:36+00:00
- **Authors**: Yisroel Mirsky, Tom Mahler, Ilan Shelef, Yuval Elovici
- **Comment**: This paper is included in the Proceedings of the 28th USENIX Security
  Symposium (USENIX Security 2019)
- **Journal**: None
- **Summary**: In 2018, clinics and hospitals were hit with numerous attacks leading to significant data breaches and interruptions in medical services. An attacker with access to medical records can do much more than hold the data for ransom or sell it on the black market.   In this paper, we show how an attacker can use deep-learning to add or remove evidence of medical conditions from volumetric (3D) medical scans. An attacker may perform this act in order to stop a political candidate, sabotage research, commit insurance fraud, perform an act of terrorism, or even commit murder. We implement the attack using a 3D conditional GAN and show how the framework (CT-GAN) can be automated. Although the body is complex and 3D medical scans are very large, CT-GAN achieves realistic results which can be executed in milliseconds.   To evaluate the attack, we focused on injecting and removing lung cancer from CT scans. We show how three expert radiologists and a state-of-the-art deep learning AI are highly susceptible to the attack. We also explore the attack surface of a modern radiology network and demonstrate one attack vector: we intercepted and manipulated CT scans in an active hospital network with a covert penetration test.   Demo video: https://youtu.be/_mkRAArj-x0   Source code: https://github.com/ymirsky/CT-GAN



### Image Disentanglement and Uncooperative Re-Entanglement for High-Fidelity Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1901.03628v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03628v2)
- **Published**: 2019-01-11 16:08:21+00:00
- **Updated**: 2019-10-20 02:07:51+00:00
- **Authors**: Adam W. Harley, Shih-En Wei, Jason Saragih, Katerina Fragkiadaki
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-domain image-to-image translation should satisfy two requirements: (1) preserve the information that is common to both domains, and (2) generate convincing images covering variations that appear in the target domain. This is challenging, especially when there are no example translations available as supervision. Adversarial cycle consistency was recently proposed as a solution, with beautiful and creative results, yielding much follow-up work. However, augmented reality applications cannot readily use such techniques to provide users with compelling translations of real scenes, because the translations do not have high-fidelity constraints. In other words, current models are liable to change details that should be preserved: while re-texturing a face, they may alter the face's expression in an unpredictable way. In this paper, we introduce the problem of high-fidelity image-to-image translation, and present a method for solving it. Our main insight is that low-fidelity translations typically escape a cycle-consistency penalty, because the back-translator learns to compensate for the forward-translator's errors. We therefore introduce an optimization technique that prevents the networks from cooperating: simply train each network only when its input data is real. Prior works, in comparison, train each network with a mix of real and generated data. Experimental results show that our method accurately disentangles the factors that separate the domains, and converges to semantics-preserving translations that prior methods miss.



### A General Optimization-based Framework for Local Odometry Estimation with Multiple Sensors
- **Arxiv ID**: http://arxiv.org/abs/1901.03638v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03638v1)
- **Published**: 2019-01-11 16:31:28+00:00
- **Updated**: 2019-01-11 16:31:28+00:00
- **Authors**: Tong Qin, Jie Pan, Shaozu Cao, Shaojie Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, more and more sensors are equipped on robots to increase robustness and autonomous ability. We have seen various sensor suites equipped on different platforms, such as stereo cameras on ground vehicles, a monocular camera with an IMU (Inertial Measurement Unit) on mobile phones, and stereo cameras with an IMU on aerial robots. Although many algorithms for state estimation have been proposed in the past, they are usually applied to a single sensor or a specific sensor suite. Few of them can be employed with multiple sensor choices. In this paper, we proposed a general optimization-based framework for odometry estimation, which supports multiple sensor sets. Every sensor is treated as a general factor in our framework. Factors which share common state variables are summed together to build the optimization problem. We further demonstrate the generality with visual and inertial sensors, which form three sensor suites (stereo cameras, a monocular camera with an IMU, and stereo cameras with an IMU). We validate the performance of our system on public datasets and through real-world experiments with multiple sensors. Results are compared against other state-of-the-art algorithms. We highlight that our system is a general framework, which can easily fuse various sensors in a pose graph optimization. Our implementations are open source\footnote{https://github.com/HKUST-Aerial-Robotics/VINS-Fusion}.



### A General Optimization-based Framework for Global Pose Estimation with Multiple Sensors
- **Arxiv ID**: http://arxiv.org/abs/1901.03642v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03642v1)
- **Published**: 2019-01-11 16:41:12+00:00
- **Updated**: 2019-01-11 16:41:12+00:00
- **Authors**: Tong Qin, Shaozu Cao, Jie Pan, Shaojie Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate state estimation is a fundamental problem for autonomous robots. To achieve locally accurate and globally drift-free state estimation, multiple sensors with complementary properties are usually fused together. Local sensors (camera, IMU, LiDAR, etc) provide precise pose within a small region, while global sensors (GPS, magnetometer, barometer, etc) supply noisy but globally drift-free localization in a large-scale environment. In this paper, we propose a sensor fusion framework to fuse local states with global sensors, which achieves locally accurate and globally drift-free pose estimation. Local estimations, produced by existing VO/VIO approaches, are fused with global sensors in a pose graph optimization. Within the graph optimization, local estimations are aligned into a global coordinate. Meanwhile, the accumulated drifts are eliminated. We evaluate the performance of our system on public datasets and with real-world experiments. Results are compared against other state-of-the-art algorithms. We highlight that our system is a general framework, which can easily fuse various global sensors in a unified pose graph optimization. Our implementations are open source\footnote{https://github.com/HKUST-Aerial-Robotics/VINS-Fusion}.



### Exploring Deep Spiking Neural Networks for Automated Driving Applications
- **Arxiv ID**: http://arxiv.org/abs/1903.02080v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.02080v1)
- **Published**: 2019-01-11 18:40:42+00:00
- **Updated**: 2019-01-11 18:40:42+00:00
- **Authors**: Sambit Mohapatra, Heinrich Gotzig, Senthil Yogamani, Stefan Milz, Raoul Zollner
- **Comment**: Accepted for Oral Presentation at VISAPP 2019
- **Journal**: None
- **Summary**: Neural networks have become the standard model for various computer vision tasks in automated driving including semantic segmentation, moving object detection, depth estimation, visual odometry, etc. The main flavors of neural networks which are used commonly are convolutional (CNN) and recurrent (RNN). In spite of rapid progress in embedded processors, power consumption and cost is still a bottleneck. Spiking Neural Networks (SNNs) are gradually progressing to achieve low-power event-driven hardware architecture which has a potential for high efficiency. In this paper, we explore the role of deep spiking neural networks (SNN) for automated driving applications. We provide an overview of progress on SNN and argue how it can be a good fit for automated driving applications.



### Optical Flow augmented Semantic Segmentation networks for Automated Driving
- **Arxiv ID**: http://arxiv.org/abs/1901.07355v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.07355v1)
- **Published**: 2019-01-11 19:10:46+00:00
- **Updated**: 2019-01-11 19:10:46+00:00
- **Authors**: Hazem Rashed, Senthil Yogamani, Ahmad El-Sallab, Pavel Krizek, Mohamed El-Helw
- **Comment**: Accepted for Oral Presentation at VISAPP 2019
- **Journal**: None
- **Summary**: Motion is a dominant cue in automated driving systems. Optical flow is typically computed to detect moving objects and to estimate depth using triangulation. In this paper, our motivation is to leverage the existing dense optical flow to improve the performance of semantic segmentation. To provide a systematic study, we construct four different architectures which use RGB only, flow only, RGBF concatenated and two-stream RGB + flow. We evaluate these networks on two automotive datasets namely Virtual KITTI and Cityscapes using the state-of-the-art flow estimator FlowNet v2. We also make use of the ground truth optical flow in Virtual KITTI to serve as an ideal estimator and a standard Farneback optical flow algorithm to study the effect of noise. Using the flow ground truth in Virtual KITTI, two-stream architecture achieves the best results with an improvement of 4% IoU. As expected, there is a large improvement for moving objects like trucks, vans and cars with 38%, 28% and 6% increase in IoU. FlowNet produces an improvement of 2.4% in average IoU with larger improvement in the moving objects corresponding to 26%, 11% and 5% in trucks, vans and cars. In Cityscapes, flow augmentation provided an improvement for moving objects like motorcycle and train with an increase of 17% and 7% in IoU.



### Anticipation and next action forecasting in video: an end-to-end model with memory
- **Arxiv ID**: http://arxiv.org/abs/1901.03728v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03728v1)
- **Published**: 2019-01-11 19:47:53+00:00
- **Updated**: 2019-01-11 19:47:53+00:00
- **Authors**: Fiora Pirri, Lorenzo Mauro, Edoardo Alati, Valsamis Ntouskos, Mahdieh Izadpanahkakhk, Elham Omrani
- **Comment**: None
- **Journal**: None
- **Summary**: Action anticipation and forecasting in videos do not require a hat-trick, as far as there are signs in the context to foresee how actions are going to be deployed. Capturing these signs is hard because the context includes the past. We propose an end-to-end network for action anticipation and forecasting with memory, to both anticipate the current action and foresee the next one. Experiments on action sequence datasets show excellent results indicating that training on histories with a dynamic memory can significantly improve forecasting performance.



### The Deeper, the Better: Analysis of Person Attributes Recognition
- **Arxiv ID**: http://arxiv.org/abs/1901.03756v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IR, cs.LG, cs.MM, I.2.1; I.2.6; I.2.10; I.5.2; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1901.03756v1)
- **Published**: 2019-01-11 21:52:57+00:00
- **Updated**: 2019-01-11 21:52:57+00:00
- **Authors**: Esube Bekele, Wallace Lawson
- **Comment**: 8 pages, 34 png figures and 1 pdf figure, uses FG2019.sty, submitted
  to FG2019
- **Journal**: None
- **Summary**: In person attributes recognition, we describe a person in terms of their appearance. Typically, this includes a wide range of traits including age, gender, clothing, and footwear. Although this could be used in a wide variety of scenarios, it generally is applied to video surveillance, where attribute recognition is impacted by low resolution, and other issues such as variable pose, occlusion and shadow. Recent approaches have used deep convolutional neural networks (CNNs) to improve the accuracy in person attribute recognition. However, many of these networks are relatively shallow and it is unclear to what extent they use contextual cues to improve classification accuracy. In this paper, we propose deeper methods for person attribute recognition. Interpreting the reasons behind the classification is highly important, as it can provide insight into how the classifier is making decisions. Interpretation suggests that deeper networks generally take more contextual information into consideration, which helps improve classification accuracy and generalizability. We present experimental analysis and results for whole body attributes using the PA-100K and PETA datasets and facial attributes using the CelebA dataset.



### Residual Pyramid FCN for Robust Follicle Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1901.03760v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03760v1)
- **Published**: 2019-01-11 22:06:10+00:00
- **Updated**: 2019-01-11 22:06:10+00:00
- **Authors**: Zhewei Wang, Weizhen Cai, Charles D. Smith, Noriko Kantake, Thomas J. Rosol, Jundong Liu
- **Comment**: 5 pages; accepted to ISBI'2019
- **Journal**: None
- **Summary**: In this paper, we propose a pyramid network structure to improve the FCN-based segmentation solutions and apply it to label thyroid follicles in histology images. Our design is based on the notion that a hierarchical updating scheme, if properly implemented, can help FCNs capture the major objects, as well as structure details in an image. To this end, we devise a residual module to be mounted on consecutive network layers, through which pixel labels would be propagated from the coarsest layer towards the finest layer in a bottom-up fashion. We add five residual units along the decoding path of a modified U-Net to make our segmentation network, Res-Seg-Net. Experiments demonstrate that the multi-resolution set-up in our model is effective in producing segmentations with improved accuracy and robustness.



### Using Scene Graph Context to Improve Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1901.03762v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03762v2)
- **Published**: 2019-01-11 22:17:46+00:00
- **Updated**: 2019-01-15 19:21:33+00:00
- **Authors**: Subarna Tripathi, Anahita Bhiwandiwalla, Alexei Bastidas, Hanlin Tang
- **Comment**: arXiv admin note: text overlap with arXiv:1804.01622 by other authors
- **Journal**: None
- **Summary**: Generating realistic images from scene graphs asks neural networks to be able to reason about object relationships and compositionality. As a relatively new task, how to properly ensure the generated images comply with scene graphs or how to measure task performance remains an open question. In this paper, we propose to harness scene graph context to improve image generation from scene graphs. We introduce a scene graph context network that pools features generated by a graph convolutional neural network that are then provided to both the image generation network and the adversarial loss. With the context network, our model is trained to not only generate realistic looking images, but also to better preserve non-spatial object relationships. We also define two novel evaluation metrics, the relation score and the mean opinion relation score, for this task that directly evaluate scene graph compliance. We use both quantitative and qualitative studies to demonstrate that our pro-posed model outperforms the state-of-the-art on this challenging task.



