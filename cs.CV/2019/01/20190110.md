# Arxiv Papers in cs.CV on 2019-01-10
### Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1901.02985v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.02985v2)
- **Published**: 2019-01-10 01:05:15+00:00
- **Updated**: 2019-04-06 19:40:44+00:00
- **Authors**: Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua, Alan Yuille, Li Fei-Fei
- **Comment**: To appear in CVPR 2019 as oral. Code for Auto-DeepLab released at
  https://github.com/tensorflow/models/tree/master/research/deeplab
- **Journal**: None
- **Summary**: Recently, Neural Architecture Search (NAS) has successfully identified neural network architectures that exceed human designed ones on large-scale image classification. In this paper, we study NAS for semantic image segmentation. Existing works often focus on searching the repeatable cell structure, while hand-designing the outer network structure that controls the spatial resolution changes. This choice simplifies the search space, but becomes increasingly problematic for dense image prediction which exhibits a lot more network level architectural variations. Therefore, we propose to search the network level structure in addition to the cell level structure, which forms a hierarchical architecture search space. We present a network level search space that includes many popular designs, and develop a formulation that allows efficient gradient-based architecture search (3 P100 GPU days on Cityscapes images). We demonstrate the effectiveness of the proposed method on the challenging Cityscapes, PASCAL VOC 2012, and ADE20K datasets. Auto-DeepLab, our architecture searched specifically for semantic image segmentation, attains state-of-the-art performance without any ImageNet pretraining.



### A Multi-Object Rectified Attention Network for Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1901.03003v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03003v1)
- **Published**: 2019-01-10 02:55:52+00:00
- **Updated**: 2019-01-10 02:55:52+00:00
- **Authors**: Canjie Luo, Lianwen Jin, Zenghui Sun
- **Comment**: 9 Tables, 9 Figures. Accepted to appear in Pattern Recognition, 2019
- **Journal**: None
- **Summary**: Irregular text is widely used. However, it is considerably difficult to recognize because of its various shapes and distorted patterns. In this paper, we thus propose a multi-object rectified attention network (MORAN) for general scene text recognition. The MORAN consists of a multi-object rectification network and an attention-based sequence recognition network. The multi-object rectification network is designed for rectifying images that contain irregular text. It decreases the difficulty of recognition and enables the attention-based sequence recognition network to more easily read irregular text. It is trained in a weak supervision way, thus requiring only images and corresponding text labels. The attention-based sequence recognition network focuses on target characters and sequentially outputs the predictions. Moreover, to improve the sensitivity of the attention-based sequence recognition network, a fractional pickup method is proposed for an attention-based decoder in the training phase. With the rectification mechanism, the MORAN can read both regular and irregular scene text. Extensive experiments on various benchmarks are conducted, which show that the MORAN achieves state-of-the-art performance. The source code is available.



### Extending Adversarial Attacks and Defenses to Deep 3D Point Cloud Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1901.03006v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.03006v4)
- **Published**: 2019-01-10 03:12:07+00:00
- **Updated**: 2019-06-28 17:55:46+00:00
- **Authors**: Daniel Liu, Ronald Yu, Hao Su
- **Comment**: Abridged version accepted at the 2019 IEEE International Conference
  on Image Processing (ICIP). Source code:
  https://github.com/Daniel-Liu-c0deb0t/3D-Neural-Network-Adversarial-Attacks
- **Journal**: None
- **Summary**: 3D object classification and segmentation using deep neural networks has been extremely successful. As the problem of identifying 3D objects has many safety-critical applications, the neural networks have to be robust against adversarial changes to the input data set. There is a growing body of research on generating human-imperceptible adversarial attacks and defenses against them in the 2D image classification domain. However, 3D objects have various differences with 2D images, and this specific domain has not been rigorously studied so far.   We present a preliminary evaluation of adversarial attacks on deep 3D point cloud classifiers, namely PointNet and PointNet++, by evaluating both white-box and black-box adversarial attacks that were proposed for 2D images and extending those attacks to reduce the perceptibility of the perturbations in 3D space. We also show the high effectiveness of simple defenses against those attacks by proposing new defenses that exploit the unique structure of 3D point clouds. Finally, we attempt to explain the effectiveness of the defenses through the intrinsic structures of both the point clouds and the neural network architectures. Overall, we find that networks that process 3D point cloud data are weak to adversarial attacks, but they are also more easily defensible compared to 2D image classifiers. Our investigation will provide the groundwork for future studies on improving the robustness of deep neural networks that handle 3D data.



### Learning Continuous Face Age Progression: A Pyramid of GANs
- **Arxiv ID**: http://arxiv.org/abs/1901.07528v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07528v1)
- **Published**: 2019-01-10 05:53:35+00:00
- **Updated**: 2019-01-10 05:53:35+00:00
- **Authors**: Hongyu Yang, Di Huang, Yunhong Wang, Anil K. Jain
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1711.10352
- **Journal**: None
- **Summary**: The two underlying requirements of face age progression, i.e. aging accuracy and identity permanence, are not well studied in the literature. This paper presents a novel generative adversarial network based approach to address the issues in a coupled manner. It separately models the constraints for the intrinsic subject-specific characteristics and the age-specific facial changes with respect to the elapsed time, ensuring that the generated faces present desired aging effects while simultaneously keeping personalized properties stable. To ensure photo-realistic facial details, high-level age-specific features conveyed by the synthesized face are estimated by a pyramidal adversarial discriminator at multiple scales, which simulates the aging effects with finer details. Further, an adversarial learning scheme is introduced to simultaneously train a single generator and multiple parallel discriminators, resulting in smooth continuous face aging sequences. The proposed method is applicable even in the presence of variations in pose, expression, makeup, etc., achieving remarkably vivid aging effects. Quantitative evaluations by a COTS face recognition system demonstrate that the target age distributions are accurately recovered, and 99.88% and 99.98% age progressed faces can be correctly verified at 0.001% FAR after age transformations of approximately 28 and 23 years elapsed time on the MORPH and CACD databases, respectively. Both visual and quantitative assessments show that the approach advances the state-of-the-art.



### Multi-feature Distance Metric Learning for Non-rigid 3D Shape Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1901.03031v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.03031v1)
- **Published**: 2019-01-10 06:28:16+00:00
- **Updated**: 2019-01-10 06:28:16+00:00
- **Authors**: Huibing Wang, Haohao Li, Xianping Fu
- **Comment**: None
- **Journal**: None
- **Summary**: In the past decades, feature-learning-based 3D shape retrieval approaches have been received widespread attention in the computer graphic community. These approaches usually explored the hand-crafted distance metric or conventional distance metric learning methods to compute the similarity of the single feature. The single feature always contains onefold geometric information, which cannot characterize the 3D shapes well. Therefore, the multiple features should be used for the retrieval task to overcome the limitation of single feature and further improve the performance. However, most conventional distance metric learning methods fail to integrate the complementary information from multiple features to construct the distance metric. To address these issue, a novel multi-feature distance metric learning method for non-rigid 3D shape retrieval is presented in this study, which can make full use of the complimentary geometric information from multiple shape features by utilizing the KL-divergences. Minimizing KL-divergence between different metric of features and a common metric is a consistency constraints, which can lead the consistency shared latent feature space of the multiple features. We apply the proposed method to 3D model retrieval, and test our method on well known benchmark database. The results show that our method substantially outperforms the state-of-the-art non-rigid 3D shape retrieval methods.



### Self-Monitoring Navigation Agent via Auxiliary Progress Estimation
- **Arxiv ID**: http://arxiv.org/abs/1901.03035v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1901.03035v1)
- **Published**: 2019-01-10 06:46:50+00:00
- **Updated**: 2019-01-10 06:46:50+00:00
- **Authors**: Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib, Zsolt Kira, Richard Socher, Caiming Xiong
- **Comment**: ICLR 2019, code is available at
  https://github.com/chihyaoma/selfmonitoring-agent
- **Journal**: None
- **Summary**: The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent .



### Image Transformation can make Neural Networks more robust against Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1901.03037v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.03037v1)
- **Published**: 2019-01-10 06:53:48+00:00
- **Updated**: 2019-01-10 06:53:48+00:00
- **Authors**: Dang Duy Thang, Toshihiro Matsui
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks are being applied in many tasks related to IoT with encouraging results. For example, neural networks can precisely detect human, objects and animal via surveillance camera for security purpose. However, neural networks have been recently found vulnerable to well-designed input samples that called adversarial examples. Such issue causes neural networks to misclassify adversarial examples that are imperceptible to humans. We found giving a rotation to an adversarial example image can defeat the effect of adversarial examples. Using MNIST number images as the original images, we first generated adversarial examples to neural network recognizer, which was completely fooled by the forged examples. Then we rotated the adversarial image and gave them to the recognizer to find the recognizer to regain the correct recognition. Thus, we empirically confirmed rotation to images can protect pattern recognizer based on neural networks from adversarial example attacks.



### Hierarchy Neighborhood Discriminative Hashing for An Unified View of Single-Label and Multi-Label Image retrieval
- **Arxiv ID**: http://arxiv.org/abs/1901.03060v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.03060v2)
- **Published**: 2019-01-10 08:53:19+00:00
- **Updated**: 2019-01-11 09:17:22+00:00
- **Authors**: Lei Ma, Hongliang Li, Qingbo Wu, Fanman Meng, King Ngi Ngan
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, deep supervised hashing methods have become popular for large-scale image retrieval task. To preserve the semantic similarity notion between examples, they typically utilize the pairwise supervision or the triplet supervised information for hash learning. However, these methods usually ignore the semantic class information which can help the improvement of the semantic discriminative ability of hash codes. In this paper, we propose a novel hierarchy neighborhood discriminative hashing method. Specifically, we construct a bipartite graph to build coarse semantic neighbourhood relationship between the sub-class feature centers and the embeddings features. Moreover, we utilize the pairwise supervised information to construct the fined semantic neighbourhood relationship between embeddings features. Finally, we propose a hierarchy neighborhood discriminative hashing loss to unify the single-label and multilabel image retrieval problem with a one-stream deep neural network architecture. Experimental results on two largescale datasets demonstrate that the proposed method can outperform the state-of-the-art hashing methods.



### PVSS: A Progressive Vehicle Search System for Video Surveillance Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.03062v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03062v1)
- **Published**: 2019-01-10 09:02:08+00:00
- **Updated**: 2019-01-10 09:02:08+00:00
- **Authors**: Xinchen Liu, Wu Liu, Huadong Ma, Shuangqun Li
- **Comment**: None
- **Journal**: None
- **Summary**: This paper is focused on the task of searching for a specific vehicle that appeared in the surveillance networks. Existing methods usually assume the vehicle images are well cropped from the surveillance videos, then use visual attributes, like colors and types, or license plate numbers to match the target vehicle in the image set. However, a complete vehicle search system should consider the problems of vehicle detection, representation, indexing, storage, matching, and so on. Besides, attribute-based search cannot accurately find the same vehicle due to intra-instance changes in different cameras and the extremely uncertain environment. Moreover, the license plates may be misrecognized in surveillance scenes due to the low resolution and noise. In this paper, a Progressive Vehicle Search System, named as PVSS, is designed to solve the above problems. PVSS is constituted of three modules: the crawler, the indexer, and the searcher. The vehicle crawler aims to detect and track vehicles in surveillance videos and transfer the captured vehicle images, metadata and contextual information to the server or cloud. Then multi-grained attributes, such as the visual features and license plate fingerprints, are extracted and indexed by the vehicle indexer. At last, a query triplet with an input vehicle image, the time range, and the spatial scope is taken as the input by the vehicle searcher. The target vehicle will be searched in the database by a progressive process. Extensive experiments on the public dataset from a real surveillance network validate the effectiveness of the PVSS.



### Multi-Granularity Reasoning for Social Relation Recognition from Images
- **Arxiv ID**: http://arxiv.org/abs/1901.03067v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03067v1)
- **Published**: 2019-01-10 09:09:44+00:00
- **Updated**: 2019-01-10 09:09:44+00:00
- **Authors**: Meng Zhang, Xinchen Liu, Wu Liu, Anfu Zhou, Huadong Ma, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: Discovering social relations in images can make machines better interpret the behavior of human beings. However, automatically recognizing social relations in images is a challenging task due to the significant gap between the domains of visual content and social relation. Existing studies separately process various features such as faces expressions, body appearance, and contextual objects, thus they cannot comprehensively capture the multi-granularity semantics, such as scenes, regional cues of persons, and interactions among persons and objects. To bridge the domain gap, we propose a Multi-Granularity Reasoning framework for social relation recognition from images. The global knowledge and mid-level details are learned from the whole scene and the regions of persons and objects, respectively. Most importantly, we explore the fine-granularity pose keypoints of persons to discover the interactions among persons and objects. Specifically, the pose-guided Person-Object Graph and Person-Pose Graph are proposed to model the actions from persons to object and the interactions between paired persons, respectively. Based on the graphs, social relation reasoning is performed by graph convolutional networks. Finally, the global features and reasoned knowledge are integrated as a comprehensive representation for social relation recognition. Extensive experiments on two public datasets show the effectiveness of the proposed framework.



### New Radon Transform Based Texture Features of Handwritten Document
- **Arxiv ID**: http://arxiv.org/abs/1901.03068v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03068v1)
- **Published**: 2019-01-10 09:13:16+00:00
- **Updated**: 2019-01-10 09:13:16+00:00
- **Authors**: Rustam Latypov, Evgeni Stolov
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present some new features describing the handwritten document as a texture. These features are based on the Radon transform. All values can be obtained easily and suit for the coarse classification of documents.



### Fast GPU-Enabled Color Normalization for Digital Pathology
- **Arxiv ID**: http://arxiv.org/abs/1901.03088v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03088v1)
- **Published**: 2019-01-10 10:42:26+00:00
- **Updated**: 2019-01-10 10:42:26+00:00
- **Authors**: Goutham Ramakrishnan, Deepak Anand, Amit Sethi
- **Comment**: None
- **Journal**: None
- **Summary**: Normalizing unwanted color variations due to differences in staining processes and scanner responses has been shown to aid machine learning in computational pathology. Of the several popular techniques for color normalization, structure preserving color normalization (SPCN) is well-motivated, convincingly tested, and published with its code base. However, SPCN makes occasional errors in color basis estimation leading to artifacts such as swapping the color basis vectors between stains or giving a colored tinge to the background with no tissue. We made several algorithmic improvements to remove these artifacts. Additionally, the original SPCN code is not readily usable on gigapixel whole slide images (WSIs) due to long run times, use of proprietary software platform and libraries, and its inability to automatically handle WSIs. We completely rewrote the software such that it can automatically handle images of any size in popular WSI formats. Our software utilizes GPU-acceleration and open-source libraries that are becoming ubiquitous with the advent of deep learning. We also made several other small improvements and achieved a multifold overall speedup on gigapixel images. Our algorithm and software is usable right out-of-the-box by the computational pathology community.



### Cricket stroke extraction: Towards creation of a large-scale cricket actions dataset
- **Arxiv ID**: http://arxiv.org/abs/1901.03107v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03107v1)
- **Published**: 2019-01-10 11:35:28+00:00
- **Updated**: 2019-01-10 11:35:28+00:00
- **Authors**: Arpan Gupta, Sakthi Balan M
- **Comment**: 14 pages (excluding references), 4 figures
- **Journal**: None
- **Summary**: In this paper, we deal with the problem of temporal action localization for a large-scale untrimmed cricket videos dataset. Our action of interest for cricket videos is a cricket stroke played by a batsman, which is, usually, covered by cameras placed at the stands of the cricket ground at both ends of the cricket pitch. After applying a sequence of preprocessing steps, we have ~73 million frames for 1110 videos in the dataset at constant frame rate and resolution. The method of localization is a generalized one which applies a trained random forest model for CUTs detection(using summed up grayscale histogram difference features) and two linear SVM camera models(CAM1 and CAM2) for first frame detection, trained on HOG features of CAM1 and CAM2 video shots. CAM1 and CAM2 are assumed to be part of the cricket stroke. At the predicted boundary positions, the HOG features of the first frames are computed and a simple algorithm was used to combine the positively predicted camera shots. In order to make the process as generic as possible, we did not consider any domain specific knowledge, such as tracking or specific shape and motion features.   The detailed analysis of our methodology is provided along with the metrics used for evaluation of individual models, and the final predicted segments. We achieved a weighted mean TIoU of 0.5097 over a small sample of the test set.



### Motion Perception in Reinforcement Learning with Dynamic Objects
- **Arxiv ID**: http://arxiv.org/abs/1901.03162v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.03162v2)
- **Published**: 2019-01-10 13:59:19+00:00
- **Updated**: 2019-02-01 15:30:32+00:00
- **Authors**: Artemij Amiranashvili, Alexey Dosovitskiy, Vladlen Koltun, Thomas Brox
- **Comment**: None
- **Journal**: None
- **Summary**: In dynamic environments, learned controllers are supposed to take motion into account when selecting the action to be taken. However, in existing reinforcement learning works motion is rarely treated explicitly; it is rather assumed that the controller learns the necessary motion representation from temporal stacks of frames implicitly. In this paper, we show that for continuous control tasks learning an explicit representation of motion improves the quality of the learned controller in dynamic scenarios. We demonstrate this on common benchmark tasks (Walker, Swimmer, Hopper), on target reaching and ball catching tasks with simulated robotic arms, and on a dynamic single ball juggling task. Moreover, we find that when equipped with an appropriate network architecture, the agent can, on some tasks, learn motion features also with pure reinforcement learning, without additional supervision. Further we find that using an image difference between the current and the previous frame as an additional input leads to better results than a temporal stack of frames.



### Early recurrence enables figure border ownership
- **Arxiv ID**: http://arxiv.org/abs/1901.03201v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03201v2)
- **Published**: 2019-01-10 14:45:13+00:00
- **Updated**: 2019-05-13 01:48:59+00:00
- **Authors**: Paria Mehrani, John K. Tsotsos
- **Comment**: 31 pages, 22 figures
- **Journal**: None
- **Summary**: The face-vase illusion introduced by Rubin demonstrates how one can switch back and forth between two different interpretations depending on how the figure outlines are assigned [1]. This border ownership assignment is an important step in the perception of forms. Zhou et al. [2] found neurons in the visual cortex whose responses not only depend on the local features present in their classical receptive fields, but also on their contextual information. Various models proposed that feedback from higher ventral areas or lateral connections could provide the required contextual information. However, some studies [3, 4, 5] ruled out the plausibility of models exclusively based on lateral connections. In addition, further evidence [6] suggests that ventral feedback even from V4 is not fast enough to provide context to border ownership neurons in either V1 or V2. As a result, the border ownership assignment mechanism in the brain is a mystery yet to be solved. Here, we test with computational simulations the hypothesis that the dorsal stream provides the global information to border ownership cells in the ventral stream. Our proposed model incorporates early recurrence from the dorsal pathway as well as lateral modulations within the ventral stream. Our simulation experiments show that our model border ownership neurons, similar to their biological counterparts, exhibit different responses to figures on either side of the border.



### Region Proposal by Guided Anchoring
- **Arxiv ID**: http://arxiv.org/abs/1901.03278v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03278v2)
- **Published**: 2019-01-10 17:13:13+00:00
- **Updated**: 2019-04-12 06:25:50+00:00
- **Authors**: Jiaqi Wang, Kai Chen, Shuo Yang, Chen Change Loy, Dahua Lin
- **Comment**: CVPR 2019 camera ready
- **Journal**: None
- **Summary**: Region anchors are the cornerstone of modern object detection techniques. State-of-the-art detectors mostly rely on a dense anchoring scheme, where anchors are sampled uniformly over the spatial domain with a predefined set of scales and aspect ratios. In this paper, we revisit this foundational stage. Our study shows that it can be done much more effectively and efficiently. Specifically, we present an alternative scheme, named Guided Anchoring, which leverages semantic features to guide the anchoring. The proposed method jointly predicts the locations where the center of objects of interest are likely to exist as well as the scales and aspect ratios at different locations. On top of predicted anchor shapes, we mitigate the feature inconsistency with a feature adaption module. We also study the use of high-quality proposals to improve detection performance. The anchoring scheme can be seamlessly integrated into proposal methods and detectors. With Guided Anchoring, we achieve 9.1% higher recall on MS COCO with 90% fewer anchors than the RPN baseline. We also adopt Guided Anchoring in Fast R-CNN, Faster R-CNN and RetinaNet, respectively improving the detection mAP by 2.2%, 2.7% and 1.2%. Code will be available at https://github.com/open-mmlab/mmdetection.



### Multispectral and Hyperspectral Image Fusion by MS/HS Fusion Net
- **Arxiv ID**: http://arxiv.org/abs/1901.03281v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03281v1)
- **Published**: 2019-01-10 17:16:59+00:00
- **Updated**: 2019-01-10 17:16:59+00:00
- **Authors**: Qi Xie, Minghao Zhou, Qian Zhao, Deyu Meng, Wangmeng Zuo, Zongben Xu
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: Hyperspectral imaging can help better understand the characteristics of different materials, compared with traditional image systems. However, only high-resolution multispectral (HrMS) and low-resolution hyperspectral (LrHS) images can generally be captured at video rate in practice. In this paper, we propose a model-based deep learning approach for merging an HrMS and LrHS images to generate a high-resolution hyperspectral (HrHS) image. In specific, we construct a novel MS/HS fusion model which takes the observation models of low-resolution images and the low-rankness knowledge along the spectral mode of HrHS image into consideration. Then we design an iterative algorithm to solve the model by exploiting the proximal gradient method. And then, by unfolding the designed algorithm, we construct a deep network, called MS/HS Fusion Net, with learning the proximal operators and model parameters by convolutional neural networks. Experimental results on simulated and real data substantiate the superiority of our method both visually and quantitatively as compared with state-of-the-art methods along this line of research.



### RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free
- **Arxiv ID**: http://arxiv.org/abs/1901.03353v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03353v1)
- **Published**: 2019-01-10 19:35:28+00:00
- **Updated**: 2019-01-10 19:35:28+00:00
- **Authors**: Cheng-Yang Fu, Mykhailo Shvets, Alexander C. Berg
- **Comment**: None
- **Journal**: None
- **Summary**: Recently two-stage detectors have surged ahead of single-shot detectors in the accuracy-vs-speed trade-off. Nevertheless single-shot detectors are immensely popular in embedded vision applications. This paper brings single-shot detectors up to the same level as current two-stage techniques. We do this by improving training for the state-of-the-art single-shot detector, RetinaNet, in three ways: integrating instance mask prediction for the first time, making the loss function adaptive and more stable, and including additional hard examples in training. We call the resulting augmented network RetinaMask. The detection component of RetinaMask has the same computational cost as the original RetinaNet, but is more accurate. COCO test-dev results are up to 41.4 mAP for RetinaMask-101 vs 39.1mAP for RetinaNet-101, while the runtime is the same during evaluation. Adding Group Normalization increases the performance of RetinaMask-101 to 41.7 mAP. Code is at:https://github.com/chengyangfu/retinamask



### Unsupervised Moving Object Detection via Contextual Information Separation
- **Arxiv ID**: http://arxiv.org/abs/1901.03360v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03360v2)
- **Published**: 2019-01-10 19:58:16+00:00
- **Updated**: 2019-04-14 07:21:07+00:00
- **Authors**: Yanchao Yang, Antonio Loquercio, Davide Scaramuzza, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an adversarial contextual model for detecting moving objects in images. A deep neural network is trained to predict the optical flow in a region using information from everywhere else but that region (context), while another network attempts to make such context as uninformative as possible. The result is a model where hypotheses naturally compete with no need for explicit regularization or hyper-parameter tuning. Although our method requires no supervision whatsoever, it outperforms several methods that are pre-trained on large annotated datasets. Our model can be thought of as a generalization of classical variational generative region-based segmentation, but in a way that avoids explicit regularization or solution of partial differential equations at run-time.



### Adversarial Pseudo Healthy Synthesis Needs Pathology Factorization
- **Arxiv ID**: http://arxiv.org/abs/1901.07295v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.07295v1)
- **Published**: 2019-01-10 20:20:59+00:00
- **Updated**: 2019-01-10 20:20:59+00:00
- **Authors**: Tian Xia, Agisilaos Chartsias, Sotirios A. Tsaftaris
- **Comment**: None
- **Journal**: None
- **Summary**: Pseudo healthy synthesis, i.e. the creation of a subject-specific `healthy' image from a pathological one, could be helpful in tasks such as anomaly detection, understanding changes induced by pathology and disease or even as data augmentation. We treat this task as a factor decomposition problem: we aim to separate what appears to be healthy and where disease is (as a map). The two factors are then recombined (by a network) to reconstruct the input disease image. We train our models in an adversarial way using either paired or unpaired settings, where we pair disease images and maps (as segmentation masks) when available. We quantitatively evaluate the quality of pseudo healthy images. We show in a series of experiments, performed in ISLES and BraTS datasets, that our method is better than conditional GAN and CycleGAN, highlighting challenges in using adversarial methods in the image translation task of pseudo healthy image generation.



### Characterizing and evaluating adversarial examples for Offline Handwritten Signature Verification
- **Arxiv ID**: http://arxiv.org/abs/1901.03398v1
- **DOI**: 10.1109/TIFS.2019.2894031
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1901.03398v1)
- **Published**: 2019-01-10 21:14:11+00:00
- **Updated**: 2019-01-10 21:14:11+00:00
- **Authors**: Luiz G. Hafemann, Robert Sabourin, Luiz S. Oliveira
- **Comment**: Accepted for the IEEE Transactions on Information Forensics and
  Security
- **Journal**: None
- **Summary**: The phenomenon of Adversarial Examples is attracting increasing interest from the Machine Learning community, due to its significant impact to the security of Machine Learning systems. Adversarial examples are similar (from a perceptual notion of similarity) to samples from the data distribution, that "fool" a machine learning classifier. For computer vision applications, these are images with carefully crafted but almost imperceptible changes, that are misclassified. In this work, we characterize this phenomenon under an existing taxonomy of threats to biometric systems, in particular identifying new attacks for Offline Handwritten Signature Verification systems. We conducted an extensive set of experiments on four widely used datasets: MCYT-75, CEDAR, GPDS-160 and the Brazilian PUC-PR, considering both a CNN-based system and a system using a handcrafted feature extractor (CLBP). We found that attacks that aim to get a genuine signature rejected are easy to generate, even in a limited knowledge scenario, where the attacker does not have access to the trained classifier nor the signatures used for training. Attacks that get a forgery to be accepted are harder to produce, and often require a higher level of noise - in most cases, no longer "imperceptible" as previous findings in object recognition. We also evaluated the impact of two countermeasures on the success rate of the attacks and the amount of noise required for generating successful attacks.



### How Can We Make GAN Perform Better in Single Medical Image Super-Resolution? A Lesion Focused Multi-Scale Approach
- **Arxiv ID**: http://arxiv.org/abs/1901.03419v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.03419v1)
- **Published**: 2019-01-10 22:24:46+00:00
- **Updated**: 2019-01-10 22:24:46+00:00
- **Authors**: Jin Zhu, Guang Yang, Pietro Lio
- **Comment**: 5 pages, 4 figure, 1 table. Accepted at 2019 IEEE International
  Symposium on Biomedical Imaging (ISBI)
- **Journal**: None
- **Summary**: Single image super-resolution (SISR) is of great importance as a low-level computer vision task. The fast development of Generative Adversarial Network (GAN) based deep learning architectures realises an efficient and effective SISR to boost the spatial resolution of natural images captured by digital cameras. However, the SISR for medical images is still a very challenging problem. This is due to (1) compared to natural images, in general, medical images have lower signal to noise ratios, (2) GAN based models pre-trained on natural images may synthesise unrealistic patterns in medical images which could affect the clinical interpretation and diagnosis, and (3) the vanilla GAN architecture may suffer from unstable training and collapse mode that can also affect the SISR results. In this paper, we propose a novel lesion focused SR (LFSR) method, which incorporates GAN to achieve perceptually realistic SISR results for brain tumour MRI images. More importantly, we test and make comparison using recently developed GAN variations, e.g., Wasserstein GAN (WGAN) and WGAN with Gradient Penalty (WGAN-GP), and propose a novel multi-scale GAN (MS-GAN), to achieve a more stabilised and efficient training and improved perceptual quality of the super-resolved results. Based on both quantitative evaluations and our designed mean opinion score, the proposed LFSR coupled with MS-GAN has performed better in terms of both perceptual quality and efficiency.



