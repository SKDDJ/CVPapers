# Arxiv Papers in cs.CV on 2019-01-05
### AVA-ActiveSpeaker: An Audio-Visual Dataset for Active Speaker Detection
- **Arxiv ID**: http://arxiv.org/abs/1901.01342v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1901.01342v2)
- **Published**: 2019-01-05 00:01:06+00:00
- **Updated**: 2019-05-25 01:28:15+00:00
- **Authors**: Joseph Roth, Sourish Chaudhuri, Ondrej Klejch, Radhika Marvin, Andrew Gallagher, Liat Kaver, Sharadh Ramaswamy, Arkadiusz Stopczynski, Cordelia Schmid, Zhonghua Xi, Caroline Pantofaru
- **Comment**: None
- **Journal**: None
- **Summary**: Active speaker detection is an important component in video analysis algorithms for applications such as speaker diarization, video re-targeting for meetings, speech enhancement, and human-robot interaction. The absence of a large, carefully labeled audio-visual dataset for this task has constrained algorithm evaluations with respect to data diversity, environments, and accuracy. This has made comparisons and improvements difficult. In this paper, we present the AVA Active Speaker detection dataset (AVA-ActiveSpeaker) that will be released publicly to facilitate algorithm development and enable comparisons. The dataset contains temporally labeled face tracks in video, where each face instance is labeled as speaking or not, and whether the speech is audible. This dataset contains about 3.65 million human labeled frames or about 38.5 hours of face tracks, and the corresponding audio. We also present a new audio-visual approach for active speaker detection, and analyze its performance, demonstrating both its strength and the contributions of the dataset.



### Adaptive Fusion for RGB-D Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1901.01369v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01369v2)
- **Published**: 2019-01-05 05:16:46+00:00
- **Updated**: 2019-01-08 09:05:58+00:00
- **Authors**: Ningning Wang, Xiaojin Gong
- **Comment**: None
- **Journal**: None
- **Summary**: RGB-D salient object detection aims to identify the most visually distinctive objects in a pair of color and depth images. Based upon an observation that most of the salient objects may stand out at least in one modality, this paper proposes an adaptive fusion scheme to fuse saliency predictions generated from two modalities. Specifically, we design a two-streamed convolutional neural network (CNN), each of which extracts features and predicts a saliency map from either RGB or depth modality. Then, a saliency fusion module learns a switch map that is used to adaptively fuse the predicted saliency maps. A loss function composed of saliency supervision, switch map supervision, and edge-preserving constraints is designed to make full supervision, and the entire network is trained in an end-to-end manner. Benefited from the adaptive fusion strategy and the edge-preserving constraint, our approach outperforms state-of-the-art methods on three publicly available datasets.



### Stereoscopic Dark Flash for Low-light Photography
- **Arxiv ID**: http://arxiv.org/abs/1901.01370v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01370v2)
- **Published**: 2019-01-05 05:26:27+00:00
- **Updated**: 2019-01-09 04:25:37+00:00
- **Authors**: Jian Wang, Tianfan Xue, Jonathan T. Barron, Jiawen Chen
- **Comment**: 9 pages, 8 figures
- **Journal**: None
- **Summary**: In this work, we present a camera configuration for acquiring "stereoscopic dark flash" images: a simultaneous stereo pair in which one camera is a conventional RGB sensor, but the other camera is sensitive to near-infrared and near-ultraviolet instead of R and B. When paired with a "dark" flash (i.e., one having near-infrared and near-ultraviolet light, but no visible light) this camera allows us to capture the two images in a flash/no-flash image pair at the same time, all while not disturbing any human subjects or onlookers with a dazzling visible flash. We present a hardware prototype of this camera that approximates an idealized camera, and we present an imaging procedure that let us acquire dark flash stereo pairs that closely resemble those we would get from that idealized camera. We then present a technique for fusing these stereo pairs, first by performing registration and warping, and then by using recent advances in hyperspectral image fusion and deep learning to produce a final image. Because our camera configuration and our data acquisition process allow us to capture true low-noise long exposure RGB images alongside our dark flash stereo pairs, our learned model can be trained end-to-end to produce a fused image that retains the color and tone of a real RGB image while having the low-noise properties of a flash image.



### Brain segmentation based on multi-atlas guided 3D fully convolutional network ensembles
- **Arxiv ID**: http://arxiv.org/abs/1901.01381v2
- **DOI**: 10.1016/j.patcog.2021.107904
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01381v2)
- **Published**: 2019-01-05 08:23:48+00:00
- **Updated**: 2023-08-09 21:05:56+00:00
- **Authors**: Jiong Wu, Xiaoying Tang
- **Comment**: None
- **Journal**: Pattern Recognition, 115 (2021), 107904
- **Summary**: In this study, we proposed and validated a multi-atlas guided 3D fully convolutional network (FCN) ensemble model (M-FCN) for segmenting brain regions of interest (ROIs) from structural magnetic resonance images (MRIs). One major limitation of existing state-of-the-art 3D FCN segmentation models is that they often apply image patches of fixed size throughout training and testing, which may miss some complex tissue appearance patterns of different brain ROIs. To address this limitation, we trained a 3D FCN model for each ROI using patches of adaptive size and embedded outputs of the convolutional layers in the deconvolutional layers to further capture the local and global context patterns. In addition, with an introduction of multi-atlas based guidance in M-FCN, our segmentation was generated by combining the information of images and labels, which is highly robust. To reduce over-fitting of the FCN model on the training data, we adopted an ensemble strategy in the learning procedure. Evaluation was performed on two brain MRI datasets, aiming respectively at segmenting 14 subcortical and ventricular structures and 54 brain ROIs. The segmentation results of the proposed method were compared with those of a state-of-the-art multi-atlas based segmentation method and an existing 3D FCN segmentation model. Our results suggested that the proposed method had a superior segmentation performance.



### Curriculum Model Adaptation with Synthetic and Real Data for Semantic Foggy Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/1901.01415v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01415v2)
- **Published**: 2019-01-05 13:38:53+00:00
- **Updated**: 2019-05-01 12:17:57+00:00
- **Authors**: Dengxin Dai, Christos Sakaridis, Simon Hecker, Luc Van Gool
- **Comment**: accepted by IJCV (22 pages), an extension of arXiv:1808.01265 our
  eccv18 paper
- **Journal**: None
- **Summary**: This work addresses the problem of semantic scene understanding under fog. Although marked progress has been made in semantic scene understanding, it is mainly concentrated on clear-weather scenes. Extending semantic segmentation methods to adverse weather conditions such as fog is crucial for outdoor applications. In this paper, we propose a novel method, named Curriculum Model Adaptation (CMAda), which gradually adapts a semantic segmentation model from light synthetic fog to dense real fog in multiple steps, using both labeled synthetic foggy data and unlabeled real foggy data. The method is based on the fact that the results of semantic segmentation in moderately adverse conditions (light fog) can be bootstrapped to solve the same problem in highly adverse conditions (dense fog). CMAda is extensible to other adverse conditions and provides a new paradigm for learning with synthetic data and unlabeled real data. In addition, we present three other main stand-alone contributions: 1) a novel method to add synthetic fog to real, clear-weather scenes using semantic input; 2) a new fog density estimator; 3) a novel fog densification method to densify the fog in real foggy scenes without using depth; and 4) the Foggy Zurich dataset comprising 3808 real foggy images, with pixel-level semantic annotations for 40 images under dense fog. Our experiments show that 1) our fog simulation and fog density estimator outperform their state-of-the-art counterparts with respect to the task of semantic foggy scene understanding (SFSU); 2) CMAda improves the performance of state-of-the-art models for SFSU significantly, benefiting both from our synthetic and real foggy data. The datasets and code are available at the project website.



### Forensic shoe-print identification: a brief survey
- **Arxiv ID**: http://arxiv.org/abs/1901.01431v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01431v3)
- **Published**: 2019-01-05 15:45:00+00:00
- **Updated**: 2020-12-28 15:38:17+00:00
- **Authors**: Imad Rida, Lunke Fei, Hugo Proen√ßa, Amine Nait-Ali, Abdenour Hadid
- **Comment**: None
- **Journal**: None
- **Summary**: As an advanced research topic in forensics science, automatic shoe-print identification has been extensively studied in the last two decades, since shoe marks are the clues most frequently left in a crime scene. Hence, these impressions provide a pertinent evidence for the proper progress of investigations in order to identify the potential criminals. The main goal of this survey is to provide a cohesive overview of the research carried out in forensic shoe-print identification and its basic background. Apart defining the problem and describing the phases that typically compose the processing chain of shoe-print identification, we provide a summary/comparison of the state-of-the-art approaches, in order to guide the neophyte and help to advance the research topic. This is done through introducing simple and basic taxonomies as well as summaries of the state-of-the-art performance. Lastly, we discuss the current open problems and challenges in this research topic, point out for promising directions in this field.



### Deep Convolutional Neural Networks for Imaging Data Based Survival Analysis of Rectal Cancer
- **Arxiv ID**: http://arxiv.org/abs/1901.01449v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01449v1)
- **Published**: 2019-01-05 18:09:38+00:00
- **Updated**: 2019-01-05 18:09:38+00:00
- **Authors**: Hongming Li, Pamela Boimel, James Janopaul-Naylor, Haoyu Zhong, Ying Xiao, Edgar Ben-Josef, Yong Fan
- **Comment**: Accepted by ISBI 2019
- **Journal**: None
- **Summary**: Recent radiomic studies have witnessed promising performance of deep learning techniques in learning radiomic features and fusing multimodal imaging data. Most existing deep learning based radiomic studies build predictive models in a setting of pattern classification, not appropriate for survival analysis studies where some data samples have incomplete observations. To improve existing survival analysis techniques whose performance is hinged on imaging features, we propose a deep learning method to build survival regression models by optimizing imaging features with deep convolutional neural networks (CNNs) in a proportional hazards model. To make the CNNs applicable to tumors with varied sizes, a spatial pyramid pooling strategy is adopted. Our method has been validated based on a simulated imaging dataset and a FDG-PET/CT dataset of rectal cancer patients treated for locally advanced rectal cancer. Compared with survival prediction models built upon hand-crafted radiomic features using Cox proportional hazards model and random survival forests, our method achieved competitive prediction performance.



### Early Prediction of Alzheimer's Disease Dementia Based on Baseline Hippocampal MRI and 1-Year Follow-Up Cognitive Measures Using Deep Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.01451v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01451v1)
- **Published**: 2019-01-05 18:19:45+00:00
- **Updated**: 2019-01-05 18:19:45+00:00
- **Authors**: Hongming Li, Yong Fan
- **Comment**: Accepted by ISBI 2019
- **Journal**: None
- **Summary**: Multi-modal biological, imaging, and neuropsychological markers have demonstrated promising performance for distinguishing Alzheimer's disease (AD) patients from cognitively normal elders. However, it remains difficult to early predict when and which mild cognitive impairment (MCI) individuals will convert to AD dementia. Informed by pattern classification studies which have demonstrated that pattern classifiers built on longitudinal data could achieve better classification performance than those built on cross-sectional data, we develop a deep learning model based on recurrent neural networks (RNNs) to learn informative representation and temporal dynamics of longitudinal cognitive measures of individual subjects and combine them with baseline hippocampal MRI for building a prognostic model of AD dementia progression. Experimental results on a large cohort of MCI subjects have demonstrated that the deep learning model could learn informative measures from longitudinal data for characterizing the progression of MCI subjects to AD dementia, and the prognostic model could early predict AD progression with high accuracy.



### Fully-automatic segmentation of kidneys in clinical ultrasound images using a boundary distance regression network
- **Arxiv ID**: http://arxiv.org/abs/1901.01982v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01982v1)
- **Published**: 2019-01-05 19:44:23+00:00
- **Updated**: 2019-01-05 19:44:23+00:00
- **Authors**: Shi Yin, Zhengqiang Zhang, Hongming Li, Qinmu Peng, Xinge You, Susan L. Furth, Gregory E. Tasian, Yong Fan
- **Comment**: 4 pages. arXiv admin note: substantial text overlap with
  arXiv:1811.04815
- **Journal**: None
- **Summary**: It remains challenging to automatically segment kidneys in clinical ultrasound images due to the kidneys' varied shapes and image intensity distributions, although semi-automatic methods have achieved promising performance. In this study, we developed a novel boundary distance regression deep neural network to segment the kidneys, informed by the fact that the kidney boundaries are relatively consistent across images in terms of their appearance. Particularly, we first use deep neural networks pre-trained for classification of natural images to extract high-level image features from ultrasound images, then these feature maps are used as input to learn kidney boundary distance maps using a boundary distance regression network, and finally the predicted boundary distance maps are classified as kidney pixels or non-kidney pixels using a pixel classification network in an end-to-end learning fashion. Experimental results have demonstrated that our method could effectively improve the performance of automatic kidney segmentation, significantly better than deep learning based pixel classification networks.



### Bilinear Supervised Hashing Based on 2D Image Features
- **Arxiv ID**: http://arxiv.org/abs/1901.01474v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1901.01474v1)
- **Published**: 2019-01-05 23:39:27+00:00
- **Updated**: 2019-01-05 23:39:27+00:00
- **Authors**: Yujuan Ding, Wai Kueng Wong, Zhihui Lai, Zheng Zhang
- **Comment**: Accepted by IEEE Transactions on Circuits and Systems for Video
  Technology (TCSVT)
- **Journal**: None
- **Summary**: Hashing has been recognized as an efficient representation learning method to effectively handle big data due to its low computational complexity and memory cost. Most of the existing hashing methods focus on learning the low-dimensional vectorized binary features based on the high-dimensional raw vectorized features. However, studies on how to obtain preferable binary codes from the original 2D image features for retrieval is very limited. This paper proposes a bilinear supervised discrete hashing (BSDH) method based on 2D image features which utilizes bilinear projections to binarize the image matrix features such that the intrinsic characteristics in the 2D image space are preserved in the learned binary codes. Meanwhile, the bilinear projection approximation and vectorization binary codes regression are seamlessly integrated together to formulate the final robust learning framework. Furthermore, a discrete optimization strategy is developed to alternatively update each variable for obtaining the high-quality binary codes. In addition, two 2D image features, traditional SURF-based FVLAD feature and CNN-based AlexConv5 feature are designed for further improving the performance of the proposed BSDH method. Results of extensive experiments conducted on four benchmark datasets show that the proposed BSDH method almost outperforms all competing hashing methods with different input features by different evaluation protocols.



