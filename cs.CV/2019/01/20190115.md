# Arxiv Papers in cs.CV on 2019-01-15
### Spatial Filtering Pipeline Evaluation of Cortically Coupled Computer Vision System for Rapid Serial Visual Presentation
- **Arxiv ID**: http://arxiv.org/abs/1901.04618v1
- **DOI**: 10.1080/2326263X.2019.1568821
- **Categories**: **eess.IV**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1901.04618v1)
- **Published**: 2019-01-15 00:27:40+00:00
- **Updated**: 2019-01-15 00:27:40+00:00
- **Authors**: Zhengwei Wang, Graham Healy, Alan F. Smeaton, Tomas E. Ward
- **Comment**: None
- **Journal**: None
- **Summary**: Rapid Serial Visual Presentation (RSVP) is a paradigm that supports the application of cortically coupled computer vision to rapid image search. In RSVP, images are presented to participants in a rapid serial sequence which can evoke Event-related Potentials (ERPs) detectable in their Electroencephalogram (EEG). The contemporary approach to this problem involves supervised spatial filtering techniques which are applied for the purposes of enhancing the discriminative information in the EEG data. In this paper we make two primary contributions to that field: 1) We propose a novel spatial filtering method which we call the Multiple Time Window LDA Beamformer (MTWLB) method; 2) we provide a comprehensive comparison of nine spatial filtering pipelines using three spatial filtering schemes namely, MTWLB, xDAWN, Common Spatial Pattern (CSP) and three linear classification methods Linear Discriminant Analysis (LDA), Bayesian Linear Regression (BLR) and Logistic Regression (LR). Three pipelines without spatial filtering are used as baseline comparison. The Area Under Curve (AUC) is used as an evaluation metric in this paper. The results reveal that MTWLB and xDAWN spatial filtering techniques enhance the classification performance of the pipeline but CSP does not. The results also support the conclusion that LR can be effective for RSVP based BCI if discriminative features are available.



### Whole-Slide Image Focus Quality: Automatic Assessment and Impact on AI Cancer Detection
- **Arxiv ID**: http://arxiv.org/abs/1901.04619v2
- **DOI**: 10.4103/jpi.jpi_11_19
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.04619v2)
- **Published**: 2019-01-15 00:31:35+00:00
- **Updated**: 2019-02-05 20:32:06+00:00
- **Authors**: Timo Kohlberger, Yun Liu, Melissa Moran, Po-Hsuan, Chen, Trissia Brown, Craig H. Mermel, Jason D. Hipp, Martin C. Stumpe
- **Comment**: None
- **Journal**: Pathology Informatics (2019)
- **Summary**: Digital pathology enables remote access or consults and powerful image analysis algorithms. However, the slide digitization process can create artifacts such as out-of-focus (OOF). OOF is often only detected upon careful review, potentially causing rescanning and workflow delays. Although scan-time operator screening for whole-slide OOF is feasible, manual screening for OOF affecting only parts of a slide is impractical. We developed a convolutional neural network (ConvFocus) to exhaustively localize and quantify the severity of OOF regions on digitized slides. ConvFocus was developed using our refined semi-synthetic OOF data generation process, and evaluated using real whole-slide images spanning 3 different tissue types and 3 different stain types that were digitized by two different scanners. ConvFocus's predictions were compared with pathologist-annotated focus quality grades across 514 distinct regions representing 37,700 35x35 $\mu$m image patches, and 21 digitized "z-stack" whole-slide images that contain known OOF patterns. When compared to pathologist-graded focus quality, ConvFocus achieved Spearman rank coefficients of 0.81 and 0.94 on two scanners, and reproduced the expected OOF patterns from z-stack scanning. We also evaluated the impact of OOF on the accuracy of a state-of-the-art metastatic breast cancer detector and saw a consistent decrease in performance with increasing OOF. Comprehensive whole-slide OOF categorization could enable rescans prior to pathologist review, potentially reducing the impact of digitization focus issues on the clinical workflow. We show that the algorithm trained on our semi-synthetic OOF data generalizes well to real OOF regions across tissue types, stains, and scanners. Finally, quantitative OOF maps can flag regions that might otherwise be misclassified by image analysis algorithms, preventing OOF-induced errors.



### Fast and Robust Dynamic Hand Gesture Recognition via Key Frames Extraction and Feature Fusion
- **Arxiv ID**: http://arxiv.org/abs/1901.04622v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.04622v1)
- **Published**: 2019-01-15 00:47:59+00:00
- **Updated**: 2019-01-15 00:47:59+00:00
- **Authors**: Hao Tang, Hong Liu, Wei Xiao, Nicu Sebe
- **Comment**: 11 pages, 3 figures, accepted to NeuroComputing
- **Journal**: None
- **Summary**: Gesture recognition is a hot topic in computer vision and pattern recognition, which plays a vitally important role in natural human-computer interface. Although great progress has been made recently, fast and robust hand gesture recognition remains an open problem, since the existing methods have not well balanced the performance and the efficiency simultaneously. To bridge it, this work combines image entropy and density clustering to exploit the key frames from hand gesture video for further feature extraction, which can improve the efficiency of recognition. Moreover, a feature fusion strategy is also proposed to further improve feature representation, which elevates the performance of recognition. To validate our approach in a "wild" environment, we also introduce two new datasets called HandGesture and Action3D datasets. Experiments consistently demonstrate that our strategy achieves competitive results on Northwestern University, Cambridge, HandGesture and Action3D hand gesture datasets. Our code and datasets will release at https://github.com/Ha0Tang/HandGestureRecognition.



### Multi-modal Ensemble Classification for Generalized Zero Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1901.04623v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.04623v2)
- **Published**: 2019-01-15 00:50:58+00:00
- **Updated**: 2019-02-06 03:55:33+00:00
- **Authors**: Rafael Felix, Michele Sasdelli, Ian Reid, Gustavo Carneiro
- **Comment**: 10 pages, 3 Figures, 4 Tables
- **Journal**: None
- **Summary**: Generalized zero shot learning (GZSL) is defined by a training process containing a set of visual samples from seen classes and a set of semantic samples from seen and unseen classes, while the testing process consists of the classification of visual samples from seen and unseen classes. Current approaches are based on testing processes that focus on only one of the modalities (visual or semantic), even when the training uses both modalities (mostly for regularizing the training process). This under-utilization of modalities, particularly during testing, can hinder the classification accuracy of the method. In addition, we note a scarce attention to the development of learning methods that explicitly optimize a balanced performance of seen and unseen classes. Such issue is one of the reasons behind the vastly superior classification accuracy of seen classes in GZSL methods. In this paper, we mitigate these issues by proposing a new GZSL method based on multi-modal training and testing processes, where the optimization explicitly promotes a balanced classification accuracy between seen and unseen classes. Furthermore, we explore Bayesian inference for the visual and semantic classifiers, which is another novelty of our work in the GZSL framework. Experiments show that our method holds the state of the art (SOTA) results in terms of harmonic mean (H-mean) classification between seen and unseen classes and area under the seen and unseen curve (AUSUC) on several public GZSL benchmarks.



### Resampling detection of recompressed images via dual-stream convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/1901.04637v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.04637v3)
- **Published**: 2019-01-15 02:15:26+00:00
- **Updated**: 2019-05-10 08:10:33+00:00
- **Authors**: Gang Cao, Antao Zhou, Xianglin Huang, Gege Song, Lifang Yang, Yonggui Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Resampling detection plays an important role in identifying image tampering, such as image splicing. Currently, the resampling detection is still difficult in recompressed images, which are yielded by applying resampling followed by post-JPEG compression to primary JPEG images. Except for the scenario of low quality primary compression, it remains rather challenging due to the widespread use of middle/high quality compression in imaging devices. In this paper, we propose a new convolution neural network (CNN) method to learn the resampling trace features directly from the recompressed images. To this end, a noise extraction layer based on low-order high pass filters is deployed to yield the image residual domain, which is more beneficial to extract manipulation trace features. A dual-stream CNN is presented to capture the resampling trails along different directions, where the horizontal and vertical streams are interleaved and concatenated. Lastly, the learned features are fed into Sigmoid/Softmax layer, which acts as a binary/multiple classifier for achieving the blind detection and parameter estimation of resampling, respectively. Extensive experimental results demonstrate that our proposed method could detect resampling effectively in recompressed images and outperform the state-of-the-art detectors.



### SISC: End-to-end Interpretable Discovery Radiomics-Driven Lung Cancer Prediction via Stacked Interpretable Sequencing Cells
- **Arxiv ID**: http://arxiv.org/abs/1901.04641v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.04641v1)
- **Published**: 2019-01-15 02:55:04+00:00
- **Updated**: 2019-01-15 02:55:04+00:00
- **Authors**: Vignesh Sankar, Devinder Kumar, David A. Clausi, Graham W. Taylor, Alexander Wong
- **Comment**: First two authors have equal contribution
- **Journal**: None
- **Summary**: Objective: Lung cancer is the leading cause of cancer-related death worldwide. Computer-aided diagnosis (CAD) systems have shown significant promise in recent years for facilitating the effective detection and classification of abnormal lung nodules in computed tomography (CT) scans. While hand-engineered radiomic features have been traditionally used for lung cancer prediction, there have been significant recent successes achieving state-of-the-art results in the area of discovery radiomics. Here, radiomic sequencers comprising of highly discriminative radiomic features are discovered directly from archival medical data. However, the interpretation of predictions made using such radiomic sequencers remains a challenge. Method: A novel end-to-end interpretable discovery radiomics-driven lung cancer prediction pipeline has been designed, build, and tested. The radiomic sequencer being discovered possesses a deep architecture comprised of stacked interpretable sequencing cells (SISC). Results: The SISC architecture is shown to outperform previous approaches while providing more insight in to its decision making process. Conclusion: The SISC radiomic sequencer is able to achieve state-of-the-art results in lung cancer prediction, and also offers prediction interpretability in the form of critical response maps. Significance: The critical response maps are useful for not only validating the predictions of the proposed SISC radiomic sequencer, but also provide improved radiologist-machine collaboration for effective diagnosis.



### Measuring Effectiveness of Video Advertisements
- **Arxiv ID**: http://arxiv.org/abs/1901.07366v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07366v2)
- **Published**: 2019-01-15 03:41:37+00:00
- **Updated**: 2019-01-28 20:15:03+00:00
- **Authors**: James Hahn, Adriana Kovashka
- **Comment**: 9 pages, 7 figures, 2 tables
- **Journal**: None
- **Summary**: Advertisements are unavoidable in modern society. Times Square is notorious for its incessant display of advertisements. Its popularity is worldwide and smaller cities possess miniature versions of the display, such as Pittsburgh and its digital works in Oakland on Forbes Avenue. Tokyo's Ginza district recently rose to popularity due to its upscale shops and constant onslaught of advertisements to pedestrians. Advertisements arise in other mediums as well. For example, they help popular streaming services, such as Spotify, Hulu, and Youtube TV gather significant streams of revenue to reduce the cost of monthly subscriptions for consumers. Ads provide an additional source of money for companies and entire industries to allocate resources toward alternative business motives. They are attractive to companies and nearly unavoidable for consumers. One challenge for advertisers is examining a advertisement's effectiveness or usefulness in conveying a message to their targeted demographics. Rather than constructing a single, static image of content, a video advertisement possesses hundreds of frames of data with varying scenes, actors, objects, and complexity. Therefore, measuring effectiveness of video advertisements is important to impacting a billion-dollar industry. This paper explores the combination of human-annotated features and common video processing techniques to predict effectiveness ratings of advertisements collected from Youtube. This task is seen as a binary (effective vs. non-effective), four-way, and five-way machine learning classification task. The first findings in terms of accuracy and inference on this dataset, as well as some of the first ad research, on a small dataset are presented. Accuracies of 84\%, 65\%, and 55\% are reached on the binary, four-way, and five-way tasks respectively.



### Spatiotemporal Recurrent Convolutional Networks for Recognizing Spontaneous Micro-expressions
- **Arxiv ID**: http://arxiv.org/abs/1901.04656v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.04656v1)
- **Published**: 2019-01-15 04:33:07+00:00
- **Updated**: 2019-01-15 04:33:07+00:00
- **Authors**: Zhaoqiang Xia, Xiaopeng Hong, Xingyu Gao, Xiaoyi Feng, Guoying Zhao
- **Comment**: Submitted to IEEE TMM
- **Journal**: None
- **Summary**: Recently, the recognition task of spontaneous facial micro-expressions has attracted much attention with its various real-world applications. Plenty of handcrafted or learned features have been employed for a variety of classifiers and achieved promising performances for recognizing micro-expressions. However, the micro-expression recognition is still challenging due to the subtle spatiotemporal changes of micro-expressions. To exploit the merits of deep learning, we propose a novel deep recurrent convolutional networks based micro-expression recognition approach, capturing the spatial-temporal deformations of micro-expression sequence. Specifically, the proposed deep model is constituted of several recurrent convolutional layers for extracting visual features and a classificatory layer for recognition. It is optimized by an end-to-end manner and obviates manual feature design. To handle sequential data, we exploit two types of extending the connectivity of convolutional networks across temporal domain, in which the spatiotemporal deformations are modeled in views of facial appearance and geometry separately. Besides, to overcome the shortcomings of limited and imbalanced training samples, temporal data augmentation strategies as well as a balanced loss are jointly used for our deep network. By performing the experiments on three spontaneous micro-expression datasets, we verify the effectiveness of our proposed micro-expression recognition approach compared to the state-of-the-art methods.



### The Limitations of Adversarial Training and the Blind-Spot Attack
- **Arxiv ID**: http://arxiv.org/abs/1901.04684v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CR, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.04684v1)
- **Published**: 2019-01-15 07:21:44+00:00
- **Updated**: 2019-01-15 07:21:44+00:00
- **Authors**: Huan Zhang, Hongge Chen, Zhao Song, Duane Boning, Inderjit S. Dhillon, Cho-Jui Hsieh
- **Comment**: Accepted by International Conference on Learning Representations
  (ICLR) 2019. Huan Zhang and Hongge Chen contributed equally
- **Journal**: None
- **Summary**: The adversarial training procedure proposed by Madry et al. (2018) is one of the most effective methods to defend against adversarial examples in deep neural networks (DNNs). In our paper, we shed some lights on the practicality and the hardness of adversarial training by showing that the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network. Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks. Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the "blind-spot attack", where the input images reside in "blind-spots" (low density regions) of the empirical distribution of training data but is still on the ground-truth data manifold. For MNIST, we found that these blind-spots can be easily found by simply scaling and shifting image pixel values. Most importantly, for large datasets with high dimensional and complex data manifold (CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data. Additionally, we find that blind-spots also exist on provable defenses including (Wong & Kolter, 2018) and (Sinha et al., 2018) because these trainable robustness certificates can only be practically optimized on a limited set of training data.



### URNet : User-Resizable Residual Networks with Conditional Gating Module
- **Arxiv ID**: http://arxiv.org/abs/1901.04687v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.04687v2)
- **Published**: 2019-01-15 07:26:42+00:00
- **Updated**: 2019-04-12 08:03:15+00:00
- **Authors**: Sang-ho Lee, Simyung Chang, Nojun Kwak
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Convolutional Neural Networks are widely used to process spatial scenes, but their computational cost is fixed and depends on the structure of the network used. There are methods to reduce the cost by compressing networks or varying its computational path dynamically according to the input image. However, since a user can not control the size of the learned model, it is difficult to respond dynamically if the amount of service requests suddenly increases. We propose User-Resizable Residual Networks (URNet), which allows users to adjust the scale of the network as needed during evaluation. URNet includes Conditional Gating Module (CGM) that determines the use of each residual block according to the input image and the desired scale. CGM is trained in a supervised manner using the newly proposed scale loss and its corresponding training methods. URNet can control the amount of computation according to user's demand without degrading the accuracy significantly. It can also be used as a general compression method by fixing the scale size during training. In the experiments on ImageNet, URNet based on ResNet-101 maintains the accuracy of the baseline even when resizing it to approximately 80% of the original network, and demonstrates only about 1% accuracy degradation when using about 65% of the computation.



### Real-world Underwater Enhancement: Challenges, Benchmarks, and Solutions
- **Arxiv ID**: http://arxiv.org/abs/1901.05320v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.05320v2)
- **Published**: 2019-01-15 10:36:16+00:00
- **Updated**: 2019-03-06 06:49:36+00:00
- **Authors**: Risheng Liu, Xin Fan, Ming Zhu, Minjun Hou, Zhongxuan Luo
- **Comment**: arXiv admin note: text overlap with arXiv:1712.04143 by other authors
- **Journal**: None
- **Summary**: Underwater image enhancement is such an important low-level vision task with many applications that numerous algorithms have been proposed in recent years. These algorithms developed upon various assumptions demonstrate successes from various aspects using different data sets and different metrics. In this work, we setup an undersea image capturing system, and construct a large-scale Real-world Underwater Image Enhancement (RUIE) data set divided into three subsets. The three subsets target at three challenging aspects for enhancement, i.e., image visibility quality, color casts, and higher-level detection/classification, respectively. We conduct extensive and systematic experiments on RUIE to evaluate the effectiveness and limitations of various algorithms to enhance visibility and correct color casts on images with hierarchical categories of degradation. Moreover, underwater image enhancement in practice usually serves as a preprocessing step for mid-level and high-level vision tasks. We thus exploit the object detection performance on enhanced images as a brand new task-specific evaluation criterion. The findings from these evaluations not only confirm what is commonly believed, but also suggest promising solutions and new directions for visibility enhancement, color correction, and object detection on real-world underwater images.



### DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion
- **Arxiv ID**: http://arxiv.org/abs/1901.04780v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1901.04780v1)
- **Published**: 2019-01-15 11:58:04+00:00
- **Updated**: 2019-01-15 11:58:04+00:00
- **Authors**: Chen Wang, Danfei Xu, Yuke Zhu, Roberto Martín-Martín, Cewu Lu, Li Fei-Fei, Silvio Savarese
- **Comment**: None
- **Journal**: None
- **Summary**: A key technical challenge in performing 6D object pose estimation from RGB-D image is to fully leverage the two complementary data sources. Prior works either extract information from the RGB image and depth separately or use costly post-processing steps, limiting their performances in highly cluttered scenes and real-time applications. In this work, we present DenseFusion, a generic framework for estimating 6D pose of a set of known objects from RGB-D images. DenseFusion is a heterogeneous architecture that processes the two data sources individually and uses a novel dense fusion network to extract pixel-wise dense feature embedding, from which the pose is estimated. Furthermore, we integrate an end-to-end iterative pose refinement procedure that further improves the pose estimation while achieving near real-time inference. Our experiments show that our method outperforms state-of-the-art approaches in two datasets, YCB-Video and LineMOD. We also deploy our proposed method to a real robot to grasp and manipulate objects based on the estimated pose.



### Soil Texture Classification with 1D Convolutional Neural Networks based on Hyperspectral Data
- **Arxiv ID**: http://arxiv.org/abs/1901.04846v3
- **DOI**: 10.5194/isprs-annals-IV-2-W5-615-2019
- **Categories**: **cs.CV**, cs.LG, physics.geo-ph, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.04846v3)
- **Published**: 2019-01-15 14:29:04+00:00
- **Updated**: 2019-03-30 13:57:12+00:00
- **Authors**: Felix M. Riese, Sina Keller
- **Comment**: Accepted to the ISPRS Geospatial Week 2019 in Enschede (NL)
- **Journal**: None
- **Summary**: Soil texture is important for many environmental processes. In this paper, we study the classification of soil texture based on hyperspectral data. We develop and implement three 1-dimensional (1D) convolutional neural networks (CNN): the LucasCNN, the LucasResNet which contains an identity block as residual network, and the LucasCoordConv with an additional coordinates layer. Furthermore, we modify two existing 1D CNN approaches for the presented classification task. The code of all five CNN approaches is available on GitHub (Riese, 2019). We evaluate the performance of the CNN approaches and compare them to a random forest classifier. Thereby, we rely on the freely available LUCAS topsoil dataset. The CNN approach with the least depth turns out to be the best performing classifier. The LucasCoordConv achieves the best performance regarding the average accuracy. In future work, we can further enhance the introduced LucasCNN, LucasResNet and LucasCoordConv and include additional variables of the rich LUCAS dataset.



### Toward Explainable Fashion Recommendation
- **Arxiv ID**: http://arxiv.org/abs/1901.04870v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.04870v3)
- **Published**: 2019-01-15 15:02:12+00:00
- **Updated**: 2019-07-27 06:32:43+00:00
- **Authors**: Pongsate Tangseng, Takayuki Okatani
- **Comment**: None
- **Journal**: None
- **Summary**: Many studies have been conducted so far to build systems for recommending fashion items and outfits. Although they achieve good performances in their respective tasks, most of them cannot explain their judgments to the users, which compromises their usefulness. Toward explainable fashion recommendation, this study proposes a system that is able not only to provide a goodness score for an outfit but also to explain the score by providing reason behind it. For this purpose, we propose a method for quantifying how influential each feature of each item is to the score. Using this influence value, we can identify which item and what feature make the outfit good or bad. We represent the image of each item with a combination of human-interpretable features, and thereby the identification of the most influential item-feature pair gives useful explanation of the output score. To evaluate the performance of this approach, we design an experiment that can be performed without human annotation; we replace a single item-feature pair in an outfit so that the score will decrease, and then we test if the proposed method can detect the replaced item correctly using the above influence values. The experimental results show that the proposed method can accurately detect bad items in outfits lowering their scores.



### Feature Boosting Network For 3D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1901.04877v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.04877v2)
- **Published**: 2019-01-15 15:20:05+00:00
- **Updated**: 2019-05-15 07:05:58+00:00
- **Authors**: Jun Liu, Henghui Ding, Amir Shahroudy, Ling-Yu Duan, Xudong Jiang, Gang Wang, Alex C. Kot
- **Comment**: Accepted to T-PAMI. DOI: 10.1109/TPAMI.2019.2894422
- **Journal**: None
- **Summary**: In this paper, a feature boosting network is proposed for estimating 3D hand pose and 3D body pose from a single RGB image. In this method, the features learned by the convolutional layers are boosted with a new long short-term dependence-aware (LSTD) module, which enables the intermediate convolutional feature maps to perceive the graphical long short-term dependency among different hand (or body) parts using the designed Graphical ConvLSTM. Learning a set of features that are reliable and discriminatively representative of the pose of a hand (or body) part is difficult due to the ambiguities, texture and illumination variation, and self-occlusion in the real application of 3D pose estimation. To improve the reliability of the features for representing each body part and enhance the LSTD module, we further introduce a context consistency gate (CCG) in this paper, with which the convolutional feature maps are modulated according to their consistency with the context representations. We evaluate the proposed method on challenging benchmark datasets for 3D hand pose estimation and 3D full body pose estimation. Experimental results show the effectiveness of our method that achieves state-of-the-art performance on both of the tasks.



### A deep learning approach to solar-irradiance forecasting in sky-videos
- **Arxiv ID**: http://arxiv.org/abs/1901.04881v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.04881v1)
- **Published**: 2019-01-15 15:24:36+00:00
- **Updated**: 2019-01-15 15:24:36+00:00
- **Authors**: Talha A. Siddiqui, Samarth Bharadwaj, Shivkumar Kalyanaraman
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Ahead-of-time forecasting of incident solar-irradiance on a panel is indicative of expected energy yield and is essential for efficient grid distribution and planning. Traditionally, these forecasts are based on meteorological physics models whose parameters are tuned by coarse-grained radiometric tiles sensed from geo-satellites. This research presents a novel application of deep neural network approach to observe and estimate short-term weather effects from videos. Specifically, we use time-lapsed videos (sky-videos) obtained from upward facing wide-lensed cameras (sky-cameras) to directly estimate and forecast solar irradiance. We introduce and present results on two large publicly available datasets obtained from weather stations in two regions of North America using relatively inexpensive optical hardware. These datasets contain over a million images that span for 1 and 12 years respectively, the largest such collection to our knowledge. Compared to satellite based approaches, the proposed deep learning approach significantly reduces the normalized mean-absolute-percentage error for both nowcasting, i.e. prediction of the solar irradiance at the instance the frame is captured, as well as forecasting, ahead-of-time irradiance prediction for a duration for upto 4 hours.



### Deep Fusion: An Attention Guided Factorized Bilinear Pooling for Audio-video Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/1901.04889v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.04889v1)
- **Published**: 2019-01-15 15:51:39+00:00
- **Updated**: 2019-01-15 15:51:39+00:00
- **Authors**: Yuanyuan Zhang, Zi-Rui Wang, Jun Du
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic emotion recognition (AER) is a challenging task due to the abstract concept and multiple expressions of emotion. Although there is no consensus on a definition, human emotional states usually can be apperceived by auditory and visual systems. Inspired by this cognitive process in human beings, it's natural to simultaneously utilize audio and visual information in AER. However, most traditional fusion approaches only build a linear paradigm, such as feature concatenation and multi-system fusion, which hardly captures complex association between audio and video. In this paper, we introduce factorized bilinear pooling (FBP) to deeply integrate the features of audio and video. Specifically, the features are selected through the embedded attention mechanism from respective modalities to obtain the emotion-related regions. The whole pipeline can be completed in a neural network. Validated on the AFEW database of the audio-video sub-challenge in EmotiW2018, the proposed approach achieves an accuracy of 62.48%, outperforming the state-of-the-art result.



### Light-weighted Saliency Detection with Distinctively Lower Memory Cost and Model Size
- **Arxiv ID**: http://arxiv.org/abs/1901.05002v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1901.05002v1)
- **Published**: 2019-01-15 16:15:02+00:00
- **Updated**: 2019-01-15 16:15:02+00:00
- **Authors**: Shanghua Xiao
- **Comment**: 7 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:1809.00644
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) based saliency detection approaches have succeed in recent years, and improved the performance by a great margin via increasingly sophisticated network architecture. Despite the performance improvement, the computational cost is excessively high for such low level visual task. In this work, we propose a light-weighted saliency detection approach with distinctively lower runtime memory cost and model size. We evaluated the performance of our approach on multiple benchmark datasets, and achieved competitive results comparing with state-of-the-art methods on multiple metrics. We also evaluated the computational cost of our approach with multiple measurements. The runtime memory cost of our approach is 42 to 99 times fewer comparing with the previous DNNs based methods. The model size of our approach is 63 to 129 times smaller, and takes less than 1 Megabytes storage space with out any deep compression technique.



### Rapid Visual Categorization is not Guided by Early Salience-Based Selection
- **Arxiv ID**: http://arxiv.org/abs/1901.04908v3
- **DOI**: 10.1371/journal.pone.0224306
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1901.04908v3)
- **Published**: 2019-01-15 16:22:24+00:00
- **Updated**: 2020-01-30 20:58:35+00:00
- **Authors**: John K. Tsotsos, Iuliia Kotseruba, Calden Wloka
- **Comment**: 22 pages, 9 figures
- **Journal**: None
- **Summary**: The current dominant visual processing paradigm in both human and machine research is the feedforward, layered hierarchy of neural-like processing elements. Within this paradigm, visual saliency is seen by many to have a specific role, namely that of early selection. Early selection is thought to enable very fast visual performance by limiting processing to only the most salient candidate portions of an image. This strategy has led to a plethora of saliency algorithms that have indeed improved processing time efficiency in machine algorithms, which in turn have strengthened the suggestion that human vision also employs a similar early selection strategy. However, at least one set of critical tests of this idea has never been performed with respect to the role of early selection in human vision. How would the best of the current saliency models perform on the stimuli used by experimentalists who first provided evidence for this visual processing paradigm? Would the algorithms really provide correct candidate sub-images to enable fast categorization on those same images? Do humans really need this early selection for their impressive performance? Here, we report on a new series of tests of these questions whose results suggest that it is quite unlikely that such an early selection process has any role in human rapid visual categorization.



### Automatic Surface Area and Volume Prediction on Ellipsoidal Ham using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1901.04947v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 28-04, 68Uxx
- **Links**: [PDF](http://arxiv.org/pdf/1901.04947v1)
- **Published**: 2019-01-15 17:26:43+00:00
- **Updated**: 2019-01-15 17:26:43+00:00
- **Authors**: Y. S. Gan, Sze-Teng Liong, Yen-Chang Huang
- **Comment**: This paper contains 19 pages, 12 figures and 4 tables
- **Journal**: None
- **Summary**: This paper presents novel methods to predict the surface and volume of the ham through a camera. This implies that the conventional weight measurement to obtain in the object's volume can be neglected and hence it is economically effective. Both of the measurements are obtained in the following two ways: manually and automatically. The former is assume as the true or exact measurement and the latter is through a computer vision technique with some geometrical analysis that includes mathematical derived functions. For the automatic implementation, most of the existing approaches extract the features of the food material based on handcrafted features and to the best of our knowledge this is the first attempt to estimate the surface area and volume on ham with deep learning features. We address the estimation task with a Mask Region-based CNN (Mask R-CNN) approach, which well performs the ham detection and semantic segmentation from a video. The experimental results demonstrate that the algorithm proposed is robust as promising surface area and volume estimation are obtained for two angles of the ellipsoidal ham (i.e., horizontal and vertical positions). Specifically, in the vertical ham point of view, it achieves an overall accuracy up to 95% whereas the horizontal ham reaches 80% of accuracy.



### Cascade Decoder: A Universal Decoding Method for Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1901.04949v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.04949v1)
- **Published**: 2019-01-15 17:35:35+00:00
- **Updated**: 2019-01-15 17:35:35+00:00
- **Authors**: Peixian Liang, Jianxu Chen, Hao Zheng, Lin Yang, Yizhe Zhang, Danny Z. Chen
- **Comment**: Accepted at ISBI 2019
- **Journal**: None
- **Summary**: The Encoder-Decoder architecture is a main stream deep learning model for biomedical image segmentation. The encoder fully compresses the input and generates encoded features, and the decoder then produces dense predictions using encoded features. However, decoders are still under-explored in such architectures. In this paper, we comprehensively study the state-of-the-art Encoder-Decoder architectures, and propose a new universal decoder, called cascade decoder, to improve semantic segmentation accuracy. Our cascade decoder can be embedded into existing networks and trained altogether in an end-to-end fashion. The cascade decoder structure aims to conduct more effective decoding of hierarchically encoded features and is more compatible with common encoders than the known decoders. We replace the decoders of state-of-the-art models with our cascade decoder for several challenging biomedical image segmentation tasks, and the considerable improvements achieved demonstrate the efficacy of our new decoding method.



### Analysis and algorithms for $\ell_p$-based semi-supervised learning on graphs
- **Arxiv ID**: http://arxiv.org/abs/1901.05031v4
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.LG, cs.NA, math.AP, 65N06, 35R02, 68T05, 68W01, 35D40
- **Links**: [PDF](http://arxiv.org/pdf/1901.05031v4)
- **Published**: 2019-01-15 20:03:12+00:00
- **Updated**: 2022-01-27 15:32:32+00:00
- **Authors**: Mauricio Flores, Jeff Calder, Gilad Lerman
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses theory and applications of $\ell_p$-based Laplacian regularization in semi-supervised learning. The graph $p$-Laplacian for $p>2$ has been proposed recently as a replacement for the standard ($p=2$) graph Laplacian in semi-supervised learning problems with very few labels, where Laplacian learning is degenerate.   In the first part of the paper we prove new discrete to continuum convergence results for $p$-Laplace problems on $k$-nearest neighbor ($k$-NN) graphs, which are more commonly used in practice than random geometric graphs. Our analysis shows that, on $k$-NN graphs, the $p$-Laplacian retains information about the data distribution as $p\to \infty$ and Lipschitz learning ($p=\infty$) is sensitive to the data distribution. This situation can be contrasted with random geometric graphs, where the $p$-Laplacian forgets the data distribution as $p\to \infty$. We also present a general framework for proving discrete to continuum convergence results in graph-based learning that only requires pointwise consistency and monotonicity.   In the second part of the paper, we develop fast algorithms for solving the variational and game-theoretic $p$-Laplace equations on weighted graphs for $p>2$. We present several efficient and scalable algorithms for both formulations, and present numerical results on synthetic data indicating their convergence properties. Finally, we conduct extensive numerical experiments on the MNIST, FashionMNIST and EMNIST datasets that illustrate the effectiveness of the $p$-Laplacian formulation for semi-supervised learning with few labels. In particular, we find that Lipschitz learning ($p=\infty$) performs well with very few labels on $k$-NN graphs, which experimentally validates our theoretical findings that Lipschitz learning retains information about the data distribution (the unlabeled data) on $k$-NN graphs.



### On complexity of branching droplets in electrical field
- **Arxiv ID**: http://arxiv.org/abs/1901.05043v1
- **DOI**: None
- **Categories**: **cs.ET**, cs.CC, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1901.05043v1)
- **Published**: 2019-01-15 20:53:40+00:00
- **Updated**: 2019-01-15 20:53:40+00:00
- **Authors**: Mohammad Mahdi Dehshibi, Jitka Cejkova, Dominik Svara, Andrew Adamatzky
- **Comment**: None
- **Journal**: None
- **Summary**: Decanol droplets in a thin layer of sodium decanoate with sodium chloride exhibit bifurcation branching growth due to interplay between osmotic pressure, diffusion and surface tension. We aimed to evaluate if morphology of the branching droplets changes when the droplets are subject to electrical potential difference. We analysed graph-theoretic structure of the droplets and applied several complexity measures. We found that, in overall, the current increases complexity of the branching droplets in terms of number of connected components and nodes in their graph presentations, morphological complexity and compressibility.



