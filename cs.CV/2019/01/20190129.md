# Arxiv Papers in cs.CV on 2019-01-29
### Cloud-Net: An end-to-end Cloud Detection Algorithm for Landsat 8 Imagery
- **Arxiv ID**: http://arxiv.org/abs/1901.10077v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.10077v1)
- **Published**: 2019-01-29 02:49:50+00:00
- **Updated**: 2019-01-29 02:49:50+00:00
- **Authors**: Sorour Mohajerani, Parvaneh Saeedi
- **Comment**: None
- **Journal**: None
- **Summary**: Cloud detection in satellite images is an important first-step in many remote sensing applications. This problem is more challenging when only a limited number of spectral bands are available. To address this problem, a deep learning-based algorithm is proposed in this paper. This algorithm consists of a Fully Convolutional Network (FCN) that is trained by multiple patches of Landsat 8 images. This network, which is called Cloud-Net, is capable of capturing global and local cloud features in an image using its convolutional blocks. Since the proposed method is an end-to-end solution, no complicated pre-processing step is required. Our experimental results prove that the proposed method outperforms the state-of-the-art method over a benchmark dataset by 8.7\% in Jaccard Index.



### Discovering Underlying Person Structure Pattern with Relative Local Distance for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1901.10100v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.10100v1)
- **Published**: 2019-01-29 04:57:49+00:00
- **Updated**: 2019-01-29 04:57:49+00:00
- **Authors**: Guangcong Wang, Jianhuang Lai, Zhenyu Xie, Xiaohua Xie
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Modeling the underlying person structure for person re-identification (re-ID) is difficult due to diverse deformable poses, changeable camera views and imperfect person detectors. How to exploit underlying person structure information without extra annotations to improve the performance of person re-ID remains largely unexplored. To address this problem, we propose a novel Relative Local Distance (RLD) method that integrates a relative local distance constraint into convolutional neural networks (CNNs) in an end-to-end way. It is the first time that the relative local constraint is proposed to guide the global feature representation learning. Specially, a relative local distance matrix is computed by using feature maps and then regarded as a regularizer to guide CNNs to learn a structure-aware feature representation. With the discovered underlying person structure, the RLD method builds a bridge between the global and local feature representation and thus improves the capacity of feature representation for person re-ID. Furthermore, RLD also significantly accelerates deep network training compared with conventional methods. The experimental results show the effectiveness of RLD on the CUHK03, Market-1501, and DukeMTMC-reID datasets. Code is available at \url{https://github.com/Wanggcong/RLD_codes}.



### Evaluating Generalization Ability of Convolutional Neural Networks and Capsule Networks for Image Classification via Top-2 Classification
- **Arxiv ID**: http://arxiv.org/abs/1901.10112v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.10112v4)
- **Published**: 2019-01-29 05:34:40+00:00
- **Updated**: 2022-11-27 03:50:33+00:00
- **Authors**: Hao Ren, Jianlin Su, Hong Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Image classification is a challenging problem which aims to identify the category of object in the image. In recent years, deep Convolutional Neural Networks (CNNs) have been applied to handle this task, and impressive improvement has been achieved. However, some research showed the output of CNNs can be easily altered by adding relatively small perturbations to the input image, such as modifying few pixels. Recently, Capsule Networks (CapsNets) are proposed, which can help eliminating this limitation. Experiments on MNIST dataset revealed that capsules can better characterize the features of object than CNNs. But it's hard to find a suitable quantitative method to compare the generalization ability of CNNs and CapsNets. In this paper, we propose a new image classification task called Top-2 classification to evaluate the generalization ability of CNNs and CapsNets. The models are trained on single label image samples same as the traditional image classification task. But in the test stage, we randomly concatenate two test image samples which contain different labels, and then use the trained models to predict the top-2 labels on the unseen newly-created two label image samples. This task can provide us precise quantitative results to compare the generalization ability of CNNs and CapsNets. Back to the CapsNet, because it uses Full Connectivity (FC) mechanism among all capsules, it requires many parameters. To reduce the number of parameters, we introduce the Parameter-Sharing (PS) mechanism between capsules. Experiments on five widely used benchmark image datasets demonstrate the method significantly reduces the number of parameters, without losing the effectiveness of extracting features. Further, on the Top-2 classification task, the proposed PS CapsNets obtain impressive higher accuracy compared to the traditional CNNs and FC CapsNets by a large margin.



### Adversarial Adaptation of Scene Graph Models for Understanding Civic Issues
- **Arxiv ID**: http://arxiv.org/abs/1901.10124v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1901.10124v1)
- **Published**: 2019-01-29 06:02:45+00:00
- **Updated**: 2019-01-29 06:02:45+00:00
- **Authors**: Shanu Kumar, Shubham Atreja, Anjali Singh, Mohit Jain
- **Comment**: Accepted at WWW'19
- **Journal**: None
- **Summary**: Citizen engagement and technology usage are two emerging trends driven by smart city initiatives. Governments around the world are adopting technology for faster resolution of civic issues. Typically, citizens report issues, such as broken roads, garbage dumps, etc. through web portals and mobile apps, in order for the government authorities to take appropriate actions. Several mediums -- text, image, audio, video -- are used to report these issues. Through a user study with 13 citizens and 3 authorities, we found that image is the most preferred medium to report civic issues. However, analyzing civic issue related images is challenging for the authorities as it requires manual effort. Moreover, previous works have been limited to identifying a specific set of issues from images. In this work, given an image, we propose to generate a Civic Issue Graph consisting of a set of objects and the semantic relations between them, which are representative of the underlying civic issue. We also release two multi-modal (text and images) datasets, that can help in further analysis of civic issues from images. We present a novel approach for adversarial training of existing scene graph models that enables the use of scene graphs for new applications in the absence of any labelled training data. We conduct several experiments to analyze the efficacy of our approach, and using human evaluation, we establish the appropriateness of our model at representing different civic issues.



### Glyce: Glyph-vectors for Chinese Character Representations
- **Arxiv ID**: http://arxiv.org/abs/1901.10125v5
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1901.10125v5)
- **Published**: 2019-01-29 06:15:36+00:00
- **Updated**: 2020-05-21 09:05:11+00:00
- **Authors**: Yuxian Meng, Wei Wu, Fei Wang, Xiaoya Li, Ping Nie, Fan Yin, Muyu Li, Qinghong Han, Xiaofei Sun, Jiwei Li
- **Comment**: Accepted by NeurIPS 2019
- **Journal**: None
- **Summary**: It is intuitive that NLP tasks for logographic languages like Chinese should benefit from the use of the glyph information in those languages. However, due to the lack of rich pictographic evidence in glyphs and the weak generalization ability of standard computer vision models on character data, an effective way to utilize the glyph information remains to be found. In this paper, we address this gap by presenting Glyce, the glyph-vectors for Chinese character representations. We make three major innovations: (1) We use historical Chinese scripts (e.g., bronzeware script, seal script, traditional Chinese, etc) to enrich the pictographic evidence in characters; (2) We design CNN structures (called tianzege-CNN) tailored to Chinese character image processing; and (3) We use image-classification as an auxiliary task in a multi-task learning setup to increase the model's ability to generalize. We show that glyph-based models are able to consistently outperform word/char ID-based models in a wide range of Chinese NLP tasks. We are able to set new state-of-the-art results for a variety of Chinese NLP tasks, including tagging (NER, CWS, POS), sentence pair classification, single sentence classification tasks, dependency parsing, and semantic role labeling. For example, the proposed model achieves an F1 score of 80.6 on the OntoNotes dataset of NER, +1.5 over BERT; it achieves an almost perfect accuracy of 99.8\% on the Fudan corpus for text classification. Code found at https://github.com/ShannonAI/glyce.



### Attention-based Context Aggregation Network for Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1901.10137v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.10137v1)
- **Published**: 2019-01-29 07:01:20+00:00
- **Updated**: 2019-01-29 07:01:20+00:00
- **Authors**: Yuru Chen, Haitao Zhao, Zhengwei Hu
- **Comment**: 12 pages, 10 figures
- **Journal**: None
- **Summary**: Depth estimation is a traditional computer vision task, which plays a crucial role in understanding 3D scene geometry. Recently, deep-convolutional-neural-networks based methods have achieved promising results in the monocular depth estimation field. Specifically, the framework that combines the multi-scale features extracted by the dilated convolution based block (atrous spatial pyramid pooling, ASPP) has gained the significant improvement in the dense labeling task. However, the discretized and predefined dilation rates cannot capture the continuous context information that differs in diverse scenes and easily introduce the grid artifacts in depth estimation. In this paper, we propose an attention-based context aggregation network (ACAN) to tackle these difficulties. Based on the self-attention model, ACAN adaptively learns the task-specific similarities between pixels to model the context information. First, we recast the monocular depth estimation as a dense labeling multi-class classification problem. Then we propose a soft ordinal inference to transform the predicted probabilities to continuous depth values, which can reduce the discretization error (about 1% decrease in RMSE). Second, the proposed ACAN aggregates both the image-level and pixel-level context information for depth estimation, where the former expresses the statistical characteristic of the whole image and the latter extracts the long-range spatial dependencies for each pixel. Third, for further reducing the inconsistency between the RGB image and depth map, we construct an attention loss to minimize their information entropy. We evaluate on public monocular depth-estimation benchmark datasets (including NYU Depth V2, KITTI). The experiments demonstrate the superiority of our proposed ACAN and achieve the competitive results with the state of the arts.



### Learning to Validate the Quality of Detected Landmarks
- **Arxiv ID**: http://arxiv.org/abs/1901.10143v3
- **DOI**: 10.1117/12.2559517
- **Categories**: **cs.CV**, 65D19, 93E35
- **Links**: [PDF](http://arxiv.org/pdf/1901.10143v3)
- **Published**: 2019-01-29 07:19:44+00:00
- **Updated**: 2020-02-11 06:49:47+00:00
- **Authors**: Wolfgang Fuhl, Enkelejda Kasneci
- **Comment**: Will be published in the proceedings of the ICMV 2019 conference
- **Journal**: None
- **Summary**: We present a new loss function for the validation of image landmarks detected via Convolutional Neural Networks (CNN). The network learns to estimate how accurate its landmark estimation is. This loss function is applicable to all regression-based location estimations and allows the exclusion of unreliable landmarks from further processing. In addition, we formulate a novel batch balancing approach which weights the importance of samples based on their produced loss. This is done by computing a probability distribution mapping on an interval from which samples can be selected using a uniform random selection scheme. We conducted experiments on the 300W, AFLW, and WFLW facial landmark datasets. In the first experiments, the influence of our batch balancing approach is evaluated by comparing it against uniform sampling. In addition, we evaluated the impact of the validation loss on the landmark accuracy based on uniform sampling. The last experiments evaluate the correlation of the validation signal with the landmark accuracy. All experiments were performed for all three datasets.



### Visual Rhythm Prediction with Feature-Aligning Network
- **Arxiv ID**: http://arxiv.org/abs/1901.10163v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.10163v1)
- **Published**: 2019-01-29 08:29:08+00:00
- **Updated**: 2019-01-29 08:29:08+00:00
- **Authors**: Yutong Xie, Haiyang Wang, Yan Hao, Zihao Xu
- **Comment**: 6 pages, 4 figures
- **Journal**: None
- **Summary**: In this paper, we propose a data-driven visual rhythm prediction method, which overcomes the previous works' deficiency that predictions are made primarily by human-crafted hard rules. In our approach, we first extract features including original frames and their residuals, optical flow, scene change, and body pose. These visual features will be next taken into an end-to-end neural network as inputs. Here we observe that there are some slight misaligning between features over the timeline and assume that this is due to the distinctions between how different features are computed. To solve this problem, the extracted features are aligned by an elaborately designed layer, which can also be applied to other models suffering from mismatched features, and boost performance. Then these aligned features are fed into sequence labeling layers implemented with BiLSTM and CRF to predict the onsets. Due to the lack of existing public training and evaluation set, we experiment on a dataset constructed by ourselves based on professionally edited Music Videos (MVs), and the F1 score of our approach reaches 79.6.



### Mask-RCNN and U-net Ensembled for Nuclei Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1901.10170v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.10170v1)
- **Published**: 2019-01-29 08:40:16+00:00
- **Updated**: 2019-01-29 08:40:16+00:00
- **Authors**: Aarno Oskar Vuola, Saad Ullah Akram, Juho Kannala
- **Comment**: To appear in IEEE International Symposium on Biomedical Imaging
  (ISBI) 2019
- **Journal**: None
- **Summary**: Nuclei segmentation is both an important and in some ways ideal task for modern computer vision methods, e.g. convolutional neural networks. While recent developments in theory and open-source software have made these tools easier to implement, expert knowledge is still required to choose the right model architecture and training setup. We compare two popular segmentation frameworks, U-Net and Mask-RCNN in the nuclei segmentation task and find that they have different strengths and failures. To get the best of both worlds, we develop an ensemble model to combine their predictions that can outperform both models by a significant margin and should be considered when aiming for best nuclei segmentation performance.



### Two-Stream Multi-Task Network for Fashion Recognition
- **Arxiv ID**: http://arxiv.org/abs/1901.10172v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.10172v3)
- **Published**: 2019-01-29 08:42:16+00:00
- **Updated**: 2019-05-12 10:04:17+00:00
- **Authors**: Peizhao Li, Yanjing Li, Xiaolong Jiang, Xiantong Zhen
- **Comment**: Accepted by ICIP 2019
- **Journal**: None
- **Summary**: In this paper, we present a two-stream multi-task network for fashion recognition. This task is challenging as fashion clothing always contain multiple attributes, which need to be predicted simultaneously for real-time industrial systems. To handle these challenges, we formulate fashion recognition into a multi-task learning problem, including landmark detection, category and attribute classifications, and solve it with the proposed deep convolutional neural network. We design two knowledge sharing strategies which enable information transfer between tasks and improve the overall performance. The proposed model achieves state-of-the-art results on large-scale fashion dataset comparing to the existing methods, which demonstrates its great effectiveness and superiority for fashion recognition.



### Unsupervised Person Re-identification by Deep Asymmetric Metric Embedding
- **Arxiv ID**: http://arxiv.org/abs/1901.10177v1
- **DOI**: 10.1109/TPAMI.2018.2886878
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.10177v1)
- **Published**: 2019-01-29 08:49:26+00:00
- **Updated**: 2019-01-29 08:49:26+00:00
- **Authors**: Hong-Xing Yu, Ancong Wu, Wei-Shi Zheng
- **Comment**: To appear in TPAMI
- **Journal**: None
- **Summary**: Person re-identification (Re-ID) aims to match identities across non-overlapping camera views. Researchers have proposed many supervised Re-ID models which require quantities of cross-view pairwise labelled data. This limits their scalabilities to many applications where a large amount of data from multiple disjoint camera views is available but unlabelled. Although some unsupervised Re-ID models have been proposed to address the scalability problem, they often suffer from the view-specific bias problem which is caused by dramatic variances across different camera views, e.g., different illumination, viewpoints and occlusion. The dramatic variances induce specific feature distortions in different camera views, which can be very disturbing in finding cross-view discriminative information for Re-ID in the unsupervised scenarios, since no label information is available to help alleviate the bias. We propose to explicitly address this problem by learning an unsupervised asymmetric distance metric based on cross-view clustering. The asymmetric distance metric allows specific feature transformations for each camera view to tackle the specific feature distortions. We then design a novel unsupervised loss function to embed the asymmetric metric into a deep neural network, and therefore develop a novel unsupervised deep framework named the DEep Clustering-based Asymmetric MEtric Learning (DECAMEL). In such a way, DECAMEL jointly learns the feature representation and the unsupervised asymmetric metric. DECAMEL learns a compact cross-view cluster structure of Re-ID data, and thus help alleviate the view-specific bias and facilitate mining the potential cross-view discriminative information for unsupervised Re-ID. Extensive experiments on seven benchmark datasets whose sizes span several orders show the effectiveness of our framework.



### Generative Adversarial Networks for geometric surfaces prediction in injection molding
- **Arxiv ID**: http://arxiv.org/abs/1901.10178v1
- **DOI**: 10.1109/ICIT.2018.8352405
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.10178v1)
- **Published**: 2019-01-29 08:49:59+00:00
- **Updated**: 2019-01-29 08:49:59+00:00
- **Authors**: Pierre Nagorny, Thomas Lacombe, Hugues Favreliere, Maurice Pillet, Eric Pairel, Ronan Le Goff, Marlene Wali, Jerome Loureaux, Patrice Kiener
- **Comment**: IEEE. 2018 IEEE International Conference on Industrial Technology
  (ICIT), Feb 2018, Lyon, France, http://www.icit2018.org
- **Journal**: IEEE, 2018 IEEE International Conference on Industrial Technology
  (ICIT), pp.1514-1519, 2018
- **Summary**: Geometrical and appearance quality requirements set the limits of the current industrial performance in injection molding. To guarantee the product's quality, it is necessary to adjust the process settings in a closed loop. Those adjustments cannot rely on the final quality because a part takes days to be geometrically stable. Thus, the final part geometry must be predicted from measurements on hot parts. In this paper, we use recent success of Generative Adversarial Networks (GAN) with the pix2pix network architecture to predict the final part geometry, using only hot parts thermographic images, measured right after production. Our dataset is really small, and the GAN learns to translate thermography to geometry. We firstly study prediction performances using different image similarity comparison algorithms. Moreover, we introduce the innovative use of Discrete Modal Decomposition (DMD) to analyze network predictions. The DMD is a geometrical parameterization technique using a modal space projection to geometrically describe surfaces. We study GAN performances to retrieve geometrical parameterization of surfaces.



### A Push-Pull Layer Improves Robustness of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.10208v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.10208v1)
- **Published**: 2019-01-29 10:42:04+00:00
- **Updated**: 2019-01-29 10:42:04+00:00
- **Authors**: Nicola Strisciuglio, Manuel Lopez-Antequera, Nicolai Petkov
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new layer in Convolutional Neural Networks (CNNs) to increase their robustness to several types of noise perturbations of the input images. We call this a push-pull layer and compute its response as the combination of two half-wave rectified convolutions, with kernels of opposite polarity. It is based on a biologically-motivated non-linear model of certain neurons in the visual system that exhibit a response suppression phenomenon, known as push-pull inhibition. We validate our method by substituting the first convolutional layer of the LeNet-5 and WideResNet architectures with our push-pull layer. We train the networks on nonperturbed training images from the MNIST, CIFAR-10 and CIFAR-100 data sets, and test on images perturbed by noise that is unseen by the training process. We demonstrate that our push-pull layers contribute to a considerable improvement in robustness of classification of images perturbed by noise, while maintaining state-of-the-art performance on the original image classification task.



### Detection of Alzheimers Disease from MRI using Convolutional Neural Networks, Exploring Transfer Learning And BellCNN
- **Arxiv ID**: http://arxiv.org/abs/1901.10231v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.10231v1)
- **Published**: 2019-01-29 11:32:06+00:00
- **Updated**: 2019-01-29 11:32:06+00:00
- **Authors**: GuruRaj Awate
- **Comment**: IEEE Conference, Intended for non-technical audiences
- **Journal**: None
- **Summary**: There is a need for automatic diagnosis of certain diseases from medical images that could help medical practitioners for further assessment towards treating the illness. Alzheimers disease is a good example of a disease that is often misdiagnosed. Alzheimers disease (Hear after referred to as AD), is caused by atrophy of certain brain regions and by brain cell death and is the leading cause of dementia and memory loss [1]. MRI scans reveal this information but atrophied regions are different for different individuals which makes the diagnosis a bit more trickier and often gets misdiagnosed [1, 13]. We believe that our approach to this particular problem would improve the assessment quality by pre-flagging the images which are more likely to have AD. We propose two solutions to this; one with transfer learning [9] and other by BellCNN [14], a custom made Convolutional Neural Network (Hear after referred to as CNN). Advantages and disadvantages of each approach will also be discussed in their respective sections. The dataset used for this project is provided by Open Access Series of Imaging Studies (Hear after referred to as OASIS) [2, 3, 4], which contains over 400 subjects, 100 of whom have mild to severe dementia. The dataset has labeled these subjects by two standards of diagnosis; MiniMental State Examination (Hear after referred to as MMSE) and Clinical Dementia Rating (Hear after referred to as CDR). These are some of the general tools and concepts which are prerequisites to our solution; CNN [5, 6], Neural Networks [10] (Hear after referred to as NN), Anaconda bundle for python, Regression, Tensorflow [7]. Keywords: Alzheimers Disease, Convolutional Neural Network, BellCNN, Image Recognition, Machine Learning, MRI, OASIS, Tensorflow



### Reconstruction of 3D Porous Media From 2D Slices
- **Arxiv ID**: http://arxiv.org/abs/1901.10233v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.10233v4)
- **Published**: 2019-01-29 11:41:39+00:00
- **Updated**: 2021-08-06 06:37:37+00:00
- **Authors**: Denis Volkhonskiy, Ekaterina Muravleva, Oleg Sudakov, Denis Orlov, Boris Belozerov, Evgeny Burnaev, Dmitry Koroteev
- **Comment**: None
- **Journal**: None
- **Summary**: In many branches of earth sciences, the problem of rock study on the micro-level arises. However, a significant number of representative samples is not always feasible. Thus the problem of the generation of samples with similar properties becomes actual. In this paper, we propose a novel deep learning architecture for three-dimensional porous media reconstruction from two-dimensional slices. We fit a distribution on all possible three-dimensional structures of a specific type based on the given dataset of samples. Then, given partial information (central slices), we recover the three-dimensional structure around such slices as the most probable one according to that constructed distribution. Technically, we implement this in the form of a deep neural network with encoder, generator and discriminator modules. Numerical experiments show that this method provides a good reconstruction in terms of Minkowski functionals.



### Automatic Whole-body Bone Age Assessment Using Deep Hierarchical Features
- **Arxiv ID**: http://arxiv.org/abs/1901.10237v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.10237v1)
- **Published**: 2019-01-29 11:53:30+00:00
- **Updated**: 2019-01-29 11:53:30+00:00
- **Authors**: Hai-Duong Nguyen, Soo-Hyung Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Bone age assessment gives us evidence to analyze the children growth status and the rejuvenation involved chronological and biological ages. All the previous works consider left-hand X-ray image of a child in their works. In this paper, we carry out a study on estimating human age using whole-body bone CT images and a novel convolutional neural network. Our model with additional connections shows an effective way to generate a massive number of vital features while reducing overfitting influence on small training data in the medical image analysis research area. A dataset and a comparison with common deep architectures will be provided for future research in this field.



### Explicit topological priors for deep-learning based image segmentation using persistent homology
- **Arxiv ID**: http://arxiv.org/abs/1901.10244v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.10244v1)
- **Published**: 2019-01-29 12:11:55+00:00
- **Updated**: 2019-01-29 12:11:55+00:00
- **Authors**: James R. Clough, Ilkay Oksuz, Nicholas Byrne, Julia A. Schnabel, Andrew P. King
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: We present a novel method to explicitly incorporate topological prior knowledge into deep learning based segmentation, which is, to our knowledge, the first work to do so. Our method uses the concept of persistent homology, a tool from topological data analysis, to capture high-level topological characteristics of segmentation results in a way which is differentiable with respect to the pixelwise probability of being assigned to a given class. The topological prior knowledge consists of the sequence of desired Betti numbers of the segmentation. As a proof-of-concept we demonstrate our approach by applying it to the problem of left-ventricle segmentation of cardiac MR images of 500 subjects from the UK Biobank dataset, where we show that it improves segmentation performance in terms of topological correctness without sacrificing pixelwise accuracy.



### Learning for Multi-Model and Multi-Type Fitting
- **Arxiv ID**: http://arxiv.org/abs/1901.10254v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.10254v1)
- **Published**: 2019-01-29 12:40:24+00:00
- **Updated**: 2019-01-29 12:40:24+00:00
- **Authors**: Xun Xu, Loong-Fah Cheong, Zhuwen Li
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-model fitting has been extensively studied from the random sampling and clustering perspectives. Most assume that only a single type/class of model is present and their generalizations to fitting multiple types of models/structures simultaneously are non-trivial. The inherent challenges include choice of types and numbers of models, sampling imbalance and parameter tuning, all of which render conventional approaches ineffective. In this work, we formulate the multi-model multi-type fitting problem as one of learning deep feature embedding that is clustering-friendly. In other words, points of the same clusters are embedded closer together through the network. For inference, we apply K-means to cluster the data in the embedded feature space and model selection is enabled by analyzing the K-means residuals. Experiments are carried out on both synthetic and real world multi-type fitting datasets, producing state-of-the-art results. Comparisons are also made on single-type multi-model fitting tasks with promising results as well.



### Implicit Diversity in Image Summarization
- **Arxiv ID**: http://arxiv.org/abs/1901.10265v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.10265v3)
- **Published**: 2019-01-29 13:13:16+00:00
- **Updated**: 2020-08-14 21:00:06+00:00
- **Authors**: L. Elisa Celis, Vijay Keswani
- **Comment**: None
- **Journal**: None
- **Summary**: Studies have shown that the people depicted in image search results tend to be of majority groups with respect to socially salient attributes. This skew goes beyond that which already exists in the world - e.g., Kay et al. showed that although 28% of CEOs in US are women, only 10% of the top 100 results for CEO in Google Image Search are women. Most existing approaches to correct for this kind of bias assume that the images of people include socially salient attribute labels. However, such labels are often unknown. Further, using automated techniques to infer these labels may often not be possible within acceptable accuracy ranges, and may not be desirable due to the additional biases this process could incur. We develop a novel approach that takes as input a visibly diverse control set of images and uses this set to select a set of images of people in response to a query. The goal is to have a resulting set that is more visibly diverse in a manner that emulates the diversity depicted in the control set. Importantly, this approach does not require images to be labelled at any point; effectively, it gives a way to implicitly diversify the set of images selected. We provide two variants of our approach: the first is a modification of the MMR algorithm to incorporate the diversity scores, and second is a more efficient variant that does not consider within-list redundancy. We evaluate these approaches empirically on two datasets 1) a new dataset containing top Google image results for 96 occupations, for which we evaluate gender and skin-tone diversity with respect to occupations and 2) the CelebA dataset for which we evaluate gender diversity with respect to facial features. Our approaches produce image sets that significantly improve the visible diversity of the results, compared to current Google search and other diverse image summarization algorithms, at a minimal cost to accuracy.



### Combined tract segmentation and orientation mapping for bundle-specific tractography
- **Arxiv ID**: http://arxiv.org/abs/1901.10271v2
- **DOI**: 10.1016/j.media.2019.101559
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.10271v2)
- **Published**: 2019-01-29 13:25:50+00:00
- **Updated**: 2019-09-25 13:02:18+00:00
- **Authors**: Jakob Wasserthal, Peter Neher, Dusan Hirjak, Klaus H. Maier-Hein
- **Comment**: None
- **Journal**: None
- **Summary**: While the major white matter tracts are of great interest to numerous studies in neuroscience and medicine, their manual dissection in larger cohorts from diffusion MRI tractograms is time-consuming, requires expert knowledge and is hard to reproduce. In previous work we presented tract orientation mapping (TOM) as a novel concept for bundle-specific tractography. It is based on a learned mapping from the original fiber orientation distribution function (FOD) peaks to tract specific peaks, called tract orientation maps. Each tract orientation map represents the voxel-wise principal orientation of one tract. Here, we present an extension of this approach that combines TOM with accurate segmentations of the tract outline and its start and end region. We also introduce a custom probabilistic tracking algorithm that samples from a Gaussian distribution with fixed standard deviation centered on each peak thus enabling more complete trackings on the tract orientation maps than deterministic tracking. These extensions enable the automatic creation of bundle-specific tractograms with previously unseen accuracy. We show for 72 different bundles on high quality, low quality and phantom data that our approach runs faster and produces more accurate bundle-specific tractograms than 7 state of the art benchmark methods while avoiding cumbersome processing steps like whole brain tractography, non-linear registration, clustering or manual dissection. Moreover, we show on 17 datasets that our approach generalizes well to datasets acquired with different scanners and settings as well as with pathologies. The code of our method is openly available at https://github.com/MIC-DKFZ/TractSeg.



### Automated Prototype for Asteroids Detection
- **Arxiv ID**: http://arxiv.org/abs/1901.10469v1
- **DOI**: 10.1109/ICCP.2017.8117033
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1901.10469v1)
- **Published**: 2019-01-29 13:27:54+00:00
- **Updated**: 2019-01-29 13:27:54+00:00
- **Authors**: D. Copandean, O. Vaduvescu, D. Gorgan
- **Comment**: 13th International Conference on Intelligent Computer Communication
  and Processing (ICCP), Cluj-Napoca, Romania. arXiv admin note: text overlap
  with arXiv:1901.02542
- **Journal**: IEEE, 2017
- **Summary**: Near Earth Asteroids (NEAs) are discovered daily, mainly by few major surveys, nevertheless many of them remain unobserved for years, even decades. Even so, there is room for new discoveries, including those submitted by smaller projects and amateur astronomers. Besides the well-known surveys that have their own automated system of asteroid detection, there are only a few software solutions designed to help amateurs and mini-surveys in NEAs discovery. Some of these obtain their results based on the blink method in which a set of reduced images are shown one after another and the astronomer has to visually detect real moving objects in a series of images. This technique becomes harder with the increase in size of the CCD cameras. Aiming to replace manual detection we propose an automated pipeline prototype for asteroids detection, written in Python under Linux, which calls some 3rd party astrophysics libraries.



### High-Quality Self-Supervised Deep Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1901.10277v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.10277v3)
- **Published**: 2019-01-29 13:37:16+00:00
- **Updated**: 2019-10-28 12:36:21+00:00
- **Authors**: Samuli Laine, Tero Karras, Jaakko Lehtinen, Timo Aila
- **Comment**: NeurIPS 2019 final version
- **Journal**: None
- **Summary**: We describe a novel method for training high-quality image denoising models based on unorganized collections of corrupted images. The training does not need access to clean reference images, or explicit pairs of corrupted images, and can thus be applied in situations where such data is unacceptably expensive or impossible to acquire. We build on a recent technique that removes the need for reference data by employing networks with a "blind spot" in the receptive field, and significantly improve two key aspects: image quality and training efficiency. Our result quality is on par with state-of-the-art neural network denoisers in the case of i.i.d. additive Gaussian noise, and not far behind with Poisson and impulse noise. We also successfully handle cases where parameters of the noise model are variable and/or unknown in both training and evaluation data.



### MultiLock: Mobile Active Authentication based on Multiple Biometric and Behavioral Patterns
- **Arxiv ID**: http://arxiv.org/abs/1901.10312v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.CY, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1901.10312v1)
- **Published**: 2019-01-29 14:39:37+00:00
- **Updated**: 2019-01-29 14:39:37+00:00
- **Authors**: Alejandro Acien, Aythami Morales, Ruben Vera-Rodriguez, Julian Fierrez
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we evaluate mobile active authentication based on an ensemble of biometrics and behavior-based profiling signals. We consider seven different data channels and their combination. Touch dynamics (touch gestures and keystroking), accelerometer, gyroscope, WiFi, GPS location and app usage are all collected during human-mobile interaction to authenticate the users. We evaluate two approaches: one-time authentication and active authentication. In one-time authentication, we employ the information of all channels available during one session. For active authentication we take advantage of mobile user behavior across multiple sessions by updating a confidence value of the authentication score. Our experiments are conducted on the semi-uncontrolled UMDAA-02 database. This database comprises smartphone sensor signals acquired during natural human-mobile interaction. Our results show that different traits can be complementary and multimodal systems clearly increase the performance with accuracies ranging from 82.2% to 97.1% depending on the authentication scenario.



### Real-time Hand Gesture Detection and Classification Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.10323v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1901.10323v3)
- **Published**: 2019-01-29 14:52:51+00:00
- **Updated**: 2019-10-18 08:14:35+00:00
- **Authors**: Okan Köpüklü, Ahmet Gunduz, Neslihan Kose, Gerhard Rigoll
- **Comment**: Published at IEEE International Conference on Automatic Face and
  Gesture Recognition (FG 2019) - Best student paper award! -
- **Journal**: None
- **Summary**: Real-time recognition of dynamic hand gestures from video streams is a challenging task since (i) there is no indication when a gesture starts and ends in the video, (ii) performed gestures should only be recognized once, and (iii) the entire architecture should be designed considering the memory and power budget. In this work, we address these challenges by proposing a hierarchical structure enabling offline-working convolutional neural network (CNN) architectures to operate online efficiently by using sliding window approach. The proposed architecture consists of two models: (1) A detector which is a lightweight CNN architecture to detect gestures and (2) a classifier which is a deep CNN to classify the detected gestures. In order to evaluate the single-time activations of the detected gestures, we propose to use Levenshtein distance as an evaluation metric since it can measure misclassifications, multiple detections, and missing detections at the same time. We evaluate our architecture on two publicly available datasets - EgoGesture and NVIDIA Dynamic Hand Gesture Datasets - which require temporal detection and classification of the performed hand gestures. ResNeXt-101 model, which is used as a classifier, achieves the state-of-the-art offline classification accuracy of 94.04% and 83.82% for depth modality on EgoGesture and NVIDIA benchmarks, respectively. In real-time detection and classification, we obtain considerable early detections while achieving performances close to offline operation. The codes and pretrained models used in this work are publicly available.



### Who's Afraid of Adversarial Queries? The Impact of Image Modifications on Content-based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1901.10332v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1901.10332v3)
- **Published**: 2019-01-29 15:09:14+00:00
- **Updated**: 2019-05-02 11:57:18+00:00
- **Authors**: Zhuoran Liu, Zhengyu Zhao, Martha Larson
- **Comment**: To appear at the ACM International Conference on Multimedia Retrieval
  (ICMR 2019). Our code is available at https://github.com/liuzrcc/PIRE
- **Journal**: None
- **Summary**: An adversarial query is an image that has been modified to disrupt content-based image retrieval (CBIR) while appearing nearly untouched to the human eye. This paper presents an analysis of adversarial queries for CBIR based on neural, local, and global features. We introduce an innovative neural image perturbation approach, called Perturbations for Image Retrieval Error (PIRE), that is capable of blocking neural-feature-based CBIR. PIRE differs significantly from existing approaches that create images adversarial with respect to CNN classifiers because it is unsupervised, i.e., it needs no labelled data from the data set to which it is applied. Our experimental analysis demonstrates the surprising effectiveness of PIRE in blocking CBIR, and also covers aspects of PIRE that must be taken into account in practical settings, including saving images, image quality and leaking adversarial queries into the background collection. Our experiments also compare PIRE (a neural approach) with existing keypoint removal and injection approaches (which modify local features). Finally, we discuss the challenges that face multimedia researchers in the future study of adversarial queries.



### Quality Measures for Speaker Verification with Short Utterances
- **Arxiv ID**: http://arxiv.org/abs/1901.10345v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.10345v2)
- **Published**: 2019-01-29 15:45:35+00:00
- **Updated**: 2019-01-31 12:33:02+00:00
- **Authors**: Arnab Poddar, Md Sahidullah, Goutam Saha
- **Comment**: Accepted for publication in Digital Signal Processing: A Review
  Journal
- **Journal**: None
- **Summary**: The performances of the automatic speaker verification (ASV) systems degrade due to the reduction in the amount of speech used for enrollment and verification. Combining multiple systems based on different features and classifiers considerably reduces speaker verification error rate with short utterances. This work attempts to incorporate supplementary information during the system combination process. We use quality of the estimated model parameters as supplementary information. We introduce a class of novel quality measures formulated using the zero-order sufficient statistics used during the i-vector extraction process. We have used the proposed quality measures as side information for combining ASV systems based on Gaussian mixture model-universal background model (GMM-UBM) and i-vector. The proposed methods demonstrate considerable improvement in speaker recognition performance on NIST SRE corpora, especially in short duration conditions. We have also observed improvement over existing systems based on different duration-based quality measures.



### Anomaly Locality in Video Surveillance
- **Arxiv ID**: http://arxiv.org/abs/1901.10364v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.10364v1)
- **Published**: 2019-01-29 16:30:17+00:00
- **Updated**: 2019-01-29 16:30:17+00:00
- **Authors**: Federico Landi, Cees G. M. Snoek, Rita Cucchiara
- **Comment**: Submitted to International Conference on Image Processing, 2019
- **Journal**: None
- **Summary**: This paper strives for the detection of real-world anomalies such as burglaries and assaults in surveillance videos. Although anomalies are generally local, as they happen in a limited portion of the frame, none of the previous works on the subject has ever studied the contribution of locality. In this work, we explore the impact of considering spatiotemporal tubes instead of whole-frame video segments. For this purpose, we enrich existing surveillance videos with spatial and temporal annotations: it is the first dataset for anomaly detection with bounding box supervision in both its train and test set. Our experiments show that a network trained with spatiotemporal tubes performs better than its analogous model trained with whole-frame videos. In addition, we discover that the locality is robust to different kinds of errors in the tube extraction phase at test time. Finally, we demonstrate that our network can provide spatiotemporal proposals for unseen surveillance videos leveraging only video-level labels. By doing, we enlarge our spatiotemporal anomaly dataset without the need for further human labeling.



### MgNet: A Unified Framework of Multigrid and Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1901.10415v2
- **DOI**: 10.1007/s11425-019-9547-2
- **Categories**: **cs.CV**, cs.LG, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1901.10415v2)
- **Published**: 2019-01-29 17:30:59+00:00
- **Updated**: 2019-05-01 20:49:32+00:00
- **Authors**: Juncai He, Jinchao Xu
- **Comment**: 30 pages
- **Journal**: Sci. China Math. 62 (2019) 1331-1354
- **Summary**: We develop a unified model, known as MgNet, that simultaneously recovers some convolutional neural networks (CNN) for image classification and multigrid (MG) methods for solving discretized partial differential equations (PDEs). This model is based on close connections that we have observed and uncovered between the CNN and MG methodologies. For example, pooling operation and feature extraction in CNN correspond directly to restriction operation and iterative smoothers in MG, respectively. As the solution space is often the dual of the data space in PDEs, the analogous concept of feature space and data space (which are dual to each other) is introduced in CNN. With such connections and new concept in the unified model, the function of various convolution operations and pooling used in CNN can be better understood. As a result, modified CNN models (with fewer weights and hyper parameters) are developed that exhibit competitive and sometimes better performance in comparison with existing CNN models when applied to both CIFAR-10 and CIFAR-100 data sets.



### Progressive Augmentation of GANs
- **Arxiv ID**: http://arxiv.org/abs/1901.10422v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.10422v3)
- **Published**: 2019-01-29 17:47:39+00:00
- **Updated**: 2019-10-28 15:52:31+00:00
- **Authors**: Dan Zhang, Anna Khoreva
- **Comment**: Accepted at NeurIPS'19
- **Journal**: None
- **Summary**: Training of Generative Adversarial Networks (GANs) is notoriously fragile, requiring to maintain a careful balance between the generator and the discriminator in order to perform well. To mitigate this issue we introduce a new regularization technique - progressive augmentation of GANs (PA-GAN). The key idea is to gradually increase the task difficulty of the discriminator by progressively augmenting its input or feature space, thus enabling continuous learning of the generator. We show that the proposed progressive augmentation preserves the original GAN objective, does not compromise the discriminator's optimality and encourages a healthy competition between the generator and discriminator, leading to the better-performing generator. We experimentally demonstrate the effectiveness of PA-GAN across different architectures and on multiple benchmarks for the image synthesis task, on average achieving ~3 point improvement of the FID score.



### Influence of segmentation on deep iris recognition performance
- **Arxiv ID**: http://arxiv.org/abs/1901.10431v2
- **DOI**: 10.1109/IWBF.2019.8739225
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.10431v2)
- **Published**: 2019-01-29 18:01:42+00:00
- **Updated**: 2020-05-08 18:25:29+00:00
- **Authors**: Juš Lozej, Dejan Štepec, Vitomir Štruc, Peter Peer
- **Comment**: 6 pages, 3 figures, 3 tables, submitted to IWBF 2019
- **Journal**: None
- **Summary**: Despite the rise of deep learning in numerous areas of computer vision and image processing, iris recognition has not benefited considerably from these trends so far. Most of the existing research on deep iris recognition is focused on new models for generating discriminative and robust iris representations and relies on methodologies akin to traditional iris recognition pipelines. Hence, the proposed models do not approach iris recognition in an end-to-end manner, but rather use standard heuristic iris segmentation (and unwrapping) techniques to produce normalized inputs for the deep learning models. However, because deep learning is able to model very complex data distributions and nonlinear data changes, an obvious question arises. How important is the use of traditional segmentation methods in a deep learning setting? To answer this question, we present in this paper an empirical analysis of the impact of iris segmentation on the performance of deep learning models using a simple two stage pipeline consisting of a segmentation and a recognition step. We evaluate how the accuracy of segmentation influences recognition performance but also examine if segmentation is needed at all. We use the CASIA Thousand and SBVPI datasets for the experiments and report several interesting findings.



### Diversity in Faces
- **Arxiv ID**: http://arxiv.org/abs/1901.10436v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.10436v6)
- **Published**: 2019-01-29 18:24:50+00:00
- **Updated**: 2019-04-08 21:27:14+00:00
- **Authors**: Michele Merler, Nalini Ratha, Rogerio S. Feris, John R. Smith
- **Comment**: Updated statistics after slight modification to dataset due to
  inactive links and deletions
- **Journal**: None
- **Summary**: Face recognition is a long standing challenge in the field of Artificial Intelligence (AI). The goal is to create systems that accurately detect, recognize, verify, and understand human faces. There are significant technical hurdles in making these systems accurate, particularly in unconstrained settings due to confounding factors related to pose, resolution, illumination, occlusion, and viewpoint. However, with recent advances in neural networks, face recognition has achieved unprecedented accuracy, largely built on data-driven deep learning methods. While this is encouraging, a critical aspect that is limiting facial recognition accuracy and fairness is inherent facial diversity. Every face is different. Every face reflects something unique about us. Aspects of our heritage - including race, ethnicity, culture, geography - and our individual identify - age, gender, and other visible manifestations of self-expression, are reflected in our faces. We expect face recognition to work equally accurately for every face. Face recognition needs to be fair. As we rely on data-driven methods to create face recognition technology, we need to ensure necessary balance and coverage in training data. However, there are still scientific questions about how to represent and extract pertinent facial features and quantitatively measure facial diversity. Towards this goal, Diversity in Faces (DiF) provides a data set of one million annotated human face images for advancing the study of facial diversity. The annotations are generated using ten well-established facial coding schemes from the scientific literature. The facial coding schemes provide human-interpretable quantitative measures of facial features. We believe that by making the extracted coding schemes available on a large set of faces, we can accelerate research and development towards creating more fair and accurate facial recognition systems.



### Semantic Redundancies in Image-Classification Datasets: The 10% You Don't Need
- **Arxiv ID**: http://arxiv.org/abs/1901.11409v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.11409v1)
- **Published**: 2019-01-29 18:27:37+00:00
- **Updated**: 2019-01-29 18:27:37+00:00
- **Authors**: Vighnesh Birodkar, Hossein Mobahi, Samy Bengio
- **Comment**: None
- **Journal**: None
- **Summary**: Large datasets have been crucial to the success of deep learning models in the recent years, which keep performing better as they are trained with more labelled data. While there have been sustained efforts to make these models more data-efficient, the potential benefit of understanding the data itself, is largely untapped. Specifically, focusing on object recognition tasks, we wonder if for common benchmark datasets we can do better than random subsets of the data and find a subset that can generalize on par with the full dataset when trained on. To our knowledge, this is the first result that can find notable redundancies in CIFAR-10 and ImageNet datasets (at least 10%). Interestingly, we observe semantic correlations between required and redundant images. We hope that our findings can motivate further research into identifying additional redundancies and exploiting them for more efficient training or data-collection.



### Geometric Interpretation of side-sharing and point-sharing solutions in the P3P Problem
- **Arxiv ID**: http://arxiv.org/abs/1902.00105v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00105v1)
- **Published**: 2019-01-29 18:41:34+00:00
- **Updated**: 2019-01-29 18:41:34+00:00
- **Authors**: Bo wang, Hao Hu, Caixia Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: It is well known that the P3P problem could have 1, 2, 3 and at most 4 positive solutions under different configurations among its 3 control points and the position of the optical center. Since in any real applications, the knowledge on the exact number of possible solutions is a prerequisite for selecting the right one among all the possible solutions, the study on the phenomenon of multiple solutions in the P3P problem has been an active topic . In this work, we provide some new geometric interpretations on the multi-solution phenomenon in the P3P problem, our main results include: (1): The necessary and sufficient condition for the P3P problem to have a pair of side-sharing solutions is the two optical centers of the solutions both lie on one of the 3 vertical planes to the base plane of control points; (2): The necessary and sufficient condition for the P3P problem to have a pair of point-sharing solutions is the two optical centers of the solutions both lie on one of the 3 so-called skewed danger cylinders;(3): If the P3P problem has other solutions in addition to a pair of side-sharing ( point-sharing) solutions, these remaining solutions must be a point-sharing ( side-sharing ) pair. In a sense, the side-sharing pair and the point-sharing pair are companion pairs. In sum, our results provide some new insights into the nature of the multi-solution phenomenon in the P3P problem, in addition to their academic value, they could also be used as some theoretical guidance for practitioners in real applications to avoid occurrence of multiple solutions by properly arranging the control points.



### Time-Space tradeoff in deep learning models for crop classification on satellite multi-spectral image time series
- **Arxiv ID**: http://arxiv.org/abs/1901.10503v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.10503v1)
- **Published**: 2019-01-29 19:25:52+00:00
- **Updated**: 2019-01-29 19:25:52+00:00
- **Authors**: Vivien Sainte Fare Garnot, Loic Landrieu, Sebastien Giordano, Nesrine Chehata
- **Comment**: Currently under review
- **Journal**: International Geoscience and Remote Sensing Symposium 2019
- **Summary**: In this article, we investigate several structured deep learning models for crop type classification on multi-spectral time series. In particular, our aim is to assess the respective importance of spatial and temporal structures in such data. With this objective, we consider several designs of convolutional, recurrent, and hybrid neural networks, and assess their performance on a large dataset of freely available Sentinel-2 imagery. We find that the best-performing approaches are hybrid configurations for which most of the parameters (up to 90%) are allocated to modeling the temporal structure of the data. Our results thus constitute a set of guidelines for the design of bespoke deep learning models for crop type classification.



### Adversarial Examples Are a Natural Consequence of Test Error in Noise
- **Arxiv ID**: http://arxiv.org/abs/1901.10513v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.10513v1)
- **Published**: 2019-01-29 20:01:39+00:00
- **Updated**: 2019-01-29 20:01:39+00:00
- **Authors**: Nic Ford, Justin Gilmer, Nicolas Carlini, Dogus Cubuk
- **Comment**: None
- **Journal**: None
- **Summary**: Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to small modifications of a correctly handled input. Less surprisingly, image classifiers also lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this paper we provide both empirical and theoretical evidence that these are two manifestations of the same underlying phenomenon, establishing close connections between the adversarial robustness and corruption robustness research programs. This suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. Based on our results we recommend that future adversarial defenses consider evaluating the robustness of their methods to distributional shift with benchmarks such as Imagenet-C.



### Attention-driven Tree-structured Convolutional LSTM for High Dimensional Data Understanding
- **Arxiv ID**: http://arxiv.org/abs/1902.10053v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10053v1)
- **Published**: 2019-01-29 20:34:17+00:00
- **Updated**: 2019-01-29 20:34:17+00:00
- **Authors**: Bin Kong, Xin Wang, Junjie Bai, Yi Lu, Feng Gao, Kunlin Cao, Qi Song, Shaoting Zhang, Siwei Lyu, Youbing Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Modeling the sequential information of image sequences has been a vital step of various vision tasks and convolutional long short-term memory (ConvLSTM) has demonstrated its superb performance in such spatiotemporal problems. Nevertheless, the hierarchical data structures in a significant amount of tasks (e.g., human body parts and vessel/airway tree in biomedical images) cannot be properly modeled by sequential models. Thus, ConvLSTM is not suitable for tree-structured image data analysis. In order to address these limitations, we present tree-structured ConvLSTM models for tree-structured image analysis tasks which can be trained end-to-end. To demonstrate the effectiveness of the proposed tree-structured ConvLSTM model, we present a tree-structured segmentation framework which consists of a tree-structured ConvLSTM and an attention fully convolutional network (FCN) model. The proposed framework is extensively validated on four large-scale coronary artery datasets. The results demonstrate the effectiveness and efficiency of the proposed method.



### Trading-off Accuracy and Energy of Deep Inference on Embedded Systems: A Co-Design Approach
- **Arxiv ID**: http://arxiv.org/abs/1901.10584v1
- **DOI**: 10.1109/TCAD.2018.2857338
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.10584v1)
- **Published**: 2019-01-29 22:07:41+00:00
- **Updated**: 2019-01-29 22:07:41+00:00
- **Authors**: Nitthilan Kannappan Jayakodi, Anwesha Chatterjee, Wonje Choi, Janardhan Rao Doppa, Partha Pratim Pande
- **Comment**: Published in IEEE Trans. on CAD of Integrated Circuits and Systems
- **Journal**: Vol. 37, No. 11, Pages 2881-2893, Nov 2018
- **Summary**: Deep neural networks have seen tremendous success for different modalities of data including images, videos, and speech. This success has led to their deployment in mobile and embedded systems for real-time applications. However, making repeated inferences using deep networks on embedded systems poses significant challenges due to constrained resources (e.g., energy and computing power). To address these challenges, we develop a principled co-design approach. Building on prior work, we develop a formalism referred to as Coarse-to-Fine Networks (C2F Nets) that allow us to employ classifiers of varying complexity to make predictions. We propose a principled optimization algorithm to automatically configure C2F Nets for a specified trade-off between accuracy and energy consumption for inference. The key idea is to select a classifier on-the-fly whose complexity is proportional to the hardness of the input example: simple classifiers for easy inputs and complex classifiers for hard inputs. We perform comprehensive experimental evaluation using four different C2F Net architectures on multiple real-world image classification tasks. Our results show that optimized C2F Net can reduce the Energy Delay Product (EDP) by 27 to 60 percent with no loss in accuracy when compared to the baseline solution, where all predictions are made using the most complex classifier in C2F Net.



### Rare geometries: revealing rare categories via dimension-driven statistics
- **Arxiv ID**: http://arxiv.org/abs/1901.10585v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.10585v2)
- **Published**: 2019-01-29 22:09:42+00:00
- **Updated**: 2019-05-28 04:36:34+00:00
- **Authors**: Henry Kvinge, Elin Farnell, Jingya Li, Yujia Chen
- **Comment**: 9 pages. Section IV substantially expanded with minor improvements to
  other parts of the paper. Two new co-authors responsible for implementation
  of the algorithm on real data added
- **Journal**: None
- **Summary**: In many situations, classes of data points of primary interest also happen to be those that are least numerous. A well-known example is detection of fraudulent transactions among the collection of all financial transactions, the vast majority of which are legitimate. These types of problems fall under the label of `rare-category detection.' There are two challenging aspects of these problems. The first is a general lack of labeled examples of the rare class and the second is the potential non-separability of the rare class from the majority (in terms of available features). Statistics related to the geometry of the rare class (such as its intrinsic dimension) can be significantly different from those for the majority class, reflecting the different dynamics driving variation in the different classes. In this paper we present a new supervised learning algorithm that uses a dimension-driven statistic, called the kappa-profile, to classify whether unlabeled points belong to a rare class. Our algorithm requires very few labeled examples and is invariant with respect to translation so that it performs equivalently on both separable and non-separable classes.



### Deep Active Learning for Efficient Training of a LiDAR 3D Object Detector
- **Arxiv ID**: http://arxiv.org/abs/1901.10609v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.10609v2)
- **Published**: 2019-01-29 23:19:26+00:00
- **Updated**: 2019-05-05 14:30:55+00:00
- **Authors**: Di Feng, Xiao Wei, Lars Rosenbaum, Atsuto Maki, Klaus Dietmayer
- **Comment**: 30th IEEE Intelligent Vehicles Symposium
- **Journal**: None
- **Summary**: Training a deep object detector for autonomous driving requires a huge amount of labeled data. While recording data via on-board sensors such as camera or LiDAR is relatively easy, annotating data is very tedious and time-consuming, especially when dealing with 3D LiDAR points or radar data. Active learning has the potential to minimize human annotation efforts while maximizing the object detector's performance. In this work, we propose an active learning method to train a LiDAR 3D object detector with the least amount of labeled training data necessary. The detector leverages 2D region proposals generated from the RGB images to reduce the search space of objects and speed up the learning process. Experiments show that our proposed method works under different uncertainty estimations and query functions, and can save up to 60% of the labeling efforts while reaching the same network performance.



