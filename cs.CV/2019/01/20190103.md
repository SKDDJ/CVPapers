# Arxiv Papers in cs.CV on 2019-01-03
### Adaptive Locality Preserving Regression
- **Arxiv ID**: http://arxiv.org/abs/1901.00563v1
- **DOI**: 10.1109/TCSVT.2018.2889727
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1901.00563v1)
- **Published**: 2019-01-03 00:36:23+00:00
- **Updated**: 2019-01-03 00:36:23+00:00
- **Authors**: Jie Wen, Zuofeng Zhong, Zheng Zhang, Lunke Fei, Zhihui Lai, Runze Chen
- **Comment**: The paper has been accepted by IEEE Transactions on Circuits and
  Systems for Video Technology (TCSVT), and the code can be available at
  https://drive.google.com/file/d/1iNzONkRByIaUhXwdEhOkkh_0d2AAXNE8/view
- **Journal**: IEEE Transactions on Circuits and Systems for Video Technology,
  2018
- **Summary**: This paper proposes a novel discriminative regression method, called adaptive locality preserving regression (ALPR) for classification. In particular, ALPR aims to learn a more flexible and discriminative projection that not only preserves the intrinsic structure of data, but also possesses the properties of feature selection and interpretability. To this end, we introduce a target learning technique to adaptively learn a more discriminative and flexible target matrix rather than the pre-defined strict zero-one label matrix for regression. Then a locality preserving constraint regularized by the adaptive learned weights is further introduced to guide the projection learning, which is beneficial to learn a more discriminative projection and avoid overfitting. Moreover, we replace the conventional `Frobenius norm' with the special l21 norm to constrain the projection, which enables the method to adaptively select the most important features from the original high-dimensional data for feature extraction. In this way, the negative influence of the redundant features and noises residing in the original data can be greatly eliminated. Besides, the proposed method has good interpretability for features owing to the row-sparsity property of the l21 norm. Extensive experiments conducted on the synthetic database with manifold structure and many real-world databases prove the effectiveness of the proposed method.



### Towards Personalized Management of Type B Aortic Dissection Using STENT: a STandard cta database with annotation of the ENtire aorta and True-false lumen
- **Arxiv ID**: http://arxiv.org/abs/1901.04584v2
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1901.04584v2)
- **Published**: 2019-01-03 02:10:35+00:00
- **Updated**: 2019-01-16 02:03:13+00:00
- **Authors**: Jianning Li, Long Cao, Yangyang Ge, Bowen Meng, Cheng Wang, Wei Guo
- **Comment**: This article has been removed by arXiv administrators because the
  submitter did not have the rights to agree to the license at the time of
  submission
- **Journal**: None
- **Summary**: Type B Aortic Dissection(TBAD) is a rare aortic disease with a high 5-year mortality.Personalized and precise management of TBAD has been increasingly desired in clinic which requires the geometric parameters of TBAD specific to the patient be measured accurately.This remains to be a challenging task for vascular surgeons as manual measurement is highly subjective and imprecise. To solve this problem,we introduce STENT-a STandard cta database with annotation of the ENtire aorta and True-false lumen. The database contains 274 CT angiography (CTA) scans from 274 unique TBAD patients and is split into a training set(254 cases including 210 preoperative and 44 postoperative scans ) and a test set(20 cases).Based on STENT,we develop a series of methods including automated TBAD segmentation and automated measurement of TBAD parameters that facilitate personalized and precise management of the disease. In this work, the database and the proposed methods are thoroughly introduced and evaluated and the results of our study shows the feasibility and effectiveness of our approach to easing the decision-making process for vascular surgeons during personalized TBAD management.



### A Remote Sensing Image Dataset for Cloud Removal
- **Arxiv ID**: http://arxiv.org/abs/1901.00600v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00600v1)
- **Published**: 2019-01-03 03:43:38+00:00
- **Updated**: 2019-01-03 03:43:38+00:00
- **Authors**: Daoyu Lin, Guangluan Xu, Xiaoke Wang, Yang Wang, Xian Sun, Kun Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Cloud-based overlays are often present in optical remote sensing images, thus limiting the application of acquired data. Removing clouds is an indispensable pre-processing step in remote sensing image analysis. Deep learning has achieved great success in the field of remote sensing in recent years, including scene classification and change detection. However, deep learning is rarely applied in remote sensing image removal clouds. The reason is the lack of data sets for training neural networks. In order to solve this problem, this paper first proposed the Remote sensing Image Cloud rEmoving dataset (RICE). The proposed dataset consists of two parts: RICE1 contains 500 pairs of images, each pair has images with cloud and cloudless size of 512*512; RICE2 contains 450 sets of images, each set contains three 512*512 size images. , respectively, the reference picture without clouds, the picture of the cloud and the mask of its cloud. The dataset is freely available at \url{https://github.com/BUPTLdy/RICE_DATASET}.



### Text line Segmentation in Compressed Representation of Handwritten Document using Tunneling Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1901.11477v1
- **DOI**: 10.18201/ijisae.2018448451
- **Categories**: **cs.CV**, cs.AI, cs.DS
- **Links**: [PDF](http://arxiv.org/pdf/1901.11477v1)
- **Published**: 2019-01-03 05:19:38+00:00
- **Updated**: 2019-01-03 05:19:38+00:00
- **Authors**: Amarnath R, P Nagabhushan
- **Comment**: Compressed Representation, Handwritten Document Image, Text-Line
  Terminal Point, Text-Line Segmentation, Search Space, Grid
- **Journal**: International Journal of Intelligent Systems and Applications in
  Engineering, Vol 6, No 4 (2018)
- **Summary**: In this research work, we perform text line segmentation directly in compressed representation of an unconstrained handwritten document image. In this relation, we make use of text line terminal points which is the current state-of-the-art. The terminal points spotted along both margins (left and right) of a document image for every text line are considered as source and target respectively. The tunneling algorithm uses a single agent (or robot) to identify the coordinate positions in the compressed representation to perform text-line segmentation of the document. The agent starts at a source point and progressively tunnels a path routing in between two adjacent text lines and reaches the probable target. The agent's navigation path from source to the target bypassing obstacles, if any, results in segregating the two adjacent text lines. However, the target point would be known only when the agent reaches the destination; this is applicable for all source points and henceforth we could analyze the correspondence between source and target nodes. Artificial Intelligence in Expert systems, dynamic programming and greedy strategies are employed for every search space while tunneling. An exhaustive experimentation is carried out on various benchmark datasets including ICDAR13 and the performances are reported.



### Volumetric Convolution: Automatic Representation Learning in Unit Ball
- **Arxiv ID**: http://arxiv.org/abs/1901.00616v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.00616v1)
- **Published**: 2019-01-03 05:53:25+00:00
- **Updated**: 2019-01-03 05:53:25+00:00
- **Authors**: Sameera Ramasinghe, Salman Khan, Nick Barnes
- **Comment**: None
- **Journal**: None
- **Summary**: Convolution is an efficient technique to obtain abstract feature representations using hierarchical layers in deep networks. Although performing convolution in Euclidean geometries is fairly straightforward, its extension to other topological spaces---such as a sphere ($\mathbb{S}^2$) or a unit ball ($\mathbb{B}^3$)---entails unique challenges. In this work, we propose a novel `\emph{volumetric convolution}' operation that can effectively convolve arbitrary functions in $\mathbb{B}^3$. We develop a theoretical framework for \emph{volumetric convolution} based on Zernike polynomials and efficiently implement it as a differentiable and an easily pluggable layer for deep networks. Furthermore, our formulation leads to derivation of a novel formula to measure the symmetry of a function in $\mathbb{B}^3$ around an arbitrary axis, that is useful in 3D shape analysis tasks. We demonstrate the efficacy of proposed volumetric convolution operation on a possible use-case i.e., 3D object recognition task.



### Edge-Semantic Learning Strategy for Layout Estimation in Indoor Environment
- **Arxiv ID**: http://arxiv.org/abs/1901.00621v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00621v1)
- **Published**: 2019-01-03 06:07:56+00:00
- **Updated**: 2019-01-03 06:07:56+00:00
- **Authors**: Weidong Zhang, Wei Zhang, Jason Gu
- **Comment**: None
- **Journal**: None
- **Summary**: Visual cognition of the indoor environment can benefit from the spatial layout estimation, which is to represent an indoor scene with a 2D box on a monocular image. In this paper, we propose to fully exploit the edge and semantic information of a room image for layout estimation. More specifically, we present an encoder-decoder network with shared encoder and two separate decoders, which are composed of multiple deconvolution (transposed convolution) layers, to jointly learn the edge maps and semantic labels of a room image. We combine these two network predictions in a scoring function to evaluate the quality of the layouts, which are generated by ray sampling and from a predefined layout pool. Guided by the scoring function, we apply a novel refinement strategy to further optimize the layout hypotheses. Experimental results show that the proposed network can yield accurate estimates of edge maps and semantic labels. By fully utilizing the two different types of labels, the proposed method achieves state-of-the-art layout estimation performance on benchmark datasets.



### Baseline Desensitizing In Translation Averaging
- **Arxiv ID**: http://arxiv.org/abs/1901.00643v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00643v1)
- **Published**: 2019-01-03 08:05:24+00:00
- **Updated**: 2019-01-03 08:05:24+00:00
- **Authors**: Bingbing Zhuang, Loong-Fah Cheong, Gim Hee Lee
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Many existing translation averaging algorithms are either sensitive to disparate camera baselines and have to rely on extensive preprocessing to improve the observed Epipolar Geometry graph, or if they are robust against disparate camera baselines, require complicated optimization to minimize the highly nonlinear angular error objective. In this paper, we carefully design a simple yet effective bilinear objective function, introducing a variable to perform the requisite normalization. The objective function enjoys the baseline-insensitive property of the angular error and yet is amenable to simple and efficient optimization by block coordinate descent, with good empirical performance. A rotation-assisted Iterative Reweighted Least Squares scheme is further put forth to help deal with outliers. We also contribute towards a better understanding of the behavior of two recent convex algorithms, LUD and Shapefit/kick, clarifying the underlying subtle difference that leads to the performance gap. Finally, we demonstrate that our algorithm achieves overall superior accuracies in benchmark dataset compared to state-of-theart methods, and is also several times faster.



### Weightless Neural Network with Transfer Learning to Detect Distress in Asphalt
- **Arxiv ID**: http://arxiv.org/abs/1901.03660v1
- **DOI**: 10.22161/ijaers.5.12.40
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.03660v1)
- **Published**: 2019-01-03 09:33:22+00:00
- **Updated**: 2019-01-03 09:33:22+00:00
- **Authors**: Suayder Milhomem, Tiago da Silva Almeida, Warley Gramacho da Silva, Edeilson Milhomem da Silva, Rafael Lima de Carvalho
- **Comment**: 6 pages, 5 figures, published on IJAERS
- **Journal**: International Journal of Advanced Engineering Research and
  Science, Page No: 294-299, vol.5,no. 12, date:2018
- **Summary**: The present paper shows a solution to the problem of automatic distress detection, more precisely the detection of holes in paved roads. To do so, the proposed solution uses a weightless neural network known as Wisard to decide whether an image of a road has any kind of cracks. In addition, the proposed architecture also shows how the use of transfer learning was able to improve the overall accuracy of the decision system. As a verification step of the research, an experiment was carried out using images from the streets at the Federal University of Tocantins, Brazil. The architecture of the developed solution presents a result of 85.71% accuracy in the dataset, proving to be superior to approaches of the state-of-the-art.



### Active Learning with TensorBoard Projector
- **Arxiv ID**: http://arxiv.org/abs/1901.00675v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00675v1)
- **Published**: 2019-01-03 10:42:36+00:00
- **Updated**: 2019-01-03 10:42:36+00:00
- **Authors**: Francois Luus, Naweed Khan, Ismail Akhalwaya
- **Comment**: None
- **Journal**: None
- **Summary**: An ML-based system for interactive labeling of image datasets is contributed in TensorBoard Projector to speed up image annotation performed by humans. The tool visualizes feature spaces and makes it directly editable by online integration of applied labels, and it is a system for verifying and managing machine learning data pertaining to labels. We propose realistic annotation emulation to evaluate the system design of interactive active learning, based on our improved semi-supervised extension of t-SNE dimensionality reduction. Our active learning tool can significantly increase labeling efficiency compared to uncertainty sampling, and we show that less than 100 labeling actions are typically sufficient for good classification on a variety of specialized image datasets. Our contribution is unique given that it needs to perform dimensionality reduction, feature space visualization and editing, interactive label propagation, low-complexity active learning, human perceptual modeling, annotation emulation and unsupervised feature extraction for specialized datasets in a production-quality implementation.



### GeoNet: Deep Geodesic Networks for Point Cloud Analysis
- **Arxiv ID**: http://arxiv.org/abs/1901.00680v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00680v1)
- **Published**: 2019-01-03 11:02:14+00:00
- **Updated**: 2019-01-03 11:02:14+00:00
- **Authors**: Tong He, Haibin Huang, Li Yi, Yuqian Zhou, Chihao Wu, Jue Wang, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: Surface-based geodesic topology provides strong cues for object semantic analysis and geometric modeling. However, such connectivity information is lost in point clouds. Thus we introduce GeoNet, the first deep learning architecture trained to model the intrinsic structure of surfaces represented as point clouds. To demonstrate the applicability of learned geodesic-aware representations, we propose fusion schemes which use GeoNet in conjunction with other baseline or backbone networks, such as PU-Net and PointNet++, for down-stream point cloud analysis. Our method improves the state-of-the-art on multiple representative tasks that can benefit from understandings of the underlying surface topology, including point upsampling, normal estimation, mesh reconstruction and non-rigid shape classification.



### Generating Multiple Objects at Spatially Distinct Locations
- **Arxiv ID**: http://arxiv.org/abs/1901.00686v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1901.00686v1)
- **Published**: 2019-01-03 11:18:52+00:00
- **Updated**: 2019-01-03 11:18:52+00:00
- **Authors**: Tobias Hinz, Stefan Heinrich, Stefan Wermter
- **Comment**: Published at ICLR 2019
- **Journal**: None
- **Summary**: Recent improvements to Generative Adversarial Networks (GANs) have made it possible to generate realistic images in high resolution based on natural language descriptions such as image captions. Furthermore, conditional GANs allow us to control the image generation process through labels or even natural language descriptions. However, fine-grained control of the image layout, i.e. where in the image specific objects should be located, is still difficult to achieve. This is especially true for images that should contain multiple distinct objects at different spatial locations. We introduce a new approach which allows us to control the location of arbitrarily many objects within an image by adding an object pathway to both the generator and the discriminator. Our approach does not need a detailed semantic layout but only bounding boxes and the respective labels of the desired objects are needed. The object pathway focuses solely on the individual objects and is iteratively applied at the locations specified by the bounding boxes. The global pathway focuses on the image background and the general image layout. We perform experiments on the Multi-MNIST, CLEVR, and the more complex MS-COCO data set. Our experiments show that through the use of the object pathway we can control object locations within images and can model complex scenes with multiple objects at various locations. We further show that the object pathway focuses on the individual objects and learns features relevant for these, while the global pathway focuses on global image characteristics and the image background.



### A Hierarchical Grocery Store Image Dataset with Visual and Semantic Labels
- **Arxiv ID**: http://arxiv.org/abs/1901.00711v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00711v1)
- **Published**: 2019-01-03 13:28:08+00:00
- **Updated**: 2019-01-03 13:28:08+00:00
- **Authors**: Marcus Klasson, Cheng Zhang, Hedvig Kjellström
- **Comment**: To appear in IEEE Winter Conference on Applications of Computer
  Vision (WACV) 2019
- **Journal**: None
- **Summary**: Image classification models built into visual support systems and other assistive devices need to provide accurate predictions about their environment. We focus on an application of assistive technology for people with visual impairments, for daily activities such as shopping or cooking. In this paper, we provide a new benchmark dataset for a challenging task in this application - classification of fruits, vegetables, and refrigerated products, e.g. milk packages and juice cartons, in grocery stores. To enable the learning process to utilize multiple sources of structured information, this dataset not only contains a large volume of natural images but also includes the corresponding information of the product from an online shopping website. Such information encompasses the hierarchical structure of the object classes, as well as an iconic image of each type of object. This dataset can be used to train and evaluate image classification models for helping visually impaired people in natural environments. Additionally, we provide benchmark results evaluated on pretrained convolutional neural networks often used for image understanding purposes, and also a multi-view variational autoencoder, which is capable of utilizing the rich product information in the dataset.



### Face Recognition: A Novel Multi-Level Taxonomy based Survey
- **Arxiv ID**: http://arxiv.org/abs/1901.00713v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00713v1)
- **Published**: 2019-01-03 13:47:53+00:00
- **Updated**: 2019-01-03 13:47:53+00:00
- **Authors**: Alireza Sepas-Moghaddam, Fernando Pereira, Paulo Lobato Correia
- **Comment**: This paper is a preprint of a paper submitted to IET Biometrics. If
  accepted, the copy of record will be available at the IET Digital Library
- **Journal**: None
- **Summary**: In a world where security issues have been gaining growing importance, face recognition systems have attracted increasing attention in multiple application areas, ranging from forensics and surveillance to commerce and entertainment. To help understanding the landscape and abstraction levels relevant for face recognition systems, face recognition taxonomies allow a deeper dissection and comparison of the existing solutions. This paper proposes a new, more encompassing and richer multi-level face recognition taxonomy, facilitating the organization and categorization of available and emerging face recognition solutions; this taxonomy may also guide researchers in the development of more efficient face recognition solutions. The proposed multi-level taxonomy considers levels related to the face structure, feature support and feature extraction approach. Following the proposed taxonomy, a comprehensive survey of representative face recognition solutions is presented. The paper concludes with a discussion on current algorithmic and application related challenges which may define future research directions for face recognition.



### Demystifying Multi-Faceted Video Summarization: Tradeoff Between Diversity,Representation, Coverage and Importance
- **Arxiv ID**: http://arxiv.org/abs/1901.01153v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01153v1)
- **Published**: 2019-01-03 14:03:37+00:00
- **Updated**: 2019-01-03 14:03:37+00:00
- **Authors**: Vishal Kaushal, Rishabh Iyer, Khoshrav Doctor, Anurag Sahoo, Pratik Dubal, Suraj Kothawade, Rohan Mahadev, Kunal Dargan, Ganesh Ramakrishnan
- **Comment**: Accepted to WACV 2019. arXiv admin note: substantial text overlap
  with arXiv:1704.01466, arXiv:1809.08846
- **Journal**: None
- **Summary**: This paper addresses automatic summarization of videos in a unified manner. In particular, we propose a framework for multi-faceted summarization for extractive, query base and entity summarization (summarization at the level of entities like objects, scenes, humans and faces in the video). We investigate several summarization models which capture notions of diversity, coverage, representation and importance, and argue the utility of these different models depending on the application. While most of the prior work on submodular summarization approaches has focused oncombining several models and learning weighted mixtures, we focus on the explainability of different models and featurizations, and how they apply to different domains. We also provide implementation details on summarization systems and the different modalities involved. We hope that the study from this paper will give insights into practitioners to appropriately choose the right summarization models for the problems at hand.



### Learning From Less Data: A Unified Data Subset Selection and Active Learning Framework for Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1901.01151v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01151v1)
- **Published**: 2019-01-03 14:07:08+00:00
- **Updated**: 2019-01-03 14:07:08+00:00
- **Authors**: Vishal Kaushal, Rishabh Iyer, Suraj Kothawade, Rohan Mahadev, Khoshrav Doctor, Ganesh Ramakrishnan
- **Comment**: Accepted to WACV 2019. arXiv admin note: substantial text overlap
  with arXiv:1805.11191
- **Journal**: None
- **Summary**: Supervised machine learning based state-of-the-art computer vision techniques are in general data hungry. Their data curation poses the challenges of expensive human labeling, inadequate computing resources and larger experiment turn around times. Training data subset selection and active learning techniques have been proposed as possible solutions to these challenges. A special class of subset selection functions naturally model notions of diversity, coverage and representation and can be used to eliminate redundancy thus lending themselves well for training data subset selection. They can also help improve the efficiency of active learning in further reducing human labeling efforts by selecting a subset of the examples obtained using the conventional uncertainty sampling based techniques. In this work, we empirically demonstrate the effectiveness of two diversity models, namely the Facility-Location and Dispersion models for training-data subset selection and reducing labeling effort. We demonstrate this across the board for a variety of computer vision tasks including Gender Recognition, Face Recognition, Scene Recognition, Object Detection and Object Recognition. Our results show that diversity based subset selection done in the right way can increase the accuracy by upto 5 - 10% over existing baselines, particularly in settings in which less training data is available. This allows the training of complex machine learning models like Convolutional Neural Networks with much less training data and labeling costs while incurring minimal performance loss.



### Mapping Informal Settlements in Developing Countries using Machine Learning and Low Resolution Multi-spectral Data
- **Arxiv ID**: http://arxiv.org/abs/1901.00861v3
- **DOI**: 10.1145/3306618.3314253
- **Categories**: **cs.CY**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.00861v3)
- **Published**: 2019-01-03 16:51:40+00:00
- **Updated**: 2019-05-30 11:11:39+00:00
- **Authors**: Bradley Gram-Hansen, Patrick Helber, Indhu Varatharajan, Faiza Azam, Alejandro Coca-Castro, Veronika Kopackova, Piotr Bilinski
- **Comment**: Published at the AAAI/ACM Conference on AI, ethics and society.
  Extended results from our previous workshop: arXiv:1812.00812
- **Journal**: AAAI/ACM Conference on AI, Ethics, and Society (AIES 2019)
- **Summary**: Informal settlements are home to the most socially and economically vulnerable people on the planet. In order to deliver effective economic and social aid, non-government organizations (NGOs), such as the United Nations Children's Fund (UNICEF), require detailed maps of the locations of informal settlements. However, data regarding informal and formal settlements is primarily unavailable and if available is often incomplete. This is due, in part, to the cost and complexity of gathering data on a large scale. To address these challenges, we, in this work, provide three contributions. 1) A brand new machine learning data-set, purposely developed for informal settlement detection. 2) We show that it is possible to detect informal settlements using freely available low-resolution (LR) data, in contrast to previous studies that use very-high resolution (VHR) satellite and aerial imagery, something that is cost-prohibitive for NGOs. 3) We demonstrate two effective classification schemes on our curated data set, one that is cost-efficient for NGOs and another that is cost-prohibitive for NGOs, but has additional utility. We integrate these schemes into a semi-automated pipeline that converts either a LR or VHR satellite image into a binary map that encodes the locations of informal settlements.



### CLEVR-Ref+: Diagnosing Visual Reasoning with Referring Expressions
- **Arxiv ID**: http://arxiv.org/abs/1901.00850v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.00850v2)
- **Published**: 2019-01-03 18:58:06+00:00
- **Updated**: 2019-04-06 19:59:25+00:00
- **Authors**: Runtao Liu, Chenxi Liu, Yutong Bai, Alan Yuille
- **Comment**: To appear in CVPR 2019. All data and code concerning CLEVR-Ref+ and
  IEP-Ref have been released at https://cs.jhu.edu/~cxliu/2019/clevr-ref+
- **Journal**: None
- **Summary**: Referring object detection and referring image segmentation are important tasks that require joint understanding of visual information and natural language. Yet there has been evidence that current benchmark datasets suffer from bias, and current state-of-the-art models cannot be easily evaluated on their intermediate reasoning process. To address these issues and complement similar efforts in visual question answering, we build CLEVR-Ref+, a synthetic diagnostic dataset for referring expression comprehension. The precise locations and attributes of the objects are readily available, and the referring expressions are automatically associated with functional programs. The synthetic nature allows control over dataset bias (through sampling strategy), and the modular programs enable intermediate reasoning ground truth without human annotators.   In addition to evaluating several state-of-the-art models on CLEVR-Ref+, we also propose IEP-Ref, a module network approach that significantly outperforms other models on our dataset. In particular, we present two interesting and important findings using IEP-Ref: (1) the module trained to transform feature maps into segmentation masks can be attached to any intermediate module to reveal the entire reasoning process step-by-step; (2) even if all training data has at least one object referred, IEP-Ref can correctly predict no-foreground when presented with false-premise referring expressions. To the best of our knowledge, this is the first direct and quantitative proof that neural modules behave in the way they are intended.



### Polarimetric Thermal to Visible Face Verification via Attribute Preserved Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1901.00889v1
- **DOI**: 10.1109/BTAS.2018.8698554
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00889v1)
- **Published**: 2019-01-03 19:38:33+00:00
- **Updated**: 2019-01-03 19:38:33+00:00
- **Authors**: Xing Di, He Zhang, Vishal M. Patel
- **Comment**: This work has been accepted by the 9th IEEE International Conference
  on Biometrics: Theory, Applications, and Systems (BTAS 2018)
- **Journal**: None
- **Summary**: Thermal to visible face verification is a challenging problem due to the large domain discrepancy between the modalities. Existing approaches either attempt to synthesize visible faces from thermal faces or extract robust features from these modalities for cross-modal matching. In this paper, we take a different approach in which we make use of the attributes extracted from the visible image to synthesize the attribute-preserved visible image from the input thermal image for cross-modal matching. A pre-trained VGG-Face network is used to extract the attributes from the visible image. Then, a novel Attribute Preserved Generative Adversarial Network (AP-GAN) is proposed to synthesize the visible image from the thermal image guided by the extracted attributes. Finally, a deep network is used to extract features from the synthesized image and the input visible image for verification. Extensive experiments on the ARL Polarimetric face dataset show that the proposed method achieves significant improvements over the state-of-the-art methods.



### I Can See Clearly Now : Image Restoration via De-Raining
- **Arxiv ID**: http://arxiv.org/abs/1901.00893v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00893v1)
- **Published**: 2019-01-03 19:45:39+00:00
- **Updated**: 2019-01-03 19:45:39+00:00
- **Authors**: Horia Porav, Tom Bruls, Paul Newman
- **Comment**: Submitted to ICRA2019
- **Journal**: None
- **Summary**: We present a method for improving segmentation tasks on images affected by adherent rain drops and streaks. We introduce a novel stereo dataset recorded using a system that allows one lens to be affected by real water droplets while keeping the other lens clear. We train a denoising generator using this dataset and show that it is effective at removing the effect of real water droplets, in the context of image reconstruction and road marking segmentation. To further test our de-noising approach, we describe a method of adding computer-generated adherent water droplets and streaks to any images, and use this technique as a proxy to demonstrate the effectiveness of our model in the context of general semantic segmentation. We benchmark our results using the CamVid road marking segmentation dataset, Cityscapes semantic segmentation datasets and our own real-rain dataset, and show significant improvement on all tasks.



### Imminent Collision Mitigation with Reinforcement Learning and Vision
- **Arxiv ID**: http://arxiv.org/abs/1901.00898v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.00898v1)
- **Published**: 2019-01-03 20:09:40+00:00
- **Updated**: 2019-01-03 20:09:40+00:00
- **Authors**: Horia Porav, Paul Newman
- **Comment**: Presented at ITSC2018
- **Journal**: None
- **Summary**: This work examines the role of reinforcement learning in reducing the severity of on-road collisions by controlling velocity and steering in situations in which contact is imminent. We construct a model, given camera images as input, that is capable of learning and predicting the dynamics of obstacles, cars and pedestrians, and train our policy using this model. Two policies that control both braking and steering are compared against a baseline where the only action taken is (conventional) braking in a straight line. The two policies are trained using two distinct reward structures, one where any and all collisions incur a fixed penalty, and a second one where the penalty is calculated based on already established delta-v models of injury severity. The results show that both policies exceed the performance of the baseline, with the policy trained using injury models having the highest performance.



### Local Area Transform for Cross-Modality Correspondence Matching and Deep Scene Recognition
- **Arxiv ID**: http://arxiv.org/abs/1901.00927v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00927v1)
- **Published**: 2019-01-03 22:12:20+00:00
- **Updated**: 2019-01-03 22:12:20+00:00
- **Authors**: Seungchul Ryu
- **Comment**: Doctoral Dissertation
- **Journal**: None
- **Summary**: Establishing correspondences is a fundamental task in variety of image processing and computer vision applications. In particular, finding the correspondences between a non-linearly deformed image pair induced by different modality conditions is a challenging problem. This paper describes a efficient but powerful image transform called local area transform (LAT) for modality-robust correspondence estimation. Specifically, LAT transforms an image from the intensity domain to the local area domain, which is invariant under nonlinear intensity deformations, especially radiometric, photometric, and spectral deformations. In addition, robust feature descriptors are reformulated with LAT for several practical applications. Furthermore, LAT-convolution layer and Aception block are proposed and, with these novel components, deep neural network called LAT-Net is proposed especially for scene recognition task. Experimental results show that LATransformed images provide a consistency for nonlinearly deformed images, even under random intensity deformations. LAT reduces the mean absolute difference as compared to conventional methods. Furthermore, the reformulation of descriptors with LAT shows superiority to conventional methods, which is a promising result for the tasks of cross-spectral and modality correspondence matching. the local area can be considered as an alternative domain to the intensity domain to achieve robust correspondence matching, image recognition, and a lot of applications: such as feature matching, stereo matching, dense correspondence matching, image recognition, and image retrieval.



