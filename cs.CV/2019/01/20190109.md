# Arxiv Papers in cs.CV on 2019-01-09
### Neural RGB->D Sensing: Depth and Uncertainty from a Video Camera
- **Arxiv ID**: http://arxiv.org/abs/1901.02571v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02571v1)
- **Published**: 2019-01-09 01:14:46+00:00
- **Updated**: 2019-01-09 01:14:46+00:00
- **Authors**: Chao Liu, Jinwei Gu, Kihwan Kim, Srinivasa Narasimhan, Jan Kautz
- **Comment**: None
- **Journal**: None
- **Summary**: Depth sensing is crucial for 3D reconstruction and scene understanding. Active depth sensors provide dense metric measurements, but often suffer from limitations such as restricted operating ranges, low spatial resolution, sensor interference, and high power consumption. In this paper, we propose a deep learning (DL) method to estimate per-pixel depth and its uncertainty continuously from a monocular video stream, with the goal of effectively turning an RGB camera into an RGB-D camera. Unlike prior DL-based methods, we estimate a depth probability distribution for each pixel rather than a single depth value, leading to an estimate of a 3D depth probability volume for each input frame. These depth probability volumes are accumulated over time under a Bayesian filtering framework as more incoming frames are processed sequentially, which effectively reduces depth uncertainty and improves accuracy, robustness, and temporal stability. Compared to prior work, the proposed approach achieves more accurate and stable results, and generalizes better to new datasets. Experimental results also show the output of our approach can be directly fed into classical RGB-D based 3D scanning methods for 3D scene reconstruction.



### Interactive Image Segmentation using Label Propagation through Complex Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.02573v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02573v1)
- **Published**: 2019-01-09 01:22:23+00:00
- **Updated**: 2019-01-09 01:22:23+00:00
- **Authors**: Fabricio Aparecido Breve
- **Comment**: Paper accepted for publication in Expert Systems With Applications
- **Journal**: None
- **Summary**: Interactive image segmentation is a topic of many studies in image processing. In a conventional approach, a user marks some pixels of the object(s) of interest and background, and an algorithm propagates these labels to the rest of the image. This paper presents a new graph-based method for interactive segmentation with two stages. In the first stage, nodes representing pixels are connected to their $k$-nearest neighbors to build a complex network with the small-world property to propagate the labels quickly. In the second stage, a regular network in a grid format is used to refine the segmentation on the object borders. Despite its simplicity, the proposed method can perform the task with high accuracy. Computer simulations are performed using some real-world images to show its effectiveness in both two-classes and multi-classes problems. It is also applied to all the images from the Microsoft GrabCut dataset for comparison, and the segmentation accuracy is comparable to those achieved by some state-of-the-art methods, while it is faster than them. In particular, it outperforms some recent approaches when the user input is composed only by a few "scribbles" draw over the objects. Its computational complexity is only linear on the image size at the best-case scenario and linearithmic in the worst case.



### Manipulation-skill Assessment from Videos with Spatial Attention Network
- **Arxiv ID**: http://arxiv.org/abs/1901.02579v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02579v2)
- **Published**: 2019-01-09 01:45:06+00:00
- **Updated**: 2019-04-10 05:03:51+00:00
- **Authors**: Zhenqiang Li, Yifei Huang, Minjie Cai, Yoichi Sato
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in computer vision have made it possible to automatically assess from videos the manipulation skills of humans in performing a task, which breeds many important applications in domains such as health rehabilitation and manufacturing. Previous methods of video-based skill assessment did not consider the attention mechanism humans use in assessing videos, limiting their performance as only a small part of video regions is informative for skill assessment. Our motivation here is to estimate attention in videos that helps to focus on critically important video regions for better skill assessment. In particular, we propose a novel RNN-based spatial attention model that considers accumulated attention state from previous frames as well as high-level knowledge about the progress of an undergoing task. We evaluate our approach on a newly collected dataset of infant grasping task and four existing datasets of hand manipulation tasks. Experiment results demonstrate that state-of-the-art performance can be achieved by considering attention in automatic skill assessment.



### Individual common dolphin identification via metric embedding learning
- **Arxiv ID**: http://arxiv.org/abs/1901.03662v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.03662v1)
- **Published**: 2019-01-09 02:29:20+00:00
- **Updated**: 2019-01-09 02:29:20+00:00
- **Authors**: Soren Bouma, Matthew D. M. Pawley, Krista Hupman, Andrew Gilman
- **Comment**: Published in IVCNZ 2018
- **Journal**: None
- **Summary**: Photo-identification (photo-id) of dolphin individuals is a commonly used technique in ecological sciences to monitor state and health of individuals, as well as to study the social structure and distribution of a population. Traditional photo-id involves a laborious manual process of matching each dolphin fin photograph captured in the field to a catalogue of known individuals.   We examine this problem in the context of open-set recognition and utilise a triplet loss function to learn a compact representation of fin images in a Euclidean embedding, where the Euclidean distance metric represents fin similarity. We show that this compact representation can be successfully learnt from a fairly small (in deep learning context) training set and still generalise well to out-of-sample identities (completely new dolphin individuals), with top-1 and top-5 test set (37 individuals) accuracy of $90.5\pm2$ and $93.6\pm1$ percent. In the presence of 1200 distractors, top-1 accuracy dropped by $12\%$; however, top-5 accuracy saw only a $2.8\%$ drop



### MSR: Multi-Scale Shape Regression for Scene Text Detection
- **Arxiv ID**: http://arxiv.org/abs/1901.02596v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02596v2)
- **Published**: 2019-01-09 04:00:03+00:00
- **Updated**: 2019-07-09 08:17:03+00:00
- **Authors**: Chuhui Xue, Shijian Lu, Wei Zhang
- **Comment**: Accepted by IJCAI19
- **Journal**: None
- **Summary**: State-of-the-art scene text detection techniques predict quadrilateral boxes that are prone to localization errors while dealing with straight or curved text lines of different orientations and lengths in scenes. This paper presents a novel multi-scale shape regression network (MSR) that is capable of locating text lines of different lengths, shapes and curvatures in scenes. The proposed MSR detects scene texts by predicting dense text boundary points that inherently capture the location and shape of text lines accurately and are also more tolerant to the variation of text line length as compared with the state of the arts using proposals or segmentation. Additionally, the multi-scale network extracts and fuses features at different scales which demonstrates superb tolerance to the text scale variation. Extensive experiments over several public datasets show that the proposed MSR obtains superior detection performance for both curved and straight text lines of different lengths and orientations.



### D3TW: Discriminative Differentiable Dynamic Time Warping for Weakly Supervised Action Alignment and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1901.02598v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02598v2)
- **Published**: 2019-01-09 04:12:01+00:00
- **Updated**: 2019-04-11 23:48:53+00:00
- **Authors**: Chien-Yi Chang, De-An Huang, Yanan Sui, Li Fei-Fei, Juan Carlos Niebles
- **Comment**: To appear in CVPR 2019
- **Journal**: None
- **Summary**: We address weakly supervised action alignment and segmentation in videos, where only the order of occurring actions is available during training. We propose Discriminative Differentiable Dynamic Time Warping (D3TW), the first discriminative model using weak ordering supervision. The key technical challenge for discriminative modeling with weak supervision is that the loss function of the ordering supervision is usually formulated using dynamic programming and is thus not differentiable. We address this challenge with a continuous relaxation of the min-operator in dynamic programming and extend the alignment loss to be differentiable. The proposed D3TW innovatively solves sequence alignment with discriminative modeling and end-to-end training, which substantially improves the performance in weakly supervised action alignment and segmentation tasks. We show that our model is able to bypass the degenerated sequence problem usually encountered in previous work and outperform the current state-of-the-art across three evaluation metrics in two challenging datasets.



### UAV-GESTURE: A Dataset for UAV Control and Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/1901.02602v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.02602v1)
- **Published**: 2019-01-09 04:35:18+00:00
- **Updated**: 2019-01-09 04:35:18+00:00
- **Authors**: Asanka G Perera, Yee Wei Law, Javaan Chahl
- **Comment**: 12 pages, 4 figures, UAVision workshop, ECCV, 2018
- **Journal**: None
- **Summary**: Current UAV-recorded datasets are mostly limited to action recognition and object tracking, whereas the gesture signals datasets were mostly recorded in indoor spaces. Currently, there is no outdoor recorded public video dataset for UAV commanding signals. Gesture signals can be effectively used with UAVs by leveraging the UAVs visual sensors and operational simplicity. To fill this gap and enable research in wider application areas, we present a UAV gesture signals dataset recorded in an outdoor setting. We selected 13 gestures suitable for basic UAV navigation and command from general aircraft handling and helicopter handling signals. We provide 119 high-definition video clips consisting of 37151 frames. The overall baseline gesture recognition performance computed using Pose-based Convolutional Neural Network (P-CNN) is 91.9 %. All the frames are annotated with body joints and gesture classes in order to extend the dataset's applicability to a wider research area including gesture recognition, action recognition, human pose recognition and situation awareness.



### Fast CNN-Based Object Tracking Using Localization Layers and Deep Features Interpolation
- **Arxiv ID**: http://arxiv.org/abs/1901.02620v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1901.02620v1)
- **Published**: 2019-01-09 06:46:38+00:00
- **Updated**: 2019-01-09 06:46:38+00:00
- **Authors**: Al-Hussein A. El-Shafie, Mohamed Zaki, Serag El-Din Habib
- **Comment**: None
- **Journal**: None
- **Summary**: Object trackers based on Convolution Neural Network (CNN) have achieved state-of-the-art performance on recent tracking benchmarks, while they suffer from slow computational speed. The high computational load arises from the extraction of the feature maps of the candidate and training patches in every video frame. The candidate and training patches are typically placed randomly around the previous target location and the estimated target location respectively. In this paper, we propose novel schemes to speed-up the processing of the CNN-based trackers. We input the whole region-of-interest once to the CNN to eliminate the redundant computations of the random candidate patches. In addition to classifying each candidate patch as an object or background, we adapt the CNN to classify the target location inside the object patches as a coarse localization step, and we employ bilinear interpolation for the CNN feature maps as a fine localization step. Moreover, bilinear interpolation is exploited to generate CNN feature maps of the training patches without actually forwarding the training patches through the network which achieves a significant reduction of the required computations. Our tracker does not rely on offline video training. It achieves competitive performance results on the OTB benchmark with 8x speed improvements compared to the equivalent tracker.



### MOANA: An Online Learned Adaptive Appearance Model for Robust Multiple Object Tracking in 3D
- **Arxiv ID**: http://arxiv.org/abs/1901.02626v2
- **DOI**: 10.1109/ACCESS.2019.2903121
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02626v2)
- **Published**: 2019-01-09 07:58:30+00:00
- **Updated**: 2019-03-07 00:52:14+00:00
- **Authors**: Zheng Tang, Jenq-Neng Hwang
- **Comment**: Accepted to be published at IEEE Access (Special Section: AI-Driven
  Big Data Processing: Theory, Methodology, and Applications)
- **Journal**: None
- **Summary**: Multiple object tracking has been a challenging field, mainly due to noisy detection sets and identity switch caused by occlusion and similar appearance among nearby targets. Previous works rely on appearance models built on individual or several selected frames for the comparison of features, but they cannot encode long-term appearance changes caused by pose, viewing angle and lighting conditions. In this work, we propose an adaptive model that learns online a relatively long-term appearance change of each target. The proposed model is compatible with any feature of fixed dimension or their combination, whose learning rates are dynamically controlled by adaptive update and spatial weighting schemes. To handle occlusion and nearby objects sharing similar appearance, we also design cross-matching and re-identification schemes based on the application of the proposed adaptive appearance models. Additionally, the 3D geometry information is effectively incorporated in our formulation for data association. The proposed method outperforms all the state-of-the-art on the MOTChallenge 3D benchmark and achieves real-time computation with only a standard desktop CPU. It has also shown superior performance over the state-of-the-art on the 2D benchmark of MOTChallenge.



### A Biologically Inspired Visual Working Memory for Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.03665v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.03665v1)
- **Published**: 2019-01-09 09:12:56+00:00
- **Updated**: 2019-01-09 09:12:56+00:00
- **Authors**: Ethan Harris, Mahesan Niranjan, Jonathon Hare
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to look multiple times through a series of pose-adjusted glimpses is fundamental to human vision. This critical faculty allows us to understand highly complex visual scenes. Short term memory plays an integral role in aggregating the information obtained from these glimpses and informing our interpretation of the scene. Computational models have attempted to address glimpsing and visual attention but have failed to incorporate the notion of memory. We introduce a novel, biologically inspired visual working memory architecture that we term the Hebb-Rosenblatt memory. We subsequently introduce a fully differentiable Short Term Attentive Working Memory model (STAWM) which uses transformational attention to learn a memory over each image it sees. The state of our Hebb-Rosenblatt memory is embedded in STAWM as the weights space of a layer. By projecting different queries through this layer we can obtain goal-oriented latent representations for tasks including classification and visual reconstruction. Our model obtains highly competitive classification performance on MNIST and CIFAR-10. As demonstrated through the CelebA dataset, to perform reconstruction the model learns to make a sequence of updates to a canvas which constitute a parts-based representation. Classification with the self supervised representation obtained from MNIST is shown to be in line with the state of the art models (none of which use a visual attention mechanism). Finally, we show that STAWM can be trained under the dual constraints of classification and reconstruction to provide an interpretable visual sketchpad which helps open the 'black-box' of deep learning.



### Weakly Aligned Cross-Modal Learning for Multispectral Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/1901.02645v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02645v2)
- **Published**: 2019-01-09 09:16:36+00:00
- **Updated**: 2019-10-18 11:57:50+00:00
- **Authors**: Lu Zhang, Xiangyu Zhu, Xiangyu Chen, Xu Yang, Zhen Lei, Zhiyong Liu
- **Comment**: Accepted by ICCV2019
- **Journal**: None
- **Summary**: Multispectral pedestrian detection has shown great advantages under poor illumination conditions, since the thermal modality provides complementary information for the color image. However, real multispectral data suffers from the position shift problem, i.e. the color-thermal image pairs are not strictly aligned, making one object has different positions in different modalities. In deep learning based methods, this problem makes it difficult to fuse the feature maps from both modalities and puzzles the CNN training. In this paper, we propose a novel Aligned Region CNN (AR-CNN) to handle the weakly aligned multispectral data in an end-to-end way. Firstly, we design a Region Feature Alignment (RFA) module to capture the position shift and adaptively align the region features of the two modalities. Secondly, we present a new multimodal fusion method, which performs feature re-weighting to select more reliable features and suppress the useless ones. Besides, we propose a novel RoI jitter strategy to improve the robustness to unexpected shift patterns of different devices and system settings. Finally, since our method depends on a new kind of labelling: bounding boxes that match each modality, we manually relabel the KAIST dataset by locating bounding boxes in both modalities and building their relationships, providing a new KAIST-Paired Annotation. Extensive experimental validations on existing datasets are performed, demonstrating the effectiveness and robustness of the proposed method. Code and data are available at https://github.com/luzhang16/AR-CNN.



### Deep Semantic Multimodal Hashing Network for Scalable Image-Text and Video-Text Retrievals
- **Arxiv ID**: http://arxiv.org/abs/1901.02662v3
- **DOI**: 10.1109/TNNLS.2020.2997020
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02662v3)
- **Published**: 2019-01-09 10:27:57+00:00
- **Updated**: 2022-01-05 03:36:16+00:00
- **Authors**: Lu Jin, Zechao Li, Jinhui Tang
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Hashing has been widely applied to multimodal retrieval on large-scale multimedia data due to its efficiency in computation and storage. In this article, we propose a novel deep semantic multimodal hashing network (DSMHN) for scalable image-text and video-text retrieval. The proposed deep hashing framework leverages 2-D convolutional neural networks (CNN) as the backbone network to capture the spatial information for image-text retrieval, while the 3-D CNN as the backbone network to capture the spatial and temporal information for video-text retrieval. In the DSMHN, two sets of modality-specific hash functions are jointly learned by explicitly preserving both intermodality similarities and intramodality semantic labels. Specifically, with the assumption that the learned hash codes should be optimal for the classification task, two stream networks are jointly trained to learn the hash functions by embedding the semantic labels on the resultant hash codes. Moreover, a unified deep multimodal hashing framework is proposed to learn compact and high-quality hash codes by exploiting the feature representation learning, intermodality similarity-preserving learning, semantic label-preserving learning, and hash function learning with different types of loss functions simultaneously. The proposed DSMHN method is a generic and scalable deep hashing framework for both image-text and video-text retrievals, which can be flexibly integrated with different types of loss functions. We conduct extensive experiments for both single modal- and cross-modal-retrieval tasks on four widely used multimodal-retrieval data sets. Experimental results on both image-text- and video-text-retrieval tasks demonstrate that the DSMHN significantly outperforms the state-of-the-art methods.



### Low-Cost Transfer Learning of Face Tasks
- **Arxiv ID**: http://arxiv.org/abs/1901.02675v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02675v1)
- **Published**: 2019-01-09 11:09:09+00:00
- **Updated**: 2019-01-09 11:09:09+00:00
- **Authors**: Thrupthi Ann John, Isha Dua, Vineeth N Balasubramanian, C. V. Jawahar
- **Comment**: None
- **Journal**: None
- **Summary**: Do we know what the different filters of a face network represent? Can we use this filter information to train other tasks without transfer learning? For instance, can age, head pose, emotion and other face related tasks be learned from face recognition network without transfer learning? Understanding the role of these filters allows us to transfer knowledge across tasks and take advantage of large data sets in related tasks. Given a pretrained network, we can infer which tasks the network generalizes for and the best way to transfer the information to a new task.



### Image Recognition of Tea Leaf Diseases Based on Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1901.02694v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.02694v1)
- **Published**: 2019-01-09 12:30:00+00:00
- **Updated**: 2019-01-09 12:30:00+00:00
- **Authors**: Xiaoxiao Sun, Shaomin Mu, Yongyu Xu, Zhihao Cao, Tingting Su
- **Comment**: 2018 International Conference on Security, Pattern Analysis, and
  Cybernetics(SPAC 2018)
- **Journal**: None
- **Summary**: In order to identify and prevent tea leaf diseases effectively, convolution neural network (CNN) was used to realize the image recognition of tea disease leaves. Firstly, image segmentation and data enhancement are used to preprocess the images, and then these images were input into the network for training. Secondly, to reach a higher recognition accuracy of CNN, the learning rate and iteration numbers were adjusted frequently and the dropout was added properly in the case of over-fitting. Finally, the experimental results show that the recognition accuracy of CNN is 93.75%, while the accuracy of SVM and BP neural network is 89.36% and 87.69% respectively. Therefore, the recognition algorithm based on CNN is better in classification and can improve the recognition efficiency of tea leaf diseases effectively.



### Guess What's on my Screen? Clustering Smartphone Screenshots with Active Learning
- **Arxiv ID**: http://arxiv.org/abs/1901.02701v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1901.02701v2)
- **Published**: 2019-01-09 12:51:36+00:00
- **Updated**: 2019-01-10 11:22:33+00:00
- **Authors**: Agnese Chiatti, Dolzodmaa Davaasuren, Nilam Ram, Prasenjit Mitra, Byron Reeves, Thomas Robinson
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: A significant proportion of individuals' daily activities is experienced through digital devices. Smartphones in particular have become one of the preferred interfaces for content consumption and social interaction. Identifying the content embedded in frequently-captured smartphone screenshots is thus a crucial prerequisite to studies of media behavior and health intervention planning that analyze activity interplay and content switching over time. Screenshot images can depict heterogeneous contents and applications, making the a priori definition of adequate taxonomies a cumbersome task, even for humans. Privacy protection of the sensitive data captured on screens means the costs associated with manual annotation are large, as the effort cannot be crowd-sourced. Thus, there is need to examine utility of unsupervised and semi-supervised methods for digital screenshot classification. This work introduces the implications of applying clustering on large screenshot sets when only a limited amount of labels is available. In this paper we develop a framework for combining K-Means clustering with Active Learning for efficient leveraging of labeled and unlabeled samples, with the goal of discovering latent classes and describing a large collection of screenshot data. We tested whether SVM-embedded or XGBoost-embedded solutions for class probability propagation provide for more well-formed cluster configurations. Visual and textual vector representations of the screenshot images are derived and combined to assess the relative contribution of multi-modal features to the overall performance.



### On Finding Gray Pixels
- **Arxiv ID**: http://arxiv.org/abs/1901.03198v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03198v3)
- **Published**: 2019-01-09 13:44:13+00:00
- **Updated**: 2019-05-02 12:49:32+00:00
- **Authors**: Yanlin Qian, Joni-Kristian Kämäräinen, Jarno Nikkanen, Jiri Matas
- **Comment**: appear in IEEE International Conference on Computer Vision and
  Pattern Recognition (CVPR) 2019. 9 pages, 7 figures. this article is an
  extension of arXiv:1803.08326
- **Journal**: None
- **Summary**: We propose a novel grayness index for finding gray pixels and demonstrate its effectiveness and efficiency in illumination estimation. The grayness index, GI in short, is derived using the Dichromatic Reflection Model and is learning-free. GI allows to estimate one or multiple illumination sources in color-biased images. On standard single-illumination and multiple-illumination estimation benchmarks, GI outperforms state-of-the-art statistical methods and many recent deep methods. GI is simple and fast, written in a few dozen lines of code, processing a 1080p image in ~0.4 seconds with a non-optimized Matlab code.



### The Use of Mutual Coherence to Prove $\ell^1/\ell^0$-Equivalence in Classification Problems
- **Arxiv ID**: http://arxiv.org/abs/1901.02783v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02783v1)
- **Published**: 2019-01-09 15:38:32+00:00
- **Updated**: 2019-01-09 15:38:32+00:00
- **Authors**: Chelsea Weaver, Naoki Saito
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the decomposition of a signal over an overcomplete set of vectors. Minimization of the $\ell^1$-norm of the coefficient vector can often retrieve the sparsest solution (so-called "$\ell^1/\ell^0$-equivalence"), a generally NP-hard task, and this fact has powered the field of compressed sensing. Wright et al.'s sparse representation-based classification (SRC) applies this relationship to machine learning, wherein the signal to be decomposed represents the test sample and columns of the dictionary are training samples. We investigate the relationships between $\ell^1$-minimization, sparsity, and classification accuracy in SRC. After proving that the tractable, deterministic approach to verifying $\ell^1/\ell^0$-equivalence fundamentally conflicts with the high coherence between same-class training samples, we demonstrate that $\ell^1$-minimization can still recover the sparsest solution when the classes are well-separated. Further, using a nonlinear transform so that sparse recovery conditions may be satisfied, we demonstrate that approximate (not strict) equivalence is key to the success of SRC.



### Learnable Manifold Alignment (LeMA) : A Semi-supervised Cross-modality Learning Framework for Land Cover and Land Use Classification
- **Arxiv ID**: http://arxiv.org/abs/1901.02838v1
- **DOI**: 10.1016/j.isprsjprs.2018.10.006
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02838v1)
- **Published**: 2019-01-09 17:22:36+00:00
- **Updated**: 2019-01-09 17:22:36+00:00
- **Authors**: Danfeng Hong, Naoto Yokoya, Nan Ge, Jocelyn Chanussot, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: ISPRS Journal of Photogrammetry and Remote
  Sensing,2019,147:193-205
- **Summary**: In this paper, we aim at tackling a general but interesting cross-modality feature learning question in remote sensing community --- can a limited amount of highly-discrimin-ative (e.g., hyperspectral) training data improve the performance of a classification task using a large amount of poorly-discriminative (e.g., multispectral) data? Traditional semi-supervised manifold alignment methods do not perform sufficiently well for such problems, since the hyperspectral data is very expensive to be largely collected in a trade-off between time and efficiency, compared to the multispectral data. To this end, we propose a novel semi-supervised cross-modality learning framework, called learnable manifold alignment (LeMA). LeMA learns a joint graph structure directly from the data instead of using a given fixed graph defined by a Gaussian kernel function. With the learned graph, we can further capture the data distribution by graph-based label propagation, which enables finding a more accurate decision boundary. Additionally, an optimization strategy based on the alternating direction method of multipliers (ADMM) is designed to solve the proposed model. Extensive experiments on two hyperspectral-multispectral datasets demonstrate the superiority and effectiveness of the proposed method in comparison with several state-of-the-art methods.



### SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1901.02839v2
- **DOI**: 10.1109/TPAMI.2019.2944808
- **Categories**: **cs.HC**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1901.02839v2)
- **Published**: 2019-01-09 17:28:57+00:00
- **Updated**: 2019-11-18 22:52:44+00:00
- **Authors**: Jean Kossaifi, Robert Walecki, Yannis Panagakis, Jie Shen, Maximilian Schmitt, Fabien Ringeval, Jing Han, Vedhas Pandit, Antoine Toisoul, Bjorn Schuller, Kam Star, Elnar Hajiyev, Maja Pantic
- **Comment**: None
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI), 2019
- **Summary**: Natural human-computer interaction and audio-visual human behaviour sensing systems, which would achieve robust performance in-the-wild are more needed than ever as digital devices are increasingly becoming an indispensable part of our life. Accurately annotated real-world data are the crux in devising such systems. However, existing databases usually consider controlled settings, low demographic variability, and a single task. In this paper, we introduce the SEWA database of more than 2000 minutes of audio-visual data of 398 people coming from six cultures, 50% female, and uniformly spanning the age range of 18 to 65 years old. Subjects were recorded in two different contexts: while watching adverts and while discussing adverts in a video chat. The database includes rich annotations of the recordings in terms of facial landmarks, facial action units (FAU), various vocalisations, mirroring, and continuously valued valence, arousal, liking, agreement, and prototypic examples of (dis)liking. This database aims to be an extremely valuable resource for researchers in affective computing and automatic human sensing and is expected to push forward the research in human behaviour analysis, including cultural studies. Along with the database, we provide extensive baseline experiments for automatic FAU detection and automatic valence, arousal and (dis)liking intensity estimation.



### GIF2Video: Color Dequantization and Temporal Interpolation of GIF images
- **Arxiv ID**: http://arxiv.org/abs/1901.02840v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02840v2)
- **Published**: 2019-01-09 17:31:11+00:00
- **Updated**: 2019-04-08 19:30:06+00:00
- **Authors**: Yang Wang, Haibin Huang, Chuan Wang, Tong He, Jue Wang, Minh Hoai
- **Comment**: to appear in CVPR 2019
- **Journal**: None
- **Summary**: Graphics Interchange Format (GIF) is a highly portable graphics format that is ubiquitous on the Internet. Despite their small sizes, GIF images often contain undesirable visual artifacts such as flat color regions, false contours, color shift, and dotted patterns. In this paper, we propose GIF2Video, the first learning-based method for enhancing the visual quality of GIFs in the wild. We focus on the challenging task of GIF restoration by recovering information lost in the three steps of GIF creation: frame sampling, color quantization, and color dithering. We first propose a novel CNN architecture for color dequantization. It is built upon a compositional architecture for multi-step color correction, with a comprehensive loss function designed to handle large quantization errors. We then adapt the SuperSlomo network for temporal interpolation of GIF frames. We introduce two large datasets, namely GIF-Faces and GIF-Moments, for both training and evaluation. Experimental results show that our method can significantly improve the visual quality of GIFs, and outperforms direct baseline and state-of-the-art approaches.



### Adaptive Feature Processing for Robust Human Activity Recognition on a Novel Multi-Modal Dataset
- **Arxiv ID**: http://arxiv.org/abs/1901.02858v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02858v1)
- **Published**: 2019-01-09 18:25:14+00:00
- **Updated**: 2019-01-09 18:25:14+00:00
- **Authors**: Mirco Moencks, Varuna De Silva, Jamie Roche, Ahmet Kondoz
- **Comment**: Working Draft
- **Journal**: None
- **Summary**: Human Activity Recognition (HAR) is a key building block of many emerging applications such as intelligent mobility, sports analytics, ambient-assisted living and human-robot interaction. With robust HAR, systems will become more human-aware, leading towards much safer and empathetic autonomous systems. While human pose detection has made significant progress with the dawn of deep convolutional neural networks (CNNs), the state-of-the-art research has almost exclusively focused on a single sensing modality, especially video. However, in safety critical applications it is imperative to utilize multiple sensor modalities for robust operation. To exploit the benefits of state-of-the-art machine learning techniques for HAR, it is extremely important to have multimodal datasets. In this paper, we present a novel, multi-modal sensor dataset that encompasses nine indoor activities, performed by 16 participants, and captured by four types of sensors that are commonly used in indoor applications and autonomous vehicles. This multimodal dataset is the first of its kind to be made openly available and can be exploited for many applications that require HAR, including sports analytics, healthcare assistance and indoor intelligent mobility. We propose a novel data preprocessing algorithm to enable adaptive feature extraction from the dataset to be utilized by different machine learning algorithms. Through rigorous experimental evaluations, this paper reviews the performance of machine learning approaches to posture recognition, and analyses the robustness of the algorithms. When performing HAR with the RGB-Depth data from our new dataset, machine learning algorithms such as a deep neural network reached a mean accuracy of up to 96.8% for classification across all stationary and dynamic activities



### Learning to Infer and Execute 3D Shape Programs
- **Arxiv ID**: http://arxiv.org/abs/1901.02875v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.02875v3)
- **Published**: 2019-01-09 18:55:03+00:00
- **Updated**: 2019-08-09 23:07:36+00:00
- **Authors**: Yonglong Tian, Andrew Luo, Xingyuan Sun, Kevin Ellis, William T. Freeman, Joshua B. Tenenbaum, Jiajun Wu
- **Comment**: ICLR 2019. Project page: http://shape2prog.csail.mit.edu
- **Journal**: None
- **Summary**: Human perception of 3D shapes goes beyond reconstructing them as a set of points or a composition of geometric primitives: we also effortlessly understand higher-level shape structure such as the repetition and reflective symmetry of object parts. In contrast, recent advances in 3D shape sensing focus more on low-level geometry but less on these higher-level relationships. In this paper, we propose 3D shape programs, integrating bottom-up recognition systems with top-down, symbolic program structure to capture both low-level geometry and high-level structural priors for 3D shapes. Because there are no annotations of shape programs for real shapes, we develop neural modules that not only learn to infer 3D shape programs from raw, unannotated shapes, but also to execute these programs for shape reconstruction. After initial bootstrapping, our end-to-end differentiable model learns 3D shape programs by reconstructing shapes in a self-supervised manner. Experiments demonstrate that our model accurately infers and executes 3D shape programs for highly complex shapes from various categories. It can also be integrated with an image-to-shape module to infer 3D shape programs directly from an RGB image, leading to 3D shape reconstructions that are both more accurate and more physically plausible.



### Myocardial Infarction Quantification From Late Gadolinium Enhancement MRI Using Top-hat Transforms and Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.02911v1
- **DOI**: 10.3390/a14080249
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02911v1)
- **Published**: 2019-01-09 19:32:31+00:00
- **Updated**: 2019-01-09 19:32:31+00:00
- **Authors**: Ezequiel de la Rosa, Désiré Sidibé, Thomas Decourselle, Thibault Leclercq, Alexandre Cochet, Alain Lalande
- **Comment**: Submitted to IEEE
- **Journal**: Algorithms 14.8 (2021): 249
- **Summary**: Significance: Late gadolinium enhanced magnetic resonance imaging (LGE-MRI) is the gold standard technique for myocardial viability assessment. Although the technique accurately reflects the damaged tissue, there is no clinical standard for quantifying myocardial infarction (MI), demanding most algorithms to be expert dependent. Objectives and Methods: In this work a new automatic method for MI quantification from LGE-MRI is proposed. Our novel segmentation approach is devised for accurately detecting not only hyper-enhanced lesions, but also microvascular-obstructed areas. Moreover, it includes a myocardial disease detection step which extends the algorithm for working under healthy scans. The method is based on a cascade approach where firstly, diseased slices are identified by a convolutional neural network (CNN). Secondly, by means of morphological operations a fast coarse scar segmentation is obtained. Thirdly, the segmentation is refined by a boundary-voxel reclassification strategy using an ensemble of CNNs. For its validation, reproducibility and further comparison against other methods, we tested the method on a big multi-field expert annotated LGE-MRI database including healthy and diseased cases. Results and Conclusion: In an exhaustive comparison against nine reference algorithms, the proposal achieved state-of-the-art segmentation performances and showed to be the only method agreeing in volumetric scar quantification with the expert delineations. Moreover, the method was able to reproduce the intra- and inter-observer variability ranges. It is concluded that the method could suitably be transferred to clinical scenarios.



### Revealing interpretable object representations from human behavior
- **Arxiv ID**: http://arxiv.org/abs/1901.02915v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1901.02915v1)
- **Published**: 2019-01-09 20:04:42+00:00
- **Updated**: 2019-01-09 20:04:42+00:00
- **Authors**: Charles Y. Zheng, Francisco Pereira, Chris I. Baker, Martin N. Hebart
- **Comment**: Accepted in ICLR 2019
- **Journal**: None
- **Summary**: To study how mental object representations are related to behavior, we estimated sparse, non-negative representations of objects using human behavioral judgments on images representative of 1,854 object categories. These representations predicted a latent similarity structure between objects, which captured most of the explainable variance in human behavioral judgments. Individual dimensions in the low-dimensional embedding were found to be highly reproducible and interpretable as conveying degrees of taxonomic membership, functionality, and perceptual attributes. We further demonstrated the predictive power of the embeddings for explaining other forms of human behavior, including categorization, typicality judgments, and feature ratings, suggesting that the dimensions reflect human conceptual representations of objects beyond the specific task.



### TraceCaps: A Capsule-based Neural Network for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1901.02920v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.02920v2)
- **Published**: 2019-01-09 20:23:13+00:00
- **Updated**: 2020-07-15 19:22:44+00:00
- **Authors**: Tao Sun, Zhewei Wang, C. D. Smith, Jundong Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a capsule-based neural network model to solve the semantic segmentation problem. By taking advantage of the extractable part-whole dependencies available in capsule layers, we derive the probabilities of the class labels for individual capsules through a recursive, layer-by-layer procedure. We model this procedure as a traceback pipeline and take it as a central piece to build an end-to-end segmentation network. Under the proposed framework, image-level class labels and object boundaries are jointly sought in an explicit manner, which poses a significant advantage over the state-of-the-art fully convolutional network (FCN) solutions. With the capability to extracted part-whole information, our traceback pipeline can potentially be utilized as the building blocks to design interpretable neural networks. Experiments conducted on modified MNIST and neuroimages demonstrate that our model considerably enhance the segmentation performance compared to the leading FCN variants.



### SalSi: A new seismic attribute for salt dome detection
- **Arxiv ID**: http://arxiv.org/abs/1901.02937v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02937v1)
- **Published**: 2019-01-09 21:33:16+00:00
- **Updated**: 2019-01-09 21:33:16+00:00
- **Authors**: Muhammad Amir Shafiq, Tariq Alshawi, Zhiling Long, Ghassan AlRegib
- **Comment**: Proceedings of IEEE Intl. Conf. on Acoustics, Speech and Signal
  Processing (ICASSP), Shanghai, China, Mar. 2016. arXiv admin note: text
  overlap with arXiv:1812.11960
- **Journal**: None
- **Summary**: In this paper, we propose a saliency-based attribute, SalSi, to detect salt dome bodies within seismic volumes. SalSi is based on the saliency theory and modeling of the human vision system (HVS). In this work, we aim to highlight the parts of the seismic volume that receive highest attention from the human interpreter, and based on the salient features of a seismic image, we detect the salt domes. Experimental results show the effectiveness of SalSi on the real seismic dataset acquired from the North Sea, F3 block. Subjectively, we have used the ground truth and the output of different salt dome delineation algorithms to validate the results of SalSi. For the objective evaluation of results, we have used the receiver operating characteristics (ROC) curves and area under the curves (AUC) to demonstrate SalSi is a promising and an effective attribute for seismic interpretation.



### DASPS: A Database for Anxious States based on a Psychological Stimulation
- **Arxiv ID**: http://arxiv.org/abs/1901.02942v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1901.02942v2)
- **Published**: 2019-01-09 21:54:35+00:00
- **Updated**: 2019-08-05 23:46:48+00:00
- **Authors**: Asma Baghdadi, Yassine Aribi, Rahma Fourati, Najla Halouani, Patrick Siarry, Adel M. Alimi
- **Comment**: 11 pages, IEEE transactions on SMC:systems
- **Journal**: None
- **Summary**: Anxiety affects human capabilities and behavior as much as it affects productivity and quality of life. It can be considered as the main cause of depression and suicide. Anxious states are easily detectable by humans due to their acquired cognition, humans interpret the interlocutor's tone of speech, gesture, facial expressions and recognize their mental state. There is a need for non-invasive reliable techniques that performs the complex task of anxiety detection. In this paper, we present DASPS database containing recorded Electroencephalogram (EEG) signals of 23 participants during anxiety elicitation by means of face-to-face psychological stimuli. EEG signals were captured with Emotiv Epoc headset as it's a wireless wearable low-cost equipment. In our study, we investigate the impact of different parameters, notably: trial duration, feature type, feature combination and anxiety levels number. Our findings showed that anxiety is well elicited in 1 second. For instance, stacked sparse autoencoder with different type of features achieves 83.50% and 74.60% for 2 and 4 anxiety levels detection, respectively. The presented results prove the benefits of the use of a low-cost EEG headset instead of medical non-wireless devices and create a starting point for new researches in the field of anxiety detection.



### Composite Shape Modeling via Latent Space Factorization
- **Arxiv ID**: http://arxiv.org/abs/1901.02968v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02968v2)
- **Published**: 2019-01-09 23:25:38+00:00
- **Updated**: 2019-10-30 13:53:56+00:00
- **Authors**: Anastasia Dubrovina, Fei Xia, Panos Achlioptas, Mira Shalah, Raphael Groscot, Leonidas Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel neural network architecture, termed Decomposer-Composer, for semantic structure-aware 3D shape modeling. Our method utilizes an auto-encoder-based pipeline, and produces a novel factorized shape embedding space, where the semantic structure of the shape collection translates into a data-dependent sub-space factorization, and where shape composition and decomposition become simple linear operations on the embedding coordinates. We further propose to model shape assembly using an explicit learned part deformation module, which utilizes a 3D spatial transformer network to perform an in-network volumetric grid deformation, and which allows us to train the whole system end-to-end. The resulting network allows us to perform part-level shape manipulation, unattainable by existing approaches. Our extensive ablation study, comparison to baseline methods and qualitative analysis demonstrate the improved performance of the proposed method.



### Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation
- **Arxiv ID**: http://arxiv.org/abs/1901.02970v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02970v2)
- **Published**: 2019-01-09 23:31:40+00:00
- **Updated**: 2019-06-23 08:24:32+00:00
- **Authors**: He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, Leonidas J. Guibas
- **Comment**: CVPR2019
- **Journal**: None
- **Summary**: The goal of this paper is to estimate the 6D pose and dimensions of unseen object instances in an RGB-D image. Contrary to "instance-level" 6D pose estimation tasks, our problem assumes that no exact object CAD models are available during either training or testing time. To handle different and unseen object instances in a given category, we introduce a Normalized Object Coordinate Space (NOCS)---a shared canonical representation for all possible object instances within a category. Our region-based neural network is then trained to directly infer the correspondence from observed pixels to this shared object representation (NOCS) along with other object information such as class label and instance mask. These predictions can be combined with the depth map to jointly estimate the metric 6D pose and dimensions of multiple objects in a cluttered scene. To train our network, we present a new context-aware technique to generate large amounts of fully annotated mixed reality data. To further improve our model and evaluate its performance on real data, we also provide a fully annotated real-world dataset with large environment and instance variation. Extensive experiments demonstrate that the proposed method is able to robustly estimate the pose and size of unseen object instances in real environments while also achieving state-of-the-art performance on standard 6D pose estimation benchmarks.



### Deep Learning for Human Affect Recognition: Insights and New Developments
- **Arxiv ID**: http://arxiv.org/abs/1901.02884v1
- **DOI**: 10.1109/TAFFC.2018.2890471
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.HC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.02884v1)
- **Published**: 2019-01-09 23:33:47+00:00
- **Updated**: 2019-01-09 23:33:47+00:00
- **Authors**: Philipp V. Rouast, Marc T. P. Adam, Raymond Chiong
- **Comment**: To be published in IEEE Transactions on Affective Computing. 20
  pages, 7 figures, 6 tables
- **Journal**: None
- **Summary**: Automatic human affect recognition is a key step towards more natural human-computer interaction. Recent trends include recognition in the wild using a fusion of audiovisual and physiological sensors, a challenging setting for conventional machine learning algorithms. Since 2010, novel deep learning algorithms have been applied increasingly in this field. In this paper, we review the literature on human affect recognition between 2010 and 2017, with a special focus on approaches using deep neural networks. By classifying a total of 950 studies according to their usage of shallow or deep architectures, we are able to show a trend towards deep learning. Reviewing a subset of 233 studies that employ deep neural networks, we comprehensively quantify their applications in this field. We find that deep learning is used for learning of (i) spatial feature representations, (ii) temporal feature representations, and (iii) joint feature representations for multimodal sensor data. Exemplary state-of-the-art architectures illustrate the progress. Our findings show the role deep architectures will play in human affect recognition, and can serve as a reference point for researchers working on related applications.



