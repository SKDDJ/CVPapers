# Arxiv Papers in cs.CV on 2019-01-24
### Correcting rural building annotations in OpenStreetMap using convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1901.08190v1
- **DOI**: 10.1016/j.isprsjprs.2018.11.010
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08190v1)
- **Published**: 2019-01-24 01:44:12+00:00
- **Updated**: 2019-01-24 01:44:12+00:00
- **Authors**: John E. Vargas-Muñoz, Sylvain Lobry, Alexandre X. Falcão, Devis Tuia
- **Comment**: None
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing, 147, pages 283
  - 293, 2019
- **Summary**: Rural building mapping is paramount to support demographic studies and plan actions in response to crisis that affect those areas. Rural building annotations exist in OpenStreetMap (OSM), but their quality and quantity are not sufficient for training models that can create accurate rural building maps. The problems with these annotations essentially fall into three categories: (i) most commonly, many annotations are geometrically misaligned with the updated imagery; (ii) some annotations do not correspond to buildings in the images (they are misannotations or the buildings have been destroyed); and (iii) some annotations are missing for buildings in the images (the buildings were never annotated or were built between subsequent image acquisitions). First, we propose a method based on Markov Random Field (MRF) to align the buildings with their annotations. The method maximizes the correlation between annotations and a building probability map while enforcing that nearby buildings have similar alignment vectors. Second, the annotations with no evidence in the building probability map are removed. Third, we present a method to detect non-annotated buildings with predefined shapes and add their annotation. The proposed methodology shows considerable improvement in accuracy of the OSM annotations for two regions of Tanzania and Zimbabwe, being more accurate than state-of-the-art baselines.



### A PCB Dataset for Defects Detection and Classification
- **Arxiv ID**: http://arxiv.org/abs/1901.08204v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08204v1)
- **Published**: 2019-01-24 02:52:52+00:00
- **Updated**: 2019-01-24 02:52:52+00:00
- **Authors**: Weibo Huang, Peng Wei
- **Comment**: None
- **Journal**: None
- **Summary**: To coupe with the difficulties in the process of inspection and classification of defects in Printed Circuit Board (PCB), other researchers have proposed many methods. However, few of them published their dataset before, which hindered the introduction and comparison of new methods. In this paper, we published a synthesized PCB dataset containing 1386 images with 6 kinds of defects for the use of detection, classification and registration tasks. Besides, we proposed a reference based method to inspect and trained an end-to-end convolutional neural network to classify the defects. Unlike conventional approaches that require pixel-by-pixel processing, our method firstly locate the defects and then classify them by neural networks, which shows superior performance on our dataset.



### Synergistic Image and Feature Adaptation: Towards Cross-Modality Domain Adaptation for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1901.08211v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08211v4)
- **Published**: 2019-01-24 03:16:09+00:00
- **Updated**: 2019-06-18 11:52:33+00:00
- **Authors**: Cheng Chen, Qi Dou, Hao Chen, Jing Qin, Pheng-Ann Heng
- **Comment**: AAAI 2019 (oral)
- **Journal**: None
- **Summary**: This paper presents a novel unsupervised domain adaptation framework, called Synergistic Image and Feature Adaptation (SIFA), to effectively tackle the problem of domain shift. Domain adaptation has become an important and hot topic in recent studies on deep learning, aiming to recover performance degradation when applying the neural networks to new testing domains. Our proposed SIFA is an elegant learning diagram which presents synergistic fusion of adaptations from both image and feature perspectives. In particular, we simultaneously transform the appearance of images across domains and enhance domain-invariance of the extracted features towards the segmentation task. The feature encoder layers are shared by both perspectives to grasp their mutual benefits during the end-to-end learning procedure. Without using any annotation from the target domain, the learning of our unified model is guided by adversarial losses, with multiple discriminators employed from various aspects. We have extensively validated our method with a challenging application of cross-modality medical image segmentation of cardiac structures. Experimental results demonstrate that our SIFA model recovers the degraded performance from 17.2% to 73.0%, and outperforms the state-of-the-art methods by a significant margin.



### Semi-Supervised Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1901.08212v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08212v1)
- **Published**: 2019-01-24 03:26:00+00:00
- **Updated**: 2019-01-24 03:26:00+00:00
- **Authors**: Manan Oza, Himanshu Vaghela, Sudhir Bagul
- **Comment**: None
- **Journal**: None
- **Summary**: Image-to-image translation is a long-established and a difficult problem in computer vision. In this paper we propose an adversarial based model for image-to-image translation. The regular deep neural-network based methods perform the task of image-to-image translation by comparing gram matrices and using image segmentation which requires human intervention. Our generative adversarial network based model works on a conditional probability approach. This approach makes the image translation independent of any local, global and content or style features. In our approach we use a bidirectional reconstruction model appended with the affine transform factor that helps in conserving the content and photorealism as compared to other models. The advantage of using such an approach is that the image-to-image translation is semi-supervised, independant of image segmentation and inherits the properties of generative adversarial networks tending to produce realistic. This method has proven to produce better results than Multimodal Unsupervised Image-to-image translation.



### MREAK : Morphological Retina Keypoint Descriptor
- **Arxiv ID**: http://arxiv.org/abs/1901.08213v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08213v1)
- **Published**: 2019-01-24 03:26:07+00:00
- **Updated**: 2019-01-24 03:26:07+00:00
- **Authors**: Himanshu Vaghela, Manan Oza, Sudhir Bagul
- **Comment**: None
- **Journal**: None
- **Summary**: A variety of computer vision applications depend on the efficiency of image matching algorithms used. Various descriptors are designed to detect and match features in images. Deployment of this algorithms in mobile applications creates a need for low computation time. Binary descriptors requires less computation time than float-point based descriptors because of the intensity comparison between pairs of sample points and comparing after creating a binary string. In order to decrease time complexity, quality of keypoints matched is often compromised. We propose a keypoint descriptor named Morphological Retina Keypoint Descriptor (MREAK) inspired by the function of human pupil which dilates and constricts responding to the amount of light. By using morphological operators of opening and closing and modifying the retinal sampling pattern accordingly, an increase in the number of accurately matched keypoints is observed. Our results show that matched keypoints are more efficient than FREAK descriptor and requires low computation time than various descriptors like SIFT, BRISK and SURF.



### Object Detection based on Region Decomposition and Assembly
- **Arxiv ID**: http://arxiv.org/abs/1901.08225v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08225v2)
- **Published**: 2019-01-24 04:09:10+00:00
- **Updated**: 2020-11-16 04:30:17+00:00
- **Authors**: Seung-Hwan Bae
- **Comment**: Accepted to 2019 AAAI Conference on Artificial Intelligence (AAAI)
- **Journal**: None
- **Summary**: Region-based object detection infers object regions for one or more categories in an image. Due to the recent advances in deep learning and region proposal methods, object detectors based on convolutional neural networks (CNNs) have been flourishing and provided the promising detection results. However, the detection accuracy is degraded often because of the low discriminability of object CNN features caused by occlusions and inaccurate region proposals. In this paper, we therefore propose a region decomposition and assembly detector (R-DAD) for more accurate object detection.   In the proposed R-DAD, we first decompose an object region into multiple small regions. To capture an entire appearance and part details of the object jointly, we extract CNN features within the whole object region and decomposed regions. We then learn the semantic relations between the object and its parts by combining the multi-region features stage by stage with region assembly blocks, and use the combined and high-level semantic features for the object classification and localization. In addition, for more accurate region proposals, we propose a multi-scale proposal layer that can generate object proposals of various scales. We integrate the R-DAD into several feature extractors, and prove the distinct performance improvement on PASCAL07/12 and MSCOCO18 compared to the recent convolutional detectors.



### Trajectory Normalized Gradients for Distributed Optimization
- **Arxiv ID**: http://arxiv.org/abs/1901.08227v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DC, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.08227v1)
- **Published**: 2019-01-24 04:24:31+00:00
- **Updated**: 2019-01-24 04:24:31+00:00
- **Authors**: Jianqiao Wangni, Ke Li, Jianbo Shi, Jitendra Malik
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, researchers proposed various low-precision gradient compression, for efficient communication in large-scale distributed optimization. Based on these work, we try to reduce the communication complexity from a new direction. We pursue an ideal bijective mapping between two spaces of gradient distribution, so that the mapped gradient carries greater information entropy after the compression. In our setting, all servers should share a reference gradient in advance, and they communicate via the normalized gradients, which are the subtraction or quotient, between current gradients and the reference. To obtain a reference vector that yields a stronger signal-to-noise ratio, dynamically in each iteration, we extract and fuse information from the past trajectory in hindsight, and search for an optimal reference for compression. We name this to be the trajectory-based normalized gradients (TNG). It bridges the research from different societies, like coding, optimization, systems, and learning. It is easy to implement and can universally combine with existing algorithms. Our experiments on benchmarking hard non-convex functions, convex problems like logistic regression demonstrate that TNG is more compression-efficient for communication of distributed optimization of general functions.



### Reciprocal Translation between SAR and Optical Remote Sensing Images with Cascaded-Residual Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.08236v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08236v2)
- **Published**: 2019-01-24 05:17:39+00:00
- **Updated**: 2019-10-09 07:11:16+00:00
- **Authors**: Shilei Fu, Feng Xu, Ya-Qiu Jin
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the advantages of all-weather and all-day high-resolution imaging, synthetic aperture radar (SAR) images are much less viewed and used by general people because human vision is not adapted to microwave scattering phenomenon. However, expert interpreters can be trained by comparing side-by-side SAR and optical images to learn the mapping rules from SAR to optical. This paper attempts to develop machine intelligence that are trainable with large-volume co-registered SAR and optical images to translate SAR image to optical version for assisted SAR image interpretation. Reciprocal SAR-Optical image translation is a challenging task because it is raw data translation between two physically very different sensing modalities. This paper proposes a novel reciprocal adversarial network scheme where cascaded residual connections and hybrid L1-GAN loss are employed. It is trained and tested on both spaceborne GF-3 and airborne UAVSAR images. Results are presented for datasets of different resolutions and polarizations and compared with other state-of-the-art methods. The FID is used to quantitatively evaluate the translation performance. The possibility of unsupervised learning with unpaired SAR and optical images is also explored. Results show that the proposed translation network works well under many scenarios and it could potentially be used for assisted SAR interpretation.



### Visualizing Topographic Independent Component Analysis with Movies
- **Arxiv ID**: http://arxiv.org/abs/1901.08239v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.CO
- **Links**: [PDF](http://arxiv.org/pdf/1901.08239v1)
- **Published**: 2019-01-24 05:47:01+00:00
- **Updated**: 2019-01-24 05:47:01+00:00
- **Authors**: Zhimin Chen, Darius Parvin, Maedbh King, Susan Hao
- **Comment**: None
- **Journal**: None
- **Summary**: Independent component analysis (ICA) has often been used as a tool to model natural image statistics by separating multivariate signals in the image into components that are assumed to be independent. However, these estimated components oftentimes have higher order dependencies, such as co-activation of components, that are not accounted for in the model. Topographic independent component analysis(TICA), a modification of ICA, takes into account higher order dependencies and orders components topographically as a function of dependence. Here, we aim to visualize the time course of TICA basis activations to movie stimuli. We find that the activity of TICA bases are often clustered and move continuously, potentially resembling activity of topographically organized cells in the visual cortex.



### Unsupervised Image-to-Image Translation with Self-Attention Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.08242v4
- **DOI**: 10.1109/BigComp48618.2020.00-92
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08242v4)
- **Published**: 2019-01-24 05:59:40+00:00
- **Updated**: 2020-04-20 15:56:06+00:00
- **Authors**: Taewon Kang, Kwang Hee Lee
- **Comment**: Accepted by 2020 IEEE International Conference on Big Data and Smart
  Computing (IEEE BigComp 2020). This paper has been accepted as a REGULAR
  paper presented at IEEE International Conference on BigComp 2020. 7 pages, 11
  figures; v4: corrected typos, figures
- **Journal**: 2020 IEEE International Conference on Big Data and Smart Computing
  (BigComp), (2020), 102-108
- **Summary**: Unsupervised image translation aims to learn the transformation from a source domain to another target domain given unpaired training data. Several state-of-the-art works have yielded impressive results in the GANs-based unsupervised image-to-image translation. It fails to capture strong geometric or structural changes between domains, or it produces unsatisfactory result for complex scenes, compared to local texture mapping tasks such as style transfer. Recently, SAGAN (Han Zhang, 2018) showed that the self-attention network produces better results than the convolution-based GAN. However, the effectiveness of the self-attention network in unsupervised image-to-image translation tasks have not been verified. In this paper, we propose an unsupervised image-to-image translation with self-attention networks, in which long range dependency helps to not only capture strong geometric change but also generate details using cues from all feature locations. In experiments, we qualitatively and quantitatively show superiority of the proposed method compared to existing state-of-the-art unsupervised image-to-image translation task. The source code and our results are online: https://github.com/itsss/img2img_sa and http://itsc.kr/2019/01/24/2019_img2img_sa



### Generative Adversarial Network with Multi-Branch Discriminator for Cross-Species Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1901.10895v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.10895v1)
- **Published**: 2019-01-24 07:14:07+00:00
- **Updated**: 2019-01-24 07:14:07+00:00
- **Authors**: Ziqiang Zheng, Zhibin Yu, Haiyong Zheng, Yang Wu, Bing Zheng, Ping Lin
- **Comment**: 10 pages, 16 figures
- **Journal**: None
- **Summary**: Current approaches have made great progress on image-to-image translation tasks benefiting from the success of image synthesis methods especially generative adversarial networks (GANs). However, existing methods are limited to handling translation tasks between two species while keeping the content matching on the semantic level. A more challenging task would be the translation among more than two species. To explore this new area, we propose a simple yet effective structure of a multi-branch discriminator for enhancing an arbitrary generative adversarial architecture (GAN), named GAN-MBD. It takes advantage of the boosting strategy to break a common discriminator into several smaller ones with fewer parameters, which can enhance the generation and synthesis abilities of GANs efficiently and effectively. Comprehensive experiments show that the proposed multi-branch discriminator can dramatically improve the performance of popular GANs on cross-species image-to-image translation tasks while reducing the number of parameters for computation. The code and some datasets are attached as supplementary materials for reference.



### Learning V1 Simple Cells with Vector Representation of Local Content and Matrix Representation of Local Motion
- **Arxiv ID**: http://arxiv.org/abs/1902.03871v5
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.03871v5)
- **Published**: 2019-01-24 08:09:19+00:00
- **Updated**: 2022-04-06 01:31:28+00:00
- **Authors**: Ruiqi Gao, Jianwen Xie, Siyuan Huang, Yufan Ren, Song-Chun Zhu, Ying Nian Wu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a representational model for image pairs such as consecutive video frames that are related by local pixel displacements, in the hope that the model may shed light on motion perception in primary visual cortex (V1). The model couples the following two components: (1) the vector representations of local contents of images and (2) the matrix representations of local pixel displacements caused by the relative motions between the agent and the objects in the 3D scene. When the image frame undergoes changes due to local pixel displacements, the vectors are multiplied by the matrices that represent the local displacements. Thus the vector representation is equivariant as it varies according to the local displacements. Our experiments show that our model can learn Gabor-like filter pairs of quadrature phases. The profiles of the learned filters match those of simple cells in Macaque V1. Moreover, we demonstrate that the model can learn to infer local motions in either a supervised or unsupervised manner. With such a simple model, we achieve competitive results on optical flow estimation.



### A Novel Self-Intersection Penalty Term for Statistical Body Shape Models and Its Applications in 3D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1901.08274v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08274v1)
- **Published**: 2019-01-24 08:19:37+00:00
- **Updated**: 2019-01-24 08:19:37+00:00
- **Authors**: Zaiqiang Wu, Wei Jiang, Hao Luo, Lin Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Statistical body shape models are widely used in 3D pose estimation due to their low-dimensional parameters representation. However, it is difficult to avoid self-intersection between body parts accurately. Motivated by this fact, we proposed a novel self-intersection penalty term for statistical body shape models applied in 3D pose estimation. To avoid the trouble of computing self-intersection for complex surfaces like the body meshes, the gradient of our proposed self-intersection penalty term is manually derived from the perspective of geometry. First, the self-intersection penalty term is defined as the volume of the self-intersection region. To calculate the partial derivatives with respect to the coordinates of the vertices, we employed detection rays to divide vertices of statistical body shape models into different groups depending on whether the vertex is in the region of self-intersection. Second, the partial derivatives could be easily derived by the normal vectors of neighboring triangles of the vertices. Finally, this penalty term could be applied in gradient-based optimization algorithms to remove the self-intersection of triangular meshes without using any approximation. Qualitative and quantitative evaluations were conducted to demonstrate the effectiveness and generality of our proposed method compared with previous approaches. The experimental results show that our proposed penalty term can avoid self-intersection to exclude unreasonable predictions and improves the accuracy of 3D pose estimation indirectly. Further more, the proposed method could be employed universally in triangular mesh based 3D reconstruction.



### Heavy-Tailed Universality Predicts Trends in Test Accuracies for Very Large Pre-Trained Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.08278v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.08278v2)
- **Published**: 2019-01-24 08:27:03+00:00
- **Updated**: 2020-01-26 05:24:40+00:00
- **Authors**: Charles H. Martin, Michael W. Mahoney
- **Comment**: Updated as will appear in SDM20
- **Journal**: None
- **Summary**: Given two or more Deep Neural Networks (DNNs) with the same or similar architectures, and trained on the same dataset, but trained with different solvers, parameters, hyper-parameters, regularization, etc., can we predict which DNN will have the best test accuracy, and can we do so without peeking at the test data? In this paper, we show how to use a new Theory of Heavy-Tailed Self-Regularization (HT-SR) to answer this. HT-SR suggests, among other things, that modern DNNs exhibit what we call Heavy-Tailed Mechanistic Universality (HT-MU), meaning that the correlations in the layer weight matrices can be fit to a power law (PL) with exponents that lie in common Universality classes from Heavy-Tailed Random Matrix Theory (HT-RMT). From this, we develop a Universal capacity control metric that is a weighted average of PL exponents. Rather than considering small toy NNs, we examine over 50 different, large-scale pre-trained DNNs, ranging over 15 different architectures, trained on ImagetNet, each of which has been reported to have different test accuracies. We show that this new capacity metric correlates very well with the reported test accuracies of these DNNs, looking across each architecture (VGG16/.../VGG19, ResNet10/.../ResNet152, etc.). We also show how to approximate the metric by the more familiar Product Norm capacity measure, as the average of the log Frobenius norm of the layer weight matrices. Our approach requires no changes to the underlying DNN or its loss function, it does not require us to train a model (although it could be used to monitor training), and it does not even require access to the ImageNet data.



### Anomaly Detection in Road Traffic Using Visual Surveillance: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1901.08292v1
- **DOI**: 10.1145/3417989
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08292v1)
- **Published**: 2019-01-24 09:02:05+00:00
- **Updated**: 2019-01-24 09:02:05+00:00
- **Authors**: Santhosh Kelathodi Kumaran, Debi Prosad Dogra, Partha Pratim Roy
- **Comment**: None
- **Journal**: ACM Computing Surveys (2020), 6(53):Article 119, 2020
- **Summary**: Computer vision has evolved in the last decade as a key technology for numerous applications replacing human supervision. In this paper, we present a survey on relevant visual surveillance related researches for anomaly detection in public places, focusing primarily on roads. Firstly, we revisit the surveys done in the last 10 years in this field. Since the underlying building block of a typical anomaly detection is learning, we emphasize more on learning methods applied on video scenes. We then summarize the important contributions made during last six years on anomaly detection primarily focusing on features, underlying techniques, applied scenarios and types of anomalies using single static camera. Finally, we discuss the challenges in the computer vision related anomaly detection techniques and some of the important future possibilities.



### Deep Learning on Attributed Graphs: A Journey from Graphs to Their Embeddings and Back
- **Arxiv ID**: http://arxiv.org/abs/1901.08296v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.08296v1)
- **Published**: 2019-01-24 09:12:33+00:00
- **Updated**: 2019-01-24 09:12:33+00:00
- **Authors**: Martin Simonovsky
- **Comment**: PhD Thesis
- **Journal**: None
- **Summary**: A graph is a powerful concept for representation of relations between pairs of entities. Data with underlying graph structure can be found across many disciplines and there is a natural desire for understanding such data better. Deep learning (DL) has achieved significant breakthroughs in a variety of machine learning tasks in recent years, especially where data is structured on a grid, such as in text, speech, or image understanding. However, surprisingly little has been done to explore the applicability of DL on arbitrary graph-structured data directly.   The goal of this thesis is to investigate architectures for DL on graphs and study how to transfer, adapt or generalize concepts that work well on sequential and image data to this domain. We concentrate on two important primitives: embedding graphs or their nodes into a continuous vector space representation (encoding) and, conversely, generating graphs from such vectors back (decoding). To that end, we make the following contributions.   First, we introduce Edge-Conditioned Convolutions (ECC), a convolution-like operation on graphs performed in the spatial domain where filters are dynamically generated based on edge attributes. The method is used to encode graphs with arbitrary and varying structure.   Second, we propose SuperPoint Graph, an intermediate point cloud representation with rich edge attributes encoding the contextual relationship between object parts. Based on this representation, ECC is employed to segment large-scale point clouds without major sacrifice in fine details.   Third, we present GraphVAE, a graph generator allowing us to decode graphs with variable but upper-bounded number of nodes making use of approximate graph matching for aligning the predictions of an autoencoder with its inputs. The method is applied to the task of molecule generation.



### Whole slide image registration for the study of tumor heterogeneity
- **Arxiv ID**: http://arxiv.org/abs/1901.08317v1
- **DOI**: 10.1007/978-3-030-00949-6_12
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1901.08317v1)
- **Published**: 2019-01-24 10:02:18+00:00
- **Updated**: 2019-01-24 10:02:18+00:00
- **Authors**: Leslie Solorzano, Gabriela M. Almeida, Bárbara Mesquita, Diana Martins, Carla Oliveira, Carolina Wählby
- **Comment**: MICCAI2018 - Computational Pathology and Ophthalmic Medical Image
  Analysis - COMPAY
- **Journal**: vol 11039, 2018, p95-102
- **Summary**: Consecutive thin sections of tissue samples make it possible to study local variation in e.g. protein expression and tumor heterogeneity by staining for a new protein in each section. In order to compare and correlate patterns of different proteins, the images have to be registered with high accuracy. The problem we want to solve is registration of gigapixel whole slide images (WSI). This presents 3 challenges: (i) Images are very large; (ii) Thin sections result in artifacts that make global affine registration prone to very large local errors; (iii) Local affine registration is required to preserve correct tissue morphology (local size, shape and texture). In our approach we compare WSI registration based on automatic and manual feature selection on either the full image or natural sub-regions (as opposed to square tiles). Working with natural sub-regions, in an interactive tool makes it possible to exclude regions containing scientifically irrelevant information. We also present a new way to visualize local registration quality by a Registration Confidence Map (RCM). With this method, intra-tumor heterogeneity and charateristics of the tumor microenvironment can be observed and quantified.



### Semi-Supervised Semantic Matching
- **Arxiv ID**: http://arxiv.org/abs/1901.08339v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08339v1)
- **Published**: 2019-01-24 10:46:41+00:00
- **Updated**: 2019-01-24 10:46:41+00:00
- **Authors**: Zakaria Laskar, Juho Kannala
- **Comment**: Accepted to ECCVW (GMDL) 2018
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have been successfully applied to solve the problem of correspondence estimation between semantically related images. Due to non-availability of large training datasets, existing methods resort to self-supervised or unsupervised training paradigm. In this paper we propose a semi-supervised learning framework that imposes cyclic consistency constraint on unlabeled image pairs. Together with the supervised loss the proposed model achieves state-of-the-art on a benchmark semantic matching dataset.



### Semantic Matching by Weakly Supervised 2D Point Set Registration
- **Arxiv ID**: http://arxiv.org/abs/1901.08341v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08341v1)
- **Published**: 2019-01-24 10:48:39+00:00
- **Updated**: 2019-01-24 10:48:39+00:00
- **Authors**: Zakaria Laskar, Hamed R. Tavakoli, Juho Kannala
- **Comment**: Accepted to WACV 2019
- **Journal**: None
- **Summary**: In this paper we address the problem of establishing correspondences between different instances of the same object. The problem is posed as finding the geometric transformation that aligns a given image pair. We use a convolutional neural network (CNN) to directly regress the parameters of the transformation model. The alignment problem is defined in the setting where an unordered set of semantic key-points per image are available, but, without the correspondence information. To this end we propose a novel loss function based on cyclic consistency that solves this 2D point set registration problem by inferring the optimal geometric transformation model parameters. We train and test our approach on a standard benchmark dataset Proposal-Flow (PF-PASCAL)\cite{proposal_flow}. The proposed approach achieves state-of-the-art results demonstrating the effectiveness of the method. In addition, we show our approach further benefits from additional training samples in PF-PASCAL generated by using category level information.



### Deep Reasoning with Multi-Scale Context for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1901.08362v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08362v2)
- **Published**: 2019-01-24 11:34:56+00:00
- **Updated**: 2019-03-25 07:37:47+00:00
- **Authors**: Zun Li, Congyan Lang, Yunpeng Chen, Junhao Liew, Jiashi Feng
- **Comment**: 10 pages, 8 figures, 3 table
- **Journal**: None
- **Summary**: To detect salient objects accurately, existing methods usually design complex backbone network architectures to learn and fuse powerful features. However, the saliency inference module that performs saliency prediction from the fused features receives much less attention on its architecture design and typically adopts only a few fully convolutional layers. In this paper, we find the limited capacity of the saliency inference module indeed makes a fundamental performance bottleneck, and enhancing its capacity is critical for obtaining better saliency prediction. Correspondingly, we propose a deep yet light-weight saliency inference module that adopts a multi-dilated depth-wise convolution architecture. Such a deep inference module, though with simple architecture, can directly perform reasoning about salient objects from the multi-scale convolutional features fast, and give superior salient object detection performance with less computational cost. To our best knowledge, we are the first to reveal the importance of the inference module for salient object detection, and present a novel architecture design with attractive efficiency and accuracy. Extensive experimental evaluations demonstrate that our simple framework performs favorably compared with the state-of-the-art methods with complex backbone design.



### Three-dimensional Backbone Network for 3D Object Detection in Traffic Scenes
- **Arxiv ID**: http://arxiv.org/abs/1901.08373v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08373v2)
- **Published**: 2019-01-24 12:11:05+00:00
- **Updated**: 2019-09-14 08:12:30+00:00
- **Authors**: Xuesong Li, Jose Guivant, Ngaiming Kwok, Yongzhi Xu, Ruowei Li, Hongkun Wu
- **Comment**: None
- **Journal**: None
- **Summary**: The task of detecting 3D objects in traffic scenes has a pivotal role in many real-world applications. However, the performance of 3D object detection is lower than that of 2D object detection due to the lack of powerful 3D feature extraction methods. To address this issue, this study proposes a 3D backbone network to acquire comprehensive 3D feature maps for 3D object detection. It primarily consists of sparse 3D convolutional neural network operations in the point cloud. The 3D backbone network can inherently learn 3D features from the raw data without compressing the point cloud into multiple 2D images. The sparse 3D convolutional neural network takes full advantage of the sparsity in the 3D point cloud to accelerate computation and save memory, which makes the 3D backbone network feasible in a real-world application. Empirical experiments were conducted on the KITTI benchmark and comparable results were obtained with respect to the state-of-the-art performance for 3D object detection.



### Using CycleGANs for effectively reducing image variability across OCT devices and improving retinal fluid segmentation
- **Arxiv ID**: http://arxiv.org/abs/1901.08379v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08379v2)
- **Published**: 2019-01-24 12:37:14+00:00
- **Updated**: 2019-01-25 09:12:41+00:00
- **Authors**: Philipp Seeböck, David Romo-Bucheli, Sebastian Waldstein, Hrvoje Bogunović, José Ignacio Orlando, Bianca S. Gerendas, Georg Langs, Ursula Schmidt-Erfurth
- **Comment**: * Contributed equally (order was defined by flipping a coin)
  --------------- Accepted for publication in the "IEEE International Symposium
  on Biomedical Imaging (ISBI) 2019"
- **Journal**: None
- **Summary**: Optical coherence tomography (OCT) has become the most important imaging modality in ophthalmology. A substantial amount of research has recently been devoted to the development of machine learning (ML) models for the identification and quantification of pathological features in OCT images. Among the several sources of variability the ML models have to deal with, a major factor is the acquisition device, which can limit the ML model's generalizability. In this paper, we propose to reduce the image variability across different OCT devices (Spectralis and Cirrus) by using CycleGAN, an unsupervised unpaired image transformation algorithm. The usefulness of this approach is evaluated in the setting of retinal fluid segmentation, namely intraretinal cystoid fluid (IRC) and subretinal fluid (SRF). First, we train a segmentation model on images acquired with a source OCT device. Then we evaluate the model on (1) source, (2) target and (3) transformed versions of the target OCT images. The presented transformation strategy shows an F1 score of 0.4 (0.51) for IRC (SRF) segmentations. Compared with traditional transformation approaches, this means an F1 score gain of 0.2 (0.12).



### Application of Decision Rules for Handling Class Imbalance in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1901.08394v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML, 68T45, 62-07, 62C05
- **Links**: [PDF](http://arxiv.org/pdf/1901.08394v1)
- **Published**: 2019-01-24 13:20:25+00:00
- **Updated**: 2019-01-24 13:20:25+00:00
- **Authors**: Robin Chan, Matthias Rottmann, Fabian Hüger, Peter Schlicht, Hanno Gottschalk
- **Comment**: 11 pages, 21 figures
- **Journal**: None
- **Summary**: As part of autonomous car driving systems, semantic segmentation is an essential component to obtain a full understanding of the car's environment. One difficulty, that occurs while training neural networks for this purpose, is class imbalance of training data. Consequently, a neural network trained on unbalanced data in combination with maximum a-posteriori classification may easily ignore classes that are rare in terms of their frequency in the dataset. However, these classes are often of highest interest. We approach such potential misclassifications by weighting the posterior class probabilities with the prior class probabilities which in our case are the inverse frequencies of the corresponding classes in the training dataset. More precisely, we adopt a localized method by computing the priors pixel-wise such that the impact can be analyzed at pixel level as well. In our experiments, we train one network from scratch using a proprietary dataset containing 20,000 annotated frames of video sequences recorded from street scenes. The evaluation on our test set shows an increase of average recall with regard to instances of pedestrians and info signs by $25\%$ and $23.4\%$, respectively. In addition, we significantly reduce the non-detection rate for instances of the same classes by $61\%$ and $38\%$.



### Self-Supervised Deep Learning on Point Clouds by Reconstructing Space
- **Arxiv ID**: http://arxiv.org/abs/1901.08396v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.08396v2)
- **Published**: 2019-01-24 13:30:19+00:00
- **Updated**: 2019-06-02 20:06:50+00:00
- **Authors**: Jonathan Sauder, Bjarne Sievers
- **Comment**: None
- **Journal**: None
- **Summary**: Point clouds provide a flexible and natural representation usable in countless applications such as robotics or self-driving cars. Recently, deep neural networks operating on raw point cloud data have shown promising results on supervised learning tasks such as object classification and semantic segmentation. While massive point cloud datasets can be captured using modern scanning technology, manually labelling such large 3D point clouds for supervised learning tasks is a cumbersome process. This necessitates methods that can learn from unlabelled data to significantly reduce the number of annotated samples needed in supervised learning. We propose a self-supervised learning task for deep learning on raw point cloud data in which a neural network is trained to reconstruct point clouds whose parts have been randomly rearranged. While solving this task, representations that capture semantic properties of the point cloud are learned. Our method is agnostic of network architecture and outperforms current unsupervised learning approaches in downstream object classification tasks. We show experimentally, that pre-training with our method before supervised training improves the performance of state-of-the-art models and significantly improves sample efficiency.



### CT synthesis from MR images for orthopedic applications in the lower arm using a conditional generative adversarial network
- **Arxiv ID**: http://arxiv.org/abs/1901.08449v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1901.08449v1)
- **Published**: 2019-01-24 15:16:38+00:00
- **Updated**: 2019-01-24 15:16:38+00:00
- **Authors**: Frank Zijlstra, Koen Willemsen, Mateusz C. Florkow, Ralph J. B. Sakkers, Harrie H. Weinans, Bart C. H. van der Wal, Marijn van Stralen, Peter R. Seevinck
- **Comment**: This work has been accepted at the SPIE Medical Imaging 2019, Image
  Processing conference, paper 10949-54
- **Journal**: None
- **Summary**: Purpose: To assess the feasibility of deep learning-based high resolution synthetic CT generation from MRI scans of the lower arm for orthopedic applications.   Methods: A conditional Generative Adversarial Network was trained to synthesize CT images from multi-echo MR images. A training set of MRI and CT scans of 9 ex vivo lower arms was acquired and the CT images were registered to the MRI images. Three-fold cross-validation was applied to generate independent results for the entire dataset. The synthetic CT images were quantitatively evaluated with the mean absolute error metric, and Dice similarity and surface to surface distance on cortical bone segmentations.   Results: The mean absolute error was 63.5 HU on the overall tissue volume and 144.2 HU on the cortical bone. The mean Dice similarity of the cortical bone segmentations was 0.86. The average surface to surface distance between bone on real and synthetic CT was 0.48 mm. Qualitatively, the synthetic CT images corresponded well with the real CT scans and partially maintained high resolution structures in the trabecular bone. The bone segmentations on synthetic CT images showed some false positives on tendons, but the general shape of the bone was accurately reconstructed.   Conclusions: This study demonstrates that high quality synthetic CT can be generated from MRI scans of the lower arm. The good correspondence of the bone segmentations demonstrates that synthetic CT could be competitive with real CT in applications that depend on such segmentations, such as planning of orthopedic surgery and 3D printing.



### Never Forget: Balancing Exploration and Exploitation via Learning Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1901.08486v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1901.08486v1)
- **Published**: 2019-01-24 16:26:16+00:00
- **Updated**: 2019-01-24 16:26:16+00:00
- **Authors**: Hsuan-Kung Yang, Po-Han Chiang, Kuan-Wei Ho, Min-Fong Hong, Chun-Yi Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Exploration bonus derived from the novelty of the states in an environment has become a popular approach to motivate exploration for deep reinforcement learning agents in the past few years. Recent methods such as curiosity-driven exploration usually estimate the novelty of new observations by the prediction errors of their system dynamics models. Due to the capacity limitation of the models and difficulty of performing next-frame prediction, however, these methods typically fail to balance between exploration and exploitation in high-dimensional observation tasks, resulting in the agents forgetting the visited paths and exploring those states repeatedly. Such inefficient exploration behavior causes significant performance drops, especially in large environments with sparse reward signals. In this paper, we propose to introduce the concept of optical flow estimation from the field of computer vision to deal with the above issue. We propose to employ optical flow estimation errors to examine the novelty of new observations, such that agents are able to memorize and understand the visited states in a more comprehensive fashion. We compare our method against the previous approaches in a number of experimental experiments. Our results indicate that the proposed method appears to deliver superior and long-lasting performance than the previous methods. We further provide a set of comprehensive ablative analysis of the proposed method, and investigate the impact of optical flow estimation on the learning curves of the DRL agents.



### Learning Disentangled Representations with Reference-Based Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1901.08534v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08534v1)
- **Published**: 2019-01-24 17:54:54+00:00
- **Updated**: 2019-01-24 17:54:54+00:00
- **Authors**: Adria Ruiz, Oriol Martinez, Xavier Binefa, Jakob Verbeek
- **Comment**: None
- **Journal**: None
- **Summary**: Learning disentangled representations from visual data, where different high-level generative factors are independently encoded, is of importance for many computer vision tasks. Solving this problem, however, typically requires to explicitly label all the factors of interest in training images. To alleviate the annotation cost, we introduce a learning setting which we refer to as "reference-based disentangling". Given a pool of unlabeled images, the goal is to learn a representation where a set of target factors are disentangled from others. The only supervision comes from an auxiliary "reference set" containing images where the factors of interest are constant. In order to address this problem, we propose reference-based variational autoencoders, a novel deep generative model designed to exploit the weak-supervision provided by the reference set. By addressing tasks such as feature learning, conditional image generation or attribute transfer, we validate the ability of the proposed model to learn disentangled representations from this minimal form of supervision.



### Boosting Standard Classification Architectures Through a Ranking Regularizer
- **Arxiv ID**: http://arxiv.org/abs/1901.08616v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08616v3)
- **Published**: 2019-01-24 19:19:31+00:00
- **Updated**: 2020-03-02 17:08:34+00:00
- **Authors**: Ahmed Taha, Yi-Ting Chen, Teruhisa Misu, Abhinav Shrivastava, Larry Davis
- **Comment**: WACV 2020 Camera ready + supplementary material
- **Journal**: None
- **Summary**: We employ triplet loss as a feature embedding regularizer to boost classification performance. Standard architectures, like ResNet and Inception, are extended to support both losses with minimal hyper-parameter tuning. This promotes generality while fine-tuning pretrained networks. Triplet loss is a powerful surrogate for recently proposed embedding regularizers. Yet, it is avoided due to large batch-size requirement and high computational cost. Through our experiments, we re-assess these assumptions.   During inference, our network supports both classification and embedding tasks without any computational overhead. Quantitative evaluation highlights a steady improvement on five fine-grained recognition datasets. Further evaluation on an imbalanced video dataset achieves significant improvement. Triplet loss brings feature embedding characteristics like nearest neighbor to classification models. Code available at \url{http://bit.ly/2LNYEqL}.



### Is Pretraining Necessary for Hyperspectral Image Classification?
- **Arxiv ID**: http://arxiv.org/abs/1901.08658v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08658v1)
- **Published**: 2019-01-24 22:01:48+00:00
- **Updated**: 2019-01-24 22:01:48+00:00
- **Authors**: Hyungtae Lee, Sungmin Eum, Heesung Kwon
- **Comment**: IGARSS 2019 submission
- **Journal**: None
- **Summary**: We address two questions for training a convolutional neural network (CNN) for hyperspectral image classification: i) is it possible to build a pre-trained network? and ii) is the pre-training effective in furthering the performance? To answer the first question, we have devised an approach that pre-trains a network on multiple source datasets that differ in their hyperspectral characteristics and fine-tunes on a target dataset. This approach effectively resolves the architectural issue that arises when transferring meaningful information between the source and the target networks. To answer the second question, we carried out several ablation experiments. Based on the experimental results, a network trained from scratch performs as good as a network fine-tuned from a pre-trained network. However, we observed that pre-training the network has its own advantage in achieving better performances when deeper networks are required.



### One-Class Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1901.08688v1
- **DOI**: 10.1109/LSP.2018.2889273
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08688v1)
- **Published**: 2019-01-24 23:31:11+00:00
- **Updated**: 2019-01-24 23:31:11+00:00
- **Authors**: Poojan Oza, Vishal M. Patel
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel Convolutional Neural Network (CNN) based approach for one class classification. The idea is to use a zero centered Gaussian noise in the latent space as the pseudo-negative class and train the network using the cross-entropy loss to learn a good representation as well as the decision boundary for the given class. A key feature of the proposed approach is that any pre-trained CNN can be used as the base network for one class classification. The proposed One Class CNN (OC-CNN) is evaluated on the UMDAA-02 Face, Abnormality-1001, FounderType-200 datasets. These datasets are related to a variety of one class application problems such as user authentication, abnormality detection and novelty detection. Extensive experiments demonstrate that the proposed method achieves significant improvements over the recent state-of-the-art methods. The source code is available at : github.com/otkupjnoz/oc-cnn.



