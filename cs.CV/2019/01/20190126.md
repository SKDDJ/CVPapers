# Arxiv Papers in cs.CV on 2019-01-26
### DeepSZ: A Novel Framework to Compress Deep Neural Networks by Using Error-Bounded Lossy Compression
- **Arxiv ID**: http://arxiv.org/abs/1901.09124v2
- **DOI**: 10.1145/3307681.3326608
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09124v2)
- **Published**: 2019-01-26 00:24:15+00:00
- **Updated**: 2019-04-23 02:10:32+00:00
- **Authors**: Sian Jin, Sheng Di, Xin Liang, Jiannan Tian, Dingwen Tao, Franck Cappello
- **Comment**: 12 pages, 6 figures, accepted by HPDC'19
- **Journal**: None
- **Summary**: DNNs have been quickly and broadly exploited to improve the data analysis quality in many complex science and engineering applications. Today's DNNs are becoming deeper and wider because of increasing demand on the analysis quality and more and more complex applications to resolve. The wide and deep DNNs, however, require large amounts of resources, significantly restricting their utilization on resource-constrained systems. Although some network simplification methods have been proposed to address this issue, they suffer from either low compression ratios or high compression errors, which may introduce a costly retraining process for the target accuracy. In this paper, we propose DeepSZ: an accuracy-loss bounded neural network compression framework, which involves four key steps: network pruning, error bound assessment, optimization for error bound configuration, and compressed model generation, featuring a high compression ratio and low encoding time. The contribution is three-fold. (1) We develop an adaptive approach to select the feasible error bounds for each layer. (2) We build a model to estimate the overall loss of accuracy based on the accuracy degradation caused by individual decompressed layers. (3) We develop an efficient optimization algorithm to determine the best-fit configuration of error bounds in order to maximize the compression ratio under the user-set accuracy constraint. Experiments show that DeepSZ can compress AlexNet and VGG-16 on the ImageNet by a compression ratio of 46X and 116X, respectively, and compress LeNet-300-100 and LeNet-5 on the MNIST by a compression ratio of 57X and 56X, respectively, with only up to 0.3% loss of accuracy. Compared with other state-of-the-art methods, DeepSZ can improve the compression ratio by up to 1.43X, the DNN encoding performance by up to 4.0X (with four Nvidia Tesla V100 GPUs), and the decoding performance by up to 6.2X.



### See Better Before Looking Closer: Weakly Supervised Data Augmentation Network for Fine-Grained Visual Classification
- **Arxiv ID**: http://arxiv.org/abs/1901.09891v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09891v2)
- **Published**: 2019-01-26 02:03:27+00:00
- **Updated**: 2019-03-23 16:27:57+00:00
- **Authors**: Tao Hu, Honggang Qi, Qingming Huang, Yan Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation is usually adopted to increase the amount of training data, prevent overfitting and improve the performance of deep models. However, in practice, random data augmentation, such as random image cropping, is low-efficiency and might introduce many uncontrolled background noises. In this paper, we propose Weakly Supervised Data Augmentation Network (WS-DAN) to explore the potential of data augmentation. Specifically, for each training image, we first generate attention maps to represent the object's discriminative parts by weakly supervised learning. Next, we augment the image guided by these attention maps, including attention cropping and attention dropping. The proposed WS-DAN improves the classification accuracy in two folds. In the first stage, images can be seen better since more discriminative parts' features will be extracted. In the second stage, attention regions provide accurate location of object, which ensures our model to look at the object closer and further improve the performance. Comprehensive experiments in common fine-grained visual classification datasets show that our WS-DAN surpasses the state-of-the-art methods, which demonstrates its effectiveness.



### Human Pose Estimation using Motion Priors and Ensemble Models
- **Arxiv ID**: http://arxiv.org/abs/1901.09156v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09156v1)
- **Published**: 2019-01-26 04:22:30+00:00
- **Updated**: 2019-01-26 04:22:30+00:00
- **Authors**: Norimichi Ukita
- **Comment**: 6 pages
- **Journal**: Presented at the 2017 International Conference on Advanced
  Computer Science and Information Systems (ICACSIS)
- **Summary**: Human pose estimation in images and videos is one of key technologies for realizing a variety of human activity recognition tasks (e.g., human-computer interaction, gesture recognition, surveillance, and video summarization). This paper presents two types of human pose estimation methodologies; 1) 3D human pose tracking using motion priors and 2) 2D human pose estimation with ensemble modeling.



### Scene Text Synthesis for Efficient and Effective Deep Network Training
- **Arxiv ID**: http://arxiv.org/abs/1901.09193v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09193v3)
- **Published**: 2019-01-26 10:15:24+00:00
- **Updated**: 2023-04-24 12:35:49+00:00
- **Authors**: Changgong Zhang, Fangneng Zhan, Hongyuan Zhu, Shijian Lu
- **Comment**: This work has been merged into another project
- **Journal**: None
- **Summary**: A large amount of annotated training images is critical for training accurate and robust deep network models but the collection of a large amount of annotated training images is often time-consuming and costly. Image synthesis alleviates this constraint by generating annotated training images automatically by machines which has attracted increasing interest in the recent deep learning research. We develop an innovative image synthesis technique that composes annotated training images by realistically embedding foreground objects of interest (OOI) into background images. The proposed technique consists of two key components that in principle boost the usefulness of the synthesized images in deep network training. The first is context-aware semantic coherence which ensures that the OOI are placed around semantically coherent regions within the background image. The second is harmonious appearance adaptation which ensures that the embedded OOI are agreeable to the surrounding background from both geometry alignment and appearance realism. The proposed technique has been evaluated over two related but very different computer vision challenges, namely, scene text detection and scene text recognition. Experiments over a number of public datasets demonstrate the effectiveness of our proposed image synthesis technique - the use of our synthesized images in deep network training is capable of achieving similar or even better scene text detection and scene text recognition performance as compared with using real images.



### Deep Convolutional Encoder-Decoders with Aggregated Multi-Resolution Skip Connections for Skin Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1901.09197v2
- **DOI**: 10.1109/ISBI.2019.8759172
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09197v2)
- **Published**: 2019-01-26 10:55:27+00:00
- **Updated**: 2019-04-04 13:27:37+00:00
- **Authors**: Ahmed H. Shahin, Karim Amer, Mustafa A. Elattar
- **Comment**: Accepted for publication at the IEEE International Symposium on
  Biomedical Imaging (ISBI 2019)
- **Journal**: None
- **Summary**: The prevalence of skin melanoma is rapidly increasing as well as the recorded death cases of its patients. Automatic image segmentation tools play an important role in providing standardized computer-assisted analysis for skin melanoma patients. Current state-of-the-art segmentation methods are based on fully convolutional neural networks, which utilize an encoder-decoder approach. However, these methods produce coarse segmentation masks due to the loss of location information during the encoding layers. Inspired by Pyramid Scene Parsing Network (PSP-Net), we propose an encoder-decoder model that utilizes pyramid pooling modules in the deep skip connections which aggregate the global context and compensate for the lost spatial information. We trained and validated our approach using ISIC 2018: Skin Lesion Analysis Towards Melanoma Detection grand challenge dataset. Our approach showed a validation accuracy with a Jaccard index of 0.837, which outperforms U-Net. We believe that with this reported reliable accuracy, this method can be introduced for clinical practice.



### Progressive Image Deraining Networks: A Better and Simpler Baseline
- **Arxiv ID**: http://arxiv.org/abs/1901.09221v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09221v3)
- **Published**: 2019-01-26 14:44:17+00:00
- **Updated**: 2019-05-16 04:05:09+00:00
- **Authors**: Dongwei Ren, Wangmeng Zuo, Qinghua Hu, Pengfei Zhu, Deyu Meng
- **Comment**: Accepted to CVPR 2019. The codes, pre-trained models and results are
  available at https://github.com/csdwren/PReNet
- **Journal**: None
- **Summary**: Along with the deraining performance improvement of deep networks, their structures and learning become more and more complicated and diverse, making it difficult to analyze the contribution of various network modules when developing new deraining networks. To handle this issue, this paper provides a better and simpler baseline deraining network by considering network architecture, input and output, and loss functions. Specifically, by repeatedly unfolding a shallow ResNet, progressive ResNet (PRN) is proposed to take advantage of recursive computation. A recurrent layer is further introduced to exploit the dependencies of deep features across stages, forming our progressive recurrent network (PReNet). Furthermore, intra-stage recursive computation of ResNet can be adopted in PRN and PReNet to notably reduce network parameters with graceful degradation in deraining performance. For network input and output, we take both stage-wise result and original rainy image as input to each ResNet and finally output the prediction of {residual image}. As for loss functions, single MSE or negative SSIM losses are sufficient to train PRN and PReNet. Experiments show that PRN and PReNet perform favorably on both synthetic and real rainy images. Considering its simplicity, efficiency and effectiveness, our models are expected to serve as a suitable baseline in future deraining research. The source codes are available at https://github.com/csdwren/PReNet.



### On Detecting GANs and Retouching based Synthetic Alterations
- **Arxiv ID**: http://arxiv.org/abs/1901.09237v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09237v1)
- **Published**: 2019-01-26 16:13:03+00:00
- **Updated**: 2019-01-26 16:13:03+00:00
- **Authors**: Anubhav Jain, Richa Singh, Mayank Vatsa
- **Comment**: The 9th IEEE International Conference on Biometrics: Theory,
  Applications, and Systems (BTAS 2018)
- **Journal**: None
- **Summary**: Digitally retouching images has become a popular trend, with people posting altered images on social media and even magazines posting flawless facial images of celebrities. Further, with advancements in Generative Adversarial Networks (GANs), now changing attributes and retouching have become very easy. Such synthetic alterations have adverse effect on face recognition algorithms. While researchers have proposed to detect image tampering, detecting GANs generated images has still not been explored. This paper proposes a supervised deep learning algorithm using Convolutional Neural Networks (CNNs) to detect synthetically altered images. The algorithm yields an accuracy of 99.65% on detecting retouching on the ND-IIITD dataset. It outperforms the previous state of the art which reported an accuracy of 87% on the database. For distinguishing between real images and images generated using GANs, the proposed algorithm yields an accuracy of 99.83%.



### DistInit: Learning Video Representations Without a Single Labeled Video
- **Arxiv ID**: http://arxiv.org/abs/1901.09244v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09244v2)
- **Published**: 2019-01-26 16:50:38+00:00
- **Updated**: 2019-08-20 05:02:40+00:00
- **Authors**: Rohit Girdhar, Du Tran, Lorenzo Torresani, Deva Ramanan
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Video recognition models have progressed significantly over the past few years, evolving from shallow classifiers trained on hand-crafted features to deep spatiotemporal networks. However, labeled video data required to train such models have not been able to keep up with the ever-increasing depth and sophistication of these networks. In this work, we propose an alternative approach to learning video representations that require no semantically labeled videos and instead leverages the years of effort in collecting and labeling large and clean still-image datasets. We do so by using state-of-the-art models pre-trained on image datasets as "teachers" to train video models in a distillation framework. We demonstrate that our method learns truly spatiotemporal features, despite being trained only using supervision from still-image networks. Moreover, it learns good representations across different input modalities, using completely uncurated raw video data sources and with different 2D teacher models. Our method obtains strong transfer performance, outperforming standard techniques for bootstrapping video architectures with image-based models by 16%. We believe that our approach opens up new approaches for learning spatiotemporal representations from unlabeled video data.



### 4D Generic Video Object Proposals
- **Arxiv ID**: http://arxiv.org/abs/1901.09260v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1901.09260v3)
- **Published**: 2019-01-26 18:31:23+00:00
- **Updated**: 2020-05-20 18:45:15+00:00
- **Authors**: Aljosa Osep, Paul Voigtlaender, Mark Weber, Jonathon Luiten, Bastian Leibe
- **Comment**: ICRA 2020
- **Journal**: None
- **Summary**: Many high-level video understanding methods require input in the form of object proposals. Currently, such proposals are predominantly generated with the help of networks that were trained for detecting and segmenting a set of known object classes, which limits their applicability to cases where all objects of interest are represented in the training set. This is a restriction for automotive scenarios, where unknown objects can frequently occur. We propose an approach that can reliably extract spatio-temporal object proposals for both known and unknown object categories from stereo video. Our 4D Generic Video Tubes (4D-GVT) method leverages motion cues, stereo data, and object instance segmentation to compute a compact set of video-object proposals that precisely localizes object candidates and their contours in 3D space and time. We show that given only a small amount of labeled data, our 4D-GVT proposal generator generalizes well to real-world scenarios, in which unknown categories appear. It outperforms other approaches that try to detect as many objects as possible by increasing the number of classes in the training set to several thousand.



### Soft labeling by Distilling Anatomical knowledge for Improved MS Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1901.09263v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09263v1)
- **Published**: 2019-01-26 18:52:46+00:00
- **Updated**: 2019-01-26 18:52:46+00:00
- **Authors**: Eytan Kats, Jacob Goldberger, Hayit Greenspan
- **Comment**: Accepted for ISBI 2019
- **Journal**: None
- **Summary**: This paper explores the use of a soft ground-truth mask ("soft mask'') to train a Fully Convolutional Neural Network (FCNN) for segmentation of Multiple Sclerosis (MS) lesions. Detection and segmentation of MS lesions is a complex task largely due to the extreme unbalanced data, with very small number of lesion pixels that can be used for training. Utilizing the anatomical knowledge that the lesion surrounding pixels may also include some lesion level information, we suggest to increase the data set of the lesion class with neighboring pixel data - with a reduced confidence weight. A soft mask is constructed by morphological dilation of the binary segmentation mask provided by a given expert, where expert-marked voxels receive label 1 and voxels of the dilated region are assigned a soft label. In the methodology proposed, the FCNN is trained using the soft mask. On the ISBI 2015 challenge dataset, this is shown to provide a better precision-recall tradeoff and to achieve a higher average Dice similarity coefficient. We also show that by using this soft mask scheme we can improve the network segmentation performance when compared to a second independent expert.



### Challenges in Designing Datasets and Validation for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1901.09270v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.09270v1)
- **Published**: 2019-01-26 19:51:59+00:00
- **Updated**: 2019-01-26 19:51:59+00:00
- **Authors**: Michal Uricar, David Hurych, Pavel Krizek, Senthil Yogamani
- **Comment**: Accepted at VISAPP 2019
- **Journal**: None
- **Summary**: Autonomous driving is getting a lot of attention in the last decade and will be the hot topic at least until the first successful certification of a car with Level 5 autonomy. There are many public datasets in the academic community. However, they are far away from what a robust industrial production system needs. There is a large gap between academic and industrial setting and a substantial way from a research prototype, built on public datasets, to a deployable solution which is a challenging task. In this paper, we focus on bad practices that often happen in the autonomous driving from an industrial deployment perspective. Data design deserves at least the same amount of attention as the model design. There is very little attention paid to these issues in the scientific community, and we hope this paper encourages better formalization of dataset design. More specifically, we focus on the datasets design and validation scheme for autonomous driving, where we would like to highlight the common problems, wrong assumptions, and steps towards avoiding them, as well as some open problems.



### Points2Pix: 3D Point-Cloud to Image Translation using conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.09280v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09280v2)
- **Published**: 2019-01-26 21:17:21+00:00
- **Updated**: 2019-09-16 12:02:41+00:00
- **Authors**: Stefan Milz, Martin Simon, Kai Fischer, Maximillian Pöpperl
- **Comment**: None
- **Journal**: None
- **Summary**: We present the first approach for 3D point-cloud to image translation based on conditional Generative Adversarial Networks (cGAN). The model handles multi-modal information sources from different domains, i.e. raw point-sets and images. The generator is capable of processing three conditions, whereas the point-cloud is encoded as raw point-set and camera projection. An image background patch is used as constraint to bias environmental texturing. A global approximation function within the generator is directly applied on the point-cloud (Point-Net). Hence, the representative learning model incorporates global 3D characteristics directly at the latent feature space. Conditions are used to bias the background and the viewpoint of the generated image. This opens up new ways in augmenting or texturing 3D data to aim the generation of fully individual images. We successfully evaluated our method on the Kitti and SunRGBD dataset with an outstanding object detection inception score.



### Real-time Video Summarization on Commodity Hardware
- **Arxiv ID**: http://arxiv.org/abs/1901.09287v1
- **DOI**: 10.1145/3243394.3243689 10.1145/3243394.3243689
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09287v1)
- **Published**: 2019-01-26 22:20:50+00:00
- **Updated**: 2019-01-26 22:20:50+00:00
- **Authors**: Wesley Taylor, Faisal Z. Qureshi
- **Comment**: Appeared in Proc. 12th ACM International Conference on Distributed
  Smart Cameras (ICDSC 18), pages 8pp, Eidenhoven, September 2018
- **Journal**: None
- **Summary**: We present a method for creating video summaries in real-time on commodity hardware. Real-time here refers to the fact that the time required for video summarization is less than the duration of the input video. First, low-level features are use to discard undesirable frames. Next, video is divided into segments, and segment-level features are extracted for each segment. Tree-based models trained on widely available video summarization and computational aesthetics datasets are then used to rank individual segments, and top-ranked segments are selected to generate the final video summary. We evaluate the proposed method on SUMME dataset and show that our method is able to achieve summarization accuracy that is comparable to that of a current state-of-the-art deep learning method, while posting significantly faster run-times. Our method on average is able to generate a video summary in time that is shorter than the duration of the video.



### Hotels-50K: A Global Hotel Recognition Dataset
- **Arxiv ID**: http://arxiv.org/abs/1901.11397v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.11397v1)
- **Published**: 2019-01-26 22:22:46+00:00
- **Updated**: 2019-01-26 22:22:46+00:00
- **Authors**: Abby Stylianou, Hong Xuan, Maya Shende, Jonathan Brandt, Richard Souvenir, Robert Pless
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing a hotel from an image of a hotel room is important for human trafficking investigations. Images directly link victims to places and can help verify where victims have been trafficked, and where their traffickers might move them or others in the future. Recognizing the hotel from images is challenging because of low image quality, uncommon camera perspectives, large occlusions (often the victim), and the similarity of objects (e.g., furniture, art, bedding) across different hotel rooms.   To support efforts towards this hotel recognition task, we have curated a dataset of over 1 million annotated hotel room images from 50,000 hotels. These images include professionally captured photographs from travel websites and crowd-sourced images from a mobile application, which are more similar to the types of images analyzed in real-world investigations. We present a baseline approach based on a standard network architecture and a collection of data-augmentation approaches tuned to this problem domain.



