# Arxiv Papers in cs.CV on 2019-01-28
### An End-to-End Solution for Effectively Demoting Watermarked Images in Image Search
- **Arxiv ID**: http://arxiv.org/abs/1901.09473v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09473v2)
- **Published**: 2019-01-28 00:38:16+00:00
- **Updated**: 2019-08-26 05:34:39+00:00
- **Authors**: Ning Ma, Xin Zhao, Mark Bolin
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an end-to-end solution, from watermark feature generation to metric design, for effectively demoting watermarked images surfed by a real world image search engine. We use a few fundamental techniques to obtain effective watermark features of images in the image search index, and utilize the signals in a commercial search engine to improve the image search quality. We collect a diverse and large set (about 1M) of images with human labels indicating whether the image contains visible watermark. We train a few deep convolutional neural networks to extract watermark information from the raw images. The deep CNN classifiers we trained can achieve high accuracy on the watermark test data set. We also analyze the images based on their domains to get watermark information from a domain-based watermark classifier. We design a new novel hybrid metric which includes the relevance, image attractiveness and watermark information all together. We demonstrate that using these watermark signals together with the new metric in image search ranker can significantly demote the watermarked images during the online image ranking.



### Bridging the Gap Between Computational Photography and Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1901.09482v3
- **DOI**: 10.1109/TPAMI.2020.2996538
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09482v3)
- **Published**: 2019-01-28 01:34:32+00:00
- **Updated**: 2020-02-19 19:05:12+00:00
- **Authors**: Rosaura G. VidalMata, Sreya Banerjee, Brandon RichardWebster, Michael Albright, Pedro Davalos, Scott McCloskey, Ben Miller, Asong Tambo, Sushobhan Ghosh, Sudarshan Nagesh, Ye Yuan, Yueyu Hu, Junru Wu, Wenhan Yang, Xiaoshuai Zhang, Jiaying Liu, Zhangyang Wang, Hwann-Tzong Chen, Tzu-Wei Huang, Wen-Chi Chin, Yi-Chun Li, Mahmoud Lababidi, Charles Otto, Walter J. Scheirer
- **Comment**: CVPR Prize Challenge: http://www.ug2challenge.org
- **Journal**: None
- **Summary**: What is the current state-of-the-art for image restoration and enhancement applied to degraded images acquired under less than ideal circumstances? Can the application of such algorithms as a pre-processing step to improve image interpretability for manual analysis or automatic visual recognition to classify scene content? While there have been important advances in the area of computational photography to restore or enhance the visual quality of an image, the capabilities of such techniques have not always translated in a useful way to visual recognition tasks. Consequently, there is a pressing need for the development of algorithms that are designed for the joint problem of improving visual appearance and recognition, which will be an enabling factor for the deployment of visual recognition tools in many real-world scenarios. To address this, we introduce the UG^2 dataset as a large-scale benchmark composed of video imagery captured under challenging conditions, and two enhancement tasks designed to test algorithmic impact on visual quality and automatic object recognition. Furthermore, we propose a set of metrics to evaluate the joint improvement of such tasks as well as individual algorithmic advances, including a novel psychophysics-based evaluation regime for human assessment and a realistic set of quantitative measures for object recognition performance. We introduce six new algorithms for image restoration or enhancement, which were created as part of the IARPA sponsored UG^2 Challenge workshop held at CVPR 2018. Under the proposed evaluation regime, we present an in-depth analysis of these algorithms and a host of deep learning-based and classic baseline approaches. From the observed results, it is evident that we are in the early days of building a bridge between computational photography and visual recognition, leaving many opportunities for innovation in this area.



### End-to-End Discriminative Deep Network for Liver Lesion Classification
- **Arxiv ID**: http://arxiv.org/abs/1901.09483v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09483v1)
- **Published**: 2019-01-28 01:41:16+00:00
- **Updated**: 2019-01-28 01:41:16+00:00
- **Authors**: Francisco Perdigon Romero, Andre Diler, Gabriel Bisson-Gregoire, Simon Turcotte, Real Lapointe, Franck Vandenbroucke-Menu, An Tang, Samuel Kadoury
- **Comment**: None
- **Journal**: None
- **Summary**: Colorectal liver metastasis is one of most aggressive liver malignancies. While the definition of lesion type based on CT images determines the diagnosis and therapeutic strategy, the discrimination between cancerous and non-cancerous lesions are critical and requires highly skilled expertise, experience and time. In the present work we introduce an end-to-end deep learning approach to assist in the discrimination between liver metastases from colorectal cancer and benign cysts in abdominal CT images of the liver. Our approach incorporates the efficient feature extraction of InceptionV3 combined with residual connections and pre-trained weights from ImageNet. The architecture also includes fully connected classification layers to generate a probabilistic output of lesion type. We use an in-house clinical biobank with 230 liver lesions originating from 63 patients. With an accuracy of 0.96 and a F1-score of 0.92, the results obtained with the proposed approach surpasses state of the art methods. Our work provides the basis for incorporating machine learning tools in specialized radiology software to assist physicians in the early detection and treatment of liver lesions.



### CURE: Curvature Regularization For Missing Data Recovery
- **Arxiv ID**: http://arxiv.org/abs/1901.09548v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1901.09548v3)
- **Published**: 2019-01-28 08:42:39+00:00
- **Updated**: 2019-11-26 00:28:54+00:00
- **Authors**: Bin Dong, Haocheng Ju, Yiping Lu, Zuoqiang Shi
- **Comment**: 17 pages, 7 figures, 4 tables
- **Journal**: None
- **Summary**: Missing data recovery is an important and yet challenging problem in imaging and data science. Successful models often adopt certain carefully chosen regularization. Recently, the low dimension manifold model (LDMM) was introduced by S.Osher et al. and shown effective in image inpainting. They observed that enforcing low dimensionality on image patch manifold serves as a good image regularizer. In this paper, we observe that having only the low dimension manifold regularization is not enough sometimes, and we need smoothness as well. For that, we introduce a new regularization by combining the low dimension manifold regularization with a higher order Curvature Regularization, and we call this new regularization CURE for short. The key step of solving CURE is to solve a biharmonic equation on a manifold. We further introduce a weighted version of CURE, called WeCURE, in a similar manner as the weighted nonlocal Laplacian (WNLL) method. Numerical experiments for image inpainting and semi-supervised learning show that the proposed CURE and WeCURE significantly outperform LDMM and WNLL respectively.



### Enhancing Quality for VVC Compressed Videos by Jointly Exploiting Spatial Details and Temporal Structure
- **Arxiv ID**: http://arxiv.org/abs/1901.09575v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09575v2)
- **Published**: 2019-01-28 09:42:43+00:00
- **Updated**: 2019-05-22 05:30:06+00:00
- **Authors**: Xiandong Meng, Xuan Deng, Shuyuan Zhu, Bing Zeng
- **Comment**: Accepted to IEEE International Conference on Image Processing (ICIP)
  2019
- **Journal**: None
- **Summary**: In this paper, we propose a quality enhancement network of versatile video coding (VVC) compressed videos by jointly exploiting spatial details and temporal structure (SDTS). The proposed network consists of a temporal structure fusion subnet and a spatial detail enhancement subnet. The former subnet is used to estimate and compensate the temporal motion across frames, and the latter subnet is used to reduce the compression artifacts and enhance the reconstruction quality of compressed video. Experimental results demonstrate the effectiveness of our SDTS-based method.



### Automatic Information Extraction from Piping and Instrumentation Diagrams
- **Arxiv ID**: http://arxiv.org/abs/1901.11383v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.11383v1)
- **Published**: 2019-01-28 09:50:45+00:00
- **Updated**: 2019-01-28 09:50:45+00:00
- **Authors**: Rohit Rahul, Shubham Paliwal, Monika Sharma, Lovekesh Vig
- **Comment**: None
- **Journal**: IEEE ICPRAM 2019
- **Summary**: One of the most common modes of representing engineering schematics are Piping and Instrumentation diagrams (P&IDs) that describe the layout of an engineering process flow along with the interconnected process equipment. Over the years, P&ID diagrams have been manually generated, scanned and stored as image files. These files need to be digitized for purposes of inventory management and updation, and easy reference to different components of the schematics. There are several challenging vision problems associated with digitizing real world P&ID diagrams. Real world P&IDs come in several different resolutions, and often contain noisy textual information. Extraction of instrumentation information from these diagrams involves accurate detection of symbols that frequently have minute visual differences between them. Identification of pipelines that may converge and diverge at different points in the image is a further cause for concern. Due to these reasons, to the best of our knowledge, no system has been proposed for end-to-end data extraction from P&ID diagrams. However, with the advent of deep learning and the spectacular successes it has achieved in vision, we hypothesized that it is now possible to re-examine this problem armed with the latest deep learning models. To that end, we present a novel pipeline for information extraction from P&ID sheets via a combination of traditional vision techniques and state-of-the-art deep learning models to identify and isolate pipeline codes, pipelines, inlets and outlets, and for detecting symbols. This is followed by association of the detected components with the appropriate pipeline. The extracted pipeline information is used to populate a tree-like data-structure for capturing the structure of the piping schematics. We evaluated proposed method on a real world dataset of P&ID sheets obtained from an oil firm and have obtained promising results.



### Learning to Clean: A GAN Perspective
- **Arxiv ID**: http://arxiv.org/abs/1901.11382v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.11382v1)
- **Published**: 2019-01-28 09:50:54+00:00
- **Updated**: 2019-01-28 09:50:54+00:00
- **Authors**: Monika Sharma, Abhishek Verma, Lovekesh Vig
- **Comment**: None
- **Journal**: IWRR 2018 Workshop at ACCV 2018
- **Summary**: In the big data era, the impetus to digitize the vast reservoirs of data trapped in unstructured scanned documents such as invoices, bank documents and courier receipts has gained fresh momentum. The scanning process often results in the introduction of artifacts such as background noise, blur due to camera motion, watermarkings, coffee stains, or faded text. These artifacts pose many readability challenges to current text recognition algorithms and significantly degrade their performance. Existing learning based denoising techniques require a dataset comprising of noisy documents paired with cleaned versions. In such scenarios, a model can be trained to generate clean documents from noisy versions. However, very often in the real world such a paired dataset is not available, and all we have for training our denoising model are unpaired sets of noisy and clean images. This paper explores the use of GANs to generate denoised versions of the noisy documents. In particular, where paired information is available, we formulate the problem as an image-to-image translation task i.e, translating a document from noisy domain ( i.e., background noise, blurred, faded, watermarked ) to a target clean document using Generative Adversarial Networks (GAN). However, in the absence of paired images for training, we employed CycleGAN which is known to learn a mapping between the distributions of the noisy images to the denoised images using unpaired data to achieve image-to-image translation for cleaning the noisy documents. We compare the performance of CycleGAN for document cleaning tasks using unpaired images with a Conditional GAN trained on paired data from the same dataset. Experiments were performed on a public document dataset on which different types of noise were artificially induced, results demonstrate that CycleGAN learns a more robust mapping from the space of noisy to clean documents.



### Fast Hierarchical Depth Map Computation from Stereo
- **Arxiv ID**: http://arxiv.org/abs/1901.09593v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09593v1)
- **Published**: 2019-01-28 10:56:04+00:00
- **Updated**: 2019-01-28 10:56:04+00:00
- **Authors**: Vinay Kaushik, Brejesh Lall
- **Comment**: Submitted to International Conference on Pattern Recognition and
  Artificial Intelligence, 2018
- **Journal**: None
- **Summary**: Disparity by Block Matching stereo is usually used in applications with limited computational power in order to get depth estimates. However, the research on simple stereo methods has been lesser than the energy based counterparts which promise a better quality depth map with more potential for future improvements. Semi-global-matching (SGM) methods offer good performance and easy implementation but suffer from the problem of very high memory footprint because it's working on the full disparity space image. On the other hand, Block matching stereo needs much less memory. In this paper, we introduce a novel multi-scale-hierarchical block-matching approach using a pyramidal variant of depth and cost functions which drastically improves the results of standard block matching stereo techniques while preserving the low memory footprint and further reducing the complexity of standard block matching. We tested our new multi block matching scheme on the Middlebury stereo benchmark. For the Middlebury benchmark we get results that are only slightly worse than state of the art SGM implementations.



### A Simple Method to Reduce Off-chip Memory Accesses on Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.09614v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.09614v1)
- **Published**: 2019-01-28 11:43:25+00:00
- **Updated**: 2019-01-28 11:43:25+00:00
- **Authors**: Doyun Kim, Kyoung-Young Kim, Sangsoo Ko, Sanghyuck Ha
- **Comment**: 9 pages, 10 figures, under review (by ICML2019)
- **Journal**: None
- **Summary**: For convolutional neural networks, a simple algorithm to reduce off-chip memory accesses is proposed by maximally utilizing on-chip memory in a neural process unit. Especially, the algorithm provides an effective way to process a module which consists of multiple branches and a merge layer. For Inception-V3 on Samsung's NPU in Exynos, our evaluation shows that the proposed algorithm makes off-chip memory accesses reduced by 1/50, and accordingly achieves 97.59 % reduction in the amount of feature-map data to be transferred from/to off-chip memory.



### Convolutional Neural Networks with Layer Reuse
- **Arxiv ID**: http://arxiv.org/abs/1901.09615v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09615v2)
- **Published**: 2019-01-28 11:45:50+00:00
- **Updated**: 2019-02-01 10:32:50+00:00
- **Authors**: Okan Köpüklü, Maryam Babaee, Stefan Hörmann, Gerhard Rigoll
- **Comment**: Computer Vision and Pattern Recognition
- **Journal**: None
- **Summary**: A convolutional layer in a Convolutional Neural Network (CNN) consists of many filters which apply convolution operation to the input, capture some special patterns and pass the result to the next layer. If the same patterns also occur at the deeper layers of the network, why wouldn't the same convolutional filters be used also in those layers? In this paper, we propose a CNN architecture, Layer Reuse Network (LruNet), where the convolutional layers are used repeatedly without the need of introducing new layers to get a better performance. This approach introduces several advantages: (i) Considerable amount of parameters are saved since we are reusing the layers instead of introducing new layers, (ii) the Memory Access Cost (MAC) can be reduced since reused layer parameters can be fetched only once, (iii) the number of nonlinearities increases with layer reuse, and (iv) reused layers get gradient updates from multiple parts of the network. The proposed approach is evaluated on CIFAR-10, CIFAR-100 and Fashion-MNIST datasets for image classification task, and layer reuse improves the performance by 5.14%, 5.85% and 2.29%, respectively. The source code and pretrained models are publicly available.



### Edge, Ridge, and Blob Detection with Symmetric Molecules
- **Arxiv ID**: http://arxiv.org/abs/1901.09723v3
- **DOI**: 10.1137/19M1240861
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09723v3)
- **Published**: 2019-01-28 15:08:37+00:00
- **Updated**: 2021-06-19 10:00:45+00:00
- **Authors**: Rafael Reisenhofer, Emily J. King
- **Comment**: Accepted version. Supplemental materials available at
  www.math.uni-bremen.de/cda/publications.html
- **Journal**: SIAM J. Imaging Sci. 12(4), 2019, 1585-1626
- **Summary**: We present a novel approach to the detection and characterization of edges, ridges, and blobs in two-dimensional images which exploits the symmetry properties of directionally sensitive analyzing functions in multiscale systems that are constructed in the framework of alpha-molecules. The proposed feature detectors are inspired by the notion of phase congruency, stable in the presence of noise, and by definition invariant to changes in contrast. We also show how the behavior of coefficients corresponding to differently scaled and oriented analyzing functions can be used to obtain a comprehensive characterization of the geometry of features in terms of local tangent directions, widths, and heights. The accuracy and robustness of the proposed measures are validated and compared to various state-of-the-art algorithms in extensive numerical experiments in which we consider sets of clean and distorted synthetic images that are associated with reliable ground truths. To further demonstrate the applicability, we show how the proposed ridge measure can be used to detect and characterize blood vessels in digital retinal images and how the proposed blob measure can be applied to automatically count the number of cell colonies in a Petri dish.



### CollaGAN : Collaborative GAN for Missing Image Data Imputation
- **Arxiv ID**: http://arxiv.org/abs/1901.09764v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.09764v3)
- **Published**: 2019-01-28 16:10:12+00:00
- **Updated**: 2019-04-29 19:46:06+00:00
- **Authors**: Dongwook Lee, Junyoung Kim, Won-Jin Moon, Jong Chul Ye
- **Comment**: CVPR 2019 Camera Ready Version (oral presentation)
- **Journal**: None
- **Summary**: In many applications requiring multiple inputs to obtain a desired output, if any of the input data is missing, it often introduces large amounts of bias. Although many techniques have been developed for imputing missing data, the image imputation is still difficult due to complicated nature of natural images. To address this problem, here we proposed a novel framework for missing image data imputation, called Collaborative Generative Adversarial Network (CollaGAN). CollaGAN converts an image imputation problem to a multi-domain images-to-image translation task so that a single generator and discriminator network can successfully estimate the missing data using the remaining clean data set. We demonstrate that CollaGAN produces the images with a higher visual quality compared to the existing competing approaches in various image imputation tasks.



### Attribute-Guided Sketch Generation
- **Arxiv ID**: http://arxiv.org/abs/1901.09774v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1901.09774v2)
- **Published**: 2019-01-28 16:20:20+00:00
- **Updated**: 2019-04-14 15:27:38+00:00
- **Authors**: Hao Tang, Xinya Chen, Wei Wang, Dan Xu, Jason J. Corso, Nicu Sebe, Yan Yan
- **Comment**: 7 pages, 6 figures, accepted to FG 2019
- **Journal**: None
- **Summary**: Facial attributes are important since they provide a detailed description and determine the visual appearance of human faces. In this paper, we aim at converting a face image to a sketch while simultaneously generating facial attributes. To this end, we propose a novel Attribute-Guided Sketch Generative Adversarial Network (ASGAN) which is an end-to-end framework and contains two pairs of generators and discriminators, one of which is used to generate faces with attributes while the other one is employed for image-to-sketch translation. The two generators form a W-shaped network (W-net) and they are trained jointly with a weight-sharing constraint. Additionally, we also propose two novel discriminators, the residual one focusing on attribute generation and the triplex one helping to generate realistic looking sketches. To validate our model, we have created a new large dataset with 8,804 images, named the Attribute Face Photo & Sketch (AFPS) dataset which is the first dataset containing attributes associated to face sketch images. The experimental results demonstrate that the proposed network (i) generates more photo-realistic faces with sharper facial attributes than baselines and (ii) has good generalization capability on different generative tasks.



### Leveraging Outdoor Webcams for Local Descriptor Learning
- **Arxiv ID**: http://arxiv.org/abs/1901.09780v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09780v2)
- **Published**: 2019-01-28 16:28:52+00:00
- **Updated**: 2021-01-01 19:12:28+00:00
- **Authors**: Milan Pultar, Dmytro Mishkin, Jiří Matas
- **Comment**: None
- **Journal**: None
- **Summary**: We present AMOS Patches, a large set of image cut-outs, intended primarily for the robustification of trainable local feature descriptors to illumination and appearance changes. Images contributing to AMOS Patches originate from the AMOS dataset of recordings from a large set of outdoor webcams.   The semiautomatic method used to generate AMOS Patches is described. It includes camera selection, viewpoint clustering and patch selection. For training, we provide both the registered full source images as well as the patches.   A new descriptor, trained on the AMOS Patches and 6Brown datasets, is introduced. It achieves state-of-the-art in matching under illumination changes on standard benchmarks.



### Generalization of feature embeddings transferred from different video anomaly detection domains
- **Arxiv ID**: http://arxiv.org/abs/1901.09819v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09819v1)
- **Published**: 2019-01-28 17:10:47+00:00
- **Updated**: 2019-01-28 17:10:47+00:00
- **Authors**: Fernando Pereira dos Santos, Leonardo Sampaio Ferraz Ribeiro, Moacir Antonelli Ponti
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting anomalous activity in video surveillance often involves using only normal activity data in order to learn an accurate detector. Due to lack of annotated data for some specific target domain, one could employ existing data from a source domain to produce better predictions. Hence, transfer learning presents itself as an important tool. But how to analyze the resulting data space? This paper investigates video anomaly detection, in particular feature embeddings of pre-trained CNN that can be used with non-fully supervised data. By proposing novel cross-domain generalization measures, we study how source features can generalize for different target video domains, as well as analyze unsupervised transfer learning. The proposed generalization measures are not only a theorical approach, but show to be useful in practice as a way to understand which datasets can be used or transferred to describe video frames, which it is possible to better discriminate between normal and anomalous activity.



### Multi-modal dialog for browsing large visual catalogs using exploration-exploitation paradigm in a joint embedding space
- **Arxiv ID**: http://arxiv.org/abs/1901.09854v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09854v2)
- **Published**: 2019-01-28 17:49:55+00:00
- **Updated**: 2019-01-29 21:13:03+00:00
- **Authors**: Indrani Bhattacharya, Arkabandhu Chowdhury, Vikas Raykar
- **Comment**: 10 pages including reference, 8 figures. First two authors are equal
  contributors
- **Journal**: None
- **Summary**: We present a multi-modal dialog system to assist online shoppers in visually browsing through large catalogs. Visual browsing is different from visual search in that it allows the user to explore the wide range of products in a catalog, beyond the exact search matches. We focus on a slightly asymmetric version of the complete multi-modal dialog where the system can understand both text and image queries but responds only in images. We formulate our problem of "showing $k$ best images to a user" based on the dialog context so far, as sampling from a Gaussian Mixture Model in a high dimensional joint multi-modal embedding space, that embed both the text and the image queries. Our system remembers the context of the dialog and uses an exploration-exploitation paradigm to assist in visual browsing. We train and evaluate the system on a multi-modal dialog dataset that we generate from large catalog data. Our experiments are promising and show that the agent is capable of learning and can display relevant results with an average cosine similarity of 0.85 to the ground truth. Our preliminary human evaluation also corroborates the fact that such a multi-modal dialog system for visual browsing is well-received and is capable of engaging human users.



### CapsAttacks: Robust and Imperceptible Adversarial Attacks on Capsule Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.09878v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.09878v2)
- **Published**: 2019-01-28 18:50:52+00:00
- **Updated**: 2019-05-24 12:52:46+00:00
- **Authors**: Alberto Marchisio, Giorgio Nanfa, Faiq Khalid, Muhammad Abdullah Hanif, Maurizio Martina, Muhammad Shafique
- **Comment**: None
- **Journal**: None
- **Summary**: Capsule Networks preserve the hierarchical spatial relationships between objects, and thereby bears a potential to surpass the performance of traditional Convolutional Neural Networks (CNNs) in performing tasks like image classification. A large body of work has explored adversarial examples for CNNs, but their effectiveness on Capsule Networks has not yet been well studied. In our work, we perform an analysis to study the vulnerabilities in Capsule Networks to adversarial attacks. These perturbations, added to the test inputs, are small and imperceptible to humans, but can fool the network to mispredict. We propose a greedy algorithm to automatically generate targeted imperceptible adversarial examples in a black-box attack scenario. We show that this kind of attacks, when applied to the German Traffic Sign Recognition Benchmark (GTSRB), mislead Capsule Networks. Moreover, we apply the same kind of adversarial attacks to a 5-layer CNN and a 9-layer CNN, and analyze the outcome, compared to the Capsule Networks to study differences in their behavior.



### CoCoNet: A Collaborative Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/1901.09886v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09886v4)
- **Published**: 2019-01-28 18:58:50+00:00
- **Updated**: 2020-11-09 20:44:25+00:00
- **Authors**: Tapabrata Chakraborti, Brendan McCane, Steven Mills, Umapada Pal
- **Comment**: None
- **Journal**: None
- **Summary**: We present an end-to-end deep network for fine-grained visual categorization called Collaborative Convolutional Network (CoCoNet). The network uses a collaborative layer after the convolutional layers to represent an image as an optimal weighted collaboration of features learned from training samples as a whole rather than one at a time. This gives CoCoNet more power to encode the fine-grained nature of the data with limited samples. We perform a detailed study of the performance with 1-stage and 2-stage transfer learning. The ablation study shows that the proposed method outperforms its constituent parts consistently. CoCoNet also outperforms few state-of-the-art competing methods. Experiments have been performed on the fine-grained bird species classification problem as a representative example, but the method may be applied to other similar tasks. We also introduce a new public dataset for fine-grained species recognition, that of Indian endemic birds and have reported initial results on it.



### TGAN: Deep Tensor Generative Adversarial Nets for Large Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1901.09953v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.09953v2)
- **Published**: 2019-01-28 19:29:39+00:00
- **Updated**: 2019-03-04 23:07:37+00:00
- **Authors**: Zihan Ding, Xiao-Yang Liu, Miao Yin, Linghe Kong
- **Comment**: None
- **Journal**: None
- **Summary**: Deep generative models have been successfully applied to many applications. However, existing works experience limitations when generating large images (the literature usually generates small images, e.g. 32 * 32 or 128 * 128). In this paper, we propose a novel scheme, called deep tensor adversarial generative nets (TGAN), that generates large high-quality images by exploring tensor structures. Essentially, the adversarial process of TGAN takes place in a tensor space. First, we impose tensor structures for concise image representation, which is superior in capturing the pixel proximity information and the spatial patterns of elementary objects in images, over the vectorization preprocess in existing works. Secondly, we propose TGAN that integrates deep convolutional generative adversarial networks and tensor super-resolution in a cascading manner, to generate high-quality images from random distributions. More specifically, we design a tensor super-resolution process that consists of tensor dictionary learning and tensor coefficients learning. Finally, on three datasets, the proposed TGAN generates images with more realistic textures, compared with state-of-the-art adversarial autoencoders. The size of the generated images is increased by over 8.5 times, namely 374 * 374 in PASCAL2.



### Using Pre-Training Can Improve Model Robustness and Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/1901.09960v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.09960v5)
- **Published**: 2019-01-28 19:37:07+00:00
- **Updated**: 2019-10-20 20:09:20+00:00
- **Authors**: Dan Hendrycks, Kimin Lee, Mantas Mazeika
- **Comment**: ICML 2019. PyTorch code here:
  https://github.com/hendrycks/pre-training Figure 3 updated
- **Journal**: None
- **Summary**: He et al. (2018) have called into question the utility of pre-training by showing that training from scratch can often yield similar performance to pre-training. We show that although pre-training may not improve performance on traditional classification metrics, it improves model robustness and uncertainty estimates. Through extensive experiments on adversarial examples, label corruption, class imbalance, out-of-distribution detection, and confidence calibration, we demonstrate large gains from pre-training and complementary effects with task-specific methods. We introduce adversarial pre-training and show approximately a 10% absolute improvement over the previous state-of-the-art in adversarial robustness. In some cases, using pre-training without task-specific methods also surpasses the state-of-the-art, highlighting the need for pre-training when evaluating future methods on robustness and uncertainty tasks.



### DeGraF-Flow: Extending DeGraF Features for accurate and efficient sparse-to-dense optical flow estimation
- **Arxiv ID**: http://arxiv.org/abs/1901.09971v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1901.09971v2)
- **Published**: 2019-01-28 19:55:40+00:00
- **Updated**: 2019-05-20 16:42:22+00:00
- **Authors**: Felix Stephenson, Toby Breckon, Ioannis Katramados
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: Modern optical flow methods make use of salient scene feature points detected and matched within the scene as a basis for sparse-to-dense optical flow estimation. Current feature detectors however either give sparse, non uniform point clouds (resulting in flow inaccuracies) or lack the efficiency for frame-rate real-time applications. In this work we use the novel Dense Gradient Based Features (DeGraF) as the input to a sparse-to-dense optical flow scheme. This consists of three stages: 1) efficient detection of uniformly distributed Dense Gradient Based Features (DeGraF); 2) feature tracking via robust local optical flow; and 3) edge preserving flow interpolation to recover overall dense optical flow. The tunable density and uniformity of DeGraF features yield superior dense optical flow estimation compared to other popular feature detectors within this three stage pipeline. Furthermore, the comparable speed of feature detection also lends itself well to the aim of real-time optical flow recovery. Evaluation on established real-world benchmark datasets show test performance in an autonomous vehicle setting where DeGraF-Flow shows promising results in terms of accuracy with competitive computational efficiency among non-GPU based methods, including a marked increase in speed over the conceptually similar EpicFlow approach.



### Heartbeat Anomaly Detection using Adversarial Oversampling
- **Arxiv ID**: http://arxiv.org/abs/1901.09972v1
- **DOI**: 10.1109/IJCNN.2019.8852242
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.09972v1)
- **Published**: 2019-01-28 19:55:42+00:00
- **Updated**: 2019-01-28 19:55:42+00:00
- **Authors**: Jefferson L. P. Lima, David Macêdo, Cleber Zanchettin
- **Comment**: None
- **Journal**: 2019 International Joint Conference on Neural Networks (IJCNN)
- **Summary**: Cardiovascular diseases are one of the most common causes of death in the world. Prevention, knowledge of previous cases in the family, and early detection is the best strategy to reduce this fact. Different machine learning approaches to automatic diagnostic are being proposed to this task. As in most health problems, the imbalance between examples and classes is predominant in this problem and affects the performance of the automated solution. In this paper, we address the classification of heartbeats images in different cardiovascular diseases. We propose a two-dimensional Convolutional Neural Network for classification after using a InfoGAN architecture for generating synthetic images to unbalanced classes. We call this proposal Adversarial Oversampling and compare it with the classical oversampling methods as SMOTE, ADASYN, and RandomOversampling. The results show that the proposed approach improves the classifier performance for the minority classes without harming the performance in the balanced classes.



### Compressed Domain Image Classification Using a Dynamic-Rate Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1901.09983v2
- **DOI**: 10.1109/ACCESS.2020.3041807
- **Categories**: **cs.CV**, I.2.10; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/1901.09983v2)
- **Published**: 2019-01-28 20:16:24+00:00
- **Updated**: 2020-12-14 16:23:51+00:00
- **Authors**: Yibo Xu, Weidi Liu, Kevin F. Kelly
- **Comment**: None
- **Journal**: in IEEE Access, vol. 8, pp. 217711-217722, 2020
- **Summary**: Compressed domain image classification performs classification directly on compressive measurements acquired from the single-pixel camera, bypassing the image reconstruction step. It is of great importance for extending high-speed object detection and classification beyond the visible spectrum in a cost-effective manner especially for resource-limited platforms. Previous neural network methods require training a dedicated neural network for each different measurement rate (MR), which is costly in computation and storage. In this work, we develop an efficient training scheme that provides a neural network with dynamic-rate property, where a single neural network is capable of classifying over any MR within the range of interest with a given sensing matrix. This training scheme uses only a few selected MRs for training and the trained neural network is valid over the full range of MRs of interest. We demonstrate the performance of the dynamic-rate neural network on datasets of MNIST, CIFAR-10, Fashion-MNIST, COIL-100, and show that it generates approximately equal performance at each MR as that of a single-rate neural network valid only for one MR. Robustness to noise of the dynamic-rate model is also demonstrated. The dynamic-rate training scheme can be regarded as a general approach compatible with different types of sensing matrices, various neural network architectures, and is a valuable step towards wider adoption of compressive inference techniques and other compressive sensing related tasks via neural networks.



### Dense Depth Posterior (DDP) from Single Image and Sparse Range
- **Arxiv ID**: http://arxiv.org/abs/1901.10034v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.10034v2)
- **Published**: 2019-01-28 23:26:07+00:00
- **Updated**: 2019-04-17 07:26:47+00:00
- **Authors**: Yanchao Yang, Alex Wong, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: We present a deep learning system to infer the posterior distribution of a dense depth map associated with an image, by exploiting sparse range measurements, for instance from a lidar. While the lidar may provide a depth value for a small percentage of the pixels, we exploit regularities reflected in the training set to complete the map so as to have a probability over depth for each pixel in the image. We exploit a Conditional Prior Network, that allows associating a probability to each depth value given an image, and combine it with a likelihood term that uses the sparse measurements. Optionally we can also exploit the availability of stereo during training, but in any case only require a single image and a sparse point cloud at run-time. We test our approach on both unsupervised and supervised depth completion using the KITTI benchmark, and improve the state-of-the-art in both.



