# Arxiv Papers in cs.CV on 2019-01-08
### Spatial-Winograd Pruning Enabling Sparse Winograd Convolution
- **Arxiv ID**: http://arxiv.org/abs/1901.02132v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1901.02132v1)
- **Published**: 2019-01-08 02:17:44+00:00
- **Updated**: 2019-01-08 02:17:44+00:00
- **Authors**: Jiecao Yu, Jongsoo Park, Maxim Naumov
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) are deployed in various applications but demand immense computational requirements. Pruning techniques and Winograd convolution are two typical methods to reduce the CNN computation. However, they cannot be directly combined because Winograd transformation fills in the sparsity resulting from pruning. Li et al. (2017) propose sparse Winograd convolution in which weights are directly pruned in the Winograd domain, but this technique is not very practical because Winograd-domain retraining requires low learning rates and hence significantly longer training time. Besides, Liu et al. (2018) move the ReLU function into the Winograd domain, which can help increase the weight sparsity but requires changes in the network structure. To achieve a high Winograd-domain weight sparsity without changing network structures, we propose a new pruning method, spatial-Winograd pruning. As the first step, spatial-domain weights are pruned in a structured way, which efficiently transfers the spatial-domain sparsity into the Winograd domain and avoids Winograd-domain retraining. For the next step, we also perform pruning and retraining directly in the Winograd domain but propose to use an importance factor matrix to adjust weight importance and weight gradients. This adjustment makes it possible to effectively retrain the pruned Winograd-domain network without changing the network structure. For the three models on the datasets of CIFAR10, CIFAR-100, and ImageNet, our proposed method can achieve the Winograd domain sparsities of 63%, 50%, and 74%, respectively.



### Ensembles of feedforward-designed convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1901.02154v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02154v1)
- **Published**: 2019-01-08 04:48:22+00:00
- **Updated**: 2019-01-08 04:48:22+00:00
- **Authors**: Yueru Chen, Yijing Yang, Wei Wang, C. -C. Jay Kuo
- **Comment**: None
- **Journal**: None
- **Summary**: An ensemble method that fuses the output decision vectors of multiple feedforward-designed convolutional neural networks (FF-CNNs) to solve the image classification problem is proposed in this work. To enhance the performance of the ensemble system, it is critical to increasing the diversity of FF-CNN models. To achieve this objective, we introduce diversities by adopting three strategies: 1) different parameter settings in convolutional layers, 2) flexible feature subsets fed into the Fully-connected (FC) layers, and 3) multiple image embeddings of the same input source. Furthermore, we partition input samples into easy and hard ones based on their decision confidence scores. As a result, we can develop a new ensemble system tailored to hard samples to further boost classification accuracy. Experiments are conducted on the MNIST and CIFAR-10 datasets to demonstrate the effectiveness of the ensemble method.



### Explaining AlphaGo: Interpreting Contextual Effects in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.02184v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02184v1)
- **Published**: 2019-01-08 07:23:05+00:00
- **Updated**: 2019-01-08 07:23:05+00:00
- **Authors**: Zenan Ling, Haotian Ma, Yu Yang, Robert C. Qiu, Song-Chun Zhu, Quanshi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose to disentangle and interpret contextual effects that are encoded in a pre-trained deep neural network. We use our method to explain the gaming strategy of the alphaGo Zero model. Unlike previous studies that visualized image appearances corresponding to the network output or a neural activation only from a global perspective, our research aims to clarify how a certain input unit (dimension) collaborates with other units (dimensions) to constitute inference patterns of the neural network and thus contribute to the network output. The analysis of local contextual effects w.r.t. certain input units is of special values in real applications. Explaining the logic of the alphaGo Zero model is a typical application. In experiments, our method successfully disentangled the rationale of each move during the Go game.



### FIGR: Few-shot Image Generation with Reptile
- **Arxiv ID**: http://arxiv.org/abs/1901.02199v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.02199v1)
- **Published**: 2019-01-08 08:15:08+00:00
- **Updated**: 2019-01-08 08:15:08+00:00
- **Authors**: Louis Clou√¢tre, Marc Demers
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GAN) boast impressive capacity to generate realistic images. However, like much of the field of deep learning, they require an inordinate amount of data to produce results, thereby limiting their usefulness in generating novelty. In the same vein, recent advances in meta-learning have opened the door to many few-shot learning applications. In the present work, we propose Few-shot Image Generation using Reptile (FIGR), a GAN meta-trained with Reptile. Our model successfully generates novel images on both MNIST and Omniglot with as little as 4 images from an unseen class. We further contribute FIGR-8, a new dataset for few-shot image generation, which contains 1,548,944 icons categorized in over 18,409 classes. Trained on FIGR-8, initial results show that our model can generalize to more advanced concepts (such as "bird" and "knife") from as few as 8 samples from a previously unseen class of images and as little as 10 training steps through those 8 images. This work demonstrates the potential of training a GAN for few-shot image generation and aims to set a new benchmark for future work in the domain.



### Translating SAR to Optical Images for Assisted Interpretation
- **Arxiv ID**: http://arxiv.org/abs/1901.03749v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.03749v1)
- **Published**: 2019-01-08 08:48:47+00:00
- **Updated**: 2019-01-08 08:48:47+00:00
- **Authors**: Shilei Fu, Feng Xu, Ya-Qiu Jin
- **Comment**: 4 pages, 5 figures, 2 tables, conference
- **Journal**: None
- **Summary**: Despite the advantages of all-weather and all-day high-resolution imaging, SAR remote sensing images are much less viewed and used by general people because human vision is not adapted to microwave scattering phenomenon. However, expert interpreters can be trained by compare side-by-side SAR and optical images to learn the translation rules from SAR to optical. This paper attempts to develop machine intelligence that are trainable with large-volume co-registered SAR and optical images to translate SAR image to optical version for assisted SAR interpretation. A novel reciprocal GAN scheme is proposed for this translation task. It is trained and tested on both spaceborne GF-3 and airborne UAVSAR images. Comparisons and analyses are presented for datasets of different resolutions and polarizations. Results show that the proposed translation network works well under many scenarios and it could potentially be used for assisted SAR interpretation.



### FakeCatcher: Detection of Synthetic Portrait Videos using Biological Signals
- **Arxiv ID**: http://arxiv.org/abs/1901.02212v3
- **DOI**: 10.1109/TPAMI.2020.3009287
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02212v3)
- **Published**: 2019-01-08 09:20:36+00:00
- **Updated**: 2020-07-19 03:02:54+00:00
- **Authors**: Umur Aybars Ciftci, Ilke Demir
- **Comment**: To appear in IEEE Transactions on Pattern Analysis and Machine
  Intelligence (PAMI), accepted July 2020. Dataset: http://bit.ly/FakeCatcher
- **Journal**: None
- **Summary**: The recent proliferation of fake portrait videos poses direct threats on society, law, and privacy. Believing the fake video of a politician, distributing fake pornographic content of celebrities, fabricating impersonated fake videos as evidence in courts are just a few real world consequences of deep fakes. We present a novel approach to detect synthetic content in portrait videos, as a preventive solution for the emerging threat of deep fakes. In other words, we introduce a deep fake detector. We observe that detectors blindly utilizing deep learning are not effective in catching fake content, as generative models produce formidably realistic results. Our key assertion follows that biological signals hidden in portrait videos can be used as an implicit descriptor of authenticity, because they are neither spatially nor temporally preserved in fake content. To prove and exploit this assertion, we first engage several signal transformations for the pairwise separation problem, achieving 99.39% accuracy. Second, we utilize those findings to formulate a generalized classifier for fake content, by analyzing proposed signal transformations and corresponding feature sets. Third, we generate novel signal maps and employ a CNN to improve our traditional classifier for detecting synthetic content. Lastly, we release an "in the wild" dataset of fake portrait videos that we collected as a part of our evaluation process. We evaluate FakeCatcher on several datasets, resulting with 96%, 94.65%, 91.50%, and 91.07% accuracies, on Face Forensics, Face Forensics++, CelebDF, and on our new Deep Fakes Dataset respectively. We also analyze signals from various facial regions, under image distortions, with varying segment durations, from different generators, against unseen datasets, and under several dimensionality reduction techniques.



### Interpretable BoW Networks for Adversarial Example Detection
- **Arxiv ID**: http://arxiv.org/abs/1901.02229v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02229v1)
- **Published**: 2019-01-08 10:04:33+00:00
- **Updated**: 2019-01-08 10:04:33+00:00
- **Authors**: Krishna Kanth Nakka, Mathieu Salzmann
- **Comment**: None
- **Journal**: None
- **Summary**: The standard approach to providing interpretability to deep convolutional neural networks (CNNs) consists of visualizing either their feature maps, or the image regions that contribute the most to the prediction. In this paper, we introduce an alternative strategy to interpret the results of a CNN. To this end, we leverage a Bag of visual Word representation within the network and associate a visual and semantic meaning to the corresponding codebook elements via the use of a generative adversarial network. The reason behind the prediction for a new sample can then be interpreted by looking at the visual representation of the most highly activated codeword. We then propose to exploit our interpretable BoW networks for adversarial example detection. To this end, we build upon the intuition that, while adversarial samples look very similar to real images, to produce incorrect predictions, they should activate codewords with a significantly different visual representation. We therefore cast the adversarial example detection problem as that of comparing the input image with the most highly activated visual codeword. As evidenced by our experiments, this allows us to outperform the state-of-the-art adversarial example detection methods on standard benchmarks, independently of the attack strategy.



### 3D Object Detection Using Scale Invariant and Feature Reweighting Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.02237v1
- **DOI**: 10.1609/aaai.v33i01.33019267
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02237v1)
- **Published**: 2019-01-08 10:31:38+00:00
- **Updated**: 2019-01-08 10:31:38+00:00
- **Authors**: Xin Zhao, Zhe Liu, Ruolan Hu, Kaiqi Huang
- **Comment**: The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19)
- **Journal**: None
- **Summary**: 3D object detection plays an important role in a large number of real-world applications. It requires us to estimate the localizations and the orientations of 3D objects in real scenes. In this paper, we present a new network architecture which focuses on utilizing the front view images and frustum point clouds to generate 3D detection results. On the one hand, a PointSIFT module is utilized to improve the performance of 3D segmentation. It can capture the information from different orientations in space and the robustness to different scale shapes. On the other hand, our network obtains the useful features and suppresses the features with less information by a SENet module. This module reweights channel features and estimates the 3D bounding boxes more effectively. Our method is evaluated on both KITTI dataset for outdoor scenes and SUN-RGBD dataset for indoor scenes. The experimental results illustrate that our method achieves better performance than the state-of-the-art methods especially when point clouds are highly sparse.



### Unpaired Pose Guided Human Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1901.02284v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02284v2)
- **Published**: 2019-01-08 12:39:21+00:00
- **Updated**: 2019-06-04 19:58:44+00:00
- **Authors**: Xu Chen, Jie Song, Otmar Hilliges
- **Comment**: camera ready for CVPR 2019 VUHCS workshop
- **Journal**: None
- **Summary**: This paper studies the task of full generative modelling of realistic images of humans, guided only by coarse sketch of the pose, while providing control over the specific instance or type of outfit worn by the user. This is a difficult problem because input and output domain are very different and direct image-to-image translation becomes infeasible. We propose an end-to-end trainable network under the generative adversarial framework, that provides detailed control over the final appearance while not requiring paired training data and hence allows us to forgo the challenging problem of fitting 3D poses to 2D images. The model allows to generate novel samples conditioned on either an image taken from the target domain or a class label indicating the style of clothing (e.g., t-shirt). We thoroughly evaluate the architecture and the contributions of the individual components experimentally. Finally, we show in a large scale perceptual study that our approach can generate realistic looking images and that participants struggle in detecting fake images versus real samples, especially if faces are blurred.



### Selective metamorphosis for growth modelling with applications to landmarks
- **Arxiv ID**: http://arxiv.org/abs/1901.02826v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1901.02826v2)
- **Published**: 2019-01-08 14:09:35+00:00
- **Updated**: 2019-05-22 14:47:30+00:00
- **Authors**: Andreas Bock, Alexis Arnaudon, Colin Cotter
- **Comment**: None
- **Journal**: None
- **Summary**: We present a framework for shape matching in computational anatomy allowing users control of the degree to which the matching is diffeomorphic. This control is given as a function defined over the image and parameterises the template deformation. By modelling localised template deformation we have a mathematical description of growth only in specified parts of an image. The location can either be specified from prior knowledge of the growth location or learned from data. For simplicity, we consider landmark matching and infer the distribution of a finite dimensional parameterisation of the control via Markov chain Monte Carlo. Preliminary numerical results are shown and future paths of investigation are laid out. Well-posedness of this new problem is studied together with an analysis of the associated geodesic equations.



### GILT: Generating Images from Long Text
- **Arxiv ID**: http://arxiv.org/abs/1901.02404v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.02404v1)
- **Published**: 2019-01-08 16:59:46+00:00
- **Updated**: 2019-01-08 16:59:46+00:00
- **Authors**: Ori Bar El, Ori Licht, Netanel Yosephian
- **Comment**: None
- **Journal**: None
- **Summary**: Creating an image reflecting the content of a long text is a complex process that requires a sense of creativity. For example, creating a book cover or a movie poster based on their summary or a food image based on its recipe. In this paper we present the new task of generating images from long text that does not describe the visual content of the image directly. For this, we build a system for generating high-resolution 256 $\times$ 256 images of food conditioned on their recipes. The relation between the recipe text (without its title) to the visual content of the image is vague, and the textual structure of recipes is complex, consisting of two sections (ingredients and instructions) both containing multiple sentences.   We used the recipe1M dataset to train and evaluate our model that is based on a the StackGAN-v2 architecture.



### Face Recognition System
- **Arxiv ID**: http://arxiv.org/abs/1901.02452v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02452v1)
- **Published**: 2019-01-08 17:07:25+00:00
- **Updated**: 2019-01-08 17:07:25+00:00
- **Authors**: Yang Li, Sangwhan Cha
- **Comment**: Deep neural network, face recognition, server-client model, business
  model, deep multi-model fusion, convolutional neural network. arXiv admin
  note: substantial text overlap with arXiv:1811.07339
- **Journal**: None
- **Summary**: Deep learning is one of the new and important branches in machine learning. Deep learning refers to a set of algorithms that solve various problems such as images and texts by using various machine learning algorithms in multi-layer neural networks. Deep learning can be classified as a neural network from the general category, but there are many changes in the concrete realization. At the core of deep learning is feature learning, which is designed to obtain hierarchical information through hierarchical networks, so as to solve the important problems that previously required artificial design features. Deep Learning is a framework that contains several important algorithms. For different applications (images, voice, text), you need to use different network models to achieve better results. With the development of deep learning and the introduction of deep convolutional neural networks, the accuracy and speed of face recognition have made great strides. However, as we said above, the results from different networks and models are very different. In this paper, facial features are extracted by merging and comparing multiple models, and then a deep neural network is constructed to train and construct the combined features. In this way, the advantages of multiple models can be combined to mention the recognition accuracy. After getting a model with high accuracy, we build a product model. This article compares the pure-client model with the server-client model, analyzes the pros and cons of the two models, and analyzes the various commercial products that are required for the server-client model.



### Neural Inverse Rendering of an Indoor Scene from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1901.02453v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02453v3)
- **Published**: 2019-01-08 17:12:42+00:00
- **Updated**: 2019-09-14 07:24:52+00:00
- **Authors**: Soumyadip Sengupta, Jinwei Gu, Kihwan Kim, Guilin Liu, David W. Jacobs, Jan Kautz
- **Comment**: None
- **Journal**: None
- **Summary**: Inverse rendering aims to estimate physical attributes of a scene, e.g., reflectance, geometry, and lighting, from image(s). Inverse rendering has been studied primarily for single objects or with methods that solve for only one of the scene attributes. We propose the first learning-based approach that jointly estimates albedo, normals, and lighting of an indoor scene from a single image. Our key contribution is the Residual Appearance Renderer (RAR), which can be trained to synthesize complex appearance effects (e.g., inter-reflection, cast shadows, near-field illumination, and realistic shading), which would be neglected otherwise. This enables us to perform self-supervised learning on real data using a reconstruction loss, based on re-synthesizing the input image from the estimated components. We finetune with real data after pretraining with synthetic data. To this end, we use physically-based rendering to create a large-scale synthetic dataset, which is a significant improvement over prior datasets. Experimental results show that our approach outperforms state-of-the-art methods that estimate one or more scene attributes.



### Morphological Networks for Image De-raining
- **Arxiv ID**: http://arxiv.org/abs/1901.02411v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02411v1)
- **Published**: 2019-01-08 17:13:47+00:00
- **Updated**: 2019-01-08 17:13:47+00:00
- **Authors**: Ranjan Mondal, Pulak Purkait, Sanchayan Santra, Bhabatosh Chanda
- **Comment**: Mathematical Morphology \and Optimization \and Morphological Network
  \and Image Filtering
- **Journal**: https://dgci2019.sciencesconf.org/
- **Summary**: Mathematical morphological methods have successfully been applied to filter out (emphasize or remove) different structures of an image. However, it is argued that these methods could be suitable for the task only if the type and order of the filter(s) as well as the shape and size of operator kernel are designed properly. Thus the existing filtering operators are problem (instance) specific and are designed by the domain experts. In this work we propose a morphological network that emulates classical morphological filtering consisting of a series of erosion and dilation operators with trainable structuring elements. We evaluate the proposed network for image de-raining task where the SSIM and mean absolute error (MAE) loss corresponding to predicted and ground-truth clean image is back-propagated through the network to train the structuring elements. We observe that a single morphological network can de-rain an image with any arbitrary shaped rain-droplets and achieves similar performance with the contemporary CNNs for this task with a fraction of trainable parameters (network size). The proposed morphological network(MorphoN) is not designed specifically for de-raining and can readily be applied to similar filtering / noise cleaning tasks. The source code can be found here https://github.com/ranjanZ/2D-Morphological-Network



### Interpretable CNNs for Object Classification
- **Arxiv ID**: http://arxiv.org/abs/1901.02413v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.02413v2)
- **Published**: 2019-01-08 17:15:19+00:00
- **Updated**: 2020-03-12 07:04:28+00:00
- **Authors**: Quanshi Zhang, Xin Wang, Ying Nian Wu, Huilin Zhou, Song-Chun Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a generic method to learn interpretable convolutional filters in a deep convolutional neural network (CNN) for object classification, where each interpretable filter encodes features of a specific object part. Our method does not require additional annotations of object parts or textures for supervision. Instead, we use the same training data as traditional CNNs. Our method automatically assigns each interpretable filter in a high conv-layer with an object part of a certain category during the learning process. Such explicit knowledge representations in conv-layers of CNN help people clarify the logic encoded in the CNN, i.e., answering what patterns the CNN extracts from an input image and uses for prediction. We have tested our method using different benchmark CNNs with various structures to demonstrate the broad applicability of our method. Experiments have shown that our interpretable filters are much more semantically meaningful than traditional filters.



### Richer and Deeper Supervision Network for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1901.02425v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02425v1)
- **Published**: 2019-01-08 18:02:05+00:00
- **Updated**: 2019-01-08 18:02:05+00:00
- **Authors**: Sen Jia, Neil D. B. Bruce
- **Comment**: None
- **Journal**: None
- **Summary**: Recent Salient Object Detection (SOD) systems are mostly based on Convolutional Neural Networks (CNNs). Specifically, Deeply Supervised Saliency (DSS) system has shown it is very useful to add short connections to the network and supervising on the side output. In this work, we propose a new SOD system which aims at designing a more efficient and effective way to pass back global information. Richer and Deeper Supervision (RDS) is applied to better combine features from each side output without demanding much extra computational space. Meanwhile, the backbone network used for SOD is normally pre-trained on the object classification dataset, ImageNet. But the pre-trained model has been trained on cropped images in order to only focus on distinguishing features within the region of the object. But the ignored background information is also significant in the task of SOD. We try to solve this problem by introducing the training data designed for object detection. A coarse global information is learned based on an entire image with its bounding box before training on the SOD dataset. The large-scale of object images can slightly improve the performance of SOD. Our experiment shows the proposed RDS network achieves the state-of-the-art results on five public SOD datasets.



### Learning with Collaborative Neural Network Group by Reflection
- **Arxiv ID**: http://arxiv.org/abs/1901.02433v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1901.02433v2)
- **Published**: 2019-01-08 18:20:44+00:00
- **Updated**: 2019-01-20 13:29:20+00:00
- **Authors**: Liyao Gao, Zehua Cheng
- **Comment**: 6 Pages. Ubicomp Workshop 2018
- **Journal**: None
- **Summary**: For the present engineering of neural systems, the preparing of extensive scale learning undertakings generally not just requires a huge neural system with a mind boggling preparing process yet additionally troublesome discover a clarification for genuine applications. In this paper, we might want to present the Collaborative Neural Network Group (CNNG). CNNG is a progression of neural systems that work cooperatively to deal with various errands independently in a similar learning framework. It is advanced from a solitary neural system by reflection. Along these lines, in light of various circumstances removed by the calculation, the CNNG can perform diverse techniques when handling the information. The examples of chose methodology can be seen by human to make profound adapting more reasonable. In our execution, the CNNG is joined by a few moderately little neural systems. We give a progression of examinations to assess the execution of CNNG contrasted with other learning strategies. The CNNG is able to get a higher accuracy with a much lower training cost. We can reduce the error rate by 74.5% and reached the accuracy of 99.45% in MNIST with three feedforward networks (4 layers) in one training epoch.



### Stable Electromyographic Sequence Prediction During Movement Transitions using Temporal Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.02442v1
- **DOI**: 10.1109/NER.2019.8717169
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02442v1)
- **Published**: 2019-01-08 18:48:50+00:00
- **Updated**: 2019-01-08 18:48:50+00:00
- **Authors**: Joseph L. Betthauser, John T. Krall, Rahul R. Kaliki, Matthew S. Fifer, Nitish V. Thakor
- **Comment**: 4 pages, 5 figures, accepted for Neural Engineering (NER) 2019
  Conference
- **Journal**: None
- **Summary**: Transient muscle movements influence the temporal structure of myoelectric signal patterns, often leading to unstable prediction behavior from movement-pattern classification methods. We show that temporal convolutional network sequential models leverage the myoelectric signal's history to discover contextual temporal features that aid in correctly predicting movement intentions, especially during interclass transitions. We demonstrate myoelectric classification using temporal convolutional networks to effect 3 simultaneous hand and wrist degrees-of-freedom in an experiment involving nine human-subjects. Temporal convolutional networks yield significant $(p<0.001)$ performance improvements over other state-of-the-art methods in terms of both classification accuracy and stability.



### Unseen Object Segmentation in Videos via Transferable Representations
- **Arxiv ID**: http://arxiv.org/abs/1901.02444v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02444v1)
- **Published**: 2019-01-08 18:49:51+00:00
- **Updated**: 2019-01-08 18:49:51+00:00
- **Authors**: Yi-Wen Chen, Yi-Hsuan Tsai, Chu-Ya Yang, Yen-Yu Lin, Ming-Hsuan Yang
- **Comment**: Accepted in ACCV'18 (oral). Code is available at
  https://github.com/wenz116/TransferSeg
- **Journal**: None
- **Summary**: In order to learn object segmentation models in videos, conventional methods require a large amount of pixel-wise ground truth annotations. However, collecting such supervised data is time-consuming and labor-intensive. In this paper, we exploit existing annotations in source images and transfer such visual information to segment videos with unseen object categories. Without using any annotations in the target video, we propose a method to jointly mine useful segments and learn feature representations that better adapt to the target frames. The entire process is decomposed into two tasks: 1) solving a submodular function for selecting object-like segments, and 2) learning a CNN model with a transferable module for adapting seen categories in the source domain to the unseen target video. We present an iterative update scheme between two tasks to self-learn the final solution for object segmentation. Experimental results on numerous benchmark datasets show that the proposed method performs favorably against the state-of-the-art algorithms.



### Panoptic Feature Pyramid Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.02446v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02446v2)
- **Published**: 2019-01-08 18:55:31+00:00
- **Updated**: 2019-04-10 18:09:43+00:00
- **Authors**: Alexander Kirillov, Ross Girshick, Kaiming He, Piotr Doll√°r
- **Comment**: accepted to CVPR 2019
- **Journal**: None
- **Summary**: The recently introduced panoptic segmentation task has renewed our community's interest in unifying the tasks of instance segmentation (for thing classes) and semantic segmentation (for stuff classes). However, current state-of-the-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation, without performing any shared computation. In this work, we aim to unify these methods at the architectural level, designing a single network for both tasks. Our approach is to endow Mask R-CNN, a popular instance segmentation method, with a semantic segmentation branch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for instance segmentation, but also yields a lightweight, top-performing method for semantic segmentation. In this work, we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and accurate baseline for both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and aid future research in panoptic segmentation.



### Grey matter sublayer thickness estimation in themouse cerebellum
- **Arxiv ID**: http://arxiv.org/abs/1901.02499v1
- **DOI**: 10.1007/978-3-319-24574-4_77
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02499v1)
- **Published**: 2019-01-08 20:23:24+00:00
- **Updated**: 2019-01-08 20:23:24+00:00
- **Authors**: Da Ma, Manuel J. Cardoso, Maria A. Zuluaga, Marc Modat, Nick. Powell, Frances Wiseman, Victor Tybulewicz, Elizabeth Fisher, Mark. F. Lythgoe, Sebastien Ourselin
- **Comment**: 8 pages, 7 figures, International Conference on Medical Image
  Computing and Computer-Assisted Intervention 2015
- **Journal**: International Conference on Medical Image Computing and
  Computer-Assisted Intervention. Springer, Cham, 2015
- **Summary**: The cerebellar grey matter morphology is an important feature to study neurodegenerative diseases such as Alzheimer's disease or Down's syndrome. Its volume or thickness is commonly used as a surrogate imaging biomarker for such diseases. Most studies about grey matter thickness estimation focused on the cortex, and little attention has been drawn on the morphology of the cerebellum. Using ex vivo high-resolution MRI, it is now possible to visualise the different cell layers in the mouse cerebellum. In this work, we introduce a framework to extract the Purkinje layer within the grey matter, enabling the estimation of the thickness of the cerebellar grey matter, the granular layer and molecular layer from gadolinium-enhanced ex vivo mouse brain MRI. Application to mouse model of Down's syndrome found reduced cortical and layer thicknesses in the transchromosomic group.



### An Application of Manifold Learning in Global Shape Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1901.02508v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CG, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1901.02508v1)
- **Published**: 2019-01-08 20:41:49+00:00
- **Updated**: 2019-01-08 20:41:49+00:00
- **Authors**: Fereshteh S. Bashiri, Reihaneh Rostami, Peggy Peissig, Roshan M. D'Souza, Zeyun Yu
- **Comment**: None
- **Journal**: None
- **Summary**: With the rapid expansion of applied 3D computational vision, shape descriptors have become increasingly important for a wide variety of applications and objects from molecules to planets. Appropriate shape descriptors are critical for accurate (and efficient) shape retrieval and 3D model classification. Several spectral-based shape descriptors have been introduced by solving various physical equations over a 3D surface model. In this paper, for the first time, we incorporate a specific group of techniques in statistics and machine learning, known as manifold learning, to develop a global shape descriptor in the computer graphics domain. The proposed descriptor utilizes the Laplacian Eigenmap technique in which the Laplacian eigenvalue problem is discretized using an exponential weighting scheme. As a result, our descriptor eliminates the limitations tied to the existing spectral descriptors, namely dependency on triangular mesh representation and high intra-class quality of 3D models. We also present a straightforward normalization method to obtain a scale-invariant descriptor. The extensive experiments performed in this study show that the present contribution provides a highly discriminative and robust shape descriptor under the presence of a high level of noise, random scale variations, and low sampling rate, in addition to the known isometric-invariance property of the Laplace-Beltrami operator. The proposed method significantly outperforms state-of-the-art algorithms on several non-rigid shape retrieval benchmarks.



### Multi-stream CNN based Video Semantic Segmentation for Automated Driving
- **Arxiv ID**: http://arxiv.org/abs/1901.02511v1
- **DOI**: 10.5220/0007248401730180
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.02511v1)
- **Published**: 2019-01-08 20:45:49+00:00
- **Updated**: 2019-01-08 20:45:49+00:00
- **Authors**: Ganesh Sistu, Sumanth Chennupati, Senthil Yogamani
- **Comment**: Accepted for Oral Presentation at VISAPP 2019
- **Journal**: None
- **Summary**: Majority of semantic segmentation algorithms operate on a single frame even in the case of videos. In this work, the goal is to exploit temporal information within the algorithm model for leveraging motion cues and temporal consistency. We propose two simple high-level architectures based on Recurrent FCN (RFCN) and Multi-Stream FCN (MSFCN) networks. In case of RFCN, a recurrent network namely LSTM is inserted between the encoder and decoder. MSFCN combines the encoders of different frames into a fused encoder via 1x1 channel-wise convolution. We use a ResNet50 network as the baseline encoder and construct three networks namely MSFCN of order 2 & 3 and RFCN of order 2. MSFCN-3 produces the best results with an accuracy improvement of 9% and 15% for Highway and New York-like city scenarios in the SYNTHIA-CVPR'16 dataset using mean IoU metric. MSFCN-3 also produced 11% and 6% for SegTrack V2 and DAVIS datasets over the baseline FCN network. We also designed an efficient version of MSFCN-2 and RFCN-2 using weight sharing among the two encoders. The efficient MSFCN-2 provided an improvement of 11% and 5% for KITTI and SYNTHIA with negligible increase in computational complexity compared to the baseline version.



### Combining nonparametric spatial context priors with nonparametric shape priors for dendritic spine segmentation in 2-photon microscopy images
- **Arxiv ID**: http://arxiv.org/abs/1901.02513v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.02513v2)
- **Published**: 2019-01-08 20:47:46+00:00
- **Updated**: 2019-02-17 12:21:03+00:00
- **Authors**: Ertunc Erdil, Ali Ozgur Argunsah, Tolga Tasdizen, Devrim Unay, Mujdat Cetin
- **Comment**: IEEE International Symposium on Biomedical Imaging
- **Journal**: None
- **Summary**: Data driven segmentation is an important initial step of shape prior-based segmentation methods since it is assumed that the data term brings a curve to a plausible level so that shape and data terms can then work together to produce better segmentations. When purely data driven segmentation produces poor results, the final segmentation is generally affected adversely. One challenge faced by many existing data terms is due to the fact that they consider only pixel intensities to decide whether to assign a pixel to the foreground or to the background region. When the distributions of the foreground and background pixel intensities have significant overlap, such data terms become ineffective, as they produce uncertain results for many pixels in a test image. In such cases, using prior information about the spatial context of the object to be segmented together with the data term can bring a curve to a plausible stage, which would then serve as a good initial point to launch shape-based segmentation. In this paper, we propose a new segmentation approach that combines nonparametric context priors with a learned-intensity-based data term and nonparametric shape priors. We perform experiments for dendritic spine segmentation in both 2D and 3D 2-photon microscopy images. The experimental results demonstrate that using spatial context priors leads to significant improvements.



### Robust Change Captioning
- **Arxiv ID**: http://arxiv.org/abs/1901.02527v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1901.02527v2)
- **Published**: 2019-01-08 21:29:42+00:00
- **Updated**: 2019-04-17 00:18:00+00:00
- **Authors**: Dong Huk Park, Trevor Darrell, Anna Rohrbach
- **Comment**: None
- **Journal**: None
- **Summary**: Describing what has changed in a scene can be useful to a user, but only if generated text focuses on what is semantically relevant. It is thus important to distinguish distractors (e.g. a viewpoint change) from relevant changes (e.g. an object has moved). We present a novel Dual Dynamic Attention Model (DUDA) to perform robust Change Captioning. Our model learns to distinguish distractors from semantic changes, localize the changes via Dual Attention over "before" and "after" images, and accurately describe them in natural language via Dynamic Speaker, by adaptively focusing on the necessary visual inputs (e.g. "before" or "after" image). To study the problem in depth, we collect a CLEVR-Change dataset, built off the CLEVR engine, with 5 types of scene changes. We benchmark a number of baselines on our dataset, and systematically study different change types and robustness to distractors. We show the superiority of our DUDA model in terms of both change captioning and localization. We also show that our approach is general, obtaining state-of-the-art results on the recent realistic Spot-the-Diff dataset which has no distractors.



### A Spatial-temporal 3D Human Pose Reconstruction Framework
- **Arxiv ID**: http://arxiv.org/abs/1901.02529v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1901.02529v2)
- **Published**: 2019-01-08 21:46:52+00:00
- **Updated**: 2019-01-10 09:20:23+00:00
- **Authors**: X. T. Nguyen, T. D. Ngo, T. H. Le
- **Comment**: 10 pages. JIPS Journal 2018
- **Journal**: None
- **Summary**: 3D human pose reconstruction from single-view camera is a difficult and challenging topic. Many approaches have been proposed, but almost focusing on frame-by-frame independently while inter-frames are highly correlated in a pose sequence. In contrast, we introduce a novel spatial-temporal 3D reconstruction framework that leverages both intra and inter frame relationships in consecutive 2D pose sequences. Orthogonal Matching Pursuit (OMP) algorithm, pre-trained Pose-angle Limits and Temporal Models have been implemented. We quantitatively compare our framework versus recent works on CMU motion capture dataset and Vietnamese traditional dance sequences. Our method outperforms others with 10 percent lower of Euclidean reconstruction error and robustness against Gaussian noise. Additionally, it is also important to mention that our reconstructed 3D pose sequences are smoother and more natural than others.



### Fast 3D Line Segment Detection From Unorganized Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/1901.02532v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02532v1)
- **Published**: 2019-01-08 21:51:51+00:00
- **Updated**: 2019-01-08 21:51:51+00:00
- **Authors**: Xiaohu Lu, Yahui Liu, Kai Li
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: This paper presents a very simple but efficient algorithm for 3D line segment detection from large scale unorganized point cloud. Unlike traditional methods which usually extract 3D edge points first and then link them to fit for 3D line segments, we propose a very simple 3D line segment detection algorithm based on point cloud segmentation and 2D line detection. Given the input unorganized point cloud, three steps are performed to detect 3D line segments. Firstly, the point cloud is segmented into 3D planes via region growing and region merging. Secondly, for each 3D plane, all the points belonging to it are projected onto the plane itself to form a 2D image, which is followed by 2D contour extraction and Least Square Fitting to get the 2D line segments. Those 2D line segments are then re-projected onto the 3D plane to get the corresponding 3D line segments. Finally, a post-processing procedure is proposed to eliminate outliers and merge adjacent 3D line segments. Experiments on several public datasets demonstrate the efficiency and robustness of our method. More results and the C++ source code of the proposed algorithm are publicly available at https://github.com/xiaohulugo/3DLineDetection.



### Collaborative Execution of Deep Neural Networks on Internet of Things Devices
- **Arxiv ID**: http://arxiv.org/abs/1901.02537v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02537v1)
- **Published**: 2019-01-08 22:05:16+00:00
- **Updated**: 2019-01-08 22:05:16+00:00
- **Authors**: Ramyad Hadidi, Jiashen Cao, Micheal S. Ryoo, Hyesoon Kim
- **Comment**: Updated version after sysML
- **Journal**: None
- **Summary**: With recent advancements in deep neural networks (DNNs), we are able to solve traditionally challenging problems. Since DNNs are compute intensive, consumers, to deploy a service, need to rely on expensive and scarce compute resources in the cloud. This approach, in addition to its dependability on high-quality network infrastructure and data centers, raises new privacy concerns. These challenges may limit DNN-based applications, so many researchers have tried optimize DNNs for local and in-edge execution. However, inadequate power and computing resources of edge devices along with small number of requests limits current optimizations applicability, such as batch processing. In this paper, we propose an approach that utilizes aggregated existing computing power of Internet of Things (IoT) devices surrounding an environment by creating a collaborative network. In this approach, IoT devices cooperate to conduct single-batch inferencing in real time. While exploiting several new model-parallelism methods and their distribution characteristics, our approach enhances the collaborative network by creating a balanced and distributed processing pipeline. We have illustrated our work using many Raspberry Pis with studying DNN models such as AlexNet, VGG16, Xception, and C3D.



### Asteroids Detection Technique: Classic "Blink" An Automated Approch
- **Arxiv ID**: http://arxiv.org/abs/1901.02542v1
- **DOI**: 10.1109/AQTR.2018.8402768
- **Categories**: **astro-ph.IM**, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1901.02542v1)
- **Published**: 2019-01-08 22:34:18+00:00
- **Updated**: 2019-01-08 22:34:18+00:00
- **Authors**: D. Copandean, C. Nandra, D. Gorgan, O. Vaduvescu
- **Comment**: Conference: 2018 IEEE International Conference on Automation, Quality
  and Testing, Robotics (AQTR), 24-26 May 2018, Cluj-Napoca, Romania
- **Journal**: IEEE, 2018
- **Summary**: Asteroids detection is a very important research field that received increased attention in the last couple of decades. Some major surveys have their own dedicated people, equipment and detection applications, so they are discovering Near Earth Asteroids (NEAs) daily. The interest in asteroids is not limited to those major surveys, it is shared by amateurs and mini-surveys too. A couple of them are using the few existent software solutions, most of which are developed by amateurs. The rest obtain their results in a visual manner: they "blink" a sequence of reduced images of the same field, taken at a specific time interval, and they try to detect a real moving object in the resulting animation. Such a technique becomes harder with the increase in size of the CCD cameras. Aiming to replace manual detection, we propose an automated "blink" technique for asteroids detection.



### NEARBY Platform: Algorithm for Automated Asteroids Detection in Astronomical Images
- **Arxiv ID**: http://arxiv.org/abs/1901.02545v1
- **DOI**: 10.1109/ICCP.2018.8516594
- **Categories**: **astro-ph.IM**, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1901.02545v1)
- **Published**: 2019-01-08 22:45:28+00:00
- **Updated**: 2019-01-08 22:45:28+00:00
- **Authors**: T. Stefanut, V. Bacu, C. Nandra, D. Balasz, D. Gorgan, O. Vaduvescu
- **Comment**: IEEE 14th International Conference on Intelligent Computer
  Communication and Processing (ICCP), Sep 6-8, 2018, Cluj-Napoca, Romania
- **Journal**: IEEE, 2018
- **Summary**: In the past two decades an increasing interest in discovering Near Earth Objects has been noted in the astronomical community. Dedicated surveys have been operated for data acquisition and processing, resulting in the present discovery of over 18.000 objects that are closer than 30 million miles of Earth. Nevertheless, recent events have shown that there still are many undiscovered asteroids that can be on collision course to Earth. This article presents an original NEO detection algorithm developed in the NEARBY research object, that has been integrated into an automated MOPS processing pipeline aimed at identifying moving space objects based on the blink method. Proposed solution can be considered an approach of Big Data processing and analysis, implementing visual analytics techniques for rapid human data validation.



### Thinking Outside the Pool: Active Training Image Creation for Relative Attributes
- **Arxiv ID**: http://arxiv.org/abs/1901.02551v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02551v1)
- **Published**: 2019-01-08 23:10:03+00:00
- **Updated**: 2019-01-08 23:10:03+00:00
- **Authors**: Aron Yu, Kristen Grauman
- **Comment**: None
- **Journal**: None
- **Summary**: Current wisdom suggests more labeled image data is always better, and obtaining labels is the bottleneck. Yet curating a pool of sufficiently diverse and informative images is itself a challenge. In particular, training image curation is problematic for fine-grained attributes, where the subtle visual differences of interest may be rare within traditional image sources. We propose an active image generation approach to address this issue. The main idea is to jointly learn the attribute ranking task while also learning to generate novel realistic image samples that will benefit that task. We introduce an end-to-end framework that dynamically "imagines" image pairs that would confuse the current model, presents them to human annotators for labeling, then improves the predictive model with the new examples. With results on two datasets, we show that by thinking outside the pool of real images, our approach gains generalization accuracy for challenging fine-grained attribute comparisons.



