# Arxiv Papers in cs.CV on 2019-01-04
### Contrastive Adaptation Network for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1901.00976v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00976v2)
- **Published**: 2019-01-04 03:58:10+00:00
- **Updated**: 2019-04-10 21:19:04+00:00
- **Authors**: Guoliang Kang, Lu Jiang, Yi Yang, Alexander G Hauptmann
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: Unsupervised Domain Adaptation (UDA) makes predictions for the target domain data while manual annotations are only available in the source domain. Previous methods minimize the domain discrepancy neglecting the class information, which may lead to misalignment and poor generalization performance. To address this issue, this paper proposes Contrastive Adaptation Network (CAN) optimizing a new metric which explicitly models the intra-class domain discrepancy and the inter-class domain discrepancy. We design an alternating update strategy for training CAN in an end-to-end manner. Experiments on two real-world benchmarks Office-31 and VisDA-2017 demonstrate that CAN performs favorably against the state-of-the-art methods and produces more discriminative features.



### Unsupervised Learning of Depth and Ego-Motion from Cylindrical Panoramic Video
- **Arxiv ID**: http://arxiv.org/abs/1901.00979v2
- **DOI**: 10.1109/aivr46125.2019.00018
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1901.00979v2)
- **Published**: 2019-01-04 04:09:28+00:00
- **Updated**: 2019-10-29 18:18:07+00:00
- **Authors**: Alisha Sharma, Jonathan Ventura
- **Comment**: Accepted to IEEE AIVR 2019
- **Journal**: None
- **Summary**: We introduce a convolutional neural network model for unsupervised learning of depth and ego-motion from cylindrical panoramic video. Panoramic depth estimation is an important technology for applications such as virtual reality, 3D modeling, and autonomous robotic navigation. In contrast to previous approaches for applying convolutional neural networks to panoramic imagery, we use the cylindrical panoramic projection which allows for the use of the traditional CNN layers such as convolutional filters and max pooling without modification. Our evaluation of synthetic and real data shows that unsupervised learning of depth and ego-motion on cylindrical panoramic images can produce high-quality depth maps and that an increased field-of-view improves ego-motion estimation accuracy. We also introduce Headcam, a novel dataset of panoramic video collected from a helmet-mounted camera while biking in an urban setting.



### Vehicle Re-Identification: an Efficient Baseline Using Triplet Embedding
- **Arxiv ID**: http://arxiv.org/abs/1901.01015v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01015v4)
- **Published**: 2019-01-04 08:13:54+00:00
- **Updated**: 2019-08-08 20:03:45+00:00
- **Authors**: Ratnesh Kumar, Edwin Weill, Farzin Aghdasi, Parthsarathy Sriram
- **Comment**: Accepted at IJCNN 2019. This arxiv version adds result on newer
  datasets post conference submission
- **Journal**: None
- **Summary**: In this paper we tackle the problem of vehicle re-identification in a camera network utilizing triplet embeddings. Re-identification is the problem of matching appearances of objects across different cameras. With the proliferation of surveillance cameras enabling smart and safer cities, there is an ever-increasing need to re-identify vehicles across cameras. Typical challenges arising in smart city scenarios include variations of viewpoints, illumination and self occlusions. Most successful approaches for re-identification involve (deep) learning an embedding space such that the vehicles of same identities are projected closer to one another, compared to the vehicles representing different identities. Popular loss functions for learning an embedding (space) include contrastive or triplet loss. In this paper we provide an extensive evaluation of these losses applied to vehicle re-identification and demonstrate that using the best practices for learning embeddings outperform most of the previous approaches proposed in the vehicle re-identification literature. Compared to most existing state-of-the-art approaches, our approach is simpler and more straightforward for training utilizing only identity-level annotations, along with one of the smallest published embedding dimensions for efficient inference. Furthermore in this work we introduce a formal evaluation of a triplet sampling variant (batch sample) into the re-identification literature.



### Transformed $\ell_1$ Regularization for Learning Sparse Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.01021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01021v1)
- **Published**: 2019-01-04 08:46:16+00:00
- **Updated**: 2019-01-04 08:46:16+00:00
- **Authors**: Rongrong Ma, Jianyu Miao, Lingfeng Niu, Peng Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have achieved extraordinary success in numerous areas. However, to attain this success, DNNs often carry a large number of weight parameters, leading to heavy costs of memory and computation resources. Overfitting is also likely to happen in such network when the training data are insufficient. These shortcomings severely hinder the application of DNNs in resource-constrained platforms. In fact, many network weights are known to be redundant and can be removed from the network without much loss of performance. To this end, we introduce a new non-convex integrated transformed $\ell_1$ regularizer to promote sparsity for DNNs, which removes both redundant connections and unnecessary neurons simultaneously. To be specific, we apply the transformed $\ell_1$ to the matrix space of network weights and utilize it to remove redundant connections. Besides, group sparsity is also employed as an auxiliary to remove unnecessary neurons. An efficient stochastic proximal gradient algorithm is presented to solve the new model at the same time. To the best of our knowledge, this is the first work to utilize a non-convex regularizer in sparse optimization based method to promote sparsity for DNNs. Experiments on several public datasets demonstrate the effectiveness of the proposed method.



### Iris Recognition with Image Segmentation Employing Retrained Off-the-Shelf Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.01028v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01028v1)
- **Published**: 2019-01-04 09:20:13+00:00
- **Updated**: 2019-01-04 09:20:13+00:00
- **Authors**: Daniel Kerrigan, Mateusz Trokielewicz, Adam Czajka, Kevin Bowyer
- **Comment**: Paper submitted for the IEEE International Conference on Biometrics
  (ICB2019)
- **Journal**: None
- **Summary**: This paper offers three new, open-source, deep learning-based iris segmentation methods, and the methodology how to use irregular segmentation masks in a conventional Gabor-wavelet-based iris recognition. To train and validate the methods, we used a wide spectrum of iris images acquired by different teams and different sensors and offered publicly, including data taken from CASIA-Iris-Interval-v4, BioSec, ND-Iris-0405, UBIRIS, Warsaw-BioBase-Post-Mortem-Iris v2.0 (post-mortem iris images), and ND-TWINS-2009-2010 (iris images acquired from identical twins). This varied training data should increase the generalization capabilities of the proposed segmentation techniques. In database-disjoint training and testing, we show that deep learning-based segmentation outperforms the conventional (OSIRIS) segmentation in terms of Intersection over Union calculated between the obtained results and manually annotated ground-truth. Interestingly, the Gabor-based iris matching is not always better when deep learning-based segmentation is used, and is on par with the method employing Daugman's based segmentation.



### Instance Segmentation of Fibers from Low Resolution CT Scans via 3D Deep Embedding Learning
- **Arxiv ID**: http://arxiv.org/abs/1901.01034v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01034v1)
- **Published**: 2019-01-04 09:53:07+00:00
- **Updated**: 2019-01-04 09:53:07+00:00
- **Authors**: Tomasz Konopczyński, Thorben Kröger, Lei Zheng, Jürgen Hesser
- **Comment**: Accepted to BMVC 2018
- **Journal**: None
- **Summary**: We propose a novel approach for automatic extraction (instance segmentation) of fibers from low resolution 3D X-ray computed tomography scans of short glass fiber reinforced polymers. We have designed a 3D instance segmentation architecture built upon a deep fully convolutional network for semantic segmentation with an extra output for embedding learning. We show that the embedding learning is capable of learning a mapping of voxels to an embedded space in which a standard clustering algorithm can be used to distinguish between different instances of an object in a volume. In addition, we discuss a merging post-processing method which makes it possible to process volumes of any size. The proposed 3D instance segmentation network together with our merging algorithm is the first known to authors knowledge procedure that produces results good enough, that they can be used for further analysis of low resolution fiber composites CT scans.



### Relative Geometry-Aware Siamese Neural Network for 6DOF Camera Relocalization
- **Arxiv ID**: http://arxiv.org/abs/1901.01049v2
- **DOI**: 10.1016/j.neucom.2020.09.071
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01049v2)
- **Published**: 2019-01-04 10:54:55+00:00
- **Updated**: 2019-01-21 02:37:04+00:00
- **Authors**: Qing Li, Jiasong Zhu, Rui Cao, Ke Sun, Jonathan M. Garibaldi, Qingquan Li, Bozhi Liu, Guoping Qiu
- **Comment**: None
- **Journal**: Neurocomputing 2020
- **Summary**: 6DOF camera relocalization is an important component of autonomous driving and navigation. Deep learning has recently emerged as a promising technique to tackle this problem. In this paper, we present a novel relative geometry-aware Siamese neural network to enhance the performance of deep learning-based methods through explicitly exploiting the relative geometry constraints between images. We perform multi-task learning and predict the absolute and relative poses simultaneously. We regularize the shared-weight twin networks in both the pose and feature domains to ensure that the estimated poses are globally as well as locally correct. We employ metric learning and design a novel adaptive metric distance loss to learn a feature that is capable of distinguishing poses of visually similar images from different locations. We evaluate the proposed method on public indoor and outdoor benchmarks and the experimental results demonstrate that our method can significantly improve localization performance. Furthermore, extensive ablation evaluations are conducted to demonstrate the effectiveness of different terms of the loss function.



### PointCleanNet: Learning to Denoise and Remove Outliers from Dense Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1901.01060v3
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1901.01060v3)
- **Published**: 2019-01-04 11:28:26+00:00
- **Updated**: 2019-06-28 15:22:52+00:00
- **Authors**: Marie-Julie Rakotosaona, Vittorio La Barbera, Paul Guerrero, Niloy J. Mitra, Maks Ovsjanikov
- **Comment**: None
- **Journal**: Computer Graphics Forum, 2020
- **Summary**: Point clouds obtained with 3D scanners or by image-based reconstruction techniques are often corrupted with significant amount of noise and outliers. Traditional methods for point cloud denoising largely rely on local surface fitting (e.g., jets or MLS surfaces), local or non-local averaging, or on statistical assumptions about the underlying noise model. In contrast, we develop a simple data-driven method for removing outliers and reducing noise in unordered point clouds. We base our approach on a deep learning architecture adapted from PCPNet, which was recently proposed for estimating local 3D shape properties in point clouds. Our method first classifies and discards outlier samples, and then estimates correction vectors that project noisy points onto the original clean surfaces. The approach is efficient and robust to varying amounts of noise and outliers, while being able to handle large densely-sampled point clouds. In our extensive evaluation, both on synthesic and real data, we show an increased robustness to strong noise levels compared to various state-of-the-art methods, enabling accurate surface reconstruction from extremely noisy real data obtained by range scans. Finally, the simplicity and universality of our approach makes it very easy to integrate in any existing geometry processing pipeline.



### Generic Primitive Detection in Point Clouds Using Novel Minimal Quadric Fits
- **Arxiv ID**: http://arxiv.org/abs/1901.01255v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.GR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1901.01255v1)
- **Published**: 2019-01-04 12:09:50+00:00
- **Updated**: 2019-01-04 12:09:50+00:00
- **Authors**: Tolga Birdal, Benjamin Busam, Nassir Navab, Slobodan Ilic, Peter Sturm
- **Comment**: Submitted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence (T-PAMI). arXiv admin note: substantial text overlap with
  arXiv:1803.07191
- **Journal**: None
- **Summary**: We present a novel and effective method for detecting 3D primitives in cluttered, unorganized point clouds, without axillary segmentation or type specification. We consider the quadric surfaces for encapsulating the basic building blocks of our environments - planes, spheres, ellipsoids, cones or cylinders, in a unified fashion. Moreover, quadrics allow us to model higher degree of freedom shapes, such as hyperboloids or paraboloids that could be used in non-rigid settings.   We begin by contributing two novel quadric fits targeting 3D point sets that are endowed with tangent space information. Based upon the idea of aligning the quadric gradients with the surface normals, our first formulation is exact and requires as low as four oriented points. The second fit approximates the first, and reduces the computational effort. We theoretically analyze these fits with rigor, and give algebraic and geometric arguments. Next, by re-parameterizing the solution, we devise a new local Hough voting scheme on the null-space coefficients that is combined with RANSAC, reducing the complexity from $O(N^4)$ to $O(N^3)$ (three points). To the best of our knowledge, this is the first method capable of performing a generic cross-type multi-object primitive detection in difficult scenes without segmentation. Our extensive qualitative and quantitative results show that our method is efficient and flexible, as well as being accurate.



### Low-Shot Learning from Imaginary 3D Model
- **Arxiv ID**: http://arxiv.org/abs/1901.01868v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.01868v1)
- **Published**: 2019-01-04 12:19:58+00:00
- **Updated**: 2019-01-04 12:19:58+00:00
- **Authors**: Frederik Pahde, Mihai Puscas, Jannik Wolff, Tassilo Klein, Nicu Sebe, Moin Nabi
- **Comment**: To appear at WACV 2019. arXiv admin note: text overlap with
  arXiv:1811.09192
- **Journal**: None
- **Summary**: Since the advent of deep learning, neural networks have demonstrated remarkable results in many visual recognition tasks, constantly pushing the limits. However, the state-of-the-art approaches are largely unsuitable in scarce data regimes. To address this shortcoming, this paper proposes employing a 3D model, which is derived from training images. Such a model can then be used to hallucinate novel viewpoints and poses for the scarce samples of the few-shot learning scenario. A self-paced learning approach allows for the selection of a diverse set of high-quality images, which facilitates the training of a classifier. The performance of the proposed approach is showcased on the fine-grained CUB-200-2011 dataset in a few-shot setting and significantly improves our baseline accuracy.



### Adaptive Density Estimation for Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1901.01091v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.01091v3)
- **Published**: 2019-01-04 13:43:18+00:00
- **Updated**: 2020-01-03 15:03:37+00:00
- **Authors**: Thomas Lucas, Konstantin Shmelkov, Karteek Alahari, Cordelia Schmid, Jakob Verbeek
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised learning of generative models has seen tremendous progress over recent years, in particular due to generative adversarial networks (GANs), variational autoencoders, and flow-based models. GANs have dramatically improved sample quality, but suffer from two drawbacks: (i) they mode-drop, i.e., do not cover the full support of the train data, and (ii) they do not allow for likelihood evaluations on held-out data. In contrast, likelihood-based training encourages models to cover the full support of the train data, but yields poorer samples. These mutual shortcomings can in principle be addressed by training generative latent variable models in a hybrid adversarial-likelihood manner. However, we show that commonly made parametric assumptions create a conflict between them, making successful hybrid models non trivial. As a solution, we propose to use deep invertible transformations in the latent variable decoder. This approach allows for likelihood computations in image space, is more efficient than fully invertible models, and can take full advantage of adversarial training. We show that our model significantly improves over existing hybrid models: offering GAN-like samples, IS and FID scores that are competitive with fully adversarial models, and improved likelihood scores.



### Intelligent Intersection: Two-Stream Convolutional Networks for Real-time Near Accident Detection in Traffic Video
- **Arxiv ID**: http://arxiv.org/abs/1901.01138v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01138v1)
- **Published**: 2019-01-04 15:05:32+00:00
- **Updated**: 2019-01-04 15:05:32+00:00
- **Authors**: Xiaohui Huang, Pan He, Anand Rangarajan, Sanjay Ranka
- **Comment**: Submitted to ACM Transactions on Spatial Algorithms and Systems
  (TSAS); Special issue on Urban Mobility: Algorithms and Systems. arXiv admin
  note: text overlap with arXiv:1703.07402 by other authors
- **Journal**: None
- **Summary**: In Intelligent Transportation System, real-time systems that monitor and analyze road users become increasingly critical as we march toward the smart city era. Vision-based frameworks for Object Detection, Multiple Object Tracking, and Traffic Near Accident Detection are important applications of Intelligent Transportation System, particularly in video surveillance and etc. Although deep neural networks have recently achieved great success in many computer vision tasks, a uniformed framework for all the three tasks is still challenging where the challenges multiply from demand for real-time performance, complex urban setting, highly dynamic traffic event, and many traffic movements. In this paper, we propose a two-stream Convolutional Network architecture that performs real-time detection, tracking, and near accident detection of road users in traffic video data. The two-stream model consists of a spatial stream network for Object Detection and a temporal stream network to leverage motion features for Multiple Object Tracking. We detect near accidents by incorporating appearance features and motion features from two-stream networks. Using aerial videos, we propose a Traffic Near Accident Dataset (TNAD) covering various types of traffic interactions that is suitable for vision-based traffic analysis tasks. Our experiments demonstrate the advantage of our framework with an overall competitive qualitative and quantitative performance at high frame rates on the TNAD dataset.



### Reference Setup for Quantitative Comparison of Segmentation Techniques for Short Glass Fiber CT Data
- **Arxiv ID**: http://arxiv.org/abs/1901.01210v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01210v1)
- **Published**: 2019-01-04 16:57:47+00:00
- **Updated**: 2019-01-04 16:57:47+00:00
- **Authors**: Tomasz Konopczyński, Jitendra Rathore, Thorben Kröger, Lei Zheng, Christoph S. Garbe, Simone Carmignato, Jürgen Hesser
- **Comment**: Accepted to 7th Conference on Industrial Computed Tomography, Leuven,
  Belgium (iCT 2017)
- **Journal**: None
- **Summary**: Comparing different algorithms for segmenting glass fibers in industrial computed tomography (CT) scans is difficult due to the absence of a standard reference dataset. In this work, we introduce a set of annotated scans of short-fiber reinforced polymers (SFRP) as well as synthetically created CT volume data together with the evaluation metrics. We suggest both the metrics and this data set as a reference for studying the performance of different algorithms. The real scans were acquired by a Nikon MCT225 X-ray CT system. The simulated scans were created by the use of an in-house computational model and third-party commercial software. For both types of data, corresponding ground truth annotations have been prepared, including hand annotations for the real scans and STL models for the synthetic scans. Additionally, a Hessian-based Frangi vesselness filter for fiber segmentation has been implemented and open-sourced to serve as a reference for comparisons.



### Fully Convolutional Deep Network Architectures for Automatic Short Glass Fiber Semantic Segmentation from CT scans
- **Arxiv ID**: http://arxiv.org/abs/1901.01211v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01211v1)
- **Published**: 2019-01-04 17:00:54+00:00
- **Updated**: 2019-01-04 17:00:54+00:00
- **Authors**: Tomasz Konopczyński, Danish Rathore, Jitendra Rathore, Thorben Kröger, Lei Zheng, Christoph S. Garbe, Simone Carmignato, Jürgen Hesser
- **Comment**: Accepted to 8th Conference on Industrial Computed Tomography, Wels,
  Austria (iCT 2018)
- **Journal**: None
- **Summary**: We present the first attempt to perform short glass fiber semantic segmentation from X-ray computed tomography volumetric datasets at medium (3.9 {\mu}m isotropic) and low (8.3 {\mu}m isotropic) resolution using deep learning architectures. We performed experiments on both synthetic and real CT scans and evaluated deep fully convolutional architectures with both 2D and 3D kernels. Our artificial neural networks outperform existing methods at both medium and low resolution scans.



### Adversarial Examples Versus Cloud-based Detectors: A Black-box Empirical Study
- **Arxiv ID**: http://arxiv.org/abs/1901.01223v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.01223v4)
- **Published**: 2019-01-04 17:34:13+00:00
- **Updated**: 2019-09-14 14:30:25+00:00
- **Authors**: Xurong Li, Shouling Ji, Meng Han, Juntao Ji, Zhenyu Ren, Yushan Liu, Chunming Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has been broadly leveraged by major cloud providers, such as Google, AWS and Baidu, to offer various computer vision related services including image classification, object identification, illegal image detection, etc. While recent works extensively demonstrated that deep learning classification models are vulnerable to adversarial examples, cloud-based image detection models, which are more complicated than classifiers, may also have similar security concern but not get enough attention yet. In this paper, we mainly focus on the security issues of real-world cloud-based image detectors. Specifically, (1) based on effective semantic segmentation, we propose four attacks to generate semantics-aware adversarial examples via only interacting with black-box APIs; and (2) we make the first attempt to conduct an extensive empirical study of black-box attacks against real-world cloud-based image detectors. Through the comprehensive evaluations on five major cloud platforms: AWS, Azure, Google Cloud, Baidu Cloud, and Alibaba Cloud, we demonstrate that our image processing based attacks can reach a success rate of approximately 100%, and the semantic segmentation based attacks have a success rate over 90% among different detection services, such as violence, politician, and pornography detection. We also proposed several possible defense strategies for these security challenges in the real-life situation.



### A Distance Map Regularized CNN for Cardiac Cine MR Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1901.01238v2
- **DOI**: 10.1002/mp.13853
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01238v2)
- **Published**: 2019-01-04 18:24:52+00:00
- **Updated**: 2019-04-18 20:02:16+00:00
- **Authors**: Shusil Dangi, Cristian Linte, Ziv Yaniv
- **Comment**: 11 pages manuscript, 5 pages supplementary materials
- **Journal**: None
- **Summary**: Cardiac image segmentation is a critical process for generating personalized models of the heart and for quantifying cardiac performance parameters. Several convolutional neural network (CNN) architectures have been proposed to segment the heart chambers from cardiac cine MR images. Here we propose a multi-task learning (MTL)-based regularization framework for cardiac MR image segmentation. The network is trained to perform the main task of semantic segmentation, along with a simultaneous, auxiliary task of pixel-wise distance map regression. The proposed distance map regularizer is a decoder network added to the bottleneck layer of an existing CNN architecture, facilitating the network to learn robust global features. The regularizer block is removed after training, so that the original number of network parameters does not change. We show that the proposed regularization method improves both binary and multi-class segmentation performance over the corresponding state-of-the-art CNN architectures on two publicly available cardiac cine MRI datasets, obtaining average dice coefficient of 0.84$\pm$0.03 and 0.91$\pm$0.04, respectively. Furthermore, we also demonstrate improved generalization performance of the distance map regularized network on cross-dataset segmentation, showing as much as 42% improvement in myocardium Dice coefficient from 0.56$\pm$0.28 to 0.80$\pm$0.14.



### Projective Decomposition and Matrix Equivalence up to Scale
- **Arxiv ID**: http://arxiv.org/abs/1901.01336v1
- **DOI**: None
- **Categories**: **stat.OT**, cs.CV, math.MG
- **Links**: [PDF](http://arxiv.org/pdf/1901.01336v1)
- **Published**: 2019-01-04 22:47:42+00:00
- **Updated**: 2019-01-04 22:47:42+00:00
- **Authors**: Max Robinson
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: A data matrix may be seen simply as a means of organizing observations into rows ( e.g., by measured object) and into columns ( e.g., by measured variable) so that the observations can be analyzed with mathematical tools. As a mathematical object, a matrix defines a linear mapping between points representing weighted combinations of its rows (the row vector space) and points representing weighted combinations of its columns (the column vector space). From this perspective, a data matrix defines a relationship between the information that labels its rows and the information that labels its columns, and numerical methods are used to analyze this relationship. A first step is to normalize the data, transforming each observation from scales convenient for measurement to a common scale, on which addition and multiplication can meaningfully combine the different observations. For example, z-transformation rescales every variable to the same scale, standardized variation from an expected value, but ignores scale differences between measured objects. Here we develop the concepts and properties of projective decomposition, which applies the same normalization strategy to both rows and columns by separating the matrix into row- and column-scaling factors and a scale-normalized matrix. We show that different scalings of the same scale-normalized matrix form an equivalence class, and call the scale-normalized, canonical member of the class its scale-invariant form that preserves all pairwise relative ratios. Projective decomposition therefore provides a means of normalizing the broad class of ratio-scale data, in which relative ratios are of primary interest, onto a common scale without altering the ratios of interest, and simultaneously accounting for scale effects for both organizations of the matrix values. Both of these properties distinguish it from z-transformation.



