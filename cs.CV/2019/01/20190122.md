# Arxiv Papers in cs.CV on 2019-01-22
### Linearized Multi-Sampling for Differentiable Image Transformation
- **Arxiv ID**: http://arxiv.org/abs/1901.07124v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07124v3)
- **Published**: 2019-01-22 00:07:12+00:00
- **Updated**: 2019-09-10 17:17:32+00:00
- **Authors**: Wei Jiang, Weiwei Sun, Andrea Tagliasacchi, Eduard Trulls, Kwang Moo Yi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel image sampling method for differentiable image transformation in deep neural networks. The sampling schemes currently used in deep learning, such as Spatial Transformer Networks, rely on bilinear interpolation, which performs poorly under severe scale changes, and more importantly, results in poor gradient propagation. This is due to their strict reliance on direct neighbors. Instead, we propose to generate random auxiliary samples in the vicinity of each pixel in the sampled image, and create a linear approximation with their intensity values. We then use this approximation as a differentiable formula for the transformed image. We demonstrate that our approach produces more representative gradients with a wider basin of convergence for image alignment, which leads to considerable performance improvements when training networks for classification tasks. This is not only true under large downsampling, but also when there are no scale changes. We compare our approach with multi-scale sampling and show that we outperform it. We then demonstrate that our improvements to the sampler are compatible with other tangential improvements to Spatial Transformer Networks and that it further improves their performance.



### Energy Confused Adversarial Metric Learning for Zero-Shot Image Retrieval and Clustering
- **Arxiv ID**: http://arxiv.org/abs/1901.07169v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07169v1)
- **Published**: 2019-01-22 04:23:21+00:00
- **Updated**: 2019-01-22 04:23:21+00:00
- **Authors**: Binghui Chen, Weihong Deng
- **Comment**: AAAI 2019, Spotlight
- **Journal**: None
- **Summary**: Deep metric learning has been widely applied in many computer vision tasks, and recently, it is more attractive in \emph{zero-shot image retrieval and clustering}(ZSRC) where a good embedding is requested such that the unseen classes can be distinguished well. Most existing works deem this 'good' embedding just to be the discriminative one and thus race to devise powerful metric objectives or hard-sample mining strategies for leaning discriminative embedding. However, in this paper, we first emphasize that the generalization ability is a core ingredient of this 'good' embedding as well and largely affects the metric performance in zero-shot settings as a matter of fact. Then, we propose the Energy Confused Adversarial Metric Learning(ECAML) framework to explicitly optimize a robust metric. It is mainly achieved by introducing an interesting Energy Confusion regularization term, which daringly breaks away from the traditional metric learning idea of discriminative objective devising, and seeks to 'confuse' the learned model so as to encourage its generalization ability by reducing overfitting on the seen classes. We train this confusion term together with the conventional metric objective in an adversarial manner. Although it seems weird to 'confuse' the network, we show that our ECAML indeed serves as an efficient regularization technique for metric learning and is applicable to various conventional metric methods. This paper empirically and experimentally demonstrates the importance of learning embedding with good generalization, achieving state-of-the-art performances on the popular CUB, CARS, Stanford Online Products and In-Shop datasets for ZSRC tasks. \textcolor[rgb]{1, 0, 0}{Code available at http://www.bhchen.cn/}.



### Efficient Image Splicing Localization via Contrastive Feature Extraction
- **Arxiv ID**: http://arxiv.org/abs/1901.07172v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1901.07172v1)
- **Published**: 2019-01-22 04:37:46+00:00
- **Updated**: 2019-01-22 04:37:46+00:00
- **Authors**: Ronald Salloum, C. -C. Jay Kuo
- **Comment**: This manuscript was submitted for publication
- **Journal**: None
- **Summary**: In this work, we propose a new data visualization and clustering technique for discovering discriminative structures in high-dimensional data. This technique, referred to as cPCA++, utilizes the fact that the interesting features of a "target" dataset may be obscured by high variance components during traditional PCA. By analyzing what is referred to as a "background" dataset (i.e., one that exhibits the high variance principal components but not the interesting structures), our technique is capable of efficiently highlighting the structure that is unique to the "target" dataset. Similar to another recently proposed algorithm called "contrastive PCA" (cPCA), the proposed cPCA++ method identifies important dataset specific patterns that are not detected by traditional PCA in a wide variety of settings. However, the proposed cPCA++ method is significantly more efficient than cPCA, because it does not require the parameter sweep in the latter approach. We applied the cPCA++ method to the problem of image splicing localization. In this application, we utilize authentic edges as the background dataset and the spliced edges as the target dataset. The proposed method is significantly more efficient than state-of-the-art methods, as the former does not require iterative updates of filter weights via stochastic gradient descent and backpropagation, nor the training of a classifier. Furthermore, the cPCA++ method is shown to provide performance scores comparable to the state-of-the-art Multi-task Fully Convolutional Network (MFCN).



### CAE-ADMM: Implicit Bitrate Optimization via ADMM-based Pruning in Compressive Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1901.07196v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.07196v4)
- **Published**: 2019-01-22 07:57:22+00:00
- **Updated**: 2019-02-09 03:27:31+00:00
- **Authors**: Haimeng Zhao, Peiyuan Liao
- **Comment**: 5 pages, 4 figures, 2 tables, 1 algorithm
- **Journal**: None
- **Summary**: We introduce ADMM-pruned Compressive AutoEncoder (CAE-ADMM) that uses Alternative Direction Method of Multipliers (ADMM) to optimize the trade-off between distortion and efficiency of lossy image compression. Specifically, ADMM in our method is to promote sparsity to implicitly optimize the bitrate, different from entropy estimators used in the previous research. The experiments on public datasets show that our method outperforms the original CAE and some traditional codecs in terms of SSIM/MS-SSIM metrics, at reasonable inference speed.



### Reducing the Model Variance of a Rectal Cancer Segmentation Network
- **Arxiv ID**: http://arxiv.org/abs/1901.07213v5
- **DOI**: 10.1109/ACCESS.2019.2960371
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.07213v5)
- **Published**: 2019-01-22 08:59:53+00:00
- **Updated**: 2019-12-31 02:04:51+00:00
- **Authors**: Joohyung Lee, Ji Eun Oh, Min Ju Kim, Bo Yun Hur, Dae Kyung Sohn
- **Comment**: published at IEEE ACCESS
- **Journal**: IEEE Access, vol. 7, Issue. 1, pp. 182725-182733, 2019
- **Summary**: In preoperative imaging, the demarcation of rectal cancer with magnetic resonance images provides an important basis for cancer staging and treatment planning. Recently, deep learning has greatly improved the state-of-the-art method in automatic segmentation. However, limitations in data availability in the medical field can cause large variance and consequent overfitting to medical image segmentation networks. In this study, we propose methods to reduce the model variance of a rectal cancer segmentation network by adding a rectum segmentation task and performing data augmentation; the geometric correlation between the rectum and rectal cancer motivated the former approach. Moreover, we propose a method to perform a bias-variance analysis within an arbitrary region-of-interest (ROI) of a segmentation network, which we applied to assess the efficacy of our approaches in reducing model variance. As a result, adding a rectum segmentation task reduced the model variance of the rectal cancer segmentation network within tumor regions by a factor of 0.90; data augmentation further reduced the variance by a factor of 0.89. These approaches also reduced the training duration by a factor of 0.96 and a further factor of 0.78, respectively. Our approaches will improve the quality of rectal cancer staging by increasing the accuracy of its automatic demarcation and by providing rectum boundary information since rectal cancer staging requires the demarcation of both rectum and rectal cancer. Besides such clinical benefits, our method also enables segmentation networks to be assessed with bias-variance analysis within an arbitrary ROI, such as a cancerous region.



### Unsupervised Automated Event Detection using an Iterative Clustering based Segmentation Approach
- **Arxiv ID**: http://arxiv.org/abs/1901.07222v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.07222v1)
- **Published**: 2019-01-22 09:24:58+00:00
- **Updated**: 2019-01-22 09:24:58+00:00
- **Authors**: Deepak K. Gupta, Rohit K. Shrivastava, Suhas Phadke, Jeroen Goudswaard
- **Comment**: 14 pages, 13 figures
- **Journal**: None
- **Summary**: A class of vision problems, less commonly studied, consists of detecting objects in imagery obtained from physics-based experiments. These objects can span in 4D (x, y, z, t) and are visible as disturbances (caused due to physical phenomena) in the image with background distribution being approximately uniform. Such objects, occasionally referred to as `events', can be considered as high energy blobs in the image. Unlike the images analyzed in conventional vision problems, very limited features are associated with such events, and their shape, size and count can vary significantly. This poses a challenge on the use of pre-trained models obtained from supervised approaches.   In this paper, we propose an unsupervised approach involving iterative clustering based segmentation (ICS) which can detect target objects (events) in real-time. In this approach, a test image is analyzed over several cycles, and one event is identified per cycle. Each cycle consists of the following steps: (1) image segmentation using a modified k-means clustering method, (2) elimination of empty (with no events) segments based on statistical analysis of each segment, (3) merging segments that overlap (correspond to same event), and (4) selecting the strongest event. These four steps are repeated until all the events have been identified. The ICS approach consists of a few hyper-parameters that have been chosen based on statistical study performed over a set of test images. The applicability of ICS method is demonstrated on several 2D and 3D test examples.



### DF-SLAM: A Deep-Learning Enhanced Visual SLAM System based on Deep Local Features
- **Arxiv ID**: http://arxiv.org/abs/1901.07223v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1901.07223v2)
- **Published**: 2019-01-22 09:25:08+00:00
- **Updated**: 2019-01-24 11:22:55+00:00
- **Authors**: Rong Kang, Jieqi Shi, Xueming Li, Yang Liu, Xiao Liu
- **Comment**: None
- **Journal**: None
- **Summary**: As the foundation of driverless vehicle and intelligent robots, Simultaneous Localization and Mapping(SLAM) has attracted much attention these days. However, non-geometric modules of traditional SLAM algorithms are limited by data association tasks and have become a bottleneck preventing the development of SLAM. To deal with such problems, many researchers seek to Deep Learning for help. But most of these studies are limited to virtual datasets or specific environments, and even sacrifice efficiency for accuracy. Thus, they are not practical enough.   We propose DF-SLAM system that uses deep local feature descriptors obtained by the neural network as a substitute for traditional hand-made features. Experimental results demonstrate its improvements in efficiency and stability. DF-SLAM outperforms popular traditional SLAM systems in various scenes, including challenging scenes with intense illumination changes. Its versatility and mobility fit well into the need for exploring new environments. Since we adopt a shallow network to extract local descriptors and remain others the same as original SLAM systems, our DF-SLAM can still run in real-time on GPU.



### RPC: A Large-Scale Retail Product Checkout Dataset
- **Arxiv ID**: http://arxiv.org/abs/1901.07249v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07249v1)
- **Published**: 2019-01-22 10:38:46+00:00
- **Updated**: 2019-01-22 10:38:46+00:00
- **Authors**: Xiu-Shen Wei, Quan Cui, Lei Yang, Peng Wang, Lingqiao Liu
- **Comment**: Project page: https://rpc-dataset.github.io/
- **Journal**: None
- **Summary**: Over recent years, emerging interest has occurred in integrating computer vision technology into the retail industry. Automatic checkout (ACO) is one of the critical problems in this area which aims to automatically generate the shopping list from the images of the products to purchase. The main challenge of this problem comes from the large scale and the fine-grained nature of the product categories as well as the difficulty for collecting training images that reflect the realistic checkout scenarios due to continuous update of the products. Despite its significant practical and research value, this problem is not extensively studied in the computer vision community, largely due to the lack of a high-quality dataset. To fill this gap, in this work we propose a new dataset to facilitate relevant research. Our dataset enjoys the following characteristics: (1) It is by far the largest dataset in terms of both product image quantity and product categories. (2) It includes single-product images taken in a controlled environment and multi-product images taken by the checkout system. (3) It provides different levels of annotations for the check-out images. Comparing with the existing datasets, ours is closer to the realistic setting and can derive a variety of research problems. Besides the dataset, we also benchmark the performance on this dataset with various approaches. The dataset and related resources can be found at \url{https://rpc-dataset.github.io/}.



### Driver Distraction Identification with an Ensemble of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.09097v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.09097v1)
- **Published**: 2019-01-22 10:47:00+00:00
- **Updated**: 2019-01-22 10:47:00+00:00
- **Authors**: Hesham M. Eraqi, Yehya Abouelnaga, Mohamed H. Saad, Mohamed N. Moustafa
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1706.09498
- **Journal**: Journal of Advanced Transportation, Machine Learning in
  Transportation (MLT) Issue, 2019
- **Summary**: The World Health Organization (WHO) reported 1.25 million deaths yearly due to road traffic accidents worldwide and the number has been continuously increasing over the last few years. Nearly fifth of these accidents are caused by distracted drivers. Existing work of distracted driver detection is concerned with a small set of distractions (mostly, cell phone usage). Unreliable ad-hoc methods are often used.In this paper, we present the first publicly available dataset for driver distraction identification with more distraction postures than existing alternatives. In addition, we propose a reliable deep learning-based solution that achieves a 90% accuracy. The system consists of a genetically-weighted ensemble of convolutional neural networks, we show that a weighted ensemble of classifiers using a genetic algorithm yields in a better classification confidence. We also study the effect of different visual elements in distraction detection by means of face and hand localizations, and skin segmentation. Finally, we present a thinned version of our ensemble that could achieve 84.64% classification accuracy and operate in a real-time environment.



### Fast, Accurate and Lightweight Super-Resolution with Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1901.07261v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.07261v3)
- **Published**: 2019-01-22 11:08:14+00:00
- **Updated**: 2020-07-21 13:45:56+00:00
- **Authors**: Xiangxiang Chu, Bo Zhang, Hailong Ma, Ruijun Xu, Qingyuan Li
- **Comment**: Accepted to ICPR20
- **Journal**: None
- **Summary**: Deep convolutional neural networks demonstrate impressive results in the super-resolution domain. A series of studies concentrate on improving peak signal noise ratio (PSNR) by using much deeper layers, which are not friendly to constrained resources. Pursuing a trade-off between the restoration capacity and the simplicity of models is still non-trivial. Recent contributions are struggling to manually maximize this balance, while our work achieves the same goal automatically with neural architecture search. Specifically, we handle super-resolution with a multi-objective approach. We also propose an elastic search tactic at both micro and macro level, based on a hybrid controller that profits from evolutionary computation and reinforcement learning. Quantitative experiments help us to draw a conclusion that our generated models dominate most of the state-of-the-art methods with respect to the individual FLOPS.



### Super-Trajectories: A Compact Yet Rich Video Representation
- **Arxiv ID**: http://arxiv.org/abs/1901.07273v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07273v1)
- **Published**: 2019-01-22 11:47:18+00:00
- **Updated**: 2019-01-22 11:47:18+00:00
- **Authors**: Ijaz Akhter, Cheong Loong Fah, Richard Hartley
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new video representation in terms of an over-segmentation of dense trajectories covering the whole video. Trajectories are often used to encode long-temporal information in several computer vision applications. Similar to temporal superpixels, a temporal slice of super-trajectories are superpixels, but the later contains more information because it maintains the long dense pixel-wise tracking information as well. The main challenge in using trajectories for any application, is the accumulation of tracking error in the trajectory construction. For our problem, this results in disconnected superpixels. We exploit constraints for edges in addition to trajectory based color and position similarity. Analogous to superpixels as a preprocessing tool for images, the proposed representation has its applications for videos, especially in trajectory based video analysis.



### Ego-motion Sensor for Unmanned Aerial Vehicles Based on a Single-Board Computer
- **Arxiv ID**: http://arxiv.org/abs/1901.07278v1
- **DOI**: 10.1142/9789813231047_0025
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1901.07278v1)
- **Published**: 2019-01-22 12:01:00+00:00
- **Updated**: 2019-01-22 12:01:00+00:00
- **Authors**: Gaël Écorchard, Adam Heinrich, Libor Přeučil
- **Comment**: Proceedings of CLAWAR 2017: 20th International Conference on Climbing
  and Walking Robots and the Support Technologies for Mobile Machines
- **Journal**: Human-Centric Robotics, pp. 189-196, 2017
- **Summary**: This paper describes the design and implementation of a ground-related odometry sensor suitable for micro aerial vehicles. The sensor is based on a ground-facing camera and a single-board Linux-based embedded computer with a multimedia System on a Chip (SoC). The SoC features a hardware video encoder which is used to estimate the optical flow online. The optical flow is then used in combination with a distance sensor to estimate the vehicle's velocity. The proposed sensor is compared to a similar existing solution and evaluated in both indoor and outdoor environments.



### Unsupervised Learning-based Depth Estimation aided Visual SLAM Approach
- **Arxiv ID**: http://arxiv.org/abs/1901.07288v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07288v1)
- **Published**: 2019-01-22 13:07:53+00:00
- **Updated**: 2019-01-22 13:07:53+00:00
- **Authors**: Mingyang Geng, Suning Shang, Bo Ding, Huaimin Wang, Pengfei Zhang, Lei Zhang
- **Comment**: 27 pages
- **Journal**: None
- **Summary**: The RGB-D camera maintains a limited range for working and is hard to accurately measure the depth information in a far distance. Besides, the RGB-D camera will easily be influenced by strong lighting and other external factors, which will lead to a poor accuracy on the acquired environmental depth information. Recently, deep learning technologies have achieved great success in the visual SLAM area, which can directly learn high-level features from the visual inputs and improve the estimation accuracy of the depth information. Therefore, deep learning technologies maintain the potential to extend the source of the depth information and improve the performance of the SLAM system. However, the existing deep learning-based methods are mainly supervised and require a large amount of ground-truth depth data, which is hard to acquire because of the realistic constraints. In this paper, we first present an unsupervised learning framework, which not only uses image reconstruction for supervising but also exploits the pose estimation method to enhance the supervised signal and add training constraints for the task of monocular depth and camera motion estimation. Furthermore, we successfully exploit our unsupervised learning framework to assist the traditional ORB-SLAM system when the initialization module of ORB-SLAM method could not match enough features. Qualitative and quantitative experiments have shown that our unsupervised learning framework performs the depth estimation task comparable to the supervised methods and outperforms the previous state-of-the-art approach by $13.5\%$ on KITTI dataset. Besides, our unsupervised learning framework could significantly accelerate the initialization process of ORB-SLAM system and effectively improve the accuracy on environmental mapping in strong lighting and weak texture scenes.



### Quantifying Legibility of Indoor Spaces Using Deep Convolutional Neural Networks: Case Studies in Train Stations
- **Arxiv ID**: http://arxiv.org/abs/1901.10553v1
- **DOI**: 10.1016/j.buildenv.2019.04.035
- **Categories**: **cs.CY**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1901.10553v1)
- **Published**: 2019-01-22 14:52:07+00:00
- **Updated**: 2019-01-22 14:52:07+00:00
- **Authors**: Zhoutong Wang, Qianhui Liang, Fabio Duarte, Fan Zhang, Louis Charron, Lenna Johnsen, Bill Cai, Carlo Ratti
- **Comment**: 20 pages, 19 figures, 7 tables
- **Journal**: None
- **Summary**: Legibility is the extent to which a space can be easily recognized. Evaluating legibility is particularly desirable in indoor spaces, since it has a large impact on human behavior and the efficiency of space utilization. However, indoor space legibility has only been studied through survey and trivial simulations and lacks reliable quantitative measurement. We utilized a Deep Convolutional Neural Network (DCNN), which is structurally similar to a human perception system, to model legibility in indoor spaces. To implement the modeling of legibility for any indoor spaces, we designed an end-to-end processing pipeline from indoor data retrieving to model training to spatial legibility analysis. Although the model performed very well (98% top-1 accuracy) overall, there are still discrepancies in accuracy among different spaces, reflecting legibility differences. To prove the validity of the pipeline, we deployed a survey on Amazon Mechanical Turk, collecting 4,015 samples. The human samples showed a similar behavior pattern and mechanism as the DCNN models. Further, we used model results to visually explain legibility in different architectural programs, building age, building style, visual clusterings of spaces and visual explanations for building age and architectural functions.



### Simultaneous lesion and neuroanatomy segmentation in Multiple Sclerosis using deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/1901.07419v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07419v3)
- **Published**: 2019-01-22 15:35:14+00:00
- **Updated**: 2020-11-11 09:19:14+00:00
- **Authors**: Richard McKinley, Rik Wepfer, Fabian Aschwanden, Lorenz Grunder, Raphaela Muri, Christian Rummel, Rajeev Verma, Christian Weisstanner, Mauricio Reyes, Anke Salmen, Andrew Chan, Franca Wagner, Roland Wiest
- **Comment**: Substantially revised version after comments from reviewers,
  including comparison to 3D Unet
- **Journal**: None
- **Summary**: Segmentation of white matter lesions and deep grey matter structures is an important task in the quantification of magnetic resonance imaging in multiple sclerosis. In this paper we explore segmentation solutions based on convolutional neural networks (CNNs) for providing fast, reliable segmentations of lesions and grey-matter structures in multi-modal MR imaging, and the performance of these methods when applied to out-of-centre data.   We trained two state-of-the-art fully convolutional CNN architectures on the 2016 MSSEG training dataset, which was annotated by seven independent human raters: a reference implementation of a 3D Unet, and a more recently proposed 3D-to-2D architecture (DeepSCAN). We then retrained those methods on a larger dataset from a single centre, with and without labels for other brain structures. We quantified changes in performance owing to dataset shift, and changes in performance by adding the additional brain-structure labels. We also compared performance with freely available reference methods.   Both fully-convolutional CNN methods substantially outperform other approaches in the literature when trained and evaluated in cross-validation on the MSSEG dataset, showing agreement with human raters in the range of human inter-rater variability. Both architectures showed drops in performance when trained on single-centre data and tested on the MSSEG dataset. When trained with the addition of weak anatomical labels derived from Freesurfer, the performance of the 3D Unet degraded, while the performance of the DeepSCAN net improved. Overall, the DeepSCAN network predicting both lesion and anatomical labels was the best-performing network examined.



### Multiple Graph Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1901.07439v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07439v1)
- **Published**: 2019-01-22 16:02:26+00:00
- **Updated**: 2019-01-22 16:02:26+00:00
- **Authors**: Bo Jiang, Ziyan Zhang, Jin Tang, Bin Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Graph Convolutional Networks (GCNs) have been widely studied for graph-structured data representation and learning. However, in many real applications, data are coming with multiple graphs, and it is non-trivial to adapt GCNs to deal with data representation with multiple graph structures. One main challenge for multi-graph representation is how to exploit both structure information of each individual graph and correlation information across multiple graphs simultaneously. In this paper, we propose a novel Multiple Graph Adversarial Learning (MGAL) framework for multi-graph representation and learning. MGAL aims to learn an optimal structure-invariant and consistent representation for multiple graphs in a common subspace via a novel adversarial learning framework, which thus incorporates both structure information of intra-graph and correlation information of inter-graphs simultaneously. Based on MGAL, we then provide a unified network for semi-supervised learning task. Promising experimental results demonstrate the effectiveness of MGAL model.



### PadChest: A large chest x-ray image dataset with multi-label annotated reports
- **Arxiv ID**: http://arxiv.org/abs/1901.07441v2
- **DOI**: 10.1016/j.media.2020.101797
- **Categories**: **eess.IV**, cs.CV, 92B20, 92C50, 68T50, 92B10
- **Links**: [PDF](http://arxiv.org/pdf/1901.07441v2)
- **Published**: 2019-01-22 16:04:27+00:00
- **Updated**: 2019-02-07 09:41:01+00:00
- **Authors**: Aurelia Bustos, Antonio Pertusa, Jose-Maria Salinas, Maria de la Iglesia-Vayá
- **Comment**: None
- **Journal**: Med. Image Anal., 66 (2020), 101797
- **Summary**: We present a labeled large-scale, high resolution chest x-ray dataset for the automated exploration of medical images along with their associated reports. This dataset includes more than 160,000 images obtained from 67,000 patients that were interpreted and reported by radiologists at Hospital San Juan Hospital (Spain) from 2009 to 2017, covering six different position views and additional information on image acquisition and patient demography. The reports were labeled with 174 different radiographic findings, 19 differential diagnoses and 104 anatomic locations organized as a hierarchical taxonomy and mapped onto standard Unified Medical Language System (UMLS) terminology. Of these reports, 27% were manually annotated by trained physicians and the remaining set was labeled using a supervised method based on a recurrent neural network with attention mechanisms. The labels generated were then validated in an independent test set achieving a 0.93 Micro-F1 score. To the best of our knowledge, this is one of the largest public chest x-ray database suitable for training supervised models concerning radiographs, and the first to contain radiographic reports in Spanish. The PadChest dataset can be downloaded from http://bimcv.cipf.es/bimcv-projects/padchest/.



### Use of First and Third Person Views for Deep Intersection Classification
- **Arxiv ID**: http://arxiv.org/abs/1901.07446v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07446v1)
- **Published**: 2019-01-22 16:26:05+00:00
- **Updated**: 2019-01-22 16:26:05+00:00
- **Authors**: Koji Takeda, Kanji Tanaka
- **Comment**: 5 pages, 5 figures, technical report
- **Journal**: None
- **Summary**: We explore the problem of intersection classification using monocular on-board passive vision, with the goal of classifying traffic scenes with respect to road topology. We divide the existing approaches into two broad categories according to the type of input data: (a) first person vision (FPV) approaches, which use an egocentric view sequence as the intersection is passed; and (b) third person vision (TPV) approaches, which use a single view immediately before entering the intersection. The FPV and TPV approaches each have advantages and disadvantages. Therefore, we aim to combine them into a unified deep learning framework. Experimental results show that the proposed FPV-TPV scheme outperforms previous methods and only requires minimal FPV/TPV measurements.



### Pedestrian Attribute Recognition: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1901.07474v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.07474v2)
- **Published**: 2019-01-22 17:16:49+00:00
- **Updated**: 2023-08-26 05:06:59+00:00
- **Authors**: Xiao Wang, Shaofei Zheng, Rui Yang, Aihua Zheng, Zhe Chen, Jin Tang, Bin Luo
- **Comment**: Check the most recent works on PAR on our Github:
  https://github.com/wangxiao5791509/Pedestrian-Attribute-Recognition-Paper-List
- **Journal**: None
- **Summary**: Recognizing pedestrian attributes is an important task in the computer vision community due to it plays an important role in video surveillance. Many algorithms have been proposed to handle this task. The goal of this paper is to review existing works using traditional methods or based on deep learning networks. Firstly, we introduce the background of pedestrian attribute recognition (PAR, for short), including the fundamental concepts of pedestrian attributes and corresponding challenges. Secondly, we introduce existing benchmarks, including popular datasets and evaluation criteria. Thirdly, we analyze the concept of multi-task learning and multi-label learning and also explain the relations between these two learning algorithms and pedestrian attribute recognition. We also review some popular network architectures which have been widely applied in the deep learning community. Fourthly, we analyze popular solutions for this task, such as attributes group, part-based, etc. Fifthly, we show some applications that take pedestrian attributes into consideration and achieve better performance. Finally, we summarize this paper and give several possible research directions for pedestrian attribute recognition. We continuously update the following GitHub to keep tracking the most cutting-edge related works on pedestrian attribute recognition~\url{https://github.com/wangxiao5791509/Pedestrian-Attribute-Recognition-Paper-List}



### Hybrid Task Cascade for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1901.07518v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07518v2)
- **Published**: 2019-01-22 18:50:36+00:00
- **Updated**: 2019-04-09 17:58:40+00:00
- **Authors**: Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, Chen Change Loy, Dahua Lin
- **Comment**: CVPR 2019 camera ready
- **Journal**: None
- **Summary**: Cascade is a classic yet powerful architecture that has boosted performance on various tasks. However, how to introduce cascade to instance segmentation remains an open question. A simple combination of Cascade R-CNN and Mask R-CNN only brings limited gain. In exploring a more effective approach, we find that the key to a successful instance segmentation cascade is to fully leverage the reciprocal relationship between detection and segmentation. In this work, we propose a new framework, Hybrid Task Cascade (HTC), which differs in two important aspects: (1) instead of performing cascaded refinement on these two tasks separately, it interweaves them for a joint multi-stage processing; (2) it adopts a fully convolutional branch to provide spatial context, which can help distinguishing hard foreground from cluttered background. Overall, this framework can learn more discriminative features progressively while integrating complementary features together in each stage. Without bells and whistles, a single HTC obtains 38.4 and 1.5 improvement over a strong Cascade Mask R-CNN baseline on MSCOCO dataset. Moreover, our overall system achieves 48.6 mask AP on the test-challenge split, ranking 1st in the COCO 2018 Challenge Object Detection Task. Code is available at: https://github.com/open-mmlab/mmdetection.



### MONet: Unsupervised Scene Decomposition and Representation
- **Arxiv ID**: http://arxiv.org/abs/1901.11390v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.11390v1)
- **Published**: 2019-01-22 18:55:34+00:00
- **Updated**: 2019-01-22 18:55:34+00:00
- **Authors**: Christopher P. Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick, Alexander Lerchner
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to decompose scenes in terms of abstract building blocks is crucial for general intelligence. Where those basic building blocks share meaningful properties, interactions and other regularities across scenes, such decompositions can simplify reasoning and facilitate imagination of novel scenarios. In particular, representing perceptual observations in terms of entities should improve data efficiency and transfer performance on a wide range of tasks. Thus we need models capable of discovering useful decompositions of scenes by identifying units with such regularities and representing them in a common format. To address this problem, we have developed the Multi-Object Network (MONet). In this model, a VAE is trained end-to-end together with a recurrent attention network -- in a purely unsupervised manner -- to provide attention masks around, and reconstructions of, regions of images. We show that this model is capable of learning to decompose and represent challenging 3D scenes into semantically meaningful components, such as objects and background elements.



### Striking the Right Balance with Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/1901.07590v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07590v3)
- **Published**: 2019-01-22 19:34:36+00:00
- **Updated**: 2019-04-10 12:32:59+00:00
- **Authors**: Salman Khan, Munawar Hayat, Waqas Zamir, Jianbing Shen, Ling Shao
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Learning unbiased models on imbalanced datasets is a significant challenge. Rare classes tend to get a concentrated representation in the classification space which hampers the generalization of learned boundaries to new test examples. In this paper, we demonstrate that the Bayesian uncertainty estimates directly correlate with the rarity of classes and the difficulty level of individual samples. Subsequently, we present a novel framework for uncertainty based class imbalance learning that follows two key insights: First, classification boundaries should be extended further away from a more uncertain (rare) class to avoid overfitting and enhance its generalization. Second, each sample should be modeled as a multi-variate Gaussian distribution with a mean vector and a covariance matrix defined by the sample's uncertainty. The learned boundaries should respect not only the individual samples but also their distribution in the feature space. Our proposed approach efficiently utilizes sample and class uncertainty information to learn robust features and more generalizable classifiers. We systematically study the class imbalance problem and derive a novel loss formulation for max-margin learning based on Bayesian uncertainty measure. The proposed method shows significant performance improvements on six benchmark datasets for face verification, attribute prediction, digit/object classification and skin lesion detection.



### Aggregated Pairwise Classification of Statistical Shapes
- **Arxiv ID**: http://arxiv.org/abs/1901.07593v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.07593v1)
- **Published**: 2019-01-22 19:43:37+00:00
- **Updated**: 2019-01-22 19:43:37+00:00
- **Authors**: Min Ho Cho, Sebastian Kurtek, Steven N. MacEachern
- **Comment**: None
- **Journal**: None
- **Summary**: The classification of shapes is of great interest in diverse areas ranging from medical imaging to computer vision and beyond. While many statistical frameworks have been developed for the classification problem, most are strongly tied to early formulations of the problem - with an object to be classified described as a vector in a relatively low-dimensional Euclidean space. Statistical shape data have two main properties that suggest a need for a novel approach: (i) shapes are inherently infinite dimensional with strong dependence among the positions of nearby points, and (ii) shape space is not Euclidean, but is fundamentally curved. To accommodate these features of the data, we work with the square-root velocity function of the curves to provide a useful formal description of the shape, pass to tangent spaces of the manifold of shapes at different projection points which effectively separate shapes for pairwise classification in the training data, and use principal components within these tangent spaces to reduce dimensionality. We illustrate the impact of the projection point and choice of subspace on the misclassification rate with a novel method of combining pairwise classifiers.



### Understanding Geometry of Encoder-Decoder CNNs
- **Arxiv ID**: http://arxiv.org/abs/1901.07647v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.07647v2)
- **Published**: 2019-01-22 23:37:43+00:00
- **Updated**: 2019-05-07 13:56:00+00:00
- **Authors**: Jong Chul Ye, Woon Kyoung Sung
- **Comment**: Accepted to ICML 2019
- **Journal**: None
- **Summary**: Encoder-decoder networks using convolutional neural network (CNN) architecture have been extensively used in deep learning literatures thanks to its excellent performance for various inverse problems. However, it is still difficult to obtain coherent geometric view why such an architecture gives the desired performance. Inspired by recent theoretical understanding on generalizability, expressivity and optimization landscape of neural networks, as well as the theory of convolutional framelets, here we provide a unified theoretical framework that leads to a better understanding of geometry of encoder-decoder CNNs. Our unified mathematical framework shows that encoder-decoder CNN architecture is closely related to nonlinear basis representation using combinatorial convolution frames, whose expressibility increases exponentially with the network depth. We also demonstrate the importance of skipped connection in terms of expressibility, and optimization landscape.



