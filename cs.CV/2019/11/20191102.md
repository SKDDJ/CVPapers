# Arxiv Papers in cs.CV on 2019-11-02
### Unsupervised Multi-Domain Multimodal Image-to-Image Translation with Explicit Domain-Constrained Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/1911.00622v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.00622v1)
- **Published**: 2019-11-02 01:09:18+00:00
- **Updated**: 2019-11-02 01:09:18+00:00
- **Authors**: Weihao Xia, Yujiu Yang, Jing-Hao Xue
- **Comment**: 20 pages
- **Journal**: None
- **Summary**: Image-to-image translation has drawn great attention during the past few years. It aims to translate an image in one domain to a given reference image in another domain. Due to its effectiveness and efficiency, many applications can be formulated as image-to-image translation problems. However, three main challenges remain in image-to-image translation: 1) the lack of large amounts of aligned training pairs for different tasks; 2) the ambiguity of multiple possible outputs from a single input image; and 3) the lack of simultaneous training of multiple datasets from different domains within a single network. We also found in experiments that the implicit disentanglement of content and style could lead to unexpect results. In this paper, we propose a unified framework for learning to generate diverse outputs using unpaired training data and allow simultaneous training of multiple datasets from different domains via a single network. Furthermore, we also investigate how to better extract domain supervision information so as to learn better disentangled representations and achieve better image translation. Experiments show that the proposed method outperforms or is comparable with the state-of-the-art methods.



### Automated Inline Analysis of Myocardial Perfusion MRI with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.00625v2
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00625v2)
- **Published**: 2019-11-02 01:33:56+00:00
- **Updated**: 2020-05-29 14:22:58+00:00
- **Authors**: Hui Xue, Rhodri Davies, Louis AE Brown, Kristopher D Knott, Tushar Kotecha, Marianna Fontana, Sven Plein, James C Moon, Peter Kellman
- **Comment**: This work has been submitted to Radiology: Artificial Intelligence
  for possible publication
- **Journal**: None
- **Summary**: Recent development of quantitative myocardial blood flow (MBF) mapping allows direct evaluation of absolute myocardial perfusion, by computing pixel-wise flow maps. Clinical studies suggest quantitative evaluation would be more desirable for objectivity and efficiency. Objective assessment can be further facilitated by segmenting the myocardium and automatically generating reports following the AHA model. This will free user interaction for analysis and lead to a 'one-click' solution to improve workflow. This paper proposes a deep neural network based computational workflow for inline myocardial perfusion analysis. Adenosine stress and rest perfusion scans were acquired from three hospitals. Training set included N=1,825 perfusion series from 1,034 patients. Independent test set included 200 scans from 105 patients. Data were consecutively acquired at each site. A convolution neural net (CNN) model was trained to provide segmentation for LV cavity, myocardium and right ventricular by processing incoming 2D+T perfusion Gd series. Model outputs were compared to manual ground-truth for accuracy of segmentation and flow measures derived on global and per-sector basis. The trained models were integrated onto MR scanners for effective inference. Segmentation accuracy and myocardial flow measures were compared between CNN models and manual ground-truth. The mean Dice ratio of CNN derived myocardium was 0.93 +/- 0.04. Both global flow and per-sector values showed no significant difference, compared to manual results. The AHA 16 segment model was automatically generated and reported on the MR scanner. As a result, the fully automated analysis of perfusion flow mapping was achieved. This solution was integrated on the MR scanner, enabling 'one-click' analysis and reporting of myocardial blood flow.



### Quadratic video interpolation
- **Arxiv ID**: http://arxiv.org/abs/1911.00627v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.00627v1)
- **Published**: 2019-11-02 02:23:33+00:00
- **Updated**: 2019-11-02 02:23:33+00:00
- **Authors**: Xiangyu Xu, Li Siyao, Wenxiu Sun, Qian Yin, Ming-Hsuan Yang
- **Comment**: NeurIPS 2019, project website:
  https://sites.google.com/view/xiangyuxu/qvi_nips19
- **Journal**: None
- **Summary**: Video interpolation is an important problem in computer vision, which helps overcome the temporal limitation of camera sensors. Existing video interpolation methods usually assume uniform motion between consecutive frames and use linear models for interpolation, which cannot well approximate the complex motion in the real world. To address these issues, we propose a quadratic video interpolation method which exploits the acceleration information in videos. This method allows prediction with curvilinear trajectory and variable velocity, and generates more accurate interpolation results. For high-quality frame synthesis, we develop a flow reversal layer to estimate flow fields starting from the unknown target frame to the source frame. In addition, we present techniques for flow refinement. Extensive experiments demonstrate that our approach performs favorably against the existing linear models on a wide variety of video datasets.



### FDDWNet: A Lightweight Convolutional Neural Network for Real-time Sementic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.00632v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.00632v2)
- **Published**: 2019-11-02 02:46:54+00:00
- **Updated**: 2019-11-08 04:59:57+00:00
- **Authors**: Jia Liu, Quan Zhou, Yong Qiang, Bin Kang, Xiaofu Wu, Baoyu Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a lightweight convolutional neural network, called FDDWNet, for real-time accurate semantic segmentation. In contrast to recent advances of lightweight networks that prefer to utilize shallow structure, FDDWNet makes an effort to design more deeper network architecture, while maintains faster inference speed and higher segmentation accuracy. Our network uses factorized dilated depth-wise separable convolutions (FDDWC) to learn feature representations from different scale receptive fields with fewer model parameters. Additionally, FDDWNet has multiple branches of skipped connections to gather context cues from intermediate convolution layers. The experiments show that FDDWNet only has 0.8M model size, while achieves 60 FPS running speed on a single RTX 2080Ti GPU with a 1024x512 input image. The comprehensive experiments demonstrate that our model achieves state-of-the-art results in terms of available speed and accuracy trade-off on CityScapes and CamVid datasets.



### DeepBlindness: Fast Blindness Map Estimation and Blindness Type Classification for Outdoor Scene from Single Color Image
- **Arxiv ID**: http://arxiv.org/abs/1911.00652v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00652v1)
- **Published**: 2019-11-02 05:04:46+00:00
- **Updated**: 2019-11-02 05:04:46+00:00
- **Authors**: Jiaxiong Qiu, Xinyuan Yu, Guoqiang Yang, Shuaicheng Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Outdoor vision robotic systems and autonomous cars suffer from many image-quality issues, particularly haze, defocus blur, and motion blur, which we will define generically as "blindness issues". These blindness issues may seriously affect the performance of robotic systems and could lead to unsafe decisions being made. However, existing solutions either focus on one type of blindness only or lack the ability to estimate the degree of blindness accurately. Besides, heavy computation is needed so that these solutions cannot run in real-time on practical systems. In this paper, we provide a method which could simultaneously detect the type of blindness and provide a blindness map indicating to what degree the vision is limited on a pixel-by-pixel basis. Both the blindness type and the estimate of per-pixel blindness are essential for tasks like deblur, dehaze, or the fail-safe functioning of robotic systems. We demonstrate the effectiveness of our approach on the KITTI and CUHK datasets where experiments show that our method outperforms other state-of-the-art approaches, achieving speeds of about 130 frames per second (fps).



### A Method for Identifying Origin of Digital Images Using a Convolution Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1911.00655v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.00655v1)
- **Published**: 2019-11-02 05:25:19+00:00
- **Updated**: 2019-11-02 05:25:19+00:00
- **Authors**: Rong Huang, Fuming Fang, Huy H. Nguyen, Junichi Yamagishi, Isao Echizen
- **Comment**: Submitted to ICASSP 2020
- **Journal**: None
- **Summary**: The rapid development of deep learning techniques has created new challenges in identifying the origin of digital images because generative adversarial networks and variational autoencoders can create plausible digital images whose contents are not present in natural scenes. In this paper, we consider the origin that can be broken down into three categories: natural photographic image (NPI), computer generated graphic (CGG), and deep network generated image (DGI). A method is presented for effectively identifying the origin of digital images that is based on a convolutional neural network (CNN) and uses a local-to-global framework to reduce training complexity. By feeding labeled data, the CNN is trained to predict the origin of local patches cropped from an image. The origin of the full-size image is then determined by majority voting. Unlike previous forensic methods, the CNN takes the raw pixels as input without the aid of "residual map". Experimental results revealed that not only the high-frequency components but also the middle-frequency ones contribute to origin identification. The proposed method achieved up to 95.21% identification accuracy and behaved robustly against several common post-processing operations including JPEG compression, scaling, geometric transformation, and contrast stretching. The quantitative results demonstrate that the proposed method is more effective than handcrafted feature-based methods.



### Security of Facial Forensics Models Against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1911.00660v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.00660v2)
- **Published**: 2019-11-02 06:03:51+00:00
- **Updated**: 2020-05-20 12:56:34+00:00
- **Authors**: Rong Huang, Fuming Fang, Huy H. Nguyen, Junichi Yamagishi, Isao Echizen
- **Comment**: Accepted by ICIP 2020
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have been used in digital forensics to identify fake facial images. We investigated several DNN-based forgery forensics models (FFMs) to examine whether they are secure against adversarial attacks. We experimentally demonstrated the existence of individual adversarial perturbations (IAPs) and universal adversarial perturbations (UAPs) that can lead a well-performed FFM to misbehave. Based on iterative procedure, gradient information is used to generate two kinds of IAPs that can be used to fabricate classification and segmentation outputs. In contrast, UAPs are generated on the basis of over-firing. We designed a new objective function that encourages neurons to over-fire, which makes UAP generation feasible even without using training data. Experiments demonstrated the transferability of UAPs across unseen datasets and unseen FFMs. Moreover, we conducted subjective assessment for imperceptibility of the adversarial perturbations, revealing that the crafted UAPs are visually negligible. These findings provide a baseline for evaluating the adversarial security of FFMs.



### Progressive Sample Mining and Representation Learning for One-Shot Person Re-identification with Adversarial Samples
- **Arxiv ID**: http://arxiv.org/abs/1911.00666v1
- **DOI**: 10.1016/j.patcog.2020.107614
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.00666v1)
- **Published**: 2019-11-02 06:44:58+00:00
- **Updated**: 2019-11-02 06:44:58+00:00
- **Authors**: Hui Li, Jimin Xiao, Mingjie Sun, Eng Gee Lim, Yao Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we aim to tackle the one-shot person re-identification problem where only one image is labelled for each person, while other images are unlabelled. This task is challenging due to the lack of sufficient labelled training data. To tackle this problem, we propose to iteratively guess pseudo labels for the unlabeled image samples, which are later used to update the re-identification model together with the labelled samples. A new sampling mechanism is designed to select unlabeled samples to pseudo labelled samples based on the distance matrix, and to form a training triplet batch including both labelled samples and pseudo labelled samples. We also design an HSoften-Triplet-Loss to soften the negative impact of the incorrect pseudo label, considering the unreliable nature of pseudo labelled samples. Finally, we deploy an adversarial learning method to expand the image samples to different camera views. Our experiments show that our framework achieves a new state-of-the-art one-shot Re-ID performance on Market-1501 (mAP 42.7%) and DukeMTMC-Reid dataset (mAP 40.3%). Code will be available soon.



### Domain Fingerprints for No-reference Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/1911.00673v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00673v3)
- **Published**: 2019-11-02 07:45:12+00:00
- **Updated**: 2020-09-16 13:37:24+00:00
- **Authors**: Weihao Xia, Yujiu Yang, Jing-Hao Xue, Jing Xiao
- **Comment**: accepted by IEEE Transactions on Circuits and Systems for Video
  Technology (TCSVT)
- **Journal**: None
- **Summary**: Human fingerprints are detailed and nearly unique markers of human identity. Such a unique and stable fingerprint is also left on each acquired image. It can reveal how an image was degraded during the image acquisition procedure and thus is closely related to the quality of an image. In this work, we propose a new no-reference image quality assessment (NR-IQA) approach called domain-aware IQA (DA-IQA), which for the first time introduces the concept of domain fingerprint to the NR-IQA field. The domain fingerprint of an image is learned from image collections of different degradations and then used as the unique characteristics to identify the degradation sources and assess the quality of the image. To this end, we design a new domain-aware architecture, which enables simultaneous determination of both the distortion sources and the quality of an image. With the distortion in an image better characterized, the image quality can be more accurately assessed, as verified by extensive experiments, which show that the proposed DA-IQA performs better than almost all the compared state-of-the-art NR-IQA methods.



### On Modelling Label Uncertainty in Deep Neural Networks: Automatic Estimation of Intra-observer Variability in 2D Echocardiography Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/1911.00674v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.00674v1)
- **Published**: 2019-11-02 07:51:05+00:00
- **Updated**: 2019-11-02 07:51:05+00:00
- **Authors**: Zhibin Liao, Hany Girgis, Amir Abdi, Hooman Vaseli, Jorden Hetherington, Robert Rohling, Ken Gin, Teresa Tsang, Purang Abolmaesumi
- **Comment**: None
- **Journal**: None
- **Summary**: Uncertainty of labels in clinical data resulting from intra-observer variability can have direct impact on the reliability of assessments made by deep neural networks. In this paper, we propose a method for modelling such uncertainty in the context of 2D echocardiography (echo), which is a routine procedure for detecting cardiovascular disease at point-of-care. Echo imaging quality and acquisition time is highly dependent on the operator's experience level. Recent developments have shown the possibility of automating echo image quality quantification by mapping an expert's assessment of quality to the echo image via deep learning techniques. Nevertheless, the observer variability in the expert's assessment can impact the quality quantification accuracy. Here, we aim to model the intra-observer variability in echo quality assessment as an aleatoric uncertainty modelling regression problem with the introduction of a novel method that handles the regression problem with categorical labels. A key feature of our design is that only a single forward pass is sufficient to estimate the level of uncertainty for the network output. Compared to the $0.11 \pm 0.09$ absolute error (in a scale from 0 to 1) archived by the conventional regression method, the proposed method brings the error down to $0.09 \pm 0.08$, where the improvement is statistically significant and equivalents to $5.7\%$ test accuracy improvement. The simplicity of the proposed approach means that it could be generalized to other applications of deep learning in medical imaging, where there is often uncertainty in clinical labels.



### 3D tissue reconstruction with Kinect to evaluate neck lymphedema
- **Arxiv ID**: http://arxiv.org/abs/1911.00678v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1911.00678v1)
- **Published**: 2019-11-02 08:37:14+00:00
- **Updated**: 2019-11-02 08:37:14+00:00
- **Authors**: Gerrit Brugman, Beril Sirmacek
- **Comment**: 13 pages, original work
- **Journal**: None
- **Summary**: Lymphedema is a condition of localized tissue swelling caused by a damaged lymphatic system. Therapy to these tissues is applied manually. Some of the methods are lymph drainage, compression therapy or bandaging. However, the therapy methods are still insufficiently evaluated. Especially, because of not having a reliable method to measure the change of such a soft and flexible tissue. In this research, our goal has been providing a 3d computer vision based method to measure the changes of the neck tissues. To do so, we used Kinect as a depth sensor and built our algorithms for the point cloud data acquired from this sensor. The resulting 3D models of the patient necks are used for comparing the models in time and measuring the volumetric changes accurately. Our discussions with the medical doctors validate that, when used in practice this approach would be able to give better indication on which therapy method is helping and how the tissue is changing in time.



### Cooperative Semantic Segmentation and Image Restoration in Adverse Environmental Conditions
- **Arxiv ID**: http://arxiv.org/abs/1911.00679v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00679v3)
- **Published**: 2019-11-02 08:39:52+00:00
- **Updated**: 2020-03-03 00:45:01+00:00
- **Authors**: Weihao Xia, Zhanglin Cheng, Yujiu Yang, Jing-Hao Xue
- **Comment**: None
- **Journal**: None
- **Summary**: Most state-of-the-art semantic segmentation approaches only achieve high accuracy in good conditions. In practically-common but less-discussed adverse environmental conditions, their performance can decrease enormously. Existing studies usually cast the handling of segmentation in adverse conditions as a separate post-processing step after signal restoration, making the segmentation performance largely depend on the quality of restoration. In this paper, we propose a novel deep-learning framework to tackle semantic segmentation and image restoration in adverse environmental conditions in a holistic manner. The proposed approach contains two components: Semantically-Guided Adaptation, which exploits semantic information from degraded images to refine the segmentation; and Exemplar-Guided Synthesis, which restores images from semantic label maps given degraded exemplars as the guidance. Our method cooperatively leverages the complementarity and interdependence of low-level restoration and high-level segmentation in adverse environmental conditions. Extensive experiments on various datasets demonstrate that our approach can not only improve the accuracy of semantic segmentation with degradation cues, but also boost the perceptual quality and structural similarity of image restoration with semantic guidance.



### Unmasking DeepFakes with simple Features
- **Arxiv ID**: http://arxiv.org/abs/1911.00686v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.00686v3)
- **Published**: 2019-11-02 09:42:25+00:00
- **Updated**: 2020-03-04 13:51:41+00:00
- **Authors**: Ricard Durall, Margret Keuper, Franz-Josef Pfreundt, Janis Keuper
- **Comment**: None
- **Journal**: None
- **Summary**: Deep generative models have recently achieved impressive results for many real-world applications, successfully generating high-resolution and diverse samples from complex datasets. Due to this improvement, fake digital contents have proliferated growing concern and spreading distrust in image content, leading to an urgent need for automated ways to detect these AI-generated fake images.   Despite the fact that many face editing algorithms seem to produce realistic human faces, upon closer examination, they do exhibit artifacts in certain domains which are often hidden to the naked eye. In this work, we present a simple way to detect such fake face images - so-called DeepFakes. Our method is based on a classical frequency domain analysis followed by basic classifier. Compared to previous systems, which need to be fed with large amounts of labeled data, our approach showed very good results using only a few annotated training samples and even achieved good accuracies in fully unsupervised scenarios. For the evaluation on high resolution face images, we combined several public datasets of real and fake faces into a new benchmark: Faces-HQ. Given such high-resolution images, our approach reaches a perfect classification accuracy of 100% when it is trained on as little as 20 annotated samples. In a second experiment, in the evaluation of the medium-resolution images of the CelebA dataset, our method achieves 100% accuracy supervised and 96% in an unsupervised setting. Finally, evaluating a low-resolution video sequences of the FaceForensics++ dataset, our method achieves 91% accuracy detecting manipulated videos.   Source Code: https://github.com/cc-hpc-itwm/DeepFakeDetection



### Pixel-wise Conditioning of Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.00689v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00689v1)
- **Published**: 2019-11-02 10:13:39+00:00
- **Updated**: 2019-11-02 10:13:39+00:00
- **Authors**: Cyprien Ruffino, Romain Hérault, Eric Laloy, Gilles Gasso
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have proven successful for unsupervised image generation. Several works extended GANs to image inpainting by conditioning the generation with parts of the image one wants to reconstruct. However, these methods have limitations in settings where only a small subset of the image pixels is known beforehand. In this paper, we study the effectiveness of conditioning GANs by adding an explicit regularization term to enforce pixel-wise conditions when very few pixel values are provided. In addition, we also investigate the influence of this regularization term on the quality of the generated images and the satisfaction of the conditions. Conducted experiments on MNIST and FashionMNIST show evidence that this regularization term allows for controlling the trade-off between quality of the generated images and constraint satisfaction.



### Anthropometric clothing measurements from 3D body scans
- **Arxiv ID**: http://arxiv.org/abs/1911.00694v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.00694v1)
- **Published**: 2019-11-02 10:55:45+00:00
- **Updated**: 2019-11-02 10:55:45+00:00
- **Authors**: Song Yan, Johan Wirta, Joni-Kristian Kämäräinen
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a full processing pipeline to acquire anthropometric measurements from 3D measurements. The first stage of our pipeline is a commercial point cloud scanner. In the second stage, a pre-defined body model is fitted to the captured point cloud. We have generated one male and one female model from the SMPL library. The fitting process is based on non-rigid Iterative Closest Point (ICP) algorithm that minimizes overall energy of point distance and local stiffness energy terms. In the third stage, we measure multiple circumference paths on the fitted model surface and use a non-linear regressor to provide the final estimates of anthropometric measurements. We scanned 194 male and 181 female subjects and the proposed pipeline provides mean absolute errors from 2.5 mm to 16.0 mm depending on the anthropometric measurement.



### Visual Relationship Detection with Relative Location Mining
- **Arxiv ID**: http://arxiv.org/abs/1911.00713v1
- **DOI**: 10.1145/3343031.3351024
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1911.00713v1)
- **Published**: 2019-11-02 13:33:15+00:00
- **Updated**: 2019-11-02 13:33:15+00:00
- **Authors**: Hao Zhou, Chongyang Zhang, Chuanping Hu
- **Comment**: Accepted to ACM MM 2019
- **Journal**: None
- **Summary**: Visual relationship detection, as a challenging task used to find and distinguish the interactions between object pairs in one image, has received much attention recently. In this work, we propose a novel visual relationship detection framework by deeply mining and utilizing relative location of object-pair in every stage of the procedure. In both the stages, relative location information of each object-pair is abstracted and encoded as auxiliary feature to improve the distinguishing capability of object-pairs proposing and predicate recognition, respectively; Moreover, one Gated Graph Neural Network(GGNN) is introduced to mine and measure the relevance of predicates using relative location. With the location-based GGNN, those non-exclusive predicates with similar spatial position can be clustered firstly and then be smoothed with close classification scores, thus the accuracy of top $n$ recall can be increased further. Experiments on two widely used datasets VRD and VG show that, with the deeply mining and exploiting of relative location information, our proposed model significantly outperforms the current state-of-the-art.



### Self-supervised Deformation Modeling for Facial Expression Editing
- **Arxiv ID**: http://arxiv.org/abs/1911.00735v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.00735v2)
- **Published**: 2019-11-02 15:22:17+00:00
- **Updated**: 2019-11-05 21:47:33+00:00
- **Authors**: ShahRukh Athar, Zhixin Shu, Dimitris Samaras
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in deep generative models have demonstrated impressive results in photo-realistic facial image synthesis and editing. Facial expressions are inherently the result of muscle movement. However, existing neural network-based approaches usually only rely on texture generation to edit expressions and largely neglect the motion information. In this work, we propose a novel end-to-end network that disentangles the task of facial editing into two steps: a " "motion-editing" step and a "texture-editing" step. In the "motion-editing" step, we explicitly model facial movement through image deformation, warping the image into the desired expression. In the "texture-editing" step, we generate necessary textures, such as teeth and shading effects, for a photo-realistic result. Our physically-based task-disentanglement system design allows each step to learn a focused task, removing the need of generating texture to hallucinate motion. Our system is trained in a self-supervised manner, requiring no ground truth deformation annotation. Using Action Units [8] as the representation for facial expression, our method improves the state-of-the-art facial expression editing performance in both qualitative and quantitative evaluations.



### Single-Shot Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.00764v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1911.00764v2)
- **Published**: 2019-11-02 18:41:10+00:00
- **Updated**: 2020-08-30 13:54:04+00:00
- **Authors**: Mark Weber, Jonathon Luiten, Bastian Leibe
- **Comment**: Accepted to IROS 2020
- **Journal**: None
- **Summary**: We present a novel end-to-end single-shot method that segments countable object instances (things) as well as background regions (stuff) into a non-overlapping panoptic segmentation at almost video frame rate. Current state-of-the-art methods are far from reaching video frame rate and mostly rely on merging instance segmentation with semantic background segmentation, making them impractical to use in many applications such as robotics. Our approach relaxes this requirement by using an object detector but is still able to resolve inter- and intra-class overlaps to achieve a non-overlapping segmentation. On top of a shared encoder-decoder backbone, we utilize multiple branches for semantic segmentation, object detection, and instance center prediction. Finally, our panoptic head combines all outputs into a panoptic segmentation and can even handle conflicting predictions between branches as well as certain false predictions. Our network achieves 32.6% PQ on MS-COCO at 23.5 FPS, opening up panoptic segmentation to a broader field of applications.



### Learning to Infer Implicit Surfaces without 3D Supervision
- **Arxiv ID**: http://arxiv.org/abs/1911.00767v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1911.00767v1)
- **Published**: 2019-11-02 19:05:23+00:00
- **Updated**: 2019-11-02 19:05:23+00:00
- **Authors**: Shichen Liu, Shunsuke Saito, Weikai Chen, Hao Li
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in 3D deep learning have shown that it is possible to train highly effective deep models for 3D shape generation, directly from 2D images. This is particularly interesting since the availability of 3D models is still limited compared to the massive amount of accessible 2D images, which is invaluable for training. The representation of 3D surfaces itself is a key factor for the quality and resolution of the 3D output. While explicit representations, such as point clouds and voxels, can span a wide range of shape variations, their resolutions are often limited. Mesh-based representations are more efficient but are limited by their ability to handle varying topologies. Implicit surfaces, however, can robustly handle complex shapes, topologies, and also provide flexible resolution control. We address the fundamental problem of learning implicit surfaces for shape inference without the need of 3D supervision. Despite their advantages, it remains nontrivial to (1) formulate a differentiable connection between implicit surfaces and their 2D renderings, which is needed for image-based supervision; and (2) ensure precise geometric properties and control, such as local smoothness. In particular, sampling implicit surfaces densely is also known to be a computationally demanding and very slow operation. To this end, we propose a novel ray-based field probing technique for efficient image-to-field supervision, as well as a general geometric regularizer for implicit surfaces, which provides natural shape priors in unconstrained regions. We demonstrate the effectiveness of our framework on the task of single-view image-based 3D shape digitization and show how we outperform state-of-the-art techniques both quantitatively and qualitatively.



### Robustness and Imperceptibility Enhancement in Watermarked Images by Color Transformation
- **Arxiv ID**: http://arxiv.org/abs/1911.00772v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00772v1)
- **Published**: 2019-11-02 19:19:24+00:00
- **Updated**: 2019-11-02 19:19:24+00:00
- **Authors**: Maedeh Jamali, Mahnoosh Bagheri, Nader Karimi, Shadrokh Samavi
- **Comment**: 5 pages 3 figures
- **Journal**: None
- **Summary**: One of the effective methods for the preservation of copyright ownership of digital media is watermarking. Different watermarking techniques try to set a tradeoff between robustness and transparency of the process. In this research work, we have used color space conversion and frequency transform to achieve high robustness and transparency. Due to the distribution of image information in the RGB domain, we use the YUV color space, which concentrates the visual information in the Y channel. Embedding of the watermark is performed in the DCT coefficients of the specific wavelet subbands. Experimental results show high transparency and robustness of the proposed method.



### An Algorithm for Routing Capsules in All Domains
- **Arxiv ID**: http://arxiv.org/abs/1911.00792v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00792v6)
- **Published**: 2019-11-02 22:13:18+00:00
- **Updated**: 2020-02-28 16:57:39+00:00
- **Authors**: Franz A. Heinsen
- **Comment**: None
- **Journal**: None
- **Summary**: Building on recent work on capsule networks, we propose a new, general-purpose form of "routing by agreement" that activates output capsules in a layer as a function of their net benefit to use and net cost to ignore input capsules from earlier layers. To illustrate the usefulness of our routing algorithm, we present two capsule networks that apply it in different domains: vision and language. The first network achieves new state-of-the-art accuracy of 99.1% on the smallNORB visual recognition task with fewer parameters and an order of magnitude less training than previous capsule models, and we find evidence that it learns to perform a form of "reverse graphics." The second network achieves new state-of-the-art accuracies on the root sentences of the Stanford Sentiment Treebank: 58.5% on fine-grained and 95.6% on binary labels with a single-task model that routes frozen embeddings from a pretrained transformer as capsules. In both domains, we train with the same regime. Code is available at https://github.com/glassroom/heinsen_routing along with replication instructions.



### Efficient Global Multi-object Tracking Under Minimum-cost Circulation Framework
- **Arxiv ID**: http://arxiv.org/abs/1911.00796v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.00796v2)
- **Published**: 2019-11-02 23:28:20+00:00
- **Updated**: 2020-02-11 17:14:49+00:00
- **Authors**: Congchao Wang, Yizhi Wang, Guoqiang Yu
- **Comment**: 18 pages
- **Journal**: None
- **Summary**: We developed a minimum-cost circulation framework for solving the global data association problem, which plays a key role in the tracking-by-detection paradigm of multi-object tracking. The global data association problem was extensively studied under the minimum-cost flow framework, which is theoretically attractive as being flexible and globally solvable. However, the high computational burden has been a long-standing obstacle to its wide adoption in practice. While enjoying the same theoretical advantages and maintaining the same optimal solution as the minimum-cost flow framework, our new framework has a better theoretical complexity bound and leads to orders of practical efficiency improvement. This new framework is motivated by the observation that minimum-cost flow only partially models the data association problem and must be accompanied by an additional and time-consuming searching scheme to determine the optimal object number. By employing a minimum-cost circulation framework, we eliminate the searching step and naturally integrate the number of objects into the optimization problem. By exploring the special property of the associated graph, that is, an overwhelming majority of the vertices are with unit capacity, we designed an implementation of the framework and proved it has the best theoretical complexity so far for the global data association problem. We evaluated our method with 40 experiments on five MOT benchmark datasets. Our method was always the most efficient and averagely 53 to 1,192 times faster than the three state-of-the-art methods. When our method served as a sub-module for global data association methods using higher-order constraints, similar efficiency improvement was attained. We further illustrated through several case studies how the improved computational efficiency enables more sophisticated tracking models and yields better tracking accuracy.



