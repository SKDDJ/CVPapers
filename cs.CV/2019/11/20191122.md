# Arxiv Papers in cs.CV on 2019-11-22
### A Comparative Evaluation of SGM Variants (including a New Variant, tMGM) for Dense Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/1911.09800v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09800v1)
- **Published**: 2019-11-22 01:11:14+00:00
- **Updated**: 2019-11-22 01:11:14+00:00
- **Authors**: Sonali Patil, Tanmay Prakash, Bharath Comandur, Avinash Kak
- **Comment**: None
- **Journal**: None
- **Summary**: Our goal here is threefold: [1] To present a new dense-stereo matching algorithm, tMGM, that by combining the hierarchical logic of tSGM with the support structure of MGM achieves 6-8\% performance improvement over the baseline SGM (these performance numbers are posted under tMGM-16 in the Middlebury Benchmark V3 ); and [2] Through an exhaustive quantitative and qualitative comparative study, to compare how the major variants of the SGM approach to dense stereo matching, including the new tMGM, perform in the presence of: (a) illumination variations and shadows, (b) untextured or weakly textured regions, (c) repetitive patterns in the scene in the presence of large stereo rectification errors. [3] To present a novel DEM-Sculpting approach for estimating initial disparity search bounds for multi-date satellite stereo pairs. Based on our study, we have found that tMGM generally performs best with respect to all these data conditions. Both tSGM and MGM improve the density of stereo disparity maps and combining the two in tMGM makes it possible to accurately estimate the disparities at a significant number of pixels that would otherwise be declared invalid by SGM. The datasets we have used in our comparative evaluation include the Middlebury2014, KITTI2015, and ETH3D datasets and the satellite images over the San Fernando area from the MVS Challenge dataset.



### Crowd Density Forecasting by Modeling Patch-based Dynamics
- **Arxiv ID**: http://arxiv.org/abs/1911.09814v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09814v1)
- **Published**: 2019-11-22 02:18:30+00:00
- **Updated**: 2019-11-22 02:18:30+00:00
- **Authors**: Hiroaki Minoura, Ryo Yonetani, Mai Nishimura, Yoshitaka Ushiku
- **Comment**: None
- **Journal**: None
- **Summary**: Forecasting human activities observed in videos is a long-standing challenge in computer vision, which leads to various real-world applications such as mobile robots, autonomous driving, and assistive systems. In this work, we present a new visual forecasting task called crowd density forecasting. Given a video of a crowd captured by a surveillance camera, our goal is to predict how that crowd will move in future frames. To address this task, we have developed the patch-based density forecasting network (PDFN), which enables forecasting over a sequence of crowd density maps describing how crowded each location is in each video frame. PDFN represents a crowd density map based on spatially overlapping patches and learns density dynamics patch-wise in a compact latent space. This enables us to model diverse and complex crowd density dynamics efficiently, even when the input video involves a variable number of crowds that each move independently. Experimental results with several public datasets demonstrate the effectiveness of our approach compared with state-of-the-art forecasting methods.



### Two-stage dimension reduction for noisy high-dimensional images and application to Cryogenic Electron Microscopy
- **Arxiv ID**: http://arxiv.org/abs/1911.09816v4
- **DOI**: 10.4310/AMSA.2020.v5.n2.a4
- **Categories**: **eess.IV**, cs.CV, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1911.09816v4)
- **Published**: 2019-11-22 02:30:37+00:00
- **Updated**: 2021-02-27 11:27:44+00:00
- **Authors**: Szu-Chi Chung, Shao-Hsuan Wang, Po-Yao Niu, Su-Yun Huang, Wei-Hau Chang, I-Ping Tu
- **Comment**: 29 pages, 8 figures and 3 tables
- **Journal**: Annals of Mathematical Sciences and Applications. Volume 5, Number
  2, 283-316, 2020
- **Summary**: Principal component analysis (PCA) is arguably the most widely used dimension-reduction method for vector-type data. When applied to a sample of images, PCA requires vectorization of the image data, which in turn entails solving an eigenvalue problem for the sample covariance matrix. We propose herein a two-stage dimension reduction (2SDR) method for image reconstruction from high-dimensional noisy image data. The first stage treats the image as a matrix, which is a tensor of order 2, and uses multilinear principal component analysis (MPCA) for matrix rank reduction and image denoising. The second stage vectorizes the reduced-rank matrix and achieves further dimension and noise reduction. Simulation studies demonstrate excellent performance of 2SDR, for which we also develop an asymptotic theory that establishes consistency of its rank selection. Applications to cryo-EM (cryogenic electronic microscopy), which has revolutionized structural biology, organic and medical chemistry, cellular and molecular physiology in the past decade, are also provided and illustrated with benchmark cryo-EM datasets. Connections to other contemporaneous developments in image reconstruction and high-dimensional statistical inference are also discussed.



### Graph Pruning for Model Compression
- **Arxiv ID**: http://arxiv.org/abs/1911.09817v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09817v2)
- **Published**: 2019-11-22 02:32:15+00:00
- **Updated**: 2021-09-23 03:14:23+00:00
- **Authors**: Mingyang Zhang, Xinyi Yu, Jingtao Rong, Linlin Ou
- **Comment**: accepted by Applied Intelligence
- **Journal**: None
- **Summary**: Previous AutoML pruning works utilized individual layer features to automatically prune filters. We analyze the correlation for two layers from the different blocks which have a short-cut structure. It shows that, in one block, the deeper layer has many redundant filters which can be represented by filters in the former layer. So, it is necessary to take information from other layers into consideration in pruning. In this paper, a novel pruning method, named GraphPruning, is proposed. Any series of the network is viewed as a graph. To automatically aggregate neighboring features for each node, a graph aggregator based on graph convolution networks(GCN) is designed. In the training stage, a PruningNet that is given aggregated node features generates reasonable weights for any size of the sub-network. Subsequently, the best configuration of the Pruned Network is searched by reinforcement learning. Different from previous work, we take the node features from a well-trained graph aggregator instead of the hand-craft features, as the states in reinforcement learning. Compared with other AutoML pruning works, our method has achieved the state-of-the-art under the same conditions on ImageNet-2012.



### Identify the cells' nuclei based on the deep learning neural network
- **Arxiv ID**: http://arxiv.org/abs/1911.09830v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.09830v1)
- **Published**: 2019-11-22 03:30:05+00:00
- **Updated**: 2019-11-22 03:30:05+00:00
- **Authors**: Tianyang Zhang, Rui Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Identify the cells' nuclei is the important point for most medical analyses. To assist doctors finding the accurate cell' nuclei location automatically is highly demanded in the clinical practice. Recently, fully convolutional neural network (FCNs) serve as the back-bone in many image segmentation, like liver and tumer segmentation in medical field, human body block in technical filed. The cells' nuclei identification task is also kind of image segmentation. To achieve this, we prefer to use deep learning algorithms. we construct three general frameworks, one is Mask Region-based Convolutional Neural Network (Mask RCNN), which has the high performance in many image segmentations, one is U-net, which has the high generalization performance on small dataset and the other is DenseUNet, which is mixture network architecture with Dense Net and U-net. we compare the performance of these three frameworks. And we evaluated our method on the dataset of data science bowl 2018 challenge. For single model without any ensemble, they all have good performance.



### Graph Convolution Networks for Probabilistic Modeling of Driving Acceleration
- **Arxiv ID**: http://arxiv.org/abs/1911.09837v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1911.09837v3)
- **Published**: 2019-11-22 03:48:43+00:00
- **Updated**: 2020-05-07 23:40:38+00:00
- **Authors**: Jianyu Su, Peter A. Beling, Rui Guo, Kyungtae Han
- **Comment**: Accepted by ITSC 2020
- **Journal**: None
- **Summary**: The ability to model and predict ego-vehicle's surrounding traffic is crucial for autonomous pilots and intelligent driver-assistance systems. Acceleration prediction is important as one of the major components of traffic prediction. This paper proposes novel approaches to the acceleration prediction problem. By representing spatial relationships between vehicles with a graph model, we build a generalized acceleration prediction framework. This paper studies the effectiveness of proposed Graph Convolution Networks, which operate on graphs predicting the acceleration distribution for vehicles driving on highways. We further investigate prediction improvement through integrating of Recurrent Neural Networks to disentangle the temporal complexity inherent in the traffic data. Results from simulation studies using comprehensive performance metrics support the conclusion that our proposed networks outperform state-of-the-art methods in generating realistic trajectories over a prediction horizon.



### Real-time Ultrasound-enhanced Multimodal Imaging of Tongue using 3D Printable Stabilizer System: A Deep Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/1911.09840v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.09840v1)
- **Published**: 2019-11-22 03:54:31+00:00
- **Updated**: 2019-11-22 03:54:31+00:00
- **Authors**: M. Hamed Mozaffari, Won-Sook Lee
- **Comment**: 12 figures, 1 table
- **Journal**: Canadian Acoustics. 48, 1 (Mar. 2020)
- **Summary**: Despite renewed awareness of the importance of articulation, it remains a challenge for instructors to handle the pronunciation needs of language learners. There are relatively scarce pedagogical tools for pronunciation teaching and learning. Unlike inefficient, traditional pronunciation instructions like listening and repeating, electronic visual feedback (EVF) systems such as ultrasound technology have been employed in new approaches. Recently, an ultrasound-enhanced multimodal method has been developed for visualizing tongue movements of a language learner overlaid on the face-side of the speaker's head. That system was evaluated for several language courses via a blended learning paradigm at the university level. The result was asserted that visualizing the articulator's system as biofeedback to language learners will significantly improve articulation learning efficiency. In spite of the successful usage of multimodal techniques for pronunciation training, it still requires manual works and human manipulation. In this article, we aim to contribute to this growing body of research by addressing difficulties of the previous approaches by proposing a new comprehensive, automatic, real-time multimodal pronunciation training system, benefits from powerful artificial intelligence techniques. The main objective of this research was to combine the advantages of ultrasound technology, three-dimensional printing, and deep learning algorithms to enhance the performance of previous systems. Our preliminary pedagogical evaluation of the proposed system revealed a significant improvement in flexibility, control, robustness, and autonomy.



### SCR-Graph: Spatial-Causal Relationships based Graph Reasoning Network for Human Action Prediction
- **Arxiv ID**: http://arxiv.org/abs/1912.05003v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.05003v1)
- **Published**: 2019-11-22 04:54:40+00:00
- **Updated**: 2019-11-22 04:54:40+00:00
- **Authors**: Bo Chen, Decai Li, Yuqing He, Chunsheng Hua
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Technologies to predict human actions are extremely important for applications such as human robot cooperation and autonomous driving. However, a majority of the existing algorithms focus on exploiting visual features of the videos and do not consider the mining of relationships, which include spatial relationships between human and scene elements as well as causal relationships in temporal action sequences. In fact, human beings are good at using spatial and causal relational reasoning mechanism to predict the actions of others. Inspired by this idea, we proposed a Spatial and Causal Relationship based Graph Reasoning Network (SCR-Graph), which can be used to predict human actions by modeling the action-scene relationship, and causal relationship between actions, in spatial and temporal dimensions respectively. Here, in spatial dimension, a hierarchical graph attention module is designed by iteratively aggregating the features of different kinds of scene elements in different level. In temporal dimension, we designed a knowledge graph based causal reasoning module and map the past actions to temporal causal features through Diffusion RNN. Finally, we integrated the causality features into the heterogeneous graph in the form of shadow node, and introduced a self-attention module to determine the time when the knowledge graph information should be activated. Extensive experimental results on the VIRAT datasets demonstrate the favorable performance of the proposed framework.



### Shape Detection In 2D Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/1911.09863v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.09863v1)
- **Published**: 2019-11-22 05:27:00+00:00
- **Updated**: 2019-11-22 05:27:00+00:00
- **Authors**: Ruturaj Gole, Haixia Wu, Subho Ghose
- **Comment**: None
- **Journal**: None
- **Summary**: Ultrasound images are one of the most widely used techniques in clinical settings to analyze and detect different organs for study or diagnoses of diseases. The dependence on subjective opinions of experts such as radiologists calls for an automatic recognition and detection system that can provide an objective analysis. Previous work done on this topic is limited and can be classified by the organ of interest. Hybrid neural networks, linear and logistic regression models, 3D reconstructed models, and various machine learning techniques have been used to solve complex problems such as detection of lesions and cancer. Our project aims to use Dual Path Networks (DPN) to segment and detect shapes in ultrasound images taken from 3D printed models of the liver. Further the DPN deep architectures could be coupled with Fully Convolutional Network (FCN) to refine the results. Data denoised with various filters would be used to gauge how they fare against each other and provide the best results. Small amount of dataset works with DPNs, and hence, that should be appropriate for us as our dataset shall be limited in size. Moreover, the ultrasound scans shall need to be taken from different orientations of the scanner with respect to the organ, such that the training dataset can accurately perform segmentation and shape detection.



### PAG-Net: Progressive Attention Guided Depth Super-resolution Network
- **Arxiv ID**: http://arxiv.org/abs/1911.09878v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09878v1)
- **Published**: 2019-11-22 06:38:53+00:00
- **Updated**: 2019-11-22 06:38:53+00:00
- **Authors**: Arpit Bansal, Sankaraganesh Jonna, Rajiv R. Sahay
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: In this paper, we propose a novel method for the challenging problem of guided depth map super-resolution, called PAGNet. It is based on residual dense networks and involves the attention mechanism to suppress the texture copying problem arises due to improper guidance by RGB images. The attention module mainly involves providing the spatial attention to guidance image based on the depth features. We evaluate the proposed trained models on test dataset and provide comparisons with the state-of-the-art depth super-resolution methods.



### Visual Relationship Detection with Low Rank Non-Negative Tensor Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1911.09895v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1911.09895v1)
- **Published**: 2019-11-22 07:23:02+00:00
- **Updated**: 2019-11-22 07:23:02+00:00
- **Authors**: Mohammed Haroon Dupty, Zhen Zhang, Wee Sun Lee
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of Visual Relationship Detection (VRD) which aims to describe the relationships between pairs of objects in the form of triplets of (subject, predicate, object). We observe that given a pair of bounding box proposals, objects often participate in multiple relations implying the distribution of triplets is multimodal. We leverage the strong correlations within triplets to learn the joint distribution of triplet variables conditioned on the image and the bounding box proposals, doing away with the hitherto used independent distribution of triplets. To make learning the triplet joint distribution feasible, we introduce a novel technique of learning conditional triplet distributions in the form of their normalized low rank non-negative tensor decompositions. Normalized tensor decompositions take form of mixture distributions of discrete variables and thus are able to capture multimodality. This allows us to efficiently learn higher order discrete multimodal distributions and at the same time keep the parameter size manageable. We further model the probability of selecting an object proposal pair and include a relation triplet prior in our model. We show that each part of the model improves performance and the combination outperforms state-of-the-art score on the Visual Genome (VG) and Visual Relationship Detection (VRD) datasets.



### Modeling emotion in complex stories: the Stanford Emotional Narratives Dataset
- **Arxiv ID**: http://arxiv.org/abs/1912.05008v1
- **DOI**: 10.1109/TAFFC.2019.2955949
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1912.05008v1)
- **Published**: 2019-11-22 07:55:08+00:00
- **Updated**: 2019-11-22 07:55:08+00:00
- **Authors**: Desmond C. Ong, Zhengxuan Wu, Tan Zhi-Xuan, Marianne Reddan, Isabella Kahhale, Alison Mattek, Jamil Zaki
- **Comment**: 16 pages, 7 figures; accepted for publication at IEEE Transactions on
  Affective Computing
- **Journal**: None
- **Summary**: Human emotions unfold over time, and more affective computing research has to prioritize capturing this crucial component of real-world affect. Modeling dynamic emotional stimuli requires solving the twin challenges of time-series modeling and of collecting high-quality time-series datasets. We begin by assessing the state-of-the-art in time-series emotion recognition, and we review contemporary time-series approaches in affective computing, including discriminative and generative models. We then introduce the first version of the Stanford Emotional Narratives Dataset (SENDv1): a set of rich, multimodal videos of self-paced, unscripted emotional narratives, annotated for emotional valence over time. The complex narratives and naturalistic expressions in this dataset provide a challenging test for contemporary time-series emotion recognition models. We demonstrate several baseline and state-of-the-art modeling approaches on the SEND, including a Long Short-Term Memory model and a multimodal Variational Recurrent Neural Network, which perform comparably to the human-benchmark. We end by discussing the implications for future research in time-series affective computing.



### Retinal Vessel Segmentation based on Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.09915v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.09915v1)
- **Published**: 2019-11-22 08:21:53+00:00
- **Updated**: 2019-11-22 08:21:53+00:00
- **Authors**: Zhengyuan Liu
- **Comment**: None
- **Journal**: None
- **Summary**: The morphological attributes of retinal vessels, such as length, width, tortuosity and branching pattern and angles, play an important role in diagnosis, screening, treatment, and evaluation of various cardiovascular and ophthalmologic diseases such as diabetes, hypertension and arteriosclerosis. The crucial step before extracting these morphological characteristics of retinal vessels from retinal fundus images is vessel segmentation. In this work, we propose a method for retinal vessel segmentation based on fully convolutional networks. Thousands of patches are extracted from each retinal image and then fed into the network, and data argumentation is applied by rotating extracted patches. Two architectures of fully convolutional networks, U-Net and LadderNet, are used for vessel segmentation. The performance of our method is evaluated on three public datasets: DRIVE, STARE, and CHASE\_DB1. Experimental results of our method show superior performance compared to recent state-of-the-art methods.



### Simplified_edition_Multi-robot SLAM Multi-view Target Tracking based on Panoramic Vision in Irregular Environment
- **Arxiv ID**: http://arxiv.org/abs/1911.09918v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.09918v1)
- **Published**: 2019-11-22 08:36:05+00:00
- **Updated**: 2019-11-22 08:36:05+00:00
- **Authors**: R. Q. Wang, Z. Q. Yuan, G. H. Chen
- **Comment**: None
- **Journal**: None
- **Summary**: In order to improve the precision of multi-robot SLAM multi-view target tracking process, a improved multi-robot SLAM multi-view target tracking algorithm based on panoramic vision in irregular environment was put forward, adding an correction factor to renew the existing Extended Kalman Filter (EKF) model, obtaining new coordinates X and Y after twice iterations. The paper has been accepted by Computing and Visualization in Science and this is a simplified version.



### SM-NAS: Structural-to-Modular Neural Architecture Search for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.09929v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09929v2)
- **Published**: 2019-11-22 08:58:36+00:00
- **Updated**: 2019-11-30 17:25:22+00:00
- **Authors**: Lewei Yao, Hang Xu, Wei Zhang, Xiaodan Liang, Zhenguo Li
- **Comment**: Accepted by AAAI 2020
- **Journal**: None
- **Summary**: The state-of-the-art object detection method is complicated with various modules such as backbone, feature fusion neck, RPN and RCNN head, where each module may have different designs and structures. How to leverage the computational cost and accuracy trade-off for the structural combination as well as the modular selection of multiple modules? Neural architecture search (NAS) has shown great potential in finding an optimal solution. Existing NAS works for object detection only focus on searching better design of a single module such as backbone or feature fusion neck, while neglecting the balance of the whole system. In this paper, we present a two-stage coarse-to-fine searching strategy named Structural-to-Modular NAS (SM-NAS) for searching a GPU-friendly design of both an efficient combination of modules and better modular-level architecture for object detection. Specifically, Structural-level searching stage first aims to find an efficient combination of different modules; Modular-level searching stage then evolves each specific module and pushes the Pareto front forward to a faster task-specific network. We consider a multi-objective search where the search space covers many popular designs of detection methods. We directly search a detection backbone without pre-trained models or any proxy task by exploring a fast training from scratch strategy. The resulting architectures dominate state-of-the-art object detection systems in both inference time and accuracy and demonstrate the effectiveness on multiple detection datasets, e.g. halving the inference time with additional 1% mAP improvement compared to FPN and reaching 46% mAP with the similar inference time of MaskRCNN.



### Unsupervised Learning for Intrinsic Image Decomposition from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1911.09930v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09930v2)
- **Published**: 2019-11-22 09:00:55+00:00
- **Updated**: 2020-05-26 23:37:30+00:00
- **Authors**: Yunfei Liu, Yu Li, Shaodi You, Feng Lu
- **Comment**: Accepted by CVPR 2020
- **Journal**: None
- **Summary**: Intrinsic image decomposition, which is an essential task in computer vision, aims to infer the reflectance and shading of the scene. It is challenging since it needs to separate one image into two components. To tackle this, conventional methods introduce various priors to constrain the solution, yet with limited performance. Meanwhile, the problem is typically solved by supervised learning methods, which is actually not an ideal solution since obtaining ground truth reflectance and shading for massive general natural scenes is challenging and even impossible. In this paper, we propose a novel unsupervised intrinsic image decomposition framework, which relies on neither labeled training data nor hand-crafted priors. Instead, it directly learns the latent feature of reflectance and shading from unsupervised and uncorrelated data. To enable this, we explore the independence between reflectance and shading, the domain invariant content constraint and the physical constraint. Extensive experiments on both synthetic and real image datasets demonstrate consistently superior performance of the proposed method.



### Oktoberfest Food Dataset
- **Arxiv ID**: http://arxiv.org/abs/1912.05007v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.05007v1)
- **Published**: 2019-11-22 09:28:59+00:00
- **Updated**: 2019-11-22 09:28:59+00:00
- **Authors**: Alexander Ziller, Julius Hansjakob, Vitalii Rusinov, Daniel Zügner, Peter Vogel, Stephan Günnemann
- **Comment**: Dataset publication of Oktoberfest Food Dataset. 4 pages, 6 figures
- **Journal**: None
- **Summary**: We release a realistic, diverse, and challenging dataset for object detection on images. The data was recorded at a beer tent in Germany and consists of 15 different categories of food and drink items. We created more than 2,500 object annotations by hand for 1,110 images captured by a video camera above the checkout. We further make available the remaining 600GB of (unlabeled) data containing days of footage. Additionally, we provide our trained models as a benchmark. Possible applications include automated checkout systems which could significantly speed up the process.



### DLGAN: Disentangling Label-Specific Fine-Grained Features for Image Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1911.09943v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.09943v2)
- **Published**: 2019-11-22 09:42:52+00:00
- **Updated**: 2020-08-25 04:47:37+00:00
- **Authors**: Guanqi Zhan, Yihao Zhao, Bingchan Zhao, Haoqi Yuan, Baoquan Chen, Hao Dong
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies have shown how disentangling images into content and feature spaces can provide controllable image translation/ manipulation. In this paper, we propose a framework to enable utilizing discrete multi-labels to control which features to be disentangled, i.e., disentangling label-specific fine-grained features for image manipulation (dubbed DLGAN). By mapping the discrete label-specific attribute features into a continuous prior distribution, we leverage the advantages of both discrete labels and reference images to achieve image manipulation in a hybrid fashion. For example, given a face image dataset (e.g., CelebA) with multiple discrete fine-grained labels, we can learn to smoothly interpolate a face image between black hair and blond hair through reference images while immediately controlling the gender and age through discrete input labels. To the best of our knowledge, this is the first work that realizes such a hybrid manipulation within a single model. More importantly, it is the first work to achieve image interpolation between two different domains without requiring continuous labels as the supervision. Qualitative and quantitative experiments demonstrate the effectiveness of the proposed method.



### Class-specific residual constraint non-negative representation for pattern classification
- **Arxiv ID**: http://arxiv.org/abs/1911.09953v2
- **DOI**: 10.1117/1.JEI.29.2.023014
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09953v2)
- **Published**: 2019-11-22 10:03:14+00:00
- **Updated**: 2020-01-19 06:49:11+00:00
- **Authors**: He-Feng Yin, Xiao-Jun Wu
- **Comment**: submitted to Journal of Electronic Imaging
- **Journal**: None
- **Summary**: Representation based classification method (RBCM) remains one of the hottest topics in the community of pattern recognition, and the recently proposed non-negative representation based classification (NRC) achieved impressive recognition results in various classification tasks. However, NRC ignores the relationship between the coding and classification stages. Moreover, there is no regularization term other than the reconstruction error term in the formulation of NRC, which may result in unstable solution leading to misclassification. To overcome these drawbacks of NRC, in this paper, we propose a class-specific residual constraint non-negative representation (CRNR) for pattern classification. CRNR introduces a class-specific residual constraint into the formulation of NRC, which encourages training samples from different classes to competitively represent the test sample. Based on the proposed CRNR, we develop a CRNR based classifier (CRNRC) for pattern classification. Experimental results on several benchmark datasets demonstrate the superiority of CRNRC over conventional RBCM as well as the recently proposed NRC. Moreover, CRNRC works better or comparable to some state-of-the-art deep approaches on diverse challenging pattern classification tasks. The source code of our proposed CRNRC is accessible at https://github.com/yinhefeng/CRNRC.



### Computational Ceramicology
- **Arxiv ID**: http://arxiv.org/abs/1911.09960v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.09960v1)
- **Published**: 2019-11-22 10:32:52+00:00
- **Updated**: 2019-11-22 10:32:52+00:00
- **Authors**: Barak Itkin, Lior Wolf, Nachum Dershowitz
- **Comment**: None
- **Journal**: None
- **Summary**: Field archeologists are called upon to identify potsherds, for which purpose they rely on their experience and on reference works. We have developed two complementary machine-learning tools to propose identifications based on images captured on site. One method relies on the shape of the fracture outline of a sherd; the other is based on decorative features. For the outline-identification tool, a novel deep-learning architecture was employed, one that integrates shape information from points along the inner and outer surfaces. The decoration classifier is based on relatively standard architectures used in image recognition. In both cases, training the classifiers required tackling challenges that arise when working with real-world archeological data: paucity of labeled data; extreme imbalance between instances of the different categories; and the need to avoid neglecting rare classes and to take note of minute distinguishing features of some classes. The scarcity of training data was overcome by using synthetically-produced virtual potsherds and by employing multiple data-augmentation techniques. A novel form of training loss allowed us to overcome the problems caused by under-populated classes and non-homogeneous distribution of discriminative features.



### Background Suppression Network for Weakly-supervised Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/1911.09963v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09963v1)
- **Published**: 2019-11-22 10:39:57+00:00
- **Updated**: 2019-11-22 10:39:57+00:00
- **Authors**: Pilhyeon Lee, Youngjung Uh, Hyeran Byun
- **Comment**: Accepted by the 34th AAAI Conference on Artificial Intelligence (AAAI
  2020)
- **Journal**: None
- **Summary**: Weakly-supervised temporal action localization is a very challenging problem because frame-wise labels are not given in the training stage while the only hint is video-level labels: whether each video contains action frames of interest. Previous methods aggregate frame-level class scores to produce video-level prediction and learn from video-level action labels. This formulation does not fully model the problem in that background frames are forced to be misclassified as action classes to predict video-level labels accurately. In this paper, we design Background Suppression Network (BaS-Net) which introduces an auxiliary class for background and has a two-branch weight-sharing architecture with an asymmetrical training strategy. This enables BaS-Net to suppress activations from background frames to improve localization performance. Extensive experiments demonstrate the effectiveness of BaS-Net and its superiority over the state-of-the-art methods on the most popular benchmarks - THUMOS'14 and ActivityNet. Our code and the trained model are available at https://github.com/Pilhyeon/BaSNet-pytorch.



### SelfVIO: Self-Supervised Deep Monocular Visual-Inertial Odometry and Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1911.09968v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.09968v2)
- **Published**: 2019-11-22 10:51:09+00:00
- **Updated**: 2020-07-23 13:37:41+00:00
- **Authors**: Yasin Almalioglu, Mehmet Turan, Alp Eren Sari, Muhamad Risqi U. Saputra, Pedro P. B. de Gusmão, Andrew Markham, Niki Trigoni
- **Comment**: 15 pages, submitted to The IEEE Transactions on Robotics (T-RO)
  journal, under review
- **Journal**: None
- **Summary**: In the last decade, numerous supervised deep learning approaches requiring large amounts of labeled data have been proposed for visual-inertial odometry (VIO) and depth map estimation. To overcome the data limitation, self-supervised learning has emerged as a promising alternative, exploiting constraints such as geometric and photometric consistency in the scene. In this study, we introduce a novel self-supervised deep learning-based VIO and depth map recovery approach (SelfVIO) using adversarial training and self-adaptive visual-inertial sensor fusion. SelfVIO learns to jointly estimate 6 degrees-of-freedom (6-DoF) ego-motion and a depth map of the scene from unlabeled monocular RGB image sequences and inertial measurement unit (IMU) readings. The proposed approach is able to perform VIO without the need for IMU intrinsic parameters and/or the extrinsic calibration between the IMU and the camera. estimation and single-view depth recovery network. We provide comprehensive quantitative and qualitative evaluations of the proposed framework comparing its performance with state-of-the-art VIO, VO, and visual simultaneous localization and mapping (VSLAM) approaches on the KITTI, EuRoC and Cityscapes datasets. Detailed comparisons prove that SelfVIO outperforms state-of-the-art VIO approaches in terms of pose estimation and depth recovery, making it a promising approach among existing methods in the literature.



### Instance Cross Entropy for Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.09976v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.09976v1)
- **Published**: 2019-11-22 11:12:48+00:00
- **Updated**: 2019-11-22 11:12:48+00:00
- **Authors**: Xinshao Wang, Elyor Kodirov, Yang Hua, Neil Robertson
- **Comment**: None
- **Journal**: None
- **Summary**: Loss functions play a crucial role in deep metric learning thus a variety of them have been proposed. Some supervise the learning process by pairwise or tripletwise similarity constraints while others take advantage of structured similarity information among multiple data points. In this work, we approach deep metric learning from a novel perspective. We propose instance cross entropy (ICE) which measures the difference between an estimated instance-level matching distribution and its ground-truth one. ICE has three main appealing properties. Firstly, similar to categorical cross entropy (CCE), ICE has clear probabilistic interpretation and exploits structured semantic similarity information for learning supervision. Secondly, ICE is scalable to infinite training data as it learns on mini-batches iteratively and is independent of the training set size. Thirdly, motivated by our relative weight analysis, seamless sample reweighting is incorporated. It rescales samples' gradients to control the differentiation degree over training examples instead of truncating them by sample mining. In addition to its simplicity and intuitiveness, extensive experiments on three real-world benchmarks demonstrate the superiority of ICE.



### HybridNetSeg: A Compact Hybrid Network for Retinal Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.09982v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.09982v1)
- **Published**: 2019-11-22 11:42:09+00:00
- **Updated**: 2019-11-22 11:42:09+00:00
- **Authors**: Ling Luo, Dingyu Xue, Xinglong Feng
- **Comment**: 16 pages, 3 figures
- **Journal**: None
- **Summary**: A large number of retinal vessel analysis methods based on image segmentation have emerged in recent years. However, existing methods depend on cumbersome backbones, such as VGG16 and ResNet-50, benefiting from their powerful feature extraction capabilities but suffering from high computational costs. In this paper, we propose a novel neural network (HybridNetSeg) dedicated to solving this drawback while further improving overall performance. Considering deformable convolution can extract complex and variable structural information, and larger kernel in mixed depthwise convolution makes contribution to higher accuracy. We have integrated these two modules and propose a Hybrid Convolution Block (HCB) using the idea of heuristic learning. Inspired by the U-Net, we use HCB to replace a part of the common convolution of the U-Net encoder, drastically reducing the parameter count to 0.71M while accelerating the inference process. Not only that, we also propose a multi-scale mixed loss mechanism. Extensive experiments on three major benchmark datasets demonstrate the effectiveness of our proposed method



### Characterizing the impact of using features extracted from pre-trained models on the quality of video captioning sequence-to-sequence models
- **Arxiv ID**: http://arxiv.org/abs/1911.09989v1
- **DOI**: 10.1007/978-3-030-59830-3_21
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09989v1)
- **Published**: 2019-11-22 12:06:19+00:00
- **Updated**: 2019-11-22 12:06:19+00:00
- **Authors**: Menatallh Hammad, May Hammad, Mohamed Elshenawy
- **Comment**: Submitted to conference ICPRAI2020
- **Journal**: None
- **Summary**: The task of video captioning, that is, the automatic generation of sentences describing a sequence of actions in a video, has attracted an increasing attention recently. The complex and high-dimensional representation of video data makes it difficult for a typical encoder-decoder architectures to recognize relevant features and encode them in a proper format. Video data contains different modalities that can be recognized using a mix image, scene, action and audio features. In this paper, we characterize the different features affecting video descriptions and explore the interactions among these features and how they affect the final quality of a video representation. Building on existing encoder-decoder models that utilize limited range of video information, our comparisons show how the inclusion of multi-modal video features can make a significant effect on improving the quality of generated statements. The work is of special interest to scientists and practitioners who are using sequence-to-sequence models to generate video captions.



### Orderless Recurrent Models for Multi-label Classification
- **Arxiv ID**: http://arxiv.org/abs/1911.09996v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09996v3)
- **Published**: 2019-11-22 12:25:14+00:00
- **Updated**: 2020-03-12 17:10:18+00:00
- **Authors**: Vacit Oguz Yazici, Abel Gonzalez-Garcia, Arnau Ramisa, Bartlomiej Twardowski, Joost van de Weijer
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Recurrent neural networks (RNN) are popular for many computer vision tasks, including multi-label classification. Since RNNs produce sequential outputs, labels need to be ordered for the multi-label classification task. Current approaches sort labels according to their frequency, typically ordering them in either rare-first or frequent-first. These imposed orderings do not take into account that the natural order to generate the labels can change for each image, e.g.\ first the dominant object before summing up the smaller objects in the image. Therefore, in this paper, we propose ways to dynamically order the ground truth labels with the predicted label sequence. This allows for the faster training of more optimal LSTM models for multi-label classification. Analysis evidences that our method does not suffer from duplicate generation, something which is common for other models. Furthermore, it outperforms other CNN-RNN models, and we show that a standard architecture of an image encoder and language decoder trained with our proposed loss obtains the state-of-the-art results on the challenging MS-COCO, WIDER Attribute and PA-100K and competitive results on NUS-WIDE.



### Locality Constraint Dictionary Learning with Support Vector for Pattern Classification
- **Arxiv ID**: http://arxiv.org/abs/1911.10003v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10003v1)
- **Published**: 2019-11-22 12:35:35+00:00
- **Updated**: 2019-11-22 12:35:35+00:00
- **Authors**: He-Feng Yin, Xiao-Jun Wu, Su-Gen Chen
- **Comment**: submitted to IEEE Access
- **Journal**: None
- **Summary**: Discriminative dictionary learning (DDL) has recently gained significant attention due to its impressive performance in various pattern classification tasks. However, the locality of atoms is not fully explored in conventional DDL approaches which hampers their classification performance. In this paper, we propose a locality constraint dictionary learning with support vector discriminative term (LCDL-SV), in which the locality information is preserved by employing the graph Laplacian matrix of the learned dictionary. To jointly learn a classifier during the training phase, a support vector discriminative term is incorporated into the proposed objective function. Moreover, in the classification stage, the identity of test data is jointly determined by the regularized residual and the learned multi-class support vector machine. Finally, the resulting optimization problem is solved by utilizing the alternative strategy. Experimental results on benchmark databases demonstrate the superiority of our proposed method over previous dictionary learning approaches on both hand-crafted and deep features. The source code of our proposed LCDL-SV is accessible at https://github.com/yinhefeng/LCDL-SV



### Direct Classification of Type 2 Diabetes From Retinal Fundus Images in a Population-based Sample From The Maastricht Study
- **Arxiv ID**: http://arxiv.org/abs/1911.10022v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.10022v1)
- **Published**: 2019-11-22 13:17:04+00:00
- **Updated**: 2019-11-22 13:17:04+00:00
- **Authors**: Friso G. Heslinga, Josien P. W. Pluim, A. J. H. M. Houben, Miranda T. Schram, Ronald M. A. Henry, Coen D. A. Stehouwer, Marleen J. van Greevenbroek, Tos T. J. M. Berendschot, Mitko Veta
- **Comment**: to be published in the proceeding of SPIE - Medical Imaging 2020, 6
  pages, 1 figure
- **Journal**: None
- **Summary**: Type 2 Diabetes (T2D) is a chronic metabolic disorder that can lead to blindness and cardiovascular disease. Information about early stage T2D might be present in retinal fundus images, but to what extent these images can be used for a screening setting is still unknown. In this study, deep neural networks were employed to differentiate between fundus images from individuals with and without T2D. We investigated three methods to achieve high classification performance, measured by the area under the receiver operating curve (ROC-AUC). A multi-target learning approach to simultaneously output retinal biomarkers as well as T2D works best (AUC = 0.746 [$\pm$0.001]). Furthermore, the classification performance can be improved when images with high prediction uncertainty are referred to a specialist. We also show that the combination of images of the left and right eye per individual can further improve the classification performance (AUC = 0.758 [$\pm$0.003]), using a simple averaging approach. The results are promising, suggesting the feasibility of screening for T2D from retinal fundus images.



### Spotting insects from satellites: modeling the presence of Culicoides imicola through Deep CNNs
- **Arxiv ID**: http://arxiv.org/abs/1911.10024v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.10024v1)
- **Published**: 2019-11-22 13:17:19+00:00
- **Updated**: 2019-11-22 13:17:19+00:00
- **Authors**: Stefano Vincenzi, Angelo Porrello, Pietro Buzzega, Annamaria Conte, Carla Ippoliti, Luca Candeloro, Alessio Di Lorenzo, Andrea Capobianco Dondona, Simone Calderara
- **Comment**: 8 pages, 2 figures. Accepted in the 15th International Conference on
  SIGNAL IMAGE TECHNOLOGY & INTERNET BASED SYSTEMS (SITIS-2019)
- **Journal**: None
- **Summary**: Nowadays, Vector-Borne Diseases (VBDs) raise a severe threat for public health, accounting for a considerable amount of human illnesses. Recently, several surveillance plans have been put in place for limiting the spread of such diseases, typically involving on-field measurements. Such a systematic and effective plan still misses, due to the high costs and efforts required for implementing it. Ideally, any attempt in this field should consider the triangle vectors-host-pathogen, which is strictly linked to the environmental and climatic conditions. In this paper, we exploit satellite imagery from Sentinel-2 mission, as we believe they encode the environmental factors responsible for the vector's spread. Our analysis - conducted in a data-driver fashion - couples spectral images with ground-truth information on the abundance of Culicoides imicola. In this respect, we frame our task as a binary classification problem, underpinning Convolutional Neural Networks (CNNs) as being able to learn useful representation from multi-band images. Additionally, we provide a multi-instance variant, aimed at extracting temporal patterns from a short sequence of spectral images. Experiments show promising results, providing the foundations for novel supportive tools, which could depict where surveillance and prevention measures could be prioritized.



### Domain Adaptation for Object Detection via Style Consistency
- **Arxiv ID**: http://arxiv.org/abs/1911.10033v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10033v1)
- **Published**: 2019-11-22 13:31:20+00:00
- **Updated**: 2019-11-22 13:31:20+00:00
- **Authors**: Adrian Lopez Rodriguez, Krystian Mikolajczyk
- **Comment**: BMVC 2019
- **Journal**: None
- **Summary**: We propose a domain adaptation approach for object detection. We introduce a two-step method: the first step makes the detector robust to low-level differences and the second step adapts the classifiers to changes in the high-level features. For the first step, we use a style transfer method for pixel-adaptation of source images to the target domain. We find that enforcing low distance in the high-level features of the object detector between the style transferred images and the source images improves the performance in the target domain. For the second step, we propose a robust pseudo labelling approach to reduce the noise in both positive and negative sampling. Experimental evaluation is performed using the detector SSD300 on PASCAL VOC extended with the dataset proposed in arxiv:1803.11365 where the target domain images are of different styles. Our approach significantly improves the state-of-the-art performance in this benchmark.



### Computer Vision-based Accident Detection in Traffic Surveillance
- **Arxiv ID**: http://arxiv.org/abs/1911.10037v1
- **DOI**: 10.1109/ICCCNT45670.2019.8944469
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10037v1)
- **Published**: 2019-11-22 13:38:06+00:00
- **Updated**: 2019-11-22 13:38:06+00:00
- **Authors**: Earnest Paul Ijjina, Dhananjai Chand, Savyasachi Gupta, Goutham K
- **Comment**: Accepted in 10th ICCCNT 2019
- **Journal**: 10th ICCCNT, 2019, pp. 1-6
- **Summary**: Computer vision-based accident detection through video surveillance has become a beneficial but daunting task. In this paper, a neoteric framework for detection of road accidents is proposed. The proposed framework capitalizes on Mask R-CNN for accurate object detection followed by an efficient centroid based object tracking algorithm for surveillance footage. The probability of an accident is determined based on speed and trajectory anomalies in a vehicle after an overlap with other vehicles. The proposed framework provides a robust method to achieve a high Detection Rate and a low False Alarm Rate on general road-traffic CCTV surveillance footage. This framework was evaluated on diverse conditions such as broad daylight, low visibility, rain, hail, and snow using the proposed dataset. This framework was found effective and paves the way to the development of general-purpose vehicular accident detection algorithms in real-time.



### Deriving star cluster parameters with convolutional neural networks. II. Extinction and cluster/background classification
- **Arxiv ID**: http://arxiv.org/abs/1911.10059v1
- **DOI**: 10.1051/0004-6361/201936185
- **Categories**: **astro-ph.GA**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.10059v1)
- **Published**: 2019-11-22 14:21:01+00:00
- **Updated**: 2019-11-22 14:21:01+00:00
- **Authors**: J. Bialopetravičius, D. Narbutis
- **Comment**: 17 pages, 21 figures
- **Journal**: A&A 633, A148 (2020)
- **Summary**: Context. Convolutional neural networks (CNNs) have been established as the go-to method for fast object detection and classification on natural images. This opens the door for astrophysical parameter inference on the exponentially increasing amount of sky survey data. Until now, star cluster analysis was based on integral or resolved stellar photometry, which limits the amount of information that can be extracted from individual pixels of cluster images.   Aims. We aim to create a CNN capable of inferring star cluster evolutionary, structural, and environmental parameters from multi-band images, as well to demonstrate its capabilities in discriminating genuine clusters from galactic stellar backgrounds.   Methods. A CNN based on the deep residual network (ResNet) architecture was created and trained to infer cluster ages, masses, sizes, and extinctions, with respect to the degeneracies between them. Mock clusters placed on M83 Hubble Space Telescope (HST) images utilizing three photometric passbands (F336W, F438W, and F814W) were used. The CNN is also capable of predicting the likelihood of a cluster's presence in an image, as well as quantifying its visibility (signal-to-noise).   Results. The CNN was tested on mock images of artificial clusters and has demonstrated reliable inference results for clusters of ages $\lesssim$100 Myr, extinctions $A_V$ between 0 and 3 mag, masses between $3\times10^3$ and $3\times10^5$ ${\rm M_\odot}$, and sizes between 0.04 and 0.4 arcsec at the distance of the M83 galaxy. Real M83 galaxy cluster parameter inference tests were performed with objects taken from previous studies and have demonstrated consistent results.



### Injecting Prior Knowledge into Image Caption Generation
- **Arxiv ID**: http://arxiv.org/abs/1911.10082v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.10082v2)
- **Published**: 2019-11-22 15:22:34+00:00
- **Updated**: 2020-08-06 14:19:51+00:00
- **Authors**: Arushi Goel, Basura Fernando, Thanh-Son Nguyen, Hakan Bilen
- **Comment**: ECCV20 VIPriors Workshop; 14 pages, 5 figures, 4 tables
- **Journal**: None
- **Summary**: Automatically generating natural language descriptions from an image is a challenging problem in artificial intelligence that requires a good understanding of the visual and textual signals and the correlations between them. The state-of-the-art methods in image captioning struggles to approach human level performance, especially when data is limited. In this paper, we propose to improve the performance of the state-of-the-art image captioning models by incorporating two sources of prior knowledge: (i) a conditional latent topic attention, that uses a set of latent variables (topics) as an anchor to generate highly probable words and, (ii) a regularization technique that exploits the inductive biases in syntactic and semantic structure of captions and improves the generalization of image captioning models. Our experiments validate that our method produces more human interpretable captions and also leads to significant improvements on the MSCOCO dataset in both the full and low data regimes.



### Learning End-To-End Scene Flow by Distilling Single Tasks Knowledge
- **Arxiv ID**: http://arxiv.org/abs/1911.10090v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1911.10090v1)
- **Published**: 2019-11-22 15:38:14+00:00
- **Updated**: 2019-11-22 15:38:14+00:00
- **Authors**: Filippo Aleotti, Matteo Poggi, Fabio Tosi, Stefano Mattoccia
- **Comment**: Accepted to AAAI 2020. Project page:
  https://vision.disi.unibo.it/~faleotti/dwarf.html
- **Journal**: None
- **Summary**: Scene flow is a challenging task aimed at jointly estimating the 3D structure and motion of the sensed environment. Although deep learning solutions achieve outstanding performance in terms of accuracy, these approaches divide the whole problem into standalone tasks (stereo and optical flow) addressing them with independent networks. Such a strategy dramatically increases the complexity of the training procedure and requires power-hungry GPUs to infer scene flow barely at 1 FPS. Conversely, we propose DWARF, a novel and lightweight architecture able to infer full scene flow jointly reasoning about depth and optical flow easily and elegantly trainable end-to-end from scratch. Moreover, since ground truth images for full scene flow are scarce, we propose to leverage on the knowledge learned by networks specialized in stereo or flow, for which much more data are available, to distill proxy annotations. Exhaustive experiments show that i) DWARF runs at about 10 FPS on a single high-end GPU and about 1 FPS on NVIDIA Jetson TX2 embedded at KITTI resolution, with moderate drop in accuracy compared to 10x deeper models, ii) learning from many distilled samples is more effective than from the few, annotated ones available. Code available at: https://github.com/FilippoAleotti/Dwarf-Tensorflow



### Machine: The New Art Connoisseur
- **Arxiv ID**: http://arxiv.org/abs/1911.10091v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.10091v2)
- **Published**: 2019-11-22 15:44:56+00:00
- **Updated**: 2019-12-03 18:19:34+00:00
- **Authors**: Yucheng Zhu, Yanrong Ji, Yueying Zhang, Linxin Xu, Aven Le Zhou, Ellick Chan
- **Comment**: None
- **Journal**: None
- **Summary**: The process of identifying and understanding art styles to discover artistic influences is essential to the study of art history. Traditionally, trained experts review fine details of the works and compare them to other known works. To automate and scale this task, we use several state-of-the-art CNN architectures to explore how a machine may help perceive and quantify art styles. This study explores: (1) How accurately can a machine classify art styles? (2) What may be the underlying relationships among different styles and artists? To help answer the first question, our best-performing model using Inception V3 achieves a 9-class classification accuracy of 88.35%, which outperforms the model in Elgammal et al.'s study by more than 20 percent. Visualizations using Grad-CAM heat maps confirm that the model correctly focuses on the characteristic parts of paintings. To help address the second question, we conduct network analysis on the influences among styles and artists by extracting 512 features from the best-performing classification model. Through 2D and 3D T-SNE visualizations, we observe clear chronological patterns of development and separation among the art styles. The network analysis also appears to show anticipated artist level connections from an art historical perspective. This technique appears to help identify some previously unknown linkages that may shed light upon new directions for further exploration by art historians. We hope that humans and machines working in concert may bring new opportunities to the field.



### HAL: Improved Text-Image Matching by Mitigating Visual Semantic Hubs
- **Arxiv ID**: http://arxiv.org/abs/1911.10097v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.10097v1)
- **Published**: 2019-11-22 15:51:08+00:00
- **Updated**: 2019-11-22 15:51:08+00:00
- **Authors**: Fangyu Liu, Rongtian Ye, Xun Wang, Shuaipeng Li
- **Comment**: AAAI-20 (to appear)
- **Journal**: None
- **Summary**: The hubness problem widely exists in high-dimensional embedding space and is a fundamental source of error for cross-modal matching tasks. In this work, we study the emergence of hubs in Visual Semantic Embeddings (VSE) with application to text-image matching. We analyze the pros and cons of two widely adopted optimization objectives for training VSE and propose a novel hubness-aware loss function (HAL) that addresses previous methods' defects. Unlike (Faghri et al.2018) which simply takes the hardest sample within a mini-batch, HAL takes all samples into account, using both local and global statistics to scale up the weights of "hubs". We experiment our method with various configurations of model architectures and datasets. The method exhibits exceptionally good robustness and brings consistent improvement on the task of text-image matching across all settings. Specifically, under the same model architectures as (Faghri et al. 2018) and (Lee at al. 2018), by switching only the learning objective, we report a maximum R@1improvement of 7.4% on MS-COCO and 8.3% on Flickr30k.



### TPsgtR: Neural-Symbolic Tensor Product Scene-Graph-Triplet Representation for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1911.10115v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.10115v1)
- **Published**: 2019-11-22 16:17:21+00:00
- **Updated**: 2019-11-22 16:17:21+00:00
- **Authors**: Chiranjib Sur
- **Comment**: None
- **Journal**: None
- **Summary**: Image captioning can be improved if the structure of the graphical representations can be formulated with conceptual positional binding. In this work, we have introduced a novel technique for caption generation using the neural-symbolic encoding of the scene-graphs, derived from regional visual information of the images and we call it Tensor Product Scene-Graph-Triplet Representation (TP$_{sgt}$R). While, most of the previous works concentrated on identification of the object features in images, we introduce a neuro-symbolic embedding that can embed identified relationships among different regions of the image into concrete forms, instead of relying on the model to compose for any/all combinations. These neural symbolic representation helps in better definition of the neural symbolic space for neuro-symbolic attention and can be transformed to better captions. With this approach, we introduced two novel architectures (TP$_{sgt}$R-TDBU and TP$_{sgt}$R-sTDBU) for comparison and experiment result demonstrates that our approaches outperformed the other models, and generated captions are more comprehensive and natural.



### Spectral Graph Transformer Networks for Brain Surface Parcellation
- **Arxiv ID**: http://arxiv.org/abs/1911.10118v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.10118v1)
- **Published**: 2019-11-22 16:18:07+00:00
- **Updated**: 2019-11-22 16:18:07+00:00
- **Authors**: Ran He, Karthik Gopinath, Christian Desrosiers, Herve Lombaert
- **Comment**: Equal contribution of R. He and K. Gopinath
- **Journal**: None
- **Summary**: The analysis of the brain surface modeled as a graph mesh is a challenging task. Conventional deep learning approaches often rely on data lying in the Euclidean space. As an extension to irregular graphs, convolution operations are defined in the Fourier or spectral domain. This spectral domain is obtained by decomposing the graph Laplacian, which captures relevant shape information. However, the spectral decomposition across different brain graphs causes inconsistencies between the eigenvectors of individual spectral domains, causing the graph learning algorithm to fail. Current spectral graph convolution methods handle this variance by separately aligning the eigenvectors to a reference brain in a slow iterative step. This paper presents a novel approach for learning the transformation matrix required for aligning brain meshes using a direct data-driven approach. Our alignment and graph processing method provides a fast analysis of brain surfaces. The novel Spectral Graph Transformer (SGT) network proposed in this paper uses very few randomly sub-sampled nodes in the spectral domain to learn the alignment matrix for multiple brain surfaces. We validate the use of this SGT network along with a graph convolution network to perform cortical parcellation. Our method on 101 manually-labeled brain surfaces shows improved parcellation performance over a no-alignment strategy, gaining a significant speed (1400 fold) over traditional iterative alignment approaches.



### BlendedMVS: A Large-scale Dataset for Generalized Multi-view Stereo Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.10127v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10127v2)
- **Published**: 2019-11-22 16:29:12+00:00
- **Updated**: 2020-04-13 15:17:04+00:00
- **Authors**: Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, Long Quan
- **Comment**: Accepted to CVPR2020
- **Journal**: None
- **Summary**: While deep learning has recently achieved great success on multi-view stereo (MVS), limited training data makes the trained model hard to be generalized to unseen scenarios. Compared with other computer vision tasks, it is rather difficult to collect a large-scale MVS dataset as it requires expensive active scanners and labor-intensive process to obtain ground truth 3D structures. In this paper, we introduce BlendedMVS, a novel large-scale dataset, to provide sufficient training ground truth for learning-based MVS. To create the dataset, we apply a 3D reconstruction pipeline to recover high-quality textured meshes from images of well-selected scenes. Then, we render these mesh models to color images and depth maps. To introduce the ambient lighting information during training, the rendered color images are further blended with the input images to generate the training input. Our dataset contains over 17k high-resolution images covering a variety of scenes, including cities, architectures, sculptures and small objects. Extensive experiments demonstrate that BlendedMVS endows the trained model with significantly better generalization ability compared with other MVS datasets. The dataset and pretrained models are available at \url{https://github.com/YoYo000/BlendedMVS}.



### Learnable Pooling in Graph Convolution Networks for Brain Surface Analysis
- **Arxiv ID**: http://arxiv.org/abs/1911.10129v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10129v1)
- **Published**: 2019-11-22 16:34:58+00:00
- **Updated**: 2019-11-22 16:34:58+00:00
- **Authors**: Karthik Gopinath, Christian Desrosiers, Herve Lombaert
- **Comment**: None
- **Journal**: None
- **Summary**: Brain surface analysis is essential to neuroscience, however, the complex geometry of the brain cortex hinders computational methods for this task. The difficulty arises from a discrepancy between 3D imaging data, which is represented in Euclidean space, and the non-Euclidean geometry of the highly-convoluted brain surface. Recent advances in machine learning have enabled the use of neural networks for non-Euclidean spaces. These facilitate the learning of surface data, yet pooling strategies often remain constrained to a single fixed-graph. This paper proposes a new learnable graph pooling method for processing multiple surface-valued data to output subject-based information. The proposed method innovates by learning an intrinsic aggregation of graph nodes based on graph spectral embedding. We illustrate the advantages of our approach with in-depth experiments on two large-scale benchmark datasets. The flexibility of the pooling strategy is evaluated on four different prediction tasks, namely, subject-sex classification, regression of cortical region sizes, classification of Alzheimer's disease stages, and brain age regression. Our experiments demonstrate the superiority of our learnable pooling approach compared to other pooling techniques for graph convolution networks, with results improving the state-of-the-art in brain surface analysis.



### Adversarial Learning of Privacy-Preserving and Task-Oriented Representations
- **Arxiv ID**: http://arxiv.org/abs/1911.10143v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.10143v1)
- **Published**: 2019-11-22 17:06:28+00:00
- **Updated**: 2019-11-22 17:06:28+00:00
- **Authors**: Taihong Xiao, Yi-Hsuan Tsai, Kihyuk Sohn, Manmohan Chandraker, Ming-Hsuan Yang
- **Comment**: None
- **Journal**: AAAI 2020
- **Summary**: Data privacy has emerged as an important issue as data-driven deep learning has been an essential component of modern machine learning systems. For instance, there could be a potential privacy risk of machine learning systems via the model inversion attack, whose goal is to reconstruct the input data from the latent representation of deep networks. Our work aims at learning a privacy-preserving and task-oriented representation to defend against such model inversion attacks. Specifically, we propose an adversarial reconstruction learning framework that prevents the latent representations decoded into original input data. By simulating the expected behavior of adversary, our framework is realized by minimizing the negative pixel reconstruction loss or the negative feature reconstruction (i.e., perceptual distance) loss. We validate the proposed method on face attribute prediction, showing that our method allows protecting visual privacy with a small decrease in utility performance. In addition, we show the utility-privacy trade-off with different choices of hyperparameter for negative perceptual distance loss at training, allowing service providers to determine the right level of privacy-protection with a certain utility performance. Moreover, we provide an extensive study with different selections of features, tasks, and the data to further analyze their influence on privacy protection.



### PointPainting: Sequential Fusion for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.10150v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.10150v2)
- **Published**: 2019-11-22 17:19:50+00:00
- **Updated**: 2020-05-06 17:17:18+00:00
- **Authors**: Sourabh Vora, Alex H. Lang, Bassam Helou, Oscar Beijbom
- **Comment**: 11 pages, 6 figures, 8 tables. v1 is initial submission to CVPR 2020.
  v2 is final version accepted for publication at CVPR 2020
- **Journal**: None
- **Summary**: Camera and lidar are important sensor modalities for robotics in general and self-driving cars in particular. The sensors provide complementary information offering an opportunity for tight sensor-fusion. Surprisingly, lidar-only methods outperform fusion methods on the main benchmark datasets, suggesting a gap in the literature. In this work, we propose PointPainting: a sequential fusion method to fill this gap. PointPainting works by projecting lidar points into the output of an image-only semantic segmentation network and appending the class scores to each point. The appended (painted) point cloud can then be fed to any lidar-only method. Experiments show large improvements on three different state-of-the art methods, Point-RCNN, VoxelNet and PointPillars on the KITTI and nuScenes datasets. The painted version of PointRCNN represents a new state of the art on the KITTI leaderboard for the bird's-eye view detection task. In ablation, we study how the effects of Painting depends on the quality and format of the semantic segmentation output, and demonstrate how latency can be minimized through pipelining.



### Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.10194v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10194v3)
- **Published**: 2019-11-22 18:59:51+00:00
- **Updated**: 2020-03-11 17:59:11+00:00
- **Authors**: Bowen Cheng, Maxwell D. Collins, Yukun Zhu, Ting Liu, Thomas S. Huang, Hartwig Adam, Liang-Chieh Chen
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: In this work, we introduce Panoptic-DeepLab, a simple, strong, and fast system for panoptic segmentation, aiming to establish a solid baseline for bottom-up methods that can achieve comparable performance of two-stage methods while yielding fast inference speed. In particular, Panoptic-DeepLab adopts the dual-ASPP and dual-decoder structures specific to semantic, and instance segmentation, respectively. The semantic segmentation branch is the same as the typical design of any semantic segmentation model (e.g., DeepLab), while the instance segmentation branch is class-agnostic, involving a simple instance center regression. As a result, our single Panoptic-DeepLab simultaneously ranks first at all three Cityscapes benchmarks, setting the new state-of-art of 84.2% mIoU, 39.0% AP, and 65.5% PQ on test set. Additionally, equipped with MobileNetV3, Panoptic-DeepLab runs nearly in real-time with a single 1025x2049 image (15.8 frames per second), while achieving a competitive performance on Cityscapes (54.1 PQ% on test set). On Mapillary Vistas test set, our ensemble of six models attains 42.7% PQ, outperforming the challenge winner in 2018 by a healthy margin of 1.5%. Finally, our Panoptic-DeepLab also performs on par with several top-down approaches on the challenging COCO dataset. For the first time, we demonstrate a bottom-up approach could deliver state-of-the-art results on panoptic segmentation.



### Multi-source Distilling Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1911.11554v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.11554v2)
- **Published**: 2019-11-22 19:30:15+00:00
- **Updated**: 2020-02-07 18:21:22+00:00
- **Authors**: Sicheng Zhao, Guangzhi Wang, Shanghang Zhang, Yang Gu, Yaxian Li, Zhichao Song, Pengfei Xu, Runbo Hu, Hua Chai, Kurt Keutzer
- **Comment**: Accepted by AAAI 2020
- **Journal**: None
- **Summary**: Deep neural networks suffer from performance decay when there is domain shift between the labeled source domain and unlabeled target domain, which motivates the research on domain adaptation (DA). Conventional DA methods usually assume that the labeled data is sampled from a single source distribution. However, in practice, labeled data may be collected from multiple sources, while naive application of the single-source DA algorithms may lead to suboptimal solutions. In this paper, we propose a novel multi-source distilling domain adaptation (MDDA) network, which not only considers the different distances among multiple sources and the target, but also investigates the different similarities of the source samples to the target ones. Specifically, the proposed MDDA includes four stages: (1) pre-train the source classifiers separately using the training data from each source; (2) adversarially map the target into the feature space of each source respectively by minimizing the empirical Wasserstein distance between source and target; (3) select the source training samples that are closer to the target to fine-tune the source classifiers; and (4) classify each encoded target feature by corresponding source classifier, and aggregate different predictions using respective domain weight, which corresponds to the discrepancy between each source and target. Extensive experiments are conducted on public DA benchmarks, and the results demonstrate that the proposed MDDA significantly outperforms the state-of-the-art approaches. Our source code is released at: https://github.com/daoyuan98/MDDA.



### ViewSynth: Learning Local Features from Depth using View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1911.10248v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10248v4)
- **Published**: 2019-11-22 21:01:33+00:00
- **Updated**: 2020-09-01 21:19:21+00:00
- **Authors**: Jisan Mahmud, Rajat Vikram Singh, Peri Akiva, Spondon Kundu, Kuan-Chuan Peng, Jan-Michael Frahm
- **Comment**: Accepted to BMVC 2020
- **Journal**: None
- **Summary**: The rapid development of inexpensive commodity depth sensors has made keypoint detection and matching in the depth image modality an important problem in computer vision. Despite great improvements in recent RGB local feature learning methods, adapting them directly in the depth modality leads to unsatisfactory performance. Most of these methods do not explicitly reason beyond the visible pixels in the images. To address the limitations of these methods, we propose a framework ViewSynth, to jointly learn: (1) viewpoint invariant keypoint-descriptor from depth images using a proposed Contrastive Matching Loss, and (2) view synthesis of depth images from different viewpoints using the proposed View Synthesis Module and View Synthesis Loss. By learning view synthesis, we explicitly encourage the feature extractor to encode information about not only the visible, but also the occluded parts of the scene. We demonstrate that in the depth modality, ViewSynth outperforms the state-of-the-art depth and RGB local feature extraction techniques in the 3D keypoint matching and camera localization tasks on the RGB-D datasets 7-Scenes, TUM RGBD and CoRBS in most scenarios. We also show the generalizability of ViewSynth in 3D keypoint matching across different datasets.



### Real-Time 3D Model Tracking in Color and Depth on a Single CPU Core
- **Arxiv ID**: http://arxiv.org/abs/1911.10249v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10249v1)
- **Published**: 2019-11-22 21:02:24+00:00
- **Updated**: 2019-11-22 21:02:24+00:00
- **Authors**: Wadim Kehl, Federico Tombari, Slobodan Ilic, Nassir Navab
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: We present a novel method to track 3D models in color and depth data. To this end, we introduce approximations that accelerate the state-of-the-art in region-based tracking by an order of magnitude while retaining similar accuracy. Furthermore, we show how the method can be made more robust in the presence of depth data and consequently formulate a new joint contour and ICP tracking energy. We present better results than the state-of-the-art while being much faster then most other methods and achieving all of the above on a single CPU core.



### Enhancing Cross-task Black-Box Transferability of Adversarial Examples with Dispersion Reduction
- **Arxiv ID**: http://arxiv.org/abs/1911.11616v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CR, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.11616v1)
- **Published**: 2019-11-22 23:08:17+00:00
- **Updated**: 2019-11-22 23:08:17+00:00
- **Authors**: Yantao Lu, Yunhan Jia, Jianyu Wang, Bai Li, Weiheng Chai, Lawrence Carin, Senem Velipasalar
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1905.03333
- **Journal**: None
- **Summary**: Neural networks are known to be vulnerable to carefully crafted adversarial examples, and these malicious samples often transfer, i.e., they remain adversarial even against other models. Although great efforts have been delved into the transferability across models, surprisingly, less attention has been paid to the cross-task transferability, which represents the real-world cybercriminal's situation, where an ensemble of different defense/detection mechanisms need to be evaded all at once. In this paper, we investigate the transferability of adversarial examples across a wide range of real-world computer vision tasks, including image classification, object detection, semantic segmentation, explicit content detection, and text detection. Our proposed attack minimizes the ``dispersion'' of the internal feature map, which overcomes existing attacks' limitation of requiring task-specific loss functions and/or probing a target model. We conduct evaluation on open source detection and segmentation models as well as four different computer vision tasks provided by Google Cloud Vision (GCV) APIs, to show how our approach outperforms existing attacks by degrading performance of multiple CV tasks by a large margin with only modest perturbations linf=16.



