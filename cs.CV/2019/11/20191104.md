# Arxiv Papers in cs.CV on 2019-11-04
### Multiple Futures Prediction
- **Arxiv ID**: http://arxiv.org/abs/1911.00997v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.MA, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.00997v2)
- **Published**: 2019-11-04 00:42:01+00:00
- **Updated**: 2019-12-06 23:36:01+00:00
- **Authors**: Yichuan Charlie Tang, Ruslan Salakhutdinov
- **Comment**: In proceedings of NeurIPS 2019, Vancouver, British Columbia, Canada
- **Journal**: None
- **Summary**: Temporal prediction is critical for making intelligent and robust decisions in complex dynamic environments. Motion prediction needs to model the inherently uncertain future which often contains multiple potential outcomes, due to multi-agent interactions and the latent goals of others. Towards these goals, we introduce a probabilistic framework that efficiently learns latent variables to jointly model the multi-step future motions of agents in a scene. Our framework is data-driven and learns semantically meaningful latent variables to represent the multimodal future, without requiring explicit labels. Using a dynamic attention-based state encoder, we learn to encode the past as well as the future interactions among agents, efficiently scaling to any number of agents. Finally, our model can be used for planning via computing a conditional probability density over the trajectories of other agents given a hypothetical rollout of the 'self' agent. We demonstrate our algorithms by predicting vehicle trajectories of both simulated and real data, demonstrating the state-of-the-art results on several vehicle trajectory datasets.



### Rolling-Shutter Modelling for Direct Visual-Inertial Odometry
- **Arxiv ID**: http://arxiv.org/abs/1911.01015v1
- **DOI**: 10.1109/IROS40897.2019.8968539
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.01015v1)
- **Published**: 2019-11-04 02:54:15+00:00
- **Updated**: 2019-11-04 02:54:15+00:00
- **Authors**: David Schubert, Nikolaus Demmel, Lukas von Stumberg, Vladyslav Usenko, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: We present a direct visual-inertial odometry (VIO) method which estimates the motion of the sensor setup and sparse 3D geometry of the environment based on measurements from a rolling-shutter camera and an inertial measurement unit (IMU).   The visual part of the system performs a photometric bundle adjustment on a sparse set of points. This direct approach does not extract feature points and is able to track not only corners, but any pixels with sufficient gradient magnitude. Neglecting rolling-shutter effects in the visual part severely degrades accuracy and robustness of the system. In this paper, we incorporate a rolling-shutter model into the photometric bundle adjustment that estimates a set of recent keyframe poses and the inverse depth of a sparse set of points.   IMU information is accumulated between several frames using measurement preintegration, and is inserted into the optimization as an additional constraint between selected keyframes. For every keyframe we estimate not only the pose but also velocity and biases to correct the IMU measurements. Unlike systems with global-shutter cameras, we use both IMU measurements and rolling-shutter effects of the camera to estimate velocity and biases for every state.   Last, we evaluate our system on a novel dataset that contains global-shutter and rolling-shutter images, IMU data and ground-truth poses for ten different sequences, which we make publicly available. Evaluation shows that the proposed method outperforms a system where rolling shutter is not modelled and achieves similar accuracy to the global-shutter method on global-shutter data.



### Ternary MobileNets via Per-Layer Hybrid Filter Banks
- **Arxiv ID**: http://arxiv.org/abs/1911.01028v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.01028v1)
- **Published**: 2019-11-04 04:32:59+00:00
- **Updated**: 2019-11-04 04:32:59+00:00
- **Authors**: Dibakar Gope, Jesse Beu, Urmish Thakker, Matthew Mattina
- **Comment**: None
- **Journal**: None
- **Summary**: MobileNets family of computer vision neural networks have fueled tremendous progress in the design and organization of resource-efficient architectures in recent years. New applications with stringent real-time requirements on highly constrained devices require further compression of MobileNets-like already compute-efficient networks. Model quantization is a widely used technique to compress and accelerate neural network inference and prior works have quantized MobileNets to 4-6 bits albeit with a modest to significant drop in accuracy. While quantization to sub-byte values (i.e. precision less than or equal to 8 bits) has been valuable, even further quantization of MobileNets to binary or ternary values is necessary to realize significant energy savings and possibly runtime speedups on specialized hardware, such as ASICs and FPGAs. Under the key observation that convolutional filters at each layer of a deep neural network may respond differently to ternary quantization, we propose a novel quantization method that generates per-layer hybrid filter banks consisting of full-precision and ternary weight filters for MobileNets. The layer-wise hybrid filter banks essentially combine the strengths of full-precision and ternary weight filters to derive a compact, energy-efficient architecture for MobileNets. Using this proposed quantization method, we quantized a substantial portion of weight filters of MobileNets to ternary values resulting in 27.98% savings in energy, and a 51.07% reduction in the model size, while achieving comparable accuracy and no degradation in throughput on specialized hardware in comparison to the baseline full-precision MobileNets.



### FCSR-GAN: Joint Face Completion and Super-resolution via Multi-task Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.01045v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.01045v1)
- **Published**: 2019-11-04 06:35:30+00:00
- **Updated**: 2019-11-04 06:35:30+00:00
- **Authors**: Jiancheng Cai, Hu Han, Shiguang Shan, Xilin Chen
- **Comment**: To appear in IEEE Trans. BIOM
- **Journal**: None
- **Summary**: Combined variations containing low-resolution and occlusion often present in face images in the wild, e.g., under the scenario of video surveillance. While most of the existing face image recovery approaches can handle only one type of variation per model, in this work, we propose a deep generative adversarial network (FCSR-GAN) for performing joint face completion and face super-resolution via multi-task learning. The generator of FCSR-GAN aims to recover a high-resolution face image without occlusion given an input low-resolution face image with occlusion. The discriminator of FCSR-GAN uses a set of carefully designed losses (an adversarial loss, a perceptual loss, a pixel loss, a smooth loss, a style loss, and a face prior loss) to assure the high quality of the recovered high-resolution face images without occlusion. The whole network of FCSR-GAN can be trained end-to-end using our two-stage training strategy. Experimental results on the public-domain CelebA and Helen databases show that the proposed approach outperforms the state-of-the-art methods in jointly performing face super-resolution (up to 8 $\times$) and face completion, and shows good generalization ability in cross-database testing. Our FCSR-GAN is also useful for improving face identification performance when there are low-resolution and occlusion in face images.



### Deep Heterogeneous Hashing for Face Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1911.01048v1
- **DOI**: 10.1109/TIP.2019.2940683
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.01048v1)
- **Published**: 2019-11-04 07:00:59+00:00
- **Updated**: 2019-11-04 07:00:59+00:00
- **Authors**: Shishi Qiao, Ruiping Wang, Shiguang Shan, Xilin Chen
- **Comment**: 14 pages, 17 figures, 4 tables, accepted by IEEE Transactions on
  Image Processing (TIP) 2019
- **Journal**: IEEE Transactions on Image Processing 2019
- **Summary**: Retrieving videos of a particular person with face image as a query via hashing technique has many important applications. While face images are typically represented as vectors in Euclidean space, characterizing face videos with some robust set modeling techniques (e.g. covariance matrices as exploited in this study, which reside on Riemannian manifold), has recently shown appealing advantages. This hence results in a thorny heterogeneous spaces matching problem. Moreover, hashing with handcrafted features as done in many existing works is clearly inadequate to achieve desirable performance for this task. To address such problems, we present an end-to-end Deep Heterogeneous Hashing (DHH) method that integrates three stages including image feature learning, video modeling, and heterogeneous hashing in a single framework, to learn unified binary codes for both face images and videos. To tackle the key challenge of hashing on the manifold, a well-studied Riemannian kernel mapping is employed to project data (i.e. covariance matrices) into Euclidean space and thus enables to embed the two heterogeneous representations into a common Hamming space, where both intra-space discriminability and inter-space compatibility are considered. To perform network optimization, the gradient of the kernel mapping is innovatively derived via structured matrix backpropagation in a theoretically principled way. Experiments on three challenging datasets show that our method achieves quite competitive performance compared with existing hashing methods.



### Eye Semantic Segmentation with a Lightweight Model
- **Arxiv ID**: http://arxiv.org/abs/1911.01049v1
- **DOI**: 10.1109/ACCESS.2020.3010011
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.01049v1)
- **Published**: 2019-11-04 07:04:14+00:00
- **Updated**: 2019-11-04 07:04:14+00:00
- **Authors**: Van Thong Huynh, Soo-Hyung Kim, Guee-Sang Lee, Hyung-Jeong Yang
- **Comment**: To appear in ICCVW 2019. Pre-trained models and source code are
  available https://github.com/th2l/Eye_VR_Segmentation
- **Journal**: None
- **Summary**: In this paper, we present a multi-class eye segmentation method that can run the hardware limitations for real-time inference. Our approach includes three major stages: get a grayscale image from the input, segment three distinct eye region with a deep network, and remove incorrect areas with heuristic filters. Our model based on the encoder decoder structure with the key is the depthwise convolution operation to reduce the computation cost. We experiment on OpenEDS, a large scale dataset of eye images captured by a head-mounted display with two synchronized eye facing cameras. We achieved the mean intersection over union (mIoU) of 94.85% with a model of size 0.4 megabytes. The source code are available https://github.com/th2l/Eye_VR_Segmentation



### Scene Text Recognition with Temporal Convolutional Encoder
- **Arxiv ID**: http://arxiv.org/abs/1911.01051v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.01051v2)
- **Published**: 2019-11-04 07:10:11+00:00
- **Updated**: 2020-02-16 05:32:08+00:00
- **Authors**: Xiangcheng Du, Tianlong Ma, Yingbin Zheng, Hao Ye, Xingjiao Wu, Liang He
- **Comment**: None
- **Journal**: IEEE ICASSP 2020
- **Summary**: Texts from scene images typically consist of several characters and exhibit a characteristic sequence structure. Existing methods capture the structure with the sequence-to-sequence models by an encoder to have the visual representations and then a decoder to translate the features into the label sequence. In this paper, we study text recognition framework by considering the long-term temporal dependencies in the encoder stage. We demonstrate that the proposed Temporal Convolutional Encoder with increased sequential extents improves the accuracy of text recognition. We also study the impact of different attention modules in convolutional blocks for learning accurate text representations. We conduct comparisons on seven datasets and the experiments demonstrate the effectiveness of our proposed approach.



### SoildNet: Soiling Degradation Detection in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1911.01054v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.01054v2)
- **Published**: 2019-11-04 07:13:26+00:00
- **Updated**: 2019-11-21 03:56:41+00:00
- **Authors**: Arindam Das
- **Comment**: Accepted at the NeurIPS 2019 Workshop on Machine Learning for
  Autonomous Driving
- **Journal**: None
- **Summary**: In the field of autonomous driving, camera sensors are extremely prone to soiling because they are located outside of the car and interact with environmental sources of soiling such as rain drops, snow, dust, sand, mud and so on. This can lead to either partial or complete vision degradation. Hence detecting such decay in vision is very important for safety and overall to preserve the functionality of the "autonomous" components in autonomous driving. The contribution of this work involves: 1) Designing a Deep Convolutional Neural Network (DCNN) based baseline network, 2) Exploiting several network remodelling techniques such as employing static and dynamic group convolution, channel reordering to compress the baseline architecture and make it suitable for low power embedded systems with nearly 1 TOPS, 3) Comparing various result metrics of all interim networks dedicated for soiling degradation detection at tile level of size 64 x 64 on input resolution 1280 x 768. The compressed network, is called SoildNet (Sand, snOw, raIn/dIrt, oiL, Dust/muD) that uses only 9.72% trainable parameters of the base network and reduces the model size by more than 7 times with no loss in accuracy



### A Spectral Nonlocal Block for Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.01059v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.01059v4)
- **Published**: 2019-11-04 07:36:36+00:00
- **Updated**: 2020-02-10 09:42:36+00:00
- **Authors**: Lei Zhu, Qi She, Lidan Zhang, Ping Guo
- **Comment**: None
- **Journal**: None
- **Summary**: The nonlocal-based blocks are designed for capturing long-range spatial-temporal dependencies in computer vision tasks. Although having shown excellent performances, they lack the mechanism to encode the rich, structured information among elements in an image. In this paper, to theoretically analyze the property of these nonlocal-based blocks, we provide a unified approach to interpreting them, where we view them as a graph filter generated on a fully-connected graph. When the graph filter is approximated by Chebyshev polynomials, a generalized formulation can be derived for explaining the existing nonlocal-based blocks ($\mathit{e.g.,}$ nonlocal block, nonlocal stage, double attention block). Furthermore, we propose an efficient and robust spectral nonlocal block, which can be flexibly inserted into deep neural networks to catch the long-range dependencies between spatial pixels or temporal frames. Experimental results demonstrate the clear-cut improvements and practical applicabilities of the spectral nonlocal block on image classification (Cifar-10/100, ImageNet), fine-grained image classification (CUB-200), action recognition (UCF-101), and person re-identification (ILID-SVID, Mars, Prid-2011) tasks.



### Temporal Action Localization using Long Short-Term Dependency
- **Arxiv ID**: http://arxiv.org/abs/1911.01060v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.01060v1)
- **Published**: 2019-11-04 07:38:15+00:00
- **Updated**: 2019-11-04 07:38:15+00:00
- **Authors**: Yuan Zhou, Hongru Li, Sun-Yuan Kung
- **Comment**: 12pages, Trans
- **Journal**: None
- **Summary**: Temporal action localization in untrimmed videos is an important but difficult task. Difficulties are encountered in the application of existing methods when modeling temporal structures of videos. In the present study, we developed a novel method, referred to as Gemini Network, for effective modeling of temporal structures and achieving high-performance temporal action localization. The significant improvements afforded by the proposed method are attributable to three major factors. First, the developed network utilizes two subnets for effective modeling of temporal structures. Second, three parallel feature extraction pipelines are used to prevent interference between the extractions of different stage features. Third, the proposed method utilizes auxiliary supervision, with the auxiliary classifier losses affording additional constraints for improving the modeling capability of the network. As a demonstration of its effectiveness, the Gemini Network was used to achieve state-of-the-art temporal action localization performance on two challenging datasets, namely, THUMOS14 and ActivityNet.



### PGU-net+: Progressive Growing of U-net+ for Automated Cervical Nuclei Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.01062v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.01062v3)
- **Published**: 2019-11-04 07:38:59+00:00
- **Updated**: 2019-11-12 04:09:10+00:00
- **Authors**: Jie Zhao, Lei Dai, Mo Zhang, Fei Yu, Meng Li, Hongfeng Li, Wenjia Wang, Li Zhang
- **Comment**: MICCAI workshop MMMI2019 Best Student Paper Award
- **Journal**: None
- **Summary**: Automated cervical nucleus segmentation based on deep learning can effectively improve the quantitative analysis of cervical cancer. However, accurate nuclei segmentation is still challenging. The classic U-net has not achieved satisfactory results on this task, because it mixes the information of different scales that affect each other, which limits the segmentation accuracy of the model. To solve this problem, we propose a progressive growing U-net (PGU-net+) model, which uses two paradigms to extract image features at different scales in a more independent way. First, we add residual modules between different scales of U-net, which enforces the model to learn the approximate shape of the annotation in the coarser scale, and to learn the residual between the annotation and the approximate shape in the finer scale. Second, we start to train the model with the coarsest part and then progressively add finer part to the training until the full model is included. When we train a finer part, we will reduce the learning rate of the previous coarser part, which further ensures that the model independently extracts information from different scales. We conduct several comparative experiments on the Herlev dataset. The experimental results show that the PGU-net+ has superior accuracy than the previous state-of-the-art methods on cervical nuclei segmentation.



### Supervised level-wise pretraining for recurrent neural network initialization in multi-class classification
- **Arxiv ID**: http://arxiv.org/abs/1911.01071v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.01071v1)
- **Published**: 2019-11-04 08:30:01+00:00
- **Updated**: 2019-11-04 08:30:01+00:00
- **Authors**: Dino Ienco, Roberto Interdonato, Raffaele Gaetano
- **Comment**: None
- **Journal**: None
- **Summary**: Recurrent Neural Networks (RNNs) can be seriously impacted by the initial parameters assignment, which may result in poor generalization performances on new unseen data. With the objective to tackle this crucial issue, in the context of RNN based classification, we propose a new supervised layer-wise pretraining strategy to initialize network parameters. The proposed approach leverages a data-aware strategy that sets up a taxonomy of classification problems automatically derived by the model behavior. To the best of our knowledge, despite the great interest in RNN-based classification, this is the first data-aware strategy dealing with the initialization of such models. The proposed strategy has been tested on four benchmarks coming from two different domains, i.e., Speech Recognition and Remote Sensing. Results underline the significance of our approach and point out that data-aware strategies positively support the initialization of Recurrent Neural Network based classification models.



### Technical Report: Co-learning of geometry and semantics for online 3D mapping
- **Arxiv ID**: http://arxiv.org/abs/1911.01082v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1911.01082v1)
- **Published**: 2019-11-04 09:19:17+00:00
- **Updated**: 2019-11-04 09:19:17+00:00
- **Authors**: Marcela Carvalho, Maxime Ferrera, Alexandre Boulch, Julien Moras, Bertrand Le Saux, Pauline Trouvé-Peloux
- **Comment**: None
- **Journal**: None
- **Summary**: This paper is a technical report about our submission for the ECCV 2018 3DRMS Workshop Challenge on Semantic 3D Reconstruction \cite{Tylecek2018rms}. In this paper, we address 3D semantic reconstruction for autonomous navigation using co-learning of depth map and semantic segmentation. The core of our pipeline is a deep multi-task neural network which tightly refines depth and also produces accurate semantic segmentation maps. Its inputs are an image and a raw depth map produced from a pair of images by standard stereo vision. The resulting semantic 3D point clouds are then merged in order to create a consistent 3D mesh, in turn used to produce dense semantic 3D reconstruction maps. The performances of each step of the proposed method are evaluated on the dataset and multiple tasks of the 3DRMS Challenge, and repeatedly surpass state-of-the-art approaches.



### Learning One-Shot Imitation from Humans without Humans
- **Arxiv ID**: http://arxiv.org/abs/1911.01103v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.01103v1)
- **Published**: 2019-11-04 10:07:27+00:00
- **Updated**: 2019-11-04 10:07:27+00:00
- **Authors**: Alessandro Bonardi, Stephen James, Andrew J. Davison
- **Comment**: Videos can be found here:
  https://sites.google.com/view/tecnets-humans
- **Journal**: None
- **Summary**: Humans can naturally learn to execute a new task by seeing it performed by other individuals once, and then reproduce it in a variety of configurations. Endowing robots with this ability of imitating humans from third person is a very immediate and natural way of teaching new tasks. Only recently, through meta-learning, there have been successful attempts to one-shot imitation learning from humans; however, these approaches require a lot of human resources to collect the data in the real world to train the robot. But is there a way to remove the need for real world human demonstrations during training? We show that with Task-Embedded Control Networks, we can infer control polices by embedding human demonstrations that can condition a control policy and achieve one-shot imitation learning. Importantly, we do not use a real human arm to supply demonstrations during training, but instead leverage domain randomisation in an application that has not been seen before: sim-to-real transfer on humans. Upon evaluating our approach on pushing and placing tasks in both simulation and in the real world, we show that in comparison to a system that was trained on real-world data we are able to achieve similar results by utilising only simulation data.



### Singular points detection with semantic segmentation networks
- **Arxiv ID**: http://arxiv.org/abs/1911.01106v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.01106v1)
- **Published**: 2019-11-04 10:10:36+00:00
- **Updated**: 2019-11-04 10:10:36+00:00
- **Authors**: Jiong Chen, Heng Zhao, Zhicheng Cao, Liaojun Pang
- **Comment**: None
- **Journal**: None
- **Summary**: Singular points detection is one of the most classical and important problem in the field of fingerprint recognition. However, current detection rates of singular points are still unsatisfactory, especially for low-quality fingerprints. Compared with traditional image processing-based detection methods, methods based on deep learning only need the original fingerprint image but not the fingerprint orientation field. In this paper, different from other detection methods based on deep learning, we treat singular points detection as a semantic segmentation problem and just use few data for training. Furthermore, we propose a new convolutional neural network called SinNet to extract the singular regions of interest and then use a blob detection method called SimpleBlobDetector to locate the singular points. The experiments are carried out on the test dataset from SPD2010, and the proposed method has much better performance than the other advanced methods in most aspects. Compared with the state-of-art algorithms in SPD2010, our method achieves an increase of 11% in the percentage of correctly detected fingerprints and an increase of more than 18% in the core detection rate.



### Automated Estimation of the Spinal Curvature via Spine Centerline Extraction with Ensembles of Cascaded Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.01126v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.01126v2)
- **Published**: 2019-11-04 10:57:36+00:00
- **Updated**: 2019-12-11 13:44:30+00:00
- **Authors**: Florian Dubost, Benjamin Collery, Antonin Renaudier, Axel Roc, Nicolas Posocco, Gerda Bortsova, Wiro Niessen, Marleen de Bruijne
- **Comment**: None
- **Journal**: None
- **Summary**: Scoliosis is a condition defined by an abnormal spinal curvature. For diagnosis and treatment planning of scoliosis, spinal curvature can be estimated using Cobb angles. We propose an automated method for the estimation of Cobb angles from X-ray scans. First, the centerline of the spine was segmented using a cascade of two convolutional neural networks. After smoothing the centerline, Cobb angles were automatically estimated using the derivative of the centerline. We evaluated the results using the mean absolute error and the average symmetric mean absolute percentage error between the manual assessment by experts and the automated predictions. For optimization, we used 609 X-ray scans from the London Health Sciences Center, and for evaluation, we participated in the international challenge "Accurate Automated Spinal Curvature Estimation, MICCAI 2019" (100 scans). On the challenge's test set, we obtained an average symmetric mean absolute percentage error of 22.96.



### Disentangling Human Dynamics for Pedestrian Locomotion Forecasting with Noisy Supervision
- **Arxiv ID**: http://arxiv.org/abs/1911.01138v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1911.01138v2)
- **Published**: 2019-11-04 11:30:12+00:00
- **Updated**: 2020-04-13 19:33:42+00:00
- **Authors**: Karttikeya Mangalam, Ehsan Adeli, Kuan-Hui Lee, Adrien Gaidon, Juan Carlos Niebles
- **Comment**: Accepted to WACV 2020 (Oral)
- **Journal**: None
- **Summary**: We tackle the problem of Human Locomotion Forecasting, a task for jointly predicting the spatial positions of several keypoints on the human body in the near future under an egocentric setting. In contrast to the previous work that aims to solve either the task of pose prediction or trajectory forecasting in isolation, we propose a framework to unify the two problems and address the practically useful task of pedestrian locomotion prediction in the wild. Among the major challenges in solving this task is the scarcity of annotated egocentric video datasets with dense annotations for pose, depth, or egomotion. To surmount this difficulty, we use state-of-the-art models to generate (noisy) annotations and propose robust forecasting models that can learn from this noisy supervision. We present a method to disentangle the overall pedestrian motion into easier to learn subparts by utilizing a pose completion and a decomposition module. The completion module fills in the missing key-point annotations and the decomposition module breaks the cleaned locomotion down to global (trajectory) and local (pose keypoint movements). Further, with Quasi RNN as our backbone, we propose a novel hierarchical trajectory forecasting network that utilizes low-level vision domain specific signals like egomotion and depth to predict the global trajectory. Our method leads to state-of-the-art results for the prediction of human locomotion in the egocentric view. Project pade: https://karttikeya.github.io/publication/plf/



### LapNet : Automatic Balanced Loss and Optimal Assignment for Real-Time Dense Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.01149v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.01149v2)
- **Published**: 2019-11-04 12:13:07+00:00
- **Updated**: 2020-03-17 15:18:17+00:00
- **Authors**: Florian Chabot, Quoc-Cuong Pham, Mohamed Chaouch
- **Comment**: None
- **Journal**: None
- **Summary**: Real-time single-stage object detectors based on deep learning still remain less accurate than more complex ones. The trade-off between model performance and computational speed is a major challenge. In this paper, we propose a new way to efficiently learn a single-shot detector which offers a very good compromise between these two objectives. To this end, we introduce LapNet, an anchor based detector, trained end-to-end without any sampling strategy. Our approach aims to overcome two important problems encountered in training an anchor based detector: (1) ambiguity in the assignment of anchor to ground truth and (2) class and object size imbalance. To address the first limitation, we propose a soft positive/negative anchor assignment procedure based on a new overlapping function called "Per-Object Normalized Overlap" (PONO). This soft assignment can be self-corrected by the network itself to avoid ambiguity between close objects. To cope with the second limitation, we propose to learn additional weights, that are not used at inference, to efficiently manage sample imbalance. These two contributions make the detector learning more generic whatever the training dataset. Various experiments show the effectiveness of the proposed approach.



### Field of View Extension in Computed Tomography Using Deep Learning Prior
- **Arxiv ID**: http://arxiv.org/abs/1911.01178v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1911.01178v2)
- **Published**: 2019-11-04 13:11:34+00:00
- **Updated**: 2019-12-09 11:48:59+00:00
- **Authors**: Yixing Huang, Lei Gao, Alexander Preuhs, Andreas Maier
- **Comment**: Submitted to Bildverarbeitung fuer die Medizin 2020
- **Journal**: None
- **Summary**: In computed tomography (CT), data truncation is a common problem. Images reconstructed by the standard filtered back-projection algorithm from truncated data suffer from cupping artifacts inside the field-of-view (FOV), while anatomical structures are severely distorted or missing outside the FOV. Deep learning, particularly the U-Net, has been applied to extend the FOV as a post-processing method. Since image-to-image prediction neglects the data fidelity to measured projection data, incorrect structures, even inside the FOV, might be reconstructed by such an approach. Therefore, generating reconstructed images directly from a post-processing neural network is inadequate. In this work, we propose a data consistent reconstruction method, which utilizes deep learning reconstruction as prior for extrapolating truncated projections and a conventional iterative reconstruction to constrain the reconstruction consistent to measured raw data. Its efficacy is demonstrated in our study, achieving small average root-mean-square error of 24 HU inside the FOV and a high structure similarity index of 0.993 for the whole body area on a test patient's CT data.



### Semi-Supervised Medical Image Segmentation via Learning Consistency under Transformations
- **Arxiv ID**: http://arxiv.org/abs/1911.01218v1
- **DOI**: 10.1007/978-3-030-32226-7_90
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.01218v1)
- **Published**: 2019-11-04 13:51:17+00:00
- **Updated**: 2019-11-04 13:51:17+00:00
- **Authors**: Gerda Bortsova, Florian Dubost, Laurens Hogeweg, Ioannis Katramados, Marleen de Bruijne
- **Comment**: None
- **Journal**: In proceedings of Medical Image Computing and Computer Assisted
  Intervention - MICCAI 2019
- **Summary**: The scarcity of labeled data often limits the application of supervised deep learning techniques for medical image segmentation. This has motivated the development of semi-supervised techniques that learn from a mixture of labeled and unlabeled images. In this paper, we propose a novel semi-supervised method that, in addition to supervised learning on labeled training images, learns to predict segmentations consistent under a given class of transformations on both labeled and unlabeled images. More specifically, in this work we explore learning equivariance to elastic deformations. We implement this through: 1) a Siamese architecture with two identical branches, each of which receives a differently transformed image, and 2) a composite loss function with a supervised segmentation loss term and an unsupervised term that encourages segmentation consistency between the predictions of the two branches. We evaluate the method on a public dataset of chest radiographs with segmentations of anatomical structures using 5-fold cross-validation. The proposed method reaches significantly higher segmentation accuracy compared to supervised learning. This is due to learning transformation consistency on both labeled and unlabeled images, with the latter contributing the most. We achieve the performance comparable to state-of-the-art chest X-ray segmentation methods while using substantially fewer labeled images.



### Superpixel-Based Background Recovery from Multiple Images
- **Arxiv ID**: http://arxiv.org/abs/1911.01223v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.01223v1)
- **Published**: 2019-11-04 13:59:29+00:00
- **Updated**: 2019-11-04 13:59:29+00:00
- **Authors**: Lei Gao, Yixing Huang, Andreas Maier
- **Comment**: Technical Report
- **Journal**: None
- **Summary**: In this paper, we propose an intuitive method to recover background from multiple images. The implementation consists of three stages: model initialization, model update, and background output. We consider the pixels whose values change little in all input images as background seeds. Images are then segmented into superpixels with simple linear iterative clustering. When the number of pixels labelled as background in a superpixel is bigger than a predefined threshold, we label the superpixel as background to initialize the background candidate masks. Background candidate images are obtained from input raw images with the masks. Combining all candidate images, a background image is produced. The background candidate masks, candidate images, and the background image are then updated alternately until convergence. Finally, ghosting artifacts is removed with the k-nearest neighbour method. An experiment on an outdoor dataset demonstrates that the proposed algorithm can achieve promising results.



### AIM 2019 Challenge on Constrained Super-Resolution: Methods and Results
- **Arxiv ID**: http://arxiv.org/abs/1911.01249v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.01249v1)
- **Published**: 2019-11-04 14:39:51+00:00
- **Updated**: 2019-11-04 14:39:51+00:00
- **Authors**: Kai Zhang, Shuhang Gu, Radu Timofte, Zheng Hui, Xiumei Wang, Xinbo Gao, Dongliang Xiong, Shuai Liu, Ruipeng Gang, Nan Nan, Chenghua Li, Xueyi Zou, Ning Kang, Zhan Wang, Hang Xu, Chaofeng Wang, Zheng Li, Linlin Wang, Jun Shi, Wenyu Sun, Zhiqiang Lang, Jiangtao Nie, Wei Wei, Lei Zhang, Yazhe Niu, Peijin Zhuo, Xiangzhen Kong, Long Sun, Wenhao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper reviews the AIM 2019 challenge on constrained example-based single image super-resolution with focus on proposed solutions and results. The challenge had 3 tracks. Taking the three main aspects (i.e., number of parameters, inference/running time, fidelity (PSNR)) of MSRResNet as the baseline, Track 1 aims to reduce the amount of parameters while being constrained to maintain or improve the running time and the PSNR result, Tracks 2 and 3 aim to optimize running time and PSNR result with constrain of the other two aspects, respectively. Each track had an average of 64 registered participants, and 12 teams submitted the final results. They gauge the state-of-the-art in single image super-resolution.



### Synthetic Video Generation for Robust Hand Gesture Recognition in Augmented Reality Applications
- **Arxiv ID**: http://arxiv.org/abs/1911.01320v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.01320v3)
- **Published**: 2019-11-04 16:32:07+00:00
- **Updated**: 2019-12-06 03:15:32+00:00
- **Authors**: Varun Jain, Shivam Aggarwal, Suril Mehta, Ramya Hebbalaguppe
- **Comment**: Presented at the ICCV 2019 Workshop: The 5th International Workshop
  on Observing And Understanding Hands In Action
- **Journal**: None
- **Summary**: Hand gestures are a natural means of interaction in Augmented Reality and Virtual Reality (AR/VR) applications. Recently, there has been an increased focus on removing the dependence of accurate hand gesture recognition on complex sensor setup found in expensive proprietary devices such as the Microsoft HoloLens, Daqri and Meta Glasses. Most such solutions either rely on multi-modal sensor data or deep neural networks that can benefit greatly from abundance of labelled data. Datasets are an integral part of any deep learning based research. They have been the principal reason for the substantial progress in this field, both, in terms of providing enough data for the training of these models, and, for benchmarking competing algorithms. However, it is becoming increasingly difficult to generate enough labelled data for complex tasks such as hand gesture recognition. The goal of this work is to introduce a framework capable of generating photo-realistic videos that have labelled hand bounding box and fingertip that can help in designing, training, and benchmarking models for hand-gesture recognition in AR/VR applications. We demonstrate the efficacy of our framework in generating videos with diverse backgrounds.



### Using image-extracted features to determine heart rate and blink duration for driver sleepiness detection
- **Arxiv ID**: http://arxiv.org/abs/1911.01333v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.01333v2)
- **Published**: 2019-11-04 16:52:28+00:00
- **Updated**: 2019-12-08 13:12:58+00:00
- **Authors**: Erfan Darzi, Armin Mohammadie-Zand, Hamid Soltanian-Zadeh
- **Comment**: None
- **Journal**: IEEE International Conference on Biomedical Engineering, 2019
- **Summary**: Heart rate and blink duration are two vital physiological signals which give information about cardiac activity and consciousness. Monitoring these two signals is crucial for various applications such as driver drowsiness detection. As there are several problems posed by the conventional systems to be used for continuous, long-term monitoring, a remote blink and ECG monitoring system can be used as an alternative. For estimating the blink duration, two strategies are used. In the first approach, pictures of open and closed eyes are fed into an Artificial Neural Network (ANN) to decide whether the eyes are open or close. In the second approach, they are classified and labeled using Linear Discriminant Analysis (LDA). The labeled images are then be used to determine the blink duration. For heart rate variability, two strategies are used to evaluate the passing blood volume: Independent Component Analysis (ICA); and a chrominance based method. Eye recognition yielded 78-92% accuracy in classifying open/closed eyes with ANN and 71-91% accuracy with LDA. Heart rate evaluations had a mean loss of around 16 Beats Per Minute (BPM) for the ICA strategy and 13 BPM for the chrominance based technique.



### Improved BiGAN training with marginal likelihood equalization
- **Arxiv ID**: http://arxiv.org/abs/1911.01425v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.01425v2)
- **Published**: 2019-11-04 17:02:20+00:00
- **Updated**: 2020-05-23 10:38:03+00:00
- **Authors**: Pablo Sánchez-Martín, Pablo M. Olmos, Fernando Perez-Cruz
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel training procedure for improving the performance of generative adversarial networks (GANs), especially to bidirectional GANs. First, we enforce that the empirical distribution of the inverse inference network matches the prior distribution, which favors the generator network reproducibility on the seen samples. Second, we have found that the marginal log-likelihood of the samples shows a severe overrepresentation of a certain type of samples. To address this issue, we propose to train the bidirectional GAN using a non-uniform sampling for the mini-batch selection, resulting in improved quality and variety in generated samples measured quantitatively and by visual inspection. We illustrate our new procedure with the well-known CIFAR10, Fashion MNIST and CelebA datasets.



### CloudifierNet -- Deep Vision Models for Artificial Image Processing
- **Arxiv ID**: http://arxiv.org/abs/1911.01346v2
- **DOI**: 10.1016/j.procs.2019.12.043
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.01346v2)
- **Published**: 2019-11-04 17:16:35+00:00
- **Updated**: 2020-07-28 11:13:43+00:00
- **Authors**: Andrei Damian, Laurentiu Piciu, Alexandru Purdila, Nicolae Tapus
- **Comment**: ITQM 2019
- **Journal**: None
- **Summary**: Today, more and more, it is necessary that most applications and documents developed in previous or current technologies to be accessible online on cloud-based infrastructures. That is why the migration of legacy systems including their hosts of documents to new technologies and online infrastructures, using modern Artificial Intelligence techniques, is absolutely necessary. With the advancement of Artificial Intelligence and Deep Learning with its multitude of applications, a new area of research is emerging - that of automated systems development and maintenance. The underlying work objective that led to this paper aims to research and develop truly intelligent systems able to analyze user interfaces from various sources and generate real and usable inferences ranging from architecture analysis to actual code generation. One key element of such systems is that of artificial scene detection and analysis based on deep learning computer vision systems. Computer vision models and particularly deep directed acyclic graphs based on convolutional modules are generally constructed and trained based on natural images datasets. Due to this fact, the models will develop during the training process natural image feature detectors apart from the base graph modules that will learn basic primitive features. In the current paper, we will present the base principles of a deep neural pipeline for computer vision applied to artificial scenes (scenes generated by user interfaces or similar). Finally, we will present the conclusions based on experimental development and benchmarking against state-of-the-art transfer-learning implemented deep vision models.



### Self-Supervised Difference Detection for Weakly-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.01370v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.01370v2)
- **Published**: 2019-11-04 17:57:23+00:00
- **Updated**: 2019-11-12 14:40:59+00:00
- **Authors**: Wataru Shimoda, Keiji Yanai
- **Comment**: ICCV 2019, source codes: https://github.com/shimoda-uec/ssdd
- **Journal**: None
- **Summary**: To minimize the annotation costs associated with the training of semantic segmentation models, researchers have extensively investigated weakly-supervised segmentation approaches. In the current weakly-supervised segmentation methods, the most widely adopted approach is based on visualization. However, the visualization results are not generally equal to semantic segmentation. Therefore, to perform accurate semantic segmentation under the weakly supervised condition, it is necessary to consider the mapping functions that convert the visualization results into semantic segmentation. For such mapping functions, the conditional random field and iterative re-training using the outputs of a segmentation model are usually used. However, these methods do not always guarantee improvements in accuracy; therefore, if we apply these mapping functions iteratively multiple times, eventually the accuracy will not improve or will decrease.   In this paper, to make the most of such mapping functions, we assume that the results of the mapping function include noise, and we improve the accuracy by removing noise. To achieve our aim, we propose the self-supervised difference detection module, which estimates noise from the results of the mapping functions by predicting the difference between the segmentation masks before and after the mapping. We verified the effectiveness of the proposed method by performing experiments on the PASCAL Visual Object Classes 2012 dataset, and we achieved 64.9\% in the val set and 65.5\% in the test set. Both of the results become new state-of-the-art under the same setting of weakly supervised semantic segmentation.



### CANet: Cross-disease Attention Network for Joint Diabetic Retinopathy and Diabetic Macular Edema Grading
- **Arxiv ID**: http://arxiv.org/abs/1911.01376v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.01376v1)
- **Published**: 2019-11-04 18:06:35+00:00
- **Updated**: 2019-11-04 18:06:35+00:00
- **Authors**: Xiaomeng Li, Xiaowei Hu, Lequan Yu, Lei Zhu, Chi-Wing Fu, Pheng-Ann Heng
- **Comment**: IEEE Transactions on Medical Imaging; code is at
  https://github.com/xmengli999/CANet
- **Journal**: None
- **Summary**: Diabetic retinopathy (DR) and diabetic macular edema (DME) are the leading causes of permanent blindness in the working-age population. Automatic grading of DR and DME helps ophthalmologists design tailored treatments to patients, thus is of vital importance in the clinical practice. However, prior works either grade DR or DME, and ignore the correlation between DR and its complication, i.e., DME. Moreover, the location information, e.g., macula and soft hard exhaust annotations, are widely used as a prior for grading. Such annotations are costly to obtain, hence it is desirable to develop automatic grading methods with only image-level supervision. In this paper, we present a novel cross-disease attention network (CANet) to jointly grade DR and DME by exploring the internal relationship between the diseases with only image-level supervision. Our key contributions include the disease-specific attention module to selectively learn useful features for individual diseases, and the disease-dependent attention module to further capture the internal relationship between the two diseases. We integrate these two attention modules in a deep network to produce disease-specific and disease-dependent features, and to maximize the overall performance jointly for grading DR and DME. We evaluate our network on two public benchmark datasets, i.e., ISBI 2018 IDRiD challenge dataset and Messidor dataset. Our method achieves the best result on the ISBI 2018 IDRiD challenge dataset and outperforms other methods on the Messidor dataset. Our code is publicly available at https://github.com/xmengli999/CANet.



### VASTA: A Vision and Language-assisted Smartphone Task Automation System
- **Arxiv ID**: http://arxiv.org/abs/1911.01474v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.01474v1)
- **Published**: 2019-11-04 20:21:32+00:00
- **Updated**: 2019-11-04 20:21:32+00:00
- **Authors**: Alborz Rezazadeh Sereshkeh, Gary Leung, Krish Perumal, Caleb Phillips, Minfan Zhang, Afsaneh Fazly, Iqbal Mohomed
- **Comment**: Submitted to ACM IUI'20, 10 figures, 11 pages
- **Journal**: None
- **Summary**: We present VASTA, a novel vision and language-assisted Programming By Demonstration (PBD) system for smartphone task automation. Development of a robust PBD automation system requires overcoming three key challenges: first, how to make a particular demonstration robust to positional and visual changes in the user interface (UI) elements; secondly, how to recognize changes in the automation parameters to make the demonstration as generalizable as possible; and thirdly, how to recognize from the user utterance what automation the user wishes to carry out. To address the first challenge, VASTA leverages state-of-the-art computer vision techniques, including object detection and optical character recognition, to accurately label interactions demonstrated by a user, without relying on the underlying UI structures. To address the second and third challenges, VASTA takes advantage of advanced natural language understanding algorithms for analyzing the user utterance to trigger the VASTA automation scripts, and to determine the automation parameters for generalization. We run an initial user study that demonstrates the effectiveness of VASTA at clustering user utterances, understanding changes in the automation parameters, detecting desired UI elements, and, most importantly, automating various tasks. A demo video of the system is available here: http://y2u.be/kr2xE-FixjI



### Evolution-based Fine-tuning of CNNs for Prostate Cancer Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.01477v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1911.01477v1)
- **Published**: 2019-11-04 20:40:29+00:00
- **Updated**: 2019-11-04 20:40:29+00:00
- **Authors**: Khashayar Namdar, Isha Gujrathi, Masoom A. Haider, Farzad Khalvati
- **Comment**: Accepted for the 33rd Conference on Neural Information Processing
  Systems (NeurIPS 2019), Medical Imaging Meets NEURIPS Workshop
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have been used for automated detection of prostate cancer where Area Under Receiver Operating Characteristic (ROC) curve (AUC) is usually used as the performance metric. Given that AUC is not differentiable, common practice is to train the CNN using a loss functions based on other performance metrics such as cross entropy and monitoring AUC to select the best model. In this work, we propose to fine-tune a trained CNN for prostate cancer detection using a Genetic Algorithm to achieve a higher AUC. Our dataset contained 6-channel Diffusion-Weighted MRI slices of prostate. On a cohort of 2,955 training, 1,417 validation, and 1,334 test slices, we reached test AUC of 0.773; a 9.3% improvement compared to the base CNN model.



### Minimal Solvers for Rectifying from Radially-Distorted Conjugate Translations
- **Arxiv ID**: http://arxiv.org/abs/1911.01507v4
- **DOI**: 10.1109/TPAMI.2020.2992261
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.01507v4)
- **Published**: 2019-11-04 22:07:56+00:00
- **Updated**: 2020-11-21 16:00:10+00:00
- **Authors**: James Pritts, Zuzana Kukelova, Viktor Larsson, Yaroslava Lochman, Ondřej Chum
- **Comment**: None
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (2020) 1-1
- **Summary**: This paper introduces minimal solvers that jointly solve for radial lens undistortion and affine-rectification using local features extracted from the image of coplanar translated and reflected scene texture, which is common in man-made environments. The proposed solvers accommodate different types of local features and sampling strategies, and three of the proposed variants require just one feature correspondence. State-of-the-art techniques from algebraic geometry are used to simplify the formulation of the solvers. The generated solvers are stable, small and fast. Synthetic and real-image experiments show that the proposed solvers have superior robustness to noise compared to the state of the art. The solvers are integrated with an automated system for rectifying imaged scene planes from coplanar repeated texture. Accurate rectifications on challenging imagery taken with narrow to wide field-of-view lenses demonstrate the applicability of the proposed solvers.



### SAMM Long Videos: A Spontaneous Facial Micro- and Macro-Expressions Dataset
- **Arxiv ID**: http://arxiv.org/abs/1911.01519v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.01519v2)
- **Published**: 2019-11-04 22:31:30+00:00
- **Updated**: 2020-02-28 17:31:34+00:00
- **Authors**: Chuin Hong Yap, Connah Kendrick, Moi Hoon Yap
- **Comment**: None
- **Journal**: None
- **Summary**: With the growth of popularity of facial micro-expressions in recent years, the demand for long videos with micro- and macro-expressions remains high. Extended from SAMM, a micro-expressions dataset released in 2016, this paper presents SAMM Long Videos dataset for spontaneous micro- and macro-expressions recognition and spotting. SAMM Long Videos dataset consists of 147 long videos with 343 macro-expressions and 159 micro-expressions. The dataset is FACS-coded with detailed Action Units (AUs). We compare our dataset with Chinese Academy of Sciences Macro-Expressions and Micro-Expressions (CAS(ME)2) dataset, which is the only available fully annotated dataset with micro- and macro-expressions. Furthermore, we preprocess the long videos using OpenFace, which includes face alignment and detection of facial AUs. We conduct facial expression spotting using this dataset and compare it with the baseline of MEGC III. Our spotting method outperformed the baseline result with F1-score of 0.3299.



### Closing the Reality Gap with Unsupervised Sim-to-Real Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1911.01529v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1911.01529v2)
- **Published**: 2019-11-04 23:17:03+00:00
- **Updated**: 2021-06-15 10:39:20+00:00
- **Authors**: Jan Blumenkamp, Andreas Baude, Tim Laue
- **Comment**: Accepted to RoboCup Symposium 2021
- **Journal**: None
- **Summary**: Deep learning approaches have become the standard solution to many problems in computer vision and robotics, but obtaining sufficient training data in high enough quality is challenging, as human labor is error prone, time consuming, and expensive. Solutions based on simulation have become more popular in recent years, but the gap between simulation and reality is still a major issue. In this paper, we introduce a novel method for augmenting synthetic image data through unsupervised image-to-image translation by applying the style of real world images to simulated images with open source frameworks. The generated dataset is combined with conventional augmentation methods and is then applied to a neural network model running in real-time on autonomous soccer robots. Our evaluation shows a significant improvement compared to models trained on images generated entirely in simulation.



