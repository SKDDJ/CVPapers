# Arxiv Papers in cs.CV on 2019-11-07
### Model Adaption Object Detection System for Robot
- **Arxiv ID**: http://arxiv.org/abs/1911.02718v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1911.02718v2)
- **Published**: 2019-11-07 02:20:36+00:00
- **Updated**: 2019-11-20 05:08:20+00:00
- **Authors**: Jingwen Fu, Licheng Zong, Yinbing Li, Ke Li, Bingqian Yang, Xibei Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection for robot guidance is a crucial mission for autonomous robots, which has provoked extensive attention for researchers. However, the changing view of robot movement and limited available data hinder the research in this area. To address these matters, we proposed a new vision system for robots, the model adaptation object detection system. Instead of using a single one to solve problems, We made use of different object detection neural networks to guide the robot in accordance with various situations, with the help of a meta neural network to allocate the object detection neural networks. Furthermore, taking advantage of transfer learning technology and depthwise separable convolutions, our model is easy to train and can address small dataset problems.



### Fast Polynomial Approximation of Heat Kernel Convolution on Manifolds and Its Application to Brain Sulcal and Gyral Graph Pattern Analysis
- **Arxiv ID**: http://arxiv.org/abs/1911.02721v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.02721v2)
- **Published**: 2019-11-07 02:27:59+00:00
- **Updated**: 2020-01-17 18:09:51+00:00
- **Authors**: Shih-Gu Huang, Ilwoo Lyu, Anqi Qiu, Moo K. Chung
- **Comment**: Accepted for publication
- **Journal**: IEEE Transactions on Medical Imaging, 2020
- **Summary**: Heat diffusion has been widely used in brain imaging for surface fairing, mesh regularization and cortical data smoothing. Motivated by diffusion wavelets and convolutional neural networks on graphs, we present a new fast and accurate numerical scheme to solve heat diffusion on surface meshes. This is achieved by approximating the heat kernel convolution using high degree orthogonal polynomials in the spectral domain. We also derive the closed-form expression of the spectral decomposition of the Laplace-Beltrami operator and use it to solve heat diffusion on a manifold for the first time. The proposed fast polynomial approximation scheme avoids solving for the eigenfunctions of the Laplace-Beltrami operator, which is computationally costly for large mesh size, and the numerical instability associated with the finite element method based diffusion solvers. The proposed method is applied in localizing the male and female differences in cortical sulcal and gyral graph patterns obtained from MRI in an innovative way. The MATLAB code is available at http://www.stat.wisc.edu/~mchung/chebyshev.



### Analysis of CNN-based remote-PPG to understand limitations and sensitivities
- **Arxiv ID**: http://arxiv.org/abs/1911.02736v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.02736v2)
- **Published**: 2019-11-07 03:09:43+00:00
- **Updated**: 2020-02-05 06:46:06+00:00
- **Authors**: Qi Zhan, Wenjin Wang, Gerard de Haan
- **Comment**: Biomedical Optics Express journal submission. 15 pages,11 figures
- **Journal**: None
- **Summary**: Deep learning based on Convolutional Neural Network (CNN) has shown promising results in various vision-based applications, recently also in camera-based vital signs monitoring. The CNN-based Photoplethysmography (PPG) extraction has, so far, been focused on performance rather than understanding. In this paper, we try to answer four questions with experiments aiming at improving our understanding of this methodology as it gains popularity. We conclude that the network exploits the blood absorption variation to extract the physiological signals, and that the choice and parameters (phase, spectral content, etc.) of the reference-signal may be more critical than anticipated. The availability of multiple convolutional kernels is necessary for CNN to arrive at a flexible channel combination through the spatial operation, but may not provide the same motion-robustness as a multi-site measurement using knowledge-based PPG extraction. Finally, we conclude that the PPG-related prior knowledge is still helpful for the CNN-based PPG extraction. Consequently, we recommend further investigation of hybrid CNN-based methods to include prior knowledge in their design.



### DCA: Diversified Co-Attention towards Informative Live Video Commenting
- **Arxiv ID**: http://arxiv.org/abs/1911.02739v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.02739v3)
- **Published**: 2019-11-07 03:28:38+00:00
- **Updated**: 2020-08-08 13:37:10+00:00
- **Authors**: Zhihan Zhang, Zhiyi Yin, Shuhuai Ren, Xinhang Li, Shicheng Li
- **Comment**: None
- **Journal**: None
- **Summary**: We focus on the task of Automatic Live Video Commenting (ALVC), which aims to generate real-time video comments with both video frames and other viewers' comments as inputs. A major challenge in this task is how to properly leverage the rich and diverse information carried by video and text. In this paper, we aim to collect diversified information from video and text for informative comment generation. To achieve this, we propose a Diversified Co-Attention (DCA) model for this task. Our model builds bidirectional interactions between video frames and surrounding comments from multiple perspectives via metric learning, to collect a diversified and informative context for comment generation. We also propose an effective parameter orthogonalization technique to avoid excessive overlap of information learned from different perspectives. Results show that our approach outperforms existing methods in the ALVC task, achieving new state-of-the-art results.



### Detecting Driveable Area for Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1911.02740v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.02740v1)
- **Published**: 2019-11-07 03:32:46+00:00
- **Updated**: 2019-11-07 03:32:46+00:00
- **Authors**: Niral Shah, Ashwin Shankar, Jae-hong Park
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous driving is a challenging problem where there is currently an intense focus on research and development. Human drivers are forced to make thousands of complex decisions in a short amount of time,quickly processing their surroundings and moving factors. One of these aspects, recognizing regions on the road that are driveable is vital to the success of any autonomous system. This problem can be addressed with deep learning framed as a region proposal problem. Utilizing a Mask R-CNN trained on the Berkeley Deep Drive (BDD100k) dataset, we aim to see if recognizing driveable areas, while also differentiating between the car's direct (current) lane and alternative lanes is feasible.



### PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud Representation
- **Arxiv ID**: http://arxiv.org/abs/1911.02744v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.02744v1)
- **Published**: 2019-11-07 04:03:07+00:00
- **Updated**: 2019-11-07 04:03:07+00:00
- **Authors**: Can Qin, Haoxuan You, Lichen Wang, C. -C. Jay Kuo, Yun Fu
- **Comment**: 12 pages, 4 figures, 33rd Conference on Neural Information Processing
  Systems (NeurIPS 2019)
- **Journal**: None
- **Summary**: Domain Adaptation (DA) approaches achieved significant improvements in a wide range of machine learning and computer vision tasks (i.e., classification, detection, and segmentation). However, as far as we are aware, there are few methods yet to achieve domain adaptation directly on 3D point cloud data. The unique challenge of point cloud data lies in its abundant spatial geometric information, and the semantics of the whole object is contributed by including regional geometric structures. Specifically, most general-purpose DA methods that struggle for global feature alignment and ignore local geometric information are not suitable for 3D domain alignment. In this paper, we propose a novel 3D Domain Adaptation Network for point cloud data (PointDAN). PointDAN jointly aligns the global and local features in multi-level. For local alignment, we propose Self-Adaptive (SA) node module with an adjusted receptive field to model the discriminative local structures for aligning domains. To represent hierarchically scaled features, node-attention module is further introduced to weight the relationship of SA nodes across objects and domains. For global alignment, an adversarial-training strategy is employed to learn and align global features across domains. Since there is no common evaluation benchmark for 3D point cloud DA scenario, we build a general benchmark (i.e., PointDA-10) extracted from three popular 3D object/scene datasets (i.e., ModelNet, ShapeNet and ScanNet) for cross-domain 3D objects classification fashion. Extensive experiments on PointDA-10 illustrate the superiority of our model over the state-of-the-art general-purpose DA methods.



### Sparse Coding on Cascaded Residuals
- **Arxiv ID**: http://arxiv.org/abs/1911.02749v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.02749v1)
- **Published**: 2019-11-07 04:17:46+00:00
- **Updated**: 2019-11-07 04:17:46+00:00
- **Authors**: Tong Zhang, Fatih Porikli
- **Comment**: ACCV 2016
- **Journal**: None
- **Summary**: This paper seeks to combine dictionary learning and hierarchical image representation in a principled way. To make dictionary atoms capturing additional information from extended receptive fields and attain improved descriptive capacity, we present a two-pass multi-resolution cascade framework for dictionary learning and sparse coding. The cascade allows collaborative reconstructions at different resolutions using the same dimensional dictionary atoms. Our jointly learned dictionary comprises atoms that adapt to the information available at the coarsest layer where the support of atoms reaches their maximum range and the residual images where the supplementary details progressively refine the reconstruction objective. The residual at a layer is computed by the difference between the aggregated reconstructions of the previous layers and the downsampled original image at that layer. Our method generates more flexible and accurate representations using much less number of coefficients. Its computational efficiency stems from encoding at the coarsest resolution, which is minuscule, and encoding the residuals, which are relatively much sparse. Our extensive experiments on multiple datasets demonstrate that this new method is powerful in image coding, denoising, inpainting and artifact removal tasks outperforming the state-of-the-art techniques.



### Automatic Tip Detection of Surgical Instruments in Biportal Endoscopic Spine Surgery
- **Arxiv ID**: http://arxiv.org/abs/1911.02755v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.02755v2)
- **Published**: 2019-11-07 04:53:06+00:00
- **Updated**: 2020-03-12 22:32:31+00:00
- **Authors**: Sue Min Cho, Young-Gon Kim, Jinhoon Jeong, Ho-jin Lee, Namkug Kim
- **Comment**: 7 pages, 6 figures, 3 tables
- **Journal**: None
- **Summary**: Some endoscopic surgeries require a surgeon to hold the endoscope with one hand and the surgical instruments with the other hand to perform the actual surgery with correct vision. Recent technical advances in deep learning as well as in robotics can introduce robotics to these endoscopic surgeries. This can have numerous advantages by freeing one hand of the surgeon, which will allow the surgeon to use both hands and to use more intricate and sophisticated techniques. Recently, deep learning with convolutional neural network achieves state-of-the-art results in computer vision. Therefore, the aim of this study is to automatically detect the tip of the instrument, localize a point, and evaluate detection accuracy in biportal endoscopic spine surgery. The localized point could be used for the controller's inputs of robotic endoscopy in these types of endoscopic surgeries.



### Investigations of the Influences of a CNN's Receptive Field on Segmentation of Subnuclei of Bilateral Amygdalae
- **Arxiv ID**: http://arxiv.org/abs/1911.02761v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.02761v1)
- **Published**: 2019-11-07 05:39:56+00:00
- **Updated**: 2019-11-07 05:39:56+00:00
- **Authors**: Han Bao
- **Comment**: 16 pages, 10 figures, ADEIJ journal
- **Journal**: None
- **Summary**: Segmentation of objects with various sizes is relatively less explored in medical imaging, and has been very challenging in computer vision tasks in general. We hypothesize that the receptive field of a deep model corresponds closely to the size of object to be segmented, which could critically influence the segmentation accuracy of objects with varied sizes. In this study, we employed "AmygNet", a dual-branch fully convolutional neural network (FCNN) with two different sizes of receptive fields, to investigate the effects of receptive field on segmenting four major subnuclei of bilateral amygdalae. The experiment was conducted on 14 subjects, which are all 3-dimensional MRI human brain images. Since the scale of different subnuclear groups are different, by investigating the accuracy of each subnuclear group while using receptive fields of various sizes, we may find which kind of receptive field is suitable for object of which scale respectively. In the given condition, AmygNet with multiple receptive fields presents great potential in segmenting objects of different sizes.



### Improving Human Annotation in Single Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1911.02807v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.02807v1)
- **Published**: 2019-11-07 08:55:46+00:00
- **Updated**: 2019-11-07 08:55:46+00:00
- **Authors**: Yu Pang, Xinyi Li, Lin Yuan, Haibin Ling
- **Comment**: 7 pages, 7 figures, 1 table, submitted to ICRA2020
- **Journal**: None
- **Summary**: Human annotation is always considered as ground truth in video object tracking tasks. It is used in both training and evaluation purposes. Thus, ensuring its high quality is an important task for the success of trackers and evaluations between them. In this paper, we give a qualitative and quantitative analysis of the existing human annotations. We show that human annotation tends to be non-smooth and is prone to partial visibility and deformation. We propose a smoothing trajectory strategy with the ability to handle moving scenes. We use a two-step adaptive image alignment algorithm to find the canonical view of the video sequence. We then use different techniques to smooth the trajectories at certain degree. Once we convert back to the original image coordination, we can compare with the human annotation. With the experimental results, we can get more consistent trajectories. At a certain degree, it can also slightly improve the trained model. If go beyond a certain threshold, the smoothing error will start eating up the benefit. Overall, our method could help extrapolate the missing annotation frames or identify and correct human annotation outliers as well as help improve the training data quality.



### Post-mortem Iris Decomposition and its Dynamics in Morgue Conditions
- **Arxiv ID**: http://arxiv.org/abs/1911.02837v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.02837v1)
- **Published**: 2019-11-07 10:40:33+00:00
- **Updated**: 2019-11-07 10:40:33+00:00
- **Authors**: Mateusz Trokielewicz, Adam Czajka, Piotr Maciejewicz
- **Comment**: None
- **Journal**: None
- **Summary**: With increasing interest in employing iris biometrics as a forensic tool for identification by investigation authorities, there is a need for a thorough examination and understanding of post-mortem decomposition processes that take place within the human eyeball, especially the iris. This can prove useful for fast and accurate matching of ante-mortem with post-mortem data acquired at crime scenes or mass casualties, as well as for ensuring correct dispatching of bodies from the incident scene to a mortuary or funeral homes. Following these needs of forensic community, this paper offers an analysis of the coarse effects of eyeball decay done from a perspective of automatic iris recognition point of view. Therefore, we analyze post-mortem iris images acquired in both visible light as well as in near-infrared light (860 nm), as the latter wavelength is used in commercial iris recognition systems. Conclusions and suggestions are provided that may aid forensic examiners in successfully utilizing iris patterns in post-mortem identification of deceased subjects. Initial guidelines regarding the imaging process, types of illumination, resolution are also given, together with expectations with respect to the iris features decomposition rates.



### This dataset does not exist: training models from generated images
- **Arxiv ID**: http://arxiv.org/abs/1911.02888v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.02888v1)
- **Published**: 2019-11-07 13:23:39+00:00
- **Updated**: 2019-11-07 13:23:39+00:00
- **Authors**: Victor Besnier, Himalaya Jain, Andrei Bursuc, Matthieu Cord, Patrick Pérez
- **Comment**: None
- **Journal**: None
- **Summary**: Current generative networks are increasingly proficient in generating high-resolution realistic images. These generative networks, especially the conditional ones, can potentially become a great tool for providing new image datasets. This naturally brings the question: Can we train a classifier only on the generated data? This potential availability of nearly unlimited amounts of training data challenges standard practices for training machine learning models, which have been crafted across the years for limited and fixed size datasets. In this work we investigate this question and its related challenges. We identify ways to improve significantly the performance over naive training on randomly generated images with regular heuristics. We propose three standalone techniques that can be applied at different stages of the pipeline, i.e., data generation, training on generated data, and deploying on real data. We evaluate our proposed approaches on a subset of the ImageNet dataset and show encouraging results compared to classifiers trained on real images.



### Efficacy of Pixel-Level OOD Detection for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.02897v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.02897v1)
- **Published**: 2019-11-07 13:37:38+00:00
- **Updated**: 2019-11-07 13:37:38+00:00
- **Authors**: Matt Angus, Krzysztof Czarnecki, Rick Salay
- **Comment**: None
- **Journal**: None
- **Summary**: The detection of out of distribution samples for image classification has been widely researched. Safety critical applications, such as autonomous driving, would benefit from the ability to localise the unusual objects causing the image to be out of distribution. This paper adapts state-of-the-art methods for detecting out of distribution images for image classification to the new task of detecting out of distribution pixels, which can localise the unusual objects. It further experimentally compares the adapted methods on two new datasets derived from existing semantic segmentation datasets using PSPNet and DeeplabV3+ architectures, as well as proposing a new metric for the task. The evaluation shows that the performance ranking of the compared methods does not transfer to the new task and every method performs significantly worse than their image-level counterparts.



### Improved Visual Localization via Graph Smoothing
- **Arxiv ID**: http://arxiv.org/abs/1911.02961v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.02961v1)
- **Published**: 2019-11-07 15:15:24+00:00
- **Updated**: 2019-11-07 15:15:24+00:00
- **Authors**: Carlos Lassance, Yasir Latif, Ravi Garg, Vincent Gripon, Ian Reid
- **Comment**: None
- **Journal**: None
- **Summary**: Vision based localization is the problem of inferring the pose of the camera given a single image. One solution to this problem is to learn a deep neural network to infer the pose of a query image after learning on a dataset of images with known poses. Another more commonly used approach rely on image retrieval where the query image is compared against the database of images and its pose is inferred with the help of the retrieved images. The latter approach assumes that images taken from the same places consists of the same landmarks and, thus would have similar feature representations. These representation can be learned using full supervision to be robust to different variations in capture conditions like time of the day and weather. In this work, we introduce a framework to enhance the performance of these retrieval based localization methods by taking into account the additional information including GPS coordinates and temporal neighbourhood of the images provided by the acquisition process in addition to the descriptor similarity of pairs of images in the reference or query database which is used traditionally for localization. Our method constructs a graph based on this additional information and use it for robust retrieval by smoothing the feature representation of reference and/or query images. We show that the proposed method is able to significantly improve the localization accuracy on two large scale datasets over the baselines.



### Probing Contextualized Sentence Representations with Visual Awareness
- **Arxiv ID**: http://arxiv.org/abs/1911.02971v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.02971v1)
- **Published**: 2019-11-07 16:34:31+00:00
- **Updated**: 2019-11-07 16:34:31+00:00
- **Authors**: Zhuosheng Zhang, Rui Wang, Kehai Chen, Masao Utiyama, Eiichiro Sumita, Hai Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: We present a universal framework to model contextualized sentence representations with visual awareness that is motivated to overcome the shortcomings of the multimodal parallel data with manual annotations. For each sentence, we first retrieve a diversity of images from a shared cross-modal embedding space, which is pre-trained on a large-scale of text-image pairs. Then, the texts and images are respectively encoded by transformer encoder and convolutional neural network. The two sequences of representations are further fused by a simple and effective attention layer. The architecture can be easily applied to text-only natural language processing tasks without manually annotating multimodal parallel corpora. We apply the proposed method on three tasks, including neural machine translation, natural language inference and sequence labeling and experimental results verify the effectiveness.



