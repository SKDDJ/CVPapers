# Arxiv Papers in cs.CV on 2019-11-13
### Double cycle-consistent generative adversarial network for unsupervised conditional generation
- **Arxiv ID**: http://arxiv.org/abs/1911.05210v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.05210v3)
- **Published**: 2019-11-13 00:11:50+00:00
- **Updated**: 2021-04-05 15:25:55+00:00
- **Authors**: Fei Ding, Feng Luo, Yin Yang
- **Comment**: 12 pages, 4 figures, and 12 tables
- **Journal**: None
- **Summary**: Conditional generative models have achieved considerable success in the past few years, but usually require a lot of labeled data. Recently, ClusterGAN combines GAN with an encoder to achieve remarkable clustering performance via unsupervised conditional generation. However, it ignores the real conditional distribution of data, which leads to generating less diverse samples for each class and makes the encoder only achieve sub-optimal clustering performance. Here, we propose a new unsupervised conditional generation framework, Double Cycle-Consistent Conditional GAN (DC3-GAN), which can generate diverse class-conditioned samples. We enforce the encoder and the generator of GAN to form an encoder-generator pair in addition to the generator-encoder pair, which enables us to avoid the low-diversity generation and the triviality of latent features. We train the encoder-generator pair using real data, which can indirectly estimate the real conditional distribution. Meanwhile, this framework enforces the outputs of the encoder to match the inputs of GAN and the prior noise distribution, which disentangles latent space into two parts: one-hot discrete and continuous latent variables. The former can be directly expressed as clusters and the latter represents remaining unspecified factors. This work demonstrates that enhancing the diversity of unsupervised conditional generated samples can improve the clustering performance. Experiments on different benchmark datasets show that the proposed method outperforms existing generative model-based clustering methods, and also achieves the optimal disentanglement performance.



### Deep Encoder-decoder Adversarial Reconstruction (DEAR) Network for 3D CT from Few-view Data
- **Arxiv ID**: http://arxiv.org/abs/1911.05880v2
- **DOI**: 10.3390/bioengineering6040111
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.05880v2)
- **Published**: 2019-11-13 00:39:50+00:00
- **Updated**: 2019-11-15 02:50:08+00:00
- **Authors**: Huidong Xie, Hongming Shan, Ge Wang
- **Comment**: None
- **Journal**: Bioengineering 2019, 6(4), 111
- **Summary**: X-ray computed tomography (CT) is widely used in clinical practice. The involved ionizing X-ray radiation, however, could increase cancer risk. Hence, the reduction of the radiation dose has been an important topic in recent years. Few-view CT image reconstruction is one of the main ways to minimize radiation dose and potentially allow a stationary CT architecture. In this paper, we propose a deep encoder-decoder adversarial reconstruction (DEAR) network for 3D CT image reconstruction from few-view data. Since the artifacts caused by few-view reconstruction appear in 3D instead of 2D geometry, a 3D deep network has a great potential for improving the image quality in a data-driven fashion. More specifically, our proposed DEAR-3D network aims at reconstructing 3D volume directly from clinical 3D spiral cone-beam image data. DEAR is validated on a publicly available abdominal CT dataset prepared and authorized by Mayo Clinic. Compared with other 2D deep-learning methods, the proposed DEAR-3D network can utilize 3D information to produce promising reconstruction results.



### What Do Compressed Deep Neural Networks Forget?
- **Arxiv ID**: http://arxiv.org/abs/1911.05248v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.HC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.05248v3)
- **Published**: 2019-11-13 02:02:19+00:00
- **Updated**: 2021-09-06 00:47:17+00:00
- **Authors**: Sara Hooker, Aaron Courville, Gregory Clark, Yann Dauphin, Andrea Frome
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural network pruning and quantization techniques have demonstrated it is possible to achieve high levels of compression with surprisingly little degradation to test set accuracy. However, this measure of performance conceals significant differences in how different classes and images are impacted by model compression techniques. We find that models with radically different numbers of weights have comparable top-line performance metrics but diverge considerably in behavior on a narrow subset of the dataset. This small subset of data points, which we term Pruning Identified Exemplars (PIEs) are systematically more impacted by the introduction of sparsity. Compression disproportionately impacts model performance on the underrepresented long-tail of the data distribution. PIEs over-index on atypical or noisy images that are far more challenging for both humans and algorithms to classify. Our work provides intuition into the role of capacity in deep neural networks and the trade-offs incurred by compression. An understanding of this disparate impact is critical given the widespread deployment of compressed models in the wild.



### Location-aware Upsampling for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.05250v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.05250v2)
- **Published**: 2019-11-13 02:15:06+00:00
- **Updated**: 2019-11-14 02:32:57+00:00
- **Authors**: Xiangyu He, Zitao Mo, Qiang Chen, Anda Cheng, Peisong Wang, Jian Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Many successful learning targets such as minimizing dice loss and cross-entropy loss have enabled unprecedented breakthroughs in segmentation tasks. Beyond these semantic metrics, this paper aims to introduce location supervision into semantic segmentation. Based on this idea, we present a Location-aware Upsampling (LaU) that adaptively refines the interpolating coordinates with trainable offsets. Then, location-aware losses are established by encouraging pixels to move towards well-classified locations. An LaU is offset prediction coupled with interpolation, which is trained end-to-end to generate confidence score at each position from coarse to fine. Guided by location-aware losses, the new module can replace its plain counterpart (\textit{e.g.}, bilinear upsampling) in a plug-and-play manner to further boost the leading encoder-decoder approaches. Extensive experiments validate the consistent improvement over the state-of-the-art methods on benchmark datasets. Our code is available at https://github.com/HolmesShuan/Location-aware-Upsampling-for-Semantic-Segmentation



### Learning Where to Focus for Efficient Video Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.05253v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.05253v2)
- **Published**: 2019-11-13 02:17:20+00:00
- **Updated**: 2020-07-16 11:46:16+00:00
- **Authors**: Zhengkai Jiang, Yu Liu, Ceyuan Yang, Jihao Liu, Peng Gao, Qian Zhang, Shiming Xiang, Chunhong Pan
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: Transferring existing image-based detectors to the video is non-trivial since the quality of frames is always deteriorated by part occlusion, rare pose, and motion blur. Previous approaches exploit to propagate and aggregate features across video frames by using optical flow-warping. However, directly applying image-level optical flow onto the high-level features might not establish accurate spatial correspondences. Therefore, a novel module called Learnable Spatio-Temporal Sampling (LSTS) has been proposed to learn semantic-level correspondences among adjacent frame features accurately. The sampled locations are first randomly initialized, then updated iteratively to find better spatial correspondences guided by detection supervision progressively. Besides, Sparsely Recursive Feature Updating (SRFU) module and Dense Feature Aggregation (DFA) module are also introduced to model temporal relations and enhance per-frame features, respectively. Without bells and whistles, the proposed method achieves state-of-the-art performance on the ImageNet VID dataset with less computational complexity and real-time speed. Code will be made available at https://github.com/jiangzhengkai/LSTS.



### A Hierarchy of Graph Neural Networks Based on Learnable Local Features
- **Arxiv ID**: http://arxiv.org/abs/1911.05256v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.05256v1)
- **Published**: 2019-11-13 02:22:54+00:00
- **Updated**: 2019-11-13 02:22:54+00:00
- **Authors**: Michael Lingzhi Li, Meng Dong, Jiawei Zhou, Alexander M. Rush
- **Comment**: None
- **Journal**: None
- **Summary**: Graph neural networks (GNNs) are a powerful tool to learn representations on graphs by iteratively aggregating features from node neighbourhoods. Many variant models have been proposed, but there is limited understanding on both how to compare different architectures and how to construct GNNs systematically. Here, we propose a hierarchy of GNNs based on their aggregation regions. We derive theoretical results about the discriminative power and feature representation capabilities of each class. Then, we show how this framework can be utilized to systematically construct arbitrarily powerful GNNs. As an example, we construct a simple architecture that exceeds the expressiveness of the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theory on both synthetic and real-world benchmarks, and demonstrate our example's theoretical power translates to strong results on node classification, graph classification, and graph regression tasks.



### Image-Based Feature Representation for Insider Threat Classification
- **Arxiv ID**: http://arxiv.org/abs/1911.05879v1
- **DOI**: 10.3390/app10144945
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.05879v1)
- **Published**: 2019-11-13 03:00:55+00:00
- **Updated**: 2019-11-13 03:00:55+00:00
- **Authors**: Gayathri R G, Atul Sajjanhar, Yong Xiang
- **Comment**: 8 pages, 5 figures
- **Journal**: Applied Sciences, vol. 10, no. 14, p. 4945, 2020
- **Summary**: Insiders are the trusted entities in the organization, but poses threat to the with access to sensitive information network and resources. The insider threat detection is a well studied problem in security analytics. Identifying the features from data sources and using them with the right data analytics algorithms makes various kinds of threat analysis possible. The insider threat analysis is mainly done using the frequency based attributes extracted from the raw data available from data sources. In this paper, we propose an image-based feature representation of the daily resource usage pattern of users in the organization. The features extracted from the audit files of the organization are represented as gray scale images. Hence, these images are used to represent the resource access patterns and thereby the behavior of users. Classification models are applied to the representative images to detect anomalous behavior of insiders. The images are classified to malicious and non-malicious. The effectiveness of the proposed representation is evaluated using the CMU CERT data V4.2, and state-of-art image classification models like Mobilenet, VGG and ResNet. The experimental results showed improved accuracy. The comparison with existing works show a performance improvement in terms of high recall and precision values.



### Learning Non-Parametric Invariances from Data with Permanent Random Connectomes
- **Arxiv ID**: http://arxiv.org/abs/1911.05266v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.05266v3)
- **Published**: 2019-11-13 03:03:48+00:00
- **Updated**: 2020-08-14 00:54:18+00:00
- **Authors**: Dipan K. Pal, Akshay Chawla, Marios Savvides
- **Comment**: Preprint (accepted at NeurIPS SVRHM 2019 Workshop)
- **Journal**: None
- **Summary**: One of the fundamental problems in supervised classification and in machine learning in general, is the modelling of non-parametric invariances that exist in data. Most prior art has focused on enforcing priors in the form of invariances to parametric nuisance transformations that are expected to be present in data. Learning non-parametric invariances directly from data remains an important open problem. In this paper, we introduce a new architectural layer for convolutional networks which is capable of learning general invariances from data itself. This layer can learn invariance to non-parametric transformations and interestingly, motivates and incorporates permanent random connectomes, thereby being called Permanent Random Connectome Non-Parametric Transformation Networks (PRC-NPTN). PRC-NPTN networks are initialized with random connections (not just weights) which are a small subset of the connections in a fully connected convolution layer. Importantly, these connections in PRC-NPTNs once initialized remain permanent throughout training and testing. Permanent random connectomes make these architectures loosely more biologically plausible than many other mainstream network architectures which require highly ordered structures. We motivate randomly initialized connections as a simple method to learn invariance from data itself while invoking invariance towards multiple nuisance transformations simultaneously. We find that these randomly initialized permanent connections have positive effects on generalization, outperform much larger ConvNet baselines and the recently proposed Non-Parametric Transformation Network (NPTN) on benchmarks that enforce learning invariances from the data itself.



### Learning from a Teacher using Unlabeled Data
- **Arxiv ID**: http://arxiv.org/abs/1911.05275v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.05275v1)
- **Published**: 2019-11-13 03:43:29+00:00
- **Updated**: 2019-11-13 03:43:29+00:00
- **Authors**: Gaurav Menghani, Sujith Ravi
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge distillation is a widely used technique for model compression. We posit that the teacher model used in a distillation setup, captures relationships between classes, that extend beyond the original dataset. We empirically show that a teacher model can transfer this knowledge to a student model even on an {\it out-of-distribution} dataset. Using this approach, we show promising results on MNIST, CIFAR-10, and Caltech-256 datasets using unlabeled image data from different sources. Our results are encouraging and help shed further light from the perspective of understanding knowledge distillation and utilizing unlabeled data to improve model quality.



### Exploiting Local and Global Structure for Point Cloud Semantic Segmentation with Contextual Point Representations
- **Arxiv ID**: http://arxiv.org/abs/1911.05277v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.05277v1)
- **Published**: 2019-11-13 03:51:32+00:00
- **Updated**: 2019-11-13 03:51:32+00:00
- **Authors**: Xu Wang, Jingming He, Lin Ma
- **Comment**: Accepted by NeurIPS 2019
- **Journal**: None
- **Summary**: In this paper, we propose one novel model for point cloud semantic segmentation, which exploits both the local and global structures within the point cloud based on the contextual point representations. Specifically, we enrich each point representation by performing one novel gated fusion on the point itself and its contextual points. Afterwards, based on the enriched representation, we propose one novel graph pointnet module, relying on the graph attention block to dynamically compose and update each point representation within the local point cloud structure. Finally, we resort to the spatial-wise and channel-wise attention strategies to exploit the point cloud global structure and thereby yield the resulting semantic label for each point. Extensive results on the public point cloud databases, namely the S3DIS and ScanNet datasets, demonstrate the effectiveness of our proposed model, outperforming the state-of-the-art approaches. Our code for this paper is available at https://github.com/fly519/ELGS.



### Rotation Differential Invariants of Images Generated by Two Fundamental Differential Operators
- **Arxiv ID**: http://arxiv.org/abs/1911.05327v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.05327v2)
- **Published**: 2019-11-13 07:10:46+00:00
- **Updated**: 2021-03-15 11:12:18+00:00
- **Authors**: Hanlin Mo, Hua Li
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we design two fundamental differential operators for the derivation of rotation differential invariants of images. Each differential invariant obtained by using the new method can be expressed as a homogeneous polynomial of image partial derivatives, which preserve their values when the image is rotated by arbitrary angles. We produce all possible instances of homogeneous invariants up to the given order and degree, and discuss the independence of them in detail. As far as we know, no previous papers have published so many explicit forms of high-order rotation differential invariants of images. In the experimental part, texture classification and image patch verification are carried out on popular real databases. These rotation differential invariants are used as image feature vector. We mainly evaluate the effects of various factors on the performance of them. The experimental results also validate that they have better performance than some commonly used image features in some cases.



### Knowledge Representing: Efficient, Sparse Representation of Prior Knowledge for Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/1911.05329v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.05329v1)
- **Published**: 2019-11-13 07:14:25+00:00
- **Updated**: 2019-11-13 07:14:25+00:00
- **Authors**: Junjie Liu, Dongchao Wen, Hongxing Gao, Wei Tao, Tse-Wei Chen, Kinya Osa, Masami Kato
- **Comment**: None
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR 2019)
- **Summary**: Despite the recent works on knowledge distillation (KD) have achieved a further improvement through elaborately modeling the decision boundary as the posterior knowledge, their performance is still dependent on the hypothesis that the target network has a powerful capacity (representation ability). In this paper, we propose a knowledge representing (KR) framework mainly focusing on modeling the parameters distribution as prior knowledge. Firstly, we suggest a knowledge aggregation scheme in order to answer how to represent the prior knowledge from teacher network. Through aggregating the parameters distribution from teacher network into more abstract level, the scheme is able to alleviate the phenomenon of residual accumulation in the deeper layers. Secondly, as the critical issue of what the most important prior knowledge is for better distilling, we design a sparse recoding penalty for constraining the student network to learn with the penalized gradients. With the proposed penalty, the student network can effectively avoid the over-regularization during knowledge distilling and converge faster. The quantitative experiments exhibit that the proposed framework achieves the state-ofthe-arts performance, even though the target network does not have the expected capacity. Moreover, the framework is flexible enough for combining with other KD methods based on the posterior knowledge.



### DupNet: Towards Very Tiny Quantized CNN with Improved Accuracy for Face Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.05341v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.05341v1)
- **Published**: 2019-11-13 08:00:26+00:00
- **Updated**: 2019-11-13 08:00:26+00:00
- **Authors**: Hongxing Gao, Wei Tao, Dongchao Wen, Junjie Liu, Tse-Wei Chen, Kinya Osa, Masami Kato
- **Comment**: None
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR 2019) Workshops
- **Summary**: Deploying deep learning based face detectors on edge devices is a challenging task due to the limited computation resources. Even though binarizing the weights of a very tiny network gives impressive compactness on model size (e.g. 240.9 KB for IFQ-Tinier-YOLO), it is not tiny enough to fit in the embedded devices with strict memory constraints. In this paper, we propose DupNet which consists of two parts. Firstly, we employ weights with duplicated channels for the weight-intensive layers to reduce the model size. Secondly, for the quantization-sensitive layers whose quantization causes notable accuracy drop, we duplicate its input feature maps. It allows us to use more weights channels for convolving more representative outputs. Based on that, we propose a very tiny face detector, DupNet-Tinier-YOLO, which is 6.5X times smaller on model size and 42.0% less complex on computation and meanwhile achieves 2.4% higher detection than IFQ-Tinier-YOLO. Comparing with the full precision Tiny-YOLO, our DupNet-Tinier-YOLO gives 1,694.2X and 389.9X times savings on model size and computation complexity respectively with only 4.0% drop on detection rate (0.880 vs. 0.920). Moreover, our DupNet-Tinier-YOLO is only 36.9 KB, which is the tiniest deep face detector to our best knowledge.



### Adversarial Transformations for Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.06181v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.06181v2)
- **Published**: 2019-11-13 08:01:47+00:00
- **Updated**: 2019-11-18 06:53:12+00:00
- **Authors**: Teppei Suzuki, Ikuro Sato
- **Comment**: Accepted by AAAI 2020
- **Journal**: None
- **Summary**: We propose a Regularization framework based on Adversarial Transformations (RAT) for semi-supervised learning. RAT is designed to enhance robustness of the output distribution of class prediction for a given data against input perturbation. RAT is an extension of Virtual Adversarial Training (VAT) in such a way that RAT adversarialy transforms data along the underlying data distribution by a rich set of data transformation functions that leave class label invariant, whereas VAT simply produces adversarial additive noises. In addition, we verified that a technique of gradually increasing of perturbation region further improve the robustness. In experiments, we show that RAT significantly improves classification performance on CIFAR-10 and SVHN compared to existing regularization methods under standard semi-supervised image classification settings.



### GANprintR: Improved Fakes and Evaluation of the State of the Art in Face Manipulation Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.05351v4
- **DOI**: 10.1109/JSTSP.2020.3007250
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.05351v4)
- **Published**: 2019-11-13 08:48:55+00:00
- **Updated**: 2020-07-01 13:51:33+00:00
- **Authors**: João C. Neves, Ruben Tolosana, Ruben Vera-Rodriguez, Vasco Lopes, Hugo Proença, Julian Fierrez
- **Comment**: None
- **Journal**: IEEE Journal of Selected Topics in Signal Processing, 2020
- **Summary**: The availability of large-scale facial databases, together with the remarkable progresses of deep learning technologies, in particular Generative Adversarial Networks (GANs), have led to the generation of extremely realistic fake facial content, raising obvious concerns about the potential for misuse. Such concerns have fostered the research on manipulation detection methods that, contrary to humans, have already achieved astonishing results in various scenarios. In this study, we focus on the synthesis of entire facial images, which is a specific type of facial manipulation. The main contributions of this study are four-fold: i) a novel strategy to remove GAN "fingerprints" from synthetic fake images based on autoencoders is described, in order to spoof facial manipulation detection systems while keeping the visual quality of the resulting images; ii) an in-depth analysis of the recent literature in facial manipulation detection; iii) a complete experimental assessment of this type of facial manipulation, considering the state-of-the-art fake detection systems (based on holistic deep networks, steganalysis, and local artifacts), remarking how challenging is this task in unconstrained scenarios; and finally iv) we announce a novel public database, named iFakeFaceDB, yielding from the application of our proposed GAN-fingerprint Removal approach (GANprintR) to already very realistic synthetic fake images.   The results obtained in our empirical evaluation show that additional efforts are required to develop robust facial manipulation detection systems against unseen conditions and spoof techniques, such as the one proposed in this study.



### SynSig2Vec: Learning Representations from Synthetic Dynamic Signatures for Real-world Verification
- **Arxiv ID**: http://arxiv.org/abs/1911.05358v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.05358v2)
- **Published**: 2019-11-13 08:58:19+00:00
- **Updated**: 2019-11-14 13:04:51+00:00
- **Authors**: Songxuan Lai, Lianwen Jin, Luojun Lin, Yecheng Zhu, Huiyun Mao
- **Comment**: To appear in AAAI 2020
- **Journal**: None
- **Summary**: An open research problem in automatic signature verification is the skilled forgery attacks. However, the skilled forgeries are very difficult to acquire for representation learning. To tackle this issue, this paper proposes to learn dynamic signature representations through ranking synthesized signatures. First, a neuromotor inspired signature synthesis method is proposed to synthesize signatures with different distortion levels for any template signature. Then, given the templates, we construct a lightweight one-dimensional convolutional network to learn to rank the synthesized samples, and directly optimize the average precision of the ranking to exploit relative and fine-grained signature similarities. Finally, after training, fixed-length representations can be extracted from dynamic signatures of variable lengths for verification. One highlight of our method is that it requires neither skilled nor random forgeries for training, yet it surpasses the state-of-the-art by a large margin on two public benchmarks.



### Self-labelling via simultaneous clustering and representation learning
- **Arxiv ID**: http://arxiv.org/abs/1911.05371v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1911.05371v3)
- **Published**: 2019-11-13 09:47:49+00:00
- **Updated**: 2020-02-19 18:03:39+00:00
- **Authors**: Yuki Markus Asano, Christian Rupprecht, Andrea Vedaldi
- **Comment**: Accepted paper at the International Conference on Learning
  Representations (ICLR) 2020
- **Journal**: None
- **Summary**: Combining clustering and representation learning is one of the most promising approaches for unsupervised learning of deep neural networks. However, doing so naively leads to ill posed learning problems with degenerate solutions. In this paper, we propose a novel and principled learning formulation that addresses these issues. The method is obtained by maximizing the information between labels and input data indices. We show that this criterion extends standard crossentropy minimization to an optimal transport problem, which we solve efficiently for millions of input images and thousands of labels using a fast variant of the Sinkhorn-Knopp algorithm. The resulting method is able to self-label visual data so as to train highly competitive image representations without manual labels. Our method achieves state of the art representation learning performance for AlexNet and ResNet-50 on SVHN, CIFAR-10, CIFAR-100 and ImageNet and yields the first self-supervised AlexNet that outperforms the supervised Pascal VOC detection baseline. Code and models are available.



### CSPN++: Learning Context and Resource Aware Convolutional Spatial Propagation Networks for Depth Completion
- **Arxiv ID**: http://arxiv.org/abs/1911.05377v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.05377v2)
- **Published**: 2019-11-13 10:04:05+00:00
- **Updated**: 2019-11-22 03:44:45+00:00
- **Authors**: Xinjing Cheng, Peng Wang, Chenye Guan, Ruigang Yang
- **Comment**: Camera Ready Version. Accepted by AAAI 2020
- **Journal**: None
- **Summary**: Depth Completion deals with the problem of converting a sparse depth map to a dense one, given the corresponding color image. Convolutional spatial propagation network (CSPN) is one of the state-of-the-art (SoTA) methods of depth completion, which recovers structural details of the scene. In this paper, we propose CSPN++, which further improves its effectiveness and efficiency by learning adaptive convolutional kernel sizes and the number of iterations for the propagation, thus the context and computational resources needed at each pixel could be dynamically assigned upon requests. Specifically, we formulate the learning of the two hyper-parameters as an architecture selection problem where various configurations of kernel sizes and numbers of iterations are first defined, and then a set of soft weighting parameters are trained to either properly assemble or select from the pre-defined configurations at each pixel. In our experiments, we find weighted assembling can lead to significant accuracy improvements, which we referred to as "context-aware CSPN", while weighted selection, "resource-aware CSPN" can reduce the computational resource significantly with similar or better accuracy. Besides, the resource needed for CSPN++ can be adjusted w.r.t. the computational budget automatically. Finally, to avoid the side effects of noise or inaccurate sparse depths, we embed a gated network inside CSPN++, which further improves the performance. We demonstrate the effectiveness of CSPN++on the KITTI depth completion benchmark, where it significantly improves over CSPN and other SoTA methods.



### A Smartphone-Based Skin Disease Classification Using MobileNet CNN
- **Arxiv ID**: http://arxiv.org/abs/1911.07929v1
- **DOI**: 10.30534/ijatcse/2019/116852019
- **Categories**: **cs.CV**, cs.CY, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.07929v1)
- **Published**: 2019-11-13 11:04:05+00:00
- **Updated**: 2019-11-13 11:04:05+00:00
- **Authors**: Jessica Velasco, Cherry Pascion, Jean Wilmar Alberio, Jonathan Apuang, John Stephen Cruz, Mark Angelo Gomez, Benjamin Jr. Molina, Lyndon Tuala, August Thio-ac, Romeo Jr. Jorda
- **Comment**: None
- **Journal**: International Journal of Advanced Trends in Computer Science and
  Engineering (2019) 2632-2637
- **Summary**: The MobileNet model was used by applying transfer learning on the 7 skin diseases to create a skin disease classification system on Android application. The proponents gathered a total of 3,406 images and it is considered as imbalanced dataset because of the unequal number of images on its classes. Using different sampling method and preprocessing of input data was explored to further improved the accuracy of the MobileNet. Using under-sampling method and the default preprocessing of input data achieved an 84.28% accuracy. While, using imbalanced dataset and default preprocessing of input data achieved a 93.6% accuracy. Then, researchers explored oversampling the dataset and the model attained a 91.8% accuracy. Lastly, by using oversampling technique and data augmentation on preprocessing the input data provide a 94.4% accuracy and this model was deployed on the developed Android application.



### Statistical Deformation Reconstruction Using Multi-organ Shape Features for Pancreatic Cancer Localization
- **Arxiv ID**: http://arxiv.org/abs/1911.05439v1
- **DOI**: 10.1016/j.media.2020.101829
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.05439v1)
- **Published**: 2019-11-13 13:10:10+00:00
- **Updated**: 2019-11-13 13:10:10+00:00
- **Authors**: Megumi Nakao, Mitsuhiro Nakamura, Takashi Mizowaki, Tetsuya Matsuda
- **Comment**: None
- **Journal**: Medical Image Analysis, Vol. 67, 101829, 2021
- **Summary**: Respiratory motion and the associated deformations of abdominal organs and tumors are essential information in clinical applications. However, inter- and intra-patient multi-organ deformations are complex and have not been statistically formulated, whereas single organ deformations have been widely studied. In this paper, we introduce a multi-organ deformation library and its application to deformation reconstruction based on the shape features of multiple abdominal organs. Statistical multi-organ motion/deformation models of the stomach, liver, left and right kidneys, and duodenum were generated by shape matching their region labels defined on four-dimensional computed tomography images. A total of 250 volumes were measured from 25 pancreatic cancer patients. This paper also proposes a per-region-based deformation learning using the reproducing kernel to predict the displacement of pancreatic cancer for adaptive radiotherapy. The experimental results show that the proposed concept estimates deformations better than general per-patient-based learning models and achieves a clinically acceptable estimation error with a mean distance of 1.2 $\pm$ 0.7 mm and a Hausdorff distance of 4.2 $\pm$ 2.3 mm throughout the respiratory motion.



### Crowd Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/1911.05449v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.05449v1)
- **Published**: 2019-11-13 13:38:17+00:00
- **Updated**: 2019-11-13 13:38:17+00:00
- **Authors**: Liqi Yan, Mingjian Zhu, Changbin Yu
- **Comment**: None
- **Journal**: IECON 2019
- **Summary**: Describing a video automatically with natural language is a challenging task in the area of computer vision. In most cases, the on-site situation of great events is reported in news, but the situation of the off-site spectators in the entrance and exit is neglected which also arouses people's interest. Since the deployment of reporters in the entrance and exit costs lots of manpower, how to automatically describe the behavior of a crowd of off-site spectators is significant and remains a problem. To tackle this problem, we propose a new task called crowd video captioning (CVC) which aims to describe the crowd of spectators. We also provide baseline methods for this task and evaluate them on the dataset WorldExpo'10. Our experimental results show that captioning models have a fairly deep understanding of the crowd in video and perform satisfactorily in the CVC task.



### Multi-domain CT Metal Artifacts Reduction Using Partial Convolution Based Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1911.05530v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.05530v2)
- **Published**: 2019-11-13 15:08:33+00:00
- **Updated**: 2020-05-11 09:43:01+00:00
- **Authors**: Artem Pimkin, Alexander Samoylenko, Natalia Antipina, Anna Ovechkina, Andrey Golanov, Alexandra Dalechina, Mikhail Belyaev
- **Comment**: None
- **Journal**: None
- **Summary**: Recent CT Metal Artifacts Reduction (MAR) methods are often based on image-to-image convolutional neural networks for adjustment of corrupted sinograms or images themselves. In this paper, we are exploring the capabilities of a multi-domain method which consists of both sinogram correction (projection domain step) and restored image correction (image-domain step). Moreover, we propose a formulation of the first step problem as sinogram inpainting which allows us to use methods of this specific field such as partial convolutions. The proposed method allows to achieve state-of-the-art (-75% MSE) improvement in comparison with a classic benchmark - Li-MAR.



### Vehicle-Rear: A New Dataset to Explore Feature Fusion for Vehicle Identification Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.05541v3
- **DOI**: 10.1109/ACCESS.2021.3097964
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.05541v3)
- **Published**: 2019-11-13 15:23:04+00:00
- **Updated**: 2021-07-25 11:39:59+00:00
- **Authors**: Icaro O. de Oliveira, Rayson Laroca, David Menotti, Keiko V. O. Fonseca, Rodrigo Minetto
- **Comment**: None
- **Journal**: IEEE Access, vol. 9, pp. 101065-101077, 2021
- **Summary**: This work addresses the problem of vehicle identification through non-overlapping cameras. As our main contribution, we introduce a novel dataset for vehicle identification, called Vehicle-Rear, that contains more than three hours of high-resolution videos, with accurate information about the make, model, color and year of nearly 3,000 vehicles, in addition to the position and identification of their license plates. To explore our dataset we design a two-stream CNN that simultaneously uses two of the most distinctive and persistent features available: the vehicle's appearance and its license plate. This is an attempt to tackle a major problem: false alarms caused by vehicles with similar designs or by very close license plate identifiers. In the first network stream, shape similarities are identified by a Siamese CNN that uses a pair of low-resolution vehicle patches recorded by two different cameras. In the second stream, we use a CNN for OCR to extract textual information, confidence scores, and string similarities from a pair of high-resolution license plate patches. Then, features from both streams are merged by a sequence of fully connected layers for decision. In our experiments, we compared the two-stream network against several well-known CNN architectures using single or multiple vehicle features. The architectures, trained models, and dataset are publicly available at https://github.com/icarofua/vehicle-rear.



### IStego100K: Large-scale Image Steganalysis Dataset
- **Arxiv ID**: http://arxiv.org/abs/1911.05542v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.05542v1)
- **Published**: 2019-11-13 15:25:45+00:00
- **Updated**: 2019-11-13 15:25:45+00:00
- **Authors**: Zhongliang Yang, Ke Wang, Sai Ma, Yongfeng Huang, Xiangui Kang, Xianfeng Zhao
- **Comment**: Accepted by IWDW2019
- **Journal**: None
- **Summary**: In order to promote the rapid development of image steganalysis technology, in this paper, we construct and release a multivariable large-scale image steganalysis dataset called IStego100K. It contains 208,104 images with the same size of 1024*1024. Among them, 200,000 images (100,000 cover-stego image pairs) are divided as the training set and the remaining 8,104 as testing set. In addition, we hope that IStego100K can help researchers further explore the development of universal image steganalysis algorithms, so we try to reduce limits on the images in IStego100K. For each image in IStego100K, the quality factors is randomly set in the range of 75-95, the steganographic algorithm is randomly selected from three well-known steganographic algorithms, which are J-uniward, nsF5 and UERD, and the embedding rate is also randomly set to be a value of 0.1-0.4. In addition, considering the possible mismatch between training samples and test samples in real environment, we add a test set (DS-Test) whose source of samples are different from the training set. We hope that this test set can help to evaluate the robustness of steganalysis algorithms. We tested the performance of some latest steganalysis algorithms on IStego100K, with specific results and analysis details in the experimental part. We hope that the IStego100K dataset will further promote the development of universal image steganalysis technology. The description of IStego100K and instructions for use can be found at https://github.com/YangzlTHU/IStego100K



### Avoiding hashing and encouraging visual semantics in referential emergent language games
- **Arxiv ID**: http://arxiv.org/abs/1911.05546v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.05546v1)
- **Published**: 2019-11-13 15:31:48+00:00
- **Updated**: 2019-11-13 15:31:48+00:00
- **Authors**: Daniela Mihai, Jonathon Hare
- **Comment**: 4 pages, presented at Emergent Communication: Towards Natural
  Language workshop (NeurIPS 2019)
- **Journal**: None
- **Summary**: There has been an increasing interest in the area of emergent communication between agents which learn to play referential signalling games with realistic images. In this work, we consider the signalling game setting of Havrylov and Titov and investigate the effect of the feature extractor's weights and of the task being solved on the visual semantics learned or captured by the models. We impose various augmentation to the input images and additional tasks in the game with the aim to induce visual representations which capture conceptual properties of images. Through our set of experiments, we demonstrate that communication systems which capture visual semantics can be learned in a completely self-supervised manner by playing the right types of game.



### Cost-efficient segmentation of electron microscopy images using active learning
- **Arxiv ID**: http://arxiv.org/abs/1911.05548v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.05548v1)
- **Published**: 2019-11-13 15:35:38+00:00
- **Updated**: 2019-11-13 15:35:38+00:00
- **Authors**: Joris Roels, Yvan Saeys
- **Comment**: None
- **Journal**: None
- **Summary**: Over the last decade, electron microscopy has improved up to a point that generating high quality gigavoxel sized datasets only requires a few hours. Automated image analysis, particularly image segmentation, however, has not evolved at the same pace. Even though state-of-the-art methods such as U-Net and DeepLab have improved segmentation performance substantially, the required amount of labels remains too expensive. Active learning is the subfield in machine learning that aims to mitigate this burden by selecting the samples that require labeling in a smart way. Many techniques have been proposed, particularly for image classification, to increase the steepness of learning curves. In this work, we extend these techniques to deep CNN based image segmentation. Our experiments on three different electron microscopy datasets show that active learning can improve segmentation quality by 10 to 15% in terms of Jaccard score compared to standard randomized sampling.



### DARTS: DenseUnet-based Automatic Rapid Tool for brain Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.05567v2
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.05567v2)
- **Published**: 2019-11-13 15:57:26+00:00
- **Updated**: 2019-11-14 13:37:03+00:00
- **Authors**: Aakash Kaku, Chaitra V. Hegde, Jeffrey Huang, Sohae Chung, Xiuyuan Wang, Matthew Young, Alireza Radmanesh, Yvonne W. Lui, Narges Razavian
- **Comment**: None
- **Journal**: None
- **Summary**: Quantitative, volumetric analysis of Magnetic Resonance Imaging (MRI) is a fundamental way researchers study the brain in a host of neurological conditions including normal maturation and aging. Despite the availability of open-source brain segmentation software, widespread clinical adoption of volumetric analysis has been hindered due to processing times and reliance on manual corrections. Here, we extend the use of deep learning models from proof-of-concept, as previously reported, to present a comprehensive segmentation of cortical and deep gray matter brain structures matching the standard regions of aseg+aparc included in the commonly used open-source tool, Freesurfer. The work presented here provides a real-life, rapid deep learning-based brain segmentation tool to enable clinical translation as well as research application of quantitative brain segmentation. The advantages of the presented tool include short (~1 minute) processing time and improved segmentation quality. This is the first study to perform quick and accurate segmentation of 102 brain regions based on the surface-based protocol (DMK protocol), widely used by experts in the field. This is also the first work to include an expert reader study to assess the quality of the segmentation obtained using a deep-learning-based model. We show the superior performance of our deep-learning-based models over the traditional segmentation tool, Freesurfer. We refer to the proposed deep learning-based tool as DARTS (DenseUnet-based Automatic Rapid Tool for brain Segmentation). Our tool and trained models are available at https://github.com/NYUMedML/DARTS



### Are We Ready for Service Robots? The OpenLORIS-Scene Datasets for Lifelong SLAM
- **Arxiv ID**: http://arxiv.org/abs/1911.05603v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.05603v2)
- **Published**: 2019-11-13 16:43:16+00:00
- **Updated**: 2020-03-14 03:39:16+00:00
- **Authors**: Xuesong Shi, Dongjiang Li, Pengpeng Zhao, Qinbin Tian, Yuxin Tian, Qiwei Long, Chunhao Zhu, Jingwei Song, Fei Qiao, Le Song, Yangquan Guo, Zhigang Wang, Yimin Zhang, Baoxing Qin, Wei Yang, Fangshi Wang, Rosa H. M. Chan, Qi She
- **Comment**: To be published on ICRA 2020; 7 pages, 3 figures; v2 fixed a number
  in Table III
- **Journal**: None
- **Summary**: Service robots should be able to operate autonomously in dynamic and daily changing environments over an extended period of time. While Simultaneous Localization And Mapping (SLAM) is one of the most fundamental problems for robotic autonomy, most existing SLAM works are evaluated with data sequences that are recorded in a short period of time. In real-world deployment, there can be out-of-sight scene changes caused by both natural factors and human activities. For example, in home scenarios, most objects may be movable, replaceable or deformable, and the visual features of the same place may be significantly different in some successive days. Such out-of-sight dynamics pose great challenges to the robustness of pose estimation, and hence a robot's long-term deployment and operation. To differentiate the forementioned problem from the conventional works which are usually evaluated in a static setting in a single run, the term \textit{lifelong SLAM} is used here to address SLAM problems in an ever-changing environment over a long period of time. To accelerate lifelong SLAM research, we release the OpenLORIS-Scene datasets. The data are collected in real-world indoor scenes, for multiple times in each place to include scene changes in real life. We also design benchmarking metrics for lifelong SLAM, with which the robustness and accuracy of pose estimation are evaluated separately. The datasets and benchmark are available online at https://lifelong-robotic-vision.github.io/dataset/scene.



### Deep Learning Captures More Accurate Diffusion Fiber Orientations Distributions than Constrained Spherical Deconvolution
- **Arxiv ID**: http://arxiv.org/abs/1911.07927v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.07927v1)
- **Published**: 2019-11-13 17:00:11+00:00
- **Updated**: 2019-11-13 17:00:11+00:00
- **Authors**: Vishwesh Nath, Kurt G. Schilling, Colin B. Hansen, Prasanna Parvathaneni, Allison E. Hainline, Camilo Bermudez, Andrew J. Plassard, Vaibhav Janve, Yurui Gao, Justin A. Blaber, Iwona Stępniewska, Adam W. Anderson, Bennett A. Landman
- **Comment**: 2 pages, 4 figures. This work was accepted and published as an
  abstract at ISMRM 2018 held in Paris, France
- **Journal**: None
- **Summary**: Confocal histology provides an opportunity to establish intra-voxel fiber orientation distributions that can be used to quantitatively assess the biological relevance of diffusion weighted MRI models, e.g., constrained spherical deconvolution (CSD). Here, we apply deep learning to investigate the potential of single shell diffusion weighted MRI to explain histologically observed fiber orientation distributions (FOD) and compare the derived deep learning model with a leading CSD approach. This study (1) demonstrates that there exists additional information in the diffusion signal that is not currently exploited by CSD, and (2) provides an illustrative data-driven model that makes use of this information.



### Extracting 2D weak labels from volume labels using multiple instance learning in CT hemorrhage detection
- **Arxiv ID**: http://arxiv.org/abs/1911.05650v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.05650v1)
- **Published**: 2019-11-13 17:24:21+00:00
- **Updated**: 2019-11-13 17:24:21+00:00
- **Authors**: Samuel W. Remedios, Zihao Wu, Camilo Bermudez, Cailey I. Kerley, Snehashis Roy, Mayur B. Patel, John A. Butman, Bennett A. Landman, Dzung L. Pham
- **Comment**: None
- **Journal**: None
- **Summary**: Multiple instance learning (MIL) is a supervised learning methodology that aims to allow models to learn instance class labels from bag class labels, where a bag is defined to contain multiple instances. MIL is gaining traction for learning from weak labels but has not been widely applied to 3D medical imaging. MIL is well-suited to clinical CT acquisitions since (1) the highly anisotropic voxels hinder application of traditional 3D networks and (2) patch-based networks have limited ability to learn whole volume labels. In this work, we apply MIL with a deep convolutional neural network to identify whether clinical CT head image volumes possess one or more large hemorrhages (> 20cm$^3$), resulting in a learned 2D model without the need for 2D slice annotations. Individual image volumes are considered separate bags, and the slices in each volume are instances. Such a framework sets the stage for incorporating information obtained in clinical reports to help train a 2D segmentation approach. Within this context, we evaluate the data requirements to enable generalization of MIL by varying the amount of training data. Our results show that a training size of at least 400 patient image volumes was needed to achieve accurate per-slice hemorrhage detection. Over a five-fold cross-validation, the leading model, which made use of the maximum number of training volumes, had an average true positive rate of 98.10%, an average true negative rate of 99.36%, and an average precision of 0.9698. The models have been made available along with source code to enabled continued exploration and adaption of MIL in CT neuroimaging.



### Momentum Contrast for Unsupervised Visual Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.05722v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.05722v3)
- **Published**: 2019-11-13 18:53:26+00:00
- **Updated**: 2020-03-23 18:36:55+00:00
- **Authors**: Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick
- **Comment**: CVPR 2020 camera-ready. Code:
  https://github.com/facebookresearch/moco
- **Journal**: None
- **Summary**: We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.



### Visual-Inertial Localization for Skid-Steering Robots with Kinematic Constraints
- **Arxiv ID**: http://arxiv.org/abs/1911.05787v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.05787v1)
- **Published**: 2019-11-13 20:00:51+00:00
- **Updated**: 2019-11-13 20:00:51+00:00
- **Authors**: Xingxing Zuo, Mingming Zhang, Yiming Chen, Yong Liu, Guoquan Huang, Mingyang Li
- **Comment**: 16 pages, 5 figures, Published
- **Journal**: None
- **Summary**: While visual localization or SLAM has witnessed great progress in past decades, when deploying it on a mobile robot in practice, few works have explicitly considered the kinematic (or dynamic) constraints of the real robotic system when designing state estimators. To promote the practical deployment of current state-of-the-art visual-inertial localization algorithms, in this work we propose a low-cost kinematics-constrained localization system particularly for a skid-steering mobile robot. In particular, we derive in a principle way the robot's kinematic constraints based on the instantaneous centers of rotation (ICR) model and integrate them in a tightly-coupled manner into the sliding-window bundle adjustment (BA)-based visual-inertial estimator. Because the ICR model parameters are time-varying due to, for example, track-to-terrain interaction and terrain roughness, we estimate these kinematic parameters online along with the navigation state. To this end, we perform in-depth the observability analysis and identify motion conditions under which the state/parameter estimation is viable. The proposed kinematics-constrained visual-inertial localization system has been validated extensively in different terrain scenarios.



### BiNet: Degraded-Manuscript Binarization in Diverse Document Textures and Layouts using Deep Encoder-Decoder Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.07930v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07930v1)
- **Published**: 2019-11-13 20:12:35+00:00
- **Updated**: 2019-11-13 20:12:35+00:00
- **Authors**: Maruf A. Dhali, Jan Willem de Wit, Lambert Schomaker
- **Comment**: 26 pages, 15 figures, 11 tables
- **Journal**: None
- **Summary**: Handwritten document-image binarization is a semantic segmentation process to differentiate ink pixels from background pixels. It is one of the essential steps towards character recognition, writer identification, and script-style evolution analysis. The binarization task itself is challenging due to the vast diversity of writing styles, inks, and paper materials. It is even more difficult for historical manuscripts due to the aging and degradation of the documents over time. One of such manuscripts is the Dead Sea Scrolls (DSS) image collection, which poses extreme challenges for the existing binarization techniques. This article proposes a new binarization technique for the DSS images using the deep encoder-decoder networks. Although the artificial neural network proposed here is primarily designed to binarize the DSS images, it can be trained on different manuscript collections as well. Additionally, the use of transfer learning makes the network already utilizable for a wide range of handwritten documents, making it a unique multi-purpose tool for binarization. Qualitative results and several quantitative comparisons using both historical manuscripts and datasets from handwritten document image binarization competition (H-DIBCO and DIBCO) exhibit the robustness and the effectiveness of the system. The best performing network architecture proposed here is a variant of the U-Net encoder-decoders.



### Variable Star Classification Using Multi-View Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.05821v1
- **DOI**: 10.1093/mnras/stz3165
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.05821v1)
- **Published**: 2019-11-13 21:37:53+00:00
- **Updated**: 2019-11-13 21:37:53+00:00
- **Authors**: K. B. Johnston, S. M. Caballero-Nieves, V. Petit, A. M. Peter, R. Haber
- **Comment**: 16 pages, 11 figures
- **Journal**: None
- **Summary**: Our multi-view metric learning framework enables robust characterization of star categories by directly learning to discriminate in a multi-faceted feature space, thus, eliminating the need to combine feature representations prior to fitting the machine learning model. We also demonstrate how to extend standard multi-view learning, which employs multiple vectorized views, to the matrix-variate case which allows very novel variable star signature representations. The performance of our proposed methods is evaluated on the UCR Starlight and LINEAR datasets. Both the vector and matrix-variate versions of our multi-view learning framework perform favorably --- demonstrating the ability to discriminate variable star categories.



### Accelerating cardiac cine MRI using a deep learning-based ESPIRiT reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1911.05845v3
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.05845v3)
- **Published**: 2019-11-13 22:41:25+00:00
- **Updated**: 2020-05-18 22:44:50+00:00
- **Authors**: Christopher M. Sandino, Peng Lai, Shreyas S. Vasanawala, Joseph Y. Cheng
- **Comment**: 29 pages, 9 figures, 1 table, 7 supplementary videos, Submitted to
  Magnetic Resonance in Medicine
- **Journal**: None
- **Summary**: A novel neural network architecture, known as DL-ESPIRiT, is proposed to reconstruct rapidly acquired cardiac MRI data without field-of-view limitations which are present in previously proposed deep learning-based reconstruction frameworks. Additionally, a novel convolutional neural network based on separable 3D convolutions is integrated into DL-ESPIRiT to more efficiently learn spatiotemporal priors for dynamic image reconstruction. The network is trained on fully-sampled 2D cardiac cine datasets collected from eleven healthy volunteers with IRB approval. DL-ESPIRiT is compared against a state-of-the-art parallel imaging and compressed sensing method known as $l_1$-ESPIRiT. The reconstruction accuracy of both methods is evaluated on retrospectively undersampled datasets (R=12) with respect to standard image quality metrics as well as automatic deep learning-based segmentations of left ventricular volumes. Feasibility of this approach is demonstrated in reconstructions of prospectively undersampled data which were acquired in a single heartbeat per slice.



### SpiralNet++: A Fast and Highly Efficient Mesh Convolution Operator
- **Arxiv ID**: http://arxiv.org/abs/1911.05856v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.05856v1)
- **Published**: 2019-11-13 22:59:19+00:00
- **Updated**: 2019-11-13 22:59:19+00:00
- **Authors**: Shunwang Gong, Lei Chen, Michael Bronstein, Stefanos Zafeiriou
- **Comment**: The IEEE International Conference on Computer Vision (ICCV)
  Workshops, 2019
- **Journal**: None
- **Summary**: Intrinsic graph convolution operators with differentiable kernel functions play a crucial role in analyzing 3D shape meshes. In this paper, we present a fast and efficient intrinsic mesh convolution operator that does not rely on the intricate design of kernel function. We explicitly formulate the order of aggregating neighboring vertices, instead of learning weights between nodes, and then a fully connected layer follows to fuse local geometric structure information with vertex features. We provide extensive evidence showing that models based on this convolution operator are easier to train, and can efficiently learn invariant shape features. Specifically, we evaluate our method on three different types of tasks of dense shape correspondence, 3D facial expression classification, and 3D shape reconstruction, and show that it significantly outperforms state-of-the-art approaches while being significantly faster, without relying on shape descriptors. Our source code is available on GitHub.



### Motion Reasoning for Goal-Based Imitation Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.05864v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.05864v1)
- **Published**: 2019-11-13 23:59:44+00:00
- **Updated**: 2019-11-13 23:59:44+00:00
- **Authors**: De-An Huang, Yu-Wei Chao, Chris Paxton, Xinke Deng, Li Fei-Fei, Juan Carlos Niebles, Animesh Garg, Dieter Fox
- **Comment**: None
- **Journal**: None
- **Summary**: We address goal-based imitation learning, where the aim is to output the symbolic goal from a third-person video demonstration. This enables the robot to plan for execution and reproduce the same goal in a completely different environment. The key challenge is that the goal of a video demonstration is often ambiguous at the level of semantic actions. The human demonstrators might unintentionally achieve certain subgoals in the demonstrations with their actions. Our main contribution is to propose a motion reasoning framework that combines task and motion planning to disambiguate the true intention of the demonstrator in the video demonstration. This allows us to robustly recognize the goals that cannot be disambiguated by previous action-based approaches. We evaluate our approach by collecting a dataset of 96 video demonstrations in a mockup kitchen environment. We show that our motion reasoning plays an important role in recognizing the actual goal of the demonstrator and improves the success rate by over 20%. We further show that by using the automatically inferred goal from the video demonstration, our robot is able to reproduce the same task in a real kitchen environment.



