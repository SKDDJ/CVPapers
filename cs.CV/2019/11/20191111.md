# Arxiv Papers in cs.CV on 2019-11-11
### Hierarchically Robust Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.04047v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.04047v2)
- **Published**: 2019-11-11 02:51:46+00:00
- **Updated**: 2020-03-27 18:21:02+00:00
- **Authors**: Qi Qian, Juhua Hu, Hao Li
- **Comment**: accepted by CVPR'20
- **Journal**: None
- **Summary**: With the tremendous success of deep learning in visual tasks, the representations extracted from intermediate layers of learned models, that is, deep features, attract much attention of researchers. Previous empirical analysis shows that those features can contain appropriate semantic information. Therefore, with a model trained on a large-scale benchmark data set (e.g., ImageNet), the extracted features can work well on other tasks. In this work, we investigate this phenomenon and demonstrate that deep features can be suboptimal due to the fact that they are learned by minimizing the empirical risk. When the data distribution of the target task is different from that of the benchmark data set, the performance of deep features can degrade. Hence, we propose a hierarchically robust optimization method to learn more generic features. Considering the example-level and concept-level robustness simultaneously, we formulate the problem as a distributionally robust optimization problem with Wasserstein ambiguity set constraints, and an efficient algorithm with the conventional training pipeline is proposed. Experiments on benchmark data sets demonstrate the effectiveness of the robust deep representations.



### Open-Ended Visual Question Answering by Multi-Modal Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1911.04058v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.04058v1)
- **Published**: 2019-11-11 03:26:58+00:00
- **Updated**: 2019-11-11 03:26:58+00:00
- **Authors**: Yiming Xu, Lin Chen, Zhongwei Cheng, Lixin Duan, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of visual question answering (VQA) in images by exploiting supervised domain adaptation, where there is a large amount of labeled data in the source domain but only limited labeled data in the target domain with the goal to train a good target model. A straightforward solution is to fine-tune a pre-trained source model by using those limited labeled target data, but it usually cannot work well due to the considerable difference between the data distributions of the source and target domains. Moreover, the availability of multiple modalities (i.e., images, questions and answers) in VQA poses further challenges to model the transferability between those different modalities. In this paper, we tackle the above issues by proposing a novel supervised multi-modal domain adaptation method for VQA to learn joint feature embeddings across different domains and modalities. Specifically, we align the data distributions of the source and target domains by considering all modalities together as well as separately for each individual modality. Based on the extensive experiments on the benchmark VQA 2.0 and VizWiz datasets for the realistic open-ended VQA task, we demonstrate that our proposed method outperforms the existing state-of-the-art approaches in this challenging domain adaptation setting for VQA.



### Cluster-wise Unsupervised Hashing for Cross-Modal Similarity Search
- **Arxiv ID**: http://arxiv.org/abs/1911.07923v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1911.07923v2)
- **Published**: 2019-11-11 05:50:01+00:00
- **Updated**: 2019-12-29 11:51:18+00:00
- **Authors**: Lu Wang, Jie Yang
- **Comment**: 13 pages, 26 figures
- **Journal**: None
- **Summary**: Large-scale cross-modal hashing similarity retrieval has attracted more and more attention in modern search applications such as search engines and autopilot, showing great superiority in computation and storage. However, current unsupervised cross-modal hashing methods still have some limitations: (1)many methods relax the discrete constraints to solve the optimization objective which may significantly degrade the retrieval performance;(2)most existing hashing model project heterogenous data into a common latent space, which may always lose sight of diversity in heterogenous data;(3)transforming real-valued data point to binary codes always results in abundant loss of information, producing the suboptimal continuous latent space. To overcome above problems, in this paper, a novel Cluster-wise Unsupervised Hashing (CUH) method is proposed. Specifically, CUH jointly performs the multi-view clustering that projects the original data points from different modalities into its own low-dimensional latent semantic space and finds the cluster centroid points and the common clustering indicators in its own low-dimensional space, and learns the compact hash codes and the corresponding linear hash functions. An discrete optimization framework is developed to learn the unified binary codes across modalities under the guidance cluster-wise code-prototypes. The reasonableness and effectiveness of CUH is well demonstrated by comprehensive experiments on diverse benchmark datasets.



### Activity Monitoring of Islamic Prayer (Salat) Postures using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.04102v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.04102v1)
- **Published**: 2019-11-11 06:31:40+00:00
- **Updated**: 2019-11-11 06:31:40+00:00
- **Authors**: Anis Koubaa, Adel Ammar, Bilel Benjdira, Abdullatif Al-Hadid, Belal Kawaf, Saleh Ali Al-Yahri, Abdelrahman Babiker, Koutaiba Assaf, Mohannad Ba Ras
- **Comment**: Submitted to the 6th International Conference on Data Science and
  Machine Learning Applications (CDMA 2020)
- **Journal**: None
- **Summary**: In the Muslim community, the prayer (i.e. Salat) is the second pillar of Islam, and it is the most essential and fundamental worshiping activity that believers have to perform five times a day. From a gestures' perspective, there are predefined human postures that must be performed in a precise manner. However, for several people, these postures are not correctly performed, due to being new to Salat or even having learned prayers in an incorrect manner. Furthermore, the time spent in each posture has to be balanced. To address these issues, we propose to develop an artificial intelligence assistive framework that guides worshippers to evaluate the correctness of the postures of their prayers. This paper represents the first step to achieve this objective and addresses the problem of the recognition of the basic gestures of Islamic prayer using Convolutional Neural Networks (CNN). The contribution of this paper lies in building a dataset for the basic Salat positions, and train a YOLOv3 neural network for the recognition of the gestures. Experimental results demonstrate that the mean average precision attains 85% for a training dataset of 764 images of the different postures. To the best of our knowledge, this is the first work that addresses human activity recognition of Salat using deep learning.



### Fast Learning of Temporal Action Proposal via Dense Boundary Generator
- **Arxiv ID**: http://arxiv.org/abs/1911.04127v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.04127v1)
- **Published**: 2019-11-11 08:15:13+00:00
- **Updated**: 2019-11-11 08:15:13+00:00
- **Authors**: Chuming Lin, Jian Li, Yabiao Wang, Ying Tai, Donghao Luo, Zhipeng Cui, Chengjie Wang, Jilin Li, Feiyue Huang, Rongrong Ji
- **Comment**: Accepted by AAAI 2020. Ranked No. 1 on ActivityNet Challenge 2019 on
  Temporal Action Proposals
  (http://activity-net.org/challenges/2019/evaluation.html)
- **Journal**: None
- **Summary**: Generating temporal action proposals remains a very challenging problem, where the main issue lies in predicting precise temporal proposal boundaries and reliable action confidence in long and untrimmed real-world videos. In this paper, we propose an efficient and unified framework to generate temporal action proposals named Dense Boundary Generator (DBG), which draws inspiration from boundary-sensitive methods and implements boundary classification and action completeness regression for densely distributed proposals. In particular, the DBG consists of two modules: Temporal boundary classification (TBC) and Action-aware completeness regression (ACR). The TBC aims to provide two temporal boundary confidence maps by low-level two-stream features, while the ACR is designed to generate an action completeness score map by high-level action-aware features. Moreover, we introduce a dual stream BaseNet (DSB) to encode RGB and optical flow information, which helps to capture discriminative boundary and actionness features. Extensive experiments on popular benchmarks ActivityNet-1.3 and THUMOS14 demonstrate the superiority of DBG over the state-of-the-art proposal generator (e.g., MGG and BMN). Our code will be made available upon publication.



### Learning Graph Convolutional Network for Skeleton-based Human Action Recognition by Neural Searching
- **Arxiv ID**: http://arxiv.org/abs/1911.04131v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.04131v1)
- **Published**: 2019-11-11 08:24:10+00:00
- **Updated**: 2019-11-11 08:24:10+00:00
- **Authors**: Wei Peng, Xiaopeng Hong, Haoyu Chen, Guoying Zhao
- **Comment**: Accepted by AAAI2020
- **Journal**: None
- **Summary**: Human action recognition from skeleton data, fueled by the Graph Convolutional Network (GCN), has attracted lots of attention, due to its powerful capability of modeling non-Euclidean structure data. However, many existing GCN methods provide a pre-defined graph and fix it through the entire network, which can loss implicit joint correlations. Besides, the mainstream spectral GCN is approximated by one-order hop, thus higher-order connections are not well involved. Therefore, huge efforts are required to explore a better GCN architecture. To address these problems, we turn to Neural Architecture Search (NAS) and propose the first automatically designed GCN for skeleton-based action recognition. Specifically, we enrich the search space by providing multiple dynamic graph modules after fully exploring the spatial-temporal correlations between nodes. Besides, we introduce multiple-hop modules and expect to break the limitation of representational capacity caused by one-order approximation. Moreover, a sampling- and memory-efficient evolution strategy is proposed to search an optimal architecture for this task. The resulted architecture proves the effectiveness of the higher-order approximation and the dynamic graph modeling mechanism with temporal interactions, which is barely discussed before. To evaluate the performance of the searched model, we conduct extensive experiments on two very large scaled datasets and the results show that our model gets the state-of-the-art results.



### Guided Weak Supervision for Action Recognition with Scarce Data to Assess Skills of Children with Autism
- **Arxiv ID**: http://arxiv.org/abs/1911.04140v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.04140v3)
- **Published**: 2019-11-11 08:51:40+00:00
- **Updated**: 2020-01-28 21:17:28+00:00
- **Authors**: Prashant Pandey, Prathosh AP, Manu Kohli, Josh Pritchard
- **Comment**: AAAI 2020
- **Journal**: None
- **Summary**: Diagnostic and intervention methodologies for skill assessment of autism typically requires a clinician repetitively initiating several stimuli and recording the child's response. In this paper, we propose to automate the response measurement through video recording of the scene following the use of Deep Neural models for human action recognition from videos. However, supervised learning of neural networks demand large amounts of annotated data that are hard to come by. This issue is addressed by leveraging the `similarities' between the action categories in publicly available large-scale video action (source) datasets and the dataset of interest. A technique called guided weak supervision is proposed, where every class in the target data is matched to a class in the source data using the principle of posterior likelihood maximization. Subsequently, classifier on the target data is re-trained by augmenting samples from the matched source classes, along with a new loss encouraging inter-class separability. The proposed method is evaluated on two skill assessment autism datasets, SSBD and a real world Autism dataset comprising 37 children of different ages and ethnicity who are diagnosed with autism. Our proposed method is found to improve the performance of the state-of-the-art multi-class human action recognition models in-spite of supervision with scarce data.



### Part-based Multi-stream Model for Vehicle Searching
- **Arxiv ID**: http://arxiv.org/abs/1911.04144v1
- **DOI**: 10.1109/ICPR.2018.8546191
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.04144v1)
- **Published**: 2019-11-11 08:58:07+00:00
- **Updated**: 2019-11-11 08:58:07+00:00
- **Authors**: Ya Sun, Minxian Li, Jianfeng Lu
- **Comment**: Published in International Conference on Pattern Recognition 2018
- **Journal**: None
- **Summary**: Due to the enormous requirement in public security and intelligent transportation system, searching an identical vehicle has become more and more important. Current studies usually treat vehicle as an integral object and then train a distance metric to measure the similarity among vehicles. However, these raw images may be exactly similar to ones with different identification and include some pixels in background that may disturb the distance metric learning. In this paper, we propose a novel and useful method to segment an original vehicle image into several discriminative foreground parts, and these parts consist of some fine grained regions that are named discriminative patches. After that, these parts combined with the raw image are fed into the proposed deep learning network. We can easily measure the similarity of two vehicle images by computing the Euclidean distance of the features from FC layer. Two main contributions of this paper are as follows. Firstly, a method is proposed to estimate if a patch in a raw vehicle image is discriminative or not. Secondly, a new Part-based Multi-Stream Model (PMSM) is designed and optimized for vehicle retrieval and re-identification tasks. We evaluate the proposed method on the VehicleID dataset, and the experimental results show that our method can outperform the baseline.



### Logo-2K+: A Large-Scale Logo Dataset for Scalable Logo Classification
- **Arxiv ID**: http://arxiv.org/abs/1911.07924v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07924v1)
- **Published**: 2019-11-11 09:24:08+00:00
- **Updated**: 2019-11-11 09:24:08+00:00
- **Authors**: Jing Wang, Weiqing Min, Sujuan Hou, Shengnan Ma, Yuanjie Zheng, Haishuai Wang, Shuqiang Jiang
- **Comment**: Accepted by AAAI2020
- **Journal**: None
- **Summary**: Logo classification has gained increasing attention for its various applications, such as copyright infringement detection, product recommendation and contextual advertising. Compared with other types of object images, the real-world logo images have larger variety in logo appearance and more complexity in their background. Therefore, recognizing the logo from images is challenging. To support efforts towards scalable logo classification task, we have curated a dataset, Logo-2K+, a new large-scale publicly available real-world logo dataset with 2,341 categories and 167,140 images. Compared with existing popular logo datasets, such as FlickrLogos-32 and LOGO-Net, Logo-2K+ has more comprehensive coverage of logo categories and larger quantity of logo images. Moreover, we propose a Discriminative Region Navigation and Augmentation Network (DRNA-Net), which is capable of discovering more informative logo regions and augmenting these image regions for logo classification. DRNA-Net consists of four sub-networks: the navigator sub-network first selected informative logo-relevant regions guided by the teacher sub-network, which can evaluate its confidence belonging to the ground-truth logo class. The data augmentation sub-network then augments the selected regions via both region cropping and region dropping. Finally, the scrutinizer sub-network fuses features from augmented regions and the whole image for logo classification. Comprehensive experiments on Logo-2K+ and other three existing benchmark datasets demonstrate the effectiveness of proposed method. Logo-2K+ and the proposed strong baseline DRNA-Net are expected to further the development of scalable logo image recognition, and the Logo-2K+ dataset can be found at https://github.com/msn199959/Logo-2k-plus-Dataset.



### Explaining Away Results in Accurate and Tolerant Template Matching
- **Arxiv ID**: http://arxiv.org/abs/1911.04169v1
- **DOI**: 10.1016/j.patcog.2020.107337
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.04169v1)
- **Published**: 2019-11-11 10:44:42+00:00
- **Updated**: 2019-11-11 10:44:42+00:00
- **Authors**: M. W. Spratling
- **Comment**: None
- **Journal**: None
- **Summary**: Recognising and locating image patches or sets of image features is an important task underlying much work in computer vision. Traditionally this has been accomplished using template matching. However, template matching is notoriously brittle in the face of changes in appearance caused by, for example, variations in viewpoint, partial occlusion, and non-rigid deformations. This article tests a method of template matching that is more tolerant to such changes in appearance and that can, therefore, more accurately identify image patches. In traditional template matching the comparison between a template and the image is independent of the other templates. In contrast, the method advocated here takes into account the evidence provided by the image for the template at each location and the full range of alternative explanations represented by the same template at other locations and by other templates. Specifically, the proposed method of template matching is performed using a form of probabilistic inference known as "explaining away". The algorithm used to implement explaining away has previously been used to simulate several neurobiological mechanisms, and been applied to image contour detection and pattern recognition tasks. Here it is applied for the first time to image patch matching, and is shown to produce superior results in comparison to the current state-of-the-art methods.



### Compositional Hierarchical Tensor Factorization: Representing Hierarchical Intrinsic and Extrinsic Causal Factors
- **Arxiv ID**: http://arxiv.org/abs/1911.04180v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, math.DG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.04180v2)
- **Published**: 2019-11-11 11:03:53+00:00
- **Updated**: 2020-01-01 06:23:19+00:00
- **Authors**: M. Alex O. Vasilescu, Eric Kim
- **Comment**: VERS 2: Fixed out of sync ref. Added
  [7,14,15,28,37,50,52,53,61,77,78] M.A.O. Vasilescu and E.Kim. Compositional
  Hierarchical Tensor Factorization: Representing Hierarchical Intrinsic and
  Extrinsic Causal Factors. In 25th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining (KDD'19): Tensor Methods for Emerging Data Science
  Challenges, August 04-08, 2019, Anchorage, AK.ACM, New York, NY
- **Journal**: 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
  (KDD'19): Tensor Methods for Emerging Data Science Challenges Workshop,
  August 04-08, 2019, Anchorage, AK.ACM, New York, NY
- **Summary**: Visual objects are composed of a recursive hierarchy of perceptual wholes and parts, whose properties, such as shape, reflectance, and color, constitute a hierarchy of intrinsic causal factors of object appearance. However, object appearance is the compositional consequence of both an object's intrinsic and extrinsic causal factors, where the extrinsic causal factors are related to illumination, and imaging conditions. Therefore, this paper proposes a unified tensor model of wholes and parts, and introduces a compositional hierarchical tensor factorization that disentangles the hierarchical causal structure of object image formation, and subsumes multilinear block tensor decomposition as a special case. The resulting object representation is an interpretable combinatorial choice of wholes' and parts' representations that renders object recognition robust to occlusion and reduces training data requirements. We demonstrate ourapproach in the context of face recognition by training on an extremely reduced dataset of synthetic images, and report encouragingface verification results on two datasets - the Freiburg dataset, andthe Labeled Face in the Wild (LFW) dataset consisting of real world images, thus, substantiating the suitability of our approach for data starved domains.



### Keep it Consistent: Topic-Aware Storytelling from an Image Stream via Iterative Multi-agent Communication
- **Arxiv ID**: http://arxiv.org/abs/1911.04192v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.04192v2)
- **Published**: 2019-11-11 11:35:21+00:00
- **Updated**: 2020-10-30 07:08:10+00:00
- **Authors**: Ruize Wang, Zhongyu Wei, Ying Cheng, Piji Li, Haijun Shan, Ji Zhang, Qi Zhang, Xuanjing Huang
- **Comment**: Accepted to COLING 2020
- **Journal**: None
- **Summary**: Visual storytelling aims to generate a narrative paragraph from a sequence of images automatically. Existing approaches construct text description independently for each image and roughly concatenate them as a story, which leads to the problem of generating semantically incoherent content. In this paper, we propose a new way for visual storytelling by introducing a topic description task to detect the global semantic context of an image stream. A story is then constructed with the guidance of the topic description. In order to combine the two generation tasks, we propose a multi-agent communication framework that regards the topic description generator and the story generator as two agents and learn them simultaneously via iterative updating mechanism. We validate our approach on VIST dataset, where quantitative results, ablations, and human evaluation demonstrate our method's good ability in generating stories with higher quality compared to state-of-the-art methods.



### PVN3D: A Deep Point-wise 3D Keypoints Voting Network for 6DoF Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1911.04231v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1911.04231v2)
- **Published**: 2019-11-11 13:13:21+00:00
- **Updated**: 2020-03-24 10:16:31+00:00
- **Authors**: Yisheng He, Wei Sun, Haibin Huang, Jianran Liu, Haoqiang Fan, Jian Sun
- **Comment**: Accepted to Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition, 2020. (CVPR 2020)
- **Journal**: None
- **Summary**: In this work, we present a novel data-driven method for robust 6DoF object pose estimation from a single RGBD image. Unlike previous methods that directly regressing pose parameters, we tackle this challenging task with a keypoint-based approach. Specifically, we propose a deep Hough voting network to detect 3D keypoints of objects and then estimate the 6D pose parameters within a least-squares fitting manner. Our method is a natural extension of 2D-keypoint approaches that successfully work on RGB based 6DoF estimation. It allows us to fully utilize the geometric constraint of rigid objects with the extra depth information and is easy for a network to learn and optimize. Extensive experiments were conducted to demonstrate the effectiveness of 3D-keypoint detection in the 6D pose estimation task. Experimental results also show our method outperforms the state-of-the-art methods by large margins on several benchmarks. Code and video are available at https://github.com/ethnhe/PVN3D.git.



### PoshakNet: Framework for matching dresses from real-life photos using GAN and Siamese Network
- **Arxiv ID**: http://arxiv.org/abs/1911.04237v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.04237v1)
- **Published**: 2019-11-11 13:17:01+00:00
- **Updated**: 2019-11-11 13:17:01+00:00
- **Authors**: Abhigyan Khaund, Daksh Thapar, Aditya Nigam
- **Comment**: Accepted in NCVPRIPG 2019
- **Journal**: None
- **Summary**: Online garment shopping has gained many customers in recent years. Describing a dress using keywords does not always yield the proper results, which in turn leads to dissatisfaction of customers. A visual search based system will be enormously beneficent to the industry. Hence, we propose a framework that can retrieve similar clothes that can be found in an image. The first task is to extract the garment from the input image (street photo). There are various challenges for that, including pose, illumination, and background clutter. We use a Generative Adversarial Network for the task of retrieving the garment that the person in the image was wearing. It has been shown that GAN can retrieve the garment very efficiently despite the challenges of street photos. Finally, a siamese based matching system takes the retrieved cloth image and matches it with the clothes in the dataset, giving us the top k matches. We take a pre-trained inception-ResNet v1 module as a siamese network (trained using triplet loss for face detection) and fine-tune it on the shopping dataset using center loss. The dataset has been collected inhouse. For training the GAN, we use the LookBook dataset, which is publically available.



### Recognition of Images of Korean Characters Using Embedded Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.04241v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.04241v2)
- **Published**: 2019-11-11 13:21:02+00:00
- **Updated**: 2019-12-03 11:31:28+00:00
- **Authors**: Sergey A. Ilyuhin, Alexander V. Sheshkus, Vladimir L. Arlazarov
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the significant success in the field of text recognition, complex and unsolved problems still exist in this field. In recent years, the recognition accuracy of the English language has greatly increased, while the problem of recognition of hieroglyphs has received much less attention. Hieroglyph recognition or image recognition with Korean, Japanese or Chinese characters have differences from the traditional text recognition task. This article discusses the main differences between hieroglyph languages and the Latin alphabet in the context of image recognition. A light-weight method for recognizing images of the hieroglyphs is proposed and tested on a public dataset of Korean hieroglyph images. Despite the existing solutions, the proposed method is suitable for mobile devices. Its recognition accuracy is better than the accuracy of the open-source OCR framework. The presented method of training embedded net bases on the similarities in the recognition data.



### Kernelized Similarity Learning and Embedding for Dynamic Texture Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1911.04254v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.04254v4)
- **Published**: 2019-11-11 13:37:27+00:00
- **Updated**: 2022-07-01 14:32:40+00:00
- **Authors**: Shiming Chen, Peng Zhang, Guo-Sen Xie, Qinmu Peng, Zehong Cao, Wei Yuan, Xinge You
- **Comment**: Accepted to IEEE Transactions on Systems, Man, and Cybernetics:
  Systems
- **Journal**: None
- **Summary**: Dynamic texture (DT) exhibits statistical stationarity in the spatial domain and stochastic repetitiveness in the temporal dimension, indicating that different frames of DT possess a high similarity correlation that is critical prior knowledge. However, existing methods cannot effectively learn a promising synthesis model for high-dimensional DT from a small number of training data. In this paper, we propose a novel DT synthesis method, which makes full use of similarity prior knowledge to address this issue. Our method bases on the proposed kernel similarity embedding, which not only can mitigate the high-dimensionality and small sample issues, but also has the advantage of modeling nonlinear feature relationship. Specifically, we first raise two hypotheses that are essential for DT model to generate new frames using similarity correlation. Then, we integrate kernel learning and extreme learning machine into a unified synthesis model to learn kernel similarity embedding for representing DT. Extensive experiments on DT videos collected from the internet and two benchmark datasets, i.e., Gatech Graphcut Textures and Dyntex, demonstrate that the learned kernel similarity embedding can effectively exhibit the discriminative representation for DT. Accordingly, our method is capable of preserving the long-term temporal continuity of the synthesized DT sequences with excellent sustainability and generalization. Meanwhile, it effectively generates realistic DT videos with fast speed and low computation, compared with the state-of-the-art methods. The code and more synthesis videos are available at our project page https://shiming-chen.github.io/Similarity-page/Similarit.html.



### Limited View and Sparse Photoacoustic Tomography for Neuroimaging with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.04357v2
- **DOI**: 10.1038/s41598-020-65235-2
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1911.04357v2)
- **Published**: 2019-11-11 15:59:11+00:00
- **Updated**: 2020-06-27 18:01:53+00:00
- **Authors**: Steven Guan, Amir A. Khan, Siddhartha Sikdar, Parag V. Chitnis
- **Comment**: None
- **Journal**: Sci Rep 10, 8510 (2020)
- **Summary**: Photoacoustic tomography (PAT) is a nonionizing imaging modality capable of acquiring high contrast and resolution images of optical absorption at depths greater than traditional optical imaging techniques. Practical considerations with instrumentation and geometry limit the number of available acoustic sensors and their view of the imaging target, which result in significant image reconstruction artifacts degrading image quality. Iterative reconstruction methods can be used to reduce artifacts but are computationally expensive. In this work, we propose a novel deep learning approach termed pixelwise deep learning (PixelDL) that first employs pixelwise interpolation governed by the physics of photoacoustic wave propagation and then uses a convolution neural network to directly reconstruct an image. Simulated photoacoustic data from synthetic vasculature phantom and mouse-brain vasculature were used for training and testing, respectively. Results demonstrated that PixelDL achieved comparable performance to iterative methods and outperformed other CNN-based approaches for correcting artifacts. PixelDL is a computationally efficient approach that enables for realtime PAT rendering and for improved image quality, quantification, and interpretation.



### Conditionally Learn to Pay Attention for Sequential Visual Task
- **Arxiv ID**: http://arxiv.org/abs/1911.04365v1
- **DOI**: 10.1109/ACCESS.2020.2982571
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.04365v1)
- **Published**: 2019-11-11 16:11:46+00:00
- **Updated**: 2019-11-11 16:11:46+00:00
- **Authors**: Jun He, Quan-Jie Cao, Lei Zhang
- **Comment**: None
- **Journal**: IEEE Access 8(2020) 56695-56710
- **Summary**: Sequential visual task usually requires to pay attention to its current interested object conditional on its previous observations. Different from popular soft attention mechanism, we propose a new attention framework by introducing a novel conditional global feature which represents the weak feature descriptor of the current focused object. Specifically, for a standard CNN (Convolutional Neural Network) pipeline, the convolutional layers with different receptive fields are used to produce the attention maps by measuring how the convolutional features align to the conditional global feature. The conditional global feature can be generated by different recurrent structure according to different visual tasks, such as a simple recurrent neural network for multiple objects recognition, or a moderate complex language model for image caption. Experiments show that our proposed conditional attention model achieves the best performance on the SVHN (Street View House Numbers) dataset with / without extra bounding box; and for image caption, our attention model generates better scores than the popular soft attention model.



### Disentangle, align and fuse for multimodal and semi-supervised image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.04417v5
- **DOI**: 10.1109/TMI.2020.3036584
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.04417v5)
- **Published**: 2019-11-11 17:44:00+00:00
- **Updated**: 2020-11-09 19:18:39+00:00
- **Authors**: Agisilaos Chartsias, Giorgos Papanastasiou, Chengjia Wang, Scott Semple, David E. Newby, Rohan Dharmakumar, Sotirios A. Tsaftaris
- **Comment**: None
- **Journal**: IEEE Transactions on Medical Imaging (2020)
- **Summary**: Magnetic resonance (MR) protocols rely on several sequences to assess pathology and organ status properly. Despite advances in image analysis, we tend to treat each sequence, here termed modality, in isolation. Taking advantage of the common information shared between modalities (an organ's anatomy) is beneficial for multi-modality processing and learning. However, we must overcome inherent anatomical misregistrations and disparities in signal intensity across the modalities to obtain this benefit. We present a method that offers improved segmentation accuracy of the modality of interest (over a single input model), by learning to leverage information present in other modalities, even if few (semi-supervised) or no (unsupervised) annotations are available for this specific modality. Core to our method is learning a disentangled decomposition into anatomical and imaging factors. Shared anatomical factors from the different inputs are jointly processed and fused to extract more accurate segmentation masks. Image misregistrations are corrected with a Spatial Transformer Network, which non-linearly aligns the anatomical factors. The imaging factor captures signal intensity characteristics across different modality data and is used for image reconstruction, enabling semi-supervised learning. Temporal and slice pairing between inputs are learned dynamically. We demonstrate applications in Late Gadolinium Enhanced (LGE) and Blood Oxygenation Level Dependent (BOLD) cardiac segmentation, as well as in T2 abdominal segmentation. Code is available at https://github.com/vios-s/multimodal_segmentation.



### Streaming convolutional neural networks for end-to-end learning with multi-megapixel images
- **Arxiv ID**: http://arxiv.org/abs/1911.04432v1
- **DOI**: 10.1109/TPAMI.2020.3019563
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.04432v1)
- **Published**: 2019-11-11 18:18:22+00:00
- **Updated**: 2019-11-11 18:18:22+00:00
- **Authors**: Hans Pinckaers, Bram van Ginneken, Geert Litjens
- **Comment**: In review
- **Journal**: None
- **Summary**: Due to memory constraints on current hardware, most convolution neural networks (CNN) are trained on sub-megapixel images. For example, most popular datasets in computer vision contain images much less than a megapixel in size (0.09MP for ImageNet and 0.001MP for CIFAR-10). In some domains such as medical imaging, multi-megapixel images are needed to identify the presence of disease accurately. We propose a novel method to directly train convolutional neural networks using any input image size end-to-end. This method exploits the locality of most operations in modern convolutional neural networks by performing the forward and backward pass on smaller tiles of the image. In this work, we show a proof of concept using images of up to 66-megapixels (8192x8192), saving approximately 50GB of memory per image. Using two public challenge datasets, we demonstrate that CNNs can learn to extract relevant information from these large images and benefit from increasing resolution. We improved the area under the receiver-operating characteristic curve from 0.580 (4MP) to 0.706 (66MP) for metastasis detection in breast cancer (CAMELYON17). We also obtained a Spearman correlation metric approaching state-of-the-art performance on the TUPAC16 dataset, from 0.485 (1MP) to 0.570 (16MP). Code to reproduce a subset of the experiments is available at https://github.com/DIAGNijmegen/StreamingCNN.



### Structural Pruning in Deep Neural Networks: A Small-World Approach
- **Arxiv ID**: http://arxiv.org/abs/1911.04453v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.04453v1)
- **Published**: 2019-11-11 18:53:50+00:00
- **Updated**: 2019-11-11 18:53:50+00:00
- **Authors**: Gokul Krishnan, Xiaocong Du, Yu Cao
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) are usually over-parameterized, causing excessive memory and interconnection cost on the hardware platform. Existing pruning approaches remove secondary parameters at the end of training to reduce the model size; but without exploiting the intrinsic network property, they still require the full interconnection to prepare the network. Inspired by the observation that brain networks follow the Small-World model, we propose a novel structural pruning scheme, which includes (1) hierarchically trimming the network into a Small-World model before training, (2) training the network for a given dataset, and (3) optimizing the network for accuracy. The new scheme effectively reduces both the model size and the interconnection needed before training, achieving a locally clustered and globally sparse model. We demonstrate our approach on LeNet-5 for MNIST and VGG-16 for CIFAR-10, decreasing the number of parameters to 2.3% and 9.02% of the baseline model, respectively.



### 360SD-Net: 360° Stereo Depth Estimation with Learnable Cost Volume
- **Arxiv ID**: http://arxiv.org/abs/1911.04460v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.04460v2)
- **Published**: 2019-11-11 18:56:49+00:00
- **Updated**: 2020-03-26 15:51:54+00:00
- **Authors**: Ning-Hsu Wang, Bolivar Solarte, Yi-Hsuan Tsai, Wei-Chen Chiu, Min Sun
- **Comment**: Accepted to 2020 IEEE International Conference on Robotics and
  Automation (ICRA 2020). Project page and code are at
  https://albert100121.github.io/360SD-Net-Project-Page
- **Journal**: None
- **Summary**: Recently, end-to-end trainable deep neural networks have significantly improved stereo depth estimation for perspective images. However, 360{\deg} images captured under equirectangular projection cannot benefit from directly adopting existing methods due to distortion introduced (i.e., lines in 3D are not projected onto lines in 2D). To tackle this issue, we present a novel architecture specifically designed for spherical disparity using the setting of top-bottom 360{\deg} camera pairs. Moreover, we propose to mitigate the distortion issue by (1) an additional input branch capturing the position and relation of each pixel in the spherical coordinate, and (2) a cost volume built upon a learnable shifting filter. Due to the lack of 360{\deg} stereo data, we collect two 360{\deg} stereo datasets from Matterport3D and Stanford3D for training and evaluation. Extensive experiments and ablation study are provided to validate our method against existing algorithms. Finally, we show promising results on real-world environments capturing images with two consumer-level cameras.



### Self-training with Noisy Student improves ImageNet classification
- **Arxiv ID**: http://arxiv.org/abs/1911.04252v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.04252v4)
- **Published**: 2019-11-11 18:59:27+00:00
- **Updated**: 2020-06-19 17:36:57+00:00
- **Authors**: Qizhe Xie, Minh-Thang Luong, Eduard Hovy, Quoc V. Le
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: We present Noisy Student Training, a semi-supervised learning approach that works well even when labeled data is abundant. Noisy Student Training achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2.   Noisy Student Training extends the idea of self-training and distillation with the use of equal-or-larger student models and noise added to the student during learning. On ImageNet, we first train an EfficientNet model on labeled images and use it as a teacher to generate pseudo labels for 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the learning of the student, we inject noise such as dropout, stochastic depth, and data augmentation via RandAugment to the student so that the student generalizes better than the teacher. Models are available at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet. Code is available at https://github.com/google-research/noisystudent.



### Learning From Brains How to Regularize Machines
- **Arxiv ID**: http://arxiv.org/abs/1911.05072v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1911.05072v1)
- **Published**: 2019-11-11 21:53:26+00:00
- **Updated**: 2019-11-11 21:53:26+00:00
- **Authors**: Zhe Li, Wieland Brendel, Edgar Y. Walker, Erick Cobos, Taliah Muhammad, Jacob Reimer, Matthias Bethge, Fabian H. Sinz, Xaq Pitkow, Andreas S. Tolias
- **Comment**: 14 pages, 7 figures, NeurIPS 2019
- **Journal**: None
- **Summary**: Despite impressive performance on numerous visual tasks, Convolutional Neural Networks (CNNs) --- unlike brains --- are often highly sensitive to small perturbations of their input, e.g. adversarial noise leading to erroneous decisions. We propose to regularize CNNs using large-scale neuroscience data to learn more robust neural features in terms of representational similarity. We presented natural images to mice and measured the responses of thousands of neurons from cortical visual areas. Next, we denoised the notoriously variable neural activity using strong predictive models trained on this large corpus of responses from the mouse visual system, and calculated the representational similarity for millions of pairs of images from the model's predictions. We then used the neural representation similarity to regularize CNNs trained on image classification by penalizing intermediate representations that deviated from neural ones. This preserved performance of baseline models when classifying images under standard benchmarks, while maintaining substantially higher performance compared to baseline or control models when classifying noisy images. Moreover, the models regularized with cortical representations also improved model robustness in terms of adversarial attacks. This demonstrates that regularizing with neural data can be an effective tool to create an inductive bias towards more robust inference.



### ContamiNet: Detecting Contamination in Municipal Solid Waste
- **Arxiv ID**: http://arxiv.org/abs/1911.04583v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.04583v1)
- **Published**: 2019-11-11 22:10:40+00:00
- **Updated**: 2019-11-11 22:10:40+00:00
- **Authors**: Khoury Ibrahim, Danielle A. Savage, Addie Schnirel, Paul Intrevado, Yannet Interian
- **Comment**: 8 pages, 3 figures, ICMLA 2020
- **Journal**: None
- **Summary**: Leveraging over 30,000 images each with up to 89 labels collected by Recology---an integrated resource recovery company with both residential and commercial trash, recycling and composting services---the authors develop ContamiNet, a convolutional neural network, to identify contaminating material in residential recycling and compost bins. When training the model on a subset of labels that meet a minimum frequency threshold, ContamiNet preforms almost as well human experts in detecting contamination (0.86 versus 0.88 AUC). Recology is actively piloting ContamiNet in their daily municipal solid waste (MSW) collection to identify contaminants in recycling and compost bins to subsequently inform and educate customers about best sorting practices.



