# Arxiv Papers in cs.CV on 2019-11-28
### 3D Shape Completion with Multi-view Consistent Inference
- **Arxiv ID**: http://arxiv.org/abs/1911.12465v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12465v1)
- **Published**: 2019-11-28 00:01:52+00:00
- **Updated**: 2019-11-28 00:01:52+00:00
- **Authors**: Tao Hu, Zhizhong Han, Matthias Zwicker
- **Comment**: Accepted to AAAI 2020 as oral presentation
- **Journal**: None
- **Summary**: 3D shape completion is important to enable machines to perceive the complete geometry of objects from partial observations. To address this problem, view-based methods have been presented. These methods represent shapes as multiple depth images, which can be back-projected to yield corresponding 3D point clouds, and they perform shape completion by learning to complete each depth image using neural networks. While view-based methods lead to state-of-the-art results, they currently do not enforce geometric consistency among the completed views during the inference stage. To resolve this issue, we propose a multi-view consistent inference technique for 3D shape completion, which we express as an energy minimization problem including a data term and a regularization term. We formulate the regularization term as a consistency loss that encourages geometric consistency among multiple views, while the data term guarantees that the optimized views do not drift away too much from a learned shape descriptor. Experimental results demonstrate that our method completes shapes more accurately than previous techniques.



### Towards Reliable Evaluation of Road Network Reconstructions
- **Arxiv ID**: http://arxiv.org/abs/1911.12467v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12467v1)
- **Published**: 2019-11-28 00:16:16+00:00
- **Updated**: 2019-11-28 00:16:16+00:00
- **Authors**: Leonardo Citraro, Mateusz Kozi≈Ñski, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: Existing performance measures rank delineation algorithms inconsistently, which makes it difficult to decide which one is best in any given situation. We show that these inconsistencies stem from design flaws that make the metrics insensitive to whole classes of errors. To provide more reliable evaluation, we design three new metrics that are far more consistent even though they use very different approaches to comparing ground-truth and reconstructed road networks. We use both synthetic and real data to demonstrate this and advocate the use of these corrected metrics as a tool to gauge future progress.



### Learning Multi-level Weight-centric Features for Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.12476v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.12476v2)
- **Published**: 2019-11-28 01:22:59+00:00
- **Updated**: 2021-05-04 05:58:52+00:00
- **Authors**: Mingjiang Liang, Shaoli Huang, Shirui Pan, Mingming Gong, Wei Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot learning is currently enjoying a considerable resurgence of interest, aided by the recent advance of deep learning. Contemporary approaches based on weight-generation scheme delivers a straightforward and flexible solution to the problem. However, they did not fully consider both the representation power for unseen categories and weight generation capacity in feature learning, making it a significant performance bottleneck. This paper proposes a multi-level weight-centric feature learning to give full play to feature extractor's dual roles in few-shot learning. Our proposed method consists of two essential techniques: a weight-centric training strategy to improve the features' prototype-ability and a multi-level feature incorporating a mid- and relation-level information. The former increases the feasibility of constructing a discriminative decision boundary based on a few samples. Simultaneously, the latter helps improve the transferability for characterizing novel classes and preserve classification capability for base classes. We extensively evaluate our approach to low-shot classification benchmarks. Experiments demonstrate our proposed method significantly outperforms its counterparts in both standard and generalized settings and using different network backbones.



### QKD: Quantization-aware Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/1911.12491v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.12491v1)
- **Published**: 2019-11-28 02:27:27+00:00
- **Updated**: 2019-11-28 02:27:27+00:00
- **Authors**: Jangho Kim, Yash Bhalgat, Jinwon Lee, Chirag Patel, Nojun Kwak
- **Comment**: None
- **Journal**: None
- **Summary**: Quantization and Knowledge distillation (KD) methods are widely used to reduce memory and power consumption of deep neural networks (DNNs), especially for resource-constrained edge devices. Although their combination is quite promising to meet these requirements, it may not work as desired. It is mainly because the regularization effect of KD further diminishes the already reduced representation power of a quantized model. To address this short-coming, we propose Quantization-aware Knowledge Distillation (QKD) wherein quantization and KD are care-fully coordinated in three phases. First, Self-studying (SS) phase fine-tunes a quantized low-precision student network without KD to obtain a good initialization. Second, Co-studying (CS) phase tries to train a teacher to make it more quantizaion-friendly and powerful than a fixed teacher. Finally, Tutoring (TU) phase transfers knowledge from the trained teacher to the student. We extensively evaluate our method on ImageNet and CIFAR-10/100 datasets and show an ablation study on networks with both standard and depthwise-separable convolutions. The proposed QKD outperformed existing state-of-the-art methods (e.g., 1.3% improvement on ResNet-18 with W4A4, 2.6% on MobileNetV2 with W4A4). Additionally, QKD could recover the full-precision accuracy at as low as W3A3 quantization on ResNet and W6A6 quantization on MobilenetV2.



### An End-to-end Framework for Unconstrained Monocular 3D Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1911.12501v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.12501v1)
- **Published**: 2019-11-28 02:55:21+00:00
- **Updated**: 2019-11-28 02:55:21+00:00
- **Authors**: Sanjeev Sharma, Shaoli Huang, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: This work addresses the challenging problem of unconstrained 3D hand pose estimation using monocular RGB images. Most of the existing approaches assume some prior knowledge of hand (such as hand locations and side information) is available for 3D hand pose estimation. This restricts their use in unconstrained environments. We, therefore, present an end-to-end framework that robustly predicts hand prior information and accurately infers 3D hand pose by learning ConvNet models while only using keypoint annotations. To achieve robustness, the proposed framework uses a novel keypoint-based method to simultaneously predict hand regions and side labels, unlike existing methods that suffer from background color confusion caused by using segmentation or detection-based technology. Moreover, inspired by the biological structure of the human hand, we introduce two geometric constraints directly into the 3D coordinates prediction that further improves its performance in a weakly-supervised training. Experimental results show that our proposed framework not only performs robustly on unconstrained setting, but also outperforms the state-of-art methods on standard benchmark datasets.



### Error Resilient Deep Compressive Sensing
- **Arxiv ID**: http://arxiv.org/abs/1911.12507v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.12507v1)
- **Published**: 2019-11-28 03:16:39+00:00
- **Updated**: 2019-11-28 03:16:39+00:00
- **Authors**: Thuong, Nguyen Canh, Chien, Trinh Van
- **Comment**: 4 pages, 2 figures
- **Journal**: None
- **Summary**: Compressive sensing (CS) is an emerging sampling technology that enables reconstructing signals from a subset of measurements and even corrupted measurements. Deep learning-based compressive sensing (DCS) has improved CS performance while maintaining a fast reconstruction but requires a training network for each measurement rate. Also, concerning the transmission scheme of measurement lost, DCS cannot recover the original signal. Thereby, it fails to maintain the error-resilient property. In this work, we proposed a robust deep reconstruction network to preserve the error-resilient property under the assumption of random measurement lost. Measurement lost layer is proposed to simulate the measurement lost in an end-to-end framework.



### Action Recognition via Pose-Based Graph Convolutional Networks with Intermediate Dense Supervision
- **Arxiv ID**: http://arxiv.org/abs/1911.12509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12509v1)
- **Published**: 2019-11-28 03:28:50+00:00
- **Updated**: 2019-11-28 03:28:50+00:00
- **Authors**: Lei Shi, Yifan Zhang, Jian Cheng, Hanqing Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Pose-based action recognition has drawn considerable attention recently. Existing methods exploit the joint positions to extract the body-part features from the activation map of the convolutional networks to assist human action recognition. However, these features are simply concatenated or max-pooled in previous works. The structured correlations among the body parts, which are essential for understanding complex human actions, are not fully exploited. To address the problem, we propose a pose-based graph convolutional network (PGCN), which encodes the body-part features into a human-based spatiotemporal graph, and explicitly models their correlations with a novel light-weight adaptive graph convolutional module to produce a highly discriminative representation for human action recognition. Besides, we discover that the backbone network tends to identify patterns from the most discriminative areas of the input regardless of the others. Thus the features pooled by the joint positions from other areas are less informative, which consequently hampers the performance of the followed aggregation process for recognizing actions. To alleviate this issue, we introduce a simple intermediate dense supervision mechanism for the backbone network, which adequately addresses the problem with no extra computation cost during inference. We evaluate the proposed approach on three popular benchmarks for pose-based action recognition tasks, i.e., Sub-JHMDB, PennAction and NTU-RGBD, where our approach significantly outperforms state-of-the-arts without the bells and whistles.



### Rethinking Temporal Fusion for Video-based Person Re-identification on Semantic and Time Aspect
- **Arxiv ID**: http://arxiv.org/abs/1911.12512v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.12512v1)
- **Published**: 2019-11-28 03:35:57+00:00
- **Updated**: 2019-11-28 03:35:57+00:00
- **Authors**: Xinyang Jiang, Yifei Gong, Xiaowei Guo, Qize Yang, Feiyue Huang, Weishi Zheng, Feng Zheng, Xing Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, the research interest of person re-identification (ReID) has gradually turned to video-based methods, which acquire a person representation by aggregating frame features of an entire video. However, existing video-based ReID methods do not consider the semantic difference brought by the outputs of different network stages, which potentially compromises the information richness of the person features. Furthermore, traditional methods ignore important relationship among frames, which causes information redundancy in fusion along the time axis. To address these issues, we propose a novel general temporal fusion framework to aggregate frame features on both semantic aspect and time aspect. As for the semantic aspect, a multi-stage fusion network is explored to fuse richer frame features at multiple semantic levels, which can effectively reduce the information loss caused by the traditional single-stage fusion. While, for the time axis, the existing intra-frame attention method is improved by adding a novel inter-frame attention module, which effectively reduces the information redundancy in temporal fusion by taking the relationship among frames into consideration. The experimental results show that our approach can effectively improve the video-based re-identification accuracy, achieving the state-of-the-art performance.



### Palmprint Recognition in Uncontrolled and Uncooperative Environment
- **Arxiv ID**: http://arxiv.org/abs/1911.12514v1
- **DOI**: 10.1109/TIFS.2019.2945183
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12514v1)
- **Published**: 2019-11-28 03:38:32+00:00
- **Updated**: 2019-11-28 03:38:32+00:00
- **Authors**: Wojciech Michal Matkowski, Tingting Chai, Adams Wai Kin Kong
- **Comment**: Accepted in the IEEE Transactions on Information Forensics and
  Security
- **Journal**: None
- **Summary**: Online palmprint recognition and latent palmprint identification are two branches of palmprint studies. The former uses middle-resolution images collected by a digital camera in a well-controlled or contact-based environment with user cooperation for commercial applications and the latter uses high-resolution latent palmprints collected in crime scenes for forensic investigation. However, these two branches do not cover some palmprint images which have the potential for forensic investigation. Due to the prevalence of smartphone and consumer camera, more evidence is in the form of digital images taken in uncontrolled and uncooperative environment, e.g., child pornographic images and terrorist images, where the criminals commonly hide or cover their face. However, their palms can be observable. To study palmprint identification on images collected in uncontrolled and uncooperative environment, a new palmprint database is established and an end-to-end deep learning algorithm is proposed. The new database named NTU Palmprints from the Internet (NTU-PI-v1) contains 7881 images from 2035 palms collected from the Internet. The proposed algorithm consists of an alignment network and a feature extraction network and is end-to-end trainable. The proposed algorithm is compared with the state-of-the-art online palmprint recognition methods and evaluated on three public contactless palmprint databases, IITD, CASIA, and PolyU and two new databases, NTU-PI-v1 and NTU contactless palmprint database. The experimental results showed that the proposed algorithm outperforms the existing palmprint recognition methods.



### A Discriminative Learned CNN Embedding for Remote Sensing Image Scene Classification
- **Arxiv ID**: http://arxiv.org/abs/1911.12517v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12517v2)
- **Published**: 2019-11-28 03:51:57+00:00
- **Updated**: 2019-12-02 08:52:39+00:00
- **Authors**: Wen Wang, Lijun Du, Yinxing Gao, Yanzhou Su, Feng Wang, Jian Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, a discriminatively learned CNN embedding is proposed for remote sensing image scene classification. Our proposed siamese network simultaneously computes the classification loss function and the metric learning loss function of the two input images. Specifically, for the classification loss, we use the standard cross-entropy loss function to predict the classes of the images. For the metric learning loss, our siamese network learns to map the intra-class and inter-class input pairs to a feature space where intra-class inputs are close and inter-class inputs are separated by a margin. Concretely, for remote sensing image scene classification, we would like to map images from the same scene to feature vectors that are close, and map images from different scenes to feature vectors that are widely separated. Experiments are conducted on three different remote sensing image datasets to evaluate the effectiveness of our proposed approach. The results demonstrate that the proposed method achieves an excellent classification performance.



### Human Gist Processing Augments Deep Learning Breast Cancer Risk Assessment
- **Arxiv ID**: http://arxiv.org/abs/1912.05470v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.05470v1)
- **Published**: 2019-11-28 04:27:06+00:00
- **Updated**: 2019-11-28 04:27:06+00:00
- **Authors**: Skylar W. Wurster, Arkadiusz Sitek, Jian Chen, Karla Evans, Gaeun Kim, Jeremy M. Wolfe
- **Comment**: None
- **Journal**: None
- **Summary**: Radiologists can classify a mammogram as normal or abnormal at better than chance levels after less than a second's exposure to the images. In this work, we combine these radiologists' gist inputs into pre-trained machine learning models to validate that integrating gist with a CNN model can achieve an AUC (area under the curve) statistically significantly higher than either the gist perception of radiologists or the model without gist input.



### Sparse-GAN: Sparsity-constrained Generative Adversarial Network for Anomaly Detection in Retinal OCT Image
- **Arxiv ID**: http://arxiv.org/abs/1911.12527v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1911.12527v3)
- **Published**: 2019-11-28 04:46:48+00:00
- **Updated**: 2020-02-03 15:27:32+00:00
- **Authors**: Kang Zhou, Shenghua Gao, Jun Cheng, Zaiwang Gu, Huazhu Fu, Zhi Tu, Jianlong Yang, Yitian Zhao, Jiang Liu
- **Comment**: Accepted to ISBI 2020
- **Journal**: None
- **Summary**: With the development of convolutional neural network, deep learning has shown its success for retinal disease detection from optical coherence tomography (OCT) images. However, deep learning often relies on large scale labelled data for training, which is oftentimes challenging especially for disease with low occurrence. Moreover, a deep learning system trained from data-set with one or a few diseases is unable to detect other unseen diseases, which limits the practical usage of the system in disease screening. To address the limitation, we propose a novel anomaly detection framework termed Sparsity-constrained Generative Adversarial Network (Sparse-GAN) for disease screening where only healthy data are available in the training set. The contributions of Sparse-GAN are two-folds: 1) The proposed Sparse-GAN predicts the anomalies in latent space rather than image-level; 2) Sparse-GAN is constrained by a novel Sparsity Regularization Net. Furthermore, in light of the role of lesions for disease screening, we present to leverage on an anomaly activation map to show the heatmap of lesions. We evaluate our proposed Sparse-GAN on a publicly available dataset, and the results show that the proposed method outperforms the state-of-the-art methods.



### Unbiased Evaluation of Deep Metric Learning Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1911.12528v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.12528v1)
- **Published**: 2019-11-28 04:54:14+00:00
- **Updated**: 2019-11-28 04:54:14+00:00
- **Authors**: Istvan Fehervari, Avinash Ravichandran, Srikar Appalaraju
- **Comment**: None
- **Journal**: None
- **Summary**: Deep metric learning (DML) is a popular approach for images retrieval, solving verification (same or not) problems and addressing open set classification. Arguably, the most common DML approach is with triplet loss, despite significant advances in the area of DML. Triplet loss suffers from several issues such as collapse of the embeddings, high sensitivity to sampling schemes and more importantly a lack of performance when compared to more modern methods. We attribute this adoption to a lack of fair comparisons between various methods and the difficulty in adopting them for novel problem statements. In this paper, we perform an unbiased comparison of the most popular DML baseline methods under same conditions and more importantly, not obfuscating any hyper parameter tuning or adjustment needed to favor a particular method. We find, that under equal conditions several older methods perform significantly better than previously believed. In fact, our unified implementation of 12 recently introduced DML algorithms achieve state-of-the art performance on CUB200, CAR196, and Stanford Online products datasets which establishes a new set of baselines for future DML research. The codebase and all tuned hyperparameters will be open-sourced for reproducibility and to serve as a source of benchmark.



### One-Shot Object Detection with Co-Attention and Co-Excitation
- **Arxiv ID**: http://arxiv.org/abs/1911.12529v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.12529v1)
- **Published**: 2019-11-28 05:14:23+00:00
- **Updated**: 2019-11-28 05:14:23+00:00
- **Authors**: Ting-I Hsieh, Yi-Chen Lo, Hwann-Tzong Chen, Tyng-Luh Liu
- **Comment**: NeurIPS 2019
- **Journal**: None
- **Summary**: This paper aims to tackle the challenging problem of one-shot object detection. Given a query image patch whose class label is not included in the training data, the goal of the task is to detect all instances of the same class in a target image. To this end, we develop a novel {\em co-attention and co-excitation} (CoAE) framework that makes contributions in three key technical aspects. First, we propose to use the non-local operation to explore the co-attention embodied in each query-target pair and yield region proposals accounting for the one-shot situation. Second, we formulate a squeeze-and-co-excitation scheme that can adaptively emphasize correlated feature channels to help uncover relevant proposals and eventually the target objects. Third, we design a margin-based ranking loss for implicitly learning a metric to predict the similarity of a region proposal to the underlying query, no matter its class label is seen or unseen in training. The resulting model is therefore a two-stage detector that yields a strong baseline on both VOC and MS-COCO under one-shot setting of detecting objects from both seen and never-seen classes. Codes are available at https://github.com/timy90022/One-Shot-Object-Detection.



### Cycle-Consistent Adversarial Networks for Realistic Pervasive Change Generation in Remote Sensing Imagery
- **Arxiv ID**: http://arxiv.org/abs/1911.12546v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.12546v3)
- **Published**: 2019-11-28 06:03:18+00:00
- **Updated**: 2020-05-15 16:18:04+00:00
- **Authors**: Christopher X. Ren, Amanda Ziemann, Alice M. S. Durieux, James Theiler
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a new method of generating realistic pervasive changes in the context of evaluating the effectiveness of change detection algorithms in controlled settings. The method, a cycle-consistent adversarial network (CycleGAN), requires low quantities of training data to generate realistic changes. Here we show an application of CycleGAN in creating realistic snow-covered scenes of multispectral Sentinel-2 imagery, and demonstrate how these images can be used as a test bed for anomalous change detection algorithms.



### Unsupervised Many-to-Many Image-to-Image Translation Across Multiple Domains
- **Arxiv ID**: http://arxiv.org/abs/1911.12552v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.12552v2)
- **Published**: 2019-11-28 06:39:35+00:00
- **Updated**: 2020-11-24 02:20:43+00:00
- **Authors**: Ye Lin, Keren Fu, Shenggui Ling, Cheng Peng
- **Comment**: 13 pages, 14 figures, 6 tables
- **Journal**: None
- **Summary**: Unsupervised multi-domain image-to-image translation aims to synthesis images among multiple domains without labeled data, which is more general and complicated than one-to-one image mapping. However, existing methods mainly focus on reducing the large costs of modeling and do not pay enough attention to the quality of generated images. In some target domains, their translation results may not be expected or even it has model collapse. To improve the image quality, we propose an effective many-to-many mapping framework for unsupervised multi-domain image-to-image translation. There are two key aspects in our method. The first is a proposed many-to-many architecture with only one domain-shared encoder and several domain-specialized decoders to effectively and simultaneously translate images across multiple domains. The second is two proposed constraints extended from one-to-one mappings to further help improve the generation. All the evaluations demonstrate our framework is superior to existing methods and provides an effective solution for multi-domain image-to-image translation.



### AutoRemover: Automatic Object Removal for Autonomous Driving Videos
- **Arxiv ID**: http://arxiv.org/abs/1911.12588v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12588v1)
- **Published**: 2019-11-28 08:29:41+00:00
- **Updated**: 2019-11-28 08:29:41+00:00
- **Authors**: Rong Zhang, Wei Li, Peng Wang, Chenye Guan, Jin Fang, Yuhang Song, Jinhui Yu, Baoquan Chen, Weiwei Xu, Ruigang Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Motivated by the need for photo-realistic simulation in autonomous driving, in this paper we present a video inpainting algorithm \emph{AutoRemover}, designed specifically for generating street-view videos without any moving objects. In our setup we have two challenges: the first is the shadow, shadows are usually unlabeled but tightly coupled with the moving objects. The second is the large ego-motion in the videos. To deal with shadows, we build up an autonomous driving shadow dataset and design a deep neural network to detect shadows automatically. To deal with large ego-motion, we take advantage of the multi-source data, in particular the 3D data, in autonomous driving. More specifically, the geometric relationship between frames is incorporated into an inpainting deep neural network to produce high-quality structurally consistent video output. Experiments show that our method outperforms other state-of-the-art (SOTA) object removal algorithms, reducing the RMSE by over $19\%$.



### Lidar-Camera Co-Training for Semi-Supervised Road Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.12597v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12597v1)
- **Published**: 2019-11-28 08:52:27+00:00
- **Updated**: 2019-11-28 08:52:27+00:00
- **Authors**: Luca Caltagirone, Lennart Svensson, Mattias Wahde, Martin Sanfridson
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in the field of machine learning and computer vision have enabled the development of fast and accurate road detectors. Commonly such systems are trained within a supervised learning paradigm where both an input sensor's data and the corresponding ground truth label must be provided. The task of generating labels is commonly carried out by human annotators and it is notoriously time consuming and expensive. In this work, it is shown that a semi-supervised approach known as co-training can provide significant F1-score average improvements compared to supervised learning. In co-training, two classifiers acting on different views of the data cooperatively improve each other's performance by leveraging unlabeled examples. Depending on the amount of labeled data used, the improvements ranged from 1.12 to 6.10 percentage points for a camera-based road detector and from 1.04 to 8.14 percentage points for a lidar-based road detector. Lastly, the co-training algorithm is validated on the KITTI road benchmark, achieving high performance using only 36 labeled training examples together with several thousands unlabeled ones.



### PhIT-Net: Photo-consistent Image Transform for Robust Illumination Invariant Matching
- **Arxiv ID**: http://arxiv.org/abs/1911.12641v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.12641v4)
- **Published**: 2019-11-28 10:55:55+00:00
- **Updated**: 2021-10-24 14:47:14+00:00
- **Authors**: Damian Kaliroff, Guy Gilboa
- **Comment**: Paper accepted for publication at BMVC 2021. This version has the
  same content as in the published version, including the supplementary
  material
- **Journal**: None
- **Summary**: We propose a new and completely data-driven approach for generating a photo-consistent image transform. We show that simple classical algorithms which operate in the transform domain become extremely resilient to illumination changes. This considerably improves matching accuracy, outperforming the use of state-of-the-art invariant representations as well as new matching methods based on deep features. The transform is obtained by training a neural network with a specialized triplet loss, designed to emphasize actual scene changes while attenuating illumination changes. The transform yields an illumination invariant representation, structured as an image map, which is highly flexible and can be easily used for various tasks.



### Self-Supervised Learning by Cross-Modal Audio-Video Clustering
- **Arxiv ID**: http://arxiv.org/abs/1911.12667v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12667v3)
- **Published**: 2019-11-28 12:17:36+00:00
- **Updated**: 2020-10-26 14:02:35+00:00
- **Authors**: Humam Alwassel, Dhruv Mahajan, Bruno Korbar, Lorenzo Torresani, Bernard Ghanem, Du Tran
- **Comment**: Accepted to NeurIPS 2020 (spotlight presentation)
- **Journal**: None
- **Summary**: Visual and audio modalities are highly correlated, yet they contain different information. Their strong correlation makes it possible to predict the semantics of one from the other with good accuracy. Their intrinsic differences make cross-modal prediction a potentially more rewarding pretext task for self-supervised learning of video and audio representations compared to within-modality learning. Based on this intuition, we propose Cross-Modal Deep Clustering (XDC), a novel self-supervised method that leverages unsupervised clustering in one modality (e.g., audio) as a supervisory signal for the other modality (e.g., video). This cross-modal supervision helps XDC utilize the semantic correlation and the differences between the two modalities. Our experiments show that XDC outperforms single-modality clustering and other multi-modal variants. XDC achieves state-of-the-art accuracy among self-supervised methods on multiple video and audio benchmarks. Most importantly, our video model pretrained on large-scale unlabeled data significantly outperforms the same model pretrained with full-supervision on ImageNet and Kinetics for action recognition on HMDB51 and UCF101. To the best of our knowledge, XDC is the first self-supervised learning method that outperforms large-scale fully-supervised pretraining for action recognition on the same architecture.



### Continuous Dropout
- **Arxiv ID**: http://arxiv.org/abs/1911.12675v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.12675v1)
- **Published**: 2019-11-28 12:37:48+00:00
- **Updated**: 2019-11-28 12:37:48+00:00
- **Authors**: Xu Shen, Xinmei Tian, Tongliang Liu, Fang Xu, Dacheng Tao
- **Comment**: Accepted by TNNLS
- **Journal**: None
- **Summary**: Dropout has been proven to be an effective algorithm for training robust deep networks because of its ability to prevent overfitting by avoiding the co-adaptation of feature detectors. Current explanations of dropout include bagging, naive Bayes, regularization, and sex in evolution. According to the activation patterns of neurons in the human brain, when faced with different situations, the firing rates of neurons are random and continuous, not binary as current dropout does. Inspired by this phenomenon, we extend the traditional binary dropout to continuous dropout. On the one hand, continuous dropout is considerably closer to the activation characteristics of neurons in the human brain than traditional binary dropout. On the other hand, we demonstrate that continuous dropout has the property of avoiding the co-adaptation of feature detectors, which suggests that we can extract more independent feature detectors for model averaging in the test stage. We introduce the proposed continuous dropout to a feedforward neural network and comprehensively compare it with binary dropout, adaptive dropout, and DropConnect on MNIST, CIFAR-10, SVHN, NORB, and ILSVRC-12. Thorough experiments demonstrate that our method performs better in preventing the co-adaptation of feature detectors and improves test performance. The code is available at: https://github.com/jasonustc/caffe-multigpu/tree/dropout.



### xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.12676v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12676v2)
- **Published**: 2019-11-28 12:38:05+00:00
- **Updated**: 2020-03-30 19:24:04+00:00
- **Authors**: Maximilian Jaritz, Tuan-Hung Vu, Raoul de Charette, √âmilie Wirbel, Patrick P√©rez
- **Comment**: Accepted at CVPR 2020. For a demo video, see http://tiny.cc/xmuda
- **Journal**: None
- **Summary**: Unsupervised Domain Adaptation (UDA) is crucial to tackle the lack of annotations in a new domain. There are many multi-modal datasets, but most UDA approaches are uni-modal. In this work, we explore how to learn from multi-modality and propose cross-modal UDA (xMUDA) where we assume the presence of 2D images and 3D point clouds for 3D semantic segmentation. This is challenging as the two input spaces are heterogeneous and can be impacted differently by domain shift. In xMUDA, modalities learn from each other through mutual mimicking, disentangled from the segmentation objective, to prevent the stronger modality from adopting false predictions from the weaker one. We evaluate on new UDA scenarios including day-to-night, country-to-country and dataset-to-dataset, leveraging recent autonomous driving datasets. xMUDA brings large improvements over uni-modal UDA on all tested scenarios, and is complementary to state-of-the-art UDA techniques. Code is available at https://github.com/valeoai/xmuda.



### Patch Reordering: a Novel Way to Achieve Rotation and Translation Invariance in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.12682v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.12682v1)
- **Published**: 2019-11-28 12:49:57+00:00
- **Updated**: 2019-11-28 12:49:57+00:00
- **Authors**: Xu Shen, Xinmei Tian, Shaoyan Sun, Dacheng Tao
- **Comment**: Accepted AAAI17
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have demonstrated state-of-the-art performance on many visual recognition tasks. However, the combination of convolution and pooling operations only shows invariance to small local location changes in meaningful objects in input. Sometimes, such networks are trained using data augmentation to encode this invariance into the parameters, which restricts the capacity of the model to learn the content of these objects. A more efficient use of the parameter budget is to encode rotation or translation invariance into the model architecture, which relieves the model from the need to learn them. To enable the model to focus on learning the content of objects other than their locations, we propose to conduct patch ranking of the feature maps before feeding them into the next layer. When patch ranking is combined with convolution and pooling operations, we obtain consistent representations despite the location of meaningful objects in input. We show that the patch ranking module improves the performance of the CNN on many benchmark tasks, including MNIST digit recognition, large-scale image recognition, and image retrieval. The code is available at https://github.com//jasonustc/caffe-multigpu/tree/TICNN .



### A novel classification-selection approach for the self updating of template-based face recognition systems
- **Arxiv ID**: http://arxiv.org/abs/1911.12688v1
- **DOI**: 10.1016/j.patcog.2019.107121
- **Categories**: **cs.CV**, 00-01:99-00
- **Links**: [PDF](http://arxiv.org/pdf/1911.12688v1)
- **Published**: 2019-11-28 12:58:17+00:00
- **Updated**: 2019-11-28 12:58:17+00:00
- **Authors**: Giulia Orr√π, Gian Luca Marcialis, Fabio Roli
- **Comment**: This is an original manuscript of an article published by Elsevier in
  Pattern Recognition on 27 November 2019. Available online:
  https://doi.org/10.1016/j.patcog.2019.107121
- **Journal**: None
- **Summary**: The boosting on the need of security notably increased the amount of possible facial recognition applications, especially due to the success of the Internet of Things (IoT) paradigm. However, although handcrafted and deep learning-inspired facial features reached a significant level of compactness and expressive power, the facial recognition performance still suffers from intra-class variations such as ageing, facial expressions, lighting changes, and pose. These variations cannot be captured in a single acquisition and require multiple acquisitions of long duration, which are expensive and need a high level of collaboration from the users. Among others, self-update algorithms have been proposed in order to mitigate these problems. Self-updating aims to add novel templates to the users' gallery among the inputs submitted during system operations. Consequently, computational complexity and storage space tend to be among the critical requirements of these algorithms. The present paper deals with the above problems by a novel template-based self-update algorithm, able to keep over time the expressive power of a limited set of templates stored in the system database. The rationale behind the proposed approach is in the working hypothesis that a dominating mode characterises the features' distribution given the client. Therefore, the key point is to select the best templates around that mode. We propose two methods, which are tested on systems based on handcrafted features and deep-learning-inspired autoencoders at the state-of-the-art. Three benchmark data sets are used. Experimental results confirm that, by effective and compact feature sets which can support our working hypothesis, the proposed classification-selection approaches overcome the problem of manual updating and, in case, stringent computational requirements.



### Transform-Invariant Convolutional Neural Networks for Image Classification and Search
- **Arxiv ID**: http://arxiv.org/abs/1912.01447v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.01447v1)
- **Published**: 2019-11-28 13:09:21+00:00
- **Updated**: 2019-11-28 13:09:21+00:00
- **Authors**: Xu Shen, Xinmei Tian, Anfeng He, Shaoyan Sun, Dacheng Tao
- **Comment**: Accepted by ACM Multimedia. arXiv admin note: text overlap with
  arXiv:1911.12682
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have achieved state-of-the-art results on many visual recognition tasks. However, current CNN models still exhibit a poor ability to be invariant to spatial transformations of images. Intuitively, with sufficient layers and parameters, hierarchical combinations of convolution (matrix multiplication and non-linear activation) and pooling operations should be able to learn a robust mapping from transformed input images to transform-invariant representations. In this paper, we propose randomly transforming (rotation, scale, and translation) feature maps of CNNs during the training stage. This prevents complex dependencies of specific rotation, scale, and translation levels of training images in CNN models. Rather, each convolutional kernel learns to detect a feature that is generally helpful for producing the transform-invariant answer given the combinatorially large variety of transform levels of its input feature maps. In this way, we do not require any extra training supervision or modification to the optimization process and training images. We show that random transformation provides significant improvements of CNNs on many benchmark tasks, including small-scale image recognition, large-scale image recognition, and image retrieval. The code is available at https://github.com/jasonustc/caffe-multigpu/tree/TICNN.



### Cameras Viewing Cameras Geometry
- **Arxiv ID**: http://arxiv.org/abs/1911.12706v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1911.12706v1)
- **Published**: 2019-11-28 13:40:06+00:00
- **Updated**: 2019-11-28 13:40:06+00:00
- **Authors**: Danail Brezov, Michael Werman
- **Comment**: None
- **Journal**: None
- **Summary**: A basic problem in computer vision is to understand the structure of a real-world scene given several images of it. Here we study several theoretical aspects of the intra multi-view geometry of calibrated cameras when all that they can reliably recognize is each other. With the proliferation of wearable cameras, autonomous vehicles and drones, the geometry of these multiple cameras is a timely and relevant problem to study.



### Continuous Adaptation for Interactive Object Segmentation by Learning from Corrections
- **Arxiv ID**: http://arxiv.org/abs/1911.12709v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12709v4)
- **Published**: 2019-11-28 13:43:54+00:00
- **Updated**: 2020-11-08 15:55:14+00:00
- **Authors**: Theodora Kontogianni, Michael Gygli, Jasper Uijlings, Vittorio Ferrari
- **Comment**: ECCV 2020 Camera Ready
- **Journal**: None
- **Summary**: In interactive object segmentation a user collaborates with a computer vision model to segment an object. Recent works employ convolutional neural networks for this task: Given an image and a set of corrections made by the user as input, they output a segmentation mask. These approaches achieve strong performance by training on large datasets but they keep the model parameters unchanged at test time. Instead, we recognize that user corrections can serve as sparse training examples and we propose a method that capitalizes on that idea to update the model parameters on-the-fly to the data at hand. Our approach enables the adaptation to a particular object and its background, to distributions shifts in a test set, to specific object classes, and even to large domain changes, where the imaging modality changes between training and testing. We perform extensive experiments on 8 diverse datasets and show: Compared to a model with frozen parameters, our method reduces the required corrections (i) by 9%-30% when distribution shifts are small between training and testing; (ii) by 12%-44% when specializing to a specific class; (iii) and by 60% and 77% when we completely change domain between training and testing.



### Training Multi-Object Detector by Estimating Bounding Box Distribution for Input Image
- **Arxiv ID**: http://arxiv.org/abs/1911.12721v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12721v4)
- **Published**: 2019-11-28 14:08:55+00:00
- **Updated**: 2021-09-05 07:03:07+00:00
- **Authors**: Jaeyoung Yoo, Hojun Lee, Inseop Chung, Geonseok Seo, Nojun Kwak
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: In multi-object detection using neural networks, the fundamental problem is, "How should the network learn a variable number of bounding boxes in different input images?". Previous methods train a multi-object detection network through a procedure that directly assigns the ground truth bounding boxes to the specific locations of the network's output. However, this procedure makes the training of a multi-object detection network too heuristic and complicated. In this paper, we reformulate the multi-object detection task as a problem of density estimation of bounding boxes. Instead of assigning each ground truth to specific locations of network's output, we train a network by estimating the probability density of bounding boxes in an input image using a mixture model. For this purpose, we propose a novel network for object detection called Mixture Density Object Detector (MDOD), and the corresponding objective function for the density-estimation-based training. We applied MDOD to MS COCO dataset. Our proposed method not only deals with multi-object detection problems in a new approach, but also improves detection performances through MDOD. The code is available: https://github.com/yoojy31/MDOD.



### Every Frame Counts: Joint Learning of Video Segmentation and Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1911.12739v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.12739v1)
- **Published**: 2019-11-28 15:01:35+00:00
- **Updated**: 2019-11-28 15:01:35+00:00
- **Authors**: Mingyu Ding, Zhe Wang, Bolei Zhou, Jianping Shi, Zhiwu Lu, Ping Luo
- **Comment**: Published in AAAI 2020
- **Journal**: None
- **Summary**: A major challenge for video semantic segmentation is the lack of labeled data. In most benchmark datasets, only one frame of a video clip is annotated, which makes most supervised methods fail to utilize information from the rest of the frames. To exploit the spatio-temporal information in videos, many previous works use pre-computed optical flows, which encode the temporal consistency to improve the video segmentation. However, the video segmentation and optical flow estimation are still considered as two separate tasks. In this paper, we propose a novel framework for joint video semantic segmentation and optical flow estimation. Semantic segmentation brings semantic information to handle occlusion for more robust optical flow estimation, while the non-occluded optical flow provides accurate pixel-level temporal correspondences to guarantee the temporal consistency of the segmentation. Moreover, our framework is able to utilize both labeled and unlabeled frames in the video through joint training, while no additional calculation is required in inference. Extensive experiments show that the proposed model makes the video semantic segmentation and optical flow estimation benefit from each other and outperforms existing methods under the same settings in both tasks.



### ASR is all you need: cross-modal distillation for lip reading
- **Arxiv ID**: http://arxiv.org/abs/1911.12747v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1911.12747v2)
- **Published**: 2019-11-28 15:15:27+00:00
- **Updated**: 2020-03-31 06:53:12+00:00
- **Authors**: Triantafyllos Afouras, Joon Son Chung, Andrew Zisserman
- **Comment**: ICASSP 2020
- **Journal**: None
- **Summary**: The goal of this work is to train strong models for visual speech recognition without requiring human annotated ground truth data. We achieve this by distilling from an Automatic Speech Recognition (ASR) model that has been trained on a large-scale audio-only corpus. We use a cross-modal distillation method that combines Connectionist Temporal Classification (CTC) with a frame-wise cross-entropy loss. Our contributions are fourfold: (i) we show that ground truth transcriptions are not necessary to train a lip reading system; (ii) we show how arbitrary amounts of unlabelled video data can be leveraged to improve performance; (iii) we demonstrate that distillation significantly speeds up training; and, (iv) we obtain state-of-the-art results on the challenging LRS2 and LRS3 datasets for training only on publicly available data.



### Dividing and Conquering Cross-Modal Recipe Retrieval: from Nearest Neighbours Baselines to SoTA
- **Arxiv ID**: http://arxiv.org/abs/1911.12763v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12763v2)
- **Published**: 2019-11-28 16:00:09+00:00
- **Updated**: 2021-07-13 15:32:46+00:00
- **Authors**: Mikhail Fain, Niall Twomey, Andrey Ponikar, Ryan Fox, Danushka Bollegala
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel non-parametric method for cross-modal recipe retrieval which is applied on top of precomputed image and text embeddings. By combining our method with standard approaches for building image and text encoders, trained independently with a self-supervised classification objective, we create a baseline model which outperforms most existing methods on a challenging image-to-recipe task. We also use our method for comparing image and text encoders trained using different modern approaches, thus addressing the issues hindering the development of novel methods for cross-modal recipe retrieval. We demonstrate how to use the insights from model comparison and extend our baseline model with standard triplet loss that improves state-of-the-art on the Recipe1M dataset by a large margin, while using only precomputed features and with much less complexity than existing methods. Further, our approach readily generalizes beyond recipe retrieval to other challenging domains, achieving state-of-the-art performance on Politics and GoodNews cross-modal retrieval tasks.



### Detection and Mitigation of Rare Subclasses in Deep Neural Network Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1911.12780v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.12780v2)
- **Published**: 2019-11-28 16:41:35+00:00
- **Updated**: 2021-07-07 15:06:42+00:00
- **Authors**: Colin Paterson, Radu Calinescu, Chiara Picardi
- **Comment**: 8 pages, 7 Figures, 2 Tables
- **Journal**: None
- **Summary**: Regions of high-dimensional input spaces that are underrepresented in training datasets reduce machine-learnt classifier performance, and may lead to corner cases and unwanted bias for classifiers used in decision making systems. When these regions belong to otherwise well-represented classes, their presence and negative impact are very hard to identify. We propose an approach for the detection and mitigation of such rare subclasses in deep neural network classifiers. The new approach is underpinned by an easy-to-compute commonality metric that supports the detection of rare subclasses, and comprises methods for reducing the impact of these subclasses during both model training and model exploitation. We demonstrate our approach using two well-known datasets, MNIST's handwritten digits and Kaggle's cats/dogs, identifying rare subclasses and producing models which compensate for subclass rarity. In addition we demonstrate how our run-time approach increases the ability of users to identify samples likely to be misclassified at run-time.



### Light-weight Calibrator: a Separable Component for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1911.12796v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.12796v2)
- **Published**: 2019-11-28 17:18:03+00:00
- **Updated**: 2020-02-28 14:12:02+00:00
- **Authors**: Shaokai Ye, Kailu Wu, Mu Zhou, Yunfei Yang, Sia huat Tan, Kaidi Xu, Jiebo Song, Chenglong Bao, Kaisheng Ma
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: Existing domain adaptation methods aim at learning features that can be generalized among domains. These methods commonly require to update source classifier to adapt to the target domain and do not properly handle the trade off between the source domain and the target domain. In this work, instead of training a classifier to adapt to the target domain, we use a separable component called data calibrator to help the fixed source classifier recover discrimination power in the target domain, while preserving the source domain's performance. When the difference between two domains is small, the source classifier's representation is sufficient to perform well in the target domain and outperforms GAN-based methods in digits. Otherwise, the proposed method can leverage synthetic images generated by GANs to boost performance and achieve state-of-the-art performance in digits datasets and driving scene semantic segmentation. Our method empirically reveals that certain intriguing hints, which can be mitigated by adversarial attack to domain discriminators, are one of the sources for performance degradation under the domain shift.



### Motion Equivariance OF Event-based Camera Data with the Temporal Normalization Transform
- **Arxiv ID**: http://arxiv.org/abs/1911.12801v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.12801v1)
- **Published**: 2019-11-28 17:26:36+00:00
- **Updated**: 2019-11-28 17:26:36+00:00
- **Authors**: Ziyun Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we focus on using convolution neural networks (CNN) to perform object recognition on the event data. In object recognition, it is important for a neural network to be robust to the variations of the data during testing. For traditional cameras, translations are well handled because CNNs are naturally equivariant to translations. However, because event cameras record the change of light intensity of an image, the geometric shape of event volumes will not only depend on the objects but also on their relative motions with respect to the camera. The deformation of the events caused by motions causes the CNN to be less robust to unseen motions during inference. To address this problem, we would like to explore the equivariance property of CNNs, a well-studied area that demonstrates to produce predictable deformation of features under certain transformations of the input image.



### Siam R-CNN: Visual Tracking by Re-Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.12836v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12836v2)
- **Published**: 2019-11-28 19:21:34+00:00
- **Updated**: 2020-04-02 11:09:54+00:00
- **Authors**: Paul Voigtlaender, Jonathon Luiten, Philip H. S. Torr, Bastian Leibe
- **Comment**: CVPR 2020 camera-ready version
- **Journal**: None
- **Summary**: We present Siam R-CNN, a Siamese re-detection architecture which unleashes the full power of two-stage object detection approaches for visual object tracking. We combine this with a novel tracklet-based dynamic programming algorithm, which takes advantage of re-detections of both the first-frame template and previous-frame predictions, to model the full history of both the object to be tracked and potential distractor objects. This enables our approach to make better tracking decisions, as well as to re-detect tracked objects after long occlusion. Finally, we propose a novel hard example mining strategy to improve Siam R-CNN's robustness to similar looking objects. Siam R-CNN achieves the current best performance on ten tracking benchmarks, with especially strong results for long-term tracking. We make our code and models available at www.vision.rwth-aachen.de/page/siamrcnn.



### Quality analysis of DCGAN-generated mammography lesions
- **Arxiv ID**: http://arxiv.org/abs/1911.12850v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.6.6, I.4.10, I.6.6; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/1911.12850v2)
- **Published**: 2019-11-28 20:11:19+00:00
- **Updated**: 2020-02-06 11:40:09+00:00
- **Authors**: Basel Alyafi, Oliver Diaz, Joan C Vilanova, Javier del Riego, Robert Marti
- **Comment**: Abstract accepted in the International Workshop Breast Imaging IWBI
  (2020), 4 pages, 3 figures
- **Journal**: None
- **Summary**: Medical image synthesis has gained a great focus recently, especially after the introduction of Generative Adversarial Networks (GANs). GANs have been used widely to provide anatomically-plausible and diverse samples for augmentation and other applications, including segmentation and super resolution. In our previous work, Deep Convolutional GANs were used to generate synthetic mammogram lesions, masses mainly, that could enhance the classification performance in imbalanced datasets. In this new work, a deeper investigation was carried out to explore other aspects of the generated images evaluation, i.e., realism, feature space distribution, and observers studies. t-Stochastic Neighbor Embedding (t-SNE) was used to reduce the dimensionality of real and fake images to enable 2D visualisations. Additionally, two expert radiologists performed a realism-evaluation study. Visualisations showed that the generated images have a similar feature distribution of the real ones, avoiding outliers. Moreover, Receiver Operating Characteristic (ROC) curve showed that the radiologists could not, in many cases, distinguish between synthetic and real lesions, giving 48% and 61% accuracies in a balanced sample set.



### SEAN: Image Synthesis with Semantic Region-Adaptive Normalization
- **Arxiv ID**: http://arxiv.org/abs/1911.12861v2
- **DOI**: 10.1109/CVPR42600.2020.00515
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.12861v2)
- **Published**: 2019-11-28 20:54:35+00:00
- **Updated**: 2020-05-24 13:47:09+00:00
- **Authors**: Peihao Zhu, Rameen Abdal, Yipeng Qin, Peter Wonka
- **Comment**: Accepted as a CVPR 2020 oral paper. The interactive demo is available
  at https://youtu.be/0Vbj9xFgoUw
- **Journal**: None
- **Summary**: We propose semantic region-adaptive normalization (SEAN), a simple but effective building block for Generative Adversarial Networks conditioned on segmentation masks that describe the semantic regions in the desired output image. Using SEAN normalization, we can build a network architecture that can control the style of each semantic region individually, e.g., we can specify one style reference image per region. SEAN is better suited to encode, transfer, and synthesize style than the best previous method in terms of reconstruction quality, variability, and visual quality. We evaluate SEAN on multiple datasets and report better quantitative metrics (e.g. FID, PSNR) than the current state of the art. SEAN also pushes the frontier of interactive image editing. We can interactively edit images by changing segmentation masks or the style for any given region. We can also interpolate styles from two reference images per region.



### CG-GAN: An Interactive Evolutionary GAN-based Approach for Facial Composite Generation)
- **Arxiv ID**: http://arxiv.org/abs/1912.05020v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05020v1)
- **Published**: 2019-11-28 21:08:43+00:00
- **Updated**: 2019-11-28 21:08:43+00:00
- **Authors**: Nicola Zaltron, Luisa Zurlo, Sebastian Risi
- **Comment**: None
- **Journal**: None
- **Summary**: Facial composites are graphical representations of an eyewitness's memory of a face. Many digital systems are available for the creation of such composites but are either unable to reproduce features unless previously designed or do not allow holistic changes to the image. In this paper, we improve the efficiency of composite creation by removing the reliance on expert knowledge and letting the system learn to represent faces from examples. The novel approach, Composite Generating GAN (CG-GAN), applies generative and evolutionary computation to allow casual users to easily create facial composites. Specifically, CG-GAN utilizes the generator network of a pg-GAN to create high-resolution human faces. Users are provided with several functions to interactively breed and edit faces. CG-GAN offers a novel way of generating and handling static and animated photo-realistic facial composites, with the possibility of combining multiple representations of the same perpetrator, generated by different eyewitnesses.



### Region segmentation via deep learning and convex optimization
- **Arxiv ID**: http://arxiv.org/abs/1911.12870v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, 65D19, 68T45, I.4.6; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1911.12870v1)
- **Published**: 2019-11-28 21:42:21+00:00
- **Updated**: 2019-11-28 21:42:21+00:00
- **Authors**: Matthias Sonntag, Veniamin I. Morgenshtern
- **Comment**: 15 pages, 6 figures
- **Journal**: None
- **Summary**: In this paper, we propose a method to segment regions in three-dimensional point clouds. We assume that (i) the shape and the number of regions in the point cloud are not known and (ii) the point cloud may be noisy. The method consists of two steps. In the first step we use a deep neural network to predict the probability that a pair of small patches from the point cloud belongs to the same region. In the second step, we use a convex-optimization based method to improve the predictions of the network by enforcing consistency constraints. We evaluate the accuracy of our method on a custom dataset of convex polyhedra, where the regions correspond to the faces of the polyhedra. The method can be seen as a robust and flexible alternative to the famous region growing segmentation algorithm. All reported results are reproducible and come with easy to use code that could serve as a baseline for future research.



### Geometric Back-projection Network for Point Cloud Classification
- **Arxiv ID**: http://arxiv.org/abs/1911.12885v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.12885v5)
- **Published**: 2019-11-28 22:37:06+00:00
- **Updated**: 2021-04-13 05:57:13+00:00
- **Authors**: Shi Qiu, Saeed Anwar, Nick Barnes
- **Comment**: 14 pages, 8 figures
- **Journal**: None
- **Summary**: As the basic task of point cloud analysis, classification is fundamental but always challenging. To address some unsolved problems of existing methods, we propose a network that captures geometric features of point clouds for better representations. To achieve this, on the one hand, we enrich the geometric information of points in low-level 3D space explicitly. On the other hand, we apply CNN-based structures in high-level feature spaces to learn local geometric context implicitly. Specifically, we leverage an idea of error-correcting feedback structure to capture the local features of point clouds comprehensively. Furthermore, an attention module based on channel affinity assists the feature map to avoid possible redundancy by emphasizing its distinct channels. The performance on both synthetic and real-world point clouds datasets demonstrate the superiority and applicability of our network. Comparing with other state-of-the-art methods, our approach balances accuracy and efficiency.



### Artificial Intelligence in Glioma Imaging: Challenges and Advances
- **Arxiv ID**: http://arxiv.org/abs/1911.12886v3
- **DOI**: 10.1088/1741-2552/ab8131
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.12886v3)
- **Published**: 2019-11-28 22:40:56+00:00
- **Updated**: 2020-04-10 10:03:23+00:00
- **Authors**: Weina Jin, Mostafa Fatehi, Kumar Abhishek, Mayur Mallya, Brian Toyota, Ghassan Hamarneh
- **Comment**: 31 pages, 6 figures. Accepted for publication in the Journal of
  Neural Engineering
- **Journal**: None
- **Summary**: Primary brain tumors including gliomas continue to pose significant management challenges to clinicians. While the presentation, the pathology, and the clinical course of these lesions are variable, the initial investigations are usually similar. Patients who are suspected to have a brain tumor will be assessed with computed tomography (CT) and magnetic resonance imaging (MRI). The imaging findings are used by neurosurgeons to determine the feasibility of surgical resection and plan such an undertaking. Imaging studies are also an indispensable tool in tracking tumor progression or its response to treatment. As these imaging studies are non-invasive, relatively cheap and accessible to patients, there have been many efforts over the past two decades to increase the amount of clinically-relevant information that can be extracted from brain imaging. Most recently, artificial intelligence (AI) techniques have been employed to segment and characterize brain tumors, as well as to detect progression or treatment-response. However, the clinical utility of such endeavours remains limited due to challenges in data collection and annotation, model training, and the reliability of AI-generated information.   We provide a review of recent advances in addressing the above challenges. First, to overcome the challenge of data paucity, different image imputation and synthesis techniques along with annotation collection efforts are summarized. Next, various training strategies are presented to meet multiple desiderata, such as model performance, generalization ability, data privacy protection, and learning with sparse annotations. Finally, standardized performance evaluation and model interpretability methods have been reviewed. We believe that these technical approaches will facilitate the development of a fully-functional AI tool in the clinical care of patients with gliomas.



### Fruit Detection, Segmentation and 3D Visualisation of Environments in Apple Orchards
- **Arxiv ID**: http://arxiv.org/abs/1911.12889v1
- **DOI**: 10.1016/j.compag.2020.105302
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.12889v1)
- **Published**: 2019-11-28 22:49:48+00:00
- **Updated**: 2019-11-28 22:49:48+00:00
- **Authors**: Hanwen Kang, Chao Chen
- **Comment**: 17 pages, 7 figures
- **Journal**: Computers and Electronics in Agriculture 171 (2020) 105302
- **Summary**: Robotic harvesting of fruits in orchards is a challenging task, since high density and overlapping of fruits and branches can heavily impact the success rate of robotic harvesting. Therefore, the vision system is demanded to provide comprehensive information of the working environment to guide the manipulator and gripping system to successful detach the target fruits. In this study, a deep learning based one-stage detector DaSNet-V2 is developed to perform the multi-task vision sensing in the working environment of apple orchards. DaSNet-V2 combines the detection and instance segmentation of fruits and semantic segmentation of branch into a single network architecture. Meanwhile, a light-weight backbone network LW-net is utilised in the DaSNet-V2 model to improve the computational efficiency of the model. In the experiment, DaSNet-V2 is tested and evaluated on the RGB-D images of the orchard. From the experiment results, DaSNet-V2 with lightweight backbone achieves 0.844, 0.858, and 0.795 on the F 1 score of the detection, and mean intersection of union on the instance segmentation of fruits and semantic segmentation of branches, respectively. To provide a direct-viewing of the working environment in orchards, the obtained sensing results are illustrated by 3D visualisation . The robustness and efficiency of the DaSNet-V2 in detection and segmentation are validated by the experiments in the real-environment of apple orchard.



### Land Cover Change Detection via Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.12903v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12903v1)
- **Published**: 2019-11-28 23:54:36+00:00
- **Updated**: 2019-11-28 23:54:36+00:00
- **Authors**: Renee Su, Rong Chen
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a change detection method that identifies land cover changes from aerial imagery, using semantic segmentation, a machine learning approach. We present a land cover classification training pipeline with Deeplab v3+, state-of-the-art semantic segmentation technology, including data preparation, model training for seven land cover types, and model exporting modules. In the land cover change detection system, the inputs are images retrieved from Google Earth at the same location but from different times. The system then predicts semantic segmentation results on these images using the trained model and calculates the land cover class percentage for each input image. We see an improvement in the accuracy of the land cover semantic segmentation model, with a mean IoU of 0.756 compared to 0.433, as reported in the DeepGlobe land cover classification challenge. The land cover change detection system that leverages the state-of-the-art semantic segmentation technology is proposed and can be used for deforestation analysis, land management, and urban planning.



