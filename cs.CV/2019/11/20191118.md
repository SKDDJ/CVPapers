# Arxiv Papers in cs.CV on 2019-11-18
### Visual Similarity Attention
- **Arxiv ID**: http://arxiv.org/abs/1911.07381v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.07381v2)
- **Published**: 2019-11-18 00:46:40+00:00
- **Updated**: 2022-05-03 19:36:25+00:00
- **Authors**: Meng Zheng, Srikrishna Karanam, Terrence Chen, Richard J. Radke, Ziyan Wu
- **Comment**: 10 pages, 7 figures, 4 tables
- **Journal**: None
- **Summary**: While there has been substantial progress in learning suitable distance metrics, these techniques in general lack transparency and decision reasoning, i.e., explaining why the input set of images is similar or dissimilar. In this work, we solve this key problem by proposing the first method to generate generic visual similarity explanations with gradient-based attention. We demonstrate that our technique is agnostic to the specific similarity model type, e.g., we show applicability to Siamese, triplet, and quadruplet models. Furthermore, we make our proposed similarity attention a principled part of the learning process, resulting in a new paradigm for learning similarity functions. We demonstrate that our learning mechanism results in more generalizable, as well as explainable, similarity models. Finally, we demonstrate the generality of our framework by means of experiments on a variety of tasks, including image retrieval, person re-identification, and low-shot semantic segmentation.



### Towards Robust RGB-D Human Mesh Recovery
- **Arxiv ID**: http://arxiv.org/abs/1911.07383v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1911.07383v1)
- **Published**: 2019-11-18 00:55:37+00:00
- **Updated**: 2019-11-18 00:55:37+00:00
- **Authors**: Ren Li, Changjiang Cai, Georgios Georgakis, Srikrishna Karanam, Terrence Chen, Ziyan Wu
- **Comment**: 10 pages, 4 figures, 4 tables
- **Journal**: None
- **Summary**: We consider the problem of human pose estimation. While much recent work has focused on the RGB domain, these techniques are inherently under-constrained since there can be many 3D configurations that explain the same 2D projection. To this end, we propose a new method that uses RGB-D data to estimate a parametric human mesh model. Our key innovations include (a) the design of a new dynamic data fusion module that facilitates learning with a combination of RGB-only and RGB-D datasets, (b) a new constraint generator module that provides SMPL supervisory signals when explicit SMPL annotations are not available, and (c) the design of a new depth ranking learning objective, all of which enable principled model training with RGB-D data. We conduct extensive experiments on a variety of RGB-D datasets to demonstrate efficacy.



### Distributed Low Precision Training Without Mixed Precision
- **Arxiv ID**: http://arxiv.org/abs/1911.07384v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1911.07384v2)
- **Published**: 2019-11-18 00:56:52+00:00
- **Updated**: 2019-12-27 05:37:54+00:00
- **Authors**: Zehua Cheng, Weiyang Wang, Yan Pan, Thomas Lukasiewicz
- **Comment**: None
- **Journal**: None
- **Summary**: Low precision training is one of the most popular strategies for deploying the deep model on limited hardware resources. Fixed point implementation of DCNs has the potential to alleviate complexities and facilitate potential deployment on embedded hardware. However, most low precision training solution is based on a mixed precision strategy. In this paper, we have presented an ablation study on different low precision training strategy and propose a solution for IEEE FP-16 format throughout the training process. We tested the ResNet50 on 128 GPU cluster on ImageNet-full dataset. We have viewed that it is not essential to use FP32 format to train the deep models. We have viewed that communication cost reduction, model compression, and large-scale distributed training are three coupled problems.



### Towards Visually Explaining Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1911.07389v7
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.07389v7)
- **Published**: 2019-11-18 01:05:41+00:00
- **Updated**: 2020-04-14 16:52:49+00:00
- **Authors**: Wenqian Liu, Runze Li, Meng Zheng, Srikrishna Karanam, Ziyan Wu, Bir Bhanu, Richard J. Radke, Octavia Camps
- **Comment**: 10 pages, 9 figures, 2 tables, CVPR 2020
- **Journal**: None
- **Summary**: Recent advances in Convolutional Neural Network (CNN) model interpretability have led to impressive progress in visualizing and understanding model predictions. In particular, gradient-based visual attention methods have driven much recent effort in using visual attention maps as a means for visual explanations. A key problem, however, is these methods are designed for classification and categorization tasks, and their extension to explaining generative models, e.g. variational autoencoders (VAE) is not trivial. In this work, we take a step towards bridging this crucial gap, proposing the first technique to visually explain VAEs by means of gradient-based attention. We present methods to generate visual attention from the learned latent space, and also demonstrate such attention explanations serve more than just explaining VAE predictions. We show how these attention maps can be used to localize anomalies in images, demonstrating state-of-the-art performance on the MVTec-AD dataset. We also show how they can be infused into model training, helping bootstrap the VAE into learning improved latent space disentanglement, demonstrated on the Dsprites dataset.



### Neural Random Subspace
- **Arxiv ID**: http://arxiv.org/abs/1911.07845v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.07845v3)
- **Published**: 2019-11-18 02:28:04+00:00
- **Updated**: 2020-09-15 01:07:47+00:00
- **Authors**: Yun-Hao Cao, Jianxin Wu, Hanchen Wang, Joan Lasenby
- **Comment**: 38 pages
- **Journal**: None
- **Summary**: The random subspace method, known as the pillar of random forests, is good at making precise and robust predictions. However, there is not a straightforward way yet to combine it with deep learning. In this paper, we therefore propose Neural Random Subspace (NRS), a novel deep learning based random subspace method. In contrast to previous forest methods, NRS enjoys the benefits of end-to-end, data-driven representation learning, as well as pervasive support from deep learning software and hardware platforms, hence achieving faster inference speed and higher accuracy. Furthermore, as a non-linear component to be encoded into Convolutional Neural Networks (CNNs), NRS learns non-linear feature representations in CNNs more efficiently than previous higher-order pooling methods, producing good results with negligible increase in parameters, floating point operations (FLOPs) and real running time. Compared with random subspaces, random forests and gradient boosting decision trees (GBDTs), NRS achieves superior performance on 35 machine learning datasets. Moreover, on both 2D image and 3D point cloud recognition tasks, integration of NRS with CNN architectures achieves consistent improvements with minor extra cost. Code is available at https://github.com/CupidJay/NRS_pytorch.



### SSRNet: Scalable 3D Surface Reconstruction Network
- **Arxiv ID**: http://arxiv.org/abs/1911.07401v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07401v2)
- **Published**: 2019-11-18 02:41:39+00:00
- **Updated**: 2020-04-14 03:24:28+00:00
- **Authors**: Zhenxing Mi, Yiming Luo, Wenbing Tao
- **Comment**: Accepted by CVPR2020, typos corrected, references added, images
  revised
- **Journal**: None
- **Summary**: Existing learning-based surface reconstruction methods from point clouds are still facing challenges in terms of scalability and preservation of details on large-scale point clouds. In this paper, we propose the SSRNet, a novel scalable learning-based method for surface reconstruction. The proposed SSRNet constructs local geometry-aware features for octree vertices and designs a scalable reconstruction pipeline, which not only greatly enhances the predication accuracy of the relative position between the vertices and the implicit surface facilitating the surface reconstruction quality, but also allows dividing the point cloud and octree vertices and processing different parts in parallel for superior scalability on large-scale point clouds with millions of points. Moreover, SSRNet demonstrates outstanding generalization capability and only needs several surface data for training, much less than other learning-based reconstruction methods, which can effectively avoid overfitting. The trained model of SSRNet on one dataset can be directly used on other datasets with superior performance. Finally, the time consumption with SSRNet on a large-scale point cloud is acceptable and competitive. To our knowledge, the proposed SSRNet is the first to really bring a convincing solution to the scalability issue of the learning-based surface reconstruction methods, and is an important step to make learning-based methods competitive with respect to geometry processing methods on real-world and challenging data. Experiments show that our method achieves a breakthrough in scalability and quality compared with state-of-the-art learning-based methods.



### Multi-Temporal Recurrent Neural Networks For Progressive Non-Uniform Single Image Deblurring With Incremental Temporal Training
- **Arxiv ID**: http://arxiv.org/abs/1911.07410v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.07410v1)
- **Published**: 2019-11-18 03:36:59+00:00
- **Updated**: 2019-11-18 03:36:59+00:00
- **Authors**: Dongwon Park, Dong Un Kang, Jisoo Kim, Se Young Chun
- **Comment**: 10 pages, 8 figures, 6 tables, work in progress
- **Journal**: None
- **Summary**: Multi-scale (MS) approaches have been widely investigated for blind single image / video deblurring that sequentially recovers deblurred images in low spatial scale first and then in high spatial scale later with the output of lower scales. MS approaches have been effective especially for severe blurs induced by large motions in high spatial scale since those can be seen as small blurs in low spatial scale. In this work, we investigate alternative approach to MS, called multi-temporal (MT) approach, for non-uniform single image deblurring. We propose incremental temporal training with constructed MT level dataset from time-resolved dataset, develop novel MT-RNNs with recurrent feature maps, and investigate progressive single image deblurring over iterations. Our proposed MT methods outperform state-of-the-art MS methods on the GoPro dataset in PSNR with the smallest number of parameters.



### Potential Field: Interpretable and Unified Representation for Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/1911.07414v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07414v2)
- **Published**: 2019-11-18 04:00:34+00:00
- **Updated**: 2020-04-07 21:46:07+00:00
- **Authors**: Shan Su, Cheng Peng, Jianbo Shi, Chiho Choi
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting an agent's future trajectory is a challenging task given the complicated stimuli (environmental/inertial/social) of motion. Prior works learn individual stimulus from different modules and fuse the representations in an end-to-end manner, which makes it hard to understand what are actually captured and how they are fused. In this work, we borrow the notion of potential field from physics as an interpretable and unified representation to model all stimuli. This allows us to not only supervise the intermediate learning process, but also have a coherent method to fuse the information of different sources. From the generated potential fields, we further estimate future motion direction and speed, which are modeled as Gaussian distributions to account for the multi-modal nature of the problem. The final prediction results are generated by recurrently moving past location based on the estimated motion direction and speed. We show state-of-the-art results on the ETH, UCY, and Stanford Drone datasets.



### Deep Verifier Networks: Verification of Deep Discriminative Models with Deep Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1911.07421v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1911.07421v3)
- **Published**: 2019-11-18 04:23:12+00:00
- **Updated**: 2021-01-01 21:08:11+00:00
- **Authors**: Tong Che, Xiaofeng Liu, Site Li, Yubin Ge, Ruixiang Zhang, Caiming Xiong, Yoshua Bengio
- **Comment**: Accepted to AAAI 2021
- **Journal**: None
- **Summary**: AI Safety is a major concern in many deep learning applications such as autonomous driving. Given a trained deep learning model, an important natural problem is how to reliably verify the model's prediction. In this paper, we propose a novel framework -- deep verifier networks (DVN) to verify the inputs and outputs of deep discriminative models with deep generative models. Our proposed model is based on conditional variational auto-encoders with disentanglement constraints. We give both intuitive and theoretical justifications of the model. Our verifier network is trained independently with the prediction model, which eliminates the need of retraining the verifier network for a new model. We test the verifier network on out-of-distribution detection and adversarial example detection problems, as well as anomaly detection problems in structured prediction tasks such as image caption generation. We achieve state-of-the-art results in all of these problems.



### Multiple Face Analyses through Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.07846v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.07846v1)
- **Published**: 2019-11-18 04:24:17+00:00
- **Updated**: 2019-11-18 04:24:17+00:00
- **Authors**: Shangfei Wang, Shi Yin, Longfei Hao, Guang Liang
- **Comment**: None
- **Journal**: None
- **Summary**: This inherent relations among multiple face analysis tasks, such as landmark detection, head pose estimation, gender recognition and face attribute estimation are crucial to boost the performance of each task, but have not been thoroughly explored since typically these multiple face analysis tasks are handled as separate tasks. In this paper, we propose a novel deep multi-task adversarial learning method to localize facial landmark, estimate head pose and recognize gender jointly or estimate multiple face attributes simultaneously through exploring their dependencies from both image representation-level and label-level. Specifically, the proposed method consists of a deep recognition network R and a discriminator D. The deep recognition network is used to learn the shared middle-level image representation and conducts multiple face analysis tasks simultaneously. Through multi-task learning mechanism, the recognition network explores the dependencies among multiple face analysis tasks, such as facial landmark localization, head pose estimation, gender recognition and face attribute estimation from image representation-level. The discriminator is introduced to enforce the distribution of the multiple face analysis tasks to converge to that inherent in the ground-truth labels. During training, the recognizer tries to confuse the discriminator, while the discriminator competes with the recognizer through distinguishing the predicted label combination from the ground-truth one. Though adversarial learning, we explore the dependencies among multiple face analysis tasks from label-level. Experimental results on four benchmark databases, i.e., the AFLW database, the Multi-PIE database, the CelebA database and the LFWA database, demonstrate the effectiveness of the proposed method for multiple face analyses.



### Learning to Predict More Accurate Text Instances for Scene Text Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.07423v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07423v2)
- **Published**: 2019-11-18 04:35:47+00:00
- **Updated**: 2020-04-16 01:27:35+00:00
- **Authors**: XiaoQian Li, Jie Liu, ShuWu Zhang, GuiXuan Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: At present, multi-oriented text detection methods based on deep neural network have achieved promising performances on various benchmarks. Nevertheless, there are still some difficulties for arbitrary shape text detection, especially for a simple and proper representation of arbitrary shape text instances. In this paper, a pixel-based text detector is proposed to facilitate the representation and prediction of text instances with arbitrary shapes in a simple manner. Firstly, to alleviate the effect of the target vertex sorting and achieve the direct regression of arbitrary shape text instances, the starting-point-independent coordinates regression loss is proposed. Furthermore, to predict more accurate text instances, the text instance accuracy loss is proposed as an assistant task to refine the predicted coordinates under the guidance of IoU. To evaluate the effectiveness of our detector, extensive experiments have been carried on public benchmarks which contain arbitrary shape text instances and multi-oriented text instances. We obtain 84.8% of F-measure on Total-Text benchmark. The results show that our method can reach state-of-the-art performance.



### Fast and Accurate 3D Hand Pose Estimation via Recurrent Neural Network for Capturing Hand Articulations
- **Arxiv ID**: http://arxiv.org/abs/1911.07424v2
- **DOI**: 10.1109/ACCESS.2020.3001637
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.07424v2)
- **Published**: 2019-11-18 04:38:25+00:00
- **Updated**: 2020-03-18 04:36:31+00:00
- **Authors**: Cheol-hwan Yoo, Seo-won Ji, Yong-goo Shin, Seung-wook Kim, Sung-jea Ko
- **Comment**: None
- **Journal**: IEEE Access. 8 (2020) 114010-114019
- **Summary**: 3D hand pose estimation from a single depth image plays an important role in computer vision and human-computer interaction. Although recent hand pose estimation methods using convolution neural network (CNN) have shown notable improvements in accuracy, most of them have a limitation that they rely on a complex network structure without fully exploiting the articulated structure of the hand. A hand, which is an articulated object, is composed of six local parts: the palm and five independent fingers. Each finger consists of sequential-joints that provide constrained motion, referred to as a kinematic chain. In this paper, we propose a hierarchically-structured convolutional recurrent neural network (HCRNN) with six branches that estimate the 3D position of the palm and five fingers independently. The palm position is predicted via fully-connected layers. Each sequential-joint, i.e. finger position, is obtained using a recurrent neural network (RNN) to capture the spatial dependencies between adjacent joints. Then the output features of the palm and finger branches are concatenated to estimate the global hand position. HCRNN directly takes the depth map as an input without a time-consuming data conversion, such as 3D voxels and point clouds. Experimental results on public datasets demonstrate that the proposed HCRNN not only outperforms most 2D CNN-based methods using the depth image as their inputs but also achieves competitive results with state-of-the-art 3D CNN-based methods with a highly efficient running speed of 285 fps on a single GPU.



### Efficient Hardware Implementation of Incremental Learning and Inference on Chip
- **Arxiv ID**: http://arxiv.org/abs/1911.07847v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07847v1)
- **Published**: 2019-11-18 04:42:09+00:00
- **Updated**: 2019-11-18 04:42:09+00:00
- **Authors**: Ghouthi Boukli Hacene, Vincent Gripon, Nicolas Farrugia, Matthieu Arzel, Michel Jezequel
- **Comment**: In 2019 IEEE International NEWCAS Conference
- **Journal**: None
- **Summary**: In this paper, we tackle the problem of incrementally learning a classifier, one example at a time, directly on chip. To this end, we propose an efficient hardware implementation of a recently introduced incremental learning procedure that achieves state-of-the-art performance by combining transfer learning with majority votes and quantization techniques. The proposed design is able to accommodate for both new examples and new classes directly on the chip. We detail the hardware implementation of the method (implemented on FPGA target) and show it requires limited resources while providing a significant acceleration compared to using a CPU.



### Large Scale Open-Set Deep Logo Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.07440v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07440v4)
- **Published**: 2019-11-18 05:44:17+00:00
- **Updated**: 2022-03-12 23:47:45+00:00
- **Authors**: Muhammet Bastan, Hao-Yu Wu, Tian Cao, Bhargava Kota, Mehmet Tek
- **Comment**: Open Set Logo Detection (OSLD) dataset available at
  https://github.com/mubastan/osld
- **Journal**: None
- **Summary**: We present an open-set logo detection (OSLD) system, which can detect (localize and recognize) any number of unseen logo classes without re-training; it only requires a small set of canonical logo images for each logo class. We achieve this using a two-stage approach: (1) Generic logo detection to detect candidate logo regions in an image. (2) Logo matching for matching the detected logo regions to a set of canonical logo images to recognize them.   We constructed an open-set logo detection dataset with 12.1k logo classes and released it for research purposes.We demonstrate the effectiveness of OSLD on our dataset and on the standard Flickr-32 logo dataset, outperforming the state-of-the-art open-set and closed-set logo detection methods by a large margin. OSLD is scalable to millions of logo classes.



### NAIS: Neural Architecture and Implementation Search and its Applications in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1911.07446v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.07446v1)
- **Published**: 2019-11-18 06:17:14+00:00
- **Updated**: 2019-11-18 06:17:14+00:00
- **Authors**: Cong Hao, Yao Chen, Xinheng Liu, Atif Sarwari, Daryl Sew, Ashutosh Dhar, Bryan Wu, Dongdong Fu, Jinjun Xiong, Wen-mei Hwu, Junli Gu, Deming Chen
- **Comment**: 8 pages, ICCAD 2019
- **Journal**: None
- **Summary**: The rapidly growing demands for powerful AI algorithms in many application domains have motivated massive investment in both high-quality deep neural network (DNN) models and high-efficiency implementations. In this position paper, we argue that a simultaneous DNN/implementation co-design methodology, named Neural Architecture and Implementation Search (NAIS), deserves more research attention to boost the development productivity and efficiency of both DNN models and implementation optimization. We propose a stylized design methodology that can drastically cut down the search cost while preserving the quality of the end solution.As an illustration, we discuss this DNN/implementation methodology in the context of both FPGAs and GPUs. We take autonomous driving as a key use case as it is one of the most demanding areas for high quality AI algorithms and accelerators. We discuss how such a co-design methodology can impact the autonomous driving industry significantly. We identify several research opportunities in this exciting domain.



### Accurate Trajectory Prediction for Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1911.08568v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.08568v1)
- **Published**: 2019-11-18 06:38:33+00:00
- **Updated**: 2019-11-18 06:38:33+00:00
- **Authors**: Michael Diodato, Yu Li, Antonia Lovjer, Minsu Yeom, Albert Song, Yiyang Zeng, Abhay Khosla, Benedikt Schifferer, Manik Goyal, Iddo Drori
- **Comment**: arXiv admin note: text overlap with arXiv:1910.10318,
  arXiv:1910.10317
- **Journal**: None
- **Summary**: Predicting vehicle trajectories, angle and speed is important for safe and comfortable driving. We demonstrate the best predicted angle, speed, and best performance overall winning the top three places of the ICCV 2019 Learning to Drive challenge. Our key contributions are (i) a general neural network system architecture which embeds and fuses together multiple inputs by encoding, and decodes multiple outputs using neural networks, (ii) using pre-trained neural networks for augmenting the given input data with segmentation maps and semantic information, and (iii) leveraging the form and distribution of the expected output in the model.



### Unsupervised Reinforcement Learning of Transferable Meta-Skills for Embodied Navigation
- **Arxiv ID**: http://arxiv.org/abs/1911.07450v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07450v2)
- **Published**: 2019-11-18 06:53:51+00:00
- **Updated**: 2020-04-06 03:53:02+00:00
- **Authors**: Juncheng Li, Xin Wang, Siliang Tang, Haizhou Shi, Fei Wu, Yueting Zhuang, William Yang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Visual navigation is a task of training an embodied agent by intelligently navigating to a target object (e.g., television) using only visual observations. A key challenge for current deep reinforcement learning models lies in the requirements for a large amount of training data. It is exceedingly expensive to construct sufficient 3D synthetic environments annotated with the target object information. In this paper, we focus on visual navigation in the low-resource setting, where we have only a few training environments annotated with object information. We propose a novel unsupervised reinforcement learning approach to learn transferable meta-skills (e.g., bypass obstacles, go straight) from unannotated environments without any supervisory signals. The agent can then fast adapt to visual navigation through learning a high-level master policy to combine these meta-skills, when the visual-navigation-specified reward is provided. Evaluation in the AI2-THOR environments shows that our method significantly outperforms the baseline by 53.34% relatively on SPL, and further qualitative analysis demonstrates that our method learns transferable motor primitives for visual navigation.



### DirectPose: Direct End-to-End Multi-Person Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1911.07451v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07451v2)
- **Published**: 2019-11-18 06:55:08+00:00
- **Updated**: 2019-11-24 05:03:13+00:00
- **Authors**: Zhi Tian, Hao Chen, Chunhua Shen
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: We propose the first direct end-to-end multi-person pose estimation framework, termed DirectPose. Inspired by recent anchor-free object detectors, which directly regress the two corners of target bounding-boxes, the proposed framework directly predicts instance-aware keypoints for all the instances from a raw input image, eliminating the need for heuristic grouping in bottom-up methods or bounding-box detection and RoI operations in top-down ones. We also propose a novel Keypoint Alignment (KPAlign) mechanism, which overcomes the main difficulty: lack of the alignment between the convolutional features and predictions in this end-to-end framework. KPAlign improves the framework's performance by a large margin while still keeping the framework end-to-end trainable. With the only postprocessing non-maximum suppression (NMS), our proposed framework can detect multi-person keypoints with or without bounding-boxes in a single shot. Experiments demonstrate that the end-to-end paradigm can achieve competitive or better performance than previous strong baselines, in both bottom-up and top-down methods. We hope that our end-to-end approach can provide a new perspective for the human pose estimation task.



### Walking the Tightrope: An Investigation of the Convolutional Autoencoder Bottleneck
- **Arxiv ID**: http://arxiv.org/abs/1911.07460v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.07460v2)
- **Published**: 2019-11-18 07:19:14+00:00
- **Updated**: 2020-05-12 19:27:36+00:00
- **Authors**: Ilja Manakov, Markus Rohm, Volker Tresp
- **Comment**: code available at https://github.com/IljaManakov/WalkingTheTightrope
- **Journal**: None
- **Summary**: In this paper, we present an in-depth investigation of the convolutional autoencoder (CAE) bottleneck. Autoencoders (AE), and especially their convolutional variants, play a vital role in the current deep learning toolbox. Researchers and practitioners employ CAEs for a variety of tasks, ranging from outlier detection and compression to transfer and representation learning. Despite their widespread adoption, we have limited insight into how the bottleneck shape impacts the emergent properties of the CAE. We demonstrate that increased height and width of the bottleneck drastically improves generalization, which in turn leads to better performance of the latent codes in downstream transfer learning tasks. The number of channels in the bottleneck, on the other hand, is secondary in importance. Furthermore, we show empirically that, contrary to popular belief, CAEs do not learn to copy their input, even when the bottleneck has the same number of neurons as there are pixels in the input. Copying does not occur, despite training the CAE for 1,000 epochs on a tiny ($\approx$ 600 images) dataset. We believe that the findings in this paper are directly applicable and will lead to improvements in models that rely on CAEs.



### Preparing Lessons: Improve Knowledge Distillation with Better Supervision
- **Arxiv ID**: http://arxiv.org/abs/1911.07471v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07471v3)
- **Published**: 2019-11-18 07:47:29+00:00
- **Updated**: 2020-07-24 14:16:45+00:00
- **Authors**: Tiancheng Wen, Shenqi Lai, Xueming Qian
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge distillation (KD) is widely used for training a compact model with the supervision of another large model, which could effectively improve the performance. Previous methods mainly focus on two aspects: 1) training the student to mimic representation space of the teacher; 2) training the model progressively or adding extra module like discriminator. Knowledge from teacher is useful, but it is still not exactly right compared with ground truth. Besides, overly uncertain supervision also influences the result. We introduce two novel approaches, Knowledge Adjustment (KA) and Dynamic Temperature Distillation (DTD), to penalize bad supervision and improve student model. Experiments on CIFAR-100, CINIC-10 and Tiny ImageNet show that our methods get encouraging performance compared with state-of-the-art methods. When combined with other KD-based methods, the performance will be further improved.



### Learning to Synthesize Fashion Textures
- **Arxiv ID**: http://arxiv.org/abs/1911.07472v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07472v1)
- **Published**: 2019-11-18 07:48:12+00:00
- **Updated**: 2019-11-18 07:48:12+00:00
- **Authors**: Wu Shi, Tak-Wai Hui, Ziwei Liu, Dahua Lin, Chen Change Loy
- **Comment**: None
- **Journal**: None
- **Summary**: Existing unconditional generative models mainly focus on modeling general objects, such as faces and indoor scenes. Fashion textures, another important type of visual elements around us, have not been extensively studied. In this work, we propose an effective generative model for fashion textures and also comprehensively investigate the key components involved: internal representation, latent space sampling and the generator architecture. We use Gram matrix as a suitable internal representation for modeling realistic fashion textures, and further design two dedicated modules for modulating Gram matrix into a low-dimension vector. Since fashion textures are scale-dependent, we propose a recursive auto-encoder to capture the dependency between multiple granularity levels of texture feature. Another important observation is that fashion textures are multi-modal. We fit and sample from a Gaussian mixture model in the latent space to improve the diversity of the generated textures. Extensive experiments demonstrate that our approach is capable of synthesizing more realistic and diverse fashion textures over other state-of-the-art methods.



### Fine-Grained Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1911.07478v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07478v1)
- **Published**: 2019-11-18 07:56:06+00:00
- **Updated**: 2019-11-18 07:56:06+00:00
- **Authors**: Heewon Kim, Seokil Hong, Bohyung Han, Heesoo Myeong, Kyoung Mu Lee
- **Comment**: None
- **Journal**: None
- **Summary**: We present an elegant framework of fine-grained neural architecture search (FGNAS), which allows to employ multiple heterogeneous operations within a single layer and can even generate compositional feature maps using several different base operations. FGNAS runs efficiently in spite of significantly large search space compared to other methods because it trains networks end-to-end by a stochastic gradient descent method. Moreover, the proposed framework allows to optimize the network under predefined resource constraints in terms of number of parameters, FLOPs and latency. FGNAS has been applied to two crucial applications in resource demanding computer vision tasks---large-scale image classification and image super-resolution---and demonstrates the state-of-the-art performance through flexible operation search and channel pruning.



### Modality to Modality Translation: An Adversarial Representation Learning and Graph Fusion Network for Multimodal Fusion
- **Arxiv ID**: http://arxiv.org/abs/1911.07848v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1911.07848v4)
- **Published**: 2019-11-18 08:29:20+00:00
- **Updated**: 2020-12-10 01:52:20+00:00
- **Authors**: Sijie Mai, Haifeng Hu, Songlong Xing
- **Comment**: Accepted by AAAI-2020; code is available at:
  https://github.com/TmacMai/ARGF_multimodal_fusion
- **Journal**: None
- **Summary**: Learning joint embedding space for various modalities is of vital importance for multimodal fusion. Mainstream modality fusion approaches fail to achieve this goal, leaving a modality gap which heavily affects cross-modal fusion. In this paper, we propose a novel adversarial encoder-decoder-classifier framework to learn a modality-invariant embedding space. Since the distributions of various modalities vary in nature, to reduce the modality gap, we translate the distributions of source modalities into that of target modality via their respective encoders using adversarial training. Furthermore, we exert additional constraints on embedding space by introducing reconstruction loss and classification loss. Then we fuse the encoded representations using hierarchical graph neural network which explicitly explores unimodal, bimodal and trimodal interactions in multi-stage. Our method achieves state-of-the-art performance on multiple datasets. Visualization of the learned embeddings suggests that the joint embedding space learned by our method is discriminative. code is available at: \url{https://github.com/TmacMai/ARGF_multimodal_fusion}



### AI-based Pilgrim Detection using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.07509v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1911.07509v2)
- **Published**: 2019-11-18 09:46:54+00:00
- **Updated**: 2020-02-18 19:06:10+00:00
- **Authors**: Marwa Ben Jabra, Adel Ammar, Anis Koubaa, Omar Cheikhrouhou, Habib Hamam
- **Comment**: Accepted in ATSIP'2020
- **Journal**: None
- **Summary**: Pilgrimage represents the most important Islamic religious gathering in the world where millions of pilgrims visit the holy places of Makkah and Madinah to perform their rituals. The safety and security of pilgrims is the highest priority for the authorities. In Makkah, 5000 cameras are spread around the holy for monitoring pilgrims, but it is almost impossible to track all events by humans considering the huge number of images collected every second. To address this issue, we propose to use artificial intelligence technique based on deep learning and convolution neural networks to detect and identify Pilgrims and their features. For this purpose, we built a comprehensive dataset for the detection of pilgrims and their genders. Then, we develop two convolutional neural networks based on YOLOv3 and Faster-RCNN for the detection of Pilgrims. Experiments results show that Faster RCNN with Inception v2 feature extractor provides the best mean average precision over all classes of 51%.



### Automated Human Claustrum Segmentation using Deep Learning Technologies
- **Arxiv ID**: http://arxiv.org/abs/1911.07515v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.07515v1)
- **Published**: 2019-11-18 09:59:09+00:00
- **Updated**: 2019-11-18 09:59:09+00:00
- **Authors**: Ahmed Awad Albishri, Syed Jawad Hussain Shah, Anthony Schmiedler, Seung Suk Kang, Yugyung Lee
- **Comment**: 6 pages, 4 figures
- **Journal**: None
- **Summary**: In recent years, Deep Learning (DL) has shown promising results in conducting AI tasks such as computer vision and image segmentation. Specifically, Convolutional Neural Network (CNN) models in DL have been applied to prevention,detection, and diagnosis in predictive medicine. Image segmentation plays a significant role in disease detection and prevention.However, there are enormous challenges in performing DL-based automatic segmentation due to the nature of medical images such as heterogeneous modalities and formats, insufficient labeled training data, and the high-class imbalance in the labeled data. Furthermore, automating segmentation of medical images,like magnetic resonance images (MRI), becomes a challenging task. The need for automated segmentation or annotation is what motivates our work. In this paper, we propose a fully automated approach that aims to segment the human claustrum for analytical purposes. We applied a U-Net CNN model to segment the claustrum (Cl) from a MRI dataset. With this approach, we have achieved an average Dice per case score of 0.72 for Cl segmentation, with K=5 for cross-validation. The expert in the medical domain also evaluates these results.



### Constructing Multiple Tasks for Augmentation: Improving Neural Image Classification With K-means Features
- **Arxiv ID**: http://arxiv.org/abs/1911.07518v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07518v1)
- **Published**: 2019-11-18 10:04:08+00:00
- **Updated**: 2019-11-18 10:04:08+00:00
- **Authors**: Tao Gui, Lizhi Qing, Qi Zhang, Jiacheng Ye, Hang Yan, Zichu Fei, Xuanjing Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-task learning (MTL) has received considerable attention, and numerous deep learning applications benefit from MTL with multiple objectives. However, constructing multiple related tasks is difficult, and sometimes only a single task is available for training in a dataset. To tackle this problem, we explored the idea of using unsupervised clustering to construct a variety of auxiliary tasks from unlabeled data or existing labeled data. We found that some of these newly constructed tasks could exhibit semantic meanings corresponding to certain human-specific attributes, but some were non-ideal. In order to effectively reduce the impact of non-ideal auxiliary tasks on the main task, we further proposed a novel meta-learning-based multi-task learning approach, which trained the shared hidden layers on auxiliary tasks, while the meta-optimization objective was to minimize the loss on the main task, ensuring that the optimizing direction led to an improvement on the main task. Experimental results across five image datasets demonstrated that the proposed method significantly outperformed existing single task learning, semi-supervised learning, and some data augmentation methods, including an improvement of more than 9% on the Omniglot dataset.



### The Devil is in the Details: Delving into Unbiased Data Processing for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1911.07524v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07524v2)
- **Published**: 2019-11-18 10:17:12+00:00
- **Updated**: 2020-12-31 04:39:42+00:00
- **Authors**: Junjie Huang, Zheng Zhu, Feng Guo, Guan Huang, Dalong Du
- **Comment**: project:https://github.com/HuangJunJie2017/UDP-Pose
- **Journal**: CVPR2020
- **Summary**: Being a fundamental component in training and inference, data processing has not been systematically considered in human pose estimation community, to the best of our knowledge. In this paper, we focus on this problem and find that the devil of human pose estimation evolution is in the biased data processing. Specifically, by investigating the standard data processing in state-of-the-art approaches mainly including coordinate system transformation and keypoint format transformation (i.e., encoding and decoding), we find that the results obtained by common flipping strategy are unaligned with the original ones in inference. Moreover, there is a statistical error in some keypoint format transformation methods. Two problems couple together, significantly degrade the pose estimation performance and thus lay a trap for the research community. This trap has given bone to many suboptimal remedies, which are always unreported, confusing but influential. By causing failure in reproduction and unfair in comparison, the unreported remedies seriously impedes the technological development. To tackle this dilemma from the source, we propose Unbiased Data Processing (UDP) consist of two technique aspect for the two aforementioned problems respectively (i.e., unbiased coordinate system transformation and unbiased keypoint format transformation). As a model-agnostic approach and a superior solution, UDP successfully pushes the performance boundary of human pose estimation and offers a higher and more reliable baseline for research community. Code is public available in https://github.com/HuangJunJie2017/UDP-Pose



### SOGNet: Scene Overlap Graph Network for Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.07527v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07527v1)
- **Published**: 2019-11-18 10:26:35+00:00
- **Updated**: 2019-11-18 10:26:35+00:00
- **Authors**: Yibo Yang, Hongyang Li, Xia Li, Qijie Zhao, Jianlong Wu, Zhouchen Lin
- **Comment**: To appear in AAAI 2020. Our method also won the Innovation Award in
  COCO 2019 challenge
- **Journal**: None
- **Summary**: The panoptic segmentation task requires a unified result from semantic and instance segmentation outputs that may contain overlaps. However, current studies widely ignore modeling overlaps. In this study, we aim to model overlap relations among instances and resolve them for panoptic segmentation. Inspired by scene graph representation, we formulate the overlapping problem as a simplified case, named scene overlap graph. We leverage each object's category, geometry and appearance features to perform relational embedding, and output a relation matrix that encodes overlap relations. In order to overcome the lack of supervision, we introduce a differentiable module to resolve the overlap between any pair of instances. The mask logits after removing overlaps are fed into per-pixel instance \verb|id| classification, which leverages the panoptic supervision to assist in the modeling of overlap relations. Besides, we generate an approximate ground truth of overlap relations as the weak supervision, to quantify the accuracy of overlap relations predicted by our method. Experiments on COCO and Cityscapes demonstrate that our method is able to accurately predict overlap relations, and outperform the state-of-the-art performance for panoptic segmentation. Our method also won the Innovation Award in COCO 2019 challenge.



### Ladder Loss for Coherent Visual-Semantic Embedding
- **Arxiv ID**: http://arxiv.org/abs/1911.07528v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07528v1)
- **Published**: 2019-11-18 10:31:17+00:00
- **Updated**: 2019-11-18 10:31:17+00:00
- **Authors**: Mo Zhou, Zhenxing Niu, Le Wang, Zhanning Gao, Qilin Zhang, Gang Hua
- **Comment**: Accepted to AAAI-2020
- **Journal**: None
- **Summary**: For visual-semantic embedding, the existing methods normally treat the relevance between queries and candidates in a bipolar way -- relevant or irrelevant, and all "irrelevant" candidates are uniformly pushed away from the query by an equal margin in the embedding space, regardless of their various proximity to the query. This practice disregards relatively discriminative information and could lead to suboptimal ranking in the retrieval results and poorer user experience, especially in the long-tail query scenario where a matching candidate may not necessarily exist. In this paper, we introduce a continuous variable to model the relevance degree between queries and multiple candidates, and propose to learn a coherent embedding space, where candidates with higher relevance degrees are mapped closer to the query than those with lower relevance degrees. In particular, the new ladder loss is proposed by extending the triplet loss inequality to a more general inequality chain, which implements variable push-away margins according to respective relevance degrees. In addition, a proper Coherent Score metric is proposed to better measure the ranking results including those "irrelevant" candidates. Extensive experiments on multiple datasets validate the efficacy of our proposed method, which achieves significant improvement over existing state-of-the-art methods.



### Finding Missing Children: Aging Deep Face Features
- **Arxiv ID**: http://arxiv.org/abs/1911.07538v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07538v2)
- **Published**: 2019-11-18 10:58:04+00:00
- **Updated**: 2019-11-19 04:03:42+00:00
- **Authors**: Debayan Deb, Divyansh Aggarwal, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Given a gallery of face images of missing children, state-of-the-art face recognition systems fall short in identifying a child (probe) recovered at a later age. We propose an age-progression module that can age-progress deep face features output by any commodity face matcher. For time lapses larger than 10 years (the missing child is found after 10 or more years), the proposed age-progression module improves the closed-set identification accuracy of FaceNet from 40% to 49.56% and CosFace from 56.88% to 61.25% on a child celebrity dataset, namely ITWCC. The proposed method also outperforms state-of-the-art approaches with a rank-1 identification rate from 94.91% to 95.91% on a public aging dataset, FG-NET, and from 99.50% to 99.58% on CACD-VS. These results suggest that aging face features enhances the ability to identify young children who are possible victims of child trafficking or abduction.



### Multi-Task Learning of Height and Semantics from Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/1911.07543v1
- **DOI**: 10.1109/LGRS.2019.2947783
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.07543v1)
- **Published**: 2019-11-18 11:08:11+00:00
- **Updated**: 2019-11-18 11:08:11+00:00
- **Authors**: Marcela Carvalho, Bertrand Le Saux, Pauline Trouvé-Peloux, Frédéric Champagnat, Andrés Almansa
- **Comment**: Published IEEE Geoscience and Remote Sensing Letters. Code
  https://github.com/marcelampc/mtl_aerial_images
- **Journal**: None
- **Summary**: Aerial or satellite imagery is a great source for land surface analysis, which might yield land use maps or elevation models. In this investigation, we present a neural network framework for learning semantics and local height together. We show how this joint multi-task learning benefits to each task on the large dataset of the 2018 Data Fusion Contest. Moreover, our framework also yields an uncertainty map which allows assessing the prediction of the model. Code is available at https://github.com/marcelampc/mtl_aerial_images .



### FFA-Net: Feature Fusion Attention Network for Single Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/1911.07559v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07559v2)
- **Published**: 2019-11-18 11:43:58+00:00
- **Updated**: 2019-12-05 06:33:42+00:00
- **Authors**: Xu Qin, Zhilin Wang, Yuanchao Bai, Xiaodong Xie, Huizhu Jia
- **Comment**: Accepted by AAAI2020
- **Journal**: None
- **Summary**: In this paper, we propose an end-to-end feature fusion at-tention network (FFA-Net) to directly restore the haze-free image. The FFA-Net architecture consists of three key components:   1) A novel Feature Attention (FA) module combines Channel Attention with Pixel Attention mechanism, considering that different channel-wise features contain totally different weighted information and haze distribution is uneven on the different image pixels. FA treats different features and pixels unequally, which provides additional flexibility in dealing with different types of information, expanding the representational ability of CNNs. 2) A basic block structure consists of Local Residual Learning and Feature Attention, Local Residual Learning allowing the less important information such as thin haze region or low-frequency to be bypassed through multiple local residual connections, let main network architecture focus on more effective information. 3) An Attention-based different levels Feature Fusion (FFA) structure, the feature weights are adaptively learned from the Feature Attention (FA) module, giving more weight to important features. This structure can also retain the information of shallow layers and pass it into deep layers.   The experimental results demonstrate that our proposed FFA-Net surpasses previous state-of-the-art single image dehazing methods by a very large margin both quantitatively and qualitatively, boosting the best published PSNR metric from 30.23db to 36.39db on the SOTS indoor test dataset.   Code has been made available at GitHub.



### Automated fetal brain extraction from clinical Ultrasound volumes using 3D Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.07566v2
- **DOI**: 10.1007/978-3-030-39343-4_13
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.07566v2)
- **Published**: 2019-11-18 11:56:28+00:00
- **Updated**: 2019-11-19 10:16:23+00:00
- **Authors**: Felipe Moser, Ruobing Huang, Aris T. Papageorghiou, Bartlomiej W. Papiez, Ana I. L. Namburete
- **Comment**: 13 pages, 7 figures, MIUA conference
- **Journal**: None
- **Summary**: To improve the performance of most neuroimiage analysis pipelines, brain extraction is used as a fundamental first step in the image processing. But in the case of fetal brain development, there is a need for a reliable US-specific tool. In this work we propose a fully automated 3D CNN approach to fetal brain extraction from 3D US clinical volumes with minimal preprocessing. Our method accurately and reliably extracts the brain regardless of the large data variation inherent in this imaging modality. It also performs consistently throughout a gestational age range between 14 and 31 weeks, regardless of the pose variation of the subject, the scale, and even partial feature-obstruction in the image, outperforming all current alternatives.



### Bias-Aware Heapified Policy for Active Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.07574v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.07574v1)
- **Published**: 2019-11-18 12:08:09+00:00
- **Updated**: 2019-11-18 12:08:09+00:00
- **Authors**: Wen-Yen Chang, Wen-Huan Chiang, Shao-Hao Lu, Tingfan Wu, Min Sun
- **Comment**: None
- **Journal**: None
- **Summary**: The data efficiency of learning-based algorithms is more and more important since high-quality and clean data is expensive as well as hard to collect. In order to achieve high model performance with the least number of samples, active learning is a technique that queries the most important subset of data from the original dataset. In active learning domain, one of the mainstream research is the heuristic uncertainty-based method which is useful for the learning-based system. Recently, a few works propose to apply policy reinforcement learning (PRL) for querying important data. It seems more general than heuristic uncertainty-based method owing that PRL method depends on data feature which is reliable than human prior. However, there have two problems - sample inefficiency of policy learning and overconfidence, when applying PRL on active learning. To be more precise, sample inefficiency of policy learning occurs when sampling within a large action space, in the meanwhile, class imbalance can lead to the overconfidence. In this paper, we propose a bias-aware policy network called Heapified Active Learning (HAL), which prevents overconfidence, and improves sample efficiency of policy learning by heapified structure without ignoring global inforamtion(overview of the whole unlabeled set). In our experiment, HAL outperforms other baseline methods on MNIST dataset and duplicated MNIST. Last but not least, we investigate the generalization of the HAL policy learned on MNIST dataset by directly applying it on MNIST-M. We show that the agent can generalize and outperform directly-learned policy under constrained labeled sets.



### Co-Attentive Equivariant Neural Networks: Focusing Equivariance On Transformations Co-Occurring In Data
- **Arxiv ID**: http://arxiv.org/abs/1911.07849v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.07849v2)
- **Published**: 2019-11-18 12:41:12+00:00
- **Updated**: 2020-02-10 13:56:10+00:00
- **Authors**: David W. Romero, Mark Hoogendoorn
- **Comment**: Proceedings of the 8th International Conference on Learning
  Representations (ICLR), 2020
- **Journal**: Proceedings of the International Conference on Learning
  Representations, 2020
- **Summary**: Equivariance is a nice property to have as it produces much more parameter efficient neural architectures and preserves the structure of the input through the feature mapping. Even though some combinations of transformations might never appear (e.g. an upright face with a horizontal nose), current equivariant architectures consider the set of all possible transformations in a transformation group when learning feature representations. Contrarily, the human visual system is able to attend to the set of relevant transformations occurring in the environment and utilizes this information to assist and improve object recognition. Based on this observation, we modify conventional equivariant feature mappings such that they are able to attend to the set of co-occurring transformations in data and generalize this notion to act on groups consisting of multiple symmetries. We show that our proposed co-attentive equivariant neural networks consistently outperform conventional rotation equivariant and rotation & reflection equivariant neural networks on rotated MNIST and CIFAR-10.



### Signal Clustering with Class-independent Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.07590v1
- **DOI**: 10.1109/ICASSP40776.2020.9053409
- **Categories**: **cs.CV**, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1911.07590v1)
- **Published**: 2019-11-18 12:43:57+00:00
- **Updated**: 2019-11-18 12:43:57+00:00
- **Authors**: Stefano Gasperini, Magdalini Paschali, Carsten Hopke, David Wittmann, Nassir Navab
- **Comment**: Under Review for IEEE ICASSP 2020
- **Journal**: None
- **Summary**: Radar signals have been dramatically increasing in complexity, limiting the source separation ability of traditional approaches. In this paper we propose a Deep Learning-based clustering method, which encodes concurrent signals into images, and, for the first time, tackles clustering with image segmentation. Novel loss functions are introduced to optimize a Neural Network to separate the input pulses into pure and non-fragmented clusters. Outperforming a variety of baselines, the proposed approach is capable of clustering inputs directly with a Neural Network, in an end-to-end fashion.



### The inD Dataset: A Drone Dataset of Naturalistic Road User Trajectories at German Intersections
- **Arxiv ID**: http://arxiv.org/abs/1911.07602v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1911.07602v1)
- **Published**: 2019-11-18 13:20:41+00:00
- **Updated**: 2019-11-18 13:20:41+00:00
- **Authors**: Julian Bock, Robert Krajewski, Tobias Moers, Steffen Runde, Lennart Vater, Lutz Eckstein
- **Comment**: None
- **Journal**: None
- **Summary**: Automated vehicles rely heavily on data-driven methods, especially for complex urban environments. Large datasets of real world measurement data in the form of road user trajectories are crucial for several tasks like road user prediction models or scenario-based safety validation. So far, though, this demand is unmet as no public dataset of urban road user trajectories is available in an appropriate size, quality and variety. By contrast, the highway drone dataset (highD) has recently shown that drones are an efficient method for acquiring naturalistic road user trajectories. Compared to driving studies or ground-level infrastructure sensors, one major advantage of using a drone is the possibility to record naturalistic behavior, as road users do not notice measurements taking place. Due to the ideal viewing angle, an entire intersection scenario can be measured with significantly less occlusion than with sensors at ground level. Both the class and the trajectory of each road user can be extracted from the video recordings with high precision using state-of-the-art deep neural networks. Therefore, we propose the creation of a comprehensive, large-scale urban intersection dataset with naturalistic road user behavior using camera-equipped drones as successor of the highD dataset. The resulting dataset contains more than 11500 road users including vehicles, bicyclists and pedestrians at intersections in Germany and is called inD. The dataset consists of 10 hours of measurement data from four intersections and is available online for non-commercial research at: http://www.inD-dataset.com



### Localizing Occluders with Compositional Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.08571v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08571v1)
- **Published**: 2019-11-18 13:49:04+00:00
- **Updated**: 2019-11-18 13:49:04+00:00
- **Authors**: Adam Kortylewski, Qing Liu, Huiyu Wang, Zhishuai Zhang, Alan Yuille
- **Comment**: Presented at "NeurIPS 2019 workshop on Perception as generative
  reasoning" and "NeurIPS 2019 workshop on Context and Compositionality in
  Biological and Artificial Neural Systems". arXiv admin note: text overlap
  with arXiv:1905.11826
- **Journal**: None
- **Summary**: Compositional convolutional networks are generative compositional models of neural network features, that achieve state of the art results when classifying partially occluded objects, even when they have not been exposed to occluded objects during training. In this work, we study the performance of CompositionalNets at localizing occluders in images. We show that the original model is not able to localize occluders well. We propose to overcome this limitation by modeling the feature activations as a mixture of von-Mises-Fisher distributions, which also allows for an end-to-end training of CompositionalNets. Our experimental results demonstrate that the proposed extensions increase the model's performance at localizing occluders as well as at classifying partially occluded objects.



### Domain Generalization Using a Mixture of Multiple Latent Domains
- **Arxiv ID**: http://arxiv.org/abs/1911.07661v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.07661v1)
- **Published**: 2019-11-18 14:31:36+00:00
- **Updated**: 2019-11-18 14:31:36+00:00
- **Authors**: Toshihiko Matsuura, Tatsuya Harada
- **Comment**: None
- **Journal**: None
- **Summary**: When domains, which represent underlying data distributions, vary during training and testing processes, deep neural networks suffer a drop in their performance. Domain generalization allows improvements in the generalization performance for unseen target domains by using multiple source domains. Conventional methods assume that the domain to which each sample belongs is known in training. However, many datasets, such as those collected via web crawling, contain a mixture of multiple latent domains, in which the domain of each sample is unknown. This paper introduces domain generalization using a mixture of multiple latent domains as a novel and more realistic scenario, where we try to train a domain-generalized model without using domain labels. To address this scenario, we propose a method that iteratively divides samples into latent domains via clustering, and which trains the domain-invariant feature extractor shared among the divided latent domains via adversarial learning. We assume that the latent domain of images is reflected in their style, and thus, utilize style features for clustering. By using these features, our proposed method successfully discovers latent domains and achieves domain generalization even if the domain labels are not given. Experiments show that our proposed method can train a domain-generalized model without using domain labels. Moreover, it outperforms conventional domain generalization methods, including those that utilize domain labels.



### GLMNet: Graph Learning-Matching Networks for Feature Matching
- **Arxiv ID**: http://arxiv.org/abs/1911.07681v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07681v1)
- **Published**: 2019-11-18 15:04:59+00:00
- **Updated**: 2019-11-18 15:04:59+00:00
- **Authors**: Bo Jiang, Pengfei Sun, Jin Tang, Bin Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, graph convolutional networks (GCNs) have shown great potential for the task of graph matching. It can integrate graph node feature embedding, node-wise affinity learning and matching optimization together in a unified end-to-end model. One important aspect of graph matching is the construction of two matching graphs. However, the matching graphs we feed to existing graph convolutional matching networks are generally fixed and independent of graph matching, which thus are not guaranteed to be optimal for the graph matching task. Also, existing GCN matching method employs several general smoothing-based graph convolutional layers to generate graph node embeddings, in which extensive smoothing convolution operation may dilute the desired discriminatory information of graph nodes. To overcome these issues, we propose a novel Graph Learning-Matching Network (GLMNet) for graph matching problem. GLMNet has three main aspects. (1) It integrates graph learning into graph matching which thus adaptively learn a pair of optimal graphs that best serve graph matching task. (2) It further employs a Laplacian sharpening convolutional module to generate more discriminative node embeddings for graph matching. (3) A new constraint regularized loss is designed for GLMNet training which can encode the desired one-to-one matching constraints in matching optimization. Experiments on two benchmarks demonstrate the effectiveness of GLMNet and advantages of its main modules.



### Automatic Image Co-Segmentation: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1911.07685v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07685v1)
- **Published**: 2019-11-18 15:07:34+00:00
- **Updated**: 2019-11-18 15:07:34+00:00
- **Authors**: Xiabi Liu, Xin Duan
- **Comment**: None
- **Journal**: None
- **Summary**: Image co-segmentation is important for its advantage of alleviating the ill-pose nature of image segmentation through exploring the correlation between related images. Many automatic image co-segmentation algorithms have been developed in the last decade, which are investigated comprehensively in this paper. We firstly analyze visual/semantic cues for guiding image co-segmentation, including object cues and correlation cues. Then we describe the traditional methods in three categories of object elements based, object regions/contours based, common object model based. In the next part, deep learning based methods are reviewed. Furthermore, widely used test datasets and evaluation criteria are introduced and the reported performances of the surveyed algorithms are compared with each other. Finally, we discuss the current challenges and possible future directions and conclude the paper. Hopefully, this comprehensive investigation will be helpful for the development of image co-segmentation technique.



### Affine Self Convolution
- **Arxiv ID**: http://arxiv.org/abs/1911.07704v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07704v1)
- **Published**: 2019-11-18 15:33:00+00:00
- **Updated**: 2019-11-18 15:33:00+00:00
- **Authors**: Nichita Diaconu, Daniel E Worrall
- **Comment**: None
- **Journal**: None
- **Summary**: Attention mechanisms, and most prominently self-attention, are a powerful building block for processing not only text but also images. These provide a parameter efficient method for aggregating inputs. We focus on self-attention in vision models, and we combine it with convolution, which as far as we know, are the first to do. What emerges is a convolution with data dependent filters. We call this an Affine Self Convolution. While this is applied differently at each spatial location, we show that it is translation equivariant. We also modify the Squeeze and Excitation variant of attention, extending both variants of attention to the roto-translation group. We evaluate these new models on CIFAR10 and CIFAR100 and show an improvement in the number of parameters, while reaching comparable or higher accuracy at test time against self-trained baselines.



### The Effectiveness of Variational Autoencoders for Active Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.07716v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.07716v1)
- **Published**: 2019-11-18 15:42:20+00:00
- **Updated**: 2019-11-18 15:42:20+00:00
- **Authors**: Farhad Pourkamali-Anaraki, Michael B. Wakin
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: The high cost of acquiring labels is one of the main challenges in deploying supervised machine learning algorithms. Active learning is a promising approach to control the learning process and address the difficulties of data labeling by selecting labeled training examples from a large pool of unlabeled instances. In this paper, we propose a new data-driven approach to active learning by choosing a small set of labeled data points that are both informative and representative. To this end, we present an efficient geometric technique to select a diverse core-set in a low-dimensional latent space obtained by training a Variational Autoencoder (VAE). Our experiments demonstrate an improvement in accuracy over two related techniques and, more importantly, signify the representation power of generative modeling for developing new active learning methods in high-dimensional data settings.



### Multi-modal Deep Guided Filtering for Comprehensible Medical Image Processing
- **Arxiv ID**: http://arxiv.org/abs/1911.07731v2
- **DOI**: 10.1109/TMI.2019.2955184
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.07731v2)
- **Published**: 2019-11-18 16:01:09+00:00
- **Updated**: 2020-05-28 09:50:48+00:00
- **Authors**: Bernhard Stimpel, Christopher Syben, Franziska Schirrmacher, Philipp Hoelter, Arnd Dörfler, Andreas Maier
- **Comment**: None
- **Journal**: IEEE Transactions on Medical Imaging, vol. 39, no. 5, pp.
  1703-1711, May 2020
- **Summary**: Deep learning-based image processing is capable of creating highly appealing results. However, it is still widely considered as a "blackbox" transformation. In medical imaging, this lack of comprehensibility of the results is a sensitive issue. The integration of known operators into the deep learning environment has proven to be advantageous for the comprehensibility and reliability of the computations. Consequently, we propose the use of the locally linear guided filter in combination with a learned guidance map for general purpose medical image processing. The output images are only processed by the guided filter while the guidance map can be trained to be task-optimal in an end-to-end fashion. We investigate the performance based on two popular tasks: image super resolution and denoising. The evaluation is conducted based on pairs of multi-modal magnetic resonance imaging and cross-modal computed tomography and magnetic resonance imaging datasets. For both tasks, the proposed approach is on par with state-of-the-art approaches. Additionally, we can show that the input image's content is almost unchanged after the processing which is not the case for conventional deep learning approaches. On top, the proposed pipeline offers increased robustness against degraded input as well as adversarial attacks.



### Oriented Boxes for Accurate Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.07732v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07732v3)
- **Published**: 2019-11-18 16:01:22+00:00
- **Updated**: 2020-03-13 10:36:01+00:00
- **Authors**: Patrick Follmann, Rebecca König
- **Comment**: v3 with appendix
- **Journal**: None
- **Summary**: State-of-the-art instance-aware semantic segmentation algorithms use axis-aligned bounding boxes as an intermediate processing step to infer the final instance mask output. This often leads to coarse and inaccurate mask proposals due to the following reasons: Axis-aligned boxes have a high background to foreground pixel-ratio, there is a strong variation of mask targets with respect to the underlying box, and neighboring instances frequently reach into the axis-aligned bounding box of the instance mask of interest. In this work, we overcome these problems by proposing to use oriented boxes as the basis to infer instance masks. We show that oriented instance segmentation improves the mask predictions, especially when objects are diagonally aligned, touching, or overlapping each other. We evaluate our model on the D2S and Screws datasets and show that we can significantly improve the mask accuracy by 10% and 12% mAP compared to instance segmentation using axis-aligned bounding boxes, respectively. On the newly introduced Pill Bags dataset we outperform the baseline using only 10% of the mask annotations.



### Modeling Gestalt Visual Reasoning on the Raven's Progressive Matrices Intelligence Test Using Generative Image Inpainting Techniques
- **Arxiv ID**: http://arxiv.org/abs/1911.07736v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.07736v2)
- **Published**: 2019-11-18 16:16:55+00:00
- **Updated**: 2019-11-26 08:32:20+00:00
- **Authors**: Tianyu Hua, Maithilee Kunda
- **Comment**: None
- **Journal**: None
- **Summary**: Psychologists recognize Raven's Progressive Matrices as a very effective test of general human intelligence. While many computational models have been developed by the AI community to investigate different forms of top-down, deliberative reasoning on the test, there has been less research on bottom-up perceptual processes, like Gestalt image completion, that are also critical in human test performance. In this work, we investigate how Gestalt visual reasoning on the Raven's test can be modeled using generative image inpainting techniques from computer vision. We demonstrate that a self-supervised inpainting model trained only on photorealistic images of objects achieves a score of 27/36 on the Colored Progressive Matrices, which corresponds to average performance for nine-year-old children. We also show that models trained on other datasets (faces, places, and textures) do not perform as well. Our results illustrate how learning visual regularities in real-world images can translate into successful reasoning about artificial test stimuli. On the flip side, our results also highlight the limitations of such transfer, which may explain why intelligence tests like the Raven's are often sensitive to people's individual sociocultural backgrounds.



### Satellite Image Time Series Classification with Pixel-Set Encoders and Temporal Self-Attention
- **Arxiv ID**: http://arxiv.org/abs/1911.07757v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07757v1)
- **Published**: 2019-11-18 16:39:06+00:00
- **Updated**: 2019-11-18 16:39:06+00:00
- **Authors**: Vivien Sainte Fare Garnot, Loic Landrieu, Sebastien Giordano, Nesrine Chehata
- **Comment**: None
- **Journal**: None
- **Summary**: Satellite image time series, bolstered by their growing availability, are at the forefront of an extensive effort towards automated Earth monitoring by international institutions. In particular, large-scale control of agricultural parcels is an issue of major political and economic importance. In this regard, hybrid convolutional-recurrent neural architectures have shown promising results for the automated classification of satellite image time series.We propose an alternative approach in which the convolutional layers are advantageously replaced with encoders operating on unordered sets of pixels to exploit the typically coarse resolution of publicly available satellite images. We also propose to extract temporal features using a bespoke neural architecture based on self-attention instead of recurrent networks. We demonstrate experimentally that our method not only outperforms previous state-of-the-art approaches in terms of precision, but also significantly decreases processing time and memory requirements. Lastly, we release a large open-access annotated dataset as a benchmark for future work on satellite image time series.



### Frequency Separation for Real-World Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1911.07850v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.07850v1)
- **Published**: 2019-11-18 17:08:28+00:00
- **Updated**: 2019-11-18 17:08:28+00:00
- **Authors**: Manuel Fritsche, Shuhang Gu, Radu Timofte
- **Comment**: winner of AIM 2019 Real World Super-Resolution challenge, paper
  published in ICCV 2019 Workshops
- **Journal**: None
- **Summary**: Most of the recent literature on image super-resolution (SR) assumes the availability of training data in the form of paired low resolution (LR) and high resolution (HR) images or the knowledge of the downgrading operator (usually bicubic downscaling). While the proposed methods perform well on standard benchmarks, they often fail to produce convincing results in real-world settings. This is because real-world images can be subject to corruptions such as sensor noise, which are severely altered by bicubic downscaling. Therefore, the models never see a real-world image during training, which limits their generalization capabilities. Moreover, it is cumbersome to collect paired LR and HR images in the same source domain.   To address this problem, we propose DSGAN to introduce natural image characteristics in bicubically downscaled images. It can be trained in an unsupervised fashion on HR images, thereby generating LR images with the same characteristics as the original images. We then use the generated data to train a SR model, which greatly improves its performance on real-world images. Furthermore, we propose to separate the low and high image frequencies and treat them differently during training. Since the low frequencies are preserved by downsampling operations, we only require adversarial training to modify the high frequencies. This idea is applied to our DSGAN model as well as the SR model. We demonstrate the effectiveness of our method in several experiments through quantitative and qualitative analysis. Our solution is the winner of the AIM Challenge on Real World SR at ICCV 2019.



### MaskedFusion: Mask-based 6D Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1911.07771v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07771v2)
- **Published**: 2019-11-18 17:09:19+00:00
- **Updated**: 2020-03-18 14:10:00+00:00
- **Authors**: Nuno Pereira, Luís A. Alexandre
- **Comment**: None
- **Journal**: None
- **Summary**: MaskedFusion is a framework to estimate the 6D pose of objects using RGB-D data, with an architecture that leverages multiple sub-tasks in a pipeline to achieve accurate 6D poses. 6D pose estimation is an open challenge due to complex world objects and many possible problems when capturing data from the real world, e.g., occlusions, truncations, and noise in the data. Achieving accurate 6D poses will improve results in other open problems like robot grasping or positioning objects in augmented reality. MaskedFusion improves the state-of-the-art by using object masks to eliminate non-relevant data. With the inclusion of the masks on the neural network that estimates the 6D pose of an object we also have features that represent the object shape. MaskedFusion is a modular pipeline where each sub-task can have different methods that achieve the objective. MaskedFusion achieved 97.3% on average using the ADD metric on the LineMOD dataset and 93.3% using the ADD-S AUC metric on YCB-Video Dataset, which is an improvement, compared to the state-of-the-art methods. The code is available on GitHub (https://github.com/kroglice/MaskedFusion).



### DeepPFCN: Deep Parallel Feature Consensus Network For Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1911.07776v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07776v1)
- **Published**: 2019-11-18 17:21:35+00:00
- **Updated**: 2019-11-18 17:21:35+00:00
- **Authors**: Shubham Kumar Singh, Krishna P Miyapuram, Shanmuganathan Raman
- **Comment**: 8 pages, 3 Figures, 3 Tables
- **Journal**: None
- **Summary**: Person re-identification aims to associate images of the same person over multiple non-overlapping camera views at different times. Depending on the human operator, manual re-identification in large camera networks is highly time consuming and erroneous. Automated person re-identification is required due to the extensive quantity of visual data produced by rapid inflation of large scale distributed multi-camera systems. The state-of-the-art works focus on learning and factorize person appearance features into latent discriminative factors at multiple semantic levels. We propose Deep Parallel Feature Consensus Network (DeepPFCN), a novel network architecture that learns multi-scale person appearance features using convolutional neural networks. This model factorizes the visual appearance of a person into latent discriminative factors at multiple semantic levels. Finally consensus is built. The feature representations learned by DeepPFCN are more robust for the person re-identification task, as we learn discriminative scale-specific features and maximize multi-scale feature fusion selections in multi-scale image inputs. We further exploit average and max pooling in separate scale for person-specific task to discriminate features globally and locally. We demonstrate the re-identification advantages of the proposed DeepPFCN model over the state-of-the-art re-identification methods on three benchmark datasets: Market1501, DukeMTMCreID, and CUHK03. We have achieved mAP results of 75.8%, 64.3%, and 52.6% respectively on these benchmark datasets.



### AIM 2019 Challenge on Real-World Image Super-Resolution: Methods and Results
- **Arxiv ID**: http://arxiv.org/abs/1911.07783v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07783v2)
- **Published**: 2019-11-18 17:31:02+00:00
- **Updated**: 2019-11-19 09:38:19+00:00
- **Authors**: Andreas Lugmayr, Martin Danelljan, Radu Timofte, Manuel Fritsche, Shuhang Gu, Kuldeep Purohit, Praveen Kandula, Maitreya Suin, A N Rajagopalan, Nam Hyung Joon, Yu Seung Won, Guisik Kim, Dokyeong Kwon, Chih-Chung Hsu, Chia-Hsiang Lin, Yuanfei Huang, Xiaopeng Sun, Wen Lu, Jie Li, Xinbo Gao, Sefi Bell-Kligler
- **Comment**: None
- **Journal**: None
- **Summary**: This paper reviews the AIM 2019 challenge on real world super-resolution. It focuses on the participating methods and final results. The challenge addresses the real world setting, where paired true high and low-resolution images are unavailable. For training, only one set of source input images is therefore provided in the challenge. In Track 1: Source Domain the aim is to super-resolve such images while preserving the low level image characteristics of the source input domain. In Track 2: Target Domain a set of high-quality images is also provided for training, that defines the output domain and desired quality of the super-resolved images. To allow for quantitative evaluation, the source input images in both tracks are constructed using artificial, but realistic, image degradations. The challenge is the first of its kind, aiming to advance the state-of-the-art and provide a standard benchmark for this newly emerging task. In total 7 teams competed in the final testing phase, demonstrating new and innovative solutions to the problem.



### Action Anticipation with RBF Kernelized Feature Mapping RNN
- **Arxiv ID**: http://arxiv.org/abs/1911.07806v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07806v3)
- **Published**: 2019-11-18 18:13:56+00:00
- **Updated**: 2021-07-11 16:07:38+00:00
- **Authors**: Yuge Shi, Basura Fernando, Richard Hartley
- **Comment**: Accepted for publication in ECCV2018
- **Journal**: None
- **Summary**: We introduce a novel Recurrent Neural Network-based algorithm for future video feature generation and action anticipation called feature mapping RNN. Our novel RNN architecture builds upon three effective principles of machine learning, namely parameter sharing, Radial Basis Function kernels and adversarial training. Using only some of the earliest frames of a video, the feature mapping RNN is able to generate future features with a fraction of the parameters needed in traditional RNN. By feeding these future features into a simple multi-layer perceptron facilitated with an RBF kernel layer, we are able to accurately predict the action in the video. In our experiments, we obtain 18% improvement on JHMDB-21 dataset, 6% on UCF101-24 and 13% improvement on UT-Interaction datasets over prior state-of-the-art for action anticipation.



### Unsupervised Representation Learning by Discovering Reliable Image Relations
- **Arxiv ID**: http://arxiv.org/abs/1911.07808v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07808v1)
- **Published**: 2019-11-18 18:15:28+00:00
- **Updated**: 2019-11-18 18:15:28+00:00
- **Authors**: Timo Milbich, Omair Ghori, Ferran Diego, Björn Ommer
- **Comment**: Accepted for Publication in 'Pattern Recognition Journal'
- **Journal**: None
- **Summary**: Learning robust representations that allow to reliably establish relations between images is of paramount importance for virtually all of computer vision. Annotating the quadratic number of pairwise relations between training images is simply not feasible, while unsupervised inference is prone to noise, thus leaving the vast majority of these relations to be unreliable. To nevertheless find those relations which can be reliably utilized for learning, we follow a divide-and-conquer strategy: We find reliable similarities by extracting compact groups of images and reliable dissimilarities by partitioning these groups into subsets, converting the complicated overall problem into few reliable local subproblems. For each of the subsets we obtain a representation by learning a mapping to a target feature space so that their reliable relations are kept. Transitivity relations between the subsets are then exploited to consolidate the local solutions into a concerted global representation. While iterating between grouping, partitioning, and learning, we can successively use more and more reliable relations which, in turn, improves our image representation. In experiments, our approach shows state-of-the-art performance on unsupervised classification on ImageNet with 46.0% and competes favorably on different transfer learning tasks on PASCAL VOC.



### Skin Lesion Classification Using Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1911.07817v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.07817v1)
- **Published**: 2019-11-18 18:29:41+00:00
- **Updated**: 2019-11-18 18:29:41+00:00
- **Authors**: Alla Eddine Guissous
- **Comment**: 7 pages, 3 figures, accepted paper in 13th International Conference
  on Interactive Mobile and Communication Technologies and Learning 2019 "IMCL
  Conference"
- **Journal**: None
- **Summary**: This paper reports the methods and techniques we have developed for classify dermoscopic images (task 1) of the ISIC 2019 challenge dataset for skin lesion classification, our approach aims to use ensemble deep neural network with some powerful techniques to deal with unbalance data sets as its the main problem for this challenge in a move to increase the performance of CNNs model.



### Vision-Language Navigation with Self-Supervised Auxiliary Reasoning Tasks
- **Arxiv ID**: http://arxiv.org/abs/1911.07883v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07883v4)
- **Published**: 2019-11-18 19:17:57+00:00
- **Updated**: 2020-04-01 04:24:49+00:00
- **Authors**: Fengda Zhu, Yi Zhu, Xiaojun Chang, Xiaodan Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Vision-Language Navigation (VLN) is a task where agents learn to navigate following natural language instructions. The key to this task is to perceive both the visual scene and natural language sequentially. Conventional approaches exploit the vision and language features in cross-modal grounding. However, the VLN task remains challenging, since previous works have neglected the rich semantic information contained in the environment (such as implicit navigation graphs or sub-trajectory semantics). In this paper, we introduce Auxiliary Reasoning Navigation (AuxRN), a framework with four self-supervised auxiliary reasoning tasks to take advantage of the additional training signals derived from the semantic information. The auxiliary tasks have four reasoning objectives: explaining the previous actions, estimating the navigation progress, predicting the next orientation, and evaluating the trajectory consistency. As a result, these additional training signals help the agent to acquire knowledge of semantic representations in order to reason about its activity and build a thorough perception of the environment. Our experiments indicate that auxiliary reasoning tasks improve both the performance of the main task and the model generalizability by a large margin. Empirically, we demonstrate that an agent trained with self-supervised auxiliary reasoning tasks substantially outperforms the previous state-of-the-art method, being the best existing approach on the standard benchmark.



### A Deep Learning Approach for Robust Corridor Following
- **Arxiv ID**: http://arxiv.org/abs/1911.07896v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.07896v1)
- **Published**: 2019-11-18 19:46:54+00:00
- **Updated**: 2019-11-18 19:46:54+00:00
- **Authors**: Vishnu Sashank Dorbala, A. H. Abdul Hafez, C. V. Jawahar
- **Comment**: 7 pages, 7 figures. Paper published at 2019 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS)
- **Journal**: None
- **Summary**: For an autonomous corridor following task where the environment is continuously changing, several forms of environmental noise prevent an automated feature extraction procedure from performing reliably. Moreover, in cases where pre-defined features are absent from the captured data, a well defined control signal for performing the servoing task fails to get produced. In order to overcome these drawbacks, we present in this work, using a convolutional neural network (CNN) to directly estimate the required control signal from an image, encompassing feature extraction and control law computation into one single end-to-end framework. In particular, we study the task of autonomous corridor following using a CNN and present clear advantages in cases where a traditional method used for performing the same task fails to give a reliable outcome. We evaluate the performance of our method on this task on a Wheelchair Platform developed at our institute for this purpose.



### Dont Even Look Once: Synthesizing Features for Zero-Shot Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.07933v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07933v3)
- **Published**: 2019-11-18 20:38:04+00:00
- **Updated**: 2020-04-10 15:34:04+00:00
- **Authors**: Pengkai Zhu, Hanxiao Wang, Venkatesh Saligrama
- **Comment**: Accepted at CVPR 2020. 10 pages, 4 figures, 3 tables
- **Journal**: None
- **Summary**: Zero-shot detection, namely, localizing both seen and unseen objects, increasingly gains importance for large-scale applications, with large number of object classes, since, collecting sufficient annotated data with ground truth bounding boxes is simply not scalable. While vanilla deep neural networks deliver high performance for objects available during training, unseen object detection degrades significantly. At a fundamental level, while vanilla detectors are capable of proposing bounding boxes, which include unseen objects, they are often incapable of assigning high-confidence to unseen objects, due to the inherent precision/recall tradeoffs that requires rejecting background objects. We propose a novel detection algorithm Dont Even Look Once (DELO), that synthesizes visual features for unseen objects and augments existing training algorithms to incorporate unseen object detection. Our proposed scheme is evaluated on Pascal VOC and MSCOCO, and we demonstrate significant improvements in test accuracy over vanilla and other state-of-art zero-shot detectors



### ISP4ML: Understanding the Role of Image Signal Processing in Efficient Deep Learning Vision Systems
- **Arxiv ID**: http://arxiv.org/abs/1911.07954v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.07954v4)
- **Published**: 2019-11-18 21:05:44+00:00
- **Updated**: 2021-03-17 15:15:35+00:00
- **Authors**: Patrick Hansen, Alexey Vilkin, Yury Khrustalev, James Imber, David Hanwell, Matthew Mattina, Paul N. Whatmough
- **Comment**: 13 pages, 11 figures
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) are now predominant components in a variety of computer vision (CV) systems. These systems typically include an image signal processor (ISP), even though the ISP is traditionally designed to produce images that look appealing to humans. In CV systems, it is not clear what the role of the ISP is, or if it is even required at all for accurate prediction. In this work, we investigate the efficacy of the ISP in CNN classification tasks, and outline the system-level trade-offs between prediction accuracy and computational cost. To do so, we build software models of a configurable ISP and an imaging sensor in order to train CNNs on ImageNet with a range of different ISP settings and functionality. Results on ImageNet show that an ISP improves accuracy by 4.6%-12.2% on MobileNet architectures of different widths. Results using ResNets demonstrate that these trends also generalize to deeper networks. An ablation study of the various processing stages in a typical ISP reveals that the tone mapper is the most significant stage when operating on high dynamic range (HDR) images, by providing 5.8% average accuracy improvement alone. Overall, the ISP benefits system efficiency because the memory and computational costs of the ISP is minimal compared to the cost of using a larger CNN to achieve the same accuracy.



### Implicit Regularization and Convergence for Weight Normalization
- **Arxiv ID**: http://arxiv.org/abs/1911.07956v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.07956v5)
- **Published**: 2019-11-18 21:10:21+00:00
- **Updated**: 2022-08-30 06:17:01+00:00
- **Authors**: Xiaoxia Wu, Edgar Dobriban, Tongzheng Ren, Shanshan Wu, Zhiyuan Li, Suriya Gunasekar, Rachel Ward, Qiang Liu
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Normalization methods such as batch [Ioffe and Szegedy, 2015], weight [Salimansand Kingma, 2016], instance [Ulyanov et al., 2016], and layer normalization [Baet al., 2016] have been widely used in modern machine learning. Here, we study the weight normalization (WN) method [Salimans and Kingma, 2016] and a variant called reparametrized projected gradient descent (rPGD) for overparametrized least-squares regression. WN and rPGD reparametrize the weights with a scale g and a unit vector w and thus the objective function becomes non-convex. We show that this non-convex formulation has beneficial regularization effects compared to gradient descent on the original objective. These methods adaptively regularize the weights and converge close to the minimum l2 norm solution, even for initializations far from zero. For certain stepsizes of g and w , we show that they can converge close to the minimum norm solution. This is different from the behavior of gradient descent, which converges to the minimum norm solution only when started at a point in the range space of the feature matrix, and is thus more sensitive to initialization.



### TracKlinic: Diagnosis of Challenge Factors in Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1911.07959v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07959v2)
- **Published**: 2019-11-18 21:15:25+00:00
- **Updated**: 2019-11-25 20:28:34+00:00
- **Authors**: Heng Fan, Fan Yang, Peng Chu, Lin Yuan, Haibin Ling
- **Comment**: Tech. Report
- **Journal**: None
- **Summary**: Generic visual tracking is difficult due to many challenge factors (e.g., occlusion, blur, etc.). Each of these factors may cause serious problems for a tracking algorithm, and when they work together can make things even more complicated. Despite a great amount of efforts devoted to understanding the behavior of tracking algorithms, reliable and quantifiable ways for studying the per factor tracking behavior remain barely available. Addressing this issue, in this paper we contribute to the community a tracking diagnosis toolkit, TracKlinic, for diagnosis of challenge factors of tracking algorithms.   TracKlinic consists of two novel components focusing on the data and analysis aspects, respectively. For the data component, we carefully prepare a set of 2,390 annotated videos, each involving one and only one major challenge factor. When analyzing an algorithm for a specific challenge factor, such one-factor-per-sequence rule greatly inhibits the disturbance from other factors and consequently leads to more faithful analysis. For the analysis component, given the tracking results on all sequences, it investigates the behavior of the tracker under each individual factor and generates the report automatically. With TracKlinic, a thorough study is conducted on ten state-of-the-art trackers on nine challenge factors (including two compound ones). The results suggest that, heavy shape variation and occlusion are the two most challenging factors faced by most trackers. Besides, out-of-view, though does not happen frequently, is often fatal. By sharing TracKlinic, we expect to make it much easier for diagnosing tracking algorithms, and to thus facilitate developing better ones.



### Improving the Robustness of Capsule Networks to Image Affine Transformations
- **Arxiv ID**: http://arxiv.org/abs/1911.07968v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07968v3)
- **Published**: 2019-11-18 21:43:17+00:00
- **Updated**: 2020-03-31 08:03:12+00:00
- **Authors**: Jindong Gu, Volker Tresp
- **Comment**: None
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  2020
- **Summary**: Convolutional neural networks (CNNs) achieve translational invariance by using pooling operations. However, the operations do not preserve the spatial relationships in the learned representations. Hence, CNNs cannot extrapolate to various geometric transformations of inputs. Recently, Capsule Networks (CapsNets) have been proposed to tackle this problem. In CapsNets, each entity is represented by a vector and routed to high-level entity representations by a dynamic routing algorithm. CapsNets have been shown to be more robust than CNNs to affine transformations of inputs. However, there is still a huge gap between their performance on transformed inputs compared to untransformed versions. In this work, we first revisit the routing procedure by (un)rolling its forward and backward passes. Our investigation reveals that the routing procedure contributes neither to the generalization ability nor to the affine robustness of the CapsNets. Furthermore, we explore the limitations of capsule transformations and propose affine CapsNets (Aff-CapsNets), which are more robust to affine transformations. On our benchmark task, where models are trained on the MNIST dataset and tested on the AffNIST dataset, our Aff-CapsNets improve the benchmark performance by a large margin (from 79% to 93.21%), without using any routing mechanism.



### Simultaneous Mapping and Target Driven Navigation
- **Arxiv ID**: http://arxiv.org/abs/1911.07980v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1911.07980v1)
- **Published**: 2019-11-18 22:11:03+00:00
- **Updated**: 2019-11-18 22:11:03+00:00
- **Authors**: Georgios Georgakis, Yimeng Li, Jana Kosecka
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents a modular architecture for simultaneous mapping and target driven navigation in indoors environments. The semantic and appearance stored in 2.5D map is distilled from RGB images, semantic segmentation and outputs of object detectors by convolutional neural networks. Given this representation, the mapping module learns to localize the agent and register consecutive observations in the map. The navigation task is then formulated as a problem of learning a policy for reaching semantic targets using current observations and the up-to-date map. We demonstrate that the use of semantic information improves localization accuracy and the ability of storing spatial semantic map aids the target driven navigation policy. The two modules are evaluated separately and jointly on Active Vision Dataset and Matterport3D environments, demonstrating improved performance on both localization and navigation tasks.



### Unsupervised Domain Adaptation via Structured Prediction Based Selective Pseudo-Labeling
- **Arxiv ID**: http://arxiv.org/abs/1911.07982v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.MM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.07982v1)
- **Published**: 2019-11-18 22:21:47+00:00
- **Updated**: 2019-11-18 22:21:47+00:00
- **Authors**: Qian Wang, Toby P. Breckon
- **Comment**: Accepted to AAAI 2020
- **Journal**: None
- **Summary**: Unsupervised domain adaptation aims to address the problem of classifying unlabeled samples from the target domain whilst labeled samples are only available from the source domain and the data distributions are different in these two domains. As a result, classifiers trained from labeled samples in the source domain suffer from significant performance drop when directly applied to the samples from the target domain. To address this issue, different approaches have been proposed to learn domain-invariant features or domain-specific classifiers. In either case, the lack of labeled samples in the target domain can be an issue which is usually overcome by pseudo-labeling. Inaccurate pseudo-labeling, however, could result in catastrophic error accumulation during learning. In this paper, we propose a novel selective pseudo-labeling strategy based on structured prediction. The idea of structured prediction is inspired by the fact that samples in the target domain are well clustered within the deep feature space so that unsupervised clustering analysis can be used to facilitate accurate pseudo-labeling. Experimental results on four datasets (i.e. Office-Caltech, Office31, ImageCLEF-DA and Office-Home) validate our approach outperforms contemporary state-of-the-art methods.



### Learning Permutation Invariant Representations using Memory Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.07984v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.07984v2)
- **Published**: 2019-11-18 22:28:30+00:00
- **Updated**: 2020-07-03 16:27:23+00:00
- **Authors**: Shivam Kalra, Mohammed Adnan, Graham Taylor, Hamid Tizhoosh
- **Comment**: Accepted at ECCV 2020
- **Journal**: None
- **Summary**: Many real-world tasks such as classification of digital histopathology images and 3D object detection involve learning from a set of instances. In these cases, only a group of instances or a set, collectively, contains meaningful information and therefore only the sets have labels, and not individual data instances. In this work, we present a permutation invariant neural network called Memory-based Exchangeable Model (MEM) for learning set functions. The MEM model consists of memory units that embed an input sequence to high-level features enabling the model to learn inter-dependencies among instances through a self-attention mechanism. We evaluated the learning ability of MEM on various toy datasets, point cloud classification, and classification of lung whole slide images (WSIs) into two subtypes of lung cancer---Lung Adenocarcinoma, and Lung Squamous Cell Carcinoma. We systematically extracted patches from lung WSIs downloaded from The Cancer Genome Atlas~(TCGA) dataset, the largest public repository of WSIs, achieving a competitive accuracy of 84.84\% for classification of two sub-types of lung cancer. The results on other datasets are promising as well, and demonstrate the efficacy of our model.



### WITCHcraft: Efficient PGD attacks with random step size
- **Arxiv ID**: http://arxiv.org/abs/1911.07989v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.07989v1)
- **Published**: 2019-11-18 22:40:08+00:00
- **Updated**: 2019-11-18 22:40:08+00:00
- **Authors**: Ping-Yeh Chiang, Jonas Geiping, Micah Goldblum, Tom Goldstein, Renkun Ni, Steven Reich, Ali Shafahi
- **Comment**: Authors contributed equally and are listed in alphabetical order
- **Journal**: None
- **Summary**: State-of-the-art adversarial attacks on neural networks use expensive iterative methods and numerous random restarts from different initial points. Iterative FGSM-based methods without restarts trade off performance for computational efficiency because they do not adequately explore the image space and are highly sensitive to the choice of step size. We propose a variant of Projected Gradient Descent (PGD) that uses a random step size to improve performance without resorting to expensive random restarts. Our method, Wide Iterative Stochastic crafting (WITCHcraft), achieves results superior to the classical PGD attack on the CIFAR-10 and MNIST data sets but without additional computational cost. This simple modification of PGD makes crafting attacks more economical, which is important in situations like adversarial training where attacks need to be crafted in real time.



### Crowd Counting via Segmentation Guided Attention Networks and Curriculum Loss
- **Arxiv ID**: http://arxiv.org/abs/1911.07990v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1911.07990v2)
- **Published**: 2019-11-18 22:40:13+00:00
- **Updated**: 2020-08-03 21:06:44+00:00
- **Authors**: Qian Wang, Toby P. Breckon
- **Comment**: Technical Report, Durham University
- **Journal**: None
- **Summary**: Automatic crowd behaviour analysis is an important task for intelligent transportation systems to enable effective flow control and dynamic route planning for varying road participants. Crowd counting is one of the keys to automatic crowd behaviour analysis. Crowd counting using deep convolutional neural networks (CNN) has achieved encouraging progress in recent years. Researchers have devoted much effort to the design of variant CNN architectures and most of them are based on the pre-trained VGG16 model. Due to the insufficient expressive capacity, the backbone network of VGG16 is usually followed by another cumbersome network specially designed for good counting performance. Although VGG models have been outperformed by Inception models in image classification tasks, the existing crowd counting networks built with Inception modules still only have a small number of layers with basic types of Inception modules. To fill in this gap, in this paper, we firstly benchmark the baseline Inception-v3 model on commonly used crowd counting datasets and achieve surprisingly good performance comparable with or better than most existing crowd counting models. Subsequently, we push the boundary of this disruptive work further by proposing a Segmentation Guided Attention Network (SGANet) with Inception-v3 as the backbone and a novel curriculum loss for crowd counting. We conduct thorough experiments to compare the performance of our SGANet with prior arts and the proposed model can achieve state-of-the-art performance with MAE of 57.6, 6.3 and 87.6 on ShanghaiTechA, ShanghaiTechB and UCF\_QNRF, respectively.



### CD2 : Combined Distances of Contrast Distributions for the Assessment of Perceptual Quality of Image Processing
- **Arxiv ID**: http://arxiv.org/abs/1911.07995v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.07995v2)
- **Published**: 2019-11-18 22:58:09+00:00
- **Updated**: 2021-02-09 14:06:17+00:00
- **Authors**: Sascha Xu, Jan Bauer, Benjamin Axmann
- **Comment**: None
- **Journal**: None
- **Summary**: The quality of visual input is very important for both human and machine perception. Consequently many processing techniques exist that deal with different distortions. Usually image processing is applied freely and lacks redundancy regarding safety. We propose a novel image comparison method called the Combined Distances of Contrast Distributions (CD2) to protect against errors that arise during processing. Based on the distribution of image contrasts a new reduced-reference image quality assessment (IQA) method is introduced. By combining various distance functions excellent performance on IQA benchmarks is achieved with only a small data and computation overhead.



### Streetify: Using Street View Imagery And Deep Learning For Urban Streets Development
- **Arxiv ID**: http://arxiv.org/abs/1911.08007v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08007v1)
- **Published**: 2019-11-18 23:45:29+00:00
- **Updated**: 2019-11-18 23:45:29+00:00
- **Authors**: Fahad Alhasoun, Marta Gonzalez
- **Comment**: Paper to appear at IEEE Big Data 2019
- **Journal**: None
- **Summary**: The classification of streets on road networks has been focused on the vehicular transportational features of streets such as arterials, major roads, minor roads and so forth based on their transportational use. City authorities on the other hand have been shifting to more urban inclusive planning of streets, encompassing the side use of a street combined with the transportational features of a street. In such classification schemes, streets are labeled for example as commercial throughway, residential neighborhood, park etc. This modern approach to urban planning has been adopted by major cities such as the city of San Francisco, the states of Florida and Pennsylvania among many others. Currently, the process of labeling streets according to their contexts is manual and hence is tedious and time consuming. In this paper, we propose an approach to collect and label imagery data then deploy advancements in computer vision towards modern urban planning. We collect and label street imagery then train deep convolutional neural networks (CNN) to perform the classification of street context. We show that CNN models can perform well achieving accuracies in the 81% to 87%, we then visualize samples from the embedding space of streets using the t-SNE method and apply class activation mapping methods to interpret the features in street imagery contributing to output classification from a model.



### Towards a complete 3D morphable model of the human head
- **Arxiv ID**: http://arxiv.org/abs/1911.08008v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08008v2)
- **Published**: 2019-11-18 23:58:34+00:00
- **Updated**: 2020-02-19 23:23:21+00:00
- **Authors**: Stylianos Ploumpis, Evangelos Ververas, Eimear O' Sullivan, Stylianos Moschoglou, Haoyang Wang, Nick Pears, William A. P. Smith, Baris Gecer, Stefanos Zafeiriou
- **Comment**: 18 pages, 18 figures, submitted to Transactions on Pattern Analysis
  and Machine Intelligence (TPAMI) on the 9th of October as an extension paper
  of the original oral CVPR paper : arXiv:1903.03785
- **Journal**: None
- **Summary**: Three-dimensional Morphable Models (3DMMs) are powerful statistical tools for representing the 3D shapes and textures of an object class. Here we present the most complete 3DMM of the human head to date that includes face, cranium, ears, eyes, teeth and tongue. To achieve this, we propose two methods for combining existing 3DMMs of different overlapping head parts: i. use a regressor to complete missing parts of one model using the other, ii. use the Gaussian Process framework to blend covariance matrices from multiple models. Thus we build a new combined face-and-head shape model that blends the variability and facial detail of an existing face model (the LSFM) with the full head modelling capability of an existing head model (the LYHM). Then we construct and fuse a highly-detailed ear model to extend the variation of the ear shape. Eye and eye region models are incorporated into the head model, along with basic models of the teeth, tongue and inner mouth cavity. The new model achieves state-of-the-art performance. We use our model to reconstruct full head representations from single, unconstrained images allowing us to parameterize craniofacial shape and texture, along with the ear shape, eye gaze and eye color.



