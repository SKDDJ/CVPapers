# Arxiv Papers in cs.CV on 2019-11-09
### CenterFace: Joint Face Detection and Alignment Using Face as Point
- **Arxiv ID**: http://arxiv.org/abs/1911.03599v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.03599v1)
- **Published**: 2019-11-09 03:06:11+00:00
- **Updated**: 2019-11-09 03:06:11+00:00
- **Authors**: Yuanyuan Xu, Wan Yan, Haixin Sun, Genke Yang, Jiliang Luo
- **Comment**: 11 pages, 3 figures. A demo of CenterFace can be available at
  https://github.com/Star-Clouds/CenterFace
- **Journal**: None
- **Summary**: Face detection and alignment in unconstrained environment is always deployed on edge devices which have limited memory storage and low computing power. This paper proposes a one-stage method named CenterFace to simultaneously predict facial box and landmark location with real-time speed and high accuracy. The proposed method also belongs to the anchor free category. This is achieved by: (a) learning face existing possibility by the semantic maps, (b) learning bounding box, offsets and five landmarks for each position that potentially contains a face. Specifically, the method can run in real-time on a single CPU core and 200 FPS using NVIDIA 2080TI for VGA-resolution images, and can simultaneously achieve superior accuracy (WIDER FACE Val/Test-Easy: 0.935/0.932, Medium: 0.924/0.921, Hard: 0.875/0.873 and FDDB discontinuous: 0.980, continuous: 0.732). A demo of CenterFace can be available at https://github.com/Star-Clouds/CenterFace.



### Dense 3D Reconstruction for Visual Tunnel Inspection using Unmanned Aerial Vehicle
- **Arxiv ID**: http://arxiv.org/abs/1911.03603v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.03603v1)
- **Published**: 2019-11-09 03:22:10+00:00
- **Updated**: 2019-11-09 03:22:10+00:00
- **Authors**: Ramanpreet Singh Pahwa, Kennard Yanting Chan, Jiamin Bai, Vincensius Billy Saputra, Minh N. Do, Shaohui Foong
- **Comment**: 8 pages, 12 figures
- **Journal**: IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS), 2019
- **Summary**: Advances in Unmanned Aerial Vehicle (UAV) opens venues for application such as tunnel inspection. Owing to its versatility to fly inside the tunnels, it can quickly identify defects and potential problems related to safety. However, long tunnels, especially with repetitive or uniform structures pose a significant problem for UAV navigation. Furthermore, post-processing visual data from the camera mounted on the UAV is required to generate useful information for the inspection task. In this work, we design a UAV with a single rotating camera to accomplish the task. Compared to other platforms, our solution can fit the stringent requirement for tunnel inspection, in terms of battery life, size and weight. While the current state-of-the-art can estimate camera pose and 3D geometry from a sequence of images, they assume large overlap, small rotational motion, and many distinct matching points between images. These assumptions severely limit their effectiveness in tunnel-like scenarios where the camera has erratic or large rotational motion, such as the one mounted on the UAV. This paper presents a novel solution which exploits Structure-from-Motion, Bundle Adjustment, and available geometry priors to robustly estimate camera pose and automatically reconstruct a fully-dense 3D scene using the least possible number of images in various challenging tunnel-like environments. We validate our system with both Virtual Reality application and experimentation with a real dataset. The results demonstrate that the proposed reconstruction along with texture mapping allows for remote navigation and inspection of tunnel-like environments, even those which are inaccessible for humans.



### FaultNet: Faulty Rail-Valves Detection using Deep Learning and Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1912.04219v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04219v1)
- **Published**: 2019-11-09 03:33:53+00:00
- **Updated**: 2019-11-09 03:33:53+00:00
- **Authors**: Ramanpreet Singh Pahwa, Jin Chao, Jestine Paul, Yiqun Li, Ma Tin Lay Nwe, Shudong Xie, Ashish James, Arulmurugan Ambikapathi, Zeng Zeng, Vijay Ramaseshan Chandrasekhar
- **Comment**: 8 pages, 8 figures, ITSC 2019
- **Journal**: IEEE INTELLIGENT TRANSPORTATION SYSTEMS CONFERENCE - ITSC 2019
- **Summary**: Regular inspection of rail valves and engines is an important task to ensure the safety and efficiency of railway networks around the globe. Over the past decade, computer vision and pattern recognition based techniques have gained traction for such inspection and defect detection tasks. An automated end-to-end trained system can potentially provide a low-cost, high throughput, and cheap alternative to manual visual inspection of these components. However, such systems require a huge amount of defective images for networks to understand complex defects. In this paper, a multi-phase deep learning based technique is proposed to perform accurate fault detection of rail-valves. Our approach uses a two-step method to perform high precision image segmentation of rail-valves resulting in pixel-wise accurate segmentation. Thereafter, a computer vision technique is used to identify faulty valves. We demonstrate that the proposed approach results in improved detection performance when compared to current state-of-theart techniques used in fault detection.



### DeepMask: an algorithm for cloud and cloud shadow detection in optical satellite remote sensing images using deep residual network
- **Arxiv ID**: http://arxiv.org/abs/1911.03607v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.03607v1)
- **Published**: 2019-11-09 03:44:07+00:00
- **Updated**: 2019-11-09 03:44:07+00:00
- **Authors**: Ke Xu, Kaiyu Guan, Jian Peng, Yunan Luo, Sibo Wang
- **Comment**: 17 pages, 4 figures, 6 tables
- **Journal**: None
- **Summary**: Detecting and masking cloud and cloud shadow from satellite remote sensing images is a pervasive problem in the remote sensing community. Accurate and efficient detection of cloud and cloud shadow is an essential step to harness the value of remotely sensed data for almost all downstream analysis. DeepMask, a new algorithm for cloud and cloud shadow detection in optical satellite remote sensing imagery, is proposed in this study. DeepMask utilizes ResNet, a deep convolutional neural network, for pixel-level cloud mask generation. The algorithm is trained and evaluated on the Landsat 8 Cloud Cover Assessment Validation Dataset distributed across 8 different land types. Compared with CFMask, the most widely used cloud detection algorithm, land-type-specific DeepMask models achieve higher accuracy across all land types. The average accuracy is 93.56%, compared with 85.36% from CFMask. DeepMask also achieves 91.02% accuracy on all-land-type dataset. Compared with other CNN-based cloud mask algorithms, DeepMask benefits from the parsimonious architecture and the residual connection of ResNet. It is compatible with input of any size and shape. DeepMask still maintains high performance when using only red, green, blue, and NIR bands, indicating its potential to be applied to other satellite platforms that only have limited optical bands.



### Learning Deep Bilinear Transformation for Fine-grained Image Representation
- **Arxiv ID**: http://arxiv.org/abs/1911.03621v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.03621v1)
- **Published**: 2019-11-09 06:33:54+00:00
- **Updated**: 2019-11-09 06:33:54+00:00
- **Authors**: Heliang Zheng, Jianlong Fu, Zheng-Jun Zha, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Bilinear feature transformation has shown the state-of-the-art performance in learning fine-grained image representations. However, the computational cost to learn pairwise interactions between deep feature channels is prohibitively expensive, which restricts this powerful transformation to be used in deep neural networks. In this paper, we propose a deep bilinear transformation (DBT) block, which can be deeply stacked in convolutional neural networks to learn fine-grained image representations. The DBT block can uniformly divide input channels into several semantic groups. As bilinear transformation can be represented by calculating pairwise interactions within each group, the computational cost can be heavily relieved. The output of each block is further obtained by aggregating intra-group bilinear features, with residuals from the entire input features. We found that the proposed network achieves new state-of-the-art in several fine-grained image recognition benchmarks, including CUB-Bird, Stanford-Car, and FGVC-Aircraft.



### Natural and Realistic Single Image Super-Resolution with Explicit Natural Manifold Discrimination
- **Arxiv ID**: http://arxiv.org/abs/1911.03624v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.03624v1)
- **Published**: 2019-11-09 06:48:53+00:00
- **Updated**: 2019-11-09 06:48:53+00:00
- **Authors**: Jae Woong Soh, Gu Yong Park, Junho Jo, Nam Ik Cho
- **Comment**: Presented in CVPR 2019
- **Journal**: None
- **Summary**: Recently, many convolutional neural networks for single image super-resolution (SISR) have been proposed, which focus on reconstructing the high-resolution images in terms of objective distortion measures. However, the networks trained with objective loss functions generally fail to reconstruct the realistic fine textures and details that are essential for better perceptual quality. Recovering the realistic details remains a challenging problem, and only a few works have been proposed which aim at increasing the perceptual quality by generating enhanced textures. However, the generated fake details often make undesirable artifacts and the overall image looks somewhat unnatural. Therefore, in this paper, we present a new approach to reconstructing realistic super-resolved images with high perceptual quality, while maintaining the naturalness of the result. In particular, we focus on the domain prior properties of SISR problem. Specifically, we define the naturalness prior in the low-level domain and constrain the output image in the natural manifold, which eventually generates more natural and realistic images. Our results show better naturalness compared to the recent super-resolution algorithms including perception-oriented ones.



### Action Recognition Using Supervised Spiking Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.03630v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1911.03630v2)
- **Published**: 2019-11-09 07:16:10+00:00
- **Updated**: 2020-01-11 11:07:11+00:00
- **Authors**: Aref Moqadam Mehr, Saeed Reza Kheradpisheh, Hadi Farahani
- **Comment**: We found a bug in our implementations and we should admit that our
  reported results were wrong
- **Journal**: None
- **Summary**: Biological neurons use spikes to process and learn temporally dynamic inputs in an energy and computationally efficient way. However, applying the state-of-the-art gradient-based supervised algorithms to spiking neural networks (SNN) is a challenge due to the non-differentiability of the activation function of spiking neurons. Employing surrogate gradients is one of the main solutions to overcome this challenge. Although SNNs naturally work in the temporal domain, recent studies have focused on developing SNNs to solve static image categorization tasks. In this paper, we employ a surrogate gradient descent learning algorithm to recognize twelve human hand gestures recorded by dynamic vision sensor (DVS) cameras. The proposed SNN could reach 97.2% recognition accuracy on test data.



### Bootstrapping Disjoint Datasets for Multilingual Multimodal Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.03678v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.03678v1)
- **Published**: 2019-11-09 12:34:01+00:00
- **Updated**: 2019-11-09 12:34:01+00:00
- **Authors**: Ákos Kádár, Grzegorz Chrupała, Afra Alishahi, Desmond Elliott
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Recent work has highlighted the advantage of jointly learning grounded sentence representations from multiple languages. However, the data used in these studies has been limited to an aligned scenario: the same images annotated with sentences in multiple languages. We focus on the more realistic disjoint scenario in which there is no overlap between the images in multilingual image--caption datasets. We confirm that training with aligned data results in better grounded sentence representations than training with disjoint data, as measured by image--sentence retrieval performance. In order to close this gap in performance, we propose a pseudopairing method to generate synthetically aligned English--German--image triplets from the disjoint sets. The method works by first training a model on the disjoint data, and then creating new triples across datasets using sentence similarity under the learned model. Experiments show that pseudopairs improve image--sentence retrieval performance compared to disjoint training, despite requiring no external data or models. However, we do find that using an external machine translation model to generate the synthetic data sets results in better performance.



### Feature Extraction in Augmented Reality
- **Arxiv ID**: http://arxiv.org/abs/1911.09177v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.09177v1)
- **Published**: 2019-11-09 14:24:56+00:00
- **Updated**: 2019-11-09 14:24:56+00:00
- **Authors**: Jekishan K. Parmar, Ankit Desai
- **Comment**: None
- **Journal**: None
- **Summary**: Augmented Reality (AR) is used for various applications associated with the real world. In this paper, first, describe characteristics and essential services of AR. Brief history on Virtual Reality (VR) and AR is also mentioned in the introductory section. Then, AR Technologies along with its workflow is depicted, which includes the complete AR Process consisting of the stages of Image Acquisition, Feature Extraction, Feature Matching, Geometric Verification, and Associated Information Retrieval. Feature extraction is the essence of AR hence its details are furnished in the paper.



### CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1911.03705v4
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.03705v4)
- **Published**: 2019-11-09 14:53:59+00:00
- **Updated**: 2020-11-30 07:53:50+00:00
- **Authors**: Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, Xiang Ren
- **Comment**: Accepted to EMNLP 2020 Findings. Add one more human reference for
  each test example: Table 1,3 & Figure 4 & Section 3.3, 3.4 are updated.
  Project page: https://inklab.usc.edu/CommonGen/
- **Journal**: None
- **Summary**: Recently, large-scale pre-trained language models have demonstrated impressive performance on several commonsense-reasoning benchmark datasets. However, building machines with commonsense to compose realistically plausible sentences remains challenging. In this paper, we present a constrained text generation task, CommonGen associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts (e.g., {dog, frisbee, catch, throw}); the task is to generate a coherent sentence describing an everyday scenario using these concepts (e.g., "a man throws a frisbee and his dog catches it").   The CommonGen task is challenging because it inherently requires 1) relational reasoning with background commonsense knowledge, and 2) compositional generalization ability to work on unseen concept combinations. Our dataset, constructed through a combination of crowdsourced and existing caption corpora, consists of 79k commonsense descriptions over 35k unique concept-sets. Experiments show that there is a large gap between state-of-the-art text generation models (e.g., T5) and human performance. Furthermore, we demonstrate that the learned generative commonsense reasoning capability can be transferred to improve downstream tasks such as CommonsenseQA by generating additional context.



### Unsupervised adulterated red-chili pepper content transformation for hyperspectral classification
- **Arxiv ID**: http://arxiv.org/abs/1911.03711v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.03711v1)
- **Published**: 2019-11-09 15:12:27+00:00
- **Updated**: 2019-11-09 15:12:27+00:00
- **Authors**: Muhammad Hussain Khan, Zainab Saleem, Muhammad Ahmad, Ahmed Sohaib, Hamail Ayaz
- **Comment**: 10 pages,
- **Journal**: None
- **Summary**: Preserving red-chili quality is of utmost importance in which the authorities demand the quality techniques to detect, classify and prevent it from the impurities. For example, salt, wheat flour, wheat bran, and rice bran contamination in grounded red chili, which typically a food, are a serious threat to people who are allergic to such items. This work presents the feasibility of utilizing visible and near-infrared (VNIR) hyperspectral imaging (HSI) to detect and classify the aforementioned adulterants in red chili. However, adulterated red chili data annotation is a big challenge for classification because the acquisition of labeled data for real-time supervised learning is expensive in terms of cost and time. Therefore, this study, for the very first time proposes a novel approach to annotate the red chili samples using a clustering mechanism at 500~nm wavelength spectral response due to its dark appearance at a specified wavelength. Later the spectral samples are classified into pure or adulterated using one-class SVM. The classification performance achieves 99% in case of pure adulterants or red chili whereas 85% for adulterated samples. We further investigate that the single classification model is enough to detect any foreign substance in red chili pepper rather than cascading multiple PLS regression models.



### Deep learning for cardiac image segmentation: A review
- **Arxiv ID**: http://arxiv.org/abs/1911.03723v1
- **DOI**: 10.3389/fcvm.2020.00025
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1911.03723v1)
- **Published**: 2019-11-09 15:58:48+00:00
- **Updated**: 2019-11-09 15:58:48+00:00
- **Authors**: Chen Chen, Chen Qin, Huaqi Qiu, Giacomo Tarroni, Jinming Duan, Wenjia Bai, Daniel Rueckert
- **Comment**: Under review
- **Journal**: None
- **Summary**: Deep learning has become the most widely used approach for cardiac image segmentation in recent years. In this paper, we provide a review of over 100 cardiac image segmentation papers using deep learning, which covers common imaging modalities including magnetic resonance imaging (MRI), computed tomography (CT), and ultrasound (US) and major anatomical structures of interest (ventricles, atria and vessels). In addition, a summary of publicly available cardiac image datasets and code repositories are included to provide a base for encouraging reproducible research. Finally, we discuss the challenges and limitations with current deep learning-based approaches (scarcity of labels, model generalizability across different domains, interpretability) and suggest potential directions for future research.



### On the design of convolutional neural networks for automatic detection of Alzheimer's disease
- **Arxiv ID**: http://arxiv.org/abs/1911.03740v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.03740v3)
- **Published**: 2019-11-09 17:08:34+00:00
- **Updated**: 2020-04-05 01:47:59+00:00
- **Authors**: Sheng Liu, Chhavi Yadav, Carlos Fernandez-Granda, Narges Razavian
- **Comment**: Machine Learning for Health Workshop, NeurIPS2019. Authors
  Fernandez-Granda and Razavian are joint last authors
- **Journal**: Proceedings of Machine Learning Research, 2019
- **Summary**: Early detection is a crucial goal in the study of Alzheimer's Disease (AD). In this work, we describe several techniques to boost the performance of 3D deep convolutional neural networks (CNNs) trained to detect AD using structural brain MRI scans. Specifically, we provide evidence that (1) instance normalization outperforms batch normalization, (2) early spatial downsampling negatively affects performance, (3) widening the model brings consistent gains while increasing the depth does not, and (4) incorporating age information yields moderate improvement. Together, these insights yield an increment of approximately 14% in test accuracy over existing models when distinguishing between patients with AD, mild cognitive impairment, and controls in the ADNI dataset. Similar performance is achieved on an independent dataset.



### A Proposed Artificial intelligence Model for Real-Time Human Action Localization and Tracking
- **Arxiv ID**: http://arxiv.org/abs/1911.04469v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.04469v1)
- **Published**: 2019-11-09 19:59:17+00:00
- **Updated**: 2019-11-09 19:59:17+00:00
- **Authors**: Ahmed Ali Hammam, Mona Soliman, Aboul Ella Hassanien
- **Comment**: SUBMITTED TO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING
  SYSTEMS
- **Journal**: None
- **Summary**: In recent years, artificial intelligence (AI) based on deep learning (DL) has sparked tremendous global interest. DL is widely used today and has expanded into various interesting areas. It is becoming more popular in cross-subject research, such as studies of smart city systems, which combine computer science with engineering applications. Human action detection is one of these areas. Human action detection is an interesting challenge due to its stringent requirements in terms of computing speed and accuracy. High-accuracy real-time object tracking is also considered a significant challenge. This paper integrates the YOLO detection network, which is considered a state-of-the-art tool for real-time object detection, with motion vectors and the Coyote Optimization Algorithm (COA) to construct a real-time human action localization and tracking system. The proposed system starts with the extraction of motion information from a compressed video stream and the extraction of appearance information from RGB frames using an object detector. Then, a fusion step between the two streams is performed, and the results are fed into the proposed action tracking model. The COA is used in object tracking due to its accuracy and fast convergence. The basic foundation of the proposed model is the utilization of motion vectors, which already exist in a compressed video bit stream and provide sufficient information to improve the localization of the target action without requiring high consumption of computational resources compared with other popular methods of extracting motion information, such as optical flows. This advantage allows the proposed approach to be implemented in challenging environments where the computational resources are limited, such as Internet of Things (IoT) systems.



### Spatially Regularized Parametric Map Reconstruction for Fast Magnetic Resonance Fingerprinting
- **Arxiv ID**: http://arxiv.org/abs/1911.03786v2
- **DOI**: 10.1016/j.media.2020.101741
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.03786v2)
- **Published**: 2019-11-09 22:10:24+00:00
- **Updated**: 2020-08-10 15:00:29+00:00
- **Authors**: Fabian Balsiger, Alain Jungo, Olivier Scheidegger, Pierre G. Carlier, Mauricio Reyes, Benjamin Marty
- **Comment**: Accepted to Medical Image Analysis
- **Journal**: Medical Image Analysis (2020), 64, 101741
- **Summary**: Magnetic resonance fingerprinting (MRF) provides a unique concept for simultaneous and fast acquisition of multiple quantitative MR parameters. Despite acquisition efficiency, adoption of MRF into the clinics is hindered by its dictionary matching-based reconstruction, which is computationally demanding and lacks scalability. Here, we propose a convolutional neural network-based reconstruction, which enables both accurate and fast reconstruction of parametric maps, and is adaptable based on the needs of spatial regularization and the capacity for the reconstruction. We evaluated the method using MRF T1-FF, an MRF sequence for T1 relaxation time of water (T1H2O) and fat fraction (FF) mapping. We demonstrate the method's performance on a highly heterogeneous dataset consisting of 164 patients with various neuromuscular diseases imaged at thighs and legs. We empirically show the benefit of incorporating spatial regularization during the reconstruction and demonstrate that the method learns meaningful features from MR physics perspective. Further, we investigate the ability of the method to handle highly heterogeneous morphometric variations and its generalization to anatomical regions unseen during training. The obtained results outperform the state-of-the-art in deep learning-based MRF reconstruction. The method achieved normalized root mean squared errors of 0.048 $\pm$ 0.011 for T1H2O maps and 0.027 $\pm$ 0.004 for FF maps when compared to the dictionary matching in a test set of 50 patients. Coupled with fast MRF sequences, the proposed method has the potential of enabling multiparametric MR imaging in clinically feasible time.



### Exactly Sparse Gaussian Variational Inference with Application to Derivative-Free Batch Nonlinear State Estimation
- **Arxiv ID**: http://arxiv.org/abs/1911.08333v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.08333v2)
- **Published**: 2019-11-09 22:41:01+00:00
- **Updated**: 2020-04-09 20:18:27+00:00
- **Authors**: Timothy D. Barfoot, James R. Forbes, David Yoon
- **Comment**: Accepted to the International Journal of Robotics Research (IJRR) on
  8 April 2020, # IJR-19-3748; 31 pages, 10 figures
- **Journal**: None
- **Summary**: We present a Gaussian Variational Inference (GVI) technique that can be applied to large-scale nonlinear batch state estimation problems. The main contribution is to show how to fit both the mean and (inverse) covariance of a Gaussian to the posterior efficiently, by exploiting factorization of the joint likelihood of the state and data, as is common in practical problems. This is different than Maximum A Posteriori (MAP) estimation, which seeks the point estimate for the state that maximizes the posterior (i.e., the mode). The proposed Exactly Sparse Gaussian Variational Inference (ESGVI) technique stores the inverse covariance matrix, which is typically very sparse (e.g., block-tridiagonal for classic state estimation). We show that the only blocks of the (dense) covariance matrix that are required during the calculations correspond to the non-zero blocks of the inverse covariance matrix, and further show how to calculate these blocks efficiently in the general GVI problem. ESGVI operates iteratively, and while we can use analytical derivatives at each iteration, Gaussian cubature can be substituted, thereby producing an efficient derivative-free batch formulation. ESGVI simplifies to precisely the Rauch-Tung-Striebel (RTS) smoother in the batch linear estimation case, but goes beyond the 'extended' RTS smoother in the nonlinear case since it finds the best-fit Gaussian (mean and covariance), not the MAP point estimate. We demonstrate the technique on controlled simulation problems and a batch nonlinear Simultaneous Localization and Mapping (SLAM) problem with an experimental dataset.



