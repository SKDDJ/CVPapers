# Arxiv Papers in cs.CV on 2019-11-20
### Sibling Neural Estimators: Improving Iterative Image Decoding with Gradient Communication
- **Arxiv ID**: http://arxiv.org/abs/1911.08478v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.08478v1)
- **Published**: 2019-11-20 00:18:44+00:00
- **Updated**: 2019-11-20 00:18:44+00:00
- **Authors**: Ankur Mali, Alexander G. Ororbia, Clyde Lee Giles
- **Comment**: 11 Pages, 2 figures, 1 Table
- **Journal**: None
- **Summary**: For lossy image compression, we develop a neural-based system which learns a nonlinear estimator for decoding from quantized representations. The system links two recurrent networks that \help" each other reconstruct same target image patches using complementary portions of spatial context that communicate via gradient signals. This dual agent system builds upon prior work that proposed the iterative refinement algorithm for recurrent neural network (RNN)based decoding which improved image reconstruction compared to standard decoding techniques. Our approach, which works with any encoder, neural or non-neural, This system progressively reduces image patch reconstruction error over a fixed number of steps. Experiment with variants of RNN memory cells, with and without future information, find that our model consistently creates lower distortion images of higher perceptual quality compared to other approaches. Specifically, on the Kodak Lossless True Color Image Suite, we observe as much as a 1:64 decibel (dB) gain over JPEG, a 1:46 dB gain over JPEG 2000, a 1:34 dB gain over the GOOG neural baseline, 0:36 over E2E (a modern competitive neural compression model), and 0:37 over a single iterative neural decoder.



### Unified Multifaceted Feature Learning for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1911.08651v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08651v2)
- **Published**: 2019-11-20 00:56:16+00:00
- **Updated**: 2019-11-21 22:18:25+00:00
- **Authors**: Cheng Yan, Guansong Pang, Xiao Bai, Chunhua Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification (ReID) aims at re-identifying persons from different viewpoints across multiple cameras, of which it is of great importance to learn multifaceted features expressed in different parts of a person, e.g., clothes, bags, and other accessories in the main body, appearance in the head, and shoes in the foot. To learn such features, existing methods are focused on the striping-based approach that builds multi-branch neural networks to learn local features in each part of the identities, with one-branch network dedicated to one part. This results in complex models with a large number of parameters. To address this issue, this paper proposes to learn the multifaceted features in a simple unified single-branch neural network. The Unified Multifaceted Feature Learning (UMFL) framework is introduced to fulfill this goal, which consists of two key collaborative modules: compound batch image erasing (including batch constant erasing and random erasing) and hierarchical structured loss. The loss structures the augmented images resulted by the two types of image erasing in a two-level hierarchy and enforces multifaceted attention to different parts. As we show in the extensive experimental results on four benchmark person ReID datasets, despite the use of significantly simplified network structure, our method performs substantially better than state-of-the-art competing methods. Our method can also effectively generalize to vehicle ReID, achieving similar improvement on two vehicle ReID datasets.



### W-Net: Two-stage U-Net with misaligned data for raw-to-RGB mapping
- **Arxiv ID**: http://arxiv.org/abs/1911.08656v3
- **DOI**: 10.1109/ICCVW.2019.00448
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.08656v3)
- **Published**: 2019-11-20 01:17:41+00:00
- **Updated**: 2019-11-22 02:59:47+00:00
- **Authors**: Kwang-Hyun Uhm, Seung-Wook Kim, Seo-Won Ji, Sung-Jin Cho, Jun-Pyo Hong, Sung-Jea Ko
- **Comment**: Accepted by ICCVW 2019
- **Journal**: None
- **Summary**: Recent research on learning a mapping between raw Bayer images and RGB images has progressed with the development of deep convolutional neural networks. A challenging data set namely the Zurich Raw-to-RGB data set (ZRR) has been released in the AIM 2019 raw-to-RGB mapping challenge. In ZRR, input raw and target RGB images are captured by two different cameras and thus not perfectly aligned. Moreover, camera metadata such as white balance gains and color correction matrix are not provided, which makes the challenge more difficult. In this paper, we explore an effective network structure and a loss function to address these issues. We exploit a two-stage U-Net architecture and also introduce a loss function that is less variant to alignment and more sensitive to color differences. In addition, we show an ensemble of networks trained with different loss functions can bring a significant performance gain. We demonstrate the superiority of our method by achieving the highest score in terms of both the peak signal-to-noise ratio and the structural similarity and obtaining the second-best mean-opinion-score in the challenge.



### MMTM: Multimodal Transfer Module for CNN Fusion
- **Arxiv ID**: http://arxiv.org/abs/1911.08670v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.08670v2)
- **Published**: 2019-11-20 02:32:16+00:00
- **Updated**: 2020-03-30 22:40:45+00:00
- **Authors**: Hamid Reza Vaezi Joze, Amirreza Shaban, Michael L. Iuzzolino, Kazuhito Koishida
- **Comment**: None
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2020
- **Summary**: In late fusion, each modality is processed in a separate unimodal Convolutional Neural Network (CNN) stream and the scores of each modality are fused at the end. Due to its simplicity late fusion is still the predominant approach in many state-of-the-art multimodal applications. In this paper, we present a simple neural network module for leveraging the knowledge from multiple modalities in convolutional neural networks. The propose unit, named Multimodal Transfer Module (MMTM), can be added at different levels of the feature hierarchy, enabling slow modality fusion. Using squeeze and excitation operations, MMTM utilizes the knowledge of multiple modalities to recalibrate the channel-wise features in each CNN stream. Despite other intermediate fusion methods, the proposed module could be used for feature modality fusion in convolution layers with different spatial dimensions. Another advantage of the proposed method is that it could be added among unimodal branches with minimum changes in the their network architectures, allowing each branch to be initialized with existing pretrained weights. Experimental results show that our framework improves the recognition accuracy of well-known multimodal networks. We demonstrate state-of-the-art or competitive performance on four datasets that span the task domains of dynamic hand gesture recognition, speech enhancement, and action recognition with RGB and body joints.



### Discriminative Local Sparse Representation by Robust Adaptive Dictionary Pair Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.08680v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08680v1)
- **Published**: 2019-11-20 03:13:49+00:00
- **Updated**: 2019-11-20 03:13:49+00:00
- **Authors**: Yulin Sun, Zhao Zhang, Weiming Jiang, Zheng Zhang, Li Zhang, Shuicheng Yan, Meng Wang
- **Comment**: Accepted by IEEE TNNLS
- **Journal**: None
- **Summary**: In this paper, we propose a structured Robust Adaptive Dic-tionary Pair Learning (RA-DPL) framework for the discrim-inative sparse representation learning. To achieve powerful representation ability of the available samples, the setting of RA-DPL seamlessly integrates the robust projective dictionary pair learning, locality-adaptive sparse representations and discriminative coding coefficients learning into a unified learning framework. Specifically, RA-DPL improves existing projective dictionary pair learning in four perspectives. First, it applies a sparse l2,1-norm based metric to encode the recon-struction error to deliver the robust projective dictionary pairs, and the l2,1-norm has the potential to minimize the error. Sec-ond, it imposes the robust l2,1-norm clearly on the analysis dictionary to ensure the sparse property of the coding coeffi-cients rather than using the costly l0/l1-norm. As such, the robustness of the data representation and the efficiency of the learning process are jointly considered to guarantee the effi-cacy of our RA-DPL. Third, RA-DPL conceives a structured reconstruction weight learning paradigm to preserve the local structures of the coding coefficients within each class clearly in an adaptive manner, which encourages to produce the locality preserving representations. Fourth, it also considers improving the discriminating ability of coding coefficients and dictionary by incorporating a discriminating function, which can ensure high intra-class compactness and inter-class separation in the code space. Extensive experiments show that our RA-DPL can obtain superior performance over other state-of-the-arts.



### SSAH: Semi-supervised Adversarial Deep Hashing with Self-paced Hard Sample Generation
- **Arxiv ID**: http://arxiv.org/abs/1911.08688v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.08688v1)
- **Published**: 2019-11-20 03:45:34+00:00
- **Updated**: 2019-11-20 03:45:34+00:00
- **Authors**: Sheng Jin, Shangchen Zhou, Yao Liu, Chao Chen, Xiaoshuai Sun, Hongxun Yao, Xiansheng Hua
- **Comment**: None
- **Journal**: None
- **Summary**: Deep hashing methods have been proved to be effective and efficient for large-scale Web media search. The success of these data-driven methods largely depends on collecting sufficient labeled data, which is usually a crucial limitation in practical cases. The current solutions to this issue utilize Generative Adversarial Network (GAN) to augment data in semi-supervised learning. However, existing GAN-based methods treat image generations and hashing learning as two isolated processes, leading to generation ineffectiveness. Besides, most works fail to exploit the semantic information in unlabeled data. In this paper, we propose a novel Semi-supervised Self-pace Adversarial Hashing method, named SSAH to solve the above problems in a unified framework. The SSAH method consists of an adversarial network (A-Net) and a hashing network (H-Net). To improve the quality of generative images, first, the A-Net learns hard samples with multi-scale occlusions and multi-angle rotated deformations which compete against the learning of accurate hashing codes. Second, we design a novel self-paced hard generation policy to gradually increase the hashing difficulty of generated samples. To make use of the semantic information in unlabeled ones, we propose a semi-supervised consistent loss. The experimental results show that our method can significantly improve state-of-the-art models on both the widely-used hashing datasets and fine-grained datasets.



### DRNet: Dissect and Reconstruct the Convolutional Neural Network via Interpretable Manners
- **Arxiv ID**: http://arxiv.org/abs/1911.08691v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08691v2)
- **Published**: 2019-11-20 03:52:28+00:00
- **Updated**: 2020-02-26 07:41:37+00:00
- **Authors**: Xiaolong Hu, Zhulin An, Chuanguang Yang, Hui Zhu, Kaiqaing Xu, Yongjun Xu
- **Comment**: ECAI2020
- **Journal**: None
- **Summary**: Convolutional neural networks (ConvNets) are widely used in real life. People usually use ConvNets which pre-trained on a fixed number of classes. However, for different application scenarios, we usually do not need all of the classes, which means ConvNets are redundant when dealing with these tasks. This paper focuses on the redundancy of ConvNet channels. We proposed a novel idea: using an interpretable manner to find the most important channels for every single class (dissect), and dynamically run channels according to classes in need (reconstruct). For VGG16 pre-trained on CIFAR-10, we only run 11\% parameters for two-classes sub-tasks on average with negligible accuracy loss. For VGG16 pre-trained on ImageNet, our method averagely gains 14.29\% accuracy promotion for two-classes sub-tasks. In addition, analysis show that our method captures some semantic meanings of channels, and uses the context information more targeted for sub-tasks of ConvNets.



### Computer-Aided Clinical Skin Disease Diagnosis Using CNN and Object Detection Models
- **Arxiv ID**: http://arxiv.org/abs/1911.08705v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.08705v1)
- **Published**: 2019-11-20 04:53:18+00:00
- **Updated**: 2019-11-20 04:53:18+00:00
- **Authors**: Xin He, Shihao Wang, Shaohuai Shi, Zhenheng Tang, Yuxin Wang, Zhihao Zhao, Jing Dai, Ronghao Ni, Xiaofeng Zhang, Xiaoming Liu, Zhili Wu, Wu Yu, Xiaowen Chu
- **Comment**: KDDBHI Workshop 2019, IEEE BigData Conference
- **Journal**: None
- **Summary**: Skin disease is one of the most common types of human diseases, which may happen to everyone regardless of age, gender or race. Due to the high visual diversity, human diagnosis highly relies on personal experience; and there is a serious shortage of experienced dermatologists in many countries. To alleviate this problem, computer-aided diagnosis with state-of-the-art (SOTA) machine learning techniques would be a promising solution. In this paper, we aim at understanding the performance of convolutional neural network (CNN) based approaches. We first build two versions of skin disease datasets from Internet images: (a) Skin-10, which contains 10 common classes of skin disease with a total of 10,218 images; (b) Skin-100, which is a larger dataset that consists of 19,807 images of 100 skin disease classes. Based on these datasets, we benchmark several SOTA CNN models and show that the accuracy of skin-100 is much lower than the accuracy of skin-10. We then implement an ensemble method based on several CNN models and achieve the best accuracy of 79.01\% for Skin-10 and 53.54\% for Skin-100. We also present an object detection based approach by introducing bounding boxes into the Skin-10 dataset. Our results show that object detection can help improve the accuracy of some skin disease classes.



### Take an Emotion Walk: Perceiving Emotions from Gaits Using Hierarchical Attention Pooling and Affective Mapping
- **Arxiv ID**: http://arxiv.org/abs/1911.08708v4
- **DOI**: 10.1007/978-3-030-58607-2_9
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.08708v4)
- **Published**: 2019-11-20 05:04:16+00:00
- **Updated**: 2021-07-31 15:40:55+00:00
- **Authors**: Uttaran Bhattacharya, Christian Roncal, Trisha Mittal, Rohan Chandra, Kyra Kapsaskis, Kurt Gray, Aniket Bera, Dinesh Manocha
- **Comment**: 18 pages, 5 figures, 3 tables
- **Journal**: In ECCV 2020, Lecture Notes in Computer Science, vol 12355,
  Springer
- **Summary**: We present an autoencoder-based semi-supervised approach to classify perceived human emotions from walking styles obtained from videos or motion-captured data and represented as sequences of 3D poses. Given the motion on each joint in the pose at each time step extracted from 3D pose sequences, we hierarchically pool these joint motions in a bottom-up manner in the encoder, following the kinematic chains in the human body. We also constrain the latent embeddings of the encoder to contain the space of psychologically-motivated affective features underlying the gaits. We train the decoder to reconstruct the motions per joint per time step in a top-down manner from the latent embeddings. For the annotated data, we also train a classifier to map the latent embeddings to emotion labels. Our semi-supervised approach achieves a mean average precision of 0.84 on the Emotion-Gait benchmark dataset, which contains both labeled and unlabeled gaits collected from multiple sources. We outperform current state-of-art algorithms for both emotion recognition and action recognition from 3D gaits by 7%--23% on the absolute. More importantly, we improve the average precision by 10%--50% on the absolute on classes that each makes up less than 25% of the labeled part of the Emotion-Gait benchmark dataset.



### Dual Reconstruction with Densely Connected Residual Network for Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1911.08711v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.08711v1)
- **Published**: 2019-11-20 05:24:00+00:00
- **Updated**: 2019-11-20 05:24:00+00:00
- **Authors**: Chih-Chung Hsu, Chia-Hsiang Lin
- **Comment**: Accepted to ICCV Workshop 2019
- **Journal**: None
- **Summary**: Deep learning-based single image super-resolution enables very fast and high-visual-quality reconstruction. Recently, an enhanced super-resolution based on generative adversarial network (ESRGAN) has achieved excellent performance in terms of both qualitative and quantitative quality of the reconstructed high-resolution image. In this paper, we propose to add one more shortcut between two dense-blocks, as well as add shortcut between two convolution layers inside a dense-block. With this simple strategy of adding more shortcuts in the proposed network, it enables a faster learning process as the gradient information can be back-propagated more easily. Based on the improved ESRGAN, the dual reconstruction is proposed to learn different aspects of the super-resolved image for judiciously enhancing the quality of the reconstructed image. In practice, the super-resolution model is pre-trained solely based on pixel distance, followed by fine-tuning the parameters in the model based on adversarial loss and perceptual loss. Finally, we fuse two different models by weighted-summing their parameters to obtain the final super-resolution model. Experimental results demonstrated that the proposed method achieves excellent performance in the real-world image super-resolution challenge. We have also verified that the proposed dual reconstruction does further improve the quality of the reconstructed image in terms of both PSNR and SSIM.



### Instance-Invariant Domain Adaptive Object Detection via Progressive Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/1911.08712v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08712v4)
- **Published**: 2019-11-20 05:24:15+00:00
- **Updated**: 2021-02-13 17:01:17+00:00
- **Authors**: Aming Wu, Yahong Han, Linchao Zhu, Yi Yang
- **Comment**: Accepted in T-PAMI
- **Journal**: None
- **Summary**: Most state-of-the-art methods of object detection suffer from poor generalization ability when the training and test data are from different domains, e.g., with different styles. To address this problem, previous methods mainly use holistic representations to align feature-level and pixel-level distributions of different domains, which may neglect the instance-level characteristics of objects in images. Besides, when transferring detection ability across different domains, it is important to obtain the instance-level features that are domain-invariant, instead of the styles that are domain-specific. Therefore, in order to extract instance-invariant features, we should disentangle the domain-invariant features from the domain-specific features. To this end, a progressive disentangled framework is first proposed to solve domain adaptive object detection. Particularly, base on disentangled learning used for feature decomposition, we devise two disentangled layers to decompose domain-invariant and domain-specific features. And the instance-invariant features are extracted based on the domain-invariant features. Finally, to enhance the disentanglement, a three-stage training mechanism including multiple loss functions is devised to optimize our model. In the experiment, we verify the effectiveness of our method on three domain-shift scenes. Our method is separately 2.3\%, 3.6\%, and 4.0\% higher than the baseline method \cite{saito2019strong}.



### An Inception Inspired Deep Network to Analyse Fundus Images
- **Arxiv ID**: http://arxiv.org/abs/1911.08715v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.08715v1)
- **Published**: 2019-11-20 05:48:05+00:00
- **Updated**: 2019-11-20 05:48:05+00:00
- **Authors**: Fatmatulzehra Uslu
- **Comment**: 5 pages, 5 figures, accepted at ELECO 2019
- **Journal**: None
- **Summary**: A fundus image usually contains the optic disc, pathologies and other structures in addition to vessels to be segmented. This study proposes a deep network for vessel segmentation, whose architecture is inspired by inception modules. The network contains three sub-networks, each with a different filter size, which are connected in the last layer of the proposed network. According to experiments conducted in the DRIVE and IOSTAR, the performance of our network is found to be better than or comparable to that of the previous methods. We also observe that the sub-networks pay attention to different parts of an input image when producing an output map in the last layer of the proposed network; though, training of the proposed network is not constrained for this purpose.



### DermGAN: Synthetic Generation of Clinical Skin Images with Pathology
- **Arxiv ID**: http://arxiv.org/abs/1911.08716v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.08716v1)
- **Published**: 2019-11-20 05:48:16+00:00
- **Updated**: 2019-11-20 05:48:16+00:00
- **Authors**: Amirata Ghorbani, Vivek Natarajan, David Coz, Yuan Liu
- **Comment**: In full proceedings of NeurIPS ML4H workshop, 2019
- **Journal**: None
- **Summary**: Despite the recent success in applying supervised deep learning to medical imaging tasks, the problem of obtaining large and diverse expert-annotated datasets required for the development of high performant models remains particularly challenging. In this work, we explore the possibility of using Generative Adverserial Networks (GAN) to synthesize clinical images with skin condition. We propose DermGAN, an adaptation of the popular Pix2Pix architecture, to create synthetic images for a pre-specified skin condition while being able to vary its size, location and the underlying skin color. We demonstrate that the generated images are of high fidelity using objective GAN evaluation metrics. In a Human Turing test, we note that the synthetic images are not only visually similar to real images, but also embody the respective skin condition in dermatologists' eyes. Finally, when using the synthetic images as a data augmentation technique for training a skin condition classifier, we observe that the model performs comparably to the baseline model overall while improving on rare but malignant conditions.



### Towards Ghost-free Shadow Removal via Dual Hierarchical Aggregation Network and Shadow Matting GAN
- **Arxiv ID**: http://arxiv.org/abs/1911.08718v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08718v2)
- **Published**: 2019-11-20 05:52:14+00:00
- **Updated**: 2019-11-21 04:59:03+00:00
- **Authors**: Xiaodong Cun, Chi-Man Pun, Cheng Shi
- **Comment**: Accepted by AAAI 2020
- **Journal**: None
- **Summary**: Shadow removal is an essential task for scene understanding. Many studies consider only matching the image contents, which often causes two types of ghosts: color in-consistencies in shadow regions or artifacts on shadow boundaries. In this paper, we tackle these issues in two ways. First, to carefully learn the border artifacts-free image, we propose a novel network structure named the dual hierarchically aggregation network~(DHAN). It contains a series of growth dilated convolutions as the backbone without any down-samplings, and we hierarchically aggregate multi-context features for attention and prediction, respectively. Second, we argue that training on a limited dataset restricts the textural understanding of the network, which leads to the shadow region color in-consistencies. Currently, the largest dataset contains 2k+ shadow/shadow-free image pairs. However, it has only 0.1k+ unique scenes since many samples share exactly the same background with different shadow positions. Thus, we design a shadow matting generative adversarial network~(SMGAN) to synthesize realistic shadow mattings from a given shadow mask and shadow-free image. With the help of novel masks or scenes, we enhance the current datasets using synthesized shadow images. Experiments show that our DHAN can erase the shadows and produce high-quality ghost-free images. After training on the synthesized and real datasets, our network outperforms other state-of-the-art methods by a large margin. The code is available: http://github.com/vinthony/ghost-free-shadow-removal/



### Fast and Flexible Image Blind Denoising via Competition of Experts
- **Arxiv ID**: http://arxiv.org/abs/1911.08724v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.08724v1)
- **Published**: 2019-11-20 06:13:23+00:00
- **Updated**: 2019-11-20 06:13:23+00:00
- **Authors**: Shunta Maeda
- **Comment**: 9 pages, 9 figures
- **Journal**: None
- **Summary**: Fast and flexible processing are two essential requirements for a number of practical applications of image denoising. Current state-of-the-art methods, however, still require either high computational cost or limited scopes of the target. We introduce an efficient ensemble network trained via a competition of expert networks, as an application for image blind denoising. We realize automatic division of unlabeled noisy datasets into clusters respectively optimized to enhance denoising performance. The architecture is scalable, can be extended to deal with diverse noise sources/levels without increasing the computation time. Taking advantage of this method, we save up to approximately 90% of computational cost without sacrifice of the denoising performance compared to single network models with identical architectures. We also compare the proposed method with several existing algorithms and observe significant outperformance over prior arts in terms of computational efficiency.



### Event-based Object Detection and Tracking for Space Situational Awareness
- **Arxiv ID**: http://arxiv.org/abs/1911.08730v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08730v1)
- **Published**: 2019-11-20 06:38:31+00:00
- **Updated**: 2019-11-20 06:38:31+00:00
- **Authors**: Saeed Afshar, Andrew P Nicholson, Andre van Schaik, Gregory Cohen
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present optical space imaging using an unconventional yet promising class of imaging devices known as neuromorphic event-based sensors. These devices, which are modeled on the human retina, do not operate with frames, but rather generate asynchronous streams of events in response to changes in log-illumination at each pixel. These devices are therefore extremely fast, do not have fixed exposure times, allow for imaging whilst the device is moving and enable low power space imaging during daytime as well as night without modification of the sensors. Recorded at multiple remote sites, we present the first event-based space imaging dataset including recordings from multiple event-based sensors from multiple providers, greatly lowering the barrier to entry for other researchers given the scarcity of such sensors and the expertise required to operate them. The dataset contains 236 separate recordings and 572 labeled resident space objects. The event-based imaging paradigm presents unique opportunities and challenges motivating the development of specialized event-based algorithms that can perform tasks such as detection and tracking in an event-based manner. Here we examine a range of such event-based algorithms for detection and tracking. The presented methods are designed specifically for space situational awareness applications and are evaluated in terms of accuracy and speed and suitability for implementation in neuromorphic hardware on remote or space-based imaging platforms.



### Pan-Cancer Diagnostic Consensus Through Searching Archival Histopathology Images Using Artificial Intelligence
- **Arxiv ID**: http://arxiv.org/abs/1911.08736v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.08736v1)
- **Published**: 2019-11-20 06:53:07+00:00
- **Updated**: 2019-11-20 06:53:07+00:00
- **Authors**: Shivam Kalra, H. R. Tizhoosh, Sultaan Shah, Charles Choi, Savvas Damaskinos, Amir Safarpoor, Sobhan Shafiei, Morteza Babaie, Phedias Diamandis, Clinton JV Campbell, Liron Pantanowitz
- **Comment**: None
- **Journal**: None
- **Summary**: The emergence of digital pathology has opened new horizons for histopathology and cytology. Artificial-intelligence algorithms are able to operate on digitized slides to assist pathologists with diagnostic tasks. Whereas machine learning involving classification and segmentation methods have obvious benefits for image analysis in pathology, image search represents a fundamental shift in computational pathology. Matching the pathology of new patients with already diagnosed and curated cases offers pathologist a novel approach to improve diagnostic accuracy through visual inspection of similar cases and computational majority vote for consensus building. In this study, we report the results from searching the largest public repository (The Cancer Genome Atlas [TCGA] program by National Cancer Institute, USA) of whole slide images from almost 11,000 patients depicting different types of malignancies. For the first time, we successfully indexed and searched almost 30,000 high-resolution digitized slides constituting 16 terabytes of data comprised of 20 million 1000x1000 pixels image patches. The TCGA image database covers 25 anatomic sites and contains 32 cancer subtypes. High-performance storage and GPU power were employed for experimentation. The results were assessed with conservative "majority voting" to build consensus for subtype diagnosis through vertical search and demonstrated high accuracy values for both frozen sections slides (e.g., bladder urothelial carcinoma 93%, kidney renal clear cell carcinoma 97%, and ovarian serous cystadenocarcinoma 99%) and permanent histopathology slides (e.g., prostate adenocarcinoma 98%, skin cutaneous melanoma 99%, and thymoma 100%). The key finding of this validation study was that computational consensus appears to be possible for rendering diagnoses if a sufficiently large number of searchable cases are available for each cancer subtype.



### Vision: A Deep Learning Approach to provide walking assistance to the visually impaired
- **Arxiv ID**: http://arxiv.org/abs/1911.08739v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1911.08739v1)
- **Published**: 2019-11-20 07:02:00+00:00
- **Updated**: 2019-11-20 07:02:00+00:00
- **Authors**: Nikhil Thakurdesai, Anupam Tripathi, Dheeraj Butani, Smita Sankhe
- **Comment**: 10 pages, 10 figures
- **Journal**: None
- **Summary**: Blind people face a lot of problems in their daily routines. They have to struggle a lot just to do their day-to-day chores. In this paper, we have proposed a system with the objective to help the visually impaired by providing audio aid guiding them to avoid obstacles, which will assist them to move in their surroundings. Object Detection using YOLO will help them detect the nearby objects and Depth Estimation using monocular vision will tell the approximate distance of the detected objects from the user. Despite a higher accuracy, stereo vision has many hardware constraints, which makes monocular vision the preferred choice for this application.



### Learning to Localize Sound Sources in Visual Scenes: Analysis and Applications
- **Arxiv ID**: http://arxiv.org/abs/1911.09649v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09649v1)
- **Published**: 2019-11-20 07:29:33+00:00
- **Updated**: 2019-11-20 07:29:33+00:00
- **Authors**: Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan Yang, In So Kweon
- **Comment**: To appear in TPAMI. arXiv admin note: substantial text overlap with
  arXiv:1803.03849
- **Journal**: None
- **Summary**: Visual events are usually accompanied by sounds in our daily lives. However, can the machines learn to correlate the visual scene and sound, as well as localize the sound source only by observing them like humans? To investigate its empirical learnability, in this work we first present a novel unsupervised algorithm to address the problem of localizing sound sources in visual scenes. In order to achieve this goal, a two-stream network structure which handles each modality with attention mechanism is developed for sound source localization. The network naturally reveals the localized response in the scene without human annotation. In addition, a new sound source dataset is developed for performance evaluation. Nevertheless, our empirical evaluation shows that the unsupervised method generates false conclusions in some cases. Thereby, we show that this false conclusion cannot be fixed without human prior knowledge due to the well-known correlation and causality mismatch misconception. To fix this issue, we extend our network to the supervised and semi-supervised network settings via a simple modification due to the general architecture of our two-stream network. We show that the false conclusions can be effectively corrected even with a small amount of supervision, i.e., semi-supervised setup. Furthermore, we present the versatility of the learned audio and visual embeddings on the cross-modal content alignment and we extend this proposed algorithm to a new application, sound saliency based automatic camera view panning in 360-degree{\deg} videos.



### Yottixel -- An Image Search Engine for Large Archives of Histopathology Whole Slide Images
- **Arxiv ID**: http://arxiv.org/abs/1911.08748v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.08748v1)
- **Published**: 2019-11-20 07:34:49+00:00
- **Updated**: 2019-11-20 07:34:49+00:00
- **Authors**: S. Kalra, C. Choi, S. Shah, L. Pantanowitz, H. R. Tizhoosh
- **Comment**: None
- **Journal**: None
- **Summary**: With the emergence of digital pathology, searching for similar images in large archives has gained considerable attention. Image retrieval can provide pathologists with unprecedented access to the evidence embodied in already diagnosed and treated cases from the past. This paper proposes a search engine specialized for digital pathology, called Yottixel, a portmanteau for "one yotta pixel," alluding to the big-data nature of histopathology images. The most impressive characteristic of Yottixel is its ability to represent whole slide images (WSIs) in a compact manner. Yottixel can perform millions of searches in real-time with a high search accuracy and low storage profile. Yottixel uses an intelligent indexing algorithm capable of representing WSIs with a mosaic of patches by converting them into a small number of methodically extracted barcodes, called "Bunch of Barcodes" (BoB), the most prominent performance enabler of Yottixel. The performance of the prototype platform is qualitatively tested using 300 WSIs from the University of Pittsburgh Medical Center (UPMC) and 2,020 WSIs from The Cancer Genome Atlas Program (TCGA) provided by the National Cancer Institute. Both datasets amount to more than 4,000,000 patches of 1000x1000 pixels. We report three sets of experiments that show that Yottixel can accurately retrieve organs and malignancies, and its semantic ordering shows good agreement with the subjective evaluation of human observers.



### Unsupervised Domain Adaptation by Optical Flow Augmentation in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.09652v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.09652v1)
- **Published**: 2019-11-20 07:36:45+00:00
- **Updated**: 2019-11-20 07:36:45+00:00
- **Authors**: Oluwafemi Azeez
- **Comment**: arXiv admin note: text overlap with arXiv:1910.10369 by other authors
- **Journal**: None
- **Summary**: It is expensive to generate real-life image labels and there is a domain gap between real-life and simulated images, hence a model trained on the latter cannot adapt to the former. Solving this can totally eliminate the need for labeling real-life datasets completely. Class balanced self-training is one of the existing techniques that attempt to reduce the domain gap. Moreover, augmenting RGB with flow maps has improved performance in simple semantic segmentation and geometry is preserved across domains. Hence, by augmenting images with dense optical flow map, domain adaptation in semantic segmentation can be improved.



### Learning mappings onto regularized latent spaces for biometric authentication
- **Arxiv ID**: http://arxiv.org/abs/1911.08764v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1911.08764v1)
- **Published**: 2019-11-20 08:40:44+00:00
- **Updated**: 2019-11-20 08:40:44+00:00
- **Authors**: Matteo Testa, Arslan Ali, Tiziano Bianchi, Enrico Magli
- **Comment**: Accepted at IEEE MMSP 2019
- **Journal**: None
- **Summary**: We propose a novel architecture for generic biometric authentication based on deep neural networks: RegNet. Differently from other methods, RegNet learns a mapping of the input biometric traits onto a target distribution in a well-behaved space in which users can be separated by means of simple and tunable boundaries. More specifically, authorized and unauthorized users are mapped onto two different and well behaved Gaussian distributions. The novel approach of learning the mapping instead of the boundaries further avoids the problem encountered in typical classifiers for which the learnt boundaries may be complex and difficult to analyze. RegNet achieves high performance in terms of security metrics such as Equal Error Rate (EER), False Acceptance Rate (FAR) and Genuine Acceptance Rate (GAR). The experiments we conducted on publicly available datasets of face and fingerprint confirm the effectiveness of the proposed system.



### Inspect Transfer Learning Architecture with Dilated Convolution
- **Arxiv ID**: http://arxiv.org/abs/1911.08769v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.08769v1)
- **Published**: 2019-11-20 08:45:56+00:00
- **Updated**: 2019-11-20 08:45:56+00:00
- **Authors**: Syeda Noor Jaha Azim, Md. Aminur Rab Ratul
- **Comment**: None
- **Journal**: None
- **Summary**: There are many award-winning pre-trained Convolutional Neural Network (CNN), which have a common phenomenon of increasing depth in convolutional layers. However, I inspect on VGG network, which is one of the famous model submitted to ILSVRC-2014, to show that slight modification in the basic architecture can enhance the accuracy result of the image classification task. In this paper, We present two improve architectures of pre-trained VGG-16 and VGG-19 networks that apply transfer learning when trained on a different dataset. I report a series of experimental result on various modification of the primary VGG networks and achieved significant out-performance on image classification task by: (1) freezing the first two blocks of the convolutional layers to prevent over-fitting and (2) applying different combination of dilation rate in the last three blocks of convolutional layer to reduce image resolution for feature extraction. Both the proposed architecture achieves a competitive result on CIFAR-10 and CIFAR-100 dataset.



### Hierarchical Attention Networks for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.08777v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08777v2)
- **Published**: 2019-11-20 09:07:16+00:00
- **Updated**: 2019-11-25 13:35:29+00:00
- **Authors**: Fei Ding, Gang Yang, Jinlu Liu, Jun Wu, Dayong Ding, Jie Xv, Gangwei Cheng, Xirong Li
- **Comment**: None
- **Journal**: None
- **Summary**: The medical image is characterized by the inter-class indistinction, high variability, and noise, where the recognition of pixels is challenging. Unlike previous self-attention based methods that capture context information from one level, we reformulate the self-attention mechanism from the view of the high-order graph and propose a novel method, namely Hierarchical Attention Network (HANet), to address the problem of medical image segmentation. Concretely, an HA module embedded in the HANet captures context information from neighbors of multiple levels, where these neighbors are extracted from the high-order graph. In the high-order graph, there will be an edge between two nodes only if the correlation between them is high enough, which naturally reduces the noisy attention information caused by the inter-class indistinction. The proposed HA module is robust to the variance of input and can be flexibly inserted into the existing convolution neural networks. We conduct experiments on three medical image segmentation tasks including optic disc/cup segmentation, blood vessel segmentation, and lung segmentation. Extensive results show our method is more effective and robust than the existing state-of-the-art methods.



### Analysis of Deep Networks for Monocular Depth Estimation Through Adversarial Attacks with Proposal of a Defense Method
- **Arxiv ID**: http://arxiv.org/abs/1911.08790v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08790v1)
- **Published**: 2019-11-20 09:41:53+00:00
- **Updated**: 2019-11-20 09:41:53+00:00
- **Authors**: Junjie Hu, Takayuki Okatani
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we consider adversarial attacks against a system of monocular depth estimation (MDE) based on convolutional neural networks (CNNs). The motivation is two-fold. One is to study the security of MDE systems, which has not been actively considered in the community. The other is to improve our understanding of the computational mechanism of CNNs performing MDE. Toward this end, we apply the method recently proposed for visualization of MDE to defending attacks. It trains another CNN to predict a saliency map from an input image, such that the CNN for MDE continues to accurately estimate the depth map from the image with its non-salient part masked out. We report the following findings. First, unsurprisingly, attacks by IFGSM (or equivalently PGD) succeed in making the CNNs yield inaccurate depth estimates. Second, the attacks can be defended by masking out non-salient pixels, indicating that the attacks function by perturbing mostly non-salient pixels. However, the prediction of saliency maps is itself vulnerable to the attacks, even though it is not the direct target of the attacks. We show that the attacks can be defended by using a saliency map predicted by a CNN trained to be robust to the attacks. These results provide an effective defense method as well as a clue to understanding the computational mechanism of CNNs for MDE.



### You Are Here: Geolocation by Embedding Maps and Images
- **Arxiv ID**: http://arxiv.org/abs/1911.08797v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1911.08797v2)
- **Published**: 2019-11-20 10:05:09+00:00
- **Updated**: 2020-07-20 21:25:03+00:00
- **Authors**: Noe Samano, Mengjie Zhou, Andrew Calway
- **Comment**: 18 pages, new version accepted for ECCV 2020 (poster), with new
  results on publicly available dataset and comparison with implementation of
  previously published alternative approach
- **Journal**: None
- **Summary**: We present a novel approach to geolocalising panoramic images on a 2-D cartographic map based on learning a low dimensional embedded space, which allows a comparison between an image captured at a location and local neighbourhoods of the map. The representation is not sufficiently discriminatory to allow localisation from a single image, but when concatenated along a route, localisation converges quickly, with over 90% accuracy being achieved for routes of around 200m in length when using Google Street View and Open Street Map data. The method generalises a previous fixed semantic feature based approach and achieves significantly higher localisation accuracy and faster convergence.



### Segmentation of Defective Skulls from CT Data for Tissue Modelling
- **Arxiv ID**: http://arxiv.org/abs/1911.08805v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.08805v2)
- **Published**: 2019-11-20 10:31:38+00:00
- **Updated**: 2020-11-04 10:02:53+00:00
- **Authors**: Oldřich Kodym, Michal Španěl, Adam Herout
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we present a method of automatic segmentation of defective skulls for custom cranial implant design and 3D printing purposes. Since such tissue models are usually required in patient cases with complex anatomical defects and variety of external objects present in the acquired data, most deep learning-based approaches fall short because it is not possible to create a sufficient training dataset that would encompass the spectrum of all possible structures. Because CNN segmentation experiments in this application domain have been so far limited to simple patch-based CNN architectures, we first show how the usage of the encoder-decoder architecture can substantially improve the segmentation accuracy. Then, we show how the number of segmentation artifacts, which usually require manual corrections, can be further reduced by adding a boundary term to CNN training and by globally optimizing the segmentation with graph-cut. Finally, we show that using the proposed method, 3D segmentation accurate enough for clinical application can be achieved with 2D CNN architectures as well as their 3D counterparts.



### Self-supervised Learning of 3D Objects from Natural Images
- **Arxiv ID**: http://arxiv.org/abs/1911.08850v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.08850v1)
- **Published**: 2019-11-20 12:07:12+00:00
- **Updated**: 2019-11-20 12:07:12+00:00
- **Authors**: Hiroharu Kato, Tatsuya Harada
- **Comment**: Technical report. Project page:
  http://hiroharu-kato.com/projects_en/cifar10_3d.html
- **Journal**: None
- **Summary**: We present a method to learn single-view reconstruction of the 3D shape, pose, and texture of objects from categorized natural images in a self-supervised manner. Since this is a severely ill-posed problem, carefully designing a training method and introducing constraints are essential. To avoid the difficulty of training all elements at the same time, we propose training category-specific base shapes with fixed pose distribution and simple textures first, and subsequently training poses and textures using the obtained shapes. Another difficulty is that shapes and backgrounds sometimes become excessively complicated to mistakenly reconstruct textures on object surfaces. To suppress it, we propose using strong regularization and constraints on object surfaces and background images. With these two techniques, we demonstrate that we can use natural image collections such as CIFAR-10 and PASCAL objects for training, which indicates the possibility to realize 3D object reconstruction on diverse object categories beyond synthetic datasets.



### The dynamics of the stomatognathic system from 4D multimodal data
- **Arxiv ID**: http://arxiv.org/abs/1911.08854v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1911.08854v1)
- **Published**: 2019-11-20 12:25:58+00:00
- **Updated**: 2019-11-20 12:25:58+00:00
- **Authors**: Agnieszka A. Tomaka, Leszek Luchowski, Dariusz Pojda, Michał Tarnawski, Krzysztof Domino
- **Comment**: Chapter 3 in A.Gadomski (ed.): Multiscale Locomotion: Its
  Active-Matter Addressing Physical Principles; UTP University of Science &
  Technology
- **Journal**: None
- **Summary**: The purpose of this chapter is to discuss methods of acquisition, visualization and analysis of the dynamics of a complex biomedical system, illustrated by the human stomatognathic system. The stomatognathic system consists of the teeth and the skull bones with the maxilla and the mandible. Its dynamics can be described by the change of mutual position of the lower/mandibular part versus the upper/maxillary one due to the physiological motion of opening, chewing and swallowing. In order to analyse the dynamics of the stomatognathic system its morphology and motion has to be digitized, which is done using static and dynamic multimodal imagery like CBCT and 3D scans data and temporal measurements of motion. The integration of multimodal data incorporates different direct and indirect methods of registration - aligning of all the data in the same coordinate system. The integrated sets of data form 4D multimodal data which can be further visualized, modeled, and subjected to multivariate time series analysis. Example results are shown. Although there is no direct method of imaging the TMJ motion, the integration of multimodal data forms an adequate tool. As medical imaging becomes ever more diverse and ever more accessible, organizing the imagery and measurements into unified, comprehensive records can deliver to the doctor the most information in the most accessible form, creating a new quality in data simulation, analysis and interpretation.



### RefineDetLite: A Lightweight One-stage Object Detection Framework for CPU-only Devices
- **Arxiv ID**: http://arxiv.org/abs/1911.08855v2
- **DOI**: 10.1109/CVPRW50498.2020.00358
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08855v2)
- **Published**: 2019-11-20 12:27:26+00:00
- **Updated**: 2020-03-23 07:41:44+00:00
- **Authors**: Chen Chen, Mengyuan Liu, Xiandong Meng, Wanpeng Xiao, Qi Ju
- **Comment**: 16 pages, 8 figures
- **Journal**: 2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition Workshops (CVPRW)
- **Summary**: Previous state-of-the-art real-time object detectors have been reported on GPUs which are extremely expensive for processing massive data and in resource-restricted scenarios. Therefore, high efficiency object detectors on CPU-only devices are urgently-needed in industry. The floating-point operations (FLOPs) of networks are not strictly proportional to the running speed on CPU devices, which inspires the design of an exactly "fast" and "accurate" object detector. After investigating the concern gaps between classification networks and detection backbones, and following the design principles of efficient networks, we propose a lightweight residual-like backbone with large receptive fields and wide dimensions for low-level features, which are crucial for detection tasks. Correspondingly, we also design a light-head detection part to match the backbone capability. Furthermore, by analyzing the drawbacks of current one-stage detector training strategies, we also propose three orthogonal training strategies---IOU-guided loss, classes-aware weighting method and balanced multi-task training approach. Without bells and whistles, our proposed RefineDetLite achieves 26.8 mAP on the MSCOCO benchmark at a speed of 130 ms/pic on a single-thread CPU. The detection accuracy can be further increased to 29.6 mAP by integrating all the proposed training strategies, without apparent speed drop.



### Efficient Derivative Computation for Cumulative B-Splines on Lie Groups
- **Arxiv ID**: http://arxiv.org/abs/1911.08860v2
- **DOI**: 10.1109/CVPR42600.2020.01116
- **Categories**: **cs.CV**, cs.NA, cs.RO, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1911.08860v2)
- **Published**: 2019-11-20 12:33:46+00:00
- **Updated**: 2020-05-30 16:38:23+00:00
- **Authors**: Christiane Sommer, Vladyslav Usenko, David Schubert, Nikolaus Demmel, Daniel Cremers
- **Comment**: First two authors contributed equally
- **Journal**: None
- **Summary**: Continuous-time trajectory representation has recently gained popularity for tasks where the fusion of high-frame-rate sensors and multiple unsynchronized devices is required. Lie group cumulative B-splines are a popular way of representing continuous trajectories without singularities. They have been used in near real-time SLAM and odometry systems with IMU, LiDAR, regular, RGB-D and event cameras, as well as for offline calibration. These applications require efficient computation of time derivatives (velocity, acceleration), but all prior works rely on a computationally suboptimal formulation. In this work we present an alternative derivation of time derivatives based on recurrence relations that needs $\mathcal{O}(k)$ instead of $\mathcal{O}(k^2)$ matrix operations (for a spline of order $k$) and results in simple and elegant expressions. While producing the same result, the proposed approach significantly speeds up the trajectory optimization and allows for computing simple analytic derivatives with respect to spline knots. The results presented in this paper pave the way for incorporating continuous-time trajectory representations into more applications where real-time performance is required.



### D3S -- A Discriminative Single Shot Segmentation Tracker
- **Arxiv ID**: http://arxiv.org/abs/1911.08862v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08862v2)
- **Published**: 2019-11-20 12:41:21+00:00
- **Updated**: 2020-04-14 10:04:34+00:00
- **Authors**: Alan Lukežič, Jiří Matas, Matej Kristan
- **Comment**: The paper is accepted to the CVPR2020
- **Journal**: None
- **Summary**: Template-based discriminative trackers are currently the dominant tracking paradigm due to their robustness, but are restricted to bounding box tracking and a limited range of transformation models, which reduces their localization accuracy. We propose a discriminative single-shot segmentation tracker - D3S, which narrows the gap between visual object tracking and video object segmentation. A single-shot network applies two target models with complementary geometric properties, one invariant to a broad range of transformations, including non-rigid deformations, the other assuming a rigid object to simultaneously achieve high robustness and online target segmentation. Without per-dataset finetuning and trained only for segmentation as the primary output, D3S outperforms all trackers on VOT2016, VOT2018 and GOT-10k benchmarks and performs close to the state-of-the-art trackers on the TrackingNet. D3S outperforms the leading segmentation tracker SiamMask on video object segmentation benchmark and performs on par with top video object segmentation algorithms, while running an order of magnitude faster, close to real-time.



### Improving Semantic Segmentation of Aerial Images Using Patch-based Attention
- **Arxiv ID**: http://arxiv.org/abs/1911.08877v1
- **DOI**: 10.1109/TGRS.2020.2994150
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08877v1)
- **Published**: 2019-11-20 13:12:04+00:00
- **Updated**: 2019-11-20 13:12:04+00:00
- **Authors**: Lei Ding, Hao Tang, Lorenzo Bruzzone
- **Comment**: [J]. IEEE Transactions on Geoscience and Remote Sensing, 2020
- **Journal**: Ding L, Tang H, Bruzzone L. LANet: Local attention embedding to
  improve the semantic segmentation of remote sensing images[J]. IEEE
  Transactions on Geoscience and Remote Sensing, 2020
- **Summary**: The trade-off between feature representation power and spatial localization accuracy is crucial for the dense classification/semantic segmentation of aerial images. High-level features extracted from the late layers of a neural network are rich in semantic information, yet have blurred spatial details; low-level features extracted from the early layers of a network contain more pixel-level information, but are isolated and noisy. It is therefore difficult to bridge the gap between high and low-level features due to their difference in terms of physical information content and spatial distribution. In this work, we contribute to solve this problem by enhancing the feature representation in two ways. On the one hand, a patch attention module (PAM) is proposed to enhance the embedding of context information based on a patch-wise calculation of local attention. On the other hand, an attention embedding module (AEM) is proposed to enrich the semantic information of low-level features by embedding local focus from high-level features. Both of the proposed modules are light-weight and can be applied to process the extracted features of convolutional neural networks (CNNs). Experiments show that, by integrating the proposed modules into the baseline Fully Convolutional Network (FCN), the resulting local attention network (LANet) greatly improves the performance over the baseline and outperforms other attention based methods on two aerial image datasets.



### Shift Convolution Network for Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/1911.08896v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08896v1)
- **Published**: 2019-11-20 13:29:21+00:00
- **Updated**: 2019-11-20 13:29:21+00:00
- **Authors**: Jian Xie
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present Shift Convolution Network (ShiftConvNet) to provide matching capability between two feature maps for stereo estimation. The proposed method can speedily produce a highly accurate disparity map from stereo images. A module called shift convolution layer is proposed to replace the traditional correlation layer to perform patch comparisons between two feature maps. By using a novel architecture of convolutional network to learn the matching process, ShiftConvNet can produce better results than DispNet-C[1], also running faster with 5 fps. Moreover, with a proposed auto shift convolution refine part, further improvement is obtained. The proposed approach was evaluated on FlyingThings 3D. It achieves state-of-the-art results on the benchmark dataset. Codes will be made available at github.



### AssemblyNet: A large ensemble of CNNs for 3D Whole Brain MRI Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.09098v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.09098v1)
- **Published**: 2019-11-20 13:37:16+00:00
- **Updated**: 2019-11-20 13:37:16+00:00
- **Authors**: Pierrick Coupé, Boris Mansencal, Michaël Clément, Rémi Giraud, Baudouin Denis de Senneville, Vinh-Thong Ta, Vincent Lepetit, José V. Manjon
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1906.01862
- **Journal**: None
- **Summary**: Whole brain segmentation using deep learning (DL) is a very challenging task since the number of anatomical labels is very high compared to the number of available training images. To address this problem, previous DL methods proposed to use a single convolution neural network (CNN) or few independent CNNs. In this paper, we present a novel ensemble method based on a large number of CNNs processing different overlapping brain areas. Inspired by parliamentary decision-making systems, we propose a framework called AssemblyNet, made of two "assemblies" of U-Nets. Such a parliamentary system is capable of dealing with complex decisions, unseen problem and reaching a consensus quickly. AssemblyNet introduces sharing of knowledge among neighboring U-Nets, an "amendment" procedure made by the second assembly at higher-resolution to refine the decision taken by the first one, and a final decision obtained by majority voting. During our validation, AssemblyNet showed competitive performance compared to state-of-the-art methods such as U-Net, Joint label fusion and SLANT. Moreover, we investigated the scan-rescan consistency and the robustness to disease effects of our method. These experiences demonstrated the reliability of AssemblyNet. Finally, we showed the interest of using semi-supervised learning to improve the performance of our method.



### Deep Learning based HEp-2 Image Classification: A Comprehensive Review
- **Arxiv ID**: http://arxiv.org/abs/1911.08916v2
- **DOI**: 10.1016/j.media.2020.101764
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08916v2)
- **Published**: 2019-11-20 14:03:27+00:00
- **Updated**: 2020-08-06 09:32:42+00:00
- **Authors**: Saimunur Rahman, Lei Wang, Changming Sun, Luping Zhou
- **Comment**: Published in Medical Image Analysis
- **Journal**: Medical Image Analysis (2020): 101764
- **Summary**: Classification of HEp-2 cell patterns plays a significant role in the indirect immunofluorescence test for identifying autoimmune diseases in the human body. Many automatic HEp-2 cell classification methods have been proposed in recent years, amongst which deep learning based methods have shown impressive performance. This paper provides a comprehensive review of the existing deep learning based HEp-2 cell image classification methods. These methods perform HEp-2 image classification at two levels, namely, cell-level and specimen-level. Both levels are covered in this review. At each level, the methods are organized with a deep network usage based taxonomy. The core idea, notable achievements, and key strengths and weaknesses of each method are critically analyzed. Furthermore, a concise review of the existing HEp-2 datasets that are commonly used in the literature is given. The paper ends with a discussion on novel opportunities and future research directions in this field. It is hoped that this paper would provide readers with a thorough reference of this novel, challenging, and thriving field.



### A Human Action Descriptor Based on Motion Coordination
- **Arxiv ID**: http://arxiv.org/abs/1911.08928v1
- **DOI**: 10.1109/LRA.2017.2652494
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.08928v1)
- **Published**: 2019-11-20 14:22:28+00:00
- **Updated**: 2019-11-20 14:22:28+00:00
- **Authors**: Pietro Falco, Matteo Saveriano, Eka Gibran Hasany, Nicholas H. Kirk, Dongheui Lee
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a descriptor for human whole-body actions based on motion coordination. We exploit the principle, well known in neuromechanics, that humans move their joints in a coordinated fashion. Our coordination-based descriptor (CODE) is computed by two main steps. The first step is to identify the most informative joints which characterize the motion. The second step enriches the descriptor considering minimum and maximum joint velocities and the correlations between the most informative joints. In order to compute the distances between action descriptors, we propose a novel correlation-based similarity measure. The performance of CODE is tested on two public datasets, namely HDM05 and Berkeley MHAD, and compared with state-of-the-art approaches, showing recognition results.



### Real-time Scene Text Detection with Differentiable Binarization
- **Arxiv ID**: http://arxiv.org/abs/1911.08947v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08947v2)
- **Published**: 2019-11-20 14:56:47+00:00
- **Updated**: 2019-12-03 13:33:45+00:00
- **Authors**: Minghui Liao, Zhaoyi Wan, Cong Yao, Kai Chen, Xiang Bai
- **Comment**: Accepted to AAAI 2020
- **Journal**: None
- **Summary**: Recently, segmentation-based methods are quite popular in scene text detection, as the segmentation results can more accurately describe scene text of various shapes such as curve text. However, the post-processing of binarization is essential for segmentation-based detection, which converts probability maps produced by a segmentation method into bounding boxes/regions of text. In this paper, we propose a module named Differentiable Binarization (DB), which can perform the binarization process in a segmentation network. Optimized along with a DB module, a segmentation network can adaptively set the thresholds for binarization, which not only simplifies the post-processing but also enhances the performance of text detection. Based on a simple segmentation network, we validate the performance improvements of DB on five benchmark datasets, which consistently achieves state-of-the-art results, in terms of both detection accuracy and speed. In particular, with a light-weight backbone, the performance improvements by DB are significant so that we can look for an ideal tradeoff between detection accuracy and efficiency. Specifically, with a backbone of ResNet-18, our detector achieves an F-measure of 82.8, running at 62 FPS, on the MSRA-TD500 dataset. Code is available at: https://github.com/MhLiao/DB



### Video Segment Copy Detection Using Memory Constrained Hierarchical Batch-Normalized LSTM Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1911.09518v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NI
- **Links**: [PDF](http://arxiv.org/pdf/1911.09518v1)
- **Published**: 2019-11-20 15:00:02+00:00
- **Updated**: 2019-11-20 15:00:02+00:00
- **Authors**: Arjun Krishna, A S Akil Arif Ibrahim
- **Comment**: Undergraduate Thesis
- **Journal**: None
- **Summary**: In this report, we introduce a video hashing method for scalable video segment copy detection. The objective of video segment copy detection is to find the video (s) present in a large database, one of whose segments (cropped in time) is a (transformed) copy of the given query video. This transformation may be temporal (for example frame dropping, change in frame rate) or spatial (brightness and contrast change, addition of noise etc.) in nature although the primary focus of this report is detecting temporal attacks. The video hashing method proposed by us uses a deep learning neural network to learn variable length binary hash codes for the entire video considering both temporal and spatial features into account. This is in contrast to most existing video hashing methods, as they use conventional image hashing techniques to obtain hash codes for a video after extracting features for every frame or certain key frames, in which case the temporal information present in the video is not exploited. Our hashing method is specifically resilient to time cropping making it extremely useful in video segment copy detection. Experimental results obtained on the large augmented dataset consisting of around 25,000 videos with segment copies demonstrate the efficacy of our proposed video hashing method.



### MetH: A family of high-resolution and variable-shape image challenges
- **Arxiv ID**: http://arxiv.org/abs/1911.08953v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08953v4)
- **Published**: 2019-11-20 15:01:22+00:00
- **Updated**: 2020-09-29 11:37:54+00:00
- **Authors**: Ferran Parés, Dario Garcia-Gasulla, Harald Servat, Jesús Labarta, Eduard Ayguadé
- **Comment**: An improved and extended version of this paper has been published in
  arXiv:2007.13693 This version is now obsolete
- **Journal**: None
- **Summary**: High-resolution and variable-shape images have not yet been properly addressed by the AI community. The approach of down-sampling data often used with convolutional neural networks is sub-optimal for many tasks, and has too many drawbacks to be considered a sustainable alternative. In sight of the increasing importance of problems that can benefit from exploiting high-resolution (HR) and variable-shape, and with the goal of promoting research in that direction, we introduce a new family of datasets (MetH). The four proposed problems include two image classification, one image regression and one super resolution task. Each of these datasets contains thousands of art pieces captured by HR and variable-shape images, labeled by experts at the Metropolitan Museum of Art. We perform an analysis, which shows how the proposed tasks go well beyond current public alternatives in both pixel size and aspect ratio variance. At the same time, the performance obtained by popular architectures on these tasks shows that there is ample room for improvement. To wrap up the relevance of the contribution we review the fields, both in AI and high-performance computing, that could benefit from the proposed challenges.



### Evaluating the Transferability and Adversarial Discrimination of Convolutional Neural Networks for Threat Object Detection and Classification within X-Ray Security Imagery
- **Arxiv ID**: http://arxiv.org/abs/1911.08966v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.08966v1)
- **Published**: 2019-11-20 15:29:12+00:00
- **Updated**: 2019-11-20 15:29:12+00:00
- **Authors**: Yona Falinie A. Gaus, Neelanjan Bhowmik, Samet Akcay, Toby P. Breckon
- **Comment**: None
- **Journal**: None
- **Summary**: X-ray imagery security screening is essential to maintaining transport security against a varying profile of threat or prohibited items. Particular interest lies in the automatic detection and classification of weapons such as firearms and knives within complex and cluttered X-ray security imagery. Here, we address this problem by exploring various end-to-end object detection Convolutional Neural Network (CNN) architectures. We evaluate several leading variants spanning the Faster R-CNN, Mask R-CNN, and RetinaNet architectures to explore the transferability of such models between varying X-ray scanners with differing imaging geometries, image resolutions and material colour profiles. Whilst the limited availability of X-ray threat imagery can pose a challenge, we employ a transfer learning approach to evaluate whether such inter-scanner generalisation may exist over a multiple class detection problem. Overall, we achieve maximal detection performance using a Faster R-CNN architecture with a ResNet$_{101}$ classification network, obtaining 0.88 and 0.86 of mean Average Precision (mAP) for a three-class and two class item from varying X-ray imaging sources. Our results exhibit a remarkable degree of generalisability in terms of cross-scanner performance (mAP: 0.87, firearm detection: 0.94 AP). In addition, we examine the inherent adversarial discriminative capability of such networks using a specifically generated adversarial dataset for firearms detection - with a variable low false positive, as low as 5%, this shows both the challenge and promise of such threat detection within X-ray security imagery.



### SINet: Extreme Lightweight Portrait Segmentation Networks with Spatial Squeeze Modules and Information Blocking Decoder
- **Arxiv ID**: http://arxiv.org/abs/1911.09099v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09099v4)
- **Published**: 2019-11-20 15:39:24+00:00
- **Updated**: 2020-02-09 05:17:09+00:00
- **Authors**: Hyojin Park, Lars Lowe Sjösund, YoungJoon Yoo, Nicolas Monet, Jihwan Bang, Nojun Kwak
- **Comment**: https://github.com/HYOJINPARK/ExtPortraitSeg. arXiv admin note: text
  overlap with arXiv:1908.03093
- **Journal**: None
- **Summary**: Designing a lightweight and robust portrait segmentation algorithm is an important task for a wide range of face applications. However, the problem has been considered as a subset of the object segmentation problem and less handled in the semantic segmentation field. Obviously, portrait segmentation has its unique requirements. First, because the portrait segmentation is performed in the middle of a whole process of many real-world applications, it requires extremely lightweight models. Second, there has not been any public datasets in this domain that contain a sufficient number of images with unbiased statistics. To solve the first problem, we introduce the new extremely lightweight portrait segmentation model SINet, containing an information blocking decoder and spatial squeeze modules. The information blocking decoder uses confidence estimates to recover local spatial information without spoiling global consistency. The spatial squeeze module uses multiple receptive fields to cope with various sizes of consistency in the image. To tackle the second problem, we propose a simple method to create additional portrait segmentation data which can improve accuracy on the EG1800 dataset. In our qualitative and quantitative analysis on the EG1800 dataset, we show that our method outperforms various existing lightweight segmentation models. Our method reduces the number of parameters from 2.1M to 86.9K (around 95.9% reduction), while maintaining the accuracy under an 1% margin from the state-of-the-art portrait segmentation method. We also show our model is successfully executed on a real mobile device with 100.6 FPS. In addition, we demonstrate that our method can be used for general semantic segmentation on the Cityscapes dataset. The code and dataset are available in https://github.com/HYOJINPARK/ExtPortraitSeg .



### Unsupervised Monocular Depth Prediction for Indoor Continuous Video Streams
- **Arxiv ID**: http://arxiv.org/abs/1911.08995v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08995v1)
- **Published**: 2019-11-20 16:08:10+00:00
- **Updated**: 2019-11-20 16:08:10+00:00
- **Authors**: Yinglong Feng, Shuncheng Wu, Okan Köpüklü, Xueyang Kang, Federico Tombari
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies unsupervised monocular depth prediction problem. Most of existing unsupervised depth prediction algorithms are developed for outdoor scenarios, while the depth prediction work in the indoor environment is still very scarce to our knowledge. Therefore, this work focuses on narrowing the gap by firstly evaluating existing approaches in the indoor environments and then improving the state-of-the-art design of architecture. Unlike typical outdoor training dataset, such as KITTI with motion constraints, data for indoor environment contains more arbitrary camera movement and short baseline between two consecutive images, which deteriorates the network training for the pose estimation. To address this issue, we propose two methods: Firstly, we propose a novel reconstruction loss function to constraint pose estimation, resulting in accuracy improvement of the predicted disparity map; secondly, we use an ensemble learning with a flipping strategy along with a median filter, directly taking operation on the output disparity map. We evaluate our approaches on the TUM RGB-D and self-collected datasets. The results have shown that both approaches outperform the previous state-of-the-art unsupervised learning approaches.



### Experimental Exploration of Compact Convolutional Neural Network Architectures for Non-temporal Real-time Fire Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.09010v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.09010v1)
- **Published**: 2019-11-20 16:27:10+00:00
- **Updated**: 2019-11-20 16:27:10+00:00
- **Authors**: Ganesh Samarth C. A., Neelanjan Bhowmik, Toby P. Breckon
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we explore different Convolutional Neural Network (CNN) architectures and their variants for non-temporal binary fire detection and localization in video or still imagery. We consider the performance of experimentally defined, reduced complexity deep CNN architectures for this task and evaluate the effects of different optimization and normalization techniques applied to different CNN architectures (spanning the Inception, ResNet and EfficientNet architectural concepts). Contrary to contemporary trends in the field, our work illustrates a maximum overall accuracy of 0.96 for full frame binary fire detection and 0.94 for superpixel localization using an experimentally defined reduced CNN architecture based on the concept of InceptionV4. We notably achieve a lower false positive rate of 0.06 compared to prior work in the field presenting an efficient, robust and real-time solution for fire region detection.



### Towards a Unified Evaluation of Explanation Methods without Ground Truth
- **Arxiv ID**: http://arxiv.org/abs/1911.09017v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.09017v1)
- **Published**: 2019-11-20 16:44:48+00:00
- **Updated**: 2019-11-20 16:44:48+00:00
- **Authors**: Hao Zhang, Jiayi Chen, Haotian Xue, Quanshi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a set of criteria to evaluate the objectiveness of explanation methods of neural networks, which is crucial for the development of explainable AI, but it also presents significant challenges. The core challenge is that people usually cannot obtain ground-truth explanations of the neural network. To this end, we design four metrics to evaluate explanation results without ground-truth explanations. Our metrics can be broadly applied to nine benchmark methods of interpreting neural networks, which provides new insights of explanation methods.



### Exploiting Spatial Invariance for Scalable Unsupervised Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1911.09033v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.09033v1)
- **Published**: 2019-11-20 17:03:51+00:00
- **Updated**: 2019-11-20 17:03:51+00:00
- **Authors**: Eric Crawford, Joelle Pineau
- **Comment**: Accepted at AAAI 2020. Code: https://github.com/e2crawfo/silot.
  Visualizations: https://sites.google.com/view/silot
- **Journal**: None
- **Summary**: The ability to detect and track objects in the visual world is a crucial skill for any intelligent agent, as it is a necessary precursor to any object-level reasoning process. Moreover, it is important that agents learn to track objects without supervision (i.e. without access to annotated training videos) since this will allow agents to begin operating in new environments with minimal human assistance. The task of learning to discover and track objects in videos, which we call \textit{unsupervised object tracking}, has grown in prominence in recent years; however, most architectures that address it still struggle to deal with large scenes containing many objects. In the current work, we propose an architecture that scales well to the large-scene, many-object setting by employing spatially invariant computations (convolutions and spatial attention) and representations (a spatially local object specification scheme). In a series of experiments, we demonstrate a number of attractive features of our architecture; most notably, that it outperforms competing methods at tracking objects in cluttered scenes with many objects, and that it can generalize well to videos that are larger and/or contain more objects than videos encountered during training.



### 3D-Rotation-Equivariant Quaternion Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.09040v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.09040v2)
- **Published**: 2019-11-20 17:10:52+00:00
- **Updated**: 2020-10-11 08:24:30+00:00
- **Authors**: Wen Shen, Binbin Zhang, Shikun Huang, Zhihua Wei, Quanshi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a set of rules to revise various neural networks for 3D point cloud processing to rotation-equivariant quaternion neural networks (REQNNs). We find that when a neural network uses quaternion features under certain conditions, the network feature naturally has the rotation-equivariance property. Rotation equivariance means that applying a specific rotation transformation to the input point cloud is equivalent to applying the same rotation transformation to all intermediate-layer quaternion features. Besides, the REQNN also ensures that the intermediate-layer features are invariant to the permutation of input points. Compared with the original neural network, the REQNN exhibits higher rotation robustness.



### Learning Cross-modal Context Graph for Visual Grounding
- **Arxiv ID**: http://arxiv.org/abs/1911.09042v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09042v2)
- **Published**: 2019-11-20 17:16:04+00:00
- **Updated**: 2019-11-23 07:56:23+00:00
- **Authors**: Yongfei Liu, Bo Wan, Xiaodan Zhu, Xuming He
- **Comment**: AAAI-2020
- **Journal**: None
- **Summary**: Visual grounding is a ubiquitous building block in many vision-language tasks and yet remains challenging due to large variations in visual and linguistic features of grounding entities, strong context effect and the resulting semantic ambiguities. Prior works typically focus on learning representations of individual phrases with limited context information. To address their limitations, this paper proposes a language-guided graph representation to capture the global context of grounding entities and their relations, and develop a cross-modal graph matching strategy for the multiple-phrase visual grounding task. In particular, we introduce a modular graph neural network to compute context-aware representations of phrases and object proposals respectively via message propagation, followed by a graph-based matching module to generate globally consistent localization of grounding phrases. We train the entire graph neural network jointly in a two-stage strategy and evaluate it on the Flickr30K Entities benchmark. Extensive experiments show that our method outperforms the prior state of the arts by a sizable margin, evidencing the efficacy of our grounding framework. Code is available at "https://github.com/youngfly11/LCMCG-PyTorch".



### Heterogeneous Graph-based Knowledge Transfer for Generalized Zero-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.09046v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.09046v1)
- **Published**: 2019-11-20 17:20:05+00:00
- **Updated**: 2019-11-20 17:20:05+00:00
- **Authors**: Junjie Wang, Xiangfeng Wang, Bo Jin, Junchi Yan, Wenjie Zhang, Hongyuan Zha
- **Comment**: None
- **Journal**: None
- **Summary**: Generalized zero-shot learning (GZSL) tackles the problem of learning to classify instances involving both seen classes and unseen ones. The key issue is how to effectively transfer the model learned from seen classes to unseen classes. Existing works in GZSL usually assume that some prior information about unseen classes are available. However, such an assumption is unrealistic when new unseen classes appear dynamically. To this end, we propose a novel heterogeneous graph-based knowledge transfer method (HGKT) for GZSL, agnostic to unseen classes and instances, by leveraging graph neural network. Specifically, a structured heterogeneous graph is constructed with high-level representative nodes for seen classes, which are chosen through Wasserstein barycenter in order to simultaneously capture inter-class and intra-class relationship. The aggregation and embedding functions can be learned through graph neural network, which can be used to compute the embeddings of unseen classes by transferring the knowledge from their neighbors. Extensive experiments on public benchmark datasets show that our method achieves state-of-the-art results.



### Verifiability and Predictability: Interpreting Utilities of Network Architectures for Point Cloud Processing
- **Arxiv ID**: http://arxiv.org/abs/1911.09053v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.09053v3)
- **Published**: 2019-11-20 17:33:19+00:00
- **Updated**: 2021-04-01 03:54:03+00:00
- **Authors**: Wen Shen, Zhihua Wei, Shikun Huang, Binbin Zhang, Panyue Chen, Ping Zhao, Quanshi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we diagnose deep neural networks for 3D point cloud processing to explore utilities of different intermediate-layer network architectures. We propose a number of hypotheses on the effects of specific intermediate-layer network architectures on the representation capacity of DNNs. In order to prove the hypotheses, we design five metrics to diagnose various types of DNNs from the following perspectives, information discarding, information concentration, rotation robustness, adversarial robustness, and neighborhood inconsistency. We conduct comparative studies based on such metrics to verify the hypotheses. We further use the verified hypotheses to revise intermediate-layer architectures of existing DNNs and improve their utilities. Experiments demonstrate the effectiveness of our method.



### Robust Lane Marking Detection Algorithm Using Drivable Area Segmentation and Extended SLT
- **Arxiv ID**: http://arxiv.org/abs/1911.09054v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.09054v1)
- **Published**: 2019-11-20 17:34:31+00:00
- **Updated**: 2019-11-20 17:34:31+00:00
- **Authors**: Umar Ozgunalp, Rui Fan, Shanshan Cheng, Yuxiang Sun, Weixun Zuo, Yilong Zhu, Bohuan Xue, Linwei Zheng, Qing Liang, Ming Liu
- **Comment**: 4 pages, 3 figures, 2019 IEEE International Conference on Robotics
  and Biomimetics
- **Journal**: None
- **Summary**: In this paper, a robust lane detection algorithm is proposed, where the vertical road profile of the road is estimated using dynamic programming from the v-disparity map and, based on the estimated profile, the road area is segmented. Since the lane markings are on the road area and any feature point above the ground will be a noise source for the lane detection, a mask is created for the road area to remove some of the noise for lane detection. The estimated mask is multiplied by the lane feature map in a bird's eye view (BEV). The lane feature points are extracted by using an extended version of symmetrical local threshold (SLT), which not only considers dark light dark transition (DLD) of the lane markings, like (SLT), but also considers parallelism on the lane marking borders. The segmentation then uses only the feature points that are on the road area. A maximum of two linear lane markings are detected using an efficient 1D Hough transform. Then, the detected linear lane markings are used to create a region of interest (ROI) for parabolic lane detection. Finally, based on the estimated region of interest, parabolic lane models are fitted using robust fitting. Due to the robust lane feature extraction and road area segmentation, the proposed algorithm robustly detects lane markings and achieves lane marking detection with an accuracy of 91% when tested on a sequence from the KITTI dataset.



### Fine-grained Synthesis of Unrestricted Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1911.09058v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.09058v2)
- **Published**: 2019-11-20 17:42:12+00:00
- **Updated**: 2020-10-22 16:53:26+00:00
- **Authors**: Omid Poursaeed, Tianxing Jiang, Yordanos Goshu, Harry Yang, Serge Belongie, Ser-Nam Lim
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel approach for generating unrestricted adversarial examples by manipulating fine-grained aspects of image generation. Unlike existing unrestricted attacks that typically hand-craft geometric transformations, we learn stylistic and stochastic modifications leveraging state-of-the-art generative models. This allows us to manipulate an image in a controlled, fine-grained manner without being bounded by a norm threshold. Our approach can be used for targeted and non-targeted unrestricted attacks on classification, semantic segmentation and object detection models. Our attacks can bypass certified defenses, yet our adversarial images look indistinguishable from natural images as verified by human evaluation. Moreover, we demonstrate that adversarial training with our examples improves performance of the model on clean images without requiring any modifications to the architecture. We perform experiments on LSUN, CelebA-HQ and COCO-Stuff as high resolution datasets to validate efficacy of our proposed approach.



### Toward Filament Segmentation Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.02743v1
- **DOI**: None
- **Categories**: **astro-ph.SR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.02743v1)
- **Published**: 2019-11-20 17:45:41+00:00
- **Updated**: 2019-11-20 17:45:41+00:00
- **Authors**: Azim Ahmadzadeh, Sushant S. Mahajan, Dustin J. Kempton, Rafal A. Angryk, Shihao Ji
- **Comment**: 10 pages, 10 figures, 1 table, accepted in IEEE BigData 2019
- **Journal**: None
- **Summary**: We use a well-known deep neural network framework, called Mask R-CNN, for identification of solar filaments in full-disk H-alpha images from Big Bear Solar Observatory (BBSO). The image data, collected from BBSO's archive, are integrated with the spatiotemporal metadata of filaments retrieved from the Heliophysics Events Knowledgebase (HEK) system. This integrated data is then treated as the ground-truth in the training process of the model. The available spatial metadata are the output of a currently running filament-detection module developed and maintained by the Feature Finding Team; an international consortium selected by NASA. Despite the known challenges in the identification and characterization of filaments by the existing module, which in turn are inherited into any other module that intends to learn from such outputs, Mask R-CNN shows promising results. Trained and validated on two years worth of BBSO data, this model is then tested on the three following years. Our case-by-case and overall analyses show that Mask R-CNN can clearly compete with the existing module and in some cases even perform better. Several cases of false positives and false negatives, that are correctly segmented by this model are also shown. The overall advantages of using the proposed model are two-fold: First, deep neural networks' performance generally improves as more annotated data, or better annotations are provided. Second, such a model can be scaled up to detect other solar events, as well as a single multi-purpose module. The results presented in this study introduce a proof of concept in benefits of employing deep neural networks for detection of solar events, and in particular, filaments.



### EfficientDet: Scalable and Efficient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.09070v7
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.09070v7)
- **Published**: 2019-11-20 18:16:09+00:00
- **Updated**: 2020-07-27 15:55:16+00:00
- **Authors**: Mingxing Tan, Ruoming Pang, Quoc V. Le
- **Comment**: CVPR 2020
- **Journal**: Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition (2020)
- **Summary**: Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multiscale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and better backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with single model and single-scale, our EfficientDet-D7 achieves state-of-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs, being 4x - 9x smaller and using 13x - 42x fewer FLOPs than previous detectors. Code is available at https://github.com/google/automl/tree/master/efficientdet.



### The Origins and Prevalence of Texture Bias in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.09071v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1911.09071v3)
- **Published**: 2019-11-20 18:16:38+00:00
- **Updated**: 2020-11-03 22:51:23+00:00
- **Authors**: Katherine L. Hermann, Ting Chen, Simon Kornblith
- **Comment**: NeurIPS'2020
- **Journal**: None
- **Summary**: Recent work has indicated that, unlike humans, ImageNet-trained CNNs tend to classify images by texture rather than by shape. How pervasive is this bias, and where does it come from? We find that, when trained on datasets of images with conflicting shape and texture, CNNs learn to classify by shape at least as easily as by texture. What factors, then, produce the texture bias in CNNs trained on ImageNet? Different unsupervised training objectives and different architectures have small but significant and largely independent effects on the level of texture bias. However, all objectives and architectures still lead to models that make texture-based classification decisions a majority of the time, even if shape information is decodable from their hidden representations. The effect of data augmentation is much larger. By taking less aggressive random crops at training time and applying simple, naturalistic augmentation (color distortion, noise, and blur), we train models that classify ambiguous images by shape a majority of the time, and outperform baselines on out-of-distribution test sets. Our results indicate that apparent differences in the way humans and ImageNet-trained CNNs process images may arise not primarily from differences in their internal workings, but from differences in the data that they see.



### Search to Distill: Pearls are Everywhere but not the Eyes
- **Arxiv ID**: http://arxiv.org/abs/1911.09074v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.09074v2)
- **Published**: 2019-11-20 18:19:25+00:00
- **Updated**: 2020-03-17 03:48:49+00:00
- **Authors**: Yu Liu, Xuhui Jia, Mingxing Tan, Raviteja Vemulapalli, Yukun Zhu, Bradley Green, Xiaogang Wang
- **Comment**: Accepted as an oral representation to CVPR 2020
- **Journal**: None
- **Summary**: Standard Knowledge Distillation (KD) approaches distill the knowledge of a cumbersome teacher model into the parameters of a student model with a pre-defined architecture. However, the knowledge of a neural network, which is represented by the network's output distribution conditioned on its input, depends not only on its parameters but also on its architecture. Hence, a more generalized approach for KD is to distill the teacher's knowledge into both the parameters and architecture of the student. To achieve this, we present a new Architecture-aware Knowledge Distillation (AKD) approach that finds student models (pearls for the teacher) that are best for distilling the given teacher model. In particular, we leverage Neural Architecture Search (NAS), equipped with our KD-guided reward, to search for the best student architectures for a given teacher. Experimental results show our proposed AKD consistently outperforms the conventional NAS plus KD approach, and achieves state-of-the-art results on the ImageNet classification task under various latency settings. Furthermore, the best AKD student architecture for the ImageNet classification task also transfers well to other tasks such as million level face recognition and ensemble learning.



### Impact of perfusion ROI detection to the quality of CBV perfusion map
- **Arxiv ID**: http://arxiv.org/abs/1912.05471v1
- **DOI**: 10.15587/2312-8372.2019.182789
- **Categories**: **physics.med-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.05471v1)
- **Published**: 2019-11-20 19:46:43+00:00
- **Updated**: 2019-11-20 19:46:43+00:00
- **Authors**: Svitlana Alkhimova
- **Comment**: None
- **Journal**: Technology Audit and Production Reserves. - 2019. - V. 5, N. 2
  (49). - P.4-7
- **Summary**: The object of research in this study is quality of CBV perfusion map, considering detection of perfusion ROI as a key component in processing of dynamic susceptibility contrast magnetic resonance images of a human head. CBV map is generally accepted to be the best among others to evaluate location and size of stroke lesions and angiogenesis of brain tumors. Its poor accuracy can cause failed results for both quantitative measurements and visual assessment of cerebral blood volume. The impact of perfusion ROI detection on the quality of maps was analyzed through comparison of maps produced from threshold and reference images of the same datasets from 12 patients with cerebrovascular disease. Brain perfusion ROI was placed to exclude low intensity (air and non-brain tissues regions) and high intensity (cerebrospinal fluid regions) pixels. Maps were produced using area under the curve and deconvolution methods. For both methods compared maps were primarily correlational according to Pearson correlation analysis: r=0.8752 and r=0.8706 for area under the curve and deconvolution, respectively, p<2.2*10^-16. In spite of this, for both methods scatter plots had data points associated with missed blood regions and regression lines indicated presence of scale and offset errors for maps produced from threshold images. Obtained results indicate that thresholding is an ineffective way to detect brain perfusion ROI, which usage can cause degradation of CBV map quality. Perfusion ROI detection should be standardized and accepted into validation protocols of new systems for perfusion data analysis.



### ID-aware Quality for Set-based Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1911.09143v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.09143v1)
- **Published**: 2019-11-20 19:49:27+00:00
- **Updated**: 2019-11-20 19:49:27+00:00
- **Authors**: Xinshao Wang, Elyor Kodirov, Yang Hua, Neil M. Robertson
- **Comment**: A Set-based Person Re-identification Baseline: Simple Average Fusion
  of Global Spatial Representations, without temporal information, without
  parts/poses/attributes information
- **Journal**: None
- **Summary**: Set-based person re-identification (SReID) is a matching problem that aims to verify whether two sets are of the same identity (ID). Existing SReID models typically generate a feature representation per image and aggregate them to represent the set as a single embedding. However, they can easily be perturbed by noises--perceptually/semantically low quality images--which are inevitable due to imperfect tracking/detection systems, or overfit to trivial images. In this work, we present a novel and simple solution to this problem based on ID-aware quality that measures the perceptual and semantic quality of images guided by their ID information. Specifically, we propose an ID-aware Embedding that consists of two key components: (1) Feature learning attention that aims to learn robust image embeddings by focusing on 'medium' hard images. This way it can prevent overfitting to trivial images, and alleviate the influence of outliers. (2) Feature fusion attention is to fuse image embeddings in the set to obtain the set-level embedding. It ignores noisy information and pays more attention to discriminative images to aggregate more discriminative information. Experimental results on four datasets show that our method outperforms state-of-the-art approaches despite the simplicity of our approach.



### Active Learning for Deep Detection Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.09168v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.09168v1)
- **Published**: 2019-11-20 20:57:44+00:00
- **Updated**: 2019-11-20 20:57:44+00:00
- **Authors**: Hamed H. Aghdam, Abel Gonzalez-Garcia, Joost van de Weijer, Antonio M. López
- **Comment**: Accepted at ICCV 2019
- **Journal**: None
- **Summary**: The cost of drawing object bounding boxes (i.e. labeling) for millions of images is prohibitively high. For instance, labeling pedestrians in a regular urban image could take 35 seconds on average. Active learning aims to reduce the cost of labeling by selecting only those images that are informative to improve the detection network accuracy. In this paper, we propose a method to perform active learning of object detectors based on convolutional neural networks. We propose a new image-level scoring process to rank unlabeled images for their automatic selection, which clearly outperforms classical scores. The proposed method can be applied to videos and sets of still images. In the former case, temporal selection rules can complement our scoring process. As a relevant use case, we extensively study the performance of our method on the task of pedestrian detection. Overall, the experiments show that the proposed method performs better than random selection. Our codes are publicly available at www.gitlab.com/haghdam/deep_active_learning.



### RIS-GAN: Explore Residual and Illumination with Generative Adversarial Networks for Shadow Removal
- **Arxiv ID**: http://arxiv.org/abs/1911.09178v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.09178v2)
- **Published**: 2019-11-20 21:29:48+00:00
- **Updated**: 2019-12-24 19:05:50+00:00
- **Authors**: Ling Zhang, Chengjiang Long, Xiaolong Zhang, Chunxia Xiao
- **Comment**: The paper was accepted to the Thirty-Fourth AAAI Conference on
  Artificial Intelligence (AAAI'2020)
- **Journal**: None
- **Summary**: Residual images and illumination estimation have been proved very helpful in image enhancement. In this paper, we propose a general and novel framework RIS-GAN which explores residual and illumination with Generative Adversarial Networks for shadow removal. Combined with the coarse shadow-removal image, the estimated negative residual images and inverse illumination maps can be used to generate indirect shadow-removal images to refine the coarse shadow-removal result to the fine shadow-free image in a coarse-to-fine fashion. Three discriminators are designed to distinguish whether the predicted negative residual images, shadow-removal images, and the inverse illumination maps are real or fake jointly compared with the corresponding ground-truth information. To our best knowledge, we are the first one to explore residual and illumination for shadow removal. We evaluate our proposed method on two benchmark datasets, i.e., SRD and ISTD, and the extensive experiments demonstrate that our proposed method achieves the superior performance to state-of-the-arts, although we have no particular shadow-aware components designed in our generators.



### Localized Compression: Applying Convolutional Neural Networks to Compressed Images
- **Arxiv ID**: http://arxiv.org/abs/1911.09188v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09188v1)
- **Published**: 2019-11-20 21:55:36+00:00
- **Updated**: 2019-11-20 21:55:36+00:00
- **Authors**: Christopher A. George, Bradley M. West
- **Comment**: 6 pages, 1 figure
- **Journal**: None
- **Summary**: We address the challenge of applying existing convolutional neural network (CNN) architectures to compressed images. Existing CNN architectures represent images as a matrix of pixel intensities with a specified dimension; this desired dimension is achieved by downgrading or cropping. Downgrading and cropping are attractive in that the result is also an image; however, an algorithm producing an alternative "compressed" representation could yield better classification performance. This compression algorithm need not be reversible, but must be compatible with the CNN's operations. This problem is thus the counterpart of the well-studied problem of applying compressed CNNs to uncompressed images, which has attracted great interest as CNNs are deployed to size-, weight-, and power- (SWaP)-limited devices. We introduce Localized Compression, a generalization of downgrading in which the original image is divided into blocks and each block is compressed to a smaller size using either sampling- or random-matrix-based techniques. By aligning the size of the compressed blocks with the size of the CNN's convolutional region, localized compression can be made compatible with any CNN architecture. Our experimental results show that Localized Compression results in classification accuracy approximately 1-2% higher than is achieved by downgrading to the equivalent resolution.



### Object-Guided Instance Segmentation for Biological Images
- **Arxiv ID**: http://arxiv.org/abs/1911.09199v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09199v1)
- **Published**: 2019-11-20 22:38:35+00:00
- **Updated**: 2019-11-20 22:38:35+00:00
- **Authors**: Jingru Yi, Hui Tang, Pengxiang Wu, Bo Liu, Daniel J. Hoeppner, Dimitris N. Metaxas, Lianyi Han, Wei Fan
- **Comment**: accepted to AAAI2020
- **Journal**: None
- **Summary**: Instance segmentation of biological images is essential for studying object behaviors and properties. The challenges, such as clustering, occlusion, and adhesion problems of the objects, make instance segmentation a non-trivial task. Current box-free instance segmentation methods typically rely on local pixel-level information. Due to a lack of global object view, these methods are prone to over- or under-segmentation. On the contrary, the box-based instance segmentation methods incorporate object detection into the segmentation, performing better in identifying the individual instances. In this paper, we propose a new box-based instance segmentation method. Mainly, we locate the object bounding boxes from their center points. The object features are subsequently reused in the segmentation branch as a guide to separate the clustered instances within an RoI patch. Along with the instance normalization, the model is able to recover the target object distribution and suppress the distribution of neighboring attached objects. Consequently, the proposed model performs excellently in segmenting the clustered objects while retaining the target object details. The proposed method achieves state-of-the-art performances on three biological datasets: cell nuclei, plant phenotyping dataset, and neural cells.



### DR-KFS: A Differentiable Visual Similarity Metric for 3D Shape Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1911.09204v4
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.09204v4)
- **Published**: 2019-11-20 22:57:51+00:00
- **Updated**: 2020-03-31 18:16:52+00:00
- **Authors**: Jiongchao Jin, Akshay Gadi Patil, Zhang Xiong, Hao Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a differential visual similarity metric to train deep neural networks for 3D reconstruction, aimed at improving reconstruction quality. The metric compares two 3D shapes by measuring distances between multi-view images differentiably rendered from the shapes. Importantly, the image-space distance is also differentiable and measures visual similarity, rather than pixel-wise distortion. Specifically, the similarity is defined by mean-squared errors over HardNet features computed from probabilistic keypoint maps of the compared images. Our differential visual shape similarity metric can be easily plugged into various 3D reconstruction networks, replacing their distortion-based losses, such as Chamfer or Earth Mover distances, so as to optimize the network weights to produce reconstructions with better structural fidelity and visual quality. We demonstrate this both objectively, using well-known shape metrics for retrieval and classification tasks that are independent from our new metric, and subjectively through a perceptual study.



### REVAMP$^2$T: Real-time Edge Video Analytics for Multi-camera Privacy-aware Pedestrian Tracking
- **Arxiv ID**: http://arxiv.org/abs/1911.09217v2
- **DOI**: 10.1109/JIOT.2019.2954804
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.09217v2)
- **Published**: 2019-11-20 23:34:20+00:00
- **Updated**: 2019-11-25 16:40:40+00:00
- **Authors**: Christopher Neff, Matías Mendieta, Shrey Mohan, Mohammadreza Baharani, Samuel Rogers, Hamed Tabkhi
- **Comment**: Published as an article paper in IEEE Internet of Things Journal:
  Special Issue on Privacy and Security in Distributed Edge Computing and
  Evolving IoT
- **Journal**: None
- **Summary**: This article presents REVAMP$^2$T, Real-time Edge Video Analytics for Multi-camera Privacy-aware Pedestrian Tracking, as an integrated end-to-end IoT system for privacy-built-in decentralized situational awareness. REVAMP$^2$T presents novel algorithmic and system constructs to push deep learning and video analytics next to IoT devices (i.e. video cameras). On the algorithm side, REVAMP$^2$T proposes a unified integrated computer vision pipeline for detection, re-identification, and tracking across multiple cameras without the need for storing the streaming data. At the same time, it avoids facial recognition, and tracks and re-identifies pedestrians based on their key features at runtime. On the IoT system side, REVAMP$^2$T provides infrastructure to maximize hardware utilization on the edge, orchestrates global communications, and provides system-wide re-identification, without the use of personally identifiable information, for a distributed IoT network. For the results and evaluation, this article also proposes a new metric, Accuracy$\cdot$Efficiency (\AE), for holistic evaluation of IoT systems for real-time video analytics based on accuracy, performance, and power efficiency. REVAMP$^2$T outperforms current state-of-the-art by as much as thirteen-fold \AE~improvement.



