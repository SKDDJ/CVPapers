# Arxiv Papers in cs.CV on 2019-11-19
### Convolutional Neural Network and decision support in medical imaging: case study of the recognition of blood cell subtypes
- **Arxiv ID**: http://arxiv.org/abs/1911.08010v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.08010v2)
- **Published**: 2019-11-19 00:10:41+00:00
- **Updated**: 2020-10-22 01:11:47+00:00
- **Authors**: Daouda Diouf, Djibril Seck, Mountaga Diop, Abdoulye Ba
- **Comment**: 7 pages, 6 figures, 1 table
- **Journal**: CEUR-WS.org/Vol-2647 (2019), pp. 128-140
- **Summary**: Identifying and characterizing the patient's blood samples is indispensable in diagnostics of malignance suspicious. A painstaking and sometimes subjective task is used in laboratories to manually classify white blood cells. Neural mathematical methods as deep learnings can be very useful in the automated recognition of blood cells. This study uses a particular type of deep learning i.e., convolutional neural networks (CNNs or ConvNets) for image recognition of the four (4) blood cell types (neutrophil, eosinophil, lymphocyte and monocyte) and to enable it to tag them employing a dataset of blood cells with labels for the corresponding cell types. The elements of the database are the input of our CNN and they allowed us to create learning models for the image recognition/classification of the blood cells. We evaluated the recognition performance and outputs learned by the networks in order to implement a neural image recognition model capable of distinguishing polynuclear cells (neutrophil and eosinophil) from those of mononuclear cells (lymphocyte and monocyte). The validation accuracy is 97.77%.



### Online Learned Continual Compression with Adaptive Quantization Modules
- **Arxiv ID**: http://arxiv.org/abs/1911.08019v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.08019v3)
- **Published**: 2019-11-19 00:43:16+00:00
- **Updated**: 2020-08-20 19:19:56+00:00
- **Authors**: Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Joelle Pineau
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce and study the problem of Online Continual Compression, where one attempts to simultaneously learn to compress and store a representative dataset from a non i.i.d data stream, while only observing each sample once. A naive application of auto-encoders in this setting encounters a major challenge: representations derived from earlier encoder states must be usable by later decoder states. We show how to use discrete auto-encoders to effectively address this challenge and introduce Adaptive Quantization Modules (AQM) to control variation in the compression ability of the module at any given stage of learning. This enables selecting an appropriate compression for incoming samples, while taking into account overall memory constraints and current progress of the learned compression. Unlike previous methods, our approach does not require any pretraining, even on challenging datasets. We show that using AQM to replace standard episodic memory in continual learning settings leads to significant gains on continual learning benchmarks. Furthermore we demonstrate this approach with larger images, LiDAR, and reinforcement learning environments.



### Simultaneous Region Localization and Hash Coding for Fine-grained Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1911.08028v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08028v1)
- **Published**: 2019-11-19 01:12:41+00:00
- **Updated**: 2019-11-19 01:12:41+00:00
- **Authors**: Haien Zeng, Hanjiang Lai, Jian Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Fine-grained image hashing is a challenging problem due to the difficulties of discriminative region localization and hash code generation. Most existing deep hashing approaches solve the two tasks independently. While these two tasks are correlated and can reinforce each other. In this paper, we propose a deep fine-grained hashing to simultaneously localize the discriminative regions and generate the efficient binary codes. The proposed approach consists of a region localization module and a hash coding module. The region localization module aims to provide informative regions to the hash coding module. The hash coding module aims to generate effective binary codes and give feedback for learning better localizer. Moreover, to better capture subtle differences, multi-scale regions at different layers are learned without the need of bounding-box/part annotations. Extensive experiments are conducted on two public benchmark fine-grained datasets. The results demonstrate significant improvements in the performance of our method relative to other fine-grained hashing algorithms.



### Reliability Does Matter: An End-to-End Weakly Supervised Semantic Segmentation Approach
- **Arxiv ID**: http://arxiv.org/abs/1911.08039v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08039v1)
- **Published**: 2019-11-19 01:58:16+00:00
- **Updated**: 2019-11-19 01:58:16+00:00
- **Authors**: Bingfeng Zhang, Jimin Xiao, Yunchao Wei, Mingjie Sun, Kaizhu Huang
- **Comment**: AAAI 2020
- **Journal**: None
- **Summary**: Weakly supervised semantic segmentation is a challenging task as it only takes image-level information as supervision for training but produces pixel-level predictions for testing. To address such a challenging task, most recent state-of-the-art approaches propose to adopt two-step solutions, \emph{i.e. } 1) learn to generate pseudo pixel-level masks, and 2) engage FCNs to train the semantic segmentation networks with the pseudo masks. However, the two-step solutions usually employ many bells and whistles in producing high-quality pseudo masks, making this kind of methods complicated and inelegant. In this work, we harness the image-level labels to produce reliable pixel-level annotations and design a fully end-to-end network to learn to predict segmentation maps. Concretely, we firstly leverage an image classification branch to generate class activation maps for the annotated categories, which are further pruned into confident yet tiny object/background regions. Such reliable regions are then directly served as ground-truth labels for the parallel segmentation branch, where a newly designed dense energy loss function is adopted for optimization. Despite its apparent simplicity, our one-step solution achieves competitive mIoU scores (\emph{val}: 62.6, \emph{test}: 62.9) on Pascal VOC compared with those two-step state-of-the-arts. By extending our one-step method to two-step, we get a new state-of-the-art performance on the Pascal VOC (\emph{val}: 66.3, \emph{test}: 66.5).



### Poison as a Cure: Detecting & Neutralizing Variable-Sized Backdoor Attacks in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.08040v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.08040v1)
- **Published**: 2019-11-19 01:59:59+00:00
- **Updated**: 2019-11-19 01:59:59+00:00
- **Authors**: Alvin Chan, Yew-Soon Ong
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models have recently shown to be vulnerable to backdoor poisoning, an insidious attack where the victim model predicts clean images correctly but classifies the same images as the target class when a trigger poison pattern is added. This poison pattern can be embedded in the training dataset by the adversary. Existing defenses are effective under certain conditions such as a small size of the poison pattern, knowledge about the ratio of poisoned training samples or when a validated clean dataset is available. Since a defender may not have such prior knowledge or resources, we propose a defense against backdoor poisoning that is effective even when those prerequisites are not met. It is made up of several parts: one to extract a backdoor poison signal, detect poison target and base classes, and filter out poisoned from clean samples with proven guarantees. The final part of our defense involves retraining the poisoned model on a dataset augmented with the extracted poison signal and corrective relabeling of poisoned samples to neutralize the backdoor. Our approach has shown to be effective in defending against backdoor attacks that use both small and large-sized poison patterns on nine different target-base class pairs from the CIFAR10 dataset.



### Modal-aware Features for Multimodal Hashing
- **Arxiv ID**: http://arxiv.org/abs/1911.08479v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08479v1)
- **Published**: 2019-11-19 02:17:21+00:00
- **Updated**: 2019-11-19 02:17:21+00:00
- **Authors**: Haien Zeng, Hanjiang Lai, Hanlu Chu, Yong Tang, Jian Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Many retrieval applications can benefit from multiple modalities, e.g., text that contains images on Wikipedia, for which how to represent multimodal data is the critical component. Most deep multimodal learning methods typically involve two steps to construct the joint representations: 1) learning of multiple intermediate features, with each intermediate feature corresponding to a modality, using separate and independent deep models; 2) merging the intermediate features into a joint representation using a fusion strategy. However, in the first step, these intermediate features do not have previous knowledge of each other and cannot fully exploit the information contained in the other modalities. In this paper, we present a modal-aware operation as a generic building block to capture the non-linear dependences among the heterogeneous intermediate features that can learn the underlying correlation structures in other multimodal data as soon as possible. The modal-aware operation consists of a kernel network and an attention network. The kernel network is utilized to learn the non-linear relationships with other modalities. Then, to learn better representations for binary hash codes, we present an attention network that finds the informative regions of these modal-aware features that are favorable for retrieval. Experiments conducted on three public benchmark datasets demonstrate significant improvements in the performance of our method relative to state-of-the-art methods.



### A novel method for identifying the deep neural network model with the Serial Number
- **Arxiv ID**: http://arxiv.org/abs/1911.08053v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08053v1)
- **Published**: 2019-11-19 02:43:11+00:00
- **Updated**: 2019-11-19 02:43:11+00:00
- **Authors**: XiangRui Xu, YaQin Li, Cao Yuan
- **Comment**: 9pages,9 figures,conference
- **Journal**: None
- **Summary**: Deep neural network (DNN) with the state of art performance has emerged as a viable and lucrative business service. However, those impressive performances require a large number of computational resources, which comes at a high cost for the model creators. The necessity for protecting DNN models from illegal reproducing and distribution appears salient now. Recently, trigger-set watermarking, breaking the white-box restriction, relying on adversarial training pre-defined (incorrect) labels for crafted inputs, and subsequently using them to verify the model authenticity, has been the main topic of DNN ownership verification. While these methods have successfully demonstrated robustness against removal attacks, few are effective against the tampering attacks from competitors forging the fake watermarks and dogging in the manager. In this paper, we put forth a new framework of the trigger-set watermark by embedding a unique Serial Number (relatedness less original labels) to the deep neural network for model ownership identification, which is both robust to model pruning and resist to tampering attacks. Experiment results demonstrate that the DNN Serial Number only incurs slight accuracy degradation of the original performance and is valid for ownership verification.



### IFQ-Net: Integrated Fixed-point Quantization Networks for Embedded Vision
- **Arxiv ID**: http://arxiv.org/abs/1911.08076v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08076v1)
- **Published**: 2019-11-19 03:29:03+00:00
- **Updated**: 2019-11-19 03:29:03+00:00
- **Authors**: Hongxing Gao, Wei Tao, Dongchao Wen, Tse-Wei Chen, Kinya Osa, Masami Kato
- **Comment**: 9 pages, 6 figures
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR 2018) Workshops
- **Summary**: Deploying deep models on embedded devices has been a challenging problem since the great success of deep learning based networks. Fixed-point networks, which represent their data with low bits fixed-point and thus give remarkable savings on memory usage, are generally preferred. Even though current fixed-point networks employ relative low bits (e.g. 8-bits), the memory saving is far from enough for the embedded devices. On the other hand, quantization deep networks, for example XNOR-Net and HWGQNet, quantize the data into 1 or 2 bits resulting in more significant memory savings but still contain lots of floatingpoint data. In this paper, we propose a fixed-point network for embedded vision tasks through converting the floatingpoint data in a quantization network into fixed-point. Furthermore, to overcome the data loss caused by the conversion, we propose to compose floating-point data operations across multiple layers (e.g. convolution, batch normalization and quantization layers) and convert them into fixedpoint. We name the fixed-point network obtained through such integrated conversion as Integrated Fixed-point Quantization Networks (IFQ-Net). We demonstrate that our IFQNet gives 2.16x and 18x more savings on model size and runtime feature map memory respectively with similar accuracy on ImageNet. Furthermore, based on YOLOv2, we design IFQ-Tinier-YOLO face detector which is a fixed-point network with 256x reduction in model size (246k Bytes) than Tiny-YOLO. We illustrate the promising performance of our face detector in terms of detection rate on Face Detection Data Set and Bencmark (FDDB) and qualitative results of detecting small faces of Wider Face dataset.



### Two-Stream FCNs to Balance Content and Style for Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1911.08079v2
- **DOI**: 10.1007/s00138-020-01086-1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08079v2)
- **Published**: 2019-11-19 03:41:18+00:00
- **Updated**: 2020-05-07 14:55:48+00:00
- **Authors**: Duc Minh Vo, Akihiro Sugimoto
- **Comment**: published in Machine Vision and Applications
- **Journal**: None
- **Summary**: Style transfer is to render given image contents in given styles, and it has an important role in both computer vision fundamental research and industrial applications. Following the success of deep learning based approaches, this problem has been re-launched recently, but still remains a difficult task because of trade-off between preserving contents and faithful rendering of styles. Indeed, how well-balanced content and style are is crucial in evaluating the quality of stylized images. In this paper, we propose an end-to-end two-stream Fully Convolutional Networks (FCNs) aiming at balancing the contributions of the content and the style in rendered images. Our proposed network consists of the encoder and decoder parts. The encoder part utilizes a FCN for content and a FCN for style where the two FCNs have feature injections and are independently trained to preserve the semantic content and to learn the faithful style representation in each. The semantic content feature and the style representation feature are then concatenated adaptively and fed into the decoder to generate style-transferred (stylized) images. In order to train our proposed network, we employ a loss network, the pre-trained VGG-16, to compute content loss and style loss, both of which are efficiently used for the feature injection as well as the feature concatenation. Our intensive experiments show that our proposed model generates more balanced stylized images in content and style than state-of-the-art methods. Moreover, our proposed network achieves efficiency in speed.



### Jointly De-biasing Face Recognition and Demographic Attribute Estimation
- **Arxiv ID**: http://arxiv.org/abs/1911.08080v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08080v4)
- **Published**: 2019-11-19 03:44:34+00:00
- **Updated**: 2020-07-31 07:41:15+00:00
- **Authors**: Sixue Gong, Xiaoming Liu, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of bias in automated face recognition and demographic attribute estimation algorithms, where errors are lower on certain cohorts belonging to specific demographic groups. We present a novel de-biasing adversarial network (DebFace) that learns to extract disentangled feature representations for both unbiased face recognition and demographics estimation. The proposed network consists of one identity classifier and three demographic classifiers (for gender, age, and race) that are trained to distinguish identity and demographic attributes, respectively. Adversarial learning is adopted to minimize correlation among feature factors so as to abate bias influence from other factors. We also design a new scheme to combine demographics with identity features to strengthen robustness of face representation in different demographic groups. The experimental results show that our approach is able to reduce bias in face recognition as well as demographics estimation while achieving state-of-the-art performance.



### Deep Detector Health Management under Adversarial Campaigns
- **Arxiv ID**: http://arxiv.org/abs/1911.08090v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.08090v1)
- **Published**: 2019-11-19 04:33:05+00:00
- **Updated**: 2019-11-19 04:33:05+00:00
- **Authors**: Javier Echauz, Keith Kenemer, Sarfaraz Hussein, Jay Dhaliwal, Saurabh Shintre, Slawomir Grzonkowski, Andrew Gardner
- **Comment**: International Journal of Prognostics and Health Management, Special
  Issue: PHM Applications of Deep Learning and Emerging Analytics, 2019
- **Journal**: None
- **Summary**: Machine learning models are vulnerable to adversarial inputs that induce seemingly unjustifiable errors. As automated classifiers are increasingly used in industrial control systems and machinery, these adversarial errors could grow to be a serious problem. Despite numerous studies over the past few years, the field of adversarial ML is still considered alchemy, with no practical unbroken defenses demonstrated to date, leaving PHM practitioners with few meaningful ways of addressing the problem. We introduce turbidity detection as a practical superset of the adversarial input detection problem, coping with adversarial campaigns rather than statistically invisible one-offs. This perspective is coupled with ROC-theoretic design guidance that prescribes an inexpensive domain adaptation layer at the output of a deep learning model during an attack campaign. The result aims to approximate the Bayes optimal mitigation that ameliorates the detection model's degraded health. A proactively reactive type of prognostics is achieved via Monte Carlo simulation of various adversarial campaign scenarios, by sampling from the model's own turbidity distribution to quickly deploy the correct mitigation during a real-world campaign.



### AddNet: Deep Neural Networks Using FPGA-Optimized Multipliers
- **Arxiv ID**: http://arxiv.org/abs/1911.08097v1
- **DOI**: 10.1109/TVLSI.2019.2939429
- **Categories**: **eess.SP**, cs.AR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.08097v1)
- **Published**: 2019-11-19 05:00:41+00:00
- **Updated**: 2019-11-19 05:00:41+00:00
- **Authors**: Julian Faraone, Martin Kumm, Martin Hardieck, Peter Zipf, Xueyuan Liu, David Boland, Philip H. W. Leong
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Low-precision arithmetic operations to accelerate deep-learning applications on field-programmable gate arrays (FPGAs) have been studied extensively, because they offer the potential to save silicon area or increase throughput. However, these benefits come at the cost of a decrease in accuracy. In this article, we demonstrate that reconfigurable constant coefficient multipliers (RCCMs) offer a better alternative for saving the silicon area than utilizing low-precision arithmetic. RCCMs multiply input values by a restricted choice of coefficients using only adders, subtractors, bit shifts, and multiplexers (MUXes), meaning that they can be heavily optimized for FPGAs. We propose a family of RCCMs tailored to FPGA logic elements to ensure their efficient utilization. To minimize information loss from quantization, we then develop novel training techniques that map the possible coefficient representations of the RCCMs to neural network weight parameter distributions. This enables the usage of the RCCMs in hardware, while maintaining high accuracy. We demonstrate the benefits of these techniques using AlexNet, ResNet-18, and ResNet-50 networks. The resulting implementations achieve up to 50% resource savings over traditional 8-bit quantized networks, translating to significant speedups and power savings. Our RCCM with the lowest resource requirements exceeds 6-bit fixed point accuracy, while all other implementations with RCCMs achieve at least similar accuracy to an 8-bit uniformly quantized design, while achieving significant resource savings.



### HighEr-Resolution Network for Image Demosaicing and Enhancing
- **Arxiv ID**: http://arxiv.org/abs/1911.08098v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.08098v1)
- **Published**: 2019-11-19 05:02:35+00:00
- **Updated**: 2019-11-19 05:02:35+00:00
- **Authors**: Kangfu Mei, Juncheng Li, Jiajie Zhang, Haoyu Wu, Jie Li, Rui Huang
- **Comment**: Accepted in ICCV 2019 Workshop (AIM2019 Raw to RGB Challenge Winner)
- **Journal**: None
- **Summary**: Neural-networks based image restoration methods tend to use low-resolution image patches for training. Although higher-resolution image patches can provide more global information, state-of-the-art methods cannot utilize them due to their huge GPU memory usage, as well as the instable training process. However, plenty of studies have shown that global information is crucial for image restoration tasks like image demosaicing and enhancing. In this work, we propose a HighEr-Resolution Network (HERN) to fully learning global information in high-resolution image patches. To achieve this, the HERN employs two parallel paths to learn image features in two different resolutions, respectively. By combining global-aware features and multi-scale features, our HERN is able to learn global information with feasible GPU memory usage. Besides, we introduce a progressive training method to solve the instability issue and accelerate model convergence. On the task of image demosaicing and enhancing, our HERN achieves state-of-the-art performance on the AIM2019 RAW to RGB mapping challenge. The source code of our implementation is available at https://github.com/MKFMIKU/RAW2RGBNet.



### Three-dimensional Generative Adversarial Nets for Unsupervised Metal Artifact Reduction
- **Arxiv ID**: http://arxiv.org/abs/1911.08105v3
- **DOI**: 10.1109/ACCESS.2020.3002090
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.08105v3)
- **Published**: 2019-11-19 05:56:54+00:00
- **Updated**: 2020-08-21 04:50:09+00:00
- **Authors**: Megumi Nakao, Keiho Imanishi, Nobuhiro Ueda, Yuichiro Imai, Tadaaki Kirita, Tetsuya Matsuda
- **Comment**: None
- **Journal**: IEEE Access, 8, 109453-109465 (2020)
- **Summary**: The reduction of metal artifacts in computed tomography (CT) images, specifically for strong artifacts generated from multiple metal objects, is a challenging issue in medical imaging research. Although there have been some studies on supervised metal artifact reduction through the learning of synthesized artifacts, it is difficult for simulated artifacts to cover the complexity of the real physical phenomena that may be observed in X-ray propagation. In this paper, we introduce metal artifact reduction methods based on an unsupervised volume-to-volume translation learned from clinical CT images. We construct three-dimensional adversarial nets with a regularized loss function designed for metal artifacts from multiple dental fillings. The results of experiments using 915 CT volumes from real patients demonstrate that the proposed framework has an outstanding capacity to reduce strong artifacts and to recover underlying missing voxels, while preserving the anatomical features of soft tissues and tooth structures from the original images.



### Neural Network Pruning with Residual-Connections and Limited-Data
- **Arxiv ID**: http://arxiv.org/abs/1911.08114v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08114v3)
- **Published**: 2019-11-19 06:43:34+00:00
- **Updated**: 2020-04-25 08:02:47+00:00
- **Authors**: Jian-Hao Luo, Jianxin Wu
- **Comment**: CVPR 2020 Oral
- **Journal**: None
- **Summary**: Filter level pruning is an effective method to accelerate the inference speed of deep CNN models. Although numerous pruning algorithms have been proposed, there are still two open issues. The first problem is how to prune residual connections. We propose to prune both channels inside and outside the residual connections via a KL-divergence based criterion. The second issue is pruning with limited data. We observe an interesting phenomenon: directly pruning on a small dataset is usually worse than fine-tuning a small model which is pruned or trained from scratch on the large dataset. Knowledge distillation is an effective approach to compensate for the weakness of limited data. However, the logits of a teacher model may be noisy. In order to avoid the influence of label noise, we propose a label refinement approach to solve this problem. Experiments have demonstrated the effectiveness of our method (CURL, Compression Using Residual-connections and Limited-data). CURL significantly outperforms previous state-of-the-art methods on ImageNet. More importantly, when pruning on small datasets, CURL achieves comparable or much better performance than fine-tuning a pretrained small model.



### Adaptive Routing Between Capsules
- **Arxiv ID**: http://arxiv.org/abs/1911.08119v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.08119v1)
- **Published**: 2019-11-19 06:56:36+00:00
- **Updated**: 2019-11-19 06:56:36+00:00
- **Authors**: Qiang Ren, Shaohua Shang, Lianghua He
- **Comment**: None
- **Journal**: None
- **Summary**: Capsule network is the most recent exciting advancement in the deep learning field and represents positional information by stacking features into vectors. The dynamic routing algorithm is used in the capsule network, however, there are some disadvantages such as the inability to stack multiple layers and a large amount of computation. In this paper, we propose an adaptive routing algorithm that can solve the problems mentioned above. First, the low-layer capsules adaptively adjust their direction and length in the routing algorithm and removing the influence of the coupling coefficient on the gradient propagation, so that the network can work when stacked in multiple layers. Then, the iterative process of routing is simplified to reduce the amount of computation and we introduce the gradient coefficient $\lambda$. Further, we tested the performance of our proposed adaptive routing algorithm on CIFAR10, Fashion-MNIST, SVHN and MNIST, while achieving better results than the dynamic routing algorithm.



### Estimation of Orientation and Camera Parameters from Cryo-Electron Microscopy Images with Variational Autoencoders and Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.08121v2
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.08121v2)
- **Published**: 2019-11-19 07:04:43+00:00
- **Updated**: 2021-05-23 19:11:06+00:00
- **Authors**: Nina Miolane, Frédéric Poitevin, Yee-Ting Li, Susan Holmes
- **Comment**: None
- **Journal**: None
- **Summary**: Cryo-electron microscopy (cryo-EM) is capable of producing reconstructed 3D images of biomolecules at near-atomic resolution. As such, it represents one of the most promising imaging techniques in structural biology. However, raw cryo-EM images are only highly corrupted - noisy and band-pass filtered - 2D projections of the target 3D biomolecules. Reconstructing the 3D molecular shape starts with the removal of image outliers, the estimation of the orientation of the biomolecule that has produced the given 2D image, and the estimation of camera parameters to correct for intensity defects. Current techniques performing these tasks are often computationally expensive, while the dataset sizes keep growing. There is a need for next-generation algorithms that preserve accuracy while improving speed and scalability. In this paper, we combine variational autoencoders (VAEs) and generative adversarial networks (GANs) to learn a low-dimensional latent representation of cryo-EM images. We perform an exploratory analysis of the obtained latent space, that is shown to have a structure of "orbits", in the sense of Lie group theory, consistent with the acquisition procedure of cryo-EM images. This analysis leads us to design an estimation method for orientation and camera parameters of single-particle cryo-EM images, together with an outliers detection procedure. As such, it opens the door to geometric approaches for unsupervised estimations of orientations and camera parameters, making possible fast cryo-EM biomolecule reconstruction.



### MarioNETte: Few-shot Face Reenactment Preserving Identity of Unseen Targets
- **Arxiv ID**: http://arxiv.org/abs/1911.08139v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08139v1)
- **Published**: 2019-11-19 08:02:59+00:00
- **Updated**: 2019-11-19 08:02:59+00:00
- **Authors**: Sungjoo Ha, Martin Kersner, Beomsu Kim, Seokjun Seo, Dongyoung Kim
- **Comment**: In AAAI 2020
- **Journal**: None
- **Summary**: When there is a mismatch between the target identity and the driver identity, face reenactment suffers severe degradation in the quality of the result, especially in a few-shot setting. The identity preservation problem, where the model loses the detailed information of the target leading to a defective output, is the most common failure mode. The problem has several potential sources such as the identity of the driver leaking due to the identity mismatch, or dealing with unseen large poses. To overcome such problems, we introduce components that address the mentioned problem: image attention block, target feature alignment, and landmark transformer. Through attending and warping the relevant features, the proposed architecture, called MarioNETte, produces high-quality reenactments of unseen identities in a few-shot setting. In addition, the landmark transformer dramatically alleviates the identity preservation problem by isolating the expression geometry through landmark disentanglement. Comprehensive experiments are performed to verify that the proposed framework can generate highly realistic faces, outperforming all other baselines, even under a significant mismatch of facial characteristics between the target and the driver.



### Tell Me What They're Holding: Weakly-supervised Object Detection with Transferable Knowledge from Human-object Interaction
- **Arxiv ID**: http://arxiv.org/abs/1911.08141v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.08141v1)
- **Published**: 2019-11-19 08:03:11+00:00
- **Updated**: 2019-11-19 08:03:11+00:00
- **Authors**: Daesik Kim, Gyujeong Lee, Jisoo Jeong, Nojun Kwak
- **Comment**: AAAI 2020 Oral Camera Ready
- **Journal**: None
- **Summary**: In this work, we introduce a novel weakly supervised object detection (WSOD) paradigm to detect objects belonging to rare classes that have not many examples using transferable knowledge from human-object interactions (HOI). While WSOD shows lower performance than full supervision, we mainly focus on HOI as the main context which can strongly supervise complex semantics in images. Therefore, we propose a novel module called RRPN (relational region proposal network) which outputs an object-localizing attention map only with human poses and action verbs. In the source domain, we fully train an object detector and the RRPN with full supervision of HOI. With transferred knowledge about localization map from the trained RRPN, a new object detector can learn unseen objects with weak verbal supervision of HOI without bounding box annotations in the target domain. Because the RRPN is designed as an add-on type, we can apply it not only to the object detection but also to other domains such as semantic segmentation. The experimental results on HICO-DET dataset show the possibility that the proposed method can be a cheap alternative for the current supervised object detection paradigm. Moreover, qualitative results demonstrate that our model can properly localize unseen objects on HICO-DET and V-COCO datasets.



### GraphTER: Unsupervised Learning of Graph Transformation Equivariant Representations via Auto-Encoding Node-wise Transformations
- **Arxiv ID**: http://arxiv.org/abs/1911.08142v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.08142v2)
- **Published**: 2019-11-19 08:03:12+00:00
- **Updated**: 2020-03-19 02:50:11+00:00
- **Authors**: Xiang Gao, Wei Hu, Guo-Jun Qi
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in Graph Convolutional Neural Networks (GCNNs) have shown their efficiency for non-Euclidean data on graphs, which often require a large amount of labeled data with high cost. It it thus critical to learn graph feature representations in an unsupervised manner in practice. To this end, we propose a novel unsupervised learning of Graph Transformation Equivariant Representations (GraphTER), aiming to capture intrinsic patterns of graph structure under both global and local transformations. Specifically, we allow to sample different groups of nodes from a graph and then transform them node-wise isotropically or anisotropically. Then, we self-train a representation encoder to capture the graph structures by reconstructing these node-wise transformations from the feature representations of the original and transformed graphs. In experiments, we apply the learned GraphTER to graphs of 3D point cloud data, and results on point cloud segmentation/classification show that GraphTER significantly outperforms state-of-the-art unsupervised approaches and pushes greatly closer towards the upper bound set by the fully supervised counterparts. The code is available at: https://github.com/gyshgx868/graph-ter.



### Differentiating Features for Scene Segmentation Based on Dedicated Attention Mechanisms
- **Arxiv ID**: http://arxiv.org/abs/1911.08149v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08149v1)
- **Published**: 2019-11-19 08:17:59+00:00
- **Updated**: 2019-11-19 08:17:59+00:00
- **Authors**: Zhiqiang Xiong, Zhicheng Wang, Zhaohui Yu, Xi Gu
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation is a challenge in scene parsing. It requires both context information and rich spatial information. In this paper, we differentiate features for scene segmentation based on dedicated attention mechanisms (DF-DAM), and two attention modules are proposed to optimize the high-level and low-level features in the encoder, respectively. Specifically, we use the high-level and low-level features of ResNet as the source of context information and spatial information, respectively, and optimize them with attention fusion module and 2D position attention module, respectively. For attention fusion module, we adopt dual channel weight to selectively adjust the channel map for the highest two stage features of ResNet, and fuse them to get context information. For 2D position attention module, we use the context information obtained by attention fusion module to assist the selection of the lowest-stage features of ResNet as supplementary spatial information. Finally, the two sets of information obtained by the two modules are simply fused to obtain the prediction. We evaluate our approach on Cityscapes and PASCAL VOC 2012 datasets. In particular, there aren't complicated and redundant processing modules in our architecture, which greatly reduces the complexity, and we achieving 82.3% Mean IoU on PASCAL VOC 2012 test dataset without pre-training on MS-COCO dataset.



### Projection-to-Projection Translation for Hybrid X-ray and Magnetic Resonance Imaging
- **Arxiv ID**: http://arxiv.org/abs/1911.08163v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.08163v1)
- **Published**: 2019-11-19 09:05:30+00:00
- **Updated**: 2019-11-19 09:05:30+00:00
- **Authors**: Bernhard Stimpel, Christopher Syben, Tobias Würfl, Katharina Breininger, Philipp Hoelter, Arnd Dörfler, Andreas Maier
- **Comment**: None
- **Journal**: None
- **Summary**: Hybrid X-ray and magnetic resonance (MR) imaging promises large potential in interventional medical imaging applications due to the broad variety of contrast of MRI combined with fast imaging of X-ray-based modalities. To fully utilize the potential of the vast amount of existing image enhancement techniques, the corresponding information from both modalities must be present in the same domain. For image-guided interventional procedures, X-ray fluoroscopy has proven to be the modality of choice. Synthesizing one modality from another in this case is an ill-posed problem due to ambiguous signal and overlapping structures in projective geometry. To take on these challenges, we present a learning-based solution to MR to X-ray projection-to-projection translation. We propose an image generator network that focuses on high representation capacity in higher resolution layers to allow for accurate synthesis of fine details in the projection images. Additionally, a weighting scheme in the loss computation that favors high-frequency structures is proposed to focus on the important details and contours in projection imaging. The proposed extensions prove valuable in generating X-ray projection images with natural appearance. Our approach achieves a deviation from the ground truth of only $6$% and structural similarity measure of $0.913\,\pm\,0.005$. In particular the high frequency weighting assists in generating projection images with sharp appearance and reduces erroneously synthesized fine details.



### Dense Fusion Classmate Network for Land Cover Classification
- **Arxiv ID**: http://arxiv.org/abs/1911.08169v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08169v1)
- **Published**: 2019-11-19 09:25:59+00:00
- **Updated**: 2019-11-19 09:25:59+00:00
- **Authors**: Chao Tian, Cong Li, Jianping Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, FCNs based methods have made great progress in semantic segmentation. Different with ordinary scenes, satellite image owns specific characteristics, which elements always extend to large scope and no regular or clear boundaries. Therefore, effective mid-level structure information extremely missing, precise pixel-level classification becomes tough issues. In this paper, a Dense Fusion Classmate Network (DFCNet) is proposed to adopt in land cover classification.



### Rethinking deep active learning: Using unlabeled data at model training
- **Arxiv ID**: http://arxiv.org/abs/1911.08177v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.08177v1)
- **Published**: 2019-11-19 09:42:33+00:00
- **Updated**: 2019-11-19 09:42:33+00:00
- **Authors**: Oriane Siméoni, Mateusz Budnik, Yannis Avrithis, Guillaume Gravier
- **Comment**: None
- **Journal**: None
- **Summary**: Active learning typically focuses on training a model on few labeled examples alone, while unlabeled ones are only used for acquisition. In this work we depart from this setting by using both labeled and unlabeled data during model training across active learning cycles. We do so by using unsupervised feature learning at the beginning of the active learning pipeline and semi-supervised learning at every active learning cycle, on all available data. The former has not been investigated before in active learning, while the study of latter in the context of deep learning is scarce and recent findings are not conclusive with respect to its benefit. Our idea is orthogonal to acquisition strategies by using more data, much like ensemble methods use more models. By systematically evaluating on a number of popular acquisition strategies and datasets, we find that the use of unlabeled data during model training brings a surprising accuracy improvement in image classification, compared to the differences between acquisition strategies. We thus explore smaller label budgets, even one label per class.



### Weak Supervision for Generating Pixel-Level Annotations in Scene Text Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.09026v1
- **DOI**: 10.1016/j.patrec.2020.06.023
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.09026v1)
- **Published**: 2019-11-19 09:55:27+00:00
- **Updated**: 2019-11-19 09:55:27+00:00
- **Authors**: Simone Bonechi, Paolo Andreini, Monica Bianchini, Franco Scarselli
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1904.00818
- **Journal**: None
- **Summary**: Providing pixel-level supervisions for scene text segmentation is inherently difficult and costly, so that only few small datasets are available for this task. To face the scarcity of training data, previous approaches based on Convolutional Neural Networks (CNNs) rely on the use of a synthetic dataset for pre-training. However, synthetic data cannot reproduce the complexity and variability of natural images. In this work, we propose to use a weakly supervised learning approach to reduce the domain-shift between synthetic and real data. Leveraging the bounding-box supervision of the COCO-Text and the MLT datasets, we generate weak pixel-level supervisions of real images. In particular, the COCO-Text-Segmentation (COCO_TS) and the MLT-Segmentation (MLT_S) datasets are created and released. These two datasets are used to train a CNN, the Segmentation Multiscale Attention Network (SMANet), which is specifically designed to face some peculiarities of the scene text segmentation task. The SMANet is trained end-to-end on the proposed datasets, and the experiments show that COCO_TS and MLT_S are a valid alternative to synthetic images, allowing to use only a fraction of the training samples and improving significantly the performances.



### Simple yet Effective Way for Improving the Performance of GAN
- **Arxiv ID**: http://arxiv.org/abs/1911.10979v4
- **DOI**: 10.1109/TNNLS.2020.3045000
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.10979v4)
- **Published**: 2019-11-19 10:31:19+00:00
- **Updated**: 2021-01-19 16:19:15+00:00
- **Authors**: Yong-Goo Shin, Yoon-Jae Yeo, Sung-Jea Ko
- **Comment**: Accepted to IEEE transactions on neural networks and learning systems
- **Journal**: None
- **Summary**: In adversarial learning, discriminator often fails to guide the generator successfully since it distinguishes between real and generated images using silly or non-robust features. To alleviate this problem, this brief presents a simple but effective way that improves the performance of generative adversarial network (GAN) without imposing the training overhead or modifying the network architectures of existing methods. The proposed method employs a novel cascading rejection (CR) module for discriminator, which extracts multiple non-overlapped features in an iterative manner using the vector rejection operation. Since the extracted diverse features prevent the discriminator from concentrating on non-meaningful features, the discriminator can guide the generator effectively to produce the images that are more similar to the real images. In addition, since the proposed CR module requires only a few simple vector operations, it can be readily applied to existing frameworks with marginal training overheads. Quantitative evaluations on various datasets including CIFAR-10, CelebA, CelebA-HQ, LSUN, and tiny-ImageNet confirm that the proposed method significantly improves the performance of GAN and conditional GAN in terms of Frechet inception distance (FID) indicating the diversity and visual appearance of the generated images.



### Weakly-Supervised Video Moment Retrieval via Semantic Completion Network
- **Arxiv ID**: http://arxiv.org/abs/1911.08199v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1911.08199v3)
- **Published**: 2019-11-19 10:31:43+00:00
- **Updated**: 2020-01-15 11:09:43+00:00
- **Authors**: Zhijie Lin, Zhou Zhao, Zhu Zhang, Qi Wang, Huasheng Liu
- **Comment**: Accepted by AAAI 2020 as a full paper
- **Journal**: None
- **Summary**: Video moment retrieval is to search the moment that is most relevant to the given natural language query. Existing methods are mostly trained in a fully-supervised setting, which requires the full annotations of temporal boundary for each query. However, manually labeling the annotations is actually time-consuming and expensive. In this paper, we propose a novel weakly-supervised moment retrieval framework requiring only coarse video-level annotations for training. Specifically, we devise a proposal generation module that aggregates the context information to generate and score all candidate proposals in one single pass. We then devise an algorithm that considers both exploitation and exploration to select top-K proposals. Next, we build a semantic completion module to measure the semantic similarity between the selected proposals and query, compute reward and provide feedbacks to the proposal generation module for scoring refinement. Experiments on the ActivityCaptions and Charades-STA demonstrate the effectiveness of our proposed method.



### Mimic The Raw Domain: Accelerating Action Recognition in the Compressed Domain
- **Arxiv ID**: http://arxiv.org/abs/1911.08206v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08206v3)
- **Published**: 2019-11-19 11:05:46+00:00
- **Updated**: 2020-04-22 08:21:16+00:00
- **Authors**: Barak Battash, Haim Barad, Hanlin Tang, Amit Bleiweiss
- **Comment**: CVPR 2020: Joint Workshop on Efficient Deep Learning in Computer
  Vision
- **Journal**: None
- **Summary**: Video understanding usually requires expensive computation that prohibits its deployment, yet videos contain significant spatiotemporal redundancy that can be exploited. In particular, operating directly on the motion vectors and residuals in the compressed video domain can significantly accelerate the compute, by not using the raw videos which demand colossal storage capacity. Existing methods approach this task as a multiple modalities problem. In this paper we are approaching the task in a completely different way; we are looking at the data from the compressed stream as a one unit clip and propose that the residual frames can replace the original RGB frames from the raw domain. Furthermore, we are using teacher-student method to aid the network in the compressed domain to mimic the teacher network in the raw domain. We show experiments on three leading datasets (HMDB51, UCF1, and Kinetics) that approach state-of-the-art accuracy on raw video data by using compressed data. Our model MFCD-Net outperforms prior methods in the compressed domain and more importantly, our model has 11X fewer parameters and 3X fewer Flops, dramatically improving the efficiency of video recognition inference. This approach enables applying neural networks exclusively in the compressed domain without compromising accuracy while accelerating performance.



### On the Impact of Object and Sub-component Level Segmentation Strategies for Supervised Anomaly Detection within X-ray Security Imagery
- **Arxiv ID**: http://arxiv.org/abs/1911.08216v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.08216v1)
- **Published**: 2019-11-19 11:54:18+00:00
- **Updated**: 2019-11-19 11:54:18+00:00
- **Authors**: Neelanjan Bhowmik, Yona Falinie A. Gaus, Samet Akcay, Jack W. Barker, Toby P. Breckon
- **Comment**: None
- **Journal**: None
- **Summary**: X-ray security screening is in widespread use to maintain transportation security against a wide range of potential threat profiles. Of particular interest is the recent focus on the use of automated screening approaches, including the potential anomaly detection as a methodology for concealment detection within complex electronic items. Here we address this problem considering varying segmentation strategies to enable the use of both object level and sub-component level anomaly detection via the use of secondary convolutional neural network (CNN) architectures. Relative performance is evaluated over an extensive dataset of exemplar cluttered X-ray imagery, with a focus on consumer electronics items. We find that sub-component level segmentation produces marginally superior performance in the secondary anomaly detection via classification stage, with true positive of ~98% of anomalies, with a ~3% false positive.



### Constrained R-CNN: A general image manipulation detection model
- **Arxiv ID**: http://arxiv.org/abs/1911.08217v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1911.08217v3)
- **Published**: 2019-11-19 12:12:20+00:00
- **Updated**: 2020-03-15 11:01:38+00:00
- **Authors**: Chao Yang, Huizhou Li, Fangting Lin, Bin Jiang, Hao Zhao
- **Comment**: Accepted to IEEE International Conference on Multimedia and Expo
  (ICME2020)
- **Journal**: None
- **Summary**: Recently, deep learning-based models have exhibited remarkable performance for image manipulation detection. However, most of them suffer from poor universality of handcrafted or predetermined features. Meanwhile, they only focus on manipulation localization and overlook manipulation classification. To address these issues, we propose a coarse-to-fine architecture named Constrained R-CNN for complete and accurate image forensics. First, the learnable manipulation feature extractor learns a unified feature representation directly from data. Second, the attention region proposal network effectively discriminates manipulated regions for the next manipulation classification and coarse localization. Then, the skip structure fuses low-level and high-level information to refine the global manipulation features. Finally, the coarse localization information guides the model to further learn the finer local features and segment out the tampered region. Experimental results show that our model achieves state-of-the-art performance. Especially, the F1 score is increased by 28.4%, 73.2%, 13.3% on the NIST16, COVERAGE, and Columbia dataset.



### Dual affine moment invariants
- **Arxiv ID**: http://arxiv.org/abs/1911.08233v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08233v1)
- **Published**: 2019-11-19 12:55:17+00:00
- **Updated**: 2019-11-19 12:55:17+00:00
- **Authors**: You Hao, Hanlin Mo, Qi Li, He Zhang, Hua Li
- **Comment**: None
- **Journal**: None
- **Summary**: Affine transformation is one of the most common transformations in nature, which is an important issue in the field of computer vision and shape analysis. And affine transformations often occur in both shape and color space simultaneously, which can be termed as Dual-Affine Transformation (DAT). In general, we should derive invariants of different data formats separately, such as 2D color images, 3D color objects, or even higher-dimensional data. To the best of our knowledge, there is no general framework to derive invariants for all of these data formats. In this paper, we propose a general framework to derive moment invariants under DAT for objects in M-dimensional space with N channels, which can be called dual-affine moment invariants (DAMI). Following this framework, we present the generating formula of DAMI under DAT for 3D color objects. Then, we instantiated a complete set of DAMI for 3D color objects with orders and degrees no greater than 4. Finally, we analyze the characteristic of these DAMI and conduct classification experiments to evaluate the stability and discriminability of them. The results prove that DAMI is robust for DAT. Our derivation framework can be applied to data in any dimension with any number of channels.



### General $E(2)$-Equivariant Steerable CNNs
- **Arxiv ID**: http://arxiv.org/abs/1911.08251v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.08251v2)
- **Published**: 2019-11-19 13:25:49+00:00
- **Updated**: 2021-04-06 17:46:56+00:00
- **Authors**: Maurice Weiler, Gabriele Cesa
- **Comment**: Conference on Neural Information Processing Systems (NeurIPS), 2019
- **Journal**: None
- **Summary**: The big empirical success of group equivariant networks has led in recent years to the sprouting of a great variety of equivariant network architectures. A particular focus has thereby been on rotation and reflection equivariant CNNs for planar images. Here we give a general description of $E(2)$-equivariant convolutions in the framework of Steerable CNNs. The theory of Steerable CNNs thereby yields constraints on the convolution kernels which depend on group representations describing the transformation laws of feature spaces. We show that these constraints for arbitrary group representations can be reduced to constraints under irreducible representations. A general solution of the kernel space constraint is given for arbitrary representations of the Euclidean group $E(2)$ and its subgroups. We implement a wide range of previously proposed and entirely new equivariant network architectures and extensively compare their performances. $E(2)$-steerable convolutions are further shown to yield remarkable gains on CIFAR-10, CIFAR-100 and STL-10 when used as a drop-in replacement for non-equivariant convolutions.



### Unsupervised AER Object Recognition Based on Multiscale Spatio-Temporal Features and Spiking Neurons
- **Arxiv ID**: http://arxiv.org/abs/1911.08261v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.08261v1)
- **Published**: 2019-11-19 13:47:47+00:00
- **Updated**: 2019-11-19 13:47:47+00:00
- **Authors**: Qianhui Liu, Gang Pan, Haibo Ruan, Dong Xing, Qi Xu, Huajin Tang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes an unsupervised address event representation (AER) object recognition approach. The proposed approach consists of a novel multiscale spatio-temporal feature (MuST) representation of input AER events and a spiking neural network (SNN) using spike-timing-dependent plasticity (STDP) for object recognition with MuST. MuST extracts the features contained in both the spatial and temporal information of AER event flow, and meanwhile forms an informative and compact feature spike representation. We show not only how MuST exploits spikes to convey information more effectively, but also how it benefits the recognition using SNN. The recognition process is performed in an unsupervised manner, which does not need to specify the desired status of every single neuron of SNN, and thus can be flexibly applied in real-world recognition tasks. The experiments are performed on five AER datasets including a new one named GESTURE-DVS. Extensive experimental results show the effectiveness and advantages of this proposed approach.



### Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression
- **Arxiv ID**: http://arxiv.org/abs/1911.08287v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08287v1)
- **Published**: 2019-11-19 14:20:07+00:00
- **Updated**: 2019-11-19 14:20:07+00:00
- **Authors**: Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang Ye, Dongwei Ren
- **Comment**: Accepted to AAAI 2020. The source code and trained models are
  available at https://github.com/Zzh-tju/DIoU
- **Journal**: None
- **Summary**: Bounding box regression is the crucial step in object detection. In existing methods, while $\ell_n$-norm loss is widely adopted for bounding box regression, it is not tailored to the evaluation metric, i.e., Intersection over Union (IoU). Recently, IoU loss and generalized IoU (GIoU) loss have been proposed to benefit the IoU metric, but still suffer from the problems of slow convergence and inaccurate regression. In this paper, we propose a Distance-IoU (DIoU) loss by incorporating the normalized distance between the predicted box and the target box, which converges much faster in training than IoU and GIoU losses. Furthermore, this paper summarizes three geometric factors in bounding box regression, \ie, overlap area, central point distance and aspect ratio, based on which a Complete IoU (CIoU) loss is proposed, thereby leading to faster convergence and better performance. By incorporating DIoU and CIoU losses into state-of-the-art object detection algorithms, e.g., YOLO v3, SSD and Faster RCNN, we achieve notable performance gains in terms of not only IoU metric but also GIoU metric. Moreover, DIoU can be easily adopted into non-maximum suppression (NMS) to act as the criterion, further boosting performance improvement. The source code and trained models are available at https://github.com/Zzh-tju/DIoU.



### Learning Modulated Loss for Rotated Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.08299v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08299v3)
- **Published**: 2019-11-19 14:37:41+00:00
- **Updated**: 2019-12-20 13:21:28+00:00
- **Authors**: Wen Qian, Xue Yang, Silong Peng, Yue Guo, Junchi Yan
- **Comment**: 11 pages, 8 figures
- **Journal**: None
- **Summary**: Popular rotated detection methods usually use five parameters (coordinates of the central point, width, height, and rotation angle) to describe the rotated bounding box and l1-loss as the loss function. In this paper, we argue that the aforementioned integration can cause training instability and performance degeneration, due to the loss discontinuity resulted from the inherent periodicity of angles and the associated sudden exchange of width and height. This problem is further pronounced given the regression inconsistency among five parameters with different measurement units. We refer to the above issues as rotation sensitivity error (RSE) and propose a modulated rotation loss to dismiss the loss discontinuity. Our new loss is combined with the eight-parameter regression to further solve the problem of inconsistent parameter regression. Experiments show the state-of-art performances of our method on the public aerial image benchmark DOTA and UCAS-AOD. Its generalization abilities are also verified on ICDAR2015, HRSC2016, and FDDB. Qualitative improvements can be seen in Fig 1, and the source code will be released with the publication of the paper.



### Automatic Brain Tumour Segmentation and Biophysics-Guided Survival Prediction
- **Arxiv ID**: http://arxiv.org/abs/1911.08483v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.08483v1)
- **Published**: 2019-11-19 14:44:55+00:00
- **Updated**: 2019-11-19 14:44:55+00:00
- **Authors**: Shuo Wang, Chengliang Dai, Yuanhan Mo, Elsa Angelini, Yike Guo, Wenjia Bai
- **Comment**: MICCAI BraTS 2019 Challenge
- **Journal**: None
- **Summary**: Gliomas are the most common malignant brain tumourswith intrinsic heterogeneity. Accurate segmentation of gliomas and theirsub-regions on multi-parametric magnetic resonance images (mpMRI)is of great clinical importance, which defines tumour size, shape andappearance and provides abundant information for preoperative diag-nosis, treatment planning and survival prediction. Recent developmentson deep learning have significantly improved the performance of auto-mated medical image segmentation. In this paper, we compare severalstate-of-the-art convolutional neural network models for brain tumourimage segmentation. Based on the ensembled segmentation, we presenta biophysics-guided prognostic model for patient overall survival predic-tion which outperforms a data-driven radiomics approach. Our methodwon the second place of the MICCAI 2019 BraTS Challenge for theoverall survival prediction.



### Single-Stage 6D Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1911.08324v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08324v2)
- **Published**: 2019-11-19 14:56:54+00:00
- **Updated**: 2020-03-20 21:54:00+00:00
- **Authors**: Yinlin Hu, Pascal Fua, Wei Wang, Mathieu Salzmann
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Most recent 6D pose estimation frameworks first rely on a deep network to establish correspondences between 3D object keypoints and 2D image locations and then use a variant of a RANSAC-based Perspective-n-Point (PnP) algorithm. This two-stage process, however, is suboptimal: First, it is not end-to-end trainable. Second, training the deep network relies on a surrogate loss that does not directly reflect the final 6D pose estimation task.   In this work, we introduce a deep architecture that directly regresses 6D poses from correspondences. It takes as input a group of candidate correspondences for each 3D keypoint and accounts for the fact that the order of the correspondences within each group is irrelevant, while the order of the groups, that is, of the 3D keypoints, is fixed. Our architecture is generic and can thus be exploited in conjunction with existing correspondence-extraction networks so as to yield single-stage 6D pose estimation frameworks. Our experiments demonstrate that these single-stage frameworks consistently outperform their two-stage counterparts in terms of both accuracy and speed.



### FollowMeUp Sports: New Benchmark for 2D Human Keypoint Recognition
- **Arxiv ID**: http://arxiv.org/abs/1911.08344v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08344v1)
- **Published**: 2019-11-19 15:23:23+00:00
- **Updated**: 2019-11-19 15:23:23+00:00
- **Authors**: Ying Huang, Bin Sun, Haipeng Kan, Jiankai Zhuang, Zengchang Qin
- **Comment**: 12 pages, accepted at PRCV 2019
- **Journal**: None
- **Summary**: Human pose estimation has made significant advancement in recent years. However, the existing datasets are limited in their coverage of pose variety. In this paper, we introduce a novel benchmark FollowMeUp Sports that makes an important advance in terms of specific postures, self-occlusion and class balance, a contribution that we feel is required for future development in human body models. This comprehensive dataset was collected using an established taxonomy of over 200 standard workout activities with three different shot angles. The collected videos cover a wider variety of specific workout activities than previous datasets including push-up, squat and body moving near the ground with severe self-occlusion or occluded by some sport equipment and outfits. Given these rich images, we perform a detailed analysis of the leading human pose estimation approaches gaining insights for the success and failures of these methods.



### Live Face De-Identification in Video
- **Arxiv ID**: http://arxiv.org/abs/1911.08348v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.GR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.08348v1)
- **Published**: 2019-11-19 15:28:35+00:00
- **Updated**: 2019-11-19 15:28:35+00:00
- **Authors**: Oran Gafni, Lior Wolf, Yaniv Taigman
- **Comment**: ICCV 2019
- **Journal**: Proceedings of the IEEE International Conference on Computer
  Vision (2019) 9378--9387
- **Summary**: We propose a method for face de-identification that enables fully automatic video modification at high frame rates. The goal is to maximally decorrelate the identity, while having the perception (pose, illumination and expression) fixed. We achieve this by a novel feed-forward encoder-decoder network architecture that is conditioned on the high-level representation of a person's facial image. The network is global, in the sense that it does not need to be retrained for a given video or for a given identity, and it creates natural looking image sequences with little distortion in time.



### Solar Event Tracking with Deep Regression Networks: A Proof of Concept Evaluation
- **Arxiv ID**: http://arxiv.org/abs/1911.08350v1
- **DOI**: 10.1109/BigData47090.2019.9006273
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.08350v1)
- **Published**: 2019-11-19 15:32:10+00:00
- **Updated**: 2019-11-19 15:32:10+00:00
- **Authors**: Toqi Tahamid Sarker, Juan M. Banda
- **Comment**: 8 pages, 5 figures, this has been submitted and accepted for
  publication at IEEE Big Data 2019 - SABID Workshop
- **Journal**: 2019 IEEE International Conference on Big Data (Big Data)
- **Summary**: With the advent of deep learning for computer vision tasks, the need for accurately labeled data in large volumes is vital for any application. The increasingly available large amounts of solar image data generated by the Solar Dynamic Observatory (SDO) mission make this domain particularly interesting for the development and testing of deep learning systems. The currently available labeled solar data is generated by the SDO mission's Feature Finding Team's (FFT) specialized detection modules. The major drawback of these modules is that detection and labeling is performed with a cadence of every 4 to 12 hours, depending on the module. Since SDO image data products are created every 10 seconds, there is a considerable gap between labeled observations and the continuous data stream. In order to address this shortcoming, we trained a deep regression network to track the movement of two solar phenomena: Active Region and Coronal Hole events. To the best of our knowledge, this is the first attempt of solar event tracking using a deep learning approach. Since it is impossible to fully evaluate the performance of the suggested event tracks with the original data (only partial ground truth is available), we demonstrate with several metrics the effectiveness of our approach. With the purpose of generating continuously labeled solar image data, we present this feasibility analysis showing the great promise of deep regression networks for this task.



### Multi-Resolution 3D CNN for MRI Brain Tumor Segmentation and Survival Prediction
- **Arxiv ID**: http://arxiv.org/abs/1911.08388v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.08388v1)
- **Published**: 2019-11-19 16:36:54+00:00
- **Updated**: 2019-11-19 16:36:54+00:00
- **Authors**: Mehdi Amian, Mohammadreza Soltaninejad
- **Comment**: Submitted to Lecture Notes in Computer Science (LNCS) BraTS
  proceedings
- **Journal**: None
- **Summary**: In this study, an automated three dimensional (3D) deep segmentation approach for detecting gliomas in 3D pre-operative MRI scans is proposed. Then, a classi-fication algorithm based on random forests, for survival prediction is presented. The objective is to segment the glioma area and produce segmentation labels for its different sub-regions, i.e. necrotic and the non-enhancing tumor core, the peri-tumoral edema, and enhancing tumor. The proposed deep architecture for the segmentation task encompasses two parallel streamlines with two different reso-lutions. One deep convolutional neural network is to learn local features of the input data while the other one is set to have a global observation on whole image. Deemed to be complementary, the outputs of each stream are then merged to pro-vide an ensemble complete learning of the input image. The proposed network takes the whole image as input instead of patch-based approaches in order to con-sider the semantic features throughout the whole volume. The algorithm is trained on BraTS 2019 which included 335 training cases, and validated on 127 unseen cases from the validation dataset using a blind testing approach. The proposed method was also evaluated on the BraTS 2019 challenge test dataset of 166 cases. The results show that the proposed methods provide promising segmentations as well as survival prediction. The mean Dice overlap measures of automatic brain tumor segmentation for validation set were 0.84, 0.74 and 0.71 for the whole tu-mor, core and enhancing tumor, respectively. The corresponding results for the challenge test dataset were 0.82, 0.72, and 0.70, respectively. The overall accura-cy of the proposed model for the survival prediction task is %52 for the valida-tion and %49 for the test dataset.



### KISS: Keeping It Simple for Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1911.08400v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08400v1)
- **Published**: 2019-11-19 17:13:18+00:00
- **Updated**: 2019-11-19 17:13:18+00:00
- **Authors**: Christian Bartz, Joseph Bethge, Haojin Yang, Christoph Meinel
- **Comment**: Code and Models available at https://github.com/Bartzi/kiss
- **Journal**: None
- **Summary**: Over the past few years, several new methods for scene text recognition have been proposed. Most of these methods propose novel building blocks for neural networks. These novel building blocks are specially tailored for the task of scene text recognition and can thus hardly be used in any other tasks. In this paper, we introduce a new model for scene text recognition that only consists of off-the-shelf building blocks for neural networks. Our model (KISS) consists of two ResNet based feature extractors, a spatial transformer, and a transformer. We train our model only on publicly available, synthetic training data and evaluate it on a range of scene text recognition benchmarks, where we reach state-of-the-art or competitive performance, although our model does not use methods like 2D-attention, or image rectification.



### A Promotion Method for Generation Error Based Video Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.08402v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08402v4)
- **Published**: 2019-11-19 17:18:26+00:00
- **Updated**: 2020-03-17 02:24:24+00:00
- **Authors**: Zhiguo Wang, Zhongliang Yang, Yu-Jin Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Surveillance video anomaly detection is to detect events that rarely or never happened in a certain scene. The generation error (GE)-based methods exhibit excellent performance on this task. They firstly train a generative neural network (GNN) to generate normal samples, then judge the samples with large GEs as anomalies. Almost all the GE-based methods utilize frame-level GEs to detect anomalies. However, anomalies generally occur in local areas, the frame-level GE introduces GEs of normal areas to anomaly discriminations, that brings two problems: i) The GE of normal areas reduces the anomaly saliency of the anomalous frame. ii) Different videos have different normal-GE-levels, thus it is hard to set a uniform threshold for all videos to detect anomalies. To address these problems, we propose a promotion method: utilize the maximum of block-level GEs on the frame to detect anomaly. Firstly, we calculate the block-level GEs at each position on the frame. Then, we utilize the maximum of the block-level GEs on the frame to detect anomalies. Based on the existed GNN models, experiments are carried out on multiple datasets. The results demonstrate the effectiveness of the proposed method and achieve state-of-the-art performance.



### Defective Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.08432v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08432v2)
- **Published**: 2019-11-19 17:56:22+00:00
- **Updated**: 2020-04-06 20:47:57+00:00
- **Authors**: Tiange Luo, Tianle Cai, Mengxiao Zhang, Siyu Chen, Di He, Liwei Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Robustness of convolutional neural networks (CNNs) has gained in importance on account of adversarial examples, i.e., inputs added as well-designed perturbations that are imperceptible to humans but can cause the model to predict incorrectly. Recent research suggests that the noises in adversarial examples break the textural structure, which eventually leads to wrong predictions. To mitigate the threat of such adversarial attacks, we propose defective convolutional networks that make predictions relying less on textural information but more on shape information by properly integrating defective convolutional layers into standard CNNs. The defective convolutional layers contain defective neurons whose activations are set to be a constant function. As defective neurons contain no information and are far different from standard neurons in its spatial neighborhood, the textural features cannot be accurately extracted, and so the model has to seek other features for classification, such as the shape. We show extensive evidence to justify our proposal and demonstrate that defective CNNs can defense against black-box attacks better than standard CNNs. In particular, they achieve state-of-the-art performance against transfer-based attacks without any adversarial training being applied.



### LNDb: A Lung Nodule Database on Computed Tomography
- **Arxiv ID**: http://arxiv.org/abs/1911.08434v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.08434v3)
- **Published**: 2019-11-19 17:58:59+00:00
- **Updated**: 2019-12-19 09:40:20+00:00
- **Authors**: João Pedrosa, Guilherme Aresta, Carlos Ferreira, Márcio Rodrigues, Patrícia Leitão, André Silva Carvalho, João Rebelo, Eduardo Negrão, Isabel Ramos, António Cunha, Aurélio Campilho
- **Comment**: None
- **Journal**: None
- **Summary**: Lung cancer is the deadliest type of cancer worldwide and late detection is the major factor for the low survival rate of patients. Low dose computed tomography has been suggested as a potential screening tool but manual screening is costly, time-consuming and prone to variability. This has fueled the development of automatic methods for the detection, segmentation and characterisation of pulmonary nodules but its application to clinical routine is challenging. In this study, a new database for the development and testing of pulmonary nodule computer-aided strategies is presented which intends to complement current databases by giving additional focus to radiologist variability and local clinical reality. State-of-the-art nodule detection, segmentation and characterization methods are tested and compared to manual annotations as well as collaborative strategies combining multiple radiologists and radiologists and computer-aided systems. It is shown that state-of-the-art methodologies can determine a patient's follow-up recommendation as accurately as a radiologist, though the nodule detection method used shows decreased performance in this database.



### Shared Visual Abstractions
- **Arxiv ID**: http://arxiv.org/abs/1912.04217v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.04217v1)
- **Published**: 2019-11-19 18:51:02+00:00
- **Updated**: 2019-11-19 18:51:02+00:00
- **Authors**: Tom White
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents abstract art created by neural networks and broadly recognizable across various computer vision systems. The existence of abstract forms that trigger specific labels independent of neural architecture or training set suggests convolutional neural networks build shared visual representations for the categories they understand. Computer vision classifiers encountering these drawings often respond with strong responses for specific labels - in extreme cases stronger than all examples from the validation set. By surveying human subjects we confirm that these abstract artworks are also broadly recognizable by people, suggesting visual representations triggered by these drawings are shared across human and computer vision systems.



### Action Recognition Using Volumetric Motion Representations
- **Arxiv ID**: http://arxiv.org/abs/1911.08511v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.08511v1)
- **Published**: 2019-11-19 19:13:57+00:00
- **Updated**: 2019-11-19 19:13:57+00:00
- **Authors**: Michael Peven, Gregory D. Hager, Austin Reiter
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional action recognition models are constructed around the paradigm of 2D perspective imagery. Though sophisticated time-series models have pushed the field forward, much of the information is still not exploited by confining the domain to 2D. In this work, we introduce a novel representation of motion as a voxelized 3D vector field and demonstrate how it can be used to improve performance of action recognition networks. This volumetric representation is a natural fit for 3D CNNs, and allows out-of-plane data augmentation techniques during training of these networks. Both the construction of this representation from RGB-D video and inference can be run in real time. We demonstrate superior results using this representation with our network design on the open-source NTU RGB+D dataset where it outperforms state-of-the-art on both of the defined evaluation metrics. Furthermore, we experimentally show how the out-of-plane augmentation techniques create viewpoint invariance and allow the model trained using this representation to generalize to unseen camera angles. Code is available here: https://github.com/mpeven/ntu_rgb.



### Deep Motion Blur Removal Using Noisy/Blurry Image Pairs
- **Arxiv ID**: http://arxiv.org/abs/1911.08541v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.08541v2)
- **Published**: 2019-11-19 20:13:58+00:00
- **Updated**: 2019-11-25 17:57:39+00:00
- **Authors**: Shuang Zhang, Ada Zhen, Robert L. Stevenson
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: Removing spatially variant motion blur from a blurry image is a challenging problem as blur sources are complicated and difficult to model accurately. Recent progress in deep neural networks suggests that kernel free single image deblurring can be efficiently performed, but questions about deblurring performance persist. Thus, we propose to restore a sharp image by fusing a pair of noisy/blurry images captured in a burst. Two neural network structures, DeblurRNN and DeblurMerger, are presented to exploit the pair of images in a sequential manner or parallel manner. To boost the training, gradient loss, adversarial loss and spectral normalization are leveraged. The training dataset that consists of pairs of noisy/blurry images and the corresponding ground truth sharp image is synthesized based on the benchmark dataset GOPRO. We evaluated the trained networks on a variety of synthetic datasets and real image pairs. The results demonstrate that the proposed approach outperforms the state-of-the-art both qualitatively and quantitatively.



### Superpixel Soup: Monocular Dense 3D Reconstruction of a Complex Dynamic Scene
- **Arxiv ID**: http://arxiv.org/abs/1911.09092v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09092v1)
- **Published**: 2019-11-19 20:16:06+00:00
- **Updated**: 2019-11-19 20:16:06+00:00
- **Authors**: Suryansh Kumar, Yuchao Dai, Hongdong Li
- **Comment**: 12 pages, 18 Figures, 2 Tables. Accepted for publication in IEEE,
  T-PAMI 2019 Journal. arXiv version is slightly different from the camera
  ready submission. arXiv admin note: text overlap with arXiv:1708.04398
- **Journal**: None
- **Summary**: This work addresses the task of dense 3D reconstruction of a complex dynamic scene from images. The prevailing idea to solve this task is composed of a sequence of steps and is dependent on the success of several pipelines in its execution. To overcome such limitations with the existing algorithm, we propose a unified approach to solve this problem. We assume that a dynamic scene can be approximated by numerous piecewise planar surfaces, where each planar surface enjoys its own rigid motion, and the global change in the scene between two frames is as-rigid-as-possible (ARAP). Consequently, our model of a dynamic scene reduces to a soup of planar structures and rigid motion of these local planar structures. Using planar over-segmentation of the scene, we reduce this task to solving a "3D jigsaw puzzle" problem. Hence, the task boils down to correctly assemble each rigid piece to construct a 3D shape that complies with the geometry of the scene under the ARAP assumption. Further, we show that our approach provides an effective solution to the inherent scale-ambiguity in structure-from-motion under perspective projection. We provide extensive experimental results and evaluation on several benchmark datasets. Quantitative comparison with competing approaches shows state-of-the-art performance.



### Cross-Class Relevance Learning for Temporal Concept Localization
- **Arxiv ID**: http://arxiv.org/abs/1911.08548v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08548v1)
- **Published**: 2019-11-19 20:31:04+00:00
- **Updated**: 2019-11-19 20:31:04+00:00
- **Authors**: Junwei Ma, Satya Krishna Gorti, Maksims Volkovs, Ilya Stanevich, Guangwei Yu
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel Cross-Class Relevance Learning approach for the task of temporal concept localization. Most localization architectures rely on feature extraction layers followed by a classification layer which outputs class probabilities for each segment. However, in many real-world applications classes can exhibit complex relationships that are difficult to model with this architecture. In contrast, we propose to incorporate target class and class-related features as input, and learn a pairwise binary model to predict general segment to class relevance. This facilitates learning of shared information between classes, and allows for arbitrary class-specific feature engineering. We apply this approach to the 3rd YouTube-8M Video Understanding Challenge together with other leading models, and achieve first place out of over 280 teams. In this paper we describe our approach and show some empirical results.



### Joint Super-Resolution and Alignment of Tiny Faces
- **Arxiv ID**: http://arxiv.org/abs/1911.08566v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08566v1)
- **Published**: 2019-11-19 20:39:49+00:00
- **Updated**: 2019-11-19 20:39:49+00:00
- **Authors**: Yu Yin, Joseph P. Robinson, Yulun Zhang, Yun Fu
- **Comment**: Accepted by AAAI 2020
- **Journal**: None
- **Summary**: Super-resolution (SR) and landmark localization of tiny faces are highly correlated tasks. On the one hand, landmark localization could obtain higher accuracy with faces of high-resolution (HR). On the other hand, face SR would benefit from prior knowledge of facial attributes such as landmarks. Thus, we propose a joint alignment and SR network to simultaneously detect facial landmarks and super-resolve tiny faces. More specifically, a shared deep encoder is applied to extract features for both tasks by leveraging complementary information. To exploit the representative power of the hierarchical encoder, intermediate layers of a shared feature extraction module are fused to form efficient feature representations. The fused features are then fed to task-specific modules to detect landmarks and super-resolve face images in parallel. Extensive experiments demonstrate that the proposed model significantly outperforms the state-of-the-art in both landmark localization and SR of faces. We show a large improvement for landmark localization of tiny faces (i.e., 16*16). Furthermore, the proposed framework yields comparable results for landmark localization on low-resolution (LR) faces (i.e., 64*64) to existing methods on HR (i.e., 256*256). As for SR, the proposed method recovers sharper edges and more details from LR face images than other state-of-the-art methods, which we demonstrate qualitatively and quantitatively.



### Mini Lesions Detection on Diabetic Retinopathy Images via Large Scale CNN Features
- **Arxiv ID**: http://arxiv.org/abs/1911.08588v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1911.08588v1)
- **Published**: 2019-11-19 21:06:50+00:00
- **Updated**: 2019-11-19 21:06:50+00:00
- **Authors**: Qilei Chen, Xinzi Sun, Ning Zhang, Yu Cao, Benyuan Liu
- **Comment**: diabetic retinopathy, mini lesion detection, FPN
- **Journal**: None
- **Summary**: Diabetic retinopathy (DR) is a diabetes complication that affects eyes. DR is a primary cause of blindness in working-age people and it is estimated that 3 to 4 million people with diabetes are blinded by DR every year worldwide. Early diagnosis have been considered an effective way to mitigate such problem. The ultimate goal of our research is to develop novel machine learning techniques to analyze the DR images generated by the fundus camera for automatically DR diagnosis. In this paper, we focus on identifying small lesions on DR fundus images. The results from our analysis, which include the lesion category and their exact locations in the image, can be used to facilitate the determination of DR severity (indicated by DR stages). Different from traditional object detection for natural images, lesion detection for fundus images have unique challenges. Specifically, the size of a lesion instance is usually very small, compared with the original resolution of the fundus images, making them diffcult to be detected. We analyze the lesion-vs-image scale carefully and propose a large-size feature pyramid network (LFPN) to preserve more image details for mini lesion instance detection. Our method includes an effective region proposal strategy to increase the sensitivity. The experimental results show that our proposed method is superior to the original feature pyramid network (FPN) method and Faster RCNN.



### Learning Stylized Character Expressions from Humans
- **Arxiv ID**: http://arxiv.org/abs/1911.08591v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1911.08591v1)
- **Published**: 2019-11-19 21:12:43+00:00
- **Updated**: 2019-11-19 21:12:43+00:00
- **Authors**: Deepali Aneja, Alex Colburn, Gary Faigin, Linda Shapiro, Barbara Mones
- **Comment**: 2017 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR) Women in Computer Vision (WiCV) Workshop Honolulu, Hawaii, USA, July
  21st - July 26th, 2017
- **Journal**: None
- **Summary**: We present DeepExpr, a novel expression transfer system from humans to multiple stylized characters via deep learning. We developed : 1) a data-driven perceptual model of facial expressions, 2) a novel stylized character data set with cardinal expression annotations : FERG (Facial Expression Research Group) - DB (added two new characters), and 3) . We evaluated our method on a set of retrieval tasks on our collected stylized character dataset of expressions. We have also shown that the ranking order predicted by the proposed features is highly correlated with the ranking order provided by a facial expression expert and Mechanical Turk (MT) experiments.



### CoopNet: Cooperative Convolutional Neural Network for Low-Power MCUs
- **Arxiv ID**: http://arxiv.org/abs/1911.08606v3
- **DOI**: 10.1109/ICECS46596.2019.8964993
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.08606v3)
- **Published**: 2019-11-19 21:47:23+00:00
- **Updated**: 2020-01-25 12:10:51+00:00
- **Authors**: Luca Mocerino, Andrea Calimera
- **Comment**: None
- **Journal**: 2019 26th IEEE International Conference on Electronics, Circuits
  and Systems (ICECS)
- **Summary**: Fixed-point quantization and binarization are two reduction methods adopted to deploy Convolutional Neural Networks (CNN) on end-nodes powered by low-power micro-controller units (MCUs). While most of the existing works use them as stand-alone optimizations, this work aims at demonstrating there is margin for a joint cooperation that leads to inferential engines with lower latency and higher accuracy. Called CoopNet, the proposed heterogeneous model is conceived, implemented and tested on off-the-shelf MCUs with small on-chip memory and few computational resources. Experimental results conducted on three different CNNs using as test-bench the low-power RISC core of the Cortex-M family by ARM validate the CoopNet proposal by showing substantial improvements w.r.t. designs where quantization and binarization are applied separately.



### Hybrid Composition with IdleBlock: More Efficient Networks for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1911.08609v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08609v1)
- **Published**: 2019-11-19 22:09:11+00:00
- **Updated**: 2019-11-19 22:09:11+00:00
- **Authors**: Bing Xu, Andrew Tulloch, Yunpeng Chen, Xiaomeng Yang, Lin Qiao
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new building block, IdleBlock, which naturally prunes connections within the block. To fully utilize the IdleBlock we break the tradition of monotonic design in state-of-the-art networks, and introducing hybrid composition with IdleBlock. We study hybrid composition on MobileNet v3 and EfficientNet-B0, two of the most efficient networks. Without any neural architecture search, the deeper "MobileNet v3" with hybrid composition design surpasses possibly all state-of-the-art image recognition network designed by human experts or neural architecture search algorithms. Similarly, the hybridized EfficientNet-B0 networks are more efficient than previous state-of-the-art networks with similar computation budgets. These results suggest a new simpler and more efficient direction for network design and neural architecture search.



### Attention Guided Anomaly Localization in Images
- **Arxiv ID**: http://arxiv.org/abs/1911.08616v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.08616v4)
- **Published**: 2019-11-19 22:28:17+00:00
- **Updated**: 2020-07-17 02:38:04+00:00
- **Authors**: Shashanka Venkataramanan, Kuan-Chuan Peng, Rajat Vikram Singh, Abhijit Mahalanobis
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: Anomaly localization is an important problem in computer vision which involves localizing anomalous regions within images with applications in industrial inspection, surveillance, and medical imaging. This task is challenging due to the small sample size and pixel coverage of the anomaly in real-world scenarios. Most prior works need to use anomalous training images to compute a class-specific threshold to localize anomalies. Without the need of anomalous training images, we propose Convolutional Adversarial Variational autoencoder with Guided Attention (CAVGA), which localizes the anomaly with a convolutional latent variable to preserve the spatial information. In the unsupervised setting, we propose an attention expansion loss where we encourage CAVGA to focus on all normal regions in the image. Furthermore, in the weakly-supervised setting we propose a complementary guided attention loss, where we encourage the attention map to focus on all normal regions while minimizing the attention map corresponding to anomalous regions in the image. CAVGA outperforms the state-of-the-art (SOTA) anomaly localization methods on MVTec Anomaly Detection (MVTAD), modified ShanghaiTech Campus (mSTC) and Large-scale Attention based Glaucoma (LAG) datasets in the unsupervised setting and when using only 2% anomalous images in the weakly-supervised setting. CAVGA also outperforms SOTA anomaly detection methods on the MNIST, CIFAR-10, Fashion-MNIST, MVTAD, mSTC and LAG datasets.



### Explanation vs Attention: A Two-Player Game to Obtain Attention for VQA
- **Arxiv ID**: http://arxiv.org/abs/1911.08618v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.08618v1)
- **Published**: 2019-11-19 22:30:13+00:00
- **Updated**: 2019-11-19 22:30:13+00:00
- **Authors**: Badri N. Patro, Anupriy, Vinay P. Namboodiri
- **Comment**: AAAI-2020(Accepted)
- **Journal**: None
- **Summary**: In this paper, we aim to obtain improved attention for a visual question answering (VQA) task. It is challenging to provide supervision for attention. An observation we make is that visual explanations as obtained through class activation mappings (specifically Grad-CAM) that are meant to explain the performance of various networks could form a means of supervision. However, as the distributions of attention maps and that of Grad-CAMs differ, it would not be suitable to directly use these as a form of supervision. Rather, we propose the use of a discriminator that aims to distinguish samples of visual explanation and attention maps. The use of adversarial training of the attention regions as a two-player game between attention and explanation serves to bring the distributions of attention maps and visual explanations closer. Significantly, we observe that providing such a means of supervision also results in attention maps that are more closely related to human attention resulting in a substantial improvement over baseline stacked attention network (SAN) models. It also results in a good improvement in rank correlation metric on the VQA task. This method can also be combined with recent MCB based methods and results in consistent improvement. We also provide comparisons with other means for learning distributions such as based on Correlation Alignment (Coral), Maximum Mean Discrepancy (MMD) and Mean Square Error (MSE) losses and observe that the adversarial loss outperforms the other forms of learning the attention maps. Visualization of the results also confirms our hypothesis that attention maps improve using this form of supervision.



### Open Cross-Domain Visual Search
- **Arxiv ID**: http://arxiv.org/abs/1911.08621v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08621v2)
- **Published**: 2019-11-19 22:42:01+00:00
- **Updated**: 2020-07-28 13:36:23+00:00
- **Authors**: William Thong, Pascal Mettes, Cees G. M. Snoek
- **Comment**: Accepted at Computer Vision and Image Understanding (CVIU)
- **Journal**: None
- **Summary**: This paper addresses cross-domain visual search, where visual queries retrieve category samples from a different domain. For example, we may want to sketch an airplane and retrieve photographs of airplanes. Despite considerable progress, the search occurs in a closed setting between two pre-defined domains. In this paper, we make the step towards an open setting where multiple visual domains are available. This notably translates into a search between any pair of domains, from a combination of domains or within multiple domains. We introduce a simple -- yet effective -- approach. We formulate the search as a mapping from every visual domain to a common semantic space, where categories are represented by hyperspherical prototypes. Open cross-domain visual search is then performed by searching in the common semantic space, regardless of which domains are used as source or target. Domains are combined in the common space to search from or within multiple domains simultaneously. A separate training of every domain-specific mapping function enables an efficient scaling to any number of domains without affecting the search performance. We empirically illustrate our capability to perform open cross-domain visual search in three different scenarios. Our approach is competitive with respect to existing closed settings, where we obtain state-of-the-art results on several benchmarks for three sketch-based search tasks.



### CUP: Cluster Pruning for Compressing Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.08630v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08630v1)
- **Published**: 2019-11-19 23:44:59+00:00
- **Updated**: 2019-11-19 23:44:59+00:00
- **Authors**: Rahul Duggal, Cao Xiao, Richard Vuduc, Jimeng Sun
- **Comment**: None
- **Journal**: None
- **Summary**: We propose Cluster Pruning (CUP) for compressing and accelerating deep neural networks. Our approach prunes similar filters by clustering them based on features derived from both the incoming and outgoing weight connections. With CUP, we overcome two limitations of prior work-(1) non-uniform pruning: CUP can efficiently determine the ideal number of filters to prune in each layer of a neural network. This is in contrast to prior methods that either prune all layers uniformly or otherwise use resource-intensive methods such as manual sensitivity analysis or reinforcement learning to determine the ideal number. (2) Single-shot operation: We extend CUP to CUP-SS (for CUP single shot) whereby pruning is integrated into the initial training phase itself. This leads to large savings in training time compared to traditional pruning pipelines. Through extensive evaluation on multiple datasets (MNIST, CIFAR-10, and Imagenet) and models(VGG-16, Resnets-18/34/56) we show that CUP outperforms recent state of the art. Specifically, CUP-SS achieves 2.2x flops reduction for a Resnet-50 model trained on Imagenet while staying within 0.9% top-5 accuracy. It saves over 14 hours in training time with respect to the original Resnet-50. The code to reproduce results is available.



