# Arxiv Papers in cs.CV on 2019-11-29
### Enhancing Passive Non-Line-of-Sight Imaging Using Polarization Cues
- **Arxiv ID**: http://arxiv.org/abs/1911.12906v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.12906v1)
- **Published**: 2019-11-29 00:22:31+00:00
- **Updated**: 2019-11-29 00:22:31+00:00
- **Authors**: Kenichiro Tanaka, Yasuhiro Mukaigawa, Achuta Kadambi
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a method of passive non-line-of-sight (NLOS) imaging using polarization cues. A key observation is that the oblique light has a different polarimetric signal. It turns out this effect is due to the polarization axis rotation, a phenomena which can be used to better condition the light transport matrix for non-line-of-sight imaging. Our analysis and results show that the use of a polarizer in front of the camera is not only a separate technique, but it can be seen as an enhancement technique for more advanced forms of passive NLOS imaging. For example, this paper shows that polarization can enhance passive NLOS imaging both with and without occluders. In all tested cases, despite the light attenuation from polarization optics, recovery of the occluded images is improved.



### Unlocking the Full Potential of Small Data with Diverse Supervision
- **Arxiv ID**: http://arxiv.org/abs/1911.12911v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12911v3)
- **Published**: 2019-11-29 00:56:06+00:00
- **Updated**: 2021-04-26 23:24:04+00:00
- **Authors**: Ziqi Pang, Zhiyuan Hu, Pavel Tokmakov, Yu-Xiong Wang, Martial Hebert
- **Comment**: Learning from Limited and Imperfect Data (L2ID) Workshop @ CVPR 2021
- **Journal**: None
- **Summary**: Virtually all of deep learning literature relies on the assumption of large amounts of available training data. Indeed, even the majority of few-shot learning methods rely on a large set of "base classes" for pretraining. This assumption, however, does not always hold. For some tasks, annotating a large number of classes can be infeasible, and even collecting the images themselves can be a challenge in some scenarios. In this paper, we study this problem and call it "Small Data" setting, in contrast to "Big Data". To unlock the full potential of small data, we propose to augment the models with annotations for other related tasks, thus increasing their generalization abilities. In particular, we use the richly annotated scene parsing dataset ADE20K to construct our realistic Long-tail Recognition with Diverse Supervision (LRDS) benchmark by splitting the object categories into head and tail based on their distribution. Following the standard few-shot learning protocol, we use the head classes for representation learning and the tail classes for evaluation. Moreover, we further subsample the head categories and images to generate two novel settings which we call "Scarce-Class" and "Scarce-Image", respectively corresponding to the shortage of samples for rare classes and training images. Finally, we analyze the effect of applying various additional supervision sources under the proposed settings. Our experiments demonstrate that densely labeling a small set of images can indeed largely remedy the small data constraints.



### Learning Semantic Correspondence Exploiting an Object-level Prior
- **Arxiv ID**: http://arxiv.org/abs/1911.12914v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12914v2)
- **Published**: 2019-11-29 01:13:11+00:00
- **Updated**: 2020-07-21 06:29:40+00:00
- **Authors**: Junghyup Lee, Dohyung Kim, Wonkyung Lee, Jean Ponce, Bumsub Ham
- **Comment**: Accepted to TPAMI. arXiv admin note: substantial text overlap with
  arXiv:1904.01810
- **Journal**: None
- **Summary**: We address the problem of semantic correspondence, that is, establishing a dense flow field between images depicting different instances of the same object or scene category. We propose to use images annotated with binary foreground masks and subjected to synthetic geometric deformations to train a convolutional neural network (CNN) for this task. Using these masks as part of the supervisory signal provides an object-level prior for the semantic correspondence task and offers a good compromise between semantic flow methods, where the amount of training data is limited by the cost of manually selecting point correspondences, and semantic alignment ones, where the regression of a single global geometric transformation between images may be sensitive to image-specific details such as background clutter. We propose a new CNN architecture, dubbed SFNet, which implements this idea. It leverages a new and differentiable version of the argmax function for end-to-end training, with a loss that combines mask and flow consistency with smoothness terms. Experimental results demonstrate the effectiveness of our approach, which significantly outperforms the state of the art on standard benchmarks.



### Deep Object Co-segmentation via Spatial-Semantic Network Modulation
- **Arxiv ID**: http://arxiv.org/abs/1911.12950v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.12950v1)
- **Published**: 2019-11-29 04:40:30+00:00
- **Updated**: 2019-11-29 04:40:30+00:00
- **Authors**: Kaihua Zhang, Jin Chen, Bo Liu, Qingshan Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Object co-segmentation is to segment the shared objects in multiple relevant images, which has numerous applications in computer vision. This paper presents a spatial and semantic modulated deep network framework for object co-segmentation. A backbone network is adopted to extract multi-resolution image features. With the multi-resolution features of the relevant images as input, we design a spatial modulator to learn a mask for each image. The spatial modulator captures the correlations of image feature descriptors via unsupervised learning. The learned mask can roughly localize the shared foreground object while suppressing the background. For the semantic modulator, we model it as a supervised image classification task. We propose a hierarchical second-order pooling module to transform the image features for classification use. The outputs of the two modulators manipulate the multi-resolution features by a shift-and-scale operation so that the features focus on segmenting co-object regions. The proposed model is trained end-to-end without any intricate post-processing. Extensive experiments on four image co-segmentation benchmark datasets demonstrate the superior accuracy of the proposed method compared to state-of-the-art methods.



### Correlation-aware Adversarial Domain Adaptation and Generalization
- **Arxiv ID**: http://arxiv.org/abs/1911.12983v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.12983v1)
- **Published**: 2019-11-29 07:22:15+00:00
- **Updated**: 2019-11-29 07:22:15+00:00
- **Authors**: Mohammad Mahfujur Rahman, Clinton Fookes, Mahsa Baktashmotlagh, Sridha Sridharan
- **Comment**: Preprint submitted to Pattern Recognition, Accepted in Pattern
  Recognition
- **Journal**: None
- **Summary**: Domain adaptation (DA) and domain generalization (DG) have emerged as a solution to the domain shift problem where the distribution of the source and target data is different. The task of DG is more challenging than DA as the target data is totally unseen during the training phase in DG scenarios. The current state-of-the-art employs adversarial techniques, however, these are rarely considered for the DG problem. Furthermore, these approaches do not consider correlation alignment which has been proven highly beneficial for minimizing domain discrepancy. In this paper, we propose a correlation-aware adversarial DA and DG framework where the features of the source and target data are minimized using correlation alignment along with adversarial learning. Incorporating the correlation alignment module along with adversarial learning helps to achieve a more domain agnostic model due to the improved ability to reduce domain discrepancy with unlabeled target data more effectively. Experiments on benchmark datasets serve as evidence that our proposed method yields improved state-of-the-art performance.



### Online Structured Sparsity-based Moving Object Detection from Satellite Videos
- **Arxiv ID**: http://arxiv.org/abs/1911.12989v3
- **DOI**: 10.1109/TGRS.2020.2976855
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12989v3)
- **Published**: 2019-11-29 07:54:14+00:00
- **Updated**: 2019-12-10 10:56:11+00:00
- **Authors**: Junpeng Zhang, Xiuping Jia, Jiankun Hu, Jocelyn Chanussot
- **Comment**: None
- **Journal**: None
- **Summary**: Inspired by the recent developments in computer vision, low-rank and structured sparse matrix decomposition can be potentially be used for extract moving objects in satellite videos. This set of approaches seeks for rank minimization on the background that typically requires batch-based optimization over a sequence of frames, which causes delays in processing and limits their applications. To remedy this delay, we propose an Online Low-rank and Structured Sparse Decomposition (O-LSD). O-LSD reformulates the batch-based low-rank matrix decomposition with the structured sparse penalty to its equivalent frame-wise separable counterpart, which then defines a stochastic optimization problem for online subspace basis estimation. In order to promote online processing, O-LSD conducts the foreground and background separation and the subspace basis update alternatingly for every frame in a video. We also show the convergence of O-LSD theoretically. Experimental results on two satellite videos demonstrate the performance of O-LSD in term of accuracy and time consumption is comparable with the batch-based approaches with significantly reduced delay in processing.



### Semi-Relaxed Quantization with DropBits: Training Low-Bit Neural Networks via Bit-wise Regularization
- **Arxiv ID**: http://arxiv.org/abs/1911.12990v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.12990v3)
- **Published**: 2019-11-29 07:58:43+00:00
- **Updated**: 2021-09-07 07:03:39+00:00
- **Authors**: Jung Hyun Lee, Jihun Yun, Sung Ju Hwang, Eunho Yang
- **Comment**: New submission with another link
- **Journal**: None
- **Summary**: Network quantization, which aims to reduce the bit-lengths of the network weights and activations, has emerged as one of the key ingredients to reduce the size of neural networks for their deployments to resource-limited devices. In order to overcome the nature of transforming continuous activations and weights to discrete ones, recent study called Relaxed Quantization (RQ) [Louizos et al. 2019] successfully employ the popular Gumbel-Softmax that allows this transformation with efficient gradient-based optimization. However, RQ with this Gumbel-Softmax relaxation still suffers from bias-variance trade-off depending on the temperature parameter of Gumbel-Softmax. To resolve the issue, we propose a novel method, Semi-Relaxed Quantization (SRQ) that uses multi-class straight-through estimator to effectively reduce the bias and variance, along with a new regularization technique, DropBits that replaces dropout regularization to randomly drop the bits instead of neurons to further reduce the bias of the multi-class straight-through estimator in SRQ. As a natural extension of DropBits, we further introduce the way of learning heterogeneous quantization levels to find proper bit-length for each layer using DropBits. We experimentally validate our method on various benchmark datasets and network architectures, and also support the quantized lottery ticket hypothesis: learning heterogeneous quantization levels outperforms the case using the same but fixed quantization levels from scratch.



### Investigations on the inference optimization techniques and their impact on multiple hardware platforms for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.12993v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.12993v1)
- **Published**: 2019-11-29 08:08:28+00:00
- **Updated**: 2019-11-29 08:08:28+00:00
- **Authors**: Sethu Hareesh Kolluru
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, the task of pixel-wise semantic segmentation in the context of self-driving with a goal to reduce the inference time is explored. Fully Convolutional Network (FCN-8s, FCN-16s, and FCN-32s) with a VGG16 encoder architecture and skip connections is trained and validated on the Cityscapes dataset. Numerical investigations are carried out for several inference optimization techniques built into TensorFlow and TensorRT to quantify their impact on the inference time and network size. Finally, the trained network is ported on to an embedded platform (Nvidia Jetson TX1) and the inference time, as well as the total energy consumed for inference across hardware platforms, are compared.



### Collaborative Attention Network for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1911.13008v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.13008v2)
- **Published**: 2019-11-29 09:18:20+00:00
- **Updated**: 2020-09-08 06:46:22+00:00
- **Authors**: Wenpeng Li, Yongli Sun, Jinjun Wang, Han Xu, Xiangru Yang, Long Cui
- **Comment**: None
- **Journal**: None
- **Summary**: Jointly utilizing global and local features to improve model accuracy is becoming a popular approach for the person re-identification (ReID) problem, because previous works using global features alone have very limited capacity at extracting discriminative local patterns in the obtained feature representation. Existing works that attempt to collect local patterns either explicitly slice the global feature into several local pieces in a handcrafted way, or apply the attention mechanism to implicitly infer the importance of different local regions. In this paper, we show that by explicitly learning the importance of small local parts and part combinations, we can further improve the final feature representation for Re-ID. Specifically, we first separate the global feature into multiple local slices at different scale with a proposed multi-branch structure. Then we introduce the Collaborative Attention Network (CAN) to automatically learn the combination of features from adjacent slices. In this way, the combination keeps the intrinsic relation between adjacent features across local regions and scales, without losing information by partitioning the global features. Experiment results on several widely-used public datasets including Market-1501, DukeMTMC-ReID and CUHK03 prove that the proposed method outperforms many existing state-of-the-art methods.



### Indirect Local Attacks for Context-aware Semantic Segmentation Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.13038v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.13038v2)
- **Published**: 2019-11-29 10:28:11+00:00
- **Updated**: 2019-12-02 09:21:00+00:00
- **Authors**: Krishna Kanth Nakka, Mathieu Salzmann
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, deep networks have achieved impressive semantic segmentation performance, in particular thanks to their use of larger contextual information. In this paper, we show that the resulting networks are sensitive not only to global attacks, where perturbations affect the entire input image, but also to indirect local attacks where perturbations are confined to a small image region that does not overlap with the area that we aim to fool. To this end, we introduce several indirect attack strategies, including adaptive local attacks, aiming to find the best image location to perturb, and universal local attacks. Furthermore, we propose attack detection techniques both for the global image level and to obtain a pixel-wise localization of the fooled regions. Our results are unsettling: Because they exploit a larger context, more accurate semantic segmentation networks are more sensitive to indirect local attacks.



### Learning Structured Representations of Spatial and Interactive Dynamics for Trajectory Prediction in Crowded Scenes
- **Arxiv ID**: http://arxiv.org/abs/1911.13044v6
- **DOI**: 10.1109/LRA.2020.3047778
- **Categories**: **cs.LG**, cs.CV, cs.MA, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.13044v6)
- **Published**: 2019-11-29 10:42:10+00:00
- **Updated**: 2021-01-02 13:24:01+00:00
- **Authors**: Todor Davchev, Michael Burke, Subramanian Ramamoorthy
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters 2021
- **Summary**: Context plays a significant role in the generation of motion for dynamic agents in interactive environments. This work proposes a modular method that utilises a learned model of the environment for motion prediction. This modularity explicitly allows for unsupervised adaptation of trajectory prediction models to unseen environments and new tasks by relying on unlabelled image data only. We model both the spatial and dynamic aspects of a given environment alongside the per agent motions. This results in more informed motion prediction and allows for performance comparable to the state-of-the-art. We highlight the model's prediction capability using a benchmark pedestrian prediction problem and a robot manipulation task and show that we can transfer the predictor across these tasks in a completely unsupervised way. The proposed approach allows for robust and label efficient forward modelling, and relaxes the need for full model re-training in new environments.



### Blockwisely Supervised Neural Architecture Search with Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/1911.13053v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1911.13053v2)
- **Published**: 2019-11-29 11:00:30+00:00
- **Updated**: 2020-03-06 06:08:31+00:00
- **Authors**: Changlin Li, Jiefeng Peng, Liuchun Yuan, Guangrun Wang, Xiaodan Liang, Liang Lin, Xiaojun Chang
- **Comment**: To be appear in CVPR 2020. We achieve a state-of-the-art 78.4% top-1
  accuracy on ImageNet in a mobile setting, which is about a 2.1% gain over
  EfficientNet-B0
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS), aiming at automatically designing network architectures by machines, is hoped and expected to bring about a new revolution in machine learning. Despite these high expectation, the effectiveness and efficiency of existing NAS solutions are unclear, with some recent works going so far as to suggest that many existing NAS solutions are no better than random architecture selection. The inefficiency of NAS solutions may be attributed to inaccurate architecture evaluation. Specifically, to speed up NAS, recent works have proposed under-training different candidate architectures in a large search space concurrently by using shared network parameters; however, this has resulted in incorrect architecture ratings and furthered the ineffectiveness of NAS.   In this work, we propose to modularize the large search space of NAS into blocks to ensure that the potential candidate architectures are fully trained; this reduces the representation shift caused by the shared parameters and leads to the correct rating of the candidates. Thanks to the block-wise search, we can also evaluate all of the candidate architectures within a block. Moreover, we find that the knowledge of a network model lies not only in the network parameters but also in the network architecture. Therefore, we propose to distill the neural architecture (DNA) knowledge from a teacher model as the supervision to guide our block-wise architecture search, which significantly improves the effectiveness of NAS. Remarkably, the capacity of our searched architecture has exceeded the teacher model, demonstrating the practicability and scalability of our method. Finally, our method achieves a state-of-the-art 78.4\% top-1 accuracy on ImageNet in a mobile setting, which is about a 2.1\% gain over EfficientNet-B0. All of our searched models along with the evaluation code are available online.



### Detecting anthropogenic cloud perturbations with deep learning
- **Arxiv ID**: http://arxiv.org/abs/1911.13061v1
- **DOI**: None
- **Categories**: **physics.ao-ph**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.13061v1)
- **Published**: 2019-11-29 11:22:48+00:00
- **Updated**: 2019-11-29 11:22:48+00:00
- **Authors**: Duncan Watson-Parris, Samuel Sutherland, Matthew Christensen, Anthony Caterini, Dino Sejdinovic, Philip Stier
- **Comment**: Awarded Best Paper and Spotlight Oral at Climate Change: How Can AI
  Help? (Workshop) at International Conference on Machine Learning (ICML), Long
  Beach, California, 2019
- **Journal**: None
- **Summary**: One of the most pressing questions in climate science is that of the effect of anthropogenic aerosol on the Earth's energy balance. Aerosols provide the `seeds' on which cloud droplets form, and changes in the amount of aerosol available to a cloud can change its brightness and other physical properties such as optical thickness and spatial extent. Clouds play a critical role in moderating global temperatures and small perturbations can lead to significant amounts of cooling or warming. Uncertainty in this effect is so large it is not currently known if it is negligible, or provides a large enough cooling to largely negate present-day warming by CO2. This work uses deep convolutional neural networks to look for two particular perturbations in clouds due to anthropogenic aerosol and assess their properties and prevalence, providing valuable insights into their climatic effects.



### Attributional Robustness Training using Input-Gradient Spatial Alignment
- **Arxiv ID**: http://arxiv.org/abs/1911.13073v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.13073v4)
- **Published**: 2019-11-29 12:08:41+00:00
- **Updated**: 2020-07-18 16:07:35+00:00
- **Authors**: Mayank Singh, Nupur Kumari, Puneet Mangla, Abhishek Sinha, Vineeth N Balasubramanian, Balaji Krishnamurthy
- **Comment**: ECCV 2020, Code at
  https://github.com/nupurkmr9/Attributional-Robustness
- **Journal**: None
- **Summary**: Interpretability is an emerging area of research in trustworthy machine learning. Safe deployment of machine learning system mandates that the prediction and its explanation be reliable and robust. Recently, it has been shown that the explanations could be manipulated easily by adding visually imperceptible perturbations to the input while keeping the model's prediction intact. In this work, we study the problem of attributional robustness (i.e. models having robust explanations) by showing an upper bound for attributional vulnerability in terms of spatial correlation between the input image and its explanation map. We propose a training methodology that learns robust features by minimizing this upper bound using soft-margin triplet loss. Our methodology of robust attribution training (\textit{ART}) achieves the new state-of-the-art attributional robustness measure by a margin of $\approx$ 6-18 $\%$ on several standard datasets, ie. SVHN, CIFAR-10 and GTSRB. We further show the utility of the proposed robust training technique (\textit{ART}) in the downstream task of weakly supervised object localization by achieving the new state-of-the-art performance on CUB-200 dataset.



### Weakly Supervised Cell Instance Segmentation by Propagating from Detection Response
- **Arxiv ID**: http://arxiv.org/abs/1911.13077v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1911.13077v1)
- **Published**: 2019-11-29 12:29:15+00:00
- **Updated**: 2019-11-29 12:29:15+00:00
- **Authors**: Kazuya Nishimura, Dai Fei Elmer Ker, Ryoma Bise
- **Comment**: 9 pages, 3 figures, Accepted in MICCAI 2019
- **Journal**: None
- **Summary**: Cell shape analysis is important in biomedical research. Deep learning methods may perform to segment individual cells if they use sufficient training data that the boundary of each cell is annotated. However, it is very time-consuming for preparing such detailed annotation for many cell culture conditions. In this paper, we propose a weakly supervised method that can segment individual cell regions who touch each other with unclear boundaries in dense conditions without the training data for cell regions. We demonstrated the efficacy of our method using several data-set including multiple cell types captured by several types of microscopy. Our method achieved the highest accuracy compared with several conventional methods. In addition, we demonstrated that our method can perform without any annotation by using fluorescence images that cell nuclear were stained as training data. Code is publicly available in "https://github.com/naivete5656/WSISPDR".



### Color inference from semantic labeling for person search in videos
- **Arxiv ID**: http://arxiv.org/abs/1911.13114v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.13114v2)
- **Published**: 2019-11-29 14:07:08+00:00
- **Updated**: 2020-04-06 22:06:36+00:00
- **Authors**: Jules Simon, Guillaume-Alexandre Bilodeau, David Steele, Harshad Mahadik
- **Comment**: 8 pages, 7 figures ICIAR 2020
- **Journal**: None
- **Summary**: We propose an explainable model to generate semantic color labels for person search. In this context, persons are described from their semantic parts, such as hat, shirt, etc. Person search consists in looking for people based on these descriptions. In this work, we aim to improve the accuracy of color labels for people. Our goal is to handle the high variability of human perception. Existing solutions are based on hand-crafted features or learnt features that are not explainable. Moreover most of them only focus on a limited set of colors. We propose a method based on binary search trees and a large peer-labelled color name dataset. This allows us to synthesize the human perception of colors. Using semantic segmentation and our color labeling method, we label segments of pedestrians with their associated colors. We evaluate our solution on person search on datasets such as PCN, and show a precision as high as 80.4%.



### Sanity Checks for Saliency Metrics
- **Arxiv ID**: http://arxiv.org/abs/1912.01451v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.01451v1)
- **Published**: 2019-11-29 14:30:56+00:00
- **Updated**: 2019-11-29 14:30:56+00:00
- **Authors**: Richard Tomsett, Dan Harborne, Supriyo Chakraborty, Prudhvi Gurram, Alun Preece
- **Comment**: Accepted for publication at the Thirty Fourth AAAI conference on
  Artificial Intelligence (AAAI-20)
- **Journal**: None
- **Summary**: Saliency maps are a popular approach to creating post-hoc explanations of image classifier outputs. These methods produce estimates of the relevance of each pixel to the classification output score, which can be displayed as a saliency map that highlights important pixels. Despite a proliferation of such methods, little effort has been made to quantify how good these saliency maps are at capturing the true relevance of the pixels to the classifier output (i.e. their "fidelity"). We therefore investigate existing metrics for evaluating the fidelity of saliency methods (i.e. saliency metrics). We find that there is little consistency in the literature in how such metrics are calculated, and show that such inconsistencies can have a significant effect on the measured fidelity. Further, we apply measures of reliability developed in the psychometric testing literature to assess the consistency of saliency metrics when applied to individual saliency maps. Our results show that saliency metrics can be statistically unreliable and inconsistent, indicating that comparative rankings between saliency methods generated using such metrics can be untrustworthy.



### SketchZooms: Deep multi-view descriptors for matching line drawings
- **Arxiv ID**: http://arxiv.org/abs/1912.05019v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.05019v2)
- **Published**: 2019-11-29 14:31:33+00:00
- **Updated**: 2021-01-02 19:50:29+00:00
- **Authors**: Pablo Navarro, José Ignacio Orlando, Claudio Delrieux, Emmanuel Iarussi
- **Comment**: None
- **Journal**: None
- **Summary**: Finding point-wise correspondences between images is a long-standing problem in image analysis. This becomes particularly challenging for sketch images, due to the varying nature of human drawing style, projection distortions and viewport changes. In this paper we present the first attempt to obtain a learned descriptor for dense registration in line drawings. Based on recent deep learning techniques for corresponding photographs, we designed descriptors to locally match image pairs where the object of interest belongs to the same semantic category, yet still differ drastically in shape, form, and projection angle. To this end, we have specifically crafted a data set of synthetic sketches using non-photorealistic rendering over a large collection of part-based registered 3D models. After training, a neural network generates descriptors for every pixel in an input image, which are shown to generalize correctly in unseen sketches hand-drawn by humans. We evaluate our method against a baseline of correspondences data collected from expert designers, in addition to comparisons with other descriptors that have been proven effective in sketches. Code, data and further resources will be publicly released by the time of publication.



### An adaptive and fully automatic method for estimating the 3D position of bendable instruments using endoscopic images
- **Arxiv ID**: http://arxiv.org/abs/1911.13125v1
- **DOI**: 10.1002/rcs.1812
- **Categories**: **eess.IV**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1911.13125v1)
- **Published**: 2019-11-29 14:40:13+00:00
- **Updated**: 2019-11-29 14:40:13+00:00
- **Authors**: Paolo Cabras, Florent Nageotte, Philippe Zanne, Christophe Doignon
- **Comment**: The International Journal of Medical Robotics and Computer Assisted
  Surgery, John Wiley & Sons, Inc., 2017
- **Journal**: None
- **Summary**: Background. Flexible bendable instruments are key tools for performing surgical endoscopy. Being able to measure the 3D position of such instruments can be useful for various tasks, such as controlling automatically robotized instruments and analyzing motions. Methods. We propose an automatic method to infer the 3D pose of a single bending section instrument, using only the images provided by a monocular camera embedded at the tip of the endoscope. The proposed method relies on colored markers attached onto the bending section. The image of the instrument is segmented using a graph-based method and the corners of the markers are extracted by detecting the color transition along B{\'e}zier curves fitted on edge points. These features are accurately located and then used to estimate the 3D pose of the instrument using an adaptive model that allows to take into account the mechanical play between the instrument and its housing channel. Results. The feature extraction method provides good localization of markers corners with images of in vivo environment despite sensor saturation due to strong lighting. The RMS error on the estimation of the tip position of the instrument for laboratory experiments was 2.1, 1.96, 3.18 mm in the x, y and z directions respectively. Qualitative analysis in the case of in vivo images shows the ability to correctly estimate the 3D position of the instrument tip during real motions. Conclusions. The proposed method provides an automatic and accurate estimation of the 3D position of the tip of a bendable instrument in realistic conditions, where standard approaches fail.



### Radon Sobolev Variational Auto-Encoders
- **Arxiv ID**: http://arxiv.org/abs/1911.13135v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1911.13135v3)
- **Published**: 2019-11-29 15:02:28+00:00
- **Updated**: 2021-04-14 18:08:35+00:00
- **Authors**: Gabriel Turinici
- **Comment**: None
- **Journal**: None
- **Summary**: The quality of generative models (such as Generative adversarial networks and Variational Auto-Encoders) depends heavily on the choice of a good probability distance. However some popular metrics like the Wasserstein or the Sliced Wasserstein distances, the Jensen-Shannon divergence, the Kullback-Leibler divergence, lack convenient properties such as (geodesic) convexity, fast evaluation and so on. To address these shortcomings, we introduce a class of distances that have built-in convexity. We investigate the relationship with some known paradigms (sliced distances - a synonym for Radon distances -, reproducing kernel Hilbert spaces, energy distances). The distances are shown to possess fast implementations and are included in an adapted Variational Auto-Encoder termed Radon Sobolev Variational Auto-Encoder (RS-VAE) which produces high quality results on standard generative datasets.   Keywords: Variational Auto-Encoder; Generative model; Sobolev spaces; Radon Sobolev Variational Auto-Encoder;



### Deep autofocus with cone-beam CT consistency constraint
- **Arxiv ID**: http://arxiv.org/abs/1911.13162v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.13162v3)
- **Published**: 2019-11-29 15:54:38+00:00
- **Updated**: 2019-12-04 21:43:50+00:00
- **Authors**: Alexander Preuhs, Michael Manhart, Philipp Roser, Bernhard Stimpel, Christopher Syben, Marios Psychogios, Markus Kowarschik, Andreas Maier
- **Comment**: Accepted at BVM 2020, review score under Top-6 of the conference
- **Journal**: None
- **Summary**: High quality reconstruction with interventional C-arm cone-beam computed tomography (CBCT) requires exact geometry information. If the geometry information is corrupted, e. g., by unexpected patient or system movement, the measured signal is misplaced in the backprojection operation. With prolonged acquisition times of interventional C-arm CBCT the likelihood of rigid patient motion increases. To adapt the backprojection operation accordingly, a motion estimation strategy is necessary. Recently, a novel learning-based approach was proposed, capable of compensating motions within the acquisition plane. We extend this method by a CBCT consistency constraint, which was proven to be efficient for motions perpendicular to the acquisition plane. By the synergistic combination of these two measures, in and out-plane motion is well detectable, achieving an average artifact suppression of 93 [percent]. This outperforms the entropy-based state-of-the-art autofocus measure which achieves on average an artifact suppression of 54 [percent].



### CAGNet: Content-Aware Guidance for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.13168v2
- **DOI**: 10.1016/j.patcog.2020.107303
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.13168v2)
- **Published**: 2019-11-29 16:03:47+00:00
- **Updated**: 2020-04-04 20:11:23+00:00
- **Authors**: Sina Mohammadi, Mehrdad Noori, Ali Bahri, Sina Ghofrani Majelan, Mohammad Havaei
- **Comment**: 25 pages, 10 figures, 5 tables, Accepted by Elsevier, Pattern
  Recognition
- **Journal**: Pattern Recognition, Volume 103, 2020, 107303, ISSN 0031-3203
- **Summary**: Beneficial from Fully Convolutional Neural Networks (FCNs), saliency detection methods have achieved promising results. However, it is still challenging to learn effective features for detecting salient objects in complicated scenarios, in which i) non-salient regions may have "salient-like" appearance; ii) the salient objects may have different-looking regions. To handle these complex scenarios, we propose a Feature Guide Network which exploits the nature of low-level and high-level features to i) make foreground and background regions more distinct and suppress the non-salient regions which have "salient-like" appearance; ii) assign foreground label to different-looking salient regions. Furthermore, we utilize a Multi-scale Feature Extraction Module (MFEM) for each level of abstraction to obtain multi-scale contextual information. Finally, we design a loss function which outperforms the widely-used Cross-entropy loss. By adopting four different pre-trained models as the backbone, we prove that our method is very general with respect to the choice of the backbone model. Experiments on five challenging datasets demonstrate that our method achieves the state-of-the-art performance in terms of different evaluation metrics. Additionally, our approach contains fewer parameters than the existing ones, does not need any post-processing, and runs fast at a real-time speed of 28 FPS when processing a 480 x 480 image.



### Learning from Irregularly Sampled Data for Endomicroscopy Super-resolution: A Comparative Study of Sparse and Dense Approaches
- **Arxiv ID**: http://arxiv.org/abs/1911.13169v1
- **DOI**: 10.1007/s11548-020-02170-7
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.13169v1)
- **Published**: 2019-11-29 16:04:38+00:00
- **Updated**: 2019-11-29 16:04:38+00:00
- **Authors**: Agnieszka Barbara Szczotka, Dzhoshkun Ismail Shakir, DanieleRavi, Matthew J. Clarkson, Stephen P. Pereira, Tom Vercauteren
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: Probe-based Confocal Laser Endomicroscopy (pCLE) enables performing an optical biopsy, providing real-time microscopic images, via a probe. pCLE probes consist of multiple optical fibres arranged in a bundle, which taken together generate signals in an irregularly sampled pattern. Current pCLE reconstruction is based on interpolating irregular signals onto an over-sampled Cartesian grid, using a naive linear interpolation. It was shown that Convolutional Neural Networks (CNNs) could improve pCLE image quality. Although classical CNNs were applied to pCLE, input data were limited to reconstructed images in contrast to irregular data produced by pCLE. Methods: We compare pCLE reconstruction and super-resolution (SR) methods taking irregularly sampled or reconstructed pCLE images as input. We also propose to embed a Nadaraya-Watson (NW) kernel regression into the CNN framework as a novel trainable CNN layer. Using the NW layer and exemplar-based super-resolution, we design an NWNetSR architecture that allows for reconstructing high-quality pCLE images directly from the irregularly sampled input data. We created synthetic sparse pCLE images to evaluate our methodology. Results: The results were validated through an image quality assessment based on a combination of the following metrics: Peak signal-to-noise ratio, the Structural Similarity Index. Conclusion: Both dense and sparse CNNs outperform the reconstruction method currently used in the clinic. The main contributions of our study are a comparison of sparse and dense approach in pCLE image reconstruction, implementing trainable generalised NW kernel regression, and adaptation of synthetic data for training pCLE SR.



### Mean Shift Rejection: Training Deep Neural Networks Without Minibatch Statistics or Normalization
- **Arxiv ID**: http://arxiv.org/abs/1911.13173v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.13173v1)
- **Published**: 2019-11-29 16:19:00+00:00
- **Updated**: 2019-11-29 16:19:00+00:00
- **Authors**: Brendan Ruff, Taylor Beck, Joscha Bach
- **Comment**: under review at ECAI2020
- **Journal**: None
- **Summary**: Deep convolutional neural networks are known to be unstable during training at high learning rate unless normalization techniques are employed. Normalizing weights or activations allows the use of higher learning rates, resulting in faster convergence and higher test accuracy. Batch normalization requires minibatch statistics that approximate the dataset statistics but this incurs additional compute and memory costs and causes a communication bottleneck for distributed training. Weight normalization and initialization-only schemes do not achieve comparable test accuracy.   We introduce a new understanding of the cause of training instability and provide a technique that is independent of normalization and minibatch statistics. Our approach treats training instability as a spatial common mode signal which is suppressed by placing the model on a channel-wise zero-mean isocline that is maintained throughout training. Firstly, we apply channel-wise zero-mean initialization of filter kernels with overall unity kernel magnitude. At each training step we modify the gradients of spatial kernels so that their weighted channel-wise mean is subtracted in order to maintain the common mode rejection condition. This prevents the onset of mean shift. This new technique allows direct training of the test graph so that training and test models are identical. We also demonstrate that injecting random noise throughout the network during training improves generalization. This is based on the idea that, as a side effect, batch normalization performs deep data augmentation by injecting minibatch noise due to the weakness of the dataset approximation.   Our technique achieves higher accuracy compared to batch normalization and for the first time shows that minibatches and normalization are unnecessary for state-of-the-art training.



### CURL: Neural Curve Layers for Global Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1911.13175v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.13175v4)
- **Published**: 2019-11-29 16:20:05+00:00
- **Updated**: 2020-10-23 07:42:56+00:00
- **Authors**: Sean Moran, Steven McDonagh, Gregory Slabaugh
- **Comment**: Accepted to ICPR 2020
- **Journal**: None
- **Summary**: We present a novel approach to adjust global image properties such as colour, saturation, and luminance using human-interpretable image enhancement curves, inspired by the Photoshop curves tool. Our method, dubbed neural CURve Layers (CURL), is designed as a multi-colour space neural retouching block trained jointly in three different colour spaces (HSV, CIELab, RGB) guided by a novel multi-colour space loss. The curves are fully differentiable and are trained end-to-end for different computer vision problems including photo enhancement (RGB-to-RGB) and as part of the image signal processing pipeline for image formation (RAW-to-RGB). To demonstrate the effectiveness of CURL we combine this global image transformation block with a pixel-level (local) image multi-scale encoder-decoder backbone network. In an extensive experimental evaluation we show that CURL produces state-of-the-art image quality versus recently proposed deep learning approaches in both objective and perceptual metrics, setting new state-of-the-art performance on multiple public datasets. Our code is publicly available at: https://github.com/sjmoran/CURL.



### DIST: Rendering Deep Implicit Signed Distance Function with Differentiable Sphere Tracing
- **Arxiv ID**: http://arxiv.org/abs/1911.13225v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1911.13225v2)
- **Published**: 2019-11-29 17:27:46+00:00
- **Updated**: 2020-06-11 07:19:07+00:00
- **Authors**: Shaohui Liu, Yinda Zhang, Songyou Peng, Boxin Shi, Marc Pollefeys, Zhaopeng Cui
- **Comment**: Camera-ready version to appear in CVPR 2020. Project page:
  http://b1ueber2y.me/projects/DIST-Renderer
- **Journal**: None
- **Summary**: We propose a differentiable sphere tracing algorithm to bridge the gap between inverse graphics methods and the recently proposed deep learning based implicit signed distance function. Due to the nature of the implicit function, the rendering process requires tremendous function queries, which is particularly problematic when the function is represented as a neural network. We optimize both the forward and backward passes of our rendering layer to make it run efficiently with affordable memory consumption on a commodity graphics card. Our rendering method is fully differentiable such that losses can be directly computed on the rendered 2D observations, and the gradients can be propagated backwards to optimize the 3D geometry. We show that our rendering method can effectively reconstruct accurate 3D shapes from various inputs, such as sparse depth and multi-view images, through inverse optimization. With the geometry based reasoning, our 3D shape prediction methods show excellent generalization capability and robustness against various noises.



### Zero-Shot Sketch-Based Image Retrieval with Structure-aware Asymmetric Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/1911.13251v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.13251v2)
- **Published**: 2019-11-29 17:43:45+00:00
- **Updated**: 2020-09-18 02:13:30+00:00
- **Authors**: Jiangtong Li, Zhixin Ling, Li Niu, Liqing Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of Sketch-Based Image Retrieval (SBIR) is using free-hand sketches to retrieve images of the same category from a natural image gallery. However, SBIR requires all test categories to be seen during training, which cannot be guaranteed in real-world applications. So we investigate more challenging Zero-Shot SBIR (ZS-SBIR), in which test categories do not appear in the training stage. After realizing that sketches mainly contain structure information while images contain additional appearance information, we attempt to achieve structure-aware retrieval via asymmetric disentanglement.For this purpose, we propose our STRucture-aware Asymmetric Disentanglement (STRAD) method, in which image features are disentangled into structure features and appearance features while sketch features are only projected to structure space. Through disentangling structure and appearance space, bi-directional domain translation is performed between the sketch domain and the image domain. Extensive experiments demonstrate that our STRAD method remarkably outperforms state-of-the-art methods on three large-scale benchmark datasets.



### Extracting deep local features to detect manipulated images of human faces
- **Arxiv ID**: http://arxiv.org/abs/1911.13269v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.13269v2)
- **Published**: 2019-11-29 18:10:36+00:00
- **Updated**: 2020-02-11 11:24:54+00:00
- **Authors**: Michail Tarasiou, Stefanos Zafeiriou
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Recent developments in computer vision and machine learning have made it possible to create realistic manipulated videos of human faces, raising the issue of ensuring adequate protection against the malevolent effects unlocked by such capabilities. In this paper we propose local image features that are shared across manipulated regions are the key element for the automatic detection of manipulated face images. We also design a lightweight architecture with the correct structural biases for extracting such features and derive a multitask training scheme that consistently outperforms image class supervision alone. The trained networks achieve state-of-the-art results in the FaceForensics++ dataset using significantly reduced number of parameters and are shown to work well in detecting fully generated face images.



### Transflow Learning: Repurposing Flow Models Without Retraining
- **Arxiv ID**: http://arxiv.org/abs/1911.13270v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.13270v2)
- **Published**: 2019-11-29 18:14:53+00:00
- **Updated**: 2019-12-05 14:09:04+00:00
- **Authors**: Andrew Gambardella, Atılım Güneş Baydin, Philip H. S. Torr
- **Comment**: None
- **Journal**: None
- **Summary**: It is well known that deep generative models have a rich latent space, and that it is possible to smoothly manipulate their outputs by traversing this latent space. Recently, architectures have emerged that allow for more complex manipulations, such as making an image look as though it were from a different class, or painted in a certain style. These methods typically require large amounts of training in order to learn a single class of manipulations. We present Transflow Learning, a method for transforming a pre-trained generative model so that its outputs more closely resemble data that we provide afterwards. In contrast to previous methods, Transflow Learning does not require any training at all, and instead warps the probability distribution from which we sample latent vectors using Bayesian inference. Transflow Learning can be used to solve a wide variety of tasks, such as neural style transfer and few-shot classification.



### Unpaired Image Translation via Adaptive Convolution-based Normalization
- **Arxiv ID**: http://arxiv.org/abs/1911.13271v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.13271v1)
- **Published**: 2019-11-29 18:16:03+00:00
- **Updated**: 2019-11-29 18:16:03+00:00
- **Authors**: Wonwoong Cho, Kangyeol Kim, Eungyeup Kim, Hyunwoo J. Kim, Jaegul Choo
- **Comment**: None
- **Journal**: None
- **Summary**: Disentangling content and style information of an image has played an important role in recent success in image translation. In this setting, how to inject given style into an input image containing its own content is an important issue, but existing methods followed relatively simple approaches, leaving room for improvement especially when incorporating significant style changes. In response, we propose an advanced normalization technique based on adaptive convolution (AdaCoN), in order to properly impose style information into the content of an input image. In detail, after locally standardizing the content representation in a channel-wise manner, AdaCoN performs adaptive convolution where the convolution filter weights are dynamically estimated using the encoded style representation. The flexibility of AdaCoN can handle complicated image translation tasks involving significant style changes. Our qualitative and quantitative experiments demonstrate the superiority of our proposed method against various existing approaches that inject the style into the content.



### Confidence Calibration and Predictive Uncertainty Estimation for Deep Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.13273v2
- **DOI**: 10.1109/TMI.2020.3006437
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.13273v2)
- **Published**: 2019-11-29 18:20:26+00:00
- **Updated**: 2020-06-29 19:33:15+00:00
- **Authors**: Alireza Mehrtash, William M. Wells III, Clare M. Tempany, Purang Abolmaesumi, Tina Kapur
- **Comment**: Journal of IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Fully convolutional neural networks (FCNs), and in particular U-Nets, have achieved state-of-the-art results in semantic segmentation for numerous medical imaging applications. Moreover, batch normalization and Dice loss have been used successfully to stabilize and accelerate training. However, these networks are poorly calibrated i.e. they tend to produce overconfident predictions both in correct and erroneous classifications, making them unreliable and hard to interpret. In this paper, we study predictive uncertainty estimation in FCNs for medical image segmentation. We make the following contributions: 1) We systematically compare cross entropy loss with Dice loss in terms of segmentation quality and uncertainty estimation of FCNs; 2) We propose model ensembling for confidence calibration of the FCNs trained with batch normalization and Dice loss; 3) We assess the ability of calibrated FCNs to predict segmentation quality of structures and detect out-of-distribution test examples. We conduct extensive experiments across three medical image segmentation applications of the brain, the heart, and the prostate to evaluate our contributions. The results of this study offer considerable insight into the predictive uncertainty estimation and out-of-distribution detection in medical image segmentation and provide practical recipes for confidence calibration. Moreover, we consistently demonstrate that model ensembling improves confidence calibration.



### Domain-invariant Stereo Matching Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.13287v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.13287v1)
- **Published**: 2019-11-29 18:41:26+00:00
- **Updated**: 2019-11-29 18:41:26+00:00
- **Authors**: Feihu Zhang, Xiaojuan Qi, Ruigang Yang, Victor Prisacariu, Benjamin Wah, Philip Torr
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art stereo matching networks have difficulties in generalizing to new unseen environments due to significant domain differences, such as color, illumination, contrast, and texture. In this paper, we aim at designing a domain-invariant stereo matching network (DSMNet) that generalizes well to unseen scenes. To achieve this goal, we propose i) a novel "domain normalization" approach that regularizes the distribution of learned representations to allow them to be invariant to domain differences, and ii) a trainable non-local graph-based filter for extracting robust structural and geometric representations that can further enhance domain-invariant generalizations. When trained on synthetic data and generalized to real test sets, our model performs significantly better than all state-of-the-art models. It even outperforms some deep learning models (e.g. MC-CNN) fine-tuned with test-domain data.



### What's Hidden in a Randomly Weighted Neural Network?
- **Arxiv ID**: http://arxiv.org/abs/1911.13299v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.13299v2)
- **Published**: 2019-11-29 18:56:53+00:00
- **Updated**: 2020-03-31 01:30:39+00:00
- **Authors**: Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, Mohammad Rastegari
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Training a neural network is synonymous with learning the values of the weights. By contrast, we demonstrate that randomly weighted neural networks contain subnetworks which achieve impressive performance without ever training the weight values. Hidden in a randomly weighted Wide ResNet-50 we show that there is a subnetwork (with random weights) that is smaller than, but matches the performance of a ResNet-34 trained on ImageNet. Not only do these "untrained subnetworks" exist, but we provide an algorithm to effectively find them. We empirically show that as randomly weighted neural networks with fixed weights grow wider and deeper, an "untrained subnetwork" approaches a network with learned weights in accuracy. Our code and pretrained models are available at https://github.com/allenai/hidden-networks.



### SG-NN: Sparse Generative Neural Networks for Self-Supervised Scene Completion of RGB-D Scans
- **Arxiv ID**: http://arxiv.org/abs/1912.00036v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00036v2)
- **Published**: 2019-11-29 19:00:14+00:00
- **Updated**: 2020-03-24 18:57:00+00:00
- **Authors**: Angela Dai, Christian Diller, Matthias Nießner
- **Comment**: CVPR 2020; Video: https://youtu.be/rN6D3QmMNuU
- **Journal**: None
- **Summary**: We present a novel approach that converts partial and noisy RGB-D scans into high-quality 3D scene reconstructions by inferring unobserved scene geometry. Our approach is fully self-supervised and can hence be trained solely on real-world, incomplete scans. To achieve self-supervision, we remove frames from a given (incomplete) 3D scan in order to make it even more incomplete; self-supervision is then formulated by correlating the two levels of partialness of the same scan while masking out regions that have never been observed. Through generalization across a large training set, we can then predict 3D scene completion without ever seeing any 3D scan of entirely complete geometry. Combined with a new 3D sparse generative neural network architecture, our method is able to predict highly-detailed surfaces in a coarse-to-fine hierarchical fashion, generating 3D scenes at 2cm resolution, more than twice the resolution of existing state-of-the-art methods as well as outperforming them by a significant margin in reconstruction quality.



### Learning Likelihoods with Conditional Normalizing Flows
- **Arxiv ID**: http://arxiv.org/abs/1912.00042v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.00042v1)
- **Published**: 2019-11-29 19:17:58+00:00
- **Updated**: 2019-11-29 19:17:58+00:00
- **Authors**: Christina Winkler, Daniel Worrall, Emiel Hoogeboom, Max Welling
- **Comment**: 18 pages, 8 Tables, 9 Figures, Preprint
- **Journal**: None
- **Summary**: Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y|x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.



### Square Attack: a query-efficient black-box adversarial attack via random search
- **Arxiv ID**: http://arxiv.org/abs/1912.00049v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.00049v3)
- **Published**: 2019-11-29 19:29:32+00:00
- **Updated**: 2020-07-29 07:53:10+00:00
- **Authors**: Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, Matthias Hein
- **Comment**: Accepted at ECCV 2020; added imperceptible perturbations, analysis of
  examples that require more queries, results on dilated CNNs
- **Journal**: None
- **Summary**: We propose the Square Attack, a score-based black-box $l_2$- and $l_\infty$-adversarial attack that does not rely on local gradient information and thus is not affected by gradient masking. Square Attack is based on a randomized search scheme which selects localized square-shaped updates at random positions so that at each iteration the perturbation is situated approximately at the boundary of the feasible set. Our method is significantly more query efficient and achieves a higher success rate compared to the state-of-the-art methods, especially in the untargeted setting. In particular, on ImageNet we improve the average query efficiency in the untargeted setting for various deep networks by a factor of at least $1.8$ and up to $3$ compared to the recent state-of-the-art $l_\infty$-attack of Al-Dujaili & O'Reilly. Moreover, although our attack is black-box, it can also outperform gradient-based white-box attacks on the standard benchmarks achieving a new state-of-the-art in terms of the success rate. The code of our attack is available at https://github.com/max-andr/square-attack.



### Prior-based Domain Adaptive Object Detection for Hazy and Rainy Conditions
- **Arxiv ID**: http://arxiv.org/abs/1912.00070v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00070v3)
- **Published**: 2019-11-29 21:09:13+00:00
- **Updated**: 2020-07-15 17:35:19+00:00
- **Authors**: Vishwanath A. Sindagi, Poojan Oza, Rajeev Yasarla, Vishal M. Patel
- **Comment**: Accepted at ECCV 2020
- **Journal**: None
- **Summary**: Adverse weather conditions such as haze and rain corrupt the quality of captured images, which cause detection networks trained on clean images to perform poorly on these images. To address this issue, we propose an unsupervised prior-based domain adversarial object detection framework for adapting the detectors to hazy and rainy conditions. In particular, we use weather-specific prior knowledge obtained using the principles of image formation to define a novel prior-adversarial loss. The prior-adversarial loss used to train the adaptation process aims to reduce the weather-specific information in the features, thereby mitigating the effects of weather on the detection performance. Additionally, we introduce a set of residual feature recovery blocks in the object detection pipeline to de-distort the feature space, resulting in further improvements. Evaluations performed on various datasets (Foggy-Cityscapes, Rainy-Cityscapes, RTTS and UFDD) for rainy and hazy conditions demonstrates the effectiveness of the proposed approach.



### OptiBox: Breaking the Limits of Proposals for Visual Grounding
- **Arxiv ID**: http://arxiv.org/abs/1912.00076v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00076v1)
- **Published**: 2019-11-29 21:46:31+00:00
- **Updated**: 2019-11-29 21:46:31+00:00
- **Authors**: Zicong Fan, Si Yi Meng, Leonid Sigal, James J. Little
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of language grounding has attracted much attention in recent years due to its pivotal role in more general image-lingual high level reasoning tasks (e.g., image captioning, VQA). Despite the tremendous progress in visual grounding, the performance of most approaches has been hindered by the quality of bounding box proposals obtained in the early stages of all recent pipelines. To address this limitation, we propose a general progressive query-guided bounding box refinement architecture (OptiBox) that leverages global image encoding for added context. We apply this architecture in the context of the GroundeR model, first introduced in 2016, which has a number of unique and appealing properties, such as the ability to learn in the semi-supervised setting by leveraging cyclic language-reconstruction. Using GroundeR + OptiBox and a simple semantic language reconstruction loss that we propose, we achieve state-of-the-art grounding performance in the supervised setting on Flickr30k Entities dataset. More importantly, we are able to surpass many recent fully supervised models with only 50% of training data and perform competitively with as low as 3%.



### Learning Perceptual Inference by Contrasting
- **Arxiv ID**: http://arxiv.org/abs/1912.00086v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.00086v1)
- **Published**: 2019-11-29 23:02:43+00:00
- **Updated**: 2019-11-29 23:02:43+00:00
- **Authors**: Chi Zhang, Baoxiong Jia, Feng Gao, Yixin Zhu, Hongjing Lu, Song-Chun Zhu
- **Comment**: NeurIPS 2019 spotlight. Project page:
  http://wellyzhang.github.io/project/copinet.html
- **Journal**: None
- **Summary**: "Thinking in pictures," [1] i.e., spatial-temporal reasoning, effortless and instantaneous for humans, is believed to be a significant ability to perform logical induction and a crucial factor in the intellectual history of technology development. Modern Artificial Intelligence (AI), fueled by massive datasets, deeper models, and mighty computation, has come to a stage where (super-)human-level performances are observed in certain specific tasks. However, current AI's ability in "thinking in pictures" is still far lacking behind. In this work, we study how to improve machines' reasoning ability on one challenging task of this kind: Raven's Progressive Matrices (RPM). Specifically, we borrow the very idea of "contrast effects" from the field of psychology, cognition, and education to design and train a permutation-invariant model. Inspired by cognitive studies, we equip our model with a simple inference module that is jointly trained with the perception backbone. Combining all the elements, we propose the Contrastive Perceptual Inference network (CoPINet) and empirically demonstrate that CoPINet sets the new state-of-the-art for permutation-invariant models on two major datasets. We conclude that spatial-temporal reasoning depends on envisaging the possibilities consistent with the relations between objects and can be solved from pixel-level inputs.



### FusionMapping: Learning Depth Prediction with Monocular Images and 2D Laser Scans
- **Arxiv ID**: http://arxiv.org/abs/1912.00096v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00096v1)
- **Published**: 2019-11-29 23:41:45+00:00
- **Updated**: 2019-11-29 23:41:45+00:00
- **Authors**: Peng Yin, Jianing Qian, Yibo Cao, David Held, Howie Choset
- **Comment**: None
- **Journal**: None
- **Summary**: Acquiring accurate three-dimensional depth information conventionally requires expensive multibeam LiDAR devices. Recently, researchers have developed a less expensive option by predicting depth information from two-dimensional color imagery. However, there still exists a substantial gap in accuracy between depth information estimated from two-dimensional images and real LiDAR point-cloud. In this paper, we introduce a fusion-based depth prediction method, called FusionMapping. This is the first method that fuses colored imagery and two-dimensional laser scan to estimate depth in-formation. More specifically, we propose an autoencoder-based depth prediction network and a novel point-cloud refinement network for depth estimation. We analyze the performance of our FusionMapping approach on the KITTI LiDAR odometry dataset and an indoor mobile robot system. The results show that our introduced approach estimates depth with better accuracy when compared to existing methods.



