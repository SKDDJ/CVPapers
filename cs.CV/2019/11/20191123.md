# Arxiv Papers in cs.CV on 2019-11-23
### Deep-Learning Assisted High-Resolution Binocular Stereo Depth Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1912.05012v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.05012v2)
- **Published**: 2019-11-23 00:55:28+00:00
- **Updated**: 2020-02-28 20:11:08+00:00
- **Authors**: Yaoyu Hu, Weikun Zhen, Sebastian Scherer
- **Comment**: Submitted to International Conference on Robotics and Automation
  (ICRA2020)
- **Journal**: None
- **Summary**: This work presents dense stereo reconstruction using high-resolution images for infrastructure inspections. The state-of-the-art stereo reconstruction methods, both learning and non-learning ones, consume too much computational resource on high-resolution data. Recent learning-based methods achieve top ranks on most benchmarks. However, they suffer from the generalization issue due to lack of task-specific training data. We propose to use a less resource demanding non-learning method, guided by a learning-based model, to handle high-resolution images and achieve accurate stereo reconstruction. The deep-learning model produces an initial disparity prediction with uncertainty for each pixel of the down-sampled stereo image pair. The uncertainty serves as a self-measurement of its generalization ability and the per-pixel searching range around the initially predicted disparity. The downstream process performs a modified version of the Semi-Global Block Matching method with the up-sampled per-pixel searching range. The proposed deep-learning assisted method is evaluated on the Middlebury dataset and high-resolution stereo images collected by our customized binocular stereo camera. The combination of learning and non-learning methods achieves better performance on 12 out of 15 cases of the Middlebury dataset. In our infrastructure inspection experiments, the average 3D reconstruction error is less than 0.004m.



### Line-based Camera Pose Estimation in Point Cloud of Structured Environments
- **Arxiv ID**: http://arxiv.org/abs/1912.05013v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.05013v2)
- **Published**: 2019-11-23 00:58:46+00:00
- **Updated**: 2019-12-12 02:29:29+00:00
- **Authors**: Huai Yu, Weikun Zhen, Wen Yang, Sebastian Scherer
- **Comment**: 8 pages, Work has been submitted to ICRA 2020
- **Journal**: None
- **Summary**: Accurate registration of 2D imagery with point clouds is a key technology for image-LiDAR point cloud fusion, camera to laser scanner calibration and camera localization. Despite continuous improvements, automatic registration of 2D and 3D data without using additional textured information still faces great challenges. In this paper, we propose a new 2D-3D registration method to estimate 2D-3D line feature correspondences and the camera pose in untextured point clouds of structured environments. Specifically, we first use geometric constraints between vanishing points and 3D parallel lines to compute all feasible camera rotations. Then, we utilize a hypothesis testing strategy to estimate the 2D-3D line correspondences and the translation vector. By checking the consistency with computed correspondences, the best rotation matrix can be found. Finally, the camera pose is further refined using non-linear optimization with all the 2D-3D line correspondences. The experimental results demonstrate the effectiveness of the proposed method on the synthetic and real dataset (outdoors and indoors) with repeated structures and rapid depth changes.



### Self-Enhanced Convolutional Network for Facial Video Hallucination
- **Arxiv ID**: http://arxiv.org/abs/1911.11136v1
- **DOI**: 10.1109/TIP.2019.2955640
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11136v1)
- **Published**: 2019-11-23 01:06:50+00:00
- **Updated**: 2019-11-23 01:06:50+00:00
- **Authors**: Chaowei Fang, Guanbin Li, Xiaoguang Han, Yizhou Yu
- **Comment**: None
- **Journal**: None
- **Summary**: As a domain-specific super-resolution problem, facial image hallucination has enjoyed a series of breakthroughs thanks to the advances of deep convolutional neural networks. However, the direct migration of existing methods to video is still difficult to achieve good performance due to its lack of alignment and consistency modelling in temporal domain. Taking advantage of high inter-frame dependency in videos, we propose a self-enhanced convolutional network for facial video hallucination. It is implemented by making full usage of preceding super-resolved frames and a temporal window of adjacent low-resolution frames. Specifically, the algorithm first obtains the initial high-resolution inference of each frame by taking into consideration a sequence of consecutive low-resolution inputs through temporal consistency modelling. It further recurrently exploits the reconstructed results and intermediate features of a sequence of preceding frames to improve the initial super-resolution of the current frame by modelling the coherence of structural facial features across frames. Quantitative and qualitative evaluations demonstrate the superiority of the proposed algorithm against state-of-the-art methods. Moreover, our algorithm also achieves excellent performance in the task of general video super-resolution in a single-shot setting.



### Invert and Defend: Model-based Approximate Inversion of Generative Adversarial Networks for Secure Inference
- **Arxiv ID**: http://arxiv.org/abs/1911.10291v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.10291v1)
- **Published**: 2019-11-23 01:15:32+00:00
- **Updated**: 2019-11-23 01:15:32+00:00
- **Authors**: Wei-An Lin, Yogesh Balaji, Pouya Samangouei, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: Inferring the latent variable generating a given test sample is a challenging problem in Generative Adversarial Networks (GANs). In this paper, we propose InvGAN - a novel framework for solving the inference problem in GANs, which involves training an encoder network capable of inverting a pre-trained generator network without access to any training data. Under mild assumptions, we theoretically show that using InvGAN, we can approximately invert the generations of any latent code of a trained GAN model. Furthermore, we empirically demonstrate the superiority of our inference scheme by quantitative and qualitative comparisons with other methods that perform a similar task. We also show the effectiveness of our framework in the problem of adversarial defenses where InvGAN can successfully be used as a projection-based defense mechanism. Additionally, we show how InvGAN can be used to implement reparameterization white-box attacks on projection-based defense mechanisms. Experimental validation on several benchmark datasets demonstrate the efficacy of our method in achieving improved performance on several white-box and black-box attacks. Our code is available at https://github.com/yogeshbalaji/InvGAN.



### Joint Spatial and Angular Super-Resolution from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1911.11619v3
- **DOI**: 10.1109/ACCESS.2020.3002921
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.11619v3)
- **Published**: 2019-11-23 02:35:10+00:00
- **Updated**: 2020-06-27 07:10:14+00:00
- **Authors**: Andre Ivan, Williem, In Kyu Park
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1903.12364
- **Journal**: IEEE Access, vol. 8, June 2020 page(s): 112562-112573
- **Summary**: Synthesizing a densely sampled light field from a single image is highly beneficial for many applications. Moreover, jointly solving both angular and spatial super-resolution problem also introduces new possibilities in light field imaging. The conventional method relies on physical-based rendering and a secondary network to solve the angular super-resolution problem. In addition, pixel-based loss limits the network capability to infer scene geometry globally. In this paper, we show that both super-resolution problems can be solved jointly from a single image by proposing a single end-to-end deep neural network that does not require a physical-based approach. Two novel loss functions based on known light field domain knowledge are proposed to enable the network to preserve the spatio-angular consistency between sub-aperture images. Experimental results show that the proposed model successfully synthesizes dense high resolution light field and it outperforms the state-of-the-art method in both quantitative and qualitative criteria. The method can be generalized to arbitrary scenes, rather than focusing on a particular subject. The synthesized light field can be used for various applications, such as depth estimation and refocusing.



### Learning a Representation with the Block-Diagonal Structure for Pattern Classification
- **Arxiv ID**: http://arxiv.org/abs/1911.10301v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10301v1)
- **Published**: 2019-11-23 02:40:35+00:00
- **Updated**: 2019-11-23 02:40:35+00:00
- **Authors**: He-Feng Yin, Xiao-Jun Wu, Josef Kittler, Zhen-Hua Feng
- **Comment**: accepted by Pattern Analysis and Applications
- **Journal**: None
- **Summary**: Sparse-representation-based classification (SRC) has been widely studied and developed for various practical signal classification applications. However, the performance of a SRC-based method is degraded when both the training and test data are corrupted. To counteract this problem, we propose an approach that learns Representation with Block-Diagonal Structure (RBDS) for robust image recognition. To be more specific, we first introduce a regularization term that captures the block-diagonal structure of the target representation matrix of the training data. The resulting problem is then solved by an optimizer. Last, based on the learned representation, a simple yet effective linear classifier is used for the classification task. The experimental results obtained on several benchmarking datasets demonstrate the efficacy of the proposed RBDS method.



### Hybrid Style Siamese Network: Incorporating style loss in complementary apparels retrieval
- **Arxiv ID**: http://arxiv.org/abs/1912.05014v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1912.05014v2)
- **Published**: 2019-11-23 05:56:50+00:00
- **Updated**: 2020-06-09 23:48:47+00:00
- **Authors**: Mayukh Bhattacharyya, Sayan Nag
- **Comment**: Paper Accepted in the Third Workshop on Computer Vision for Fashion,
  Art and Design, CVPR 2020
- **Journal**: None
- **Summary**: Image Retrieval grows to be an integral part of fashion e-commerce ecosystem as it keeps expanding in multitudes. Other than the retrieval of visually similar items, the retrieval of visually compatible or complementary items is also an important aspect of it. Normal Siamese Networks tend to work well on complementary items retrieval. But it fails to identify low level style features which make items compatible in human eyes. These low level style features are captured to a large extent in techniques used in neural style transfer. This paper proposes a mechanism of utilising those methods in this retrieval task and capturing the low level style features through a hybrid siamese network coupled with a hybrid loss. The experimental results indicate that the proposed method outperforms traditional siamese networks in retrieval tasks for complementary items.



### PlantDoc: A Dataset for Visual Plant Disease Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.10317v1
- **DOI**: 10.1145/3371158.3371196
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.10317v1)
- **Published**: 2019-11-23 06:45:03+00:00
- **Updated**: 2019-11-23 06:45:03+00:00
- **Authors**: Davinder Singh, Naman Jain, Pranjali Jain, Pratik Kayal, Sudhakar Kumawat, Nipun Batra
- **Comment**: 5 Pages, 6 figures, 3 tables
- **Journal**: None
- **Summary**: India loses 35% of the annual crop yield due to plant diseases. Early detection of plant diseases remains difficult due to the lack of lab infrastructure and expertise. In this paper, we explore the possibility of computer vision approaches for scalable and early plant disease detection. The lack of availability of sufficiently large-scale non-lab data set remains a major challenge for enabling vision based plant disease detection. Against this background, we present PlantDoc: a dataset for visual plant disease detection. Our dataset contains 2,598 data points in total across 13 plant species and up to 17 classes of diseases, involving approximately 300 human hours of effort in annotating internet scraped images. To show the efficacy of our dataset, we learn 3 models for the task of plant disease classification. Our results show that modelling using our dataset can increase the classification accuracy by up to 31%. We believe that our dataset can help reduce the entry barrier of computer vision techniques in plant disease detection.



### Tabulated MLP for Fast Point Feature Embedding
- **Arxiv ID**: http://arxiv.org/abs/1912.00790v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00790v1)
- **Published**: 2019-11-23 06:46:09+00:00
- **Updated**: 2019-11-23 06:46:09+00:00
- **Authors**: Yusuke Sekikawa, Teppei Suzuki
- **Comment**: None
- **Journal**: None
- **Summary**: Aiming at a drastic speedup for point-data embeddings at test time, we propose a new framework that uses a pair of multi-layer perceptron (MLP) and look-up table (LUT) to transform point-coordinate inputs into high-dimensional features. When compared with PointNet's feature embedding part realized by MLP that requires millions of dot products, ours at test time requires no such layers of matrix-vector products but requires only looking up the nearest entities followed by interpolation, from the tabulated MLP defined over discrete inputs on a 3D lattice. We call this framework as "LUTI-MLP: LUT Interpolation MLP" that provides a way to train end-to-end tabulated MLP coupled to a LUT in a specific manner without the need for any approximation at test time. LUTI-MLP also provides significant speedup for Jacobian computation of the embedding function wrt global pose coordinate on Lie algebra $\mathfrak{se}(3)$ at test time, which could be used for point-set registration problems. After extensive architectural analysis using ModelNet40 dataset, we confirmed that our LUTI-MLP even with a small-sized table ($8\times 8\times 8$) yields performance comparable to that of MLP while achieving significant speedup: $80\times$ for embedding, $12\times$ for approximate Jacobian, and $860\times$ for canonical Jacobian.



### Iteratively-Refined Interactive 3D Medical Image Segmentation with Multi-Agent Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.10334v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10334v1)
- **Published**: 2019-11-23 09:20:19+00:00
- **Updated**: 2019-11-23 09:20:19+00:00
- **Authors**: Xuan Liao, Wenhao Li, Qisen Xu, Xiangfeng Wang, Bo Jin, Xiaoyun Zhang, Ya Zhang, Yanfeng Wang
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Existing automatic 3D image segmentation methods usually fail to meet the clinic use. Many studies have explored an interactive strategy to improve the image segmentation performance by iteratively incorporating user hints. However, the dynamic process for successive interactions is largely ignored. We here propose to model the dynamic process of iterative interactive image segmentation as a Markov decision process (MDP) and solve it with reinforcement learning (RL). Unfortunately, it is intractable to use single-agent RL for voxel-wise prediction due to the large exploration space. To reduce the exploration space to a tractable size, we treat each voxel as an agent with a shared voxel-level behavior strategy so that it can be solved with multi-agent reinforcement learning. An additional advantage of this multi-agent model is to capture the dependency among voxels for segmentation task. Meanwhile, to enrich the information of previous segmentations, we reserve the prediction uncertainty in the state space of MDP and derive an adjustment action space leading to a more precise and finer segmentation. In addition, to improve the efficiency of exploration, we design a relative cross-entropy gain-based reward to update the policy in a constrained direction. Experimental results on various medical datasets have shown that our method significantly outperforms existing state-of-the-art methods, with the advantage of fewer interactions and a faster convergence.



### Attention Deep Model with Multi-Scale Deep Supervision for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1911.10335v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.10335v3)
- **Published**: 2019-11-23 09:27:53+00:00
- **Updated**: 2020-10-22 08:08:09+00:00
- **Authors**: Di Wu, Chao Wang, Yong Wu, De-Shuang Huang
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: In recent years, person re-identification (PReID) has become a hot topic in computer vision duo to it is an important part in intelligent surveillance. Many state-of-the-art PReID methods are attention-based or multi-scale feature learning deep models. However, introducing attention mechanism may lead to some important feature information losing issue. Besides, most of the multi-scale models embedding the multi-scale feature learning block into the feature extraction deep network, which reduces the efficiency of inference network. To address these issue, in this study, we introduce an attention deep architecture with multi-scale deep supervision for PReID. Technically, we contribute a reverse attention block to complement the attention block, and a novel multi-scale layer with deep supervision operator for training the backbone network. The proposed block and operator are only used for training, and discard in test phase. Experiments have been performed on Market-1501, DukeMTMC-reID and CUHK03 datasets. All the experiment results show that the proposed model significantly outperforms the other competitive state-of-the-art methods.



### Simple and Lightweight Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1911.10346v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10346v2)
- **Published**: 2019-11-23 10:42:07+00:00
- **Updated**: 2020-01-30 11:55:58+00:00
- **Authors**: Zhe Zhang, Jie Tang, Gangshan Wu
- **Comment**: results of inference speed corrected, github url added
- **Journal**: None
- **Summary**: Recent research on human pose estimation has achieved significant improvement. However, most existing methods tend to pursue higher scores using complex architecture or computationally expensive models on benchmark datasets, ignoring the deployment costs in practice. In this paper, we investigate the problem of simple and lightweight human pose estimation. We first redesign a lightweight bottleneck block with two non-novel concepts: depthwise convolution and attention mechanism. And then, based on the lightweight block, we present a Lightweight Pose Network (LPN) following the architecture design principles of SimpleBaseline. The model size (#Params) of our small network LPN-50 is only 9% of SimpleBaseline(ResNet50), and the computational complexity (FLOPs) is only 11%. To give full play to the potential of our LPN and get more accurate predicted results, we also propose an iterative training strategy and a model-agnostic post-processing function Beta-Soft-Argmax. We empirically demonstrate the effectiveness and efficiency of our methods on the benchmark dataset: the COCO keypoint detection dataset. Besides, we show the speed superiority of our lightweight network at inference time on a non-GPU platform. Specifically, our LPN-50 can achieve 68.7 in AP score on the COCO test-dev set, with only 2.7M parameters and 1.0 GFLOPs, while the inference speed is 17 FPS on an Intel i7-8700K CPU machine.



### Shape Detection of Liver From 2D Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/1911.10352v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.10352v1)
- **Published**: 2019-11-23 12:00:31+00:00
- **Updated**: 2019-11-23 12:00:31+00:00
- **Authors**: Md Abdul Mutalab Shaykat, Yashna Islam, Mohammad Ishtiaque Hossain
- **Comment**: arXiv admin note: submission has been withdrawn by arXiv
  administrators due to inappropriate overlap with external sources
- **Journal**: None
- **Summary**: Applications of ultrasound images have expanded from fetal imaging to abdominal and cardiac diagnosis. Liver-being the largest gland in the body and responsible for metabolic activities requires to be to be diagnosed and therefore subject to utmost injury. Although, ultrasound imaging has developed into three and four dimensions providing higher amount of information; it requires highly trained medical staff due to the image complexity and dimensions it contain. Since 2D ultrasound images are still considered to be the basis of clinical treatments,computer aided automated liver diagnosis is very essential. Due to the limitations of ultrasound images, such as loss of resolution leading to speckle noise, it is difficult to detect shape of organs.In this project, we propose a shape detection method for liver in 2D Ultrasound images. Then we compare the accuracies of the method for both noise and after noise removal.



### Unsupervised Keyword Extraction for Full-sentence VQA
- **Arxiv ID**: http://arxiv.org/abs/1911.10354v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.10354v3)
- **Published**: 2019-11-23 12:18:03+00:00
- **Updated**: 2020-10-12 09:20:30+00:00
- **Authors**: Kohei Uehara, Tatsuya Harada
- **Comment**: EMNLP 2020 workshop: NLP Beyond Text (NLPBT)
- **Journal**: None
- **Summary**: In the majority of the existing Visual Question Answering (VQA) research, the answers consist of short, often single words, as per instructions given to the annotators during dataset construction. This study envisions a VQA task for natural situations, where the answers are more likely to be sentences rather than single words. To bridge the gap between this natural VQA and existing VQA approaches, a novel unsupervised keyword extraction method is proposed. The method is based on the principle that the full-sentence answers can be decomposed into two parts: one that contains new information answering the question (i.e., keywords), and one that contains information already included in the question. Discriminative decoders were designed to achieve such decomposition, and the method was experimentally implemented on VQA datasets containing full-sentence answers. The results show that the proposed model can accurately extract the keywords without being given explicit annotations describing them.



### Globally Guided Progressive Fusion Network for 3D Pancreas Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.10360v1
- **DOI**: 10.1007/978-3-030-32245-8_24
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.10360v1)
- **Published**: 2019-11-23 12:51:44+00:00
- **Updated**: 2019-11-23 12:51:44+00:00
- **Authors**: Chaowei Fang, Guanbin Li, Chengwei Pan, Yiming Li, Yizhou Yu
- **Comment**: MICCAI2019
- **Journal**: None
- **Summary**: Recently 3D volumetric organ segmentation attracts much research interest in medical image analysis due to its significance in computer aided diagnosis. This paper aims to address the pancreas segmentation task in 3D computed tomography volumes. We propose a novel end-to-end network, Globally Guided Progressive Fusion Network, as an effective and efficient solution to volumetric segmentation, which involves both global features and complicated 3D geometric information. A progressive fusion network is devised to extract 3D information from a moderate number of neighboring slices and predict a probability map for the segmentation of each slice. An independent branch for excavating global features from downsampled slices is further integrated into the network. Extensive experimental results demonstrate that our method achieves state-of-the-art performance on two pancreas datasets.



### Universal Adversarial Robustness of Texture and Shape-Biased Models
- **Arxiv ID**: http://arxiv.org/abs/1911.10364v4
- **DOI**: 10.1109/ICIP42928.2021.9506325
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10364v4)
- **Published**: 2019-11-23 13:23:45+00:00
- **Updated**: 2021-08-31 02:50:56+00:00
- **Authors**: Kenneth T. Co, Luis Muñoz-González, Leslie Kanthan, Ben Glocker, Emil C. Lupu
- **Comment**: In Proceedings of the 28th IEEE International Conference on Image
  Processing (ICIP 2021), code available at:
  https://github.com/kenny-co/sgd-uap-torch
- **Journal**: None
- **Summary**: Increasing shape-bias in deep neural networks has been shown to improve robustness to common corruptions and noise. In this paper we analyze the adversarial robustness of texture and shape-biased models to Universal Adversarial Perturbations (UAPs). We use UAPs to evaluate the robustness of DNN models with varying degrees of shape-based training. We find that shape-biased models do not markedly improve adversarial robustness, and we show that ensembles of texture and shape-biased models can improve universal adversarial robustness while maintaining strong performance.



### Differentiable Meta-learning Model for Few-shot Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.10371v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10371v1)
- **Published**: 2019-11-23 14:12:17+00:00
- **Updated**: 2019-11-23 14:12:17+00:00
- **Authors**: Pinzhuo Tian, Zhangkai Wu, Lei Qi, Lei Wang, Yinghuan Shi, Yang Gao
- **Comment**: Accepted by AAAI2020
- **Journal**: None
- **Summary**: To address the annotation scarcity issue in some cases of semantic segmentation, there have been a few attempts to develop the segmentation model in the few-shot learning paradigm. However, most existing methods only focus on the traditional 1-way segmentation setting (i.e., one image only contains a single object). This is far away from practical semantic segmentation tasks where the K-way setting (K>1) is usually required by performing the accurate multi-object segmentation. To deal with this issue, we formulate the few-shot semantic segmentation task as a learning-based pixel classification problem and propose a novel framework called MetaSegNet based on meta-learning. In MetaSegNet, an architecture of embedding module consisting of the global and local feature branches is developed to extract the appropriate meta-knowledge for the few-shot segmentation. Moreover, we incorporate a linear model into MetaSegNet as a base learner to directly predict the label of each pixel for the multi-object segmentation. Furthermore, our MetaSegNet can be trained by the episodic training mechanism in an end-to-end manner from scratch. Experiments on two popular semantic segmentation datasets, i.e., PASCAL VOC and COCO, reveal the effectiveness of the proposed MetaSegNet in the K-way few-shot semantic segmentation task.



### GRASPEL: Graph Spectral Learning at Scale
- **Arxiv ID**: http://arxiv.org/abs/1911.10373v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.10373v3)
- **Published**: 2019-11-23 14:51:13+00:00
- **Updated**: 2020-07-28 21:08:51+00:00
- **Authors**: Yongyu Wang, Zhiqiang Zhao, Zhuo Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Learning meaningful graphs from data plays important roles in many data mining and machine learning tasks, such as data representation and analysis, dimension reduction, data clustering, and visualization, etc. In this work, for the first time, we present a highly-scalable spectral approach (GRASPEL) for learning large graphs from data. By limiting the precision matrix to be a graph Laplacian, our approach aims to estimate ultra-sparse (tree-like) weighted undirected graphs and shows a clear connection with the prior graphical Lasso method. By interleaving the latest high-performance nearly-linear time spectral methods for graph sparsification, coarsening and embedding, ultra-sparse yet spectrally-robust graphs can be learned by identifying and including the most spectrally-critical edges into the graph. Compared with prior state-of-the-art graph learning approaches, GRASPEL is more scalable and allows substantially improving computing efficiency and solution quality of a variety of data mining and machine learning applications, such as spectral clustering (SC), and t-Distributed Stochastic Neighbor Embedding (t-SNE). {For example, when comparing with graphs constructed using existing methods, GRASPEL achieved the best spectral clustering efficiency and accuracy.



### Region Normalization for Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1911.10375v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10375v2)
- **Published**: 2019-11-23 15:16:36+00:00
- **Updated**: 2023-03-14 11:38:46+00:00
- **Authors**: Tao Yu, Zongyu Guo, Xin Jin, Shilin Wu, Zhibo Chen, Weiping Li, Zhizheng Zhang, Sen Liu
- **Comment**: Accepted to AAAI-2020. Code URL:https://github.com/geekyutao/RN
- **Journal**: None
- **Summary**: Feature Normalization (FN) is an important technique to help neural network training, which typically normalizes features across spatial dimensions. Most previous image inpainting methods apply FN in their networks without considering the impact of the corrupted regions of the input image on normalization, e.g. mean and variance shifts. In this work, we show that the mean and variance shifts caused by full-spatial FN limit the image inpainting network training and we propose a spatial region-wise normalization named Region Normalization (RN) to overcome the limitation. RN divides spatial pixels into different regions according to the input mask, and computes the mean and variance in each region for normalization. We develop two kinds of RN for our image inpainting network: (1) Basic RN (RN-B), which normalizes pixels from the corrupted and uncorrupted regions separately based on the original inpainting mask to solve the mean and variance shift problem; (2) Learnable RN (RN-L), which automatically detects potentially corrupted and uncorrupted regions for separate normalization, and performs global affine transformation to enhance their fusion. We apply RN-B in the early layers and RN-L in the latter layers of the network respectively. Experiments show that our method outperforms current state-of-the-art methods quantitatively and qualitatively. We further generalize RN to other inpainting networks and achieve consistent performance improvements. Our code is available at https://github.com/geekyutao/RN.



### SAL: Sign Agnostic Learning of Shapes from Raw Data
- **Arxiv ID**: http://arxiv.org/abs/1911.10414v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.10414v2)
- **Published**: 2019-11-23 20:18:29+00:00
- **Updated**: 2020-03-31 19:50:00+00:00
- **Authors**: Matan Atzmon, Yaron Lipman
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Recently, neural networks have been used as implicit representations for surface reconstruction, modelling, learning, and generation. So far, training neural networks to be implicit representations of surfaces required training data sampled from a ground-truth signed implicit functions such as signed distance or occupancy functions, which are notoriously hard to compute.   In this paper we introduce Sign Agnostic Learning (SAL), a deep learning approach for learning implicit shape representations directly from raw, unsigned geometric data, such as point clouds and triangle soups.   We have tested SAL on the challenging problem of surface reconstruction from an un-oriented point cloud, as well as end-to-end human shape space learning directly from raw scans dataset, and achieved state of the art reconstructions compared to current approaches. We believe SAL opens the door to many geometric deep learning applications with real-world data, alleviating the usual painstaking, often manual pre-process.



### Visualizing Point Cloud Classifiers by Curvature Smoothing
- **Arxiv ID**: http://arxiv.org/abs/1911.10415v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10415v3)
- **Published**: 2019-11-23 20:36:56+00:00
- **Updated**: 2020-09-01 04:11:49+00:00
- **Authors**: Chen Ziwen, Wenxuan Wu, Zhongang Qi, Li Fuxin
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, several networks that operate directly on point clouds have been proposed. There is significant utility in understanding their mechanisms to classify point clouds, which can potentially help diagnosing these networks and designing better architectures. In this paper, we propose a novel approach to visualize features important to the point cloud classifiers. Our approach is based on smoothing curved areas on a point cloud. After prominent features were smoothed, the resulting point cloud can be evaluated on the network to assess whether the feature is important to the classifier. A technical contribution of the paper is an approximated curvature smoothing algorithm, which can smoothly transition from the original point cloud to one of constant curvature, such as a uniform sphere. Based on the smoothing algorithm, we propose PCI-GOS (Point Cloud Integrated-Gradients Optimized Saliency), a visualization technique that can automatically find the minimal saliency map that covers the most important features on a shape. Experiment results revealed insights into different point cloud classifiers.



### On Symbiosis of Attribute Prediction and Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.11612v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11612v1)
- **Published**: 2019-11-23 20:51:05+00:00
- **Updated**: 2019-11-23 20:51:05+00:00
- **Authors**: Mahdi M. Kalayeh, Mubarak Shah
- **Comment**: Accepted for publication in PAMI. arXiv admin note: substantial text
  overlap with arXiv:1704.08740
- **Journal**: None
- **Summary**: In this paper, we propose to employ semantic segmentation to improve person-related attribute prediction. The core idea lies in the fact that the probability of an attribute to appear in an image is far from being uniform in the spatial domain. We build our attribute prediction model jointly with a deep semantic segmentation network. This harnesses the localization cues learned by the semantic segmentation to guide the attention of the attribute prediction to the regions where different attributes naturally show up. Therefore, in addition to prediction, we are able to localize the attributes despite merely having access to image-level labels (weak supervision) during training. We first propose semantic segmentation-based pooling and gating, respectively denoted as SSP and SSG. In the former, the estimated segmentation masks are used to pool the final activations of the attribute prediction network, from multiple semantically homogeneous regions. In SSG, the same idea is applied to the intermediate layers of the network. SSP and SSG, while effective, impose heavy memory utilization since each channel of the activations is pooled/gated with all the semantic segmentation masks. To circumvent this, we propose Symbiotic Augmentation (SA), where we learn only one mask per activation channel. SA allows the model to either pick one, or combine (weighted superposition) multiple semantic maps, in order to generate the proper mask for each channel. SA simultaneously applies the same mechanism to the reverse problem by leveraging output logits of attribute prediction to guide the semantic segmentation task. We evaluate our proposed methods for facial attributes on CelebA and LFWA datasets, while benchmarking WIDER Attribute and Berkeley Attributes of People for whole body attributes. Our proposed methods achieve superior results compared to the previous works.



### Atlas Based Segmentations via Semi-Supervised Diffeomorphic Registrations
- **Arxiv ID**: http://arxiv.org/abs/1911.10417v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10417v1)
- **Published**: 2019-11-23 21:06:22+00:00
- **Updated**: 2019-11-23 21:06:22+00:00
- **Authors**: Charles Huang, Masoud Badiei, Hyunseok Seo, Ming Ma, Xiaokun Liang, Dante Capaldi, Michael Gensheimer, Lei Xing
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: Segmentation of organs-at-risk (OARs) is a bottleneck in current radiation oncology pipelines and is often time consuming and labor intensive. In this paper, we propose an atlas-based semi-supervised registration algorithm to generate accurate segmentations of OARs for which there are ground truth contours and rough segmentations of all other OARs in the atlas. To the best of our knowledge, this is the first study to use learning-based registration methods for the segmentation of head and neck patients and demonstrate its utility in clinical applications. Methods: Our algorithm cascades rigid and deformable deformation blocks, and takes on an atlas image (M), set of atlas-space segmentations (S_A), and a patient image (F) as inputs, while outputting patient-space segmentations of all OARs defined on the atlas. We train our model on 475 CT images taken from public archives and Stanford RadOnc Clinic (SROC), validate on 5 CT images from SROC, and test our model on 20 CT images from SROC. Results: Our method outperforms current state of the art learning-based registration algorithms and achieves an overall dice score of 0.789 on our test set. Moreover, our method yields a performance comparable to manual segmentation and supervised segmentation, while solving a much more complex registration problem. Whereas supervised segmentation methods only automate the segmentation process for a select few number of OARs, we demonstrate that our methods can achieve similar performance for OARs of interest, while also providing segmentations for every other OAR on the provided atlas. Conclusions: Our proposed algorithm has significant clinical applications and could help reduce the bottleneck for segmentation of head and neck OARs. Further, our results demonstrate that semi-supervised diffeomorphic registration can be accurately applied to both registration and segmentation problems.



### Constrained Linear Data-feature Mapping for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1911.10428v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1911.10428v2)
- **Published**: 2019-11-23 22:39:47+00:00
- **Updated**: 2020-07-06 15:27:07+00:00
- **Authors**: Juncai He, Yuyan Chen, Lian Zhang, Jinchao Xu
- **Comment**: 15 page, 2 figures
- **Journal**: None
- **Summary**: In this paper, we propose a constrained linear data-feature mapping model as an interpretable mathematical model for image classification using convolutional neural network (CNN) such as the ResNet. From this viewpoint, we establish the detailed connections in a technical level between the traditional iterative schemes for constrained linear system and the architecture for the basic blocks of ResNet. Under these connections, we propose some natural modifications of ResNet type models which will have less parameters but still maintain almost the same accuracy as these corresponding original models. Some numerical experiments are shown to demonstrate the validity of this constrained learning data-feature mapping assumption.



