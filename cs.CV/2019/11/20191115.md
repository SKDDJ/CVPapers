# Arxiv Papers in cs.CV on 2019-11-15
### Gated Variational AutoEncoders: Incorporating Weak Supervision to Encourage Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/1911.06443v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.06443v1)
- **Published**: 2019-11-15 01:46:16+00:00
- **Updated**: 2019-11-15 01:46:16+00:00
- **Authors**: Matthew J. Vowels, Necati Cihan Camgoz, Richard Bowden
- **Comment**: None
- **Journal**: None
- **Summary**: Variational AutoEncoders (VAEs) provide a means to generate representational latent embeddings. Previous research has highlighted the benefits of achieving representations that are disentangled, particularly for downstream tasks. However, there is some debate about how to encourage disentanglement with VAEs and evidence indicates that existing implementations of VAEs do not achieve disentanglement consistently. The evaluation of how well a VAE's latent space has been disentangled is often evaluated against our subjective expectations of which attributes should be disentangled for a given problem. Therefore, by definition, we already have domain knowledge of what should be achieved and yet we use unsupervised approaches to achieve it. We propose a weakly-supervised approach that incorporates any available domain knowledge into the training process to form a Gated-VAE. The process involves partitioning the representational embedding and gating backpropagation. All partitions are utilised on the forward pass but gradients are backpropagated through different partitions according to selected image/target pairings. The approach can be used to modify existing VAE models such as beta-VAE, InfoVAE and DIP-VAE-II. Experiments demonstrate that using gated backpropagation, latent factors are represented in their intended partition. The approach is applied to images of faces for the purpose of disentangling head-pose from facial expression. Quantitative metrics show that using Gated-VAE improves average disentanglement, completeness and informativeness, as compared with un-gated implementations. Qualitative assessment of latent traversals demonstrate its disentanglement of head-pose from expression, even when only weak/noisy supervision is available.



### Cross-modal supervised learning for better acoustic representations
- **Arxiv ID**: http://arxiv.org/abs/1911.07917v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1911.07917v2)
- **Published**: 2019-11-15 02:23:23+00:00
- **Updated**: 2020-01-01 06:22:39+00:00
- **Authors**: Shaoyong Jia, Xin Shu, Yang Yang, Dawei Liang, Qiyue Liu, Junhui Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Obtaining large-scale human-labeled datasets to train acoustic representation models is a very challenging task. On the contrary, we can easily collect data with machine-generated labels. In this work, we propose to exploit machine-generated labels to learn better acoustic representations, based on the synchronization between vision and audio. Firstly, we collect a large-scale video dataset with 15 million samples, which totally last 16,320 hours. Each video is 3 to 5 seconds in length and annotated automatically by publicly available visual and audio classification models. Secondly, we train various classical convolutional neural networks (CNNs) including VGGish, ResNet 50 and Mobilenet v2. We also make several improvements to VGGish and achieve better results. Finally, we transfer our models on three external standard benchmarks for audio classification task, and achieve significant performance boost over the state-of-the-art results. Models and codes are available at: https://github.com/Deeperjia/vgg-like-audio-models.



### Face shape classification using Inception v3
- **Arxiv ID**: http://arxiv.org/abs/1911.07916v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.07916v1)
- **Published**: 2019-11-15 02:29:59+00:00
- **Updated**: 2019-11-15 02:29:59+00:00
- **Authors**: Adonis Emmanuel Tio
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: In this paper, we present experimental results obtained from retraining the last layer of the Inception v3 model in classifying images of human faces into one of five basic face shapes. The accuracy of the retrained Inception v3 model was compared with that of the following classification methods that uses facial landmark distance ratios and angles as features: linear discriminant analysis (LDA), support vector machines with linear kernel (SVM-LIN), support vector machines with radial basis function kernel (SVM-RBF), artificial neural networks or multilayer perceptron (MLP), and k-nearest neighbors (KNN). All classifiers were trained and tested using a total of 500 images of female celebrities with known face shapes collected from the Internet. Results show that training accuracy and overall accuracy ranges from 98.0% to 100% and from 84.4% to 84.8% for Inception v3 and from 50.6% to 73.0% and from 36.4% to 64.6% for the other classifiers depending on the training set size used. This result shows that the retrained Inception v3 model was able to fit the training data well and outperform the other classifiers without the need to handpick specific features to include in model training. Future work should consider expanding the labeled dataset, preferably one that can also be freely distributed to the research community, so that proper model cross-validation can be performed. As far as we know, this is the first in the literature to use convolutional neural networks in face-shape classification. The scripts are available at https://github.com/adonistio/inception-face-shape-classifier.



### Human Annotations Improve GAN Performances
- **Arxiv ID**: http://arxiv.org/abs/1911.06460v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06460v1)
- **Published**: 2019-11-15 03:09:38+00:00
- **Updated**: 2019-11-15 03:09:38+00:00
- **Authors**: Juanyong Duan, Sim Heng Ong, Qi Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have shown great success in many applications. In this work, we present a novel method that leverages human annotations to improve the quality of generated images. Unlike previous paradigms that directly ask annotators to distinguish between real and fake data in a straightforward way, we propose and annotate a set of carefully designed attributes that encode important image information at various levels, to understand the differences between fake and real images. Specifically, we have collected an annotated dataset that contains 600 fake images and 400 real images. These images are evaluated by 10 workers from the Amazon Mechanical Turk (AMT) based on eight carefully defined attributes. Statistical analyses have revealed different distributions of the proposed attributes between real and fake images. These attributes are shown to be useful in discriminating fake images from real ones, and deep neural networks are developed to automatically predict the attributes. We further utilize the information by integrating the attributes into GANs to generate better images. Experimental results evaluated by multiple metrics show performance improvement of the proposed model.



### Multiple Style-Transfer in Real-Time
- **Arxiv ID**: http://arxiv.org/abs/1911.06464v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1911.06464v2)
- **Published**: 2019-11-15 03:49:41+00:00
- **Updated**: 2019-11-18 03:56:40+00:00
- **Authors**: Michael Maring, Kaustav Chakraborty
- **Comment**: Authors agreed that there is not much novelty in the work so
  presented
- **Journal**: None
- **Summary**: Style transfer aims to combine the content of one image with the artistic style of another. It was discovered that lower levels of convolutional networks captured style information, while higher levels captures content information. The original style transfer formulation used a weighted combination of VGG-16 layer activations to achieve this goal. Later, this was accomplished in real-time using a feed-forward network to learn the optimal combination of style and content features from the respective images. The first aim of our project was to introduce a framework for capturing the style from several images at once. We propose a method that extends the original real-time style transfer formulation by combining the features of several style images. This method successfully captures color information from the separate style images. The other aim of our project was to improve the temporal style continuity from frame to frame. Accordingly, we have experimented with the temporal stability of the output images and discussed the various available techniques that could be employed as alternatives.



### DeepSat V2: Feature Augmented Convolutional Neural Nets for Satellite Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1911.07747v1
- **DOI**: 10.1080/2150704X.2019.1693071
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.07747v1)
- **Published**: 2019-11-15 04:07:09+00:00
- **Updated**: 2019-11-15 04:07:09+00:00
- **Authors**: Qun Liu, Saikat Basu, Sangram Ganguly, Supratik Mukhopadhyay, Robert DiBiano, Manohar Karki, Ramakrishna Nemani
- **Comment**: This is an Accepted Manuscript of an article published by Taylor &
  Francis Group in Remote Sensing Letters. arXiv admin note: text overlap with
  arXiv:1509.03602
- **Journal**: None
- **Summary**: Satellite image classification is a challenging problem that lies at the crossroads of remote sensing, computer vision, and machine learning. Due to the high variability inherent in satellite data, most of the current object classification approaches are not suitable for handling satellite datasets. The progress of satellite image analytics has also been inhibited by the lack of a single labeled high-resolution dataset with multiple class labels. In a preliminary version of this work, we introduced two new high resolution satellite imagery datasets (SAT-4 and SAT-6) and proposed DeepSat framework for classification based on "handcrafted" features and a deep belief network (DBN). The present paper is an extended version, we present an end-to-end framework leveraging an improved architecture that augments a convolutional neural network (CNN) with handcrafted features (instead of using DBN-based architecture) for classification. Our framework, having access to fused spatial information obtained from handcrafted features as well as CNN feature maps, have achieved accuracies of 99.90% and 99.84% respectively, on SAT-4 and SAT-6, surpassing all the other state-of-the-art results. A statistical analysis based on Distribution Separability Criterion substantiates the robustness of our approach in learning better representations for satellite imagery.



### Self-supervised Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1911.06470v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.06470v2)
- **Published**: 2019-11-15 04:13:11+00:00
- **Updated**: 2020-02-01 12:10:27+00:00
- **Authors**: Kejiang Chen, Hang Zhou, Yuefeng Chen, Xiaofeng Mao, Yuhong Li, Yuan He, Hui Xue, Weiming Zhang, Nenghai Yu
- **Comment**: Accepted to ICASSP 2020
- **Journal**: None
- **Summary**: Recent work has demonstrated that neural networks are vulnerable to adversarial examples. To escape from the predicament, many works try to harden the model in various ways, in which adversarial training is an effective way which learns robust feature representation so as to resist adversarial attacks. Meanwhile, the self-supervised learning aims to learn robust and semantic embedding from data itself. With these views, we introduce self-supervised learning to against adversarial examples in this paper. Specifically, the self-supervised representation coupled with k-Nearest Neighbour is proposed for classification. To further strengthen the defense ability, self-supervised adversarial training is proposed, which maximizes the mutual information between the representations of original examples and the corresponding adversarial examples. Experimental results show that the self-supervised representation outperforms its supervised version in respect of robustness and self-supervised adversarial training can further improve the defense ability efficiently.



### Interpreting chest X-rays via CNNs that exploit hierarchical disease dependencies and uncertainty labels
- **Arxiv ID**: http://arxiv.org/abs/1911.06475v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.06475v3)
- **Published**: 2019-11-15 04:29:43+00:00
- **Updated**: 2020-06-12 15:05:42+00:00
- **Authors**: Hieu H. Pham, Tung T. Le, Dat Q. Tran, Dat T. Ngo, Ha Q. Nguyen
- **Comment**: This is a pre-print of our paper that was accepted by Neurocomputing
  - Its shorter version has been accepted by Medical Imaging with Deep Learning
  conference (MIDL 2020)
- **Journal**: None
- **Summary**: Chest radiography is one of the most common types of diagnostic radiology exams, which is critical for screening and diagnosis of many different thoracic diseases. Specialized algorithms have been developed to detect several specific pathologies such as lung nodule or lung cancer. However, accurately detecting the presence of multiple diseases from chest X-rays (CXRs) is still a challenging task. This paper presents a supervised multi-label classification framework based on deep convolutional neural networks (CNNs) for predicting the risk of 14 common thoracic diseases. We tackle this problem by training state-of-the-art CNNs that exploit dependencies among abnormality labels. We also propose to use the label smoothing technique for a better handling of uncertain samples, which occupy a significant portion of almost every CXR dataset. Our model is trained on over 200,000 CXRs of the recently released CheXpert dataset and achieves a mean area under the curve (AUC) of 0.940 in predicting 5 selected pathologies from the validation set. This is the highest AUC score yet reported to date. The proposed method is also evaluated on the independent test set of the CheXpert competition, which is composed of 500 CXR studies annotated by a panel of 5 experienced radiologists. The performance is on average better than 2.6 out of 3 other individual radiologists with a mean AUC of 0.930, which ranks first on the CheXpert leaderboard at the time of writing this paper.



### On Model Robustness Against Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1911.06479v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.06479v2)
- **Published**: 2019-11-15 05:02:25+00:00
- **Updated**: 2020-06-10 05:26:51+00:00
- **Authors**: Shufei Zhang, Kaizhu Huang, Zenglin Xu
- **Comment**: some theoretical bounds need to be revised
- **Journal**: None
- **Summary**: We study the model robustness against adversarial examples, referred to as small perturbed input data that may however fool many state-of-the-art deep learning models. Unlike previous research, we establish a novel theory addressing the robustness issue from the perspective of stability of the loss function in the small neighborhood of natural examples. We propose to exploit an energy function to describe the stability and prove that reducing such energy guarantees the robustness against adversarial examples. We also show that the traditional training methods including adversarial training with the $l_2$ norm constraint (AT) and Virtual Adversarial Training (VAT) tend to minimize the lower bound of our proposed energy function. We make an analysis showing that minimization of such lower bound can however lead to insufficient robustness within the neighborhood around the input sample. Furthermore, we design a more rational method with the energy regularization which proves to achieve better robustness than previous methods. Through a series of experiments, we demonstrate the superiority of our model on both supervised tasks and semi-supervised tasks. In particular, our proposed adversarial framework achieves the best performance compared with previous adversarial training methods on benchmark datasets MNIST, CIFAR-10, and SVHN. Importantly, they demonstrate much better robustness against adversarial examples than all the other comparison methods.



### Automated Augmentation with Reinforcement Learning and GANs for Robust Identification of Traffic Signs using Front Camera Images
- **Arxiv ID**: http://arxiv.org/abs/1911.06486v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.06486v1)
- **Published**: 2019-11-15 06:23:50+00:00
- **Updated**: 2019-11-15 06:23:50+00:00
- **Authors**: Sohini Roy Chowdhury, Lars Tornberg, Robin Halvfordsson, Jonatan Nordh, Adam Suhren Gustafsson, Joel Wall, Mattias Westerberg, Adam Wirehed, Louis Tilloy, Zhanying Hu, Haoyuan Tan, Meng Pan, Jonas Sjoberg
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: Traffic sign identification using camera images from vehicles plays a critical role in autonomous driving and path planning. However, the front camera images can be distorted due to blurriness, lighting variations and vandalism which can lead to degradation of detection performances. As a solution, machine learning models must be trained with data from multiple domains, and collecting and labeling more data in each new domain is time consuming and expensive. In this work, we present an end-to-end framework to augment traffic sign training data using optimal reinforcement learning policies and a variety of Generative Adversarial Network (GAN) models, that can then be used to train traffic sign detector modules. Our automated augmenter enables learning from transformed nightime, poor lighting, and varying degrees of occlusions using the LISA Traffic Sign and BDD-Nexar dataset. The proposed method enables mapping training data from one domain to another, thereby improving traffic sign detection precision/recall from 0.70/0.66 to 0.83/0.71 for nighttime images.



### OpenLORIS-Object: A Robotic Vision Dataset and Benchmark for Lifelong Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.06487v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.06487v2)
- **Published**: 2019-11-15 06:27:27+00:00
- **Updated**: 2020-03-06 05:31:06+00:00
- **Authors**: Qi She, Fan Feng, Xinyue Hao, Qihan Yang, Chuanlin Lan, Vincenzo Lomonaco, Xuesong Shi, Zhengwei Wang, Yao Guo, Yimin Zhang, Fei Qiao, Rosa H. M. Chan
- **Comment**: 7 pages, 7 figures, 4 tables
- **Journal**: None
- **Summary**: The recent breakthroughs in computer vision have benefited from the availability of large representative datasets (e.g. ImageNet and COCO) for training. Yet, robotic vision poses unique challenges for applying visual algorithms developed from these standard computer vision datasets due to their implicit assumption over non-varying distributions for a fixed set of tasks. Fully retraining models each time a new task becomes available is infeasible due to computational, storage and sometimes privacy issues, while na\"{i}ve incremental strategies have been shown to suffer from catastrophic forgetting. It is crucial for the robots to operate continuously under open-set and detrimental conditions with adaptive visual perceptual systems, where lifelong learning is a fundamental capability. However, very few datasets and benchmarks are available to evaluate and compare emerging techniques. To fill this gap, we provide a new lifelong robotic vision dataset ("OpenLORIS-Object") collected via RGB-D cameras. The dataset embeds the challenges faced by a robot in the real-life application and provides new benchmarks for validating lifelong object recognition algorithms. Moreover, we have provided a testbed of $9$ state-of-the-art lifelong learning algorithms. Each of them involves $48$ tasks with $4$ evaluation metrics over the OpenLORIS-Object dataset. The results demonstrate that the object recognition task in the ever-changing difficulty environments is far from being solved and the bottlenecks are at the forward/backward transfer designs. Our dataset and benchmark are publicly available at at \href{https://lifelong-robotic-vision.github.io/dataset/object}{\underline{https://lifelong-robotic-vision.github.io/dataset/object}}.



### Simple iterative method for generating targeted universal adversarial perturbations
- **Arxiv ID**: http://arxiv.org/abs/1911.06502v2
- **DOI**: 10.3390/a13110268
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.06502v2)
- **Published**: 2019-11-15 08:02:20+00:00
- **Updated**: 2019-11-18 05:53:03+00:00
- **Authors**: Hokuto Hirano, Kazuhiro Takemoto
- **Comment**: 4 pages, 3 figures, 1 table
- **Journal**: Algorithms 13, 268 (2020)
- **Summary**: Deep neural networks (DNNs) are vulnerable to adversarial attacks. In particular, a single perturbation known as the universal adversarial perturbation (UAP) can foil most classification tasks conducted by DNNs. Thus, different methods for generating UAPs are required to fully evaluate the vulnerability of DNNs. A realistic evaluation would be with cases that consider targeted attacks; wherein the generated UAP causes DNN to classify an input into a specific class. However, the development of UAPs for targeted attacks has largely fallen behind that of UAPs for non-targeted attacks. Therefore, we propose a simple iterative method to generate UAPs for targeted attacks. Our method combines the simple iterative method for generating non-targeted UAPs and the fast gradient sign method for generating a targeted adversarial perturbation for an input. We applied the proposed method to state-of-the-art DNN models for image classification and proved the existence of almost imperceptible UAPs for targeted attacks; further, we demonstrated that such UAPs are easily generatable.



### Single View Distortion Correction using Semantic Guidance
- **Arxiv ID**: http://arxiv.org/abs/1911.06505v1
- **DOI**: 10.1109/IJCNN.2019.8852065
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.06505v1)
- **Published**: 2019-11-15 08:05:49+00:00
- **Updated**: 2019-11-15 08:05:49+00:00
- **Authors**: Szabolcs-Botond Lőrincz, Szabolcs Pável, Lehel Csató
- **Comment**: None
- **Journal**: None
- **Summary**: Most distortion correction methods focus on simple forms of distortion, such as radial or linear distortions. These works undistort images either based on measurements in the presence of a calibration grid, or use multiple views to find point correspondences and predict distortion parameters. When possible distortions are more complex, e.g. in the case of a camera being placed behind a refractive surface such as glass, the standard method is to use a calibration grid. Considering a high variety of distortions, it is nonviable to conduct these measurements. In this work, we present a single view distortion correction method which is capable of undistorting images containing arbitrarily complex distortions by exploiting recent advancements in differentiable image sampling and in the usage of semantic information to augment various tasks. The results of this work show that our model is able to estimate and correct highly complex distortions, and that incorporating semantic information mitigates the process of image undistortion.



### QC-Automator: Deep Learning-based Automated Quality Control for Diffusion MR Images
- **Arxiv ID**: http://arxiv.org/abs/1911.06816v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.06816v1)
- **Published**: 2019-11-15 08:11:17+00:00
- **Updated**: 2019-11-15 08:11:17+00:00
- **Authors**: Zahra Riahi Samani, Jacob Antony Alappatt, Drew Parker, Abdol Aziz Ould Ismail, Ragini Verma
- **Comment**: None
- **Journal**: None
- **Summary**: Quality assessment of diffusion MRI (dMRI) data is essential prior to any analysis, so that appropriate pre-processing can be used to improve data quality and ensure that the presence of MRI artifacts do not affect the results of subsequent image analysis. Manual quality assessment of the data is subjective, possibly error-prone, and infeasible, especially considering the growing number of consortium-like studies, underlining the need for automation of the process. In this paper, we have developed a deep-learning-based automated quality control (QC) tool, QC-Automator, for dMRI data, that can handle a variety of artifacts such as motion, multiband interleaving, ghosting, susceptibility, herringbone and chemical shifts. QC-Automator uses convolutional neural networks along with transfer learning to train the automated artifact detection on a labeled dataset of ~332000 slices of dMRI data, from 155 unique subjects and 5 scanners with different dMRI acquisitions, achieving a 98% accuracy in detecting artifacts. The method is fast and paves the way for efficient and effective artifact detection in large datasets. It is also demonstrated to be replicable on other datasets with different acquisition parameters.



### A3GAN: An Attribute-aware Attentive Generative Adversarial Network for Face Aging
- **Arxiv ID**: http://arxiv.org/abs/1911.06531v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06531v1)
- **Published**: 2019-11-15 09:21:53+00:00
- **Updated**: 2019-11-15 09:21:53+00:00
- **Authors**: Yunfan Liu, Qi Li, Zhenan Sun, Tieniu Tan
- **Comment**: None
- **Journal**: None
- **Summary**: Face aging, which aims at aesthetically rendering a given face to predict its future appearance, has received significant research attention in recent years. Although great progress has been achieved with the success of Generative Adversarial Networks (GANs) in synthesizing realistic images, most existing GAN-based face aging methods have two main problems: 1) unnatural changes of high-level semantic information (e.g. facial attributes) due to the insufficient utilization of prior knowledge of input faces, and 2) distortions of low-level image content including ghosting artifacts and modifications in age-irrelevant regions. In this paper, we introduce A3GAN, an Attribute-Aware Attentive face aging model to address the above issues. Facial attribute vectors are regarded as the conditional information and embedded into both the generator and discriminator, encouraging synthesized faces to be faithful to attributes of corresponding inputs. To improve the visual fidelity of generation results, we leverage the attention mechanism to restrict modifications to age-related areas and preserve image details. Moreover, the wavelet packet transform is employed to capture textural features at multiple scales in the frequency space. Extensive experimental results demonstrate the effectiveness of our model in synthesizing photorealistic aged face images and achieving state-of-the-art performance on popular face aging datasets.



### Learning To Characterize Adversarial Subspaces
- **Arxiv ID**: http://arxiv.org/abs/1911.06587v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06587v1)
- **Published**: 2019-11-15 12:28:18+00:00
- **Updated**: 2019-11-15 12:28:18+00:00
- **Authors**: Xiaofeng Mao, Yuefeng Chen, Yuhong Li, Yuan He, Hui Xue
- **Comment**: Submitted to ICASSP 2020
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) are known to be vulnerable to the maliciously generated adversarial examples. To detect these adversarial examples, previous methods use artificially designed metrics to characterize the properties of \textit{adversarial subspaces} where adversarial examples lie. However, we find these methods are not working in practical attack detection scenarios. Because the artificially defined features are lack of robustness and show limitation in discriminative power to detect strong attacks. To solve this problem, we propose a novel adversarial detection method which identifies adversaries by adaptively learning reasonable metrics to characterize adversarial subspaces. As auxiliary context information, \textit{k} nearest neighbors are used to represent the surrounded subspace of the detected sample. We propose an innovative model called Neighbor Context Encoder (NCE) to learn from \textit{k} neighbors context and infer if the detected sample is normal or adversarial. We conduct thorough experiment on CIFAR-10, CIFAR-100 and ImageNet dataset. The results demonstrate that our approach surpasses all existing methods under three settings: \textit{attack-aware black-box detection}, \textit{attack-unaware black-box detection} and \textit{white-box detection}.



### AdvKnn: Adversarial Attacks On K-Nearest Neighbor Classifiers With Approximate Gradients
- **Arxiv ID**: http://arxiv.org/abs/1911.06591v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.06591v2)
- **Published**: 2019-11-15 12:42:10+00:00
- **Updated**: 2019-11-29 11:10:18+00:00
- **Authors**: Xiaodan Li, Yuefeng Chen, Yuan He, Hui Xue
- **Comment**: Submitted to ICASSP 2020, Implementation
  https://github.com/fiona-lxd/AdvKnn
- **Journal**: None
- **Summary**: Deep neural networks have been shown to be vulnerable to adversarial examples---maliciously crafted examples that can trigger the target model to misbehave by adding imperceptible perturbations. Existing attack methods for k-nearest neighbor~(kNN) based algorithms either require large perturbations or are not applicable for large k. To handle this problem, this paper proposes a new method called AdvKNN for evaluating the adversarial robustness of kNN-based models. Firstly, we propose a deep kNN block to approximate the output of kNN methods, which is differentiable thus can provide gradients for attacks to cross the decision boundary with small distortions. Second, a new consistency learning for distribution instead of classification is proposed for the effectiveness in distribution based methods. Extensive experimental results indicate that the proposed method significantly outperforms state of the art in terms of attack success rate and the added perturbations.



### GraphX-Convolution for Point Cloud Deformation in 2D-to-3D Conversion
- **Arxiv ID**: http://arxiv.org/abs/1911.06600v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.06600v1)
- **Published**: 2019-11-15 13:14:13+00:00
- **Updated**: 2019-11-15 13:14:13+00:00
- **Authors**: Anh-Duc Nguyen, Seonghwa Choi, Woojae Kim, Sanghoon Lee
- **Comment**: In Proceedings of the IEEE International Conference on Computer
  Vision 2019. Fixed minor details and added some updates. Project page:
  https://git.io/JeovA
- **Journal**: None
- **Summary**: In this paper, we present a novel deep method to reconstruct a point cloud of an object from a single still image. Prior arts in the field struggle to reconstruct an accurate and scalable 3D model due to either the inefficient and expensive 3D representations, the dependency between the output and number of model parameters or the lack of a suitable computing operation. We propose to overcome these by deforming a random point cloud to the object shape through two steps: feature blending and deformation. In the first step, the global and point-specific shape features extracted from a 2D object image are blended with the encoded feature of a randomly generated point cloud, and then this mixture is sent to the deformation step to produce the final representative point set of the object. In the deformation process, we introduce a new layer termed as GraphX that considers the inter-relationship between points like common graph convolutions but operates on unordered sets. Moreover, with a simple trick, the proposed model can generate an arbitrary-sized point cloud, which is the first deep method to do so. Extensive experiments verify that we outperform existing models and halve the state-of-the-art distance score in single image 3D reconstruction.



### Single Image Reflection Removal through Cascaded Refinement
- **Arxiv ID**: http://arxiv.org/abs/1911.06634v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06634v2)
- **Published**: 2019-11-15 13:52:31+00:00
- **Updated**: 2020-04-05 07:04:01+00:00
- **Authors**: Chao Li, Yixiao Yang, Kun He, Stephen Lin, John E. Hopcroft
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: We address the problem of removing undesirable reflections from a single image captured through a glass surface, which is an ill-posed, challenging but practically important problem for photo enhancement. Inspired by iterative structure reduction for hidden community detection in social networks, we propose an Iterative Boost Convolutional LSTM Network (IBCLN) that enables cascaded prediction for reflection removal. IBCLN is a cascaded network that iteratively refines the estimates of transmission and reflection layers in a manner that they can boost the prediction quality to each other, and information across steps of the cascade is transferred using an LSTM. The intuition is that the transmission is the strong, dominant structure while the reflection is the weak, hidden structure. They are complementary to each other in a single image and thus a better estimate and reduction on one side from the original image leads to a more accurate estimate on the other side. To facilitate training over multiple cascade steps, we employ LSTM to address the vanishing gradient problem, and propose residual reconstruction loss as further training guidance. Besides, we create a dataset of real-world images with reflection and ground-truth transmission layers to mitigate the problem of insufficient data. Comprehensive experiments demonstrate that the proposed method can effectively remove reflections in real and synthetic images compared with state-of-the-art reflection removal methods.



### You Only Watch Once: A Unified CNN Architecture for Real-Time Spatiotemporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/1911.06644v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06644v5)
- **Published**: 2019-11-15 14:09:47+00:00
- **Updated**: 2021-10-18 12:53:47+00:00
- **Authors**: Okan Köpüklü, Xiangyu Wei, Gerhard Rigoll
- **Comment**: None
- **Journal**: None
- **Summary**: Spatiotemporal action localization requires the incorporation of two sources of information into the designed architecture: (1) temporal information from the previous frames and (2) spatial information from the key frame. Current state-of-the-art approaches usually extract these information with separate networks and use an extra mechanism for fusion to get detections. In this work, we present YOWO, a unified CNN architecture for real-time spatiotemporal action localization in video streams. YOWO is a single-stage architecture with two branches to extract temporal and spatial information concurrently and predict bounding boxes and action probabilities directly from video clips in one evaluation. Since the whole architecture is unified, it can be optimized end-to-end. The YOWO architecture is fast providing 34 frames-per-second on 16-frames input clips and 62 frames-per-second on 8-frames input clips, which is currently the fastest state-of-the-art architecture on spatiotemporal action localization task. Remarkably, YOWO outperforms the previous state-of-the art results on J-HMDB-21 and UCF101-24 with an impressive improvement of ~3% and ~12%, respectively. Moreover, YOWO is the first and only single-stage architecture that provides competitive results on AVA dataset. We make our code and pretrained models publicly available.



### MMGAN: Generative Adversarial Networks for Multi-Modal Distributions
- **Arxiv ID**: http://arxiv.org/abs/1911.06663v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1911.06663v1)
- **Published**: 2019-11-15 14:31:02+00:00
- **Updated**: 2019-11-15 14:31:02+00:00
- **Authors**: Teodora Pandeva, Matthias Schubert
- **Comment**: None
- **Journal**: None
- **Summary**: Over the past years, Generative Adversarial Networks (GANs) have shown a remarkable generation performance especially in image synthesis. Unfortunately, they are also known for having an unstable training process and might loose parts of the data distribution for heterogeneous input data. In this paper, we propose a novel GAN extension for multi-modal distribution learning (MMGAN). In our approach, we model the latent space as a Gaussian mixture model with a number of clusters referring to the number of disconnected data manifolds in the observation space, and include a clustering network, which relates each data manifold to one Gaussian cluster. Thus, the training gets more stable. Moreover, MMGAN allows for clustering real data according to the learned data manifold in the latent space. By a series of benchmark experiments, we illustrate that MMGAN outperforms competitive state-of-the-art models in terms of clustering performance.



### CenterMask : Real-Time Anchor-Free Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.06667v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06667v6)
- **Published**: 2019-11-15 14:38:12+00:00
- **Updated**: 2020-04-02 12:32:33+00:00
- **Authors**: Youngwan Lee, Jongyoul Park
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: We propose a simple yet efficient anchor-free instance segmentation, called CenterMask, that adds a novel spatial attention-guided mask (SAG-Mask) branch to anchor-free one stage object detector (FCOS) in the same vein with Mask R-CNN. Plugged into the FCOS object detector, the SAG-Mask branch predicts a segmentation mask on each box with the spatial attention map that helps to focus on informative pixels and suppress noise. We also present an improved backbone networks, VoVNetV2, with two effective strategies: (1) residual connection for alleviating the optimization problem of larger VoVNet \cite{lee2019energy} and (2) effective Squeeze-Excitation (eSE) dealing with the channel information loss problem of original SE. With SAG-Mask and VoVNetV2, we deign CenterMask and CenterMask-Lite that are targeted to large and small models, respectively. Using the same ResNet-101-FPN backbone, CenterMask achieves 38.3%, surpassing all previous state-of-the-art methods while at a much faster speed. CenterMask-Lite also outperforms the state-of-the-art by large margins at over 35fps on Titan Xp. We hope that CenterMask and VoVNetV2 can serve as a solid baseline of real-time instance segmentation and backbone network for various vision tasks, respectively. The Code is available at https://github.com/youngwanLEE/CenterMask.



### Deep radiomic features from MRI scans predict survival outcome of recurrent glioblastoma
- **Arxiv ID**: http://arxiv.org/abs/1911.06687v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1911.06687v1)
- **Published**: 2019-11-15 15:18:38+00:00
- **Updated**: 2019-11-15 15:18:38+00:00
- **Authors**: Ahmad Chaddad, Saima Rathore, Mingli Zhang, Christian Desrosiers, Tamim Niazi
- **Comment**: Accepted in MICCAI RNO workshop
- **Journal**: None
- **Summary**: This paper proposes to use deep radiomic features (DRFs) from a convolutional neural network (CNN) to model fine-grained texture signatures in the radiomic analysis of recurrent glioblastoma (rGBM). We use DRFs to predict survival of rGBM patients with preoperative T1-weighted post-contrast MR images (n=100). DRFs are extracted from regions of interest labelled by a radiation oncologist and used to compare between short-term and long-term survival patient groups. Random forest (RF) classification is employed to predict survival outcome (i.e., short or long survival), as well as to identify highly group-informative descriptors. Classification using DRFs results in an area under the ROC curve (AUC) of 89.15% (p<0.01) in predicting rGBM patient survival, compared to 78.07% (p<0.01) when using standard radiomic features (SRF). These results indicate the potential of DRFs as a prognostic marker for patients with rGBM.



### In-domain representation learning for remote sensing
- **Arxiv ID**: http://arxiv.org/abs/1911.06721v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06721v1)
- **Published**: 2019-11-15 16:09:38+00:00
- **Updated**: 2019-11-15 16:09:38+00:00
- **Authors**: Maxim Neumann, Andre Susano Pinto, Xiaohua Zhai, Neil Houlsby
- **Comment**: None
- **Journal**: None
- **Summary**: Given the importance of remote sensing, surprisingly little attention has been paid to it by the representation learning community. To address it and to establish baselines and a common evaluation protocol in this domain, we provide simplified access to 5 diverse remote sensing datasets in a standardized form. Specifically, we investigate in-domain representation learning to develop generic remote sensing representations and explore which characteristics are important for a dataset to be a good source for remote sensing representation learning. The established baselines achieve state-of-the-art performance on these datasets.



### TinyCNN: A Tiny Modular CNN Accelerator for Embedded FPGA
- **Arxiv ID**: http://arxiv.org/abs/1911.06777v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.06777v1)
- **Published**: 2019-11-15 17:42:52+00:00
- **Updated**: 2019-11-15 17:42:52+00:00
- **Authors**: Ali Jahanshahi
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, Convolutional Neural Network (CNN) based methods have achieved great success in a large number of applications and have been among the most powerful and widely used techniques in computer vision. However, CNN-based methods are computational-intensive and resource-consuming, and thus are hard to be integrated into embedded systems such as smart phones, smart glasses, and robots. FPGA is one of the most promising platforms for accelerating CNN, but the limited on-chip memory size limit the performance of FPGA accelerator for CNN. In this paper, we propose a framework for designing CNN accelerator on embedded FPGA for image classification. The proposed framework provides a tool for FPGA resource-aware design space exploration of CNNs and automatically generates the hardware description of the CNN to be programmed on a target FPGA. The framework consists of three main backends; software, hardware generation, and simulation/precision adjustment. The software backend serves as an API to the designer to design the CNN and train it according to the hardware resources that are available. Using the CNN model, hardware backend generates the necessary hardware components and integrates them to generate the hardware description of the CNN. Finaly, Simulation/precision adjustment backend adjusts the inter-layer precision units to minimize the classification error. We used 16-bit fixed-point data in a CNN accelerator (FPGA) and compared it to the exactly similar software version running on an ARM processor (32-bit floating point data). We encounter about 3% accuracy loss in classification of the accelerated (FPGA) version. In return, we got up to 15.75x speedup by classifying with the accelerated version on the FPGA.



### Data Efficient Stagewise Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/1911.06786v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.06786v3)
- **Published**: 2019-11-15 18:06:26+00:00
- **Updated**: 2020-06-23 09:02:52+00:00
- **Authors**: Akshay Kulkarni, Navid Panchi, Sharath Chandra Raparthy, Shital Chiddarwar
- **Comment**: 15 pages, 1 figure, 6 tables and 1 algorithm
- **Journal**: None
- **Summary**: Despite the success of Deep Learning (DL), the deployment of modern DL models requiring large computational power poses a significant problem for resource-constrained systems. This necessitates building compact networks that reduce computations while preserving performance. Traditional Knowledge Distillation (KD) methods that transfer knowledge from teacher to student (a) use a single-stage and (b) require the whole data set while distilling the knowledge to the student. In this work, we propose a new method called Stagewise Knowledge Distillation (SKD) which builds on traditional KD methods by progressive stagewise training to leverage the knowledge gained from the teacher, resulting in data-efficient distillation process. We evaluate our method on classification and semantic segmentation tasks. We show, across the tested tasks, significant performance gains even with a fraction of the data used in distillation, without compromising on the metric. We also compare our method with existing KD techniques and show that SKD outperforms them. Moreover, our method can be viewed as a generalized model compression technique that complements other model compression methods such as quantization or pruning.



### ASV: Accelerated Stereo Vision System
- **Arxiv ID**: http://arxiv.org/abs/1911.07919v1
- **DOI**: 10.1145/3352460.3358253
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07919v1)
- **Published**: 2019-11-15 18:44:25+00:00
- **Updated**: 2019-11-15 18:44:25+00:00
- **Authors**: Yu Feng, Paul Whatmough, Yuhao Zhu
- **Comment**: MICRO 2019
- **Journal**: In Proceedings of the 52nd Annual IEEE/ACM International Symposium
  on Microarchitecture (MICRO '52). ACM, New York, NY, USA, 643-656 (2019)
- **Summary**: Estimating depth from stereo vision cameras, i.e., "depth from stereo", is critical to emerging intelligent applications deployed in energy- and performance-constrained devices, such as augmented reality headsets and mobile autonomous robots. While existing stereo vision systems make trade-offs between accuracy, performance and energy-efficiency, we describe ASV, an accelerated stereo vision system that simultaneously improves both performance and energy-efficiency while achieving high accuracy. The key to ASV is to exploit unique characteristics inherent to stereo vision, and apply stereo-specific optimizations, both algorithmically and computationally. We make two contributions. Firstly, we propose a new stereo algorithm, invariant-based stereo matching (ISM), that achieves significant speedup while retaining high accuracy. The algorithm combines classic "hand-crafted" stereo algorithms with recent developments in Deep Neural Networks (DNNs), by leveraging the correspondence invariant unique to stereo vision systems. Secondly, we observe that the bottleneck of the ISM algorithm is the DNN inference, and in particular the deconvolution operations that introduce massive compute-inefficiencies. We propose a set of software optimizations that mitigate these inefficiencies. We show that with less than 0.5% hardware area overhead, these algorithmic and computational optimizations can be effectively integrated within a conventional DNN accelerator. Overall, ASV achieves 5x speedup and 85% energy saving with 0.02% accuracy loss compared to today DNN-based stereo vision systems.



### A Molecular-MNIST Dataset for Machine Learning Study on Diffraction Imaging and Microscopy
- **Arxiv ID**: http://arxiv.org/abs/1911.07644v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.07644v1)
- **Published**: 2019-11-15 18:48:02+00:00
- **Updated**: 2019-11-15 18:48:02+00:00
- **Authors**: Yan Zhang, Steve Farrell, Michael Crowley, Lee Makowski, Jack Deslippe
- **Comment**: None
- **Journal**: None
- **Summary**: An image dataset of 10 different size molecules, where each molecule has 2,000 structural variants, is generated from the 2D cross-sectional projection of Molecular Dynamics trajectories. The purpose of this dataset is to provide a benchmark dataset for the increasing need of machine learning, deep learning and image processing on the study of scattering, imaging and microscopy.



### Handwritten and Machine printed OCR for Geez Numbers Using Artificial Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1911.06845v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.06845v1)
- **Published**: 2019-11-15 19:37:54+00:00
- **Updated**: 2019-11-15 19:37:54+00:00
- **Authors**: Eyob Gebretinsae Beyene
- **Comment**: Presented at NeurIPS 2019 Workshop on Machine Learning for the
  Developing World
- **Journal**: None
- **Summary**: Researches have been done on Ethiopic scripts. However studies excluded the Geez numbers from the studies because of different reasons. This paper presents offline handwritten and machine printed Geez number recognition using feed forward back propagation artificial neural network. On this study, different Geez image characters were collected from google image search and three persons are instructed to write the numbers using pencil. In total we have collected 560 numbers of characters. We have used 460 of the characters for training and 100 are used for testing. Accordingly we have achieved overall all classification ~89:88%



### Curriculum Self-Paced Learning for Cross-Domain Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.06849v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.06849v4)
- **Published**: 2019-11-15 19:43:23+00:00
- **Updated**: 2021-01-20 19:11:38+00:00
- **Authors**: Petru Soviany, Radu Tudor Ionescu, Paolo Rota, Nicu Sebe
- **Comment**: Accepted for publication in Computer Vision and Image Understanding
- **Journal**: None
- **Summary**: Training (source) domain bias affects state-of-the-art object detectors, such as Faster R-CNN, when applied to new (target) domains. To alleviate this problem, researchers proposed various domain adaptation methods to improve object detection results in the cross-domain setting, e.g. by translating images with ground-truth labels from the source domain to the target domain using Cycle-GAN. On top of combining Cycle-GAN transformations and self-paced learning in a smart and efficient way, in this paper, we propose a novel self-paced algorithm that learns from easy to hard. Our method is simple and effective, without any overhead during inference. It uses only pseudo-labels for samples taken from the target domain, i.e. the domain adaptation is unsupervised. We conduct experiments on four cross-domain benchmarks, showing better results than the state of the art. We also perform an ablation study demonstrating the utility of each component in our framework. Additionally, we study the applicability of our framework to other object detectors. Furthermore, we compare our difficulty measure with other measures from the related literature, proving that it yields superior results and that it correlates well with the performance metric.



### Multi-attention Networks for Temporal Localization of Video-level Labels
- **Arxiv ID**: http://arxiv.org/abs/1911.06866v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06866v1)
- **Published**: 2019-11-15 20:42:26+00:00
- **Updated**: 2019-11-15 20:42:26+00:00
- **Authors**: Lijun Zhang, Srinath Nizampatnam, Ahana Gangopadhyay, Marcos V. Conde
- **Comment**: 7 pages, 3 figures; This work was presented at the 3rd Workshop on
  YouTube-8M Large-Scale Video Understanding, at the International Conference
  on Computer Vision (ICCV 2019) in Seoul, Korea
- **Journal**: None
- **Summary**: Temporal localization remains an important challenge in video understanding. In this work, we present our solution to the 3rd YouTube-8M Video Understanding Challenge organized by Google Research. Participants were required to build a segment-level classifier using a large-scale training data set with noisy video-level labels and a relatively small-scale validation data set with accurate segment-level labels. We formulated the problem as a multiple instance multi-label learning and developed an attention-based mechanism to selectively emphasize the important frames by attention weights. The model performance is further improved by constructing multiple sets of attention networks. We further fine-tuned the model using the segment-level data set. Our final model consists of an ensemble of attention/multi-attention networks, deep bag of frames models, recurrent neural networks and convolutional neural networks. It ranked 13th on the private leader board and stands out for its efficient usage of resources.



### Label-similarity Curriculum Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.06902v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.06902v2)
- **Published**: 2019-11-15 23:03:58+00:00
- **Updated**: 2020-07-23 00:48:48+00:00
- **Authors**: Urun Dogan, Aniket Anand Deshmukh, Marcin Machura, Christian Igel
- **Comment**: Accepted as a conference paper at ECCV 2020
- **Journal**: None
- **Summary**: Curriculum learning can improve neural network training by guiding the optimization to desirable optima. We propose a novel curriculum learning approach for image classification that adapts the loss function by changing the label representation. The idea is to use a probability distribution over classes as target label, where the class probabilities reflect the similarity to the true class. Gradually, this label representation is shifted towards the standard one-hot-encoding. That is, in the beginning minor mistakes are corrected less than large mistakes, resembling a teaching process in which broad concepts are explained first before subtle differences are taught.   The class similarity can be based on prior knowledge. For the special case of the labels being natural words, we propose a generic way to automatically compute the similarities. The natural words are embedded into Euclidean space using a standard word embedding. The probability of each class is then a function of the cosine similarity between the vector representations of the class and the true label. The proposed label-similarity curriculum learning (LCL) approach was empirically evaluated using several popular deep learning architectures for image classification tasks applied to five datasets including ImageNet, CIFAR100, and AWA2. In all scenarios, LCL was able to improve the classification accuracy on the test data compared to standard training.



