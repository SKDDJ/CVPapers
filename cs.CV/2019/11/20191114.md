# Arxiv Papers in cs.CV on 2019-11-14
### Character Keypoint-based Homography Estimation in Scanned Documents for Efficient Information Extraction
- **Arxiv ID**: http://arxiv.org/abs/1911.05870v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.05870v1)
- **Published**: 2019-11-14 00:44:55+00:00
- **Updated**: 2019-11-14 00:44:55+00:00
- **Authors**: Kushagra Mahajan, Monika Sharma, Lovekesh Vig
- **Comment**: 6 pages, 4 figures
- **Journal**: None
- **Summary**: Precise homography estimation between multiple images is a pre-requisite for many computer vision applications. One application that is particularly relevant in today's digital era is the alignment of scanned or camera-captured document images such as insurance claim forms for information extraction. Traditional learning based approaches perform poorly due to the absence of an appropriate gradient. Feature based keypoint extraction techniques for homography estimation in real scene images either detect an extremely large number of inconsistent keypoints due to sharp textual edges, or produce inaccurate keypoint correspondences due to variations in illumination and viewpoint differences between document images. In this paper, we propose a novel algorithm for aligning scanned or camera-captured document images using character based keypoints and a reference template. The algorithm is both fast and accurate and utilizes a standard Optical character recognition (OCR) engine such as Tesseract to find character based unambiguous keypoints, which are utilized to identify precise keypoint correspondences between two images. Finally, the keypoints are used to compute the homography mapping between a test document and a template. We evaluated the proposed approach for information extraction on two real world anonymized datasets comprised of health insurance claim forms and the results support the viability of the proposed technique.



### LiDAR ICPS-net: Indoor Camera Positioning based-on Generative Adversarial Network for RGB to Point-Cloud Translation
- **Arxiv ID**: http://arxiv.org/abs/1911.05871v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.05871v1)
- **Published**: 2019-11-14 00:46:17+00:00
- **Updated**: 2019-11-14 00:46:17+00:00
- **Authors**: Ali Ghofrani, Rahil Mahdian Toroghi, Seyed Mojtaba Tabatabaie, Seyed Maziar Tabasi
- **Comment**: 5 pages, 10 figures, ICASSP2020 Conference
- **Journal**: None
- **Summary**: Indoor positioning aims at navigation inside areas with no GPS-data availability and could be employed in many applications such as augmented reality, autonomous driving specially inside closed areas and tunnels. In this paper, a deep neural network-based architecture has been proposed to address this problem. In this regard, a tandem set of convolutional neural networks, as well as a Pix2Pix GAN network have been leveraged to perform as the scene classifier, scene RGB image to point cloud converter, and position regressor, respectively. The proposed architecture outperforms the previous works, including our recent work, in the sense that it makes data generation task easier and more robust against scene small variations, whilst the accuracy of the positioning is remarkably well, for both Cartesian position and quaternion information of the camera.



### RWF-2000: An Open Large Scale Video Database for Violence Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.05913v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.05913v3)
- **Published**: 2019-11-14 02:59:09+00:00
- **Updated**: 2020-10-19 05:37:15+00:00
- **Authors**: Ming Cheng, Kunjing Cai, Ming Li
- **Comment**: Accepted by ICPR'20
- **Journal**: None
- **Summary**: In recent years, surveillance cameras are widely deployed in public places, and the general crime rate has been reduced significantly due to these ubiquitous devices. Usually, these cameras provide cues and evidence after crimes are conducted, while they are rarely used to prevent or stop criminal activities in time. It is both time and labor consuming to manually monitor a large amount of video data from surveillance cameras. Therefore, automatically recognizing violent behaviors from video signals becomes essential. This paper summarizes several existing video datasets for violence detection and proposes the RWF-2000 database with 2,000 videos captured by surveillance cameras in real-world scenes. Also, we present a new method that utilizes both the merits of 3D-CNNs and optical flow, namely Flow Gated Network. The proposed approach obtains an accuracy of 87.25% on the test set of our proposed database. The database and source codes are currently open to access.



### Adversarial Margin Maximization Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.05916v1
- **DOI**: 10.1109/TPAMI.2019.2948348
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.05916v1)
- **Published**: 2019-11-14 03:13:17+00:00
- **Updated**: 2019-11-14 03:13:17+00:00
- **Authors**: Ziang Yan, Yiwen Guo, Changshui Zhang
- **Comment**: 11 pages + 1 page appendix, accepted by T-PAMI
- **Journal**: None
- **Summary**: The tremendous recent success of deep neural networks (DNNs) has sparked a surge of interest in understanding their predictive ability. Unlike the human visual system which is able to generalize robustly and learn with little supervision, DNNs normally require a massive amount of data to learn new concepts. In addition, research works also show that DNNs are vulnerable to adversarial examples-maliciously generated images which seem perceptually similar to the natural ones but are actually formed to fool learning models, which means the models have problem generalizing to unseen data with certain type of distortions. In this paper, we analyze the generalization ability of DNNs comprehensively and attempt to improve it from a geometric point of view. We propose adversarial margin maximization (AMM), a learning-based regularization which exploits an adversarial perturbation as a proxy. It encourages a large margin in the input space, just like the support vector machines. With a differentiable formulation of the perturbation, we train the regularized DNNs simply through back-propagation in an end-to-end manner. Experimental results on various datasets (including MNIST, CIFAR-10/100, SVHN and ImageNet) and different DNN architectures demonstrate the superiority of our method over previous state-of-the-arts. Code and models for reproducing our results will be made publicly available.



### Understanding the Disharmony between Weight Normalization Family and Weight Decay: $Îµ-$shifted $L_2$ Regularizer
- **Arxiv ID**: http://arxiv.org/abs/1911.05920v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.05920v1)
- **Published**: 2019-11-14 03:31:13+00:00
- **Updated**: 2019-11-14 03:31:13+00:00
- **Authors**: Li Xiang, Chen Shuo, Xia Yan, Yang Jian
- **Comment**: 12 pages, 9 figures
- **Journal**: None
- **Summary**: The merits of fast convergence and potentially better performance of the weight normalization family have drawn increasing attention in recent years. These methods use standardization or normalization that changes the weight $\boldsymbol{W}$ to $\boldsymbol{W}'$, which makes $\boldsymbol{W}'$ independent to the magnitude of $\boldsymbol{W}$. Surprisingly, $\boldsymbol{W}$ must be decayed during gradient descent, otherwise we will observe a severe under-fitting problem, which is very counter-intuitive since weight decay is widely known to prevent deep networks from over-fitting. In this paper, we \emph{theoretically} prove that the weight decay term $\frac{1}{2}\lambda||{\boldsymbol{W}}||^2$ merely modulates the effective learning rate for improving objective optimization, and has no influence on generalization when the weight normalization family is compositely employed. Furthermore, we also expose several critical problems when introducing weight decay term to weight normalization family, including the missing of global minimum and training instability. To address these problems, we propose an $\epsilon-$shifted $L_2$ regularizer, which shifts the $L_2$ objective by a positive constant $\epsilon$. Such a simple operation can theoretically guarantee the existence of global minimum, while preventing the network weights from being too small and thus avoiding gradient float overflow. It significantly improves the training stability and can achieve slightly better performance in our practice. The effectiveness of $\epsilon-$shifted $L_2$ regularizer is comprehensively validated on the ImageNet, CIFAR-100, and COCO datasets. Our codes and pretrained models will be released in https://github.com/implus/PytorchInsight.



### VisionISP: Repurposing the Image Signal Processor for Computer Vision Applications
- **Arxiv ID**: http://arxiv.org/abs/1911.05931v1
- **DOI**: 10.1109/ICIP.2019.8803607
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.05931v1)
- **Published**: 2019-11-14 04:19:28+00:00
- **Updated**: 2019-11-14 04:19:28+00:00
- **Authors**: Chyuan-Tyng Wu, Leo F. Isikdogan, Sushma Rao, Bhavin Nayak, Timo Gerasimow, Aleksandar Sutic, Liron Ain-kedem, Gilad Michael
- **Comment**: None
- **Journal**: IEEE International Conference on Image Processing (ICIP), 2019,
  pp. 4624-4628
- **Summary**: Traditional image signal processors (ISPs) are primarily designed and optimized to improve the image quality perceived by humans. However, optimal perceptual image quality does not always translate into optimal performance for computer vision applications. We propose a set of methods, which we collectively call VisionISP, to repurpose the ISP for machine consumption. VisionISP significantly reduces data transmission needs by reducing the bit-depth and resolution while preserving the relevant information. The blocks in VisionISP are simple, content-aware, and trainable. Experimental results show that VisionISP boosts the performance of a subsequent computer vision system trained to detect objects in an autonomous driving setting. The results demonstrate the potential and the practicality of VisionISP for computer vision applications.



### GIFT: Learning Transformation-Invariant Dense Visual Descriptors via Group CNNs
- **Arxiv ID**: http://arxiv.org/abs/1911.05932v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.05932v1)
- **Published**: 2019-11-14 04:20:57+00:00
- **Updated**: 2019-11-14 04:20:57+00:00
- **Authors**: Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, Xiaowei Zhou
- **Comment**: Accepted by NeurIPS 2019
- **Journal**: None
- **Summary**: Finding local correspondences between images with different viewpoints requires local descriptors that are robust against geometric transformations. An approach for transformation invariance is to integrate out the transformations by pooling the features extracted from transformed versions of an image. However, the feature pooling may sacrifice the distinctiveness of the resulting descriptors. In this paper, we introduce a novel visual descriptor named Group Invariant Feature Transform (GIFT), which is both discriminative and robust to geometric transformations. The key idea is that the features extracted from the transformed versions of an image can be viewed as a function defined on the group of the transformations. Instead of feature pooling, we use group convolutions to exploit underlying structures of the extracted features on the group, resulting in descriptors that are both discriminative and provably invariant to the group of transformations. Extensive experiments show that GIFT outperforms state-of-the-art methods on several benchmark datasets and practically improves the performance of relative pose estimation.



### SimVODIS: Simultaneous Visual Odometry, Object Detection, and Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.05939v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.05939v2)
- **Published**: 2019-11-14 05:03:47+00:00
- **Updated**: 2019-11-16 10:39:06+00:00
- **Authors**: Ue-Hwan Kim, Se-Ho Kim, Jong-Hwan Kim
- **Comment**: Submitted to TPAMI
- **Journal**: None
- **Summary**: Intelligent agents need to understand the surrounding environment to provide meaningful services to or interact intelligently with humans. The agents should perceive geometric features as well as semantic entities inherent in the environment. Contemporary methods in general provide one type of information regarding the environment at a time, making it difficult to conduct high-level tasks. Moreover, running two types of methods and associating two resultant information requires a lot of computation and complicates the software architecture. To overcome these limitations, we propose a neural architecture that simultaneously performs both geometric and semantic tasks in a single thread: simultaneous visual odometry, object detection, and instance segmentation (SimVODIS). Training SimVODIS requires unlabeled video sequences and the photometric consistency between input image frames generates self-supervision signals. The performance of SimVODIS outperforms or matches the state-of-the-art performance in pose estimation, depth map prediction, object detection, and instance segmentation tasks while completing all the tasks in a single thread. We expect SimVODIS would enhance the autonomy of intelligent agents and let the agents provide effective services to humans.



### Progressive Feature Polishing Network for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.05942v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.05942v1)
- **Published**: 2019-11-14 05:22:12+00:00
- **Updated**: 2019-11-14 05:22:12+00:00
- **Authors**: Bo Wang, Quan Chen, Min Zhou, Zhiqiang Zhang, Xiaogang Jin, Kun Gai
- **Comment**: Accepted by AAAI 2020
- **Journal**: None
- **Summary**: Feature matters for salient object detection. Existing methods mainly focus on designing a sophisticated structure to incorporate multi-level features and filter out cluttered features. We present Progressive Feature Polishing Network (PFPN), a simple yet effective framework to progressively polish the multi-level features to be more accurate and representative. By employing multiple Feature Polishing Modules (FPMs) in a recurrent manner, our approach is able to detect salient objects with fine details without any post-processing. A FPM parallelly updates the features of each level by directly incorporating all higher level context information. Moreover, it can keep the dimensions and hierarchical structures of the feature maps, which makes it flexible to be integrated with any CNN-based models. Empirical experiments show that our results are monotonically getting better with increasing number of FPMs. Without bells and whistles, PFPN outperforms the state-of-the-art methods significantly on five benchmark datasets under various evaluation metrics.



### A Scalable Approach for Facial Action Unit Classifier Training UsingNoisy Data for Pre-Training
- **Arxiv ID**: http://arxiv.org/abs/1911.05946v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.05946v1)
- **Published**: 2019-11-14 05:39:59+00:00
- **Updated**: 2019-11-14 05:39:59+00:00
- **Authors**: Alberto Fung, Daniel McDuff
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning systems are being used to automate many types of laborious labeling tasks. Facial actioncoding is an example of such a labeling task that requires copious amounts of time and a beyond average level of human domain expertise. In recent years, the use of end-to-end deep neural networks has led to significant improvements in action unit recognition performance and many network architectures have been proposed. Do the more complex deep neural network(DNN) architectures perform sufficiently well to justify the additional complexity? We show that pre-training on a large diverse set of noisy data can result in even a simple CNN model improving over the current state-of-the-art DNN architectures.The average F1-score achieved with our proposed method on the DISFA dataset is 0.60, compared to a previous state-of-the-art of 0.57. Additionally, we show how the number of subjects and number of images used for pre-training impacts the model performance. The approach that we have outlined is open-source, highly scalable, and not dependent on the model architecture. We release the code and data: https://github.com/facialactionpretrain/facs.



### HUSE: Hierarchical Universal Semantic Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1911.05978v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.05978v1)
- **Published**: 2019-11-14 07:45:32+00:00
- **Updated**: 2019-11-14 07:45:32+00:00
- **Authors**: Pradyumna Narayana, Aniket Pednekar, Abishek Krishnamoorthy, Kazoo Sone, Sugato Basu
- **Comment**: None
- **Journal**: None
- **Summary**: There is a recent surge of interest in cross-modal representation learning corresponding to images and text. The main challenge lies in mapping images and text to a shared latent space where the embeddings corresponding to a similar semantic concept lie closer to each other than the embeddings corresponding to different semantic concepts, irrespective of the modality. Ranking losses are commonly used to create such shared latent space -- however, they do not impose any constraints on inter-class relationships resulting in neighboring clusters to be completely unrelated. The works in the domain of visual semantic embeddings address this problem by first constructing a semantic embedding space based on some external knowledge and projecting image embeddings onto this fixed semantic embedding space. These works are confined only to image domain and constraining the embeddings to a fixed space adds additional burden on learning. This paper proposes a novel method, HUSE, to learn cross-modal representation with semantic information. HUSE learns a shared latent space where the distance between any two universal embeddings is similar to the distance between their corresponding class embeddings in the semantic embedding space. HUSE also uses a classification objective with a shared classification layer to make sure that the image and text embeddings are in the same shared latent space. Experiments on UPMC Food-101 show our method outperforms previous state-of-the-art on retrieval, hierarchical precision and classification results.



### A Neural Network Based on the Johnson $S_\mathrm{U}$ Translation System and Related Application to Electromyogram Classification
- **Arxiv ID**: http://arxiv.org/abs/1912.04218v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.04218v1)
- **Published**: 2019-11-14 10:28:37+00:00
- **Updated**: 2019-11-14 10:28:37+00:00
- **Authors**: Hideaki Hayashi, Taro Shibanoki, Toshio Tsuji
- **Comment**: None
- **Journal**: None
- **Summary**: Electromyogram (EMG) classification is a key technique in EMG-based control systems. The existing EMG classification methods do not consider the characteristics of EMG features that the distribution has skewness and kurtosis, causing drawbacks such as the requirement of hyperparameter tuning. In this paper, we propose a neural network based on the Johnson $S_\mathrm{U}$ translation system that is capable of representing distributions with skewness and kurtosis. The Johnson system is a normalizing translation that transforms non-normal data to a normal distribution, thereby enabling the representation of a wide range of distributions. In this study, a discriminative model based on the multivariate Johnson $S_\mathrm{U}$ translation system is transformed into a linear combination of coefficients and input vectors using log-linearization. This is then incorporated into a neural network structure, thereby allowing the calculation of the posterior probability of the input vectors for each class and the determination of model parameters as weight coefficients of the network. The uniqueness of convergence of the network learning is theoretically guaranteed. In the experiments, the suitability of the proposed network for distributions including skewness and kurtosis is evaluated using artificially generated data. Its applicability for real biological data is also evaluated via an EMG classification experiment. The results show that the proposed network achieves high classification performance without the need for hyperparameter optimization.



### CAGFuzz: Coverage-Guided Adversarial Generative Fuzzing Testing of Deep Learning Systems
- **Arxiv ID**: http://arxiv.org/abs/1911.07931v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07931v2)
- **Published**: 2019-11-14 10:32:43+00:00
- **Updated**: 2020-05-21 02:32:29+00:00
- **Authors**: Pengcheng Zhang, Qiyin Dai, Patrizio Pelliccione
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Learning systems (DL) based on Deep Neural Networks (DNNs) are more and more used in various aspects of our life, including unmanned vehicles, speech processing, and robotics. However, due to the limited dataset and the dependence on manual labeling data, DNNs often fail to detect their erroneous behaviors, which may lead to serious problems. Several approaches have been proposed to enhance the input examples for testing DL systems. However, they have the following limitations. First, they design and generate adversarial examples from the perspective of model, which may cause low generalization ability when they are applied to other models. Second, they only use surface feature constraints to judge the difference between the adversarial example generated and the original example. The deep feature constraints, which contain high-level semantic information, such as image object category and scene semantics are completely neglected. To address these two problems, in this paper, we propose CAGFuzz, a Coverage-guided Adversarial Generative Fuzzing testing approach, which generates adversarial examples for a targeted DNN to discover its potential defects. First, we train an adversarial case generator (AEG) from the perspective of general data set. Second, we extract the depth features of the original and adversarial examples, and constrain the adversarial examples by cosine similarity to ensure that the semantic information of adversarial examples remains unchanged. Finally, we retrain effective adversarial examples to improve neuron testing coverage rate. Based on several popular data sets, we design a set of dedicated experiments to evaluate CAGFuzz. The experimental results show that CAGFuzz can improve the neuron coverage rate, detect hidden errors, and also improve the accuracy of the target DNN.



### Self-Supervised Learning For Few-Shot Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1911.06045v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06045v3)
- **Published**: 2019-11-14 11:24:47+00:00
- **Updated**: 2021-02-23 08:02:35+00:00
- **Authors**: Da Chen, Yuefeng Chen, Yuhong Li, Feng Mao, Yuan He, Hui Xue
- **Comment**: To appear at ICASSP 2021. https://github.com/phecy/ssl-few-shot
- **Journal**: None
- **Summary**: Few-shot image classification aims to classify unseen classes with limited labelled samples. Recent works benefit from the meta-learning process with episodic tasks and can fast adapt to class from training to testing. Due to the limited number of samples for each task, the initial embedding network for meta-learning becomes an essential component and can largely affect the performance in practice. To this end, most of the existing methods highly rely on the efficient embedding network. Due to the limited labelled data, the scale of embedding network is constrained under a supervised learning(SL) manner which becomes a bottleneck of the few-shot learning methods. In this paper, we proposed to train a more generalized embedding network with self-supervised learning (SSL) which can provide robust representation for downstream tasks by learning from the data itself. We evaluate our work by extensive comparisons with previous baseline methods on two few-shot classification datasets ({\em i.e.,} MiniImageNet and CUB) and achieve better performance over baselines. Tests on four datasets in cross-domain few-shot learning classification show that the proposed method achieves state-of-the-art results and further prove the robustness of the proposed model. Our code is available at \hyperref[https://github.com/phecy/SSL-FEW-SHOT.]{https://github.com/phecy/SSL-FEW-SHOT.}



### Semantic Granularity Metric Learning for Visual Search
- **Arxiv ID**: http://arxiv.org/abs/1911.06047v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.06047v1)
- **Published**: 2019-11-14 11:36:16+00:00
- **Updated**: 2019-11-14 11:36:16+00:00
- **Authors**: Dipu Manandhar, Muhammet Bastan, Kim-Hui Yap
- **Comment**: 10 pages, 10 figures
- **Journal**: None
- **Summary**: Deep metric learning applied to various applications has shown promising results in identification, retrieval and recognition. Existing methods often do not consider different granularity in visual similarity. However, in many domain applications, images exhibit similarity at multiple granularities with visual semantic concepts, e.g. fashion demonstrates similarity ranging from clothing of the exact same instance to similar looks/design or a common category. Therefore, training image triplets/pairs used for metric learning inherently possess different degree of information. However, the existing methods often treats them with equal importance during training. This hinders capturing the underlying granularities in feature similarity required for effective visual search.   In view of this, we propose a new deep semantic granularity metric learning (SGML) that develops a novel idea of leveraging attribute semantic space to capture different granularity of similarity, and then integrate this information into deep metric learning. The proposed method simultaneously learns image attributes and embeddings using multitask CNNs. The two tasks are not only jointly optimized but are further linked by the semantic granularity similarity mappings to leverage the correlations between the tasks. To this end, we propose a new soft-binomial deviance loss that effectively integrates the degree of information in training samples, which helps to capture visual similarity at multiple granularities. Compared to recent ensemble-based methods, our framework is conceptually elegant, computationally simple and provides better performance. We perform extensive experiments on benchmark metric learning datasets and demonstrate that our method outperforms recent state-of-the-art methods, e.g., 1-4.5\% improvement in Recall@1 over the previous state-of-the-arts [1],[2] on DeepFashion In-Shop dataset.



### Copy-Move Forgery Classification via Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1911.07932v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07932v1)
- **Published**: 2019-11-14 11:49:21+00:00
- **Updated**: 2019-11-14 11:49:21+00:00
- **Authors**: Akash Kumar, Arnav Bhavsar
- **Comment**: None
- **Journal**: None
- **Summary**: In the current era, image manipulation is becoming increasingly easier, yielding more natural looking images, owing to the modern tools in image processing and computer vision techniques. The task of the segregation of forged images has become very challenging. To tackle such problems, publicly available datasets are insufficient. In this paper, we propose to create a synthetic forged dataset using deep semantic image inpainting algorithm. Furthermore, we use an unsupervised domain adaptation network to detect copy-move forgery in images. Our approach can be helpful in those cases, where the classification of data is unavailable.



### Efficient ConvNet-based Object Detection for Unmanned Aerial Vehicles by Selective Tile Processing
- **Arxiv ID**: http://arxiv.org/abs/1911.06073v1
- **DOI**: 10.1145/3243394.3243692
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06073v1)
- **Published**: 2019-11-14 12:50:27+00:00
- **Updated**: 2019-11-14 12:50:27+00:00
- **Authors**: George Plastiras, Christos Kyrkou, Theocharis Theocharides
- **Comment**: George Plastiras, Christos Kyrkou, and Theocharis Theocharides. 2018.
  Efficient ConvNet-based Object Detection for Unmanned Aerial Vehicles by
  Selective Tile Processing. In Proceedings of the 12th International
  Conference on Distributed Smart Cameras (ICDSC '18). ACM, New York, NY, USA,
  Article 3, 6 pages
- **Journal**: In Proceedings of the 12th International Conference on Distributed
  Smart Cameras (ICDSC 2018)
- **Summary**: Many applications utilizing Unmanned Aerial Vehicles (UAVs) require the use of computer vision algorithms to analyze the information captured from their on-board camera. Recent advances in deep learning have made it possible to use single-shot Convolutional Neural Network (CNN) detection algorithms that process the input image to detect various objects of interest. To keep the computational demands low these neural networks typically operate on small image sizes which, however, makes it difficult to detect small objects. This is further emphasized when considering UAVs equipped with cameras where due to the viewing range, objects tend to appear relatively small. This paper therefore, explores the trade-offs involved when maintaining the resolution of the objects of interest by extracting smaller patches (tiles) from the larger input image and processing them using a neural network. Specifically, we introduce an attention mechanism to focus on detecting objects only in some of the tiles and a memory mechanism to keep track of information for tiles that are not processed. Through the analysis of different methods and experiments we show that by carefully selecting which tiles to process we can considerably improve the detection accuracy while maintaining comparable performance to CNNs that resize and process a single image which makes the proposed approach suitable for UAV applications.



### CMSN: Continuous Multi-stage Network and Variable Margin Cosine Loss for Temporal Action Proposal Generation
- **Arxiv ID**: http://arxiv.org/abs/1911.06080v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06080v3)
- **Published**: 2019-11-14 13:08:30+00:00
- **Updated**: 2019-11-20 02:22:45+00:00
- **Authors**: Yushuai Hu, Yaochu Jin, Runhua Li, Xiangxiang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Accurately locating the start and end time of an action in untrimmed videos is a challenging task. One of the important reasons is the boundary of action is not highly distinguishable, and the features around the boundary are difficult to discriminate. To address this problem, we propose a novel framework for temporal action proposal generation, namely Continuous Multi-stage Network (CMSN), which divides a video that contains a complete action instance into six stages, namely Backgroud, Ready, Start, Confirm, End, Follow. To distinguish between Ready and Start, End and Follow more accurately, we propose a novel loss function, Variable Margin Cosine Loss (VMCL), which allows for different margins between different categories. Our experiments on THUMOS14 show that the proposed method for temporal proposal generation performs better than the state-of-the-art methods using the same network architecture and training dataset.



### PI-RCNN: An Efficient Multi-sensor 3D Object Detector with Point-based Attentive Cont-conv Fusion Module
- **Arxiv ID**: http://arxiv.org/abs/1911.06084v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06084v3)
- **Published**: 2019-11-14 13:19:12+00:00
- **Updated**: 2019-12-02 02:34:42+00:00
- **Authors**: Liang Xie, Chao Xiang, Zhengxu Yu, Guodong Xu, Zheng Yang, Deng Cai, Xiaofei He
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: LIDAR point clouds and RGB-images are both extremely essential for 3D object detection. So many state-of-the-art 3D detection algorithms dedicate in fusing these two types of data effectively. However, their fusion methods based on Birds Eye View (BEV) or voxel format are not accurate. In this paper, we propose a novel fusion approach named Point-based Attentive Cont-conv Fusion(PACF) module, which fuses multi-sensor features directly on 3D points. Except for continuous convolution, we additionally add a Point-Pooling and an Attentive Aggregation to make the fused features more expressive. Moreover, based on the PACF module, we propose a 3D multi-sensor multi-task network called Pointcloud-Image RCNN(PI-RCNN as brief), which handles the image segmentation and 3D object detection tasks. PI-RCNN employs a segmentation sub-network to extract full-resolution semantic feature maps from images and then fuses the multi-sensor features via powerful PACF module. Beneficial from the effectiveness of the PACF module and the expressive semantic features from the segmentation module, PI-RCNN can improve much in 3D object detection. We demonstrate the effectiveness of the PACF module and PI-RCNN on the KITTI 3D Detection benchmark, and our method can achieve state-of-the-art on the metric of 3D AP.



### EdgeNet: Balancing Accuracy and Performance for Edge-based Convolutional Neural Network Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/1911.06091v1
- **DOI**: 10.1145/3349801.3349809
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06091v1)
- **Published**: 2019-11-14 13:49:23+00:00
- **Updated**: 2019-11-14 13:49:23+00:00
- **Authors**: George Plastiras, Christos Kyrkou, Theocharis Theocharides
- **Comment**: George Plastiras, Christos Kyrkou, and Theocharis Theocharides. 2019.
  EdgeNet: Balancing Accuracy and Performance for Edge-based Convolutional
  Neural Network Object Detectors. In Proceedings of the 13th International
  Conference on Distributed Smart Cameras (ICDSC 2019). ACM, New York, NY, USA,
  Article 8, 6 pages
- **Journal**: In Proceedings of the 13th International Conference on Distributed
  Smart Cameras (ICDSC 2019)
- **Summary**: Visual intelligence at the edge is becoming a growing necessity for low latency applications and situations where real-time decision is vital. Object detection, the first step in visual data analytics, has enjoyed significant improvements in terms of state-of-the-art accuracy due to the emergence of Convolutional Neural Networks (CNNs) and Deep Learning. However, such complex paradigms intrude increasing computational demands and hence prevent their deployment on resource-constrained devices. In this work, we propose a hierarchical framework that enables to detect objects in high-resolution video frames, and maintain the accuracy of state-of-the-art CNN-based object detectors while outperforming existing works in terms of processing speed when targeting a low-power embedded processor using an intelligent data reduction mechanism. Moreover, a use-case for pedestrian detection from Unmanned-Areal-Vehicle (UAV) is presented showing the impact that the proposed approach has on sensitivity, average processing time and power consumption when is implemented on different platforms. Using the proposed selection process our framework manages to reduce the processed data by 100x leading to under 4W power consumption on different edge devices.



### Towards Pose-invariant Lip-Reading
- **Arxiv ID**: http://arxiv.org/abs/1911.06095v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06095v1)
- **Published**: 2019-11-14 13:57:33+00:00
- **Updated**: 2019-11-14 13:57:33+00:00
- **Authors**: Shiyang Cheng, Pingchuan Ma, Georgios Tzimiropoulos, Stavros Petridis, Adrian Bulat, Jie Shen, Maja Pantic
- **Comment**: 6 pages, 2 figures
- **Journal**: None
- **Summary**: Lip-reading models have been significantly improved recently thanks to powerful deep learning architectures. However, most works focused on frontal or near frontal views of the mouth. As a consequence, lip-reading performance seriously deteriorates in non-frontal mouth views. In this work, we present a framework for training pose-invariant lip-reading models on synthetic data instead of collecting and annotating non-frontal data which is costly and tedious. The proposed model significantly outperforms previous approaches on non-frontal views while retaining the superior performance on frontal and near frontal mouth views. Specifically, we propose to use a 3D Morphable Model (3DMM) to augment LRW, an existing large-scale but mostly frontal dataset, by generating synthetic facial data in arbitrary poses. The newly derived dataset, is used to train a state-of-the-art neural network for lip-reading. We conducted a cross-database experiment for isolated word recognition on the LRS2 dataset, and reported an absolute improvement of 2.55%. The benefit of the proposed approach becomes clearer in extreme poses where an absolute improvement of up to 20.64% over the baseline is achieved.



### CartoonRenderer: An Instance-based Multi-Style Cartoon Image Translator
- **Arxiv ID**: http://arxiv.org/abs/1911.06102v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06102v1)
- **Published**: 2019-11-14 14:15:14+00:00
- **Updated**: 2019-11-14 14:15:14+00:00
- **Authors**: Yugang Chen, Muchun Chen, Chaoyue Song, Bingbing Ni
- **Comment**: 26th International Conference on Multimedia Modeling(MMM2020)
- **Journal**: None
- **Summary**: Instance based photo cartoonization is one of the challenging image stylization tasks which aim at transforming realistic photos into cartoon style images while preserving the semantic contents of the photos. State-of-the-art Deep Neural Networks (DNNs) methods still fail to produce satisfactory results with input photos in the wild, especially for photos which have high contrast and full of rich textures. This is due to that: cartoon style images tend to have smooth color regions and emphasized edges which are contradict to realistic photos which require clear semantic contents, i.e., textures, shapes etc. Previous methods have difficulty in satisfying cartoon style textures and preserving semantic contents at the same time. In this work, we propose a novel "CartoonRenderer" framework which utilizing a single trained model to generate multiple cartoon styles. In a nutshell, our method maps photo into a feature model and renders the feature model back into image space. In particular, cartoonization is achieved by conducting some transformation manipulation in the feature space with our proposed Soft-AdaIN. Extensive experimental results show our method produces higher quality cartoon style images than prior arts, with accurate semantic content preservation. In addition, due to the decoupling of whole generating process into "Modeling-Coordinating-Rendering" parts, our method could easily process higher resolution photos, which is intractable for existing methods.



### Convolutional Neural Network for Convective Storm Nowcasting Using 3D Doppler Weather Radar Data
- **Arxiv ID**: http://arxiv.org/abs/1911.06185v2
- **DOI**: 10.1109/TGRS.2019.2948070
- **Categories**: **physics.geo-ph**, cs.CV, physics.ao-ph
- **Links**: [PDF](http://arxiv.org/pdf/1911.06185v2)
- **Published**: 2019-11-14 15:42:12+00:00
- **Updated**: 2020-01-03 08:05:16+00:00
- **Authors**: Lei Han, Juanzhen Sun, Wei Zhang
- **Comment**: This version of the paper has some fatal errors that need to be
  carefully corrected
- **Journal**: None
- **Summary**: Convective storms are one of the severe weather hazards found during the warm season. Doppler weather radar is the only operational instrument that can frequently sample the detailed structure of convective storm which has a small spatial scale and short lifetime. For the challenging task of short-term convective storm forecasting, 3-D radar images contain information about the processes in convective storm. However, effectively extracting such information from multisource raw data has been problematic due to a lack of methodology and computation limitations. Recent advancements in deep learning techniques and graphics processing units now make it possible. This article investigates the feasibility and performance of an end-to-end deep learning nowcasting method. The nowcasting problem was transformed into a classification problem first, and then, a deep learning method that uses a convolutional neural network was presented to make predictions. On the first layer of CNN, a cross-channel 3D convolution was proposed to fuse 3D raw data. The CNN method eliminates the handcrafted feature engineering, i.e., the process of using domain knowledge of the data to manually design features. Operationally produced historical data of the Beijing-Tianjin-Hebei region in China was used to train the nowcasting system and evaluate its performance; 3737332 samples were collected in the training data set. The experimental results show that the deep learning method improves nowcasting skills compared with traditional machine learning methods.



### SiamFC++: Towards Robust and Accurate Visual Tracking with Target Estimation Guidelines
- **Arxiv ID**: http://arxiv.org/abs/1911.06188v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06188v4)
- **Published**: 2019-11-14 15:43:37+00:00
- **Updated**: 2020-04-02 14:01:38+00:00
- **Authors**: Yinda Xu, Zeyu Wang, Zuoxin Li, Ye Yuan, Gang Yu
- **Comment**: Accepted by AAAI 2020
- **Journal**: None
- **Summary**: Visual tracking problem demands to efficiently perform robust classification and accurate target state estimation over a given target at the same time. Former methods have proposed various ways of target state estimation, yet few of them took the particularity of the visual tracking problem itself into consideration. After a careful analysis, we propose a set of practical guidelines of target state estimation for high-performance generic object tracker design. Following these guidelines, we design our Fully Convolutional Siamese tracker++ (SiamFC++) by introducing both classification and target state estimation branch(G1), classification score without ambiguity(G2), tracking without prior knowledge(G3), and estimation quality score(G4). Extensive analysis and ablation studies demonstrate the effectiveness of our proposed guidelines. Without bells and whistles, our SiamFC++ tracker achieves state-of-the-art performance on five challenging benchmarks(OTB2015, VOT2018, LaSOT, GOT-10k, TrackingNet), which proves both the tracking and generalization ability of the tracker. Particularly, on the large-scale TrackingNet dataset, SiamFC++ achieves a previously unseen AUC score of 75.4 while running at over 90 FPS, which is far above the real-time requirement. Code and models are available at: https://github.com/MegviiDetection/video_analyst .



### An Improved Tobit Kalman Filter with Adaptive Censoring Limits
- **Arxiv ID**: http://arxiv.org/abs/1911.06190v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, math.ST, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/1911.06190v1)
- **Published**: 2019-11-14 15:45:06+00:00
- **Updated**: 2019-11-14 15:45:06+00:00
- **Authors**: Kostas Loumponias, Nicholas Vretos, George Tsaklidis, Petros Daras
- **Comment**: 21 pages, 32 figures
- **Journal**: None
- **Summary**: This paper deals with the Tobit Kalman filtering (TKF) process when the measurements are correlated and censored. The case of interval censoring, i.e., the case of measurements which belong to some interval with given censoring limits, is considered. Two improvements of the standard TKF process are proposed, in order to estimate the hidden state vectors. Firstly, the exact covariance matrix of the censored measurements is calculated by taking into account the censoring limits. Secondly, the probability of a latent (normally distributed) measurement to belong in or out of the uncensored region is calculated by taking into account the Kalman residual. The designed algorithm is tested using both synthetic and real data sets. The real data set includes human skeleton joints' coordinates captured by the Microsoft Kinect II sensor. In order to cope with certain real-life situations that cause problems in human skeleton tracking, such as (self)-occlusions, closely interacting persons etc., adaptive censoring limits are used in the proposed TKF process. Experiments show that the proposed method outperforms other filtering processes in minimizing the overall Root Mean Square Error (RMSE) for synthetic and real data sets.



### Detecting Invasive Ductal Carcinoma with Semi-Supervised Conditional GANs
- **Arxiv ID**: http://arxiv.org/abs/1911.06216v2
- **DOI**: 10.1007/978-3-030-63092-8
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.06216v2)
- **Published**: 2019-11-14 16:16:20+00:00
- **Updated**: 2021-03-30 20:30:36+00:00
- **Authors**: Jeremiah W. Johnson
- **Comment**: 5 pages, 3 figures
- **Journal**: Proceedings of the Future Technologies Conference (FTC) 2020, vol.
  3, pp.113-120
- **Summary**: Invasive ductal carcinoma (IDC) comprises nearly 80% of all breast cancers. The detection of IDC is a necessary preprocessing step in determining the aggressiveness of the cancer, determining treatment protocols, and predicting patient outcomes, and is usually performed manually by an expert pathologist. Here, we describe a novel algorithm for automatically detecting IDC using semi-supervised conditional generative adversarial networks (cGANs). The framework is simple and effective at improving scores on a range of metrics over a baseline CNN.



### Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA
- **Arxiv ID**: http://arxiv.org/abs/1911.06258v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1911.06258v3)
- **Published**: 2019-11-14 17:32:10+00:00
- **Updated**: 2020-03-24 23:59:59+00:00
- **Authors**: Ronghang Hu, Amanpreet Singh, Trevor Darrell, Marcus Rohrbach
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Many visual scenes contain text that carries crucial information, and it is thus essential to understand text in images for downstream reasoning tasks. For example, a deep water label on a warning sign warns people about the danger in the scene. Recent work has explored the TextVQA task that requires reading and understanding text in images to answer a question. However, existing approaches for TextVQA are mostly based on custom pairwise fusion mechanisms between a pair of two modalities and are restricted to a single prediction step by casting TextVQA as a classification task. In this work, we propose a novel model for the TextVQA task based on a multimodal transformer architecture accompanied by a rich representation for text in images. Our model naturally fuses different modalities homogeneously by embedding them into a common semantic space where self-attention is applied to model inter- and intra- modality context. Furthermore, it enables iterative answer decoding with a dynamic pointer network, allowing the model to form an answer through multi-step prediction instead of one-step classification. Our model outperforms existing approaches on three benchmark datasets for the TextVQA task by a large margin.



### Harnessing spatial MRI normalization: patch individual filter layers for CNNs
- **Arxiv ID**: http://arxiv.org/abs/1911.06278v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06278v1)
- **Published**: 2019-11-14 18:01:43+00:00
- **Updated**: 2019-11-14 18:01:43+00:00
- **Authors**: Fabian Eitel, Jan Philipp Albrecht, Friedemann Paul, Kerstin Ritter
- **Comment**: None
- **Journal**: Medical Imaging meets NeurIPS (MED-NeurIPS) 2019
- **Summary**: Neuroimaging studies based on magnetic resonance imaging (MRI) typically employ rigorous forms of preprocessing. Images are spatially normalized to a standard template using linear and non-linear transformations. Thus, one can assume that a patch at location (x, y, height, width) contains the same brain region across the entire data set. Most analyses applied on brain MRI using convolutional neural networks (CNNs) ignore this distinction from natural images. Here, we suggest a new layer type called patch individual filter (PIF) layer, which trains higher-level filters locally as we assume that more abstract features are locally specific after spatial normalization. We evaluate PIF layers on three different tasks, namely sex classification as well as either Alzheimer's disease (AD) or multiple sclerosis (MS) detection. We demonstrate that CNNs using PIF layers outperform their counterparts in several, especially low sample size settings.



### Self-Supervised Learning of State Estimation for Manipulating Deformable Linear Objects
- **Arxiv ID**: http://arxiv.org/abs/1911.06283v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.06283v3)
- **Published**: 2019-11-14 18:04:51+00:00
- **Updated**: 2020-10-06 04:06:53+00:00
- **Authors**: Mengyuan Yan, Yilin Zhu, Ning Jin, Jeannette Bohg
- **Comment**: v3: update acknowledgements
- **Journal**: None
- **Summary**: We demonstrate model-based, visual robot manipulation of linear deformable objects. Our approach is based on a state-space representation of the physical system that the robot aims to control. This choice has multiple advantages, including the ease of incorporating physics priors in the dynamics model and perception model, and the ease of planning manipulation actions. In addition, physical states can naturally represent object instances of different appearances. Therefore, dynamics in the state space can be learned in one setting and directly used in other visually different settings. This is in contrast to dynamics learned in pixel space or latent space, where generalization to visual differences are not guaranteed. Challenges in taking the state-space approach are the estimation of the high-dimensional state of a deformable object from raw images, where annotations are very expensive on real data, and finding a dynamics model that is both accurate, generalizable, and efficient to compute. We are the first to demonstrate self-supervised training of rope state estimation on real images, without requiring expensive annotations. This is achieved by our novel self-supervising learning objective, which is generalizable across a wide range of visual appearances. With estimated rope states, we train a fast and differentiable neural network dynamics model that encodes the physics of mass-spring systems. Our method has a higher accuracy in predicting future states compared to models that do not involve explicit state estimation and do not use any physics prior, while only using 3\% of training data. We also show that our approach achieves more efficient manipulation, both in simulation and on a real robot, when used within a model predictive controller.



### Fetal Head and Abdomen Measurement Using Convolutional Neural Network, Hough Transform, and Difference of Gaussian Revolved along Elliptical Path (Dogell) Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1911.06298v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.06298v1)
- **Published**: 2019-11-14 18:34:38+00:00
- **Updated**: 2019-11-14 18:34:38+00:00
- **Authors**: Kezia Irene, Aditya Yudha P., Harlan Haidi, Nurul Faza, Winston Chandra
- **Comment**: 5 pages, 9 figures
- **Journal**: None
- **Summary**: The number of fetal-neonatal death in Indonesia is still high compared to developed countries. This is caused by the absence of maternal monitoring during pregnancy. This paper presents an automated measurement for fetal head circumference (HC) and abdominal circumference (AC) from the ultrasonography (USG) image. This automated measurement is beneficial to detect early fetal abnormalities during the pregnancy period. We used the convolutional neural network (CNN) method, to preprocess the USG data. After that, we approximate the head and abdominal circumference using the Hough transform algorithm and the difference of Gaussian Revolved along Elliptical Path (Dogell) Algorithm. We used the data set from national hospitals in Indonesia and for the accuracy measurement, we compared our results to the annotated images measured by professional obstetricians. The result shows that by using CNN, we reduced errors caused by a noisy image. We found that the Dogell algorithm performs better than the Hough transform algorithm in both time and accuracy. This is the first HC and AC approximation that used the CNN method to preprocess the data.



### Question-Conditioned Counterfactual Image Generation for VQA
- **Arxiv ID**: http://arxiv.org/abs/1911.06352v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1911.06352v1)
- **Published**: 2019-11-14 19:37:33+00:00
- **Updated**: 2019-11-14 19:37:33+00:00
- **Authors**: Jingjing Pan, Yash Goyal, Stefan Lee
- **Comment**: Accepted by the VQA Workshop at CVPR 2019
- **Journal**: None
- **Summary**: While Visual Question Answering (VQA) models continue to push the state-of-the-art forward, they largely remain black-boxes - failing to provide insight into how or why an answer is generated. In this ongoing work, we propose addressing this shortcoming by learning to generate counterfactual images for a VQA model - i.e. given a question-image pair, we wish to generate a new image such that i) the VQA model outputs a different answer, ii) the new image is minimally different from the original, and iii) the new image is realistic. Our hope is that providing such counterfactual examples allows users to investigate and understand the VQA model's internal mechanisms.



### Give me (un)certainty -- An exploration of parameters that affect segmentation uncertainty
- **Arxiv ID**: http://arxiv.org/abs/1911.06357v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.06357v1)
- **Published**: 2019-11-14 19:52:08+00:00
- **Updated**: 2019-11-14 19:52:08+00:00
- **Authors**: Katharina Hoebel, Ken Chang, Jay Patel, Praveer Singh, Jayashree Kalpathy-Cramer
- **Comment**: Machine Learning for Health (ML4H) at NeurIPS 2019 - Extended
  Abstract
- **Journal**: None
- **Summary**: Segmentation tasks in medical imaging are inherently ambiguous: the boundary of a target structure is oftentimes unclear due to image quality and biological factors. As such, predicted segmentations from deep learning algorithms are inherently ambiguous. Additionally, "ground truth" segmentations performed by human annotators are in fact weak labels that further increase the uncertainty of outputs of supervised models developed on these manual labels. To date, most deep learning segmentation studies utilize predicted segmentations without uncertainty quantification. In contrast, we explore the use of Monte Carlo dropout U-Nets for the segmentation with additional quantification of segmentation uncertainty. We assess the utility of three measures of uncertainty (Coefficient of Variation, Mean Pairwise Dice, and Mean Voxelwise Uncertainty) for the segmentation of a less ambiguous target structure (liver) and a more ambiguous one (liver tumors). Furthermore, we assess how the utility of these measures changes with different patch sizes and cost functions. Our results suggest that models trained using larger patches and the weighted categorical cross-entropy as cost function allow the extraction of more meaningful uncertainty measures compared to smaller patches and soft dice loss. Among the three uncertainty measures Mean Pairwise Dice shows the strongest correlation with segmentation quality. Our study serves as a proof-of-concept of how uncertainty measures can be used to assess the quality of a predicted segmentation, potentially serving to flag low quality segmentations from a given model for further human review.



### Solving Inverse Problems by Joint Posterior Maximization with a VAE Prior
- **Arxiv ID**: http://arxiv.org/abs/1911.06379v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, eess.IV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1911.06379v1)
- **Published**: 2019-11-14 20:52:09+00:00
- **Updated**: 2019-11-14 20:52:09+00:00
- **Authors**: Mario GonzÃ¡lez, AndrÃ©s Almansa, Mauricio Delbracio, Pablo MusÃ©, Pauline Tan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we address the problem of solving ill-posed inverse problems in imaging where the prior is a neural generative model. Specifically we consider the decoupled case where the prior is trained once and can be reused for many different log-concave degradation models without retraining. Whereas previous MAP-based approaches to this problem lead to highly non-convex optimization algorithms, our approach computes the joint (space-latent) MAP that naturally leads to alternate optimization algorithms and to the use of a stochastic encoder to accelerate computations. The resulting technique is called JPMAP because it performs Joint Posterior Maximization using an Autoencoding Prior. We show theoretical and experimental evidence that the proposed objective function is quite close to bi-convex. Indeed it satisfies a weak bi-convexity property which is sufficient to guarantee that our optimization scheme converges to a stationary point.   Experimental results also show the higher quality of the solutions obtained by our JPMAP approach with respect to other non-convex MAP approaches which more often get stuck in spurious local optima.



### Contrast Phase Classification with a Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1911.06395v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.06395v1)
- **Published**: 2019-11-14 21:51:44+00:00
- **Updated**: 2019-11-14 21:51:44+00:00
- **Authors**: Yucheng Tang, Ho Hin Lee, Yuchen Xu, Olivia Tang, Yunqiang Chen, Dashan Gao, Shizhong Han, Riqiang Gao, Camilo Bermudez, Michael R. Savona, Richard G. Abramson, Yuankai Huo, Bennett A. Landman
- **Comment**: 8 pages, 4 figures
- **Journal**: SPIE2020
- **Summary**: Dynamic contrast enhanced computed tomography (CT) is an imaging technique that provides critical information on the relationship of vascular structure and dynamics in the context of underlying anatomy. A key challenge for image processing with contrast enhanced CT is that phase discrepancies are latent in different tissues due to contrast protocols, vascular dynamics, and metabolism variance. Previous studies with deep learning frameworks have been proposed for classifying contrast enhancement with networks inspired by computer vision. Here, we revisit the challenge in the context of whole abdomen contrast enhanced CTs. To capture and compensate for the complex contrast changes, we propose a novel discriminator in the form of a multi-domain disentangled representation learning network. The goal of this network is to learn an intermediate representation that separates contrast enhancement from anatomy and enables classification of images with varying contrast time. Briefly, our unpaired contrast disentangling GAN(CD-GAN) Discriminator follows the ResNet architecture to classify a CT scan from different enhancement phases. To evaluate the approach, we trained the enhancement phase classifier on 21060 slices from two clinical cohorts of 230 subjects. Testing was performed on 9100 slices from 30 independent subjects who had been imaged with CT scans from all contrast phases. Performance was quantified in terms of the multi-class normalized confusion matrix. The proposed network significantly improved correspondence over baseline UNet, ResNet50 and StarGAN performance of accuracy scores 0.54. 0.55, 0.62 and 0.91, respectively. The proposed discriminator from the disentangled network presents a promising technique that may allow deeper modeling of dynamic imaging against patient specific anatomies.



### Does Face Recognition Accuracy Get Better With Age? Deep Face Matchers Say No
- **Arxiv ID**: http://arxiv.org/abs/1911.06396v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06396v1)
- **Published**: 2019-11-14 21:52:54+00:00
- **Updated**: 2019-11-14 21:52:54+00:00
- **Authors**: VÃ­tor Albiero, Kevin W. Bowyer, Kushal Vangara, Michael C. King
- **Comment**: Paper will appear at the WACV 2020
- **Journal**: None
- **Summary**: Previous studies generally agree that face recognition accuracy is higher for older persons than for younger persons. But most previous studies were before the wave of deep learning matchers, and most considered accuracy only in terms of the verification rate for genuine pairs. This paper investigates accuracy for age groups 16-29, 30-49 and 50-70, using three modern deep CNN matchers, and considers differences in the impostor and genuine distributions as well as verification rates and ROC curves. We find that accuracy is lower for older persons and higher for younger persons. In contrast, a pre deep learning matcher on the same dataset shows the traditional result of higher accuracy for older persons, although its overall accuracy is much lower than that of the deep learning matchers. Comparing the impostor and genuine distributions, we conclude that impostor scores have a larger effect than genuine scores in causing lower accuracy for the older age group. We also investigate the effects of training data across the age groups. Our results show that fine-tuning the deep CNN models on additional images of older persons actually lowers accuracy for the older age group. Also, we fine-tune and train from scratch two models using age-balanced training datasets, and these results also show lower accuracy for older age group. These results argue that the lower accuracy for the older age group is not due to imbalance in the original training data.



