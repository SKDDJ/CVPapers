# Arxiv Papers in cs.CV on 2019-11-03
### Enhanced Convolutional Neural Tangent Kernels
- **Arxiv ID**: http://arxiv.org/abs/1911.00809v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.00809v1)
- **Published**: 2019-11-03 02:24:39+00:00
- **Updated**: 2019-11-03 02:24:39+00:00
- **Authors**: Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon S. Du, Wei Hu, Ruslan Salakhutdinov, Sanjeev Arora
- **Comment**: None
- **Journal**: None
- **Summary**: Recent research shows that for training with $\ell_2$ loss, convolutional neural networks (CNNs) whose width (number of channels in convolutional layers) goes to infinity correspond to regression with respect to the CNN Gaussian Process kernel (CNN-GP) if only the last layer is trained, and correspond to regression with respect to the Convolutional Neural Tangent Kernel (CNTK) if all layers are trained. An exact algorithm to compute CNTK (Arora et al., 2019) yielded the finding that classification accuracy of CNTK on CIFAR-10 is within 6-7% of that of that of the corresponding CNN architecture (best figure being around 78%) which is interesting performance for a fixed kernel. Here we show how to significantly enhance the performance of these kernels using two ideas. (1) Modifying the kernel using a new operation called Local Average Pooling (LAP) which preserves efficient computability of the kernel and inherits the spirit of standard data augmentation using pixel shifts. Earlier papers were unable to incorporate naive data augmentation because of the quadratic training cost of kernel regression. This idea is inspired by Global Average Pooling (GAP), which we show for CNN-GP and CNTK is equivalent to full translation data augmentation. (2) Representing the input image using a pre-processing technique proposed by Coates et al. (2011), which uses a single convolutional layer composed of random image patches. On CIFAR-10, the resulting kernel, CNN-GP with LAP and horizontal flip data augmentation, achieves 89% accuracy, matching the performance of AlexNet (Krizhevsky et al., 2012). Note that this is the best such result we know of for a classifier that is not a trained neural network. Similar improvements are obtained for Fashion-MNIST.



### Image Inpainting by Adaptive Fusion of Variable Spline Interpolations
- **Arxiv ID**: http://arxiv.org/abs/1911.00825v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00825v1)
- **Published**: 2019-11-03 04:16:35+00:00
- **Updated**: 2019-11-03 04:16:35+00:00
- **Authors**: Zahra Nabizadeh, Ghazale Ghorbanzade, Nader Karimi, Shadrokh Samavi
- **Comment**: 5 pages 4 figures
- **Journal**: None
- **Summary**: There are many methods for image enhancement. Image inpainting is one of them which could be used in reconstruction and restoration of scratch images or editing images by adding or removing objects. According to its application, different algorithmic and learning methods are proposed. In this paper, the focus is on applications, which enhance the old and historical scratched images. For this purpose, we proposed an adaptive spline interpolation. In this method, a different number of neighbors in four directions are considered for each pixel in the lost block. In the previous methods, predicting the lost pixels that are on edges is the problem. To address this problem, we consider horizontal and vertical edge information. If the pixel is located on an edge, then we use the predicted value in that direction. In other situations, irrelevant predicted values are omitted, and the average of rest values is used as the value of the missing pixel. The method evaluates by PSNR and SSIM metrics on the Kodak dataset. The results show improvement in PSNR and SSIM compared to similar procedures. Also, the run time of the proposed method outperforms others.



### Leveraging Pretrained Image Classifiers for Language-Based Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.00830v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.00830v3)
- **Published**: 2019-11-03 05:03:06+00:00
- **Updated**: 2020-03-10 21:01:01+00:00
- **Authors**: David Golub, Ahmed El-Kishky, Roberto Martín-Martín
- **Comment**: None
- **Journal**: None
- **Summary**: Current semantic segmentation models cannot easily generalize to new object classes unseen during train time: they require additional annotated images and retraining. We propose a novel segmentation model that injects visual priors into semantic segmentation architectures, allowing them to segment out new target labels without retraining. As visual priors, we use the activations of pretrained image classifiers, which provide noisy indications of the spatial location of both the target object and distractor objects in the scene. We leverage language semantics to obtain these activations for a target label unseen by the classifier. Further experiments show that the visual priors obtained via language semantics for both relevant and distracting objects are key to our performance.



### Scene Graph based Image Retrieval -- A case study on the CLEVR Dataset
- **Arxiv ID**: http://arxiv.org/abs/1911.00850v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00850v1)
- **Published**: 2019-11-03 08:00:38+00:00
- **Updated**: 2019-11-03 08:00:38+00:00
- **Authors**: Sahana Ramnath, Amrita Saha, Soumen Chakrabarti, Mitesh M. Khapra
- **Comment**: 3 pages including references, Accepted at the ICCV 2019 Workshop -
  'Linguistics Meets Image and Video Retrieval' (received Best Paper Award)
- **Journal**: None
- **Summary**: With the prolification of multimodal interaction in various domains, recently there has been much interest in text based image retrieval in the computer vision community. However most of the state of the art techniques model this problem in a purely neural way, which makes it difficult to incorporate pragmatic strategies in searching a large scale catalog especially when the search requirements are insufficient and the model needs to resort to an interactive retrieval process through multiple iterations of question-answering. Motivated by this, we propose a neural-symbolic approach for a one-shot retrieval of images from a large scale catalog, given the caption description. To facilitate this, we represent the catalog and caption as scene-graphs and model the retrieval task as a learnable graph matching problem, trained end-to-end with a REINFORCE algorithm. Further, we briefly describe an extension of this pipeline to an iterative retrieval framework, based on interactive questioning and answering.



### MadNet: Using a MAD Optimization for Defending Against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1911.00870v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.00870v2)
- **Published**: 2019-11-03 11:21:35+00:00
- **Updated**: 2020-06-12 19:05:36+00:00
- **Authors**: Shai Rozenberg, Gal Elidan, Ran El-Yaniv
- **Comment**: None
- **Journal**: None
- **Summary**: This paper is concerned with the defense of deep models against adversarial attacks. Inspired by the certificate defense approach, we propose a maximal adversarial distortion (MAD) optimization method for robustifying deep networks. MAD captures the idea of increasing separability of class clusters in the embedding space while decreasing the network sensitivity to small distortions. Given a deep neural network (DNN) for a classification problem, an application of MAD optimization results in MadNet, a version of the original network, now equipped with an adversarial defense mechanism. MAD optimization is intuitive, effective and scalable, and the resulting MadNet can improve the original accuracy. We present an extensive empirical study demonstrating that MadNet improves adversarial robustness performance compared to state-of-the-art methods.



### A low-cost real-time 3D imaging system for contactless asthma observation
- **Arxiv ID**: http://arxiv.org/abs/1911.00879v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.HC, 65Dxx
- **Links**: [PDF](http://arxiv.org/pdf/1911.00879v1)
- **Published**: 2019-11-03 12:54:02+00:00
- **Updated**: 2019-11-03 12:54:02+00:00
- **Authors**: Sheona M. M. D. P. Sequeira, Beril Sirmacek
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Asthma is becoming a very serious problem with every passing day, especially in children. However, it is very difficult to detect this disorder in them, since the breathing motion of children tends to change when they reach an age of 6. This, thus makes it very difficult to monitor their respiratory state easily. In this paper, we present a cheap non-contact alternative to the current methods that are available. This is using a stereo camera, that captures a video of the patient breathing at a frame rate of 30Hz. For further processing, the captured video has to be rectified and converted into a point cloud. The obtained point clouds need to be aligned in order to have the output with respect to a common plane. They are then converted into a surface mesh. The depth is further estimated by subtracting every point cloud from the reference point cloud (the first frame). The output data, however, when plotted with respect to real time produces a very noisy plot. This is filtered by determining the signal frequency by taking the Fast Fourier Transform of the breathing signal. The system was tested under 4 different breathing conditions: deep, shallow and normal breathing and while coughing. On its success, it was tested with mixed breathing (combination of normal and shallow breathing) and was lastly compared with the output of the expensive 3dMD system. The comparison showed that using the stereo camera, we can reach to similar sensitivity for respiratory motion observation. The experimental results show that, the proposed method provides a major step towards development of low-cost home-based observation systems for asthma patients and care-givers.



### Digital phase-only holography using deep conditional generative models
- **Arxiv ID**: http://arxiv.org/abs/1911.00904v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.comp-ph, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1911.00904v1)
- **Published**: 2019-11-03 14:44:39+00:00
- **Updated**: 2019-11-03 14:44:39+00:00
- **Authors**: Jannes Gladrow
- **Comment**: 24 pages, 2 tables, and 18 figures (including supplement)
- **Journal**: None
- **Summary**: Holographic wave-shaping has found numerous applications across the physical sciences, especially since the development of digital spatial-light modulators (SLMs). A key challenge in digital holography consists in finding optimal hologram patterns which transform the incoming laser beam into desired shapes in a conjugate optical plane. The existing repertoire of approaches to solve this inverse problem is built on iterative phase-retrieval algorithms, which do not take optical aberrations and deviations from theoretical models into account. Here, we adopt a physics-free, data-driven, and probabilistic approach to the problem. Using deep conditional generative models such as Generative-Adversarial Networks (cGAN) or Variational Autoencoder (cVAE), we approximate conditional distributions of holograms for a given target laser intensity pattern. In order to reduce the cardinality of the problem, we train our models on a proxy mapping relating an 8x8-matrix of complex-valued spatial-frequency coefficients to the ensuing 100x100-shaped intensity distribution recorded on a camera. We discuss the degree of 'ill-posedness' that remains in this reduced problem and compare different generative model architectures in terms of their ability to find holograms that reconstruct given intensity patterns. Finally, we challenge our models to generalise to synthetic target intensities, where the existence of matching holograms cannot be guaranteed. We devise a forward-interpolating training scheme aimed at providing models the ability to interpolate in laser intensity space, rather than hologram space and show that this indeed enhances model performance on synthetic data sets.



### Localization of Fetal Head in Ultrasound Images by Multiscale View and Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.00908v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00908v1)
- **Published**: 2019-11-03 15:10:17+00:00
- **Updated**: 2019-11-03 15:10:17+00:00
- **Authors**: Zahra Sobhaninia, Ali Emami, Nader Karimi, Shadrokh Samavi
- **Comment**: 5 pages 4 figures
- **Journal**: None
- **Summary**: One of the routine examinations that are used for prenatal care in many countries is ultrasound imaging. This procedure provides various information about fetus health and development, the progress of the pregnancy and, the baby's due date. Some of the biometric parameters of the fetus, like fetal head circumference (HC), must be measured to check the fetus's health and growth. In this paper, we investigated the effects of using multi-scale inputs in the network. We also propose a light convolutional neural network for automatic HC measurement. Experimental results on an ultrasound dataset of the fetus in different trimesters of pregnancy show that the segmentation accuracy and HC evaluations performed by a light convolutional neural network are comparable to deep convolutional neural networks. The proposed network has fewer parameters and requires less training time.



### Gland Segmentation in Histopathological Images by Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1911.00909v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00909v1)
- **Published**: 2019-11-03 15:12:30+00:00
- **Updated**: 2019-11-03 15:12:30+00:00
- **Authors**: Safiye Rezaei, Ali Emami, Nader Karimi, Shadrokh Samavi
- **Comment**: 5 pages 3 figures
- **Journal**: None
- **Summary**: Histology method is vital in the diagnosis and prognosis of cancers and many other diseases. For the analysis of histopathological images, we need to detect and segment all gland structures. These images are very challenging, and the task of segmentation is even challenging for specialists. Segmentation of glands determines the grade of cancer such as colon, breast, and prostate. Given that deep neural networks have achieved high performance in medical images, we propose a method based on the LinkNet network for gland segmentation. We found the effects of using different loss functions. By using Warwick-Qu dataset, which contains two test sets and one train set, we show that our approach is comparable to state-of-the-art methods. Finally, it is shown that enhancing the gland edges and the use of hematoxylin components can improve the performance of the proposed model.



### Towards Learning Structure via Consensus for Face Segmentation and Parsing
- **Arxiv ID**: http://arxiv.org/abs/1911.00957v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00957v3)
- **Published**: 2019-11-03 19:53:05+00:00
- **Updated**: 2020-03-28 16:52:02+00:00
- **Authors**: Iacopo Masi, Joe Mathai, Wael AbdAlmageed
- **Comment**: To appear in the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition CVPR 2020. Project page at
  https://github.com/isi-vista/structure_via_consensus
- **Journal**: None
- **Summary**: Face segmentation is the task of densely labeling pixels on the face according to their semantics. While current methods place an emphasis on developing sophisticated architectures, use conditional random fields for smoothness, or rather employ adversarial training, we follow an alternative path towards robust face segmentation and parsing. Occlusions, along with other parts of the face, have a proper structure that needs to be propagated in the model during training. Unlike state-of-the-art methods that treat face segmentation as an independent pixel prediction problem, we argue instead that it should hold highly correlated outputs within the same object pixels. We thereby offer a novel learning mechanism to enforce structure in the prediction via consensus, guided by a robust loss function that forces pixel objects to be consistent with each other. Our face parser is trained by transferring knowledge from another model, yet it encourages spatial consistency while fitting the labels. Different than current practice, our method enjoys pixel-wise predictions, yet paves the way for fewer artifacts, less sparse masks, and spatially coherent outputs.



### Conservative Wasserstein Training for Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1911.00962v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00962v1)
- **Published**: 2019-11-03 20:26:16+00:00
- **Updated**: 2019-11-03 20:26:16+00:00
- **Authors**: Xiaofeng Liu, Yang Zou, Tong Che, Peng Ding, Ping Jia, Jane You, Kumar B. V. K
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: This paper targets the task with discrete and periodic class labels ($e.g.,$ pose/orientation estimation) in the context of deep learning. The commonly used cross-entropy or regression loss is not well matched to this problem as they ignore the periodic nature of the labels and the class similarity, or assume labels are continuous value. We propose to incorporate inter-class correlations in a Wasserstein training framework by pre-defining ($i.e.,$ using arc length of a circle) or adaptively learning the ground metric. We extend the ground metric as a linear, convex or concave increasing function $w.r.t.$ arc length from an optimization perspective. We also propose to construct the conservative target labels which model the inlier and outlier noises using a wrapped unimodal-uniform mixture distribution. Unlike the one-hot setting, the conservative label makes the computation of Wasserstein distance more challenging. We systematically conclude the practical closed-form solution of Wasserstein distance for pose data with either one-hot or conservative target label. We evaluate our method on head, body, vehicle and 3D object pose benchmarks with exhaustive ablation studies. The Wasserstein loss obtaining superior performance over the current methods, especially using convex mapping function for ground metric, conservative label, and closed-form solution.



### Unimodal-uniform Constrained Wasserstein Training for Medical Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/1911.02475v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.02475v1)
- **Published**: 2019-11-03 20:41:14+00:00
- **Updated**: 2019-11-03 20:41:14+00:00
- **Authors**: Xiaofeng Liu, Xu Han, Yukai Qiao, Yi Ge, Lu Jun
- **Comment**: ICCV VRMI workshop Oral. arXiv admin note: text overlap with
  arXiv:1911.00962
- **Journal**: None
- **Summary**: The labels in medical diagnosis task are usually discrete and successively distributed. For example, the Diabetic Retinopathy Diagnosis (DR) involves five health risk levels: no DR (0), mild DR (1), moderate DR (2), severe DR (3) and proliferative DR (4). This labeling system is common for medical disease. Previous methods usually construct a multi-binary-classification task or propose some re-parameter schemes in the output unit. In this paper, we target on this task from the perspective of loss function. More specifically, the Wasserstein distance is utilized as an alternative, explicitly incorporating the inter-class correlations by pre-defining its ground metric. Then, the ground metric which serves as a linear, convex or concave increasing function w.r.t. the Euclidean distance in a line is explored from an optimization perspective. Meanwhile, this paper also proposes of constructing the smoothed target labels that model the inlier and outlier noises by using a unimodal-uniform mixture distribution. Different from the one-hot setting, the smoothed label endues the computation of Wasserstein distance with more challenging features. With either one-hot or smoothed target label, this paper systematically concludes the practical closed-form solution. We evaluate our method on several medical diagnosis tasks (e.g., Diabetic Retinopathy and Ultrasound Breast dataset) and achieve state-of-the-art performance.



### Learning to Scaffold the Development of Robotic Manipulation Skills
- **Arxiv ID**: http://arxiv.org/abs/1911.00969v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/1911.00969v3)
- **Published**: 2019-11-03 21:15:46+00:00
- **Updated**: 2020-10-05 05:11:36+00:00
- **Authors**: Lin Shao, Toki Migimatsu, Jeannette Bohg
- **Comment**: Accepted to IEEE International Conference on Robotics and Automation
  (ICRA) 2020
- **Journal**: None
- **Summary**: Learning contact-rich, robotic manipulation skills is a challenging problem due to the high-dimensionality of the state and action space as well as uncertainty from noisy sensors and inaccurate motor control. To combat these factors and achieve more robust manipulation, humans actively exploit contact constraints in the environment. By adopting a similar strategy, robots can also achieve more robust manipulation. In this paper, we enable a robot to autonomously modify its environment and thereby discover how to ease manipulation skill learning. Specifically, we provide the robot with fixtures that it can freely place within the environment. These fixtures provide hard constraints that limit the outcome of robot actions. Thereby, they funnel uncertainty from perception and motor control and scaffold manipulation skill learning. We propose a learning system that consists of two learning loops. In the outer loop, the robot positions the fixture in the workspace. In the inner loop, the robot learns a manipulation skill and after a fixed number of episodes, returns the reward to the outer loop. Thereby, the robot is incentivised to place the fixture such that the inner loop quickly achieves a high reward. We demonstrate our framework both in simulation and in the real world on three tasks: peg insertion, wrench manipulation and shallow-depth insertion. We show that manipulation skill learning is dramatically sped up through this way of scaffolding.



