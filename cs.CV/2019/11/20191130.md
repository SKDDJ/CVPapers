# Arxiv Papers in cs.CV on 2019-11-30
### Supervised and Unsupervised End-to-End Deep Learning for Gene Ontology Classification of Neural In Situ Hybridization Images
- **Arxiv ID**: http://arxiv.org/abs/1912.01494v1
- **DOI**: 10.3390/e21030221
- **Categories**: **cs.CV**, cs.LG, cs.NE, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.01494v1)
- **Published**: 2019-11-30 01:20:12+00:00
- **Updated**: 2019-11-30 01:20:12+00:00
- **Authors**: Ido Cohen, Eli David, Nathan S. Netanyahu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1711.09663
- **Journal**: Entropy, Vol. 21, No. 3, pp. 221-238, February 2019
- **Summary**: In recent years, large datasets of high-resolution mammalian neural images have become available, which has prompted active research on the analysis of gene expression data. Traditional image processing methods are typically applied for learning functional representations of genes, based on their expressions in these brain images. In this paper, we describe a novel end-to-end deep learning-based method for generating compact representations of in situ hybridization (ISH) images, which are invariant-to-translation. In contrast to traditional image processing methods, our method relies, instead, on deep convolutional denoising autoencoders (CDAE) for processing raw pixel inputs, and generating the desired compact image representations. We provide an in-depth description of our deep learning-based approach, and present extensive experimental results, demonstrating that representations extracted by CDAE can help learn features of functional gene ontology categories for their classification in a highly accurate manner. Our methods improve the previous state-of-the-art classification rate (Liscovitch, et al.) from an average AUC of 0.92 to 0.997, i.e., it achieves 96% reduction in error rate. Furthermore, the representation vectors generated due to our method are more compact in comparison to previous state-of-the-art methods, allowing for a more efficient high-level representation of images. These results are obtained with significantly downsampled images in comparison to the original high-resolution ones, further underscoring the robustness of our proposed method.



### Representation Learning on Unit Ball with 3D Roto-Translational Equivariance
- **Arxiv ID**: http://arxiv.org/abs/1912.01454v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01454v1)
- **Published**: 2019-11-30 01:31:55+00:00
- **Updated**: 2019-11-30 01:31:55+00:00
- **Authors**: Sameera Ramasinghe, Salman Khan, Nick Barnes, Stephen Gould
- **Comment**: arXiv admin note: text overlap with arXiv:1901.00616
- **Journal**: None
- **Summary**: Convolution is an integral operation that defines how the shape of one function is modified by another function. This powerful concept forms the basis of hierarchical feature learning in deep neural networks. Although performing convolution in Euclidean geometries is fairly straightforward, its extension to other topological spaces---such as a sphere ($\mathbb{S}^2$) or a unit ball ($\mathbb{B}^3$)---entails unique challenges. In this work, we propose a novel `\emph{volumetric convolution}' operation that can effectively model and convolve arbitrary functions in $\mathbb{B}^3$. We develop a theoretical framework for \emph{volumetric convolution} based on Zernike polynomials and efficiently implement it as a differentiable and an easily pluggable layer in deep networks. By construction, our formulation leads to the derivation of a novel formula to measure the symmetry of a function in $\mathbb{B}^3$ around an arbitrary axis, that is useful in function analysis tasks. We demonstrate the efficacy of proposed volumetric convolution operation on one viable use case i.e., 3D object recognition.



### A Free Lunch in Generating Datasets: Building a VQG and VQA System with Attention and Humans in the Loop
- **Arxiv ID**: http://arxiv.org/abs/1912.00124v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.HC, cs.LG, I.2, I.4, I.6, I.7, I.2; I.4; I.6; I.7
- **Links**: [PDF](http://arxiv.org/pdf/1912.00124v2)
- **Published**: 2019-11-30 03:45:17+00:00
- **Updated**: 2020-08-28 17:52:03+00:00
- **Authors**: Jihyeon Lee, Sho Arora
- **Comment**: 9 pages, 12 figures
- **Journal**: None
- **Summary**: Despite their importance in training artificial intelligence systems, large datasets remain challenging to acquire. For example, the ImageNet dataset required fourteen million labels of basic human knowledge, such as whether an image contains a chair. Unfortunately, this knowledge is so simple that it is tedious for human annotators but also tacit enough such that they are necessary. However, human collaborative efforts for tasks like labeling massive amounts of data are costly, inconsistent, and prone to failure, and this method does not resolve the issue of the resulting dataset being static in nature. What if we asked people questions they want to answer and collected their responses as data? This would mean we could gather data at a much lower cost, and expanding a dataset would simply become a matter of asking more questions. We focus on the task of Visual Question Answering (VQA) and propose a system that uses Visual Question Generation (VQG) to produce questions, asks them to social media users, and collects their responses. We present two models that can then parse clean answers from the noisy human responses significantly better than our baselines, with the goal of eventually incorporating the answers into a Visual Question Answering (VQA) dataset. By demonstrating how our system can collect large amounts of data at little to no cost, we envision similar systems being used to improve performance on other tasks in the future.



### Sub-pixel matching method for low-resolution thermal stereo images
- **Arxiv ID**: http://arxiv.org/abs/1912.00138v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00138v1)
- **Published**: 2019-11-30 06:09:27+00:00
- **Updated**: 2019-11-30 06:09:27+00:00
- **Authors**: Yannick Wend Kuni Zoetgnande, Geoffroy Cormier, Alain-Jérôme Fougères, Jean-Louis Dillenseger
- **Comment**: None
- **Journal**: None
- **Summary**: In the context of a localization and tracking application, we developed a stereo vision system based on cheap low-resolution 80x60 pixels thermal cameras. We proposed a threefold sub-pixel stereo matching framework (called ST for Subpixel Thermal): 1) robust features extraction method based on phase congruency, 2) rough matching of these features in pixel precision, and 3) refined matching in sub-pixel accuracy based on local phase coherence. We performed experiments on our very low-resolution thermal images (acquired using a stereo system we manufactured) as for high-resolution images from a benchmark dataset. Even if phase congruency computation time is high, it was able to extract two times more features than state-of-the-art methods such as ORB or SURF. We proposed a modified version of the phase correlation applied in the phase congruency feature space for sub-pixel matching. Using simulated stereo, we investigated how the phase congruency threshold and the sub-image size of sub-pixel matching can influence the accuracy. We then proved that given our stereo setup and the resolution of our images, being wrong of 1 pixel leads to a 500 mm error in the Z position of the point. Finally, we showed that our method could extract four times more matches than a baseline method ORB + OpenCV KNN matching on low-resolution images. Moreover, our matches were more robust. More precisely, when projecting points of a standing person, ST got a standard deviation of 300 mm when ORB + OpenCV KNN gave more than 1000 mm.



### Learning Rate Dropout
- **Arxiv ID**: http://arxiv.org/abs/1912.00144v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.00144v2)
- **Published**: 2019-11-30 06:58:40+00:00
- **Updated**: 2019-12-05 08:43:16+00:00
- **Authors**: Huangxing Lin, Weihong Zeng, Xinghao Ding, Yue Huang, Chenxi Huang, John Paisley
- **Comment**: None
- **Journal**: None
- **Summary**: The performance of a deep neural network is highly dependent on its training, and finding better local optimal solutions is the goal of many optimization algorithms. However, existing optimization algorithms show a preference for descent paths that converge slowly and do not seek to avoid bad local optima. In this work, we propose Learning Rate Dropout (LRD), a simple gradient descent technique for training related to coordinate descent. LRD empirically aids the optimizer to actively explore in the parameter space by randomly setting some learning rates to zero; at each iteration, only parameters whose learning rate is not 0 are updated. As the learning rate of different parameters is dropped, the optimizer will sample a new loss descent path for the current update. The uncertainty of the descent path helps the model avoid saddle points and bad local minima. Experiments show that LRD is surprisingly effective in accelerating training while preventing overfitting.



### Point Cloud Instance Segmentation using Probabilistic Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1912.00145v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00145v2)
- **Published**: 2019-11-30 07:04:09+00:00
- **Updated**: 2021-03-30 15:03:48+00:00
- **Authors**: Biao Zhang, Peter Wonka
- **Comment**: Accepted by CVPR 2021. Project:
  http://1zb.github.io/publication/prob-embed/
- **Journal**: None
- **Summary**: In this paper we propose a new framework for point cloud instance segmentation. Our framework has two steps: an embedding step and a clustering step. In the embedding step, our main contribution is to propose a probabilistic embedding space for point cloud embedding. Specifically, each point is represented as a tri-variate normal distribution. In the clustering step, we propose a novel loss function, which benefits both the semantic segmentation and the clustering. Our experimental results show important improvements to the SOTA, i.e., 3.1% increased average per-category mAP on the PartNet dataset.



### Correction Filter for Single Image Super-Resolution: Robustifying Off-the-Shelf Deep Super-Resolvers
- **Arxiv ID**: http://arxiv.org/abs/1912.00157v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.00157v2)
- **Published**: 2019-11-30 08:04:33+00:00
- **Updated**: 2020-05-24 13:27:27+00:00
- **Authors**: Shady Abu Hussein, Tom Tirer, Raja Giryes
- **Comment**: Accepted to CVPR 2020 (Oral). Code is available at
  https://github.com/shadyabh/Correction-Filter
- **Journal**: None
- **Summary**: The single image super-resolution task is one of the most examined inverse problems in the past decade. In the recent years, Deep Neural Networks (DNNs) have shown superior performance over alternative methods when the acquisition process uses a fixed known downsampling kernel-typically a bicubic kernel. However, several recent works have shown that in practical scenarios, where the test data mismatch the training data (e.g. when the downsampling kernel is not the bicubic kernel or is not available at training), the leading DNN methods suffer from a huge performance drop. Inspired by the literature on generalized sampling, in this work we propose a method for improving the performance of DNNs that have been trained with a fixed kernel on observations acquired by other kernels. For a known kernel, we design a closed-form correction filter that modifies the low-resolution image to match one which is obtained by another kernel (e.g. bicubic), and thus improves the results of existing pre-trained DNNs. For an unknown kernel, we extend this idea and propose an algorithm for blind estimation of the required correction filter. We show that our approach outperforms other super-resolution methods, which are designed for general downsampling kernels.



### Assessing the Robustness of Visual Question Answering Models
- **Arxiv ID**: http://arxiv.org/abs/1912.01452v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1912.01452v2)
- **Published**: 2019-11-30 09:32:38+00:00
- **Updated**: 2022-03-03 14:17:46+00:00
- **Authors**: Jia-Hong Huang, Modar Alfadly, Bernard Ghanem, Marcel Worring
- **Comment**: 24 pages, 13 figures, International Journal of Computer Vision (IJCV)
  [under review]. arXiv admin note: substantial text overlap with
  arXiv:1711.06232, arXiv:1709.04625
- **Journal**: None
- **Summary**: Deep neural networks have been playing an essential role in the task of Visual Question Answering (VQA). Until recently, their accuracy has been the main focus of research. Now there is a trend toward assessing the robustness of these models against adversarial attacks by evaluating the accuracy of these models under increasing levels of noisiness in the inputs of VQA models. In VQA, the attack can target the image and/or the proposed query question, dubbed main question, and yet there is a lack of proper analysis of this aspect of VQA. In this work, we propose a new method that uses semantically related questions, dubbed basic questions, acting as noise to evaluate the robustness of VQA models. We hypothesize that as the similarity of a basic question to the main question decreases, the level of noise increases. To generate a reasonable noise level for a given main question, we rank a pool of basic questions based on their similarity with this main question. We cast this ranking problem as a LASSO optimization problem. We also propose a novel robustness measure Rscore and two large-scale basic question datasets in order to standardize robustness analysis of VQA models. The experimental results demonstrate that the proposed evaluation method is able to effectively analyze the robustness of VQA models. To foster the VQA research, we will publish our proposed datasets.



### Urban Driving with Conditional Imitation Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.00177v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.00177v2)
- **Published**: 2019-11-30 10:24:45+00:00
- **Updated**: 2019-12-05 18:17:45+00:00
- **Authors**: Jeffrey Hawke, Richard Shen, Corina Gurau, Siddharth Sharma, Daniele Reda, Nikolay Nikolov, Przemyslaw Mazur, Sean Micklethwaite, Nicolas Griffiths, Amar Shah, Alex Kendall
- **Comment**: Under submission; added acknowledgements
- **Journal**: None
- **Summary**: Hand-crafting generalised decision-making rules for real-world urban autonomous driving is hard. Alternatively, learning behaviour from easy-to-collect human driving demonstrations is appealing. Prior work has studied imitation learning (IL) for autonomous driving with a number of limitations. Examples include only performing lane-following rather than following a user-defined route, only using a single camera view or heavily cropped frames lacking state observability, only lateral (steering) control, but not longitudinal (speed) control and a lack of interaction with traffic. Importantly, the majority of such systems have been primarily evaluated in simulation - a simple domain, which lacks real-world complexities. Motivated by these challenges, we focus on learning representations of semantics, geometry and motion with computer vision for IL from human driving demonstrations. As our main contribution, we present an end-to-end conditional imitation learning approach, combining both lateral and longitudinal control on a real vehicle for following urban routes with simple traffic. We address inherent dataset bias by data balancing, training our final policy on approximately 30 hours of demonstrations gathered over six months. We evaluate our method on an autonomous vehicle by driving 35km of novel routes in European urban streets.



### SGAS: Sequential Greedy Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1912.00195v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.00195v2)
- **Published**: 2019-11-30 12:39:55+00:00
- **Updated**: 2020-04-02 12:55:03+00:00
- **Authors**: Guohao Li, Guocheng Qian, Itzel C. Delgadillo, Matthias Müller, Ali Thabet, Bernard Ghanem
- **Comment**: Accepted at CVPR'2020. Project website:
  https://www.deepgcns.org/auto/sgas
- **Journal**: None
- **Summary**: Architecture design has become a crucial component of successful deep learning. Recent progress in automatic neural architecture search (NAS) shows a lot of promise. However, discovered architectures often fail to generalize in the final evaluation. Architectures with a higher validation accuracy during the search phase may perform worse in the evaluation. Aiming to alleviate this common issue, we introduce sequential greedy architecture search (SGAS), an efficient method for neural architecture search. By dividing the search procedure into sub-problems, SGAS chooses and prunes candidate operations in a greedy fashion. We apply SGAS to search architectures for Convolutional Neural Networks (CNN) and Graph Convolutional Networks (GCN). Extensive experiments show that SGAS is able to find state-of-the-art architectures for tasks such as image classification, point cloud classification and node classification in protein-protein interaction graphs with minimal computational cost. Please visit https://www.deepgcns.org/auto/sgas for more information about SGAS.



### Design and Interpretation of Universal Adversarial Patches in Face Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.05021v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.05021v3)
- **Published**: 2019-11-30 12:43:56+00:00
- **Updated**: 2020-07-17 09:37:37+00:00
- **Authors**: Xiao Yang, Fangyun Wei, Hongyang Zhang, Jun Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: We consider universal adversarial patches for faces -- small visual elements whose addition to a face image reliably destroys the performance of face detectors. Unlike previous work that mostly focused on the algorithmic design of adversarial examples in terms of improving the success rate as an attacker, in this work we show an interpretation of such patches that can prevent the state-of-the-art face detectors from detecting the real faces. We investigate a phenomenon: patches designed to suppress real face detection appear face-like. This phenomenon holds generally across different initialization, locations, scales of patches, backbones, and state-of-the-art face detection frameworks. We propose new optimization-based approaches to automatic design of universal adversarial patches for varying goals of the attack, including scenarios in which true positives are suppressed without introducing false positives. Our proposed algorithms perform well on real-world datasets, deceiving state-of-the-art face detectors in terms of multiple precision/recall metrics and transferability.



### Facial Expression Representation Learning by Synthesizing Expression Images
- **Arxiv ID**: http://arxiv.org/abs/1912.01456v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01456v1)
- **Published**: 2019-11-30 12:45:27+00:00
- **Updated**: 2019-11-30 12:45:27+00:00
- **Authors**: Kamran Ali, Charles E. Hughes
- **Comment**: 7 pages, 3 figures. arXiv admin note: substantial text overlap with
  arXiv:1909.13135
- **Journal**: None
- **Summary**: Representations used for Facial Expression Recognition (FER) usually contain expression information along with identity features. In this paper, we propose a novel Disentangled Expression learning-Generative Adversarial Network (DE-GAN) which combines the concept of disentangled representation learning with residue learning to explicitly disentangle facial expression representation from identity information. In this method the facial expression representation is learned by reconstructing an expression image employing an encoder-decoder based generator. Unlike previous works using only expression residual learning for facial expression recognition, our method learns the disentangled expression representation along with the expressive component recorded by the encoder of DE-GAN. In order to improve the quality of synthesized expression images and the effectiveness of the learned disentangled expression representation, expression and identity classification is performed by the discriminator of DE-GAN. Experiments performed on widely used datasets (CK+, MMI, Oulu-CASIA) show that the proposed technique produces comparable or better results than state-of-the-art methods.



### Pruning at a Glance: Global Neural Pruning for Model Compression
- **Arxiv ID**: http://arxiv.org/abs/1912.00200v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1912.00200v2)
- **Published**: 2019-11-30 13:17:48+00:00
- **Updated**: 2019-12-03 09:44:06+00:00
- **Authors**: Abdullah Salama, Oleksiy Ostapenko, Tassilo Klein, Moin Nabi
- **Comment**: Extended version of the ICASSP paper
  (https://ieeexplore.ieee.org/document/8683224)
- **Journal**: None
- **Summary**: Deep Learning models have become the dominant approach in several areas due to their high performance. Unfortunately, the size and hence computational requirements of operating such models can be considerably high. Therefore, this constitutes a limitation for deployment on memory and battery constrained devices such as mobile phones or embedded systems. To address these limitations, we propose a novel and simple pruning method that compresses neural networks by removing entire filters and neurons according to a global threshold across the network without any pre-calculation of layer sensitivity. The resulting model is compact, non-sparse, with the same accuracy as the non-compressed model, and most importantly requires no special infrastructure for deployment. We prove the viability of our method by producing highly compressed models, namely VGG-16, ResNet-56, and ResNet-110 respectively on CIFAR10 without losing any performance compared to the baseline, as well as ResNet-34 and ResNet-50 on ImageNet without a significant loss of accuracy. We also provide a well-retrained 30% compressed ResNet-50 that slightly surpasses the base model accuracy. Additionally, compressing more than 56% and 97% of AlexNet and LeNet-5 respectively. Interestingly, the resulted models' pruning patterns are highly similar to the other methods using layer sensitivity pre-calculation step. Our method does not only exhibit good performance but what is more also easy to implement.



### PU-GCN: Point Cloud Upsampling using Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.03264v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.03264v3)
- **Published**: 2019-11-30 13:18:19+00:00
- **Updated**: 2021-03-29 15:34:38+00:00
- **Authors**: Guocheng Qian, Abdulellah Abualshour, Guohao Li, Ali Thabet, Bernard Ghanem
- **Comment**: Get accepted to CVPR 2021. The source code of this work is available
  at https://github.com/guochengqian/PU-GCN
- **Journal**: None
- **Summary**: The effectiveness of learning-based point cloud upsampling pipelines heavily relies on the upsampling modules and feature extractors used therein. For the point upsampling module, we propose a novel model called NodeShuffle, which uses a Graph Convolutional Network (GCN) to better encode local point information from point neighborhoods. NodeShuffle is versatile and can be incorporated into any point cloud upsampling pipeline. Extensive experiments show how NodeShuffle consistently improves state-of-the-art upsampling methods. For feature extraction, we also propose a new multi-scale point feature extractor, called Inception DenseGCN. By aggregating features at multiple scales, this feature extractor enables further performance gain in the final upsampled point clouds. We combine Inception DenseGCN with NodeShuffle into a new point upsampling pipeline called PU-GCN. PU-GCN sets new state-of-art performance with much fewer parameters and more efficient inference.



### EM-NET: Centerline-Aware Mitochondria Segmentation in EM Images via Hierarchical View-Ensemble Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/1912.00201v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00201v3)
- **Published**: 2019-11-30 13:25:15+00:00
- **Updated**: 2020-01-29 04:04:56+00:00
- **Authors**: Zhimin Yuan, Jiajin Yi, Zhengrong Luo, Zhongdao Jia, Jialin Peng
- **Comment**: None
- **Journal**: None
- **Summary**: Although deep encoder-decoder networks have achieved astonishing performance for mitochondria segmentation from electron microscopy (EM) images, they still produce coarse segmentations with lots of discontinuities and false positives. Besides, the need for labor intensive annotations of large 3D dataset and huge memory overhead by 3D models are also major limitations. To address these problems, we introduce a multi-task network named EM-Net, which includes an auxiliary centerline detection task to account for shape information of mitochondria represented by centerline. Therefore, the centerline detection sub-network is able to enhance the accuracy and robustness of segmentation task, especially when only a small set of annotated data are available. To achieve a light-weight 3D network, we introduce a novel hierarchical view-ensemble convolution module to reduce number of parameters, and facilitate multi-view information aggregation.Validations on public benchmark showed state-of-the-art performance by EM-Net. Even with significantly reduced training data, our method still showed quite promising results.



### Relation Graph Network for 3D Object Detection in Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1912.00202v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00202v1)
- **Published**: 2019-11-30 13:31:18+00:00
- **Updated**: 2019-11-30 13:31:18+00:00
- **Authors**: Mingtao Feng, Syed Zulqarnain Gilani, Yaonan Wang, Liang Zhang, Ajmal Mian
- **Comment**: Manuscript
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have emerged as a powerful strategy for most object detection tasks on 2D images. However, their power has not been fully realised for detecting 3D objects in point clouds directly without converting them to regular grids. Existing state-of-art 3D object detection methods aim to recognize 3D objects individually without exploiting their relationships during learning or inference. In this paper, we first propose a strategy that associates the predictions of direction vectors and pseudo geometric centers together leading to a win-win solution for 3D bounding box candidates regression. Secondly, we propose point attention pooling to extract uniform appearance features for each 3D object proposal, benefiting from the learned direction features, semantic features and spatial coordinates of the object points. Finally, the appearance features are used together with the position features to build 3D object-object relationship graphs for all proposals to model their co-existence. We explore the effect of relation graphs on proposals' appearance features enhancement under supervised and unsupervised settings. The proposed relation graph network consists of a 3D object proposal generation module and a 3D relation module, makes it an end-to-end trainable network for detecting 3D object in point clouds. Experiments on challenging benchmarks ( SunRGB-Dand ScanNet datasets ) of 3D point clouds show that our algorithm can perform better than the existing state-of-the-art methods.



### Probing the State of the Art: A Critical Look at Visual Representation Evaluation
- **Arxiv ID**: http://arxiv.org/abs/1912.00215v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.00215v2)
- **Published**: 2019-11-30 15:05:28+00:00
- **Updated**: 2021-08-12 17:54:53+00:00
- **Authors**: Cinjon Resnick, Zeping Zhan, Joan Bruna
- **Comment**: erd59xH@Rqt!XCMsCmnz
- **Journal**: None
- **Summary**: Self-supervised research improved greatly over the past half decade, with much of the growth being driven by objectives that are hard to quantitatively compare. These techniques include colorization, cyclical consistency, and noise-contrastive estimation from image patches. Consequently, the field has settled on a handful of measurements that depend on linear probes to adjudicate which approaches are the best. Our first contribution is to show that this test is insufficient and that models which perform poorly (strongly) on linear classification can perform strongly (weakly) on more involved tasks like temporal activity localization. Our second contribution is to analyze the capabilities of five different representations. And our third contribution is a much needed new dataset for temporal activity localization.



### Convolutional neural networks model improvements using demographics and image processing filters on chest x-rays
- **Arxiv ID**: http://arxiv.org/abs/1912.00233v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.00233v1)
- **Published**: 2019-11-30 17:00:21+00:00
- **Updated**: 2019-11-30 17:00:21+00:00
- **Authors**: Mir Muhammad Abdullah, Mir Muhammad Abdur Rahman, Mir Mohammed Assadullah
- **Comment**: 27 pages
- **Journal**: None
- **Summary**: Purpose: The purpose of this study was to observe change in accuracies of convolutional neural networks (CNN) models (ratio of correct classifications to total predictions) on thoracic radiological images by creating different binary classification models based on age, gender, and image pre-processing filters on 14 pathologies.   Methodology: This is a quantitative research exploring variation in CNN model accuracies. Radiological thoracic images were divided by age and gender and pre-processed by various image processing filters.   Findings: We found partial support for enhancement to model accuracies by segregating modeling images by age and gender and applying image processing filters even though image processing filters are sometimes thought of as information filters.   Research limitations: This study may be biased because it is based on radiological images by another research that tagged the images using an automated process that was not checked by a human.   Practical implications: Researchers may want to focus on creating models segregated by demographics and pre-process the modeling images using image processing filters. Practitioners developing assistive technologies for thoracic diagnoses may benefit from incorporating demographics and employing multiple models simultaneously with varying statistical likelihood.   Originality/value: This study uses demographics in model creation and utilizes image processing filters to improve model performance.   Keywords: Convolutional Neural Network (CNN), Chest X-Ray, ChestX-ray14, Lung, Atelectasis, Cardiomegaly, Consolidation, Edema, Effusion, Emphysema, Infiltration, Mass, Nodule, Pleural Thickening, Pneumonia, Pneumathorax



### Averaging Essential and Fundamental Matrices in Collinear Camera Settings
- **Arxiv ID**: http://arxiv.org/abs/1912.00254v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00254v3)
- **Published**: 2019-11-30 19:24:13+00:00
- **Updated**: 2020-09-15 08:41:37+00:00
- **Authors**: Amnon Geifman, Yoni Kasten, Meirav Galun, Ronen Basri
- **Comment**: None
- **Journal**: None
- **Summary**: Global methods to Structure from Motion have gained popularity in recent years. A significant drawback of global methods is their sensitivity to collinear camera settings. In this paper, we introduce an analysis and algorithms for averaging bifocal tensors (essential or fundamental matrices) when either subsets or all of the camera centers are collinear.   We provide a complete spectral characterization of bifocal tensors in collinear scenarios and further propose two averaging algorithms. The first algorithm uses rank constrained minimization to recover camera matrices in fully collinear settings. The second algorithm enriches the set of possibly mixed collinear and non-collinear cameras with additional, "virtual cameras," which are placed in general position, enabling the application of existing averaging methods to the enriched set of bifocal tensors. Our algorithms are shown to achieve state of the art results on various benchmarks that include autonomous car datasets and unordered image collections in both calibrated and unclibrated settings.



### Approximating Human Judgment of Generated Image Quality
- **Arxiv ID**: http://arxiv.org/abs/1912.12121v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.12121v1)
- **Published**: 2019-11-30 19:51:02+00:00
- **Updated**: 2019-11-30 19:51:02+00:00
- **Authors**: Y. Alex Kolchinski, Sharon Zhou, Shengjia Zhao, Mitchell Gordon, Stefano Ermon
- **Comment**: To appear in the Shared Visual Representations in Human and Machine
  Intelligence workshop at NeurIPS 2019. The first two authors contributed
  equally to the manuscript
- **Journal**: None
- **Summary**: Generative models have made immense progress in recent years, particularly in their ability to generate high quality images. However, that quality has been difficult to evaluate rigorously, with evaluation dominated by heuristic approaches that do not correlate well with human judgment, such as the Inception Score and Fr\'echet Inception Distance. Real human labels have also been used in evaluation, but are inefficient and expensive to collect for each image. Here, we present a novel method to automatically evaluate images based on their quality as perceived by humans. By not only generating image embeddings from Inception network activations and comparing them to the activations for real images, of which other methods perform a variant, but also regressing the activation statistics to match gold standard human labels, we demonstrate 66% accuracy in predicting human scores of image realism, matching the human inter-rater agreement rate. Our approach also generalizes across generative models, suggesting the potential for capturing a model-agnostic measure of image quality. We open source our dataset of human labels for the advancement of research and techniques in this area.



### Image segmentation of liver stage malaria infection with spatial uncertainty sampling
- **Arxiv ID**: http://arxiv.org/abs/1912.00262v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, eess.IV, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/1912.00262v1)
- **Published**: 2019-11-30 20:57:15+00:00
- **Updated**: 2019-11-30 20:57:15+00:00
- **Authors**: Ava P. Soleimany, Harini Suresh, Jose Javier Gonzalez Ortiz, Divya Shanmugam, Nil Gural, John Guttag, Sangeeta N. Bhatia
- **Comment**: None
- **Journal**: None
- **Summary**: Global eradication of malaria depends on the development of drugs effective against the silent, yet obligate liver stage of the disease. The gold standard in drug development remains microscopic imaging of liver stage parasites in in vitro cell culture models. Image analysis presents a major bottleneck in this pipeline since the parasite has significant variability in size, shape, and density in these models. As with other highly variable datasets, traditional segmentation models have poor generalizability as they rely on hand-crafted features; thus, manual annotation of liver stage malaria images remains standard. To address this need, we develop a convolutional neural network architecture that utilizes spatial dropout sampling for parasite segmentation and epistemic uncertainty estimation in images of liver stage malaria. Our pipeline produces high-precision segmentations nearly identical to expert annotations, generalizes well on a diverse dataset of liver stage malaria parasites, and promotes independence between learned feature maps to model the uncertainty of generated predictions.



### Biometrics Recognition Using Deep Learning: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1912.00271v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.00271v3)
- **Published**: 2019-11-30 22:00:57+00:00
- **Updated**: 2021-02-08 19:24:49+00:00
- **Authors**: Shervin Minaee, Amirali Abdolrashidi, Hang Su, Mohammed Bennamoun, David Zhang
- **Comment**: Under Review
- **Journal**: None
- **Summary**: Deep learning-based models have been very successful in achieving state-of-the-art results in many of the computer vision, speech recognition, and natural language processing tasks in the last few years. These models seem a natural fit for handling the ever-increasing scale of biometric recognition problems, from cellphone authentication to airport security systems. Deep learning-based models have increasingly been leveraged to improve the accuracy of different biometric recognition systems in recent years. In this work, we provide a comprehensive survey of more than 120 promising works on biometric recognition (including face, fingerprint, iris, palmprint, ear, voice, signature, and gait recognition), which deploy deep learning models, and show their strengths and potentials in different applications. For each biometric, we first introduce the available datasets that are widely used in the literature and their characteristics. We will then talk about several promising deep learning works developed for that biometric, and show their performance on popular public benchmarks. We will also discuss some of the main challenges while using these models for biometric recognition, and possible future directions to which research in this area is headed.



### Morphing and Sampling Network for Dense Point Cloud Completion
- **Arxiv ID**: http://arxiv.org/abs/1912.00280v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00280v1)
- **Published**: 2019-11-30 22:52:54+00:00
- **Updated**: 2019-11-30 22:52:54+00:00
- **Authors**: Minghua Liu, Lu Sheng, Sheng Yang, Jing Shao, Shi-Min Hu
- **Comment**: 8pages, 7 figures, AAAI2020
- **Journal**: None
- **Summary**: 3D point cloud completion, the task of inferring the complete geometric shape from a partial point cloud, has been attracting attention in the community. For acquiring high-fidelity dense point clouds and avoiding uneven distribution, blurred details, or structural loss of existing methods' results, we propose a novel approach to complete the partial point cloud in two stages. Specifically, in the first stage, the approach predicts a complete but coarse-grained point cloud with a collection of parametric surface elements. Then, in the second stage, it merges the coarse-grained prediction with the input point cloud by a novel sampling algorithm. Our method utilizes a joint loss function to guide the distribution of the points. Extensive experiments verify the effectiveness of our method and demonstrate that it outperforms the existing methods in both the Earth Mover's Distance (EMD) and the Chamfer Distance (CD).



