# Arxiv Papers in cs.CV on 2019-11-01
### Centroid Based Concept Learning for RGB-D Indoor Scene Classification
- **Arxiv ID**: http://arxiv.org/abs/1911.00155v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.00155v4)
- **Published**: 2019-11-01 00:09:37+00:00
- **Updated**: 2020-08-15 01:26:48+00:00
- **Authors**: Ali Ayub, Alan R. Wagner
- **Comment**: Accepted at BMVC 2020
- **Journal**: None
- **Summary**: This paper contributes a novel cognitively-inspired method for RGB-D indoor scene classification. High intra-class variance and low inter-class variance make indoor scene classification an extremely challenging task. To cope with this problem, we propose a clustering approach inspired by the concept learning model of the hippocampus and the neocortex, to generate clusters and centroids for different scene categories. Test images depicting different scenes are classified by using their distance to the closest centroids (concepts). Modeling of RGB-D scenes as centroids not only leads to state-of-the-art classification performance on benchmark datasets (SUN RGB-D and NYU Depth V2), but also offers a method for inspecting and interpreting the space of centroids. Inspection of the centroids generated by our approach on RGB-D datasets leads us to propose a method for merging conceptually similar categories, resulting in improved accuracy for all approaches.



### Learning-based Real-time Detection of Intrinsic Reflectional Symmetry
- **Arxiv ID**: http://arxiv.org/abs/1911.00189v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00189v1)
- **Published**: 2019-11-01 02:58:45+00:00
- **Updated**: 2019-11-01 02:58:45+00:00
- **Authors**: Yi-Ling Qiao, Lin Gao, Shu-Zhi Liu, Ligang Liu, Yu-Kun Lai, Xilin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Reflectional symmetry is ubiquitous in nature. While extrinsic reflectional symmetry can be easily parametrized and detected, intrinsic symmetry is much harder due to the high solution space. Previous works usually solve this problem by voting or sampling, which suffer from high computational cost and randomness. In this paper, we propose \YL{a} learning-based approach to intrinsic reflectional symmetry detection. Instead of directly finding symmetric point pairs, we parametrize this self-isometry using a functional map matrix, which can be easily computed given the signs of Laplacian eigenfunctions under the symmetric mapping. Therefore, we train a novel deep neural network to predict the sign of each eigenfunction under symmetry, which in addition takes the first few eigenfunctions as intrinsic features to characterize the mesh while avoiding coping with the connectivity explicitly. Our network aims at learning the global property of functions, and consequently converts the problem defined on the manifold to the functional domain. By disentangling the prediction of the matrix into separated basis, our method generalizes well to new shapes and is invariant under perturbation of eigenfunctions. Through extensive experiments, we demonstrate the robustness of our method in challenging cases, including different topology and incomplete shapes with holes. By avoiding random sampling, our learning-based algorithm is over 100 times faster than state-of-the-art methods, and meanwhile, is more robust, achieving higher correspondence accuracy in commonly used metrics.



### Rotation Invariant Point Cloud Classification: Where Local Geometry Meets Global Topology
- **Arxiv ID**: http://arxiv.org/abs/1911.00195v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.00195v3)
- **Published**: 2019-11-01 04:14:19+00:00
- **Updated**: 2021-06-01 09:42:19+00:00
- **Authors**: Chen Zhao, Jiaqi Yang, Xin Xiong, Angfan Zhu, Zhiguo Cao, Xin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Point cloud analysis is a fundamental task in 3D computer vision. Most previous works have conducted experiments on synthetic datasets with well-aligned data; while real-world point clouds are often not pre-aligned. How to achieve rotation invariance remains an open problem in point cloud analysis. To meet this challenge, we propose a novel approach toward achieving rotation-invariant (RI) representations by combining local geometry with global topology. In our local-global-representation (LGR)-Net, we have designed a two-branch network where one stream encodes local geometric RI features and the other encodes global topology-preserving RI features. Motivated by the observation that local geometry and global topology have different yet complementary RI responses in varying regions, two-branch RI features are fused by an innovative multi-layer perceptron (MLP) based attention module. To the best of our knowledge, this work is the first principled approach toward adaptively combining global and local information under the context of RI point cloud analysis. Extensive experiments have demonstrated that our LGR-Net achieves the state-of-the-art performance on various rotation-augmented versions of ModelNet40, ShapeNet, ScanObjectNN, and S3DIS.



### Low-Rank HOCA: Efficient High-Order Cross-Modal Attention for Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/1911.00212v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.00212v1)
- **Published**: 2019-11-01 05:53:50+00:00
- **Updated**: 2019-11-01 05:53:50+00:00
- **Authors**: Tao Jin, Siyu Huang, Yingming Li, Zhongfei Zhang
- **Comment**: Accepted as a long paper at EMNLP 2019
- **Journal**: None
- **Summary**: This paper addresses the challenging task of video captioning which aims to generate descriptions for video data. Recently, the attention-based encoder-decoder structures have been widely used in video captioning. In existing literature, the attention weights are often built from the information of an individual modality, while, the association relationships between multiple modalities are neglected. Motivated by this observation, we propose a video captioning model with High-Order Cross-Modal Attention (HOCA) where the attention weights are calculated based on the high-order correlation tensor to capture the frame-level cross-modal interaction of different modalities sufficiently. Furthermore, we novelly introduce Low-Rank HOCA which adopts tensor decomposition to reduce the extremely large space requirement of HOCA, leading to a practical and efficient implementation in real-world applications. Experimental results on two benchmark datasets, MSVD and MSR-VTT, show that Low-rank HOCA establishes a new state-of-the-art.



### Privacy-Preserving Machine Learning Using EtC Images
- **Arxiv ID**: http://arxiv.org/abs/1911.00227v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00227v1)
- **Published**: 2019-11-01 06:54:27+00:00
- **Updated**: 2019-11-01 06:54:27+00:00
- **Authors**: Ayana Kawamura, Yuma Kinoshita, Hitoshi Kiya
- **Comment**: to be presented at IWAIT2020
- **Journal**: None
- **Summary**: In this paper, we propose a novel privacy-preserving machine learning scheme with encrypted images, called EtC (Encryption-then-Compression) images. Using machine learning algorithms in cloud environments has been spreading in many fields. However, there are serious issues with it for end users, due to semi-trusted cloud providers. Accordingly, we propose using EtC images, which have been proposed for EtC systems with JPEG compression. In this paper, a novel property of EtC images is considered under the use of z-score normalization. It is demonstrated that the use of EtC images allows us not only to protect visual information of images, but also to preserve both the Euclidean distance and the inner product between vectors. In addition, dimensionality reduction is shown to can be applied to EtC images for fast and accurate matching. In an experiment, the proposed scheme is applied to a facial recognition algorithm with classifiers for confirming the effectiveness of the scheme under the use of support vector machine (SVM) with the kernel trick.



### Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/1911.00232v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00232v4)
- **Published**: 2019-11-01 07:09:36+00:00
- **Updated**: 2021-09-27 22:59:12+00:00
- **Authors**: Mathew Monfort, Bowen Pan, Kandan Ramakrishnan, Alex Andonian, Barry A McNamara, Alex Lascelles, Quanfu Fan, Dan Gutfreund, Rogerio Feris, Aude Oliva
- **Comment**: None
- **Journal**: None
- **Summary**: Videos capture events that typically contain multiple sequential, and simultaneous, actions even in the span of only a few seconds. However, most large-scale datasets built to train models for action recognition in video only provide a single label per video. Consequently, models can be incorrectly penalized for classifying actions that exist in the videos but are not explicitly labeled and do not learn the full spectrum of information present in each video in training. Towards this goal, we present the Multi-Moments in Time dataset (M-MiT) which includes over two million action labels for over one million three second videos. This multi-label dataset introduces novel challenges on how to train and analyze models for multi-action detection. Here, we present baseline results for multi-action recognition using loss functions adapted for long tail multi-label learning, provide improved methods for visualizing and interpreting models trained for multi-label action detection and show the strength of transferring models trained on M-MiT to smaller datasets.



### Semantic Feature Attention Network for Liver Tumor Segmentation in Large-scale CT database
- **Arxiv ID**: http://arxiv.org/abs/1911.00282v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00282v1)
- **Published**: 2019-11-01 10:01:16+00:00
- **Updated**: 2019-11-01 10:01:16+00:00
- **Authors**: Yao Zhang, Cheng Zhong, Yang Zhang, Zhongchao Shi, Zhiqiang He
- **Comment**: None
- **Journal**: None
- **Summary**: Liver tumor segmentation plays an important role in hepatocellular carcinoma diagnosis and surgical planning. In this paper, we propose a novel Semantic Feature Attention Network (SFAN) for liver tumor segmentation from Computed Tomography (CT) volumes, which exploits the impact of both low-level and high-level features. In the SFAN, a Semantic Attention Transmission (SAT) module is designed to select discriminative low-level localization details with the guidance of neighboring high-level semantic information. Furthermore, a Global Context Attention (GCA) module is proposed to effectively fuse the multi-level features with the guidance of global context. Our experiments are based on 2 challenging databases, the public Liver Tumor Segmentation (LiTS) Challenge database and a large-scale in-house clinical database with 912 CT volumes. Experimental results show that our proposed framework can not only achieve the state-of-the-art performance with the Dice per case on liver tumor segmentation in LiTS database, but also outperform some widely used segmentation algorithms in the large-scale clinical database.



### Picking groups instead of samples: A close look at Static Pool-based Meta-Active Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.00314v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00314v1)
- **Published**: 2019-11-01 12:08:47+00:00
- **Updated**: 2019-11-01 12:08:47+00:00
- **Authors**: Ignasi Mas, Josep Ramon Morros, Veronica Vilaplana
- **Comment**: None
- **Journal**: ICCV Workshop - MDALC 2019. Seoul, South Korea; 2019
- **Summary**: Active Learning techniques are used to tackle learning problems where obtaining training labels is costly. In this work we use Meta-Active Learning to learn to select a subset of samples from a pool of unsupervised input for further annotation. This scenario is called Static Pool-based Meta- Active Learning. We propose to extend existing approaches by performing the selection in a manner that, unlike previous works, can handle the selection of each sample based on the whole selected subset.



### Audience measurement using a top-view camera and oriented trajectories
- **Arxiv ID**: http://arxiv.org/abs/1911.00354v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.00354v1)
- **Published**: 2019-11-01 13:03:11+00:00
- **Updated**: 2019-11-01 13:03:11+00:00
- **Authors**: Manuel Lopez-Palma, Javier Gago, Montserrat Corbalan, Josep Ramon Morros
- **Comment**: None
- **Journal**: IECON 2019
- **Summary**: A crucial aspect for selecting optimal areas for commercial advertising is the probability with which that publicity will be seen. This paper presents a method based on top-view camera measurement, where the probability of viewing is estimated according to the trajectories and movements of the head of the passerby individuals in the area of interest. Using a camera with a depth sensor, the head of the people in the range of view can be detected and modeled. That method allows determining the orientation of the head which is used to estimate the direction of vision. A tracking by detection algorithm is used to compute the trajectory of each user. The attention given at each advertising spot is estimated based on the trajectories and head orientations of the individuals in the area of interest



### DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames
- **Arxiv ID**: http://arxiv.org/abs/1911.00357v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.00357v2)
- **Published**: 2019-11-01 13:07:37+00:00
- **Updated**: 2020-01-20 04:18:58+00:00
- **Authors**: Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, Dhruv Batra
- **Comment**: None
- **Journal**: None
- **Summary**: We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments. DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever stale), making it conceptually simple and easy to implement. In our experiments on training virtual robots to navigate in Habitat-Sim, DD-PPO exhibits near-linear scaling -- achieving a speedup of 107x on 128 GPUs over a serial implementation. We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -- over 6 months of GPU-time training in under 3 days of wall-clock time with 64 GPUs.   This massive-scale training not only sets the state of art on Habitat Autonomous Navigation Challenge 2019, but essentially solves the task --near-perfect autonomous navigation in an unseen environment without access to a map, directly from an RGB-D camera and a GPS+Compass sensor. Fortuitously, error vs computation exhibits a power-law-like distribution; thus, 90% of peak performance is obtained relatively early (at 100 million steps) and relatively cheaply (under 1 day with 8 GPUs). Finally, we show that the scene understanding and navigation policies learned can be transferred to other navigation tasks -- the analog of ImageNet pre-training + task-specific fine-tuning for embodied AI. Our model outperforms ImageNet pre-trained CNNs on these transfer tasks and can serve as a universal resource (all models and code are publicly available).



### Adaptive Precision Training: Quantify Back Propagation in Neural Networks with Fixed-point Numbers
- **Arxiv ID**: http://arxiv.org/abs/1911.00361v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00361v2)
- **Published**: 2019-11-01 13:12:27+00:00
- **Updated**: 2020-03-05 00:20:10+00:00
- **Authors**: Xishan Zhang, Shaoli Liu, Rui Zhang, Chang Liu, Di Huang, Shiyi Zhou, Jiaming Guo, Yu Kang, Qi Guo, Zidong Du, Yunji Chen
- **Comment**: We would like to withdraw the manuscript because it lacks of
  comparisons. The main contribution is not well verified by experiments
- **Journal**: None
- **Summary**: Adaptive Precision Training: Quantify Back Propagation in Neural Networks with Fixed-point Numbers. Recent emerged quantization technique has been applied to inference of deep neural networks for fast and efficient execution. However, directly applying quantization in training can cause significant accuracy loss, thus remaining an open challenge.



### Validation of a deep learning mammography model in a population with low screening rates
- **Arxiv ID**: http://arxiv.org/abs/1911.00364v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00364v1)
- **Published**: 2019-11-01 13:22:22+00:00
- **Updated**: 2019-11-01 13:22:22+00:00
- **Authors**: Kevin Wu, Eric Wu, Yaping Wu, Hongna Tan, Greg Sorensen, Meiyun Wang, Bill Lotter
- **Comment**: None
- **Journal**: NeurIPS 2019. Fair ML for Health Workshop
- **Summary**: A key promise of AI applications in healthcare is in increasing access to quality medical care in under-served populations and emerging markets. However, deep learning models are often only trained on data from advantaged populations that have the infrastructure and resources required for large-scale data collection. In this paper, we aim to empirically investigate the potential impact of such biases on breast cancer detection in mammograms. We specifically explore how a deep learning algorithm trained on screening mammograms from the US and UK generalizes to mammograms collected at a hospital in China, where screening is not widely implemented. For the evaluation, we use a top-scoring model developed for the Digital Mammography DREAM Challenge. Despite the change in institution and population composition, we find that the model generalizes well, exhibiting similar performance to that achieved in the DREAM Challenge, even when controlling for tumor size. We also illustrate a simple but effective method for filtering predictions based on model variance, which can be particularly useful for deployment in new settings. While there are many components in developing a clinically effective system, these results represent a promising step towards increasing access to life-saving screening mammography in populations where screening rates are currently low.



### 3D hierarchical optimization for Multi-view depth map coding
- **Arxiv ID**: http://arxiv.org/abs/1911.00376v1
- **DOI**: 10.1007/s11042-017-5409-z
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.00376v1)
- **Published**: 2019-11-01 13:45:06+00:00
- **Updated**: 2019-11-01 13:45:06+00:00
- **Authors**: Marc Maceira, David Varas, Josep-Ramon Morros, JavierRuiz-Hidalgo, Ferran Marques
- **Comment**: None
- **Journal**: Multimedia Tools and Applications, 77(15), 2018
- **Summary**: Depth data has a widespread use since the popularity of high-resolution 3D sensors. In multi-view sequences, depth information is used to supplement the color data of each view. This article proposes a joint encoding of multiple depth maps with a unique representation. Color and depth images of each view are segmented independently and combined in an optimal Rate-Distortion fashion. The resulting partitions are projected to a reference view where a coherent hierarchy for the multiple views is built. A Rate-Distortionoptimization is applied to obtain the final segmentation choosing nodes of the hierarchy. The consistent segmentation is used to robustly encode depth maps of multiple views obtaining competitive results with HEVC coding standards. Available at: http://link.springer.com/article/10.1007/s11042-017-5409-z



### Multimodal Video-based Apparent Personality Recognition Using Long Short-Term Memory and Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.00381v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.00381v1)
- **Published**: 2019-11-01 13:52:49+00:00
- **Updated**: 2019-11-01 13:52:49+00:00
- **Authors**: Süleyman Aslan, Uğur Güdükbay
- **Comment**: None
- **Journal**: None
- **Summary**: Personality computing and affective computing, where the recognition of personality traits is essential, have gained increasing interest and attention in many research areas recently. We propose a novel approach to recognize the Big Five personality traits of people from videos. Personality and emotion affect the speaking style, facial expressions, body movements, and linguistic factors in social contexts, and they are affected by environmental elements. We develop a multimodal system to recognize apparent personality based on various modalities such as the face, environment, audio, and transcription features. We use modality-specific neural networks that learn to recognize the traits independently and we obtain a final prediction of apparent personality with a feature-level fusion of these networks. We employ pre-trained deep convolutional neural networks such as ResNet and VGGish networks to extract high-level features and Long Short-Term Memory networks to integrate temporal information. We train the large model consisting of modality-specific subnetworks using a two-stage training process. We first train the subnetworks separately and then fine-tune the overall model using these trained networks. We evaluate the proposed method using ChaLearn First Impressions V2 challenge dataset. Our approach obtains the best overall "mean accuracy" score, averaged over five personality traits, compared to the state-of-the-art.



### Protein Fold Family Recognition From Unassigned Residual Dipolar Coupling Data
- **Arxiv ID**: http://arxiv.org/abs/1911.00383v1
- **DOI**: None
- **Categories**: **q-bio.BM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00383v1)
- **Published**: 2019-11-01 14:01:25+00:00
- **Updated**: 2019-11-01 14:01:25+00:00
- **Authors**: Rishi Mukhopadhyay, Paul Shealy, Homayoun Valafar
- **Comment**: BioComp 2008, 7 pages
- **Journal**: None
- **Summary**: Despite many advances in computational modeling of protein structures, these methods have not been widely utilized by experimental structural biologists. Two major obstacles are preventing the transition from a purely-experimental to a purely-computational mode of protein structure determination. The first problem is that most computational methods need a large library of computed structures that span a large variety of protein fold families, while structural genomics initiatives have slowed in their ability to provide novel protein folds in recent years. The second problem is an unwillingness to trust computational models that have no experimental backing. In this paper we test a potential solution to these problems that we have called Probability Density Profile Analysis (PDPA) that utilizes unassigned residual dipolar coupling data that are relatively cheap to acquire from NMR experiments.



### Comb Convolution for Efficient Convolutional Architecture
- **Arxiv ID**: http://arxiv.org/abs/1911.00387v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.00387v1)
- **Published**: 2019-11-01 14:05:08+00:00
- **Updated**: 2019-11-01 14:05:08+00:00
- **Authors**: Dandan Li, Yuan Zhou, Shuwei Huo, Sun-Yuan Kung
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) are inherently suffering from massively redundant computation (FLOPs) due to the dense connection pattern between feature maps and convolution kernels. Recent research has investigated the sparse relationship between channels, however, they ignored the spatial relationship within a channel. In this paper, we present a novel convolutional operator, namely comb convolution, to exploit the intra-channel sparse relationship among neurons. The proposed convolutional operator eliminates nearly 50% of connections by inserting uniform mappings into standard convolutions and removing about half of spatial connections in convolutional layer. Notably, our work is orthogonal and complementary to existing methods that reduce channel-wise redundancy. Thus, it has great potential to further increase efficiency through integrating the comb convolution to existing architectures. Experimental results demonstrate that by simply replacing standard convolutions with comb convolutions on state-of-the-art CNN architectures (e.g., VGGNets, Xception and SE-Net), we can achieve 50% FLOPs reduction while still maintaining the accuracy.



### Air-Writing Translater: A Novel Unsupervised Domain Adaptation Method for Inertia-Trajectory Translation of In-air Handwriting
- **Arxiv ID**: http://arxiv.org/abs/1911.05649v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1911.05649v1)
- **Published**: 2019-11-01 14:09:44+00:00
- **Updated**: 2019-11-01 14:09:44+00:00
- **Authors**: Songbin Xu, Yang Xue, Xin Zhang, Lianwen Jin
- **Comment**: None
- **Journal**: None
- **Summary**: As a new way of human-computer interaction, inertial sensor based in-air handwriting can provide a natural and unconstrained interaction to express more complex and richer information in 3D space. However, most of the existing in-air handwriting work is mainly focused on handwritten character recognition, which makes these work suffer from poor readability of inertial signal and lack of labeled samples. To address these two problems, we use unsupervised domain adaptation method to reconstruct the trajectory of inertial signal and generate inertial samples using online handwritten trajectories. In this paper, we propose an AirWriting Translater model to learn the bi-directional translation between trajectory domain and inertial domain in the absence of paired inertial and trajectory samples. Through semantic-level adversarial training and latent classification loss, the proposed model learns to extract domain-invariant content between inertial signal and trajectory, while preserving semantic consistency during the translation across the two domains. We carefully design the architecture, so that the proposed framework can accept inputs of arbitrary length and translate between different sampling rates. We also conduct experiments on two public datasets: 6DMG (in-air handwriting dataset) and CT (handwritten trajectory dataset), the results on the two datasets demonstrate that the proposed network successes in both Inertia-to Trajectory and Trajectory-to-Inertia translation tasks.



### Cali-Sketch: Stroke Calibration and Completion for High-Quality Face Image Generation from Human-Like Sketches
- **Arxiv ID**: http://arxiv.org/abs/1911.00426v2
- **DOI**: 10.1016/j.neucom.2021.07.029
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.00426v2)
- **Published**: 2019-11-01 15:32:42+00:00
- **Updated**: 2022-10-09 16:58:35+00:00
- **Authors**: Weihao Xia, Yujiu Yang, Jing-Hao Xue
- **Comment**: Accepted to Neurocomputing
- **Journal**: Volume 460, 14 October 2021, Pages 256-265
- **Summary**: Image generation has received increasing attention because of its wide application in security and entertainment. Sketch-based face generation brings more fun and better quality of image generation due to supervised interaction. However, when a sketch poorly aligned with the true face is given as input, existing supervised image-to-image translation methods often cannot generate acceptable photo-realistic face images. To address this problem, in this paper we propose Cali-Sketch, a human-like-sketch to photo-realistic-image generation method. Cali-Sketch explicitly models stroke calibration and image generation using two constituent networks: a Stroke Calibration Network (SCN), which calibrates strokes of facial features and enriches facial details while preserving the original intent features; and an Image Synthesis Network (ISN), which translates the calibrated and enriched sketches to photo-realistic face images. In this way, we manage to decouple a difficult cross-domain translation problem into two easier steps. Extensive experiments verify that the face photos generated by Cali-Sketch are both photo-realistic and faithful to the input sketches, compared with state-of-the-art methods.



### The reliability of a deep learning model in clinical out-of-distribution MRI data: a multicohort study
- **Arxiv ID**: http://arxiv.org/abs/1911.00515v1
- **DOI**: 10.1016/j.media.2020.101714
- **Categories**: **physics.med-ph**, cs.CV, cs.LG, eess.IV, q-bio.QM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.00515v1)
- **Published**: 2019-11-01 15:52:16+00:00
- **Updated**: 2019-11-01 15:52:16+00:00
- **Authors**: Gustav Mårtensson, Daniel Ferreira, Tobias Granberg, Lena Cavallin, Ketil Oppedal, Alessandro Padovani, Irena Rektorova, Laura Bonanni, Matteo Pardini, Milica Kramberger, John-Paul Taylor, Jakub Hort, Jón Snædal, Jaime Kulisevsky, Frederic Blanc, Angelo Antonini, Patrizia Mecocci, Bruno Vellas, Magda Tsolaki, Iwona Kłoszewska, Hilkka Soininen, Simon Lovestone, Andrew Simmons, Dag Aarsland, Eric Westman
- **Comment**: 11 pages, 3 figures
- **Journal**: None
- **Summary**: Deep learning (DL) methods have in recent years yielded impressive results in medical imaging, with the potential to function as clinical aid to radiologists. However, DL models in medical imaging are often trained on public research cohorts with images acquired with a single scanner or with strict protocol harmonization, which is not representative of a clinical setting. The aim of this study was to investigate how well a DL model performs in unseen clinical data sets---collected with different scanners, protocols and disease populations---and whether more heterogeneous training data improves generalization. In total, 3117 MRI scans of brains from multiple dementia research cohorts and memory clinics, that had been visually rated by a neuroradiologist according to Scheltens' scale of medial temporal atrophy (MTA), were included in this study. By training multiple versions of a convolutional neural network on different subsets of this data to predict MTA ratings, we assessed the impact of including images from a wider distribution during training had on performance in external memory clinic data. Our results showed that our model generalized well to data sets acquired with similar protocols as the training data, but substantially worse in clinical cohorts with visibly different tissue contrasts in the images. This implies that future DL studies investigating performance in out-of-distribution (OOD) MRI data need to assess multiple external cohorts for reliable results. Further, by including data from a wider range of scanners and protocols the performance improved in OOD data, which suggests that more heterogeneous training data makes the model generalize better. To conclude, this is the most comprehensive study to date investigating the domain shift in deep learning on MRI data, and we advocate rigorous evaluation of DL models on clinical data prior to being certified for deployment.



### Surface Reconstruction from 3D Line Segments
- **Arxiv ID**: http://arxiv.org/abs/1911.00451v1
- **DOI**: 10.1109/3DV.2019.00067
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.00451v1)
- **Published**: 2019-11-01 16:33:49+00:00
- **Updated**: 2019-11-01 16:33:49+00:00
- **Authors**: Pierre-Alain Langlois, Alexandre Boulch, Renaud Marlet
- **Comment**: In 3DV 2019 (Oral)
- **Journal**: None
- **Summary**: In man-made environments such as indoor scenes, when point-based 3D reconstruction fails due to the lack of texture, lines can still be detected and used to support surfaces. We present a novel method for watertight piecewise-planar surface reconstruction from 3D line segments with visibility information. First, planes are extracted by a novel RANSAC approach for line segments that allows multiple shape support. Then, each 3D cell of a plane arrangement is labeled full or empty based on line attachment to planes, visibility and regularization. Experiments show the robustness to sparse input data, noise and outliers.



### Toward a Better Monitoring Statistic for Profile Monitoring via Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1911.00482v2
- **DOI**: 10.1080/00224065.2021.1903821
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.00482v2)
- **Published**: 2019-11-01 17:47:49+00:00
- **Updated**: 2022-08-10 18:20:38+00:00
- **Authors**: Nurettin Sergin, Hao Yan
- **Comment**: Journal of Quality Technology 53 (2021) 454-473
- **Journal**: None
- **Summary**: Wide accessibility of imaging and profile sensors in modern industrial systems created an abundance of high-dimensional sensing variables. This led to a a growing interest in the research of high-dimensional process monitoring. However, most of the approaches in the literature assume the in-control population to lie on a linear manifold with a given basis (i.e., spline, wavelet, kernel, etc) or an unknown basis (i.e., principal component analysis and its variants), which cannot be used to efficiently model profiles with a nonlinear manifold which is common in many real-life cases. We propose deep probabilistic autoencoders as a viable unsupervised learning approach to model such manifolds. To do so, we formulate nonlinear and probabilistic extensions of the monitoring statistics from classical approaches as the expected reconstruction error (ERE) and the KL-divergence (KLD) based monitoring statistics. Through extensive simulation study, we provide insights on why latent-space based statistics are unreliable and why residual-space based ones typically perform much better for deep learning based approaches. Finally, we demonstrate the superiority of deep probabilistic models via both simulation study and a real-life case study involving images of defects from a hot steel rolling process.



### Explanation by Progressive Exaggeration
- **Arxiv ID**: http://arxiv.org/abs/1911.00483v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00483v3)
- **Published**: 2019-11-01 17:48:24+00:00
- **Updated**: 2020-02-10 20:32:23+00:00
- **Authors**: Sumedha Singla, Brian Pollack, Junxiang Chen, Kayhan Batmanghelich
- **Comment**: None
- **Journal**: None
- **Summary**: As machine learning methods see greater adoption and implementation in high stakes applications such as medical image diagnosis, the need for model interpretability and explanation has become more critical. Classical approaches that assess feature importance (e.g. saliency maps) do not explain how and why a particular region of an image is relevant to the prediction. We propose a method that explains the outcome of a classification black-box by gradually exaggerating the semantic effect of a given class. Given a query input to a classifier, our method produces a progressive set of plausible variations of that query, which gradually changes the posterior probability from its original class to its negation. These counter-factually generated samples preserve features unrelated to the classification decision, such that a user can employ our method as a "tuning knob" to traverse a data manifold while crossing the decision boundary. Our method is model agnostic and only requires the output value and gradient of the predictor with respect to its input.



### Automated Assignment of Backbone Resonances Using Residual Dipolar Couplings Acquired from a Protein with Known Structure
- **Arxiv ID**: http://arxiv.org/abs/1911.00526v1
- **DOI**: None
- **Categories**: **q-bio.BM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00526v1)
- **Published**: 2019-11-01 18:01:52+00:00
- **Updated**: 2019-11-01 18:01:52+00:00
- **Authors**: P. Shealy, R. Mukhopadhyay, S. Smith, H. Valafar
- **Comment**: BioComp 2008, 7 pages
- **Journal**: None
- **Summary**: Resonance assignment is a critical first step in the investigation of protein structures using NMR spectroscopy. The development of assignment methods that require less experimental data is possible with prior knowledge of the macromolecular structure. Automated methods of performing the task of resonance assignment can significantly reduce the financial cost and time requirement for protein structure determination. Such methods can also be beneficial in validating a protein's solution state structure. Here we present a new approach to the assignment problem. Our approach uses only RDC data to assign backbone resonances. It provides simultaneous order tensor estimation and assignment. Our approach compares independent order tensor estimates to determine when the correct order tensor has been found. We demonstrate the algorithm's viability using simulated data from the protein domain 1A1Z.



### Cylindrical Shape Decomposition for 3D Segmentation of Tubular Objects
- **Arxiv ID**: http://arxiv.org/abs/1911.00571v3
- **DOI**: 10.1109/ACCESS.2021.3056958
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/1911.00571v3)
- **Published**: 2019-11-01 19:57:18+00:00
- **Updated**: 2021-02-11 15:33:33+00:00
- **Authors**: Ali Abdollahzadeh, Alejandra Sierra, Jussi Tohka
- **Comment**: None
- **Journal**: in IEEE Access, vol. 9, pp. 23979-23995, 2021
- **Summary**: We develop a cylindrical shape decomposition (CSD) algorithm to decompose an object, a union of several tubular structures, into its semantic components. We decompose the object using its curve skeleton and restricted translational sweeps. For that, CSD partitions the curve skeleton into maximal-length sub-skeletons over an orientation cost, each sub-skeleton corresponds to a semantic component. To find the intersection of the tubular components, CSD translationally sweeps the object in decomposition intervals to identify critical points at which the shape of the object changes substantially. CSD cuts the object at critical points and assigns the same label to parts along the same sub-skeleton, thereby constructing a semantic component. The proposed method further reconstructs the acquired semantic components at the intersection of object parts using generalized cylinders. We apply CSD for segmenting axons in large 3D electron microscopy images and decomposing vascular networks and synthetic objects. We show that our proposal is robust to severe surface noise and outperforms state-of-the-art decomposition techniques in its applications.



### VoteNet+ : An Improved Deep Learning Label Fusion Method for Multi-atlas Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.00582v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.00582v2)
- **Published**: 2019-11-01 20:33:00+00:00
- **Updated**: 2020-01-13 19:45:13+00:00
- **Authors**: Zhipeng Ding, Xu Han, Marc Niethammer
- **Comment**: Accepted by ISBI-2020
- **Journal**: None
- **Summary**: In this work, we improve the performance of multi-atlas segmentation (MAS) by integrating the recently proposed VoteNet model with the joint label fusion (JLF) approach. Specifically, we first illustrate that using a deep convolutional neural network to predict atlas probabilities can better distinguish correct atlas labels from incorrect ones than relying on image intensity difference as is typical in JLF. Motivated by this finding, we propose VoteNet+, an improved deep network to locally predict the probability of an atlas label to differs from the label of the target image. Furthermore, we show that JLF is more suitable for the VoteNet framework as a label fusion method than plurality voting. Lastly, we use Platt scaling to calibrate the probabilities of our new model. Results on LPBA40 3D MR brain images show that our proposed method can achieve better performance than VoteNet.



