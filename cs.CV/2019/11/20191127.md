# Arxiv Papers in cs.CV on 2019-11-27
### Visual Physics: Discovering Physical Laws from Videos
- **Arxiv ID**: http://arxiv.org/abs/1911.11893v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11893v1)
- **Published**: 2019-11-27 00:34:38+00:00
- **Updated**: 2019-11-27 00:34:38+00:00
- **Authors**: Pradyumna Chari, Chinmay Talegaonkar, Yunhao Ba, Achuta Kadambi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we teach a machine to discover the laws of physics from video streams. We assume no prior knowledge of physics, beyond a temporal stream of bounding boxes. The problem is very difficult because a machine must learn not only a governing equation (e.g. projectile motion) but also the existence of governing parameters (e.g. velocities). We evaluate our ability to discover physical laws on videos of elementary physical phenomena, such as projectile motion or circular motion. These elementary tasks have textbook governing equations and enable ground truth verification of our approach.



### AttentionGAN: Unpaired Image-to-Image Translation using Attention-Guided Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.11897v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.11897v5)
- **Published**: 2019-11-27 00:53:27+00:00
- **Updated**: 2021-08-16 09:33:56+00:00
- **Authors**: Hao Tang, Hong Liu, Dan Xu, Philip H. S. Torr, Nicu Sebe
- **Comment**: Accepted to TNNLS, an extended version of a paper published in
  IJCNN2019. arXiv admin note: substantial text overlap with arXiv:1903.12296
- **Journal**: None
- **Summary**: State-of-the-art methods in image-to-image translation are capable of learning a mapping from a source domain to a target domain with unpaired image data. Though the existing methods have achieved promising results, they still produce visual artifacts, being able to translate low-level information but not high-level semantics of input images. One possible reason is that generators do not have the ability to perceive the most discriminative parts between the source and target domains, thus making the generated images low quality. In this paper, we propose a new Attention-Guided Generative Adversarial Networks (AttentionGAN) for the unpaired image-to-image translation task. AttentionGAN can identify the most discriminative foreground objects and minimize the change of the background. The attention-guided generators in AttentionGAN are able to produce attention masks, and then fuse the generation output with the attention masks to obtain high-quality target images. Accordingly, we also design a novel attention-guided discriminator which only considers attended regions. Extensive experiments are conducted on several generative tasks with eight public datasets, demonstrating that the proposed method is effective to generate sharper and more realistic images compared with existing competitive models. The code is available at https://github.com/Ha0Tang/AttentionGAN.



### Potential of deep features for opinion-unaware, distortion-unaware, no-reference image quality assessment
- **Arxiv ID**: http://arxiv.org/abs/1911.11903v1
- **DOI**: 10.1007/978-3-030-54407-2_8
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.11903v1)
- **Published**: 2019-11-27 01:10:56+00:00
- **Updated**: 2019-11-27 01:10:56+00:00
- **Authors**: Subhayan Mukherjee, Giuseppe Valenzise, Irene Cheng
- **Comment**: International Conference on Smart Multimedia (Springer), 16-18
  December 2019, San Diego, California, USA
- **Journal**: None
- **Summary**: Image Quality Assessment algorithms predict a quality score for a pristine or distorted input image, such that it correlates with human opinion. Traditional methods required a non-distorted "reference" version of the input image to compare with, in order to predict this score. However, recent "No-reference" methods circumvent this requirement by modelling the distribution of clean image features, thereby making them more suitable for practical use. However, majority of such methods either use hand-crafted features or require training on human opinion scores (supervised learning), which are difficult to obtain and standardise. We explore the possibility of using deep features instead, particularly, the encoded (bottleneck) feature maps of a Convolutional Autoencoder neural network architecture. Also, we do not train the network on subjective scores (unsupervised learning). The primary requirements for an IQA method are monotonic increase in predicted scores with increasing degree of input image distortion, and consistent ranking of images with the same distortion type and content, but different distortion levels. Quantitative experiments using the Pearson, Kendall and Spearman correlation scores on a diverse set of images show that our proposed method meets the above requirements better than the state-of-art method (which uses hand-crafted features) for three types of distortions: blurring, noise and compression artefacts. This demonstrates the potential for future research in this relatively unexplored sub-area within IQA.



### GhostNet: More Features from Cheap Operations
- **Arxiv ID**: http://arxiv.org/abs/1911.11907v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11907v2)
- **Published**: 2019-11-27 01:36:42+00:00
- **Updated**: 2020-03-13 06:20:33+00:00
- **Authors**: Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, Chang Xu
- **Comment**: CVPR 2020. Code is available at
  https://github.com/huawei-noah/ghostnet
- **Journal**: None
- **Summary**: Deploying convolutional neural networks (CNNs) on embedded devices is difficult due to the limited memory and computation resources. The redundancy in feature maps is an important characteristic of those successful CNNs, but has rarely been investigated in neural architecture design. This paper proposes a novel Ghost module to generate more feature maps from cheap operations. Based on a set of intrinsic feature maps, we apply a series of linear transformations with cheap cost to generate many ghost feature maps that could fully reveal information underlying intrinsic features. The proposed Ghost module can be taken as a plug-and-play component to upgrade existing convolutional neural networks. Ghost bottlenecks are designed to stack Ghost modules, and then the lightweight GhostNet can be easily established. Experiments conducted on benchmarks demonstrate that the proposed Ghost module is an impressive alternative of convolution layers in baseline models, and our GhostNet can achieve higher recognition performance (e.g. $75.7\%$ top-1 accuracy) than MobileNetV3 with similar computational cost on the ImageNet ILSVRC-2012 classification dataset. Code is available at https://github.com/huawei-noah/ghostnet



### Data Augmentation Using Adversarial Training for Construction-Equipment Classification
- **Arxiv ID**: http://arxiv.org/abs/1911.11916v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.11916v1)
- **Published**: 2019-11-27 02:16:53+00:00
- **Updated**: 2019-11-27 02:16:53+00:00
- **Authors**: Francis Baek, Somin Park, Hyoungkwan Kim
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Deep learning-based construction-site image analysis has recently made great progress with regard to accuracy and speed, but it requires a large amount of data. Acquiring sufficient amount of labeled construction-image data is a prerequisite for deep learning-based construction-image recognition and requires considerable time and effort. In this paper, we propose a "data augmentation" scheme based on generative adversarial networks (GANs) for construction-equipment classification. The proposed method combines a GAN and additional "adversarial training" to stably perform "data augmentation" for construction equipment. The "data augmentation" was verified via binary classification experiments involving excavator images, and the average accuracy improvement was 4.094%. In the experiment, three image sizes (32-32-3, 64-64-3, and 128-128-3) and 120, 240, and 480 training samples were used to demonstrate the robustness of the proposed method. These results demonstrated that the proposed method can effectively and reliably generate construction-equipment images and train deep learning-based classifiers for construction equipment.



### In Perfect Shape: Certifiably Optimal 3D Shape Reconstruction from 2D Landmarks
- **Arxiv ID**: http://arxiv.org/abs/1911.11924v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, math.OC, I.2.9; G.1.6; I.4.5; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1911.11924v2)
- **Published**: 2019-11-27 02:49:38+00:00
- **Updated**: 2020-03-21 17:20:41+00:00
- **Authors**: Heng Yang, Luca Carlone
- **Comment**: Camera-ready, CVPR 2020. 18 pages, 5 figures, 1 table
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  2020
- **Summary**: We study the problem of 3D shape reconstruction from 2D landmarks extracted in a single image. We adopt the 3D deformable shape model and formulate the reconstruction as a joint optimization of the camera pose and the linear shape parameters. Our first contribution is to apply Lasserre's hierarchy of convex Sums-of-Squares (SOS) relaxations to solve the shape reconstruction problem and show that the SOS relaxation of minimum order 2 empirically solves the original non-convex problem exactly. Our second contribution is to exploit the structure of the polynomial in the objective function and find a reduced set of basis monomials for the SOS relaxation that significantly decreases the size of the resulting semidefinite program (SDP) without compromising its accuracy. These two contributions, to the best of our knowledge, lead to the first certifiably optimal solver for 3D shape reconstruction, that we name Shape*. Our third contribution is to add an outlier rejection layer to Shape* using a truncated least squares (TLS) robust cost function and leveraging graduated non-convexity to solve TLS without initialization. The result is a robust reconstruction algorithm, named Shape#, that tolerates a large amount of outlier measurements. We evaluate the performance of Shape* and Shape# in both simulated and real experiments, showing that Shape* outperforms local optimization and previous convex relaxation techniques, while Shape# achieves state-of-the-art performance and is robust against 70% outliers in the FG3DCar dataset.



### CSPNet: A New Backbone that can Enhance Learning Capability of CNN
- **Arxiv ID**: http://arxiv.org/abs/1911.11929v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11929v1)
- **Published**: 2019-11-27 03:15:27+00:00
- **Updated**: 2019-11-27 03:15:27+00:00
- **Authors**: Chien-Yao Wang, Hong-Yuan Mark Liao, I-Hau Yeh, Yueh-Hua Wu, Ping-Yang Chen, Jun-Wei Hsieh
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks have enabled state-of-the-art approaches to achieve incredible results on computer vision tasks such as object detection. However, such success greatly relies on costly computation resources, which hinders people with cheap devices from appreciating the advanced technology. In this paper, we propose Cross Stage Partial Network (CSPNet) to mitigate the problem that previous works require heavy inference computations from the network architecture perspective. We attribute the problem to the duplicate gradient information within network optimization. The proposed networks respect the variability of the gradients by integrating feature maps from the beginning and the end of a network stage, which, in our experiments, reduces computations by 20% with equivalent or even superior accuracy on the ImageNet dataset, and significantly outperforms state-of-the-art approaches in terms of AP50 on the MS COCO object detection dataset. The CSPNet is easy to implement and general enough to cope with architectures based on ResNet, ResNeXt, and DenseNet. Source code is at https://github.com/WongKinYiu/CrossStagePartialNetworks.



### A Comprehensive Review On Various State Of Art Techniques For Eye Blink Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.05017v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05017v1)
- **Published**: 2019-11-27 03:23:42+00:00
- **Updated**: 2019-11-27 03:23:42+00:00
- **Authors**: Sannidhan MS, Sunil Kumar Aithal, Abhir Bhandary
- **Comment**: International Journal of Current Engineering and Scientific Research
  (IJCESR) 2018
- **Journal**: None
- **Summary**: Computer Vision is considered to be one of the most important areas in research and has focused on developing many applications that has proved to be useful for both research and societal benefits. Today we have been witnessing many of the road mishaps happening just because of the lack of concentration while driving.As a part of avoiding this kind of disaster happening in day to day life there are many technologies focusing on keeping track of the vehicle drivers concentration.One such technology uses the method of eye blink detection to find out the concentration level of the driver.With the advent of many high end camera devices with cost effectiveness factor today it has become more efficient and cheaper to use eye blink detection for keeping track of the concentration level of the driver.Hence this paper presents an exhaustive review on the implementations of various eye blink detection algorithms.The detection system has also extended its application in various other fields like drowsiness detection and fatigue detection and expression detection.



### Transfer Learning in Visual and Relational Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1911.11938v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.11938v2)
- **Published**: 2019-11-27 03:54:15+00:00
- **Updated**: 2020-02-15 04:26:42+00:00
- **Authors**: T. S. Jayram, Vincent Marois, Tomasz Kornuta, Vincent Albouy, Emre Sevgen, Ahmet S. Ozcan
- **Comment**: 18 pages; more baseline comparisons; additional clarifications
- **Journal**: None
- **Summary**: Transfer learning has become the de facto standard in computer vision and natural language processing, especially where labeled data is scarce. Accuracy can be significantly improved by using pre-trained models and subsequent fine-tuning. In visual reasoning tasks, such as image question answering, transfer learning is more complex. In addition to transferring the capability to recognize visual features, we also expect to transfer the system's ability to reason. Moreover, for video data, temporal reasoning adds another dimension. In this work, we formalize these unique aspects of transfer learning and propose a theoretical framework for visual reasoning, exemplified by the well-established CLEVR and COG datasets. Furthermore, we introduce a new, end-to-end differentiable recurrent model (SAMNet), which shows state-of-the-art accuracy and better performance in transfer learning on both datasets. The improved performance of SAMNet stems from its capability to decouple the abstract multi-step reasoning from the length of the sequence and its selective attention enabling to store only the question-relevant objects in the external memory.



### Novelty Detection Via Blurring
- **Arxiv ID**: http://arxiv.org/abs/1911.11943v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.11943v3)
- **Published**: 2019-11-27 04:10:18+00:00
- **Updated**: 2020-03-03 06:39:00+00:00
- **Authors**: Sungik Choi, Sae-Young Chung
- **Comment**: ICLR 2020
- **Journal**: None
- **Summary**: Conventional out-of-distribution (OOD) detection schemes based on variational autoencoder or Random Network Distillation (RND) have been observed to assign lower uncertainty to the OOD than the target distribution. In this work, we discover that such conventional novelty detection schemes are also vulnerable to the blurred images. Based on the observation, we construct a novel RND-based OOD detector, SVD-RND, that utilizes blurred images during training. Our detector is simple, efficient at test time, and outperforms baseline OOD detectors in various domains. Further results show that SVD-RND learns better target distribution representation than the baseline RND algorithm. Finally, SVD-RND combined with geometric transform achieves near-perfect detection accuracy on the CelebA dataset.



### Can Attention Masks Improve Adversarial Robustness?
- **Arxiv ID**: http://arxiv.org/abs/1911.11946v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.11946v2)
- **Published**: 2019-11-27 04:26:35+00:00
- **Updated**: 2019-12-21 22:55:53+00:00
- **Authors**: Pratik Vaishnavi, Tianji Cong, Kevin Eykholt, Atul Prakash, Amir Rahmati
- **Comment**: Version presented at AAAI-20 workshop on Engineering Dependable and
  Secure Machine Learning Systems (EDSMLS)
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) are known to be susceptible to adversarial examples. Adversarial examples are maliciously crafted inputs that are designed to fool a model, but appear normal to human beings. Recent work has shown that pixel discretization can be used to make classifiers for MNIST highly robust to adversarial examples. However, pixel discretization fails to provide significant protection on more complex datasets. In this paper, we take the first step towards reconciling these contrary findings. Focusing on the observation that discrete pixelization in MNIST makes the background completely black and foreground completely white, we hypothesize that the important property for increasing robustness is the elimination of image background using attention masks before classifying an object. To examine this hypothesis, we create foreground attention masks for two different datasets, GTSRB and MS-COCO. Our initial results suggest that using attention mask leads to improved robustness. On the adversarially trained classifiers, we see an adversarial robustness increase of over 20% on MS-COCO.



### LucidDream: Controlled Temporally-Consistent DeepDream on Videos
- **Arxiv ID**: http://arxiv.org/abs/1911.11960v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.11960v1)
- **Published**: 2019-11-27 05:29:36+00:00
- **Updated**: 2019-11-27 05:29:36+00:00
- **Authors**: Joel Ruben Antony Moniz, Eunsu Kang, Barnabás Póczos
- **Comment**: Workshop on Machine Learning for Creativity and Design, NeurIPS 2019
- **Journal**: None
- **Summary**: In this work, we aim to propose a set of techniques to improve the controllability and aesthetic appeal when DeepDream, which uses a pre-trained neural network to modify images by hallucinating objects into them, is applied to videos. In particular, we demonstrate a simple modification that improves control over the class of object that DeepDream is induced to hallucinate. We also show that the flickering artifacts which frequently appear when DeepDream is applied on videos can be mitigated by the use of an additional temporal consistency loss term.



### AdapNet: Adaptability Decomposing Encoder-Decoder Network for Weakly Supervised Action Recognition and Localization
- **Arxiv ID**: http://arxiv.org/abs/1911.11961v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11961v1)
- **Published**: 2019-11-27 05:29:47+00:00
- **Updated**: 2019-11-27 05:29:47+00:00
- **Authors**: Xiao-Yu Zhang, Changsheng Li, Haichao Shi, Xiaobin Zhu, Peng Li, Jing Dong
- **Comment**: None
- **Journal**: None
- **Summary**: The point process is a solid framework to model sequential data, such as videos, by exploring the underlying relevance. As a challenging problem for high-level video understanding, weakly supervised action recognition and localization in untrimmed videos has attracted intensive research attention. Knowledge transfer by leveraging the publicly available trimmed videos as external guidance is a promising attempt to make up for the coarse-grained video-level annotation and improve the generalization performance. However, unconstrained knowledge transfer may bring about irrelevant noise and jeopardize the learning model. This paper proposes a novel adaptability decomposing encoder-decoder network to transfer reliable knowledge between trimmed and untrimmed videos for action recognition and localization via bidirectional point process modeling, given only video-level annotations. By decomposing the original features into domain-adaptable and domain-specific ones based on their adaptability, trimmed-untrimmed knowledge transfer can be safely confined within a more coherent subspace. An encoder-decoder based structure is carefully designed and jointly optimized to facilitate effective action classification and temporal localization. Extensive experiments are conducted on two benchmark datasets (i.e., THUMOS14 and ActivityNet1.3), and experimental results clearly corroborate the efficacy of our method.



### Graph Representation for Face Analysis in Image Collections
- **Arxiv ID**: http://arxiv.org/abs/1911.11970v1
- **DOI**: None
- **Categories**: **cs.SI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.11970v1)
- **Published**: 2019-11-27 06:02:05+00:00
- **Updated**: 2019-11-27 06:02:05+00:00
- **Authors**: Domingo Mery, Florencia Valdes
- **Comment**: 6 pages, 6 figures
- **Journal**: None
- **Summary**: Given an image collection of a social event with a huge number of pictures, it is very useful to have tools that can be used to analyze how the individuals --that are present in the collection-- interact with each other. In this paper, we propose an optimal graph representation that is based on the `connectivity' of them. The connectivity of a pair of subjects gives a score that represents how `connected' they are. It is estimated based on co-occurrence, closeness, facial expressions, and the orientation of the head when they are looking to each other. In our proposed graph, the nodes represent the subjects of the collection, and the edges correspond to their connectivities. The location of the nodes is estimated according to their connectivity (the closer the nodes, the more connected are the subjects). Finally, we developed a graphical user interface in which we can click onto the nodes (or the edges) to display the corresponding images of the collection in which the subject of the nodes (or the connected subjects) are present. We present relevant results by analyzing a wedding celebration, a sitcom video, a volleyball game and images extracted from Twitter given a hashtag. We believe that this tool can be very helpful to detect the existing social relations in an image collection.



### Class-Conditional Domain Adaptation on Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.11981v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11981v2)
- **Published**: 2019-11-27 06:38:28+00:00
- **Updated**: 2019-11-28 01:53:08+00:00
- **Authors**: Yue Wang, Yuke Li, James H. Elder, Runmin Wu, Huchuan Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation is an important sub-task for many applications, but pixel-level ground truth labeling is costly and there is a tendency to overfit the training data, limiting generalization. Unsupervised domain adaptation can potentially address these problems, allowing systems trained on labelled datasets from one or more source domains (including less expensive synthetic domains) to be adapted to novel target domains. The conventional approach is to automatically align the representational distributions of source and target domains. One limitation of this approach is that it tends to disadvantage lower probability classes. We address this problem by introducing a Class-Conditional Domain Adaptation method (CCDA). It includes a class-conditional multi-scale discriminator and the class-conditional loss. This novel CCDA method encourages the network to shift the domain in a class-conditional manner, and it equalizes loss over classes. We evaluate our CCDA method on two transfer tasks and demonstrate performance comparable to state-of-the-art methods.



### Semantic Head Enhanced Pedestrian Detection in a Crowd
- **Arxiv ID**: http://arxiv.org/abs/1911.11985v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11985v1)
- **Published**: 2019-11-27 06:52:04+00:00
- **Updated**: 2019-11-27 06:52:04+00:00
- **Authors**: Ruiqi Lu, Huimin Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Pedestrian detection in the crowd is a challenging task because of intra-class occlusion. More prior information is needed for the detector to be robust against it. Human head area is naturally a strong cue because of its stable appearance, visibility and relative location to body. Inspired by it, we adopt an extra branch to conduct semantic head detection in parallel with traditional body branch. Instead of manually labeling the head regions, we use weak annotations inferred directly from body boxes, which is named as `semantic head'. In this way, the head detection is formulated into using a special part of labeled box to detect the corresponding part of human body, which surprisingly improves the performance and robustness to occlusion. Moreover, the head-body alignment structure is explicitly explored by introducing Alignment Loss, which functions in a self-supervised manner. Based on these, we propose the head-body alignment net (HBAN) in this work, which aims to enhance pedestrian detection by fully utilizing the human head prior. Comprehensive evaluations are conducted to demonstrate the effectiveness of HBAN on CityPersons dataset.



### GRIm-RePR: Prioritising Generating Important Features for Pseudo-Rehearsal
- **Arxiv ID**: http://arxiv.org/abs/1911.11988v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.11988v1)
- **Published**: 2019-11-27 07:06:03+00:00
- **Updated**: 2019-11-27 07:06:03+00:00
- **Authors**: Craig Atkinson, Brendan McCane, Lech Szymanski, Anthony Robins
- **Comment**: None
- **Journal**: None
- **Summary**: Pseudo-rehearsal allows neural networks to learn a sequence of tasks without forgetting how to perform in earlier tasks. Preventing forgetting is achieved by introducing a generative network which can produce data from previously seen tasks so that it can be rehearsed along side learning the new task. This has been found to be effective in both supervised and reinforcement learning. Our current work aims to further prevent forgetting by encouraging the generator to accurately generate features important for task retention. More specifically, the generator is improved by introducing a second discriminator into the Generative Adversarial Network which learns to classify between real and fake items from the intermediate activation patterns that they produce when fed through a continual learning agent. Using Atari 2600 games, we experimentally find that improving the generator can considerably reduce catastrophic forgetting compared to the standard pseudo-rehearsal methods used in deep reinforcement learning. Furthermore, we propose normalising the Q-values taught to the long-term system as we observe this substantially reduces catastrophic forgetting by minimising the interference between tasks' reward functions.



### DoveNet: Deep Image Harmonization via Domain Verification
- **Arxiv ID**: http://arxiv.org/abs/1911.13239v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.13239v3)
- **Published**: 2019-11-27 07:14:50+00:00
- **Updated**: 2020-10-30 08:38:11+00:00
- **Authors**: Wenyan Cong, Jianfu Zhang, Li Niu, Liu Liu, Zhixin Ling, Weiyuan Li, Liqing Zhang
- **Comment**: Accepted by CVPR2020. arXiv admin note: text overlap with
  arXiv:1908.10526
- **Journal**: None
- **Summary**: Image composition is an important operation in image processing, but the inconsistency between foreground and background significantly degrades the quality of composite image. Image harmonization, aiming to make the foreground compatible with the background, is a promising yet challenging task. However, the lack of high-quality publicly available dataset for image harmonization greatly hinders the development of image harmonization techniques. In this work, we contribute an image harmonization dataset iHarmony4 by generating synthesized composite images based on COCO (resp., Adobe5k, Flickr, day2night) dataset, leading to our HCOCO (resp., HAdobe5k, HFlickr, Hday2night) sub-dataset. Moreover, we propose a new deep image harmonization method DoveNet using a novel domain verification discriminator, with the insight that the foreground needs to be translated to the same domain as background. Extensive experiments on our constructed dataset demonstrate the effectiveness of our proposed method. Our dataset and code are available at https://github.com/bcmi/Image_Harmonization_Datasets.



### Recovering Facial Reflectance and Geometry from Multi-view Images
- **Arxiv ID**: http://arxiv.org/abs/1911.11999v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1911.11999v1)
- **Published**: 2019-11-27 07:50:23+00:00
- **Updated**: 2019-11-27 07:50:23+00:00
- **Authors**: Guoxian Song, Jianmin Zheng, Jianfei Cai, Tat-Jen Cham
- **Comment**: None
- **Journal**: None
- **Summary**: While the problem of estimating shapes and diffuse reflectances of human faces from images has been extensively studied, there is relatively less work done on recovering the specular albedo. This paper presents a lightweight solution for inferring photorealistic facial reflectance and geometry. Our system processes video streams from two views of a subject, and outputs two reflectance maps for diffuse and specular albedos, as well as a vector map of surface normals. A model-based optimization approach is used, consisting of the three stages of multi-view face model fitting, facial reflectance inference and facial geometry refinement. Our approach is based on a novel formulation built upon the 3D morphable model (3DMM) for representing 3D textured faces in conjunction with the Blinn-Phong reflection model. It has the advantage of requiring only a simple setup with two video streams, and is able to exploit the interaction between the diffuse and specular reflections across multiple views as well as time frames. As a result, the method is able to reliably recover high-fidelity facial reflectance and geometry, which facilitates various applications such as generating photorealistic facial images under new viewpoints or illumination conditions.



### Deep Stereo using Adaptive Thin Volume Representation with Uncertainty Awareness
- **Arxiv ID**: http://arxiv.org/abs/1911.12012v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1911.12012v2)
- **Published**: 2019-11-27 08:14:52+00:00
- **Updated**: 2020-04-18 23:09:41+00:00
- **Authors**: Shuo Cheng, Zexiang Xu, Shilin Zhu, Zhuwen Li, Li Erran Li, Ravi Ramamoorthi, Hao Su
- **Comment**: Accepted to CVPR 2020 (Oral)
- **Journal**: None
- **Summary**: We present Uncertainty-aware Cascaded Stereo Network (UCS-Net) for 3D reconstruction from multiple RGB images. Multi-view stereo (MVS) aims to reconstruct fine-grained scene geometry from multi-view images. Previous learning-based MVS methods estimate per-view depth using plane sweep volumes with a fixed depth hypothesis at each plane; this generally requires densely sampled planes for desired accuracy, and it is very hard to achieve high-resolution depth. In contrast, we propose adaptive thin volumes (ATVs); in an ATV, the depth hypothesis of each plane is spatially varying, which adapts to the uncertainties of previous per-pixel depth predictions. Our UCS-Net has three stages: the first stage processes a small standard plane sweep volume to predict low-resolution depth; two ATVs are then used in the following stages to refine the depth with higher resolution and higher accuracy. Our ATV consists of only a small number of planes; yet, it efficiently partitions local depth ranges within learned small intervals. In particular, we propose to use variance-based uncertainty estimates to adaptively construct ATVs; this differentiable process introduces reasonable and fine-grained spatial partitioning. Our multi-stage framework progressively subdivides the vast scene space with increasing depth resolution and precision, which enables scene reconstruction with high completeness and accuracy in a coarse-to-fine fashion. We demonstrate that our method achieves superior performance compared with state-of-the-art benchmarks on various challenging datasets.



### Non-Autoregressive Coarse-to-Fine Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/1911.12018v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12018v6)
- **Published**: 2019-11-27 08:36:41+00:00
- **Updated**: 2021-03-24 05:43:53+00:00
- **Authors**: Bang Yang, Yuexian Zou, Fenglin Liu, Can Zhang
- **Comment**: 9 pages, 6 figures, to be published in AAAI2021. Our code is
  available at
  https://github.com/yangbang18/Non-Autoregressive-Video-Captioning
- **Journal**: None
- **Summary**: It is encouraged to see that progress has been made to bridge videos and natural language. However, mainstream video captioning methods suffer from slow inference speed due to the sequential manner of autoregressive decoding, and prefer generating generic descriptions due to the insufficient training of visual words (e.g., nouns and verbs) and inadequate decoding paradigm. In this paper, we propose a non-autoregressive decoding based model with a coarse-to-fine captioning procedure to alleviate these defects. In implementations, we employ a bi-directional self-attention based network as our language model for achieving inference speedup, based on which we decompose the captioning procedure into two stages, where the model has different focuses. Specifically, given that visual words determine the semantic correctness of captions, we design a mechanism of generating visual words to not only promote the training of scene-related words but also capture relevant details from videos to construct a coarse-grained sentence "template". Thereafter, we devise dedicated decoding algorithms that fill in the "template" with suitable words and modify inappropriate phrasing via iterative refinement to obtain a fine-grained description. Extensive experiments on two mainstream video captioning benchmarks, i.e., MSVD and MSR-VTT, demonstrate that our approach achieves state-of-the-art performance, generates diverse descriptions, and obtains high inference efficiency. Our code is available at https://github.com/yangbang18/Non-Autoregressive-Video-Captioning.



### Methods of Weighted Combination for Text Field Recognition in a Video Stream
- **Arxiv ID**: http://arxiv.org/abs/1911.12028v1
- **DOI**: 10.1117/12.2559378
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12028v1)
- **Published**: 2019-11-27 08:48:54+00:00
- **Updated**: 2019-11-27 08:48:54+00:00
- **Authors**: Olga Petrova, Konstantin Bulatov, Vladimir L. Arlazarov
- **Comment**: 6 pages, 4 figures, 1 table, accepted and presented at International
  Conference on Machine Vision 2019 (ICMV 2019)
- **Journal**: Proc. SPIE 11433 ICMV-2019 (2020), 114332L
- **Summary**: Due to a noticeable expansion of document recognition applicability, there is a high demand for recognition on mobile devices. A mobile camera, unlike a scanner, cannot always ensure the absence of various image distortions, therefore the task of improving the recognition precision is relevant. The advantage of mobile devices over scanners is the ability to use video stream input, which allows to get multiple images of a recognized document. Despite this, not enough attention is currently paid to the issue of combining recognition results obtained from different frames when using video stream input. In this paper we propose a weighted text string recognition results combination method and weighting criteria, and provide experimental data for verifying their validity and effectiveness. Based on the obtained results, it is concluded that the use of such weighted combination is appropriate for improving the quality of the video stream recognition result.



### Discriminative Adversarial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1911.12036v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.12036v2)
- **Published**: 2019-11-27 09:19:16+00:00
- **Updated**: 2019-12-17 03:49:04+00:00
- **Authors**: Hui Tang, Kui Jia
- **Comment**: 18 pages, 10 figures, 12 tables, accepted by AAAI-20
- **Journal**: None
- **Summary**: Given labeled instances on a source domain and unlabeled ones on a target domain, unsupervised domain adaptation aims to learn a task classifier that can well classify target instances. Recent advances rely on domain-adversarial training of deep networks to learn domain-invariant features. However, due to an issue of mode collapse induced by the separate design of task and domain classifiers, these methods are limited in aligning the joint distributions of feature and category across domains. To overcome it, we propose a novel adversarial learning method termed Discriminative Adversarial Domain Adaptation (DADA). Based on an integrated category and domain classifier, DADA has a novel adversarial objective that encourages a mutually inhibitory relation between category and domain predictions for any input instance. We show that under practical conditions, it defines a minimax game that can promote the joint distribution alignment. Except for the traditional closed set domain adaptation, we also extend DADA for extremely challenging problem settings of partial and open set domain adaptation. Experiments show the efficacy of our proposed methods and we achieve the new state of the art for all the three settings on benchmark datasets.



### Locality Aware Appearance Metric for Multi-Target Multi-Camera Tracking
- **Arxiv ID**: http://arxiv.org/abs/1911.12037v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12037v1)
- **Published**: 2019-11-27 09:22:32+00:00
- **Updated**: 2019-11-27 09:22:32+00:00
- **Authors**: Yunzhong Hou, Liang Zheng, Zhongdao Wang, Shengjin Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-target multi-camera tracking (MTMCT) systems track targets across cameras. Due to the continuity of target trajectories, tracking systems usually restrict their data association within a local neighborhood. In single camera tracking, local neighborhood refers to consecutive frames; in multi-camera tracking, it refers to neighboring cameras that the target may appear successively. For similarity estimation, tracking systems often adopt appearance features learned from the re-identification (re-ID) perspective. Different from tracking, re-ID usually does not have access to the trajectory cues that can limit the search space to a local neighborhood. Due to its global matching property, the re-ID perspective requires to learn global appearance features. We argue that the mismatch between the local matching procedure in tracking and the global nature of re-ID appearance features may compromise MTMCT performance.   To fit the local matching procedure in MTMCT, in this work, we introduce locality aware appearance metric (LAAM). Specifically, we design an intra-camera metric for single camera tracking, and an inter-camera metric for multi-camera tracking. Both metrics are trained with data pairs sampled from their corresponding local neighborhoods, as opposed to global sampling in the re-ID perspective. We show that the locally learned metrics can be successfully applied on top of several globally learned re-ID features. With the proposed method, we report new state-of-the-art performance on the DukeMTMC dataset, and a substantial improvement on the CityFlow dataset.



### Exploring Frequency Domain Interpretation of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.12044v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12044v2)
- **Published**: 2019-11-27 09:41:39+00:00
- **Updated**: 2020-02-28 14:07:30+00:00
- **Authors**: Zhongfan Jia, Chenglong Bao, Kaisheng Ma
- **Comment**: The main conclusion of this paper is ambiguous and cannot be fully
  supported by the experiments. We decide to withdraw this paper
- **Journal**: None
- **Summary**: Many existing interpretation methods of convolutional neural networks (CNNs) mainly analyze in spatial domain, yet model interpretability in frequency domain has been rarely studied. To the best of our knowledge, there is no study on the interpretation of modern CNNs from the perspective of the frequency proportion of filters. In this work, we analyze the frequency properties of filters in the first layer as it is the entrance of information and relatively more convenient for analysis. By controlling the proportion of different frequency filters in the training stage, the network classification accuracy and model robustness is evaluated and our results reveal that it has a great impact on the robustness to common corruptions. Moreover, a learnable modulation of frequency proportion with perturbation in power spectrum is proposed from the perspective of frequency domain. Experiments on CIFAR-10-C show 10.97% average robustness gains for ResNet-18 with negligible natural accuracy degradation.



### Residual Bi-Fusion Feature Pyramid Network for Accurate Single-shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.12051v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12051v2)
- **Published**: 2019-11-27 09:53:49+00:00
- **Updated**: 2019-12-10 06:54:49+00:00
- **Authors**: Ping-Yang Chen, Jun-Wei Hsieh, Chien-Yao Wang, Hong-Yuan Mark Liao, Munkhjargal Gochoo
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art (SoTA) models have improved the accuracy of object detection with a large margin via a FP (feature pyramid). FP is a top-down aggregation to collect semantically strong features to improve scale invariance in both two-stage and one-stage detectors. However, this top-down pathway cannot preserve accurate object positions due to the shift-effect of pooling. Thus, the advantage of FP to improve detection accuracy will disappear when more layers are used. The original FP lacks a bottom-up pathway to offset the lost information from lower-layer feature maps. It performs well in large-sized object detection but poor in small-sized object detection. A new structure "residual feature pyramid" is proposed in this paper. It is bidirectional to fuse both deep and shallow features towards more effective and robust detection for both small-sized and large-sized objects. Due to the "residual" nature, it can be easily trained and integrated to different backbones (even deeper or lighter) than other bi-directional methods. One important property of this residual FP is: accuracy improvement is still found even if more layers are adopted. Extensive experiments on VOC and MS COCO datasets showed the proposed method achieved the SoTA results for highly-accurate and efficient object detection..



### Grapy-ML: Graph Pyramid Mutual Learning for Cross-dataset Human Parsing
- **Arxiv ID**: http://arxiv.org/abs/1911.12053v1
- **DOI**: 10.1609/aaai.v34i07.6728
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12053v1)
- **Published**: 2019-11-27 09:59:25+00:00
- **Updated**: 2019-11-27 09:59:25+00:00
- **Authors**: Haoyu He, Jing Zhang, Qiming Zhang, Dacheng Tao
- **Comment**: Accepted as an oral paper in AAAI2020. 9 pages, 4 figures.
  https://www.aaai.org/Papers/AAAI/2020GB/AAAI-HeH.2317.pdf
- **Journal**: AAAI 2020
- **Summary**: Human parsing, or human body part semantic segmentation, has been an active research topic due to its wide potential applications. In this paper, we propose a novel GRAph PYramid Mutual Learning (Grapy-ML) method to address the cross-dataset human parsing problem, where the annotations are at different granularities. Starting from the prior knowledge of the human body hierarchical structure, we devise a graph pyramid module (GPM) by stacking three levels of graph structures from coarse granularity to fine granularity subsequently. At each level, GPM utilizes the self-attention mechanism to model the correlations between context nodes. Then, it adopts a top-down mechanism to progressively refine the hierarchical features through all the levels. GPM also enables efficient mutual learning. Specifically, the network weights of the first two levels are shared to exchange the learned coarse-granularity information across different datasets. By making use of the multi-granularity labels, Grapy-ML learns a more discriminative feature representation and achieves state-of-the-art performance, which is demonstrated by extensive experiments on the three popular benchmarks, e.g. CIHP dataset. The source code is publicly available at https://github.com/Charleshhy/Grapy-ML.



### SpoC: Spoofing Camera Fingerprints
- **Arxiv ID**: http://arxiv.org/abs/1911.12069v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.12069v2)
- **Published**: 2019-11-27 10:41:19+00:00
- **Updated**: 2021-04-22 09:51:53+00:00
- **Authors**: Davide Cozzolino, Justus Thies, Andreas Rössler, Matthias Nießner, Luisa Verdoliva
- **Comment**: None
- **Journal**: None
- **Summary**: Thanks to the fast progress in synthetic media generation, creating realistic false images has become very easy. Such images can be used to wrap "rich" fake news with enhanced credibility, spawning a new wave of high-impact, high-risk misinformation campaigns. Therefore, there is a fast-growing interest in reliable detectors of manipulated media. The most powerful detectors, to date, rely on the subtle traces left by any device on all images acquired by it. In particular, due to proprietary in-camera processes, like demosaicing or compression, each camera model leaves trademark traces that can be exploited for forensic analyses. The absence or distortion of such traces in the target image is a strong hint of manipulation. In this paper, we challenge such detectors to gain better insight into their vulnerabilities. This is an important study in order to build better forgery detectors able to face malicious attacks. Our proposal consists of a GAN-based approach that injects camera traces into synthetic images. Given a GAN-generated image, we insert the traces of a specific camera model into it and deceive state-of-the-art detectors into believing the image was acquired by that model. Likewise, we deceive independent detectors of synthetic GAN images into believing the image is real. Experiments prove the effectiveness of the proposed method in a wide array of conditions. Moreover, no prior information on the attacked detectors is needed, but only sample images from the target camera.



### Decision Propagation Networks for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1911.12101v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12101v1)
- **Published**: 2019-11-27 12:13:06+00:00
- **Updated**: 2019-11-27 12:13:06+00:00
- **Authors**: Keke Tang, Peng Song, Yuexin Ma, Zhaoquan Gu, Yu Su, Zhihong Tian, Wenping Wang
- **Comment**: None
- **Journal**: None
- **Summary**: High-level (e.g., semantic) features encoded in the latter layers of convolutional neural networks are extensively exploited for image classification, leaving low-level (e.g., color) features in the early layers underexplored. In this paper, we propose a novel Decision Propagation Module (DPM) to make an intermediate decision that could act as category-coherent guidance extracted from early layers, and then propagate it to the latter layers. Therefore, by stacking a collection of DPMs into a classification network, the generated Decision Propagation Network is explicitly formulated as to progressively encode more discriminative features guided by the decision, and then refine the decision based on the new generated features layer by layer. Comprehensive results on four publicly available datasets validate DPM could bring significant improvements for existing classification networks with minimal additional computational cost and is superior to the state-of-the-art methods.



### Adaptive Initialization Method for K-means Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1911.12104v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.12104v1)
- **Published**: 2019-11-27 12:27:00+00:00
- **Updated**: 2019-11-27 12:27:00+00:00
- **Authors**: Jie Yang, Yu-Kai Wang, Xin Yao, Chin-Teng Lin
- **Comment**: 22 pages, 2 figures, 11 tables
- **Journal**: None
- **Summary**: The K-means algorithm is a widely used clustering algorithm that offers simplicity and efficiency. However, the traditional K-means algorithm uses the random method to determine the initial cluster centers, which make clustering results prone to local optima and then result in worse clustering performance. Many initialization methods have been proposed, but none of them can dynamically adapt to datasets with various characteristics. In our previous research, an initialization method for K-means based on hybrid distance was proposed, and this algorithm can adapt to datasets with different characteristics. However, it has the following drawbacks: (a) When calculating density, the threshold cannot be uniquely determined, resulting in unstable results. (b) Heavily depending on adjusting the parameter, the parameter must be adjusted five times to obtain better clustering results. (c) The time complexity of the algorithm is quadratic, which is difficult to apply to large datasets. In the current paper, we proposed an adaptive initialization method for the K-means algorithm (AIMK) to improve our previous work. AIMK can not only adapt to datasets with various characteristics but also obtain better clustering results within two interactions. In addition, we then leverage random sampling in AIMK, which is named as AIMK-RS, to reduce the time complexity. AIMK-RS is easily applied to large and high-dimensional datasets. We compared AIMK and AIMK-RS with 10 different algorithms on 16 normal and six extra-large datasets. The experimental results show that AIMK and AIMK-RS outperform the current initialization methods and several well-known clustering algorithms. Furthermore, AIMK-RS can significantly reduce the complexity of applying it to extra-large datasets with high dimensions. The time complexity of AIMK-RS is O(n).



### AdaSample: Adaptive Sampling of Hard Positives for Descriptor Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.12110v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12110v1)
- **Published**: 2019-11-27 12:38:08+00:00
- **Updated**: 2019-11-27 12:38:08+00:00
- **Authors**: Xin-Yu Zhang, Le Zhang, Zao-Yi Zheng, Yun Liu, Jia-Wang Bian, Ming-Ming Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Triplet loss has been widely employed in a wide range of computer vision tasks, including local descriptor learning. The effectiveness of the triplet loss heavily relies on the triplet selection, in which a common practice is to first sample intra-class patches (positives) from the dataset for batch construction and then mine in-batch negatives to form triplets. For high-informativeness triplet collection, researchers mostly focus on mining hard negatives in the second stage, while paying relatively less attention to constructing informative batches. To alleviate this issue, we propose AdaSample, an adaptive online batch sampler, in this paper. Specifically, hard positives are sampled based on their informativeness. In this way, we formulate a hardness-aware positive mining pipeline within a novel maximum loss minimization training protocol. The efficacy of the proposed method is evaluated on several standard benchmarks, where it demonstrates a significant and consistent performance gain on top of the existing strong baselines.



### Analysis of Explainers of Black Box Deep Neural Networks for Computer Vision: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1911.12116v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.12116v1)
- **Published**: 2019-11-27 12:58:52+00:00
- **Updated**: 2019-11-27 12:58:52+00:00
- **Authors**: Vanessa Buhrmester, David Münch, Michael Arens
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Learning is a state-of-the-art technique to make inference on extensive or complex data. As a black box model due to their multilayer nonlinear structure, Deep Neural Networks are often criticized to be non-transparent and their predictions not traceable by humans. Furthermore, the models learn from artificial datasets, often with bias or contaminated discriminating content. Through their increased distribution, decision-making algorithms can contribute promoting prejudge and unfairness which is not easy to notice due to lack of transparency. Hence, scientists developed several so-called explanators or explainers which try to point out the connection between input and output to represent in a simplified way the inner structure of machine learning black boxes. In this survey we differ the mechanisms and properties of explaining systems for Deep Neural Networks for Computer Vision tasks. We give a comprehensive overview about taxonomy of related studies and compare several survey papers that deal with explainability in general. We work out the drawbacks and gaps and summarize further research ideas.



### Fair DARTS: Eliminating Unfair Advantages in Differentiable Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1911.12126v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.12126v4)
- **Published**: 2019-11-27 13:10:25+00:00
- **Updated**: 2020-07-16 01:16:52+00:00
- **Authors**: Xiangxiang Chu, Tianbao Zhou, Bo Zhang, Jixiang Li
- **Comment**: Accepted to ECCV 2020, camera ready version
- **Journal**: None
- **Summary**: Differentiable Architecture Search (DARTS) is now a widely disseminated weight-sharing neural architecture search method. However, it suffers from well-known performance collapse due to an inevitable aggregation of skip connections. In this paper, we first disclose that its root cause lies in an unfair advantage in exclusive competition. Through experiments, we show that if either of two conditions is broken, the collapse disappears. Thereby, we present a novel approach called Fair DARTS where the exclusive competition is relaxed to be collaborative. Specifically, we let each operation's architectural weight be independent of others. Yet there is still an important issue of discretization discrepancy. We then propose a zero-one loss to push architectural weights towards zero or one, which approximates an expected multi-hot solution. Our experiments are performed on two mainstream search spaces, and we derive new state-of-the-art results on CIFAR-10 and ImageNet. Our code is available on https://github.com/xiaomi-automl/fairdarts .



### Towards Precise End-to-end Weakly Supervised Object Detection Network
- **Arxiv ID**: http://arxiv.org/abs/1911.12148v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12148v1)
- **Published**: 2019-11-27 13:34:38+00:00
- **Updated**: 2019-11-27 13:34:38+00:00
- **Authors**: Ke Yang, Dongsheng Li, Yong Dou
- **Comment**: accepted by ICCV 2019
- **Journal**: None
- **Summary**: It is challenging for weakly supervised object detection network to precisely predict the positions of the objects, since there are no instance-level category annotations. Most existing methods tend to solve this problem by using a two-phase learning procedure, i.e., multiple instance learning detector followed by a fully supervised learning detector with bounding-box regression. Based on our observation, this procedure may lead to local minima for some object categories. In this paper, we propose to jointly train the two phases in an end-to-end manner to tackle this problem. Specifically, we design a single network with both multiple instance learning and bounding-box regression branches that share the same backbone. Meanwhile, a guided attention module using classification loss is added to the backbone for effectively extracting the implicit location information in the features. Experimental results on public datasets show that our method achieves state-of-the-art performance.



### Shearlets as Feature Extractor for Semantic Edge Detection: The Model-Based and Data-Driven Realm
- **Arxiv ID**: http://arxiv.org/abs/1911.12159v1
- **DOI**: 10.1098/rspa.2019.0841
- **Categories**: **eess.IV**, cs.CV, cs.LG, 42Bxx, 35A18, 65T60, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1911.12159v1)
- **Published**: 2019-11-27 14:05:26+00:00
- **Updated**: 2019-11-27 14:05:26+00:00
- **Authors**: Héctor Andrade-Loarca, Gitta Kutyniok, Ozan Öktem
- **Comment**: 30 pages, 12 figures. To appear in Proceedings of the Royal Society.
  Mathematical, physical and engineering sciences
- **Journal**: None
- **Summary**: Semantic edge detection has recently gained a lot of attention as an image processing task, mainly due to its wide range of real-world applications. This is based on the fact that edges in images contain most of the semantic information. Semantic edge detection involves two tasks, namely pure edge detecion and edge classification. Those are in fact fundamentally distinct in terms of the level of abstraction that each task requires, which is known as the distracted supervision paradox that limits the possible performance of a supervised model in semantic edge detection. In this work, we will present a novel hybrid method to avoid the distracted supervision paradox and achieve high-performance in semantic edge detection. Our approach is based on a combination of the model-based concept of shearlets, which provides probably optimally sparse approximations of a model-class of images, and the data-driven method of a suitably designed convolutional neural netwok. Finally, we present several applications such as tomographic reconstruction and show that our approach signifiantly outperforms former methods, thereby indicating the value of such hybrid methods for the area in biomedical imaging.



### High- and Low-level image component decomposition using VAEs for improved reconstruction and anomaly detection
- **Arxiv ID**: http://arxiv.org/abs/1911.12161v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.12161v1)
- **Published**: 2019-11-27 14:08:25+00:00
- **Updated**: 2019-11-27 14:08:25+00:00
- **Authors**: David Zimmerer, Jens Petersen, Klaus Maier-Hein
- **Comment**: None
- **Journal**: None
- **Summary**: Variational Auto-Encoders have often been used for unsupervised pretraining, feature extraction and out-of-distribution and anomaly detection in the medical field. However, VAEs often lack the ability to produce sharp images and learn high-level features. We propose to alleviate these issues by adding a new branch to conditional hierarchical VAEs. This enforces a division between higher-level and lower-level features. Despite the additional computational overhead compared to a normal VAE it results in sharper and better reconstructions and can capture the data distribution similarly well (indicated by a similar or slightly better OoD detection performance).



### Document Structure Extraction using Prior based High Resolution Hierarchical Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.12170v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12170v2)
- **Published**: 2019-11-27 14:18:02+00:00
- **Updated**: 2020-09-17 14:53:19+00:00
- **Authors**: Mausoom Sarkar, Milan Aggarwal, Arneh Jain, Hiresh Gupta, Balaji Krishnamurthy
- **Comment**: This work has been accepted at ECCV 2020
- **Journal**: None
- **Summary**: Structure extraction from document images has been a long-standing research topic due to its high impact on a wide range of practical applications. In this paper, we share our findings on employing a hierarchical semantic segmentation network for this task of structure extraction. We propose a prior based deep hierarchical CNN network architecture that enables document structure extraction using very high resolution(1800 x 1000) images. We divide the document image into overlapping horizontal strips such that the network segments a strip and uses its prediction mask as prior for predicting the segmentation of the subsequent strip. We perform experiments establishing the effectiveness of our strip based network architecture through ablation methods and comparison with low-resolution variations. Further, to demonstrate our network's capabilities, we train it on only one type of documents (Forms) and achieve state-of-the-art results over other general document datasets. We introduce our new human-annotated forms dataset and show that our method significantly outperforms different segmentation baselines on this dataset in extracting hierarchical structures. Our method is currently being used in Adobe's AEM Forms for automated conversion of paper and PDF forms to modern HTML based forms.



### Orthogonal Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.12207v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12207v3)
- **Published**: 2019-11-27 15:04:26+00:00
- **Updated**: 2020-04-08 04:58:47+00:00
- **Authors**: Jiayun Wang, Yubei Chen, Rudrasis Chakraborty, Stella X. Yu
- **Comment**: To appear in CVPR 2020, project page: http://pwang.pw/ocnn.html
- **Journal**: None
- **Summary**: Deep convolutional neural networks are hindered by training instability and feature redundancy towards further performance improvement. A promising solution is to impose orthogonality on convolutional filters.   We develop an efficient approach to impose filter orthogonality on a convolutional layer based on the doubly block-Toeplitz matrix representation of the convolutional kernel instead of using the common kernel orthogonality approach, which we show is only necessary but not sufficient for ensuring orthogonal convolutions.   Our proposed orthogonal convolution requires no additional parameters and little computational overhead. This method consistently outperforms the kernel orthogonality alternative on a wide range of tasks such as image classification and inpainting under supervised, semi-supervised and unsupervised settings. Further, it learns more diverse and expressive features with better training stability, robustness, and generalization. Our code is publicly available at https://github.com/samaonline/Orthogonal-Convolutional-Neural-Networks.



### Example-Guided Scene Image Synthesis using Masked Spatial-Channel Attention and Patch-Based Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/1911.12362v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12362v1)
- **Published**: 2019-11-27 15:14:24+00:00
- **Updated**: 2019-11-27 15:14:24+00:00
- **Authors**: Haitian Zheng, Haofu Liao, Lele Chen, Wei Xiong, Tianlang Chen, Jiebo Luo
- **Comment**: 13 pages, 14 figures
- **Journal**: None
- **Summary**: Example-guided image synthesis has been recently attempted to synthesize an image from a semantic label map and an exemplary image. In the task, the additional exemplary image serves to provide style guidance that controls the appearance of the synthesized output. Despite the controllability advantage, the previous models are designed on datasets with specific and roughly aligned objects. In this paper, we tackle a more challenging and general task, where the exemplar is an arbitrary scene image that is semantically unaligned to the given label map. To this end, we first propose a new Masked Spatial-Channel Attention (MSCA) module which models the correspondence between two unstructured scenes via cross-attention. Next, we propose an end-to-end network for joint global and local feature alignment and synthesis. In addition, we propose a novel patch-based self-supervision scheme to enable training. Experiments on the large-scale CCOO-stuff dataset show significant improvements over existing methods. Moreover, our approach provides interpretability and can be readily extended to other tasks including style and spatial interpolation or extrapolation, as well as other content manipulation.



### PointRGCN: Graph Convolution Networks for 3D Vehicles Detection Refinement
- **Arxiv ID**: http://arxiv.org/abs/1911.12236v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12236v1)
- **Published**: 2019-11-27 15:51:49+00:00
- **Updated**: 2019-11-27 15:51:49+00:00
- **Authors**: Jesus Zarzar, Silvio Giancola, Bernard Ghanem
- **Comment**: None
- **Journal**: None
- **Summary**: In autonomous driving pipelines, perception modules provide a visual understanding of the surrounding road scene. Among the perception tasks, vehicle detection is of paramount importance for a safe driving as it identifies the position of other agents sharing the road. In our work, we propose PointRGCN: a graph-based 3D object detection pipeline based on graph convolutional networks (GCNs) which operates exclusively on 3D LiDAR point clouds. To perform more accurate 3D object detection, we leverage a graph representation that performs proposal feature and context aggregation. We integrate residual GCNs in a two-stage 3D object detection pipeline, where 3D object proposals are refined using a novel graph representation. In particular, R-GCN is a residual GCN that classifies and regresses 3D proposals, and C-GCN is a contextual GCN that further refines proposals by sharing contextual information between multiple proposals. We integrate our refinement modules into a novel 3D detection pipeline, PointRGCN, and achieve state-of-the-art performance on the easy difficulty for the bird eye view detection task.



### Leveraging Self-supervised Denoising for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.12239v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.12239v3)
- **Published**: 2019-11-27 15:56:27+00:00
- **Updated**: 2020-03-19 09:15:05+00:00
- **Authors**: Mangal Prakash, Tim-Oliver Buchholz, Manan Lalit, Pavel Tomancak, Florian Jug, Alexander Krull
- **Comment**: accepted at ISBI 2020
- **Journal**: None
- **Summary**: Deep learning (DL) has arguably emerged as the method of choice for the detection and segmentation of biological structures in microscopy images. However, DL typically needs copious amounts of annotated training data that is for biomedical projects typically not available and excessively expensive to generate. Additionally, tasks become harder in the presence of noise, requiring even more high-quality training data. Hence, we propose to use denoising networks to improve the performance of other DL-based image segmentation methods. More specifically, we present ideas on how state-of-the-art self-supervised CARE networks can improve cell/nuclei segmentation in microscopy data. Using two state-of-the-art baseline methods, U-Net and StarDist, we show that our ideas consistently improve the quality of resulting segmentations, especially when only limited training data for noisy micrographs are available.



### Literature Review of Action Recognition in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1911.12249v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12249v1)
- **Published**: 2019-11-27 16:14:14+00:00
- **Updated**: 2019-11-27 16:14:14+00:00
- **Authors**: Asket Kaur, Navya Rao, Tanya Joon
- **Comment**: None
- **Journal**: None
- **Summary**: The literature review presented below on Action Recognition in the wild is the in-depth study of Research Papers. Action Recognition problem in the untrimmed videos is a challenging task and most of the papers have tackled this problem using hand-crafted features with shallow learning techniques and sophisticated end-to-end deep learning techniques.



### Your Local GAN: Designing Two Dimensional Local Attention Mechanisms for Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1911.12287v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.12287v2)
- **Published**: 2019-11-27 17:03:16+00:00
- **Updated**: 2019-12-02 18:30:38+00:00
- **Authors**: Giannis Daras, Augustus Odena, Han Zhang, Alexandros G. Dimakis
- **Comment**: Added TFRC, tensorflow-gan acknowledgements. Changed "Ablation Study"
  to "Ablation Studies"
- **Journal**: None
- **Summary**: We introduce a new local sparse attention layer that preserves two-dimensional geometry and locality. We show that by just replacing the dense attention layer of SAGAN with our construction, we obtain very significant FID, Inception score and pure visual improvements. FID score is improved from $18.65$ to $15.94$ on ImageNet, keeping all other parameters the same. The sparse attention patterns that we propose for our new layer are designed using a novel information theoretic criterion that uses information flow graphs. We also present a novel way to invert Generative Adversarial Networks with attention. Our method extracts from the attention layer of the discriminator a saliency map, which we use to construct a new loss function for the inversion. This allows us to visualize the newly introduced attention heads and show that they indeed capture interesting aspects of two-dimensional geometry of real images.



### Fully Unsupervised Probabilistic Noise2Void
- **Arxiv ID**: http://arxiv.org/abs/1911.12291v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1911.12291v2)
- **Published**: 2019-11-27 17:11:59+00:00
- **Updated**: 2020-03-19 08:52:52+00:00
- **Authors**: Mangal Prakash, Manan Lalit, Pavel Tomancak, Alexander Krull, Florian Jug
- **Comment**: Accepted at ISBI 2020
- **Journal**: None
- **Summary**: Image denoising is the first step in many biomedical image analysis pipelines and Deep Learning (DL) based methods are currently best performing. A new category of DL methods such as Noise2Void or Noise2Self can be used fully unsupervised, requiring nothing but the noisy data. However, this comes at the price of reduced reconstruction quality. The recently proposed Probabilistic Noise2Void (PN2V) improves results, but requires an additional noise model for which calibration data needs to be acquired. Here, we present improvements to PN2V that (i) replace histogram based noise models by parametric noise models, and (ii) show how suitable noise models can be created even in the absence of calibration data. This is a major step since it actually renders PN2V fully unsupervised. We demonstrate that all proposed improvements are not only academic but indeed relevant.



### PanDA: Panoptic Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.12317v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12317v2)
- **Published**: 2019-11-27 17:52:00+00:00
- **Updated**: 2020-04-04 01:43:14+00:00
- **Authors**: Yang Liu, Pietro Perona, Markus Meister
- **Comment**: None
- **Journal**: None
- **Summary**: The recently proposed panoptic segmentation task presents a significant challenge of image understanding with computer vision by unifying semantic segmentation and instance segmentation tasks. In this paper we present an efficient and novel panoptic data augmentation (PanDA) method which operates exclusively in pixel space, requires no additional data or training, and is computationally cheap to implement. By retraining original state-of-the-art models on PanDA augmented datasets generated with a single frozen set of parameters, we show robust performance gains in panoptic segmentation, instance segmentation, as well as detection across models, backbones, dataset domains, and scales. Finally, the effectiveness of unrealistic-looking training images synthesized by PanDA suggest that one should rethink the need for image realism for efficient data augmentation.



### Multi-View Matching Network for 6D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1911.12330v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12330v1)
- **Published**: 2019-11-27 18:16:45+00:00
- **Updated**: 2019-11-27 18:16:45+00:00
- **Authors**: Daniel Mas Montserrat, Jianhang Chen, Qian Lin, Jan P. Allebach, Edward J. Delp
- **Comment**: None
- **Journal**: None
- **Summary**: Applications that interact with the real world such as augmented reality or robot manipulation require a good understanding of the location and pose of the surrounding objects. In this paper, we present a new approach to estimate the 6 Degree of Freedom (DoF) or 6D pose of objects from a single RGB image. Our approach can be paired with an object detection and segmentation method to estimate, refine and track the pose of the objects by matching the input image with rendered images.



### Multi-view shape estimation of transparent containers
- **Arxiv ID**: http://arxiv.org/abs/1911.12354v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12354v2)
- **Published**: 2019-11-27 18:55:20+00:00
- **Updated**: 2020-03-09 18:31:37+00:00
- **Authors**: Alessio Xompero, Ricardo Sanchez-Matilla, Apostolos Modas, Pascal Frossard, Andrea Cavallaro
- **Comment**: Accepted to International Conference on Acoustic, Speech, and Signal
  Processing (ICASSP); 5 pages, 7 figures
- **Journal**: None
- **Summary**: The 3D localisation of an object and the estimation of its properties, such as shape and dimensions, are challenging under varying degrees of transparency and lighting conditions. In this paper, we propose a method for jointly localising container-like objects and estimating their dimensions using two wide-baseline, calibrated RGB cameras. Under the assumption of circular symmetry along the vertical axis, we estimate the dimensions of an object with a generative 3D sampling model of sparse circumferences, iterative shape fitting and image re-projection to verify the sampling hypotheses in each camera using semantic segmentation masks. We evaluate the proposed method on a novel dataset of objects with different degrees of transparency and captured under different backgrounds and illumination conditions. Our method, which is based on RGB images only, outperforms in terms of localisation success and dimension estimation accuracy a deep-learning based approach that uses depth maps.



### GLA in MediaEval 2018 Emotional Impact of Movies Task
- **Arxiv ID**: http://arxiv.org/abs/1911.12361v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.12361v1)
- **Published**: 2019-11-27 18:59:55+00:00
- **Updated**: 2019-11-27 18:59:55+00:00
- **Authors**: Jennifer J. Sun, Ting Liu, Gautam Prasad
- **Comment**: MediaEval 2018, 29-31 October 2018, Sophia Antipolis, France. This
  work is presented at the workshop in MediaEval 2018 for the Emotional Impact
  of Movies Task
- **Journal**: None
- **Summary**: The visual and audio information from movies can evoke a variety of emotions in viewers. Towards a better understanding of viewer impact, we present our methods for the MediaEval 2018 Emotional Impact of Movies Task to predict the expected valence and arousal continuously in movies. This task, using the LIRIS-ACCEDE dataset, enables researchers to compare different approaches for predicting viewer impact from movies. Our approach leverages image, audio, and face based features computed using pre-trained neural networks. These features were computed over time and modeled using a gated recurrent unit (GRU) based network followed by a mixture of experts model to compute multiclass predictions. We smoothed these predictions using a Butterworth filter for our final result. Our method enabled us to achieve top performance in three evaluation metrics in the MediaEval 2018 task.



### Multimodal Attention Networks for Low-Level Vision-and-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/1911.12377v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.12377v3)
- **Published**: 2019-11-27 19:00:24+00:00
- **Updated**: 2021-07-30 09:13:11+00:00
- **Authors**: Federico Landi, Lorenzo Baraldi, Marcella Cornia, Massimiliano Corsini, Rita Cucchiara
- **Comment**: Computer Vision and Image Understanding (CVIU)
- **Journal**: None
- **Summary**: Vision-and-Language Navigation (VLN) is a challenging task in which an agent needs to follow a language-specified path to reach a target destination. The goal gets even harder as the actions available to the agent get simpler and move towards low-level, atomic interactions with the environment. This setting takes the name of low-level VLN. In this paper, we strive for the creation of an agent able to tackle three key issues: multi-modality, long-term dependencies, and adaptability towards different locomotive settings. To that end, we devise "Perceive, Transform, and Act" (PTA): a fully-attentive VLN architecture that leaves the recurrent approach behind and the first Transformer-like architecture incorporating three different modalities - natural language, images, and low-level actions for the agent control. In particular, we adopt an early fusion strategy to merge lingual and visual information efficiently in our encoder. We then propose to refine the decoding phase with a late fusion extension between the agent's history of actions and the perceptual modalities. We experimentally validate our model on two datasets: PTA achieves promising results in low-level VLN on R2R and achieves good performance in the recently proposed R4R benchmark. Our code is publicly available at https://github.com/aimagelab/perceive-transform-and-act.



### Detecting total hip replacement prosthesis design on preoperative radiographs using deep convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/1911.12387v1
- **DOI**: 10.1002/jor.24617
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.12387v1)
- **Published**: 2019-11-27 19:22:33+00:00
- **Updated**: 2019-11-27 19:22:33+00:00
- **Authors**: Alireza Borjali, Antonia F. Chen, Orhun K. Muratoglu, Mohammad A. Morid, Kartik M. Varadarajan
- **Comment**: None
- **Journal**: None
- **Summary**: Identifying the design of a failed implant is a key step in preoperative planning of revision total joint arthroplasty. Manual identification of the implant design from radiographic images is time consuming and prone to error. Failure to identify the implant design preoperatively can lead to increased operating room time, more complex surgery, increased blood loss, increased bone loss, increased recovery time, and overall increased healthcare costs. In this study, we present a novel, fully automatic and interpretable approach to identify the design of total hip replacement (THR) implants from plain radiographs using deep convolutional neural network (CNN). CNN achieved 100% accuracy in identification of three commonly used THR implant designs. Such CNN can be used to automatically identify the design of a failed THR implant preoperatively in just a few seconds, saving time and improving the identification accuracy. This can potentially improve patient outcomes, free practitioners time, and reduce healthcare costs.



### PointPWC-Net: A Coarse-to-Fine Network for Supervised and Self-Supervised Scene Flow Estimation on 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1911.12408v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.12408v2)
- **Published**: 2019-11-27 20:29:33+00:00
- **Updated**: 2020-07-31 03:43:04+00:00
- **Authors**: Wenxuan Wu, Zhiyuan Wang, Zhuwen Li, Wei Liu, Li Fuxin
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: We propose a novel end-to-end deep scene flow model, called PointPWC-Net, on 3D point clouds in a coarse-to-fine fashion. Flow computed at the coarse level is upsampled and warped to a finer level, enabling the algorithm to accommodate for large motion without a prohibitive search space. We introduce novel cost volume, upsampling, and warping layers to efficiently handle 3D point cloud data. Unlike traditional cost volumes that require exhaustively computing all the cost values on a high-dimensional grid, our point-based formulation discretizes the cost volume onto input 3D points, and a PointConv operation efficiently computes convolutions on the cost volume. Experiment results on FlyingThings3D outperform the state-of-the-art by a large margin. We further explore novel self-supervised losses to train our model and achieve comparable results to state-of-the-art trained with supervised loss. Without any fine-tuning, our method also shows great generalization ability on KITTI Scene Flow 2015 dataset, outperforming all previous methods.



### PREDICT & CLUSTER: Unsupervised Skeleton Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1911.12409v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.12409v1)
- **Published**: 2019-11-27 20:34:54+00:00
- **Updated**: 2019-11-27 20:34:54+00:00
- **Authors**: Kun Su, Xiulong Liu, Eli Shlizerman
- **Comment**: See video at: https://www.youtube.com/watch?v=-dcCFUBRmwE
- **Journal**: None
- **Summary**: We propose a novel system for unsupervised skeleton-based action recognition. Given inputs of body keypoints sequences obtained during various movements, our system associates the sequences with actions. Our system is based on an encoder-decoder recurrent neural network, where the encoder learns a separable feature representation within its hidden states formed by training the model to perform prediction task. We show that according to such unsupervised training the decoder and the encoder self-organize their hidden states into a feature space which clusters similar movements into the same cluster and distinct movements into distant clusters. Current state-of-the-art methods for action recognition are strongly supervised, i.e., rely on providing labels for training. Unsupervised methods have been proposed, however, they require camera and depth inputs (RGB+D) at each time step. In contrast, our system is fully unsupervised, does not require labels of actions at any stage, and can operate with body keypoints input only. Furthermore, the method can perform on various dimensions of body keypoints (2D or 3D) and include additional cues describing movements. We evaluate our system on three extensive action recognition benchmarks with different number of actions and examples. Our results outperform prior unsupervised skeleton-based methods, unsupervised RGB+D based methods on cross-view tests and while being unsupervised have similar performance to supervised skeleton-based action recognition.



### AdaShare: Learning What To Share For Efficient Deep Multi-Task Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.12423v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.12423v2)
- **Published**: 2019-11-27 21:07:25+00:00
- **Updated**: 2020-11-18 21:32:28+00:00
- **Authors**: Ximeng Sun, Rameswar Panda, Rogerio Feris, Kate Saenko
- **Comment**: Neurips 2020 camera-ready version
- **Journal**: None
- **Summary**: Multi-task learning is an open and challenging problem in computer vision. The typical way of conducting multi-task learning with deep neural networks is either through handcrafted schemes that share all initial layers and branch out at an adhoc point, or through separate task-specific networks with an additional feature sharing/fusion mechanism. Unlike existing methods, we propose an adaptive sharing approach, called AdaShare, that decides what to share across which tasks to achieve the best recognition accuracy, while taking resource efficiency into account. Specifically, our main idea is to learn the sharing pattern through a task-specific policy that selectively chooses which layers to execute for a given task in the multi-task network. We efficiently optimize the task-specific policy jointly with the network weights, using standard back-propagation. Experiments on several challenging and diverse benchmark datasets with a variable number of tasks well demonstrate the efficacy of our approach over state-of-the-art methods. Project page: https://cs-people.bu.edu/sunxm/AdaShare/project.html.



### Learning with less data via Weakly Labeled Patch Classification in Digital Pathology
- **Arxiv ID**: http://arxiv.org/abs/1911.12425v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12425v3)
- **Published**: 2019-11-27 21:19:09+00:00
- **Updated**: 2020-01-22 02:20:39+00:00
- **Authors**: Eu Wern Teh, Graham W. Taylor
- **Comment**: To appear in IEEE International Symposium on Biomedical Imaging
  (ISBI) 2020
- **Journal**: None
- **Summary**: In Digital Pathology (DP), labeled data is generally very scarce due to the requirement that medical experts provide annotations. We address this issue by learning transferable features from weakly labeled data, which are collected from various parts of the body and are organized by non-medical experts. In this paper, we show that features learned from such weakly labeled datasets are indeed transferable and allow us to achieve highly competitive patch classification results on the colorectal cancer (CRC) dataset [1] and the PatchCamelyon (PCam) dataset [2] while using an order of magnitude less labeled data.



### Soft Anchor-Point Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.12448v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12448v2)
- **Published**: 2019-11-27 22:26:20+00:00
- **Updated**: 2020-07-07 00:09:56+00:00
- **Authors**: Chenchen Zhu, Fangyi Chen, Zhiqiang Shen, Marios Savvides
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Recently, anchor-free detection methods have been through great progress. The major two families, anchor-point detection and key-point detection, are at opposite edges of the speed-accuracy trade-off, with anchor-point detectors having the speed advantage. In this work, we boost the performance of the anchor-point detector over the key-point counterparts while maintaining the speed advantage. To achieve this, we formulate the detection problem from the anchor point's perspective and identify ineffective training as the main problem. Our key insight is that anchor points should be optimized jointly as a group both within and across feature pyramid levels. We propose a simple yet effective training strategy with soft-weighted anchor points and soft-selected pyramid levels to address the false attention issue within each pyramid level and the feature selection issue across all the pyramid levels, respectively. To evaluate the effectiveness, we train a single-stage anchor-free detector called Soft Anchor-Point Detector (SAPD). Experiments show that our concise SAPD pushes the envelope of speed/accuracy trade-off to a new level, outperforming recent state-of-the-art anchor-free and anchor-based detectors. Without bells and whistles, our best model can achieve a single-model single-scale AP of 47.4% on COCO.



### Empirical Upper Bound in Object Detection and More
- **Arxiv ID**: http://arxiv.org/abs/1911.12451v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.12451v3)
- **Published**: 2019-11-27 22:40:41+00:00
- **Updated**: 2019-12-17 03:22:13+00:00
- **Authors**: Ali Borji, Seyed Mehdi Iranmanesh
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection remains as one of the most notorious open problems in computer vision. Despite large strides in accuracy in recent years, modern object detectors have started to saturate on popular benchmarks raising the question of how far we can reach with deep learning tools and tricks. Here, by employing 2 state-of-the-art object detection benchmarks, and analyzing more than 15 models over 4 large scale datasets, we I) carefully determine the upperbound in AP, which is 91.6% on VOC (test2007), 78.2% on COCO (val2017), and 58.9% on OpenImages V4 (validation), regardless of the IOU. These numbers are much better than the mAP of the best model1 (47.9% on VOC, and 46.9% on COCO; IOUs=.5:.95), II) characterize the sources of errors in object detectors, in a novel and intuitive way, and find that classification error (confusion with other classes and misses) explains the largest fraction of errors and weighs more than localization and duplicate errors, and III) analyze the invariance properties of models when surrounding context of an object is removed, when an object is placed in an incongruent background, and when images are blurred or flipped vertically. We find that models generate boxes on empty regions and that context is more important for detecting small objects than larger ones. Our work taps into the tight relationship between recognition and detection and offers insights for building better models.



