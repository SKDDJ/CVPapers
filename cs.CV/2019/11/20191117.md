# Arxiv Papers in cs.CV on 2019-11-17
### Extra Proximal-Gradient Inspired Non-local Network
- **Arxiv ID**: http://arxiv.org/abs/1911.07144v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1911.07144v1)
- **Published**: 2019-11-17 03:29:17+00:00
- **Updated**: 2019-11-17 03:29:17+00:00
- **Authors**: Qingchao Zhang, Yunmei Chen
- **Comment**: 12 pages, 7 figures
- **Journal**: None
- **Summary**: Variational method and deep learning method are two mainstream powerful approaches to solve inverse problems in computer vision. To take advantages of advanced optimization algorithms and powerful representation ability of deep neural networks, we propose a novel deep network for image reconstruction. The architecture of this network is inspired by our proposed accelerated extra proximal gradient algorithm. It is able to incorporate non-local operation to exploit the non-local self-similarity of the images and to learn the nonlinear transform, under which the solution is sparse. All the parameters in our network are learned from minimizing a loss function. Our experimental results show that our network outperforms several state-of-the-art deep networks with almost the same number of learnable parameter.



### Unsupervised Domain Adaptation for Object Detection via Cross-Domain Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.07158v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07158v5)
- **Published**: 2019-11-17 05:59:33+00:00
- **Updated**: 2021-08-05 02:08:19+00:00
- **Authors**: Fuxun Yu, Di Wang, Yinpeng Chen, Nikolaos Karianakis, Tong Shen, Pei Yu, Dimitrios Lymberopoulos, Sidi Lu, Weisong Shi, Xiang Chen
- **Comment**: Accepted in WACV'2022
- **Journal**: None
- **Summary**: Current state-of-the-art object detectors can have significant performance drop when deployed in the wild due to domain gaps with training data. Unsupervised Domain Adaptation (UDA) is a promising approach to adapt models for new domains/environments without any expensive label cost. However, without ground truth labels, most prior works on UDA for object detection tasks can only perform coarse image-level and/or feature-level adaptation by using adversarial learning methods. In this work, we show that such adversarial-based methods can only reduce the domain style gap, but cannot address the domain content distribution gap that is shown to be important for object detectors. To overcome this limitation, we propose the Cross-Domain Semi-Supervised Learning (CDSSL) framework by leveraging high-quality pseudo labels to learn better representations from the target domain directly. To enable SSL for cross-domain object detection, we propose fine-grained domain transfer, progressive-confidence-based label sharpening and imbalanced sampling strategy to address two challenges: (i) non-identical distribution between source and target domain data, (ii) error amplification/accumulation due to noisy pseudo labeling on the target domain. Experiment results show that our proposed approach consistently achieves new state-of-the-art performance (2.2% - 9.5% better than prior best work on mAP) under various domain gap scenarios. The code will be released.



### Improve CAM with Auto-adapted Segmentation and Co-supervised Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.07160v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07160v5)
- **Published**: 2019-11-17 06:12:36+00:00
- **Updated**: 2021-01-13 15:15:37+00:00
- **Authors**: Ziyi Kou, Guofeng Cui, Shaojie Wang, Wentian Zhao, Chenliang Xu
- **Comment**: Accepted by WACV2021. Equal contribution for the first two authors
- **Journal**: None
- **Summary**: Weakly Supervised Object Localization (WSOL) methods generate both classification and localization results by learning from only image category labels. Previous methods usually utilize class activation map (CAM) to obtain target object regions. However, most of them only focus on improving foreground object parts in CAM, but ignore the important effect of its background contents. In this paper, we propose a confidence segmentation (ConfSeg) module that builds confidence score for each pixel in CAM without introducing additional hyper-parameters. The generated sample-specific confidence mask is able to indicate the extent of determination for each pixel in CAM, and further supervises additional CAM extended from internal feature maps. Besides, we introduce Co-supervised Augmentation (CoAug) module to capture feature-level representation for foreground and background parts in CAM separately. Then a metric loss is applied at batch sample level to augment distinguish ability of our model, which helps a lot to localize more related object parts. Our final model, CSoA, combines the two modules and achieves superior performance, e.g. $37.69\%$ and $48.81\%$ Top-1 localization error on CUB-200 and ILSVRC datasets, respectively, which outperforms all previous methods and becomes the new state-of-the-art.



### ADCC: An Effective and Intelligent Attention Dense Color Constancy System for Studying Images in Smart Cities
- **Arxiv ID**: http://arxiv.org/abs/1911.07163v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07163v2)
- **Published**: 2019-11-17 06:36:39+00:00
- **Updated**: 2020-10-10 08:21:50+00:00
- **Authors**: Yilang Zhang, Neal N. Xiong, Zheng Wei, Xin Yuan, Jian Wang
- **Comment**: 8 figures and 2 tables
- **Journal**: None
- **Summary**: As a novel method eliminating chromatic aberration on objects, computational color constancy has becoming a fundamental prerequisite for many computer vision applications. Among algorithms performing this task, the learning-based ones have achieved great success in recent years. However, they fail to fully consider the spatial information of images, leaving plenty of room for improvement of the accuracy of illuminant estimation. In this paper, by exploiting the spatial information of images, we propose a color constancy algorithm called Attention Dense Color Constancy (ADCC) using convolutional neural network (CNN). Specifically, based on the 2D log-chrominance histograms of the input images as well as their specially augmented ones, ADCC estimates the illuminant with a self-attention DenseNet. The augmented images help to tell apart the edge gradients, edge pixels and non-edge ones in log-histogram, which contribute significantly to the feature extraction and color-ambiguity elimination, thereby advancing the accuracy of illuminant estimation. Simulations and experiments on benchmark datasets demonstrate that the proposed algorithm is effective for illuminant estimation compared to the state-of-the-art methods. Thus, ADCC offers great potential in promoting applications of smart cities, such as smart camera, where color is an important factor for distinguishing objects.



### Meta-Reinforced Synthetic Data for One-Shot Fine-Grained Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1911.07164v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07164v1)
- **Published**: 2019-11-17 06:37:58+00:00
- **Updated**: 2019-11-17 06:37:58+00:00
- **Authors**: Satoshi Tsutsui, Yanwei Fu, David Crandall
- **Comment**: Accepted by Conference on Neural Information Processing System 2019
- **Journal**: None
- **Summary**: One-shot fine-grained visual recognition often suffers from the problem of training data scarcity for new fine-grained classes. To alleviate this problem, an off-the-shelf image generator can be applied to synthesize additional training images, but these synthesized images are often not helpful for actually improving the accuracy of one-shot fine-grained recognition. This paper proposes a meta-learning framework to combine generated images with original images, so that the resulting ``hybrid'' training images can improve one-shot learning. Specifically, the generic image generator is updated by a few training instances of novel classes, and a Meta Image Reinforcing Network (MetaIRNet) is proposed to conduct one-shot fine-grained recognition as well as image reinforcement. The model is trained in an end-to-end manner, and our experiments demonstrate consistent improvement over baselines on one-shot fine-grained image classification benchmarks.



### LIDIA: Lightweight Learned Image Denoising with Instance Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1911.07167v2
- **DOI**: 10.1109/CVPRW50498.2020.00270
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.07167v2)
- **Published**: 2019-11-17 06:56:47+00:00
- **Updated**: 2020-03-15 20:12:42+00:00
- **Authors**: Gregory Vaksman, Michael Elad, Peyman Milanfar
- **Comment**: None
- **Journal**: None
- **Summary**: Image denoising is a well studied problem with an extensive activity that has spread over several decades. Despite the many available denoising algorithms, the quest for simple, powerful and fast denoisers is still an active and vibrant topic of research. Leading classical denoising methods are typically designed to exploit the inner structure in images by modeling local overlapping patches, while operating in an unsupervised fashion. In contrast, recent newcomers to this arena are supervised and universal neural-network-based methods that bypass this modeling altogether, targeting the inference goal directly and globally, while tending to be very deep and parameter heavy.   This work proposes a novel lightweight learnable architecture for image denoising, and presents a combination of supervised and unsupervised training of it, the first aiming for a universal denoiser and the second for adapting it to the incoming image. Our architecture embeds in it several of the main concepts taken from classical methods, relying on patch processing, leveraging non-local self-similarity, exploiting representation sparsity and providing a multiscale treatment. Our proposed universal denoiser achieves near state-of-the-art results, while using a small fraction of the typical number of parameters. In addition, we introduce and demonstrate two highly effective ways for further boosting the denoising performance, by adapting this universal network to the input image.



### 2nd Place Solution in Google AI Open Images Object Detection Track 2019
- **Arxiv ID**: http://arxiv.org/abs/1911.07171v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07171v1)
- **Published**: 2019-11-17 07:12:47+00:00
- **Updated**: 2019-11-17 07:12:47+00:00
- **Authors**: Ruoyu Guo, Cheng Cui, Yuning Du, Xianglong Meng, Xiaodi Wang, Jingwei Liu, Jianfeng Zhu, Yuan Feng, Shumin Han
- **Comment**: None
- **Journal**: None
- **Summary**: We present an object detection framework based on PaddlePaddle. We put all the strategies together (multi-scale training, FPN, Cascade, Dcnv2, Non-local, libra loss) based on ResNet200-vd backbone. Our model score on public leaderboard comes to 0.6269 with single scale test. We proposed a new voting method called top-k voting-nms, based on the SoftNMS detection results. The voting method helps us merge all the models' results more easily and achieve 2nd place in the Google AI Open Images Object Detection Track 2019.



### Fast Color Constancy with Patch-wise Bright Pixels
- **Arxiv ID**: http://arxiv.org/abs/1911.07177v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07177v1)
- **Published**: 2019-11-17 07:55:42+00:00
- **Updated**: 2019-11-17 07:55:42+00:00
- **Authors**: Yiyao Shi, Jian Wang, Xiangyang Xue
- **Comment**: 7 figures and 4 tables
- **Journal**: None
- **Summary**: In this paper, a learning-free color constancy algorithm called the Patch-wise Bright Pixels (PBP) is proposed. In this algorithm, an input image is first downsampled and then cut equally into a few patches. After that, according to the modified brightness of each patch, a proper fraction of brightest pixels in the patch is selected. Finally, Gray World (GW)-based methods are applied to the selected bright pixels to estimate the illuminant of the scene. Experiments on NUS $8$-Camera Dataset show that the PBP algorithm outperforms the state-of-the-art learning-free methods as well as a broad range of learning-based ones. In particular, PBP processes a $1080$p image within two milliseconds, which is hundreds of times faster than the existing learning-free ones. Our algorithm offers a potential solution to the full-screen smart phones whose screen-to-body ratio is $100$\%.



### Towards the Automation of Deep Image Prior
- **Arxiv ID**: http://arxiv.org/abs/1911.07185v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.07185v1)
- **Published**: 2019-11-17 08:28:42+00:00
- **Updated**: 2019-11-17 08:28:42+00:00
- **Authors**: Qianwei Zhou, Chen Zhou, Haigen Hu, Yuhang Chen, Shengyong Chen, Xiaoxin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Single image inverse problem is a notoriously challenging ill-posed problem that aims to restore the original image from one of its corrupted versions. Recently, this field has been immensely influenced by the emergence of deep-learning techniques. Deep Image Prior (DIP) offers a new approach that forces the recovered image to be synthesized from a given deep architecture. While DIP is quite an effective unsupervised approach, it is deprecated in real-world applications because of the requirement of human assistance. In this work, we aim to find the best-recovered image without the assistance of humans by adding a stopping criterion, which will reach maximum when the iteration no longer improves the image quality. More specifically, we propose to add a pseudo noise to the corrupted image and measure the pseudo-noise component in the recovered image by the orthogonality between signal and noise. The accuracy of the orthogonal stopping criterion has been demonstrated for several tested problems such as denoising, super-resolution, and inpainting, in which 38 out of 40 experiments are higher than 95%.



### Loss Aware Post-training Quantization
- **Arxiv ID**: http://arxiv.org/abs/1911.07190v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.07190v2)
- **Published**: 2019-11-17 09:10:23+00:00
- **Updated**: 2020-03-16 09:23:27+00:00
- **Authors**: Yury Nahshan, Brian Chmiel, Chaim Baskin, Evgenii Zheltonozhskii, Ron Banner, Alex M. Bronstein, Avi Mendelson
- **Comment**: None
- **Journal**: None
- **Summary**: Neural network quantization enables the deployment of large models on resource-constrained devices. Current post-training quantization methods fall short in terms of accuracy for INT4 (or lower) but provide reasonable accuracy for INT8 (or above). In this work, we study the effect of quantization on the structure of the loss landscape. Additionally, we show that the structure is flat and separable for mild quantization, enabling straightforward post-training quantization methods to achieve good results. We show that with more aggressive quantization, the loss landscape becomes highly non-separable with steep curvature, making the selection of quantization parameters more challenging. Armed with this understanding, we design a method that quantizes the layer parameters jointly, enabling significant accuracy improvement over current post-training quantization methods. Reference implementation is available at https://github.com/ynahshan/nn-quantization-pytorch/tree/master/lapq



### Transductive Zero-Shot Hashing for Multilabel Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1911.07192v2
- **DOI**: 10.1109/TNNLS.2020.3043298
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.07192v2)
- **Published**: 2019-11-17 09:21:14+00:00
- **Updated**: 2021-06-30 05:09:10+00:00
- **Authors**: Qin Zou, Zheng Zhang, Ling Cao, Long Chen, Song Wang
- **Comment**: 15 pages
- **Journal**: IEEE Transactions on Neural Networks and Learning Systems, 2020
- **Summary**: Hash coding has been widely used in approximate nearest neighbor search for large-scale image retrieval. Given semantic annotations such as class labels and pairwise similarities of the training data, hashing methods can learn and generate effective and compact binary codes. While some newly introduced images may contain undefined semantic labels, which we call unseen images, zeor-shot hashing techniques have been studied. However, existing zeor-shot hashing methods focus on the retrieval of single-label images, and cannot handle multi-label images. In this paper, for the first time, a novel transductive zero-shot hashing method is proposed for multi-label unseen image retrieval. In order to predict the labels of the unseen/target data, a visual-semantic bridge is built via instance-concept coherence ranking on the seen/source data. Then, pairwise similarity loss and focal quantization loss are constructed for training a hashing model using both the seen/source and unseen/target data. Extensive evaluations on three popular multi-label datasets demonstrate that, the proposed hashing method achieves significantly better results than the competing methods.



### Smoothed Inference for Adversarially-Trained Models
- **Arxiv ID**: http://arxiv.org/abs/1911.07198v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.07198v2)
- **Published**: 2019-11-17 09:38:45+00:00
- **Updated**: 2020-03-16 14:13:03+00:00
- **Authors**: Yaniv Nemcovsky, Evgenii Zheltonozhskii, Chaim Baskin, Brian Chmiel, Maxim Fishman, Alex M. Bronstein, Avi Mendelson
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks are known to be vulnerable to adversarial attacks. Current methods of defense from such attacks are based on either implicit or explicit regularization, e.g., adversarial training. Randomized smoothing, the averaging of the classifier outputs over a random distribution centered in the sample, has been shown to guarantee the performance of a classifier subject to bounded perturbations of the input. In this work, we study the application of randomized smoothing as a way to improve performance on unperturbed data as well as to increase robustness to adversarial attacks. The proposed technique can be applied on top of any existing adversarial defense, but works particularly well with the randomized approaches. We examine its performance on common white-box (PGD) and black-box (transfer and NAttack) attacks on CIFAR-10 and CIFAR-100, substantially outperforming previous art for most scenarios and comparable on others. For example, we achieve 60.4% accuracy under a PGD attack on CIFAR-10 using ResNet-20, outperforming previous art by 11.7%. Since our method is based on sampling, it lends itself well for trading-off between the model inference complexity and its performance. A reference implementation of the proposed techniques is provided at https://github.com/yanemcovsky/SIAM



### Countering Inconsistent Labelling by Google's Vision API for Rotated Images
- **Arxiv ID**: http://arxiv.org/abs/1911.07201v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.07201v1)
- **Published**: 2019-11-17 09:49:58+00:00
- **Updated**: 2019-11-17 09:49:58+00:00
- **Authors**: Aman Apte, Aritra Bandyopadhyay, K Akhilesh Shenoy, Jason Peter Andrews, Aditya Rathod, Manish Agnihotri, Aditya Jajodia
- **Comment**: 11 pages, 9 figures, Accepted at ICICV 2020 Jaipur India
- **Journal**: None
- **Summary**: Google's Vision API analyses images and provides a variety of output predictions, one such type is context-based labelling. In this paper, it is shown that adversarial examples that cause incorrect label prediction and spoofing can be generated by rotating the images. Due to the black-boxed nature of the API, a modular context-based pre-processing pipeline is proposed consisting of a Res-Net50 model, that predicts the angle by which the image must be rotated to correct its orientation. The pipeline successfully performs the correction whilst maintaining the image's resolution and feeds it to the API which generates labels similar to the original correctly oriented image and using a Percentage Error metric, the performance of the corrected images as compared to its rotated counter-parts is found to be significantly higher. These observations imply that the API can benefit from such a pre-processing pipeline to increase robustness to rotational perturbances.



### Real-Time Semantic Segmentation via Multiply Spatial Fusion Network
- **Arxiv ID**: http://arxiv.org/abs/1911.07217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07217v1)
- **Published**: 2019-11-17 12:10:40+00:00
- **Updated**: 2019-11-17 12:10:40+00:00
- **Authors**: Haiyang Si, Zhiqiang Zhang, Feifan Lv, Gang Yu, Feng Lu
- **Comment**: This is an under review version with 9 pages and 4 figures
- **Journal**: None
- **Summary**: Real-time semantic segmentation plays a significant role in industry applications, such as autonomous driving, robotics and so on. It is a challenging task as both efficiency and performance need to be considered simultaneously. To address such a complex task, this paper proposes an efficient CNN called Multiply Spatial Fusion Network (MSFNet) to achieve fast and accurate perception. The proposed MSFNet uses Class Boundary Supervision to process the relevant boundary information based on our proposed Multi-features Fusion Module which can obtain spatial information and enlarge receptive field. Therefore, the final upsampling of the feature maps of 1/8 original image size can achieve impressive results while maintaining a high speed. Experiments on Cityscapes and Camvid datasets show an obvious advantage of the proposed approach compared with the existing approaches. Specifically, it achieves 77.1% Mean IOU on the Cityscapes test dataset with the speed of 41 FPS for a 1024*2048 input, and 75.4% Mean IOU with the speed of 91 FPS on the Camvid test dataset.



### Enhancing Generic Segmentation with Learned Region Representations
- **Arxiv ID**: http://arxiv.org/abs/1911.08564v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.08564v2)
- **Published**: 2019-11-17 13:31:10+00:00
- **Updated**: 2020-03-24 13:46:23+00:00
- **Authors**: Or Isaacs, Oran Shayer, Michael Lindenbaum
- **Comment**: CVPR 2020. arXiv admin note: substantial text overlap with
  arXiv:1909.11735
- **Journal**: None
- **Summary**: Current successful approaches for generic (non-semantic) segmentation rely mostly on edge detection and have leveraged the strengths of deep learning mainly by improving the edge detection stage in the algorithmic pipeline. This is in contrast to semantic and instance segmentation, where DNNs are applied directly to generate pixel-wise segment representations. We propose a new method for learning a pixel-wise representation that reflects segment relatedness. This representation is combined with an edge map to yield a new segmentation algorithm. We show that the representations themselves achieve state-of-the-art segment similarity scores. Moreover, the proposed combined segmentation algorithm provides results that are either state of the art or improve upon it, for most quality measures.



### SiamCAR: Siamese Fully Convolutional Classification and Regression for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1911.07241v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07241v2)
- **Published**: 2019-11-17 14:03:12+00:00
- **Updated**: 2019-12-13 03:37:39+00:00
- **Authors**: Dongyan Guo, Jun Wang, Ying Cui, Zhenhua Wang, Shengyong Chen
- **Comment**: None
- **Journal**: None
- **Summary**: By decomposing the visual tracking task into two subproblems as classification for pixel category and regression for object bounding box at this pixel, we propose a novel fully convolutional Siamese network to solve visual tracking end-to-end in a per-pixel manner. The proposed framework SiamCAR consists of two simple subnetworks: one Siamese subnetwork for feature extraction and one classification-regression subnetwork for bounding box prediction. Our framework takes ResNet-50 as backbone. Different from state-of-the-art trackers like Siamese-RPN, SiamRPN++ and SPM, which are based on region proposal, the proposed framework is both proposal and anchor free. Consequently, we are able to avoid the tricky hyper-parameter tuning of anchors and reduce human intervention. The proposed framework is simple, neat and effective. Extensive experiments and comparisons with state-of-the-art trackers are conducted on many challenging benchmarks like GOT-10K, LaSOT, UAV123 and OTB-50. Without bells and whistles, our SiamCAR achieves the leading performance with a considerable real-time speed.



### IKEA Furniture Assembly Environment for Long-Horizon Complex Manipulation Tasks
- **Arxiv ID**: http://arxiv.org/abs/1911.07246v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.07246v1)
- **Published**: 2019-11-17 14:32:20+00:00
- **Updated**: 2019-11-17 14:32:20+00:00
- **Authors**: Youngwoon Lee, Edward S. Hu, Zhengyu Yang, Alex Yin, Joseph J. Lim
- **Comment**: Simulator
- **Journal**: None
- **Summary**: The IKEA Furniture Assembly Environment is one of the first benchmarks for testing and accelerating the automation of complex manipulation tasks. The environment is designed to advance reinforcement learning from simple toy tasks to complex tasks requiring both long-term planning and sophisticated low-level control. Our environment supports over 80 different furniture models, Sawyer and Baxter robot simulation, and domain randomization. The IKEA Furniture Assembly Environment is a testbed for methods aiming to solve complex manipulation tasks. The environment is publicly available at https://clvrai.com/furniture



### DualVD: An Adaptive Dual Encoding Model for Deep Visual Understanding in Visual Dialogue
- **Arxiv ID**: http://arxiv.org/abs/1911.07251v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1911.07251v1)
- **Published**: 2019-11-17 14:58:17+00:00
- **Updated**: 2019-11-17 14:58:17+00:00
- **Authors**: Xiaoze Jiang, Jing Yu, Zengchang Qin, Yingying Zhuang, Xingxing Zhang, Yue Hu, Qi Wu
- **Comment**: Accepted by the Thirty-Fourth AAAI Conference on Artificial
  Intelligence (AAAI-2020)
- **Journal**: None
- **Summary**: Different from Visual Question Answering task that requires to answer only one question about an image, Visual Dialogue involves multiple questions which cover a broad range of visual content that could be related to any objects, relationships or semantics. The key challenge in Visual Dialogue task is thus to learn a more comprehensive and semantic-rich image representation which may have adaptive attentions on the image for variant questions. In this research, we propose a novel model to depict an image from both visual and semantic perspectives. Specifically, the visual view helps capture the appearance-level information, including objects and their relationships, while the semantic view enables the agent to understand high-level visual semantics from the whole image to the local regions. Futhermore, on top of such multi-view image features, we propose a feature selection framework which is able to adaptively capture question-relevant information hierarchically in fine-grained level. The proposed method achieved state-of-the-art results on benchmark Visual Dialogue datasets. More importantly, we can tell which modality (visual or semantic) has more contribution in answering the current question by visualizing the gate values. It gives us insights in understanding of human cognition in Visual Dialogue.



### Spectral Geometric Matrix Completion
- **Arxiv ID**: http://arxiv.org/abs/1911.07255v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CG, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.07255v3)
- **Published**: 2019-11-17 15:06:34+00:00
- **Updated**: 2021-06-07 14:27:53+00:00
- **Authors**: Amit Boyarski, Sanketh Vedula, Alex Bronstein
- **Comment**: Accepted to Mathematical and Scientific Machine Learning (MSML) 2021
  https://msml21.github.io/
- **Journal**: None
- **Summary**: Deep Matrix Factorization (DMF) is an emerging approach to the problem of matrix completion. Recent works have established that gradient descent applied to a DMF model induces an implicit regularization on the rank of the recovered matrix. In this work we interpret the DMF model through the lens of spectral geometry. This allows us to incorporate explicit regularization without breaking the DMF structure, thus enjoying the best of both worlds. In particular, we focus on matrix completion problems with underlying geometric or topological relations between the rows and/or columns. Such relations are prevalent in matrix completion problems that arise in many applications, such as recommender systems and drug-target interaction. Our contributions enable DMF models to exploit these relations, and make them competitive on real benchmarks, while exhibiting one of the first successful applications of deep linear networks.



### Learning with Hierarchical Complement Objective
- **Arxiv ID**: http://arxiv.org/abs/1911.07257v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.07257v1)
- **Published**: 2019-11-17 15:46:38+00:00
- **Updated**: 2019-11-17 15:46:38+00:00
- **Authors**: Hao-Yun Chen, Li-Huang Tsai, Shih-Chieh Chang, Jia-Yu Pan, Yu-Ting Chen, Wei Wei, Da-Cheng Juan
- **Comment**: None
- **Journal**: None
- **Summary**: Label hierarchies widely exist in many vision-related problems, ranging from explicit label hierarchies existed in image classification to latent label hierarchies existed in semantic segmentation. Nevertheless, state-of-the-art methods often deploy cross-entropy loss that implicitly assumes class labels to be exclusive and thus independence from each other. Motivated by the fact that classes from the same parental category usually share certain similarity, we design a new training diagram called Hierarchical Complement Objective Training (HCOT) that leverages the information from label hierarchy. HCOT maximizes the probability of the ground truth class, and at the same time, neutralizes the probabilities of rest of the classes in a hierarchical fashion, making the model take advantage of the label hierarchy explicitly. The proposed HCOT is evaluated on both image classification and semantic segmentation tasks. Experimental results confirm that HCOT outperforms state-of-the-art models in CIFAR-100, ImageNet-2012, and PASCAL-Context. The study further demonstrates that HCOT can be applied on tasks with latent label hierarchies, which is a common characteristic in many machine learning tasks.



### Leveraging Multi-view Image Sets for Unsupervised Intrinsic Image Decomposition and Highlight Separation
- **Arxiv ID**: http://arxiv.org/abs/1911.07262v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07262v1)
- **Published**: 2019-11-17 15:57:41+00:00
- **Updated**: 2019-11-17 15:57:41+00:00
- **Authors**: Renjiao Yi, Ping Tan, Stephen Lin
- **Comment**: 27 pages, with supplementary material, to appear in AAAI 2020
- **Journal**: None
- **Summary**: We present an unsupervised approach for factorizing object appearance into highlight, shading, and albedo layers, trained by multi-view real images. To do so, we construct a multi-view dataset by collecting numerous customer product photos online, which exhibit large illumination variations that make them suitable for training of reflectance separation and can facilitate object-level decomposition. The main contribution of our approach is a proposed image representation based on local color distributions that allows training to be insensitive to the local misalignments of multi-view images. In addition, we present a new guidance cue for unsupervised training that exploits synergy between highlight separation and intrinsic image decomposition. Over a broad range of objects, our technique is shown to yield state-of-the-art results for both of these tasks.



### On the well-posedness of uncalibrated photometric stereo under general lighting
- **Arxiv ID**: http://arxiv.org/abs/1911.07268v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07268v2)
- **Published**: 2019-11-17 16:10:42+00:00
- **Updated**: 2020-09-17 00:11:37+00:00
- **Authors**: Mohammed Brahimi, Yvain Quéau, Bjoern Haefner, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: Uncalibrated photometric stereo aims at estimating the 3D-shape of a surface, given a set of images captured from the same viewing angle, but under unknown, varying illumination. While the theoretical foundations of this inverse problem under directional lighting are well-established, there is a lack of mathematical evidence for the uniqueness of a solution under general lighting. On the other hand, stable and accurate heuristical solutions of uncalibrated photometric stereo under such general lighting have recently been proposed. The quality of the results demonstrated therein tends to indicate that the problem may actually be well-posed, but this still has to be established. The present paper addresses this theoretical issue, considering first-order spherical harmonics approximation of general lighting. Two important theoretical results are established. First, the orthographic integrability constraint ensures uniqueness of a solution up to a global concave-convex ambiguity, which had already been conjectured, yet not proven. Second, the perspective integrability constraint makes the problem well-posed, which generalizes a previous result limited to directional lighting. Eventually, a closed-form expression for the unique least-squares solution of the problem under perspective projection is provided, allowing numerical simulations on synthetic data to empirically validate our findings.



### Unsupervised Visual Representation Learning with Increasing Object Shape Bias
- **Arxiv ID**: http://arxiv.org/abs/1911.07272v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07272v2)
- **Published**: 2019-11-17 16:26:46+00:00
- **Updated**: 2019-11-24 02:48:05+00:00
- **Authors**: Zhibo Wang, Shen Yan, Xiaoyu Zhang, Niels Lobo
- **Comment**: None
- **Journal**: None
- **Summary**: (Very early draft)Traditional supervised learning keeps pushing convolution neural network(CNN) achieving state-of-art performance. However, lack of large-scale annotation data is always a big problem due to the high cost of it, even ImageNet dataset is over-fitted by complex models now. The success of unsupervised learning method represented by the Bert model in natural language processing(NLP) field shows its great potential. And it makes that unlimited training samples becomes possible and the great universal generalization ability changes NLP research direction directly. In this article, we purpose a novel unsupervised learning method based on contrastive predictive coding. Under that, we are able to train model with any non-annotation images and improve model's performance to reach state-of-art performance at the same level of model complexity. Beside that, since the number of training images could be unlimited amplification, an universal large-scale pre-trained computer vision model is possible in the future.



### Distribution Context Aware Loss for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1911.07273v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07273v1)
- **Published**: 2019-11-17 16:28:35+00:00
- **Updated**: 2019-11-17 16:28:35+00:00
- **Authors**: Zhigang Chang, Qin Zhou, Mingyang Yu, Shibao Zheng, Hua Yang, Tai-Pang Wu
- **Comment**: IEEE VCIP
- **Journal**: None
- **Summary**: To learn the optimal similarity function between probe and gallery images in Person re-identification, effective deep metric learning methods have been extensively explored to obtain discriminative feature embedding. However, existing metric loss like triplet loss and its variants always emphasize pair-wise relations but ignore the distribution context in feature space, leading to inconsistency and sub-optimal. In fact, the similarity of one pair not only decides the match of this pair, but also has potential impacts on other sample pairs. In this paper, we propose a novel Distribution Context Aware (DCA) loss based on triplet loss to combine both numerical similarity and relation similarity in feature space for better clustering. Extensive experiments on three benchmarks including Market-1501, DukeMTMC-reID and MSMT17, evidence the favorable performance of our method against the corresponding baseline and other state-of-the-art methods.



### Detecting F-formations & Roles in Crowded Social Scenes with Wearables: Combining Proxemics & Dynamics using LSTMs
- **Arxiv ID**: http://arxiv.org/abs/1911.07279v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1911.07279v1)
- **Published**: 2019-11-17 16:43:44+00:00
- **Updated**: 2019-11-17 16:43:44+00:00
- **Authors**: Alessio Rosatelli, Ekin Gedik, Hayley Hung
- **Comment**: 2019 8th International Conference on Affective Computing and
  Intelligent Interaction Workshops and Demos (ACIIW)
- **Journal**: None
- **Summary**: In this paper, we investigate the use of proxemics and dynamics for automatically identifying conversing groups, or so-called F-formations. More formally we aim to automatically identify whether wearable sensor data coming from 2 people is indicative of F-formation membership. We also explore the problem of jointly detecting membership and more descriptive information about the pair relating to the role they take in the conversation (i.e. speaker or listener). We jointly model the concepts of proxemics and dynamics using binary proximity and acceleration obtained through a single wearable sensor per person. We test our approaches on the publicly available MatchNMingle dataset which was collected during real-life mingling events. We find out that fusion of these two modalities performs significantly better than them independently, providing an AUC of 0.975 when data from 30-second windows are used. Furthermore, our investigation into roles detection shows that each role pair requires a different time resolution for accurate detection.



### Counterfactual Vision-and-Language Navigation via Adversarial Path Sampling
- **Arxiv ID**: http://arxiv.org/abs/1911.07308v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07308v3)
- **Published**: 2019-11-17 18:02:51+00:00
- **Updated**: 2020-07-17 00:18:45+00:00
- **Authors**: Tsu-Jui Fu, Xin Eric Wang, Matthew Peterson, Scott Grafton, Miguel Eckstein, William Yang Wang
- **Comment**: ECCV 2020 (spotlight)
- **Journal**: None
- **Summary**: Vision-and-Language Navigation (VLN) is a task where agents must decide how to move through a 3D environment to reach a goal by grounding natural language instructions to the visual surroundings. One of the problems of the VLN task is data scarcity since it is difficult to collect enough navigation paths with human-annotated instructions for interactive environments. In this paper, we explore the use of counterfactual thinking as a human-inspired data augmentation method that results in robust models. Counterfactual thinking is a concept that describes the human propensity to create possible alternatives to life events that have already occurred. We propose an adversarial-driven counterfactual reasoning model that can consider effective conditions instead of low-quality augmented data. In particular, we present a model-agnostic adversarial path sampler (APS) that learns to sample challenging paths that force the navigator to improve based on the navigation performance. APS also serves to do pre-exploration of unseen environments to strengthen the model's ability to generalize. We evaluate the influence of APS on the performance of different VLN baseline models using the room-to-room dataset (R2R). The results show that the adversarial training process with our proposed APS benefits VLN models under both seen and unseen environments. And the pre-exploration process can further gain additional improvements under unseen environments.



### ELoPE: Fine-Grained Visual Classification with Efficient Localization, Pooling and Embedding
- **Arxiv ID**: http://arxiv.org/abs/1911.07344v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07344v1)
- **Published**: 2019-11-17 21:30:30+00:00
- **Updated**: 2019-11-17 21:30:30+00:00
- **Authors**: Harald Hanselmann, Hermann Ney
- **Comment**: None
- **Journal**: None
- **Summary**: The task of fine-grained visual classification (FGVC) deals with classification problems that display a small inter-class variance such as distinguishing between different bird species or car models. State-of-the-art approaches typically tackle this problem by integrating an elaborate attention mechanism or (part-) localization method into a standard convolutional neural network (CNN). Also in this work the aim is to enhance the performance of a backbone CNN such as ResNet by including three efficient and lightweight components specifically designed for FGVC. This is achieved by using global k-max pooling, a discriminative embedding layer trained by optimizing class means and an efficient bounding box estimator that only needs class labels for training. The resulting model achieves new best state-of-the-art recognition accuracies on the Stanford cars and FGVC-Aircraft datasets.



### Any-Precision Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.07346v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.07346v2)
- **Published**: 2019-11-17 21:35:32+00:00
- **Updated**: 2021-01-15 08:13:10+00:00
- **Authors**: Haichao Yu, Haoxiang Li, Honghui Shi, Thomas S. Huang, Gang Hua
- **Comment**: AAAI 2021
- **Journal**: None
- **Summary**: We present any-precision deep neural networks (DNNs), which are trained with a new method that allows the learned DNNs to be flexible in numerical precision during inference. The same model in runtime can be flexibly and directly set to different bit-widths, by truncating the least significant bits, to support dynamic speed and accuracy trade-off. When all layers are set to low-bits, we show that the model achieved accuracy comparable to dedicated models trained at the same precision. This nice property facilitates flexible deployment of deep learning models in real-world applications, where in practice trade-offs between model accuracy and runtime efficiency are often sought. Previous literature presents solutions to train models at each individual fixed efficiency/accuracy trade-off point. But how to produce a model flexible in runtime precision is largely unexplored. When the demand of efficiency/accuracy trade-off varies from time to time or even dynamically changes in runtime, it is infeasible to re-train models accordingly, and the storage budget may forbid keeping multiple models. Our proposed framework achieves this flexibility without performance degradation. More importantly, we demonstrate that this achievement is agnostic to model architectures and applicable to multiple vision tasks. Our code is released at https://github.com/SHI-Labs/Any-Precision-DNNs.



### Fast 3D Pose Refinement with RGB Images
- **Arxiv ID**: http://arxiv.org/abs/1911.07347v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07347v1)
- **Published**: 2019-11-17 21:40:05+00:00
- **Updated**: 2019-11-17 21:40:05+00:00
- **Authors**: Abhinav Jain, Frank Dellaert
- **Comment**: None
- **Journal**: None
- **Summary**: Pose estimation is a vital step in many robotics and perception tasks such as robotic manipulation, autonomous vehicle navigation, etc. Current state-of-the-art pose estimation methods rely on deep neural networks with complicated structures and long inference times. While highly robust, they require computing power often unavailable on mobile robots. We propose a CNN-based pose refinement system which takes a coarsely estimated 3D pose from a computationally cheaper algorithm along with a bounding box image of the object, and returns a highly refined pose. Our experiments on the YCB-Video dataset show that our system can refine 3D poses to an extremely high precision with minimal training data.



### Putting visual object recognition in context
- **Arxiv ID**: http://arxiv.org/abs/1911.07349v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.07349v3)
- **Published**: 2019-11-17 21:43:00+00:00
- **Updated**: 2020-03-25 22:46:13+00:00
- **Authors**: Mengmi Zhang, Claire Tseng, Gabriel Kreiman
- **Comment**: 8 pages, CVPR2020
- **Journal**: None
- **Summary**: Context plays an important role in visual recognition. Recent studies have shown that visual recognition networks can be fooled by placing objects in inconsistent contexts (e.g., a cow in the ocean). To model the role of contextual information in visual recognition, we systematically investigated ten critical properties of where, when, and how context modulates recognition, including the amount of context, context and object resolution, geometrical structure of context, context congruence, and temporal dynamics of contextual modulation. The tasks involved recognizing a target object surrounded with context in a natural image. As an essential benchmark, we conducted a series of psychophysics experiments where we altered one aspect of context at a time, and quantified recognition accuracy. We propose a biologically-inspired context-aware object recognition model consisting of a two-stream architecture. The model processes visual information at the fovea and periphery in parallel, dynamically incorporates object and contextual information, and sequentially reasons about the class label for the target object. Across a wide range of behavioral tasks, the model approximates human level performance without retraining for each task, captures the dependence of context enhancement on image properties, and provides initial steps towards integrating scene and object information for visual recognition. All source code and data are publicly available: https://github.com/kreimanlab/Put-In-Context.



### Exploiting Human Social Cognition for the Detection of Fake and Fraudulent Faces via Memory Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.07844v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.07844v1)
- **Published**: 2019-11-17 23:20:23+00:00
- **Updated**: 2019-11-17 23:20:23+00:00
- **Authors**: Tharindu Fernando, Clinton Fookes, Simon Denman, Sridha Sridharan
- **Comment**: None
- **Journal**: None
- **Summary**: Advances in computer vision have brought us to the point where we have the ability to synthesise realistic fake content. Such approaches are seen as a source of disinformation and mistrust, and pose serious concerns to governments around the world. Convolutional Neural Networks (CNNs) demonstrate encouraging results when detecting fake images that arise from the specific type of manipulation they are trained on. However, this success has not transitioned to unseen manipulation types, resulting in a significant gap in the line-of-defense. We propose a Hierarchical Memory Network (HMN) architecture, which is able to successfully detect faked faces by utilising knowledge stored in neural memories as well as visual cues to reason about the perceived face and anticipate its future semantic embeddings. This renders a generalisable face tampering detection framework. Experimental results demonstrate the proposed approach achieves superior performance for fake and fraudulent face detection compared to the state-of-the-art.



