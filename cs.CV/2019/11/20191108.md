# Arxiv Papers in cs.CV on 2019-11-08
### Collapse Resistant Deep Convolutional GAN for Multi-Object Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1911.02996v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.02996v1)
- **Published**: 2019-11-08 02:27:23+00:00
- **Updated**: 2019-11-08 02:27:23+00:00
- **Authors**: Elijah D. Bolluyt, Cristina Comaniciu
- **Comment**: Accepted to IEEE International Conference on Machine Learning and
  Applications 2019
- **Journal**: None
- **Summary**: This work introduces a novel system for the generation of images that contain multiple classes of objects. Recent work in Generative Adversarial Networks have produced high quality images, but many focus on generating images of a single object or set of objects. Our system addresses the task of image generation conditioned on a list of desired classes to be included in a single image. This enables our system to generate images with any given combination of objects, all composed into a visually realistic natural image. The system learns the interrelationships of all classes represented in a dataset, and can generate diverse samples including a set of these classes. It displays the ability to arrange these objects together, accounting for occlusions and inter-object spatial relations that characterize complex natural images. To accomplish this, we introduce a novel architecture based on Conditional Deep Convolutional GANs that is stabilized against collapse relative to both mode and condition. The system learns to rectify mode collapse during training, self-correcting to avoid suboptimal generation modes.



### Transfer Learning in 4D for Breast Cancer Diagnosis using Dynamic Contrast-Enhanced Magnetic Resonance Imaging
- **Arxiv ID**: http://arxiv.org/abs/1911.03022v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.03022v1)
- **Published**: 2019-11-08 03:45:24+00:00
- **Updated**: 2019-11-08 03:45:24+00:00
- **Authors**: Qiyuan Hu, Heather M. Whitney, Maryellen L. Giger
- **Comment**: Machine Learning for Health (ML4H) at NeurIPS 2019 - Extended
  Abstract
- **Journal**: None
- **Summary**: Deep transfer learning using dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) has shown strong predictive power in characterization of breast lesions. However, pretrained convolutional neural networks (CNNs) require 2D inputs, limiting the ability to exploit the rich 4D (volumetric and temporal) image information inherent in DCE-MRI that is clinically valuable for lesion assessment. Training 3D CNNs from scratch, a common method to utilize high-dimensional information in medical images, is computationally expensive and is not best suited for moderately sized healthcare datasets. Therefore, we propose a novel approach using transfer learning that incorporates the 4D information from DCE-MRI, where volumetric information is collapsed at feature level by max pooling along the projection perpendicular to the transverse slices and the temporal information is contained either in second-post contrast subtraction images. Our methodology yielded an area under the receiver operating characteristic curve of 0.89+/-0.01 on a dataset of 1161 breast lesions, significantly outperforming a previous approach that incorporates the 4D information in DCE-MRI by the use of maximum intensity projection (MIP) images.



### RoIMix: Proposal-Fusion among Multiple Images for Underwater Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.03029v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.03029v2)
- **Published**: 2019-11-08 03:56:22+00:00
- **Updated**: 2020-03-24 14:02:31+00:00
- **Authors**: Wei-Hong Lin, Jia-Xing Zhong, Shan Liu, Thomas Li, Ge Li
- **Comment**: ICASSP 2020
- **Journal**: None
- **Summary**: Generic object detection algorithms have proven their excellent performance in recent years. However, object detection on underwater datasets is still less explored. In contrast to generic datasets, underwater images usually have color shift and low contrast; sediment would cause blurring in underwater images. In addition, underwater creatures often appear closely to each other on images due to their living habits. To address these issues, our work investigates augmentation policies to simulate overlapping, occluded and blurred objects, and we construct a model capable of achieving better generalization. We propose an augmentation method called RoIMix, which characterizes interactions among images. Proposals extracted from different images are mixed together. Previous data augmentation methods operate on a single image while we apply RoIMix to multiple images to create enhanced samples as training data. Experiments show that our proposed method improves the performance of region-based object detectors on both Pascal VOC and URPC datasets.



### A Novel Approach for Partial Fingerprint Identification to Mitigate MasterPrint Generation
- **Arxiv ID**: http://arxiv.org/abs/1911.03052v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.03052v1)
- **Published**: 2019-11-08 05:07:25+00:00
- **Updated**: 2019-11-08 05:07:25+00:00
- **Authors**: Mahesh Joshi, Bodhisatwa Mazumdar, Somnath Dey
- **Comment**: None
- **Journal**: None
- **Summary**: Partial fingerprint recognition is a method to recognize an individual when the sensor size has a small form factor in accepting a full fingerprint. It is also used in forensic research to identify the partial fingerprints collected from the crime scenes. But the distinguishing features in the partial fingerprint are relatively low due to small fingerprint captured by the sensor. Hence, the uniqueness of a partial fingerprint cannot be guaranteed, leading to a possibility that a single partial fingerprint may identify multiple subjects. A MasterPrint is a partial fingerprint that identifies at least 4% different individuals from the enrolled template database. A fingerprint identification system with such a flaw can play a significant role in convicting an innocent in a criminal case. We propose a partial fingerprint identification approach that aims to mitigate MasterPrint generation. The proposed method, when applied to partial fingerprint dataset cropped from standard FVC 2002 DB1(A) dataset showed significant improvement in reducing the count of MasterPrints. The experimental result demonstrates improved results on other parameters, such as True match Rate (TMR) and Equal Error Rate (EER), generally used to evaluate the performance of a fingerprint biometric system.



### Are we asking the right questions in MovieQA?
- **Arxiv ID**: http://arxiv.org/abs/1911.03083v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1911.03083v1)
- **Published**: 2019-11-08 06:49:45+00:00
- **Updated**: 2019-11-08 06:49:45+00:00
- **Authors**: Bhavan Jasani, Rohit Girdhar, Deva Ramanan
- **Comment**: Spotlight presentation at CLVL workshop, ICCV 2019. Project page:
  https://bhavanj.github.io/MovieQAWithoutMovies/
- **Journal**: None
- **Summary**: Joint vision and language tasks like visual question answering are fascinating because they explore high-level understanding, but at the same time, can be more prone to language biases. In this paper, we explore the biases in the MovieQA dataset and propose a strikingly simple model which can exploit them. We find that using the right word embedding is of utmost importance. By using an appropriately trained word embedding, about half the Question-Answers (QAs) can be answered by looking at the questions and answers alone, completely ignoring narrative context from video clips, subtitles, and movie scripts. Compared to the best published papers on the leaderboard, our simple question + answer only model improves accuracy by 5% for video + subtitle category, 5% for subtitle, 15% for DVS and 6% higher for scripts.



### Stacked dense optical flows and dropout layers to predict sperm motility and morphology
- **Arxiv ID**: http://arxiv.org/abs/1911.03086v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.03086v1)
- **Published**: 2019-11-08 06:59:35+00:00
- **Updated**: 2019-11-08 06:59:35+00:00
- **Authors**: Vajira Thambawita, PÃ¥l Halvorsen, Hugo Hammer, Michael Riegler, Trine B. Haugen
- **Comment**: 3 pages, 2 figures, MediaEval 19, 27-29 October 2019, Sophia
  Antipolis, France
- **Journal**: None
- **Summary**: In this paper, we analyse two deep learning methods to predict sperm motility and sperm morphology from sperm videos. We use two different inputs: stacked pure frames of videos and dense optical flows of video frames. To solve this regression task of predicting motility and morphology, stacked dense optical flows and extracted original frames from sperm videos were used with the modified state of the art convolution neural networks. For modifications of the selected models, we have introduced an additional multi-layer perceptron to overcome the problem of over-fitting. The method which had an additional multi-layer perceptron with dropout layers, shows the best results when the inputs consist of both dense optical flows and an original frame of videos.



### Patch augmentation: Towards efficient decision boundaries for neural networks
- **Arxiv ID**: http://arxiv.org/abs/1911.07922v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.07922v2)
- **Published**: 2019-11-08 07:11:31+00:00
- **Updated**: 2019-11-25 14:05:08+00:00
- **Authors**: Marcus D. Bloice, Peter M. Roth, Andreas Holzinger
- **Comment**: Version 2: updated author list, reduced abstract length, plots
  consolidated as sub-plots
- **Journal**: None
- **Summary**: In this paper we propose a new augmentation technique, called patch augmentation, that, in our experiments, improves model accuracy and makes networks more robust to adversarial attacks. In brief, this data-independent approach creates new image data based on image/label pairs, where a patch from one of the two images in the pair is superimposed on to the other image, creating a new augmented sample. The new image's label is a linear combination of the image pair's corresponding labels. Initial experiments show a several percentage point increase in accuracy on CIFAR-10, from a baseline of approximately 81% to 89%. CIFAR-100 sees larger improvements still, from a baseline of 52% to 68% accuracy. Networks trained using patch augmentation are also more robust to adversarial attacks, which we demonstrate using the Fast Gradient Sign Method.



### Extracting temporal features into a spatial domain using autoencoders for sperm video analysis
- **Arxiv ID**: http://arxiv.org/abs/1911.03100v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.03100v1)
- **Published**: 2019-11-08 07:29:03+00:00
- **Updated**: 2019-11-08 07:29:03+00:00
- **Authors**: Vajira Thambawita, PÃ¥l Halvorsen, Hugo Hammer, Michael Riegler, Trine B. Haugen
- **Comment**: 3 pages, 1 figure, MediaEval 19, 27-29 October 2019, Sophia
  Antipolis, France
- **Journal**: None
- **Summary**: In this paper, we present a two-step deep learning method that is used to predict sperm motility and morphology-based on video recordings of human spermatozoa. First, we use an autoencoder to extract temporal features from a given semen video and plot these into image-space, which we call feature-images. Second, these feature-images are used to perform transfer learning to predict the motility and morphology values of human sperm. The presented method shows it's capability to extract temporal information into spatial domain feature-images which can be used with traditional convolutional neural networks. Furthermore, the accuracy of the predicted motility of a given semen sample shows that a deep learning-based model can capture the temporal information of microscopic recordings of human semen.



### Accurate Vision-based Manipulation through Contact Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1911.03112v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.03112v2)
- **Published**: 2019-11-08 08:05:07+00:00
- **Updated**: 2020-04-17 15:08:59+00:00
- **Authors**: Alina Kloss, Maria Bauza, Jiajun Wu, Joshua B. Tenenbaum, Alberto Rodriguez, Jeannette Bohg
- **Comment**: accepted at ICRA 2020
- **Journal**: None
- **Summary**: Planning contact interactions is one of the core challenges of many robotic tasks. Optimizing contact locations while taking dynamics into account is computationally costly and, in environments that are only partially observable, executing contact-based tasks often suffers from low accuracy. We present an approach that addresses these two challenges for the problem of vision-based manipulation. First, we propose to disentangle contact from motion optimization. Thereby, we improve planning efficiency by focusing computation on promising contact locations. Second, we use a hybrid approach for perception and state estimation that combines neural networks with a physically meaningful state representation. In simulation and real-world experiments on the task of planar pushing, we show that our method is more efficient and achieves a higher manipulation accuracy than previous vision-based approaches.



### AIM 2019 Challenge on Image Demoireing: Methods and Results
- **Arxiv ID**: http://arxiv.org/abs/1911.03461v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.03461v1)
- **Published**: 2019-11-08 08:10:12+00:00
- **Updated**: 2019-11-08 08:10:12+00:00
- **Authors**: Shanxin Yuan, Radu Timofte, Gregory Slabaugh, Ales Leonardis, Bolun Zheng, Xin Ye, Xiang Tian, Yaowu Chen, Xi Cheng, Zhenyong Fu, Jian Yang, Ming Hong, Wenying Lin, Wenjin Yang, Yanyun Qu, Hong-Kyu Shin, Joon-Yeon Kim, Sung-Jea Ko, Hang Dong, Yu Guo, Jie Wang, Xuan Ding, Zongyan Han, Sourya Dipta Das, Kuldeep Purohit, Praveen Kandula, Maitreya Suin, A. N. Rajagopalan
- **Comment**: arXiv admin note: text overlap with arXiv:1911.02498
- **Journal**: None
- **Summary**: This paper reviews the first-ever image demoireing challenge that was part of the Advances in Image Manipulation (AIM) workshop, held in conjunction with ICCV 2019. This paper describes the challenge, and focuses on the proposed solutions and their results. Demoireing is a difficult task of removing moire patterns from an image to reveal an underlying clean image. A new dataset, called LCDMoire was created for this challenge, and consists of 10,200 synthetically generated image pairs (moire and clean ground truth). The challenge was divided into 2 tracks. Track 1 targeted fidelity, measuring the ability of demoire methods to obtain a moire-free image compared with the ground truth, while Track 2 examined the perceptual quality of demoire methods. The tracks had 60 and 39 registered participants, respectively. A total of eight teams competed in the final testing phase. The entries span the current the state-of-the-art in the image demoireing problem.



### Knowledge Distillation for Incremental Learning in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.03462v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.03462v4)
- **Published**: 2019-11-08 08:17:03+00:00
- **Updated**: 2021-01-20 20:03:16+00:00
- **Authors**: Umberto Michieli, Pietro Zanuttigh
- **Comment**: Computer Vision and Image Understanding (CVIU), 2021. arXiv admin
  note: text overlap with arXiv:1907.13372
- **Journal**: None
- **Summary**: Deep learning architectures have shown remarkable results in scene understanding problems, however they exhibit a critical drop of performances when they are required to learn incrementally new tasks without forgetting old ones. This catastrophic forgetting phenomenon impacts on the deployment of artificial intelligence in real world scenarios where systems need to learn new and different representations over time. Current approaches for incremental learning deal only with image classification and object detection tasks, while in this work we formally introduce incremental learning for semantic segmentation. We tackle the problem applying various knowledge distillation techniques on the previous model. In this way, we retain the information about learned classes, whilst updating the current model to learn the new ones. We developed four main methodologies of knowledge distillation working on both output layers and internal feature representations. We do not store any image belonging to previous training stages and only the last model is used to preserve high accuracy on previously learned classes. Extensive experimental results on the Pascal VOC2012 and MSRC-v2 datasets show the effectiveness of the proposed approaches in several incremental learning scenarios.



### Quality Aware Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.03149v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.03149v1)
- **Published**: 2019-11-08 09:42:35+00:00
- **Updated**: 2019-11-08 09:42:35+00:00
- **Authors**: Parimala Kancharla, Sumohana S. Channappayya
- **Comment**: 10 pages, NeurIPS 2019
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have become a very popular tool for implicitly learning high-dimensional probability distributions. Several improvements have been made to the original GAN formulation to address some of its shortcomings like mode collapse, convergence issues, entanglement, poor visual quality etc. While a significant effort has been directed towards improving the visual quality of images generated by GANs, it is rather surprising that objective image quality metrics have neither been employed as cost functions nor as regularizers in GAN objective functions. In this work, we show how a distance metric that is a variant of the Structural SIMilarity (SSIM) index (a popular full-reference image quality assessment algorithm), and a novel quality aware discriminator gradient penalty function that is inspired by the Natural Image Quality Evaluator (NIQE, a popular no-reference image quality assessment algorithm) can each be used as excellent regularizers for GAN objective functions. Specifically, we demonstrate state-of-the-art performance using the Wasserstein GAN gradient penalty (WGAN-GP) framework over CIFAR-10, STL10 and CelebA datasets.



### Building Segmentation through a Gated Graph Convolutional Neural Network with Deep Structured Feature Embedding
- **Arxiv ID**: http://arxiv.org/abs/1911.03165v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.03165v1)
- **Published**: 2019-11-08 10:19:13+00:00
- **Updated**: 2019-11-08 10:19:13+00:00
- **Authors**: Yilei Shi, Qingyu Li, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic building extraction from optical imagery remains a challenge due to, for example, the complexity of building shapes. Semantic segmentation is an efficient approach for this task. The latest development in deep convolutional neural networks (DCNNs) has made accurate pixel-level classification tasks possible. Yet one central issue remains: the precise delineation of boundaries. Deep architectures generally fail to produce fine-grained segmentation with accurate boundaries due to their progressive down-sampling. Hence, we introduce a generic framework to overcome the issue, integrating the graph convolutional network (GCN) and deep structured feature embedding (DSFE) into an end-to-end workflow. Furthermore, instead of using a classic graph convolutional neural network, we propose a gated graph convolutional network, which enables the refinement of weak and coarse semantic predictions to generate sharp borders and fine-grained pixel-level classification. Taking the semantic segmentation of building footprints as a practical example, we compared different feature embedding architectures and graph neural networks. Our proposed framework with the new GCN architecture outperforms state-of-the-art approaches. Although our main task in this work is building footprint extraction, the proposed method can be generally applied to other binary or multi-label segmentation tasks.



### Perception-oriented Single Image Super-Resolution via Dual Relativistic Average Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.03464v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.03464v3)
- **Published**: 2019-11-08 11:09:43+00:00
- **Updated**: 2020-02-20 06:34:58+00:00
- **Authors**: Yuan Ma, Kewen Liu, Hongxia Xiong, Panpan Fang, Xiaojun Li, Yalei Chen, Chaoyang Liu
- **Comment**: Re-submit after codes reviewing
- **Journal**: None
- **Summary**: The presence of residual and dense neural networks which greatly promotes the development of image Super-Resolution(SR) have witnessed a lot of impressive results. Depending on our observation, although more layers and connections could always improve performance, the increase of model parameters is not conducive to launch application of SR algorithms. Furthermore, algorithms supervised by L1/L2 loss can achieve considerable performance on traditional metrics such as PSNR and SSIM, yet resulting in blurry and over-smoothed outputs without sufficient high-frequency details, namely low perceptual index(PI). Regarding the issues, this paper develops a perception-oriented single image SR algorithm via dual relativistic average generative adversarial networks. In the generator part, a novel residual channel attention block is proposed to recalibrate significance of specific channels, further increasing feature expression capabilities. Parameters of convolutional layers within each block are shared to expand receptive fields while maintain the amount of tunable parameters unchanged. The feature maps are subsampled using sub-pixel convolution to obtain reconstructed high-resolution images. The discriminator part consists of two relativistic average discriminators that work in pixel domain and feature domain, respectively, fully exploiting the prior that half of data in a mini-batch are fake. Different weighted combinations of perceptual loss and adversarial loss are utilized to supervise the generator to equilibrate perceptual quality and objective results. Experimental results and ablation studies show that our proposed algorithm can rival state-of-the-art SR algorithms, both perceptually(PI-minimization) and objectively(PSNR-maximization) with fewer parameters.



### Algorithmic Design and Implementation of Unobtrusive Multistatic Serial LiDAR Image
- **Arxiv ID**: http://arxiv.org/abs/1911.03267v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1911.03267v1)
- **Published**: 2019-11-08 13:58:00+00:00
- **Updated**: 2019-11-08 13:58:00+00:00
- **Authors**: Chi Ding, Zheng Cao, Matthew S. Emigh, Jose C. Principe, Bing Ouyang, Anni Vuorenkoski, Fraser Dalgleish, Brian Ramos, Yanjun Li
- **Comment**: None
- **Journal**: None
- **Summary**: To fully understand interactions between marine hydrokinetic (MHK) equipment and marine animals, a fast and effective monitoring system is required to capture relevant information whenever underwater animals appear. A new automated underwater imaging system composed of LiDAR (Light Detection and Ranging) imaging hardware and a scene understanding software module named Unobtrusive Multistatic Serial LiDAR Imager (UMSLI) to supervise the presence of animals near turbines. UMSLI integrates the front end LiDAR hardware and a series of software modules to achieve image preprocessing, detection, tracking, segmentation and classification in a hierarchical manner.



### Dynamic Multi-Task Learning for Face Recognition with Facial Expression
- **Arxiv ID**: http://arxiv.org/abs/1911.03281v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.03281v1)
- **Published**: 2019-11-08 14:30:48+00:00
- **Updated**: 2019-11-08 14:30:48+00:00
- **Authors**: Zuheng Ming, Junshi Xia, Muhammad Muzzamil Luqman, Jean-Christophe Burie, Kaixing Zhao
- **Comment**: accept by the ICCV2019 workshop
- **Journal**: None
- **Summary**: Benefiting from the joint learning of the multiple tasks in the deep multi-task networks, many applications have shown the promising performance comparing to single-task learning. However, the performance of multi-task learning framework is highly dependant on the relative weights of the tasks. How to assign the weight of each task is a critical issue in the multi-task learning. Instead of tuning the weights manually which is exhausted and time-consuming, in this paper we propose an approach which can dynamically adapt the weights of the tasks according to the difficulty for training the task. Specifically, the proposed method does not introduce the hyperparameters and the simple structure allows the other multi-task deep learning networks can easily realize or reproduce this method. We demonstrate our approach for face recognition with facial expression and facial expression recognition from a single input image based on a deep multi-task learning Conventional Neural Networks (CNNs). Both the theoretical analysis and the experimental results demonstrate the effectiveness of the proposed dynamic multi-task learning method. This multi-task learning with dynamic weights also boosts of the performance on the different tasks comparing to the state-of-art methods with single-task learning.



### Dynamic Deep Multi-task Learning for Caricature-Visual Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1911.03341v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.03341v1)
- **Published**: 2019-11-08 16:04:08+00:00
- **Updated**: 2019-11-08 16:04:08+00:00
- **Authors**: Zuheng Ming, Jean-Christophe Burie, Muhammad Muzzamil Luqman
- **Comment**: accepted by the GREC2019@ICDAR2019. arXiv admin note: text overlap
  with arXiv: 1911.03281
- **Journal**: None
- **Summary**: Rather than the visual images, the face recognition of the caricatures is far from the performance of the visual images. The challenge is the extreme non-rigid distortions of the caricatures introduced by exaggerating the facial features to strengthen the characters. In this paper, we propose dynamic multi-task learning based on deep CNNs for cross-modal caricature-visual face recognition. Instead of the conventional multi-task learning with fixed weights of the tasks, the proposed dynamic multi-task learning dynamically updates the weights of tasks according to the importance of the tasks, which enables the training of the networks focus on the hard task instead of being stuck in the overtraining of the easy task. The experimental results demonstrate the effectiveness of the dynamic multi-task learning for caricature-visual face recognition. The performance evaluated on the datasets CaVI and WebCaricature show the superiority over the state-of-art methods. The implementation code is available here.



### Content-Consistent Generation of Realistic Eyes with Style
- **Arxiv ID**: http://arxiv.org/abs/1911.03346v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.03346v1)
- **Published**: 2019-11-08 16:12:59+00:00
- **Updated**: 2019-11-08 16:12:59+00:00
- **Authors**: Marcel BÃ¼hler, Seonwook Park, Shalini De Mello, Xucong Zhang, Otmar Hilliges
- **Comment**: 4 pages, 4 figures, ICCV Workshop 2019
- **Journal**: None
- **Summary**: Accurately labeled real-world training data can be scarce, and hence recent works adapt, modify or generate images to boost target datasets. However, retaining relevant details from input data in the generated images is challenging and failure could be critical to the performance on the final task. In this work, we synthesize person-specific eye images that satisfy a given semantic segmentation mask (content), while following the style of a specified person from only a few reference images. We introduce two approaches, (a) one used to win the OpenEDS Synthetic Eye Generation Challenge at ICCV 2019, and (b) a principled approach to solving the problem involving simultaneous injection of style and content information at multiple scales. Our implementation is available at https://github.com/mcbuehler/Seg2Eye.



### Self-Assignment Flows for Unsupervised Data Labeling on Graphs
- **Arxiv ID**: http://arxiv.org/abs/1911.03472v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.03472v2)
- **Published**: 2019-11-08 16:35:13+00:00
- **Updated**: 2020-03-24 12:35:23+00:00
- **Authors**: Matthias Zisler, Artjom Zern, Stefania Petra, Christoph SchnÃ¶rr
- **Comment**: 42 pages, 17 figures
- **Journal**: None
- **Summary**: This paper extends the recently introduced assignment flow approach for supervised image labeling to unsupervised scenarios where no labels are given. The resulting self-assignment flow takes a pairwise data affinity matrix as input data and maximizes the correlation with a low-rank matrix that is parametrized by the variables of the assignment flow, which entails an assignment of the data to themselves through the formation of latent labels (feature prototypes). A single user parameter, the neighborhood size for the geometric regularization of assignments, drives the entire process. By smooth geodesic interpolation between different normalizations of self-assignment matrices on the positive definite matrix manifold, a one-parameter family of self-assignment flows is defined. Accordingly, our approach can be characterized from different viewpoints, e.g. as performing spatially regularized, rank-constrained discrete optimal transport, or as computing spatially regularized normalized spectral cuts. Regarding combinatorial optimization, our approach successfully determines completely positive factorizations of self-assignments in large-scale scenarios, subject to spatial regularization. Various experiments including the unsupervised learning of patch dictionaries using a locally invariant distance function, illustrate the properties of the approach.



### Automatic Identification of Traditional Colombian Music Genres based on Audio Content Analysis and Machine Learning Technique
- **Arxiv ID**: http://arxiv.org/abs/1911.03372v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.03372v1)
- **Published**: 2019-11-08 16:53:39+00:00
- **Updated**: 2019-11-08 16:53:39+00:00
- **Authors**: Diego A. Cruz, Sergio S. Lopez, Jorge E. Camargo
- **Comment**: None
- **Journal**: None
- **Summary**: Colombia has a diversity of genres in traditional music, which allows to express the richness of the Colombian culture according to the region. This musical diversity is the result of a mixture of African, native Indigenous, and European influences. Organizing large collections of songs is a time consuming task that requires that a human listens to fragments of audio to identify genre, singer, year, instruments and other relevant characteristics that allow to index the song dataset. This paper presents a method to automatically identify the genre of a Colombian song by means of its audio content. The method extracts audio features that are used to train a machine learning model that learns to classify the genre. The method was evaluated in a dataset of 180 musical pieces belonging to six folkloric Colombian music genres: Bambuco, Carranga, Cumbia, Joropo, Pasillo, and Vallenato. Results show that it is possible to automatically identify the music genre in spite of the complexity of Colombian rhythms reaching an average accuracy of 69\%.



### Single-shot 3D multi-person pose estimation in complex images
- **Arxiv ID**: http://arxiv.org/abs/1911.03391v2
- **DOI**: 10.1016/j.patcog.2020.107534
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.03391v2)
- **Published**: 2019-11-08 17:13:33+00:00
- **Updated**: 2021-01-07 09:59:22+00:00
- **Authors**: Abdallah Benzine, Bertrand Luvison, Quoc Cuong Pham, Catherine Achard
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a new single shot method for multi-person 3D human pose estimation in complex images. The model jointly learns to locate the human joints in the image, to estimate their 3D coordinates and to group these predictions into full human skeletons. The proposed method deals with a variable number of people and does not need bounding boxes to estimate the 3D poses. It leverages and extends the Stacked Hourglass Network and its multi-scale feature learning to manage multi-person situations. Thus, we exploit a robust 3D human pose formulation to fully describe several 3D human poses even in case of strong occlusions or crops. Then, joint grouping and human pose estimation for an arbitrary number of people are performed using the associative embedding method. Our approach significantly outperforms the state of the art on the challenging CMU Panoptic and a previous single shot method on the MuPoTS-3D dataset. Furthermore, it leads to good results on the complex and synthetic images from the newly proposed JTA Dataset.



### Joint Demosaicing and Super-Resolution (JDSR): Network Design and Perceptual Optimization
- **Arxiv ID**: http://arxiv.org/abs/1911.03558v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.03558v3)
- **Published**: 2019-11-08 22:01:09+00:00
- **Updated**: 2020-08-04 00:21:28+00:00
- **Authors**: Xuan Xu, Yanfang Ye, Xin Li
- **Comment**: IEEE Transactions on Computational Imaging
- **Journal**: None
- **Summary**: Image demosaicing and super-resolution are two important tasks in color imaging pipeline. So far they have been mostly independently studied in the open literature of deep learning; little is known about the potential benefit of formulating a joint demosaicing and super-resolution (JDSR) problem. In this paper, we propose an end-to-end optimization solution to the JDSR problem and demonstrate its practical significance in computational imaging. Our technical contributions are mainly two-fold. On network design, we have developed a Residual-Dense Squeeze-and-Excitation Networks (RDSEN) supported by a pre-demosaicing network (PDNet) as the pre-processing step. We address the issue of spatio-spectral attention for color-filter-array (CFA) data and discuss how to achieve better information flow by concatenating Residue-Dense Squeeze-and-Excitation Blocks (RDSEBs) for JDSR. Experimental results have shown that significant PSNR/SSIM gain can be achieved by RDSEN over previous network architectures including state-of-the-art RCAN. On perceptual optimization, we propose to leverage the latest ideas including relativistic discriminator and pre-excitation perceptual loss function to further improve the visual quality of textured regions in reconstructed images. Our extensive experiment results have shown that Texture-enhanced Relativistic average Generative Adversarial Network (TRaGAN) can produce both subjectively more pleasant images and objectively lower perceptual distortion scores than standard GAN for JDSR. Finally, we have verified the benefit of JDSR to high-quality image reconstruction from real-world Bayer pattern data collected by NASA Mars Curiosity.



### Recurrent Neural Network Transducer for Audio-Visual Speech Recognition
- **Arxiv ID**: http://arxiv.org/abs/1911.04890v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CL, cs.CV, cs.LG, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1911.04890v1)
- **Published**: 2019-11-08 22:01:42+00:00
- **Updated**: 2019-11-08 22:01:42+00:00
- **Authors**: Takaki Makino, Hank Liao, Yannis Assael, Brendan Shillingford, Basilio Garcia, Otavio Braga, Olivier Siohan
- **Comment**: Will be presented in 2019 IEEE Automatic Speech Recognition and
  Understanding Workshop (ASRU 2019)
- **Journal**: None
- **Summary**: This work presents a large-scale audio-visual speech recognition system based on a recurrent neural network transducer (RNN-T) architecture. To support the development of such a system, we built a large audio-visual (A/V) dataset of segmented utterances extracted from YouTube public videos, leading to 31k hours of audio-visual training content. The performance of an audio-only, visual-only, and audio-visual system are compared on two large-vocabulary test sets: a set of utterance segments from public YouTube videos called YTDEV18 and the publicly available LRS3-TED set. To highlight the contribution of the visual modality, we also evaluated the performance of our system on the YTDEV18 set artificially corrupted with background noise and overlapping speech. To the best of our knowledge, our system significantly improves the state-of-the-art on the LRS3-TED set.



### Vision-Based Lane-Changing Behavior Detection Using Deep Residual Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1911.03565v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.03565v1)
- **Published**: 2019-11-08 22:28:53+00:00
- **Updated**: 2019-11-08 22:28:53+00:00
- **Authors**: Zhensong Wei, Chao Wang, Peng Hao, Matthew Barth
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate lane localization and lane change detection are crucial in advanced driver assistance systems and autonomous driving systems for safer and more efficient trajectory planning. Conventional localization devices such as Global Positioning System only provide road-level resolution for car navigation, which is incompetent to assist in lane-level decision making. The state of art technique for lane localization is to use Light Detection and Ranging sensors to correct the global localization error and achieve centimeter-level accuracy, but the real-time implementation and popularization for LiDAR is still limited by its computational burden and current cost. As a cost-effective alternative, vision-based lane change detection has been highly regarded for affordable autonomous vehicles to support lane-level localization. A deep learning-based computer vision system is developed to detect the lane change behavior using the images captured by a front-view camera mounted on the vehicle and data from the inertial measurement unit for highway driving. Testing results on real-world driving data have shown that the proposed method is robust with real-time working ability and could achieve around 87% lane change detection accuracy. Compared to the average human reaction to visual stimuli, the proposed computer vision system works 9 times faster, which makes it capable of helping make life-saving decisions in time.



### Face Detection in Camera Captured Images of Identity Documents under Challenging Conditions
- **Arxiv ID**: http://arxiv.org/abs/1911.03567v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.03567v1)
- **Published**: 2019-11-08 22:39:06+00:00
- **Updated**: 2019-11-08 22:39:06+00:00
- **Authors**: Souhail Bakkali, Zuheng Ming, Muhammad Muzzamil Luqman, Jean-Christophe Burie
- **Comment**: accepted by the ICDAR2019 workshop CBDAR2019
- **Journal**: None
- **Summary**: Benefiting from the advance of deep convolutional neural network approaches (CNNs), many face detection algorithms have achieved state-of-the-art performance in terms of accuracy and very high speed in unconstrained applications. However, due to the lack of public datasets and due to the variation of the orientation of face images, the complex background and lighting, defocus and the varying illumination of camera captured images, face detection on identity documents under unconstrained environments has not been sufficiently studied. To address this problem more efficiently, we survey three state-of-the-art face detection methods based on general images, i.e. Cascade-CNN, MTCNN and PCN, for face detection in camera captured images of identity documents, given different image quality assessments. For that, The MIDV-500 dataset, which is the largest and most challenging dataset for identity documents, is used to evaluate the three methods. The evaluation results show the performance and the limitations of the current methods for face detection on identity documents under the wild complex environments. These results show that the face detection task in camera captured images of identity documents is challenging, providing a space to improve in the future works.



### On the Relationship between Self-Attention and Convolutional Layers
- **Arxiv ID**: http://arxiv.org/abs/1911.03584v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.03584v2)
- **Published**: 2019-11-08 23:48:38+00:00
- **Updated**: 2020-01-10 09:06:09+00:00
- **Authors**: Jean-Baptiste Cordonnier, Andreas Loukas, Martin Jaggi
- **Comment**: To appear at ICLR 2020
- **Journal**: None
- **Summary**: Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Ramachandran et al. (2019) showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that self-attention layers attend to pixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code is publicly available.



