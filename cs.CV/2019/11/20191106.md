# Arxiv Papers in cs.CV on 2019-11-06
### Spatially regularized active diffusion learning for high-dimensional images
- **Arxiv ID**: http://arxiv.org/abs/1911.02155v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ME, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.02155v1)
- **Published**: 2019-11-06 00:58:24+00:00
- **Updated**: 2019-11-06 00:58:24+00:00
- **Authors**: James M. Murphy
- **Comment**: 17 pages
- **Journal**: None
- **Summary**: An active learning algorithm for the classification of high-dimensional images is proposed in which spatially-regularized nonlinear diffusion geometry is used to characterize cluster cores. The proposed method samples from estimated cluster cores in order to generate a small but potent set of training labels which propagate to the remainder of the dataset via the underlying diffusion process. By spatially regularizing the rich, high-dimensional spectral information of the image to efficiently estimate the most significant and influential points in the data, our approach avoids redundancy in the training dataset. This allows it to produce high-accuracy labelings with a very small number of training labels. The proposed algorithm admits an efficient numerical implementation that scales essentially linearly in the number of data points under a suitable data model and enjoys state-of-the-art performance on real hyperspectral images.



### SRINet: Learning Strictly Rotation-Invariant Representations for Point Cloud Classification and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.02163v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.02163v1)
- **Published**: 2019-11-06 02:08:03+00:00
- **Updated**: 2019-11-06 02:08:03+00:00
- **Authors**: Xiao Sun, Zhouhui Lian, Jianguo Xiao
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: Point cloud analysis has drawn broader attentions due to its increasing demands in various fields. Despite the impressive performance has been achieved on several databases, researchers neglect the fact that the orientation of those point cloud data is aligned. Varying the orientation of point cloud may lead to the degradation of performance, restricting the capacity of generalizing to real applications where the prior of orientation is often unknown. In this paper, we propose the point projection feature, which is invariant to the rotation of the input point cloud. A novel architecture is designed to mine features of different levels. We adopt a PointNet-based backbone to extract global feature for point cloud, and the graph aggregation operation to perceive local shape structure. Besides, we introduce an efficient key point descriptor to assign each point with different response and help recognize the overall geometry. Mathematical analyses and experimental results demonstrate that the proposed method can extract strictly rotation-invariant representations for point cloud recognition and segmentation without data augmentation, and outperforms other state-of-the-art methods.



### Interpretable Self-Attention Temporal Reasoning for Driving Behavior Understanding
- **Arxiv ID**: http://arxiv.org/abs/1911.02172v1
- **DOI**: 10.1109/ICASSP40776.2020.9053783
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.02172v1)
- **Published**: 2019-11-06 02:49:30+00:00
- **Updated**: 2019-11-06 02:49:30+00:00
- **Authors**: Yi-Chieh Liu, Yung-An Hsieh, Min-Hung Chen, Chao-Han Huck Yang, Jesper Tegner, Yi-Chang James Tsai
- **Comment**: Submitted to IEEE ICASSP 2020; Pytorch code will be released soon
- **Journal**: 2020 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)
- **Summary**: Performing driving behaviors based on causal reasoning is essential to ensure driving safety. In this work, we investigated how state-of-the-art 3D Convolutional Neural Networks (CNNs) perform on classifying driving behaviors based on causal reasoning. We proposed a perturbation-based visual explanation method to inspect the models' performance visually. By examining the video attention saliency, we found that existing models could not precisely capture the causes (e.g., traffic light) of the specific action (e.g., stopping). Therefore, the Temporal Reasoning Block (TRB) was proposed and introduced to the models. With the TRB models, we achieved the accuracy of $\mathbf{86.3\%}$, which outperform the state-of-the-art 3D CNNs from previous works. The attention saliency also demonstrated that TRB helped models focus on the causes more precisely. With both numerical and visual evaluations, we concluded that our proposed TRB models were able to provide accurate driving behavior prediction by learning the causal reasoning of the behaviors.



### Architectural Tricks for Deep Learning in Remote Photoplethysmography
- **Arxiv ID**: http://arxiv.org/abs/1911.02202v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.02202v1)
- **Published**: 2019-11-06 05:07:38+00:00
- **Updated**: 2019-11-06 05:07:38+00:00
- **Authors**: Mikhail Kopeliovich, Yuriy Mironenko, Mikhail Petrushan
- **Comment**: Workshop on Computer Vision for Physiological Measurement, ICCV 2019
- **Journal**: None
- **Summary**: Architectural improvements are studied for convolutional network performing estimation of heart rate (HR) values on color signal patches. Color signals are time series of color components averaged over facial regions recorded by webcams in two scenarios: Stationary (without motion of a person) and Mixed Motion (different motion patterns of a person). HR estimation problem is addressed as a classification task, where classes correspond to different heart rate values within the admissible range of [40; 125] bpm. Both adding convolutional filtering layers after fully connected layers and involving combined loss function where first component is a cross entropy and second is a squared error between the network output and smoothed one-hot vector, lead to better performance of HR estimation model in Stationary and Mixed Motion scenarios.



### Semantic Image Completion and Enhancement using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.02222v2
- **DOI**: 10.1109/ICCCNT45670.2019.8944750
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.02222v2)
- **Published**: 2019-11-06 06:47:08+00:00
- **Updated**: 2020-01-05 07:07:53+00:00
- **Authors**: Vaishnav Chandak, Priyansh Saxena, Manisha Pattanaik, Gaurav Kaushal
- **Comment**: 6 pages, 8 figures. Proceedings of "The 10th International Conference
  on Computing, Communication and Networking Technologies (ICCCNT)". Conference
  Proceedings ISBN Number: ISBN: 978-1-5386-5906-9; Link:
  https://ieeexplore.ieee.org/document/8944750
- **Journal**: None
- **Summary**: In real-life applications, certain images utilized are corrupted in which the image pixels are damaged or missing, which increases the complexity of computer vision tasks. In this paper, a deep learning architecture is proposed to deal with image completion and enhancement. Generative Adversarial Networks (GAN), has been turned out to be helpful in picture completion tasks. Therefore, in GANs, Wasserstein GAN architecture is used for image completion which creates the coarse patches to filling the missing region in the distorted picture, and the enhancement network will additionally refine the resultant pictures utilizing residual learning procedures and hence give better complete pictures for computer vision applications. Experimental outcomes show that the proposed approach improves the Peak Signal to Noise ratio and Structural Similarity Index values by 2.45% and 4% respectively when compared to the recently reported data.



### Localization-aware Channel Pruning for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.02237v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.02237v3)
- **Published**: 2019-11-06 07:41:21+00:00
- **Updated**: 2020-02-29 08:29:03+00:00
- **Authors**: Zihao Xie, Wenbing Tao, Li Zhu, Lin Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Channel pruning is one of the important methods for deep model compression. Most of existing pruning methods mainly focus on classification. Few of them conduct systematic research on object detection. However, object detection is different from classification, which requires not only semantic information but also localization information. In this paper, based on discrimination-aware channel pruning (DCP) which is state-of-the-art pruning method for classification, we propose a localization-aware auxiliary network to find out the channels with key information for classification and regression so that we can conduct channel pruning directly for object detection, which saves lots of time and computing resources. In order to capture the localization information, we first design the auxiliary network with a contextual ROIAlign layer which can obtain precise localization information of the default boxes by pixel alignment and enlarges the receptive fields of the default boxes when pruning shallow layers. Then, we construct a loss function for object detection task which tends to keep the channels that contain the key information for classification and regression. Extensive experiments demonstrate the effectiveness of our method. On MS COCO, we prune 70\% parameters of the SSD based on ResNet-50 with modest accuracy drop, which outperforms the-state-of-art method.



### Predictive modeling of brain tumor: A Deep learning approach
- **Arxiv ID**: http://arxiv.org/abs/1911.02265v6
- **DOI**: 10.1007/978-981-15-6067-5_30
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.02265v6)
- **Published**: 2019-11-06 09:27:48+00:00
- **Updated**: 2023-07-16 12:39:21+00:00
- **Authors**: Priyansh Saxena, Akshat Maheshwari, Saumil Maheshwari
- **Comment**: This work is part of the conference proceeding 'Proceedings of the
  International Conference on Artificial Intelligence' and can be accessed at
  https://link.springer.com/chapter/10.1007/978-981-15-6067-5_30
- **Journal**: None
- **Summary**: Image processing concepts can visualize the different anatomy structure of the human body. Recent advancements in the field of deep learning have made it possible to detect the growth of cancerous tissue just by a patient's brain Magnetic Resonance Imaging (MRI) scans. These methods require very high accuracy and meager false negative rates to be of any practical use. This paper presents a Convolutional Neural Network (CNN) based transfer learning approach to classify the brain MRI scans into two classes using three pre-trained models. The performances of these models are compared with each other. Experimental results show that the Resnet-50 model achieves the highest accuracy and least false negative rates as 95% and zero respectively. It is followed by VGG-16 and Inception-V3 model with an accuracy of 90% and 55% respectively.



### UNO: Uncertainty-aware Noisy-Or Multimodal Fusion for Unanticipated Input Degradation
- **Arxiv ID**: http://arxiv.org/abs/1911.05611v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1911.05611v2)
- **Published**: 2019-11-06 09:42:04+00:00
- **Updated**: 2020-03-04 03:39:54+00:00
- **Authors**: Junjiao Tian, Wesley Cheung, Nathan Glaser, Yen-Cheng Liu, Zsolt Kira
- **Comment**: IEEE International Conference on Robotics and Automation (ICRA),
  2020. IROS Workshop on the Importance of Uncertainty in Deep Learning for
  Robotics, 2019
- **Journal**: None
- **Summary**: The fusion of multiple sensor modalities, especially through deep learning architectures, has been an active area of study. However, an under-explored aspect of such work is whether the methods can be robust to degradations across their input modalities, especially when they must generalize to degradations not seen during training. In this work, we propose an uncertainty-aware fusion scheme to effectively fuse inputs that might suffer from a range of known and unknown degradations. Specifically, we analyze a number of uncertainty measures, each of which captures a different aspect of uncertainty, and we propose a novel way to fuse degraded inputs by scaling modality-specific output softmax probabilities. We additionally propose a novel data-dependent spatial temperature scaling method to complement these existing uncertainty measures. Finally, we integrate the uncertainty-scaled output from each modality using a probabilistic noisy-or fusion method. In a photo-realistic simulation environment (AirSim), we show that our method achieves significantly better results on a semantic segmentation task, compared to state-of-art fusion architectures, on a range of degradations (e.g. fog, snow, frost, and various other types of noise), some of which are unknown during training. We specifically improve upon the state-of-art[1] by 28% in mean IoU on various degradations. [1] Abhinav Valada, Rohit Mohan, and Wolfram Burgard. Self-Supervised Model Adaptation for Multimodal Semantic Segmentation. In: arXiv e-prints, arXiv:1808.03833 (Aug. 2018), arXiv:1808.03833. arXiv: 1808.03833 [cs.CV].



### Where is the Fake? Patch-Wise Supervised GANs for Texture Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1911.02274v2
- **DOI**: 10.1109/ICIP40778.2020.9191340
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.02274v2)
- **Published**: 2019-11-06 09:43:45+00:00
- **Updated**: 2020-03-09 10:22:25+00:00
- **Authors**: Ahmed Ben Saad, Youssef Tamaazousti, Josselin Kherroubi, Alexis He
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle the problem of texture inpainting where the input images are textures with missing values along with masks that indicate the zones that should be generated. Many works have been done in image inpainting with the aim to achieve global and local consistency. But these works still suffer from limitations when dealing with textures. In fact, the local information in the image to be completed needs to be used in order to achieve local continuities and visually realistic texture inpainting. For this, we propose a new segmentor discriminator that performs a patch-wise real/fake classification and is supervised by input masks. During training, it aims to locate the fake and thus backpropagates consistent signal to the generator. We tested our approach on the publicly available DTD dataset and showed that it achieves state-of-the-art performances and better deals with local consistency than existing methods.



### Optimization with soft Dice can lead to a volumetric bias
- **Arxiv ID**: http://arxiv.org/abs/1911.02278v1
- **DOI**: 10.1007/978-3-030-46640-4_9
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.02278v1)
- **Published**: 2019-11-06 09:52:56+00:00
- **Updated**: 2019-11-06 09:52:56+00:00
- **Authors**: Jeroen Bertels, David Robben, Dirk Vandermeulen, Paul Suetens
- **Comment**: BrainLes Workshop - MICCAI 2019
- **Journal**: LNCS 11992, Springer Nature Switzerland AG 2019
- **Summary**: Segmentation is a fundamental task in medical image analysis. The clinical interest is often to measure the volume of a structure. To evaluate and compare segmentation methods, the similarity between a segmentation and a predefined ground truth is measured using metrics such as the Dice score. Recent segmentation methods based on convolutional neural networks use a differentiable surrogate of the Dice score, such as soft Dice, explicitly as the loss function during the learning phase. Even though this approach leads to improved Dice scores, we find that, both theoretically and empirically on four medical tasks, it can introduce a volumetric bias for tasks with high inherent uncertainty. As such, this may limit the method's clinical applicability.



### Spatial Feature Extraction in Airborne Hyperspectral Images Using Local Spectral Similarity
- **Arxiv ID**: http://arxiv.org/abs/1911.02285v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.02285v1)
- **Published**: 2019-11-06 10:11:41+00:00
- **Updated**: 2019-11-06 10:11:41+00:00
- **Authors**: Anand S Sahadevan, Arundhati Misra, Praveen Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: Local spectral similarity (LSS) algorithm has been developed for detecting homogeneous areas and edges in hyperspectral images (HSIs). The proposed algorithm transforms the 3-D data cube (within a spatial window) into a spectral similarity matrix by calculating the vector-similarity between the center pixel-spectrum and the neighborhood spectra. The final edge intensity is derived upon order statistics of the similarity matrix or spatial convolution of the similarity matrix with the spatial kernels. The LSS algorithm facilitates simultaneous use of spectral-spatial information for the edge detection by considering the spatial pattern of similar spectra within a spatial window. The proposed edge-detection method is tested on benchmark HSIs as well as the image obtained from Airborne-Visible-and-Infra-RedImaging-Spectrometer-Next-Generation (AVIRIS-NG). Robustness of the LSS method against multivariate Gaussian noise and low spatial resolution scenarios were also verified with the benchmark HSIs. Figure-of-merit, false-alarm-count and miss-count were applied to evaluate the performance of edge detection methods. Results showed that Fractional distance measure and Euclidean distance measure were able to detect the edges in HSIs more precisely as compared to other spectral similarity measures. The proposed method can be applied to radiance and reflectance data (whole spectrum) and it has shown good performance on principal component images as well. In addition, the proposed algorithm outperforms the traditional multichannel edge detectors in terms of both fastness, accuracy and the robustness. The experimental results also confirm that LSS can be applied as a pre-processing approach to reduce the errors in clustering as well as classification outputs.



### Boosting Object Recognition in Point Clouds by Saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.02286v1
- **DOI**: 10.1007/978-3-030-30754-7_32
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.02286v1)
- **Published**: 2019-11-06 10:15:03+00:00
- **Updated**: 2019-11-06 10:15:03+00:00
- **Authors**: Marlon Marcon, Riccardo Spezialetti, Samuele Salti, Luciano Silva, Luigi Di Stefano
- **Comment**: International Conference on Image Analysis and Processing (ICIAP)
  2019
- **Journal**: None
- **Summary**: Object recognition in 3D point clouds is a challenging task, mainly when time is an important factor to deal with, such as in industrial applications. Local descriptors are an amenable choice whenever the 6 DoF pose of recognized objects should also be estimated. However, the pipeline for this kind of descriptors is highly time-consuming. In this work, we propose an update to the traditional pipeline, by adding a preliminary filtering stage referred to as saliency boost. We perform tests on a standard object recognition benchmark by considering four keypoint detectors and four local descriptors, in order to compare time and recognition performance between the traditional pipeline and the boosted one. Results on time show that the boosted pipeline could turn out up to 5 times faster, with the recognition rate improving in most of the cases and exhibiting only a slight decrease in the others. These results suggest that the boosted pipeline can speed-up processing time substantially with limited impacts or even benefits in recognition accuracy.



### Melanoma detection with electrical impedance spectroscopy and dermoscopy using joint deep learning models
- **Arxiv ID**: http://arxiv.org/abs/1911.02322v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.02322v2)
- **Published**: 2019-11-06 11:39:12+00:00
- **Updated**: 2020-02-05 14:28:04+00:00
- **Authors**: Nils Gessert, Marcel Bengs, Alexander Schlaefer
- **Comment**: Accepted at SPIE Medical Imaging 2020
- **Journal**: None
- **Summary**: The initial assessment of skin lesions is typically based on dermoscopic images. As this is a difficult and time-consuming task, machine learning methods using dermoscopic images have been proposed to assist human experts. Other approaches have studied electrical impedance spectroscopy (EIS) as a basis for clinical decision support systems. Both methods represent different ways of measuring skin lesion properties as dermoscopy relies on visible light and EIS uses electric currents. Thus, the two methods might carry complementary features for lesion classification. Therefore, we propose joint deep learning models considering both EIS and dermoscopy for melanoma detection. For this purpose, we first study machine learning methods for EIS that incorporate domain knowledge and previously used heuristics into the design process. As a result, we propose a recurrent model with state-max-pooling which automatically learns the relevance of different EIS measurements. Second, we combine this new model with different convolutional neural networks that process dermoscopic images. We study ensembling approaches and also propose a cross-attention module guiding information exchange between the EIS and dermoscopy model. In general, combinations of EIS and dermoscopy clearly outperform models that only use either EIS or dermoscopy. We show that our attention-based, combined model outperforms other models with specificities of 34.4% (CI 31.3-38.4), 34.7% (CI 31.0-38.8) and 53.7% (CI 50.1-57.6) for dermoscopy, EIS and the combined model, respectively, at a clinically relevant sensitivity of 98%.



### Privacy Preserving Gaze Estimation using Synthetic Images via a Randomized Encoding Based Framework
- **Arxiv ID**: http://arxiv.org/abs/1911.07936v4
- **DOI**: 10.1145/3379156.3391364
- **Categories**: **cs.CV**, cs.CR, cs.HC, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.07936v4)
- **Published**: 2019-11-06 12:52:09+00:00
- **Updated**: 2021-07-13 13:04:07+00:00
- **Authors**: Efe Bozkir, Ali Burak Ünal, Mete Akgün, Enkelejda Kasneci, Nico Pfeifer
- **Comment**: In Symposium on Eye Tracking Research and Applications (ETRA '20).
  Authors' copy of the published paper, refer to the doi for the definitive
  version
- **Journal**: None
- **Summary**: Eye tracking is handled as one of the key technologies for applications that assess and evaluate human attention, behavior, and biometrics, especially using gaze, pupillary, and blink behaviors. One of the challenges with regard to the social acceptance of eye tracking technology is however the preserving of sensitive and personal information. To tackle this challenge, we employ a privacy-preserving framework based on randomized encoding to train a Support Vector Regression model using synthetic eye images privately to estimate the human gaze. During the computation, none of the parties learn about the data or the result that any other party has. Furthermore, the party that trains the model cannot reconstruct pupil, blinks or visual scanpath. The experimental results show that our privacy-preserving framework is capable of working in real-time, with the same accuracy as compared to non-private version and could be extended to other eye tracking related problems.



### Uninformed Students: Student-Teacher Anomaly Detection with Discriminative Latent Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1911.02357v2
- **DOI**: 10.1109/CVPR42600.2020.00424
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.02357v2)
- **Published**: 2019-11-06 13:11:13+00:00
- **Updated**: 2020-03-18 17:53:16+00:00
- **Authors**: Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: We introduce a powerful student-teacher framework for the challenging problem of unsupervised anomaly detection and pixel-precise anomaly segmentation in high-resolution images. Student networks are trained to regress the output of a descriptive teacher network that was pretrained on a large dataset of patches from natural images. This circumvents the need for prior data annotation. Anomalies are detected when the outputs of the student networks differ from that of the teacher network. This happens when they fail to generalize outside the manifold of anomaly-free training data. The intrinsic uncertainty in the student networks is used as an additional scoring function that indicates anomalies. We compare our method to a large number of existing deep learning based methods for unsupervised anomaly detection. Our experiments demonstrate improvements over state-of-the-art methods on a number of real-world datasets, including the recently introduced MVTec Anomaly Detection dataset that was specifically designed to benchmark anomaly segmentation algorithms.



### Reversible Adversarial Attack based on Reversible Image Transformation
- **Arxiv ID**: http://arxiv.org/abs/1911.02360v7
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1911.02360v7)
- **Published**: 2019-11-06 13:15:32+00:00
- **Updated**: 2021-05-25 15:11:06+00:00
- **Authors**: Zhaoxia Yin, Hua Wang, Li Chen, Jie Wang, Weiming Zhang
- **Comment**: 2021 International Workshop on Safety & Security of Deep Learning
- **Journal**: None
- **Summary**: In order to prevent illegal or unauthorized access of image data such as human faces and ensure legitimate users can use authorization-protected data, reversible adversarial attack technique is rise. Reversible adversarial examples (RAE) get both attack capability and reversibility at the same time. However, the existing technique can not meet application requirements because of serious distortion and failure of image recovery when adversarial perturbations get strong. In this paper, we take advantage of Reversible Image Transformation technique to generate RAE and achieve reversible adversarial attack. Experimental results show that proposed RAE generation scheme can ensure imperceptible image distortion and the original image can be reconstructed error-free. What's more, both the attack ability and the image quality are not limited by the perturbation amplitude.



### Probabilistic Watershed: Sampling all spanning forests for seeded segmentation and semi-supervised learning
- **Arxiv ID**: http://arxiv.org/abs/1911.02921v1
- **DOI**: None
- **Categories**: **cs.DS**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.02921v1)
- **Published**: 2019-11-06 13:26:32+00:00
- **Updated**: 2019-11-06 13:26:32+00:00
- **Authors**: Enrique Fita Sanmartin, Sebastian Damrich, Fred A. Hamprecht
- **Comment**: To be published in NeurIPS2019
- **Journal**: None
- **Summary**: The seeded Watershed algorithm / minimax semi-supervised learning on a graph computes a minimum spanning forest which connects every pixel / unlabeled node to a seed / labeled node. We propose instead to consider all possible spanning forests and calculate, for every node, the probability of sampling a forest connecting a certain seed with that node. We dub this approach "Probabilistic Watershed". Leo Grady (2006) already noted its equivalence to the Random Walker / Harmonic energy minimization. We here give a simpler proof of this equivalence and establish the computational feasibility of the Probabilistic Watershed with Kirchhoff's matrix tree theorem. Furthermore, we show a new connection between the Random Walker probabilities and the triangle inequality of the effective resistance. Finally, we derive a new and intuitive interpretation of the Power Watershed.



### Using Residual Dipolar Couplings from Two Alignment Media to Detect Structural Homology
- **Arxiv ID**: http://arxiv.org/abs/1911.02396v1
- **DOI**: None
- **Categories**: **q-bio.BM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.02396v1)
- **Published**: 2019-11-06 14:00:23+00:00
- **Updated**: 2019-11-06 14:00:23+00:00
- **Authors**: Ryan Yandle, Rishi Mukhopadhyay, Homayoun Valafar
- **Comment**: BioComp 2009, 6 pages
- **Journal**: None
- **Summary**: The method of Probability Density Profile Analysis has been introduced previously as a tool to find the best match between a set of experimentally generated Residual Dipolar Couplings and a set of known protein structures. While it proved effective on small databases in identifying protein fold families, and for picking the best result from computational protein folding tool ROBETTA, for larger data sets, more data is required. Here, the method of 2-D Probability Density Profile Analysis is presented which incorporates paired RDC data from 2 alignment media for N-H vectors. The method was tested using synthetic RDC data generated with +/-1 Hz error. The results show that the addition of information from a second alignment medium makes 2-D PDPA a much more effective tool that is able to identify a structure from a database of 600 protein fold family representatives.



### Predicting Long-Term Skeletal Motions by a Spatio-Temporal Hierarchical Recurrent Network
- **Arxiv ID**: http://arxiv.org/abs/1911.02404v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.02404v3)
- **Published**: 2019-11-06 14:07:21+00:00
- **Updated**: 2020-02-17 10:44:42+00:00
- **Authors**: Junfeng Hu, Zhencheng Fan, Jun Liao, Li Liu
- **Comment**: Accepted by the 24th European Conference on Artificial Intelligence
- **Journal**: None
- **Summary**: The primary goal of skeletal motion prediction is to generate future motion by observing a sequence of 3D skeletons. A key challenge in motion prediction is the fact that a motion can often be performed in several different ways, with each consisting of its own configuration of poses and their spatio-temporal dependencies, and as a result, the predicted poses often converge to the motionless poses or non-human like motions in long-term prediction. This leads us to define a hierarchical recurrent network model that explicitly characterizes these internal configurations of poses and their local and global spatio-temporal dependencies. The model introduces a latent vector variable from the Lie algebra to represent spatial and temporal relations simultaneously. Furthermore, a structured stack LSTM-based decoder is devised to decode the predicted poses with a new loss function defined to estimate the quantized weight of each body part in a pose. Empirical evaluations on benchmark datasets suggest our approach significantly outperforms the state-of-the-art methods on both short-term and long-term motion prediction.



### Doppler Spectrum Classification with CNNs via Heatmap Location Encoding and a Multi-head Output Layer
- **Arxiv ID**: http://arxiv.org/abs/1911.02407v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.02407v2)
- **Published**: 2019-11-06 14:24:21+00:00
- **Updated**: 2019-11-08 10:13:13+00:00
- **Authors**: Andrew Gilbert, Marit Holden, Line Eikvil, Mariia Rakhmail, Aleksandar Babic, Svein Arne Aase, Eigil Samset, Kristin McLeod
- **Comment**: copyright 2019 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: None
- **Summary**: Spectral Doppler measurements are an important part of the standard echocardiographic examination. These measurements give important insight into myocardial motion and blood flow providing clinicians with parameters for diagnostic decision making. Many of these measurements can currently be performed automatically with high accuracy, increasing the efficiency of the diagnostic pipeline. However, full automation is not yet available because the user must manually select which measurement should be performed on each image. In this work we develop a convolutional neural network (CNN) to automatically classify cardiac Doppler spectra into measurement classes. We show how the multi-modal information in each spectral Doppler recording can be combined using a meta parameter post-processing mapping scheme and heatmaps to encode coordinate locations. Additionally, we experiment with several state-of-the-art network architectures to examine the tradeoff between accuracy and memory usage for resource-constrained environments. Finally, we propose a confidence metric using the values in the last fully connected layer of the network. We analyze example images that fall outside of our proposed classes to show our confidence metric can prevent many misclassifications. Our algorithm achieves 96% accuracy on a test set drawn from a separate clinical site, indicating that the proposed method is suitable for clinical adoption and enabling a fully automatic pipeline from acquisition to Doppler spectrum measurements.



### End to end collision avoidance based on optical flow and neural networks
- **Arxiv ID**: http://arxiv.org/abs/1911.08582v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.08582v1)
- **Published**: 2019-11-06 14:50:17+00:00
- **Updated**: 2019-11-06 14:50:17+00:00
- **Authors**: Jan Blumenkamp
- **Comment**: Technical Report for project work
- **Journal**: None
- **Summary**: Optical flow is believed to play an important role in the agile flight of birds and insects. Even though it is a very simple concept, it is rarely used in computer vision for collision avoidance. This work implements a neural network based collision avoidance which was deployed and evaluated on a solely for this purpose refitted car.



### Automated Left Ventricle Dimension Measurement in 2D Cardiac Ultrasound via an Anatomically Meaningful CNN Approach
- **Arxiv ID**: http://arxiv.org/abs/1911.02448v1
- **DOI**: 10.1007/978-3-030-32875-7_4
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.02448v1)
- **Published**: 2019-11-06 15:50:06+00:00
- **Updated**: 2019-11-06 15:50:06+00:00
- **Authors**: Andrew Gilbert, Marit Holden, Line Eikvil, Svein Arne Aase, Eigil Samset, Kristin McLeod
- **Comment**: Best paper award at Smart Ultrasound Imaging Workshop (SUSI) MICCAI
  2019
- **Journal**: Smart Ultrasound Imaging and Perinatal, Preterm and Paediatric
  Image Analysis, LNCS 11978, pp. 29-37, Springer, 2019
- **Summary**: Two-dimensional echocardiography (2DE) measurements of left ventricle (LV) dimensions are highly significant markers of several cardiovascular diseases. These measurements are often used in clinical care despite suffering from large variability between observers. This variability is due to the challenging nature of accurately finding the correct temporal and spatial location of measurement endpoints in ultrasound images. These images often contain fuzzy boundaries and varying reflection patterns between frames. In this work, we present a convolutional neural network (CNN) based approach to automate 2DE LV measurements. Treating the problem as a landmark detection problem, we propose a modified U-Net CNN architecture to generate heatmaps of likely coordinate locations. To improve the network performance we use anatomically meaningful heatmaps as labels and train with a multi-component loss function. Our network achieves 13.4%, 6%, and 10.8% mean percent error on intraventricular septum (IVS), LV internal dimension (LVID), and LV posterior wall (LVPW) measurements respectively. The design outperforms other networks and matches or approaches intra-analyser expert error.



### J-MoDL: Joint Model-Based Deep Learning for Optimized Sampling and Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1911.02945v4
- **DOI**: 10.1109/JSTSP.2020.3004094
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.02945v4)
- **Published**: 2019-11-06 16:10:45+00:00
- **Updated**: 2020-07-02 19:48:27+00:00
- **Authors**: Hemant Kumar Aggarwal, Mathews Jacob
- **Comment**: None
- **Journal**: IEEE Journal of Selected Topics in Signal Processing, 14(6), 2020
- **Summary**: Modern MRI schemes, which rely on compressed sensing or deep learning algorithms to recover MRI data from undersampled multichannel Fourier measurements, are widely used to reduce scan time. The image quality of these approaches is heavily dependent on the sampling pattern. We introduce a continuous strategy to jointly optimize the sampling pattern and network parameters. We use a multichannel forward model, consisting of a non-uniform Fourier transform with continuously defined sampling locations, to realize the data consistency block within a model-based deep learning image reconstruction scheme. This approach facilitates the joint and continuous optimization of the sampling pattern and the CNN parameters to improve image quality. We observe that the joint optimization of the sampling patterns and the reconstruction module significantly improves the performance of most deep learning reconstruction algorithms. The source code of the proposed joint learning framework is available at https://github.com/hkaggarwal/J-MoDL.



### Towards Large yet Imperceptible Adversarial Image Perturbations with Perceptual Color Distance
- **Arxiv ID**: http://arxiv.org/abs/1911.02466v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.02466v2)
- **Published**: 2019-11-06 16:27:32+00:00
- **Updated**: 2020-03-31 14:16:26+00:00
- **Authors**: Zhengyu Zhao, Zhuoran Liu, Martha Larson
- **Comment**: Accepted at CVPR 2020; Code is available at
  https://github.com/ZhengyuZhao/PerC-Adversarial
- **Journal**: None
- **Summary**: The success of image perturbations that are designed to fool image classifier is assessed in terms of both adversarial effect and visual imperceptibility. The conventional assumption on imperceptibility is that perturbations should strive for tight $L_p$-norm bounds in RGB space. In this work, we drop this assumption by pursuing an approach that exploits human color perception, and more specifically, minimizing perturbation size with respect to perceptual color distance. Our first approach, Perceptual Color distance C&W (PerC-C&W), extends the widely-used C&W approach and produces larger RGB perturbations. PerC-C&W is able to maintain adversarial strength, while contributing to imperceptibility. Our second approach, Perceptual Color distance Alternating Loss (PerC-AL), achieves the same outcome, but does so more efficiently by alternating between the classification loss and perceptual color difference when updating perturbations. Experimental evaluation shows PerC approaches outperform conventional $L_p$ approaches in terms of robustness and transferability, and also demonstrates that the PerC distance can provide added value on top of existing structure-based methods to creating image perturbations.



### A deep learning framework for morphologic detail beyond the diffraction limit in infrared spectroscopic imaging
- **Arxiv ID**: http://arxiv.org/abs/1911.04410v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.04410v2)
- **Published**: 2019-11-06 16:50:27+00:00
- **Updated**: 2019-12-19 16:33:42+00:00
- **Authors**: Kianoush Falahkheirkhah, Kevin Yeh, Shachi Mittal, Luke Pfister, Rohit Bhargava
- **Comment**: corrected typos (the word "lack" was missing in the abstract)
- **Journal**: None
- **Summary**: Infrared (IR) microscopes measure spectral information that quantifies molecular content to assign the identity of biomedical cells but lack the spatial quality of optical microscopy to appreciate morphologic features. Here, we propose a method to utilize the semantic information of cellular identity from IR imaging with the morphologic detail of pathology images in a deep learning-based approach to image super-resolution. Using Generative Adversarial Networks (GANs), we enhance the spatial detail in IR imaging beyond the diffraction limit while retaining their spectral contrast. This technique can be rapidly integrated with modern IR microscopes to provide a framework useful for routine pathology.



### A Programmable Approach to Neural Network Compression
- **Arxiv ID**: http://arxiv.org/abs/1911.02497v2
- **DOI**: 10.1109/MM.2020.3012391
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.02497v2)
- **Published**: 2019-11-06 17:14:32+00:00
- **Updated**: 2020-12-01 22:55:11+00:00
- **Authors**: Vinu Joseph, Saurav Muralidharan, Animesh Garg, Michael Garland, Ganesh Gopalakrishnan
- **Comment**: This is an updated version of a paper published in IEEE Micro, vol.
  40, no. 5, pp. 17-25, Sept.-Oct. 2020 at
  https://ieeexplore.ieee.org/document/9151283
- **Journal**: IEEE Micro, Volume: 40, Issue: 5, Sept.-Oct. 2020, pp. 17-25
- **Summary**: Deep neural networks (DNNs) frequently contain far more weights, represented at a higher precision, than are required for the specific task which they are trained to perform. Consequently, they can often be compressed using techniques such as weight pruning and quantization that reduce both the model size and inference time without appreciable loss in accuracy. However, finding the best compression strategy and corresponding target sparsity for a given DNN, hardware platform, and optimization objective currently requires expensive, frequently manual, trial-and-error experimentation. In this paper, we introduce a programmable system for model compression called Condensa. Users programmatically compose simple operators, in Python, to build more complex and practically interesting compression strategies. Given a strategy and user-provided objective (such as minimization of running time), Condensa uses a novel Bayesian optimization-based algorithm to automatically infer desirable sparsities. Our experiments on four real-world DNNs demonstrate memory footprint and hardware runtime throughput improvements of 188x and 2.59x, respectively, using at most ten samples per search. We have released a reference implementation of Condensa at https://github.com/NVlabs/condensa.



### AIM 2019 Challenge on Image Demoireing: Dataset and Study
- **Arxiv ID**: http://arxiv.org/abs/1911.02498v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.02498v1)
- **Published**: 2019-11-06 17:15:52+00:00
- **Updated**: 2019-11-06 17:15:52+00:00
- **Authors**: Shanxin Yuan, Radu Timofte, Gregory Slabaugh, Ales Leonardis
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a novel dataset, called LCDMoire, which was created for the first-ever image demoireing challenge that was part of the Advances in Image Manipulation (AIM) workshop, held in conjunction with ICCV 2019. The dataset comprises 10,200 synthetically generated image pairs (consisting of an image degraded by moire and a clean ground truth image). In addition to describing the dataset and its creation, this paper also reviews the challenge tracks, competition, and results, the latter summarizing the current state-of-the-art on this dataset.



### Machine Learning Techniques for Biomedical Image Segmentation: An Overview of Technical Aspects and Introduction to State-of-Art Applications
- **Arxiv ID**: http://arxiv.org/abs/1911.02521v1
- **DOI**: 10.1002/mp.13649
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.02521v1)
- **Published**: 2019-11-06 17:59:39+00:00
- **Updated**: 2019-11-06 17:59:39+00:00
- **Authors**: Hyunseok Seo, Masoud Badiei Khuzani, Varun Vasudevan, Charles Huang, Hongyi Ren, Ruoxiu Xiao, Xiao Jia, Lei Xing
- **Comment**: Accept for publication at Medical Physics
- **Journal**: None
- **Summary**: In recent years, significant progress has been made in developing more accurate and efficient machine learning algorithms for segmentation of medical and natural images. In this review article, we highlight the imperative role of machine learning algorithms in enabling efficient and accurate segmentation in the field of medical imaging. We specifically focus on several key studies pertaining to the application of machine learning methods to biomedical image segmentation. We review classical machine learning algorithms such as Markov random fields, k-means clustering, random forest, etc. Although such classical learning models are often less accurate compared to the deep learning techniques, they are often more sample efficient and have a less complex structure. We also review different deep learning architectures, such as the artificial neural networks (ANNs), the convolutional neural networks (CNNs), and the recurrent neural networks (RNNs), and present the segmentation results attained by those learning models that were published in the past three years. We highlight the successes and limitations of each machine learning paradigm. In addition, we discuss several challenges related to the training of different machine learning models, and we present some heuristics to address those challenges.



### SCL: Towards Accurate Domain Adaptive Object Detection via Gradient Detach Based Stacked Complementary Losses
- **Arxiv ID**: http://arxiv.org/abs/1911.02559v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.02559v3)
- **Published**: 2019-11-06 18:59:01+00:00
- **Updated**: 2019-11-21 05:43:14+00:00
- **Authors**: Zhiqiang Shen, Harsh Maheshwari, Weichen Yao, Marios Savvides
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain adaptive object detection aims to learn a robust detector in the domain shift circumstance, where the training (source) domain is label-rich with bounding box annotations, while the testing (target) domain is label-agnostic and the feature distributions between training and testing domains are dissimilar or even totally different. In this paper, we propose a gradient detach based stacked complementary losses (SCL) method that uses detection losses as the primary objective, and cuts in several auxiliary losses in different network stages accompanying with gradient detach training to learn more discriminative representations. We argue that the prior methods mainly leverage more loss functions for training but ignore the interaction of different losses and also the compatible training strategy (gradient detach updating in our work). Thus, our proposed method is a more syncretic adaptation learning process. We conduct comprehensive experiments on seven datasets, the results demonstrate that our method performs favorably better than the state-of-the-art methods by a significant margin. For instance, from Cityscapes to FoggyCityscapes, we achieve 37.9% mAP, outperforming the previous art Strong-Weak by 3.6%.



### Argoverse: 3D Tracking and Forecasting with Rich Maps
- **Arxiv ID**: http://arxiv.org/abs/1911.02620v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1911.02620v1)
- **Published**: 2019-11-06 20:27:27+00:00
- **Updated**: 2019-11-06 20:27:27+00:00
- **Authors**: Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter Carr, Simon Lucey, Deva Ramanan, James Hays
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: We present Argoverse -- two datasets designed to support autonomous vehicle machine learning tasks such as 3D tracking and motion forecasting. Argoverse was collected by a fleet of autonomous vehicles in Pittsburgh and Miami. The Argoverse 3D Tracking dataset includes 360 degree images from 7 cameras with overlapping fields of view, 3D point clouds from long range LiDAR, 6-DOF pose, and 3D track annotations. Notably, it is the only modern AV dataset that provides forward-facing stereo imagery. The Argoverse Motion Forecasting dataset includes more than 300,000 5-second tracked scenarios with a particular vehicle identified for trajectory forecasting. Argoverse is the first autonomous vehicle dataset to include "HD maps" with 290 km of mapped lanes with geometric and semantic metadata. All data is released under a Creative Commons license at www.argoverse.org. In our baseline experiments, we illustrate how detailed map information such as lane direction, driveable area, and ground height improves the accuracy of 3D object tracking and motion forecasting. Our tracking and forecasting experiments represent only an initial exploration of the use of rich maps in robotic perception. We hope that Argoverse will enable the research community to explore these problems in greater depth.



### What Do We Really Need? Degenerating U-Net on Retinal Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.02660v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.02660v1)
- **Published**: 2019-11-06 22:49:55+00:00
- **Updated**: 2019-11-06 22:49:55+00:00
- **Authors**: Weilin Fu, Katharina Breininger, Zhaoya Pan, Andreas Maier
- **Comment**: 7 pages, 2 figures, submitted in BVM 2020
- **Journal**: None
- **Summary**: Retinal vessel segmentation is an essential step for fundus image analysis. With the recent advances of deep learning technologies, many convolutional neural networks have been applied in this field, including the successful U-Net. In this work, we firstly modify the U-Net with functional blocks aiming to pursue higher performance. The absence of the expected performance boost then lead us to dig into the opposite direction of shrinking the U-Net and exploring the extreme conditions such that its segmentation performance is maintained. Experiment series to simplify the network structure, reduce the network size and restrict the training conditions are designed. Results show that for retinal vessel segmentation on DRIVE database, U-Net does not degenerate until surprisingly acute conditions: one level, one filter in convolutional layers, and one training sample. This experimental discovery is both counter-intuitive and worthwhile. Not only are the extremes of the U-Net explored on a well-studied application, but also one intriguing warning is raised for the research methodology which seeks for marginal performance enhancement regardless of the resource cost.



### Shaping Visual Representations with Language for Few-shot Classification
- **Arxiv ID**: http://arxiv.org/abs/1911.02683v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1911.02683v2)
- **Published**: 2019-11-06 23:47:32+00:00
- **Updated**: 2020-06-08 18:35:31+00:00
- **Authors**: Jesse Mu, Percy Liang, Noah Goodman
- **Comment**: ACL 2020. Version 1 appeared at the NeurIPS 2019 Workshop on Visually
  Grounded Interaction and Language (ViGIL)
- **Journal**: None
- **Summary**: By describing the features and abstractions of our world, language is a crucial tool for human learning and a promising source of supervision for machine learning models. We use language to improve few-shot visual classification in the underexplored scenario where natural language task descriptions are available during training, but unavailable for novel tasks at test time. Existing models for this setting sample new descriptions at test time and use those to classify images. Instead, we propose language-shaped learning (LSL), an end-to-end model that regularizes visual representations to predict language. LSL is conceptually simpler, more data efficient, and outperforms baselines in two challenging few-shot domains.



