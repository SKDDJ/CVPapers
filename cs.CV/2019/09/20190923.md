# Arxiv Papers in cs.CV on 2019-09-23
### Field typing for improved recognition on heterogeneous handwritten forms
- **Arxiv ID**: http://arxiv.org/abs/1909.10120v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10120v1)
- **Published**: 2019-09-23 01:29:58+00:00
- **Updated**: 2019-09-23 01:29:58+00:00
- **Authors**: Ciprian Tomoiaga, Paul Feng, Mathieu Salzmann, Patrick Jayet
- **Comment**: None
- **Journal**: None
- **Summary**: Offline handwriting recognition has undergone continuous progress over the past decades. However, existing methods are typically benchmarked on free-form text datasets that are biased towards good-quality images and handwriting styles, and homogeneous content. In this paper, we show that state-of-the-art algorithms, employing long short-term memory (LSTM) layers, do not readily generalize to real-world structured documents, such as forms, due to their highly heterogeneous and out-of-vocabulary content, and to the inherent ambiguities of this content. To address this, we propose to leverage the content type within an LSTM-based architecture. Furthermore, we introduce a procedure to generate synthetic data to train this architecture without requiring expensive manual annotations. We demonstrate the effectiveness of our approach at transcribing text on a challenging, real-world dataset of European Accident Statements.



### Explainable High-order Visual Question Reasoning: A New Benchmark and Knowledge-routed Network
- **Arxiv ID**: http://arxiv.org/abs/1909.10128v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10128v1)
- **Published**: 2019-09-23 02:38:56+00:00
- **Updated**: 2019-09-23 02:38:56+00:00
- **Authors**: Qingxing Cao, Bailin Li, Xiaodan Liang, Liang Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Explanation and high-order reasoning capabilities are crucial for real-world visual question answering with diverse levels of inference complexity (e.g., what is the dog that is near the girl playing with?) and important for users to understand and diagnose the trustworthiness of the system. Current VQA benchmarks on natural images with only an accuracy metric end up pushing the models to exploit the dataset biases and cannot provide any interpretable justification, which severally hinders advances in high-level question answering. In this work, we propose a new HVQR benchmark for evaluating explainable and high-order visual question reasoning ability with three distinguishable merits: 1) the questions often contain one or two relationship triplets, which requires the model to have the ability of multistep reasoning to predict plausible answers; 2) we provide an explicit evaluation on a multistep reasoning process that is constructed with image scene graphs and commonsense knowledge bases; and 3) each relationship triplet in a large-scale knowledge base only appears once among all questions, which poses challenges for existing networks that often attempt to overfit the knowledge base that already appears in the training set and enforces the models to handle unseen questions and knowledge fact usage. We also propose a new knowledge-routed modular network (KM-net) that incorporates the multistep reasoning process over a large knowledge base into visual question reasoning. An extensive dataset analysis and comparisons with existing models on the HVQR benchmark show that our benchmark provides explainable evaluations, comprehensive reasoning requirements and realistic challenges of VQA systems, as well as our KM-net's superiority in terms of accuracy and explanation ability.



### Validation of image-guided cochlear implant programming techniques
- **Arxiv ID**: http://arxiv.org/abs/1909.10137v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1909.10137v2)
- **Published**: 2019-09-23 03:16:27+00:00
- **Updated**: 2020-07-13 14:13:50+00:00
- **Authors**: Yiyuan Zhao, Jianing Wang, Rui Li, Robert F. Labadie, Benoit M. Dawant, Jack H. Noble
- **Comment**: 37 pages, 12 figures, 7 tables
- **Journal**: None
- **Summary**: Cochlear implants (CIs) are a standard treatment for patients who experience severe to profound hearing loss. Recent studies have shown that hearing outcome is correlated with intra-cochlear anatomy and electrode placement. Our group has developed image-guided CI programming (IGCIP) techniques that use image analysis methods to both segment the inner ear structures in pre- or post-implantation CT images and localize the CI electrodes in post-implantation CT images. This permits to assist audiologists with CI programming by suggesting which among the contacts should be deactivated to reduce electrode interaction that is known to affect outcomes. Clinical studies have shown that IGCIP can improve hearing outcomes for CI recipients. However, the sensitivity of IGCIP with respect to the accuracy of the two major steps: electrode localization and intra-cochlear anatomy segmentation, is unknown. In this article, we create a ground truth dataset with conventional CT and micro-CT images of 35 temporal bone specimens to both rigorously characterize the accuracy of these two steps and assess how inaccuracies in these steps affect the overall results. Our study results show that when clinical pre- and post-implantation CTs are available, IGCIP produces results that are comparable to those obtained with the corresponding ground truth in 86.7% of the subjects tested. When only post-implantation CTs are available, this number is 83.3%. These results suggest that our current method is robust to errors in segmentation and localization but also that it can be improved upon.   Keywords: cochlear implant, ground truth, segmentation, validation



### Robust Local Features for Improving the Generalization of Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1909.10147v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10147v5)
- **Published**: 2019-09-23 04:19:34+00:00
- **Updated**: 2020-02-02 13:54:45+00:00
- **Authors**: Chuanbiao Song, Kun He, Jiadong Lin, Liwei Wang, John E. Hopcroft
- **Comment**: accepted by ICLR 2020
- **Journal**: None
- **Summary**: Adversarial training has been demonstrated as one of the most effective methods for training robust models to defend against adversarial examples. However, adversarially trained models often lack adversarially robust generalization on unseen testing data. Recent works show that adversarially trained models are more biased towards global structure features. Instead, in this work, we would like to investigate the relationship between the generalization of adversarial training and the robust local features, as the robust local features generalize well for unseen shape variation. To learn the robust local features, we develop a Random Block Shuffle (RBS) transformation to break up the global structure features on normal adversarial examples. We continue to propose a new approach called Robust Local Features for Adversarial Training (RLFAT), which first learns the robust local features by adversarial training on the RBS-transformed adversarial examples, and then transfers the robust local features into the training of normal adversarial examples. To demonstrate the generality of our argument, we implement RLFAT in currently state-of-the-art adversarial training frameworks. Extensive experiments on STL-10, CIFAR-10 and CIFAR-100 show that RLFAT significantly improves both the adversarially robust generalization and the standard generalization of adversarial training. Additionally, we demonstrate that our models capture more local features of the object on the images, aligning better with human perception.



### Smooth Extrapolation of Unknown Anatomy via Statistical Shape Models
- **Arxiv ID**: http://arxiv.org/abs/1909.10153v1
- **DOI**: 10.1117/12.2081310
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.10153v1)
- **Published**: 2019-09-23 04:40:10+00:00
- **Updated**: 2019-09-23 04:40:10+00:00
- **Authors**: Robert Grupp, Hsin-Hong Chiang, Yoshito Otake, Ryan Murphy, Chad Gordon, Mehran Armand, Russell Taylor
- **Comment**: SPIE Medical Imaging Conference 2015 Paper
- **Journal**: In Medical Imaging 2015: Image-Guided Procedures, Robotic
  Interventions, and Modeling 2015 Mar 18 (Vol. 9415, p. 941524). International
  Society for Optics and Photonics
- **Summary**: Several methods to perform extrapolation of unknown anatomy were evaluated. The primary application is to enhance surgical procedures that may use partial medical images or medical images of incomplete anatomy. Le Fort-based, face-jaw-teeth transplant is one such procedure. From CT data of 36 skulls and 21 mandibles separate Statistical Shape Models of the anatomical surfaces were created. Using the Statistical Shape Models, incomplete surfaces were projected to obtain complete surface estimates. The surface estimates exhibit non-zero error in regions where the true surface is known; it is desirable to keep the true surface and seamlessly merge the estimated unknown surface. Existing extrapolation techniques produce non-smooth transitions from the true surface to the estimated surface, resulting in additional error and a less aesthetically pleasing result. The three extrapolation techniques evaluated were: copying and pasting of the surface estimate (non-smooth baseline), a feathering between the patient surface and surface estimate, and an estimate generated via a Thin Plate Spline trained from displacements between the surface estimate and corresponding vertices of the known patient surface. Feathering and Thin Plate Spline approaches both yielded smooth transitions. However, feathering corrupted known vertex values. Leave-one-out analyses were conducted, with 5% to 50% of known anatomy removed from the left-out patient and estimated via the proposed approaches. The Thin Plate Spline approach yielded smaller errors than the other two approaches, with an average vertex error improvement of 1.46 mm and 1.38 mm for the skull and mandible respectively, over the baseline approach.



### Deep Local Global Refinement Network for Stent Analysis in IVOCT Images
- **Arxiv ID**: http://arxiv.org/abs/1909.10169v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.10169v1)
- **Published**: 2019-09-23 05:50:38+00:00
- **Updated**: 2019-09-23 05:50:38+00:00
- **Authors**: Yuyu Guo
- **Comment**: 8 pages,5 figures, MICCAI 2019
- **Journal**: None
- **Summary**: Implantation of stents into coronary arteries is a common treatment option for patients with cardiovascular disease. Assessment of safety and efficacy of the stent implantation occurs via manual visual inspection of the neointimal coverage from intravascular optical coherence tomography (IVOCT) images. However, such manual assessment requires the detection of thousands of strut points within the stent. This is a challenging, tedious, and time-consuming task because the strut points usually appear as small, irregular shaped objects with inhomogeneous textures, and are often occluded by shadows, artifacts, and vessel walls. Conventional methods based on textures, edge detection, or simple classifiers for automated detection of strut points in IVOCT images have low recall and precision as they are, unable to adequately represent the visual features of the strut point for detection. In this study, we propose a local-global refinement network to integrate local-patch content with global content for strut points detection from IVOCT images. Our method densely detects the potential strut points in local image patches and then refines them according to global appearance constraints to reduce false positives. Our experimental results on a clinical dataset of 7,000 IVOCT images demonstrated that our method outperformed the state-of-the-art methods with a recall of 0.92 and precision of 0.91 for strut points detection.



### Retrieval-based Localization Based on Domain-invariant Feature Learning under Changing Environments
- **Arxiv ID**: http://arxiv.org/abs/1909.10184v1
- **DOI**: 10.1109/IROS40897.2019.8968047
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10184v1)
- **Published**: 2019-09-23 06:47:37+00:00
- **Updated**: 2019-09-23 06:47:37+00:00
- **Authors**: Hanjiang Hu, Hesheng Wang, Zhe Liu, Chenguang Yang, Weidong Chen, Le Xie
- **Comment**: Accepted by 2019 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2019)
- **Journal**: None
- **Summary**: Visual localization is a crucial problem in mobile robotics and autonomous driving. One solution is to retrieve images with known pose from a database for the localization of query images. However, in environments with drastically varying conditions (e.g. illumination changes, seasons, occlusion, dynamic objects), retrieval-based localization is severely hampered and becomes a challenging problem. In this paper, a novel domain-invariant feature learning method (DIFL) is proposed based on ComboGAN, a multi-domain image translation network architecture. By introducing a feature consistency loss (FCL) between the encoded features of the original image and translated image in another domain, we are able to train the encoders to generate domain-invariant features in a self-supervised manner. To retrieve a target image from the database, the query image is first encoded using the encoder belonging to the query domain to obtain a domain-invariant feature vector. We then preform retrieval by selecting the database image with the most similar domain-invariant feature vector. We validate the proposed approach on the CMU-Seasons dataset, where we outperform state-of-the-art learning-based descriptors in retrieval-based localization for high and medium precision scenarios.



### Learning Coupled Spatial-temporal Attention for Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1909.10214v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10214v1)
- **Published**: 2019-09-23 08:30:11+00:00
- **Updated**: 2019-09-23 08:30:11+00:00
- **Authors**: Jiayun Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a coupled spatial-temporal attention (CSTA) model for skeleton-based action recognition, which aims to figure out the most discriminative joints and frames in spatial and temporal domains simultaneously. Conventional approaches usually consider all the joints or frames in a skeletal sequence equally important, which are unrobust to ambiguous and redundant information. To address this, we first learn two sets of weights for different joints and frames through two subnetworks respectively, which enable the model to have the ability of "paying attention to" the relatively informative section. Then, we calculate the cross product based on the weights of joints and frames for the coupled spatial-temporal attention. Moreover, our CSTA mechanisms can be easily plugged into existing hierarchical CNN models (CSTA-CNN) to realize their function. Extensive experimental results on the recently collected UESTC dataset and the currently largest NTU dataset have shown the effectiveness of our proposed method for skeleton-based action recognition.



### WiCV 2019: The Sixth Women In Computer Vision Workshop
- **Arxiv ID**: http://arxiv.org/abs/1909.10225v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10225v1)
- **Published**: 2019-09-23 08:52:33+00:00
- **Updated**: 2019-09-23 08:52:33+00:00
- **Authors**: Irene Amerini, Elena Balashova, Sayna Ebrahimi, Kathryn Leonard, Arsha Nagrani, Amaia Salvador
- **Comment**: Report of the Sixth Women In Computer Vision Workshop
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR) Workshops, 2019, pp. 0-0
- **Summary**: In this paper we present the Women in Computer Vision Workshop - WiCV 2019, organized in conjunction with CVPR 2019. This event is meant for increasing the visibility and inclusion of women researchers in the computer vision field. Computer vision and machine learning have made incredible progress over the past years, but the number of female researchers is still low both in academia and in industry. WiCV is organized especially for the following reason: to raise visibility of female researchers, to increase collaborations between them, and to provide mentorship to female junior researchers in the field. In this paper, we present a report of trends over the past years, along with a summary of statistics regarding presenters, attendees, and sponsorship for the current workshop.



### Deep Convolutions for In-Depth Automated Rock Typing
- **Arxiv ID**: http://arxiv.org/abs/1909.10227v3
- **DOI**: 10.1016/j.cageo.2019.104330
- **Categories**: **cs.CV**, cs.LG, I.4.8; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/1909.10227v3)
- **Published**: 2019-09-23 08:55:36+00:00
- **Updated**: 2019-09-26 08:13:09+00:00
- **Authors**: E. E. Baraboshkin, L. S. Ismailova, D. M. Orlov, E. A. Zhukovskaya, G. A. Kalmykov, O. V. Khotylev, E. Yu. Baraboshkin, D. A. Koroteev
- **Comment**: 25 pages, 9 figures, 3 tables, submitted to Computers and Geosciences
  Journal. Keywords: Core Image; Description; Convolutional Neural Networks;
  Representation; Geology; Lithotypes
- **Journal**: None
- **Summary**: The description of rocks is one of the most time-consuming tasks in the everyday work of a geologist, especially when very accurate description is required. We here present a method that reduces the time needed for accurate description of rocks, enabling the geologist to work more efficiently. We describe the application of methods based on color distribution analysis and feature extraction. Then we focus on a new approach, used by us, which is based on convolutional neural networks. We used several well-known neural network architectures (AlexNet, VGG, GoogLeNet, ResNet) and made a comparison of their performance. The precision of the algorithms is up to 95% on the validation set with GoogLeNet architecture. The best of the proposed algorithms can describe 50 m of full-size core in one minute.



### Scheduled Differentiable Architecture Search for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1909.10236v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10236v1)
- **Published**: 2019-09-23 09:19:57+00:00
- **Updated**: 2019-09-23 09:19:57+00:00
- **Authors**: Zhaofan Qiu, Ting Yao, Yiheng Zhang, Yongdong Zhang, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) have been regarded as a capable class of models for visual recognition problems. Nevertheless, it is not trivial to develop generic and powerful network architectures, which requires significant efforts of human experts. In this paper, we introduce a new idea for automatically exploring architectures on a remould of Differentiable Architecture Search (DAS), which possesses the efficient search via gradient descent. Specifically, we present Scheduled Differentiable Architecture Search (SDAS) for both image and video recognition that nicely integrates the selection of operations during training with a schedule. Technically, an architecture or a cell is represented as a directed graph. Our SDAS gradually fixes the operations on the edges in the graph in a progressive and scheduled manner, as opposed to a one-step decision of operations for all the edges once the training completes in existing DAS, which may make the architecture brittle. Moreover, we enlarge the search space of SDAS particularly for video recognition by devising several unique operations to encode spatio-temporal dynamics and demonstrate the impact in affecting the architecture search of SDAS. Extensive experiments of architecture learning are conducted on CIFAR10, Kinetics10, UCF101 and HMDB51 datasets, and superior results are reported when comparing to DAS method. More remarkably, the search by our SDAS is around 2-fold faster than DAS. When transferring the learnt cells on CIFAR10 and Kinetics10 respectively to large-scale ImageNet and Kinetics400 datasets, the constructed network also outperforms several state-of-the-art hand-crafted structures.



### Large Scale Joint Semantic Re-Localisation and Scene Understanding via Globally Unique Instance Coordinate Regression
- **Arxiv ID**: http://arxiv.org/abs/1909.10239v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10239v1)
- **Published**: 2019-09-23 09:26:27+00:00
- **Updated**: 2019-09-23 09:26:27+00:00
- **Authors**: Ignas Budvytis, Marvin Teichmann, Tomas Vojir, Roberto Cipolla
- **Comment**: BMVC 2019
- **Journal**: None
- **Summary**: In this work we present a novel approach to joint semantic localisation and scene understanding. Our work is motivated by the need for localisation algorithms which not only predict 6-DoF camera pose but also simultaneously recognise surrounding objects and estimate 3D geometry. Such capabilities are crucial for computer vision guided systems which interact with the environment: autonomous driving, augmented reality and robotics. In particular, we propose a two step procedure. During the first step we train a convolutional neural network to jointly predict per-pixel globally unique instance labels and corresponding local coordinates for each instance of a static object (e.g. a building). During the second step we obtain scene coordinates by combining object center coordinates and local coordinates and use them to perform 6-DoF camera pose estimation. We evaluate our approach on real world (CamVid-360) and artificial (SceneCity) autonomous driving datasets. We obtain smaller mean distance and angular errors than state-of-the-art 6-DoF pose estimation algorithms based on direct pose regression and pose estimation from scene coordinates on all datasets. Our contributions include: (i) a novel formulation of scene coordinate regression as two separate tasks of object instance recognition and local coordinate regression and a demonstration that our proposed solution allows to predict accurate 3D geometry of static objects and estimate 6-DoF pose of camera on (ii) maps larger by several orders of magnitude than previously attempted by scene coordinate regression methods, as well as on (iii) lightweight, approximate 3D maps built from 3D primitives such as building-aligned cuboids.



### Predicting Landscapes from Environmental Conditions Using Generative Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.10296v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.10296v1)
- **Published**: 2019-09-23 11:24:52+00:00
- **Updated**: 2019-09-23 11:24:52+00:00
- **Authors**: Christian Requena-Mesa, Markus Reichstein, Miguel Mahecha, Basil Kraft, Joachim Denzler
- **Comment**: Accepted conference paper at GCPR2019
- **Journal**: None
- **Summary**: Landscapes are meaningful ecological units that strongly depend on the environmental conditions. Such dependencies between landscapes and the environment have been noted since the beginning of Earth sciences and cast into conceptual models describing the interdependencies of climate, geology, vegetation and geomorphology. Here, we ask whether landscapes, as seen from space, can be statistically predicted from pertinent environmental conditions. To this end we adapted a deep learning generative model in order to establish the relationship between the environmental conditions and the view of landscapes from the Sentinel-2 satellite. We trained a conditional generative adversarial network to generate multispectral imagery given a set of climatic, terrain and anthropogenic predictors. The generated imagery of the landscapes share many characteristics with the real one. Results based on landscape patch metrics, indicative of landscape composition and structure, show that the proposed generative model creates landscapes that are more similar to the targets than the baseline models while overall reflectance and vegetation cover are predicted better. We demonstrate that for many purposes the generated landscapes behave as real with immediate application for global change studies. We envision the application of machine learning as a tool to forecast the effects of climate change on the spatial features of landscapes, while we assess its limitations and breaking points.



### Where to Look Next: Unsupervised Active Visual Exploration on 360° Input
- **Arxiv ID**: http://arxiv.org/abs/1909.10304v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.10304v2)
- **Published**: 2019-09-23 11:50:46+00:00
- **Updated**: 2019-11-28 10:38:02+00:00
- **Authors**: Soroush Seifi, Tinne Tuytelaars
- **Comment**: Oral Presentation and best Paper Award at 360 Perception and
  Interaction Workshop at ICCV 2019
- **Journal**: None
- **Summary**: We address the problem of active visual exploration of large 360{\deg} inputs. In our setting an active agent with a limited camera bandwidth explores its 360{\deg} environment by changing its viewing direction at limited discrete time steps. As such, it observes the world as a sequence of narrow field-of-view 'glimpses', deciding for itself where to look next. Our proposed method exceeds previous works' performance by a significant margin without the need for deep reinforcement learning or training separate networks as sidekicks. A key component of our system are the spatial memory maps that make the system aware of the glimpses' orientations (locations in the 360{\deg} image). Further, we stress the advantages of retina-like glimpses when the agent's sensor bandwidth and time-steps are limited. Finally, we use our trained model to do classification of the whole scene using only the information observed in the glimpses.



### Deep Multi-Facial patches Aggregation Network for Expression Classification from Face Images
- **Arxiv ID**: http://arxiv.org/abs/1909.10305v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.10305v2)
- **Published**: 2019-09-23 11:52:27+00:00
- **Updated**: 2020-02-24 11:52:19+00:00
- **Authors**: Amine Djerghri, Ahmed Rachid Hazourli, Alice Othmani
- **Comment**: we have a new version of the paper arXiv:2002.09298
- **Journal**: None
- **Summary**: Emotional Intelligence in Human-Computer Interaction has attracted increasing attention from researchers in multidisciplinary research fields including psychology, computer vision, neuroscience, artificial intelligence, and related disciplines. Human prone to naturally interact with computers face-to-face. Human Expressions is an important key to better link human and computers. Thus, designing interfaces able to understand human expressions and emotions can improve Human-Computer Interaction (HCI) for better communication. In this paper, we investigate HCI via a deep multi-facial patches aggregation network for Face Expression Recognition (FER). Deep features are extracted from facial parts and aggregated for expression classification. Several problems may affect the performance of the proposed framework like the small size of FER datasets and the high number of parameters to learn. For That, two data augmentation techniques are proposed for facial expression generation to expand the labeled training. The proposed framework is evaluated on the extended Cohn-Konade dataset (CK+) and promising results are achieved.



### Human Synthesis and Scene Compositing
- **Arxiv ID**: http://arxiv.org/abs/1909.10307v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10307v2)
- **Published**: 2019-09-23 11:59:05+00:00
- **Updated**: 2019-10-18 13:35:52+00:00
- **Authors**: Mihai Zanfir, Elisabeta Oneata, Alin-Ionut Popa, Andrei Zanfir, Cristian Sminchisescu
- **Comment**: None
- **Journal**: None
- **Summary**: Generating good quality and geometrically plausible synthetic images of humans with the ability to control appearance, pose and shape parameters, has become increasingly important for a variety of tasks ranging from photo editing, fashion virtual try-on, to special effects and image compression. In this paper, we propose HUSC, a HUman Synthesis and Scene Compositing framework for the realistic synthesis of humans with different appearance, in novel poses and scenes. Central to our formulation is 3d reasoning for both people and scenes, in order to produce realistic collages, by correctly modeling perspective effects and occlusion, by taking into account scene semantics and by adequately handling relative scales. Conceptually our framework consists of three components: (1) a human image synthesis model with controllable pose and appearance, based on a parametric representation, (2) a person insertion procedure that leverages the geometry and semantics of the 3d scene, and (3) an appearance compositing process to create a seamless blending between the colors of the scene and the generated human image, and avoid visual artifacts. The performance of our framework is supported by both qualitative and quantitative results, in particular state-of-the art synthesis scores for the DeepFashion dataset.



### How to improve CNN-based 6-DoF camera pose estimation
- **Arxiv ID**: http://arxiv.org/abs/1909.10312v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.10312v2)
- **Published**: 2019-09-23 12:12:17+00:00
- **Updated**: 2019-11-28 10:38:42+00:00
- **Authors**: Soroush Seifi, Tinne Tuytelaars
- **Comment**: Accepted at Deep Learning for Visual SLAM workshop at ICCV 2019
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) and transfer learning have recently been used for 6 degrees of freedom (6-DoF) camera pose estimation. While they do not reach the same accuracy as visual SLAM-based approaches and are restricted to a specific environment, they excel in robustness and can be applied even to a single image. In this paper, we study PoseNet [1] and investigate modifications based on datasets' characteristics to improve the accuracy of the pose estimates. In particular, we emphasize the importance of field-of-view over image resolution; we present a data augmentation scheme to reduce overfitting; we study the effect of Long-Short-Term-Memory (LSTM) cells. Lastly, we combine these modifications and improve PoseNet's performance for monocular CNN based camera pose regression.



### Object Segmentation using Pixel-wise Adversarial Loss
- **Arxiv ID**: http://arxiv.org/abs/1909.10341v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.10341v1)
- **Published**: 2019-09-23 12:52:54+00:00
- **Updated**: 2019-09-23 12:52:54+00:00
- **Authors**: Ricard Durall, Franz-Josef Pfreundt, Ullrich Köthe, Janis Keuper
- **Comment**: None
- **Journal**: None
- **Summary**: Recent deep learning based approaches have shown remarkable success on object segmentation tasks. However, there is still room for further improvement. Inspired by generative adversarial networks, we present a generic end-to-end adversarial approach, which can be combined with a wide range of existing semantic segmentation networks to improve their segmentation performance. The key element of our method is to replace the commonly used binary adversarial loss with a high resolution pixel-wise loss. In addition, we train our generator employing stochastic weight averaging fashion, which further enhances the predicted output label maps leading to state-of-the-art results. We show, that this combination of pixel-wise adversarial training and weight averaging leads to significant and consistent gains in segmentation performance, compared to the baseline models.



### RAUNet: Residual Attention U-Net for Semantic Segmentation of Cataract Surgical Instruments
- **Arxiv ID**: http://arxiv.org/abs/1909.10360v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10360v3)
- **Published**: 2019-09-23 13:34:00+00:00
- **Updated**: 2019-10-02 09:24:54+00:00
- **Authors**: Zhen-Liang Ni, Gui-Bin Bian, Xiao-Hu Zhou, Zeng-Guang Hou, Xiao-Liang Xie, Chen Wang, Yan-Jie Zhou, Rui-Qi Li, Zhen Li
- **Comment**: Accepted by the 26th International Conference on Neural Information
  Processing (ICONIP2019). arXiv admin note: cs.CV => eess.IV cs.CV
- **Journal**: None
- **Summary**: Semantic segmentation of surgical instruments plays a crucial role in robot-assisted surgery. However, accurate segmentation of cataract surgical instruments is still a challenge due to specular reflection and class imbalance issues. In this paper, an attention-guided network is proposed to segment the cataract surgical instrument. A new attention module is designed to learn discriminative features and address the specular reflection issue. It captures global context and encodes semantic dependencies to emphasize key semantic features, boosting the feature representation. This attention module has very few parameters, which helps to save memory. Thus, it can be flexibly plugged into other networks. Besides, a hybrid loss is introduced to train our network for addressing the class imbalance issue, which merges cross entropy and logarithms of Dice loss. A new dataset named Cata7 is constructed to evaluate our network. To the best of our knowledge, this is the first cataract surgical instrument dataset for semantic segmentation. Based on this dataset, RAUNet achieves state-of-the-art performance 97.71% mean Dice and 95.62% mean IOU.



### HR-CAM: Precise Localization of Pathology Using Multi-level Learning in CNNs
- **Arxiv ID**: http://arxiv.org/abs/1909.12919v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12919v1)
- **Published**: 2019-09-23 13:47:12+00:00
- **Updated**: 2019-09-23 13:47:12+00:00
- **Authors**: Sumeet Shinde, Tanay Chougule, Jitender Saini, Madhura Ingalhalikar
- **Comment**: Medical Image Computing and Computer Assisted Intervention, 2019
- **Journal**: None
- **Summary**: We propose a CNN based technique that aggregates feature maps from its multiple layers that can localize abnormalities with greater details as well as predict pathology under consideration. Existing class activation mapping (CAM) techniques extract feature maps from either the final layer or a single intermediate layer to create the discriminative maps and then interpolate to upsample to the original image resolution. In this case, the subject specific localization is coarse and is unable to capture subtle abnormalities. To mitigate this, our method builds a novel CNN based discriminative localization model that we call high resolution CAM (HR-CAM), which accounts for layers from each resolution, therefore facilitating a comprehensive map that can delineate the pathology for each subject by combining low-level, intermediate as well as high-level features from the CNN. Moreover, our model directly provides the discriminative map in the resolution of the original image facilitating finer delineation of abnormalities. We demonstrate the working of our model on a simulated abnormalities data where we illustrate how the model captures finer details in the final discriminative maps as compared to current techniques. We then apply this technique: (1) to classify ependymomas from grade IV glioblastoma on T1-weighted contrast enhanced (T1-CE) MRI and (2) to predict Parkinson's disease from neuromelanin sensitive MRI. In all these cases we demonstrate that our model not only predicts pathologies with high accuracies, but also creates clinically interpretable subject specific high resolution discriminative localizations. Overall, the technique can be generalized to any CNN and carries high relevance in a clinical setting.



### Shadow Transfer: Single Image Relighting For Urban Road Scenes
- **Arxiv ID**: http://arxiv.org/abs/1909.10363v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10363v2)
- **Published**: 2019-09-23 13:47:38+00:00
- **Updated**: 2019-09-26 17:08:30+00:00
- **Authors**: Alexandra Carlson, Ram Vasudevan, Matthew Johnson-Roberson
- **Comment**: None
- **Journal**: None
- **Summary**: Illumination effects in images, specifically cast shadows and shading, have been shown to decrease the performance of deep neural networks on a large number of vision-based detection, recognition and segmentation tasks in urban driving scenes. A key factor that contributes to this performance gap is the lack of `time-of-day' diversity within real, labeled datasets. There have been impressive advances in the realm of image to image translation in transferring previously unseen visual effects into a dataset, specifically in day to night translation. However, it is not easy to constrain what visual effects, let alone illumination effects, are transferred from one dataset to another during the training process. To address this problem, we propose deep learning framework, called Shadow Transfer, that can relight complex outdoor scenes by transferring realistic shadow, shading, and other lighting effects onto a single image. The novelty of the proposed framework is that it is both self-supervised, and is designed to operate on sensor and label information that is easily available in autonomous vehicle datasets. We show the effectiveness of this method on both synthetic and real datasets, and we provide experiments that demonstrate that the proposed method produces images of higher visual quality than state of the art image to image translation methods.



### Class-dependent Compression of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.10364v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.10364v3)
- **Published**: 2019-09-23 13:47:51+00:00
- **Updated**: 2020-04-19 15:47:42+00:00
- **Authors**: Rahim Entezari, Olga Saukh
- **Comment**: None
- **Journal**: None
- **Summary**: Today's deep neural networks require substantial computation resources for their training, storage, and inference, which limits their effective use on resource-constrained devices. Many recent research activities explore different options for compressing and optimizing deep models. On the one hand, in many real-world applications, we face the data imbalance challenge, i.e. when the number of labeled instances of one class considerably outweighs the number of labeled instances of the other class. On the other hand, applications may pose a class imbalance problem, i.e. higher number of false positives produced when training a model and optimizing its performance may be tolerable, yet the number of false negatives must stay low. The problem originates from the fact that some classes are more important for the application than others, e.g. detection problems in medical and surveillance domains. Motivated by the success of the lottery ticket hypothesis, in this paper we propose an iterative deep model compression technique, which keeps the number of false negatives of the compressed model close to the one of the original model at the price of increasing the number of false positives if necessary. Our experimental evaluation using two benchmark data sets shows that the resulting compressed sub-networks 1) achieve up to 35% lower number of false negatives than the compressed model without class optimization, 2) provide an overall higher AUC_ROC measure, and 3) use up to 99% fewer parameters compared to the original network.



### Model-Based and Data-Driven Strategies in Medical Image Computing
- **Arxiv ID**: http://arxiv.org/abs/1909.10391v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10391v3)
- **Published**: 2019-09-23 14:35:55+00:00
- **Updated**: 2019-09-30 12:41:42+00:00
- **Authors**: Daniel Rueckert, Julia A. Schnabel
- **Comment**: Accepted in IEEE Proceedings
- **Journal**: None
- **Summary**: Model-based approaches for image reconstruction, analysis and interpretation have made significant progress over the last decades. Many of these approaches are based on either mathematical, physical or biological models. A challenge for these approaches is the modelling of the underlying processes (e.g. the physics of image acquisition or the patho-physiology of a disease) with appropriate levels of detail and realism. With the availability of large amounts of imaging data and machine learning (in particular deep learning) techniques, data-driven approaches have become more widespread for use in different tasks in reconstruction, analysis and interpretation. These approaches learn statistical models directly from labelled or unlabeled image data and have been shown to be very powerful for extracting clinically useful information from medical imaging. While these data-driven approaches often outperform traditional model-based approaches, their clinical deployment often poses challenges in terms of robustness, generalization ability and interpretability. In this article, we discuss what developments have motivated the shift from model-based approaches towards data-driven strategies and what potential problems are associated with the move towards purely data-driven approaches, in particular deep learning. We also discuss some of the open challenges for data-driven approaches, e.g. generalization to new unseen data (e.g. transfer learning), robustness to adversarial attacks and interpretability. Finally, we conclude with a discussion on how these approaches may lead to the development of more closely coupled imaging pipelines that are optimized in an end-to-end fashion.



### Robot Navigation in Crowds by Graph Convolutional Networks with Attention Learned from Human Gaze
- **Arxiv ID**: http://arxiv.org/abs/1909.10400v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.10400v1)
- **Published**: 2019-09-23 14:46:11+00:00
- **Updated**: 2019-09-23 14:46:11+00:00
- **Authors**: Yuying Chen, Congcong Liu, Ming Liu, Bertram E. Shi
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: Safe and efficient crowd navigation for mobile robot is a crucial yet challenging task. Previous work has shown the power of deep reinforcement learning frameworks to train efficient policies. However, their performance deteriorates when the crowd size grows. We suggest that this can be addressed by enabling the network to identify and pay attention to the humans in the crowd that are most critical to navigation. We propose a novel network utilizing a graph representation to learn the policy. We first train a graph convolutional network based on human gaze data that accurately predicts human attention to different agents in the crowd. Then we incorporate the learned attention into a graph-based reinforcement learning architecture. The proposed attention mechanism enables the assignment of meaningful weightings to the neighbors of the robot, and has the additional benefit of interpretability. Experiments on real-world dense pedestrian datasets with various crowd sizes demonstrate that our model outperforms state-of-art methods by 18.4% in task accomplishment and by 16.4% in time efficiency.



### CochleaNet: A Robust Language-independent Audio-Visual Model for Speech Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1909.10407v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1909.10407v1)
- **Published**: 2019-09-23 14:59:47+00:00
- **Updated**: 2019-09-23 14:59:47+00:00
- **Authors**: Mandar Gogate, Kia Dashtipour, Ahsan Adeel, Amir Hussain
- **Comment**: 34 pages, 11 figures, Submitted to Information Fusion
- **Journal**: None
- **Summary**: Noisy situations cause huge problems for suffers of hearing loss as hearing aids often make the signal more audible but do not always restore the intelligibility. In noisy settings, humans routinely exploit the audio-visual (AV) nature of the speech to selectively suppress the background noise and to focus on the target speaker. In this paper, we present a causal, language, noise and speaker independent AV deep neural network (DNN) architecture for speech enhancement (SE). The model exploits the noisy acoustic cues and noise robust visual cues to focus on the desired speaker and improve the speech intelligibility. To evaluate the proposed SE framework a first of its kind AV binaural speech corpus, called ASPIRE, is recorded in real noisy environments including cafeteria and restaurant. We demonstrate superior performance of our approach in terms of objective measures and subjective listening tests over the state-of-the-art SE approaches as well as recent DNN based SE models. In addition, our work challenges a popular belief that a scarcity of multi-language large vocabulary AV corpus and wide variety of noises is a major bottleneck to build a robust language, speaker and noise independent SE systems. We show that a model trained on synthetic mixture of Grid corpus (with 33 speakers and a small English vocabulary) and ChiME 3 Noises (consisting of only bus, pedestrian, cafeteria, and street noises) generalise well not only on large vocabulary corpora but also on completely unrelated languages (such as Mandarin), wide variety of speakers and noises.



### NLVR2 Visual Bias Analysis
- **Arxiv ID**: http://arxiv.org/abs/1909.10411v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.10411v1)
- **Published**: 2019-09-23 15:10:41+00:00
- **Updated**: 2019-09-23 15:10:41+00:00
- **Authors**: Alane Suhr, Yoav Artzi
- **Comment**: Corresponding notebook available at
  http://lil.nlp.cornell.edu/nlvr/NLVR2BiasAnalysis.html
- **Journal**: None
- **Summary**: NLVR2 (Suhr et al., 2019) was designed to be robust for language bias through a data collection process that resulted in each natural language sentence appearing with both true and false labels. The process did not provide a similar measure of control for visual bias. This technical report analyzes the potential for visual bias in NLVR2. We show that some amount of visual bias likely exists. Finally, we identify a subset of the test data that allows to test for model performance in a way that is robust to such potential biases. We show that the performance of existing models (Li et al., 2019; Tan and Bansal 2019) is relatively robust to this potential bias. We propose to add the evaluation on this subset of the data to the NLVR2 evaluation protocol, and update the official release to include it. A notebook including an implementation of the code used to replicate this analysis is available at http://nlvr.ai/NLVR2BiasAnalysis.html.



### Go Wider: An Efficient Neural Network for Point Cloud Analysis via Group Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1909.10431v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10431v1)
- **Published**: 2019-09-23 15:39:46+00:00
- **Updated**: 2019-09-23 15:39:46+00:00
- **Authors**: Can Chen, Luca Zanotti Fragonara, Antonios Tsourdos
- **Comment**: None
- **Journal**: None
- **Summary**: In order to achieve better performance for point cloud analysis, many researchers apply deeper neural networks using stacked Multi-Layer-Perceptron (MLP) convolutions over irregular point cloud. However, applying dense MLP convolutions over large amount of points (e.g. autonomous driving application) leads to inefficiency in memory and computation. To achieve high performance but less complexity, we propose a deep-wide neural network, called ShufflePointNet, to exploit fine-grained local features and reduce redundancy in parallel using group convolution and channel shuffle operation. Unlike conventional operation that directly applies MLPs on high-dimensional features of point cloud, our model goes wider by splitting features into groups in advance, and each group with certain smaller depth is only responsible for respective MLP operation, which can reduce complexity and allows to encode more useful information. Meanwhile, we connect communication between groups by shuffling groups in feature channel to capture fine-grained features. We claim that, multi-branch method for wider neural networks is also beneficial to feature extraction for point cloud. We present extensive experiments for shape classification task on ModelNet40 dataset and semantic segmentation task on large scale datasets ShapeNet part, S3DIS and KITTI. We further perform ablation study and compare our model to other state-of-the-art algorithms in terms of complexity and accuracy.



### Patch-Based Image Similarity for Intraoperative 2D/3D Pelvis Registration During Periacetabular Osteotomy
- **Arxiv ID**: http://arxiv.org/abs/1909.10443v1
- **DOI**: 10.1007/978-3-030-01201-4_17
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.10443v1)
- **Published**: 2019-09-23 15:53:49+00:00
- **Updated**: 2019-09-23 15:53:49+00:00
- **Authors**: Robert Grupp, Mehran Armand, Russell Taylor
- **Comment**: Presented at MICCAI CLIP Workshop 2018
- **Journal**: In OR 2.0 Context-Aware Operating Theaters, Computer Assisted
  Robotic Endoscopy, Clinical Image-Based Procedures, and Skin Image Analysis
  2018 Sep 16 (pp. 153-163). Springer, Cham
- **Summary**: Periacetabular osteotomy is a challenging surgical procedure for treating developmental hip dysplasia, providing greater coverage of the femoral head via relocation of a patient's acetabulum. Since fluoroscopic imaging is frequently used in the surgical workflow, computer-assisted X-Ray navigation of osteotomes and the relocated acetabular fragment should be feasible. We use intensity-based 2D/3D registration to estimate the pelvis pose with respect to fluoroscopic images, recover relative poses of multiple views, and triangulate landmarks which may be used for navigation. Existing similarity metrics are unable to consistently account for the inherent mismatch between the preoperative intact pelvis, and the intraoperative reality of a fractured pelvis. To mitigate the effect of this mismatch, we continuously estimate the relevance of each pixel to solving the registration and use these values as weightings in a patch-based similarity metric. Limiting computation to randomly selected subsets of patches results in faster runtimes than existing patch-based methods. A simulation study was conducted with random fragment shapes, relocations, and fluoroscopic views, and the proposed method achieved a 1.7 mm mean triangulation error over all landmarks, compared to mean errors of 3 mm and 2.8 mm for the non-patched and image-intensity-variance-weighted patch similarity metrics, respectively.



### Automatic techniques for cochlear implant CT image analysis
- **Arxiv ID**: http://arxiv.org/abs/1909.10922v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.10922v1)
- **Published**: 2019-09-23 16:05:26+00:00
- **Updated**: 2019-09-23 16:05:26+00:00
- **Authors**: Yiyuan Zhao
- **Comment**: This is a preprint of Yiyuan Zhao's Ph.D. dissertation from
  Vanderbilt University, Nashville, TN, USA. Trivial formatting modifications
  have been made in the arxiv version for readability. Vanderbilt University
  Electronic These & Dissertation (https://etd.library.vanderbilt.edu/) has the
  original submission on May 11 2018, and will be released on May 11 2020
- **Journal**: None
- **Summary**: The goals of this dissertation are to fully automate the image processing techniques needed in the post-operative stage of IGCIP and to perform a thorough analysis of (a) the robustness of the automatic image processing techniques used in IGCIP and (b) assess the sensitivity of the IGCIP process as a whole to individual components. The automatic methods that have been developed include the automatic localization of both closely- and distantly-spaced CI electrode arrays in post-implantation CTs and the automatic selection of electrode configurations based on the stimulation patterns. Together with the existing automatic techniques developed for IGCIP, the proposed automatic methods enable an end-to-end IGCIP process that takes pre- and post-implantation CT images as input and produces a patient-customized electrode configuration as output.



### Pelvis Surface Estimation From Partial CT for Computer-Aided Pelvic Osteotomies
- **Arxiv ID**: http://arxiv.org/abs/1909.10452v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.10452v1)
- **Published**: 2019-09-23 16:11:11+00:00
- **Updated**: 2019-09-23 16:11:11+00:00
- **Authors**: Robert Grupp, Yoshito Otake, Ryan Murphy, Javad Parvizi, Mehran Armand, Russell Taylor
- **Comment**: CAOS 2015 Extended Paper
- **Journal**: In Orthopaedic Proceedings 2016 Feb (Vol. 98, No. SUPP_5, pp.
  55-55). The British Editorial Society of Bone & Joint Surgery
- **Summary**: Computer-aided surgical systems commonly use preoperative CT scans when performing pelvic osteotomies for intraoperative navigation. These systems have the potential to improve the safety and accuracy of pelvic osteotomies, however, exposing the patient to radiation is a significant drawback. In order to reduce radiation exposure, we propose a new smooth extrapolation method leveraging a partial pelvis CT and a statistical shape model (SSM) of the full pelvis in order to estimate a patient's complete pelvis. A SSM of normal, complete, female pelvis anatomy was created and evaluated from 42 subjects. A leave-one-out test was performed to characterise the inherent generalisation capability of the SSM. An additional leave-one-out test was conducted to measure performance of the smooth extrapolation method and an existing "cut-and-paste" extrapolation method. Unknown anatomy was simulated by keeping the axial slices of the patient's acetabulum intact and varying the amount of the superior iliac crest retained; from 0% to 15% of the total pelvis extent. The smooth technique showed an average improvement over the cut-and-paste method of 1.31 mm and 3.61 mm, in RMS and maximum surface error, respectively. With 5% of the iliac crest retained, the smoothly estimated surface had an RMS surface error of 2.21 mm, an improvement of 1.25 mm when retaining none of the iliac crest. This anatomical estimation method creates the possibility of a patient and surgeon benefiting from the use of a CAS system and simultaneously reducing the patient's radiation exposure.



### Hierarchical Point-Edge Interaction Network for Point Cloud Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.10469v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10469v1)
- **Published**: 2019-09-23 16:45:42+00:00
- **Updated**: 2019-09-23 16:45:42+00:00
- **Authors**: Li Jiang, Hengshuang Zhao, Shu Liu, Xiaoyong Shen, Chi-Wing Fu, Jiaya Jia
- **Comment**: Accepted by ICCV 2019
- **Journal**: None
- **Summary**: We achieve 3D semantic scene labeling by exploring semantic relation between each point and its contextual neighbors through edges. Besides an encoder-decoder branch for predicting point labels, we construct an edge branch to hierarchically integrate point features and generate edge features. To incorporate point features in the edge branch, we establish a hierarchical graph framework, where the graph is initialized from a coarse layer and gradually enriched along the point decoding process. For each edge in the final graph, we predict a label to indicate the semantic consistency of the two connected points to enhance point prediction. At different layers, edge features are also fed into the corresponding point module to integrate contextual information for message passing enhancement in local regions. The two branches interact with each other and cooperate in segmentation. Decent experimental results on several 3D semantic labeling datasets demonstrate the effectiveness of our work.



### Improving Generative Visual Dialog by Answering Diverse Questions
- **Arxiv ID**: http://arxiv.org/abs/1909.10470v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.10470v2)
- **Published**: 2019-09-23 16:47:15+00:00
- **Updated**: 2019-10-03 03:01:48+00:00
- **Authors**: Vishvak Murahari, Prithvijit Chattopadhyay, Dhruv Batra, Devi Parikh, Abhishek Das
- **Comment**: EMNLP 2019
- **Journal**: None
- **Summary**: Prior work on training generative Visual Dialog models with reinforcement learning(Das et al.) has explored a Qbot-Abot image-guessing game and shown that this 'self-talk' approach can lead to improved performance at the downstream dialog-conditioned image-guessing task. However, this improvement saturates and starts degrading after a few rounds of interaction, and does not lead to a better Visual Dialog model. We find that this is due in part to repeated interactions between Qbot and Abot during self-talk, which are not informative with respect to the image. To improve this, we devise a simple auxiliary objective that incentivizes Qbot to ask diverse questions, thus reducing repetitions and in turn enabling Abot to explore a larger state space during RL ie. be exposed to more visual concepts to talk about, and varied questions to answer. We evaluate our approach via a host of automatic metrics and human studies, and demonstrate that it leads to better dialog, ie. dialog that is more diverse (ie. less repetitive), consistent (ie. has fewer conflicting exchanges), fluent (ie. more human-like),and detailed, while still being comparably image-relevant as prior work and ablations.



### Hydrocephalus verification on brain magnetic resonance images with deep convolutional neural networks and "transfer learning" technique
- **Arxiv ID**: http://arxiv.org/abs/1909.10473v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10473v1)
- **Published**: 2019-09-23 16:54:04+00:00
- **Updated**: 2019-09-23 16:54:04+00:00
- **Authors**: Alexey Demyanchuk, Ekaterina Pushkina, Nikolay Russkikh, Dmitry Shtokalo, Sergey Mishinov
- **Comment**: None
- **Journal**: None
- **Summary**: The hydrocephalus can be either an independent disease or a concomitant symptom of a number of pathologies, therefore representing an urgent issue in the present-day clinical practice. Deep Learning is an evolving technology and the part of a broader field of Machine Learning. Deep learning is currently actively researched in the field of radiology. The aim of this study was to evaluate deep learning applicability to the diagnostics of hydrocephalus with the use of MRI images. We retrospectively collected, annotated, and preprocessed the brain MRI data of 200 patients with and without radiological signs of hydrocephalus. We applied a state-of-the-art deep convolutional neural network in conjunction with transfer learning method to train a hydrocephalus classifier model. Using deep convolutional neural networks, we achieved a high quality of machine learning model. Accuracy, sensitivity, and specificity of hydrocephalus signs identification was 97%, 98%, and 96% respectively. In this study, we demonstrated the capacity of deep neural networks to identify hydrocephalus syndrome using brain MRI images. Applying transfer learning technique, the high quality of classification was achieved although trained on rather limited data.



### Research Directions in Democratizing Innovation through Design Automation, One-Click Manufacturing Services and Intelligent Machines
- **Arxiv ID**: http://arxiv.org/abs/1909.10476v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.AI, cs.CV, cs.LG, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/1909.10476v1)
- **Published**: 2019-09-23 16:56:08+00:00
- **Updated**: 2019-09-23 16:56:08+00:00
- **Authors**: Binil Starly, Atin Angrish, Paul Cohen
- **Comment**: None
- **Journal**: None
- **Summary**: The digitalization of manufacturing has created opportunities for consumers to customize products that fit their individualized needs which in turn would drive demand for manufacturing services. However, this pull-based manufacturing system production of extremely low quantity and limitless variety for products is expensive to implement. New emerging technology in design automation driven by data-driven computational design, manufacturing-as-a-service marketplaces and digitally enabled micro-factories holds promise towards democratization of innovation. In this paper, scientific, technology and infrastructure challenges are identified and if solved, the impact of these emerging technologies on product innovation and future factory organization is discussed.



### Automatic Mouse Embryo Brain Ventricle & Body Segmentation and Mutant Classification From Ultrasound Data Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.10555v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.10555v1)
- **Published**: 2019-09-23 18:24:52+00:00
- **Updated**: 2019-09-23 18:24:52+00:00
- **Authors**: Ziming Qiu, Nitin Nair, Jack Langerman, Orlando Aristizabal, Jonathan Mamou, Daniel H. Turnbull, Jeffrey A. Ketterling, Yao Wang
- **Comment**: 4 pages, 6 figures, the 2019 IEEE International Ultrasonics Symposium
- **Journal**: None
- **Summary**: High-frequency ultrasound (HFU) is well suited for imaging embryonic mice in vivo because it is non-invasive and real-time. Manual segmentation of the brain ventricles (BVs) and whole body from 3D HFU images is time-consuming and requires specialized training. This paper presents a deep-learning-based segmentation pipeline which automates several time-consuming, repetitive tasks currently performed to study genetic mutations in developing mouse embryos. Namely, the pipeline accurately segments the BV and body regions in 3D HFU images of mouse embryos, despite significant challenges due to position and shape variation of the embryos, as well as imaging artifacts. Based on the BV segmentation, a 3D convolutional neural network (CNN) is further trained to detect embryos with the Engrailed-1 (En1) mutation. The algorithms achieve 0.896 and 0.925 Dice Similarity Coefficient (DSC) for BV and body segmentation, respectively, and 95.8% accuracy on mutant classification. Through gradient based interrogation and visualization of the trained classifier, it is demonstrated that the model focuses on the morphological structures known to be affected by the En1 mutation.



### Handwritten Amharic Character Recognition Using a Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1909.12943v1
- **DOI**: 10.5445/KSP/1000098011/09
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.12943v1)
- **Published**: 2019-09-23 21:12:22+00:00
- **Updated**: 2019-09-23 21:12:22+00:00
- **Authors**: Mesay Samuel Gondere, Lars Schmidt-Thieme, Abiot Sinamo Boltena, Hadi Samer Jomaa
- **Comment**: ECDA2019 Conference Oral Presentation
- **Journal**: None
- **Summary**: Amharic is the official language of the Federal Democratic Republic of Ethiopia. There are lots of historic Amharic and Ethiopic handwritten documents addressing various relevant issues including governance, science, religious, social rules, cultures and art works which are very reach indigenous knowledge. The Amharic language has its own alphabet derived from Ge'ez which is currently the liturgical language in Ethiopia. Handwritten character recognition for non Latin scripts like Amharic is not addressed especially using the advantages of the state of the art techniques. This research work designs for the first time a model for Amharic handwritten character recognition using a convolutional neural network. The dataset was organized from collected sample handwritten documents and data augmentation was applied for machine learning. The model was further enhanced using multi-task learning from the relationships of the characters. Promising results are observed from the later model which can further be applied to word prediction.



### Deep Imitation Learning of Sequential Fabric Smoothing From an Algorithmic Supervisor
- **Arxiv ID**: http://arxiv.org/abs/1910.04854v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.04854v2)
- **Published**: 2019-09-23 22:06:14+00:00
- **Updated**: 2020-03-02 22:26:31+00:00
- **Authors**: Daniel Seita, Aditya Ganapathi, Ryan Hoque, Minho Hwang, Edward Cen, Ajay Kumar Tanwani, Ashwin Balakrishna, Brijen Thananjeyan, Jeffrey Ichnowski, Nawid Jamali, Katsu Yamane, Soshi Iba, John Canny, Ken Goldberg
- **Comment**: Supplementary material is available at
  https://sites.google.com/view/fabric-smoothing ; Version 2 has significant
  improvements with new results and figures
- **Journal**: None
- **Summary**: Sequential pulling policies to flatten and smooth fabrics have applications from surgery to manufacturing to home tasks such as bed making and folding clothes. Due to the complexity of fabric states and dynamics, we apply deep imitation learning to learn policies that, given color (RGB), depth (D), or combined color-depth (RGBD) images of a rectangular fabric sample, estimate pick points and pull vectors to spread the fabric to maximize coverage. To generate data, we develop a fabric simulator and an algorithmic supervisor that has access to complete state information. We train policies in simulation using domain randomization and dataset aggregation (DAgger) on three tiers of difficulty in the initial randomized configuration. We present results comparing five baseline policies to learned policies and report systematic comparisons of RGB vs D vs RGBD images as inputs. In simulation, learned policies achieve comparable or superior performance to analytic baselines. In 180 physical experiments with the da Vinci Research Kit (dVRK) surgical robot, RGBD policies trained in simulation attain coverage of 83% to 95% depending on difficulty tier, suggesting that effective fabric smoothing policies can be learned from an algorithmic supervisor and that depth sensing is a valuable addition to color alone. Supplementary material is available at https://sites.google.com/view/fabric-smoothing.



### Non-monotonic Logical Reasoning Guiding Deep Learning for Explainable Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1909.10650v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.10650v1)
- **Published**: 2019-09-23 23:34:32+00:00
- **Updated**: 2019-09-23 23:34:32+00:00
- **Authors**: Heather Riley, Mohan Sridharan
- **Comment**: 28 pages, 15 figures
- **Journal**: None
- **Summary**: State of the art algorithms for many pattern recognition problems rely on deep network models. Training these models requires a large labeled dataset and considerable computational resources. Also, it is difficult to understand the working of these learned models, limiting their use in some critical applications. Towards addressing these limitations, our architecture draws inspiration from research in cognitive systems, and integrates the principles of commonsense logical reasoning, inductive learning, and deep learning. In the context of answering explanatory questions about scenes and the underlying classification problems, the architecture uses deep networks for extracting features from images and for generating answers to queries. Between these deep networks, it embeds components for non-monotonic logical reasoning with incomplete commonsense domain knowledge, and for decision tree induction. It also incrementally learns and reasons with previously unknown constraints governing the domain's states. We evaluated the architecture in the context of datasets of simulated and real-world images, and a simulated robot computing, executing, and providing explanatory descriptions of plans. Experimental results indicate that in comparison with an ``end to end'' architecture of deep networks, our architecture provides better accuracy on classification problems when the training dataset is small, comparable accuracy with larger datasets, and more accurate answers to explanatory questions. Furthermore, incremental acquisition of previously unknown constraints improves the ability to answer explanatory questions, and extending non-monotonic logical reasoning to support planning and diagnostics improves the reliability and efficiency of computing and executing plans on a simulated robot.



