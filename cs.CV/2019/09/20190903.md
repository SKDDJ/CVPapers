# Arxiv Papers in cs.CV on 2019-09-03
### Metric Learning for Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/1909.00900v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, cs.IR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.00900v2)
- **Published**: 2019-09-03 00:39:40+00:00
- **Updated**: 2019-10-28 00:43:15+00:00
- **Authors**: Chengzhi Mao, Ziyuan Zhong, Junfeng Yang, Carl Vondrick, Baishakhi Ray
- **Comment**: None
- **Journal**: None
- **Summary**: Deep networks are well-known to be fragile to adversarial attacks. We conduct an empirical analysis of deep representations under the state-of-the-art attack method called PGD, and find that the attack causes the internal representation to shift closer to the "false" class. Motivated by this observation, we propose to regularize the representation space under attack with metric learning to produce more robust classifiers. By carefully sampling examples for metric learning, our learned representation not only increases robustness, but also detects previously unseen adversarial samples. Quantitative experiments show improvement of robustness accuracy by up to 4% and detection efficiency by up to 6% according to Area Under Curve score over prior work. The code of our work is available at https://github.com/columbia/Metric_Learning_Adversarial_Robustness.



### miniSAM: A Flexible Factor Graph Non-linear Least Squares Optimization Framework
- **Arxiv ID**: http://arxiv.org/abs/1909.00903v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.00903v1)
- **Published**: 2019-09-03 00:51:29+00:00
- **Updated**: 2019-09-03 00:51:29+00:00
- **Authors**: Jing Dong, Zhaoyang Lv
- **Comment**: Accepted in IROS 2019 PPNIV workshop
- **Journal**: None
- **Summary**: Many problems in computer vision and robotics can be phrased as non-linear least squares optimization problems represented by factor graphs, for example, simultaneous localization and mapping (SLAM), structure from motion (SfM), motion planning, and control. We have developed an open-source C++/Python framework miniSAM, for solving such factor graph based least squares problems. Compared to most existing frameworks for least squares solvers, miniSAM has (1) full Python/NumPy API, which enables more agile development and easy binding with existing Python projects, and (2) a wide list of sparse linear solvers, including CUDA enabled sparse linear solvers. Our benchmarking results shows miniSAM offers comparable performances on various types of problems, with more flexible and smoother development experience.



### Hyper-Pairing Network for Multi-Phase Pancreatic Ductal Adenocarcinoma Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.00906v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.00906v1)
- **Published**: 2019-09-03 00:55:37+00:00
- **Updated**: 2019-09-03 00:55:37+00:00
- **Authors**: Yuyin Zhou, Yingwei Li, Zhishuai Zhang, Yan Wang, Angtian Wang, Elliot Fishman, Alan Yuille, Seyoun Park
- **Comment**: To appear in MICCAI 2019
- **Journal**: None
- **Summary**: Pancreatic ductal adenocarcinoma (PDAC) is one of the most lethal cancers with an overall five-year survival rate of 8%. Due to subtle texture changes of PDAC, pancreatic dual-phase imaging is recommended for better diagnosis of pancreatic disease. In this study, we aim at enhancing PDAC automatic segmentation by integrating multi-phase information (i.e., arterial phase and venous phase). To this end, we present Hyper-Pairing Network (HPN), a 3D fully convolution neural network which effectively integrates information from different phases. The proposed approach consists of a dual path network where the two parallel streams are interconnected with hyper-connections for intensive information exchange. Additionally, a pairing loss is added to encourage the commonality between high-level feature representations of different phases. Compared to prior arts which use single phase data, HPN reports a significant improvement up to 7.73% (from 56.21% to 63.94%) in terms of DSC.



### Counterfactual Depth from a Single RGB Image
- **Arxiv ID**: http://arxiv.org/abs/1909.00915v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.00915v1)
- **Published**: 2019-09-03 01:50:17+00:00
- **Updated**: 2019-09-03 01:50:17+00:00
- **Authors**: Theerasit Issaranon, Chuhang Zou, David Forsyth
- **Comment**: None
- **Journal**: None
- **Summary**: We describe a method that predicts, from a single RGB image, a depth map that describes the scene when a masked object is removed - we call this "counterfactual depth" that models hidden scene geometry together with the observations. Our method works for the same reason that scene completion works: the spatial structure of objects is simple. But we offer a much higher resolution representation of space than current scene completion methods, as we operate at pixel-level precision and do not rely on a voxel representation. Furthermore, we do not require RGBD inputs. Our method uses a standard encoder-decoder architecture, and with a decoder modified to accept an object mask. We describe a small evaluation dataset that we have collected, which allows inference about what factors affect reconstruction most strongly. Using this dataset, we show that our depth predictions for masked objects are better than other baselines.



### HarDNet: A Low Memory Traffic Network
- **Arxiv ID**: http://arxiv.org/abs/1909.00948v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.00948v1)
- **Published**: 2019-09-03 04:34:18+00:00
- **Updated**: 2019-09-03 04:34:18+00:00
- **Authors**: Ping Chao, Chao-Yang Kao, Yu-Shan Ruan, Chien-Hsiang Huang, Youn-Long Lin
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: State-of-the-art neural network architectures such as ResNet, MobileNet, and DenseNet have achieved outstanding accuracy over low MACs and small model size counterparts. However, these metrics might not be accurate for predicting the inference time. We suggest that memory traffic for accessing intermediate feature maps can be a factor dominating the inference latency, especially in such tasks as real-time object detection and semantic segmentation of high-resolution video. We propose a Harmonic Densely Connected Network to achieve high efficiency in terms of both low MACs and memory traffic. The new network achieves 35%, 36%, 30%, 32%, and 45% inference time reduction compared with FC-DenseNet-103, DenseNet-264, ResNet-50, ResNet-152, and SSD-VGG, respectively. We use tools including Nvidia profiler and ARM Scale-Sim to measure the memory traffic and verify that the inference latency is indeed proportional to the memory traffic consumption and the proposed network consumes low memory traffic. We conclude that one should take memory traffic into consideration when designing neural network architectures for high-resolution applications at the edge.



### Image Inpainting with Learnable Bidirectional Attention Maps
- **Arxiv ID**: http://arxiv.org/abs/1909.00968v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.00968v3)
- **Published**: 2019-09-03 06:18:40+00:00
- **Updated**: 2019-09-05 12:55:14+00:00
- **Authors**: Chaohao Xie, Shaohui Liu, Chao Li, Ming-Ming Cheng, Wangmeng Zuo, Xiao Liu, Shilei Wen, Errui Ding
- **Comment**: 18 pages, 16 figures, 2 tables, main paper + supplementary material
  (Accepted at ICCV, 2019)
- **Journal**: None
- **Summary**: Most convolutional network (CNN)-based inpainting methods adopt standard convolution to indistinguishably treat valid pixels and holes, making them limited in handling irregular holes and more likely to generate inpainting results with color discrepancy and blurriness. Partial convolution has been suggested to address this issue, but it adopts handcrafted feature re-normalization, and only considers forward mask-updating. In this paper, we present a learnable attention map module for learning feature renormalization and mask-updating in an end-to-end manner, which is effective in adapting to irregular holes and propagation of convolution layers. Furthermore, learnable reverse attention maps are introduced to allow the decoder of U-Net to concentrate on filling in irregular holes instead of reconstructing both holes and known regions, resulting in our learnable bidirectional attention maps. Qualitative and quantitative experiments show that our method performs favorably against state-of-the-arts in generating sharper, more coherent and visually plausible inpainting results. The source code and pre-trained models will be available.



### EleAtt-RNN: Adding Attentiveness to Neurons in Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.01939v1
- **DOI**: 10.1109/TIP.2019.2937724
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.01939v1)
- **Published**: 2019-09-03 08:15:09+00:00
- **Updated**: 2019-09-03 08:15:09+00:00
- **Authors**: Pengfei Zhang, Jianru Xue, Cuiling Lan, Wenjun Zeng, Zhanning Gao, Nanning Zheng
- **Comment**: IEEE Transactions on Image Processing (Accept). arXiv admin note:
  substantial text overlap with arXiv:1807.04445
- **Journal**: None
- **Summary**: Recurrent neural networks (RNNs) are capable of modeling temporal dependencies of complex sequential data. In general, current available structures of RNNs tend to concentrate on controlling the contributions of current and previous information. However, the exploration of different importance levels of different elements within an input vector is always ignored. We propose a simple yet effective Element-wise-Attention Gate (EleAttG), which can be easily added to an RNN block (e.g. all RNN neurons in an RNN layer), to empower the RNN neurons to have attentiveness capability. For an RNN block, an EleAttG is used for adaptively modulating the input by assigning different levels of importance, i.e., attention, to each element/dimension of the input. We refer to an RNN block equipped with an EleAttG as an EleAtt-RNN block. Instead of modulating the input as a whole, the EleAttG modulates the input at fine granularity, i.e., element-wise, and the modulation is content adaptive. The proposed EleAttG, as an additional fundamental unit, is general and can be applied to any RNN structures, e.g., standard RNN, Long Short-Term Memory (LSTM), or Gated Recurrent Unit (GRU). We demonstrate the effectiveness of the proposed EleAtt-RNN by applying it to different tasks including the action recognition, from both skeleton-based data and RGB videos, gesture recognition, and sequential MNIST classification. Experiments show that adding attentiveness through EleAttGs to RNN blocks significantly improves the power of RNNs.



### PlotQA: Reasoning over Scientific Plots
- **Arxiv ID**: http://arxiv.org/abs/1909.00997v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1909.00997v3)
- **Published**: 2019-09-03 08:23:51+00:00
- **Updated**: 2020-02-01 06:56:30+00:00
- **Authors**: Nitesh Methani, Pritha Ganguly, Mitesh M. Khapra, Pratyush Kumar
- **Comment**: This is an extension of our previous arxiv paper "Data Interpretation
  over Plots" and it is to be presented at WACV 2020
- **Journal**: None
- **Summary**: Existing synthetic datasets (FigureQA, DVQA) for reasoning over plots do not contain variability in data labels, real-valued data, or complex reasoning questions. Consequently, proposed models for these datasets do not fully address the challenge of reasoning over plots. In particular, they assume that the answer comes either from a small fixed size vocabulary or from a bounding box within the image. However, in practice, this is an unrealistic assumption because many questions require reasoning and thus have real-valued answers which appear neither in a small fixed size vocabulary nor in the image. In this work, we aim to bridge this gap between existing datasets and real-world plots. Specifically, we propose PlotQA with 28.9 million question-answer pairs over 224,377 plots on data from real-world sources and questions based on crowd-sourced question templates. Further, 80.76% of the out-of-vocabulary (OOV) questions in PlotQA have answers that are not in a fixed vocabulary. Analysis of existing models on PlotQA reveals that they cannot deal with OOV questions: their overall accuracy on our dataset is in single digits. This is not surprising given that these models were not designed for such questions. As a step towards a more holistic model which can address fixed vocabulary as well as OOV questions, we propose a hybrid approach: Specific questions are answered by choosing the answer from a fixed vocabulary or by extracting it from a predicted bounding box in the plot, while other questions are answered with a table question-answering engine which is fed with a structured table generated by detecting visual elements from the image. On the existing DVQA dataset, our model has an accuracy of 58%, significantly improving on the highest reported accuracy of 46%. On PlotQA, our model has an accuracy of 22.52%, which is significantly better than state of the art models.



### Deep User Identification Model with Multiple Biometrics
- **Arxiv ID**: http://arxiv.org/abs/1909.05417v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1909.05417v1)
- **Published**: 2019-09-03 09:13:11+00:00
- **Updated**: 2019-09-03 09:13:11+00:00
- **Authors**: Hyoung-Kyu Song, Ebrahim AlAlkeem, Jaewoong Yun, Tae-Ho Kim, Tae-Ho Kim, Hyerin Yoo, Dasom Heo, Chan Yeob Yeun, Myungsu Chae
- **Comment**: Accepted, CIKM 2019 Workshop on DTMBio
- **Journal**: None
- **Summary**: Identification using biometrics is an important yet challenging task. Abundant research has been conducted on identifying personal identity or gender using given signals. Various types of biometrics such as electrocardiogram (ECG), electroencephalogram (EEG), face, fingerprint, and voice have been used for these tasks. Most research has only focused on single modality or a single task, while the combination of input modality or tasks is yet to be investigated. In this paper, we propose deep identification and gender classification using multimodal biometrics. Our model uses ECG, fingerprint, and facial data. It then performs two tasks: gender identification and classification. By engaging multi-modality, a single model can handle various input domains without training each modality independently, and the correlation between domains can increase its generalization performance on the tasks.



### Object Viewpoint Classification Based 3D Bounding Box Estimation for Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1909.01025v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.01025v1)
- **Published**: 2019-09-03 09:48:36+00:00
- **Updated**: 2019-09-03 09:48:36+00:00
- **Authors**: Zhou Lingtao, Fang Jiaojiao, Liu Guizhong
- **Comment**: None
- **Journal**: None
- **Summary**: 3D object detection is one of the most important tasks for the perception systems of autonomous vehicles. With the significant success in the field of 2D object detection, several monocular image based 3D object detection algorithms have been proposed based on advanced 2D object detectors and the geometric constraints between the 2D and 3D bounding boxes. In this paper, we propose a novel method for determining the configuration of the 2D-3D geometric constraints which is based on the well-known 2D-3D two stage object detection framework. First, we discrete viewpoints in which the camera shots the object into 16 categories with respect to the observation relationship between camera and objects. Second, we design a viewpoint classifier by integrated a new sub-branch into the existing multi-branches CNN. Then, the configuration of geometric constraint between the 2D and 3D bounding boxes can be determined according to the output of this classifier. Extensive experiments on the KITTI dataset show that, our method not only improves the computational efficiency, but also increases the overall precision of the model, especially to the orientation angle estimation.



### PSDNet and DPDNet: Efficient channel expansion, Depthwise-Pointwise-Depthwise Inverted Bottleneck Block
- **Arxiv ID**: http://arxiv.org/abs/1909.01026v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.01026v2)
- **Published**: 2019-09-03 09:53:31+00:00
- **Updated**: 2019-12-07 08:53:02+00:00
- **Authors**: Guoqing Li, Meng Zhang, Qianru Zhang, Ziyang Chen, Wenzhao Liu, Jiaojie Li, Xuzhao Shen, Jianjun Li, Zhenyu Zhu, Chau Yuen
- **Comment**: None
- **Journal**: None
- **Summary**: In many real-time applications, the deployment of deep neural networks is constrained by high computational cost and efficient lightweight neural networks are widely concerned. In this paper, we propose that depthwise convolution (DWC) is used to expand the number of channels in a bottleneck block, which is more efficient than 1 x 1 convolution. The proposed Pointwise-Standard-Depthwise network (PSDNet) based on channel expansion with DWC has fewer number of parameters, less computational cost and higher accuracy than corresponding ResNet on CIFAR datasets. To design more efficient lightweight concolutional neural netwok, Depthwise-Pointwise-Depthwise inverted bottleneck block (DPD block) is proposed and DPDNet is designed by stacking DPD block. Meanwhile, the number of parameters of DPDNet is only about 60% of that of MobileNetV2 for networks with the same number of layers, but can achieve approximated accuracy. Additionally, two hyperparameters of DPDNet can make the trade-off between accuracy and computational cost, which makes DPDNet suitable for diverse tasks. Furthermore, we find the networks with more DWC layers outperform the networks with more 1x1 convolution layers, which indicates that extracting spatial information is more important than combining channel information.



### Unsupervised Video Depth Estimation Based on Ego-motion and Disparity Consensus
- **Arxiv ID**: http://arxiv.org/abs/1909.01028v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.01028v1)
- **Published**: 2019-09-03 09:55:56+00:00
- **Updated**: 2019-09-03 09:55:56+00:00
- **Authors**: Lingtao Zhou, Jiaojiao Fang, Guizhong Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised learning based depth estimation methods have received more and more attention as they do not need vast quantities of densely labeled data for training which are touch to acquire. In this paper, we propose a novel unsupervised monocular video depth estimation method in natural scenes by taking advantage of the state-of-the-art method of Zhou et al. which jointly estimates depth and camera motion. Our method advances beyond the baseline method by three aspects: 1) we add an additional signal as supervision to the baseline method by incorporating left-right binocular images reconstruction loss based on the estimated disparities, thus the left frame can be reconstructed by the temporal frames and right frames of stereo vision; 2) the network is trained by jointly using two kinds of view syntheses loss and left-right disparity consistency regularization to estimate depth and pose simultaneously; 3) we use the edge aware smooth L2 regularization to smooth the depth map while preserving the contour of the target. Extensive experiments on the KITTI autonomous driving dataset and Make3D dataset indicate the superiority of our algorithm in training efficiency. We can achieve competitive results with the baseline by only 3/5 times training data. The experimental results also show that our method even outperforms the classical supervised methods that using either ground truth depth or given pose for training.



### A Geometry-Sensitive Approach for Photographic Style Classification
- **Arxiv ID**: http://arxiv.org/abs/1909.01040v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.01040v1)
- **Published**: 2019-09-03 10:22:40+00:00
- **Updated**: 2019-09-03 10:22:40+00:00
- **Authors**: Koustav Ghosal, Mukta Prasad, Aljosa Smolic
- **Comment**: Irish Machine Vision and Image Processing Conference, Belfast, 2018
- **Journal**: Irish Pattern Recognition and Classication Society (iprcs.org)
  2018
- **Summary**: Photographs are characterized by different compositional attributes like the Rule of Thirds, depth of field, vanishing-lines etc. The presence or absence of one or more of these attributes contributes to the overall artistic value of an image. In this work, we analyze the ability of deep learning based methods to learn such photographic style attributes. We observe that although a standard CNN learns the texture and appearance based features reasonably well, its understanding of global and geometric features is limited by two factors. First, the data-augmentation strategies (cropping, warping, etc.) distort the composition of a photograph and affect the performance. Secondly, the CNN features, in principle, are translation-invariant and appearance-dependent. But some geometric properties important for aesthetics, e.g. the Rule of Thirds (RoT), are position-dependent and appearance-invariant. Therefore, we propose a novel input representation which is geometry-sensitive, position-cognizant and appearance-invariant. We further introduce a two-column CNN architecture that performs better than the state-of-the-art (SoA) in photographic style classification. From our results, we observe that the proposed network learns both the geometric and appearance-based attributes better than the SoA.



### MANAS: Multi-Agent Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1909.01051v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/1909.01051v4)
- **Published**: 2019-09-03 10:36:37+00:00
- **Updated**: 2023-01-12 11:00:56+00:00
- **Authors**: Vasco Lopes, Fabio Maria Carlucci, Pedro M Esperança, Marco Singh, Victor Gabillon, Antoine Yang, Hang Xu, Zewei Chen, Jun Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The Neural Architecture Search (NAS) problem is typically formulated as a graph search problem where the goal is to learn the optimal operations over edges in order to maximise a graph-level global objective. Due to the large architecture parameter space, efficiency is a key bottleneck preventing NAS from its practical use. In this paper, we address the issue by framing NAS as a multi-agent problem where agents control a subset of the network and coordinate to reach optimal architectures. We provide two distinct lightweight implementations, with reduced memory requirements (1/8th of state-of-the-art), and performances above those of much more computationally expensive methods. Theoretically, we demonstrate vanishing regrets of the form O(sqrt(T)), with T being the total number of rounds. Finally, aware that random search is an, often ignored, effective baseline we perform additional experiments on 3 alternative datasets and 2 network configurations, and achieve favourable results in comparison.



### STaDA: Style Transfer as Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.01056v1
- **DOI**: 10.5220/0007353401070114
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.01056v1)
- **Published**: 2019-09-03 10:48:42+00:00
- **Updated**: 2019-09-03 10:48:42+00:00
- **Authors**: Xu Zheng, Tejo Chalasani, Koustav Ghosal, Sebastian Lutz, Aljosa Smolic
- **Comment**: 14th International Conference on Computer Vision Theory and
  Applications, 2019
- **Journal**: None
- **Summary**: The success of training deep Convolutional Neural Networks (CNNs) heavily depends on a significant amount of labelled data. Recent research has found that neural style transfer algorithms can apply the artistic style of one image to another image without changing the latter's high-level semantic content, which makes it feasible to employ neural style transfer as a data augmentation method to add more variation to the training dataset. The contribution of this paper is a thorough evaluation of the effectiveness of the neural style transfer as a data augmentation method for image classification tasks. We explore the state-of-the-art neural style transfer algorithms and apply them as a data augmentation method on Caltech 101 and Caltech 256 dataset, where we found around 2% improvement from 83% to 85% of the image classification accuracy with VGG16, compared with traditional data augmentation strategies. We also combine this new method with conventional data augmentation approaches to further improve the performance of image classification. This work shows the potential of neural style transfer in computer vision field, such as helping us to reduce the difficulty of collecting sufficient labelled data and improve the performance of generic image-based deep learning algorithms.



### Knowledge Distillation for End-to-End Person Search
- **Arxiv ID**: http://arxiv.org/abs/1909.01058v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.01058v2)
- **Published**: 2019-09-03 10:53:17+00:00
- **Updated**: 2019-09-05 18:31:40+00:00
- **Authors**: Bharti Munjal, Fabio Galasso, Sikandar Amin
- **Comment**: The British Machine Vision conference (BMVC), 2019
- **Journal**: None
- **Summary**: We introduce knowledge distillation for end-to-end person search. End-to-End methods are the current state-of-the-art for person search that solve both detection and re-identification jointly. These approaches for joint optimization show their largest drop in performance due to a sub-optimal detector.   We propose two distinct approaches for extra supervision of end-to-end person search methods in a teacher-student setting. The first is adopted from state-of-the-art knowledge distillation in object detection. We employ this to supervise the detector of our person search model at various levels using a specialized detector. The second approach is new, simple and yet considerably more effective. This distills knowledge from a teacher re-identification technique via a pre-computed look-up table of ID features. It relaxes the learning of identification features and allows the student to focus on the detection task. This procedure not only helps fixing the sub-optimal detector training in the joint optimization and simultaneously improving the person search, but also closes the performance gap between the teacher and the student for model compression in this case. Overall, we demonstrate significant improvements for two recent state-of-the-art methods using our proposed knowledge distillation approach on two benchmark datasets. Moreover, on the model compression task our approach brings the performance of smaller models on par with the larger models.



### Face-to-Parameter Translation for Game Character Auto-Creation
- **Arxiv ID**: http://arxiv.org/abs/1909.01064v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.01064v1)
- **Published**: 2019-09-03 11:05:19+00:00
- **Updated**: 2019-09-03 11:05:19+00:00
- **Authors**: Tianyang Shi, Yi Yuan, Changjie Fan, Zhengxia Zou, Zhenwei Shi, Yong Liu
- **Comment**: Accepted by ICCV 2019
- **Journal**: None
- **Summary**: Character customization system is an important component in Role-Playing Games (RPGs), where players are allowed to edit the facial appearance of their in-game characters with their own preferences rather than using default templates. This paper proposes a method for automatically creating in-game characters of players according to an input face photo. We formulate the above "artistic creation" process under a facial similarity measurement and parameter searching paradigm by solving an optimization problem over a large set of physically meaningful facial parameters. To effectively minimize the distance between the created face and the real one, two loss functions, i.e. a "discriminative loss" and a "facial content loss", are specifically designed. As the rendering process of a game engine is not differentiable, a generative network is further introduced as an "imitator" to imitate the physical behavior of the game engine so that the proposed method can be implemented under a neural style transfer framework and the parameters can be optimized by gradient descent. Experimental results demonstrate that our method achieves a high degree of generation similarity between the input face photo and the created in-game character in terms of both global appearance and local details. Our method has been deployed in a new game last year and has now been used by players over 1 million times.



### CGC-Net: Cell Graph Convolutional Network for Grading of Colorectal Cancer Histology Images
- **Arxiv ID**: http://arxiv.org/abs/1909.01068v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.01068v1)
- **Published**: 2019-09-03 11:25:00+00:00
- **Updated**: 2019-09-03 11:25:00+00:00
- **Authors**: Yanning Zhou, Simon Graham, Navid Alemi Koohbanani, Muhammad Shaban, Pheng-Ann Heng, Nasir Rajpoot
- **Comment**: Accepted in ICCVW 2019 (Visual Recognition for Medical Images)
- **Journal**: None
- **Summary**: Colorectal cancer (CRC) grading is typically carried out by assessing the degree of gland formation within histology images. To do this, it is important to consider the overall tissue micro-environment by assessing the cell-level information along with the morphology of the gland. However, current automated methods for CRC grading typically utilise small image patches and therefore fail to incorporate the entire tissue micro-architecture for grading purposes. To overcome the challenges of CRC grading, we present a novel cell-graph convolutional neural network (CGC-Net) that converts each large histology image into a graph, where each node is represented by a nucleus within the original image and cellular interactions are denoted as edges between these nodes according to node similarity. The CGC-Net utilises nuclear appearance features in addition to the spatial location of nodes to further boost the performance of the algorithm. To enable nodes to fuse multi-scale information, we introduce Adaptive GraphSage, which is a graph convolution technique that combines multi-level features in a data-driven way. Furthermore, to deal with redundancy in the graph, we propose a sampling technique that removes nodes in areas of dense nuclear activity. We show that modeling the image as a graph enables us to effectively consider a much larger image (around 16$\times$ larger) than traditional patch-based approaches and model the complex structure of the tissue micro-environment. We construct cell graphs with an average of over 3,000 nodes on a large CRC histology image dataset and report state-of-the-art results as compared to recent patch-based as well as contextual patch-based techniques, demonstrating the effectiveness of our method.



### 3DSiameseNet to Analyze Brain MRI
- **Arxiv ID**: http://arxiv.org/abs/1909.01098v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, 68
- **Links**: [PDF](http://arxiv.org/pdf/1909.01098v1)
- **Published**: 2019-09-03 11:52:25+00:00
- **Updated**: 2019-09-03 11:52:25+00:00
- **Authors**: Cecilia Ostertag, Marie Beurton-Aimar, Thierry Urruty
- **Comment**: published in ICPRS 2019 conference
- **Journal**: None
- **Summary**: Prediction of the cognitive evolution of a person susceptible to develop a neurodegenerative disorder is crucial to provide an appropriate treatment as soon as possible. In this paper we propose a 3D siamese network designed to extract features from whole-brain 3D MRI images. We show that it is possible to extract meaningful features using convolution layers, reducing the need of classical image processing operations such as segmentation or pre-computing features such as cortical thickness. To lead this study we used the Alzheimer's Disease Neuroimaging Initiative (ADNI), a public data base of 3D MRI brain images. A set of 247 subjects has been extracted, all of the subjects having 2 images in a range of 12 months. In order to measure the evolution of the patients states we have compared these 2 images. Our work has been inspired at the beginning by an article of Bhagwat et al. in 2018, who have proposed a siamese network to predict the status of patients but without any convolutional layers and reducing the MRI images to a vector of features extracted from predefined ROIs. We show that our network achieves an accuracy of 90\% in the classification of cognitively declining VS stable patients. This result has been obtained without the help of a cognitive score and with a small number of patients comparing to the current datasets size claimed in deep learning domain.



### ForkNet: Multi-branch Volumetric Semantic Completion from a Single Depth Image
- **Arxiv ID**: http://arxiv.org/abs/1909.01106v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CG, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.01106v1)
- **Published**: 2019-09-03 12:04:39+00:00
- **Updated**: 2019-09-03 12:04:39+00:00
- **Authors**: Yida Wang, David Joseph Tan, Nassir Navab, Federico Tombari
- **Comment**: Accepted in International Conference on Computer Vision 2019
- **Journal**: None
- **Summary**: We propose a novel model for 3D semantic completion from a single depth image, based on a single encoder and three separate generators used to reconstruct different geometric and semantic representations of the original and completed scene, all sharing the same latent space. To transfer information between the geometric and semantic branches of the network, we introduce paths between them concatenating features at corresponding network layers. Motivated by the limited amount of training samples from real scenes, an interesting attribute of our architecture is the capacity to supplement the existing dataset by generating a new training dataset with high quality, realistic scenes that even includes occlusion and real noise. We build the new dataset by sampling the features directly from latent space which generates a pair of partial volumetric surface and completed volumetric semantic surface. Moreover, we utilize multiple discriminators to increase the accuracy and realism of the reconstructions. We demonstrate the benefits of our approach on standard benchmarks for the two most common completion tasks: semantic 3D scene completion and 3D object completion.



### Illuminated Decision Trees with Lucid
- **Arxiv ID**: http://arxiv.org/abs/1909.05644v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.05644v1)
- **Published**: 2019-09-03 12:32:44+00:00
- **Updated**: 2019-09-03 12:32:44+00:00
- **Authors**: David Mott, Richard Tomsett
- **Comment**: Presented at BMVC 2019: Workshop on Interpretable and Explainable
  Machine Vision, Cardiff, UK
- **Journal**: None
- **Summary**: The Lucid methods described by Olah et al. (2018) provide a way to inspect the inner workings of neural networks trained on image classification tasks using feature visualization. Such methods have generally been applied to networks trained on visually rich, large-scale image datasets like ImageNet, which enables them to produce enticing feature visualizations. To investigate these methods further, we applied them to classifiers trained to perform the much simpler (in terms of dataset size and visual richness), yet challenging task of distinguishing between different kinds of white blood cell from microscope images. Such a task makes generating useful feature visualizations difficult, as the discriminative features are inherently hard to identify and interpret. We address this by presenting the "Illuminated Decision Tree" approach, in which we use a neural network trained on the task as a feature extractor, then learn a decision tree based on these features, and provide Lucid visualizations for each node in the tree. We demonstrate our approach with several examples, showing how this approach could be useful both in model development and debugging, and when explaining model outputs to non-experts.



### Fast and Efficient Model for Real-Time Tiger Detection In The Wild
- **Arxiv ID**: http://arxiv.org/abs/1909.01122v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.01122v1)
- **Published**: 2019-09-03 12:47:23+00:00
- **Updated**: 2019-09-03 12:47:23+00:00
- **Authors**: Orest Kupyn, Dmitry Pranchuk
- **Comment**: None
- **Journal**: None
- **Summary**: The highest accuracy object detectors to date are based either on a two-stage approach such as Fast R-CNN or one-stage detectors such as Retina-Net or SSD with deep and complex backbones. In this paper we present TigerNet - simple yet efficient FPN based network architecture for Amur Tiger Detection in the wild. The model has 600k parameters, requires 0.071 GFLOPs per image and can run on the edge devices (smart cameras) in near real time. In addition, we introduce a two-stage semi-supervised learning via pseudo-labelling learning approach to distill the knowledge from the larger networks. For ATRW-ICCV 2019 tiger detection sub-challenge, based on public leaderboard score, our approach shows superior performance in comparison to other methods.



### MRI Reconstruction Using Deep Bayesian Estimation
- **Arxiv ID**: http://arxiv.org/abs/1909.01127v3
- **DOI**: 10.1002/mrm.28274
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1909.01127v3)
- **Published**: 2019-09-03 12:54:58+00:00
- **Updated**: 2022-02-17 17:32:46+00:00
- **Authors**: GuanXiong Luo, Na Zhao, Wenhao Jiang, Edward S. Hui, Peng Cao
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: To develop a deep learning-based Bayesian inference for MRI reconstruction. Methods: We modeled the MRI reconstruction problem with Bayes's theorem, following the recently proposed PixelCNN++ method. The image reconstruction from incomplete k-space measurement was obtained by maximizing the posterior possibility. A generative network was utilized as the image prior, which was computationally tractable, and the k-space data fidelity was enforced by using an equality constraint. The stochastic backpropagation was utilized to calculate the descent gradient in the process of maximum a posterior, and a projected subgradient method was used to impose the equality constraint. In contrast to the other deep learning reconstruction methods, the proposed one used the likelihood of prior as the training loss and the objective function in reconstruction to improve the image quality. Results: The proposed method showed an improved performance in preserving image details and reducing aliasing artifacts, compared with GRAPPA, $\ell_1$-ESPRiT, and MODL, a state-of-the-art deep learning reconstruction method. The proposed method generally achieved more than 5 dB peak signal-to-noise ratio improvement for compressed sensing and parallel imaging reconstructions compared with the other methods. Conclusion: The Bayesian inference significantly improved the reconstruction performance, compared with the conventional $\ell_1$-sparsity prior in compressed sensing reconstruction tasks. More importantly, the proposed reconstruction framework can be generalized for most MRI reconstruction scenarios.



### A Tool for Super-Resolving Multimodal Clinical MRI
- **Arxiv ID**: http://arxiv.org/abs/1909.01140v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.01140v1)
- **Published**: 2019-09-03 13:03:32+00:00
- **Updated**: 2019-09-03 13:03:32+00:00
- **Authors**: Mikael Brudfors, Yael Balbastre, Parashkev Nachev, John Ashburner
- **Comment**: None
- **Journal**: None
- **Summary**: We present a tool for resolution recovery in multimodal clinical magnetic resonance imaging (MRI). Such images exhibit great variability, both biological and instrumental. This variability makes automated processing with neuroimaging analysis software very challenging. This leaves intelligence extractable only from large-scale analyses of clinical data untapped, and impedes the introduction of automated predictive systems in clinical care. The tool presented in this paper enables such processing, via inference in a generative model of thick-sliced, multi-contrast MR scans. All model parameters are estimated from the observed data, without the need for manual tuning. The model-driven nature of the approach means that no type of training is needed for applicability to the diversity of MR contrasts present in a clinical context. We show on simulated data that the proposed approach outperforms conventional model-based techniques, and on a large hospital dataset of multimodal MRIs that the tool can successfully super-resolve very thick-sliced images. The implementation is available from https://github.com/brudfors/spm_superres.



### Embedding Symbolic Knowledge into Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.01161v4
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1909.01161v4)
- **Published**: 2019-09-03 13:23:25+00:00
- **Updated**: 2019-10-29 10:53:02+00:00
- **Authors**: Yaqi Xie, Ziwei Xu, Mohan S. Kankanhalli, Kuldeep S. Meel, Harold Soh
- **Comment**: *Equal contribution; Accepted at conference Neural Information
  Processing Systems (NeurIPS), 2019
- **Journal**: None
- **Summary**: In this work, we aim to leverage prior symbolic knowledge to improve the performance of deep models. We propose a graph embedding network that projects propositional formulae (and assignments) onto a manifold via an augmented Graph Convolutional Network (GCN). To generate semantically-faithful embeddings, we develop techniques to recognize node heterogeneity, and semantic regularization that incorporate structural constraints into the embedding. Experiments show that our approach improves the performance of models trained to perform entailment checking and visual relation prediction. Interestingly, we observe a connection between the tractability of the propositional theory representation and the ease of embedding. Future exploration of this connection may elucidate the relationship between knowledge compilation and vector representation learning.



### Combining Multi-Sequence and Synthetic Images for Improved Segmentation of Late Gadolinium Enhancement Cardiac MRI
- **Arxiv ID**: http://arxiv.org/abs/1909.01182v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.01182v2)
- **Published**: 2019-09-03 13:50:46+00:00
- **Updated**: 2020-01-13 14:06:44+00:00
- **Authors**: Víctor M. Campello, Carlos Martín-Isla, Cristian Izquierdo, Steffen E. Petersen, Miguel A. González Ballester, Karim Lekadir
- **Comment**: 10 pages, Accepted to MS-CMRSeg Challenge (STACOM 2019), reference
  added and affiliations updated
- **Journal**: None
- **Summary**: Accurate segmentation of the cardiac boundaries in late gadolinium enhancement magnetic resonance images (LGE-MRI) is a fundamental step for accurate quantification of scar tissue. However, while there are many solutions for automatic cardiac segmentation of cine images, the presence of scar tissue can make the correct delineation of the myocardium in LGE-MRI challenging even for human experts. As part of the Multi-Sequence Cardiac MR Segmentation Challenge, we propose a solution for LGE-MRI segmentation based on two components. First, a generative adversarial network is trained for the task of modality-to-modality translation between cine and LGE-MRI sequences to obtain extra synthetic images for both modalities. Second, a deep learning model is trained for segmentation with different combinations of original, augmented and synthetic sequences. Our results based on three magnetic resonance sequences (LGE, bSSFP and T2) from 45 different patients show that the multi-sequence model training integrating synthetic images and data augmentation improves in the segmentation over conventional training with real datasets. In conclusion, the accuracy of the segmentation of LGE-MRI images can be improved by using complementary information provided by non-contrast MRI sequences.



### Can we trust deep learning models diagnosis? The impact of domain shift in chest radiograph classification
- **Arxiv ID**: http://arxiv.org/abs/1909.01940v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.01940v2)
- **Published**: 2019-09-03 14:03:55+00:00
- **Updated**: 2020-06-22 22:50:10+00:00
- **Authors**: Eduardo H. P. Pooch, Pedro L. Ballester, Rodrigo C. Barros
- **Comment**: 10 pages, 3 figures
- **Journal**: None
- **Summary**: While deep learning models become more widespread, their ability to handle unseen data and generalize for any scenario is yet to be challenged. In medical imaging, there is a high heterogeneity of distributions among images based on the equipment that generates them and their parametrization. This heterogeneity triggers a common issue in machine learning called domain shift, which represents the difference between the training data distribution and the distribution of where a model is employed. A high domain shift tends to implicate in a poor generalization performance from the models. In this work, we evaluate the extent of domain shift on four of the largest datasets of chest radiographs. We show how training and testing with different datasets (e.g., training in ChestX-ray14 and testing in CheXpert) drastically affects model performance, posing a big question over the reliability of deep learning models trained on public datasets. We also show that models trained on CheXpert and MIMIC-CXR generalize better to other datasets.



### Self-Supervised Deep Depth Denoising
- **Arxiv ID**: http://arxiv.org/abs/1909.01193v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.01193v2)
- **Published**: 2019-09-03 14:08:32+00:00
- **Updated**: 2019-09-04 09:18:14+00:00
- **Authors**: Vladimiros Sterzentsenko, Leonidas Saroglou, Anargyros Chatzitofis, Spyridon Thermos, Nikolaos Zioulis, Alexandros Doumanoglou, Dimitrios Zarpalas, Petros Daras
- **Comment**: 18 pages, 15 figures, ICCV 2019
- **Journal**: None
- **Summary**: Depth perception is considered an invaluable source of information for various vision tasks. However, depth maps acquired using consumer-level sensors still suffer from non-negligible noise. This fact has recently motivated researchers to exploit traditional filters, as well as the deep learning paradigm, in order to suppress the aforementioned non-uniform noise, while preserving geometric details. Despite the effort, deep depth denoising is still an open challenge mainly due to the lack of clean data that could be used as ground truth. In this paper, we propose a fully convolutional deep autoencoder that learns to denoise depth maps, surpassing the lack of ground truth data. Specifically, the proposed autoencoder exploits multiple views of the same scene from different points of view in order to learn to suppress noise in a self-supervised end-to-end manner using depth and color information during training, yet only depth during inference. To enforce selfsupervision, we leverage a differentiable rendering technique to exploit photometric supervision, which is further regularized using geometric and surface priors. As the proposed approach relies on raw data acquisition, a large RGB-D corpus is collected using Intel RealSense sensors. Complementary to a quantitative evaluation, we demonstrate the effectiveness of the proposed self-supervised denoising approach on established 3D reconstruction applications. Code is avalable at https://github.com/VCL3D/DeepDepthDenoising



### Cross View Fusion for 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1909.01203v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.01203v1)
- **Published**: 2019-09-03 14:15:30+00:00
- **Updated**: 2019-09-03 14:15:30+00:00
- **Authors**: Haibo Qiu, Chunyu Wang, Jingdong Wang, Naiyan Wang, Wenjun Zeng
- **Comment**: Accepted by ICCV 2019
- **Journal**: None
- **Summary**: We present an approach to recover absolute 3D human poses from multi-view images by incorporating multi-view geometric priors in our model. It consists of two separate steps: (1) estimating the 2D poses in multi-view images and (2) recovering the 3D poses from the multi-view 2D poses. First, we introduce a cross-view fusion scheme into CNN to jointly estimate 2D poses for multiple views. Consequently, the 2D pose estimation for each view already benefits from other views. Second, we present a recursive Pictorial Structure Model to recover the 3D pose from the multi-view 2D poses. It gradually improves the accuracy of 3D pose with affordable computational cost. We test our method on two public datasets H36M and Total Capture. The Mean Per Joint Position Errors on the two datasets are 26mm and 29mm, which outperforms the state-of-the-arts remarkably (26mm vs 52mm, 29mm vs 35mm). Our code is released at \url{https://github.com/microsoft/multiview-human-pose-estimation-pytorch}.



### Few-Shot Generalization for Single-Image 3D Reconstruction via Priors
- **Arxiv ID**: http://arxiv.org/abs/1909.01205v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.01205v1)
- **Published**: 2019-09-03 14:18:42+00:00
- **Updated**: 2019-09-03 14:18:42+00:00
- **Authors**: Bram Wallace, Bharath Hariharan
- **Comment**: To appear in ICCV 2019
- **Journal**: None
- **Summary**: Recent work on single-view 3D reconstruction shows impressive results, but has been restricted to a few fixed categories where extensive training data is available. The problem of generalizing these models to new classes with limited training data is largely open. To address this problem, we present a new model architecture that reframes single-view 3D reconstruction as learnt, category agnostic refinement of a provided, category-specific prior. The provided prior shape for a novel class can be obtained from as few as one 3D shape from this class. Our model can start reconstructing objects from the novel class using this prior without seeing any training image for this class and without any retraining. Our model outperforms category-agnostic baselines and remains competitive with more sophisticated baselines that finetune on the novel categories. Additionally, our network is capable of improving the reconstruction given multiple views despite not being trained on task of multi-view reconstruction.



### Efficient Real-Time Camera Based Estimation of Heart Rate and Its Variability
- **Arxiv ID**: http://arxiv.org/abs/1909.01206v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.01206v1)
- **Published**: 2019-09-03 14:21:11+00:00
- **Updated**: 2019-09-03 14:21:11+00:00
- **Authors**: Amogh Gudi, Marian Bittner, Roelof Lochmans, Jan van Gemert
- **Comment**: International Conference on Computer Vision (ICCV) Workshop on
  Computer Vision for Physiological Measurement (CVPM) 2019
- **Journal**: None
- **Summary**: Remote photo-plethysmography (rPPG) uses a remotely placed camera to estimating a person's heart rate (HR). Similar to how heart rate can provide useful information about a person's vital signs, insights about the underlying physio/psychological conditions can be obtained from heart rate variability (HRV). HRV is a measure of the fine fluctuations in the intervals between heart beats. However, this measure requires temporally locating heart beats with a high degree of precision. We introduce a refined and efficient real-time rPPG pipeline with novel filtering and motion suppression that not only estimates heart rate more accurately, but also extracts the pulse waveform to time heart beats and measure heart rate variability. This method requires no rPPG specific training and is able to operate in real-time. We validate our method on a self-recorded dataset under an idealized lab setting, and show state-of-the-art results on two public dataset with realistic conditions (VicarPPG and PURE).



### A Low-Cost, Flexible and Portable Volumetric Capturing System
- **Arxiv ID**: http://arxiv.org/abs/1909.01207v1
- **DOI**: 10.1109/SITIS.2018.00038
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.01207v1)
- **Published**: 2019-09-03 14:23:24+00:00
- **Updated**: 2019-09-03 14:23:24+00:00
- **Authors**: Vladimiros Sterzentsenko, Antonis Karakottas, Alexandros Papachristou, Nikolaos Zioulis, Alexandros Doumanoglou, Dimitrios Zarpalas, Petros Daras
- **Comment**: System available at https://github.com/VCL3D/VolumetricCapture
- **Journal**: None
- **Summary**: Multi-view capture systems are complex systems to engineer. They require technical knowledge to install and intricate processes to setup related mainly to the sensors' spatial alignment (i.e. external calibration). However, with the ongoing developments in new production methods, we are now at a position where the production of high quality realistic 3D assets is possible even with commodity sensors. Nonetheless, the capturing systems developed with these methods are heavily intertwined with the methods themselves, relying on custom solutions and seldom - if not at all - publicly available. In light of this, we design, develop and publicly offer a multi-view capture system based on the latest RGB-D sensor technology. For our system, we develop a portable and easy-to-use external calibration method that greatly reduces the effort and knowledge required, as well as simplify the overall process.



### Translating Visual Art into Music
- **Arxiv ID**: http://arxiv.org/abs/1909.01218v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1909.01218v1)
- **Published**: 2019-09-03 14:36:19+00:00
- **Updated**: 2019-09-03 14:36:19+00:00
- **Authors**: Maximilian Müller-Eberstein, Nanne van Noord
- **Comment**: Accepted for ICCV 2019 Workshop on Fashion, Art and Design
- **Journal**: None
- **Summary**: The Synesthetic Variational Autoencoder (SynVAE) introduced in this research is able to learn a consistent mapping between visual and auditive sensory modalities in the absence of paired datasets. A quantitative evaluation on MNIST as well as the Behance Artistic Media dataset (BAM) shows that SynVAE is capable of retaining sufficient information content during the translation while maintaining cross-modal latent space consistency. In a qualitative evaluation trial, human evaluators were furthermore able to match musical samples with the images which generated them with accuracies of up to 73%.



### A CNN-based approach to classify cricket bowlers based on their bowling actions
- **Arxiv ID**: http://arxiv.org/abs/1909.01228v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.01228v1)
- **Published**: 2019-09-03 14:45:40+00:00
- **Updated**: 2019-09-03 14:45:40+00:00
- **Authors**: Md Nafee Al Islam, Tanzil Bin Hassan, Siamul Karim Khan
- **Comment**: 5 pages, 7 figures, The paper is under review in "IEEE International
  Conference on Robotics, Automation, Artificial-Intelligence and
  Internet-of-Things, 2019"
- **Journal**: None
- **Summary**: With the advances in hardware technologies and deep learning techniques, it has become feasible to apply these techniques in diverse fields. Convolutional Neural Network (CNN), an architecture from the field of deep learning, has revolutionized Computer Vision. Sports is one of the avenues in which the use of computer vision is thriving. Cricket is a complex game consisting of different types of shots, bowling actions and many other activities. Every bowler, in a game of cricket, bowls with a different bowling action. We leverage this point to identify different bowlers. In this paper, we have proposed a CNN model to identify eighteen different cricket bowlers based on their bowling actions using transfer learning. Additionally, we have created a completely new dataset containing 8100 images of these eighteen bowlers to train the proposed framework and evaluate its performance. We have used the VGG16 model pre-trained with the ImageNet dataset and added a few layers on top of it to build our model. After trying out different strategies, we found that freezing the weights for the first 14 layers of the network and training the rest of the layers works best. Our approach achieves an overall average accuracy of 93.3% on the test set and converges to a very low cross-entropy loss.



### Online Pedestrian Group Walking Event Detection Using Spectral Analysis of Motion Similarity Graph
- **Arxiv ID**: http://arxiv.org/abs/1909.01258v1
- **DOI**: 10.1109/AVSS.2015.7301744
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.01258v1)
- **Published**: 2019-09-03 15:49:47+00:00
- **Updated**: 2019-09-03 15:49:47+00:00
- **Authors**: Vahid Bastani, Damian Campo, Lucio Marcenaro, Carlo S. Regazzoni
- **Comment**: Published in: 2015 12th IEEE International Conference on Advanced
  Video and Signal Based Surveillance (AVSS)
- **Journal**: None
- **Summary**: A method for online identification of group of moving objects in the video is proposed in this paper. This method at each frame identifies group of tracked objects with similar local instantaneous motion pattern using spectral clustering on motion similarity graph. Then, the output of the algorithm is used to detect the event of more than two object moving together as required by PETS2015 challenge. The performance of the algorithm is evaluated on the PETS2015 dataset.



### Robust Invisible Video Watermarking with Attention
- **Arxiv ID**: http://arxiv.org/abs/1909.01285v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.01285v1)
- **Published**: 2019-09-03 16:28:40+00:00
- **Updated**: 2019-09-03 16:28:40+00:00
- **Authors**: Kevin Alex Zhang, Lei Xu, Alfredo Cuesta-Infante, Kalyan Veeramachaneni
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of video watermarking is to embed a message within a video file in a way such that it minimally impacts the viewing experience but can be recovered even if the video is redistributed and modified, allowing media producers to assert ownership over their content. This paper presents RivaGAN, a novel architecture for robust video watermarking which features a custom attention-based mechanism for embedding arbitrary data as well as two independent adversarial networks which critique the video quality and optimize for robustness. Using this technique, we are able to achieve state-of-the-art results in deep learning-based video watermarking and produce watermarked videos which have minimal visual distortion and are robust against common video processing operations.



### 3D Morphable Face Models -- Past, Present and Future
- **Arxiv ID**: http://arxiv.org/abs/1909.01815v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.01815v2)
- **Published**: 2019-09-03 16:49:53+00:00
- **Updated**: 2020-04-16 13:56:31+00:00
- **Authors**: Bernhard Egger, William A. P. Smith, Ayush Tewari, Stefanie Wuhrer, Michael Zollhoefer, Thabo Beeler, Florian Bernard, Timo Bolkart, Adam Kortylewski, Sami Romdhani, Christian Theobalt, Volker Blanz, Thomas Vetter
- **Comment**: ACM Transactions on Graphics (TOG)
- **Journal**: None
- **Summary**: In this paper, we provide a detailed survey of 3D Morphable Face Models over the 20 years since they were first proposed. The challenges in building and applying these models, namely capture, modeling, image formation, and image analysis, are still active research topics, and we review the state-of-the-art in each of these areas. We also look ahead, identifying unsolved challenges, proposing directions for future research and highlighting the broad range of current and future applications.



### Dual Student: Breaking the Limits of the Teacher in Semi-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.01804v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.01804v1)
- **Published**: 2019-09-03 17:32:11+00:00
- **Updated**: 2019-09-03 17:32:11+00:00
- **Authors**: Zhanghan Ke, Daoye Wang, Qiong Yan, Jimmy Ren, Rynson W. H. Lau
- **Comment**: International Conference in Computer Vision 2019 (ICCV 2019)
- **Journal**: None
- **Summary**: Recently, consistency-based methods have achieved state-of-the-art results in semi-supervised learning (SSL). These methods always involve two roles, an explicit or implicit teacher model and a student model, and penalize predictions under different perturbations by a consistency constraint. However, the weights of these two roles are tightly coupled since the teacher is essentially an exponential moving average (EMA) of the student. In this work, we show that the coupled EMA teacher causes a performance bottleneck. To address this problem, we introduce Dual Student, which replaces the teacher with another student. We also define a novel concept, stable sample, following which a stabilization constraint is designed for our structure to be trainable. Further, we discuss two variants of our method, which produce even higher performance. Extensive experiments show that our method improves the classification performance significantly on several main SSL benchmarks. Specifically, it reduces the error rate of the 13-layer CNN from 16.84% to 12.39% on CIFAR-10 with 1k labels and from 34.10% to 31.56% on CIFAR-100 with 10k labels. In addition, our method also achieves a clear improvement in domain adaptation.



### Do Cross Modal Systems Leverage Semantic Relationships?
- **Arxiv ID**: http://arxiv.org/abs/1909.01976v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.01976v1)
- **Published**: 2019-09-03 18:33:38+00:00
- **Updated**: 2019-09-03 18:33:38+00:00
- **Authors**: Shah Nawaz, Muhammad Kamran Janjua, Ignazio Gallo, Arif Mahmood, Alessandro Calefati, Faisal Shafait
- **Comment**: Accepted to cross modal learning in real world in conjunction with
  ICCV 2019. arXiv admin note: text overlap with arXiv:1807.07364
- **Journal**: None
- **Summary**: Current cross-modal retrieval systems are evaluated using R@K measure which does not leverage semantic relationships rather strictly follows the manually marked image text query pairs. Therefore, current systems do not generalize well for the unseen data in the wild. To handle this, we propose a new measure, SemanticMap, to evaluate the performance of cross-modal systems. Our proposed measure evaluates the semantic similarity between the image and text representations in the latent embedding space. We also propose a novel cross-modal retrieval system using a single stream network for bidirectional retrieval. The proposed system is based on a deep neural network trained using extended center loss, minimizing the distance of image and text descriptions in the latent space from the class centers. In our system, the text descriptions are also encoded as images which enabled us to use a single stream network for both text and images. To the best of our knowledge, our work is the first of its kind in terms of employing a single stream network for cross-modal retrieval systems. The proposed system is evaluated on two publicly available datasets including MSCOCO and Flickr30K and has shown comparable results to the current state-of-the-art methods.



### A Novel Loss Function Incorporating Imaging Acquisition Physics for PET Attenuation Map Generation using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.01394v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1909.01394v1)
- **Published**: 2019-09-03 18:40:52+00:00
- **Updated**: 2019-09-03 18:40:52+00:00
- **Authors**: Luyao Shi, John A. Onofrey, Enette Mae Revilla, Takuya Toyonaga, David Menard, Jo-seph Ankrah, Richard E. Carson, Chi Liu, Yihuan Lu
- **Comment**: Accepted at MICCAI 2019
- **Journal**: None
- **Summary**: In PET/CT imaging, CT is used for PET attenuation correction (AC). Mismatch between CT and PET due to patient body motion results in AC artifacts. In addition, artifact caused by metal, beam-hardening and count-starving in CT itself also introduces inaccurate AC for PET. Maximum likelihood reconstruction of activity and attenuation (MLAA) was proposed to solve those issues by simultaneously reconstructing tracer activity ($\lambda$-MLAA) and attenuation map ($\mu$-MLAA) based on the PET raw data only. However, $\mu$-MLAA suffers from high noise and $\lambda$-MLAA suffers from large bias as compared to the reconstruction using the CT-based attenuation map ($\mu$-CT). Recently, a convolutional neural network (CNN) was applied to predict the CT attenuation map ($\mu$-CNN) from $\lambda$-MLAA and $\mu$-MLAA, in which an image-domain loss (IM-loss) function between the $\mu$-CNN and the ground truth $\mu$-CT was used. However, IM-loss does not directly measure the AC errors according to the PET attenuation physics, where the line-integral projection of the attenuation map ($\mu$) along the path of the two annihilation events, instead of the $\mu$ itself, is used for AC. Therefore, a network trained with the IM-loss may yield suboptimal performance in the $\mu$ generation. Here, we propose a novel line-integral projection loss (LIP-loss) function that incorporates the PET attenuation physics for $\mu$ generation. Eighty training and twenty testing datasets of whole-body 18F-FDG PET and paired ground truth $\mu$-CT were used. Quantitative evaluations showed that the model trained with the additional LIP-loss was able to significantly outperform the model trained solely based on the IM-loss function.



### Mixture Probabilistic Principal Geodesic Analysis
- **Arxiv ID**: http://arxiv.org/abs/1909.01412v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.01412v2)
- **Published**: 2019-09-03 19:22:57+00:00
- **Updated**: 2019-09-05 08:23:42+00:00
- **Authors**: Youshan Zhang, Jiarui Xing, Miaomiao Zhang
- **Comment**: Seventh MICCAI Workshop on Mathematical Foundations of Computational
  Anatomy
- **Journal**: None
- **Summary**: Dimensionality reduction on Riemannian manifolds is challenging due to the complex nonlinear data structures. While probabilistic principal geodesic analysis~(PPGA) has been proposed to generalize conventional principal component analysis (PCA) onto manifolds, its effectiveness is limited to data with a single modality. In this paper, we present a novel Gaussian latent variable model that provides a unique way to integrate multiple PGA models into a maximum-likelihood framework. This leads to a well-defined mixture model of probabilistic principal geodesic analysis (MPPGA) on sub-populations, where parameters of the principal subspaces are automatically estimated by employing an Expectation Maximization algorithm. We further develop a mixture Bayesian PGA (MBPGA) model that automatically reduces data dimensionality by suppressing irrelevant principal geodesics. We demonstrate the advantages of our model in the contexts of clustering and statistical shape analysis, using synthetic sphere data, real corpus callosum, and mandible data from human brain magnetic resonance~(MR) and CT images.



### Multi-level Attention network using text, audio and video for Depression Prediction
- **Arxiv ID**: http://arxiv.org/abs/1909.01417v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1909.01417v1)
- **Published**: 2019-09-03 19:40:38+00:00
- **Updated**: 2019-09-03 19:40:38+00:00
- **Authors**: Anupama Ray, Siddharth Kumar, Rutvik Reddy, Prerana Mukherjee, Ritu Garg
- **Comment**: in Proceedings of the 9th International Workshop on Audio/Visual
  Emotion Challenge, AVEC 2019, ACM Multimedia Workshop, Nice, France
- **Journal**: None
- **Summary**: Depression has been the leading cause of mental-health illness worldwide. Major depressive disorder (MDD), is a common mental health disorder that affects both psychologically as well as physically which could lead to loss of lives. Due to the lack of diagnostic tests and subjectivity involved in detecting depression, there is a growing interest in using behavioural cues to automate depression diagnosis and stage prediction. The absence of labelled behavioural datasets for such problems and the huge amount of variations possible in behaviour makes the problem more challenging. This paper presents a novel multi-level attention based network for multi-modal depression prediction that fuses features from audio, video and text modalities while learning the intra and inter modality relevance. The multi-level attention reinforces overall learning by selecting the most influential features within each modality for the decision making. We perform exhaustive experimentation to create different regression models for audio, video and text modalities. Several fusions models with different configurations are constructed to understand the impact of each feature and modality. We outperform the current baseline by 17.52% in terms of root mean squared error.



### Topologically-Guided Color Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1909.01456v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CG, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.01456v1)
- **Published**: 2019-09-03 21:15:24+00:00
- **Updated**: 2019-09-03 21:15:24+00:00
- **Authors**: Junyi Tu, Paul Rosen
- **Comment**: None
- **Journal**: None
- **Summary**: Enhancement is an important step in post-processing digital images for personal use, in medical imaging, and for object recognition. Most existing manual techniques rely on region selection, similarity, and/or thresholding for editing, never really considering the topological structure of the image. In this paper, we leverage the contour tree to extract a hierarchical representation of the topology of an image. We propose 4 topology-aware transfer functions for editing features of the image using local topological properties, instead of global image properties. Finally, we evaluate our approach with grayscale and color images.



### Iterative Clustering with Game-Theoretic Matching for Robust Multi-consistency Correspondence
- **Arxiv ID**: http://arxiv.org/abs/1909.01497v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.01497v1)
- **Published**: 2019-09-03 23:20:34+00:00
- **Updated**: 2019-09-03 23:20:34+00:00
- **Authors**: Chen Zhao, Jiaqi Yang, Ke Xian, Zhiguo Cao, Xin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Matching corresponding features between two images is a fundamental task to computer vision with numerous applications in object recognition, robotics, and 3D reconstruction. Current state of the art in image feature matching has focused on establishing a single consistency in static scenes; by contrast, finding multiple consistencies in dynamic scenes has been under-researched. In this paper, we present an end-to-end optimization framework named "iterative clustering with Game-Theoretic Matching" (ic-GTM) for robust multi-consistency correspondence. The key idea is to formulate multi-consistency matching as a generalized clustering problem for an image pair. In our formulation, several local matching games are simultaneously carried out in different corresponding block pairs under the guidance of a novel payoff function consisting of both geometric and descriptive compatibility; the global matching results are further iteratively refined by clustering and thresholding with respect to a payoff matrix. We also propose three new metrics for evaluating the performance of multi-consistency image feature matching. Extensive experimental results have shown that the proposed framework significantly outperforms previous state-of-the-art approaches on both singleconsistency and multi-consistency datasets.



### Demystifying Brain Tumour Segmentation Networks: Interpretability and Uncertainty Analysis
- **Arxiv ID**: http://arxiv.org/abs/1909.01498v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.01498v3)
- **Published**: 2019-09-03 23:53:11+00:00
- **Updated**: 2020-01-25 03:04:49+00:00
- **Authors**: Parth Natekar, Avinash Kori, Ganapathy Krishnamurthi
- **Comment**: None
- **Journal**: None
- **Summary**: The accurate automatic segmentation of gliomas and its intra-tumoral structures is important not only for treatment planning but also for follow-up evaluations. Several methods based on 2D and 3D Deep Neural Networks (DNN) have been developed to segment brain tumors and to classify different categories of tumors from different MRI modalities. However, these networks are often black-box models and do not provide any evidence regarding the process they take to perform this task. Increasing transparency and interpretability of such deep learning techniques are necessary for the complete integration of such methods into medical practice. In this paper, we explore various techniques to explain the functional organization of brain tumor segmentation models and to extract visualizations of internal concepts to understand how these networks achieve highly accurate tumor segmentations. We use the BraTS 2018 dataset to train three different networks with standard architectures and outline similarities and differences in the process that these networks take to segment brain tumors. We show that brain tumor segmentation networks learn certain human-understandable disentangled concepts on a filter level. We also show that they take a top-down or hierarchical approach to localizing the different parts of the tumor. We then extract visualizations of some internal feature maps and also provide a measure of uncertainty with regards to the outputs of the models to give additional qualitative evidence about the predictions of these networks. We believe that the emergence of such human-understandable organization and concepts might aid in the acceptance and integration of such methods in medical diagnosis.



