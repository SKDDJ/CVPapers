# Arxiv Papers in cs.CV on 2019-09-30
### SymmetricNet: A mesoscale eddy detection method based on multivariate fusion data
- **Arxiv ID**: http://arxiv.org/abs/1909.13411v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.13411v1)
- **Published**: 2019-09-30 00:58:04+00:00
- **Updated**: 2019-09-30 00:58:04+00:00
- **Authors**: Zhenlin Fan, Guoqiang Zhong
- **Comment**: None
- **Journal**: None
- **Summary**: Mesoscale eddies play a significant role in marine energy transport, marine biological environment and marine climate. Due to their huge impact on the ocean, mesoscale eddy detection has become a hot research area in recent years. Therefore, more and more people are entering the field of mesoscale eddy detection. However, the existing detection methods mainly based on traditional detection methods typically only use Sea Surface Height (SSH) as a variable to detect, resulting in inaccurate performance. In this paper, we propose a mesoscale eddy detection method based on multivariate fusion data to solve this problem. We not only use the SSH variable, but also add the two variables: Sea Surface Temperature (SST) and velocity of flow, achieving a multivariate information fusion input. We design a novel symmetric network, which merges low-level feature maps from the downsampling pathway and high-level feature maps from the upsampling pathway by lateral connection. In addition, we apply dilated convolutions to network structure to increase the receptive field and obtain more contextual information in the case of constant parameter. In the end, we demonstrate the effectiveness of our method on dataset provided by us, achieving the test set performance of 97.06% , greatly improved the performance of previous methods of mesoscale eddy detection.



### Brain-inspired automated visual object discovery and detection
- **Arxiv ID**: http://arxiv.org/abs/1910.04864v1
- **DOI**: 10.1073/pnas.1802103115
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.04864v1)
- **Published**: 2019-09-30 01:55:46+00:00
- **Updated**: 2019-09-30 01:55:46+00:00
- **Authors**: Lichao Chen, Sudhir Singh, Thomas Kailath, Vwani Roychowdhury
- **Comment**: None
- **Journal**: PNAS January 2, 2019 116 (1) 96-105
- **Summary**: Despite significant recent progress, machine vision systems lag considerably behind their biological counterparts in performance, scalability, and robustness. A distinctive hallmark of the brain is its ability to automatically discover and model objects, at multiscale resolutions, from repeated exposures to unlabeled contextual data and then to be able to robustly detect the learned objects under various nonideal circumstances, such as partial occlusion and different view angles. Replication of such capabilities in a machine would require three key ingredients: (i) access to large-scale perceptual data of the kind that humans experience, (ii) flexible representations of objects, and (iii) an efficient unsupervised learning algorithm. The Internet fortunately provides unprecedented access to vast amounts of visual data. This paper leverages the availability of such data to develop a scalable framework for unsupervised learning of object prototypes--brain-inspired flexible, scale, and shift invariant representations of deformable objects (e.g., humans, motorcycles, cars, airplanes) comprised of parts, their different configurations and views, and their spatial relationships. Computationally, the object prototypes are represented as geometric associative networks using probabilistic constructs such as Markov random fields. We apply our framework to various datasets and show that our approach is computationally scalable and can construct accurate and operational part-aware object models much more efficiently than in much of the recent computer vision literature. We also present efficient algorithms for detection and localization in new scenes of objects and their partial views.



### Single-Network Whole-Body Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1909.13423v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.13423v1)
- **Published**: 2019-09-30 02:00:53+00:00
- **Updated**: 2019-09-30 02:00:53+00:00
- **Authors**: Gines Hidalgo, Yaadhav Raaj, Haroon Idrees, Donglai Xiang, Hanbyul Joo, Tomas Simon, Yaser Sheikh
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: We present the first single-network approach for 2D~whole-body pose estimation, which entails simultaneous localization of body, face, hands, and feet keypoints. Due to the bottom-up formulation, our method maintains constant real-time performance regardless of the number of people in the image. The network is trained in a single stage using multi-task learning, through an improved architecture which can handle scale differences between body/foot and face/hand keypoints. Our approach considerably improves upon OpenPose~\cite{cao2018openpose}, the only work so far capable of whole-body pose estimation, both in terms of speed and global accuracy. Unlike OpenPose, our method does not need to run an additional network for each hand and face candidate, making it substantially faster for multi-person scenarios. This work directly results in a reduction of computational complexity for applications that require 2D whole-body information (e.g., VR/AR, re-targeting). In addition, it yields higher accuracy, especially for occluded, blurry, and low resolution faces and hands. For code, trained models, and validation benchmarks, visit our project page: https://github.com/CMU-Perceptual-Computing-Lab/openpose_train.



### Random Bias Initialization Improves Quantized Training
- **Arxiv ID**: http://arxiv.org/abs/1909.13446v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1909.13446v2)
- **Published**: 2019-09-30 04:01:13+00:00
- **Updated**: 2020-04-20 19:50:23+00:00
- **Authors**: Xinlin Li, Vahid Partovi Nia
- **Comment**: None
- **Journal**: None
- **Summary**: Binary neural networks improve computationally efficiency of deep models with a large margin. However, there is still a performance gap between a successful full-precision training and binary training. We bring some insights about why this accuracy drop exists and call for a better understanding of binary network geometry. We start with analyzing full-precision neural networks with ReLU activation and compare it with its binarized version. This comparison suggests to initialize networks with random bias, a counter-intuitive remedy.



### ViLiVO: Virtual LiDAR-Visual Odometry for an Autonomous Vehicle with a Multi-Camera System
- **Arxiv ID**: http://arxiv.org/abs/1909.12947v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.12947v1)
- **Published**: 2019-09-30 04:50:33+00:00
- **Updated**: 2019-09-30 04:50:33+00:00
- **Authors**: Zhenzhen Xiang, Jingrui Yu, Jie Li, Jianbo Su
- **Comment**: IROS 2019
- **Journal**: None
- **Summary**: In this paper, we present a multi-camera visual odometry (VO) system for an autonomous vehicle. Our system mainly consists of a virtual LiDAR and a pose tracker. We use a perspective transformation method to synthesize a surround-view image from undistorted fisheye camera images. With a semantic segmentation model, the free space can be extracted. The scans of the virtual LiDAR are generated by discretizing the contours of the free space. As for the pose tracker, we propose a visual odometry system fusing both the feature matching and the virtual LiDAR scan matching results. Only those feature points located in the free space area are utilized to ensure the 2D-2D matching for pose estimation. Furthermore, bundle adjustment (BA) is performed to minimize the feature points reprojection error and scan matching error. We apply our system to an autonomous vehicle equipped with four fisheye cameras. The testing scenarios include an outdoor parking lot as well as an indoor garage. Experimental results demonstrate that our system achieves a more robust and accurate performance comparing with a fisheye camera based monocular visual odometry system.



### An Object Detection by using Adaptive Structural Learning of Deep Belief Network
- **Arxiv ID**: http://arxiv.org/abs/1909.13465v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.13465v1)
- **Published**: 2019-09-30 05:49:02+00:00
- **Updated**: 2019-09-30 05:49:02+00:00
- **Authors**: Shin Kamada, Takumi Ichimura
- **Comment**: 8 pages, 14 figures, The International Joint Conference on Neural
  Networks (IJCNN 2019)
- **Journal**: None
- **Summary**: Deep learning forms a hierarchical network structure for representation of multiple input features. The adaptive structural learning method of Deep Belief Network (DBN) can realize a high classification capability while searching the optimal network structure during the training. The method can find the optimal number of hidden neurons for given input data in a Restricted Boltzmann Machine (RBM) by neuron generation-annihilation algorithm. Moreover, it can generate a new hidden layer in DBN by the layer generation algorithm to actualize a deep data representation. The proposed method showed higher classification accuracy for image benchmark data sets than several deep learning methods including well-known CNN methods. In this paper, a new object detection method for the DBN architecture is proposed for localization and category of objects. The method is a task for finding semantic objects in images as Bounding Box (B-Box). To investigate the effectiveness of the proposed method, the adaptive structural learning of DBN and the object detection were evaluated on the Chest X-ray image benchmark data set (CXR8), which is one of the most commonly accessible radio-logical examination for many lung diseases. The proposed method showed higher performance for both classification (more than 94.5% classification for test data) and localization (more than 90.4% detection for test data) than the other CNN methods.



### Residual Attention Graph Convolutional Network for Geometric 3D Scene Classification
- **Arxiv ID**: http://arxiv.org/abs/1909.13470v1
- **DOI**: 10.1109/ICCVW.2019.00507
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.13470v1)
- **Published**: 2019-09-30 06:22:47+00:00
- **Updated**: 2019-09-30 06:22:47+00:00
- **Authors**: Albert Mosella-Montoro, Javier Ruiz-Hidalgo
- **Comment**: None
- **Journal**: None
- **Summary**: Geometric 3D scene classification is a very challenging task. Current methodologies extract the geometric information using only a depth channel provided by an RGB-D sensor. These kinds of methodologies introduce possible errors due to missing local geometric context in the depth channel. This work proposes a novel Residual Attention Graph Convolutional Network that exploits the intrinsic geometric context inside a 3D space without using any kind of point features, allowing the use of organized or unorganized 3D data. Experiments are done in NYU Depth v1 and SUN-RGBD datasets to study the different configurations and to demonstrate the effectiveness of the proposed method. Experimental results show that the proposed method outperforms current state-of-the-art in geometric 3D scene classification tasks.



### On Incorporating Semantic Prior Knowledge in Deep Learning Through Embedding-Space Constraints
- **Arxiv ID**: http://arxiv.org/abs/1909.13471v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.13471v2)
- **Published**: 2019-09-30 06:26:09+00:00
- **Updated**: 2019-11-17 04:07:47+00:00
- **Authors**: Damien Teney, Ehsan Abbasnejad, Anton van den Hengel
- **Comment**: None
- **Journal**: None
- **Summary**: The knowledge that humans hold about a problem often extends far beyond a set of training data and output labels. While the success of deep learning mostly relies on supervised training, important properties cannot be inferred efficiently from end-to-end annotations alone, for example causal relations or domain-specific invariances. We present a general technique to supplement supervised training with prior knowledge expressed as relations between training instances. We illustrate the method on the task of visual question answering to exploit various auxiliary annotations, including relations of equivalence and of logical entailment between questions. Existing methods to use these annotations, including auxiliary losses and data augmentation, cannot guarantee the strict inclusion of these relations into the model since they require a careful balancing against the end-to-end objective. Our method uses these relations to shape the embedding space of the model, and treats them as strict constraints on its learned representations. In the context of VQA, this approach brings significant improvements in accuracy and robustness, in particular over the common practice of incorporating the constraints as a soft regularizer. We also show that incorporating this type of prior knowledge with our method brings consistent improvements, independently from the amount of supervised data used. It demonstrates the value of an additional training signal that is otherwise difficult to extract from end-to-end annotations alone.



### Spatio-Temporal FAST 3D Convolutions for Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1909.13474v2
- **DOI**: 10.1109/ICMLA.2019.00036
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.13474v2)
- **Published**: 2019-09-30 06:34:59+00:00
- **Updated**: 2019-10-22 13:30:21+00:00
- **Authors**: Alexandros Stergiou, Ronald Poppe
- **Comment**: None
- **Journal**: None
- **Summary**: Effective processing of video input is essential for the recognition of temporally varying events such as human actions. Motivated by the often distinctive temporal characteristics of actions in either horizontal or vertical direction, we introduce a novel convolution block for CNN architectures with video input. Our proposed Fractioned Adjacent Spatial and Temporal (FAST) 3D convolutions are a natural decomposition of a regular 3D convolution. Each convolution block consist of three sequential convolution operations: a 2D spatial convolution followed by spatio-temporal convolutions in the horizontal and vertical direction, respectively. Additionally, we introduce a FAST variant that treats horizontal and vertical motion in parallel. Experiments on benchmark action recognition datasets UCF-101 and HMDB-51 with ResNet architectures demonstrate consistent increased performance of FAST 3D convolution blocks over traditional 3D convolutions. The lower validation loss indicates better generalization, especially for deeper networks. We also evaluate the performance of CNN architectures with similar memory requirements, based either on Two-stream networks or with 3D convolution blocks. DenseNet-121 with FAST 3D convolutions was shown to perform best, giving further evidence of the merits of the decoupled spatio-temporal convolutions.



### CullNet: Calibrated and Pose Aware Confidence Scores for Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1909.13476v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.13476v1)
- **Published**: 2019-09-30 06:39:05+00:00
- **Updated**: 2019-09-30 06:39:05+00:00
- **Authors**: Kartik Gupta, Lars Petersson, Richard Hartley
- **Comment**: ICCV Workshop on Recovering 6D Object Pose, 2019
- **Journal**: None
- **Summary**: We present a new approach for a single view, image-based object pose estimation. Specifically, the problem of culling false positives among several pose proposal estimates is addressed in this paper. Our proposed approach targets the problem of inaccurate confidence values predicted by CNNs which is used by many current methods to choose a final object pose prediction. We present a network called CullNet, solving this task. CullNet takes pairs of pose masks rendered from a 3D model and cropped regions in the original image as input. This is then used to calibrate the confidence scores of the pose proposals. This new set of confidence scores is found to be significantly more reliable for accurate object pose estimation as shown by our results. Our experimental results on multiple challenging datasets (LINEMOD and Occlusion LINEMOD) reflects the utility of our proposed method. Our overall pose estimation pipeline outperforms state-of-the-art object pose estimation methods on these standard object pose estimation datasets. Our code is publicly available on https://github.com/kartikgupta-at-anu/CullNet.



### A Video Recognition Method by using Adaptive Structural Learning of Long Short Term Memory based Deep Belief Network
- **Arxiv ID**: http://arxiv.org/abs/1909.13480v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.13480v1)
- **Published**: 2019-09-30 06:57:55+00:00
- **Updated**: 2019-09-30 06:57:55+00:00
- **Authors**: Shin Kamada, Takumi Ichimura
- **Comment**: 6 pages, 7 figures, IEEE 11th International Workshop on Computational
  Intelligence and Applications (IWCIA2019)
- **Journal**: None
- **Summary**: Deep learning builds deep architectures such as multi-layered artificial neural networks to effectively represent multiple features of input patterns. The adaptive structural learning method of Deep Belief Network (DBN) can realize a high classification capability while searching the optimal network structure during the training. The method can find the optimal number of hidden neurons of a Restricted Boltzmann Machine (RBM) by neuron generation-annihilation algorithm to train the given input data, and then it can make a new layer in DBN by the layer generation algorithm to actualize a deep data representation. Moreover, the learning algorithm of Adaptive RBM and Adaptive DBN was extended to the time-series analysis by using the idea of LSTM (Long Short Term Memory). In this paper, our proposed prediction method was applied to Moving MNIST, which is a benchmark data set for video recognition. We challenge to reveal the power of our proposed method in the video recognition research field, since video includes rich source of visual information. Compared with the LSTM model, our method showed higher prediction performance (more than 90% predication accuracy for test data).



### Re-learning of Child Model for Misclassified data by using KL Divergence in AffectNet: A Database for Facial Expression
- **Arxiv ID**: http://arxiv.org/abs/1909.13481v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.13481v1)
- **Published**: 2019-09-30 06:58:27+00:00
- **Updated**: 2019-09-30 06:58:27+00:00
- **Authors**: Takumi Ichimura, Shin Kamada
- **Comment**: 6 pages, 7 figures, IEEE 11th International Workshop on Computational
  Intelligence and Applications (IWCIA2019)
- **Journal**: None
- **Summary**: AffectNet contains more than 1,000,000 facial images which manually annotated for the presence of eight discrete facial expressions and the intensity of valence and arousal. Adaptive structural learning method of DBN (Adaptive DBN) is positioned as a top Deep learning model of classification capability for some large image benchmark databases. The Convolutional Neural Network and Adaptive DBN were trained for AffectNet and classification capability was compared. Adaptive DBN showed higher classification ratio. However, the model was not able to classify some test cases correctly because human emotions contain many ambiguous features or patterns leading wrong answer which includes the possibility of being a factor of adversarial examples, due to two or more annotators answer different subjective judgment for an image. In order to distinguish such cases, this paper investigated a re-learning model of Adaptive DBN with two or more child models, where the original trained model can be seen as a parent model and then new child models are generated for some misclassified cases. In addition, an appropriate child model was generated according to difference between two models by using KL divergence. The generated child models showed better performance to classify two emotion categories: `Disgust' and `Anger'.



### Predicting Responses to a Robot's Future Motion using Generative Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.13486v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.13486v2)
- **Published**: 2019-09-30 07:15:29+00:00
- **Updated**: 2020-01-28 00:36:01+00:00
- **Authors**: Stuart Eiffert, Salah Sukkarieh
- **Comment**: Accepted at Australasian Conference on Robotics and Automation (ACRA)
  2019
- **Journal**: Proceedings of the Australasian Conference on Robotics and
  Automation (ACRA) 2019
- **Summary**: Robotic navigation through crowds or herds requires the ability to both predict the future motion of nearby individuals and understand how these predictions might change in response to a robot's future action. State of the art trajectory prediction models using Recurrent Neural Networks (RNNs) do not currently account for a planned future action of a robot, and so cannot predict how an individual will move in response to a robot's planned path. We propose an approach that adapts RNNs to use a robot's next planned action as an input alongside the current position of nearby individuals. This allows the model to learn the response of individuals with regards to a robot's motion from real world observations. By linking a robot's actions to the response of those around it in training, we show that we are able to not only improve prediction accuracy in close range interactions, but also to predict the likely response of surrounding individuals to simulated actions. This allows the use of the model to simulate state transitions, without requiring any assumptions on agent interaction. We apply this model to varied datasets, including crowds of pedestrians interacting with vehicles and bicycles, and livestock interacting with a robotic vehicle.



### Robust Data Association for Object-level Semantic SLAM
- **Arxiv ID**: http://arxiv.org/abs/1909.13493v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, 68U05, 68T40, 68W32
- **Links**: [PDF](http://arxiv.org/pdf/1909.13493v1)
- **Published**: 2019-09-30 07:37:54+00:00
- **Updated**: 2019-09-30 07:37:54+00:00
- **Authors**: Xueyang Kang, Shunying Yuan
- **Comment**: 8 pages, 11 figures
- **Journal**: None
- **Summary**: Simultaneous mapping and localization (SLAM) in an real indoor environment is still a challenging task. Traditional SLAM approaches rely heavily on low-level geometric constraints like corners or lines, which may lead to tracking failure in textureless surroundings or cluttered world with dynamic objects. In this paper, a compact semantic SLAM framework is proposed, with utilization of both geometric and object-level semantic constraints jointly, a more consistent mapping result, and more accurate pose estimation can be obtained. Two main contributions are presented int the paper, a) a robust and efficient SLAM data association and optimization framework is proposed, it models both discrete semantic labeling and continuous pose. b) a compact map representation, combining 2D Lidar map with object detection is presented. Experiments on public indoor datasets, TUM-RGBD, ICL-NUIM, and our own collected datasets prove the improving of SLAM robustness and accuracy compared to other popular SLAM systems, meanwhile a map maintenance efficiency can be achieved.



### DSRGAN: Explicitly Learning Disentangled Representation of Underlying Structure and Rendering for Image Generation without Tuple Supervision
- **Arxiv ID**: http://arxiv.org/abs/1909.13501v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.GR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.13501v1)
- **Published**: 2019-09-30 08:07:11+00:00
- **Updated**: 2019-09-30 08:07:11+00:00
- **Authors**: Guang-Yuan Hao, Hong-Xing Yu, Wei-Shi Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: We focus on explicitly learning disentangled representation for natural image generation, where the underlying spatial structure and the rendering on the structure can be independently controlled respectively, yet using no tuple supervision. The setting is significant since tuple supervision is costly and sometimes even unavailable. However, the task is highly unconstrained and thus ill-posed. To address this problem, we propose to introduce an auxiliary domain which shares a common underlying-structure space with the target domain, and we make a partially shared latent space assumption. The key idea is to encourage the partially shared latent variable to represent the similar underlying spatial structures in both domains, while the two domain-specific latent variables will be unavoidably arranged to present renderings of two domains respectively. This is achieved by designing two parallel generative networks with a common Progressive Rendering Architecture (PRA), which constrains both generative networks' behaviors to model shared underlying structure and to model spatially dependent relation between rendering and underlying structure. Thus, we propose DSRGAN (GANs for Disentangling Underlying Structure and Rendering) to instantiate our method. We also propose a quantitative criterion (the Normalized Disentanglability) to quantify disentanglability. Comparison to the state-of-the-art methods shows that DSRGAN can significantly outperform them in disentanglability.



### EdgeCNN: Convolutional Neural Network Classification Model with small inputs for Edge Computing
- **Arxiv ID**: http://arxiv.org/abs/1909.13522v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1909.13522v1)
- **Published**: 2019-09-30 08:45:03+00:00
- **Updated**: 2019-09-30 08:45:03+00:00
- **Authors**: Shunzhi Yang, Zheng Gong, Kai Ye, Yungen Wei, Zheng Huang, Zhenhua Huang
- **Comment**: None
- **Journal**: None
- **Summary**: With the development of Internet of Things (IoT), data is increasingly appearing on the edge of the network. Processing tasks on the edge of the network can effectively solve the problems of personal privacy leaks and server overload. As a result, it has attracted a great deal of attention and made substantial progress. This progress includes efficient convolutional neural network (CNN) models such as MobileNet and ShuffleNet. However, all of these networks appear as a common network model and they usually need to identify multiple targets when applied. So the size of the input is very large. In some specific cases, only the target needs to be classified. Therefore, a small input network can be designed to reduce computation. In addition, other efficient neural network models are primarily designed for mobile phones. Mobile phones have faster memory access, which allows them to use group convolution. In particular, this paper finds that the recently widely used group convolution is not suitable for devices with very slow memory access. Therefore, the EdgeCNN of this paper is designed for edge computing devices with low memory access speed and low computing resources. EdgeCNN has been run successfully on the Raspberry Pi 3B+ at a speed of 1.37 frames per second. The accuracy of facial expression classification for the FER-2013 and RAF-DB datasets outperforms other proposed networks that are compatible with the Raspberry Pi 3B+. The implementation of EdgeCNN is available at https://github.com/yangshunzhi1994/EdgeCNN



### Enhancing Object Detection in Adverse Conditions using Thermal Imaging
- **Arxiv ID**: http://arxiv.org/abs/1909.13551v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.13551v1)
- **Published**: 2019-09-30 09:29:48+00:00
- **Updated**: 2019-09-30 09:29:48+00:00
- **Authors**: Kshitij Agrawal, Anbumani Subramanian
- **Comment**: IROS 2019 Workshop on Towards Cognitive Vehicles
- **Journal**: None
- **Summary**: Autonomous driving relies on deriving understanding of objects and scenes through images. These images are often captured by sensors in the visible spectrum. For improved detection capabilities we propose the use of thermal sensors to augment the vision capabilities of an autonomous vehicle. In this paper, we present our investigations on the fusion of visible and thermal spectrum images using a publicly available dataset, and use it to analyze the performance of object recognition on other known driving datasets. We present an comparison of object detection in night time imagery and qualitatively demonstrate that thermal images significantly improve detection accuracy.



### Imagine That! Leveraging Emergent Affordances for 3D Tool Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1909.13561v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML, I.2.10; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1909.13561v4)
- **Published**: 2019-09-30 09:55:33+00:00
- **Updated**: 2020-10-07 04:05:19+00:00
- **Authors**: Yizhe Wu, Sudhanshu Kasewa, Oliver Groth, Sasha Salter, Li Sun, Oiwi Parker Jones, Ingmar Posner
- **Comment**: 12 pages, 6 figures
- **Journal**: None
- **Summary**: In this paper we explore the richness of information captured by the latent space of a vision-based generative model. The model combines unsupervised generative learning with a task-based performance predictor to learn and to exploit task-relevant object affordances given visual observations from a reaching task, involving a scenario and a stick-like tool. While the learned embedding of the generative model captures factors of variation in 3D tool geometry (e.g. length, width, and shape), the performance predictor identifies sub-manifolds of the embedding that correlate with task success. Within a variety of scenarios, we demonstrate that traversing the latent space via backpropagation from the performance predictor allows us to imagine tools appropriate for the task at hand. Our results indicate that affordances-like the utility for reaching-are encoded along smooth trajectories in latent space. Accessing these emergent affordances by considering only high-level performance criteria (such as task success) enables an agent to manipulate tool geometries in a targeted and deliberate way.



### Meta-learning algorithms for Few-Shot Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1909.13579v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.13579v1)
- **Published**: 2019-09-30 10:51:16+00:00
- **Updated**: 2019-09-30 10:51:16+00:00
- **Authors**: Etienne Bennequin
- **Comment**: None
- **Journal**: None
- **Summary**: Few-Shot Learning is the challenge of training a model with only a small amount of data. Many solutions to this problem use meta-learning algorithms, i.e. algorithms that learn to learn. By sampling few-shot tasks from a larger dataset, we can teach these algorithms to solve new, unseen tasks. This document reports my work on meta-learning algorithms for Few-Shot Computer Vision. This work was done during my internship at Sicara, a French company building image recognition solutions for businesses. It contains: 1. an extensive review of the state-of-the-art in few-shot computer vision; 2. a benchmark of meta-learning algorithms for few-shot image classification; 3. the introduction to a novel meta-learning algorithm for few-shot object detection, which is still in development.



### Towards Good Practices for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.13583v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.13583v1)
- **Published**: 2019-09-30 11:00:09+00:00
- **Updated**: 2019-09-30 11:00:09+00:00
- **Authors**: Dongdong Yu, Kai Su, Hengkai Guo, Jian Wang, Kaihui Zhou, Yuanyuan Huang, Minghui Dong, Jie Shao, Changhu Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-supervised video object segmentation is an interesting yet challenging task in machine learning. In this work, we conduct a series of refinements with the propagation-based video object segmentation method and empirically evaluate their impact on the final model performance through ablation study. By taking all the refinements, we improve the space-time memory networks to achieve a Overall of 79.1 on the Youtube-VOS Challenge 2019.



### Interpretations are useful: penalizing explanations to align neural networks with prior knowledge
- **Arxiv ID**: http://arxiv.org/abs/1909.13584v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.13584v4)
- **Published**: 2019-09-30 11:02:01+00:00
- **Updated**: 2020-10-08 12:43:21+00:00
- **Authors**: Laura Rieger, Chandan Singh, W. James Murdoch, Bin Yu
- **Comment**: 18 pages; published in ICML2020; Erratum: numbers in table 1 were too
  high (now corrected) with the trend remaining the same
- **Journal**: None
- **Summary**: For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective. Too often, the litany of proposed explainable deep learning methods stop at the first step, providing practitioners with insight into a model, but no way to act on it. In this paper, we propose contextual decomposition explanation penalization (CDEP), a method which enables practitioners to leverage existing explanation methods in order to increase the predictive accuracy of deep learning models. In particular, when shown that a model has incorrectly assigned importance to some features, CDEP enables practitioners to correct these errors by directly regularizing the provided explanations. Using explanations provided by contextual decomposition (CD) (Murdoch et al., 2018), we demonstrate the ability of our method to increase performance on an array of toy and real datasets.



### Domain Adaptation for Semantic Segmentation with Maximum Squares Loss
- **Arxiv ID**: http://arxiv.org/abs/1909.13589v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.13589v1)
- **Published**: 2019-09-30 11:10:09+00:00
- **Updated**: 2019-09-30 11:10:09+00:00
- **Authors**: Minghao Chen, Hongyang Xue, Deng Cai
- **Comment**: Published in IEEE International Conference on Computer Vision (ICCV)
  2019
- **Journal**: None
- **Summary**: Deep neural networks for semantic segmentation always require a large number of samples with pixel-level labels, which becomes the major difficulty in their real-world applications. To reduce the labeling cost, unsupervised domain adaptation (UDA) approaches are proposed to transfer knowledge from labeled synthesized datasets to unlabeled real-world datasets. Recently, some semi-supervised learning methods have been applied to UDA and achieved state-of-the-art performance. One of the most popular approaches in semi-supervised learning is the entropy minimization method. However, when applying the entropy minimization to UDA for semantic segmentation, the gradient of the entropy is biased towards samples that are easy to transfer. To balance the gradient of well-classified target samples, we propose the maximum squares loss. Our maximum squares loss prevents the training process being dominated by easy-to-transfer samples in the target domain. Besides, we introduce the image-wise weighting ratio to alleviate the class imbalance in the unlabeled target domain. Both synthetic-to-real and cross-city adaptation experiments demonstrate the effectiveness of our proposed approach. The code is released at https://github. com/ZJULearning/MaxSquareLoss.



### Multi-view PointNet for 3D Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/1909.13603v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.13603v1)
- **Published**: 2019-09-30 11:45:37+00:00
- **Updated**: 2019-09-30 11:45:37+00:00
- **Authors**: Maximilian Jaritz, Jiayuan Gu, Hao Su
- **Comment**: Geometry Meets Deep Learning Workshop, ICCV 2019
- **Journal**: None
- **Summary**: Fusion of 2D images and 3D point clouds is important because information from dense images can enhance sparse point clouds. However, fusion is challenging because 2D and 3D data live in different spaces. In this work, we propose MVPNet (Multi-View PointNet), where we aggregate 2D multi-view image features into 3D point clouds, and then use a point based network to fuse the features in 3D canonical space to predict 3D semantic labels. To this end, we introduce view selection along with a 2D-3D feature aggregation module. Extensive experiments show the benefit of leveraging features from dense images and reveal superior robustness to varying point cloud density compared to 3D-only methods. On the ScanNetV2 benchmark, our MVPNet significantly outperforms prior point cloud based approaches on the task of 3D Semantic Segmentation. It is much faster to train than the large networks of the sparse voxel approach. We provide solid ablation studies to ease the future design of 2D-3D fusion methods and their extension to other tasks, as we showcase for 3D instance segmentation.



### A Quotient Space Formulation for Generative Statistical Analysis of Graphical Data
- **Arxiv ID**: http://arxiv.org/abs/1909.12907v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/1909.12907v2)
- **Published**: 2019-09-30 13:29:04+00:00
- **Updated**: 2021-04-02 01:33:17+00:00
- **Authors**: Xiaoyang Guo, Anuj Srivastava, Sudeep Sarkar
- **Comment**: None
- **Journal**: None
- **Summary**: Complex analyses involving multiple, dependent random quantities often lead to graphical models - a set of nodes denoting variables of interest, and corresponding edges denoting statistical interactions between nodes. To develop statistical analyses for graphical data, especially towards generative modeling, one needs mathematical representations and metrics for matching and comparing graphs, and subsequent tools, such as geodesics, means, and covariances. This paper utilizes a quotient structure to develop efficient algorithms for computing these quantities, leading to useful statistical tools, including principal component analysis, statistical testing, and modeling. We demonstrate the efficacy of this framework using datasets taken from several problem areas, including letters, biochemical structures, and social networks.



### Nighttime Stereo Depth Estimation using Joint Translation-Stereo Learning: Light Effects and Uninformative Regions
- **Arxiv ID**: http://arxiv.org/abs/1909.13701v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.13701v2)
- **Published**: 2019-09-30 13:52:36+00:00
- **Updated**: 2020-10-09 01:26:10+00:00
- **Authors**: Aashish Sharma, Lionel Heng, Loong-Fah Cheong, Robby T. Tan
- **Comment**: Accepted to 3DV 2020 (Oral)
- **Journal**: None
- **Summary**: Nighttime stereo depth estimation is still challenging, as assumptions associated with daytime lighting conditions do not hold any longer. Nighttime is not only about low-light and dense noise, but also about glow/glare, flares, non-uniform distribution of light, etc. One of the possible solutions is to train a network on night stereo images in a fully supervised manner. However, to obtain proper disparity ground-truths that are dense, independent from glare/glow, and have sufficiently far depth ranges is extremely intractable. To address the problem, we introduce a network joining day/night translation and stereo. In training the network, our method does not require ground-truth disparities of the night images, or paired day/night images. We utilize a translation network that can render realistic night stereo images from day stereo images. We then train a stereo network on the rendered night stereo images using the available disparity supervision from the corresponding day stereo images, and simultaneously also train the day/night translation network. We handle the fake depth problem, which occurs due to the unsupervised/unpaired translation, for light effects (e.g., glow/glare) and uninformative regions (e.g., low-light and saturated regions), by adding structure-preservation and weighted-smoothness constraints. Our experiments show that our method outperforms the baseline methods on night images.



### RandAugment: Practical automated data augmentation with a reduced search space
- **Arxiv ID**: http://arxiv.org/abs/1909.13719v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.13719v2)
- **Published**: 2019-09-30 14:05:14+00:00
- **Updated**: 2019-11-14 04:51:03+00:00
- **Authors**: Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, Quoc V. Le
- **Comment**: Added ablation experiments
- **Journal**: None
- **Summary**: Recent work has shown that data augmentation has the potential to significantly improve the generalization of deep learning models. Recently, automated augmentation strategies have led to state-of-the-art results in image classification and object detection. While these strategies were optimized for improving validation accuracy, they also led to state-of-the-art results in semi-supervised learning and improved robustness to common corruptions of images. An obstacle to a large-scale adoption of these methods is a separate search phase which increases the training complexity and may substantially increase the computational cost. Additionally, due to the separate search phase, these approaches are unable to adjust the regularization strength based on model or dataset size. Automated augmentation policies are often found by training small models on small datasets and subsequently applied to train larger models. In this work, we remove both of these obstacles. RandAugment has a significantly reduced search space which allows it to be trained on the target task with no need for a separate proxy task. Furthermore, due to the parameterization, the regularization strength may be tailored to different model and dataset sizes. RandAugment can be used uniformly across different tasks and datasets and works out of the box, matching or surpassing all previous automated augmentation approaches on CIFAR-10/100, SVHN, and ImageNet. On the ImageNet dataset we achieve 85.0% accuracy, a 0.6% increase over the previous state-of-the-art and 1.0% increase over baseline augmentation. On object detection, RandAugment leads to 1.0-1.3% improvement over baseline augmentation, and is within 0.3% mAP of AutoAugment on COCO. Finally, due to its interpretable hyperparameter, RandAugment may be used to investigate the role of data augmentation with varying model and dataset size. Code is available online.



### IPC-Net: 3D point-cloud segmentation using deep inter-point convolutional layers
- **Arxiv ID**: http://arxiv.org/abs/1909.13726v1
- **DOI**: 10.1109/ICTAI.2018.00054
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.13726v1)
- **Published**: 2019-09-30 14:14:30+00:00
- **Updated**: 2019-09-30 14:14:30+00:00
- **Authors**: Felipe Gomez Marulanda, Pieter Libin, Timothy Verstraeten, Ann Nowé
- **Comment**: None
- **Journal**: 2018 IEEE 30th International Conference on Tools with Artificial
  Intelligence (ICTAI),
- **Summary**: Over the last decade, the demand for better segmentation and classification algorithms in 3D spaces has significantly grown due to the popularity of new 3D sensor technologies and advancements in the field of robotics. Point-clouds are one of the most popular representations to store a digital description of 3D shapes. However, point-clouds are stored in irregular and unordered structures, which limits the direct use of segmentation algorithms such as Convolutional Neural Networks. The objective of our work is twofold: First, we aim to provide a full analysis of the PointNet architecture to illustrate which features are being extracted from the point-clouds. Second, to propose a new network architecture called IPC-Net to improve the state-of-the-art point cloud architectures. We show that IPC-Net extracts a larger set of unique features allowing the model to produce more accurate segmentations compared to the PointNet architecture. In general, our approach outperforms PointNet on every family of 3D geometries on which the models were tested. A high generalisation improvement was observed on every 3D shape, especially on the rockets dataset. Our experiments demonstrate that our main contribution, inter-point activation on the network's layers, is essential to accurately segment 3D point-clouds.



### X-ray and Visible Spectra Circular Motion Images Dataset
- **Arxiv ID**: http://arxiv.org/abs/1909.13730v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.13730v2)
- **Published**: 2019-09-30 14:17:12+00:00
- **Updated**: 2019-10-01 12:31:29+00:00
- **Authors**: Mikhail Chekanov, Oleg Shipitko
- **Comment**: None
- **Journal**: None
- **Summary**: We present the collections of images of the same rotating plastic object made in X-ray and visible spectra. Both parts of the dataset contain 400 images. The images are maid every 0.5 degrees of the object axial rotation. The collection of images is designed for evaluation of the performance of circular motion estimation algorithms as well as for the study of X-ray nature influence on the image analysis algorithms such as keypoints detection and description. The dataset is available at https://github.com/Visillect/xvcm-dataset.



### MLSL: Multi-Level Self-Supervised Learning for Domain Adaptation with Spatially Independent and Semantically Consistent Labeling
- **Arxiv ID**: http://arxiv.org/abs/1909.13776v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.13776v1)
- **Published**: 2019-09-30 15:16:03+00:00
- **Updated**: 2019-09-30 15:16:03+00:00
- **Authors**: Javed Iqbal, Mohsen Ali
- **Comment**: Accepted by WACV 2020
- **Journal**: None
- **Summary**: Most of the recent Deep Semantic Segmentation algorithms suffer from large generalization errors, even when powerful hierarchical representation models based on convolutional neural networks have been employed. This could be attributed to limited training data and large distribution gap in train and test domain datasets. In this paper, we propose a multi-level self-supervised learning model for domain adaptation of semantic segmentation. Exploiting the idea that an object (and most of the stuff given context) should be labeled consistently regardless of its location, we generate spatially independent and semantically consistent (SISC) pseudo-labels by segmenting multiple sub-images using base model and designing an aggregation strategy. Image level pseudo weak-labels, PWL, are computed to guide domain adaptation by capturing global context similarity in source and domain at latent space level. Thus helping latent space learn the representation even when there are very few pixels belonging to the domain category (small object for example) compared to rest of the image. Our multi-level Self-supervised learning (MLSL) outperforms existing state-of art (self or adversarial learning) algorithms. Specifically, keeping all setting similar and employing MLSL we obtain an mIoU gain of 5:1% on GTA-V to Cityscapes adaptation and 4:3% on SYNTHIA to Cityscapes adaptation compared to existing state-of-art method.



### Coarse-to-Fine Registration of Airborne LiDAR Data and Optical Imagery on Urban Scenes
- **Arxiv ID**: http://arxiv.org/abs/1909.13817v2
- **DOI**: 10.1109/JSTARS.2020.2987305
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.13817v2)
- **Published**: 2019-09-30 16:10:12+00:00
- **Updated**: 2020-04-15 02:57:11+00:00
- **Authors**: Thanh Huy Nguyen, Sylvie Daniel, Didier Gueriot, Christophe Sintes, Jean-Marc Le Caillec
- **Comment**: 20 pages, 15 figures. Accepted to be published in IEEE JSTARS
- **Journal**: IEEE Journal of Selected Topics in Applied Earth Observations and
  Remote Sensing 2020 (Early Access)
- **Summary**: Applications based on synergistic integration of optical imagery and LiDAR data are receiving a growing interest from the remote sensing community. However, a misaligned integration between these datasets may fail to fully profit the potential of both sensors. In this regard, an optimum fusion of optical imagery and LiDAR data requires an accurate registration. This is a complex problem since a versatile solution is still missing, especially when considering the context where data are collected at different times, from different platforms, under different acquisition configurations. This paper presents a coarse-to-fine registration method of aerial/satellite optical imagery with airborne LiDAR data acquired in such context. Firstly, a coarse registration involves extracting and matching of buildings from LiDAR data and optical imagery. Then, a Mutual Information-based fine registration is carried out. It involves a super-resolution approach applied to LiDAR data, and a local approach of transformation model estimation. The proposed method succeeds at overcoming the challenges associated with the aforementioned difficult context. Considering the experimented airborne LiDAR (2011) and orthorectified aerial imagery (2016) datasets, their spatial shift is reduced by 48.15% after the proposed coarse registration. Moreover, the incompatibility of size and spatial resolution is addressed by the mentioned super-resolution. Finally, a high accuracy of dataset alignment is also achieved, highlighted by a 40-cm error based on a check-point assessment and a 64-cm error based on a check-pair-line assessment. These promising results enable further research for a complete versatile fusion methodology between airborne LiDAR and optical imagery data in this challenging context.



### Unsupervised Pose Flow Learning for Pose Guided Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1909.13819v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.13819v1)
- **Published**: 2019-09-30 16:17:25+00:00
- **Updated**: 2019-09-30 16:17:25+00:00
- **Authors**: Haitian Zheng, Lele Chen, Chenliang Xu, Jiebo Luo
- **Comment**: 12 pages, 13 figures
- **Journal**: None
- **Summary**: Pose guided synthesis aims to generate a new image in an arbitrary target pose while preserving the appearance details from the source image. Existing approaches rely on either hard-coded spatial transformations or 3D body modeling. They often overlook complex non-rigid pose deformation or unmatched occluded regions, thus fail to effectively preserve appearance information. In this paper, we propose an unsupervised pose flow learning scheme that learns to transfer the appearance details from the source image. Based on such learned pose flow, we proposed GarmentNet and SynthesisNet, both of which use multi-scale feature-domain alignment for coarse-to-fine synthesis. Experiments on the DeepFashion, MVC dataset and additional real-world datasets demonstrate that our approach compares favorably with the state-of-the-art methods and generalizes to unseen poses and clothing styles.



### INTERACTION Dataset: An INTERnational, Adversarial and Cooperative moTION Dataset in Interactive Driving Scenarios with Semantic Maps
- **Arxiv ID**: http://arxiv.org/abs/1910.03088v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/1910.03088v1)
- **Published**: 2019-09-30 17:26:51+00:00
- **Updated**: 2019-09-30 17:26:51+00:00
- **Authors**: Wei Zhan, Liting Sun, Di Wang, Haojie Shi, Aubrey Clausse, Maximilian Naumann, Julius Kummerle, Hendrik Konigshof, Christoph Stiller, Arnaud de La Fortelle, Masayoshi Tomizuka
- **Comment**: None
- **Journal**: None
- **Summary**: Behavior-related research areas such as motion prediction/planning, representation/imitation learning, behavior modeling/generation, and algorithm testing, require support from high-quality motion datasets containing interactive driving scenarios with different driving cultures. In this paper, we present an INTERnational, Adversarial and Cooperative moTION dataset (INTERACTION dataset) in interactive driving scenarios with semantic maps. Five features of the dataset are highlighted. 1) The interactive driving scenarios are diverse, including urban/highway/ramp merging and lane changes, roundabouts with yield/stop signs, signalized intersections, intersections with one/two/all-way stops, etc. 2) Motion data from different countries and different continents are collected so that driving preferences and styles in different cultures are naturally included. 3) The driving behavior is highly interactive and complex with adversarial and cooperative motions of various traffic participants. Highly complex behavior such as negotiations, aggressive/irrational decisions and traffic rule violations are densely contained in the dataset, while regular behavior can also be found from cautious car-following, stop, left/right/U-turn to rational lane-change and cycling and pedestrian crossing, etc. 4) The levels of criticality span wide, from regular safe operations to dangerous, near-collision maneuvers. Real collision, although relatively slight, is also included. 5) Maps with complete semantic information are provided with physical layers, reference lines, lanelet connections and traffic rules. The data is recorded from drones and traffic cameras. Statistics of the dataset in terms of number of entities and interaction density are also provided, along with some utilization examples in a variety of behavior-related research areas. The dataset can be downloaded via https://interaction-dataset.com.



### XNOR-Net++: Improved Binary Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.13863v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.13863v1)
- **Published**: 2019-09-30 17:42:09+00:00
- **Updated**: 2019-09-30 17:42:09+00:00
- **Authors**: Adrian Bulat, Georgios Tzimiropoulos
- **Comment**: Accepted to BMVC 2019
- **Journal**: None
- **Summary**: This paper proposes an improved training algorithm for binary neural networks in which both weights and activations are binary numbers. A key but fairly overlooked feature of the current state-of-the-art method of XNOR-Net is the use of analytically calculated real-valued scaling factors for re-weighting the output of binary convolutions. We argue that analytic calculation of these factors is sub-optimal. Instead, in this work, we make the following contributions: (a) we propose to fuse the activation and weight scaling factors into a single one that is learned discriminatively via backpropagation. (b) More importantly, we explore several ways of constructing the shape of the scale factors while keeping the computational budget fixed. (c) We empirically measure the accuracy of our approximations and show that they are significantly more accurate than the analytically calculated one. (d) We show that our approach significantly outperforms XNOR-Net within the same computational budget when tested on the challenging task of ImageNet classification, offering up to 6\% accuracy gain.



### Deep learning tools for the measurement of animal behavior in neuroscience
- **Arxiv ID**: http://arxiv.org/abs/1909.13868v2
- **DOI**: 10.1016/j.conb.2019.10.008
- **Categories**: **cs.CV**, q-bio.NC, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1909.13868v2)
- **Published**: 2019-09-30 17:50:48+00:00
- **Updated**: 2019-10-18 17:40:30+00:00
- **Authors**: Mackenzie W. Mathis, Alexander Mathis
- **Comment**: 11 pages, 3 figures, review
- **Journal**: Current Opinion in Neurobiology Volume 60, February 2020, Pages
  1-11
- **Summary**: Recent advances in computer vision have made accurate, fast and robust measurement of animal behavior a reality. In the past years powerful tools specifically designed to aid the measurement of behavior have come to fruition. Here we discuss how capturing the postures of animals - pose estimation - has been rapidly advancing with new deep learning methods. While challenges still remain, we envision that the fast-paced development of new deep learning tools will rapidly change the landscape of realizable real-world neuroscience.



### Efficient Bimanual Manipulation Using Learned Task Schemas
- **Arxiv ID**: http://arxiv.org/abs/1909.13874v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.13874v2)
- **Published**: 2019-09-30 17:55:09+00:00
- **Updated**: 2020-02-27 16:58:56+00:00
- **Authors**: Rohan Chitnis, Shubham Tulsiani, Saurabh Gupta, Abhinav Gupta
- **Comment**: ICRA 2020 final version
- **Journal**: None
- **Summary**: We address the problem of effectively composing skills to solve sparse-reward tasks in the real world. Given a set of parameterized skills (such as exerting a force or doing a top grasp at a location), our goal is to learn policies that invoke these skills to efficiently solve such tasks. Our insight is that for many tasks, the learning process can be decomposed into learning a state-independent task schema (a sequence of skills to execute) and a policy to choose the parameterizations of the skills in a state-dependent manner. For such tasks, we show that explicitly modeling the schema's state-independence can yield significant improvements in sample efficiency for model-free reinforcement learning algorithms. Furthermore, these schemas can be transferred to solve related tasks, by simply re-learning the parameterizations with which the skills are invoked. We find that doing so enables learning to solve sparse-reward tasks on real-world robotic systems very efficiently. We validate our approach experimentally over a suite of robotic bimanual manipulation tasks, both in simulation and on real hardware. See videos at http://tinyurl.com/chitnis-schema.



### LIP: Learning Instance Propagation for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.00032v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.00032v1)
- **Published**: 2019-09-30 18:03:09+00:00
- **Updated**: 2019-09-30 18:03:09+00:00
- **Authors**: Ye Lyu, George Vosselman, Gui-Song Xia, Michael Ying Yang
- **Comment**: ICCVW19
- **Journal**: None
- **Summary**: In recent years, the task of segmenting foreground objects from background in a video, i.e. video object segmentation (VOS), has received considerable attention. In this paper, we propose a single end-to-end trainable deep neural network, convolutional gated recurrent Mask-RCNN, for tackling the semi-supervised VOS task. We take advantage of both the instance segmentation network (Mask-RCNN) and the visual memory module (Conv-GRU) to tackle the VOS task. The instance segmentation network predicts masks for instances, while the visual memory module learns to selectively propagate information for multiple instances simultaneously, which handles the appearance change, the variation of scale and pose and the occlusions between objects. After offline and online training under purely instance segmentation losses, our approach is able to achieve satisfactory results without any post-processing or synthetic video data augmentation. Experimental results on DAVIS 2016 dataset and DAVIS 2017 dataset have demonstrated the effectiveness of our method for video object segmentation task.



### Hidden Trigger Backdoor Attacks
- **Arxiv ID**: http://arxiv.org/abs/1910.00033v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.00033v2)
- **Published**: 2019-09-30 18:03:28+00:00
- **Updated**: 2019-12-21 02:13:34+00:00
- **Authors**: Aniruddha Saha, Akshayvarun Subramanya, Hamed Pirsiavash
- **Comment**: AAAI 2020 - Main Technical Track (Oral)
- **Journal**: None
- **Summary**: With the success of deep learning algorithms in various domains, studying adversarial attacks to secure deep models in real world applications has become an important research topic. Backdoor attacks are a form of adversarial attacks on deep networks where the attacker provides poisoned data to the victim to train the model with, and then activates the attack by showing a specific small trigger pattern at the test time. Most state-of-the-art backdoor attacks either provide mislabeled poisoning data that is possible to identify by visual inspection, reveal the trigger in the poisoned data, or use noise to hide the trigger. We propose a novel form of backdoor attack where poisoned data look natural with correct labels and also more importantly, the attacker hides the trigger in the poisoned data and keeps the trigger secret until the test time. We perform an extensive study on various image classification settings and show that our attack can fool the model by pasting the trigger at random locations on unseen images although the model performs well on clean data. We also show that our proposed attack cannot be easily defended using a state-of-the-art defense algorithm for backdoor attacks.



### Multi-Head Attention with Diversity for Learning Grounded Multilingual Multimodal Representations
- **Arxiv ID**: http://arxiv.org/abs/1910.00058v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.00058v1)
- **Published**: 2019-09-30 18:58:03+00:00
- **Updated**: 2019-09-30 18:58:03+00:00
- **Authors**: Po-Yao Huang, Xiaojun Chang, Alexander Hauptmann
- **Comment**: Accepted at EMNLP 2019
- **Journal**: None
- **Summary**: With the aim of promoting and understanding the multilingual version of image search, we leverage visual object detection and propose a model with diverse multi-head attention to learn grounded multilingual multimodal representations. Specifically, our model attends to different types of textual semantics in two languages and visual objects for fine-grained alignments between sentences and images. We introduce a new objective function which explicitly encourages attention diversity to learn an improved visual-semantic embedding space. We evaluate our model in the German-Image and English-Image matching tasks on the Multi30K dataset, and in the Semantic Textual Similarity task with the English descriptions of visual content. Results show that our model yields a significant performance gain over other methods in all of the three tasks.



### Role of Spatial Context in Adversarial Robustness for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1910.00068v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.00068v3)
- **Published**: 2019-09-30 19:41:05+00:00
- **Updated**: 2020-04-18 02:53:22+00:00
- **Authors**: Aniruddha Saha, Akshayvarun Subramanya, Koninika Patil, Hamed Pirsiavash
- **Comment**: CVPR 2020 Workshop on Adversarial Machine Learning in Computer Vision
- **Journal**: None
- **Summary**: The benefits of utilizing spatial context in fast object detection algorithms have been studied extensively. Detectors increase inference speed by doing a single forward pass per image which means they implicitly use contextual reasoning for their predictions. However, one can show that an adversary can design adversarial patches which do not overlap with any objects of interest in the scene and exploit contextual reasoning to fool standard detectors. In this paper, we examine this problem and design category specific adversarial patches which make a widely used object detector like YOLO blind to an attacker chosen object category. We also show that limiting the use of spatial context during object detector training improves robustness to such adversaries. We believe the existence of context based adversarial attacks is concerning since the adversarial patch can affect predictions without being in vicinity of any objects of interest. Hence, defending against such attacks becomes challenging and we urge the research community to give attention to this vulnerability.



### DenseRaC: Joint 3D Pose and Shape Estimation by Dense Render-and-Compare
- **Arxiv ID**: http://arxiv.org/abs/1910.00116v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.00116v2)
- **Published**: 2019-09-30 21:34:31+00:00
- **Updated**: 2019-10-09 17:52:02+00:00
- **Authors**: Yuanlu Xu, Song-Chun Zhu, Tony Tung
- **Comment**: 11 pages, 8 figures, International Conference on Computer Vision
  (ICCV) 2019, Oral Presentation
- **Journal**: None
- **Summary**: We present DenseRaC, a novel end-to-end framework for jointly estimating 3D human pose and body shape from a monocular RGB image. Our two-step framework takes the body pixel-to-surface correspondence map (i.e., IUV map) as proxy representation and then performs estimation of parameterized human pose and shape. Specifically, given an estimated IUV map, we develop a deep neural network optimizing 3D body reconstruction losses and further integrating a render-and-compare scheme to minimize differences between the input and the rendered output, i.e., dense body landmarks, body part masks, and adversarial priors. To boost learning, we further construct a large-scale synthetic dataset (MOCA) utilizing web-crawled Mocap sequences, 3D scans and animations. The generated data covers diversified camera views, human actions and body shapes, and is paired with full ground truth. Our model jointly learns to represent the 3D human body from hybrid datasets, mitigating the problem of unpaired training data. Our experiments show that DenseRaC obtains superior performance against state of the art on public benchmarks of various humanrelated tasks.



### A Mobile Manipulation System for One-Shot Teaching of Complex Tasks in Homes
- **Arxiv ID**: http://arxiv.org/abs/1910.00127v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.00127v3)
- **Published**: 2019-09-30 22:03:07+00:00
- **Updated**: 2020-03-03 21:24:02+00:00
- **Authors**: Max Bajracharya, James Borders, Dan Helmick, Thomas Kollar, Michael Laskey, John Leichty, Jeremy Ma, Umashankar Nagarajan, Akiyoshi Ochiai, Josh Petersen, Krishna Shankar, Kevin Stone, Yutaka Takaoka
- **Comment**: The video is available at: https://youtu.be/HSyAGMGikLk. 7 pages, 5
  figures, accepted by IEEE 2020 Robotics International Conference on Robotics
  and Automation (ICRA)
- **Journal**: None
- **Summary**: We describe a mobile manipulation hardware and software system capable of autonomously performing complex human-level tasks in real homes, after being taught the task with a single demonstration from a person in virtual reality. This is enabled by a highly capable mobile manipulation robot, whole-body task space hybrid position/force control, teaching of parameterized primitives linked to a robust learned dense visual embeddings representation of the scene, and a task graph of the taught behaviors. We demonstrate the robustness of the approach by presenting results for performing a variety of tasks, under different environmental conditions, in multiple real homes. Our approach achieves 85% overall success rate on three tasks that consist of an average of 45 behaviors each.



### Track to Reconstruct and Reconstruct to Track
- **Arxiv ID**: http://arxiv.org/abs/1910.00130v3
- **DOI**: 10.1109/LRA.2020.2969183
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.00130v3)
- **Published**: 2019-09-30 22:05:59+00:00
- **Updated**: 2020-04-19 04:28:24+00:00
- **Authors**: Jonathon Luiten, Tobias Fischer, Bastian Leibe
- **Comment**: RA-L 2020 and ICRA 2020
- **Journal**: IEEE Robotics and Automation Letters 5.2 (2020): 1803-1810
- **Summary**: Object tracking and 3D reconstruction are often performed together, with tracking used as input for reconstruction. However, the obtained reconstructions also provide useful information for improving tracking. We propose a novel method that closes this loop, first tracking to reconstruct, and then reconstructing to track. Our approach, MOTSFusion (Multi-Object Tracking, Segmentation and dynamic object Fusion), exploits the 3D motion extracted from dynamic object reconstructions to track objects through long periods of complete occlusion and to recover missing detections. Our approach first builds up short tracklets using 2D optical flow, and then fuses these into dynamic 3D object reconstructions. The precise 3D object motion of these reconstructions is used to merge tracklets through occlusion into long-term tracks, and to locate objects when detections are missing. On KITTI, our reconstruction-based tracking reduces the number of ID switches of the initial tracklets by more than 50%, and outperforms all previous approaches for both bounding box and segmentation tracking.



### CapsuleVOS: Semi-Supervised Video Object Segmentation Using Capsule Routing
- **Arxiv ID**: http://arxiv.org/abs/1910.00132v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.00132v1)
- **Published**: 2019-09-30 22:12:43+00:00
- **Updated**: 2019-09-30 22:12:43+00:00
- **Authors**: Kevin Duarte, Yogesh S Rawat, Mubarak Shah
- **Comment**: 8 pages, 6 figures, ICCV 2019
- **Journal**: None
- **Summary**: In this work we propose a capsule-based approach for semi-supervised video object segmentation. Current video object segmentation methods are frame-based and often require optical flow to capture temporal consistency across frames which can be difficult to compute. To this end, we propose a video based capsule network, CapsuleVOS, which can segment several frames at once conditioned on a reference frame and segmentation mask. This conditioning is performed through a novel routing algorithm for attention-based efficient capsule selection. We address two challenging issues in video object segmentation: 1) segmentation of small objects and 2) occlusion of objects across time. The issue of segmenting small objects is addressed with a zooming module which allows the network to process small spatial regions of the video. Apart from this, the framework utilizes a novel memory module based on recurrent networks which helps in tracking objects when they move out of frame or are occluded. The network is trained end-to-end and we demonstrate its effectiveness on two benchmark video object segmentation datasets; it outperforms current offline approaches on the Youtube-VOS dataset while having a run-time that is almost twice as fast as competing methods. The code is publicly available at https://github.com/KevinDuarte/CapsuleVOS.



### Custom Extended Sobel Filters
- **Arxiv ID**: http://arxiv.org/abs/1910.00138v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.00138v1)
- **Published**: 2019-09-30 22:30:09+00:00
- **Updated**: 2019-09-30 22:30:09+00:00
- **Authors**: Victor Bogdan, Cosmin Bonchiş, Ciprian Orhei
- **Comment**: None
- **Journal**: None
- **Summary**: Edge detection is widely and fundamental feature used in various algorithms in computer vision to determine the edges in an image. The edge detection algorithm is used to determine the edges in an image which are further used by various algorithms from line detection to machine learning that can determine objects based on their contour. Inspired by new convolution techniques in machine learning we discuss here the idea of extending the standard Sobel kernels, which are used to compute the gradient of an image in order to find its edges. We compare the result of our custom extended filters with the results of the standard Sobel filter and other edge detection filters using different image sets and algorithms. We present statistical results regarding the custom extended Sobel filters improvements.



### Unsupervised Projection Networks for Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.00579v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.00579v2)
- **Published**: 2019-09-30 23:31:46+00:00
- **Updated**: 2019-10-06 17:08:33+00:00
- **Authors**: Daiyaan Arfeen, Jesse Zhang
- **Comment**: 6 Pages, 8 Figures, ICCV 2019 Workshop: Sensing, Understanding and
  Synthesizing Humans
- **Journal**: None
- **Summary**: We propose the use of unsupervised learning to train projection networks that project onto the latent space of an already trained generator. We apply our method to a trained StyleGAN, and use our projection network to perform image super-resolution and clustering of images into semantically identifiable groups.



