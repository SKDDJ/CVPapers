# Arxiv Papers in cs.CV on 2019-09-22
### Volume Preserving Image Segmentation with Entropic Regularization Optimal Transport and Its Applications in Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.09931v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09931v2)
- **Published**: 2019-09-22 02:56:09+00:00
- **Updated**: 2020-01-22 18:53:24+00:00
- **Authors**: Haifeng Li, Jun Liu, Li Cui, Haiyang Huang, Xue-cheng Tai
- **Comment**: None
- **Journal**: None
- **Summary**: Image segmentation with a volume constraint is an important prior for many real applications. In this work, we present a novel volume preserving image segmentation algorithm, which is based on the framework of entropic regularized optimal transport theory. The classical Total Variation (TV) regularizer and volume preserving are integrated into a regularized optimal transport model, and the volume and classification constraints can be regarded as two measures preserving constraints in the optimal transport problem. By studying the dual problem, we develop a simple and efficient dual algorithm for our model. Moreover, to be different from many variational based image segmentation algorithms, the proposed algorithm can be directly unrolled to a new Volume Preserving and TV regularized softmax (VPTV-softmax) layer for semantic segmentation in the popular Deep Convolution Neural Network (DCNN). The experiment results show that our proposed model is very competitive and can improve the performance of many semantic segmentation nets such as the popular U-net.



### Nonlocal Patches based Gaussian Mixture Model for Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1909.09932v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09932v1)
- **Published**: 2019-09-22 03:28:05+00:00
- **Updated**: 2019-09-22 03:28:05+00:00
- **Authors**: Wei Wan, Jun Liu
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the inpainting problem for noisy images. It is very challenge to suppress noise when image inpainting is processed. An image patches based nonlocal variational method is proposed to simultaneously inpainting and denoising in this paper. Our approach is developed on an assumption that the small image patches should be obeyed a distribution which can be described by a high dimension Gaussian Mixture Model. By a maximum a posteriori (MAP) estimation, we formulate a new regularization term according to the log-likelihood function of the mixture model. To optimize this regularization term efficiently, we adopt the idea of the Expectation Maximum (EM) algorithm. In which, the expectation step can give an adaptive weighting function which can be regarded as a nonlocal connections among pixels. Using this fact, we built a framework for non-local image inpainting under noise. Moreover, we mathematically prove the existence of minimizer for the proposed inpainting model. By using a spitting algorithm, the proposed model are able to realize image inpainting and denoising simultaneously. Numerical results show that the proposed method can produce impressive reconstructed results when the inpainting region is rather large.



### Structured Binary Neural Networks for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1909.09934v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09934v4)
- **Published**: 2019-09-22 03:45:49+00:00
- **Updated**: 2022-06-06 08:57:53+00:00
- **Authors**: Bohan Zhuang, Chunhua Shen, Mingkui Tan, Peng Chen, Lingqiao Liu, Ian Reid
- **Comment**: Accepted to Int. J. Comp. Vision (IJCV). Extended version of the
  conference version arXiv:1811.10413
- **Journal**: None
- **Summary**: We propose methods to train convolutional neural networks (CNNs) with both binarized weights and activations, leading to quantized models that are specifically friendly to mobile devices with limited power capacity and computation resources. Previous works on quantizing CNNs often seek to approximate the floating-point information using a set of discrete values, which we call value approximation, typically assuming the same architecture as the full-precision networks. Here we take a novel "structure approximation" view of quantization -- it is very likely that different architectures designed for low-bit networks may be better for achieving good performance. In particular, we propose a "network decomposition" strategy, termed Group-Net, in which we divide the network into groups. Thus, each full-precision group can be effectively reconstructed by aggregating a set of homogeneous binary branches. In addition, we learn effective connections among groups to improve the representation capability. Moreover, the proposed Group-Net shows strong generalization to other tasks. For instance, we extend Group-Net for accurate semantic segmentation by embedding rich context into the binary structure. Furthermore, for the first time, we apply binary neural networks to object detection. Experiments on both classification, semantic segmentation and object detection tasks demonstrate the superior performance of the proposed methods over various quantized networks in the literature. Our methods outperform the previous best binary neural networks in terms of accuracy and computation efficiency.



### Watch, Listen and Tell: Multi-modal Weakly Supervised Dense Event Captioning
- **Arxiv ID**: http://arxiv.org/abs/1909.09944v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09944v2)
- **Published**: 2019-09-22 06:12:25+00:00
- **Updated**: 2019-10-25 08:42:41+00:00
- **Authors**: Tanzila Rahman, Bicheng Xu, Leonid Sigal
- **Comment**: None
- **Journal**: ICCV2019
- **Summary**: Multi-modal learning, particularly among imaging and linguistic modalities, has made amazing strides in many high-level fundamental visual understanding problems, ranging from language grounding to dense event captioning. However, much of the research has been limited to approaches that either do not take audio corresponding to video into account at all, or those that model the audio-visual correlations in service of sound or sound source localization. In this paper, we present the evidence, that audio signals can carry surprising amount of information when it comes to high-level visual-lingual tasks. Specifically, we focus on the problem of weakly-supervised dense event captioning in videos and show that audio on its own can nearly rival performance of a state-of-the-art visual model and, combined with video, can improve on the state-of-the-art performance. Extensive experiments on the ActivityNet Captions dataset show that our proposed multi-modal approach outperforms state-of-the-art unimodal methods, as well as validate specific feature representation and architecture design choices.



### To What Extent Does Downsampling, Compression, and Data Scarcity Impact Renal Image Analysis?
- **Arxiv ID**: http://arxiv.org/abs/1909.09945v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09945v1)
- **Published**: 2019-09-22 06:16:27+00:00
- **Updated**: 2019-09-22 06:16:27+00:00
- **Authors**: Can Peng, Kun Zhao, Arnold Wiliem, Teng Zhang, Peter Hobson, Anthony Jennings, Brian C. Lovell
- **Comment**: None
- **Journal**: None
- **Summary**: The condition of the Glomeruli, or filter sacks, in renal Direct Immunofluorescence (DIF) specimens is a critical indicator for diagnosing kidney diseases. A digital pathology system which digitizes a glass histology slide into a Whole Slide Image (WSI) and then automatically detects and zooms in on the glomeruli with a higher magnification objective will be extremely helpful for pathologists. In this paper, using glomerulus detection as the study case, we provide analysis and observations on several important issues to help with the development of Computer Aided Diagnostic (CAD) systems to process WSIs. Large image resolution, large file size, and data scarcity are always challenging to deal with. To this end, we first examine image downsampling rates in terms of their effect on detection accuracy. Second, we examine the impact of image compression. Third, we examine the relationship between the size of the training set and detection accuracy. To understand the above issues, experiments are performed on the state-of-the-art detectors: Faster R-CNN, R-FCN, Mask R-CNN and SSD. Critical findings are observed: (1) The best balance between detection accuracy, detection speed and file size is achieved at 8 times downsampling captured with a $40\times$ objective; (2) compression which reduces the file size dramatically, does not necessarily have an adverse effect on overall accuracy; (3) reducing the amount of training data to some extents causes a drop in precision but has a negligible impact on the recall; (4) in most cases, Faster R-CNN achieves the best accuracy in the glomerulus detection task. We show that the image file size of $40\times$ WSI images can be reduced by a factor of over 6000 with negligible loss of glomerulus detection accuracy.



### Semi-supervised estimation of event temporal length for cell event detection
- **Arxiv ID**: http://arxiv.org/abs/1909.09946v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09946v1)
- **Published**: 2019-09-22 06:22:36+00:00
- **Updated**: 2019-09-22 06:22:36+00:00
- **Authors**: Ha Tran Hong Phan, Ashnil Kumar, David Feng, Michael Fulham, Jinman Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Cell event detection in cell videos is essential for monitoring of cellular behavior over extended time periods. Deep learning methods have shown great success in the detection of cell events for their ability to capture more discriminative features of cellular processes compared to traditional methods. In particular, convolutional long short-term memory (LSTM) models, which exploits the changes in cell events observable in video sequences, is the state-of-the-art for mitosis detection in cell videos. However, their limitations are the determination of the input sequence length, which is often performed empirically, and the need for a large annotated training dataset which is expensive to prepare. We propose a novel semi-supervised method of optimal length detection for mitosis detection with two key contributions: (i) an unsupervised step for learning the spatial and temporal locations of cells in their normal stage and approximating the distribution of temporal lengths of cell events and, (ii) a step of inferring, from that distribution, an optimal input sequence length and a minimal number of annotated frames for training a LSTM model for each particular video. We evaluated our method in detecting mitosis in densely packed stem cells in a phase-contrast microscopy videos. Our experimental data prove that increasing the input sequence length of LSTM can lead to a decrease in performance. Our results also show that by approximating the optimal input sequence length of the tested video, a model trained with only 18 annotated frames achieved F1-scores of 0.880-0.907, which are 10% higher than those of other published methods with a full set of 110 training annotated frames.



### Learning Visual Relation Priors for Image-Text Matching and Image Captioning with Neural Scene Graph Generators
- **Arxiv ID**: http://arxiv.org/abs/1909.09953v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1909.09953v1)
- **Published**: 2019-09-22 07:30:29+00:00
- **Updated**: 2019-09-22 07:30:29+00:00
- **Authors**: Kuang-Huei Lee, Hamid Palangi, Xi Chen, Houdong Hu, Jianfeng Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Grounding language to visual relations is critical to various language-and-vision applications. In this work, we tackle two fundamental language-and-vision tasks: image-text matching and image captioning, and demonstrate that neural scene graph generators can learn effective visual relation features to facilitate grounding language to visual relations and subsequently improve the two end applications. By combining relation features with the state-of-the-art models, our experiments show significant improvement on the standard Flickr30K and MSCOCO benchmarks. Our experimental results and analysis show that relation features improve downstream models' capability of capturing visual relations in end vision-and-language applications. We also demonstrate the importance of learning scene graph generators with visually relevant relations to the effectiveness of relation features.



### FlatteNet: A Simple Versatile Framework for Dense Pixelwise Prediction
- **Arxiv ID**: http://arxiv.org/abs/1909.09961v3
- **DOI**: 10.1109/ACCESS.2019.2959640
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09961v3)
- **Published**: 2019-09-22 08:05:04+00:00
- **Updated**: 2019-11-08 02:47:21+00:00
- **Authors**: Xin Cai, Yi-Fei Pu
- **Comment**: None
- **Journal**: IEEE Access, vol. 7, pp. 179985-179996, 2019
- **Summary**: In this paper, we focus on devising a versatile framework for dense pixelwise prediction whose goal is to assign a discrete or continuous label to each pixel for an image. It is well-known that the reduced feature resolution due to repeated subsampling operations poses a serious challenge to Fully Convolutional Network (FCN) based models. In contrast to the commonly-used strategies, such as dilated convolution and encoder-decoder structure, we introduce the Flattening Module to produce high-resolution predictions without either removing any subsampling operations or building a complicated decoder module. In addition, the Flattening Module is lightweight and can be easily combined with any existing FCNs, allowing the model builder to trade off among model size, computational cost and accuracy by simply choosing different backbone networks. We empirically demonstrate the effectiveness of the proposed Flattening Module through competitive results in human pose estimation on MPII, semantic segmentation on PASCAL-Context and object detection on PASCAL VOC. We hope that the proposed approach can serve as a simple and strong alternative of current dominant dense pixelwise prediction frameworks.



### LoGANv2: Conditional Style-Based Logo Generation with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.09974v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.09974v1)
- **Published**: 2019-09-22 10:29:19+00:00
- **Updated**: 2019-09-22 10:29:19+00:00
- **Authors**: Cedric Oeldorf, Gerasimos Spanakis
- **Comment**: accepted for poster presentation at ICMLA 2019, data+code available:
  https://github.com/cedricoeldorf/ConditionalStyleGAN
- **Journal**: None
- **Summary**: Domains such as logo synthesis, in which the data has a high degree of multi-modality, still pose a challenge for generative adversarial networks (GANs). Recent research shows that progressive training (ProGAN) and mapping network extensions (StyleGAN) enable both increased training stability for higher dimensional problems and better feature separation within the embedded latent space. However, these architectures leave limited control over shaping the output of the network, which is an undesirable trait in the case of logo synthesis. This paper explores a conditional extension to the StyleGAN architecture with the aim of firstly, improving on the low resolution results of previous research and, secondly, increasing the controllability of the output through the use of synthetic class-conditions. Furthermore, methods of extracting such class conditions are explored with a focus on the human interpretability, where the challenge lies in the fact that, by nature, visual logo characteristics are hard to define. The introduced conditional style-based generator architecture is trained on the extracted class-conditions in two experiments and studied relative to the performance of an unconditional model. Results show that, whilst the unconditional model more closely matches the training distribution, high quality conditions enabled the embedding of finer details onto the latent space, leading to more diverse output.



### Variational Conditional GAN for Fine-grained Controllable Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1909.09979v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09979v1)
- **Published**: 2019-09-22 10:52:33+00:00
- **Updated**: 2019-09-22 10:52:33+00:00
- **Authors**: Mingqi Hu, Deyu Zhou, Yulan He
- **Comment**: Accepted at the 11th Asian Conference on Machine Learning (ACML 2019)
- **Journal**: None
- **Summary**: In this paper, we propose a novel variational generator framework for conditional GANs to catch semantic details for improving the generation quality and diversity. Traditional generators in conditional GANs simply concatenate the conditional vector with the noise as the input representation, which is directly employed for upsampling operations. However, the hidden condition information is not fully exploited, especially when the input is a class label. Therefore, we introduce a variational inference into the generator to infer the posterior of latent variable only from the conditional input, which helps achieve a variable augmented representation for image generation. Qualitative and quantitative experimental results show that the proposed method outperforms the state-of-the-art approaches and achieves the realistic controllable images.



### Double Anchor R-CNN for Human Detection in a Crowd
- **Arxiv ID**: http://arxiv.org/abs/1909.09998v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09998v1)
- **Published**: 2019-09-22 13:01:08+00:00
- **Updated**: 2019-09-22 13:01:08+00:00
- **Authors**: Kevin Zhang, Feng Xiong, Peize Sun, Li Hu, Boxun Li, Gang Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting human in a crowd is a challenging problem due to the uncertainties of occlusion patterns. In this paper, we propose to handle the crowd occlusion problem in human detection by leveraging the head part. Double Anchor RPN is developed to capture body and head parts in pairs. A proposal crossover strategy is introduced to generate high-quality proposals for both parts as a training augmentation. Features of coupled proposals are then aggregated efficiently to exploit the inherent relationship. Finally, a Joint NMS module is developed for robust post-processing. The proposed framework, called Double Anchor R-CNN, is able to detect the body and head for each person simultaneously in crowded scenarios. State-of-the-art results are reported on challenging human detection datasets. Our model yields log-average miss rates (MR) of 51.79pp on CrowdHuman, 55.01pp on COCOPersons~(crowded sub-dataset) and 40.02pp on CrowdPose~(crowded sub-dataset), which outperforms previous baseline detectors by 3.57pp, 3.82pp, and 4.24pp, respectively. We hope our simple and effective approach will serve as a solid baseline and help ease future research in crowded human detection.



### Tag-based Semantic Features for Scene Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1909.09999v1
- **DOI**: 10.1007/978-3-030-36718-3_8
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1909.09999v1)
- **Published**: 2019-09-22 13:17:39+00:00
- **Updated**: 2019-09-22 13:17:39+00:00
- **Authors**: Chiranjibi Sitaula, Yong Xiang, Anish Basnet, Sunil Aryal, Xuequan Lu
- **Comment**: Accepted by ICONIP2019 conference
- **Journal**: In: Gedeon T., Wong K., Lee M. (eds) Neural Information
  Processing. ICONIP 2019., vol 11955 (2019)
- **Summary**: The existing image feature extraction methods are primarily based on the content and structure information of images, and rarely consider the contextual semantic information. Regarding some types of images such as scenes and objects, the annotations and descriptions of them available on the web may provide reliable contextual semantic information for feature extraction. In this paper, we introduce novel semantic features of an image based on the annotations and descriptions of its similar images available on the web. Specifically, we propose a new method which consists of two consecutive steps to extract our semantic features. For each image in the training set, we initially search the top $k$ most similar images from the internet and extract their annotations/descriptions (e.g., tags or keywords). The annotation information is employed to design a filter bank for each image category and generate filter words (codebook). Finally, each image is represented by the histogram of the occurrences of filter words in all categories. We evaluate the performance of the proposed features in scene image classification on three commonly-used scene image datasets (i.e., MIT-67, Scene15 and Event8). Our method typically produces a lower feature dimension than existing feature extraction methods. Experimental results show that the proposed features generate better classification accuracies than vision based and tag based features, and comparable results to deep learning based features.



### mlVIRNET: Multilevel Variational Image Registration Network
- **Arxiv ID**: http://arxiv.org/abs/1909.10084v1
- **DOI**: 10.1007/978-3-030-32226-7_29
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10084v1)
- **Published**: 2019-09-22 20:13:48+00:00
- **Updated**: 2019-09-22 20:13:48+00:00
- **Authors**: Alessa Hering, Bram van Ginneken, Stefan Heldmann
- **Comment**: accepted for publication at MICCAI 2019
- **Journal**: None
- **Summary**: We present a novel multilevel approach for deep learning based image registration. Recently published deep learning based registration methods have shown promising results for a wide range of tasks. However, these algorithms are still limited to relatively small deformations. Our method addresses this shortcoming by introducing a multilevel framework, which computes deformation fields on different scales, similar to conventional methods. Thereby, a coarse-level alignment is obtained first, which is subsequently improved on finer levels. We demonstrate our method on the complex task of inhale-to-exhale lung registration. We show that the use of a deep learning multilevel approach leads to significantly better registration results.



### Using machine learning to construct velocity fields from OH-PLIF images
- **Arxiv ID**: http://arxiv.org/abs/1909.13669v1
- **DOI**: None
- **Categories**: **physics.flu-dyn**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.13669v1)
- **Published**: 2019-09-22 23:26:34+00:00
- **Updated**: 2019-09-22 23:26:34+00:00
- **Authors**: Shivam Barwey, Malik Hassanaly, Venkat Raman, Adam Steinberg
- **Comment**: None
- **Journal**: None
- **Summary**: This work utilizes data-driven methods to morph a series of time-resolved experimental OH-PLIF images into corresponding three-component planar PIV fields in the closed domain of a premixed swirl combustor. The task is carried out with a fully convolutional network, which is a type of convolutional neural network (CNN) used in many applications in machine learning, alongside an existing experimental dataset which consists of simultaneous OH-PLIF and PIV measurements in both attached and detached flame regimes. Two types of models are compared: 1) a global CNN which is trained using images from the entire domain, and 2) a set of local CNNs, which are trained only on individual sections of the domain. The locally trained models show improvement in creating mappings in the detached regime over the global models. A comparison between model performance in attached and detached regimes shows that the CNNs are much more accurate across the board in creating velocity fields for attached flames. Inclusion of time history in the PLIF input resulted in small noticeable improvement on average, which could imply a greater physical role of instantaneous spatial correlations in the decoding process over temporal dependencies from the perspective of the CNN. Additionally, the performance of local models trained to produce mappings in one section of the domain is tested on other, unexplored sections of the domain. Interestingly, local CNN performance on unseen domain regions revealed the models' ability to utilize symmetry and antisymmetry in the velocity field. Ultimately, this work shows the powerful ability of the CNN to decode the three-dimensional PIV fields from input OH-PLIF images, providing a potential groundwork for a very useful tool for experimental configurations in which accessibility of forms of simultaneous measurements are limited.



