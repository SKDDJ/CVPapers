# Arxiv Papers in cs.CV on 2019-09-19
### Graph Neural Networks for Image Understanding Based on Multiple Cues: Group Emotion Recognition and Event Recognition as Use Cases
- **Arxiv ID**: http://arxiv.org/abs/1909.12911v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12911v2)
- **Published**: 2019-09-19 00:22:36+00:00
- **Updated**: 2020-02-28 05:34:17+00:00
- **Authors**: Xin Guo, Luisa F. Polania, Bin Zhu, Charles Boncelet, Kenneth E. Barner
- **Comment**: Paper accepted for publication at the 2020 IEEE Winter Conference on
  Applications of Computer Vision (WACV)
- **Journal**: None
- **Summary**: A graph neural network (GNN) for image understanding based on multiple cues is proposed in this paper. Compared to traditional feature and decision fusion approaches that neglect the fact that features can interact and exchange information, the proposed GNN is able to pass information among features extracted from different models. Two image understanding tasks, namely group-level emotion recognition (GER) and event recognition, which are highly semantic and require the interaction of several deep models to synthesize multiple cues, were selected to validate the performance of the proposed method. It is shown through experiments that the proposed method achieves state-of-the-art performance on the selected image understanding tasks. In addition, a new group-level emotion recognition database is introduced and shared in this paper.



### ContCap: A scalable framework for continual image captioning
- **Arxiv ID**: http://arxiv.org/abs/1909.08745v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08745v2)
- **Published**: 2019-09-19 00:31:17+00:00
- **Updated**: 2020-04-21 02:56:33+00:00
- **Authors**: Giang Nguyen, Tae Joon Jun, Trung Tran, Tolcha Yalew, Daeyoung Kim
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: While advanced image captioning systems are increasingly describing images coherently and exactly, recent progress in continual learning allows deep learning models to avoid catastrophic forgetting. However, the domain where image captioning working with continual learning has not yet been explored. We define the task in which we consolidate continual learning and image captioning as continual image captioning. In this work, we propose ContCap, a framework generating captions over a series of new tasks coming, seamlessly integrating continual learning into image captioning besides addressing catastrophic forgetting. After proving forgetting in image captioning, we propose various techniques to overcome the forgetting dilemma by taking a simple fine-tuning schema as the baseline. We split MS-COCO 2014 dataset to perform experiments in class-incremental settings without revisiting dataset of previously provided tasks. Experiments show remarkable improvements in the performance on the old tasks while the figures for the new surprisingly surpass fine-tuning. Our framework also offers a scalable solution for continual image or video captioning.



### A New Few-shot Segmentation Network Based on Class Representation
- **Arxiv ID**: http://arxiv.org/abs/1909.08754v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08754v1)
- **Published**: 2019-09-19 00:59:56+00:00
- **Updated**: 2019-09-19 00:59:56+00:00
- **Authors**: Yuwei Yang, Fanman Meng, Hongliang Li, King N. Ngan, Qingbo Wu
- **Comment**: accepted by VCIP2019
- **Journal**: None
- **Summary**: This paper studies few-shot segmentation, which is a task of predicting foreground mask of unseen classes by a few of annotations only, aided by a set of rich annotations already existed. The existing methods mainly focus the task on "\textit{how to transfer segmentation cues from support images (labeled images) to query images (unlabeled images)}", and try to learn efficient and general transfer module that can be easily extended to unseen classes. However, it is proved to be a challenging task to learn the transfer module that is general to various classes. This paper solves few-shot segmentation in a new perspective of "\textit{how to represent unseen classes by existing classes}", and formulates few-shot segmentation as the representation process that represents unseen classes (in terms of forming the foreground prior) by existing classes precisely. Based on such idea, we propose a new class representation based few-shot segmentation framework, which firstly generates class activation map of unseen class based on the knowledge of existing classes, and then uses the map as foreground probability map to extract the foregrounds from query image. A new two-branch based few-shot segmentation network is proposed. Moreover, a new CAM generation module that extracts the CAM of unseen classes rather than the classical training classes is raised. We validate the effectiveness of our method on Pascal VOC 2012 dataset, the value FB-IoU of one-shot and five-shot arrives at 69.2\% and 70.1\% respectively, which outperforms the state-of-the-art method.



### EPOSIT: An Absolute Pose Estimation Method for Pinhole and Fish-Eye Cameras
- **Arxiv ID**: http://arxiv.org/abs/1909.12945v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/1909.12945v1)
- **Published**: 2019-09-19 01:11:43+00:00
- **Updated**: 2019-09-19 01:11:43+00:00
- **Authors**: Zhaobing Kang, Wei Zou, Zheng Zhu, Chi Zhang, Hongxuan Ma
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a generic 6DOF camera pose estimation method, which can be used for both the pinhole camera and the fish-eye camera. Different from existing methods, relative positions of 3D points rather than absolute coordinates in the world coordinate system are employed in our method, and it has a unique solution. The application scope of POSIT (Pose from Orthography and Scaling with Iteration) algorithm is generalized to fish-eye cameras by combining with the radially symmetric projection model. The image point relationship between the pinhole camera and the fish-eye camera is derived based on their projection model. The general pose expression which fits for different cameras can be acquired by four noncoplanar object points and their corresponding image points. Accurate estimation results are calculated iteratively. Experimental results on synthetic and real data show that the pose estimation results of our method are more stable and accurate than state-of-the-art methods. The source code is available at https://github.com/k032131/EPOSIT.



### A High-Fidelity Open Embodied Avatar with Lip Syncing and Expression Capabilities
- **Arxiv ID**: http://arxiv.org/abs/1909.08766v2
- **DOI**: 10.1145/3340555.3353744
- **Categories**: **cs.HC**, cs.AI, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1909.08766v2)
- **Published**: 2019-09-19 01:39:39+00:00
- **Updated**: 2019-10-15 05:10:21+00:00
- **Authors**: Deepali Aneja, Daniel McDuff, Shital Shah
- **Comment**: International Conference on Multimodal Interaction (ICMI 2019)
- **Journal**: None
- **Summary**: Embodied avatars as virtual agents have many applications and provide benefits over disembodied agents, allowing non-verbal social and interactional cues to be leveraged, in a similar manner to how humans interact with each other. We present an open embodied avatar built upon the Unreal Engine that can be controlled via a simple python programming interface. The avatar has lip syncing (phoneme control), head gesture and facial expression (using either facial action units or cardinal emotion categories) capabilities. We release code and models to illustrate how the avatar can be controlled like a puppet or used to create a simple conversational agent using public application programming interfaces (APIs). GITHUB link: https://github.com/danmcduff/AvatarSim



### Transfer Learning using CNN for Handwritten Devanagari Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/1909.08774v1
- **DOI**: 10.1109/ICAIT47043.2019.8987286
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.08774v1)
- **Published**: 2019-09-19 02:04:55+00:00
- **Updated**: 2019-09-19 02:04:55+00:00
- **Authors**: Nagender Aneja, Sandhya Aneja
- **Comment**: None
- **Journal**: IEEE International Conference on Advances in Information
  Technology (ICAIT), ICAIT - 2019
- **Summary**: This paper presents an analysis of pre-trained models to recognize handwritten Devanagari alphabets using transfer learning for Deep Convolution Neural Network (DCNN). This research implements AlexNet, DenseNet, Vgg, and Inception ConvNet as a fixed feature extractor. We implemented 15 epochs for each of AlexNet, DenseNet 121, DenseNet 201, Vgg 11, Vgg 16, Vgg 19, and Inception V3. Results show that Inception V3 performs better in terms of accuracy achieving 99% accuracy with average epoch time 16.3 minutes while AlexNet performs fastest with 2.2 minutes per epoch and achieving 98\% accuracy.



### Large-scale representation learning from visually grounded untranscribed speech
- **Arxiv ID**: http://arxiv.org/abs/1909.08782v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1909.08782v1)
- **Published**: 2019-09-19 02:50:23+00:00
- **Updated**: 2019-09-19 02:50:23+00:00
- **Authors**: Gabriel Ilharco, Yuan Zhang, Jason Baldridge
- **Comment**: None
- **Journal**: The SIGNLL Conference on Computational Natural Language Learning
  (CoNLL), 2019
- **Summary**: Systems that can associate images with their spoken audio captions are an important step towards visually grounded language learning. We describe a scalable method to automatically generate diverse audio for image captioning datasets. This supports pretraining deep networks for encoding both audio and images, which we do via a dual encoder that learns to align latent representations from both modalities. We show that a masked margin softmax loss for such models is superior to the standard triplet loss. We fine-tune these models on the Flickr8k Audio Captions Corpus and obtain state-of-the-art results---improving recall in the top 10 from 29.6% to 49.5%. We also obtain human ratings on retrieval outputs to better assess the impact of incidentally matching image-caption pairs that were not associated in the data, finding that automatic evaluation substantially underestimates the quality of the retrieved results.



### Dual Encoder-Decoder based Generative Adversarial Networks for Disentangled Facial Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.08797v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08797v1)
- **Published**: 2019-09-19 04:19:21+00:00
- **Updated**: 2019-09-19 04:19:21+00:00
- **Authors**: Cong Hu, Zhen-Hua Feng, Xiao-Jun Wu, Josef Kittler
- **Comment**: None
- **Journal**: None
- **Summary**: To learn disentangled representations of facial images, we present a Dual Encoder-Decoder based Generative Adversarial Network (DED-GAN). In the proposed method, both the generator and discriminator are designed with deep encoder-decoder architectures as their backbones. To be more specific, the encoder-decoder structured generator is used to learn a pose disentangled face representation, and the encoder-decoder structured discriminator is tasked to perform real/fake classification, face reconstruction, determining identity and estimating face pose. We further improve the proposed network architecture by minimising the additional pixel-wise loss defined by the Wasserstein distance at the output of the discriminator so that the adversarial framework can be better trained. Additionally, we consider face pose variation to be continuous, rather than discrete in existing literature, to inject richer pose information into our model. The pose estimation task is formulated as a regression problem, which helps to disentangle identity information from pose variations. The proposed network is evaluated on the tasks of pose-invariant face recognition (PIFR) and face synthesis across poses. An extensive quantitative and qualitative evaluation carried out on several controlled and in-the-wild benchmarking datasets demonstrates the superiority of the proposed DED-GAN method over the state-of-the-art approaches.



### Absum: Simple Regularization Method for Reducing Structural Sensitivity of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.08830v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.08830v1)
- **Published**: 2019-09-19 07:05:14+00:00
- **Updated**: 2019-09-19 07:05:14+00:00
- **Authors**: Sekitoshi Kanai, Yasutoshi Ida, Yasuhiro Fujiwara, Masanori Yamada, Shuichi Adachi
- **Comment**: 16 pages, 39 figures
- **Journal**: None
- **Summary**: We propose Absum, which is a regularization method for improving adversarial robustness of convolutional neural networks (CNNs). Although CNNs can accurately recognize images, recent studies have shown that the convolution operations in CNNs commonly have structural sensitivity to specific noise composed of Fourier basis functions. By exploiting this sensitivity, they proposed a simple black-box adversarial attack: Single Fourier attack. To reduce structural sensitivity, we can use regularization of convolution filter weights since the sensitivity of linear transform can be assessed by the norm of the weights. However, standard regularization methods can prevent minimization of the loss function because they impose a tight constraint for obtaining high robustness. To solve this problem, Absum imposes a loose constraint; it penalizes the absolute values of the summation of the parameters in the convolution layers. Absum can improve robustness against single Fourier attack while being as simple and efficient as standard regularization methods (e.g., weight decay and L1 regularization). Our experiments demonstrate that Absum improves robustness against single Fourier attack more than standard regularization methods. Furthermore, we reveal that robust CNNs with Absum are more robust against transferred attacks due to decreasing the common sensitivity and against high-frequency noise than standard regularization methods. We also reveal that Absum can improve robustness against gradient-based attacks (projected gradient descent) when used with adversarial training.



### Social and Scene-Aware Trajectory Prediction in Crowded Spaces
- **Arxiv ID**: http://arxiv.org/abs/1909.08840v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08840v1)
- **Published**: 2019-09-19 08:03:02+00:00
- **Updated**: 2019-09-19 08:03:02+00:00
- **Authors**: Matteo Lisotto, Pasquale Coscia, Lamberto Ballan
- **Comment**: Accepted to ICCV 2019 Workshop on Assistive Computer Vision and
  Robotics (ACVR)
- **Journal**: None
- **Summary**: Mimicking human ability to forecast future positions or interpret complex interactions in urban scenarios, such as streets, shopping malls or squares, is essential to develop socially compliant robots or self-driving cars. Autonomous systems may gain advantage on anticipating human motion to avoid collisions or to naturally behave alongside people. To foresee plausible trajectories, we construct an LSTM (long short-term memory)-based model considering three fundamental factors: people interactions, past observations in terms of previously crossed areas and semantics of surrounding space. Our model encompasses several pooling mechanisms to join the above elements defining multiple tensors, namely social, navigation and semantic tensors. The network is tested in unstructured environments where complex paths emerge according to both internal (intentions) and external (other people, not accessible areas) motivations. As demonstrated, modeling paths unaware of social interactions or context information, is insufficient to correctly predict future positions. Experimental results corroborate the effectiveness of the proposed framework in comparison to LSTM-based models for human path prediction.



### Localization with Limited Annotation for Chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/1909.08842v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08842v2)
- **Published**: 2019-09-19 08:07:28+00:00
- **Updated**: 2019-10-10 14:05:44+00:00
- **Authors**: Eyal Rozenberg, Daniel Freedman, Alex Bronstein
- **Comment**: None
- **Journal**: Proceedings of the Machine Learning for Health NeurIPS Workshop,
  PMLR 116:52-65, 2020
- **Summary**: Localization of an object within an image is a common task in medical imaging. Learning to localize or detect objects typically requires the collection of data which has been labelled with bounding boxes or similar annotations, which can be very time consuming and expensive. A technique which could perform such learning with much less annotation would, therefore, be quite valuable. We present such a technique for localization with limited annotation, in which the number of images with bounding boxes can be a small fraction of the total dataset (e.g. less than 1%); all other images only possess a whole image label and no bounding box. We propose a novel loss function for tackling this problem; the loss is a continuous relaxation of a well-defined discrete formulation of weakly supervised learning and is numerically well-posed. Furthermore, we propose a new architecture which accounts for both patch dependence and shift-invariance, through the inclusion of CRF layers and anti-aliasing filters, respectively. We apply our technique to the localization of thoracic diseases in chest X-ray images and demonstrate state-of-the-art localization performance on the ChestX-ray14 dataset.



### Slices of Attention in Asynchronous Video Job Interviews
- **Arxiv ID**: http://arxiv.org/abs/1909.08845v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1909.08845v1)
- **Published**: 2019-09-19 08:15:46+00:00
- **Updated**: 2019-09-19 08:15:46+00:00
- **Authors**: Léo Hemamou, Ghazi Felhi, Jean-Claude Martin, Chloé Clavel
- **Comment**: Accepted at 2019 8th International Conference on Affective Computing
  and Intelligent Interaction (ACII)
- **Journal**: None
- **Summary**: The impact of non verbal behaviour in a hiring decision remains an open question. Investigating this question is important, as it could provide a better understanding on how to train candidates for job interviews and make recruiters be aware of influential non verbal behaviour. This research has recently been accelerated due to the development of tools for the automatic analysis of social signals, and the emergence of machine learning methods. However, these studies are still mainly based on hand engineered features, which imposes a limit to the discovery of influential social signals. On the other side, deep learning methods are a promising tool to discover complex patterns without the necessity of feature engineering. In this paper, we focus on studying influential non verbal social signals in asynchronous job video interviews that are discovered by deep learning methods. We use a previously published deep learning system that aims at inferring the hirability of a candidate with regard to a sequence of interview questions. One particularity of this system is the use of attention mechanisms, which aim at identifying the relevant parts of an answer. Thus, information at a fine-grained temporal level could be extracted using global (at the interview level) annotations on hirability. While most of the deep learning systems use attention mechanisms to offer a quick visualization of slices when a rise of attention occurs, we perform an in-depth analysis to understand what happens during these moments. First, we propose a methodology to automatically extract slices where there is a rise of attention (attention slices). Second, we study the content of attention slices by comparing them with randomly sampled slices. Finally, we show that they bear significantly more information for hirability than randomly sampled slices.



### Biometric Face Presentation Attack Detection with Multi-Channel Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1909.08848v1
- **DOI**: 10.1109/TIFS.2019.2916652
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1909.08848v1)
- **Published**: 2019-09-19 08:16:35+00:00
- **Updated**: 2019-09-19 08:16:35+00:00
- **Authors**: Anjith George, Zohreh Mostaani, David Geissenbuhler, Olegs Nikisins, Andre Anjos, Sebastien Marcel
- **Comment**: 16 pages
- **Journal**: IEEE Transactions on Information Forensics and Security, 2019
- **Summary**: Face recognition is a mainstream biometric authentication method. However, vulnerability to presentation attacks (a.k.a spoofing) limits its usability in unsupervised applications. Even though there are many methods available for tackling presentation attacks (PA), most of them fail to detect sophisticated attacks such as silicone masks. As the quality of presentation attack instruments improves over time, achieving reliable PA detection with visual spectra alone remains very challenging. We argue that analysis in multiple channels might help to address this issue. In this context, we propose a multi-channel Convolutional Neural Network based approach for presentation attack detection (PAD). We also introduce the new Wide Multi-Channel presentation Attack (WMCA) database for face PAD which contains a wide variety of 2D and 3D presentation attacks for both impersonation and obfuscation attacks. Data from different channels such as color, depth, near-infrared and thermal are available to advance the research in face PAD. The proposed method was compared with feature-based approaches and found to outperform the baselines achieving an ACER of 0.3% on the introduced dataset. The database and the software to reproduce the results are made available publicly.



### Testing the robustness of attribution methods for convolutional neural networks in MRI-based Alzheimer's disease classification
- **Arxiv ID**: http://arxiv.org/abs/1909.08856v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.08856v1)
- **Published**: 2019-09-19 08:30:31+00:00
- **Updated**: 2019-09-19 08:30:31+00:00
- **Authors**: Fabian Eitel, Kerstin Ritter
- **Comment**: None
- **Journal**: None
- **Summary**: Attribution methods are an easy to use tool for investigating and validating machine learning models. Multiple methods have been suggested in the literature and it is not yet clear which method is most suitable for a given task. In this study, we tested the robustness of four attribution methods, namely gradient*input, guided backpropagation, layer-wise relevance propagation and occlusion, for the task of Alzheimer's disease classification. We have repeatedly trained a convolutional neural network (CNN) with identical training settings in order to separate structural MRI data of patients with Alzheimer's disease and healthy controls. Afterwards, we produced attribution maps for each subject in the test data and quantitatively compared them across models and attribution methods. We show that visual comparison is not sufficient and that some widely used attribution methods produce highly inconsistent outcomes.



### Data Augmentation Revisited: Rethinking the Distribution Gap between Clean and Augmented Data
- **Arxiv ID**: http://arxiv.org/abs/1909.09148v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.09148v2)
- **Published**: 2019-09-19 08:36:45+00:00
- **Updated**: 2019-11-21 15:56:49+00:00
- **Authors**: Zhuoxun He, Lingxi Xie, Xin Chen, Ya Zhang, Yanfeng Wang, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation has been widely applied as an effective methodology to improve generalization in particular when training deep neural networks. Recently, researchers proposed a few intensive data augmentation techniques, which indeed improved accuracy, yet we notice that these methods augment data have also caused a considerable gap between clean and augmented data. In this paper, we revisit this problem from an analytical perspective, for which we estimate the upper-bound of expected risk using two terms, namely, empirical risk and generalization error, respectively. We develop an understanding of data augmentation as regularization, which highlights the major features. As a result, data augmentation significantly reduces the generalization error, but meanwhile leads to a slightly higher empirical risk. On the assumption that data augmentation helps models converge to a better region, the model can benefit from a lower empirical risk achieved by a simple method, i.e., using less-augmented data to refine the model trained on fully-augmented data. Our approach achieves consistent accuracy gain on a few standard image classification benchmarks, and the gain transfers to object detection.



### Procedural Reasoning Networks for Understanding Multimodal Procedures
- **Arxiv ID**: http://arxiv.org/abs/1909.08859v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.08859v1)
- **Published**: 2019-09-19 08:39:00+00:00
- **Updated**: 2019-09-19 08:39:00+00:00
- **Authors**: Mustafa Sercan Amac, Semih Yagcioglu, Aykut Erdem, Erkut Erdem
- **Comment**: Accepted to CoNLL 2019. The project website with code and demo is
  available at https://hucvl.github.io/prn/
- **Journal**: None
- **Summary**: This paper addresses the problem of comprehending procedural commonsense knowledge. This is a challenging task as it requires identifying key entities, keeping track of their state changes, and understanding temporal and causal relations. Contrary to most of the previous work, in this study, we do not rely on strong inductive bias and explore the question of how multimodality can be exploited to provide a complementary semantic signal. Towards this end, we introduce a new entity-aware neural comprehension model augmented with external relational memory units. Our model learns to dynamically update entity states in relation to each other while reading the text instructions. Our experimental analysis on the visual reasoning tasks in the recently proposed RecipeQA dataset reveals that our approach improves the accuracy of the previously reported models by a large margin. Moreover, we find that our model learns effective dynamic representations of entities even though we do not use any supervision at the level of entity states.



### Assembly of randomly placed parts realized by using only one robot arm with a general parallel-jaw gripper
- **Arxiv ID**: http://arxiv.org/abs/1909.08862v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.08862v1)
- **Published**: 2019-09-19 08:43:58+00:00
- **Updated**: 2019-09-19 08:43:58+00:00
- **Authors**: Jie Zhao, Xin Jiang, Xiaoman Wang, Shengfan Wang, Yunhui Liu
- **Comment**: Submitted in ICRA 2020
- **Journal**: None
- **Summary**: In industry assembly lines, parts feeding machines are widely employed as the prologue of the whole procedure. They play the role of sorting the parts randomly placed in bins to the state with specified pose. With the help of the parts feeding machines, the subsequent assembly processes by robot arm can always start from the same condition. Thus it is expected that function of parting feeding machine and the robotic assembly can be integrated with one robot arm. This scheme can provide great flexibility and can also contribute to reduce the cost. The difficulties involved in this scheme lie in the fact that in the part feeding phase, the pose of the part after grasping may be not proper for the subsequent assembly. Sometimes it can not even guarantee a stable grasp. In this paper, we proposed a method to integrate parts feeding and assembly within one robot arm. This proposal utilizes a specially designed gripper tip mounted on the jaws of a two-fingered gripper. With the modified gripper, in-hand manipulation of the grasped object is realized, which can ensure the control of the orientation and offset position of the grasped object. The proposal in this paper is verified by a simulated assembly in which a robot arm completed the assembly process including parts picking from bin and a subsequent peg-in-hole assembly.



### Challenging deep image descriptors for retrieval in heterogeneous iconographic collections
- **Arxiv ID**: http://arxiv.org/abs/1909.08866v1
- **DOI**: 10.1145/3347317.3357246
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08866v1)
- **Published**: 2019-09-19 08:54:51+00:00
- **Updated**: 2019-09-19 08:54:51+00:00
- **Authors**: Dimitri Gominski, Martyna Poreba, Valérie Gouet-Brunet, Liming Chen
- **Comment**: SUMAC '19, 2019
- **Journal**: None
- **Summary**: This article proposes to study the behavior of recent and efficient state-of-the-art deep-learning based image descriptors for content-based image retrieval, facing a panel of complex variations appearing in heterogeneous image datasets, in particular in cultural collections that may involve multi-source, multi-date and multi-view Permission to make digital



### Learning to Avoid Poor Images: Towards Task-aware C-arm Cone-beam CT Trajectories
- **Arxiv ID**: http://arxiv.org/abs/1909.08868v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.08868v1)
- **Published**: 2019-09-19 09:00:44+00:00
- **Updated**: 2019-09-19 09:00:44+00:00
- **Authors**: Jan-Nico Zaech, Cong Gao, Bastian Bier, Russell Taylor, Andreas Maier, Nassir Navab, Mathias Unberath
- **Comment**: Accepted for oral presentation at the International Conference on
  Medical Image Computing and Computer Assisted Intervention (MICCAI) 2019
- **Journal**: None
- **Summary**: Metal artifacts in computed tomography (CT) arise from a mismatch between physics of image formation and idealized assumptions during tomographic reconstruction. These artifacts are particularly strong around metal implants, inhibiting widespread adoption of 3D cone-beam CT (CBCT) despite clear opportunity for intra-operative verification of implant positioning, e.g. in spinal fusion surgery. On synthetic and real data, we demonstrate that much of the artifact can be avoided by acquiring better data for reconstruction in a task-aware and patient-specific manner, and describe the first step towards the envisioned task-aware CBCT protocol. The traditional short-scan CBCT trajectory is planar, with little room for scene-specific adjustment. We extend this trajectory by autonomously adjusting out-of-plane angulation. This enables C-arm source trajectories that are scene-specific in that they avoid acquiring "poor images", characterized by beam hardening, photon starvation, and noise. The recommendation of ideal out-of-plane angulation is performed on-the-fly using a deep convolutional neural network that regresses a detectability-rank derived from imaging physics.



### PgNN: Physics-guided Neural Network for Fourier Ptychographic Microscopy
- **Arxiv ID**: http://arxiv.org/abs/1909.08869v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.08869v1)
- **Published**: 2019-09-19 09:02:25+00:00
- **Updated**: 2019-09-19 09:02:25+00:00
- **Authors**: Yongbing Zhang, Yangzhe Liu, Xiu Li, Shaowei Jiang, Krishna Dixit, Xinfeng Zhang, Xiangyang Ji
- **Comment**: None
- **Journal**: None
- **Summary**: Fourier ptychography (FP) is a newly developed computational imaging approach that achieves both high resolution and wide field of view by stitching a series of low-resolution images captured under angle-varied illumination. So far, many supervised data-driven models have been applied to solve inverse imaging problems. These models need massive amounts of data to train, and are limited by the dataset characteristics. In FP problems, generic datasets are always scarce, and the optical aberration varies greatly under different acquisition conditions. To address these dilemmas, we model the forward physical imaging process as an interpretable physics-guided neural network (PgNN), where the reconstructed image in the complex domain is considered as the learnable parameters of the neural network. Since the optimal parameters of the PgNN can be derived by minimizing the difference between the model-generated images and real captured angle-varied images corresponding to the same scene, the proposed PgNN can get rid of the problem of massive training data as in traditional supervised methods. Applying the alternate updating mechanism and the total variation regularization, PgNN can flexibly reconstruct images with improved performance. In addition, the Zernike mode is incorporated to compensate for optical aberrations to enhance the robustness of FP reconstructions. As a demonstration, we show our method can reconstruct images with smooth performance and detailed information in both simulated and experimental datasets. In particular, when validated in an extension of a high-defocus, high-exposure tissue section dataset, PgNN outperforms traditional FP methods with fewer artifacts and distinguishable structures.



### Efficient Prealignment of CT Scans for Registration through a Bodypart Regressor
- **Arxiv ID**: http://arxiv.org/abs/1909.08898v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.08898v1)
- **Published**: 2019-09-19 10:02:58+00:00
- **Updated**: 2019-09-19 10:02:58+00:00
- **Authors**: Hans Meine, Alessa Hering
- **Comment**: Extended Abstract accepted at MIDL 2019
- **Journal**: None
- **Summary**: Convolutional neural networks have not only been applied for classification of voxels, objects, or images, for instance, but have also been proposed as a bodypart regressor. We pick up this underexplored idea and evaluate its value for registration: A CNN is trained to output the relative height within the human body in axial CT scans, and the resulting scores are used for quick alignment between different timepoints. Preliminary results confirm that this allows both fast and robust prealignment compared with iterative approaches.



### Synthetic CT Generation from MRI Using Improved DualGAN
- **Arxiv ID**: http://arxiv.org/abs/1909.08942v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.08942v1)
- **Published**: 2019-09-19 12:47:13+00:00
- **Updated**: 2019-09-19 12:47:13+00:00
- **Authors**: Denis Prokopenko, Joël Valentin Stadelmann, Heinrich Schulz, Steffen Renisch, Dmitry V. Dylov
- **Comment**: None
- **Journal**: None
- **Summary**: Synthetic CT image generation from MRI scan is necessary to create radiotherapy plans without the need of co-registered MRI and CT scans. The chosen baseline adversarial model with cycle consistency permits unpaired image-to-image translation. Perceptual loss function term and coordinate convolutional layer were added to improve the quality of translated images. The proposed architecture was tested on paired MRI-CT dataset, where the synthetic CTs were compared to corresponding original CT images. The MAE between the synthetic CT images and the real CT scans is 61 HU computed inside of the true CTs body shape.



### Count, Crop and Recognise: Fine-Grained Recognition in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1909.08950v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08950v2)
- **Published**: 2019-09-19 12:57:39+00:00
- **Updated**: 2019-10-09 16:33:01+00:00
- **Authors**: Max Bain, Arsha Nagrani, Daniel Schofield, Andrew Zisserman
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of this paper is to label all the animal individuals present in every frame of a video. Unlike previous methods that have principally concentrated on labelling face tracks, we aim to label individuals even when their faces are not visible. We make the following contributions: (i) we introduce a 'Count, Crop and Recognise' (CCR) multistage recognition process for frame level labelling. The Count and Recognise stages involve specialised CNNs for the task, and we show that this simple staging gives a substantial boost in performance; (ii) we compare the recall using frame based labelling to both face and body track based labelling, and demonstrate the advantage of frame based with CCR for the specified goal; (iii) we introduce a new dataset for chimpanzee recognition in the wild; and (iv) we apply a high-granularity visualisation technique to further understand the learned CNN features for the recognition of chimpanzee individuals.



### Road Damage Detection Acquisition System based on Deep Neural Networks for Physical Asset Management
- **Arxiv ID**: http://arxiv.org/abs/1909.08991v1
- **DOI**: 10.1007/978-3-030-33749-0_1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08991v1)
- **Published**: 2019-09-19 13:43:31+00:00
- **Updated**: 2019-09-19 13:43:31+00:00
- **Authors**: A. A. Angulo, J. A. Vega-Fernández, L. M. Aguilar-Lobo, S. Natraj, G Ochoa-Ruiz
- **Comment**: None
- **Journal**: None
- **Summary**: Research on damage detection of road surfaces has been an active area of re-search, but most studies have focused so far on the detection of the presence of damages. However, in real-world scenarios, road managers need to clearly understand the type of damage and its extent in order to take effective action in advance or to allocate the necessary resources. Moreover, currently there are few uniform and openly available road damage datasets, leading to a lack of a common benchmark for road damage detection. Such dataset could be used in a great variety of applications; herein, it is intended to serve as the acquisition component of a physical asset management tool which can aid governments agencies for planning purposes, or by infrastructure mainte-nance companies. In this paper, we make two contributions to address these issues. First, we present a large-scale road damage dataset, which includes a more balanced and representative set of damages. This dataset is composed of 18,034 road damage images captured with a smartphone, with 45,435 in-stances road surface damages. Second, we trained different types of object detection methods, both traditional (an LBP-cascaded classifier) and deep learning-based, specifically, MobileNet and RetinaNet, which are amenable for embedded and mobile and implementations with an acceptable perfor-mance for many applications. We compare the accuracy and inference time of all these models with others in the state of the art.



### Learning to Conceal: A Deep Learning Based Method for Preserving Privacy and Avoiding Prejudice
- **Arxiv ID**: http://arxiv.org/abs/1909.09156v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1909.09156v1)
- **Published**: 2019-09-19 13:54:18+00:00
- **Updated**: 2019-09-19 13:54:18+00:00
- **Authors**: Moshe Hanukoglu, Nissan Goldberg, Aviv Rovshitz, Amos Azaria
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce a learning model able to conceals personal information (e.g. gender, age, ethnicity, etc.) from an image, while maintaining any additional information present in the image (e.g. smile, hair-style, brightness). Our trained model is not provided the information that it is concealing, and does not try learning it either. Namely, we created a variational autoencoder (VAE) model that is trained on a dataset including labels of the information one would like to conceal (e.g. gender, ethnicity, age). These labels are directly added to the VAE's sampled latent vector. Due to the limited number of neurons in the latent vector and its appended noise, the VAE avoids learning any relation between the given images and the given labels, as those are given directly. Therefore, the encoded image lacks any of the information one wishes to conceal. The encoding may be decoded back into an image according to any provided properties (e.g. a 40 year old woman).   The proposed architecture can be used as a mean for privacy preserving and can serve as an input to systems, which will become unbiased and not suffer from prejudice. We believe that privacy and discrimination are two of the most important aspects in which the community should try and develop methods to prevent misuse of technological advances.



### APIR-Net: Autocalibrated Parallel Imaging Reconstruction using a Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1909.09006v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, eess.IV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1909.09006v1)
- **Published**: 2019-09-19 14:06:09+00:00
- **Updated**: 2019-09-19 14:06:09+00:00
- **Authors**: Chaoping Zhang, Florian Dubost, Marleen de Bruijne, Stefan Klein, Dirk H. J. Poot
- **Comment**: To appear in the proceedings of MICCAI 2019 Workshop Machine Learning
  for Medical Image Reconstruction
- **Journal**: None
- **Summary**: Deep learning has been successfully demonstrated in MRI reconstruction of accelerated acquisitions. However, its dependence on representative training data limits the application across different contrasts, anatomies, or image sizes. To address this limitation, we propose an unsupervised, auto-calibrated k-space completion method, based on a uniquely designed neural network that reconstructs the full k-space from an undersampled k-space, exploiting the redundancy among the multiple channels in the receive coil in a parallel imaging acquisition. To achieve this, contrary to common convolutional network approaches, the proposed network has a decreasing number of feature maps of constant size. In contrast to conventional parallel imaging methods such as GRAPPA that estimate the prediction kernel from the fully sampled autocalibration signals in a linear way, our method is able to learn nonlinear relations between sampled and unsampled positions in k-space. The proposed method was compared to the start-of-the-art ESPIRiT and RAKI methods in terms of noise amplification and visual image quality in both phantom and in-vivo experiments. The experiments indicate that APIR-Net provides a promising alternative to the conventional parallel imaging methods, and results in improved image quality especially for low SNR acquisitions.



### Training Robust Deep Neural Networks via Adversarial Noise Propagation
- **Arxiv ID**: http://arxiv.org/abs/1909.09034v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.09034v2)
- **Published**: 2019-09-19 15:08:07+00:00
- **Updated**: 2020-12-23 01:17:53+00:00
- **Authors**: Aishan Liu, Xianglong Liu, Chongzhi Zhang, Hang Yu, Qiang Liu, Dacheng Tao
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: In practice, deep neural networks have been found to be vulnerable to various types of noise, such as adversarial examples and corruption. Various adversarial defense methods have accordingly been developed to improve adversarial robustness for deep models. However, simply training on data mixed with adversarial examples, most of these models still fail to defend against the generalized types of noise. Motivated by the fact that hidden layers play a highly important role in maintaining a robust model, this paper proposes a simple yet powerful training algorithm, named \emph{Adversarial Noise Propagation} (ANP), which injects noise into the hidden layers in a layer-wise manner. ANP can be implemented efficiently by exploiting the nature of the backward-forward training style. Through thorough investigations, we determine that different hidden layers make different contributions to model robustness and clean accuracy, while shallow layers are comparatively more critical than deep layers. Moreover, our framework can be easily combined with other adversarial training methods to further improve model robustness by exploiting the potential of hidden layers. Extensive experiments on MNIST, CIFAR-10, CIFAR-10-C, CIFAR-10-P, and ImageNet demonstrate that ANP enables the strong robustness for deep models against both adversarial and corrupted ones, and also significantly outperforms various adversarial defense methods.



### Responsible Facial Recognition and Beyond
- **Arxiv ID**: http://arxiv.org/abs/1909.12935v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1909.12935v1)
- **Published**: 2019-09-19 15:27:06+00:00
- **Updated**: 2019-09-19 15:27:06+00:00
- **Authors**: Yi Zeng, Enmeng Lu, Yinqian Sun, Ruochen Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Facial recognition is changing the way we live in and interact with our society. Here we discuss the two sides of facial recognition, summarizing potential risks and current concerns. We introduce current policies and regulations in different countries. Very importantly, we point out that the risks and concerns are not only from facial recognition, but also realistically very similar to other biometric recognition technology, including but not limited to gait recognition, iris recognition, fingerprint recognition, voice recognition, etc. To create a responsible future, we discuss possible technological moves and efforts that should be made to keep facial recognition (and biometric recognition in general) developing for social good.



### Self-Supervised Monocular Depth Hints
- **Arxiv ID**: http://arxiv.org/abs/1909.09051v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09051v1)
- **Published**: 2019-09-19 15:41:07+00:00
- **Updated**: 2019-09-19 15:41:07+00:00
- **Authors**: Jamie Watson, Michael Firman, Gabriel J. Brostow, Daniyar Turmukhambetov
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: Monocular depth estimators can be trained with various forms of self-supervision from binocular-stereo data to circumvent the need for high-quality laser scans or other ground-truth data. The disadvantage, however, is that the photometric reprojection losses used with self-supervised learning typically have multiple local minima. These plausible-looking alternatives to ground truth can restrict what a regression network learns, causing it to predict depth maps of limited quality. As one prominent example, depth discontinuities around thin structures are often incorrectly estimated by current state-of-the-art methods.   Here, we study the problem of ambiguous reprojections in depth prediction from stereo-based self-supervision, and introduce Depth Hints to alleviate their effects. Depth Hints are complementary depth suggestions obtained from simple off-the-shelf stereo algorithms. These hints enhance an existing photometric loss function, and are used to guide a network to learn better weights. They require no additional data, and are assumed to be right only sometimes. We show that using our Depth Hints gives a substantial boost when training several leading self-supervised-from-stereo models, not just our own. Further, combined with other good practices, we produce state-of-the-art depth predictions on the KITTI benchmark.



### Learning to Think Outside the Box: Wide-Baseline Light Field Depth Estimation with EPI-Shift
- **Arxiv ID**: http://arxiv.org/abs/1909.09059v1
- **DOI**: 10.1109/3DV.2019.00036
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1909.09059v1)
- **Published**: 2019-09-19 15:57:17+00:00
- **Updated**: 2019-09-19 15:57:17+00:00
- **Authors**: Titus Leistner, Hendrik Schilling, Radek Mackowiak, Stefan Gumhold, Carsten Rother
- **Comment**: Published at International Conference on 3D Vision (3DV) 2019
- **Journal**: None
- **Summary**: We propose a method for depth estimation from light field data, based on a fully convolutional neural network architecture. Our goal is to design a pipeline which achieves highly accurate results for small- and wide-baseline light fields. Since light field training data is scarce, all learning-based approaches use a small receptive field and operate on small disparity ranges. In order to work with wide-baseline light fields, we introduce the idea of EPI-Shift: To virtually shift the light field stack which enables to retain a small receptive field, independent of the disparity range. In this way, our approach "learns to think outside the box of the receptive field". Our network performs joint classification of integer disparities and regression of disparity-offsets. A U-Net component provides excellent long-range smoothing. EPI-Shift considerably outperforms the state-of-the-art learning-based approaches and is on par with hand-crafted methods. We demonstrate this on a publicly available, synthetic, small-baseline benchmark and on large-baseline real-world recordings.



### Adaptively Aligned Image Captioning via Adaptive Attention Time
- **Arxiv ID**: http://arxiv.org/abs/1909.09060v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1909.09060v3)
- **Published**: 2019-09-19 15:59:33+00:00
- **Updated**: 2020-01-06 09:26:01+00:00
- **Authors**: Lun Huang, Wenmin Wang, Yaxian Xia, Jie Chen
- **Comment**: Accepted to NeurIPS 2019. Code is available at
  https://github.com/husthuaan/AAT
- **Journal**: None
- **Summary**: Recent neural models for image captioning usually employ an encoder-decoder framework with an attention mechanism. However, the attention mechanism in such a framework aligns one single (attended) image feature vector to one caption word, assuming one-to-one mapping from source image regions and target caption words, which is never possible. In this paper, we propose a novel attention model, namely Adaptive Attention Time (AAT), to align the source and the target adaptively for image captioning. AAT allows the framework to learn how many attention steps to take to output a caption word at each decoding step. With AAT, an image region can be mapped to an arbitrary number of caption words while a caption word can also attend to an arbitrary number of image regions. AAT is deterministic and differentiable, and doesn't introduce any noise to the parameter gradients. In this paper, we empirically show that AAT improves over state-of-the-art methods on the task of image captioning. Code is available at https://github.com/husthuaan/AAT.



### Look, Read and Enrich. Learning from Scientific Figures and their Captions
- **Arxiv ID**: http://arxiv.org/abs/1909.09070v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.09070v1)
- **Published**: 2019-09-19 16:10:15+00:00
- **Updated**: 2019-09-19 16:10:15+00:00
- **Authors**: Jose Manuel Gomez-Perez, Raul Ortega
- **Comment**: Accepted in the 10th International Conference on Knowledge capture
  (K-CAP 2019)
- **Journal**: None
- **Summary**: Compared to natural images, understanding scientific figures is particularly hard for machines. However, there is a valuable source of information in scientific literature that until now has remained untapped: the correspondence between a figure and its caption. In this paper we investigate what can be learnt by looking at a large number of figures and reading their captions, and introduce a figure-caption correspondence learning task that makes use of our observations. Training visual and language networks without supervision other than pairs of unconstrained figures and captions is shown to successfully solve this task. We also show that transferring lexical and semantic knowledge from a knowledge graph significantly enriches the resulting features. Finally, we demonstrate the positive impact of such features in other tasks involving scientific text and figures, like multi-modal classification and machine comprehension for question answering, outperforming supervised baselines and ad-hoc approaches.



### Vision-Based Proprioceptive Sensing for Soft Inflatable Actuators
- **Arxiv ID**: http://arxiv.org/abs/1909.09096v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.09096v1)
- **Published**: 2019-09-19 17:13:03+00:00
- **Updated**: 2019-09-19 17:13:03+00:00
- **Authors**: Peter Werner, Matthias Hofer, Carmelo Sferrazza, Raffaello D'Andrea
- **Comment**: This work has been submitted to the 2020 IEEE International
  Conference on Robotics and Automation (ICRA) for possible publication.
  Accompanying video: https://youtu.be/1MJuhxVcTns
- **Journal**: None
- **Summary**: This paper presents a vision-based sensing approach for a soft linear actuator, which is equipped with an integrated camera. The proposed vision-based sensing pipeline predicts the three-dimensional position of a point of interest on the actuator. To train and evaluate the algorithm, predictions are compared to ground truth data from an external motion capture system. An off-the-shelf distance sensor is integrated in a similar actuator and its performance is used as a baseline for comparison. The resulting sensing pipeline runs at 40 Hz in real-time on a standard laptop and is additionally used for closed loop elongation control of the actuator. It is shown that the approach can achieve comparable accuracy to the distance sensor.



### Self-Supervised Learning of Depth and Motion Under Photometric Inconsistency
- **Arxiv ID**: http://arxiv.org/abs/1909.09115v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09115v1)
- **Published**: 2019-09-19 17:41:28+00:00
- **Updated**: 2019-09-19 17:41:28+00:00
- **Authors**: Tianwei Shen, Lei Zhou, Zixin Luo, Yao Yao, Shiwei Li, Jiahui Zhang, Tian Fang, Long Quan
- **Comment**: International Conference on Computer Vision (ICCV) Workshop 2019
- **Journal**: None
- **Summary**: The self-supervised learning of depth and pose from monocular sequences provides an attractive solution by using the photometric consistency of nearby frames as it depends much less on the ground-truth data. In this paper, we address the issue when previous assumptions of the self-supervised approaches are violated due to the dynamic nature of real-world scenes. Different from handling the noise as uncertainty, our key idea is to incorporate more robust geometric quantities and enforce internal consistency in the temporal image sequence. As demonstrated on commonly used benchmark datasets, the proposed method substantially improves the state-of-the-art methods on both depth and relative pose estimation for monocular image sequences, without adding inference overhead.



### Prediction of overall survival and molecular markers in gliomas via analysis of digital pathology images using deep learning
- **Arxiv ID**: http://arxiv.org/abs/1909.09124v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.09124v1)
- **Published**: 2019-09-19 17:53:42+00:00
- **Updated**: 2019-09-19 17:53:42+00:00
- **Authors**: Saima Rathore, Muhammad Aksam Iftikhar, Zissimos Mourelatos
- **Comment**: 4 Figures
- **Journal**: None
- **Summary**: Cancer histology reveals disease progression and associated molecular processes, and contains rich phenotypic information that is predictive of outcome. In this paper, we developed a computational approach based on deep learning to predict the overall survival and molecular subtypes of glioma patients from microscopic images of tissue biopsies, reflecting measures of microvascular proliferation, mitotic activity, nuclear atypia, and the presence of necrosis. Whole-slide images from 663 unique patients [IDH: 333 IDH-wildtype, 330 IDH-mutants, 1p/19q: 201 1p/19q non-codeleted, 129 1p/19q codeleted] were obtained from TCGA. Sub-images that were free of artifacts and that contained viable tumor with descriptive histologic characteristics were extracted, which were further used for training and testing a deep neural network. The output layer of the network was configured in two different ways: (i) a final Cox model layer to output a prediction of patient risk, and (ii) a final layer with sigmoid activation function, and stochastic gradient decent based optimization with binary cross-entropy loss. Both survival prediction and molecular subtype classification produced promising results using our model. The c-statistic was estimated to be 0.82 (p-value=4.8x10-5) between the risk scores of the proposed deep learning model and overall survival, while accuracies of 88% (area under the curve [AUC]=0.86) were achieved in the detection of IDH mutational status and 1p/19q codeletion. These findings suggest that the deep learning techniques can be applied to microscopic images for objective, accurate, and integrated prediction of outcome for glioma patients. The proposed marker may contribute to (i) stratification of patients into clinical trials, (ii) patient selection for targeted therapy, and (iii) personalized treatment planning.



### Deeply Matting-based Dual Generative Adversarial Network for Image and Document Label Supervision
- **Arxiv ID**: http://arxiv.org/abs/1909.12909v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1909.12909v1)
- **Published**: 2019-09-19 18:15:10+00:00
- **Updated**: 2019-09-19 18:15:10+00:00
- **Authors**: Yubao Liu, Kai Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Although many methods have been proposed to deal with nature image super-resolution (SR) and get impressive performance, the text images SR is not good due to their ignorance of document images. In this paper, we propose a matting-based dual generative adversarial network (mdGAN) for document image SR. Firstly, the input image is decomposed into document text, foreground and background layers using deep image matting. Then two parallel branches are constructed to recover text boundary information and color information respectively. Furthermore, in order to improve the restoration accuracy of characters in output image, we use the input image's corresponding ground truth text label as extra supervise information to refine the two-branch networks during training. Experiments on real text images demonstrate that our method outperforms several state-of-the-art methods quantitatively and qualitatively.



### Learning Sparse Mixture of Experts for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1909.09192v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.09192v1)
- **Published**: 2019-09-19 18:55:54+00:00
- **Updated**: 2019-09-19 18:55:54+00:00
- **Authors**: Vardaan Pahuja, Jie Fu, Christopher J. Pal
- **Comment**: Accepted in Visual Question Answering and Dialog Workshop, CVPR 2019
- **Journal**: None
- **Summary**: There has been a rapid progress in the task of Visual Question Answering with improved model architectures. Unfortunately, these models are usually computationally intensive due to their sheer size which poses a serious challenge for deployment. We aim to tackle this issue for the specific task of Visual Question Answering (VQA). A Convolutional Neural Network (CNN) is an integral part of the visual processing pipeline of a VQA model (assuming the CNN is trained along with entire VQA model). In this project, we propose an efficient and modular neural architecture for the VQA task with focus on the CNN module. Our experiments demonstrate that a sparsely activated CNN based VQA model achieves comparable performance to a standard CNN based VQA model architecture.



### Gaze Estimation for Assisted Living Environments
- **Arxiv ID**: http://arxiv.org/abs/1909.09225v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.09225v1)
- **Published**: 2019-09-19 20:29:27+00:00
- **Updated**: 2019-09-19 20:29:27+00:00
- **Authors**: Philipe A. Dias, Damiano Malafronte, Henry Medeiros, Francesca Odone
- **Comment**: Work to be published in its final version at WACV '20
- **Journal**: None
- **Summary**: Effective assisted living environments must be able to perform inferences on how their occupants interact with one another as well as with surrounding objects. To accomplish this goal using a vision-based automated approach, multiple tasks such as pose estimation, object segmentation and gaze estimation must be addressed. Gaze direction in particular provides some of the strongest indications of how a person interacts with the environment. In this paper, we propose a simple neural network regressor that estimates the gaze direction of individuals in a multi-camera assisted living scenario, relying only on the relative positions of facial keypoints collected from a single pose estimation model. To handle cases of keypoint occlusion, our model exploits a novel confidence gated unit in its input layer. In addition to the gaze direction, our model also outputs an estimation of its own prediction uncertainty. Experimental results on a public benchmark demonstrate that our approach performs on pair with a complex, dataset-specific baseline, while its uncertainty predictions are highly correlated to the actual angular error of corresponding estimations. Finally, experiments on images from a real assisted living environment demonstrate the higher suitability of our model for its final application.



### Toward Robust Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1909.12927v1
- **DOI**: 10.1007/978-3-030-29513-4
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.12927v1)
- **Published**: 2019-09-19 21:08:13+00:00
- **Updated**: 2019-09-19 21:08:13+00:00
- **Authors**: Basemah Alshemali, Alta Graham, Jugal Kalita
- **Comment**: 2019 Intelligent Systems Conference, pp 483-489
- **Journal**: Intelligent Systems and Applications. IntelliSys 2019. Advances in
  Intelligent Systems and Computing, vol 1038. Springer, Cham
- **Summary**: Neural networks are frequently used for image classification, but can be vulnerable to misclassification caused by adversarial images. Attempts to make neural network image classification more robust have included variations on preprocessing (cropping, applying noise, blurring), adversarial training, and dropout randomization. In this paper, we implemented a model for adversarial detection based on a combination of two of these techniques: dropout randomization with preprocessing applied to images within a given Bayesian uncertainty. We evaluated our model on the MNIST dataset, using adversarial images generated using Fast Gradient Sign Method (FGSM), Jacobian-based Saliency Map Attack (JSMA) and Basic Iterative Method (BIM) attacks. Our model achieved an average adversarial image detection accuracy of 97%, with an average image classification accuracy, after discarding images flagged as adversarial, of 99%. Our average detection accuracy exceeded that of recent papers using similar techniques.



### HyperLearn: A Distributed Approach for Representation Learning in Datasets With Many Modalities
- **Arxiv ID**: http://arxiv.org/abs/1909.09252v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.09252v1)
- **Published**: 2019-09-19 22:45:21+00:00
- **Updated**: 2019-09-19 22:45:21+00:00
- **Authors**: Devanshu Arya, Stevan Rudinac, Marcel Worring
- **Comment**: None
- **Journal**: None
- **Summary**: Multimodal datasets contain an enormous amount of relational information, which grows exponentially with the introduction of new modalities. Learning representations in such a scenario is inherently complex due to the presence of multiple heterogeneous information channels. These channels can encode both (a) inter-relations between the items of different modalities and (b) intra-relations between the items of the same modality. Encoding multimedia items into a continuous low-dimensional semantic space such that both types of relations are captured and preserved is extremely challenging, especially if the goal is a unified end-to-end learning framework. The two key challenges that need to be addressed are: 1) the framework must be able to merge complex intra and inter relations without losing any valuable information and 2) the learning model should be invariant to the addition of new and potentially very different modalities. In this paper, we propose a flexible framework which can scale to data streams from many modalities. To that end we introduce a hypergraph-based model for data representation and deploy Graph Convolutional Networks to fuse relational information within and across modalities. Our approach provides an efficient solution for distributing otherwise extremely computationally expensive or even unfeasible training processes across multiple-GPUs, without any sacrifices in accuracy. Moreover, adding new modalities to our model requires only an additional GPU unit keeping the computational time unchanged, which brings representation learning to truly multimodal datasets. We demonstrate the feasibility of our approach in the experiments on multimedia datasets featuring second, third and fourth order relations.



### Triplet-Aware Scene Graph Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1909.09256v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09256v1)
- **Published**: 2019-09-19 23:20:49+00:00
- **Updated**: 2019-09-19 23:20:49+00:00
- **Authors**: Brigit Schroeder, Subarna Tripathi, Hanlin Tang
- **Comment**: Accepted to Scene Graph Representation Learning workshop at ICCV 2019
- **Journal**: None
- **Summary**: Scene graphs have become an important form of structured knowledge for tasks such as for image generation, visual relation detection, visual question answering, and image retrieval. While visualizing and interpreting word embeddings is well understood, scene graph embeddings have not been fully explored. In this work, we train scene graph embeddings in a layout generation task with different forms of supervision, specifically introducing triplet super-vision and data augmentation. We see a significant performance increase in both metrics that measure the goodness of layout prediction, mean intersection-over-union (mIoU)(52.3% vs. 49.2%) and relation score (61.7% vs. 54.1%),after the addition of triplet supervision and data augmentation. To understand how these different methods affect the scene graph representation, we apply several new visualization and evaluation methods to explore the evolution of the scene graph embedding. We find that triplet supervision significantly improves the embedding separability, which is highly correlated with the performance of the layout prediction model.



### Propagated Perturbation of Adversarial Attack for well-known CNNs: Empirical Study and its Explanation
- **Arxiv ID**: http://arxiv.org/abs/1909.09263v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.09263v2)
- **Published**: 2019-09-19 23:51:07+00:00
- **Updated**: 2019-09-23 07:18:24+00:00
- **Authors**: Jihyeun Yoon, Kyungyul Kim, Jongseong Jang
- **Comment**: None
- **Journal**: ICCV 2019 Workshop on Interpreting and Explaining Visual
  Artificial Intelligence Models
- **Summary**: Deep Neural Network based classifiers are known to be vulnerable to perturbations of inputs constructed by an adversarial attack to force misclassification. Most studies have focused on how to make vulnerable noise by gradient based attack methods or to defense model from adversarial attack. The use of the denoiser model is one of a well-known solution to reduce the adversarial noise although classification performance had not significantly improved. In this study, we aim to analyze the propagation of adversarial attack as an explainable AI(XAI) point of view. Specifically, we examine the trend of adversarial perturbations through the CNN architectures. To analyze the propagated perturbation, we measured normalized Euclidean Distance and cosine distance in each CNN layer between the feature map of the perturbed image passed through denoiser and the non-perturbed original image. We used five well-known CNN based classifiers and three gradient-based adversarial attacks. From the experimental results, we observed that in most cases, Euclidean Distance explosively increases in the final fully connected layer while cosine distance fluctuated and disappeared at the last layer. This means that the use of denoiser can decrease the amount of noise. However, it failed to defense accuracy degradation.



