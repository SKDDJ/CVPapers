# Arxiv Papers in cs.CV on 2019-09-06
### Coarse2Fine: A Two-stage Training Method for Fine-grained Visual Classification
- **Arxiv ID**: http://arxiv.org/abs/1909.02680v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.02680v1)
- **Published**: 2019-09-06 00:09:17+00:00
- **Updated**: 2019-09-06 00:09:17+00:00
- **Authors**: Amir Erfan Eshratifar, David Eigen, Michael Gormish, Massoud Pedram
- **Comment**: None
- **Journal**: None
- **Summary**: Small inter-class and large intra-class variations are the main challenges in fine-grained visual classification. Objects from different classes share visually similar structures and objects in the same class can have different poses and viewpoints. Therefore, the proper extraction of discriminative local features (e.g. bird's beak or car's headlight) is crucial. Most of the recent successes on this problem are based upon the attention models which can localize and attend the local discriminative objects parts. In this work, we propose a training method for visual attention networks, Coarse2Fine, which creates a differentiable path from the input space to the attended feature maps. Coarse2Fine learns an inverse mapping function from the attended feature maps to the informative regions in the raw image, which will guide the attention maps to better attend the fine-grained features. We show Coarse2Fine and orthogonal initialization of the attention weights can surpass the state-of-the-art accuracies on common fine-grained classification tasks.



### Efficient Automatic Meta Optimization Search for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.03817v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1909.03817v1)
- **Published**: 2019-09-06 02:48:52+00:00
- **Updated**: 2019-09-06 02:48:52+00:00
- **Authors**: Xinyue Zheng, Peng Wang, Qigang Wang, Zhongchao shi, Feiyu Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Previous works on meta-learning either relied on elaborately hand-designed network structures or adopted specialized learning rules to a particular domain. We propose a universal framework to optimize the meta-learning process automatically by adopting neural architecture search technique (NAS). NAS automatically generates and evaluates meta-learner's architecture for few-shot learning problems, while the meta-learner uses meta-learning algorithm to optimize its parameters based on the distribution of learning tasks. Parameter sharing and experience replay are adopted to accelerate the architectures searching process, so it takes only 1-2 GPU days to find good architectures. Extensive experiments on Mini-ImageNet and Omniglot show that our algorithm excels in few-shot learning tasks. The best architecture found on Mini-ImageNet achieves competitive results when transferred to Omniglot, which shows the high transferability of architectures among different computer vision problems.



### Visual Semantic Reasoning for Image-Text Matching
- **Arxiv ID**: http://arxiv.org/abs/1909.02701v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.02701v1)
- **Published**: 2019-09-06 03:23:01+00:00
- **Updated**: 2019-09-06 03:23:01+00:00
- **Authors**: Kunpeng Li, Yulun Zhang, Kai Li, Yuanyuan Li, Yun Fu
- **Comment**: Accepted to ICCV 2019 (Oral)
- **Journal**: None
- **Summary**: Image-text matching has been a hot research topic bridging the vision and language areas. It remains challenging because the current representation of image usually lacks global semantic concepts as in its corresponding text caption. To address this issue, we propose a simple and interpretable reasoning model to generate visual representation that captures key objects and semantic concepts of a scene. Specifically, we first build up connections between image regions and perform reasoning with Graph Convolutional Networks to generate features with semantic relationships. Then, we propose to use the gate and memory mechanism to perform global semantic reasoning on these relationship-enhanced features, select the discriminative information and gradually generate the representation for the whole scene. Experiments validate that our method achieves a new state-of-the-art for the image-text matching on MS-COCO and Flickr30K datasets. It outperforms the current best method by 6.8% relatively for image retrieval and 4.8% relatively for caption retrieval on MS-COCO (Recall@1 using 1K test set). On Flickr30K, our model improves image retrieval by 12.6% relatively and caption retrieval by 5.8% relatively (Recall@1). Our code is available at https://github.com/KunpengLi1994/VSRN.



### PCONV: The Missing but Desirable Sparsity in DNN Weight Pruning for Real-time Execution on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/1909.05073v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DC, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.05073v4)
- **Published**: 2019-09-06 03:58:29+00:00
- **Updated**: 2020-03-04 19:39:06+00:00
- **Authors**: Xiaolong Ma, Fu-Ming Guo, Wei Niu, Xue Lin, Jian Tang, Kaisheng Ma, Bin Ren, Yanzhi Wang
- **Comment**: To appear in Proceedings of the 34th AAAI Conference on Artificial
  Intelligence (AAAI-20)
- **Journal**: None
- **Summary**: Model compression techniques on Deep Neural Network (DNN) have been widely acknowledged as an effective way to achieve acceleration on a variety of platforms, and DNN weight pruning is a straightforward and effective method. There are currently two mainstreams of pruning methods representing two extremes of pruning regularity: non-structured, fine-grained pruning can achieve high sparsity and accuracy, but is not hardware friendly; structured, coarse-grained pruning exploits hardware-efficient structures in pruning, but suffers from accuracy drop when the pruning rate is high. In this paper, we introduce PCONV, comprising a new sparsity dimension, -- fine-grained pruning patterns inside the coarse-grained structures. PCONV comprises two types of sparsities, Sparse Convolution Patterns (SCP) which is generated from intra-convolution kernel pruning and connectivity sparsity generated from inter-convolution kernel pruning. Essentially, SCP enhances accuracy due to its special vision properties, and connectivity sparsity increases pruning rate while maintaining balanced workload on filter computation. To deploy PCONV, we develop a novel compiler-assisted DNN inference framework and execute PCONV models in real-time without accuracy compromise, which cannot be achieved in prior work. Our experimental results show that, PCONV outperforms three state-of-art end-to-end DNN frameworks, TensorFlow-Lite, TVM, and Alibaba Mobile Neural Network with speedup up to 39.2x, 11.4x, and 6.3x, respectively, with no accuracy loss. Mobile devices can achieve real-time inference on large-scale DNNs.



### Automatic Weight Estimation of Harvested Fish from Images
- **Arxiv ID**: http://arxiv.org/abs/1909.02710v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.02710v1)
- **Published**: 2019-09-06 04:51:40+00:00
- **Updated**: 2019-09-06 04:51:40+00:00
- **Authors**: Dmitry A. Konovalov, Alzayat Saleh, Dina B. Efremova, Jose A. Domingos, Dean R. Jerry
- **Comment**: Accepted for IEEE Digital Image Computing: Techniques and
  Applications, 2019 (DICTA 2019), 2-4 December 2019 in Perth, Australia,
  http://dicta2019.dictaconference.org/index.html
- **Journal**: None
- **Summary**: Approximately 2,500 weights and corresponding images of harvested Lates calcarifer (Asian seabass or barramundi) were collected at three different locations in Queensland, Australia. Two instances of the LinkNet-34 segmentation Convolutional Neural Network (CNN) were trained. The first one was trained on 200 manually segmented fish masks with excluded fins and tails. The second was trained on 100 whole-fish masks. The two CNNs were applied to the rest of the images and yielded automatically segmented masks. The one-factor and two-factor simple mathematical weight-from-area models were fitted on 1072 area-weight pairs from the first two locations, where area values were extracted from the automatically segmented masks. When applied to 1,400 test images (from the third location), the one-factor whole-fish mask model achieved the best mean absolute percentage error (MAPE), MAPE=4.36%. Direct weight-from-image regression CNNs were also trained, where the no-fins based CNN performed best on the test images with MAPE=4.28%.



### On-demand teleradiology using smartphone photographs as proxies for DICOM images
- **Arxiv ID**: http://arxiv.org/abs/1909.05669v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.05669v2)
- **Published**: 2019-09-06 06:09:36+00:00
- **Updated**: 2019-11-27 14:50:09+00:00
- **Authors**: Christine Podilchuk, Siddhartha Pachhai, Robert Warfsman, Richard Mammone
- **Comment**: 4 pages, 9 figures , IEEE SPMB19 conference submission
- **Journal**: None
- **Summary**: The use of photographs of the screen of displayed medical images is explored to circumvent the challenges involved in transferring images between sites. The photographs can be conveniently taken with a smartphone and analyzed remotely by either human or AI experts. An autoencoder preprocessor is shown to improve the performance for human experts. The AI performance provided by photographs is shown to be statistically equivalent to using the original DICOM images. The autoencoder preprocessor increases the PSNR by 15 dB or greater and provides an AUC that is statistically equivalent to using the original DICOM images. The photo approach is an alternative to IHE-based teleradiology applications while avoiding the problems inherit in navigating the proprietary and security barriers that limit DICOM communication between PACS in practice.



### A Baseline for Few-Shot Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1909.02729v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.02729v5)
- **Published**: 2019-09-06 06:14:03+00:00
- **Updated**: 2020-10-21 21:13:04+00:00
- **Authors**: Guneet S. Dhillon, Pratik Chaudhari, Avinash Ravichandran, Stefano Soatto
- **Comment**: None
- **Journal**: International Conference on Learning Representations (ICLR), 2020
- **Summary**: Fine-tuning a deep network trained with the standard cross-entropy loss is a strong baseline for few-shot learning. When fine-tuned transductively, this outperforms the current state-of-the-art on standard datasets such as Mini-ImageNet, Tiered-ImageNet, CIFAR-FS and FC-100 with the same hyper-parameters. The simplicity of this approach enables us to demonstrate the first few-shot learning results on the ImageNet-21k dataset. We find that using a large number of meta-training classes results in high few-shot accuracies even for a large number of few-shot classes. We do not advocate our approach as the solution for few-shot learning, but simply use the results to highlight limitations of current benchmarks and few-shot protocols. We perform extensive studies on benchmark datasets to propose a metric that quantifies the "hardness" of a few-shot episode. This metric can be used to report the performance of few-shot algorithms in a more systematic way.



### Invisible Backdoor Attacks on Deep Neural Networks via Steganography and Regularization
- **Arxiv ID**: http://arxiv.org/abs/1909.02742v3
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.02742v3)
- **Published**: 2019-09-06 07:11:26+00:00
- **Updated**: 2020-08-31 04:14:46+00:00
- **Authors**: Shaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, Xinpeng Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have been proven vulnerable to backdoor attacks, where hidden features (patterns) trained to a normal model, which is only activated by some specific input (called triggers), trick the model into producing unexpected behavior. In this paper, we create covert and scattered triggers for backdoor attacks, invisible backdoors, where triggers can fool both DNN models and human inspection. We apply our invisible backdoors through two state-of-the-art methods of embedding triggers for backdoor attacks. The first approach on Badnets embeds the trigger into DNNs through steganography. The second approach of a trojan attack uses two types of additional regularization terms to generate the triggers with irregular shape and size. We use the Attack Success Rate and Functionality to measure the performance of our attacks. We introduce two novel definitions of invisibility for human perception; one is conceptualized by the Perceptual Adversarial Similarity Score (PASS) and the other is Learned Perceptual Image Patch Similarity (LPIPS). We show that the proposed invisible backdoors can be fairly effective across various DNN models as well as four datasets MNIST, CIFAR-10, CIFAR-100, and GTSRB, by measuring their attack success rates for the adversary, functionality for the normal users, and invisibility scores for the administrators. We finally argue that the proposed invisible backdoor attacks can effectively thwart the state-of-the-art trojan backdoor detection approaches, such as Neural Cleanse and TABOR.



### Eelgrass beds and oyster farming at a lagoon before and after the Great East Japan Earthquake 2011: potential to apply deep learning at a coastal area
- **Arxiv ID**: http://arxiv.org/abs/1909.02747v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.02747v1)
- **Published**: 2019-09-06 07:34:58+00:00
- **Updated**: 2019-09-06 07:34:58+00:00
- **Authors**: Takehisa Yamakita
- **Comment**: None
- **Journal**: None
- **Summary**: There is a small number of case studies of automatic land cover classification on the coastal area. Here, I test extraction of seagrass beds, sandy area, oyster farming rafts at Mangoku-ura Lagoon, Miyagi, Japan by comparing manual tracing, simple image segmentation, and image transformation using deep learning. The result was used to extract the changes before and after the earthquake and tsunami. The output resolution was best in the image transformation method, which showed more than 69% accuracy for vegetation classification by an assessment using random points on independent test data. The distribution of oyster farming rafts was detected by the segmentation model. Assessment of the change before and after the earthquake by the manual tracing and image transformation result revealed increase of sand area and decrease of the vegetation. By the segmentation model only the decrease of the oyster farming was detected. These results demonstrate the potential to extract the spatial pattern of these elements after an earthquake and tsunami. Index Terms: Great East Japan Earthquake of 2011, Land use land cover (LULC), Zosteracea seagrass, cultured oyster, deep learning, Mangoku Bay



### Video Interpolation and Prediction with Unsupervised Landmarks
- **Arxiv ID**: http://arxiv.org/abs/1909.02749v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.02749v1)
- **Published**: 2019-09-06 07:40:27+00:00
- **Updated**: 2019-09-06 07:40:27+00:00
- **Authors**: Kevin J. Shih, Aysegul Dundar, Animesh Garg, Robert Pottorf, Andrew Tao, Bryan Catanzaro
- **Comment**: Technical Report
- **Journal**: None
- **Summary**: Prediction and interpolation for long-range video data involves the complex task of modeling motion trajectories for each visible object, occlusions and dis-occlusions, as well as appearance changes due to viewpoint and lighting. Optical flow based techniques generalize but are suitable only for short temporal ranges. Many methods opt to project the video frames to a low dimensional latent space, achieving long-range predictions. However, these latent representations are often non-interpretable, and therefore difficult to manipulate. This work poses video prediction and interpolation as unsupervised latent structure inference followed by a temporal prediction in this latent space. The latent representations capture foreground semantics without explicit supervision such as keypoints or poses. Further, as each landmark can be mapped to a coordinate indicating where a semantic part is positioned, we can reliably interpolate within the coordinate domain to achieve predictable motion interpolation. Given an image decoder capable of mapping these landmarks back to the image domain, we are able to achieve high-quality long-range video interpolation and extrapolation by operating on the landmark representation space.



### Recovery of Future Data via Convolution Nuclear Norm Minimization
- **Arxiv ID**: http://arxiv.org/abs/1909.03889v7
- **DOI**: 10.1109/TIT.2022.3196707
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1909.03889v7)
- **Published**: 2019-09-06 07:52:22+00:00
- **Updated**: 2022-08-03 13:26:16+00:00
- **Authors**: Guangcan Liu, Wayne Zhang
- **Comment**: None
- **Journal**: IEEE Transactions on Information Theory, 2022
- **Summary**: This paper studies the problem of time series forecasting (TSF) from the perspective of compressed sensing. First of all, we convert TSF into a more inclusive problem called tensor completion with arbitrary sampling (TCAS), which is to restore a tensor from a subset of its entries sampled in an arbitrary manner. While it is known that, in the framework of Tucker low-rankness, it is theoretically impossible to identify the target tensor based on some arbitrarily selected entries, in this work we shall show that TCAS is indeed tackleable in the light of a new concept called convolutional low-rankness, which is a generalization of the well-known Fourier sparsity. Then we introduce a convex program termed Convolution Nuclear Norm Minimization (CNNM), and we prove that CNNM succeeds in solving TCAS as long as a sampling condition--which depends on the convolution rank of the target tensor--is obeyed. This theory provides a meaningful answer to the fundamental question of what is the minimum sampling size needed for making a given number of forecasts. Experiments on univariate time series, images and videos show encouraging results.



### Image anomaly detection with capsule networks and imbalanced datasets
- **Arxiv ID**: http://arxiv.org/abs/1909.02755v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.02755v1)
- **Published**: 2019-09-06 08:07:56+00:00
- **Updated**: 2019-09-06 08:07:56+00:00
- **Authors**: Claudio Piciarelli, Pankaj Mishra, Gian Luca Foresti
- **Comment**: Published in conference ICIAP 2019
- **Journal**: [978-3-030-30641-0, ICIAP 2019, Part I, LNCS 11751, paper approval
  (489497_1_En, Chapter 23)]
- **Summary**: Image anomaly detection consists in finding images with anomalous, unusual patterns with respect to a set of normal data. Anomaly detection can be applied to several fields and has numerous practical applications, e.g. in industrial inspection, medical imaging, security enforcement, etc.. However, anomaly detection techniques often still rely on traditional approaches such as one-class Support Vector Machines, while the topic has not been fully developed yet in the context of modern deep learning approaches. In this paper, we propose an image anomaly detection system based on capsule networks under the assumption that anomalous data are available for training but their amount is scarce.



### A new operation mode for depth-focused high-sensitivity ToF range finding
- **Arxiv ID**: http://arxiv.org/abs/1909.02759v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.02759v1)
- **Published**: 2019-09-06 08:16:59+00:00
- **Updated**: 2019-09-06 08:16:59+00:00
- **Authors**: Sebastian Werner, Henrik Schäfer, Matthias Hullin
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce pulsed correlation time-of-flight (PC-ToF) sensing, a new operation mode for correlation time-of-flight range sensors that combines a sub-nanosecond laser pulse source with a rectangular demodulation at the sensor side. In contrast to previous work, our proposed measurement scheme attempts not to optimize depth accuracy over the full measurement: With PC-ToF we trade the global sensitivity of a standard C-ToF setup for measurements with strongly localized high sensitivity -- we greatly enhance the depth resolution for the acquisition of scene features around a desired depth of interest. Using real-world experiments, we show that our technique is capable of achieving depth resolutions down to 2mm using a modulation frequency as low as 10MHz and an optical power as low as 1mW. This makes PC-ToF especially viable for low-power applications.



### ILP-M Conv: Optimize Convolution Algorithm for Single-Image Convolution Neural Network Inference on Mobile GPUs
- **Arxiv ID**: http://arxiv.org/abs/1909.02765v2
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/1909.02765v2)
- **Published**: 2019-09-06 08:36:05+00:00
- **Updated**: 2019-10-03 06:04:24+00:00
- **Authors**: Zhuoran Ji
- **Comment**: None
- **Journal**: None
- **Summary**: Convolution neural networks are widely used for mobile applications. However, GPU convolution algorithms are designed for mini-batch neural network training, the single-image convolution neural network inference algorithm on mobile GPUs is not well-studied. After discussing the usage difference and examining the existing convolution algorithms, we proposed the HNTMP convolution algorithm. The HNTMP convolution algorithm achieves $14.6 \times$ speedup than the most popular \textit{im2col} convolution algorithm, and $2.30 \times$ speedup than the fastest existing convolution algorithm (direct convolution) as far as we know.



### Deep Learning for Brain Tumor Segmentation in Radiosurgery: Prospective Clinical Evaluation
- **Arxiv ID**: http://arxiv.org/abs/1909.02799v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1909.02799v3)
- **Published**: 2019-09-06 10:05:24+00:00
- **Updated**: 2019-12-18 08:35:00+00:00
- **Authors**: Boris Shirokikh, Alexandra Dalechina, Alexey Shevtsov, Egor Krivov, Valery Kostjuchenko, Amayak Durgaryan, Mikhail Galkin, Ivan Osinov, Andrey Golanov, Mikhail Belyaev
- **Comment**: None
- **Journal**: None
- **Summary**: Stereotactic radiosurgery is a minimally-invasive treatment option for a large number of patients with intracranial tumors. As part of the therapy treatment, accurate delineation of brain tumors is of great importance. However, slice-by-slice manual segmentation on T1c MRI could be time-consuming (especially for multiple metastases) and subjective. In our work, we compared several deep convolutional networks architectures and training procedures and evaluated the best model in a radiation therapy department for three types of brain tumors: meningiomas, schwannomas and multiple brain metastases. The developed semiautomatic segmentation system accelerates the contouring process by 2.2 times on average and increases inter-rater agreement from 92.0% to 96.5%.



### Bayes-Factor-VAE: Hierarchical Bayesian Deep Auto-Encoder Models for Factor Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/1909.02820v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.02820v1)
- **Published**: 2019-09-06 11:20:42+00:00
- **Updated**: 2019-09-06 11:20:42+00:00
- **Authors**: Minyoung Kim, Yuting Wang, Pritish Sahu, Vladimir Pavlovic
- **Comment**: International Conference on Computer Vision (ICCV) 2019
- **Journal**: None
- **Summary**: We propose a family of novel hierarchical Bayesian deep auto-encoder models capable of identifying disentangled factors of variability in data. While many recent attempts at factor disentanglement have focused on sophisticated learning objectives within the VAE framework, their choice of a standard normal as the latent factor prior is both suboptimal and detrimental to performance. Our key observation is that the disentangled latent variables responsible for major sources of variability, the relevant factors, can be more appropriately modeled using long-tail distributions. The typical Gaussian priors are, on the other hand, better suited for modeling of nuisance factors. Motivated by this, we extend the VAE to a hierarchical Bayesian model by introducing hyper-priors on the variances of Gaussian latent priors, mimicking an infinite mixture, while maintaining tractable learning and inference of the traditional VAEs. This analysis signifies the importance of partitioning and treating in a different manner the latent dimensions corresponding to relevant factors and nuisances. Our proposed models, dubbed Bayes-Factor-VAEs, are shown to outperform existing methods both quantitatively and qualitatively in terms of latent disentanglement across several challenging benchmark tasks.



### Deep CNN frameworks comparison for malaria diagnosis
- **Arxiv ID**: http://arxiv.org/abs/1909.02829v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.02829v1)
- **Published**: 2019-09-06 11:46:04+00:00
- **Updated**: 2019-09-06 11:46:04+00:00
- **Authors**: Priyadarshini Adyasha Pattanaik, Zelong Wang, Patrick Horain
- **Comment**: None
- **Journal**: IMVIP 2019 Irish Machine Vision and Image Processing Conference,
  Sep 2019, Dublin, Ireland
- **Summary**: We compare Deep Convolutional Neural Networks (DCNN) frameworks, namely AlexNet and VGGNet, for the classification of healthy and malaria-infected cells in large, grayscale, low quality and low resolution microscopic images, in the case only a small training set is available. Experimental results deliver promising results on the path to quick, automatic and precise classification in unstained images.



### Running Event Visualization using Videos from Multiple Cameras
- **Arxiv ID**: http://arxiv.org/abs/1909.02835v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.02835v1)
- **Published**: 2019-09-06 11:51:59+00:00
- **Updated**: 2019-09-06 11:51:59+00:00
- **Authors**: Yeshwanth Napolean, Priadi Teguh Wibowo, Jan van Gemert
- **Comment**: None
- **Journal**: None
- **Summary**: Visualizing the trajectory of multiple runners with videos collected at different points in a race could be useful for sports performance analysis. The videos and the trajectories can also aid in athlete health monitoring. While the runners unique ID and their appearance are distinct, the task is not straightforward because the video data does not contain explicit information as to which runners appear in each of the videos. There is no direct supervision of the model in tracking athletes, only filtering steps to remove irrelevant detections. Other factors of concern include occlusion of runners and harsh illumination. To this end, we identify two methods for runner identification at different points of the event, for determining their trajectory. One is scene text detection which recognizes the runners by detecting a unique 'bib number' attached to their clothes and the other is person re-identification which detects the runners based on their appearance. We train our method without ground truth but to evaluate the proposed methods, we create a ground truth database which consists of video and frame interval information where the runners appear. The videos in the dataset was recorded by nine cameras at different locations during the a marathon event. This data is annotated with bib numbers of runners appearing in each video. The bib numbers of runners known to occur in the frame are used to filter irrelevant text and numbers detected. Except for this filtering step, no supervisory signal is used. The experimental evidence shows that the scene text recognition method achieves an F1-score of 74. Combining the two methods, that is - using samples collected by text spotter to train the re-identification model yields a higher F1-score of 85.8. Re-training the person re-identification model with identified inliers yields a slight improvement in performance(F1 score of 87.8).



### Linear Context Transform Block
- **Arxiv ID**: http://arxiv.org/abs/1909.03834v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.03834v2)
- **Published**: 2019-09-06 12:31:28+00:00
- **Updated**: 2019-11-23 10:57:33+00:00
- **Authors**: Dongsheng Ruan, Jun Wen, Nenggan Zheng, Min Zheng
- **Comment**: AAAI-2020 accepted
- **Journal**: None
- **Summary**: Squeeze-and-Excitation (SE) block presents a channel attention mechanism for modeling global context via explicitly capturing dependencies across channels. However, we are still far from understanding how the SE block works. In this work, we first revisit the SE block, and then present a detailed empirical study of the relationship between global context and attention distribution, based on which we propose a simple yet effective module, called Linear Context Transform (LCT) block. We divide all channels into different groups and normalize the globally aggregated context features within each channel group, reducing the disturbance from irrelevant channels. Through linear transform of the normalized context features, we model global context for each channel independently. The LCT block is extremely lightweight and easy to be plugged into different backbone models while with negligible parameters and computational burden increase. Extensive experiments show that the LCT block outperforms the SE block in image classification task on the ImageNet and object detection/segmentation on the COCO dataset with different backbone models. Moreover, LCT yields consistent performance gains over existing state-of-the-art detection architectures, e.g., 1.5$\sim$1.7% AP$^{bbox}$ and 1.0$\sim$1.2% AP$^{mask}$ improvements on the COCO benchmark, irrespective of different baseline models of varied capacities. We hope our simple yet effective approach will shed some light on future research of attention-based models.



### Geolocation of an aircraft using image registration coupling modes for autonomous navigation
- **Arxiv ID**: http://arxiv.org/abs/1909.02875v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.02875v1)
- **Published**: 2019-09-06 12:53:00+00:00
- **Updated**: 2019-09-06 12:53:00+00:00
- **Authors**: Nima Ziaei
- **Comment**: 14 pages, 15 figures
- **Journal**: None
- **Summary**: This paper proposes to study an alternative technology to the GPS system on fixed wing aircraft using the aerial shots of landscapes from a ventral monocular camera integrated into the aircraft and based on the technology of image registration for aircraft geolocation purpose. Different types of use of the image registration technology exist: the relative registration and the absolute registration. The relative one is able to readjust position of the aircraft from two successive aerial shots by knowing the aircraft s position of image 1 and the overlap between the two images. The absolute registration compare a real time aerial shot with pre-referenced images stored in a database and permit the geolocation of the aircraft in comparing aerial shot with images of the database. Each kind of image registration technology has its own flaw preventing it to be used alone for aircraft geolocation. This study proposes to evaluate, according to different physical parameters ( aircraft speed, flight altitude, density of image points of interest), the coupling of these different types of image registration. Finally, this study also aims to quantify some image registration performances, particularly its execution time or its drift.



### Testing Deep Learning Models for Image Analysis Using Object-Relevant Metamorphic Relations
- **Arxiv ID**: http://arxiv.org/abs/1909.03824v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.03824v2)
- **Published**: 2019-09-06 13:31:15+00:00
- **Updated**: 2021-05-24 07:10:53+00:00
- **Authors**: Yongqiang Tian, Shiqing Ma, Ming Wen, Yepang Liu, Shing-Chi Cheung, Xiangyu Zhang
- **Comment**: Please note that a later version of this paper is accepted by
  Empirical Software Engineering in 2021. The title of the accepted paper is:
  "To What Extent Do DNN-based Image Classification Models Make Unreliable
  Inferences?". Please contact the first author if you are interested in the
  accepted version
- **Journal**: None
- **Summary**: Deep learning models are widely used for image analysis. While they offer high performance in terms of accuracy, people are concerned about if these models inappropriately make inferences using irrelevant features that are not encoded from the target object in a given image. To address the concern, we propose a metamorphic testing approach that assesses if a given inference is made based on irrelevant features. Specifically, we propose two novel metamorphic relations to detect such inappropriate inferences. We applied our approach to 10 image classification models and 10 object detection models, with three large datasets, i.e., ImageNet, COCO, and Pascal VOC. Over 5.3% of the top-5 correct predictions made by the image classification models are subject to inappropriate inferences using irrelevant features. The corresponding rate for the object detection models is over 8.5%. Based on the findings, we further designed a new image generation strategy that can effectively attack existing models. Comparing with a baseline approach, our strategy can double the success rate of attacks.



### DublinCity: Annotated LiDAR Point Cloud and its Applications
- **Arxiv ID**: http://arxiv.org/abs/1909.03613v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CG, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.03613v1)
- **Published**: 2019-09-06 13:47:31+00:00
- **Updated**: 2019-09-06 13:47:31+00:00
- **Authors**: S. M. Iman Zolanvari, Susana Ruano, Aakanksha Rana, Alan Cummins, Rogerio Eduardo da Silva, Morteza Rahbar, Aljosa Smolic
- **Comment**: Accepted to the 30th British Machine Vision Conference
- **Journal**: None
- **Summary**: Scene understanding of full-scale 3D models of an urban area remains a challenging task. While advanced computer vision techniques offer cost-effective approaches to analyse 3D urban elements, a precise and densely labelled dataset is quintessential. The paper presents the first-ever labelled dataset for a highly dense Aerial Laser Scanning (ALS) point cloud at city-scale. This work introduces a novel benchmark dataset that includes a manually annotated point cloud for over 260 million laser scanning points into 100'000 (approx.) assets from Dublin LiDAR point cloud [12] in 2015. Objects are labelled into 13 classes using hierarchical levels of detail from large (i.e., building, vegetation and ground) to refined (i.e., window, door and tree) elements. To validate the performance of our dataset, two different applications are showcased. Firstly, the labelled point cloud is employed for training Convolutional Neural Networks (CNNs) to classify urban elements. The dataset is tested on the well-known state-of-the-art CNNs (i.e., PointNet, PointNet++ and So-Net). Secondly, the complete ALS dataset is applied as detailed ground truth for city-scale image-based 3D reconstruction.



### Blackbox Attacks on Reinforcement Learning Agents Using Approximated Temporal Information
- **Arxiv ID**: http://arxiv.org/abs/1909.02918v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.02918v2)
- **Published**: 2019-09-06 14:06:21+00:00
- **Updated**: 2019-11-21 19:07:45+00:00
- **Authors**: Yiren Zhao, Ilia Shumailov, Han Cui, Xitong Gao, Robert Mullins, Ross Anderson
- **Comment**: None
- **Journal**: None
- **Summary**: Recent research on reinforcement learning (RL) has suggested that trained agents are vulnerable to maliciously crafted adversarial samples. In this work, we show how such samples can be generalised from White-box and Grey-box attacks to a strong Black-box case, where the attacker has no knowledge of the agents, their training parameters and their training methods. We use sequence-to-sequence models to predict a single action or a sequence of future actions that a trained agent will make. First, we show our approximation model, based on time-series information from the agent, consistently predicts RL agents' future actions with high accuracy in a Black-box setup on a wide range of games and RL algorithms. Second, we find that although adversarial samples are transferable from the target model to our RL agents, they often outperform random Gaussian noise only marginally. This highlights a serious methodological deficiency in previous work on such agents; random jamming should have been taken as the baseline for evaluation. Third, we propose a novel use for adversarial samplesin Black-box attacks of RL agents: they can be used to trigger a trained agent to misbehave after a specific time delay. This appears to be a genuinely new type of attack. It potentially enables an attacker to use devices controlled by RL agents as time bombs.



### Astroalign: A Python module for astronomical image registration
- **Arxiv ID**: http://arxiv.org/abs/1909.02946v2
- **DOI**: None
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.02946v2)
- **Published**: 2019-09-06 14:56:51+00:00
- **Updated**: 2020-05-22 17:20:01+00:00
- **Authors**: Martin Beroiz, Juan B. Cabral, Bruno Sanchez
- **Comment**: 4 pages, 2 figures, Python package
- **Journal**: None
- **Summary**: We present an algorithm implemented in the astroalign Python module for image registration in astronomy. Our module does not rely on WCS information and instead matches 3-point asterisms (triangles) on the images to find the most accurate linear transformation between the two. It is especially useful in the context of aligning images prior to stacking or performing difference image analysis. Astroalign can match images of different point-spread functions, seeing, and atmospheric conditions.



### Supervised Multimodal Bitransformers for Classifying Images and Text
- **Arxiv ID**: http://arxiv.org/abs/1909.02950v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.02950v2)
- **Published**: 2019-09-06 14:59:18+00:00
- **Updated**: 2020-11-12 03:08:28+00:00
- **Authors**: Douwe Kiela, Suvrat Bhooshan, Hamed Firooz, Ethan Perez, Davide Testuggine
- **Comment**: Rejected from EMNLP, twice
- **Journal**: None
- **Summary**: Self-supervised bidirectional transformer models such as BERT have led to dramatic improvements in a wide variety of textual classification tasks. The modern digital world is increasingly multimodal, however, and textual information is often accompanied by other modalities such as images. We introduce a supervised multimodal bitransformer model that fuses information from text and image encoders, and obtain state-of-the-art performance on various multimodal classification benchmark tasks, outperforming strong baselines, including on hard test sets specifically designed to measure multimodal performance.



### Unsupervised Clustering of Quantitative Imaging Phenotypes using Autoencoder and Gaussian Mixture Model
- **Arxiv ID**: http://arxiv.org/abs/1909.02953v1
- **DOI**: 10.1007/978-3-030-32251-9_63
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1909.02953v1)
- **Published**: 2019-09-06 15:00:24+00:00
- **Updated**: 2019-09-06 15:00:24+00:00
- **Authors**: Jianan Chen, Laurent Milot, Helen M. C. Cheung, Anne L. Martel
- **Comment**: Accepted at MICCAI 2019
- **Journal**: None
- **Summary**: Quantitative medical image computing (radiomics) has been widely applied to build prediction models from medical images. However, overfitting is a significant issue in conventional radiomics, where a large number of radiomic features are directly used to train and test models that predict genotypes or clinical outcomes. In order to tackle this problem, we propose an unsupervised learning pipeline composed of an autoencoder for representation learning of radiomic features and a Gaussian mixture model based on minimum message length criterion for clustering. By incorporating probabilistic modeling, disease heterogeneity has been taken into account. The performance of the proposed pipeline was evaluated on an institutional MRI cohort of 108 patients with colorectal cancer liver metastases. Our approach is capable of automatically selecting the optimal number of clusters and assigns patients into clusters (imaging subtypes) with significantly different survival rates. Our method outperforms other unsupervised clustering methods that have been used for radiomics analysis and has comparable performance to a state-of-the-art imaging biomarker.



### Discriminative and Robust Online Learning for Siamese Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1909.02959v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.02959v2)
- **Published**: 2019-09-06 15:06:26+00:00
- **Updated**: 2019-11-21 14:31:21+00:00
- **Authors**: Jinghao Zhou, Peng Wang, Haoyang Sun
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of visual object tracking has traditionally been handled by variant tracking paradigms, either learning a model of the object's appearance exclusively online or matching the object with the target in an offline-trained embedding space. Despite the recent success, each method agonizes over its intrinsic constraint. The online-only approaches suffer from a lack of generalization of the model they learn thus are inferior in target regression, while the offline-only approaches (e.g., convolutional siamese trackers) lack the target-specific context information thus are not discriminative enough to handle distractors, and robust enough to deformation. Therefore, we propose an online module with an attention mechanism for offline siamese networks to extract target-specific features under L2 error. We further propose a filter update strategy adaptive to treacherous background noises for discriminative learning, and a template update strategy to handle large target deformations for robust learning. Effectiveness can be validated in the consistent improvement over three siamese baselines: SiamFC, SiamRPN++, and SiamMask. Beyond that, our model based on SiamRPN++ obtains the best results over six popular tracking benchmarks and can operate beyond real-time.



### Explicit Facial Expression Transfer via Fine-Grained Representations
- **Arxiv ID**: http://arxiv.org/abs/1909.02967v2
- **DOI**: 10.1109/TIP.2021.3073857
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.02967v2)
- **Published**: 2019-09-06 15:24:28+00:00
- **Updated**: 2021-04-15 06:53:14+00:00
- **Authors**: Zhiwen Shao, Hengliang Zhu, Junshu Tang, Xuequan Lu, Lizhuang Ma
- **Comment**: This paper has been accepted by IEEE Transactions on Image Processing
  (TIP)
- **Journal**: None
- **Summary**: Facial expression transfer between two unpaired images is a challenging problem, as fine-grained expression is typically tangled with other facial attributes. Most existing methods treat expression transfer as an application of expression manipulation, and use predicted global expression, landmarks or action units (AUs) as a guidance. However, the prediction may be inaccurate, which limits the performance of transferring fine-grained expression. Instead of using an intermediate estimated guidance, we propose to explicitly transfer facial expression by directly mapping two unpaired input images to two synthesized images with swapped expressions. Specifically, considering AUs semantically describe fine-grained expression details, we propose a novel multi-class adversarial training method to disentangle input images into two types of fine-grained representations: AU-related feature and AU-free feature. Then, we can synthesize new images with preserved identities and swapped expressions by combining AU-free features with swapped AU-related features. Moreover, to obtain reliable expression transfer results of the unpaired input, we introduce a swap consistency loss to make the synthesized images and self-reconstructed images indistinguishable. Extensive experiments show that our approach outperforms the state-of-the-art expression manipulation methods for transferring fine-grained expressions while preserving other attributes including identity and pose.



### Video Surveillance of Highway Traffic Events by Deep Learning Architectures
- **Arxiv ID**: http://arxiv.org/abs/1909.12235v1
- **DOI**: 10.1007/978-3-030-01424-7_57
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.12235v1)
- **Published**: 2019-09-06 15:36:02+00:00
- **Updated**: 2019-09-06 15:36:02+00:00
- **Authors**: Matteo Tiezzi, Stefano Melacci, Marco Maggini, Angelo Frosini
- **Comment**: None
- **Journal**: Lecture Notes in Computer Science, vol 11141, (2018) pp 584-593
- **Summary**: In this paper we describe a video surveillance system able to detect traffic events in videos acquired by fixed videocameras on highways. The events of interest consist in a specific sequence of situations that occur in the video, as for instance a vehicle stopping on the emergency lane. Hence, the detection of these events requires to analyze a temporal sequence in the video stream. We compare different approaches that exploit architectures based on Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs). A first approach extracts vectors of features, mostly related to motion, from each video frame and exploits a RNN fed with the resulting sequence of vectors. The other approaches are based directly on the sequence of frames, that are eventually enriched with pixel-wise motion information. The obtained stream is processed by an architecture that stacks a CNN and a RNN, and we also investigate a transfer-learning-based model. The results are very promising and the best architecture will be tested online in real operative conditions.



### One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques
- **Arxiv ID**: http://arxiv.org/abs/1909.03012v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.HC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.03012v2)
- **Published**: 2019-09-06 16:53:01+00:00
- **Updated**: 2019-09-14 15:08:57+00:00
- **Authors**: Vijay Arya, Rachel K. E. Bellamy, Pin-Yu Chen, Amit Dhurandhar, Michael Hind, Samuel C. Hoffman, Stephanie Houde, Q. Vera Liao, Ronny Luss, Aleksandra Mojsilović, Sami Mourad, Pablo Pedemonte, Ramya Raghavendra, John Richards, Prasanna Sattigeri, Karthikeyan Shanmugam, Moninder Singh, Kush R. Varshney, Dennis Wei, Yunfeng Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: As artificial intelligence and machine learning algorithms make further inroads into society, calls are increasing from multiple stakeholders for these algorithms to explain their outputs. At the same time, these stakeholders, whether they be affected citizens, government regulators, domain experts, or system developers, present different requirements for explanations. Toward addressing these needs, we introduce AI Explainability 360 (http://aix360.mybluemix.net/), an open-source software toolkit featuring eight diverse and state-of-the-art explainability methods and two evaluation metrics. Equally important, we provide a taxonomy to help entities requiring explanations to navigate the space of explanation methods, not only those in the toolkit but also in the broader literature on explainability. For data scientists and other users of the toolkit, we have implemented an extensible software architecture that organizes methods according to their place in the AI modeling pipeline. We also discuss enhancements to bring research innovations closer to consumers of explanations, ranging from simplified, more accessible versions of algorithms, to tutorials and an interactive web demo to introduce AI explainability to different audiences and application domains. Together, our toolkit and taxonomy can help identify gaps where more explainability methods are needed and provide a platform to incorporate them as they are developed.



### Self-supervised Dense 3D Reconstruction from Monocular Endoscopic Video
- **Arxiv ID**: http://arxiv.org/abs/1909.03101v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.03101v1)
- **Published**: 2019-09-06 19:41:05+00:00
- **Updated**: 2019-09-06 19:41:05+00:00
- **Authors**: Xingtong Liu, Ayushi Sinha, Masaru Ishii, Gregory D. Hager, Russell H. Taylor, Mathias Unberath
- **Comment**: None
- **Journal**: None
- **Summary**: We present a self-supervised learning-based pipeline for dense 3D reconstruction from full-length monocular endoscopic videos without a priori modeling of anatomy or shading. Our method only relies on unlabeled monocular endoscopic videos and conventional multi-view stereo algorithms, and requires neither manual interaction nor patient CT in both training and application phases. In a cross-patient study using CT scans as groundtruth, we show that our method is able to produce photo-realistic dense 3D reconstructions with submillimeter mean residual errors from endoscopic videos from unseen patients and scopes.



### High Resolution Medical Image Analysis with Spatial Partitioning
- **Arxiv ID**: http://arxiv.org/abs/1909.03108v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.03108v3)
- **Published**: 2019-09-06 19:58:11+00:00
- **Updated**: 2019-09-12 18:53:36+00:00
- **Authors**: Le Hou, Youlong Cheng, Noam Shazeer, Niki Parmar, Yeqing Li, Panagiotis Korfiatis, Travis M. Drucker, Daniel J. Blezek, Xiaodan Song
- **Comment**: None
- **Journal**: None
- **Summary**: Medical images such as 3D computerized tomography (CT) scans and pathology images, have hundreds of millions or billions of voxels/pixels. It is infeasible to train CNN models directly on such high resolution images, because neural activations of a single image do not fit in the memory of a single GPU/TPU, and naive data and model parallelism approaches do not work. Existing image analysis approaches alleviate this problem by cropping or down-sampling input images, which leads to complicated implementation and sub-optimal performance due to information loss. In this paper, we implement spatial partitioning, which internally distributes the input and output of convolutional layers across GPUs/TPUs. Our implementation is based on the Mesh-TensorFlow framework and the computation distribution is transparent to end users. With this technique, we train a 3D Unet on up to 512 by 512 by 512 resolution data. To the best of our knowledge, this is the first work for handling such high resolution images end-to-end.



### DeepInSAR: A Deep Learning Framework for SAR Interferometric Phase Restoration and Coherence Estimation
- **Arxiv ID**: http://arxiv.org/abs/1909.03120v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.03120v2)
- **Published**: 2019-09-06 20:38:04+00:00
- **Updated**: 2020-05-27 05:18:43+00:00
- **Authors**: Xinyao Sun, Aaron Zimmer, Subhayan Mukherjee, Navaneeth Kamballur Kottayil, Parwant Ghuman, Irene Cheng
- **Comment**: 19 pages
- **Journal**: None
- **Summary**: Over the past decade, Interferometric Synthetic Aperture Radar (InSAR) has become a successful remote sensing technique. However, during the acquisition step, microwave reflections received at satellite are usually disturbed by strong noise, leading to a noisy single-look complex (SLC) SAR image. The quality of their interferometric phase is even worse. InSAR phase filtering is an ill-posed problem and plays a key role in subsequent processing. However, most of existing methods usually require expert supervision or heavy runtime, which limits the usability and scalability for practical usages such as wide-area monitoring and forecasting. In this work, we propose a deep convolutional neural network (CNN) based model DeepInSAR to intelligently solve both the phase filtering and coherence estimation problems. We demonstrate our DeepInSAR using both simulated and real data. A teacher-student framework is proposed to deal with the issue that there is no ground truth sample for real-world InSAR data. Quantitative and qualitative comparisons show that DeepInSAR achieves comparable or even better results than its stacked-based teacher method on new test datasets but requiring fewer pairs of SLCs as well as outperforms three other established non-stack based methods with less running time and no human supervision.



### Geometry-Aware Video Object Detection for Static Cameras
- **Arxiv ID**: http://arxiv.org/abs/1909.03140v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.03140v1)
- **Published**: 2019-09-06 22:34:45+00:00
- **Updated**: 2019-09-06 22:34:45+00:00
- **Authors**: Dan Xu, Weidi Xie, Andrew Zisserman
- **Comment**: Accepted at BMVC 2019 as ORAL
- **Journal**: None
- **Summary**: In this paper we propose a geometry-aware model for video object detection. Specifically, we consider the setting that cameras can be well approximated as static, e.g. in video surveillance scenarios, and scene pseudo depth maps can therefore be inferred easily from the object scale on the image plane. We make the following contributions: First, we extend the recent anchor-free detector (CornerNet [17]) to video object detections. In order to exploit the spatial-temporal information while maintaining high efficiency, the proposed model accepts video clips as input, and only makes predictions for the starting and the ending frames, i.e. heatmaps of object bounding box corners and the corresponding embeddings for grouping. Second, to tackle the challenge from scale variations in object detection, scene geometry information, e.g. derived depth maps, is explicitly incorporated into deep networks for multi-scale feature selection and for the network prediction. Third, we validate the proposed architectures on an autonomous driving dataset generated from the Carla simulator [5], and on a real dataset for human detection (DukeMTMC dataset [28]). When comparing with the existing competitive single-stage or two-stage detectors, the proposed geometry-aware spatio-temporal network achieves significantly better results.



