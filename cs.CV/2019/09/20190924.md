# Arxiv Papers in cs.CV on 2019-09-24
### Relational Learning for Joint Head and Human Detection
- **Arxiv ID**: http://arxiv.org/abs/1909.10674v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10674v1)
- **Published**: 2019-09-24 01:40:50+00:00
- **Updated**: 2019-09-24 01:40:50+00:00
- **Authors**: Cheng Chi, Shifeng Zhang, Junliang Xing, Zhen Lei, Stan Z. Li, Xudong Zou
- **Comment**: None
- **Journal**: None
- **Summary**: Head and human detection have been rapidly improved with the development of deep convolutional neural networks. However, these two tasks are often studied separately without considering their inherent correlation, leading to that 1) head detection is often trapped in more false positives, and 2) the performance of human detector frequently drops dramatically in crowd scenes. To handle these two issues, we present a novel joint head and human detection network, namely JointDet, which effectively detects head and human body simultaneously. Moreover, we design a head-body relationship discriminating module to perform relational learning between heads and human bodies, and leverage this learned relationship to regain the suppressed human detections and reduce head false positives. To verify the effectiveness of the proposed method, we annotate head bounding boxes of the CityPersons and Caltech-USA datasets, and conduct extensive experiments on the CrowdHuman, CityPersons and Caltech-USA datasets. As a consequence, the proposed JointDet detector achieves state-of-the-art performance on these three benchmarks. To facilitate further studies on the head and human detection problem, all new annotations, source codes and trained models will be public.



### The Field-of-View Constraint of Markers for Mobile Robot with Pan-Tilt Camera
- **Arxiv ID**: http://arxiv.org/abs/1909.10682v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.10682v1)
- **Published**: 2019-09-24 02:13:41+00:00
- **Updated**: 2019-09-24 02:13:41+00:00
- **Authors**: Hongxuan Ma, Wei Zou, Zheng Zhu, Siyang Sun, Zhaobing Kang
- **Comment**: None
- **Journal**: None
- **Summary**: In the field of navigation and visual servo, it is common to calculate relative pose by feature points on markers, so keeping markers in camera's view is an important problem. In this paper, we propose a novel approach to calculate field-of-view (FOV) constraint of markers for camera. Our method can make the camera maintain the visibility of all feature points during the motion of mobile robot. According to the angular aperture of camera, the mobile robot can obtain the FOV constraint region where the camera cannot keep all feature points in an image. Based on the FOV constraint region, the mobile robot can be guided to move from the initial position to destination. Finally simulations and experiments are conducted based on a mobile robot equipped with a pan-tilt camera, which validates the effectiveness of the method to obtain the FOV constraints.



### Analysis of Generalized Entropies in Mutual Information Medical Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1909.10690v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.10690v1)
- **Published**: 2019-09-24 03:11:38+00:00
- **Updated**: 2019-09-24 03:11:38+00:00
- **Authors**: Vinicius Pavanelli Vianna, Luiz Otavio Murta Junior
- **Comment**: 20 pages, 14 figures and 1 table
- **Journal**: None
- **Summary**: Mutual information (MI) is the standard method used in image registration and the most studied one but can diverge and produce wrong results when used in an automated manner. In this study we compared the results of the ITK Mattes MI function, used in 3D Slicer and ITK derived software solutions, and our own MICUDA Shannon and Tsallis MI functions under the translation, rotation and scale transforms in a 3D mathematical space. This comparison allows to understand why registration fails in some circumstances and how to produce a more robust automated algorithm to register medical images. Since our algorithms were designed to use GPU computations we also have a huge gain in speed while improving the quality of registration.



### Deformable Non-local Network for Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1909.10692v2
- **DOI**: 10.1109/ACCESS.2019.2958030
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.10692v2)
- **Published**: 2019-09-24 03:20:09+00:00
- **Updated**: 2019-12-21 13:40:34+00:00
- **Authors**: Hua Wang, Dewei Su, Chuangchuang Liu, Longcun Jin, Xianfang Sun, Xinyi Peng
- **Comment**: None
- **Journal**: IEEE Access, vol. 7, pp. 177734-177744, 2019
- **Summary**: The video super-resolution (VSR) task aims to restore a high-resolution (HR) video frame by using its corresponding low-resolution (LR) frame and multiple neighboring frames. At present, many deep learning-based VSR methods rely on optical flow to perform frame alignment. The final recovery results will be greatly affected by the accuracy of optical flow. However, optical flow estimation cannot be completely accurate, and there are always some errors. In this paper, we propose a novel deformable non-local network (DNLN) which is a non-optical-flow-based method. Specifically, we apply the deformable convolution and improve its ability of adaptive alignment at the feature level. Furthermore, we utilize a non-local structure to capture the global correlation between the reference frame and the aligned neighboring frames, and simultaneously enhance desired fine details in the aligned frames. To reconstruct the final high-quality HR video frames, we use residual in residual dense blocks to take full advantage of the hierarchical features. Experimental results on benchmark datasets demonstrate that the proposed DNLN can achieve state-of-the-art performance on VSR task.



### Learning deep representations for video-based intake gesture detection
- **Arxiv ID**: http://arxiv.org/abs/1909.10695v1
- **DOI**: 10.1109/JBHI.2019.2942845
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.10695v1)
- **Published**: 2019-09-24 03:29:53+00:00
- **Updated**: 2019-09-24 03:29:53+00:00
- **Authors**: Philipp V. Rouast, Marc T. P. Adam
- **Comment**: To be published in IEEE Journal of Biomedical and Health Informatics
- **Journal**: None
- **Summary**: Automatic detection of individual intake gestures during eating occasions has the potential to improve dietary monitoring and support dietary recommendations. Existing studies typically make use of on-body solutions such as inertial and audio sensors, while video is used as ground truth. Intake gesture detection directly based on video has rarely been attempted. In this study, we address this gap and show that deep learning architectures can successfully be applied to the problem of video-based detection of intake gestures. For this purpose, we collect and label video data of eating occasions using 360-degree video of 102 participants. Applying state-of-the-art approaches from video action recognition, our results show that (1) the best model achieves an $F_1$ score of 0.858, (2) appearance features contribute more than motion features, and (3) temporal context in form of multiple video frames is essential for top model performance.



### Multi-scale discriminative Region Discovery for Weakly-Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/1909.10698v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10698v1)
- **Published**: 2019-09-24 03:54:38+00:00
- **Updated**: 2019-09-24 03:54:38+00:00
- **Authors**: Pei Lv, Haiyu Yu, Junxiao Xue, Junjin Cheng, Lisha Cui, Bing Zhou, Mingliang Xu, Yi Yang
- **Comment**: 12 pages,7 figures
- **Journal**: None
- **Summary**: Localizing objects with weak supervision in an image is a key problem of the research in computer vision community. Many existing Weakly-Supervised Object Localization (WSOL) approaches tackle this problem by estimating the most discriminative regions with feature maps (activation maps) obtained by Deep Convolutional Neural Network, that is, only the objects or parts of them with the most discriminative response will be located. However, the activation maps often display different local maximum responses or relatively weak response when one image contains multiple objects with the same type or small objects. In this paper, we propose a simple yet effective multi-scale discriminative region discovery method to localize not only more integral objects but also as many as possible with only image-level class labels. The gradient weights flowing into different convolutional layers of CNN are taken as the input of our method, which is different from previous methods only considering that of the final convolutional layer. To mine more discriminative regions for the task of object localization, the multiple local maximum from the gradient weight maps are leveraged to generate the localization map with a parallel sliding window. Furthermore, multi-scale localization maps from different convolutional layers are fused to produce the final result. We evaluate the proposed method with the foundation of VGGnet on the ILSVRC 2016, CUB-200-2011 and PASCAL VOC 2012 datasets. On ILSVRC 2016, the proposed method yields the Top-1 localization error of 48.65\%, which outperforms previous results by 2.75\%. On PASCAL VOC 2012, our approach achieve the highest localization accuracy of 0.43. Even for CUB-200-2011 dataset, our method still achieves competitive results.



### Dimension Estimation Using Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1909.10702v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.10702v1)
- **Published**: 2019-09-24 04:09:48+00:00
- **Updated**: 2019-09-24 04:09:48+00:00
- **Authors**: Nitish Bahadur, Randy Paffenroth
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Dimension Estimation (DE) and Dimension Reduction (DR) are two closely related topics, but with quite different goals. In DE, one attempts to estimate the intrinsic dimensionality or number of latent variables in a set of measurements of a random vector. However, in DR, one attempts to project a random vector, either linearly or non-linearly, to a lower dimensional space that preserves the information contained in the original higher dimensional space. Of course, these two ideas are quite closely linked since, for example, doing DR to a dimension smaller than suggested by DE will likely lead to information loss. Accordingly, in this paper we will focus on a particular class of deep neural networks called autoencoders which are used extensively for DR but are less well studied for DE. We show that several important questions arise when using autoencoders for DE, above and beyond those that arise for more classic DR/DE techniques such as Principal Component Analysis. We address autoencoder architectural choices and regularization techniques that allow one to transform autoencoder latent layer representations into estimates of intrinsic dimension.



### 6D Pose Estimation with Correlation Fusion
- **Arxiv ID**: http://arxiv.org/abs/1909.12936v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.12936v2)
- **Published**: 2019-09-24 04:12:50+00:00
- **Updated**: 2021-04-06 06:49:06+00:00
- **Authors**: Yi Cheng, Hongyuan Zhu, Ying Sun, Cihan Acar, Wei Jing, Yan Wu, Liyuan Li, Cheston Tan, Joo-Hwee Lim
- **Comment**: Accepted by ICPR2020
- **Journal**: None
- **Summary**: 6D object pose estimation is widely applied in robotic tasks such as grasping and manipulation. Prior methods using RGB-only images are vulnerable to heavy occlusion and poor illumination, so it is important to complement them with depth information. However, existing methods using RGB-D data cannot adequately exploit consistent and complementary information between RGB and depth modalities. In this paper, we present a novel method to effectively consider the correlation within and across both modalities with attention mechanism to learn discriminative and compact multi-modal features. Then, effective fusion strategies for intra- and inter-correlation modules are explored to ensure efficient information flow between RGB and depth. To our best knowledge, this is the first work to explore effective intra- and inter-modality fusion in 6D pose estimation. The experimental results show that our method can achieve the state-of-the-art performance on LineMOD and YCB-Video dataset. We also demonstrate that the proposed method can benefit a real-world robot grasping task by providing accurate object pose estimation.



### Unsupervised Deep Features for Privacy Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1909.10708v1
- **DOI**: 10.1007/978-3-030-34879-3_31
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.10708v1)
- **Published**: 2019-09-24 04:38:15+00:00
- **Updated**: 2019-09-24 04:38:15+00:00
- **Authors**: Chiranjibi Sitaula, Yong Xiang, Sunil Aryal, Xuequan Lu
- **Comment**: Accepted in PSIVT2019 Conference
- **Journal**: PSIVT 2019. Lecture Notes in Computer Science, vol 11854
- **Summary**: Sharing images online poses security threats to a wide range of users due to the unawareness of privacy information. Deep features have been demonstrated to be a powerful representation for images. However, deep features usually suffer from the issues of a large size and requiring a huge amount of data for fine-tuning. In contrast to normal images (e.g., scene images), privacy images are often limited because of sensitive information. In this paper, we propose a novel approach that can work on limited data and generate deep features of smaller size. For training images, we first extract the initial deep features from the pre-trained model and then employ the K-means clustering algorithm to learn the centroids of these initial deep features. We use the learned centroids from training features to extract the final features for each testing image and encode our final features with the triangle encoding. To improve the discriminability of the features, we further perform the fusion of two proposed unsupervised deep features obtained from different layers. Experimental results show that the proposed features outperform state-of-the-art deep features, in terms of both classification accuracy and testing time.



### Message Scheduling for Performant, Many-Core Belief Propagation
- **Arxiv ID**: http://arxiv.org/abs/1909.11469v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.AI, cs.CV, cs.LG, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/1909.11469v1)
- **Published**: 2019-09-24 05:19:33+00:00
- **Updated**: 2019-09-24 05:19:33+00:00
- **Authors**: Mark Van der Merwe, Vinu Joseph, Ganesh Gopalakrishnan
- **Comment**: None
- **Journal**: None
- **Summary**: Belief Propagation (BP) is a message-passing algorithm for approximate inference over Probabilistic Graphical Models (PGMs), finding many applications such as computer vision, error-correcting codes, and protein-folding. While general, the convergence and speed of the algorithm has limited its practical use on difficult inference problems. As an algorithm that is highly amenable to parallelization, many-core Graphical Processing Units (GPUs) could significantly improve BP performance. Improving BP through many-core systems is non-trivial: the scheduling of messages in the algorithm strongly affects performance. We present a study of message scheduling for BP on GPUs. We demonstrate that BP exhibits a tradeoff between speed and convergence based on parallelism and show that existing message schedulings are not able to utilize this tradeoff. To this end, we present a novel randomized message scheduling approach, Randomized BP (RnBP), which outperforms existing methods on the GPU.



### Multi-scale fully convolutional neural networks for histopathology image segmentation: from nuclear aberrations to the global tissue architecture
- **Arxiv ID**: http://arxiv.org/abs/1909.10726v3
- **DOI**: 10.1016/j.media.2021.101996
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/1909.10726v3)
- **Published**: 2019-09-24 06:25:29+00:00
- **Updated**: 2021-02-21 21:07:46+00:00
- **Authors**: Rüdiger Schmitz, Frederic Madesta, Maximilian Nielsen, Jenny Krause, René Werner, Thomas Rösch
- **Comment**: Accepted for Medical Image Analysis
- **Journal**: None
- **Summary**: Histopathologic diagnosis relies on simultaneous integration of information from a broad range of scales, ranging from nuclear aberrations ($\approx \mathcal{O}(0.1{\mu m})$) through cellular structures ($\approx \mathcal{O}(10{\mu m})$) to the global tissue architecture ($\gtrapprox \mathcal{O}(1{mm})$). To explicitly mimic how human pathologists combine multi-scale information, we introduce a family of multi-encoder FCNs with deep fusion. We present a simple block for merging model paths with differing spatial scales in a spatial relationship-preserving fashion, which can readily be included in standard encoder-decoder networks. Additionally, a context classification gate block is proposed as an alternative for the incorporation of global context.   Our experiments were performed on three publicly available whole-slide images of recent challenges (PAIP 2019, BACH 2020, CAMELYON 2016). The multi-scale architectures consistently outperformed the baseline single-scale U-Nets by a large margin. They benefit from local as well as global context and particularly a combination of both. If feature maps from different scales are fused, doing so in a manner preserving spatial relationships was found to be beneficial. Deep guidance by a context classification loss appeared to improve model training at low computational costs. All multi-scale models had a reduced GPU memory footprint compared to ensembles of individual U-Nets trained on different image scales. Additional path fusions were shown to be possible at low computational cost, opening up possibilities for further, systematic and task-specific architecture optimization.   The findings demonstrate the potential of the presented family of human-inspired, end-to-end trainable, multi-scale multi-encoder FCNs to improve deep histopathologic diagnosis by extensive integration of largely different spatial scales.



### FEED: Feature-level Ensemble for Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/1909.10754v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.10754v1)
- **Published**: 2019-09-24 08:14:40+00:00
- **Updated**: 2019-09-24 08:14:40+00:00
- **Authors**: SeongUk Park, Nojun Kwak
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Knowledge Distillation (KD) aims to transfer knowledge in a teacher-student framework, by providing the predictions of the teacher network to the student network in the training stage to help the student network generalize better. It can use either a teacher with high capacity or {an} ensemble of multiple teachers. However, the latter is not convenient when one wants to use feature-map-based distillation methods. For a solution, this paper proposes a versatile and powerful training algorithm named FEature-level Ensemble for knowledge Distillation (FEED), which aims to transfer the ensemble knowledge using multiple teacher networks. We introduce a couple of training algorithms that transfer ensemble knowledge to the student at the feature map level. Among the feature-map-based distillation methods, using several non-linear transformations in parallel for transferring the knowledge of the multiple teacher{s} helps the student find more generalized solutions. We name this method as parallel FEED, andexperimental results on CIFAR-100 and ImageNet show that our method has clear performance enhancements, without introducing any additional parameters or computations at test time. We also show the experimental results of sequentially feeding teacher's information to the student, hence the name sequential FEED, and discuss the lessons obtained. Additionally, the empirical results on measuring the reconstruction errors at the feature map give hints for the enhancements.



### s-LWSR: Super Lightweight Super-Resolution Network
- **Arxiv ID**: http://arxiv.org/abs/1909.10774v1
- **DOI**: 10.1109/TIP.2020.3014953
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.10774v1)
- **Published**: 2019-09-24 09:34:21+00:00
- **Updated**: 2019-09-24 09:34:21+00:00
- **Authors**: Biao Li, Jiabin Liu, Bo Wang, Zhiquan Qi, Yong Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning (DL) architectures for superresolution (SR) normally contain tremendous parameters, which has been regarded as the crucial advantage for obtaining satisfying performance. However, with the widespread use of mobile phones for taking and retouching photos, this character greatly hampers the deployment of DL-SR models on the mobile devices. To address this problem, in this paper, we propose a super lightweight SR network: s-LWSR. There are mainly three contributions in our work. Firstly, in order to efficiently abstract features from the low resolution image, we build an information pool to mix multi-level information from the first half part of the pipeline. Accordingly, the information pool feeds the second half part with the combination of hierarchical features from the previous layers. Secondly, we employ a compression module to further decrease the size of parameters. Intensive analysis confirms its capacity of trade-off between model complexity and accuracy. Thirdly, by revealing the specific role of activation in deep models, we remove several activation layers in our SR model to retain more information for performance improvement. Extensive experiments show that our s-LWSR, with limited parameters and operations, can achieve similar performance to other cumbersome DL-SR methods.



### PolSAR Image Classification Based on Dilated Convolution and Pixel-Refining Parallel Mapping network in the Complex Domain
- **Arxiv ID**: http://arxiv.org/abs/1909.10783v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10783v2)
- **Published**: 2019-09-24 09:59:47+00:00
- **Updated**: 2020-01-20 15:34:26+00:00
- **Authors**: Dongling Xiao, Chang Liu, Qi Wang, Chao Wang, Xin Zhang
- **Comment**: 15 pages, 13 figures
- **Journal**: None
- **Summary**: Efficient and accurate polarimetric synthetic aperture radar (PolSAR) image classification with a limited number of prior labels is always full of challenges. For general supervised deep learning classification algorithms, the pixel-by-pixel algorithm achieves precise yet inefficient classification with a small number of labeled pixels, whereas the pixel mapping algorithm achieves efficient yet edge-rough classification with more prior labels required. To take efficiency, accuracy and prior labels into account, we propose a novel pixel-refining parallel mapping network in the complex domain named CRPM-Net and the corresponding training algorithm for PolSAR image classification. CRPM-Net consists of two parallel sub-networks: a) A transfer dilated convolution mapping network in the complex domain (C-Dilated CNN) activated by a complex cross-convolution neural network (Cs-CNN), which is aiming at precise localization, high efficiency and the full use of phase information; b) A complex domain encoder-decoder network connected parallelly with C-Dilated CNN, which is to extract more contextual semantic features. Finally, we design a two-step algorithm to train the Cs-CNN and CRPM-Net with a small number of labeled pixels for higher accuracy by refining misclassified labeled pixels. We verify the proposed method on AIRSAR and E-SAR datasets. The experimental results demonstrate that CRPM-Net achieves the best classification results and substantially outperforms some latest state-of-the-art approaches in both efficiency and accuracy for PolSAR image classification. The source code and trained models for CRPM-Net is available at: https://github.com/PROoshio/CRPM-Net.



### Forward and Backward Information Retention for Accurate Binary Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.10788v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10788v4)
- **Published**: 2019-09-24 10:12:36+00:00
- **Updated**: 2020-03-09 16:31:03+00:00
- **Authors**: Haotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei Yu, Jingkuan Song
- **Comment**: None
- **Journal**: None
- **Summary**: Weight and activation binarization is an effective approach to deep neural network compression and can accelerate the inference by leveraging bitwise operations. Although many binarization methods have improved the accuracy of the model by minimizing the quantization error in forward propagation, there remains a noticeable performance gap between the binarized model and the full-precision one. Our empirical study indicates that the quantization brings information loss in both forward and backward propagation, which is the bottleneck of training accurate binary neural networks. To address these issues, we propose an Information Retention Network (IR-Net) to retain the information that consists in the forward activations and backward gradients. IR-Net mainly relies on two technical contributions: (1) Libra Parameter Binarization (Libra-PB): simultaneously minimizing both quantization error and information loss of parameters by balanced and standardized weights in forward propagation; (2) Error Decay Estimator (EDE): minimizing the information loss of gradients by gradually approximating the sign function in backward propagation, jointly considering the updating ability and accurate gradients. We are the first to investigate both forward and backward processes of binary networks from the unified information perspective, which provides new insight into the mechanism of network binarization. Comprehensive experiments with various network structures on CIFAR-10 and ImageNet datasets manifest that the proposed IR-Net can consistently outperform state-of-the-art quantization methods.



### Offline identification of surgical deviations in laparoscopic rectopexy
- **Arxiv ID**: http://arxiv.org/abs/1909.10790v2
- **DOI**: 10.1016/j.artmed.2020.101837
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.10790v2)
- **Published**: 2019-09-24 10:17:44+00:00
- **Updated**: 2020-03-18 10:30:25+00:00
- **Authors**: Arnaud Huaulmé, Pierre Jannin, Fabian Reche, Jean-Luc Faucheron, Alexandre Moreau-Gaudry, Sandrine Voros
- **Comment**: None
- **Journal**: None
- **Summary**: Objective: A median of 14.4% of patient undergone at least one adverse event during surgery and a third of them are preventable. The occurrence of adverse events forces surgeons to implement corrective strategies and, thus, deviate from the standard surgical process. Therefore, it is clear that the automatic identification of adverse events is a major challenge for patient safety. In this paper, we have proposed a method enabling us to identify such deviations. We have focused on identifying surgeons' deviations from standard surgical processes due to surgical events rather than anatomic specificities. This is particularly challenging, given the high variability in typical surgical procedure workflows. Methods: We have introduced a new approach designed to automatically detect and distinguish surgical process deviations based on multi-dimensional non-linear temporal scaling with a hidden semi-Markov model using manual annotation of surgical processes. The approach was then evaluated using cross-validation. Results: The best results have over 90% accuracy. Recall and precision were superior at 70%. We have provided a detailed analysis of the incorrectly-detected observations. Conclusion: Multi-dimensional non-linear temporal scaling with a hidden semi-Markov model provides promising results for detecting deviations. Our error analysis of the incorrectly-detected observations offers different leads in order to further improve our method. Significance: Our method demonstrated the feasibility of automatically detecting surgical deviations that could be implemented for both skill analysis and developing situation awareness-based computer-assisted surgical systems.



### Development of Fast Refinement Detectors on AI Edge Platforms
- **Arxiv ID**: http://arxiv.org/abs/1909.10798v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10798v2)
- **Published**: 2019-09-24 10:29:43+00:00
- **Updated**: 2020-11-26 03:46:53+00:00
- **Authors**: Min-Kook Choi, Heechul Jung
- **Comment**: 7 pages, 2 figures, IML@ICPR 2020
- **Journal**: None
- **Summary**: With the improvements in the object detection networks, several variations of object detection networks have been achieved impressive performance. However, the performance evaluation of most models has focused on detection accuracy, and performance verification is mostly based on high-end GPU hardware. In this paper, we propose real-time object detectors that guarantee balanced performance for real-time systems on embedded platforms. The proposed model utilizes the basic head structure of the RefineDet model, which is a variant of the single-shot object detector (SSD). In order to ensure real-time performance, CNN models with relatively shallow layers or fewer parameters have been used as the backbone structure. In addition to the basic VGGNet and ResNet structures, various backbone structures such as MobileNet, Xception, ResNeXt, Inception-SENet, and SE-ResNeXt have been used for this purpose. Successful training of object detection networks was achieved through an appropriate combination of intermediate layers. The accuracy of the proposed detector was estimated by the evaluation of the MS-COCO 2017 object detection dataset and the inference speed on the NVIDIA Drive PX2 and Jetson Xavier boards were tested to verify real-time performance in the embedded systems. The experiments show that the proposed models ensure balanced performance in terms of accuracy and inference speed in the embedded system environments. In addition, unlike the high-end GPUs, the use of embedded GPUs involves several additional concerns for efficient inference, which have been identified in this work. The codes and models are publicly available on the web (link).



### Image Recognition using Region Creep
- **Arxiv ID**: http://arxiv.org/abs/1909.10811v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.10811v3)
- **Published**: 2019-09-24 10:59:44+00:00
- **Updated**: 2022-12-14 10:07:24+00:00
- **Authors**: Kieran Greer
- **Comment**: None
- **Journal**: 10th International Conference on Advanced Technologies (ICAT'22),
  pp. 43 - 46, November 25-27, 2022, Van, Turkey. Virtual Conference
- **Summary**: This paper describes a new type of auto-associative image classifier that uses a shallow architecture with a very quick learning phase. The image is parsed into smaller areas and each area is saved directly for a region, along with the related output category. When a new image is presented, a direct match with each region is made and the best matching areas returned. Each area stores a list of the categories it belongs to, where there is a one-to-many relation between the input region and the output categories. The image classification process sums the category lists to return a preferred category for the whole image. These areas can overlap with each other and when moving from a region to its neighbours, there is likely to be only small changes in the area image part. It would therefore be possible to guess what the best image area is for one region by cumulating the results of its neighbours. This associative feature is being called 'Region Creep' and the cumulated region can be compared with train cases instead, when a suitable match is not found. Rules can be included and state that: if one set of pixels are present, another set should either be removed or should also be present, where this is across the whole image. The memory problems with a traditional auto-associative network may be less with this version and tests on a set of hand-written numbers have produced state-of-the-art results.



### Investigating Customization Strategies and Convergence Behaviors of Task-specific ADMM
- **Arxiv ID**: http://arxiv.org/abs/1909.10819v2
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1909.10819v2)
- **Published**: 2019-09-24 11:29:13+00:00
- **Updated**: 2021-09-05 13:54:39+00:00
- **Authors**: Risheng Liu, Pan Mu, Jin Zhang
- **Comment**: Accepted at TIP
- **Journal**: None
- **Summary**: Alternating Direction Method of Multiplier (ADMM) has been a popular algorithmic framework for separable optimization problems with linear constraints. For numerical ADMM fail to exploit the particular structure of the problem at hand nor the input data information, leveraging task-specific modules (e.g., neural networks and other data-driven architectures) to extend ADMM is a significant but challenging task. This work focuses on designing a flexible algorithmic framework to incorporate various task-specific modules (with no additional constraints) to improve the performance of ADMM in real-world applications. Specifically, we propose Guidance from Optimality (GO), a new customization strategy, to embed task-specific modules into ADMM (GO-ADMM). By introducing an optimality-based criterion to guide the propagation, GO-ADMM establishes an updating scheme agnostic to the choice of additional modules. The existing task-specific methods just plug their task-specific modules into the numerical iterations in a straightforward manner. Even with some restrictive constraints on the plug-in modules, they can only obtain some relatively weaker convergence properties for the resulted ADMM iterations. Fortunately, without any restrictions on the embedded modules, we prove the convergence of GO-ADMM regarding objective values and constraint violations, and derive the worst-case convergence rate measured by iteration complexity. Extensive experiments are conducted to verify the theoretical results and demonstrate the efficiency of GO-ADMM.



### Distortion Estimation Through Explicit Modeling of the Refractive Surface
- **Arxiv ID**: http://arxiv.org/abs/1909.10820v1
- **DOI**: 10.1007/978-3-030-30508-6_2
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.10820v1)
- **Published**: 2019-09-24 11:31:09+00:00
- **Updated**: 2019-09-24 11:31:09+00:00
- **Authors**: Szabolcs Pável, Csanád Sándor, Lehel Csató
- **Comment**: Accepted to ICANN 2019
- **Journal**: LNCS 11729, pp. 17-28, 2019
- **Summary**: Precise calibration is a must for high reliance 3D computer vision algorithms. A challenging case is when the camera is behind a protective glass or transparent object: due to refraction, the image is heavily distorted; the pinhole camera model alone can not be used and a distortion correction step is required. By directly modeling the geometry of the refractive media, we build the image generation process by tracing individual light rays from the camera to a target. Comparing the generated images to their distorted - observed - counterparts, we estimate the geometry parameters of the refractive surface via model inversion by employing an RBF neural network. We present an image collection methodology that produces data suited for finding the distortion parameters and test our algorithm on synthetic and real-world data. We analyze the results of the algorithm.



### Enhancing Traffic Scene Predictions with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.10833v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10833v1)
- **Published**: 2019-09-24 12:14:48+00:00
- **Updated**: 2019-09-24 12:14:48+00:00
- **Authors**: Peter König, Sandra Aigner, Marco Körner
- **Comment**: Accepted for presentation at the IEEE Intelligent Transportation
  Systems Conference -- ITSC 2019
- **Journal**: None
- **Summary**: We present a new two-stage pipeline for predicting frames of traffic scenes where relevant objects can still reliably be detected. Using a recent video prediction network, we first generate a sequence of future frames based on past frames. A second network then enhances these frames in order to make them appear more realistic. This ensures the quality of the predicted frames to be sufficient to enable accurate detection of objects, which is especially important for autonomously driving cars. To verify this two-stage approach, we conducted experiments on the Cityscapes dataset. For enhancing, we trained two image-to-image translation methods based on generative adversarial networks, one for blind motion deblurring and one for image super-resolution. All resulting predictions were quantitatively evaluated using both traditional metrics and a state-of-the-art object detection network showing that the enhanced frames appear qualitatively improved. While the traditional image comparison metrics, i.e., MSE, PSNR, and SSIM, failed to confirm this visual impression, the object detection evaluation resembles it well. The best performing prediction-enhancement pipeline is able to increase the average precision values for detecting cars by about 9% for each prediction step, compared to the non-enhanced predictions.



### Temporal-Coded Deep Spiking Neural Network with Easy Training and Robust Performance
- **Arxiv ID**: http://arxiv.org/abs/1909.10837v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.10837v5)
- **Published**: 2019-09-24 12:28:11+00:00
- **Updated**: 2021-09-17 12:58:08+00:00
- **Authors**: Shibo Zhou, Xiaohua LI, Ying Chen, Sanjeev T. Chandrasekaran, Arindam Sanyal
- **Comment**: None
- **Journal**: Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21),
  2021: 35(12),11143-11151
- **Summary**: Spiking neural network (SNN) is interesting both theoretically and practically because of its strong bio-inspiration nature and potentially outstanding energy efficiency. Unfortunately, its development has fallen far behind the conventional deep neural network (DNN), mainly because of difficult training and lack of widely accepted hardware experiment platforms. In this paper, we show that a deep temporal-coded SNN can be trained easily and directly over the benchmark datasets CIFAR10 and ImageNet, with testing accuracy within 1% of the DNN of equivalent size and architecture. Training becomes similar to DNN thanks to the closed-form solution to the spiking waveform dynamics. Considering that SNNs should be implemented in practical neuromorphic hardwares, we train the deep SNN with weights quantized to 8, 4, 2 bits and with weights perturbed by random noise to demonstrate its robustness in practical applications. In addition, we develop a phase-domain signal processing circuit schematic to implement our spiking neuron with 90% gain of energy efficiency over existing work. This paper demonstrates that the temporal-coded deep SNN is feasible for applications with high performance and high energy efficient.



### Single Camera Training for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1909.10848v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10848v1)
- **Published**: 2019-09-24 12:50:54+00:00
- **Updated**: 2019-09-24 12:50:54+00:00
- **Authors**: Tianyu Zhang, Lingxi Xie, Longhui Wei, Yongfei Zhang, Bo Li, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification (ReID) aims at finding the same person in different cameras. Training such systems usually requires a large amount of cross-camera pedestrians to be annotated from surveillance videos, which is labor-consuming especially when the number of cameras is large. Differently, this paper investigates ReID in an unexplored single-camera-training (SCT) setting, where each person in the training set appears in only one camera. To the best of our knowledge, this setting was never studied before. SCT enjoys the advantage of low-cost data collection and annotation, and thus eases ReID systems to be trained in a brand new environment. However, it raises major challenges due to the lack of cross-camera person occurrences, which conventional approaches heavily rely on to extract discriminative features. The key to dealing with the challenges in the SCT setting lies in designing an effective mechanism to complement cross-camera annotation. We start with a regular deep network for feature extraction, upon which we propose a novel loss function named multi-camera negative loss (MCNL). This is a metric learning loss motivated by probability, suggesting that in a multi-camera system, one image is more likely to be closer to the most similar negative sample in other cameras than to the most similar negative sample in the same camera. In experiments, MCNL significantly boosts ReID accuracy in the SCT setting, which paves the way of fast deployment of ReID systems with good performance on new target scenes.



### Multi-Person 3D Human Pose Estimation from Monocular Images
- **Arxiv ID**: http://arxiv.org/abs/1909.10854v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10854v1)
- **Published**: 2019-09-24 12:55:56+00:00
- **Updated**: 2019-09-24 12:55:56+00:00
- **Authors**: Rishabh Dabral, Nitesh B Gundavarapu, Rahul Mitra, Abhishek Sharma, Ganesh Ramakrishnan, Arjun Jain
- **Comment**: 3DV 2019
- **Journal**: None
- **Summary**: Multi-person 3D human pose estimation from a single image is a challenging problem, especially for in-the-wild settings due to the lack of 3D annotated data. We propose HG-RCNN, a Mask-RCNN based network that also leverages the benefits of the Hourglass architecture for multi-person 3D Human Pose Estimation. A two-staged approach is presented that first estimates the 2D keypoints in every Region of Interest (RoI) and then lifts the estimated keypoints to 3D. Finally, the estimated 3D poses are placed in camera-coordinates using weak-perspective projection assumption and joint optimization of focal length and root translations. The result is a simple and modular network for multi-person 3D human pose estimation that does not require any multi-person 3D pose dataset. Despite its simple formulation, HG-RCNN achieves the state-of-the-art results on MuPoTS-3D while also approximating the 3D pose in the camera-coordinate system.



### Restyling Data: Application to Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1909.10900v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10900v1)
- **Published**: 2019-09-24 13:30:58+00:00
- **Updated**: 2019-09-24 13:30:58+00:00
- **Authors**: Vasileios Gkitsas, Antonis Karakottas, Nikolaos Zioulis, Dimitrios Zarpalas, Petros Daras
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning is driven by data, yet while their availability is constantly increasing, training data require laborious, time consuming and error-prone labelling or ground truth acquisition, which in some cases is very difficult or even impossible. Recent works have resorted to synthetic data generation, but the inferior performance of models trained on synthetic data when applied to the real world, introduced the challenge of unsupervised domain adaptation. In this work we investigate an unsupervised domain adaptation technique that descends from another perspective, in order to avoid the complexity of adversarial training and cycle consistencies. We exploit the recent advances in photorealistic style transfer and take a fully data driven approach. While this concept is already implicitly formulated within the intricate objectives of domain adaptation GANs, we take an explicit approach and apply it directly as data pre-processing. The resulting technique is scalable, efficient and easy to implement, offers competitive performance to the complex state-of-the-art alternatives and can open up new pathways for domain adaptation.



### Deep Mangoes: from fruit detection to cultivar identification in colour images of mango trees
- **Arxiv ID**: http://arxiv.org/abs/1909.10939v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1909.10939v1)
- **Published**: 2019-09-24 14:05:54+00:00
- **Updated**: 2019-09-24 14:05:54+00:00
- **Authors**: Philippe Borianne, Frederic Borne, Julien Sarron, Emile Faye
- **Comment**: None
- **Journal**: DISP'19 International Conference on Digital Image and Signal
  Processing, Apr 2019, Oxford, United Kingdom
- **Summary**: This paper presents results on the detection and identification mango fruits from colour images of trees. We evaluate the behaviour and the performances of the Faster R-CNN network to determine whether it is robust enough to "detect and classify" fruits under particularly heterogeneous conditions in terms of plant cultivars, plantation scheme, and visual information acquisition contexts. The network is trained to distinguish the 'Kent', 'Keitt', and "Boucodiekhal" mango cultivars from 3,000 representative labelled fruit annotations. The validation set composed of about 7,000 annotations was then tested with a confidence threshold of 0.7 and a Non-Maximal-Suppression threshold of 0.25. With a F1-score of 0.90, the Faster R-CNN is well suitable to the simple fruit detection in tiles of 500x500 pixels. We then combine a multi-tiling approach with a Jaccard matrix to merge the different parts of objects detected several times, and thus report the detections made at the tile scale to the native 6,000x4,000 pixel size images. Nonetheless with a F1-score of 0.56, the cultivar identification Faster R-CNN network presents some limitations for simultaneously detecting the mango fruits and identifying their respective cultivars. Despite the proven errors in fruit detection, the cultivar identification rates of the detected mango fruits are in the order of 80%. The ideal solution could combine a Mask R-CNN for the image pre-segmentation of trees and a double-stream Faster R-CNN for detecting the mango fruits and identifying their respective cultivar to provide predictions more relevant to users' expectations.



### A System-Level Solution for Low-Power Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1909.10964v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1909.10964v2)
- **Published**: 2019-09-24 14:45:43+00:00
- **Updated**: 2019-10-19 13:57:12+00:00
- **Authors**: Fanrong Li, Zitao Mo, Peisong Wang, Zejian Liu, Jiayun Zhang, Gang Li, Qinghao Hu, Xiangyu He, Cong Leng, Yang Zhang, Jian Cheng
- **Comment**: Accepted by ICCV 2019 Low-Power Computer Vision Workshop
- **Journal**: None
- **Summary**: Object detection has made impressive progress in recent years with the help of deep learning. However, state-of-the-art algorithms are both computation and memory intensive. Though many lightweight networks are developed for a trade-off between accuracy and efficiency, it is still a challenge to make it practical on an embedded device. In this paper, we present a system-level solution for efficient object detection on a heterogeneous embedded device. The detection network is quantized to low bits and allows efficient implementation with shift operators. In order to make the most of the benefits of low-bit quantization, we design a dedicated accelerator with programmable logic. Inside the accelerator, a hybrid dataflow is exploited according to the heterogeneous property of different convolutional layers. We adopt a straightforward but resource-friendly column-prior tiling strategy to map the computation-intensive convolutional layers to the accelerator that can support arbitrary feature size. Other operations can be performed on the low-power CPU cores, and the entire system is executed in a pipelined manner. As a case study, we evaluate our object detection system on a real-world surveillance video with input size of 512x512, and it turns out that the system can achieve an inference speed of 18 fps at the cost of 6.9W (with display) with an mAP of 66.4 verified on the PASCAL VOC 2012 dataset.



### Monocular Pedestrian Orientation Estimation Based on Deep 2D-3D Feedforward
- **Arxiv ID**: http://arxiv.org/abs/1909.10970v2
- **DOI**: 10.1016/j.patcog.2019.107182
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10970v2)
- **Published**: 2019-09-24 14:54:07+00:00
- **Updated**: 2020-01-16 15:29:40+00:00
- **Authors**: Chenchen Zhao, Yeqiang Qian, Ming Yang
- **Comment**: 29 pages, 12 figures
- **Journal**: None
- **Summary**: Accurate pedestrian orientation estimation of autonomous driving helps the ego vehicle obtain the intentions of pedestrians in the related environment, which are the base of safety measures such as collision avoidance and prewarning. However, because of relatively small sizes and high-level deformation of pedestrians, common pedestrian orientation estimation models fail to extract sufficient and comprehensive information from them, thus having their performance restricted, especially monocular ones which fail to obtain depth information of objects and related environment. In this paper, a novel monocular pedestrian orientation estimation model, called FFNet, is proposed. Apart from camera captures, the model adds the 2D and 3D dimensions of pedestrians as two other inputs according to the logic relationship between orientation and them. The 2D and 3D dimensions of pedestrians are determined from the camera captures and further utilized through two feedforward links connected to the orientation estimator. The feedforward links strengthen the logicality and interpretability of the network structure of the proposed model. Experiments show that the proposed model has at least 1.72% AOS increase than most state-of-the-art models after identical training processes. The model also has competitive results in orientation estimation evaluation on KITTI dataset.



### Synthetic dataset generation for object-to-model deep learning in industrial applications
- **Arxiv ID**: http://arxiv.org/abs/1909.10976v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.10976v1)
- **Published**: 2019-09-24 14:58:07+00:00
- **Updated**: 2019-09-24 14:58:07+00:00
- **Authors**: Matthew Z. Wong, Kiyohito Kunii, Max Baylis, Wai Hong Ong, Pavel Kroupa, Swen Koller
- **Comment**: None
- **Journal**: None
- **Summary**: The availability of large image data sets has been a crucial factor in the success of deep learning-based classification and detection methods. While data sets for everyday objects are widely available, data for specific industrial use-cases (e.g. identifying packaged products in a warehouse) remains scarce. In such cases, the data sets have to be created from scratch, placing a crucial bottleneck on the deployment of deep learning techniques in industrial applications.   We present work carried out in collaboration with a leading UK online supermarket, with the aim of creating a computer vision system capable of detecting and identifying unique supermarket products in a warehouse setting. To this end, we demonstrate a framework for using synthetic data to create an end-to-end deep learning pipeline, beginning with real-world objects and culminating in a trained model.   Our method is based on the generation of a synthetic dataset from 3D models obtained by applying photogrammetry techniques to real-world objects. Using 100k synthetic images generated from 60 real images per class, an InceptionV3 convolutional neural network (CNN) was trained, which achieved classification accuracy of 95.8% on a separately acquired test set of real supermarket product images. The image generation process supports automatic pixel annotation. This eliminates the prohibitively expensive manual annotation typically required for detection tasks. Based on this readily available data, a one-stage RetinaNet detector was trained on the synthetic, annotated images to produce a detector that can accurately localize and classify the specimen products in real-time.



### Augmented Memory for Correlation Filters in Real-Time UAV Tracking
- **Arxiv ID**: http://arxiv.org/abs/1909.10989v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.10989v1)
- **Published**: 2019-09-24 15:12:13+00:00
- **Updated**: 2019-09-24 15:12:13+00:00
- **Authors**: Yiming Li, Changhong Fu, Fangqiang Ding, Ziyuan Huang, Jia Pan
- **Comment**: None
- **Journal**: None
- **Summary**: The outstanding computational efficiency of discriminative correlation filter (DCF) fades away with various complicated improvements. Previous appearances are also gradually forgotten due to the exponential decay of historical views in traditional appearance updating scheme of DCF framework, reducing the model's robustness. In this work, a novel tracker based on DCF framework is proposed to augment memory of previously appeared views while running at real-time speed. Several historical views and the current view are simultaneously introduced in training to allow the tracker to adapt to new appearances as well as memorize previous ones. A novel rapid compressed context learning is proposed to increase the discriminative ability of the filter efficiently. Substantial experiments on UAVDT and UAV123 datasets have validated that the proposed tracker performs competitively against other 26 top DCF and deep-based trackers with over 40 FPS on CPU.



### pISTA-SENSE-ResNet for Parallel MRI Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1910.00650v1
- **DOI**: 10.1016/j.jmr.2020.106790
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1910.00650v1)
- **Published**: 2019-09-24 15:18:05+00:00
- **Updated**: 2019-09-24 15:18:05+00:00
- **Authors**: Tieyuan Lu, Xinlin Zhang, Yihui Huang, Yonggui Yang, Gang Guo, Lijun Bao, Feng Huang, Di Guo, Xiaobo Qu
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic resonance imaging has been widely applied in clinical diagnosis, however, is limited by its long data acquisition time. Although imaging can be accelerated by sparse sampling and parallel imaging, achieving promising reconstruction images with a fast reconstruction speed remains a challenge. Recently, deep learning approaches have attracted a lot of attention for its encouraging reconstruction results but without a proper interpretability. In this letter, to enable high-quality image reconstruction for the parallel magnetic resonance imaging, we design the network structure from the perspective of sparse iterative reconstruction and enhance it with the residual structure. The experimental results of a public knee dataset show that compared with the optimization-based method and the latest deep learning parallel imaging methods, the proposed network has less error in reconstruction and is more stable under different acceleration factors.



### Posture and sequence recognition for Bharatanatyam dance performances using machine learning approach
- **Arxiv ID**: http://arxiv.org/abs/1909.11023v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/1909.11023v1)
- **Published**: 2019-09-24 16:18:01+00:00
- **Updated**: 2019-09-24 16:18:01+00:00
- **Authors**: Tanwi Mallick, Partha Pratim Das, Arun Kumar Majumdar
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding the underlying semantics of performing arts like dance is a challenging task. Dance is multimedia in nature and spans over time as well as space. Capturing and analyzing the multimedia content of the dance is useful for the preservation of cultural heritage, to build video recommendation systems, to assist learners to use tutoring systems. To develop an application for dance, three aspects of dance analysis need to be addressed: 1) Segmentation of the dance video to find the representative action elements, 2) Matching or recognition of the detected action elements, and 3) Recognition of the dance sequences formed by combining a number of action elements under certain rules. This paper attempts to solve three fundamental problems of dance analysis for understanding the underlying semantics of dance forms. Our focus is on an Indian Classical Dance (ICD) form known as Bharatanatyam. As dance is driven by music, we use the music as well as motion information for key posture extraction. Next, we recognize the key postures using machine learning as well as deep learning techniques. Finally, the dance sequence is recognized using the Hidden Markov Model (HMM). We capture the multi-modal data of Bharatanatyam dance using Kinect and build an annotated data set for research in ICD.



### COLTRANE: ConvolutiOnaL TRAjectory NEtwork for Deep Map Inference
- **Arxiv ID**: http://arxiv.org/abs/1909.11048v1
- **DOI**: 10.1145/3360322.3360853
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.11048v1)
- **Published**: 2019-09-24 16:59:33+00:00
- **Updated**: 2019-09-24 16:59:33+00:00
- **Authors**: Arian Prabowo, Piotr Koniusz, Wei Shao, Flora D. Salim
- **Comment**: BuildSys 2019
- **Journal**: BuildSys 2019
- **Summary**: The process of automatic generation of a road map from GPS trajectories, called map inference, remains a challenging task to perform on a geospatial data from a variety of domains as the majority of existing studies focus on road maps in cities. Inherently, existing algorithms are not guaranteed to work on unusual geospatial sites, such as an airport tarmac, pedestrianized paths and shortcuts, or animal migration routes, etc. Moreover, deep learning has not been explored well enough for such tasks. This paper introduces COLTRANE, ConvolutiOnaL TRAjectory NEtwork, a novel deep map inference framework which operates on GPS trajectories collected in various environments. This framework includes an Iterated Trajectory Mean Shift (ITMS) module to localize road centerlines, which copes with noisy GPS data points. Convolutional Neural Network trained on our novel trajectory descriptor is then introduced into our framework to detect and accurately classify junctions for refinement of the road maps. COLTRANE yields up to 37% improvement in F1 scores over existing methods on two distinct real-world datasets: city roads and airport tarmac.



### Unified Vision-Language Pre-Training for Image Captioning and VQA
- **Arxiv ID**: http://arxiv.org/abs/1909.11059v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.11059v3)
- **Published**: 2019-09-24 17:17:26+00:00
- **Updated**: 2019-12-04 18:48:15+00:00
- **Authors**: Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso, Jianfeng Gao
- **Comment**: AAAI 2020 camera-ready version. The code and the pre-trained models
  are available at https://github.com/LuoweiZhou/VLP
- **Journal**: None
- **Summary**: This paper presents a unified Vision-Language Pre-training (VLP) model. The model is unified in that (1) it can be fine-tuned for either vision-language generation (e.g., image captioning) or understanding (e.g., visual question answering) tasks, and (2) it uses a shared multi-layer transformer network for both encoding and decoding, which differs from many existing methods where the encoder and decoder are implemented using separate models. The unified VLP model is pre-trained on a large amount of image-text pairs using the unsupervised learning objectives of two tasks: bidirectional and sequence-to-sequence (seq2seq) masked vision-language prediction. The two tasks differ solely in what context the prediction conditions on. This is controlled by utilizing specific self-attention masks for the shared transformer network. To the best of our knowledge, VLP is the first reported model that achieves state-of-the-art results on both vision-language generation and understanding tasks, as disparate as image captioning and visual question answering, across three challenging benchmark datasets: COCO Captions, Flickr30k Captions, and VQA 2.0. The code and the pre-trained models are available at https://github.com/LuoweiZhou/VLP.



### Segmentation Transformer: Object-Contextual Representations for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.11065v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.11065v6)
- **Published**: 2019-09-24 17:39:23+00:00
- **Updated**: 2021-04-30 16:42:15+00:00
- **Authors**: Yuhui Yuan, Xiaokang Chen, Xilin Chen, Jingdong Wang
- **Comment**: We rephrase the object-contextual representation scheme using the
  Transformer encoder-decoder framework. ECCV 2020 Spotlight. Project Page:
  https://github.com/openseg-group/openseg.pytorch
  https://github.com/HRNet/HRNet-Semantic-Segmentation/tree/HRNet-OCR
- **Journal**: ECCV 2020
- **Summary**: In this paper, we address the semantic segmentation problem with a focus on the context aggregation strategy. Our motivation is that the label of a pixel is the category of the object that the pixel belongs to. We present a simple yet effective approach, object-contextual representations, characterizing a pixel by exploiting the representation of the corresponding object class. First, we learn object regions under the supervision of ground-truth segmentation. Second, we compute the object region representation by aggregating the representations of the pixels lying in the object region. Last, % the representation similarity we compute the relation between each pixel and each object region and augment the representation of each pixel with the object-contextual representation which is a weighted aggregation of all the object region representations according to their relations with the pixel. We empirically demonstrate that the proposed approach achieves competitive performance on various challenging semantic segmentation benchmarks: Cityscapes, ADE20K, LIP, PASCAL-Context, and COCO-Stuff. Cityscapes, ADE20K, LIP, PASCAL-Context, and COCO-Stuff. Our submission "HRNet + OCR + SegFix" achieves 1-st place on the Cityscapes leaderboard by the time of submission. Code is available at: https://git.io/openseg and https://git.io/HRNet.OCR. We rephrase the object-contextual representation scheme using the Transformer encoder-decoder framework. The details are presented in~Section3.3.



### Interactive Sketch & Fill: Multiclass Sketch-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1909.11081v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.11081v2)
- **Published**: 2019-09-24 17:56:37+00:00
- **Updated**: 2019-09-25 18:16:30+00:00
- **Authors**: Arnab Ghosh, Richard Zhang, Puneet K. Dokania, Oliver Wang, Alexei A. Efros, Philip H. S. Torr, Eli Shechtman
- **Comment**: ICCV 2019, Video Avaiable at https://youtu.be/T9xtpAMUDps
- **Journal**: None
- **Summary**: We propose an interactive GAN-based sketch-to-image translation method that helps novice users create images of simple objects. As the user starts to draw a sketch of a desired object type, the network interactively recommends plausible completions, and shows a corresponding synthesized image to the user. This enables a feedback loop, where the user can edit their sketch based on the network's recommendations, visualizing both the completed shape and final rendered image while they draw. In order to use a single trained model across a wide array of object classes, we introduce a gating-based approach for class conditioning, which allows us to generate distinct classes without feature mixing, from a single generator network. Video available at our website: https://arnabgho.github.io/iSketchNFill/.



### Accept Synthetic Objects as Real: End-to-End Training of Attentive Deep Visuomotor Policies for Manipulation in Clutter
- **Arxiv ID**: http://arxiv.org/abs/1909.11128v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.11128v2)
- **Published**: 2019-09-24 19:01:40+00:00
- **Updated**: 2019-11-11 22:45:55+00:00
- **Authors**: Pooya Abolghasemi, Ladislau Bölöni
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: Recent research demonstrated that it is feasible to end-to-end train multi-task deep visuomotor policies for robotic manipulation using variations of learning from demonstration (LfD) and reinforcement learning (RL). In this paper, we extend the capabilities of end-to-end LfD architectures to object manipulation in clutter. We start by introducing a data augmentation procedure called Accept Synthetic Objects as Real (ASOR). Using ASOR we develop two network architectures: implicit attention ASOR-IA and explicit attention ASOR-EA. Both architectures use the same training data (demonstrations in uncluttered environments) as previous approaches. Experimental results show that ASOR-IA and ASOR-EA succeed ina significant fraction of trials in cluttered environments where previous approaches never succeed. In addition, we find that both ASOR-IA and ASOR-EA outperform previous approaches even in uncluttered environments, with ASOR-EA performing better even in clutter compared to the previous best baseline in an uncluttered environment.



### Anchor Loss: Modulating Loss Scale based on Prediction Difficulty
- **Arxiv ID**: http://arxiv.org/abs/1909.11155v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.11155v1)
- **Published**: 2019-09-24 20:01:36+00:00
- **Updated**: 2019-09-24 20:01:36+00:00
- **Authors**: Serim Ryou, Seong-Gyun Jeong, Pietro Perona
- **Comment**: To appear in Proceedings of IEEE International Conference on Computer
  Vision (ICCV), 2019. (oral)
- **Journal**: None
- **Summary**: We propose a novel loss function that dynamically rescales the cross entropy based on prediction difficulty regarding a sample. Deep neural network architectures in image classification tasks struggle to disambiguate visually similar objects. Likewise, in human pose estimation symmetric body parts often confuse the network with assigning indiscriminative scores to them. This is due to the output prediction, in which only the highest confidence label is selected without taking into consideration a measure of uncertainty. In this work, we define the prediction difficulty as a relative property coming from the confidence score gap between positive and negative labels. More precisely, the proposed loss function penalizes the network to avoid the score of a false prediction being significant. To demonstrate the efficacy of our loss function, we evaluate it on two different domains: image classification and human pose estimation. We find improvements in both applications by achieving higher accuracy compared to the baseline methods.



### Intelligent image synthesis to attack a segmentation CNN using adversarial learning
- **Arxiv ID**: http://arxiv.org/abs/1909.11167v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.11167v1)
- **Published**: 2019-09-24 20:48:55+00:00
- **Updated**: 2019-09-24 20:48:55+00:00
- **Authors**: Liang Chen, Paul Bentley, Kensaku Mori, Kazunari Misawa, Michitaka Fujiwara, Daniel Rueckert
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning approaches based on convolutional neural networks (CNNs) have been successful in solving a number of problems in medical imaging, including image segmentation. In recent years, it has been shown that CNNs are vulnerable to attacks in which the input image is perturbed by relatively small amounts of noise so that the CNN is no longer able to perform a segmentation of the perturbed image with sufficient accuracy. Therefore, exploring methods on how to attack CNN-based models as well as how to defend models against attacks have become a popular topic as this also provides insights into the performance and generalization abilities of CNNs. However, most of the existing work assumes unrealistic attack models, i.e. the resulting attacks were specified in advance. In this paper, we propose a novel approach for generating adversarial examples to attack CNN-based segmentation models for medical images. Our approach has three key features: 1) The generated adversarial examples exhibit anatomical variations (in form of deformations) as well as appearance perturbations; 2) The adversarial examples attack segmentation models so that the Dice scores decrease by a pre-specified amount; 3) The attack is not required to be specified beforehand. We have evaluated our approach on CNN-based approaches for the multi-organ segmentation problem in 2D CT images. We show that the proposed approach can be used to attack different CNN-based segmentation models.



### Augmenting the Pathology Lab: An Intelligent Whole Slide Image Classification System for the Real World
- **Arxiv ID**: http://arxiv.org/abs/1909.11212v1
- **DOI**: 10.1038/s41598-020-59985-2
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/1909.11212v1)
- **Published**: 2019-09-24 22:26:44+00:00
- **Updated**: 2019-09-24 22:26:44+00:00
- **Authors**: Julianna D. Ianni, Rajath E. Soans, Sivaramakrishnan Sankarapandian, Ramachandra Vikas Chamarthi, Devi Ayyagari, Thomas G. Olsen, Michael J. Bonham, Coleman C. Stavish, Kiran Motaparthi, Clay J. Cockerell, Theresa A. Feeser, Jason B. Lee
- **Comment**: 23 pages, 5 figures
- **Journal**: Sci Rep 10, 3217 (2020)
- **Summary**: Standard of care diagnostic procedure for suspected skin cancer is microscopic examination of hematoxylin \& eosin stained tissue by a pathologist. Areas of high inter-pathologist discordance and rising biopsy rates necessitate higher efficiency and diagnostic reproducibility. We present and validate a deep learning system which classifies digitized dermatopathology slides into 4 categories. The system is developed using 5,070 images from a single lab, and tested on an uncurated set of 13,537 images from 3 test labs, using whole slide scanners manufactured by 3 different vendors. The system's use of deep-learning-based confidence scoring as a criterion to consider the result as accurate yields an accuracy of up to 98\%, and makes it adoptable in a real-world setting. Without confidence scoring, the system achieved an accuracy of 78\%. We anticipate that our deep learning system will serve as a foundation enabling faster diagnosis of skin cancer, identification of cases for specialist review, and targeted diagnostic classifications.



### Pretraining boosts out-of-domain robustness for pose estimation
- **Arxiv ID**: http://arxiv.org/abs/1909.11229v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.11229v2)
- **Published**: 2019-09-24 23:40:39+00:00
- **Updated**: 2020-11-12 18:46:51+00:00
- **Authors**: Alexander Mathis, Thomas Biasi, Steffen Schneider, Mert Yüksekgönül, Byron Rogers, Matthias Bethge, Mackenzie W. Mathis
- **Comment**: A.M. and T.B. co-first authors. Dataset available at http://horse10.
  deeplabcut.org . WACV 2021 conference
- **Journal**: https://openaccess.thecvf.com/content/WACV2021/html/Mathis_Pretraining_Boosts_Out-of-Domain_Robustness_for_Pose_Estimation_WACV_2021_paper.html
- **Summary**: Neural networks are highly effective tools for pose estimation. However, as in other computer vision tasks, robustness to out-of-domain data remains a challenge, especially for small training sets that are common for real-world applications. Here, we probe the generalization ability with three architecture classes (MobileNetV2s, ResNets, and EfficientNets) for pose estimation. We developed a dataset of 30 horses that allowed for both "within-domain" and "out-of-domain" (unseen horse) benchmarking - this is a crucial test for robustness that current human pose estimation benchmarks do not directly address. We show that better ImageNet-performing architectures perform better on both within- and out-of-domain data if they are first pretrained on ImageNet. We additionally show that better ImageNet models generalize better across animal species. Furthermore, we introduce Horse-C, a new benchmark for common corruptions for pose estimation, and confirm that pretraining increases performance in this domain shift context as well. Overall, our results demonstrate that transfer learning is beneficial for out-of-domain robustness.



### Carving out the low surface brightness universe with NoiseChisel
- **Arxiv ID**: http://arxiv.org/abs/1909.11230v2
- **DOI**: None
- **Categories**: **astro-ph.IM**, astro-ph.GA, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.11230v2)
- **Published**: 2019-09-24 23:42:19+00:00
- **Updated**: 2022-05-08 20:44:34+00:00
- **Authors**: Mohammad Akhlaghi
- **Comment**: Invited talk at IAU Symposium 355 (The Realm of the Low Surface
  Brightness Universe). The downloadable source (on arXiv) includes the full
  reproduction info in Maneage: http://maneage.org . It is also available with
  its Git history in https://gitlab.com/makhlaghi/iau-symposium-355 (archived
  in SoftwareHeritage), and in Zenodo at https://doi.org/10.5281/zenodo.6529419
- **Journal**: Proceedings of the International Astronomical Union (S355), 2020
- **Summary**: NoiseChisel is a program to detect very low signal-to-noise ratio (S/N) features with minimal assumptions on their morphology. It was introduced in 2015 and released within a collection of data analysis programs and libraries known as GNU Astronomy Utilities (Gnuastro). The 10th stable version of Gnuastro was released in August 2019 and NoiseChisel has significantly improved: detecting even fainter signal, enabling better user control over its inner workings, and many bug fixes. The most important change until version 0.10 is that NoiseChisel's segmentation features have been moved into a new program called Segment. Another major change is the final growth strategy of its true detections, for example NoiseChisel is able to detect the outer wings of M51 down to S/N of 0.25, or 25.97 mag/arcsec2 on a single-exposure SDSS image (r-band). Segment is also able to detect the localized HII regions as "clumps" much more successfully. For a detailed list of improvements after version 0.10, see the most recent manual. Finally, to orchestrate a controlled analysis, the concept of reproducibility is discussed: this paper itself is exactly reproducible (commit 751467d).



### Sign Language Recognition Analysis using Multimodal Data
- **Arxiv ID**: http://arxiv.org/abs/1909.11232v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.11232v1)
- **Published**: 2019-09-24 23:44:49+00:00
- **Updated**: 2019-09-24 23:44:49+00:00
- **Authors**: Al Amin Hosain, Panneer Selvam Santhalingam, Parth Pathak, Jana Kosecka, Huzefa Rangwala
- **Comment**: conference : IEEE DSAA, 2019, Washington DC
- **Journal**: None
- **Summary**: Voice-controlled personal and home assistants (such as the Amazon Echo and Apple Siri) are becoming increasingly popular for a variety of applications. However, the benefits of these technologies are not readily accessible to Deaf or Hard-ofHearing (DHH) users. The objective of this study is to develop and evaluate a sign recognition system using multiple modalities that can be used by DHH signers to interact with voice-controlled devices. With the advancement of depth sensors, skeletal data is used for applications like video analysis and activity recognition. Despite having similarity with the well-studied human activity recognition, the use of 3D skeleton data in sign language recognition is rare. This is because unlike activity recognition, sign language is mostly dependent on hand shape pattern. In this work, we investigate the feasibility of using skeletal and RGB video data for sign language recognition using a combination of different deep learning architectures. We validate our results on a large-scale American Sign Language (ASL) dataset of 12 users and 13107 samples across 51 signs. It is named as GMUASL51. We collected the dataset over 6 months and it will be publicly released in the hope of spurring further machine learning research towards providing improved accessibility for digital assistants.



