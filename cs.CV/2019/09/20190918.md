# Arxiv Papers in cs.CV on 2019-09-18
### CAMAL: Context-Aware Multi-layer Attention framework for Lightweight Environment Invariant Visual Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/1909.08153v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08153v2)
- **Published**: 2019-09-18 00:24:07+00:00
- **Updated**: 2020-08-13 15:15:53+00:00
- **Authors**: Ahmad Khaliq, Shoaib Ehsan, Michael Milford, Klaus McDonald-Maier
- **Comment**: under-review
- **Journal**: None
- **Summary**: In the last few years, Deep Convolutional Neural Networks (D-CNNs) have shown state-of-the-art (SOTA) performance for Visual Place Recognition (VPR), a pivotal component of long-term intelligent robotic vision (vision-aware localization and navigation systems). The prestigious generalization power of D-CNNs gained upon training on large scale places datasets and learned persistent image regions which are found to be robust for specific place recognition under changing conditions and camera viewpoints. However, against the computation and power intensive D-CNNs based VPR algorithms that are employed to determine the approximate location of resource-constrained mobile robots, lightweight VPR techniques are preferred. This paper presents a computation- and energy-efficient CAMAL framework that captures place-specific multi-layer convolutional attentions efficient for environment invariant-VPR. At 4x lesser power consumption, evaluating the proposed VPR framework on challenging benchmark place recognition datasets reveal better and comparable Area under Precision-Recall (AUC-PR) curves with approximately 4x improved image retrieval performance over the contemporary VPR methodologies.



### Dynamic Graph Attention for Referring Expression Comprehension
- **Arxiv ID**: http://arxiv.org/abs/1909.08164v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08164v1)
- **Published**: 2019-09-18 01:47:27+00:00
- **Updated**: 2019-09-18 01:47:27+00:00
- **Authors**: Sibei Yang, Guanbin Li, Yizhou Yu
- **Comment**: Accepted as an Oral presentation at ICCV2019
- **Journal**: None
- **Summary**: Referring expression comprehension aims to locate the object instance described by a natural language referring expression in an image. This task is compositional and inherently requires visual reasoning on top of the relationships among the objects in the image. Meanwhile, the visual reasoning process is guided by the linguistic structure of the referring expression. However, existing approaches treat the objects in isolation or only explore the first-order relationships between objects without being aligned with the potential complexity of the expression. Thus it is hard for them to adapt to the grounding of complex referring expressions. In this paper, we explore the problem of referring expression comprehension from the perspective of language-driven visual reasoning, and propose a dynamic graph attention network to perform multi-step reasoning by modeling both the relationships among the objects in the image and the linguistic structure of the expression. In particular, we construct a graph for the image with the nodes and edges corresponding to the objects and their relationships respectively, propose a differential analyzer to predict a language-guided visual reasoning process, and perform stepwise reasoning on top of the graph to update the compound object representation at every node. Experimental results demonstrate that the proposed method can not only significantly surpass all existing state-of-the-art algorithms across three common benchmark datasets, but also generate interpretable visual evidences for stepwisely locating the objects referred to in complex language descriptions.



### Multiple Human Tracking using Multi-Cues including Primitive Action Features
- **Arxiv ID**: http://arxiv.org/abs/1909.08171v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.08171v1)
- **Published**: 2019-09-18 02:14:38+00:00
- **Updated**: 2019-09-18 02:14:38+00:00
- **Authors**: Hitoshi Nishimura, Kazuyuki Tasaka, Yasutomo Kawanishi, Hiroshi Murase
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a Multiple Human Tracking method using multi-cues including Primitive Action Features (MHT-PAF). MHT-PAF can perform the accurate human tracking in dynamic aerial videos captured by a drone. PAF employs a global context, rich information by multi-label actions, and a middle level feature. The accurate human tracking result using PAF helps multi-frame-based action recognition. In the experiments, we verified the effectiveness of the proposed method using the Okutama-Action dataset. Our code is available online.



### Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.08174v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.08174v1)
- **Published**: 2019-09-18 02:28:56+00:00
- **Updated**: 2019-09-18 02:28:56+00:00
- **Authors**: Zhonghui You, Kun Yan, Jinmian Ye, Meng Ma, Ping Wang
- **Comment**: Accepted by NeurIPS'19
- **Journal**: None
- **Summary**: Filter pruning is one of the most effective ways to accelerate and compress convolutional neural networks (CNNs). In this work, we propose a global filter pruning algorithm called Gate Decorator, which transforms a vanilla CNN module by multiplying its output by the channel-wise scaling factors, i.e. gate. When the scaling factor is set to zero, it is equivalent to removing the corresponding filter. We use Taylor expansion to estimate the change in the loss function caused by setting the scaling factor to zero and use the estimation for the global filter importance ranking. Then we prune the network by removing those unimportant filters. After pruning, we merge all the scaling factors into its original module, so no special operations or structures are introduced. Moreover, we propose an iterative pruning framework called Tick-Tock to improve pruning accuracy. The extensive experiments demonstrate the effectiveness of our approaches. For example, we achieve the state-of-the-art pruning ratio on ResNet-56 by reducing 70% FLOPs without noticeable loss in accuracy. For ResNet-50 on ImageNet, our pruned model with 40% FLOPs reduction outperforms the baseline model by 0.31% in top-1 accuracy. Various datasets are used, including CIFAR-10, CIFAR-100, CUB-200, ImageNet ILSVRC-12 and PASCAL VOC 2011. Code is available at github.com/youzhonghui/gate-decorator-pruning



### Adaptive Graphical Model Network for 2D Handpose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1909.08205v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08205v2)
- **Published**: 2019-09-18 04:19:51+00:00
- **Updated**: 2023-04-28 04:08:29+00:00
- **Authors**: Deying Kong, Yifei Chen, Haoyu Ma, Xiangyi Yan, Xiaohui Xie
- **Comment**: 30th British Machine Vision Conference (BMVC)
- **Journal**: None
- **Summary**: In this paper, we propose a new architecture called Adaptive Graphical Model Network (AGMN) to tackle the task of 2D hand pose estimation from a monocular RGB image. The AGMN consists of two branches of deep convolutional neural networks for calculating unary and pairwise potential functions, followed by a graphical model inference module for integrating unary and pairwise potentials. Unlike existing architectures proposed to combine DCNNs with graphical models, our AGMN is novel in that the parameters of its graphical model are conditioned on and fully adaptive to individual input images. Experiments show that our approach outperforms the state-of-the-art method used in 2D hand keypoints estimation by a notable margin on two public datasets. Code can be found at https://github.com/deyingk/agmn.



### Sample-specific repetitive learning for photo aesthetic assessment and highlight region extraction
- **Arxiv ID**: http://arxiv.org/abs/1909.08213v1
- **DOI**: 10.1007/s11042-020-09426-z
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08213v1)
- **Published**: 2019-09-18 05:20:56+00:00
- **Updated**: 2019-09-18 05:20:56+00:00
- **Authors**: Ying Dai
- **Comment**: 15 pages, 9 figures, 3 tables
- **Journal**: Multimedia tools and application, 2020
- **Summary**: Aesthetic assessment is subjective, and the distribution of the aesthetic levels is imbalanced. In order to realize the auto-assessment of photo aesthetics, we focus on retraining the CNN-based aesthetic assessment model by dropping out the unavailable samples in the middle levels from the training data set repetitively to overcome the effect of imbalanced aesthetic data on classification. Further, the method of extracting aesthetics highlight region of the photo image by using the two repetitively trained models is presented. Therefore, the correlation of the extracted region with the aesthetic levels is analyzed to illustrate what aesthetics features influence the aesthetic quality of the photo. Moreover, the testing data set is from the different data source called 500px. Experimental results show that the proposed method is effective.



### CrackGAN: Pavement Crack Detection Using Partially Accurate Ground Truths Based on Generative Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.08216v2
- **DOI**: 10.1109/TITS.2020.2990703
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.08216v2)
- **Published**: 2019-09-18 05:52:08+00:00
- **Updated**: 2020-06-26 16:38:28+00:00
- **Authors**: Kaige Zhang, Yingtao Zhang, Heng-Da Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Fully convolutional network is a powerful tool for per-pixel semantic segmentation/detection. However, it is problematic when coping with crack detection using partially accurate ground truths (GTs): the network may easily converge to the status that treats all the pixels as background (BG) and still achieves a very good loss, named "All Black" phenomenon, due to the unavailability of accurate GTs and the data imbalance. To tackle this problem, we propose crack-patch-only (CPO) supervised generative adversarial learning for end-to-end training, which forces the network to always produce crack-GT images while reserves both crack and BG-image translation abilities by feeding a larger-size crack image into an asymmetric U-shape generator to overcome the "All Black" issue. The proposed approach is validated using four crack datasets; and achieves state-of-the-art performance comparing with that of the recently published works in efficiency and accuracy.



### Diversified Arbitrary Style Transfer via Deep Feature Perturbation
- **Arxiv ID**: http://arxiv.org/abs/1909.08223v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08223v3)
- **Published**: 2019-09-18 06:24:42+00:00
- **Updated**: 2020-03-20 10:21:57+00:00
- **Authors**: Zhizhong Wang, Lei Zhao, Haibo Chen, Lihong Qiu, Qihang Mo, Sihuan Lin, Wei Xing, Dongming Lu
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: Image style transfer is an underdetermined problem, where a large number of solutions can satisfy the same constraint (the content and style). Although there have been some efforts to improve the diversity of style transfer by introducing an alternative diversity loss, they have restricted generalization, limited diversity and poor scalability. In this paper, we tackle these limitations and propose a simple yet effective method for diversified arbitrary style transfer. The key idea of our method is an operation called deep feature perturbation (DFP), which uses an orthogonal random noise matrix to perturb the deep image feature maps while keeping the original style information unchanged. Our DFP operation can be easily integrated into many existing WCT (whitening and coloring transform)-based methods, and empower them to generate diverse results for arbitrary styles. Experimental results demonstrate that this learning-free and universal method can greatly increase the diversity while maintaining the quality of stylization.



### Memory-Efficient Hierarchical Neural Architecture Search for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1909.08228v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08228v3)
- **Published**: 2019-09-18 06:49:19+00:00
- **Updated**: 2020-02-07 04:41:32+00:00
- **Authors**: Haokui Zhang, Ying Li, Hao Chen, Chunhua Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, neural architecture search (NAS) methods have attracted much attention and outperformed manually designed architectures on a few high-level vision tasks. In this paper, we propose HiNAS (Hierarchical NAS), an effort towards employing NAS to automatically design effective neural network architectures for image denoising. HiNAS adopts gradient based search strategies and employs operations with adaptive receptive field to build an flexible hierarchical search space. During the search stage, HiNAS shares cells across different feature levels to save memory and employ an early stopping strategy to avoid the collapse issue in NAS, and considerably accelerate the search speed. The proposed HiNAS is both memory and computation efficient, which takes only about 4.5 hours for searching using a single GPU. We evaluate the effectiveness of our proposed HiNAS on two different datasets, namely an additive white Gaussian noise dataset BSD500, and a realistic noise dataset SIM1800. Experimental results show that the architecture found by HiNAS has fewer parameters and enjoys a faster inference speed, while achieving highly competitive performance compared with state-of-the-art methods. We also present analysis on the architectures found by NAS. HiNAS also shows good performance on experiments for image de-raining.



### Towards Shape Biased Unsupervised Representation Learning for Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/1909.08245v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08245v2)
- **Published**: 2019-09-18 07:07:37+00:00
- **Updated**: 2020-03-29 14:37:29+00:00
- **Authors**: Nader Asadi, Amir M. Sarfi, Mehrdad Hosseinzadeh, Zahra Karimpour, Mahdi Eftekhari
- **Comment**: Under review
- **Journal**: None
- **Summary**: It is known that, without awareness of the process, our brain appears to focus on the general shape of objects rather than superficial statistics of context. On the other hand, learning autonomously allows discovering invariant regularities which help generalization. In this work, we propose a learning framework to improve the shape bias property of self-supervised methods. Our method learns semantic and shape biased representations by integrating domain diversification and jigsaw puzzles. The first module enables the model to create a dynamic environment across arbitrary domains and provides a domain exploration vs. exploitation trade-off, while the second module allows the model to explore this environment autonomously. This universal framework does not require prior knowledge of the domain of interest. Extensive experiments are conducted on several domain generalization datasets, namely, PACS, Office-Home, VLCS, and Digits. We show that our framework outperforms state-of-the-art domain generalization methods by a large margin.



### Class Feature Pyramids for Video Explanation
- **Arxiv ID**: http://arxiv.org/abs/1909.08611v1
- **DOI**: 10.1109/ICCVW.2019.00524
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.08611v1)
- **Published**: 2019-09-18 07:10:32+00:00
- **Updated**: 2019-09-18 07:10:32+00:00
- **Authors**: Alexandros Stergiou, Georgios Kapidis, Grigorios Kalliatakis, Christos Chrysoulas, Ronald Poppe, Remco Veltkamp
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional networks are widely used in video action recognition. 3D convolutions are one prominent approach to deal with the additional time dimension. While 3D convolutions typically lead to higher accuracies, the inner workings of the trained models are more difficult to interpret. We focus on creating human-understandable visual explanations that represent the hierarchical parts of spatio-temporal networks. We introduce Class Feature Pyramids, a method that traverses the entire network structure and incrementally discovers kernels at different network depths that are informative for a specific class. Our method does not depend on the network's architecture or the type of 3D convolutions, supporting grouped and depth-wise convolutions, convolutions in fibers, and convolutions in branches. We demonstrate the method on six state-of-the-art 3D convolution neural networks (CNNs) on three action recognition (Kinetics-400, UCF-101, and HMDB-51) and two egocentric action recognition datasets (EPIC-Kitchens and EGTEA Gaze+).



### Exploring Reciprocal Attention for Salient Object Detection by Cooperative Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.08269v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08269v1)
- **Published**: 2019-09-18 07:26:58+00:00
- **Updated**: 2019-09-18 07:26:58+00:00
- **Authors**: Changqun Xia, Jia Li, Jinming Su, Yonghong Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Typically, objects with the same semantics are not always prominent in images containing different backgrounds. Motivated by this observation that accurately salient object detection is related to both foreground and background, we proposed a novel cooperative attention mechanism that jointly considers reciprocal relationships between background and foreground for efficient salient object detection. Concretely, we first aggregate the features at each side-out of traditional dilated FCN to extract the initial foreground and background local responses respectively. Then taking these responses as input, reciprocal attention module adaptively models the nonlocal dependencies between any two pixels of the foreground and background features, which is then aggregated with local features in a mutual reinforced way so as to enhance each branch to generate more discriminative foreground and background saliency map. Besides, cooperative losses are particularly designed to guide the multi-task learning of foreground and background branches, which encourages our network to obtain more complementary predictions with clear boundaries. At last, a simple but effective fusion strategy is utilized to produce the final saliency map. Comprehensive experimental results on five benchmark datasets demonstrate that our proposed method performs favorably against the state-of-the-art approaches in terms of all compared evaluation metrics.



### Global Temporal Representation based CNNs for Infrared Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1909.08287v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08287v1)
- **Published**: 2019-09-18 08:52:35+00:00
- **Updated**: 2019-09-18 08:52:35+00:00
- **Authors**: Yang Liu, Zhaoyang Lu, Jing Li, Tao Yang, Chao Yao
- **Comment**: Published in IEEE Signal Processing Letters, codes can be found at
  https://yangliu9208.github.io/TSTDDs/
- **Journal**: None
- **Summary**: Infrared human action recognition has many advantages, i.e., it is insensitive to illumination change, appearance variability, and shadows. Existing methods for infrared action recognition are either based on spatial or local temporal information, however, the global temporal information, which can better describe the movements of body parts across the whole video, is not considered. In this letter, we propose a novel global temporal representation named optical-flow stacked difference image (OFSDI) and extract robust and discriminative feature from the infrared action data by considering the local, global, and spatial temporal information together. Due to the small size of the infrared action dataset, we first apply convolutional neural networks on local, spatial, and global temporal stream respectively to obtain efficient convolutional feature maps from the raw data rather than train a classifier directly. Then these convolutional feature maps are aggregated into effective descriptors named three-stream trajectory-pooled deep-convolutional descriptors by trajectory-constrained pooling. Furthermore, we improve the robustness of these features by using the locality-constrained linear coding (LLC) method. With these features, a linear support vector machine (SVM) is adopted to classify the action data in our scheme. We conduct the experiments on infrared action recognition datasets InfAR and NTU RGB+D. The experimental results show that the proposed approach outperforms the representative state-of-the-art handcrafted features and deep learning features based methods for the infrared action recognition.



### NatCSNN: A Convolutional Spiking Neural Network for recognition of objects extracted from natural images
- **Arxiv ID**: http://arxiv.org/abs/1909.08288v1
- **DOI**: 10.1007/978-3-030-30487-4_28
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.08288v1)
- **Published**: 2019-09-18 08:52:42+00:00
- **Updated**: 2019-09-18 08:52:42+00:00
- **Authors**: Pedro Machado, Georgina Cosma, T. M McGinnity
- **Comment**: 12 pages
- **Journal**: Artificial Neural Networks and Machine Learning - ICANN 2019:
  Theoretical Neural Computation. ICANN 2019. Lecture Notes in Computer
  Science, vol 11727
- **Summary**: Biological image processing is performed by complex neural networks composed of thousands of neurons interconnected via thousands of synapses, some of which are excitatory and others inhibitory. Spiking neural models are distinguished from classical neurons by being biological plausible and exhibiting the same dynamics as those observed in biological neurons. This paper proposes a Natural Convolutional Neural Network (NatCSNN) which is a 3-layer bio-inspired Convolutional Spiking Neural Network (CSNN), for classifying objects extracted from natural images. A two-stage training algorithm is proposed using unsupervised Spike Timing Dependent Plasticity (STDP) learning (phase 1) and ReSuMe supervised learning (phase 2). The NatCSNN was trained and tested on the CIFAR-10 dataset and achieved an average testing accuracy of 84.7% which is an improvement over the 2-layer neural networks previously applied to this dataset.



### SalsaNet: Fast Road and Vehicle Segmentation in LiDAR Point Clouds for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1909.08291v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.08291v1)
- **Published**: 2019-09-18 08:57:20+00:00
- **Updated**: 2019-09-18 08:57:20+00:00
- **Authors**: Eren Erdal Aksoy, Saimir Baci, Selcuk Cavdar
- **Comment**: None
- **Journal**: IEEE Intelligent Vehicles Symposium (IV) 2020. The code can be
  found here: https://gitlab.com/aksoyeren/salsanet
- **Summary**: In this paper, we introduce a deep encoder-decoder network, named SalsaNet, for efficient semantic segmentation of 3D LiDAR point clouds. SalsaNet segments the road, i.e. drivable free-space, and vehicles in the scene by employing the Bird-Eye-View (BEV) image projection of the point cloud. To overcome the lack of annotated point cloud data, in particular for the road segments, we introduce an auto-labeling process which transfers automatically generated labels from the camera to LiDAR. We also explore the role of imagelike projection of LiDAR data in semantic segmentation by comparing BEV with spherical-front-view projection and show that SalsaNet is projection-agnostic. We perform quantitative and qualitative evaluations on the KITTI dataset, which demonstrate that the proposed SalsaNet outperforms other state-of-the-art semantic segmentation networks in terms of accuracy and computation time. Our code and data are publicly available at https://gitlab.com/aksoyeren/salsanet.git.



### Transferable Feature Representation for Visible-to-Infrared Cross-Dataset Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1909.08297v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08297v1)
- **Published**: 2019-09-18 09:10:05+00:00
- **Updated**: 2019-09-18 09:10:05+00:00
- **Authors**: Yang Liu, Zhaoyang Lu, Jing Li, Chao Yao, Yanzi Deng
- **Comment**: Published in Complexity(JCR Rank 1, IF:4.621),codes can be found at
  https://yangliu9208.github.io/CDFAG/
- **Journal**: None
- **Summary**: Recently, infrared human action recognition has attracted increasing attention for it has many advantages over visible light, that is, being robust to illumination change and shadows. However, the infrared action data is limited until now, which degrades the performance of infrared action recognition. Motivated by the idea of transfer learning, an infrared human action recognition framework using auxiliary data from visible light is proposed to solve the problem of limited infrared action data. In the proposed framework, we first construct a novel Cross-Dataset Feature Alignment and Generalization (CDFAG) framework to map the infrared data and visible light data into a common feature space, where Kernel Manifold Alignment (KEMA) and a dual alignedto-generalized encoders (AGE) model are employed to represent the feature. Then, a support vector machine (SVM) is trained, using both the infrared data and visible light data, and can classify the features derived from infrared data. The proposed method is evaluated on InfAR, which is a publicly available infrared human action dataset. To build up auxiliary data, we set up a novel visible light action dataset XD145. Experimental results show that the proposed method can achieve state-of-the-art performance compared with several transfer learning and domain adaptation methods.



### Large e-retailer image dataset for visual search and product classification
- **Arxiv ID**: http://arxiv.org/abs/1909.08612v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.08612v1)
- **Published**: 2019-09-18 09:21:53+00:00
- **Updated**: 2019-09-18 09:21:53+00:00
- **Authors**: Arnaud Bellétoile
- **Comment**: 5 pages, 4 figures, 4 tables
- **Journal**: None
- **Summary**: Recent results of deep convolutional networks in visual recognition challenges open the path to a whole new set of disruptive user experiences such as visual search or recommendation. The list of companies offering this type of service is growing everyday but the adoption rate and the relevancy of results may vary a lot. We believe that the availability of large and diverse datasets is a necessary condition to improve the relevancy of such recommendation systems and facilitate their adoption. For that purpose, we wish to share with the community this dataset of more than 12M images of the 7M products of our online store classified into 5K categories. This original dataset is introduced in this article and several features are described. We also present some aspects of the winning solutions of our image classification challenge that was organized on the Kaggle platform around this set of images.



### Unsupervised Sketch-to-Photo Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1909.08313v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08313v3)
- **Published**: 2019-09-18 09:33:10+00:00
- **Updated**: 2020-03-23 15:06:33+00:00
- **Authors**: Runtao Liu, Qian Yu, Stella Yu
- **Comment**: 23 pages, 15 figures. URL:
  https://github.com/rt219/Unpaired-Sketch-to-Photo-Translation
- **Journal**: None
- **Summary**: Humans can envision a realistic photo given a free-hand sketch that is not only spatially imprecise and geometrically distorted but also without colors and visual details. We study unsupervised sketch-to-photo synthesis for the first time, learning from unpaired sketch-photo data where the target photo for a sketch is unknown during training. Existing works only deal with style change or spatial deformation alone, synthesizing photos from edge-aligned line drawings or transforming shapes within the same modality, e.g., color images. Our key insight is to decompose unsupervised sketch-to-photo synthesis into a two-stage translation task: First shape translation from sketches to grayscale photos and then content enrichment from grayscale to color photos. We also incorporate a self-supervised denoising objective and an attention module to handle abstraction and style variations that are inherent and specific to sketches. Our synthesis is sketch-faithful and photo-realistic to enable sketch-based image retrieval in practice. An exciting corollary product is a universal and promising sketch generator that captures human visual perception beyond the edge map of a photo.



### Bayesian Strategies for Likelihood Ratio Computation in Forensic Voice Comparison with Automatic Systems
- **Arxiv ID**: http://arxiv.org/abs/1909.08315v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.08315v1)
- **Published**: 2019-09-18 09:39:41+00:00
- **Updated**: 2019-09-18 09:39:41+00:00
- **Authors**: Daniel Ramos, Juan Maroñas, Alicia Lozano-Diez
- **Comment**: None
- **Journal**: None
- **Summary**: This paper explores several strategies for Forensic Voice Comparison (FVC), aimed at improving the performance of the LRs when using generative Gaussian score-to-LR models. First, different anchoring strategies are proposed, with the objective of adapting the LR computation process to the case at hand, always respecting the propositions defined for the particular case. Second, a fully-Bayesian Gaussian model is used to tackle the sparsity in the training scores that is often present when the proposed anchoring strategies are used. Experiments are performed using the 2014 i-Vector challenge set-up, which presents high variability in a telephone speech context. The results show that the proposed fully-Bayesian model clearly outperforms a more common Maximum-Likelihood approach, leading to high robustness when the scores to train the model become sparse.



### Quantitative Impact of Label Noise on the Quality of Segmentation of Brain Tumors on MRI scans
- **Arxiv ID**: http://arxiv.org/abs/1909.08959v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.08959v1)
- **Published**: 2019-09-18 09:48:58+00:00
- **Updated**: 2019-09-18 09:48:58+00:00
- **Authors**: Michał Marcinkiewicz, Grzegorz Mrukwa
- **Comment**: None
- **Journal**: None
- **Summary**: Over the last few years, deep learning has proven to be a great solution to many problems, such as image or text classification. Recently, deep learning-based solutions have outperformed humans on selected benchmark datasets, yielding a promising future for scientific and real-world applications. Training of deep learning models requires vast amounts of high quality data to achieve such supreme performance. In real-world scenarios, obtaining a large, coherent, and properly labeled dataset is a challenging task. This is especially true in medical applications, where high-quality data and annotations are scarce and the number of expert annotators is limited. In this paper, we investigate the impact of corrupted ground-truth masks on the performance of a neural network for a brain tumor segmentation task. Our findings suggest that a) the performance degrades about 8% less than it could be expected from simulations, b) a neural network learns the simulated biases of annotators, c) biases can be partially mitigated by using an inversely-biased dice loss function.



### A Survey on Rain Removal from Video and Single Image
- **Arxiv ID**: http://arxiv.org/abs/1909.08326v2
- **DOI**: 10.1007/s11432-020-3225-9
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.08326v2)
- **Published**: 2019-09-18 10:05:24+00:00
- **Updated**: 2019-10-03 16:18:32+00:00
- **Authors**: Hong Wang, Yichen Wu, Minghan Li, Qian Zhao, Deyu Meng
- **Comment**: None
- **Journal**: SCIENCE CHINA Information Sciences 2021
- **Summary**: Rain streaks might severely degenerate the performance of video/image processing tasks. The investigations on rain removal from video or a single image has thus been attracting much research attention in the field of computer vision and pattern recognition, and various methods have been proposed against this task in the recent years. However, there is still not a comprehensive survey paper to summarize current rain removal methods and fairly compare their generalization performance, and especially, still not a off-the-shelf toolkit to accumulate recent representative methods for easy performance comparison and capability evaluation. Aiming at this meaningful task, in this study we present a comprehensive review for current rain removal methods for video and a single image. Specifically, these methods are categorized into model-driven and data-driven approaches, and more elaborate branches of each approach are further introduced. Intrinsic capabilities, especially generalization, of representative state-of-the-art methods of each approach have been evaluated and analyzed by experiments implemented on synthetic and real data both visually and quantitatively. Furthermore, we release a comprehensive repository, including direct links to 74 rain removal papers, source codes of 9 methods for video rain removal and 20 ones for single image rain removal, 19 related project pages, 6 synthetic datasets and 4 real ones, and 4 commonly used image quality metrics, to facilitate reproduction and performance comparison of current existing methods for general users. Some limitations and research issues worthy to be further investigated have also been discussed for future research of this direction.



### Corporate IT-support Help-Desk Process Hybrid-Automation Solution with Machine Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/1909.09018v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.09018v1)
- **Published**: 2019-09-18 10:07:01+00:00
- **Updated**: 2019-09-18 10:07:01+00:00
- **Authors**: Kuruparan Shanmugalingam, Nisal Chandrasekara, Calvin Hindle, Gihan Fernando, Chanaka Gunawardhana
- **Comment**: 7 pages, 8 Figures, 2 Tables
- **Journal**: The International Conference on Digital Image Computing:
  Techniques and Applications (DICTA) 2019
- **Summary**: Comprehensive IT support teams in large scale organizations require more man power for handling engagement and requests of employees from different channels on a 24*7 basis. Automated email technical queries help desk is proposed to have instant real-time quick solutions and email categorisation. Email topic modelling with various machine learning, deep-learning approaches are compared with different features for a scalable, generalised solution along with sure-shot static rules. Email's title, body, attachment, OCR text, and some feature engineered custom features are given as input elements. XGBoost cascaded hierarchical models, Bi-LSTM model with word embeddings perform well showing 77.3 overall accuracy For the real world corporate email data set. By introducing the thresholding techniques, the overall automation system architecture provides 85.6 percentage of accuracy for real world corporate emails. Combination of quick fixes, static rules, ML categorization as a low cost inference solution reduces 81 percentage of the human effort in the process of automation and real time implementation.



### Probabilistic Atlases to Enforce Topological Constraints
- **Arxiv ID**: http://arxiv.org/abs/1909.08330v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08330v1)
- **Published**: 2019-09-18 10:10:03+00:00
- **Updated**: 2019-09-18 10:10:03+00:00
- **Authors**: Udaranga Wickramasinghe, Graham Knott, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: Probabilistic atlases (PAs) have long been used in standard segmentation approaches and, more recently, in conjunction with Convolutional Neural Networks (CNNs). However, their use has been restricted to relatively standardized structures such as the brain or heart which have limited or predictable range of deformations. Here we propose an encoding-decoding CNN architecture that can exploit rough atlases that encode only the topology of the target structures that can appear in any pose and have arbitrarily complex shapes to improve the segmentation results. It relies on the output of the encoder to compute both the pose parameters used to deform the atlas and the segmentation mask itself, which makes it effective and end-to-end trainable.



### A continual learning survey: Defying forgetting in classification tasks
- **Arxiv ID**: http://arxiv.org/abs/1909.08383v3
- **DOI**: 10.1109/TPAMI.2021.3057446
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.08383v3)
- **Published**: 2019-09-18 12:07:36+00:00
- **Updated**: 2021-04-16 17:53:39+00:00
- **Authors**: Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, Tinne Tuytelaars
- **Comment**: Accepted TPAMI paper, including Appendix, code publicly available
- **Journal**: None
- **Summary**: Artificial neural networks thrive in solving the classification problem for a particular rigid task, acquiring knowledge through generalized learning behaviour from a distinct training phase. The resulting network resembles a static entity of knowledge, with endeavours to extend this knowledge without targeting the original task resulting in a catastrophic forgetting. Continual learning shifts this paradigm towards networks that can continually accumulate knowledge over different tasks without the need to retrain from scratch. We focus on task incremental classification, where tasks arrive sequentially and are delineated by clear boundaries. Our main contributions concern 1) a taxonomy and extensive overview of the state-of-the-art, 2) a novel framework to continually determine the stability-plasticity trade-off of the continual learner, 3) a comprehensive experimental comparison of 11 state-of-the-art continual learning methods and 4 baselines. We empirically scrutinize method strengths and weaknesses on three benchmarks, considering Tiny Imagenet and large-scale unbalanced iNaturalist and a sequence of recognition datasets. We study the influence of model capacity, weight decay and dropout regularization, and the order in which the tasks are presented, and qualitatively compare methods in terms of required memory, computation time, and storage.



### Semantically Interpretable Activation Maps: what-where-how explanations within CNNs
- **Arxiv ID**: http://arxiv.org/abs/1909.08442v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08442v1)
- **Published**: 2019-09-18 13:34:34+00:00
- **Updated**: 2019-09-18 13:34:34+00:00
- **Authors**: Diego Marcos, Sylvain Lobry, Devis Tuia
- **Comment**: 2019 ICCV Workshop on Interpreting and Explaining Visual Artificial
  Intelligence Models
- **Journal**: None
- **Summary**: A main issue preventing the use of Convolutional Neural Networks (CNN) in end user applications is the low level of transparency in the decision process. Previous work on CNN interpretability has mostly focused either on localizing the regions of the image that contribute to the result or on building an external model that generates plausible explanations. However, the former does not provide any semantic information and the latter does not guarantee the faithfulness of the explanation. We propose an intermediate representation composed of multiple Semantically Interpretable Activation Maps (SIAM) indicating the presence of predefined attributes at different locations of the image. These attribute maps are then linearly combined to produce the final output. This gives the user insight into what the model has seen, where, and a final output directly linked to this information in a comprehensive and interpretable way. We test the method on the task of landscape scenicness (aesthetic value) estimation, using an intermediate representation of 33 attributes from the SUN Attributes database. The results confirm that SIAM makes it possible to understand what attributes in the image are contributing to the final score and where they are located. Since it is based on learning from multiple tasks and datasets, SIAM improve the explanability of the prediction without additional annotation efforts or computational overhead at inference time, while keeping good performances on both the final and intermediate tasks.



### Pose-aware Multi-level Feature Network for Human Object Interaction Detection
- **Arxiv ID**: http://arxiv.org/abs/1909.08453v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08453v1)
- **Published**: 2019-09-18 13:47:37+00:00
- **Updated**: 2019-09-18 13:47:37+00:00
- **Authors**: Bo Wan, Desen Zhou, Yongfei Liu, Rongjie Li, Xuming He
- **Comment**: International Conference on Computer Vision 2019
- **Journal**: None
- **Summary**: Reasoning human object interactions is a core problem in human-centric scene understanding and detecting such relations poses a unique challenge to vision systems due to large variations in human-object configurations, multiple co-occurring relation instances and subtle visual difference between relation categories. To address those challenges, we propose a multi-level relation detection strategy that utilizes human pose cues to capture global spatial configurations of relations and as an attention mechanism to dynamically zoom into relevant regions at human part level. Specifically, we develop a multi-branch deep network to learn a pose-augmented relation representation at three semantic levels, incorporating interaction context, object features and detailed semantic part cues. As a result, our approach is capable of generating robust predictions on fine-grained human object interactions with interpretable outputs. Extensive experimental evaluations on public benchmarks show that our model outperforms prior methods by a considerable margin, demonstrating its efficacy in handling complex scenes.



### Unsupervised Adaptation for Synthetic-to-Real Handwritten Word Recognition
- **Arxiv ID**: http://arxiv.org/abs/1909.08473v2
- **DOI**: 10.1109/WACV45572.2020.9093392
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08473v2)
- **Published**: 2019-09-18 14:32:41+00:00
- **Updated**: 2020-05-26 21:15:08+00:00
- **Authors**: Lei Kang, Marçal Rusiñol, Alicia Fornés, Pau Riba, Mauricio Villegas
- **Comment**: Accepted to WACV 2020
- **Journal**: None
- **Summary**: Handwritten Text Recognition (HTR) is still a challenging problem because it must deal with two important difficulties: the variability among writing styles, and the scarcity of labelled data. To alleviate such problems, synthetic data generation and data augmentation are typically used to train HTR systems. However, training with such data produces encouraging but still inaccurate transcriptions in real words. In this paper, we propose an unsupervised writer adaptation approach that is able to automatically adjust a generic handwritten word recognizer, fully trained with synthetic fonts, towards a new incoming writer. We have experimentally validated our proposal using five different datasets, covering several challenges (i) the document source: modern and historic samples, which may involve paper degradation problems; (ii) different handwriting styles: single and multiple writer collections; and (iii) language, which involves different character combinations. Across these challenging collections, we show that our system is able to maintain its performance, thus, it provides a practical and generic approach to deal with new document collections without requiring any expensive and tedious manual annotation step.



### Grid Anchor based Image Cropping: A New Benchmark and An Efficient Model
- **Arxiv ID**: http://arxiv.org/abs/1909.08989v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08989v1)
- **Published**: 2019-09-18 14:41:42+00:00
- **Updated**: 2019-09-18 14:41:42+00:00
- **Authors**: Hui Zeng, Lida Li, Zisheng Cao, Lei Zhang
- **Comment**: Extension of a CVPR 2019 paper. Dataset and PyTorch Code are
  released. arXiv admin note: substantial text overlap with arXiv:1904.04441
- **Journal**: None
- **Summary**: Image cropping aims to improve the composition as well as aesthetic quality of an image by removing extraneous content from it. Most of the existing image cropping databases provide only one or several human-annotated bounding boxes as the groundtruths, which can hardly reflect the non-uniqueness and flexibility of image cropping in practice. The employed evaluation metrics such as intersection-over-union cannot reliably reflect the real performance of a cropping model, either. This work revisits the problem of image cropping, and presents a grid anchor based formulation by considering the special properties and requirements (e.g., local redundancy, content preservation, aspect ratio) of image cropping. Our formulation reduces the searching space of candidate crops from millions to no more than ninety. Consequently, a grid anchor based cropping benchmark is constructed, where all crops of each image are annotated and more reliable evaluation metrics are defined. To meet the practical demands of robust performance and high efficiency, we also design an effective and lightweight cropping model. By simultaneously considering the region of interest and region of discard, and leveraging multi-scale information, our model can robustly output visually pleasing crops for images of different scenes. With less than 2.5M parameters, our model runs at a speed of 200 FPS on one single GTX 1080Ti GPU and 12 FPS on one i7-6800K CPU. The code is available at: \url{https://github.com/HuiZeng/Grid-Anchor-based-Image-Cropping-Pytorch}.



### Visual Tracking by means of Deep Reinforcement Learning and an Expert Demonstrator
- **Arxiv ID**: http://arxiv.org/abs/1909.08487v1
- **DOI**: 10.1109/ICCVW.2019.00282
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08487v1)
- **Published**: 2019-09-18 14:55:02+00:00
- **Updated**: 2019-09-18 14:55:02+00:00
- **Authors**: Matteo Dunnhofer, Niki Martinel, Gian Luca Foresti, Christian Micheloni
- **Comment**: in 2019 IEEE/CVF International Conference on Computer Vision
  Workshops (ICCVW) - VOT2019 Challenge Workshop
- **Journal**: None
- **Summary**: In the last decade many different algorithms have been proposed to track a generic object in videos. Their execution on recent large-scale video datasets can produce a great amount of various tracking behaviours. New trends in Reinforcement Learning showed that demonstrations of an expert agent can be efficiently used to speed-up the process of policy learning. Taking inspiration from such works and from the recent applications of Reinforcement Learning to visual tracking, we propose two novel trackers, A3CT, which exploits demonstrations of a state-of-the-art tracker to learn an effective tracking policy, and A3CTD, that takes advantage of the same expert tracker to correct its behaviour during tracking. Through an extensive experimental validation on the GOT-10k, OTB-100, LaSOT, UAV123 and VOT benchmarks, we show that the proposed trackers achieve state-of-the-art performance while running in real-time.



### Student Engagement Detection Using Emotion Analysis, Eye Tracking and Head Movement with Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.12913v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.12913v5)
- **Published**: 2019-09-18 15:46:48+00:00
- **Updated**: 2023-03-23 16:43:29+00:00
- **Authors**: Prabin Sharma, Shubham Joshi, Subash Gautam, Sneha Maharjan, Salik Ram Khanal, Manuel Cabral Reis, João Barroso, Vítor Manuel de Jesus Filipe
- **Comment**: 9 pages, 9 Figures, 2 tables
- **Journal**: None
- **Summary**: With the increase of distance learning, in general, and e-learning, in particular, having a system capable of determining the engagement of students is of primordial importance, and one of the biggest challenges, both for teachers, researchers and policy makers. Here, we present a system to detect the engagement level of the students. It uses only information provided by the typical built-in web-camera present in a laptop computer, and was designed to work in real time. We combine information about the movements of the eyes and head, and facial emotions to produce a concentration index with three classes of engagement: "very engaged", "nominally engaged" and "not engaged at all". The system was tested in a typical e-learning scenario, and the results show that it correctly identifies each period of time where students were "very engaged", "nominally engaged" and "not engaged at all". Additionally, the results also show that the students with best scores also have higher concentration indexes.



### k-Relevance Vectors: Considering Relevancy Beside Nearness
- **Arxiv ID**: http://arxiv.org/abs/1909.08528v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.08528v2)
- **Published**: 2019-09-18 16:03:35+00:00
- **Updated**: 2021-03-07 21:34:57+00:00
- **Authors**: Sara Hosseinzadeh Kassani, Farhood Rismanchian, Peyman Hosseinzadeh Kassani
- **Comment**: Will be submitted to Applied Soft Computing Journal
- **Journal**: None
- **Summary**: This study combines two different learning paradigms, k-nearest neighbor (k-NN) rule, as memory-based learning paradigm and relevance vector machines (RVM), as statistical learning paradigm. This combination is performed in kernel space and is called k-relevance vector (k-RV). The purpose is to improve the performance of k-NN rule. The proposed model significantly prunes irrelevant attributes. We also introduced a new parameter, responsible for early stopping of iterations in RVM. We show that the new parameter improves the classification accuracy of k-RV. Intensive experiments are conducted on several classification datasets from University of California Irvine (UCI) repository and two real datasets from computer vision domain. The performance of k-RV is highly competitive compared to a few state-of-the-arts in terms of classification accuracy.



### Visual Measurement Integrity Monitoring for UAV Localization
- **Arxiv ID**: http://arxiv.org/abs/1909.08537v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.08537v1)
- **Published**: 2019-09-18 16:08:41+00:00
- **Updated**: 2019-09-18 16:08:41+00:00
- **Authors**: Chengyao Li, Steven L. Waslander
- **Comment**: Published in Safety, Security, and Rescue Robotics 2019
- **Journal**: None
- **Summary**: Unmanned aerial vehicles (UAVs) have increasingly been adopted for safety, security, and rescue missions, for which they need precise and reliable pose estimates relative to their environment. To ensure mission safety when relying on visual perception, it is essential to have an approach to assess the integrity of the visual localization solution. However, to the best of our knowledge, such an approach does not exist for optimization-based visual localization. Receiver autonomous integrity monitoring (RAIM) has been widely used in global navigation satellite systems (GNSS) applications such as automated aircraft landing. In this paper, we propose a novel approach inspired by RAIM to monitor the integrity of optimization-based visual localization and calculate the protection level of a state estimate, i.e. the largest possible translational error in each direction. We also propose a metric that quantitatively evaluates the performance of the error bounds. Finally, we validate the protection level using the EuRoC dataset and demonstrate that the proposed protection level provides a significantly more reliable bound than the commonly used $3\sigma$ method.



### Extremely Weak Supervised Image-to-Image Translation for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.08542v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08542v1)
- **Published**: 2019-09-18 16:09:41+00:00
- **Updated**: 2019-09-18 16:09:41+00:00
- **Authors**: Samarth Shukla, Luc Van Gool, Radu Timofte
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in generative models and adversarial training have led to a flourishing image-to-image (I2I) translation literature. The current I2I translation approaches require training images from the two domains that are either all paired (supervised) or all unpaired (unsupervised). In practice, obtaining paired training data in sufficient quantities is often very costly and cumbersome. Therefore solutions that employ unpaired data, while less accurate, are largely preferred. In this paper, we aim to bridge the gap between supervised and unsupervised I2I translation, with application to semantic image segmentation. We build upon pix2pix and CycleGAN, state-of-the-art seminal I2I translation techniques. We propose a method to select (very few) paired training samples and achieve significant improvements in both supervised and unsupervised I2I translation settings over random selection. Further, we boost the performance by incorporating both (selected) paired and unpaired samples in the training process. Our experiments show that an extremely weak supervised I2I translation solution using only one paired training sample can achieve a quantitative performance much better than the unsupervised CycleGAN model, and comparable to that of the supervised pix2pix model trained on thousands of pairs.



### Re-ID Driven Localization Refinement for Person Search
- **Arxiv ID**: http://arxiv.org/abs/1909.08580v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08580v1)
- **Published**: 2019-09-18 17:10:49+00:00
- **Updated**: 2019-09-18 17:10:49+00:00
- **Authors**: Chuchu Han, Jiacheng Ye, Yunshan Zhong, Xin Tan, Chi Zhang, Changxin Gao, Nong Sang
- **Comment**: 10 pages, 7 figures. Accepted by ICCV 2019
- **Journal**: None
- **Summary**: Person search aims at localizing and identifying a query person from a gallery of uncropped scene images. Different from person re-identification (re-ID), its performance also depends on the localization accuracy of a pedestrian detector. The state-of-the-art methods train the detector individually, and the detected bounding boxes may be sub-optimal for the following re-ID task. To alleviate this issue, we propose a re-ID driven localization refinement framework for providing the refined detection boxes for person search. Specifically, we develop a differentiable ROI transform layer to effectively transform the bounding boxes from the original images. Thus, the box coordinates can be supervised by the re-ID training other than the original detection task. With this supervision, the detector can generate more reliable bounding boxes, and the downstream re-ID model can produce more discriminative embeddings based on the refined person localizations. Extensive experimental results on the widely used benchmarks demonstrate that our proposed method performs favorably against the state-of-the-art person search methods.



### Unsupervised Segmentation of Fire and Smoke from Infra-Red Videos
- **Arxiv ID**: http://arxiv.org/abs/1909.12937v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.12937v1)
- **Published**: 2019-09-18 17:19:17+00:00
- **Updated**: 2019-09-18 17:19:17+00:00
- **Authors**: Meenu Ajith, Manel Martínez-Ramón
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a vision-based fire and smoke segmentation system which use spatial, temporal and motion information to extract the desired regions from the video frames. The fusion of information is done using multiple features such as optical flow, divergence and intensity values. These features extracted from the images are used to segment the pixels into different classes in an unsupervised way. A comparative analysis is done by using multiple clustering algorithms for segmentation. Here the Markov Random Field performs more accurately than other segmentation algorithms since it characterizes the spatial interactions of pixels using a finite number of parameters. It builds a probabilistic image model that selects the most likely labeling using the maximum a posteriori (MAP) estimation. This unsupervised approach is tested on various images and achieves a frame-wise fire detection rate of 95.39%. Hence this method can be used for early detection of fire in real-time and it can be incorporated into an indoor or outdoor surveillance system.



### Automated detection of oral pre-cancerous tongue lesions using deep learning for early diagnosis of oral cavity cancer
- **Arxiv ID**: http://arxiv.org/abs/1909.08987v1
- **DOI**: 10.13140/RG.2.2.28808.16643
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML, 68Txx
- **Links**: [PDF](http://arxiv.org/pdf/1909.08987v1)
- **Published**: 2019-09-18 17:35:18+00:00
- **Updated**: 2019-09-18 17:35:18+00:00
- **Authors**: Mohammed Zubair M. Shamim, Sadatullah Syed, Mohammad Shiblee, Mohammed Usman, Syed Ali
- **Comment**: 25 pages, 10 figures
- **Journal**: None
- **Summary**: Discovering oral cavity cancer (OCC) at an early stage is an effective way to increase patient survival rate. However, current initial screening process is done manually and is expensive for the average individual, especially in developing countries worldwide. This problem is further compounded due to the lack of specialists in such areas. Automating the initial screening process using artificial intelligence (AI) to detect pre-cancerous lesions can prove to be an effective and inexpensive technique that would allow patients to be triaged accordingly to receive appropriate clinical management. In this study, we have applied and evaluated the efficacy of six deep convolutional neural network (DCNN) models using transfer learning, for identifying pre-cancerous tongue lesions directly using a small data set of clinically annotated photographic images to diagnose early signs of OCC. DCNN model based on Vgg19 architecture was able to differentiate between benign and pre-cancerous tongue lesions with a mean classification accuracy of 0.98, sensitivity 0.89 and specificity 0.97. Additionally, the ResNet50 DCNN model was able to distinguish between five types of tongue lesions i.e. hairy tongue, fissured tongue, geographic tongue, strawberry tongue and oral hairy leukoplakia with a mean classification accuracy of 0.97. Preliminary results using an (AI+Physician) ensemble model demonstrate that an automated initial screening process of tongue lesions using DCNNs can achieve near-human level classification performance for diagnosing early signs of OCC in patients.



### Feature Pyramid Encoding Network for Real-time Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.08599v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08599v1)
- **Published**: 2019-09-18 17:40:06+00:00
- **Updated**: 2019-09-18 17:40:06+00:00
- **Authors**: Mengyu Liu, Hujun Yin
- **Comment**: Accepted to BMVC 2019
- **Journal**: None
- **Summary**: Although current deep learning methods have achieved impressive results for semantic segmentation, they incur high computational costs and have a huge number of parameters. For real-time applications, inference speed and memory usage are two important factors. To address the challenge, we propose a lightweight feature pyramid encoding network (FPENet) to make a good trade-off between accuracy and speed. Specifically, we use a feature pyramid encoding block to encode multi-scale contextual features with depthwise dilated convolutions in all stages of the encoder. A mutual embedding upsample module is introduced in the decoder to aggregate the high-level semantic features and low-level spatial details efficiently. The proposed network outperforms existing real-time methods with fewer parameters and improved inference speed on the Cityscapes and CamVid benchmark datasets. Specifically, FPENet achieves 68.0\% mean IoU on the Cityscapes test set with only 0.4M parameters and 102 FPS speed on an NVIDIA TITAN V GPU.



### Graduated Non-Convexity for Robust Spatial Perception: From Non-Minimal Solvers to Global Outlier Rejection
- **Arxiv ID**: http://arxiv.org/abs/1909.08605v4
- **DOI**: 10.1109/LRA.2020.2965893
- **Categories**: **cs.CV**, cs.RO, math.OC, 68T40, 74Pxx, 46N10, 65D19, I.2.9; G.1.6; I.4.5; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1909.08605v4)
- **Published**: 2019-09-18 17:50:04+00:00
- **Updated**: 2020-06-11 20:09:58+00:00
- **Authors**: Heng Yang, Pasquale Antonante, Vasileios Tzoumas, Luca Carlone
- **Comment**: 10 pages, 5 figures, published at IEEE Robotics and Automation
  Letters (RA-L), 2020, Best Paper Award in Robot Vision at ICRA 2020
- **Journal**: IEEE Robotics and Automation Letters (RA-L), 2020
- **Summary**: Semidefinite Programming (SDP) and Sums-of-Squares (SOS) relaxations have led to certifiably optimal non-minimal solvers for several robotics and computer vision problems. However, most non-minimal solvers rely on least-squares formulations, and, as a result, are brittle against outliers. While a standard approach to regain robustness against outliers is to use robust cost functions, the latter typically introduce other non-convexities, preventing the use of existing non-minimal solvers. In this paper, we enable the simultaneous use of non-minimal solvers and robust estimation by providing a general-purpose approach for robust global estimation, which can be applied to any problem where a non-minimal solver is available for the outlier-free case. To this end, we leverage the Black-Rangarajan duality between robust estimation and outlier processes (which has been traditionally applied to early vision problems), and show that graduated non-convexity (GNC) can be used in conjunction with non-minimal solvers to compute robust solutions, without requiring an initial guess. Although GNC's global optimality cannot be guaranteed, we demonstrate the empirical robustness of the resulting robust non-minimal solvers in applications, including point cloud and mesh registration, pose graph optimization, and image-based object pose estimation (also called shape alignment). Our solvers are robust to 70-80% of outliers, outperform RANSAC, are more accurate than specialized local solvers, and faster than specialized global solvers. We also propose the first certifiably optimal non-minimal solver for shape alignment using SOS relaxation.



### Simultaneous Segmentation and Recognition: Towards more accurate Ego Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/1909.08606v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.08606v1)
- **Published**: 2019-09-18 17:52:16+00:00
- **Updated**: 2019-09-18 17:52:16+00:00
- **Authors**: Tejo Chalasani, Aljosa Smolic
- **Comment**: Accepted at ICCV Workshop for Egocentric Perception, Interaction and
  Computing
- **Journal**: None
- **Summary**: Ego hand gestures can be used as an interface in AR and VR environments. While the context of an image is important for tasks like scene understanding, object recognition, image caption generation and activity recognition, it plays a minimal role in ego hand gesture recognition. An ego hand gesture used for AR and VR environments conveys the same information regardless of the background. With this idea in mind, we present our work on ego hand gesture recognition that produces embeddings from RBG images with ego hands, which are simultaneously used for ego hand segmentation and ego gesture recognition. To this extent, we achieved better recognition accuracy (96.9%) compared to the state of the art (92.2%) on the biggest ego hand gesture dataset available publicly. We present a gesture recognition deep neural network which recognises ego hand gestures from videos (videos containing a single gesture) by generating and recognising embeddings of ego hands from image sequences of varying length. We introduce the concept of simultaneous segmentation and recognition applied to ego hand gestures, present the network architecture, the training procedure and the results compared to the state of the art on the EgoGesture dataset



### Wasserstein Distance Based Domain Adaptation for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1909.08675v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08675v1)
- **Published**: 2019-09-18 20:00:31+00:00
- **Updated**: 2019-09-18 20:00:31+00:00
- **Authors**: Pengcheng Xu, Prudhvi Gurram, Gene Whipps, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present an adversarial unsupervised domain adaptation framework for object detection. Prior approaches utilize adversarial training based on cross entropy between the source and target domain distributions to learn a shared feature mapping that minimizes the domain gap. Here, we minimize the Wasserstein distance between the two distributions instead of cross entropy or Jensen-Shannon divergence to improve the stability of domain adaptation in high-dimensional feature spaces that are inherent to object detection task. Additionally, we remove the exact consistency constraint of the shared feature mapping between the source and target domains, so that the target feature mapping can be optimized independently, which is necessary in the case of significant domain gap. We empirically show that the proposed framework can mitigate domain shift in different scenarios, and provide improved target domain object detection performance.



### Deep Latent Space Learning for Cross-modal Mapping of Audio and Visual Signals
- **Arxiv ID**: http://arxiv.org/abs/1909.08685v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1909.08685v1)
- **Published**: 2019-09-18 20:18:44+00:00
- **Updated**: 2019-09-18 20:18:44+00:00
- **Authors**: Shah Nawaz, Muhammad Kamran Janjua, Ignazio Gallo, Arif Mahmood, Alessandro Calefati
- **Comment**: Accepted to DICTA 2019
- **Journal**: None
- **Summary**: We propose a novel deep training algorithm for joint representation of audio and visual information which consists of a single stream network (SSNet) coupled with a novel loss function to learn a shared deep latent space representation of multimodal information. The proposed framework characterizes the shared latent space by leveraging the class centers which helps to eliminate the need for pairwise or triplet supervision. We quantitatively and qualitatively evaluate the proposed approach on VoxCeleb, a benchmarks audio-visual dataset on a multitude of tasks including cross-modal verification, cross-modal matching, and cross-modal retrieval. State-of-the-art performance is achieved on cross-modal verification and matching while comparable results are observed on the remaining applications. Our experiments demonstrate the effectiveness of the technique for cross-modal biometric applications.



### VideoDP: A Universal Platform for Video Analytics with Differential Privacy
- **Arxiv ID**: http://arxiv.org/abs/1909.08729v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.08729v2)
- **Published**: 2019-09-18 22:36:40+00:00
- **Updated**: 2019-09-20 16:47:47+00:00
- **Authors**: Han Wang, Shangyu Xie, Yuan Hong
- **Comment**: None
- **Journal**: None
- **Summary**: Massive amounts of video data are ubiquitously generated in personal devices and dedicated video recording facilities. Analyzing such data would be extremely beneficial in real world (e.g., urban traffic analysis, pedestrian behavior analysis, video surveillance). However, videos contain considerable sensitive information, such as human faces, identities and activities. Most of the existing video sanitization techniques simply obfuscate the video by detecting and blurring the region of interests (e.g., faces, vehicle plates, locations and timestamps) without quantifying and bounding the privacy leakage in the sanitization. In this paper, to the best of our knowledge, we propose the first differentially private video analytics platform (VideoDP) which flexibly supports different video analyses with rigorous privacy guarantee. Different from traditional noise-injection based differentially private mechanisms, given the input video, VideoDP randomly generates a utility-driven private video in which adding or removing any sensitive visual element (e.g., human, object) does not significantly affect the output video. Then, different video analyses requested by untrusted video analysts can be flexibly performed over the utility-driven video while ensuring differential privacy. Finally, we conduct experiments on real videos, and the experimental results demonstrate that our VideoDP effectively functions video analytics with good utility.



