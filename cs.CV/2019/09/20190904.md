# Arxiv Papers in cs.CV on 2019-09-04
### Holistic++ Scene Understanding: Single-view 3D Holistic Scene Parsing and Human Pose Estimation with Human-Object Interaction and Physical Commonsense
- **Arxiv ID**: http://arxiv.org/abs/1909.01507v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.01507v1)
- **Published**: 2019-09-04 00:42:20+00:00
- **Updated**: 2019-09-04 00:42:20+00:00
- **Authors**: Yixin Chen, Siyuan Huang, Tao Yuan, Siyuan Qi, Yixin Zhu, Song-Chun Zhu
- **Comment**: Accepted by ICCV 2019
- **Journal**: None
- **Summary**: We propose a new 3D holistic++ scene understanding problem, which jointly tackles two tasks from a single-view image: (i) holistic scene parsing and reconstruction---3D estimations of object bounding boxes, camera pose, and room layout, and (ii) 3D human pose estimation. The intuition behind is to leverage the coupled nature of these two tasks to improve the granularity and performance of scene understanding. We propose to exploit two critical and essential connections between these two tasks: (i) human-object interaction (HOI) to model the fine-grained relations between agents and objects in the scene, and (ii) physical commonsense to model the physical plausibility of the reconstructed scene. The optimal configuration of the 3D scene, represented by a parse graph, is inferred using Markov chain Monte Carlo (MCMC), which efficiently traverses through the non-differentiable joint solution space. Experimental results demonstrate that the proposed algorithm significantly improves the performance of the two tasks on three datasets, showing an improved generalization ability.



### Lifelong Machine Learning with Deep Streaming Linear Discriminant Analysis
- **Arxiv ID**: http://arxiv.org/abs/1909.01520v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.01520v3)
- **Published**: 2019-09-04 02:13:22+00:00
- **Updated**: 2020-04-17 16:28:51+00:00
- **Authors**: Tyler L. Hayes, Christopher Kanan
- **Comment**: To appear in the IEEE Conference on Computer Vision and Pattern
  Recognition Workshop (CVPR-W) on Continual Learning in Computer Vision
  (CLVision) 2020
- **Journal**: None
- **Summary**: When an agent acquires new information, ideally it would immediately be capable of using that information to understand its environment. This is not possible using conventional deep neural networks, which suffer from catastrophic forgetting when they are incrementally updated, with new knowledge overwriting established representations. A variety of approaches have been developed that attempt to mitigate catastrophic forgetting in the incremental batch learning scenario, where a model learns from a series of large collections of labeled samples. However, in this setting, inference is only possible after a batch has been accumulated, which prohibits many applications. An alternative paradigm is online learning in a single pass through the training dataset on a resource constrained budget, which is known as streaming learning. Streaming learning has been much less studied in the deep learning community. In streaming learning, an agent learns instances one-by-one and can be tested at any time, rather than only after learning a large batch. Here, we revisit streaming linear discriminant analysis, which has been widely used in the data mining research community. By combining streaming linear discriminant analysis with deep learning, we are able to outperform both incremental batch learning and streaming learning algorithms on both ImageNet ILSVRC-2012 and CORe50, a dataset that involves learning to classify from temporally ordered samples.



### Accurate Esophageal Gross Tumor Volume Segmentation in PET/CT using Two-Stream Chained 3D Deep Network Fusion
- **Arxiv ID**: http://arxiv.org/abs/1909.01524v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.01524v2)
- **Published**: 2019-09-04 02:24:58+00:00
- **Updated**: 2019-09-06 02:59:32+00:00
- **Authors**: Dakai Jin, Dazhou Guo, Tsung-Ying Ho, Adam P. Harrison, Jing Xiao, Chen-kan Tseng, Le Lu
- **Comment**: MICCAI 2019 (early accept and oral presentation)
- **Journal**: None
- **Summary**: Gross tumor volume (GTV) segmentation is a critical step in esophageal cancer radiotherapy treatment planning. Inconsistencies across oncologists and prohibitive labor costs motivate automated approaches for this task. However, leading approaches are only applied to radiotherapy computed tomography (RTCT) images taken prior to treatment. This limits the performance as RTCT suffers from low contrast between the esophagus, tumor, and surrounding tissues. In this paper, we aim to exploit both RTCT and positron emission tomography (PET) imaging modalities to facilitate more accurate GTV segmentation. By utilizing PET, we emulate medical professionals who frequently delineate GTV boundaries through observation of the RTCT images obtained after prescribing radiotherapy and PET/CT images acquired earlier for cancer staging. To take advantage of both modalities, we present a two-stream chained segmentation approach that effectively fuses the CT and PET modalities via early and late 3D deep-network-based fusion. Furthermore, to effect the fusion and segmentation we propose a simple yet effective progressive semantically nested network (PSNN) model that outperforms more complicated models. Extensive 5-fold cross-validation on 110 esophageal cancer patients, the largest analysis to date, demonstrates that both the proposed two-stream chained segmentation pipeline and the PSNN model can significantly improve the quantitative performance over the previous state-of-the-art work by 11% in absolute Dice score (DSC) (from 0.654 to 0.764) and, at the same time, reducing the Hausdorff distance from 129 mm to 47 mm.



### Deep Esophageal Clinical Target Volume Delineation using Encoded 3D Spatial Context of Tumors, Lymph Nodes, and Organs At Risk
- **Arxiv ID**: http://arxiv.org/abs/1909.01526v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.01526v2)
- **Published**: 2019-09-04 02:40:12+00:00
- **Updated**: 2019-09-06 03:04:02+00:00
- **Authors**: Dakai Jin, Dazhou Guo, Tsung-Ying Ho, Adam P. Harrison, Jing Xiao, Chen-kan Tseng, Le Lu
- **Comment**: MICCAI 2019 (early accept)
- **Journal**: None
- **Summary**: Clinical target volume (CTV) delineation from radiotherapy computed tomography (RTCT) images is used to define the treatment areas containing the gross tumor volume (GTV) and/or sub-clinical malignant disease for radiotherapy (RT). High intra- and inter-user variability makes this a particularly difficult task for esophageal cancer. This motivates automated solutions, which is the aim of our work. Because CTV delineation is highly context-dependent--it must encompass the GTV and regional lymph nodes (LNs) while also avoiding excessive exposure to the organs at risk (OARs)--we formulate it as a deep contextual appearance-based problem using encoded spatial contexts of these anatomical structures. This allows the deep network to better learn from and emulate the margin- and appearance-based delineation performed by human physicians. Additionally, we develop domain-specific data augmentation to inject robustness to our system. Finally, we show that a simple 3D progressive holistically nested network (PHNN), which avoids computationally heavy decoding paths while still aggregating features at different levels of context, can outperform more complicated networks. Cross-validated experiments on a dataset of 135 esophageal cancer patients demonstrate that our encoded spatial context approach can produce concrete performance improvements, with an average Dice score of 83.9% and an average surface distance of 4.2 mm, representing improvements of 3.8% and 2.4 mm, respectively, over the state-of-the-art approach.



### GmCN: Graph Mask Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/1910.01735v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.01735v2)
- **Published**: 2019-09-04 02:59:13+00:00
- **Updated**: 2019-11-20 08:50:58+00:00
- **Authors**: Bo Jiang, Beibei Wang, Jin Tang, Bin Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Graph Convolutional Networks (GCNs) have shown very powerful for graph data representation and learning tasks. Existing GCNs usually conduct feature aggregation on a fixed neighborhood graph in which each node computes its representation by aggregating the feature representations of all its neighbors which is biased by its own representation. However, this fixed aggregation strategy is not guaranteed to be optimal for GCN based graph learning and also can be affected by some graph structure noises, such as incorrect or undesired edge connections. To address these issues, we propose a novel Graph mask Convolutional Network (GmCN) in which nodes can adaptively select the optimal neighbors in their feature aggregation to better serve GCN learning. GmCN can be theoretically interpreted by a regularization framework, based on which we derive a simple update algorithm to determine the optimal mask adaptively in GmCN training process. Experiments on several datasets validate the effectiveness of GmCN.



### Deep Morphological Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.01532v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.01532v1)
- **Published**: 2019-09-04 03:03:20+00:00
- **Updated**: 2019-09-04 03:03:20+00:00
- **Authors**: Yucong Shen, Xin Zhong, Frank Y. Shih
- **Comment**: None
- **Journal**: None
- **Summary**: Mathematical morphology is a theory and technique to collect features like geometric and topological structures in digital images. Given a target image, determining suitable morphological operations and structuring elements is a cumbersome and time-consuming task. In this paper, a morphological neural network is proposed to address this problem. Serving as a nonlinear feature extracting layer in deep learning frameworks, the efficiency of the proposed morphological layer is confirmed analytically and empirically. With a known target, a single-filter morphological layer learns the structuring element correctly, and an adaptive layer can automatically select appropriate morphological operations. For practical applications, the proposed morphological neural networks are tested on several classification datasets related to shape or geometric image features, and the experimental results have confirmed the high computational efficiency and high accuracy.



### Snowball: Iterative Model Evolution and Confident Sample Discovery for Semi-Supervised Learning on Very Small Labeled Datasets
- **Arxiv ID**: http://arxiv.org/abs/1909.01542v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.01542v1)
- **Published**: 2019-09-04 03:41:27+00:00
- **Updated**: 2019-09-04 03:41:27+00:00
- **Authors**: Yang Li, Jianhe Yuan, Zhiqun Zhao, Hao Sun, Zhihai He
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we develop a joint sample discovery and iterative model evolution method for semi-supervised learning on very small labeled training sets. We propose a master-teacher-student model framework to provide multi-layer guidance during the model evolution process with multiple iterations and generations. The teacher model is constructed by performing an exponential moving average of the student models obtained from past training steps. The master network combines the knowledge of the student and teacher models with additional access to newly discovered samples. The master and teacher models are then used to guide the training of the student network by enforcing the consistence between their predictions of unlabeled samples and evolve all models when more and more samples are discovered. Our extensive experiments demonstrate that the discovering confident samples from the unlabeled dataset, once coupled with the above master-teacher-student network evolution, can significantly improve the overall semi-supervised learning performance. For example, on the CIFAR-10 dataset, with a very small set of 250 labeled samples, our method achieves an error rate of 11.81 %, more than 38 % lower than the state-of-the-art method Mean-Teacher (49.91 %).



### Aerial multi-object tracking by detection using deep association networks
- **Arxiv ID**: http://arxiv.org/abs/1909.01547v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.01547v1)
- **Published**: 2019-09-04 03:52:37+00:00
- **Updated**: 2019-09-04 03:52:37+00:00
- **Authors**: Ajit Jadhav, Prerana Mukherjee, Vinay Kaushik, Brejesh Lall
- **Comment**: None
- **Journal**: None
- **Summary**: A lot a research is focused on object detection and it has achieved significant advances with deep learning techniques in recent years. Inspite of the existing research, these algorithms are not usually optimal for dealing with sequences or images captured by drone-based platforms, due to various challenges such as view point change, scales, density of object distribution and occlusion. In this paper, we develop a model for detection of objects in drone images using the VisDrone2019 DET dataset. Using the RetinaNet model as our base, we modify the anchor scales to better handle the detection of dense distribution and small size of the objects. We explicitly model the channel interdependencies by using "Squeeze-and-Excitation" (SE) blocks that adaptively recalibrates channel-wise feature responses. This helps to bring significant improvements in performance at a slight additional computational cost. Using this architecture for object detection, we build a custom DeepSORT network for object detection on the VisDrone2019 MOT dataset by training a custom Deep Association network for the algorithm.



### Functional Asplund metrics for pattern matching, robust to variable lighting conditions
- **Arxiv ID**: http://arxiv.org/abs/1909.01585v2
- **DOI**: 10.5566/ias.2292
- **Categories**: **cs.CV**, cs.NA, eess.SP, math.FA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1909.01585v2)
- **Published**: 2019-09-04 07:26:03+00:00
- **Updated**: 2020-07-24 07:17:58+00:00
- **Authors**: Guillaume Noyel, Michel Jourlin
- **Comment**: None
- **Journal**: Image Analysis and Stereology, International Society for
  Stereology, 2020, 39 (2), pp.53--71
- **Summary**: In this paper, we propose a complete framework to process images captured under uncontrolled lighting and especially under low lighting. By taking advantage of the Logarithmic Image Processing (LIP) context, we study two novel functional metrics: i) the LIP-multiplicative Asplund metric which is robust to object absorption variations and ii) the LIP-additive Asplund metric which is robust to variations of source intensity or camera exposure-time. We introduce robust to noise versions of these metrics. We demonstrate that the maps of their corresponding distances between an image and a reference template are linked to Mathematical Morphology. This facilitates their implementation. We assess them in various situations with different lightings and movement. Results show that those maps of distances are robust to lighting variations. Importantly, they are efficient to detect patterns in low-contrast images with a template acquired under a different lighting.



### Bidirectional One-Shot Unsupervised Domain Mapping
- **Arxiv ID**: http://arxiv.org/abs/1909.01595v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.01595v1)
- **Published**: 2019-09-04 07:49:18+00:00
- **Updated**: 2019-09-04 07:49:18+00:00
- **Authors**: Tomer Cohen, Lior Wolf
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: We study the problem of mapping between a domain $A$, in which there is a single training sample and a domain $B$, for which we have a richer training set. The method we present is able to perform this mapping in both directions. For example, we can transfer all MNIST images to the visual domain captured by a single SVHN image and transform the SVHN image to the domain of the MNIST images. Our method is based on employing one encoder and one decoder for each domain, without utilizing weight sharing. The autoencoder of the single sample domain is trained to match both this sample and the latent space of domain $B$. Our results demonstrate convincing mapping between domains, where either the source or the target domain are defined by a single sample, far surpassing existing solutions. Our code is made publicly available at https://github.com/tomercohen11/BiOST



### SSAP: Single-Shot Instance Segmentation With Affinity Pyramid
- **Arxiv ID**: http://arxiv.org/abs/1909.01616v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.01616v1)
- **Published**: 2019-09-04 08:33:07+00:00
- **Updated**: 2019-09-04 08:33:07+00:00
- **Authors**: Naiyu Gao, Yanhu Shan, Yupei Wang, Xin Zhao, Yinan Yu, Ming Yang, Kaiqi Huang
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Recently, proposal-free instance segmentation has received increasing attention due to its concise and efficient pipeline. Generally, proposal-free methods generate instance-agnostic semantic segmentation labels and instance-aware features to group pixels into different object instances. However, previous methods mostly employ separate modules for these two sub-tasks and require multiple passes for inference. We argue that treating these two sub-tasks separately is suboptimal. In fact, employing multiple separate modules significantly reduces the potential for application. The mutual benefits between the two complementary sub-tasks are also unexplored. To this end, this work proposes a single-shot proposal-free instance segmentation method that requires only one single pass for prediction. Our method is based on a pixel-pair affinity pyramid, which computes the probability that two pixels belong to the same instance in a hierarchical manner. The affinity pyramid can also be jointly learned with the semantic class labeling and achieve mutual benefits. Moreover, incorporating with the learned affinity pyramid, a novel cascaded graph partition module is presented to sequentially generate instances from coarse to fine. Unlike previous time-consuming graph partition methods, this module achieves $5\times$ speedup and 9% relative improvement on Average-Precision (AP). Our approach achieves state-of-the-art results on the challenging Cityscapes dataset.



### PASS3D: Precise and Accelerated Semantic Segmentation for 3D Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/1909.01643v2
- **DOI**: 10.1109/IROS40897.2019.8968296
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.01643v2)
- **Published**: 2019-09-04 09:26:28+00:00
- **Updated**: 2020-08-26 09:42:09+00:00
- **Authors**: Xin Kong, Guangyao Zhai, Baoquan Zhong, Yong Liu
- **Comment**: This paper has been accepted by IROS-2019
- **Journal**: None
- **Summary**: In this paper, we propose PASS3D to achieve point-wise semantic segmentation for 3D point cloud. Our framework combines the efficiency of traditional geometric methods with robustness of deep learning methods, consisting of two stages: At stage-1, our accelerated cluster proposal algorithm will generate refined cluster proposals by segmenting point clouds without ground, capable of generating less redundant proposals with higher recall in an extremely short time; stage-2 we will amplify and further process these proposals by a neural network to estimate semantic label for each point and meanwhile propose a novel data augmentation method to enhance the network's recognition capability for all categories especially for non-rigid objects. Evaluated on KITTI raw dataset, PASS3D stands out against the state-of-the-art on some results, making itself competent to 3D perception in autonomous driving system. Our source code will be open-sourced. A video demonstration is available at https://www.youtube.com/watch?v=cukEqDuP_Qw.



### 3D landmark detection for augmented reality based otologic procedures
- **Arxiv ID**: http://arxiv.org/abs/1909.01647v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.01647v1)
- **Published**: 2019-09-04 09:31:22+00:00
- **Updated**: 2019-09-04 09:31:22+00:00
- **Authors**: Raabid Hussain, Alain Lalande, Kibrom Berihu Girum, Caroline Guigou, Alexis Bozorg Grayeli
- **Comment**: None
- **Journal**: Surgetica, Jun 2019, Rennes, France
- **Summary**: Ear consists of the smallest bones in the human body and does not contain significant amount of distinct landmark points that may be used to register a preoperative CT-scan with the surgical video in an augmented reality framework. Learning based algorithms may be used to help the surgeons to identify landmark points. This paper presents a convolutional neural network approach to landmark detection in preoperative ear CT images and then discusses an augmented reality system that can be used to visualize the cochlear axis on an otologic surgical video.



### Distance transform regression for spatially-aware deep semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.01671v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.01671v1)
- **Published**: 2019-09-04 10:05:08+00:00
- **Updated**: 2019-09-04 10:05:08+00:00
- **Authors**: Nicolas Audebert, Alexandre Boulch, Bertrand Le Saux, Sébastien Lefèvre
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding visual scenes relies more and more on dense pixel-wise classification obtained via deep fully convolutional neural networks. However, due to the nature of the networks, predictions often suffer from blurry boundaries and ill-segmented shapes, fueling the need for post-processing. This work introduces a new semantic segmentation regularization based on the regression of a distance transform. After computing the distance transform on the label masks, we train a FCN in a multi-task setting in both discrete and continuous spaces by learning jointly classification and distance regression. This requires almost no modification of the network structure and adds a very low overhead to the training process. Learning to approximate the distance transform back-propagates spatial cues that implicitly regularizes the segmentation. We validate this technique with several architectures on various datasets, and we show significant improvements compared to competitive baselines.



### DurIAN: Duration Informed Attention Network For Multimodal Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1909.01700v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1909.01700v2)
- **Published**: 2019-09-04 11:35:48+00:00
- **Updated**: 2019-09-05 22:35:53+00:00
- **Authors**: Chengzhu Yu, Heng Lu, Na Hu, Meng Yu, Chao Weng, Kun Xu, Peng Liu, Deyi Tuo, Shiyin Kang, Guangzhi Lei, Dan Su, Dong Yu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a generic and robust multimodal synthesis system that produces highly natural speech and facial expression simultaneously. The key component of this system is the Duration Informed Attention Network (DurIAN), an autoregressive model in which the alignments between the input text and the output acoustic features are inferred from a duration model. This is different from the end-to-end attention mechanism used, and accounts for various unavoidable artifacts, in existing end-to-end speech synthesis systems such as Tacotron. Furthermore, DurIAN can be used to generate high quality facial expression which can be synchronized with generated speech with/without parallel speech and face data. To improve the efficiency of speech generation, we also propose a multi-band parallel generation strategy on top of the WaveRNN model. The proposed Multi-band WaveRNN effectively reduces the total computational complexity from 9.8 to 5.5 GFLOPS, and is able to generate audio that is 6 times faster than real time on a single CPU core. We show that DurIAN could generate highly natural speech that is on par with current state of the art end-to-end systems, while at the same time avoid word skipping/repeating errors in those systems. Finally, a simple yet effective approach for fine-grained control of expressiveness of speech and facial expression is introduced.



### An Efficient and Layout-Independent Automatic License Plate Recognition System Based on the YOLO detector
- **Arxiv ID**: http://arxiv.org/abs/1909.01754v4
- **DOI**: 10.1049/itr2.12030
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.01754v4)
- **Published**: 2019-09-04 12:51:51+00:00
- **Updated**: 2021-03-09 15:36:30+00:00
- **Authors**: Rayson Laroca, Luiz A. Zanlorensi, Gabriel R. Gonçalves, Eduardo Todt, William Robson Schwartz, David Menotti
- **Comment**: None
- **Journal**: IET Intelligent Transport Systems, vol. 15, no. 4, pp. 483-503,
  2021
- **Summary**: This paper presents an efficient and layout-independent Automatic License Plate Recognition (ALPR) system based on the state-of-the-art YOLO object detector that contains a unified approach for license plate (LP) detection and layout classification to improve the recognition results using post-processing rules. The system is conceived by evaluating and optimizing different models, aiming at achieving the best speed/accuracy trade-off at each stage. The networks are trained using images from several datasets, with the addition of various data augmentation techniques, so that they are robust under different conditions. The proposed system achieved an average end-to-end recognition rate of 96.9% across eight public datasets (from five different regions) used in the experiments, outperforming both previous works and commercial systems in the ChineseLP, OpenALPR-EU, SSIG-SegPlate and UFPR-ALPR datasets. In the other datasets, the proposed approach achieved competitive results to those attained by the baselines. Our system also achieved impressive frames per second (FPS) rates on a high-end GPU, being able to perform in real time even when there are four vehicles in the scene. An additional contribution is that we manually labeled 38,351 bounding boxes on 6,239 images from public datasets and made the annotations publicly available to the research community.



### PISEP^2: Pseudo Image Sequence Evolution based 3D Pose Prediction
- **Arxiv ID**: http://arxiv.org/abs/1909.01818v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.01818v1)
- **Published**: 2019-09-04 14:10:49+00:00
- **Updated**: 2019-09-04 14:10:49+00:00
- **Authors**: Xiaoli Liu, Jianqin Yin, Huaping Liu, Yilong Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Pose prediction is to predict future poses given a window of previous poses. In this paper, we propose a new problem that predicts poses using 3D joint coordinate sequences. Different from the traditional pose prediction based on Mocap frames, this problem is convenient to use in real applications due to its simple sensors to capture data. We also present a new framework, PISEP^2 (Pseudo Image Sequence Evolution based 3D Pose Prediction), to address this new problem. Specifically, a skeletal representation is proposed by transforming the joint coordinate sequence into an image sequence, which can model the different correlations of different joints. With this image based skeletal representation, we model the pose prediction as the evolution of image sequence. Moreover, a novel inference network is proposed to predict all future poses in one step by decoupling the decoders in a non-recursive manner. Compared with the recursive sequence to sequence model, we can improve the computational efficiency and avoid error accumulation significantly. Extensive experiments are carried out on two benchmark datasets (e.g. G3D and FNTU). The proposed method achieves the state-of-the-art performance on both datasets, which demonstrates the effectiveness of our proposed method.



### 'Skimming-Perusal' Tracking: A Framework for Real-Time and Robust Long-term Tracking
- **Arxiv ID**: http://arxiv.org/abs/1909.01840v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.01840v1)
- **Published**: 2019-09-04 14:34:24+00:00
- **Updated**: 2019-09-04 14:34:24+00:00
- **Authors**: Bin Yan, Haojie Zhao, Dong Wang, Huchuan Lu, Xiaoyun Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Compared with traditional short-term tracking, long-term tracking poses more challenges and is much closer to realistic applications. However, few works have been done and their performance have also been limited. In this work, we present a novel robust and real-time long-term tracking framework based on the proposed skimming and perusal modules. The perusal module consists of an effective bounding box regressor to generate a series of candidate proposals and a robust target verifier to infer the optimal candidate with its confidence score. Based on this score, our tracker determines whether the tracked object being present or absent, and then chooses the tracking strategies of local search or global search respectively in the next frame. To speed up the image-wide global search, a novel skimming module is designed to efficiently choose the most possible regions from a large number of sliding windows. Numerous experimental results on the VOT-2018 long-term and OxUvA long-term benchmarks demonstrate that the proposed method achieves the best performance and runs in real-time. The source codes are available at https://github.com/iiau-tracker/SPLT.



### Are Adversarial Robustness and Common Perturbation Robustness Independent Attributes ?
- **Arxiv ID**: http://arxiv.org/abs/1909.02436v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.02436v2)
- **Published**: 2019-09-04 14:36:23+00:00
- **Updated**: 2019-10-09 08:14:58+00:00
- **Authors**: Alfred Laugros, Alice Caplier, Matthieu Ospici
- **Comment**: To appear in ICCV Workshop on Real-World Recognition from Low-Quality
  Images and Videos (RLQ) 2019
- **Journal**: None
- **Summary**: Neural Networks have been shown to be sensitive to common perturbations such as blur, Gaussian noise, rotations, etc. They are also vulnerable to some artificial malicious corruptions called adversarial examples. The adversarial examples study has recently become very popular and it sometimes even reduces the term "adversarial robustness" to the term "robustness". Yet, we do not know to what extent the adversarial robustness is related to the global robustness. Similarly, we do not know if a robustness to various common perturbations such as translations or contrast losses for instance, could help with adversarial corruptions. We intend to study the links between the robustnesses of neural networks to both perturbations. With our experiments, we provide one of the first benchmark designed to estimate the robustness of neural networks to common perturbations. We show that increasing the robustness to carefully selected common perturbations, can make neural networks more robust to unseen common perturbations. We also prove that adversarial robustness and robustness to common perturbations are independent. Our results make us believe that neural network robustness should be addressed in a broader sense.



### 3D U$^2$-Net: A 3D Universal U-Net for Multi-Domain Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.06012v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.06012v1)
- **Published**: 2019-09-04 15:03:08+00:00
- **Updated**: 2019-09-04 15:03:08+00:00
- **Authors**: Chao Huang, Hu Han, Qingsong Yao, Shankuan Zhu, S. Kevin Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Fully convolutional neural networks like U-Net have been the state-of-the-art methods in medical image segmentation. Practically, a network is highly specialized and trained separately for each segmentation task. Instead of a collection of multiple models, it is highly desirable to learn a universal data representation for different tasks, ideally a single model with the addition of a minimal number of parameters steered to each task. Inspired by the recent success of multi-domain learning in image classification, for the first time we explore a promising universal architecture that handles multiple medical segmentation tasks and is extendable for new tasks, regardless of different organs and imaging modalities. Our 3D Universal U-Net (3D U$^2$-Net) is built upon separable convolution, assuming that {\it images from different domains have domain-specific spatial correlations which can be probed with channel-wise convolution while also share cross-channel correlations which can be modeled with pointwise convolution}. We evaluate the 3D U$^2$-Net on five organ segmentation datasets. Experimental results show that this universal network is capable of competing with traditional models in terms of segmentation accuracy, while requiring only about $1\%$ of the parameters. Additionally, we observe that the architecture can be easily and effectively adapted to a new domain without sacrificing performance in the domains used to learn the shared parameterization of the universal network. We put the code of 3D U$^2$-Net into public domain. \url{https://github.com/huangmozhilv/u2net_torch/}



### Rethinking the Number of Channels for the Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1909.01861v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.01861v1)
- **Published**: 2019-09-04 15:09:22+00:00
- **Updated**: 2019-09-04 15:09:22+00:00
- **Authors**: Hui Zhu, Zhulin An, Chuanguang Yang, Xiaolong Hu, Kaiqiang Xu, Yongjun Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Latest algorithms for automatic neural architecture search perform remarkable but few of them can effectively design the number of channels for convolutional neural networks and consume less computational efforts. In this paper, we propose a method for efficient automatic architecture search which is special to the widths of networks instead of the connections of neural architecture. Our method, functionally incremental search based on function-preserving, will explore the number of channels rapidly while controlling the number of parameters of the target network. On CIFAR-10 and CIFAR-100 classification, our method using minimal computational resources (0.4~1.3 GPU-days) can discover more efficient rules of the widths of networks to improve the accuracy by about 0.5% on CIFAR-10 and a~2.33% on CIFAR-100 with fewer number of parameters. In particular, our method is suitable for exploring the number of channels of almost any convolutional neural network rapidly.



### Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.01871v6
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.01871v6)
- **Published**: 2019-09-04 15:20:01+00:00
- **Updated**: 2019-11-22 16:11:17+00:00
- **Authors**: Khanh Nguyen, Hal Daumé III
- **Comment**: In EMNLP 2019
- **Journal**: None
- **Summary**: Mobile agents that can leverage help from humans can potentially accomplish more complex tasks than they could entirely on their own. We develop "Help, Anna!" (HANNA), an interactive photo-realistic simulator in which an agent fulfills object-finding tasks by requesting and interpreting natural language-and-vision assistance. An agent solving tasks in a HANNA environment can leverage simulated human assistants, called ANNA (Automatic Natural Navigation Assistants), which, upon request, provide natural language and visual instructions to direct the agent towards the goals. To address the HANNA problem, we develop a memory-augmented neural agent that hierarchically models multiple levels of decision-making, and an imitation learning algorithm that teaches the agent to avoid repeating past mistakes while simultaneously predicting its own chances of making future progress. Empirically, our approach is able to ask for help more effectively than competitive baselines and, thus, attains higher task success rate on both previously seen and previously unseen environments. We publicly release code and data at https://github.com/khanhptnk/hanna . A video demo is available at https://youtu.be/18P94aaaLKg .



### Optimal translational-rotational invariant dictionaries for images
- **Arxiv ID**: http://arxiv.org/abs/1909.01887v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.NA, math.FA, math.NA, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.01887v1)
- **Published**: 2019-09-04 15:34:04+00:00
- **Updated**: 2019-09-04 15:34:04+00:00
- **Authors**: Davide Barbieri, Carlos Cabrelli, Eugenio Hernández, Ursula Molter
- **Comment**: None
- **Journal**: None
- **Summary**: We provide the construction of a set of square matrices whose translates and rotates provide a Parseval frame that is optimal for approximating a given dataset of images. Our approach is based on abstract harmonic analysis techniques. Optimality is considered with respect to the quadratic error of approximation of the images in the dataset with their projection onto a linear subspace that is invariant under translations and rotations. In addition, we provide an elementary and fully self-contained proof of optimality, and the numerical results from datasets of natural images.



### Let's agree to disagree: learning highly debatable multirater labelling
- **Arxiv ID**: http://arxiv.org/abs/1909.01891v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1909.01891v1)
- **Published**: 2019-09-04 15:40:14+00:00
- **Updated**: 2019-09-04 15:40:14+00:00
- **Authors**: Carole H. Sudre, Beatriz Gomez Anson, Silvia Ingala, Chris D. Lane, Daniel Jimenez, Lukas Haider, Thomas Varsavsky, Ryutaro Tanno, Lorna Smith, Sébastien Ourselin, Rolf H. Jäger, M. Jorge Cardoso
- **Comment**: Accepted at MICCAI 2019
- **Journal**: None
- **Summary**: Classification and differentiation of small pathological objects may greatly vary among human raters due to differences in training, expertise and their consistency over time. In a radiological setting, objects commonly have high within-class appearance variability whilst sharing certain characteristics across different classes, making their distinction even more difficult. As an example, markers of cerebral small vessel disease, such as enlarged perivascular spaces (EPVS) and lacunes, can be very varied in their appearance while exhibiting high inter-class similarity, making this task highly challenging for human raters. In this work, we investigate joint models of individual rater behaviour and multirater consensus in a deep learning setting, and apply it to a brain lesion object-detection task. Results show that jointly modelling both individual and consensus estimates leads to significant improvements in performance when compared to directly predicting consensus labels, while also allowing the characterization of human-rater consistency.



### Dense Extreme Inception Network: Towards a Robust CNN Model for Edge Detection
- **Arxiv ID**: http://arxiv.org/abs/1909.01955v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.01955v2)
- **Published**: 2019-09-04 17:27:46+00:00
- **Updated**: 2020-02-04 00:08:24+00:00
- **Authors**: Xavier Soria, Edgar Riba, Angel D. Sappa
- **Comment**: WACV2020 Paper
- **Journal**: None
- **Summary**: This paper proposes a Deep Learning based edge detector, which is inspired on both HED (Holistically-Nested Edge Detection) and Xception networks. The proposed approach generates thin edge-maps that are plausible for human eyes; it can be used in any edge detection task without previous training or fine tuning process. As a second contribution, a large dataset with carefully annotated edges has been generated. This dataset has been used for training the proposed approach as well the state-of-the-art algorithms for comparisons. Quantitative and qualitative evaluations have been performed on different benchmarks showing improvements with the proposed method when F-measure of ODS and OIS are considered.



### Beyond Photo Realism for Domain Adaptation from Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/1909.01960v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML, 68T45, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1909.01960v1)
- **Published**: 2019-09-04 17:38:05+00:00
- **Updated**: 2019-09-04 17:38:05+00:00
- **Authors**: Kristofer Schlachter, Connor DeFanti, Sebastian Herscher, Ken Perlin, Jonathan Tompson
- **Comment**: Originally submitted for publication in November 2017
- **Journal**: None
- **Summary**: As synthetic imagery is used more frequently in training deep models, it is important to understand how different synthesis techniques impact the performance of such models. In this work, we perform a thorough evaluation of the effectiveness of several different synthesis techniques and their impact on the complexity of classifier domain adaptation to the "real" underlying data distribution that they seek to replicate. In addition, we propose a novel learned synthesis technique to better train classifier models than state-of-the-art offline graphical methods, while using significantly less computational resources. We accomplish this by learning a generative model to perform shading of synthetic geometry conditioned on a "g-buffer" representation of the scene to render, as well as a low sample Monte Carlo rendered image. The major contributions are (i) a dataset that allows comparison of real and synthetic versions of the same scene, (ii) an augmented data representation that boosts the stability of learning and improves the datasets accuracy, (iii) three different partially differentiable rendering techniques where lighting, denoising and shading are learned, and (iv) we improve a state of the art generative adversarial network (GAN) approach by using an ensemble of trained models to generate datasets that approach the performance of training on real data and surpass the performance of the full global illumination rendering.



### Self-Attentive Adversarial Stain Normalization
- **Arxiv ID**: http://arxiv.org/abs/1909.01963v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1909.01963v3)
- **Published**: 2019-09-04 17:41:19+00:00
- **Updated**: 2020-11-22 22:50:24+00:00
- **Authors**: Aman Shrivastava, Will Adorno, Yash Sharma, Lubaina Ehsan, S. Asad Ali, Sean R. Moore, Beatrice C. Amadi, Paul Kelly, Sana Syed, Donald E. Brown
- **Comment**: Accepted at AIDP (ICPR 2021)
- **Journal**: None
- **Summary**: Hematoxylin and Eosin (H&E) stained Whole Slide Images (WSIs) are utilized for biopsy visualization-based diagnostic and prognostic assessment of diseases. Variation in the H&E staining process across different lab sites can lead to significant variations in biopsy image appearance. These variations introduce an undesirable bias when the slides are examined by pathologists or used for training deep learning models. To reduce this bias, slides need to be translated to a common domain of stain appearance before analysis. We propose a Self-Attentive Adversarial Stain Normalization (SAASN) approach for the normalization of multiple stain appearances to a common domain. This unsupervised generative adversarial approach includes self-attention mechanism for synthesizing images with finer detail while preserving the structural consistency of the biopsy features during translation. SAASN demonstrates consistent and superior performance compared to other popular stain normalization techniques on H&E stained duodenal biopsy image data.



### The ML-EM algorithm in continuum: sparse measure solutions
- **Arxiv ID**: http://arxiv.org/abs/1909.01966v2
- **DOI**: 10.1088/1361-6420/ab6d55
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.01966v2)
- **Published**: 2019-09-04 17:47:42+00:00
- **Updated**: 2019-12-18 21:03:36+00:00
- **Authors**: Camille Pouchol, Olivier Verdier
- **Comment**: None
- **Journal**: Inverse Problems, Vol 36 (3), (2020)
- **Summary**: Linear inverse problems $A \mu = \delta$ with Poisson noise and non-negative unknown $\mu \geq 0$ are ubiquitous in applications, for instance in Positron Emission Tomography (PET) in medical imaging. The associated maximum likelihood problem is routinely solved using an expectation-maximisation algorithm (ML-EM). This typically results in images which look spiky, even with early stopping. We give an explanation for this phenomenon. We first regard the image $\mu$ as a measure. We prove that if the measurements $\delta$ are not in the cone $\{A \mu, \mu \geq 0\}$, which is typical of short exposure times, likelihood maximisers as well as ML-EM cluster points must be sparse, i.e., typically a sum of point masses. On the other hand, in the long exposure regime, we prove that cluster points of ML-EM will be measures without singular part. Finally, we provide concentration bounds for the probability to be in the sparse case.



### Assessment of Shift-Invariant CNN Gaze Mappings for PS-OG Eye Movement Sensors
- **Arxiv ID**: http://arxiv.org/abs/1909.05655v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1909.05655v1)
- **Published**: 2019-09-04 17:57:45+00:00
- **Updated**: 2019-09-04 17:57:45+00:00
- **Authors**: Henry K. Griffith, Dmytro Katrychuk, Oleg V. Komogortsev
- **Comment**: Accepted to be published in the 2019 OpenEDS Workshop: Eye Tracking
  for VR and AR at the International Conference on Computer Vision (ICCV),
  October 27- November 3, 2019, Seoul, Korea
- **Journal**: None
- **Summary**: Photosensor oculography (PS-OG) eye movement sensors offer desirable performance characteristics for integration within wireless head mounted devices (HMDs), including low power consumption and high sampling rates. To address the known performance degradation of these sensors due to HMD shifts, various machine learning techniques have been proposed for mapping sensor outputs to gaze location. This paper advances the understanding of a recently introduced convolutional neural network designed to provide shift invariant gaze mapping within a specified range of sensor translations. Performance is assessed for shift training examples which better reflect the distribution of values that would be generated through manual repositioning of the HMD during a dedicated collection of training data. The network is shown to exhibit comparable accuracy for this realistic shift distribution versus a previously considered rectangular grid, thereby enhancing the feasibility of in-field set-up. In addition, this work further demonstrates the practical viability of the proposed initialization process by demonstrating robust mapping performance versus training data scale. The ability to maintain reasonable accuracy for shifts extending beyond those introduced during training is also demonstrated.



### Online Regularization by Denoising with Applications to Phase Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1909.02040v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.02040v1)
- **Published**: 2019-09-04 18:29:10+00:00
- **Updated**: 2019-09-04 18:29:10+00:00
- **Authors**: Zihui Wu, Yu Sun, Jiaming Liu, Ulugbek S. Kamilov
- **Comment**: Accepted ICCVW 2019 (LCI)
- **Journal**: None
- **Summary**: Regularization by denoising (RED) is a powerful framework for solving imaging inverse problems. Most RED algorithms are iterative batch procedures, which limits their applicability to very large datasets. In this paper, we address this limitation by introducing a novel online RED (On-RED) algorithm, which processes a small subset of the data at a time. We establish the theoretical convergence of On-RED in convex settings and empirically discuss its effectiveness in non-convex ones by illustrating its applicability to phase retrieval. Our results suggest that On-RED is an effective alternative to the traditional RED algorithms when dealing with large datasets.



### TIGEr: Text-to-Image Grounding for Image Caption Evaluation
- **Arxiv ID**: http://arxiv.org/abs/1909.02050v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.02050v1)
- **Published**: 2019-09-04 18:43:04+00:00
- **Updated**: 2019-09-04 18:43:04+00:00
- **Authors**: Ming Jiang, Qiuyuan Huang, Lei Zhang, Xin Wang, Pengchuan Zhang, Zhe Gan, Jana Diesner, Jianfeng Gao
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new metric called TIGEr for the automatic evaluation of image captioning systems. Popular metrics, such as BLEU and CIDEr, are based solely on text matching between reference captions and machine-generated captions, potentially leading to biased evaluations because references may not fully cover the image content and natural language is inherently ambiguous. Building upon a machine-learned text-image grounding model, TIGEr allows to evaluate caption quality not only based on how well a caption represents image content, but also on how well machine-generated captions match human-generated captions. Our empirical tests show that TIGEr has a higher consistency with human judgments than alternative existing metrics. We also comprehensively assess the metric's effectiveness in caption evaluation by measuring the correlation between human judgments and metric scores.



### Faster and Accurate Classification for JPEG2000 Compressed Images in Networked Applications
- **Arxiv ID**: http://arxiv.org/abs/1909.05638v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.05638v1)
- **Published**: 2019-09-04 19:03:35+00:00
- **Updated**: 2019-09-04 19:03:35+00:00
- **Authors**: Lahiru D. Chamain, Zhi Ding
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: JPEG2000 (j2k) is a highly popular format for image and video compression.With the rapidly growing applications of cloud based image classification, most existing j2k-compatible schemes would stream compressed color images from the source before reconstruction at the processing center as inputs to deep CNNs. We propose to remove the computationally costly reconstruction step by training a deep CNN image classifier using the CDF 9/7 Discrete Wavelet Transformed (DWT) coefficients directly extracted from j2k-compressed images. We demonstrate additional computation savings by utilizing shallower CNN to achieve classification of good accuracy in the DWT domain. Furthermore, we show that traditional augmentation transforms such as flipping/shifting are ineffective in the DWT domain and present different augmentation transformations to achieve more accurate classification without any additional cost. This way, faster and more accurate classification is possible for j2k encoded images without image reconstruction. Through experiments on CIFAR-10 and Tiny ImageNet data sets, we show that the performance of the proposed solution is consistent for image transmission over limited channel bandwidth.



### DCGANs for Realistic Breast Mass Augmentation in X-ray Mammography
- **Arxiv ID**: http://arxiv.org/abs/1909.02062v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML, 68U10 (Primary) 68U20 (Secondary), I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/1909.02062v1)
- **Published**: 2019-09-04 19:09:49+00:00
- **Updated**: 2019-09-04 19:09:49+00:00
- **Authors**: Basel Alyafi, Oliver Diaz, Robert Marti
- **Comment**: 4 pages, 4 figures, SPIE Medical Imaging 2020 Conference
- **Journal**: None
- **Summary**: Early detection of breast cancer has a major contribution to curability, and using mammographic images, this can be achieved non-invasively. Supervised deep learning, the dominant CADe tool currently, has played a great role in object detection in computer vision, but it suffers from a limiting property: the need of a large amount of labelled data. This becomes stricter when it comes to medical datasets which require high-cost and time-consuming annotations. Furthermore, medical datasets are usually imbalanced, a condition that often hinders classifiers performance. The aim of this paper is to learn the distribution of the minority class to synthesise new samples in order to improve lesion detection in mammography. Deep Convolutional Generative Adversarial Networks (DCGANs) can efficiently generate breast masses. They are trained on increasing-size subsets of one mammographic dataset and used to generate diverse and realistic breast masses. The effect of including the generated images and/or applying horizontal and vertical flipping is tested in an environment where a 1:10 imbalanced dataset of masses and normal tissue patches is classified by a fully-convolutional network. A maximum of ~ 0:09 improvement of F1 score is reported by using DCGANs along with flipping augmentation over using the original images. We show that DCGANs can be used for synthesising photo-realistic breast mass patches with considerable diversity. It is demonstrated that appending synthetic images in this environment, along with flipping, outperforms the traditional augmentation method of flipping solely, offering faster improvements as a function of the training set size.



### Large-scale Tag-based Font Retrieval with Generative Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.02072v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.02072v2)
- **Published**: 2019-09-04 19:49:58+00:00
- **Updated**: 2019-10-02 15:57:07+00:00
- **Authors**: Tianlang Chen, Zhaowen Wang, Ning Xu, Hailin Jin, Jiebo Luo
- **Comment**: accepted by ICCV 2019
- **Journal**: None
- **Summary**: Font selection is one of the most important steps in a design workflow. Traditional methods rely on ordered lists which require significant domain knowledge and are often difficult to use even for trained professionals. In this paper, we address the problem of large-scale tag-based font retrieval which aims to bring semantics to the font selection process and enable people without expert knowledge to use fonts effectively. We collect a large-scale font tagging dataset of high-quality professional fonts. The dataset contains nearly 20,000 fonts, 2,000 tags, and hundreds of thousands of font-tag relations. We propose a novel generative feature learning algorithm that leverages the unique characteristics of fonts. The key idea is that font images are synthetic and can therefore be controlled by the learning algorithm. We design an integrated rendering and learning process so that the visual feature from one image can be used to reconstruct another image with different text. The resulting feature captures important font design details while is robust to nuisance factors such as text. We propose a novel attention mechanism to re-weight the visual feature for joint visual-text modeling. We combine the feature and the attention mechanism in a novel recognition-retrieval model. Experimental results show that our method significantly outperforms the state-of-the-art for the important problem of large-scale tag-based font retrieval.



### Weakly Supervised Universal Fracture Detection in Pelvic X-rays
- **Arxiv ID**: http://arxiv.org/abs/1909.02077v1
- **DOI**: 10.1007/978-3-030-32226-7_51
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.02077v1)
- **Published**: 2019-09-04 20:04:50+00:00
- **Updated**: 2019-09-04 20:04:50+00:00
- **Authors**: Yirui Wang, Le Lu, Chi-Tung Cheng, Dakai Jin, Adam P. Harrison, Jing Xiao, Chien-Hung Liao, Shun Miao
- **Comment**: MICCAI 2019 (early accept)
- **Journal**: None
- **Summary**: Hip and pelvic fractures are serious injuries with life-threatening complications. However, diagnostic errors of fractures in pelvic X-rays (PXRs) are very common, driving the demand for computer-aided diagnosis (CAD) solutions. A major challenge lies in the fact that fractures are localized patterns that require localized analyses. Unfortunately, the PXRs residing in hospital picture archiving and communication system do not typically specify region of interests. In this paper, we propose a two-stage hip and pelvic fracture detection method that executes localized fracture classification using weakly supervised ROI mining. The first stage uses a large capacity fully-convolutional network, i.e., deep with high levels of abstraction, in a multiple instance learning setting to automatically mine probable true positive and definite hard negative ROIs from the whole PXR in the training data. The second stage trains a smaller capacity model, i.e., shallower and more generalizable, with the mined ROIs to perform localized analyses to classify fractures. During inference, our method detects hip and pelvic fractures in one pass by chaining the probability outputs of the two stages together. We evaluate our method on 4 410 PXRs, reporting an area under the ROC curve value of 0.975, the highest among state-of-the-art fracture detection methods. Moreover, we show that our two-stage approach can perform comparably to human physicians (even outperforming emergency physicians and surgeons), in a preliminary reader study of 23 readers.



### Decoupled Box Proposal and Featurization with Ultrafine-Grained Semantic Labels Improve Image Captioning and Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1909.02097v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.02097v1)
- **Published**: 2019-09-04 20:37:30+00:00
- **Updated**: 2019-09-04 20:37:30+00:00
- **Authors**: Soravit Changpinyo, Bo Pang, Piyush Sharma, Radu Soricut
- **Comment**: The 2019 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2019)
- **Journal**: None
- **Summary**: Object detection plays an important role in current solutions to vision and language tasks like image captioning and visual question answering. However, popular models like Faster R-CNN rely on a costly process of annotating ground-truths for both the bounding boxes and their corresponding semantic labels, making it less amenable as a primitive task for transfer learning. In this paper, we examine the effect of decoupling box proposal and featurization for down-stream tasks. The key insight is that this allows us to leverage a large amount of labeled annotations that were previously unavailable for standard object detection benchmarks. Empirically, we demonstrate that this leads to effective transfer learning and improved image captioning and visual question answering models, as measured on publicly available benchmarks.



### Program-Guided Image Manipulators
- **Arxiv ID**: http://arxiv.org/abs/1909.02116v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.02116v1)
- **Published**: 2019-09-04 21:10:47+00:00
- **Updated**: 2019-09-04 21:10:47+00:00
- **Authors**: Jiayuan Mao, Xiuming Zhang, Yikai Li, William T. Freeman, Joshua B. Tenenbaum, Jiajun Wu
- **Comment**: ICCV 2019. First two authors contributed equally. Project page:
  http://pgim.csail.mit.edu/
- **Journal**: None
- **Summary**: Humans are capable of building holistic representations for images at various levels, from local objects, to pairwise relations, to global structures. The interpretation of structures involves reasoning over repetition and symmetry of the objects in the image. In this paper, we present the Program-Guided Image Manipulator (PG-IM), inducing neuro-symbolic program-like representations to represent and manipulate images. Given an image, PG-IM detects repeated patterns, induces symbolic programs, and manipulates the image using a neural network that is guided by the program. PG-IM learns from a single image, exploiting its internal statistics. Despite trained only on image inpainting, PG-IM is directly capable of extrapolation and regularity editing in a unified framework. Extensive experiments show that PG-IM achieves superior performance on all the tasks.



### Towards Precise Robotic Grasping by Probabilistic Post-grasp Displacement Estimation
- **Arxiv ID**: http://arxiv.org/abs/1909.02129v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.02129v1)
- **Published**: 2019-09-04 21:50:03+00:00
- **Updated**: 2019-09-04 21:50:03+00:00
- **Authors**: Jialiang Zhao, Jacky Liang, Oliver Kroemer
- **Comment**: Submitted and accepted to 12th Conference on Field and Service
  Robotics (FSR 2019)
- **Journal**: None
- **Summary**: Precise robotic grasping is important for many industrial applications, such as assembly and palletizing, where the location of the object needs to be controlled and known. However, achieving precise grasps is challenging due to noise in sensing and control, as well as unknown object properties. We propose a method to plan robotic grasps that are both robust and precise by training two convolutional neural networks - one to predict the robustness of a grasp and another to predict a distribution of post-grasp object displacements. Our networks are trained with depth images in simulation on a dataset of over 1000 industrial parts and were successfully deployed on a real robot without having to be further fine-tuned. The proposed displacement estimator achieves a mean prediction errors of 0.68cm and 3.42deg on novel objects in real world experiments.



### Understanding Human Gaze Communication by Spatio-Temporal Graph Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1909.02144v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.02144v1)
- **Published**: 2019-09-04 22:50:33+00:00
- **Updated**: 2019-09-04 22:50:33+00:00
- **Authors**: Lifeng Fan, Wenguan Wang, Siyuan Huang, Xinyu Tang, Song-Chun Zhu
- **Comment**: Accepted by ICCV 2019
- **Journal**: None
- **Summary**: This paper addresses a new problem of understanding human gaze communication in social videos from both atomic-level and event-level, which is significant for studying human social interactions. To tackle this novel and challenging problem, we contribute a large-scale video dataset, VACATION, which covers diverse daily social scenes and gaze communication behaviors with complete annotations of objects and human faces, human attention, and communication structures and labels in both atomic-level and event-level. Together with VACATION, we propose a spatio-temporal graph neural network to explicitly represent the diverse gaze interactions in the social scenes and to infer atomic-level gaze communication by message passing. We further propose an event network with encoder-decoder structure to predict the event-level gaze communication. Our experiments demonstrate that the proposed model improves various baselines significantly in predicting the atomic-level and event-level gaze



