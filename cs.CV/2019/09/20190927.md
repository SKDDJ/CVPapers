# Arxiv Papers in cs.CV on 2019-09-27
### Interpreting Undesirable Pixels for Image Classification on Black-Box Models
- **Arxiv ID**: http://arxiv.org/abs/1909.12446v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12446v2)
- **Published**: 2019-09-27 00:25:43+00:00
- **Updated**: 2019-12-16 11:04:15+00:00
- **Authors**: Sin-Han Kang, Hong-Gyu Jung, Seong-Whan Lee
- **Comment**: Accepted to 2019 ICCV Workshop on Interpreting and Explaining Visual
  Artificial Intelligence Models
- **Journal**: None
- **Summary**: In an effort to interpret black-box models, researches for developing explanation methods have proceeded in recent years. Most studies have tried to identify input pixels that are crucial to the prediction of a classifier. While this approach is meaningful to analyse the characteristic of blackbox models, it is also important to investigate pixels that interfere with the prediction. To tackle this issue, in this paper, we propose an explanation method that visualizes undesirable regions to classify an image as a target class. To be specific, we divide the concept of undesirable regions into two terms: (1) factors for a target class, which hinder that black-box models identify intrinsic characteristics of a target class and (2) factors for non-target classes that are important regions for an image to be classified as other classes. We visualize such undesirable regions on heatmaps to qualitatively validate the proposed method. Furthermore, we present an evaluation metric to provide quantitative results on ImageNet.



### ASSD: Attentive Single Shot Multibox Detector
- **Arxiv ID**: http://arxiv.org/abs/1909.12456v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12456v1)
- **Published**: 2019-09-27 01:36:41+00:00
- **Updated**: 2019-09-27 01:36:41+00:00
- **Authors**: Jingru Yi, Pengxiang Wu, Dimitris N. Metaxas
- **Comment**: accepted by Computer Vision and Image Understanding
- **Journal**: None
- **Summary**: This paper proposes a new deep neural network for object detection. The proposed network, termed ASSD, builds feature relations in the spatial space of the feature map. With the global relation information, ASSD learns to highlight useful regions on the feature maps while suppressing the irrelevant information, thereby providing reliable guidance for object detection. Compared to methods that rely on complicated CNN layers to refine the feature maps, ASSD is simple in design and is computationally efficient. Experimental results show that ASSD competes favorably with the state-of-the-arts, including SSD, DSSD, FSSD and RetinaNet.



### Leveraging Multimodal Haptic Sensory Data for Robust Cutting
- **Arxiv ID**: http://arxiv.org/abs/1909.12460v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.12460v1)
- **Published**: 2019-09-27 01:54:17+00:00
- **Updated**: 2019-09-27 01:54:17+00:00
- **Authors**: Kevin Zhang, Mohit Sharma, Manuela Veloso, Oliver Kroemer
- **Comment**: Accepted as conference paper at Humanoids'19
- **Journal**: None
- **Summary**: Cutting is a common form of manipulation when working with divisible objects such as food, rope, or clay. Cooking in particular relies heavily on cutting to divide food items into desired shapes. However, cutting food is a challenging task due to the wide range of material properties exhibited by food items. Due to this variability, the same cutting motions cannot be used for all food items. Sensations from contact events, e.g., when placing the knife on the food item, will also vary depending on the material properties, and the robot will need to adapt accordingly. In this paper, we propose using vibrations and force-torque feedback from the interactions to adapt the slicing motions and monitor for contact events. The robot learns neural networks for performing each of these tasks and generalizing across different material properties. By adapting and monitoring the skill executions, the robot is able to reliably cut through more than 20 different types of food items and even detect whether certain food items are fresh or old.



### Sampling the "Inverse Set" of a Neuron: An Approach to Understanding Neural Nets
- **Arxiv ID**: http://arxiv.org/abs/1910.04857v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.04857v2)
- **Published**: 2019-09-27 02:22:43+00:00
- **Updated**: 2020-12-25 00:49:03+00:00
- **Authors**: Suryabhan Singh Hada, Miguel Á. Carreira-Perpiñán
- **Comment**: 15 pages, 9 figures
- **Journal**: None
- **Summary**: With the recent success of deep neural networks in computer vision, it is important to understand the internal working of these networks. What does a given neuron represent? The concepts captured by a neuron may be hard to understand or express in simple terms. The approach we propose in this paper is to characterize the region of input space that excites a given neuron to a certain level; we call this the inverse set. This inverse set is a complicated high dimensional object that we explore by an optimization-based sampling approach. Inspection of samples of this set by a human can reveal regularities that help to understand the neuron. This goes beyond approaches which were limited to finding an image which maximally activates the neuron or using Markov chain Monte Carlo to sample images, but this is very slow, generates samples with little diversity and lacks control over the activation value of the generated samples. Our approach also allows us to explore the intersection of inverse sets of several neurons and other variations.



### DMM-Net: Differentiable Mask-Matching Network for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.12471v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12471v1)
- **Published**: 2019-09-27 02:25:59+00:00
- **Updated**: 2019-09-27 02:25:59+00:00
- **Authors**: Xiaohui Zeng, Renjie Liao, Li Gu, Yuwen Xiong, Sanja Fidler, Raquel Urtasun
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: In this paper, we propose the differentiable mask-matching network (DMM-Net) for solving the video object segmentation problem where the initial object masks are provided. Relying on the Mask R-CNN backbone, we extract mask proposals per frame and formulate the matching between object templates and proposals at one time step as a linear assignment problem where the cost matrix is predicted by a CNN. We propose a differentiable matching layer by unrolling a projected gradient descent algorithm in which the projection exploits the Dykstra's algorithm. We prove that under mild conditions, the matching is guaranteed to converge to the optimum. In practice, it performs similarly to the Hungarian algorithm during inference. Meanwhile, we can back-propagate through it to learn the cost matrix. After matching, a refinement head is leveraged to improve the quality of the matched mask. Our DMM-Net achieves competitive results on the largest video object segmentation dataset YouTube-VOS. On DAVIS 2017, DMM-Net achieves the best performance without online learning on the first frames. Without any fine-tuning, DMM-Net performs comparably to state-of-the-art methods on SegTrack v2 dataset. At last, our matching layer is very simple to implement; we attach the PyTorch code ($<50$ lines) in the supplementary material. Our code is released at https://github.com/ZENGXH/DMM_Net.



### A Radio Signal Modulation Recognition Algorithm Based on Residual Networks and Attention Mechanisms
- **Arxiv ID**: http://arxiv.org/abs/1909.12472v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.12472v1)
- **Published**: 2019-09-27 02:26:19+00:00
- **Updated**: 2019-09-27 02:26:19+00:00
- **Authors**: Ruisen Luo, Tao Hu, Zuodong Tang, Chen Wang, Xiaofeng Gong, Haiyan Tu
- **Comment**: None
- **Journal**: None
- **Summary**: To solve the problem of inaccurate recognition of types of communication signal modulation, a RNN neural network recognition algorithm combining residual block network with attention mechanism is proposed. In this method, 10 kinds of communication signals with Gaussian white noise are generated from standard data sets, such as MASK, MPSK, MFSK, OFDM, 16QAM, AM and FM. Based on the original RNN neural network, residual block network is added to solve the problem of gradient disappearance caused by deep network layers. Attention mechanism is added to the network to accelerate the gradient descent. In the experiment, 16QAM, 2FSK and 4FSK are used as actual samples, IQ data frames of signals are used as input, and the RNN neural network combined with residual block network and attention mechanism is trained. The final recognition results show that the average recognition rate of real-time signals is over 93%. The network has high robustness and good use value.



### Style Transfer by Rigid Alignment in Neural Net Feature Space
- **Arxiv ID**: http://arxiv.org/abs/1909.13690v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.13690v2)
- **Published**: 2019-09-27 02:54:00+00:00
- **Updated**: 2020-12-24 08:03:17+00:00
- **Authors**: Suryabhan Singh Hada, Miguel Á. Carreira-Perpiñán
- **Comment**: Accepted to WACV 2021
- **Journal**: None
- **Summary**: Arbitrary style transfer is an important problem in computer vision that aims to transfer style patterns from an arbitrary style image to a given content image. However, current methods either rely on slow iterative optimization or fast pre-determined feature transformation, but at the cost of compromised visual quality of the styled image; especially, distorted content structure. In this work, we present an effective and efficient approach for arbitrary style transfer that seamlessly transfers style patterns as well as keep content structure intact in the styled image. We achieve this by aligning style features to content features using rigid alignment; thus modifying style features, unlike the existing methods that do the opposite. We demonstrate the effectiveness of the proposed approach by generating high-quality stylized images and compare the results with the current state-of-the-art techniques for arbitrary style transfer.



### Adaptive ROI Generation for Video Object Segmentation Using Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.12482v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12482v1)
- **Published**: 2019-09-27 03:39:05+00:00
- **Updated**: 2019-09-27 03:39:05+00:00
- **Authors**: Mingjie Sun, Jimin Xiao, Eng Gee Lim, Yanchu Xie, Jiashi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we aim to tackle the task of semi-supervised video object segmentation across a sequence of frames where only the ground-truth segmentation of the first frame is provided. The challenges lie in how to online update the segmentation model initialized from the first frame adaptively and accurately, even in presence of multiple confusing instances or large object motion. The existing approaches rely on selecting the region of interest for model update, which however, is rough and inflexible, leading to performance degradation. To overcome this limitation, we propose a novel approach which utilizes reinforcement learning to select optimal adaptation areas for each frame, based on the historical segmentation information. The RL model learns to take optimal actions to adjust the region of interest inferred from the previous frame for online model updating. To speed up the model adaption, we further design a novel multi-branch tree based exploration method to fast select the best state action pairs. Our experiments show that our work improves the state-of-the-art of the mean region similarity on DAVIS 2016 dataset to 87.1%.



### Invisible Marker: Automatic Annotation of Segmentation Masks for Object Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1909.12493v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.12493v3)
- **Published**: 2019-09-27 04:49:47+00:00
- **Updated**: 2020-08-02 02:59:33+00:00
- **Authors**: Kuniyuki Takahashi, Kenta Yonekura
- **Comment**: 8 pages. Accepted to IROS 2020. An accompanying video is available at
  https://youtu.be/fnpyDYUvDA4
- **Journal**: None
- **Summary**: We propose a method to annotate segmentation masks accurately and automatically using invisible marker for object manipulation. Invisible marker is invisible under visible (regular) light conditions, but becomes visible under invisible light, such as ultraviolet (UV) light. By painting objects with the invisible marker, and by capturing images while alternately switching between regular and UV light at high speed, massive annotated datasets are created quickly and inexpensively. We show a comparison between our proposed method and manual annotations. We demonstrate semantic segmentation for deformable objects including clothes, liquids, and powders under controlled environmental light conditions. In addition, we show demonstrations of liquid pouring tasks under uncontrolled environmental light conditions in complex environments such as inside the office, house, and outdoors. Furthermore, it is possible to capture data while the camera is in motion so it becomes easier to capture large datasets, as shown in our demonstration.



### Region-wise Generative Adversarial ImageInpainting for Large Missing Areas
- **Arxiv ID**: http://arxiv.org/abs/1909.12507v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12507v1)
- **Published**: 2019-09-27 06:07:08+00:00
- **Updated**: 2019-09-27 06:07:08+00:00
- **Authors**: Yuqing Ma, Xianglong Liu, Shihao Bai, Lei Wang, Aishan Liu, Dacheng Tao, Edwin Hancock
- **Comment**: 13 pages, 8 figures, 3 tables
- **Journal**: None
- **Summary**: Recently deep neutral networks have achieved promising performance for filling large missing regions in image inpainting tasks. They usually adopted the standard convolutional architecture over the corrupted image, leading to meaningless contents, such as color discrepancy, blur and artifacts. Moreover, most inpainting approaches cannot well handle the large continuous missing area cases. To address these problems, we propose a generic inpainting framework capable of handling with incomplete images on both continuous and discontinuous large missing areas, in an adversarial manner. From which, region-wise convolution is deployed in both generator and discriminator to separately handle with the different regions, namely existing regions and missing ones. Moreover, a correlation loss is introduced to capture the non-local correlations between different patches, and thus guides the generator to obtain more information during inference. With the help of our proposed framework, we can restore semantically reasonable and visually realistic images. Extensive experiments on three widely-used datasets for image inpainting tasks have been conducted, and both qualitative and quantitative experimental results demonstrate that the proposed model significantly outperforms the state-of-the-art approaches, both on the large continuous and discontinuous missing areas.



### Learnable Tree Filter for Structure-preserving Feature Transform
- **Arxiv ID**: http://arxiv.org/abs/1909.12513v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1909.12513v1)
- **Published**: 2019-09-27 06:36:39+00:00
- **Updated**: 2019-09-27 06:36:39+00:00
- **Authors**: Lin Song, Yanwei Li, Zeming Li, Gang Yu, Hongbin Sun, Jian Sun, Nanning Zheng
- **Comment**: Accepted by NeurIPS-2019
- **Journal**: None
- **Summary**: Learning discriminative global features plays a vital role in semantic segmentation. And most of the existing methods adopt stacks of local convolutions or non-local blocks to capture long-range context. However, due to the absence of spatial structure preservation, these operators ignore the object details when enlarging receptive fields. In this paper, we propose the learnable tree filter to form a generic tree filtering module that leverages the structural property of minimal spanning tree to model long-range dependencies while preserving the details. Furthermore, we propose a highly efficient linear-time algorithm to reduce resource consumption. Thus, the designed modules can be plugged into existing deep neural networks conveniently. To this end, tree filtering modules are embedded to formulate a unified framework for semantic segmentation. We conduct extensive ablation studies to elaborate on the effectiveness and efficiency of the proposed method. Specifically, it attains better performance with much less overhead compared with the classic PSP block and Non-local operation under the same backbone. Our approach is proved to achieve consistent improvements on several benchmarks without bells-and-whistles. Code and models are available at https://github.com/StevenGrove/TreeFilter-Torch.



### GA-GAN: CT reconstruction from Biplanar DRRs using GAN with Guided Attention
- **Arxiv ID**: http://arxiv.org/abs/1909.12525v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.12525v2)
- **Published**: 2019-09-27 07:25:03+00:00
- **Updated**: 2019-10-13 12:19:55+00:00
- **Authors**: Ashish Sinha, Yohei Sugawara, Yuichiro Hirano
- **Comment**: 4 pages, 4 figures, NeurIPS workshop
- **Journal**: None
- **Summary**: This work investigates the use of guided attention in the reconstruction of CTvolumes from biplanar DRRs. We try to improve the visual image quality of the CT reconstruction using Guided Attention based GANs (GA-GAN). We also consider the use of Vector Quantization (VQ) for the CT reconstruction so that the memory usage can be reduced, maintaining the same visual image quality. To the best of our knowledge no work has been done before that explores the Vector Quantization for this purpose. Although our findings show that our approaches outperform the previous works, still there is a lot of room for improvement.



### Fitting IVIM with Variable Projection and Simplicial Optimization
- **Arxiv ID**: http://arxiv.org/abs/1910.00095v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1910.00095v3)
- **Published**: 2019-09-27 07:40:16+00:00
- **Updated**: 2020-02-15 19:49:11+00:00
- **Authors**: Shreyas Fadnavis, Hamza Farooq, Maryam Afzali, Christoph Lenglet, Tryphon Georgiou, Hu Cheng, Sharlene Newman, Shahnawaz Ahmed, Rafael Neto Henriques, Eric Peterson, Serge Koudoro, Ariel Rokem, Eleftherios Garyfallidis
- **Comment**: None
- **Journal**: None
- **Summary**: Fitting multi-exponential models to Diffusion MRI (dMRI) data has always been challenging due to various underlying complexities. In this work, we introduce a novel and robust fitting framework for the standard two-compartment IVIM microstructural model. This framework provides a significant improvement over the existing methods and helps estimate the associated diffusion and perfusion parameters of IVIM in an automatic manner. As a part of this work we provide capabilities to switch between more advanced global optimization methods such as simplicial homology (SH) and differential evolution (DE). Our experiments show that the results obtained from this simultaneous fitting procedure disentangle the model parameters in a reduced subspace. The proposed framework extends the seminal work originated in the MIX framework, with improved procedures for multi-stage fitting. This framework has been made available as an open-source Python implementation and disseminated to the community through the DIPY project.



### Fast shared response model for fMRI data
- **Arxiv ID**: http://arxiv.org/abs/1909.12537v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1909.12537v2)
- **Published**: 2019-09-27 07:46:28+00:00
- **Updated**: 2019-12-03 16:20:24+00:00
- **Authors**: Hugo Richard, Lucas Martin, Ana Luısa Pinho, Jonathan Pillow, Bertrand Thirion
- **Comment**: None
- **Journal**: None
- **Summary**: The shared response model provides a simple but effective framework to analyse fMRI data of subjects exposed to naturalistic stimuli. However when the number of subjects or runs is large, fitting the model requires a large amount of memory and computational power, which limits its use in practice. In this work, we introduce the FastSRM algorithm that relies on an intermediate atlas-based representation. It provides considerable speed-up in time and memory usage, hence it allows easy and fast large-scale analysis of naturalistic-stimulus fMRI data. Using four different datasets, we show that our method matches the performance of the original SRM algorithm while being about 5x faster and 20x to 40x more memory efficient. Based on this contribution, we use FastSRM to predict age from movie watching data on the CamCAN sample. Besides delivering accurate predictions (mean absolute error of 7.5 years), FastSRM extracts topographic patterns that are predictive of age, demonstrating that brain activity during free perception reflects age.



### RGBD-GAN: Unsupervised 3D Representation Learning From Natural Image Datasets via RGBD Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1909.12573v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12573v2)
- **Published**: 2019-09-27 09:10:12+00:00
- **Updated**: 2020-05-25 00:18:28+00:00
- **Authors**: Atsuhiro Noguchi, Tatsuya Harada
- **Comment**: 21 pages, ICLR2020
- **Journal**: None
- **Summary**: Understanding three-dimensional (3D) geometries from two-dimensional (2D) images without any labeled information is promising for understanding the real world without incurring annotation cost. We herein propose a novel generative model, RGBD-GAN, which achieves unsupervised 3D representation learning from 2D images. The proposed method enables camera parameter-conditional image generation and depth image generation without any 3D annotations, such as camera poses or depth. We use an explicit 3D consistency loss for two RGBD images generated from different camera parameters, in addition to the ordinal GAN objective. The loss is simple yet effective for any type of image generator such as DCGAN and StyleGAN to be conditioned on camera parameters. Through experiments, we demonstrated that the proposed method could learn 3D representations from 2D images with various generator architectures.



### Pruning from Scratch
- **Arxiv ID**: http://arxiv.org/abs/1909.12579v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12579v1)
- **Published**: 2019-09-27 09:38:31+00:00
- **Updated**: 2019-09-27 09:38:31+00:00
- **Authors**: Yulong Wang, Xiaolu Zhang, Lingxi Xie, Jun Zhou, Hang Su, Bo Zhang, Xiaolin Hu
- **Comment**: 12 pages, 9 figures
- **Journal**: None
- **Summary**: Network pruning is an important research field aiming at reducing computational costs of neural networks. Conventional approaches follow a fixed paradigm which first trains a large and redundant network, and then determines which units (e.g., channels) are less important and thus can be removed. In this work, we find that pre-training an over-parameterized model is not necessary for obtaining the target pruned structure. In fact, a fully-trained over-parameterized model will reduce the search space for the pruned structure. We empirically show that more diverse pruned structures can be directly pruned from randomly initialized weights, including potential models with better performance. Therefore, we propose a novel network pruning pipeline which allows pruning from scratch. In the experiments for compressing classification models on CIFAR10 and ImageNet datasets, our approach not only greatly reduces the pre-training burden of traditional pruning methods, but also achieves similar or even higher accuracy under the same computation budgets. Our results facilitate the community to rethink the effectiveness of existing techniques used for network pruning.



### Active Learning for Event Detection in Support of Disaster Analysis Applications
- **Arxiv ID**: http://arxiv.org/abs/1909.12601v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1909.12601v1)
- **Published**: 2019-09-27 10:28:10+00:00
- **Updated**: 2019-09-27 10:28:10+00:00
- **Authors**: Naina Said, Kashif Ahmad, Nicola Conci, Ala Al-Fuqaha
- **Comment**: None
- **Journal**: None
- **Summary**: Disaster analysis in social media content is one of the interesting research domains having abundance of data. However, there is a lack of labeled data that can be used to train machine learning models for disaster analysis applications. Active learning is one of the possible solutions to such problem. To this aim, in this paper we propose and assess the efficacy of an active learning based framework for disaster analysis using images shared on social media outlets. Specifically, we analyze the performance of different active learning techniques employing several sampling and disagreement strategies. Moreover, we collect a large-scale dataset covering images from eight common types of natural disasters. The experimental results show that the use of active learning techniques for disaster analysis using images results in a performance comparable to that obtained using human annotated images, and could be used in frameworks for disaster analysis in images without tedious job of manual annotation.



### Towards Real-Time Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1909.12605v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12605v2)
- **Published**: 2019-09-27 10:35:23+00:00
- **Updated**: 2020-07-14 08:22:46+00:00
- **Authors**: Zhongdao Wang, Liang Zheng, Yixuan Liu, Yali Li, Shengjin Wang
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: Modern multiple object tracking (MOT) systems usually follow the \emph{tracking-by-detection} paradigm. It has 1) a detection model for target localization and 2) an appearance embedding model for data association. Having the two models separately executed might lead to efficiency problems, as the running time is simply a sum of the two steps without investigating potential structures that can be shared between them. Existing research efforts on real-time MOT usually focus on the association step, so they are essentially real-time association methods but not real-time MOT system. In this paper, we propose an MOT system that allows target detection and appearance embedding to be learned in a shared model. Specifically, we incorporate the appearance embedding model into a single-shot detector, such that the model can simultaneously output detections and the corresponding embeddings. We further propose a simple and fast association method that works in conjunction with the joint model. In both components the computation cost is significantly reduced compared with former MOT systems, resulting in a neat and fast baseline for future follow-ups on real-time MOT algorithm design. To our knowledge, this work reports the first (near) real-time MOT system, with a running speed of 22 to 40 FPS depending on the input resolution. Meanwhile, its tracking accuracy is comparable to the state-of-the-art trackers embodying separate detection and embedding (SDE) learning ($64.4\%$ MOTA \vs $66.1\%$ MOTA on MOT-16 challenge). Code and models are available at \url{https://github.com/Zhongdao/Towards-Realtime-MOT}.



### Biomedical Image Segmentation by Retina-like Sequential Attention Mechanism Using Only A Few Training Images
- **Arxiv ID**: http://arxiv.org/abs/1909.12612v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1909.12612v1)
- **Published**: 2019-09-27 10:55:24+00:00
- **Updated**: 2019-09-27 10:55:24+00:00
- **Authors**: Shohei Hayashi, Bisser Raytchev, Toru Tamaki, Kazufumi Kaneda
- **Comment**: Submitted to MLMI 2019
- **Journal**: None
- **Summary**: In this paper we propose a novel deep learning-based algorithm for biomedical image segmentation which uses a sequential attention mechanism able to shift the focus of attention across the image in a selective way, allowing subareas which are more difficult to classify to be processed at increased resolution. The spatial distribution of class information in each subarea is learned using a retina-like representation where resolution decreases with distance from the center of attention. The final segmentation is achieved by averaging class predictions over overlapping subareas, utilizing the power of ensemble learning to increase segmentation accuracy. Experimental results for semantic segmentation task for which only a few training images are available show that a CNN using the proposed method outperforms both a patch-based classification CNN and a fully convolutional-based method.



### On the Anomalous Generalization of GANs
- **Arxiv ID**: http://arxiv.org/abs/1909.12638v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.12638v2)
- **Published**: 2019-09-27 12:00:41+00:00
- **Updated**: 2019-10-06 06:27:06+00:00
- **Authors**: Jinchen Xuan, Yunchang Yang, Ze Yang, Di He, Liwei Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Generative models, especially Generative Adversarial Networks (GANs), have received significant attention recently. However, it has been observed that in terms of some attributes, e.g. the number of simple geometric primitives in an image, GANs are not able to learn the target distribution in practice. Motivated by this observation, we discover two specific problems of GANs leading to anomalous generalization behaviour, which we refer to as the sample insufficiency and the pixel-wise combination. For the first problem of sample insufficiency, we show theoretically and empirically that the batchsize of the training samples in practice may be insufficient for the discriminator to learn an accurate discrimination function. It could result in unstable training dynamics for the generator, leading to anomalous generalization. For the second problem of pixel-wise combination, we find that besides recognizing the positive training samples as real, under certain circumstances, the discriminator could be fooled to recognize the pixel-wise combinations (e.g. pixel-wise average) of the positive training samples as real. However, those combinations could be visually different from the real samples in the target distribution. With the fooled discriminator as reference, the generator would obtain biased supervision further, leading to the anomalous generalization behaviour. Additionally, in this paper, we propose methods to mitigate the anomalous generalization of GANs. Extensive experiments on benchmark show our proposed methods improve the FID score up to 30\% on natural image dataset.



### Point Attention Network for Semantic Segmentation of 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1909.12663v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12663v1)
- **Published**: 2019-09-27 13:00:03+00:00
- **Updated**: 2019-09-27 13:00:03+00:00
- **Authors**: Mingtao Feng, Liang Zhang, Xuefei Lin, Syed Zulqarnain Gilani, Ajmal Mian
- **Comment**: Submitted to a journal
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have performed extremely well on data represented by regularly arranged grids such as images. However, directly leveraging the classic convolution kernels or parameter sharing mechanisms on sparse 3D point clouds is inefficient due to their irregular and unordered nature. We propose a point attention network that learns rich local shape features and their contextual correlations for 3D point cloud semantic segmentation. Since the geometric distribution of the neighboring points is invariant to the point ordering, we propose a Local Attention-Edge Convolution (LAE Conv) to construct a local graph based on the neighborhood points searched in multi-directions. We assign attention coefficients to each edge and then aggregate the point features as a weighted sum of its neighbors. The learned LAE-Conv layer features are then given to a point-wise spatial attention module to generate an interdependency matrix of all points regardless of their distances, which captures long-range spatial contextual features contributing to more precise semantic information. The proposed point attention network consists of an encoder and decoder which, together with the LAE-Conv layers and the point-wise spatial attention modules, make it an end-to-end trainable network for predicting dense labels for 3D point cloud segmentation. Experiments on challenging benchmarks of 3D point clouds show that our algorithm can perform at par or better than the existing state of the art methods.



### A Constructive Prediction of the Generalization Error Across Scales
- **Arxiv ID**: http://arxiv.org/abs/1909.12673v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.12673v2)
- **Published**: 2019-09-27 13:27:53+00:00
- **Updated**: 2019-12-20 18:20:34+00:00
- **Authors**: Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, Nir Shavit
- **Comment**: ICLR 2020
- **Journal**: None
- **Summary**: The dependency of the generalization error of neural networks on model and dataset size is of critical importance both in practice and for understanding the theory of neural networks. Nevertheless, the functional form of this dependency remains elusive. In this work, we present a functional form which approximates well the generalization error in practice. Capitalizing on the successful concept of model scaling (e.g., width, depth), we are able to simultaneously construct such a form and specify the exact models which can attain it across model/data scales. Our construction follows insights obtained from observations conducted over a range of model/data scales, in various model types and datasets, in vision and language tasks. We show that the form both fits the observations well across scales, and provides accurate predictions from small- to large-scale models and data.



### Maximal adversarial perturbations for obfuscation: Hiding certain attributes while preserving rest
- **Arxiv ID**: http://arxiv.org/abs/1909.12734v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.12734v1)
- **Published**: 2019-09-27 15:08:46+00:00
- **Updated**: 2019-09-27 15:08:46+00:00
- **Authors**: Indu Ilanchezian, Praneeth Vepakomma, Abhishek Singh, Otkrist Gupta, G. N. Srinivasa Prasanna, Ramesh Raskar
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we investigate the usage of adversarial perturbations for the purpose of privacy from human perception and model (machine) based detection. We employ adversarial perturbations for obfuscating certain variables in raw data while preserving the rest. Current adversarial perturbation methods are used for data poisoning with minimal perturbations of the raw data such that the machine learning model's performance is adversely impacted while the human vision cannot perceive the difference in the poisoned dataset due to minimal nature of perturbations. We instead apply relatively maximal perturbations of raw data to conditionally damage model's classification of one attribute while preserving the model performance over another attribute. In addition, the maximal nature of perturbation helps adversely impact human perception in classifying hidden attribute apart from impacting model performance. We validate our result qualitatively by showing the obfuscated dataset and quantitatively by showing the inability of models trained on clean data to predict the hidden attribute from the perturbed dataset while being able to predict the rest of attributes.



### MRCNet: Crowd Counting and Density Map Estimation in Aerial and Ground Imagery
- **Arxiv ID**: http://arxiv.org/abs/1909.12743v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.12743v1)
- **Published**: 2019-09-27 15:22:23+00:00
- **Updated**: 2019-09-27 15:22:23+00:00
- **Authors**: Reza Bahmanyar, Elenora Vig, Peter Reinartz
- **Comment**: None
- **Journal**: BMVC Workshop on Object Detection and Recognition for Security
  Screenin (BMVC-ODRSS) 2019
- **Summary**: In spite of the many advantages of aerial imagery for crowd monitoring and management at mass events, datasets of aerial images of crowds are still lacking in the field. As a remedy, in this work we introduce a novel crowd dataset, the DLR Aerial Crowd Dataset (DLR-ACD), which is composed of 33 large aerial images acquired from 16 flight campaigns over mass events with 226,291 persons annotated. To the best of our knowledge, DLR-ACD is the first aerial crowd dataset and will be released publicly. To tackle the problem of accurate crowd counting and density map estimation in aerial images of crowds, this work also proposes a new encoder-decoder convolutional neural network, the so-called Multi-Resolution Crowd Network MRCNet. The encoder is based on the VGG-16 network and the decoder is composed of a set of bilinear upsampling and convolutional layers. Using two losses, one at an earlier level and another at the last level of the decoder, MRCNet estimates crowd counts and high-resolution crowd density maps as two different but interrelated tasks. In addition, MRCNet utilizes contextual and detailed local information by combining high- and low-level features through a number of lateral connections inspired by the Feature Pyramid Network (FPN) technique. We evaluated MRCNet on the proposed DLR-ACD dataset as well as on the ShanghaiTech dataset, a CCTV-based crowd counting benchmark. The results demonstrate that MRCNet outperforms the state-of-the-art crowd counting methods in estimating the crowd counts and density maps for both aerial and CCTV-based images.



### Exploring Pose Priors for Human Pose Estimation with Joint Angle Representations
- **Arxiv ID**: http://arxiv.org/abs/1909.12761v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.12761v1)
- **Published**: 2019-09-27 16:01:10+00:00
- **Updated**: 2019-09-27 16:01:10+00:00
- **Authors**: Yaadhav Raaj
- **Comment**: None
- **Journal**: None
- **Summary**: Pose Priors are critical in human pose estimation, since they are able to enforce constraints that prevent estimated poses from tending to physically impossible positions. Human pose generally consists of up to 22 Joint Angles of various segments, and their respective bone lengths, but the way these various segments interact can affect the validity of a pose. Looking at the Knee-Ankle segment alone, we can observe that clearly, the Knee cannot bend forward beyond it's roughly 90 degree point, amongst various other impossible poses below.



### SegMap: Segment-based mapping and localization using data-driven descriptors
- **Arxiv ID**: http://arxiv.org/abs/1909.12837v1
- **DOI**: 10.1177/0278364919863090
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.12837v1)
- **Published**: 2019-09-27 16:02:02+00:00
- **Updated**: 2019-09-27 16:02:02+00:00
- **Authors**: Renaud Dubé, Andrei Cramariuc, Daniel Dugas, Hannes Sommer, Marcin Dymczyk, Juan Nieto, Roland Siegwart, Cesar Cadena
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1804.09557
- **Journal**: None
- **Summary**: Precisely estimating a robot's pose in a prior, global map is a fundamental capability for mobile robotics, e.g. autonomous driving or exploration in disaster zones. This task, however, remains challenging in unstructured, dynamic environments, where local features are not discriminative enough and global scene descriptors only provide coarse information. We therefore present SegMap: a map representation solution for localization and mapping based on the extraction of segments in 3D point clouds. Working at the level of segments offers increased invariance to view-point and local structural changes, and facilitates real-time processing of large-scale 3D data. SegMap exploits a single compact data-driven descriptor for performing multiple tasks: global localization, 3D dense map reconstruction, and semantic information extraction. The performance of SegMap is evaluated in multiple urban driving and search and rescue experiments. We show that the learned SegMap descriptor has superior segment retrieval capabilities, compared to state-of-the-art handcrafted descriptors. In consequence, we achieve a higher localization accuracy and a 6% increase in recall over state-of-the-art. These segment-based localizations allow us to reduce the open-loop odometry drift by up to 50%. SegMap is open-source available along with easy to run demonstrations.



### Global Sparse Momentum SGD for Pruning Very Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.12778v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.12778v3)
- **Published**: 2019-09-27 16:24:19+00:00
- **Updated**: 2019-10-25 15:39:02+00:00
- **Authors**: Xiaohan Ding, Guiguang Ding, Xiangxin Zhou, Yuchen Guo, Jungong Han, Ji Liu
- **Comment**: Accepted by NeurIPS 2019
- **Journal**: None
- **Summary**: Deep Neural Network (DNN) is powerful but computationally expensive and memory intensive, thus impeding its practical usage on resource-constrained front-end devices. DNN pruning is an approach for deep model compression, which aims at eliminating some parameters with tolerable performance degradation. In this paper, we propose a novel momentum-SGD-based optimization method to reduce the network complexity by on-the-fly pruning. Concretely, given a global compression ratio, we categorize all the parameters into two parts at each training iteration which are updated using different rules. In this way, we gradually zero out the redundant parameters, as we update them using only the ordinary weight decay but no gradients derived from the objective function. As a departure from prior methods that require heavy human works to tune the layer-wise sparsity ratios, prune by solving complicated non-differentiable problems or finetune the model after pruning, our method is characterized by 1) global compression that automatically finds the appropriate per-layer sparsity ratios; 2) end-to-end training; 3) no need for a time-consuming re-training process after pruning; and 4) superior capability to find better winning tickets which have won the initialization lottery.



### LoGAN: Latent Graph Co-Attention Network for Weakly-Supervised Video Moment Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1909.13784v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.13784v2)
- **Published**: 2019-09-27 16:26:30+00:00
- **Updated**: 2020-03-28 18:11:37+00:00
- **Authors**: Reuben Tan, Huijuan Xu, Kate Saenko, Bryan A. Plummer
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of weakly-supervised video moment retrieval is to localize the video segment most relevant to the given natural language query without access to temporal annotations during training. Prior strongly- and weakly-supervised approaches often leverage co-attention mechanisms to learn visual-semantic representations for localization. However, while such approaches tend to focus on identifying relationships between elements of the video and language modalities, there is less emphasis on modeling relational context between video frames given the semantic context of the query. Consequently, the above-mentioned visual-semantic representations, built upon local frame features, do not contain much contextual information. To address this limitation, we propose a Latent Graph Co-Attention Network (LoGAN) that exploits fine-grained frame-by-word interactions to reason about correspondences between all possible pairs of frames, given the semantic context of the query. Comprehensive experiments across two datasets, DiDeMo and Charades-Sta, demonstrate the effectiveness of our proposed latent co-attention model where it outperforms current state-of-the-art (SOTA) weakly-supervised approaches by a significant margin. Notably, it even achieves a 11% improvement to Recall@1 accuracy over strongly-supervised SOTA methods on DiDeMo.



### Learning to Have an Ear for Face Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1909.12780v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.12780v3)
- **Published**: 2019-09-27 16:28:55+00:00
- **Updated**: 2020-04-02 16:14:12+00:00
- **Authors**: Givi Meishvili, Simon Jenni, Paolo Favaro
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel method to use both audio and a low-resolution image to perform extreme face super-resolution (a 16x increase of the input size). When the resolution of the input image is very low (e.g., 8x8 pixels), the loss of information is so dire that important details of the original identity have been lost and audio can aid the recovery of a plausible high-resolution image. In fact, audio carries information about facial attributes, such as gender and age. To combine the aural and visual modalities, we propose a method to first build the latent representations of a face from the lone audio track and then from the lone low-resolution image. We then train a network to fuse these two representations. We show experimentally that audio can assist in recovering attributes such as the gender, the age and the identity, and thus improve the correctness of the high-resolution image reconstruction process. Our procedure does not make use of human annotation and thus can be easily trained with existing video datasets. Moreover, we show that our model builds a factorized representation of images and audio as it allows one to mix low-resolution images and audio from different videos and to generate realistic faces with semantically meaningful combinations.



### Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the Loop
- **Arxiv ID**: http://arxiv.org/abs/1909.12828v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12828v1)
- **Published**: 2019-09-27 17:56:35+00:00
- **Updated**: 2019-09-27 17:56:35+00:00
- **Authors**: Nikos Kolotouros, Georgios Pavlakos, Michael J. Black, Kostas Daniilidis
- **Comment**: To appear at ICCV 2019. Project page:
  https://seas.upenn.edu/~nkolot/projects/spin
- **Journal**: None
- **Summary**: Model-based human pose estimation is currently approached through two different paradigms. Optimization-based methods fit a parametric body model to 2D observations in an iterative manner, leading to accurate image-model alignments, but are often slow and sensitive to the initialization. In contrast, regression-based methods, that use a deep network to directly estimate the model parameters from pixels, tend to provide reasonable, but not pixel accurate, results while requiring huge amounts of supervision. In this work, instead of investigating which approach is better, our key insight is that the two paradigms can form a strong collaboration. A reasonable, directly regressed estimate from the network can initialize the iterative optimization making the fitting faster and more accurate. Similarly, a pixel accurate fit from iterative optimization can act as strong supervision for the network. This is the core of our proposed approach SPIN (SMPL oPtimization IN the loop). The deep network initializes an iterative optimization routine that fits the body model to 2D joints within the training loop, and the fitted estimate is subsequently used to supervise the network. Our approach is self-improving by nature, since better network estimates can lead the optimization to better solutions, while more accurate optimization fits provide better supervision for the network. We demonstrate the effectiveness of our approach in different settings, where 3D ground truth is scarce, or not available, and we consistently outperform the state-of-the-art model-based pose estimation approaches by significant margins. The project website with videos, results, and code can be found at https://seas.upenn.edu/~nkolot/projects/spin.



### A Topological Nomenclature for 3D Shape Analysis in Connectomics
- **Arxiv ID**: http://arxiv.org/abs/1909.12887v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12887v2)
- **Published**: 2019-09-27 19:42:20+00:00
- **Updated**: 2020-03-29 21:15:18+00:00
- **Authors**: Abhimanyu Talwar, Zudi Lin, Donglai Wei, Yuesong Wu, Bowen Zheng, Jinglin Zhao, Won-Dong Jang, Xueying Wang, Jeff W. Lichtman, Hanspeter Pfister
- **Comment**: Technical report
- **Journal**: Computer Vision for Microscopy Image Analysis: CVPR2020 workshop
- **Summary**: One of the essential tasks in connectomics is the morphology analysis of neurons and organelles like mitochondria to shed light on their biological properties. However, these biological objects often have tangled parts or complex branching patterns, which make it hard to abstract, categorize, and manipulate their morphology. In this paper, we develop a novel topological nomenclature system to name these objects like the appellation for chemical compounds to promote neuroscience analysis based on their skeletal structures. We first convert the volumetric representation into the topology-preserving reduced graph to untangle the objects. Next, we develop nomenclature rules for pyramidal neurons and mitochondria from the reduced graph and finally learn the feature embedding for shape manipulation. In ablation studies, we quantitatively show that graphs generated by our proposed method align with the perception of experts. On 3D shape retrieval and decomposition tasks, we qualitatively demonstrate that the encoded topological nomenclature features achieve better results than state-of-the-art shape descriptors. To advance neuroscience, we will release a 3D segmentation dataset of mitochondria and pyramidal neurons reconstructed from a 100um cube electron microscopy volume with their reduced graph and topological nomenclature annotations. Code is publicly available at https://github.com/donglaiw/ibexHelper.



### A weakly supervised adaptive triplet loss for deep metric learning
- **Arxiv ID**: http://arxiv.org/abs/1909.12939v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12939v1)
- **Published**: 2019-09-27 20:54:42+00:00
- **Updated**: 2019-09-27 20:54:42+00:00
- **Authors**: Xiaonan Zhao, Huan Qi, Rui Luo, Larry Davis
- **Comment**: 4 pages, ICCV Fashion Workshop
- **Journal**: None
- **Summary**: We address the problem of distance metric learning in visual similarity search, defined as learning an image embedding model which projects images into Euclidean space where semantically and visually similar images are closer and dissimilar images are further from one another. We present a weakly supervised adaptive triplet loss (ATL) capable of capturing fine-grained semantic similarity that encourages the learned image embedding models to generalize well on cross-domain data. The method uses weakly labeled product description data to implicitly determine fine grained semantic classes, avoiding the need to annotate large amounts of training data. We evaluate on the Amazon fashion retrieval benchmark and DeepFashion in-shop retrieval data. The method boosts the performance of triplet loss baseline by 10.6% on cross-domain data and out-performs the state-of-art model on all evaluation metrics.



### Deep neural networks for automated classification of colorectal polyps on histopathology slides: A multi-institutional evaluation
- **Arxiv ID**: http://arxiv.org/abs/1909.12959v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.12959v2)
- **Published**: 2019-09-27 21:18:38+00:00
- **Updated**: 2019-11-23 23:03:40+00:00
- **Authors**: Jason W. Wei, Arief A. Suriawinata, Louis J. Vaickus, Bing Ren, Xiaoying Liu, Mikhail Lisovsky, Naofumi Tomita, Behnaz Abdollahi, Adam S. Kim, Dale C. Snover, John A. Baron, Elizabeth L. Barry, Saeed Hassanpour
- **Comment**: None
- **Journal**: None
- **Summary**: Histological classification of colorectal polyps plays a critical role in both screening for colorectal cancer and care of affected patients. An accurate and automated algorithm for the classification of colorectal polyps on digitized histopathology slides could benefit clinicians and patients. Evaluate the performance and assess the generalizability of a deep neural network for colorectal polyp classification on histopathology slide images using a multi-institutional dataset. In this study, we developed a deep neural network for classification of four major colorectal polyp types, tubular adenoma, tubulovillous/villous adenoma, hyperplastic polyp, and sessile serrated adenoma, based on digitized histopathology slides from our institution, Dartmouth-Hitchcock Medical Center (DHMC), in New Hampshire. We evaluated the deep neural network on an internal dataset of 157 histopathology slide images from DHMC, as well as on an external dataset of 238 histopathology slide images from 24 different institutions spanning 13 states in the United States. We measured accuracy, sensitivity, and specificity of our model in this evaluation and compared its performance to local pathologists' diagnoses at the point-of-care retrieved from corresponding pathology laboratories. For the internal evaluation, the deep neural network had a mean accuracy of 93.5% (95% CI 89.6%-97.4%), compared with local pathologists' accuracy of 91.4% (95% CI 87.0%-95.8%). On the external test set, the deep neural network achieved an accuracy of 87.0% (95% CI 82.7%-91.3%), comparable with local pathologists' accuracy of 86.6% (95% CI 82.3%-90.9%). If confirmed in clinical settings, our model could assist pathologists by improving the diagnostic efficiency, reproducibility, and accuracy of colorectal cancer screenings.



### Celeb-DF: A Large-scale Challenging Dataset for DeepFake Forensics
- **Arxiv ID**: http://arxiv.org/abs/1909.12962v4
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.12962v4)
- **Published**: 2019-09-27 21:26:34+00:00
- **Updated**: 2020-03-16 16:20:16+00:00
- **Authors**: Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, Siwei Lyu
- **Comment**: None
- **Journal**: None
- **Summary**: AI-synthesized face-swapping videos, commonly known as DeepFakes, is an emerging problem threatening the trustworthiness of online information. The need to develop and evaluate DeepFake detection algorithms calls for large-scale datasets. However, current DeepFake datasets suffer from low visual quality and do not resemble DeepFake videos circulated on the Internet. We present a new large-scale challenging DeepFake video dataset, Celeb-DF, which contains 5,639 high-quality DeepFake videos of celebrities generated using improved synthesis process. We conduct a comprehensive evaluation of DeepFake detection methods and datasets to demonstrate the escalated level of challenges posed by Celeb-DF.



### Visual Explanation for Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.12977v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12977v4)
- **Published**: 2019-09-27 22:30:58+00:00
- **Updated**: 2021-08-28 21:11:03+00:00
- **Authors**: Sijie Zhu, Taojiannan Yang, Chen Chen
- **Comment**: None
- **Journal**: None
- **Summary**: This work explores the visual explanation for deep metric learning and its applications. As an important problem for learning representation, metric learning has attracted much attention recently, while the interpretation of such model is not as well studied as classification. To this end, we propose an intuitive idea to show where contributes the most to the overall similarity of two input images by decomposing the final activation. Instead of only providing the overall activation map of each image, we propose to generate point-to-point activation intensity between two images so that the relationship between different regions is uncovered. We show that the proposed framework can be directly deployed to a large range of metric learning applications and provides valuable information for understanding the model. Furthermore, our experiments show its effectiveness on two potential applications, i.e. cross-view pattern discovery and interactive retrieval. The source code is available at \url{https://github.com/Jeff-Zilence/Explain_Metric_Learning}.



### MutualNet: Adaptive ConvNet via Mutual Learning from Network Width and Resolution
- **Arxiv ID**: http://arxiv.org/abs/1909.12978v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12978v3)
- **Published**: 2019-09-27 22:33:15+00:00
- **Updated**: 2020-03-23 18:15:15+00:00
- **Authors**: Taojiannan Yang, Sijie Zhu, Chen Chen, Shen Yan, Mi Zhang, Andrew Willis
- **Comment**: None
- **Journal**: None
- **Summary**: We propose the width-resolution mutual learning method (MutualNet) to train a network that is executable at dynamic resource constraints to achieve adaptive accuracy-efficiency trade-offs at runtime. Our method trains a cohort of sub-networks with different widths using different input resolutions to mutually learn multi-scale representations for each sub-network. It achieves consistently better ImageNet top-1 accuracy over the state-of-the-art adaptive network US-Net under different computation constraints, and outperforms the best compound scaled MobileNet in EfficientNet by 1.5%. The superiority of our method is also validated on COCO object detection and instance segmentation as well as transfer learning. Surprisingly, the training strategy of MutualNet can also boost the performance of a single network, which substantially outperforms the powerful AutoAugmentation in both efficiency (GPU search hours: 15000 vs. 0) and accuracy (ImageNet: 77.6% vs. 78.6%). Code is available at \url{https://github.com/taoyang1122/MutualNet}.



### epBRM: Improving a Quality of 3D Object Detection using End Point Box Regression Module
- **Arxiv ID**: http://arxiv.org/abs/1910.04853v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.04853v2)
- **Published**: 2019-09-27 22:42:17+00:00
- **Updated**: 2020-12-23 23:06:28+00:00
- **Authors**: Kiwoo Shin, Masayoshi Tomizuka
- **Comment**: None
- **Journal**: Intelligent Vehicles Symposium 2020
- **Summary**: We present an endpoint box regression module(epBRM), which is designed for predicting precise 3D bounding boxes using raw LiDAR 3D point clouds. The proposed epBRM is built with sequence of small networks and is computationally lightweight. Our approach can improve a 3D object detection performance by predicting more precise 3D bounding box coordinates. The proposed approach requires 40 minutes of training to improve the detection performance. Moreover, epBRM imposes less than 12ms to network inference time for up-to 20 objects.   The proposed approach utilizes a spatial transformation mechanism to simplify the box regression task. Adopting spatial transformation mechanism into epBRM makes it possible to improve the quality of detection with a small sized network.   We conduct in-depth analysis of the effect of various spatial transformation mechanisms applied on raw LiDAR 3D point clouds. We also evaluate the proposed epBRM by applying it to several state-of-the-art 3D object detection systems.   We evaluate our approach on KITTI dataset, a standard 3D object detection benchmark for autonomous vehicles. The proposed epBRM enhances the overlaps between ground truth bounding boxes and detected bounding boxes, and improves 3D object detection. Our proposed method evaluated in KITTI test server outperforms current state-of-the-art approaches.



### MGBPv2: Scaling Up Multi-Grid Back-Projection Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.12983v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.12983v1)
- **Published**: 2019-09-27 23:27:10+00:00
- **Updated**: 2019-09-27 23:27:10+00:00
- **Authors**: Pablo Navarrete Michelini, Wenbin Chen, Hanwen Liu, Dan Zhu
- **Comment**: In ICCV 2019 Workshops. Winner of Perceptual track in AIM Extreme
  Super-Resolution Challenge 2019. Code available at
  https://github.com/pnavarre/mgbpv2
- **Journal**: None
- **Summary**: Here, we describe our solution for the AIM-2019 Extreme Super-Resolution Challenge, where we won the 1st place in terms of perceptual quality (MOS) similar to the ground truth and achieved the 5th place in terms of high-fidelity (PSNR). To tackle this challenge, we introduce the second generation of MultiGrid BackProjection networks (MGBPv2) whose major modifications make the system scalable and more general than its predecessor. It combines the scalability of the multigrid algorithm and the performance of iterative backprojections. In its original form, MGBP is limited to a small number of parameters due to a strongly recursive structure. In MGBPv2, we make full use of the multigrid recursion from the beginning of the network; we allow different parameters in every module of the network; we simplify the main modules; and finally, we allow adjustments of the number of network features based on the scale of operation. For inference tasks, we introduce an overlapping patch approach to further allow processing of very large images (e.g. 8K). Our training strategies make use of a multiscale loss, combining distortion and/or perception losses on the output as well as downscaled output images. The final system can balance between high quality and high performance.



