# Arxiv Papers in cs.CV on 2019-09-14
### Adversarial Attack on Skeleton-based Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1909.06500v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.06500v1)
- **Published**: 2019-09-14 01:44:44+00:00
- **Updated**: 2019-09-14 01:44:44+00:00
- **Authors**: Jian Liu, Naveed Akhtar, Ajmal Mian
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models achieve impressive performance for skeleton-based human action recognition. However, the robustness of these models to adversarial attacks remains largely unexplored due to their complex spatio-temporal nature that must represent sparse and discrete skeleton joints. This work presents the first adversarial attack on skeleton-based action recognition with graph convolutional networks. The proposed targeted attack, termed Constrained Iterative Attack for Skeleton Actions (CIASA), perturbs joint locations in an action sequence such that the resulting adversarial sequence preserves the temporal coherence, spatial integrity, and the anthropomorphic plausibility of the skeletons. CIASA achieves this feat by satisfying multiple physical constraints, and employing spatial skeleton realignments for the perturbed skeletons along with regularization of the adversarial skeletons with Generative networks. We also explore the possibility of semantically imperceptible localized attacks with CIASA, and succeed in fooling the state-of-the-art skeleton action recognition models with high confidence. CIASA perturbations show high transferability for black-box attacks. We also show that the perturbed skeleton sequences are able to induce adversarial behavior in the RGB videos created with computer graphics. A comprehensive evaluation with NTU and Kinetics datasets ascertains the effectiveness of CIASA for graph-based skeleton action recognition and reveals the imminent threat to the spatio-temporal deep learning tasks in general.



### Fuzzy Knowledge-Based Architecture for Learning and Interaction in Social Robots
- **Arxiv ID**: http://arxiv.org/abs/1909.11004v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1909.11004v1)
- **Published**: 2019-09-14 07:15:37+00:00
- **Updated**: 2019-09-14 07:15:37+00:00
- **Authors**: Mehdi Ghayoumi, Maryam Pourebadi
- **Comment**: 7 pages, AI-HRI 2019
- **Journal**: None
- **Summary**: In this paper, we introduce an extension of our presented cognitive-based emotion model [27][28]and [30], where we enhance our knowledge-based emotion unit of the architecture by embedding a fuzzy rule-based system to it. The model utilizes the cognitive parameters dependency and their corresponding weights to regulate the robot's behavior and fuse their behavior data to achieve the final decision in their interaction with the environment. Using this fuzzy system, our previous model can simulate linguistic parameters for better controlling and generating understandable and flexible behaviors in the robots. We implement our model on an assistive healthcare robot, named Robot Nurse Assistant (RNA) and test it with human subjects. Our model records all the emotion states and essential information based on its predefined rules and learning system. Our results show that our robot interacts with patients in a reasonable, faithful way in special conditions which are defined by rules. This work has the potential to provide better on-demand service for clinical experts to monitor the patients' emotion states and help them make better decisions accordingly.



### Street Crossing Aid Using Light-weight CNNs for the Visually Impaired
- **Arxiv ID**: http://arxiv.org/abs/1909.09598v2
- **DOI**: 10.1109/ICCVW.2019.00317
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.09598v2)
- **Published**: 2019-09-14 11:29:33+00:00
- **Updated**: 2022-06-10 10:31:31+00:00
- **Authors**: Samuel Yu, Heon Lee, Jung Hoon Kim
- **Comment**: 10 pages, 5 figures, 7 tables, ICCV 2019 - 7th International Workshop
  on Assistive Computer Vision and Robotics (ACVR 2019)
- **Journal**: None
- **Summary**: In this paper, we address an issue that the visually impaired commonly face while crossing intersections and propose a solution that takes form as a mobile application. The application utilizes a deep learning convolutional neural network model, LytNetV2, to output necessary information that the visually impaired may lack when without human companions or guide-dogs. A prototype of the application runs on iOS devices of versions 11 or above. It is designed for comprehensiveness, concision, accuracy, and computational efficiency through delivering the two most important pieces of information, pedestrian traffic light color and direction, required to cross the road in real-time. Furthermore, it is specifically aimed to support those facing financial burden as the solution takes the form of a free mobile application. Through the modification and utilization of key principles in MobileNetV3 such as depthwise seperable convolutions and squeeze-excite layers, the deep neural network model achieves a classification accuracy of 96% and average angle error of 6.15 degrees, while running at a frame rate of 16.34 frames per second. Additionally, the model is trained as an image classifier, allowing for a faster and more accurate model. The network is able to outperform other methods such as object detection and non-deep learning algorithms in both accuracy and thoroughness. The information is delivered through both auditory signals and vibrations, and it has been tested on seven visually impaired and has received above satisfactory responses.



### Blind Super-Resolution Kernel Estimation using an Internal-GAN
- **Arxiv ID**: http://arxiv.org/abs/1909.06581v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.06581v6)
- **Published**: 2019-09-14 11:29:55+00:00
- **Updated**: 2020-01-07 06:54:04+00:00
- **Authors**: Sefi Bell-Kligler, Assaf Shocher, Michal Irani
- **Comment**: None
- **Journal**: None
- **Summary**: Super resolution (SR) methods typically assume that the low-resolution (LR) image was downscaled from the unknown high-resolution (HR) image by a fixed 'ideal' downscaling kernel (e.g. Bicubic downscaling). However, this is rarely the case in real LR images, in contrast to synthetically generated SR datasets. When the assumed downscaling kernel deviates from the true one, the performance of SR methods significantly deteriorates. This gave rise to Blind-SR - namely, SR when the downscaling kernel ("SR-kernel") is unknown. It was further shown that the true SR-kernel is the one that maximizes the recurrence of patches across scales of the LR image. In this paper we show how this powerful cross-scale recurrence property can be realized using Deep Internal Learning. We introduce "KernelGAN", an image-specific Internal-GAN, which trains solely on the LR test image at test time, and learns its internal distribution of patches. Its Generator is trained to produce a downscaled version of the LR test image, such that its Discriminator cannot distinguish between the patch distribution of the downscaled image, and the patch distribution of the original LR image. The Generator, once trained, constitutes the downscaling operation with the correct image-specific SR-kernel. KernelGAN is fully unsupervised, requires no training data other than the input image itself, and leads to state-of-the-art results in Blind-SR when plugged into existing SR algorithms.



### Deep Robotic Prediction with hierarchical RGB-D Fusion
- **Arxiv ID**: http://arxiv.org/abs/1909.06585v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.06585v2)
- **Published**: 2019-09-14 12:01:57+00:00
- **Updated**: 2019-09-17 15:35:49+00:00
- **Authors**: Yaoxian Song, Jun Wen, Yuejiao Fei, Changbin Yu
- **Comment**: 8 pages, 8 figures, submit to ICRA2020
- **Journal**: None
- **Summary**: Robotic arm grasping is a fundamental operation in robotic control task goals. Most current methods for robotic grasping focus on RGB-D policy in the table surface scenario or 3D point cloud analysis and inference in the 3D space. Comparing to these methods, we propose a novel real-time multimodal hierarchical encoder-decoder neural network that fuses RGB and depth data to realize robotic humanoid grasping in 3D space with only partial observation. The quantification of raw depth data's uncertainty and depth estimation fusing RGB is considered. We develop a general labeling method to label ground-truth on common RGB-D datasets. We evaluate the effectiveness and performance of our method on a physical robot setup and our method achieves over 90\% success rate in both table surface and 3D space scenarios.



### Sem-LSD: A Learning-based Semantic Line Segment Detector
- **Arxiv ID**: http://arxiv.org/abs/1909.06591v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.06591v2)
- **Published**: 2019-09-14 13:02:00+00:00
- **Updated**: 2019-12-30 11:23:13+00:00
- **Authors**: Yi Sun, Xushen Han, Kai Sun, Boren Li, Yongjiang Chen, Mingyang Li
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduces a new type of line-shaped image representation, named semantic line segment (Sem-LS) and focus on solving its detection problem. Sem-LS contains high-level semantics and is a compact scene representation where only visually salient line segments with stable semantics are preserved. Combined with high-level semantics, Sem-LS is more robust under cluttered environment compared with existing line-shaped representations. The compactness of Sem-LS facilitates its use in large-scale applications, such as city-scale SLAM (simultaneously localization and mapping) and LCD (loop closure detection). Sem-LS detection is a challenging task due to its significantly different appearance from existing learning-based image representations such as wireframes and objects. For further investigation, we first label Sem-LS on two well-known datasets, KITTI and KAIST URBAN, as new benchmarks. Then, we propose a learning-based Sem-LS detector (Sem-LSD) and devise new module as well as metrics to address unique challenges in Sem-LS detection. Experimental results have shown both the efficacy and efficiency of Sem-LSD. Finally, the effectiveness of the proposed Sem-LS is supported by two experiments on detector repeatability and a city-scale LCD problem. Labeled datasets and code will be released shortly.



### Tapering Analysis of Airways with Bronchiectasis
- **Arxiv ID**: http://arxiv.org/abs/1909.06604v1
- **DOI**: 10.1117/12.2292306
- **Categories**: **eess.IV**, cs.CV, physics.med-ph, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1909.06604v1)
- **Published**: 2019-09-14 14:46:53+00:00
- **Updated**: 2019-09-14 14:46:53+00:00
- **Authors**: Kin Quan, Rebecca J. Shipley, Ryutaro Tanno, Graeme McPhillips, Vasileios Vavourakis, David Edwards, Joseph Jacob, John R. Hurst, David J. Hawkes
- **Comment**: 12 pages, 7 figures. Previously submitted for SPIE Medical Imaging,
  2018, Houston, Texas, United States
- **Journal**: Proc. SPIE 10574, Medical Imaging 2018: Image Processing, 105742G
  (2 March 2018)
- **Summary**: Bronchiectasis is the permanent dilation of airways. Patients with the disease can suffer recurrent exacerbations, reducing their quality of life. The gold standard to diagnose and monitor bronchiectasis is accomplished by inspection of chest computed tomography (CT) scans. A clinician examines the broncho-arterial ratio to determine if an airway is brochiectatic. The visual analysis assumes the blood vessel diameter remains constant, although this assumption is disputed in the literature. We propose a simple measurement of tapering along the airways to diagnose and monitor bronchiectasis. To this end, we constructed a pipeline to measure the cross-sectional area along the airways at contiguous intervals, starting from the carina to the most distal point observable. Using a phantom with calibrated 3D printed structures, the precision and accuracy of our algorithm extends to the sub voxel level. The tapering measurement is robust to bifurcations along the airway and was applied to chest CT images acquired in clinical practice. The result is a statistical difference in tapering rate between airways with bronchiectasis and controls. Our code is available at https://github.com/quan14/AirwayTaperingInCT.



### 3D Deep Affine-Invariant Shape Learning for Brain MR Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.06629v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.06629v2)
- **Published**: 2019-09-14 16:47:50+00:00
- **Updated**: 2019-09-17 13:32:09+00:00
- **Authors**: Zhou He, Siqi Bao, Albert Chung
- **Comment**: Accepted to 2018 MICCAI DLMIA, published at Deep Learning in Medical
  Image Analysis and Multimodal Learning for Clinical Decision Support
- **Journal**: None
- **Summary**: Recent advancements in medical image segmentation techniques have achieved compelling results. However, most of the widely used approaches do not take into account any prior knowledge about the shape of the biomedical structures being segmented. More recently, some works have presented approaches to incorporate shape information. However, many of them are indeed introducing more parameters to the segmentation network to learn the general features, which any segmentation network is able learn, instead of specifically shape features. In this paper, we present a novel approach that seamlessly integrates the shape information into the segmentation network. Experiments on human brain MRI segmentation demonstrate that our approach can achieve a lower Hausdorff distance and higher Dice coefficient than the state-of-the-art approaches.



### Joint Wasserstein Autoencoders for Aligning Multimodal Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1909.06635v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.06635v1)
- **Published**: 2019-09-14 17:25:03+00:00
- **Updated**: 2019-09-14 17:25:03+00:00
- **Authors**: Shweta Mahajan, Teresa Botschen, Iryna Gurevych, Stefan Roth
- **Comment**: Accepted at ICCV 2019 Workshop on Cross-Modal Learning in Real World
- **Journal**: None
- **Summary**: One of the key challenges in learning joint embeddings of multiple modalities, e.g. of images and text, is to ensure coherent cross-modal semantics that generalize across datasets. We propose to address this through joint Gaussian regularization of the latent representations. Building on Wasserstein autoencoders (WAEs) to encode the input in each domain, we enforce the latent embeddings to be similar to a Gaussian prior that is shared across the two domains, ensuring compatible continuity of the encoded semantic representations of images and texts. Semantic alignment is achieved through supervision from matching image-text pairs. To show the benefits of our semi-supervised representation, we apply it to cross-modal retrieval and phrase localization. We not only achieve state-of-the-art accuracy, but significantly better generalization across datasets, owing to the semantic continuity of the latent space.



### Metric-Based Few-Shot Learning for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1909.09602v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.09602v1)
- **Published**: 2019-09-14 17:53:16+00:00
- **Updated**: 2019-09-14 17:53:16+00:00
- **Authors**: Chris Careaga, Brian Hutchinson, Nathan Hodas, Lawrence Phillips
- **Comment**: None
- **Journal**: None
- **Summary**: In the few-shot scenario, a learner must effectively generalize to unseen classes given a small support set of labeled examples. While a relatively large amount of research has gone into few-shot learning for image classification, little work has been done on few-shot video classification. In this work, we address the task of few-shot video action recognition with a set of two-stream models. We evaluate the performance of a set of convolutional and recurrent neural network video encoder architectures used in conjunction with three popular metric-based few-shot algorithms. We train and evaluate using a few-shot split of the Kinetics 600 dataset. Our experiments confirm the importance of the two-stream setup, and find prototypical networks and pooled long short-term memory network embeddings to give the best performance as few-shot method and video encoder, respectively. For a 5-shot 5-way task, this setup obtains 84.2% accuracy on the test set and 59.4% on a special "challenge" test set, composed of highly confusable classes.



### Fuzzy Semantic Segmentation of Breast Ultrasound Image with Breast Anatomy Constraints
- **Arxiv ID**: http://arxiv.org/abs/1909.06645v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.06645v3)
- **Published**: 2019-09-14 18:06:46+00:00
- **Updated**: 2019-10-23 16:33:07+00:00
- **Authors**: Kuan Huang, Yingtao Zhang, H. D. Cheng, Ping Xing, Boyu Zhang
- **Comment**: 15 pages, 19 figures, submitted to IEEE Transactions on Fuzzy Systems
- **Journal**: None
- **Summary**: Breast cancer is one of the most serious disease affecting women's health. Due to low cost, portable, no radiation, and high efficiency, breast ultrasound (BUS) imaging is the most popular approach for diagnosing early breast cancer. However, ultrasound images are low resolution and poor quality. Thus, developing accurate detection system is a challenging task. In this paper, we propose a fully automatic segmentation algorithm consisting of two parts: fuzzy fully convolutional network and accurately fine-tuning post-processing based on breast anatomy constraints. In the first part, the image is preprocessed by contrast enhancement, and wavelet features are employed for image augmentation. A fuzzy membership function transforms the augmented BUS images into fuzzy domain. The features from convolutional layers are processed using fuzzy logic as well. The conditional random fields (CRFs) post-process the segmentation result. The location relation among the breast anatomy layers is utilized to improve the performance. The proposed method is applied to the dataset with 325 BUS images, and achieves state-of-art performance compared with that of existing methods with true positive rate 90.33%, false positive rate 9.00%, and intersection over union (IoU) 81.29% on tumor category, and overall intersection over union (mIoU) 80.47% over five categories: fat layer, mammary layer, muscle layer, background, and tumor.



### Progression Modelling for Online and Early Gesture Detection
- **Arxiv ID**: http://arxiv.org/abs/1909.06672v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.06672v1)
- **Published**: 2019-09-14 20:39:35+00:00
- **Updated**: 2019-09-14 20:39:35+00:00
- **Authors**: Vikram Gupta, Sai Kumar Dwivedi, Rishabh Dabral, Arjun Jain
- **Comment**: 3DV 2019 Oral paper
- **Journal**: None
- **Summary**: Online and Early detection of gestures is crucial for building touchless gesture based interfaces. These interfaces should operate on a stream of video frames instead of the complete video and detect the presence of gestures at an earlier stage than post-completion for providing real time user experience. To achieve this, it is important to recognize the progression of the gesture across different stages so that appropriate responses can be triggered on reaching the desired execution stage. To address this, we propose a simple yet effective multi-task learning framework which models the progression of the gesture along with frame level recognition. The proposed framework recognizes the gestures at an early stage with high precision and also achieves state-of-the-art recognition accuracy of 87.8% which is closer to human accuracy of 88.4% on the NVIDIA gesture dataset in the offline configuration and advances the state-of-the-art by more than 4%. We also introduce tightly segmented annotations for the NVIDIA gesture dataset and setup a strong baseline for gesture localization for this dataset. We also evaluate our framework on the Montalbano dataset and report competitive results.



### 3D Kidneys and Kidney Tumor Semantic Segmentation using Boundary-Aware Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.06684v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.06684v1)
- **Published**: 2019-09-14 21:49:09+00:00
- **Updated**: 2019-09-14 21:49:09+00:00
- **Authors**: Andriy Myronenko, Ali Hatamizadeh
- **Comment**: Manuscript of MICCAI Kidney Tumor Segmentation Challenge 2019
- **Journal**: MICCAI Kidney Tumor Segmentation Challenge 2019
- **Summary**: Automated segmentation of kidneys and kidney tumors is an important step in quantifying the tumor's morphometrical details to monitor the progression of the disease and accurately compare decisions regarding the kidney tumor treatment. Manual delineation techniques are often tedious, error-prone and require expert knowledge for creating unambiguous representation of kidneys and kidney tumors segmentation. In this work, we propose an end-to-end boundary aware fully Convolutional Neural Networks (CNNs) for reliable kidney and kidney tumor semantic segmentation from arterial phase abdominal 3D CT scans. We propose a segmentation network consisting of an encoder-decoder architecture that specifically accounts for organ and tumor edge information by devising a dedicated boundary branch supervised by edge-aware loss terms. We have evaluated our model on 2019 MICCAI KiTS Kidney Tumor Segmentation Challenge dataset and our method has achieved dice scores of 0.9742 and 0.8103 for kidney and tumor repetitively and an overall composite dice score of 0.8923.



### Automated Multiclass Cardiac Volume Segmentation and Model Generation
- **Arxiv ID**: http://arxiv.org/abs/1909.06685v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.06685v1)
- **Published**: 2019-09-14 21:58:08+00:00
- **Updated**: 2019-09-14 21:58:08+00:00
- **Authors**: Erik Gaasedelen, Alex Deakyne, Paul Iaizzo
- **Comment**: None
- **Journal**: None
- **Summary**: Many strides have been made in semantic segmentation of multiple classes within an image. This has been largely due to advancements in deep learning and convolutional neural networks (CNNs). Features within a CNN are automatically learned during training, which allows for the abstraction of semantic information within the images. These deep learning models are powerful enough to handle the segmentation of multiple classes without the need for multiple networks. Despite these advancements, few attempts have been made to automatically segment multiple anatomical features within medical imaging datasets obtained from CT or MRI scans. This offers a unique challenge because of the three dimensional nature of medical imaging data. In order to alleviate the 3D modality problem, we propose a multi-axis ensemble method, applied to a dataset of 4-cardiac-chamber segmented CT scans. Inspired by the typical three-axis view used by humans, this technique aims to maximize the 3D spatial information afforded to the model, while remaining efficient for consumer grade inference hardware. Multi-axis ensembling along with pragmatic voxel preprocessing have shown in our experiments to greatly increase the mean intersection over union of our predictions over the complete DICOM dataset.



