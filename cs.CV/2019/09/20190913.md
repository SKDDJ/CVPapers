# Arxiv Papers in cs.CV on 2019-09-13
### Center-Extraction-Based Three Dimensional Nuclei Instance Segmentation of Fluorescence Microscopy Images
- **Arxiv ID**: http://arxiv.org/abs/1909.05992v1
- **DOI**: 10.1109/BHI.2019.8834516
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.05992v1)
- **Published**: 2019-09-13 00:48:00+00:00
- **Updated**: 2019-09-13 00:48:00+00:00
- **Authors**: David Joon Ho, Shuo Han, Chichen Fu, Paul Salama, Kenneth W. Dunn, Edward J. Delp
- **Comment**: Presented at the IEEE-EMBS International Conference on Biomedical and
  Health Informatics (BHI 2019)
- **Journal**: None
- **Summary**: Fluorescence microscopy is an essential tool for the analysis of 3D subcellular structures in tissue. An important step in the characterization of tissue involves nuclei segmentation. In this paper, a two-stage method for segmentation of nuclei using convolutional neural networks (CNNs) is described. In particular, since creating labeled volumes manually for training purposes is not practical due to the size and complexity of the 3D data sets, the paper describes a method for generating synthetic microscopy volumes based on a spatially constrained cycle-consistent adversarial network. The proposed method is tested on multiple real microscopy data sets and outperforms other commonly used segmentation techniques.



### FoodTracker: A Real-time Food Detection Mobile Application by Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.05994v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.05994v2)
- **Published**: 2019-09-13 01:12:53+00:00
- **Updated**: 2019-09-16 00:51:47+00:00
- **Authors**: Jianing Sun, Katarzyna Radecka, Zeljko Zilic
- **Comment**: The 16th International Conference on Machine Vision Applications
- **Journal**: None
- **Summary**: We present a mobile application made to recognize food items of multi-object meal from a single image in real-time, and then return the nutrition facts with components and approximate amounts. Our work is organized in two parts. First, we build a deep convolutional neural network merging with YOLO, a state-of-the-art detection strategy, to achieve simultaneous multi-object recognition and localization with nearly 80% mean average precision. Second, we adapt our model into a mobile application with extending function for nutrition analysis. After inferring and decoding the model output in the app side, we present detection results that include bounding box position and class label in either real-time or local mode. Our model is well-suited for mobile devices with negligible inference time and small memory requirements with a deep learning algorithm.



### Rethinking Zero-Shot Learning: A Conditional Visual Classification Perspective
- **Arxiv ID**: http://arxiv.org/abs/1909.05995v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.05995v2)
- **Published**: 2019-09-13 01:26:55+00:00
- **Updated**: 2019-11-27 16:22:41+00:00
- **Authors**: Kai Li, Martin Renqiang Min, Yun Fu
- **Comment**: Accepted to ICCV 2019. First update: add project link and correct
  some typos
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) aims to recognize instances of unseen classes solely based on the semantic descriptions of the classes. Existing algorithms usually formulate it as a semantic-visual correspondence problem, by learning mappings from one feature space to the other. Despite being reasonable, previous approaches essentially discard the highly precious discriminative power of visual features in an implicit way, and thus produce undesirable results. We instead reformulate ZSL as a conditioned visual classification problem, i.e., classifying visual features based on the classifiers learned from the semantic descriptions. With this reformulation, we develop algorithms targeting various ZSL settings: For the conventional setting, we propose to train a deep neural network that directly generates visual feature classifiers from the semantic attributes with an episode-based training scheme; For the generalized setting, we concatenate the learned highly discriminative classifiers for seen classes and the generated classifiers for unseen classes to classify visual features of all classes; For the transductive setting, we exploit unlabeled data to effectively calibrate the classifier generator using a novel learning-without-forgetting self-training mechanism and guide the process by a robust generalized cross-entropy loss. Extensive experiments show that our proposed algorithms significantly outperform state-of-the-art methods by large margins on most benchmark datasets in all the ZSL settings. Our code is available at \url{https://github.com/kailigo/cvcZSL}



### Towards Generalizable Deepfake Detection with Locality-aware AutoEncoder
- **Arxiv ID**: http://arxiv.org/abs/1909.05999v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.05999v2)
- **Published**: 2019-09-13 02:01:32+00:00
- **Updated**: 2020-09-20 02:27:30+00:00
- **Authors**: Mengnan Du, Shiva Pentyala, Yuening Li, Xia Hu
- **Comment**: Accepted by CIKM 2020
- **Journal**: None
- **Summary**: With advancements of deep learning techniques, it is now possible to generate super-realistic images and videos, i.e., deepfakes. These deepfakes could reach mass audience and result in adverse impacts on our society. Although lots of efforts have been devoted to detect deepfakes, their performance drops significantly on previously unseen but related manipulations and the detection generalization capability remains a problem. Motivated by the fine-grained nature and spatial locality characteristics of deepfakes, we propose Locality-Aware AutoEncoder (LAE) to bridge the generalization gap. In the training process, we use a pixel-wise mask to regularize local interpretation of LAE to enforce the model to learn intrinsic representation from the forgery region, instead of capturing artifacts in the training set and learning superficial correlations to perform detection. We further propose an active learning framework to select the challenging candidates for labeling, which requires human masks for less than 3% of the training data, dramatically reducing the annotation efforts to regularize interpretations. Experimental results on three deepfake detection tasks indicate that LAE could focus on the forgery regions to make decisions. The analysis further shows that LAE outperforms the state-of-the-arts by 6.52%, 12.03%, and 3.08% respectively on three deepfake detection tasks in terms of generalization accuracy on previously unseen manipulations.



### Multiple Partitions Aligned Clustering
- **Arxiv ID**: http://arxiv.org/abs/1909.06008v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.06008v1)
- **Published**: 2019-09-13 02:45:13+00:00
- **Updated**: 2019-09-13 02:45:13+00:00
- **Authors**: Zhao Kang, Zipeng Guo, Shudong Huang, Siying Wang, Wenyu Chen, Yuanzhang Su, Zenglin Xu
- **Comment**: IJCAI 2019
- **Journal**: None
- **Summary**: Multi-view clustering is an important yet challenging task due to the difficulty of integrating the information from multiple representations. Most existing multi-view clustering methods explore the heterogeneous information in the space where the data points lie. Such common practice may cause significant information loss because of unavoidable noise or inconsistency among views. Since different views admit the same cluster structure, the natural space should be all partitions. Orthogonal to existing techniques, in this paper, we propose to leverage the multi-view information by fusing partitions. Specifically, we align each partition to form a consensus cluster indicator matrix through a distinct rotation matrix. Moreover, a weight is assigned for each view to account for the clustering capacity differences of views. Finally, the basic partitions, weights, and consensus clustering are jointly learned in a unified framework. We demonstrate the effectiveness of our approach on several real datasets, where significant improvement is found over other state-of-the-art multi-view clustering methods.



### Geometric Brain Surface Network For Brain Cortical Parcellation
- **Arxiv ID**: http://arxiv.org/abs/1909.13834v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.13834v1)
- **Published**: 2019-09-13 03:36:05+00:00
- **Updated**: 2019-09-13 03:36:05+00:00
- **Authors**: Wen Zhang, Yalin Wang
- **Comment**: 8 pages
- **Journal**: GLMI in Conjunction with MICCAI 2019
- **Summary**: A large number of surface-based analyses on brain imaging data adopt some specific brain atlases to better assess structural and functional changes in one or more brain regions. In these analyses, it is necessary to obtain an anatomically correct surface parcellation scheme in an individual brain by referring to the given atlas. Traditional ways to accomplish this goal are through a designed surface-based registration or hand-crafted surface features, although both of them are time-consuming. A recent deep learning approach depends on a regular spherical parameterization of the mesh, which is computationally prohibitive in some cases and may also demand further post-processing to refine the network output. Therefore, an accurate and fully-automatic cortical surface parcellation scheme directly working on the original brain surfaces would be highly advantageous. In this study, we propose an end-to-end deep brain cortical parcellation network, called \textbf{DBPN}. Through intrinsic and extrinsic graph convolution kernels, DBPN dynamically deciphers neighborhood graph topology around each vertex and encodes the deciphered knowledge into node features. Eventually, a non-linear mapping between the node features and parcellation labels is constructed. Our model is a two-stage deep network which contains a coarse parcellation network with a U-shape structure and a refinement network to fine-tune the coarse results. We evaluate our model in a large public dataset and our work achieves superior performance than state-of-the-art baseline methods in both accuracy and efficiency



### Part-Guided Attention Learning for Vehicle Instance Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1909.06023v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.06023v4)
- **Published**: 2019-09-13 03:58:18+00:00
- **Updated**: 2020-09-26 09:24:41+00:00
- **Authors**: Xinyu Zhang, Rufeng Zhang, Jiewei Cao, Dong Gong, Mingyu You, Chunhua Shen
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Vehicle instance retrieval often requires one to recognize the fine-grained visual differences between vehicles. Besides the holistic appearance of vehicles which is easily affected by the viewpoint variation and distortion, vehicle parts also provide crucial cues to differentiate near-identical vehicles. Motivated by these observations, we introduce a Part-Guided Attention Network (PGAN) to pinpoint the prominent part regions and effectively combine the global and part information for discriminative feature learning. PGAN first detects the locations of different part components and salient regions regardless of the vehicle identity, which serve as the bottom-up attention to narrow down the possible searching regions. To estimate the importance of detected parts, we propose a Part Attention Module (PAM) to adaptively locate the most discriminative regions with high-attention weights and suppress the distraction of irrelevant parts with relatively low weights. The PAM is guided by the instance retrieval loss and therefore provides top-down attention that enables attention to be calculated at the level of car parts and other salient regions. Finally, we aggregate the global appearance and part features to improve the feature performance further. The PGAN combines part-guided bottom-up and top-down attention, global and part visual features in an end-to-end framework. Extensive experiments demonstrate that the proposed method achieves new state-of-the-art vehicle instance retrieval performance on four large-scale benchmark datasets.



### DARTS+: Improved Differentiable Architecture Search with Early Stopping
- **Arxiv ID**: http://arxiv.org/abs/1909.06035v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.06035v2)
- **Published**: 2019-09-13 05:07:57+00:00
- **Updated**: 2020-10-20 06:21:28+00:00
- **Authors**: Hanwen Liang, Shifeng Zhang, Jiacheng Sun, Xingqiu He, Weiran Huang, Kechen Zhuang, Zhenguo Li
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, there has been a growing interest in automating the process of neural architecture design, and the Differentiable Architecture Search (DARTS) method makes the process available within a few GPU days. However, the performance of DARTS is often observed to collapse when the number of search epochs becomes large. Meanwhile, lots of "{\em skip-connect}s" are found in the selected architectures. In this paper, we claim that the cause of the collapse is that there exists overfitting in the optimization of DARTS. Therefore, we propose a simple and effective algorithm, named "DARTS+", to avoid the collapse and improve the original DARTS, by "early stopping" the search procedure when meeting a certain criterion. We also conduct comprehensive experiments on benchmark datasets and different search spaces and show the effectiveness of our DARTS+ algorithm, and DARTS+ achieves $2.32\%$ test error on CIFAR10, $14.87\%$ on CIFAR100, and $23.7\%$ on ImageNet. We further remark that the idea of "early stopping" is implicitly included in some existing DARTS variants by manually setting a small number of search epochs, while we give an {\em explicit} criterion for "early stopping".



### End-to-End Learnable Geometric Vision by Backpropagating PnP Optimization
- **Arxiv ID**: http://arxiv.org/abs/1909.06043v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.06043v3)
- **Published**: 2019-09-13 05:45:25+00:00
- **Updated**: 2020-03-16 08:05:16+00:00
- **Authors**: Bo Chen, Alvaro Parra, Jiewei Cao, Nan Li, Tat-Jun Chin
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Deep networks excel in learning patterns from large amounts of data. On the other hand, many geometric vision tasks are specified as optimization problems. To seamlessly combine deep learning and geometric vision, it is vital to perform learning and geometric optimization end-to-end. Towards this aim, we present BPnP, a novel network module that backpropagates gradients through a Perspective-n-Points (PnP) solver to guide parameter updates of a neural network. Based on implicit differentiation, we show that the gradients of a "self-contained" PnP solver can be derived accurately and efficiently, as if the optimizer block were a differentiable function. We validate BPnP by incorporating it in a deep model that can learn camera intrinsics, camera extrinsics (poses) and 3D structure from training datasets. Further, we develop an end-to-end trainable pipeline for object pose estimation, which achieves greater accuracy by combining feature-based heatmap losses with 2D-3D reprojection errors. Since our approach can be extended to other optimization problems, our work helps to pave the way to perform learnable geometric vision in a principled manner. Our PyTorch implementation of BPnP is available on http://github.com/BoChenYS/BPnP.



### A Collaborative Approach using Ridge-Valley Minutiae for More Accurate Contactless Fingerprint Identification
- **Arxiv ID**: http://arxiv.org/abs/1909.06045v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.06045v2)
- **Published**: 2019-09-13 05:56:52+00:00
- **Updated**: 2019-09-19 07:40:23+00:00
- **Authors**: Ritesh Vyas, Ajay Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: Contactless fingerprint identification has emerged as an reliable and user friendly alternative for the personal identification in a range of e-business and law-enforcement applications. It is however quite known from the literature that the contactless fingerprint images deliver remarkably low matching accuracies as compared with those obtained from the contact-based fingerprint sensors. This paper develops a new approach to significantly improve contactless fingerprint matching capabilities available today. We systematically analyze the extent of complimentary ridge-valley information and introduce new approaches to achieve significantly higher matching accuracy over state-of-art fingerprint matchers commonly employed today. We also investigate least explored options for the fingerprint color-space conversions, which can play a key-role for more accurate contactless fingerprint matching. This paper presents experimental results from different publicly available contactless fingerprint databases using NBIS, MCC and COTS matchers. Our consistently outperforming results validate the effectiveness of the proposed approach for more accurate contactless fingerprint identification.



### Human Following for Wheeled Robot with Monocular Pan-tilt Camera
- **Arxiv ID**: http://arxiv.org/abs/1909.06087v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.06087v1)
- **Published**: 2019-09-13 08:49:12+00:00
- **Updated**: 2019-09-13 08:49:12+00:00
- **Authors**: Zheng Zhu, Hongxuan Ma, Wei Zou
- **Comment**: None
- **Journal**: None
- **Summary**: Human following on mobile robots has witnessed significant advances due to its potentials for real-world applications. Currently most human following systems are equipped with depth sensors to obtain distance information between human and robot, which suffer from the perception requirements and noises. In this paper, we design a wheeled mobile robot system with monocular pan-tilt camera to follow human, which can stay the target in the field of view and keep following simultaneously. The system consists of fast human detector, real-time and accurate visual tracker, and unified controller for mobile robot and pan-tilt camera. In visual tracking algorithm, both Siamese networks and optical flow information are exploited to locate and regress human simultaneously. In order in perform following with a monocular camera, the constraint of human height is introduced to design the controller. In experiments, human following are conducted and analysed in simulations and a real robot platform, which demonstrate the effectiveness and robustness of the overall system.



### Weakly-Supervised 3D Pose Estimation from a Single Image using Multi-View Consistency
- **Arxiv ID**: http://arxiv.org/abs/1909.06119v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.06119v1)
- **Published**: 2019-09-13 09:59:12+00:00
- **Updated**: 2019-09-13 09:59:12+00:00
- **Authors**: Guillaume Rochette, Chris Russell, Richard Bowden
- **Comment**: BMVC
- **Journal**: None
- **Summary**: We present a novel data-driven regularizer for weakly-supervised learning of 3D human pose estimation that eliminates the drift problem that affects existing approaches. We do this by moving the stereo reconstruction problem into the loss of the network itself. This avoids the need to reconstruct 3D data prior to training and unlike previous semi-supervised approaches, avoids the need for a warm-up period of supervised training. The conceptual and implementational simplicity of our approach is fundamental to its appeal. Not only is it straightforward to augment many weakly-supervised approaches with our additional re-projection based loss, but it is obvious how it shapes reconstructions and prevents drift. As such we believe it will be a valuable tool for any researcher working in weakly-supervised 3D reconstruction. Evaluating on Panoptic, the largest multi-camera and markerless dataset available, we obtain an accuracy that is essentially indistinguishable from a strongly-supervised approach making full use of 3D groundtruth in training.



### Dual Graph Convolutional Network for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.06121v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.06121v3)
- **Published**: 2019-09-13 10:06:55+00:00
- **Updated**: 2020-08-26 04:52:15+00:00
- **Authors**: Li Zhang, Xiangtai Li, Anurag Arnab, Kuiyuan Yang, Yunhai Tong, Philip H. S. Torr
- **Comment**: BMVC 2019. Code is available at
  \url{https://github.com/lxtGH/GALD-DGCNet}
- **Journal**: None
- **Summary**: Exploiting long-range contextual information is key for pixel-wise prediction tasks such as semantic segmentation. In contrast to previous work that uses multi-scale feature fusion or dilated convolutions, we propose a novel graph-convolutional network (GCN) to address this problem. Our Dual Graph Convolutional Network (DGCNet) models the global context of the input feature by modelling two orthogonal graphs in a single framework. The first component models spatial relationships between pixels in the image, whilst the second models interdependencies along the channel dimensions of the network's feature map. This is done efficiently by projecting the feature into a new, lower-dimensional space where all pairwise interactions can be modelled, before reprojecting into the original space. Our simple method provides substantial benefits over a strong baseline and achieves state-of-the-art results on both Cityscapes (82.0% mean IoU) and Pascal Context (53.7% mean IoU) datasets. Code and models are made available to foster any further research (\url{https://github.com/lxtGH/GALD-DGCNet}).



### FakeSpotter: A Simple yet Robust Baseline for Spotting AI-Synthesized Fake Faces
- **Arxiv ID**: http://arxiv.org/abs/1909.06122v3
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.06122v3)
- **Published**: 2019-09-13 10:08:44+00:00
- **Updated**: 2020-07-16 06:44:53+00:00
- **Authors**: Run Wang, Felix Juefei-Xu, Lei Ma, Xiaofei Xie, Yihao Huang, Jian Wang, Yang Liu
- **Comment**: Accepted to IJCAI 2020; SOLE copyright holder is IJCAI (international
  Joint Conferences on Artificial Intelligence), all rights reserved.
  https://www.ijcai.org/Proceedings/2020/333
- **Journal**: None
- **Summary**: In recent years, generative adversarial networks (GANs) and its variants have achieved unprecedented success in image synthesis. They are widely adopted in synthesizing facial images which brings potential security concerns to humans as the fakes spread and fuel the misinformation. However, robust detectors of these AI-synthesized fake faces are still in their infancy and are not ready to fully tackle this emerging challenge. In this work, we propose a novel approach, named FakeSpotter, based on monitoring neuron behaviors to spot AI-synthesized fake faces. The studies on neuron coverage and interactions have successfully shown that they can be served as testing criteria for deep learning systems, especially under the settings of being exposed to adversarial attacks. Here, we conjecture that monitoring neuron behavior can also serve as an asset in detecting fake faces since layer-by-layer neuron activation patterns may capture more subtle features that are important for the fake detector. Experimental results on detecting four types of fake faces synthesized with the state-of-the-art GANs and evading four perturbation attacks show the effectiveness and robustness of our approach.



### Video Rain/Snow Removal by Transformed Online Multiscale Convolutional Sparse Coding
- **Arxiv ID**: http://arxiv.org/abs/1909.06148v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.06148v1)
- **Published**: 2019-09-13 11:22:29+00:00
- **Updated**: 2019-09-13 11:22:29+00:00
- **Authors**: Minghan Li, Xiangyong Cao, Qian Zhao, Lei Zhang, Chenqiang Gao, Deyu Meng
- **Comment**: 14 pages, 15 figures
- **Journal**: None
- **Summary**: Video rain/snow removal from surveillance videos is an important task in the computer vision community since rain/snow existed in videos can severely degenerate the performance of many surveillance system. Various methods have been investigated extensively, but most only consider consistent rain/snow under stable background scenes. Rain/snow captured from practical surveillance camera, however, is always highly dynamic in time with the background scene transformed occasionally. To this issue, this paper proposes a novel rain/snow removal approach, which fully considers dynamic statistics of both rain/snow and background scenes taken from a video sequence. Specifically, the rain/snow is encoded as an online multi-scale convolutional sparse coding (OMS-CSC) model, which not only finely delivers the sparse scattering and multi-scale shapes of real rain/snow, but also well encodes their temporally dynamic configurations by real-time ameliorated parameters in the model. Furthermore, a transformation operator imposed on the background scenes is further embedded into the proposed model, which finely conveys the dynamic background transformations, such as rotations, scalings and distortions, inevitably existed in a real video sequence. The approach so constructed can naturally better adapt to the dynamic rain/snow as well as background changes, and also suitable to deal with the streaming video attributed its online learning mode. The proposed model is formulated in a concise maximum a posterior (MAP) framework and is readily solved by the ADMM algorithm. Compared with the state-of-the-art online and offline video rain/snow removal methods, the proposed method achieves better performance on synthetic and real videos datasets both visually and quantitatively. Specifically, our method can be implemented in relatively high efficiency, showing its potential to real-time video rain/snow removal.



### Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs
- **Arxiv ID**: http://arxiv.org/abs/1909.06161v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, eess.IV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1909.06161v2)
- **Published**: 2019-09-13 12:09:34+00:00
- **Updated**: 2019-10-28 07:30:42+00:00
- **Authors**: Jonas Kubilius, Martin Schrimpf, Kohitij Kar, Ha Hong, Najib J. Majaj, Rishi Rajalingham, Elias B. Issa, Pouya Bashivan, Jonathan Prescott-Roy, Kailyn Schmidt, Aran Nayebi, Daniel Bear, Daniel L. K. Yamins, James J. DiCarlo
- **Comment**: NeurIPS 2019 (Oral). Code available at
  https://github.com/dicarlolab/neurips2019
- **Journal**: None
- **Summary**: Deep convolutional artificial neural networks (ANNs) are the leading class of candidate models of the mechanisms of visual processing in the primate ventral stream. While initially inspired by brain anatomy, over the past years, these ANNs have evolved from a simple eight-layer architecture in AlexNet to extremely deep and branching architectures, demonstrating increasingly better object categorization performance, yet bringing into question how brain-like they still are. In particular, typical deep models from the machine learning community are often hard to map onto the brain's anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. Here we demonstrate that better anatomical alignment to the brain and high performance on machine learning as well as neuroscience measures do not have to be in contradiction. We developed CORnet-S, a shallow ANN with four anatomically mapped areas and recurrent connectivity, guided by Brain-Score, a new large-scale composite of neural and behavioral benchmarks for quantifying the functional fidelity of models of the primate ventral visual stream. Despite being significantly shallower than most models, CORnet-S is the top model on Brain-Score and outperforms similarly compact models on ImageNet. Moreover, our extensive analyses of CORnet-S circuitry variants reveal that recurrence is the main predictive factor of both Brain-Score and ImageNet top-1 performance. Finally, we report that the temporal evolution of the CORnet-S "IT" neural population resembles the actual monkey IT population dynamics. Taken together, these results establish CORnet-S, a compact, recurrent ANN, as the current best model of the primate ventral visual stream.



### Hierarchical Scene Coordinate Classification and Regression for Visual Localization
- **Arxiv ID**: http://arxiv.org/abs/1909.06216v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.06216v3)
- **Published**: 2019-09-13 13:21:52+00:00
- **Updated**: 2020-03-31 22:28:48+00:00
- **Authors**: Xiaotian Li, Shuzhe Wang, Yi Zhao, Jakob Verbeek, Juho Kannala
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Visual localization is critical to many applications in computer vision and robotics. To address single-image RGB localization, state-of-the-art feature-based methods match local descriptors between a query image and a pre-built 3D model. Recently, deep neural networks have been exploited to regress the mapping between raw pixels and 3D coordinates in the scene, and thus the matching is implicitly performed by the forward pass through the network. However, in a large and ambiguous environment, learning such a regression task directly can be difficult for a single network. In this work, we present a new hierarchical scene coordinate network to predict pixel scene coordinates in a coarse-to-fine manner from a single RGB image. The network consists of a series of output layers, each of them conditioned on the previous ones. The final output layer predicts the 3D coordinates and the others produce progressively finer discrete location labels. The proposed method outperforms the baseline regression-only network and allows us to train compact models which scale robustly to large environments. It sets a new state-of-the-art for single-image RGB localization performance on the 7-Scenes, 12-Scenes, Cambridge Landmarks datasets, and three combined scenes. Moreover, for large-scale outdoor localization on the Aachen Day-Night dataset, we present a hybrid approach which outperforms existing scene coordinate regression methods, and reduces significantly the performance gap w.r.t. explicit feature matching methods.



### $ρ$-VAE: Autoregressive parametrization of the VAE encoder
- **Arxiv ID**: http://arxiv.org/abs/1909.06236v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.06236v1)
- **Published**: 2019-09-13 14:01:33+00:00
- **Updated**: 2019-09-13 14:01:33+00:00
- **Authors**: Sohrab Ferdowsi, Maurits Diephuis, Shideh Rezaeifar, Slava Voloshynovskiy
- **Comment**: Submitted to NeurIPS workshop on Bayesian deep learning
- **Journal**: None
- **Summary**: We make a minimal, but very effective alteration to the VAE model. This is about a drop-in replacement for the (sample-dependent) approximate posterior to change it from the standard white Gaussian with diagonal covariance to the first-order autoregressive Gaussian. We argue that this is a more reasonable choice to adopt for natural signals like images, as it does not force the existing correlation in the data to disappear in the posterior. Moreover, it allows more freedom for the approximate posterior to match the true posterior. This allows for the repararametrization trick, as well as the KL-divergence term to still have closed-form expressions, obviating the need for its sample-based estimation. Although providing more freedom to adapt to correlated distributions, our parametrization has even less number of parameters than the diagonal covariance, as it requires only two scalars, $\rho$ and $s$, to characterize correlation and scaling, respectively. As validated by the experiments, our proposition noticeably and consistently improves the quality of image generation in a plug-and-play manner, needing no further parameter tuning, and across all setups. The code to reproduce our experiments is available at \url{https://github.com/sssohrab/rho_VAE/}.



### Classifying Topological Charge in SU(3) Yang-Mills Theory with Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.06238v2
- **DOI**: None
- **Categories**: **hep-lat**, cs.CV, cs.LG, hep-ph
- **Links**: [PDF](http://arxiv.org/pdf/1909.06238v2)
- **Published**: 2019-09-13 14:02:10+00:00
- **Updated**: 2021-01-29 12:20:38+00:00
- **Authors**: Takuya Matsumoto, Masakiyo Kitazawa, Yasuhiro Kohno
- **Comment**: 28 pages, 12 figures, version to appear in PTEP
- **Journal**: None
- **Summary**: We apply a machine learning technique for identifying the topological charge of quantum gauge configurations in four-dimensional SU(3) Yang-Mills theory. The topological charge density measured on the original and smoothed gauge configurations with and without dimensional reduction is used as inputs for the neural networks (NN) with and without convolutional layers. The gradient flow is used for the smoothing of the gauge field. We find that the topological charge determined at a large flow time can be predicted with high accuracy from the data at small flow times by the trained NN; for example, the accuracy exceeds $99\%$ with the data at $t/a^2\le0.3$. High robustness against the change of simulation parameters is also confirmed with a fixed physical volume. We find that the best performance is obtained when the spatial coordinates of the topological charge density are fully integrated out in preprocessing, which implies that our convolutional NN does not find characteristic structures in multi-dimensional space relevant for the determination of the topological charge.



### A superpixel-driven deep learning approach for the analysis of dermatological wounds
- **Arxiv ID**: http://arxiv.org/abs/1909.06264v2
- **DOI**: 10.1016/j.cmpb.2019.105079
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.06264v2)
- **Published**: 2019-09-13 14:41:19+00:00
- **Updated**: 2019-09-20 21:49:54+00:00
- **Authors**: Gustavo Blanco, Agma J. M. Traina, Caetano Traina Jr., Paulo M. Azevedo-Marques, Ana E. S. Jorge, Daniel de Oliveira, Marcos V. N. Bedo
- **Comment**: None
- **Journal**: None
- **Summary**: Background. The image-based identification of distinct tissues within dermatological wounds enhances patients' care since it requires no intrusive evaluations. This manuscript presents an approach, we named QTDU, that combines deep learning models with superpixel-driven segmentation methods for assessing the quality of tissues from dermatological ulcers.   Method. QTDU consists of a three-stage pipeline for the obtaining of ulcer segmentation, tissues' labeling, and wounded area quantification. We set up our approach by using a real and annotated set of dermatological ulcers for training several deep learning models to the identification of ulcered superpixels.   Results. Empirical evaluations on 179,572 superpixels divided into four classes showed QTDU accurately spot wounded tissues (AUC = 0.986, sensitivity = 0.97, and specificity = 0.974) and outperformed machine-learning approaches in up to 8.2% regarding F1-Score through fine-tuning of a ResNet-based model. Last, but not least, experimental evaluations also showed QTDU correctly quantified wounded tissue areas within a 0.089 Mean Absolute Error ratio.   Conclusions. Results indicate QTDU effectiveness for both tissue segmentation and wounded area quantification tasks. When compared to existing machine-learning approaches, the combination of superpixels and deep learning models outperformed the competitors within strong significant levels.



### Semantic and Visual Similarities for Efficient Knowledge Transfer in CNN Training
- **Arxiv ID**: http://arxiv.org/abs/1909.12916v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1909.12916v1)
- **Published**: 2019-09-13 14:49:38+00:00
- **Updated**: 2019-09-13 14:49:38+00:00
- **Authors**: Lucas Pascal, Xavier Bost, Benoît Huet
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, representation learning approaches have disrupted many multimedia computing tasks. Among those approaches, deep convolutional neural networks (CNNs) have notably reached human level expertise on some constrained image classification tasks. Nonetheless, training CNNs from scratch for new task or simply new data turns out to be complex and time-consuming. Recently, transfer learning has emerged as an effective methodology for adapting pre-trained CNNs to new data and classes, by only retraining the last classification layer. This paper focuses on improving this process, in order to better transfer knowledge between CNN architectures for faster trainings in the case of fine tuning for image classification. This is achieved by combining and transfering supplementary weights, based on similarity considerations between source and target classes. The study includes a comparison between semantic and content-based similarities, and highlights increased initial performances and training speed, along with superior long term performances when limited training samples are available.



### White-Box Adversarial Defense via Self-Supervised Data Estimation
- **Arxiv ID**: http://arxiv.org/abs/1909.06271v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.06271v1)
- **Published**: 2019-09-13 14:51:50+00:00
- **Updated**: 2019-09-13 14:51:50+00:00
- **Authors**: Zudi Lin, Hanspeter Pfister, Ziming Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the problem of how to defend classifiers against adversarial attacks that fool the classifiers using subtly modified input data. In contrast to previous works, here we focus on the white-box adversarial defense where the attackers are granted full access to not only the classifiers but also defenders to produce as strong attacks as possible. In such a context we propose viewing a defender as a functional, a higher-order function that takes functions as its argument to represent a function space, rather than fixed functions conventionally. From this perspective, a defender should be realized and optimized individually for each adversarial input. To this end, we propose RIDE, an efficient and provably convergent self-supervised learning algorithm for individual data estimation to protect the predictions from adversarial attacks. We demonstrate the significant improvement of adversarial defense performance on image recognition, eg, 98%, 76%, 43% test accuracy on MNIST, CIFAR-10, and ImageNet datasets respectively under the state-of-the-art BPDA attacker.



### Risk-Aware Planning by Confidence Estimation using Deep Learning-Based Perception
- **Arxiv ID**: http://arxiv.org/abs/1910.00101v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.SY, eess.SY, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.00101v1)
- **Published**: 2019-09-13 15:20:41+00:00
- **Updated**: 2019-09-13 15:20:41+00:00
- **Authors**: Maymoonah Toubeh, Pratap Tokekar
- **Comment**: None
- **Journal**: None
- **Summary**: This work proposes the use of Bayesian approximations of uncertainty from deep learning in a robot planner, showing that this produces more cautious actions in safety-critical scenarios. The case study investigated is motivated by a setup where an aerial robot acts as a "scout" for a ground robot. This is useful when the below area is unknown or dangerous, with applications in space exploration, military, or search-and-rescue. Images taken from the aerial view are used to provide a less obstructed map to guide the navigation of the robot on the ground. Experiments are conducted using a deep learning semantic image segmentation, followed by a path planner based on the resulting cost map, to provide an empirical analysis of the proposed method. A comparison with similar approaches is presented to portray the usefulness of certain techniques, or variations within a technique, in similar experimental settings. The method is analyzed to assess the impact of variations in the uncertainty extraction, as well as the absence of an uncertainty metric, on the overall system with the use of a defined metric which measures surprise to the planner. The analysis is performed on multiple datasets, showing a similar trend of lower surprise when uncertainty information is incorporated in the planning, given threshold values of the hyperparameters in the uncertainty extraction have been met. We find that taking uncertainty into account leads to paths that could be 18% less risky on an average.



### Flow Models for Arbitrary Conditional Likelihoods
- **Arxiv ID**: http://arxiv.org/abs/1909.06319v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.06319v2)
- **Published**: 2019-09-13 16:35:17+00:00
- **Updated**: 2020-08-06 13:30:33+00:00
- **Authors**: Yang Li, Shoaib Akbar, Junier B. Oliva
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding the dependencies among features of a dataset is at the core of most unsupervised learning tasks. However, a majority of generative modeling approaches are focused solely on the joint distribution $p(x)$ and utilize models where it is intractable to obtain the conditional distribution of some arbitrary subset of features $x_u$ given the rest of the observed covariates $x_o$: $p(x_u \mid x_o)$. Traditional conditional approaches provide a model for a fixed set of covariates conditioned on another fixed set of observed covariates. Instead, in this work we develop a model that is capable of yielding all conditional distributions $p(x_u \mid x_o)$ (for arbitrary $x_u$) via tractable conditional likelihoods. We propose a novel extension of (change of variables based) flow generative models, arbitrary conditioning flow models (AC-Flow), that can be conditioned on arbitrary subsets of observed covariates, which was previously infeasible. We apply AC-Flow to the imputation of features, and also develop a unified platform for both multiple and single imputation by introducing an auxiliary objective that provides a principled single "best guess" for flow models. Extensive empirical evaluations show that our models achieve state-of-the-art performance in both single and multiple imputation across image inpainting and feature imputation in synthetic and real-world datasets. Code is available at https://github.com/lupalab/ACFlow.



### Measuring the Effects of Non-Identical Data Distribution for Federated Visual Classification
- **Arxiv ID**: http://arxiv.org/abs/1909.06335v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.06335v1)
- **Published**: 2019-09-13 17:26:20+00:00
- **Updated**: 2019-09-13 17:26:20+00:00
- **Authors**: Tzu-Ming Harry Hsu, Hang Qi, Matthew Brown
- **Comment**: None
- **Journal**: None
- **Summary**: Federated Learning enables visual models to be trained in a privacy-preserving way using real-world data from mobile devices. Given their distributed nature, the statistics of the data across these devices is likely to differ significantly. In this work, we look at the effect such non-identical data distributions has on visual classification via Federated Learning. We propose a way to synthesize datasets with a continuous range of identicalness and provide performance measures for the Federated Averaging algorithm. We show that performance degrades as distributions differ more, and propose a mitigation strategy via server momentum. Experiments on CIFAR-10 demonstrate improved classification performance over a range of non-identicalness, with classification accuracy improved from 30.1% to 76.9% in the most skewed settings.



### MRI Brain Tumor Segmentation using Random Forests and Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.06337v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.06337v1)
- **Published**: 2019-09-13 17:26:56+00:00
- **Updated**: 2019-09-13 17:26:56+00:00
- **Authors**: Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou, Guang Yang, Nigel Allinson, Xujiong Ye
- **Comment**: Published in the pre-conference proceeding of "2017 International
  MICCAI BraTS Challenge"
- **Journal**: In Proceeding of 2017 International MICCAI BraTS Challenge, pp.
  279-283 (2017)
- **Summary**: In this paper, we propose a novel learning based method for automated segmentation of brain tumor in multimodal MRI images, which incorporates two sets of machine -learned and hand crafted features. Fully convolutional networks (FCN) forms the machine learned features and texton based features are considered as hand-crafted features. Random forest (RF) is used to classify the MRI image voxels into normal brain tissues and different parts of tumors, i.e. edema, necrosis and enhancing tumor. The method was evaluated on BRATS 2017 challenge dataset. The results show that the proposed method provides promising segmentations. The mean Dice overlap measure for automatic brain tumor segmentation against ground truth is 0.86, 0.78 and 0.66 for whole tumor, core and enhancing tumor, respectively.



### Magnetic Resonance Fingerprinting Reconstruction Using Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.06395v1
- **DOI**: 10.3233/SHTI190816
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.06395v1)
- **Published**: 2019-09-13 18:17:19+00:00
- **Updated**: 2019-09-13 18:17:19+00:00
- **Authors**: Elisabeth Hoppe, Florian Thamm, Gregor Körzdörfer, Christopher Syben, Franziska Schirrmacher, Mathias Nittka, Josef Pfeuffer, Heiko Meyer, Andreas Maier
- **Comment**: Accepted and presented at the German Medical Data Sciences (GMDS)
  conference 2019 (Dortmund, Germany)
- **Journal**: Studies in Health Technology and Informatics [01 Sep 2019,
  267:126-133]
- **Summary**: Magnetic Resonance Fingerprinting (MRF) is an imaging technique acquiring unique time signals for different tissues. Although the acquisition is highly accelerated, the reconstruction time remains a problem, as the state-of-the-art template matching compares every signal with a set of possible signals. To overcome this limitation, deep learning based approaches, e.g. Convolutional Neural Networks (CNNs) have been proposed. In this work, we investigate the applicability of Recurrent Neural Networks (RNNs) for this reconstruction problem, as the signals are correlated in time. Compared to previous methods based on CNNs, RNN models yield significantly improved results using in-vivo data.



### Image inpainting: A review
- **Arxiv ID**: http://arxiv.org/abs/1909.06399v1
- **DOI**: 10.1007/s11063-019-10163-0
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.06399v1)
- **Published**: 2019-09-13 18:33:38+00:00
- **Updated**: 2019-09-13 18:33:38+00:00
- **Authors**: Omar Elharrouss, Noor Almaadeed, Somaya Al-Maadeed, Younes Akbari
- **Comment**: None
- **Journal**: None
- **Summary**: Although image inpainting, or the art of repairing the old and deteriorated images, has been around for many years, it has gained even more popularity because of the recent development in image processing techniques. With the improvement of image processing tools and the flexibility of digital image editing, automatic image inpainting has found important applications in computer vision and has also become an important and challenging topic of research in image processing. This paper is a brief review of the existing image inpainting approaches we first present a global vision on the existing methods for image inpainting. We attempt to collect most of the existing approaches and classify them into three categories, namely, sequential-based, CNN-based and GAN-based methods. In addition, for each category, a list of methods for the different types of distortion on the images is presented. Furthermore, collect a list of the available datasets and discuss these in our paper. This is a contribution for digital image inpainting researchers trying to look for the available datasets because there is a lack of datasets available for image inpainting. As the final step in this overview, we present the results of real evaluations of the three categories of image inpainting methods performed on the datasets used, for the different types of image distortion. In the end, we also present the evaluations metrics and discuss the performance of these methods in terms of these metrics. This overview can be used as a reference for image inpainting researchers, and it can also facilitate the comparison of the methods as well as the datasets used. The main contribution of this paper is the presentation of the three categories of image inpainting methods along with a list of available datasets that the researchers can use to evaluate their proposed methodology against.



### Zero-Shot Action Recognition in Videos: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1909.06423v2
- **DOI**: 10.1016/j.neucom.2021.01.036
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.06423v2)
- **Published**: 2019-09-13 19:57:27+00:00
- **Updated**: 2020-11-17 17:10:24+00:00
- **Authors**: Valter Estevam, Helio Pedrini, David Menotti
- **Comment**: Preprint
- **Journal**: None
- **Summary**: Zero-Shot Action Recognition has attracted attention in the last years and many approaches have been proposed for recognition of objects, events and actions in images and videos. There is a demand for methods that can classify instances from classes that are not present in the training of models, especially in the complex problem of automatic video understanding, since collecting, annotating and labeling videos are difficult and laborious tasks. We have identified that there are many methods available in the literature, however, it is difficult to categorize which techniques can be considered state of the art. Despite the existence of some surveys about zero-shot action recognition in still images and experimental protocol, there is no work focused on videos. Therefore, we present a survey of the methods that comprise techniques to perform visual feature extraction and semantic feature extraction as well to learn the mapping between these features considering specifically zero-shot action recognition in videos. We also provide a complete description of datasets, experiments and protocols, presenting open issues and directions for future work, essential for the development of the computer vision research field.



### Coupling Rendering and Generative Adversarial Networks for Artificial SAS Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1909.06436v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.06436v2)
- **Published**: 2019-09-13 20:30:17+00:00
- **Updated**: 2019-10-02 20:21:33+00:00
- **Authors**: Albert Reed, Isaac Gerg, John McKay, Daniel Brown, David Williams, Suren Jayasuriya
- **Comment**: 10 pages, 9 figures. Submitted to IEEE OCEANS 2019 (Seattle). Updated
  acknowledgements
- **Journal**: None
- **Summary**: Acquisition of Synthetic Aperture Sonar (SAS) datasets is bottlenecked by the costly deployment of SAS imaging systems, and even when data acquisition is possible,the data is often skewed towards containing barren seafloor rather than objects of interest. We present a novel pipeline, called SAS GAN, which couples an optical renderer with a generative adversarial network (GAN) to synthesize realistic SAS images of targets on the seafloor. This coupling enables high levels of SAS image realism while enabling control over image geometry and parameters. We demonstrate qualitative results by presenting examples of images created with our pipeline. We also present quantitative results through the use of t-SNE and the Fr\'echet Inception Distance to argue that our generated SAS imagery potentially augments SAS datasets more effectively than an off-the-shelf GAN.



### MinneApple: A Benchmark Dataset for Apple Detection and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.06441v2
- **DOI**: 10.1109/LRA.2020.2965061
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.06441v2)
- **Published**: 2019-09-13 20:47:31+00:00
- **Updated**: 2020-01-03 21:15:05+00:00
- **Authors**: Nicolai Häni, Pravakar Roy, Volkan Isler
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a new dataset to advance the state-of-the-art in fruit detection, segmentation, and counting in orchard environments. While there has been significant recent interest in solving these problems, the lack of a unified dataset has made it difficult to compare results. We hope to enable direct comparisons by providing a large variety of high-resolution images acquired in orchards, together with human annotations of the fruit on trees. The fruits are labeled using polygonal masks for each object instance to aid in precise object detection, localization, and segmentation. Additionally, we provide data for patch-based counting of clustered fruits. Our dataset contains over 41, 000 annotated object instances in 1000 images. We present a detailed overview of the dataset together with baseline performance analysis for bounding box detection, segmentation, and fruit counting as well as representative results for yield estimation. We make this dataset publicly available and host a CodaLab challenge to encourage comparison of results on a common dataset. To download the data and learn more about MinneApple please see the project website: http://rsn.cs.umn.edu/index.php/MinneApple. Up to date information is available online.



### Spatio-spectral networks for color-texture analysis
- **Arxiv ID**: http://arxiv.org/abs/1909.06446v1
- **DOI**: 10.1016/j.ins.2019.11.042
- **Categories**: **cs.CV**, cs.LG, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/1909.06446v1)
- **Published**: 2019-09-13 20:54:59+00:00
- **Updated**: 2019-09-13 20:54:59+00:00
- **Authors**: Leonardo F. S. Scabini, Lucas C. Ribas, Odemir M. Bruno
- **Comment**: None
- **Journal**: None
- **Summary**: Texture is one of the most-studied visual attribute for image characterization since the 1960s. However, most hand-crafted descriptors are monochromatic, focusing on the gray scale images and discarding the color information. In this context, this work focus on a new method for color texture analysis considering all color channels in a more intrinsic approach. Our proposal consists of modeling color images as directed complex networks that we named Spatio-Spectral Network (SSN). Its topology includes within-channel edges that cover spatial patterns throughout individual image color channels, while between-channel edges tackle spectral properties of channel pairs in an opponent fashion. Image descriptors are obtained through a concise topological characterization of the modeled network in a multiscale approach with radially symmetric neighborhoods. Experiments with four datasets cover several aspects of color-texture analysis, and results demonstrate that SSN overcomes all the compared literature methods, including known deep convolutional networks, and also has the most stable performance between datasets, achieving $98.5(\pm1.1)$ of average accuracy against $97.1(\pm1.3)$ of MCND and $96.8(\pm3.2)$ of AlexNet. Additionally, an experiment verifies the performance of the methods under different color spaces, where results show that SSN also has higher performance and robustness.



### F-Cooper: Feature based Cooperative Perception for Autonomous Vehicle Edge Computing System Using 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1909.06459v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.06459v1)
- **Published**: 2019-09-13 21:33:18+00:00
- **Updated**: 2019-09-13 21:33:18+00:00
- **Authors**: Qi Chen
- **Comment**: Accepted by SEC2019
- **Journal**: None
- **Summary**: Autonomous vehicles are heavily reliant upon their sensors to perfect the perception of surrounding environments, however, with the current state of technology, the data which a vehicle uses is confined to that from its own sensors. Data sharing between vehicles and/or edge servers is limited by the available network bandwidth and the stringent real-time constraints of autonomous driving applications. To address these issues, we propose a point cloud feature based cooperative perception framework (F-Cooper) for connected autonomous vehicles to achieve a better object detection precision. Not only will feature based data be sufficient for the training process, we also use the features' intrinsically small size to achieve real-time edge computing, without running the risk of congesting the network. Our experiment results show that by fusing features, we are able to achieve a better object detection result, around 10% improvement for detection within 20 meters and 30% for further distances, as well as achieve faster edge computing with a low communication delay, requiring 71 milliseconds in certain feature selections. To the best of our knowledge, we are the first to introduce feature-level data fusion to connected autonomous vehicles for the purpose of enhancing object detection and making real-time edge computing on inter-vehicle data feasible for autonomous vehicles.



### Multi-Perspective, Simultaneous Embedding
- **Arxiv ID**: http://arxiv.org/abs/1909.06485v3
- **DOI**: None
- **Categories**: **cs.DS**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.06485v3)
- **Published**: 2019-09-13 23:20:17+00:00
- **Updated**: 2020-08-06 00:55:29+00:00
- **Authors**: Md Iqbal Hossain, Vahan Huroyan, Stephen Kobourov, Raymundo Navarrete
- **Comment**: None
- **Journal**: None
- **Summary**: We describe MPSE: a Multi-Perspective Simultaneous Embedding method for visualizing high-dimensional data, based on multiple pairwise distances between the data points. Specifically, MPSE computes positions for the points in 3D and provides different views into the data by means of 2D projections (planes) that preserve each of the given distance matrices. We consider two versions of the problem: fixed projections and variable projections. MPSE with fixed projections takes as input a set of pairwise distance matrices defined on the data points, along with the same number of projections and embeds the points in 3D so that the pairwise distances are preserved in the given projections. MPSE with variable projections takes as input a set of pairwise distance matrices and embeds the points in 3D while also computing the appropriate projections that preserve the pairwise distances. The proposed approach can be useful in multiple scenarios: from creating simultaneous embedding of multiple graphs on the same set of vertices, to reconstructing a 3D object from multiple 2D snapshots, to analyzing data from multiple points of view. We provide a functional prototype of MPSE that is based on an adaptive and stochastic generalization of multi-dimensional scaling to multiple distances and multiple variable projections. We provide an extensive quantitative evaluation with datasets of different sizes and using different number of projections, as well as several examples that illustrate the quality of the resulting solutions.



