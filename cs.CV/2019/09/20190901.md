# Arxiv Papers in cs.CV on 2019-09-01
### VisualPhishNet: Zero-Day Phishing Website Detection by Visual Similarity
- **Arxiv ID**: http://arxiv.org/abs/1909.00300v4
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.00300v4)
- **Published**: 2019-09-01 00:55:10+00:00
- **Updated**: 2020-07-05 15:24:44+00:00
- **Authors**: Sahar Abdelnabi, Katharina Krombholz, Mario Fritz
- **Comment**: None
- **Journal**: None
- **Summary**: Phishing websites are still a major threat in today's Internet ecosystem. Despite numerous previous efforts, similarity-based detection methods do not offer sufficient protection for the trusted websites - in particular against unseen phishing pages. This paper contributes VisualPhishNet, a new similarity-based phishing detection framework, based on a triplet Convolutional Neural Network (CNN). VisualPhishNet learns profiles for websites in order to detect phishing websites by a similarity metric that can generalize to pages with new visual appearances. We furthermore present VisualPhish, the largest dataset to date that facilitates visual phishing detection in an ecologically valid manner. We show that our method outperforms previous visual similarity phishing detection approaches by a large margin while being robust against a range of evasion attacks.



### READ: Recursive Autoencoders for Document Layout Generation
- **Arxiv ID**: http://arxiv.org/abs/1909.00302v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1909.00302v4)
- **Published**: 2019-09-01 01:58:31+00:00
- **Updated**: 2020-04-16 23:26:17+00:00
- **Authors**: Akshay Gadi Patil, Omri Ben-Eliezer, Or Perel, Hadar Averbuch-Elor
- **Comment**: None
- **Journal**: None
- **Summary**: Layout is a fundamental component of any graphic design. Creating large varieties of plausible document layouts can be a tedious task, requiring numerous constraints to be satisfied, including local ones relating different semantic elements and global constraints on the general appearance and spacing. In this paper, we present a novel framework, coined READ, for REcursive Autoencoders for Document layout generation, to generate plausible 2D layouts of documents in large quantities and varieties. First, we devise an exploratory recursive method to extract a structural decomposition of a single document. Leveraging a dataset of documents annotated with labeled bounding boxes, our recursive neural network learns to map the structural representation, given in the form of a simple hierarchy, to a compact code, the space of which is approximated by a Gaussian distribution. Novel hierarchies can be sampled from this space, obtaining new document layouts. Moreover, we introduce a combinatorial metric to measure structural similarity among document layouts. We deploy it to show that our method is able to generate highly variable and realistic layouts. We further demonstrate the utility of our generated layouts in the context of standard detection tasks on documents, showing that detection performance improves when the training data is augmented with generated documents whose layouts are produced by READ.



### Multiple Object Tracking with Motion and Appearance Cues
- **Arxiv ID**: http://arxiv.org/abs/1909.00318v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.00318v1)
- **Published**: 2019-09-01 03:54:40+00:00
- **Updated**: 2019-09-01 03:54:40+00:00
- **Authors**: Weiqiang Li, Jiatong Mu, Guizhong Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Due to better video quality and higher frame rate, the performance of multiple object tracking issues has been greatly improved in recent years. However, in real application scenarios, camera motion and noisy per frame detection results degrade the performance of trackers significantly. High-speed and high-quality multiple object trackers are still in urgent demand. In this paper, we propose a new multiple object tracker following the popular tracking-by-detection scheme. We tackle the camera motion problem with an optical flow network and utilize an auxiliary tracker to deal with the missing detection problem. Besides, we use both the appearance and motion information to improve the matching quality. The experimental results on the VisDrone-MOT dataset show that our approach can improve the performance of multiple object tracking significantly while achieving a high efficiency.



### Flow Guided Short-term Trackers with Cascade Detection for Long-term Tracking
- **Arxiv ID**: http://arxiv.org/abs/1909.00319v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.00319v1)
- **Published**: 2019-09-01 04:06:42+00:00
- **Updated**: 2019-09-01 04:06:42+00:00
- **Authors**: Han Wu, Xueyuan Yang, Yong Yang, Guizhong Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Object tracking has been studied for decades, but most of the existing works are focused on the short-term tracking. For a long sequence, the object is often fully occluded or out of view for a long time, and existing short-term object tracking algorithms often lose the target, and it is difficult to re-catch the target even if it reappears again. In this paper a novel long-term object tracking algorithm flow_MDNet_RPN is proposed, in which a tracking result judgement module and a detection module are added to the short-term object tracking algorithm. Experiments show that the proposed long-term tracking algorithm is effective to the problem of target disappearance.



### Deep Mesh Reconstruction from Single RGB Images via Topology Modification Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.00321v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.00321v1)
- **Published**: 2019-09-01 04:17:41+00:00
- **Updated**: 2019-09-01 04:17:41+00:00
- **Authors**: Junyi Pan, Xiaoguang Han, Weikai Chen, Jiapeng Tang, Kui Jia
- **Comment**: 10 pages, 11 figures, to be presented at ICCV 2019
- **Journal**: None
- **Summary**: Reconstructing the 3D mesh of a general object from a single image is now possible thanks to the latest advances of deep learning technologies. However, due to the nontrivial difficulty of generating a feasible mesh structure, the state-of-the-art approaches often simplify the problem by learning the displacements of a template mesh that deforms it to the target surface. Though reconstructing a 3D shape with complex topology can be achieved by deforming multiple mesh patches, it remains difficult to stitch the results to ensure a high meshing quality. In this paper, we present an end-to-end single-view mesh reconstruction framework that is able to generate high-quality meshes with complex topologies from a single genus-0 template mesh. The key to our approach is a novel progressive shaping framework that alternates between mesh deformation and topology modification. While a deformation network predicts the per-vertex translations that reduce the gap between the reconstructed mesh and the ground truth, a novel topology modification network is employed to prune the error-prone faces, enabling the evolution of topology. By iterating over the two procedures, one can progressively modify the mesh topology while achieving higher reconstruction accuracy. Moreover, a boundary refinement network is designed to refine the boundary conditions to further improve the visual quality of the reconstructed mesh. Extensive experiments demonstrate that our approach outperforms the current state-of-the-art methods both qualitatively and quantitatively, especially for the shapes with complex topologies.



### Deep Learning Algorithms to Isolate and Quantify the Structures of the Anterior Segment in Optical Coherence Tomography Images
- **Arxiv ID**: http://arxiv.org/abs/1909.00331v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.00331v1)
- **Published**: 2019-09-01 06:27:05+00:00
- **Updated**: 2019-09-01 06:27:05+00:00
- **Authors**: Tan Hung Pham, Sripad Krishna Devalla, Aloysius Ang, Soh Zhi Da, Alexandre H. Thiery, Craig Boote, Ching-Yu Cheng, Victor Koh, Michael J. A. Girard
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate isolation and quantification of intraocular dimensions in the anterior segment (AS) of the eye using optical coherence tomography (OCT) images is important in the diagnosis and treatment of many eye diseases, especially angle closure glaucoma. In this study, we developed a deep convolutional neural network (DCNN) for the localization of the scleral spur, and the segmentation of anterior segment structures (iris, corneo-sclera shell, anterior chamber). With limited training data, the DCNN was able to detect the scleral spur on unseen ASOCT images as accurately as an experienced ophthalmologist; and simultaneously isolated the anterior segment structures with a Dice coefficient of 95.7%. We then automatically extracted eight clinically relevant ASOCT parameters and proposed an automated quality check process that asserts the reliability of these parameters. When combined with an OCT machine capable of imaging multiple radial sections, the algorithms can provide a more complete objective assessment. This is an essential step toward providing a robust automated framework for reliable quantification of ASOCT scans, for applications in the diagnosis and management of angle closure glaucoma.



### Video Affective Effects Prediction with Multi-modal Fusion and Shot-Long Temporal Context
- **Arxiv ID**: http://arxiv.org/abs/1909.01763v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.01763v1)
- **Published**: 2019-09-01 07:22:20+00:00
- **Updated**: 2019-09-01 07:22:20+00:00
- **Authors**: Jie Zhang, Yin Zhao, Longjun Cai, Chaoping Tu, Wu Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting the emotional impact of videos using machine learning is a challenging task considering the varieties of modalities, the complicated temporal contex of the video as well as the time dependency of the emotional states. Feature extraction, multi-modal fusion and temporal context fusion are crucial stages for predicting valence and arousal values in the emotional impact, but have not been successfully exploited. In this paper, we propose a comprehensive framework with novel designs of modal structure and multi-modal fusion strategy. We select the most suitable modalities for valence and arousal tasks respectively and each modal feature is extracted using the modality-specific pre-trained deep model on large generic dataset. Two-time-scale structures, one for the intra-clip and the other for the inter-clip, are proposed to capture the temporal dependency of video content and emotion states. To combine the complementary information from multiple modalities, an effective and efficient residual-based progressive training strategy is proposed. Each modality is step-wisely combined into the multi-modal model, responsible for completing the missing parts of features. With all those improvements above, our proposed prediction framework achieves better performance on the LIRIS-ACCEDE dataset with a large margin compared to the state-of-the-art.



### Learning Visual Features Under Motion Invariance
- **Arxiv ID**: http://arxiv.org/abs/1909.00350v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.00350v3)
- **Published**: 2019-09-01 08:20:44+00:00
- **Updated**: 2020-04-24 10:00:44+00:00
- **Authors**: Alessandro Betti, Marco Gori, Stefano Melacci
- **Comment**: 73 pages, 9 figures. arXiv admin note: substantial text overlap with
  arXiv:1801.07110
- **Journal**: Neural Networks 126 (2020), pp. 275--299
- **Summary**: Humans are continuously exposed to a stream of visual data with a natural temporal structure. However, most successful computer vision algorithms work at image level, completely discarding the precious information carried by motion. In this paper, we claim that processing visual streams naturally leads to formulate the motion invariance principle, which enables the construction of a new theory of learning that originates from variational principles, just like in physics. Such principled approach is well suited for a discussion on a number of interesting questions that arise in vision, and it offers a well-posed computational scheme for the discovery of convolutional filters over the retina. Differently from traditional convolutional networks, which need massive supervision, the proposed theory offers a truly new scenario for the unsupervised processing of video signals, where features are extracted in a multi-layer architecture with motion invariance. While the theory enables the implementation of novel computer vision systems, it also sheds light on the role of information-based principles to drive possible biological solutions.



### Performance Evaluation of Histogram Equalization and Fuzzy image Enhancement Techniques on Low Contrast Images
- **Arxiv ID**: http://arxiv.org/abs/1909.03957v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.03957v1)
- **Published**: 2019-09-01 08:47:17+00:00
- **Updated**: 2019-09-01 08:47:17+00:00
- **Authors**: E Onyedinma, I Onyenwe, H Inyiama
- **Comment**: None
- **Journal**: International Journal of Computer Science and Software Engineering
  (IJCSSE), Volume 8, Issue 7, Page: 144-150, July2019. ISSN (Online):
  2409-4285w w w.IJCSSE.org
- **Summary**: Image enhancement aims at improving the information content of original image for a specific purpose. This purpose could be for visual interpretation or for effective extraction of required details. Nevertheless, some acquired images are often associated with pixels of low dynamic range and as such result in low contrast images. Enhancing the contrast therefore tends to increase the dynamic range of the gray levels in the acquired image so as to span the full intensity range. Techniques such as Histogram Equalization (HE) and fuzzy technique can be adopted for contrast enhancement. HE adjusts the contrast of an input image by modifying the intensity distribution of its histogram. It is characterized by providing a global approach to image enhancement, computationally fast and easy to implement approach but can introduce unnatural artifacts and other undesirable elements to the resulting image. Fuzzy technique on its part enhances image by mapping the image gray level intensities into a fuzzy plane using membership functions; modifying the membership functions as desired and mapping back into the gray level plane. Thus, details at desired areas can be enhanced at the expense of increase in computational cost. This paper explores the effect of the use of HE and fuzzy technique to enhance low contrast images. Their performances are evaluated using the Mean squared error (MSE), Peak to signal noise ratio (PSNR), entropy and Absolute mean brightness error (AMBE).



### 3D Bounding Box Estimation for Autonomous Vehicles by Cascaded Geometric Constraints and Depurated 2D Detections Using 3D Results
- **Arxiv ID**: http://arxiv.org/abs/1909.01867v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.01867v1)
- **Published**: 2019-09-01 11:50:17+00:00
- **Updated**: 2019-09-01 11:50:17+00:00
- **Authors**: Jiaojiao Fang, Lingtao Zhou, Guizhong Liu
- **Comment**: None
- **Journal**: None
- **Summary**: 3D object detection is one of the most important tasks in 3D vision perceptual system of autonomous vehicles. In this paper, we propose a novel two stage 3D object detection method aimed at get the optimal solution of object location in 3D space based on regressing two additional 3D object properties by a deep convolutional neural network and combined with cascaded geometric constraints between the 2D and 3D boxes. First, we modify the existing 3D properties regressing network by adding two additional components, viewpoints classification and the center projection of the 3D bounding box s bottom face. Second, we use the predicted center projection combined with similar triangle constraint to acquire an initial 3D bounding box by a closed-form solution. Then, the location predicted by previous step is used as the initial value of the over-determined equations constructed by 2D and 3D boxes fitting constraint with the configuration determined with the classified viewpoint. Finally, we use the recovered physical world information by the 3D detections to filter out the false detection and false alarm in 2D detections. We compare our method with the state-of-the-arts on the KITTI dataset show that although conceptually simple, our method outperforms more complex and computational expensive methods not only by improving the overall precision of 3D detections, but also increasing the orientation estimation precision. Furthermore our method can deal with the truncated objects to some extent and remove the false alarm and false detections in both 2D and 3D detections.



### DeepHealth: Review and challenges of artificial intelligence in health informatics
- **Arxiv ID**: http://arxiv.org/abs/1909.00384v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.00384v2)
- **Published**: 2019-09-01 11:54:38+00:00
- **Updated**: 2020-08-08 05:54:41+00:00
- **Authors**: Gloria Hyunjung Kwak, Pan Hui
- **Comment**: 42 pages, 19 figures, under review
- **Journal**: None
- **Summary**: Artificial intelligence has provided us with an exploration of a whole new research era. As more data and better computational power become available, the approach is being implemented in various fields. The demand for it in health informatics is also increasing, and we can expect to see the potential benefits of its applications in healthcare. It can help clinicians diagnose disease, identify drug effects for each patient, understand the relationship between genotypes and phenotypes, explore new phenotypes or treatment recommendations, and predict infectious disease outbreaks with high accuracy. In contrast to traditional models, recent artificial intelligence approaches do not require domain-specific data pre-processing, and it is expected that it will ultimately change life in the future. Despite its notable advantages, there are some key challenges on data (high dimensionality, heterogeneity, time dependency, sparsity, irregularity, lack of label, bias) and model (reliability, interpretability, feasibility, security, scalability) for practical use. This article presents a comprehensive review of research applying artificial intelligence in health informatics, focusing on the last seven years in the fields of medical imaging, electronic health records, genomics, sensing, and online communication health, as well as challenges and promising directions for future research. We highlight ongoing popular approaches' research and identify several challenges in building models.



### Improved Image Augmentation for Convolutional Neural Networks by Copyout and CopyPairing
- **Arxiv ID**: http://arxiv.org/abs/1909.00390v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.00390v2)
- **Published**: 2019-09-01 12:59:09+00:00
- **Updated**: 2019-09-22 05:26:07+00:00
- **Authors**: Philip May
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: Image augmentation is a widely used technique to improve the performance of convolutional neural networks (CNNs). In common image shifting, cropping, flipping, shearing and rotating are used for augmentation. But there are more advanced techniques like Cutout and SamplePairing. In this work we present two improvements of the state-of-the-art Cutout and SamplePairing techniques. Our new method called Copyout takes a square patch of another random training image and copies it onto a random location of each image used for training. The second technique we discovered is called CopyPairing. It combines Copyout and SamplePairing for further augmentation and even better performance. We apply different experiments with these augmentation techniques on the CIFAR-10 dataset to evaluate and compare them under different configurations. In our experiments we show that Copyout reduces the test error rate by 8.18% compared with Cutout and 4.27% compared with SamplePairing. CopyPairing reduces the test error rate by 11.97% compared with Cutout and 8.21% compared with SamplePairing. Copyout and CopyPairing implementations are available at https://github.com/t-systems-on-site-services-gmbh/coocop.



### Towards Robust Learning-Based Pose Estimation of Noncooperative Spacecraft
- **Arxiv ID**: http://arxiv.org/abs/1909.00392v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.00392v1)
- **Published**: 2019-09-01 13:22:19+00:00
- **Updated**: 2019-09-01 13:22:19+00:00
- **Authors**: Tae Ha Park, Sumant Sharma, Simone D'Amico
- **Comment**: Presented at 2019 AAS/AIAA Astrodynamics Specialist Conference
- **Journal**: None
- **Summary**: This work presents a novel Convolutional Neural Network (CNN) architecture and a training procedure to enable robust and accurate pose estimation of a noncooperative spacecraft. First, a new CNN architecture is introduced that has scored a fourth place in the recent Pose Estimation Challenge hosted by Stanford's Space Rendezvous Laboratory (SLAB) and the Advanced Concepts Team (ACT) of the European Space Agency (ESA). The proposed architecture first detects the object by regressing a 2D bounding box, then a separate network regresses the 2D locations of the known surface keypoints from an image of the target cropped around the detected Region-of-Interest (RoI). In a single-image pose estimation problem, the extracted 2D keypoints can be used in conjunction with corresponding 3D model coordinates to compute relative pose via the Perspective-n-Point (PnP) problem. These keypoint locations have known correspondences to those in the 3D model, since the CNN is trained to predict the corners in a pre-defined order, allowing for bypassing the computationally expensive feature matching processes. This work also introduces and explores the texture randomization to train a CNN for spaceborne applications. Specifically, Neural Style Transfer (NST) is applied to randomize the texture of the spacecraft in synthetically rendered images. It is shown that using the texture-randomized images of spacecraft for training improves the network's performance on spaceborne images without exposure to them during training. It is also shown that when using the texture-randomized spacecraft images during training, regressing 3D bounding box corners leads to better performance on spaceborne images than regressing surface keypoints, as NST inevitably distorts the spacecraft's geometric features to which the surface keypoints have closer relation.



### Visual Deprojection: Probabilistic Recovery of Collapsed Dimensions
- **Arxiv ID**: http://arxiv.org/abs/1909.00475v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.00475v1)
- **Published**: 2019-09-01 21:34:28+00:00
- **Updated**: 2019-09-01 21:34:28+00:00
- **Authors**: Guha Balakrishnan, Adrian V. Dalca, Amy Zhao, John V. Guttag, Fredo Durand, William T. Freeman
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: We introduce visual deprojection: the task of recovering an image or video that has been collapsed along a dimension. Projections arise in various contexts, such as long-exposure photography, where a dynamic scene is collapsed in time to produce a motion-blurred image, and corner cameras, where reflected light from a scene is collapsed along a spatial dimension because of an edge occluder to yield a 1D video. Deprojection is ill-posed-- often there are many plausible solutions for a given input. We first propose a probabilistic model capturing the ambiguity of the task. We then present a variational inference strategy using convolutional neural networks as functional approximators. Sampling from the inference network at test time yields plausible candidates from the distribution of original signals that are consistent with a given input projection. We evaluate the method on several datasets for both spatial and temporal deprojection tasks. We first demonstrate the method can recover human gait videos and face images from spatial projections, and then show that it can recover videos of moving digits from dramatically motion-blurred images obtained via temporal projection.



### A Semi-Automated Usability Evaluation Framework for Interactive Image Segmentation Systems
- **Arxiv ID**: http://arxiv.org/abs/1909.00482v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.00482v1)
- **Published**: 2019-09-01 22:33:06+00:00
- **Updated**: 2019-09-01 22:33:06+00:00
- **Authors**: Mario Amrehn, Stefan Steidl, Reinier Kortekaas, Maddalena Strumia, Markus Weingarten, Markus Kowarschik, Andreas Maier
- **Comment**: Accepted as research article at the International Journal of
  Biomedical Imaging, Hindawi
- **Journal**: None
- **Summary**: For complex segmentation tasks, the achievable accuracy of fully automated systems is inherently limited. Specifically, when a precise segmentation result is desired for a small amount of given data sets, semi-automatic methods exhibit a clear benefit for the user. The optimization of human computer interaction (HCI) is an essential part of interactive image segmentation. Nevertheless, publications introducing novel interactive segmentation systems (ISS) often lack an objective comparison of HCI aspects. It is demonstrated, that even when the underlying segmentation algorithm is the same throughout interactive prototypes, their user experience may vary substantially. As a result, users prefer simple interfaces as well as a considerable degree of freedom to control each iterative step of the segmentation. In this article, an objective method for the comparison of ISS is proposed, based on extensive user studies. A summative qualitative content analysis is conducted via abstraction of visual and verbal feedback given by the participants. A direct assessment of the segmentation system is executed by the users via the system usability scale (SUS) and AttrakDiff-2 questionnaires. Furthermore, an approximation of the findings regarding usability aspects in those studies is introduced, conducted solely from the system-measurable user actions during their usage of interactive segmentation prototypes. The prediction of all questionnaire results has an average relative error of 8.9%, which is close to the expected precision of the questionnaire results themselves. This automated evaluation scheme may significantly reduce the resources necessary to investigate each variation of a prototype's user interface (UI) features and segmentation methodologies.



