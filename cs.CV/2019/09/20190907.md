# Arxiv Papers in cs.CV on 2019-09-07
### Look and Modify: Modification Networks for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1909.03169v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.03169v2)
- **Published**: 2019-09-07 02:15:24+00:00
- **Updated**: 2020-03-09 06:20:49+00:00
- **Authors**: Fawaz Sammani, Mahmoud Elsayed
- **Comment**: Published in BMVC 2019
- **Journal**: None
- **Summary**: Attention-based neural encoder-decoder frameworks have been widely used for image captioning. Many of these frameworks deploy their full focus on generating the caption from scratch by relying solely on the image features or the object detection regional features. In this paper, we introduce a novel framework that learns to modify existing captions from a given framework by modeling the residual information, where at each timestep the model learns what to keep, remove or add to the existing caption allowing the model to fully focus on "what to modify" rather than on "what to predict". We evaluate our method on the COCO dataset, trained on top of several image captioning frameworks and show that our model successfully modifies captions yielding better ones with better evaluation scores.



### Non-discriminative data or weak model? On the relative importance of data and model resolution
- **Arxiv ID**: http://arxiv.org/abs/1909.03205v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.03205v2)
- **Published**: 2019-09-07 07:19:29+00:00
- **Updated**: 2019-10-17 23:13:32+00:00
- **Authors**: Mark Sandler, Jonathan Baccash, Andrey Zhmoginov, Andrew Howard
- **Comment**: ICCV 2019 Workshop on Real-World Recognition from Low-Quality Images
  and Videos
- **Journal**: None
- **Summary**: We explore the question of how the resolution of the input image ("input resolution") affects the performance of a neural network when compared to the resolution of the hidden layers ("internal resolution"). Adjusting these characteristics is frequently used as a hyperparameter providing a trade-off between model performance and accuracy. An intuitive interpretation is that the reduced information content in the low-resolution input causes decay in the accuracy. In this paper, we show that up to a point, the input resolution alone plays little role in the network performance, and it is the internal resolution that is the critical driver of model quality. We then build on these insights to develop novel neural network architectures that we call \emph{Isometric Neural Networks}. These models maintain a fixed internal resolution throughout their entire depth. We demonstrate that they lead to high accuracy models with low activation footprint and parameter count.



### Scene Recognition with Prototype-agnostic Scene Layout
- **Arxiv ID**: http://arxiv.org/abs/1909.03234v1
- **DOI**: 10.1109/TIP.2020.2986599
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.03234v1)
- **Published**: 2019-09-07 10:04:01+00:00
- **Updated**: 2019-09-07 10:04:01+00:00
- **Authors**: Gongwei Chen, Xinhang Song, Haitao Zeng, Shuqiang Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Abstract--- Exploiting the spatial structure in scene images is a key research direction for scene recognition. Due to the large intra-class structural diversity, building and modeling flexible structural layout to adapt various image characteristics is a challenge. Existing structural modeling methods in scene recognition either focus on predefined grids or rely on learned prototypes, which all have limited representative ability. In this paper, we propose Prototype-agnostic Scene Layout (PaSL) construction method to build the spatial structure for each image without conforming to any prototype. Our PaSL can flexibly capture the diverse spatial characteristic of scene images and have considerable generalization capability. Given a PaSL, we build Layout Graph Network (LGN) where regions in PaSL are defined as nodes and two kinds of independent relations between regions are encoded as edges. The LGN aims to incorporate two topological structures (formed in spatial and semantic similarity dimensions) into image representations through graph convolution. Extensive experiments show that our approach achieves state-of-the-art results on widely recognized MIT67 and SUN397 datasets without multi-model or multi-scale fusion. Moreover, we also conduct the experiments on one of the largest scale datasets, Places365. The results demonstrate the proposed method can be well generalized and obtains competitive performance.



### Graph Convolutional Networks for Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/1909.03252v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.03252v1)
- **Published**: 2019-09-07 11:51:43+00:00
- **Updated**: 2019-09-07 11:51:43+00:00
- **Authors**: Runhao Zeng, Wenbing Huang, Mingkui Tan, Yu Rong, Peilin Zhao, Junzhou Huang, Chuang Gan
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Most state-of-the-art action localization systems process each action proposal individually, without explicitly exploiting their relations during learning. However, the relations between proposals actually play an important role in action localization, since a meaningful action always consists of multiple proposals in a video. In this paper, we propose to exploit the proposal-proposal relations using Graph Convolutional Networks (GCNs). First, we construct an action proposal graph, where each proposal is represented as a node and their relations between two proposals as an edge. Here, we use two types of relations, one for capturing the context information for each proposal and the other one for characterizing the correlations between distinct actions. Then we apply the GCNs over the graph to model the relations among different proposals and learn powerful representations for the action classification and localization. Experimental results show that our approach significantly outperforms the state-of-the-art on THUMOS14 (49.1% versus 42.8%). Moreover, augmentation experiments on ActivityNet also verify the efficacy of modeling action proposal relationships. Codes are available at https://github.com/Alvin-Zeng/PGCN.



### Recognition Of Surface Defects On Steel Sheet Using Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.03258v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.03258v2)
- **Published**: 2019-09-07 12:19:10+00:00
- **Updated**: 2019-09-17 13:17:24+00:00
- **Authors**: Jingwen Fu, Xiaoyan Zhu, Yingbin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic defect recognition is one of the research hotspots in steel production, but most of the current methods mainly extract features manually and use machine learning classifiers to recognize defects, which cannot tackle the situation, where there are few data available to train and confine to a certain scene. Therefore, in this paper, a new approach is proposed which consists of part of pretrained VGG16 as a feature extractor and a new CNN neural network as a classifier to recognize the defect of steel strip surface based on the feature maps created by the feature extractor. Our method achieves an accuracy of 99.1% and 96.0% while the dataset contains 150 images each class and 10 images each class respectively, which is much better than previous methods.



### Unsupervised Image Regression for Heterogeneous Change Detection
- **Arxiv ID**: http://arxiv.org/abs/1909.05948v1
- **DOI**: 10.1109/TGRS.2019.2930348
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.05948v1)
- **Published**: 2019-09-07 12:26:11+00:00
- **Updated**: 2019-09-07 12:26:11+00:00
- **Authors**: Luigi T. Luppino, Filippo M. Bianchi, Gabriele Moser, Stian N. Anfinsen
- **Comment**: arXiv admin note: text overlap with arXiv:1807.11766
- **Journal**: None
- **Summary**: Change detection in heterogeneous multitemporal satellite images is an emerging and challenging topic in remote sensing. In particular, one of the main challenges is to tackle the problem in an unsupervised manner. In this paper we propose an unsupervised framework for bitemporal heterogeneous change detection based on the comparison of affinity matrices and image regression. First, our method quantifies the similarity of affinity matrices computed from co-located image patches in the two images. This is done to automatically identify pixels that are likely to be unchanged. With the identified pixels as pseudo-training data, we learn a transformation to map the first image to the domain of the other image, and vice versa. Four regression methods are selected to carry out the transformation: Gaussian process regression, support vector regression, random forest regression, and a recently proposed kernel regression method called homogeneous pixel transformation. To evaluate the potentials and limitations of our framework, and also the benefits and disadvantages of each regression method, we perform experiments on two real data sets. The results indicate that the comparison of the affinity matrices can already be considered a change detection method by itself. However, image regression is shown to improve the results obtained by the previous step alone and produces accurate change detection maps despite of the heterogeneity of the multitemporal input data. Notably, the random forest regression approach excels by achieving similar accuracy as the other methods, but with a significantly lower computational cost and with fast and robust tuning of hyperparameters.



### Road Mapping In LiDAR Images Using A Joint-Task Dense Dilated Convolutions Merging Network
- **Arxiv ID**: http://arxiv.org/abs/1909.04588v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.04588v1)
- **Published**: 2019-09-07 16:35:55+00:00
- **Updated**: 2019-09-07 16:35:55+00:00
- **Authors**: Qinghui Liu, Michael Kampffmeyer, Robert Jenssen, Arnt-Børre Salberg
- **Comment**: IGARSS 2019. arXiv admin note: text overlap with arXiv:1908.11799
- **Journal**: None
- **Summary**: It is important, but challenging, for the forest industry to accurately map roads which are used for timber transport by trucks. In this work, we propose a Dense Dilated Convolutions Merging Network (DDCM-Net) to detect these roads in lidar images. The DDCM-Net can effectively recognize multi-scale and complex shaped roads with similar texture and colors, and also is shown to have superior performance over existing methods. To further improve its ability to accurately infer categories of roads, we propose the use of a joint-task learning strategy that utilizes two auxiliary output branches, i.e, multi-class classification and binary segmentation, joined with the main output of full-class segmentation. This pushes the network towards learning more robust representations that are expected to boost the ultimate performance of the main task. In addition, we introduce an iterative-random-weighting method to automatically weigh the joint losses for auxiliary tasks. This can avoid the difficult and expensive process of tuning the weights of each task's loss by hand. The experiments demonstrate that our proposed joint-task DDCM-Net can achieve better performance with fewer parameters and higher computational efficiency than previous state-of-the-art approaches.



### Exploring Temporal Differences in 3D Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.03309v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.03309v1)
- **Published**: 2019-09-07 17:45:27+00:00
- **Updated**: 2019-09-07 17:45:27+00:00
- **Authors**: Gagan Kanojia, Sudhakar Kumawat, Shanmuganathan Raman
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional 3D convolutions are computationally expensive, memory intensive, and due to large number of parameters, they often tend to overfit. On the other hand, 2D CNNs are less computationally expensive and less memory intensive than 3D CNNs and have shown remarkable results in applications like image classification and object recognition. However, in previous works, it has been observed that they are inferior to 3D CNNs when applied on a spatio-temporal input. In this work, we propose a convolutional block which extracts the spatial information by performing a 2D convolution and extracts the temporal information by exploiting temporal differences, i.e., the change in the spatial information at different time instances, using simple operations of shift, subtract and add without utilizing any trainable parameters. The proposed convolutional block has same number of parameters as of a 2D convolution kernel of size nxn, i.e. n^2, and has n times lesser parameters than an nxnxn 3D convolution kernel. We show that the 3D CNNs perform better when the 3D convolution kernels are replaced by the proposed convolutional blocks. We evaluate the proposed convolutional block on UCF101 and ModelNet datasets.



### Relationships from Entity Stream
- **Arxiv ID**: http://arxiv.org/abs/1909.03315v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.03315v1)
- **Published**: 2019-09-07 18:24:57+00:00
- **Updated**: 2019-09-07 18:24:57+00:00
- **Authors**: Martin Andrews, Sam Witteveen
- **Comment**: Accepted paper for the ViGIL workshop at NIPS 2017. (4 pages +
  references)
- **Journal**: None
- **Summary**: Relational reasoning is a central component of intelligent behavior, but has proven difficult for neural networks to learn. The Relation Network (RN) module was recently proposed by DeepMind to solve such problems, and demonstrated state-of-the-art results on a number of datasets. However, the RN module scales quadratically in the size of the input, since it calculates relationship factors between every patch in the visual field, including those that do not correspond to entities. In this paper, we describe an architecture that enables relationships to be determined from a stream of entities obtained by an attention mechanism over the input field. The model is trained end-to-end, and demonstrates equivalent performance with greater interpretability while requiring only a fraction of the model parameters of the original RN module.



### Explainable Deep Learning for Video Recognition Tasks: A Framework & Recommendations
- **Arxiv ID**: http://arxiv.org/abs/1909.05667v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.05667v1)
- **Published**: 2019-09-07 19:34:48+00:00
- **Updated**: 2019-09-07 19:34:48+00:00
- **Authors**: Liam Hiley, Alun Preece, Yulia Hicks
- **Comment**: None
- **Journal**: None
- **Summary**: The popularity of Deep Learning for real-world applications is ever-growing. With the introduction of high performance hardware, applications are no longer limited to image recognition. With the introduction of more complex problems comes more and more complex solutions, and the increasing need for explainable AI. Deep Neural Networks for Video tasks are amongst the most complex models, with at least twice the parameters of their Image counterparts. However, explanations for these models are often ill-adapted to the video domain. The current work in explainability for video models is still overshadowed by Image techniques, while Video Deep Learning itself is quickly gaining on methods for still images. This paper seeks to highlight the need for explainability methods designed with video deep learning models, and by association spatio-temporal input in mind, by first illustrating the cutting edge for video deep learning, and then noting the scarcity of research into explanations for these methods.



