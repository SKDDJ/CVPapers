# Arxiv Papers in cs.CV on 2019-09-21
### IntersectGAN: Learning Domain Intersection for Generating Images with Multiple Attributes
- **Arxiv ID**: http://arxiv.org/abs/1909.09767v2
- **DOI**: 10.1145/3343031.3350908
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09767v2)
- **Published**: 2019-09-21 03:40:02+00:00
- **Updated**: 2019-10-03 10:18:21+00:00
- **Authors**: Zehui Yao, Boyan Zhang, Zhiyong Wang, Wanli Ouyang, Dong Xu, Dagan Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) have demonstrated great success in generating various visual content. However, images generated by existing GANs are often of attributes (e.g., smiling expression) learned from one image domain. As a result, generating images of multiple attributes requires many real samples possessing multiple attributes which are very resource expensive to be collected. In this paper, we propose a novel GAN, namely IntersectGAN, to learn multiple attributes from different image domains through an intersecting architecture. For example, given two image domains $X_1$ and $X_2$ with certain attributes, the intersection $X_1 \cap X_2$ denotes a new domain where images possess the attributes from both $X_1$ and $X_2$ domains. The proposed IntersectGAN consists of two discriminators $D_1$ and $D_2$ to distinguish between generated and real samples of different domains, and three generators where the intersection generator is trained against both discriminators. And an overall adversarial loss function is defined over three generators. As a result, our proposed IntersectGAN can be trained on multiple domains of which each presents one specific attribute, and eventually eliminates the need of real sample images simultaneously possessing multiple attributes. By using the CelebFaces Attributes dataset, our proposed IntersectGAN is able to produce high quality face images possessing multiple attributes (e.g., a face with black hair and a smiling expression). Both qualitative and quantitative evaluations are conducted to compare our proposed IntersectGAN with other baseline methods. Besides, several different applications of IntersectGAN have been explored with promising results.



### Generating Positive Bounding Boxes for Balanced Training of Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/1909.09777v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09777v3)
- **Published**: 2019-09-21 05:27:15+00:00
- **Updated**: 2020-06-19 07:50:03+00:00
- **Authors**: Kemal Oksuz, Baris Can Cam, Emre Akbas, Sinan Kalkan
- **Comment**: To appear in WACV 20
- **Journal**: None
- **Summary**: Two-stage deep object detectors generate a set of regions-of-interest (RoI) in the first stage, then, in the second stage, identify objects among the proposed RoIs that sufficiently overlap with a ground truth (GT) box. The second stage is known to suffer from a bias towards RoIs that have low intersection-over-union (IoU) with the associated GT boxes. To address this issue, we first propose a sampling method to generate bounding boxes (BB) that overlap with a given reference box more than a given IoU threshold. Then, we use this BB generation method to develop a positive RoI (pRoI) generator that produces RoIs following any desired spatial or IoU distribution, for the second-stage. We show that our pRoI generator is able to simulate other sampling methods for positive examples such as hard example mining and prime sampling. Using our generator as an analysis tool, we show that (i) IoU imbalance has an adverse effect on performance, (ii) hard positive example mining improves the performance only for certain input IoU distributions, and (iii) the imbalance among the foreground classes has an adverse effect on performance and that it can be alleviated at the batch level. Finally, we train Faster R-CNN using our pRoI generator and, compared to conventional training, obtain better or on-par performance for low IoUs and significant improvements when trained for higher IoUs for Pascal VOC and MS COCO datasets. The code is available at: https://github.com/kemaloksuz/BoundingBoxGenerator.



### Adversarial Learning of General Transformations for Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.09801v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.09801v1)
- **Published**: 2019-09-21 09:43:24+00:00
- **Updated**: 2019-09-21 09:43:24+00:00
- **Authors**: Saypraseuth Mounsaveng, David Vazquez, Ismail Ben Ayed, Marco Pedersoli
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation (DA) is fundamental against overfitting in large convolutional neural networks, especially with a limited training dataset. In images, DA is usually based on heuristic transformations, like geometric or color transformations. Instead of using predefined transformations, our work learns data augmentation directly from the training data by learning to transform images with an encoder-decoder architecture combined with a spatial transformer network. The transformed images still belong to the same class but are new, more complex samples for the classifier. Our experiments show that our approach is better than previous generative data augmentation methods, and comparable to predefined transformation methods when training an image classifier.



### Visual Odometry Revisited: What Should Be Learnt?
- **Arxiv ID**: http://arxiv.org/abs/1909.09803v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09803v4)
- **Published**: 2019-09-21 10:00:21+00:00
- **Updated**: 2020-02-18 02:32:35+00:00
- **Authors**: Huangying Zhan, Chamara Saroj Weerasekera, Jiawang Bian, Ian Reid
- **Comment**: ICRA2020. Demo video: https://youtu.be/Nl8mFU4SJKY Code:
  https://github.com/Huangying-Zhan/DF-VO
- **Journal**: None
- **Summary**: In this work we present a monocular visual odometry (VO) algorithm which leverages geometry-based methods and deep learning. Most existing VO/SLAM systems with superior performance are based on geometry and have to be carefully designed for different application scenarios. Moreover, most monocular systems suffer from scale-drift issue.Some recent deep learning works learn VO in an end-to-end manner but the performance of these deep systems is still not comparable to geometry-based methods. In this work, we revisit the basics of VO and explore the right way for integrating deep learning with epipolar geometry and Perspective-n-Point (PnP) method. Specifically, we train two convolutional neural networks (CNNs) for estimating single-view depths and two-view optical flows as intermediate outputs. With the deep predictions, we design a simple but robust frame-to-frame VO algorithm (DF-VO) which outperforms pure deep learning-based and geometry-based methods. More importantly, our system does not suffer from the scale-drift issue being aided by a scale consistent single-view depth CNN. Extensive experiments on KITTI dataset shows the robustness of our system and a detailed ablation study shows the effect of different factors in our system.



### Video Skimming: Taxonomy and Comprehensive Survey
- **Arxiv ID**: http://arxiv.org/abs/1909.12948v1
- **DOI**: 10.1145/3347712
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.12948v1)
- **Published**: 2019-09-21 11:21:09+00:00
- **Updated**: 2019-09-21 11:21:09+00:00
- **Authors**: Vivekraj V. K., Debashis Sen, Balasubramanian Raman
- **Comment**: None
- **Journal**: ACM Computing Surveys (CSUR), Volume 52, Issue 5, 2019
- **Summary**: Video skimming, also known as dynamic video summarization, generates a temporally abridged version of a given video. Skimming can be achieved by identifying significant components either in uni-modal or multi-modal features extracted from the video. Being dynamic in nature, video skimming, through temporal connectivity, allows better understanding of the video from its summary. Having this obvious advantage, recently, video skimming has drawn the focus of many researchers benefiting from the easy availability of the required computing resources. In this paper, we provide a comprehensive survey on video skimming focusing on the substantial amount of literature from the past decade. We present a taxonomy of video skimming approaches, and discuss their evolution highlighting key advances. We also provide a study on the components required for the evaluation of a video skimming performance.



### Using theoretical ROC curves for analysing machine learning binary classifiers
- **Arxiv ID**: http://arxiv.org/abs/1909.09816v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.09816v1)
- **Published**: 2019-09-21 11:48:19+00:00
- **Updated**: 2019-09-21 11:48:19+00:00
- **Authors**: Luma Omar, Ioannis Ivrissimtzis
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Most binary classifiers work by processing the input to produce a scalar response and comparing it to a threshold value. The various measures of classifier performance assume, explicitly or implicitly, probability distributions $P_s$ and $P_n$ of the response belonging to either class, probability distributions for the cost of each type of misclassification, and compute a performance score from the expected cost.   In machine learning, classifier responses are obtained experimentally and performance scores are computed directly from them, without any assumptions on $P_s$ and $P_n$. Here, we argue that the omitted step of estimating theoretical distributions for $P_s$ and $P_n$ can be useful. In a biometric security example, we fit beta distributions to the responses of two classifiers, one based on logistic regression and one on ANNs, and use them to establish a categorisation into a small number of classes with different extremal behaviours at the ends of the ROC curves.



### CANZSL: Cycle-Consistent Adversarial Networks for Zero-Shot Learning from Natural Language
- **Arxiv ID**: http://arxiv.org/abs/1909.09822v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1909.09822v1)
- **Published**: 2019-09-21 13:19:15+00:00
- **Updated**: 2019-09-21 13:19:15+00:00
- **Authors**: Zhi Chen, Jingjing Li, Yadan Luo, Zi Huang, Yang Yang
- **Comment**: WACV 2020
- **Journal**: None
- **Summary**: Existing methods using generative adversarial approaches for Zero-Shot Learning (ZSL) aim to generate realistic visual features from class semantics by a single generative network, which is highly under-constrained. As a result, the previous methods cannot guarantee that the generated visual features can truthfully reflect the corresponding semantics. To address this issue, we propose a novel method named Cycle-consistent Adversarial Networks for Zero-Shot Learning (CANZSL). It encourages a visual feature generator to synthesize realistic visual features from semantics, and then inversely translate back synthesized the visual feature to corresponding semantic space by a semantic feature generator. Furthermore, in this paper a more challenging and practical ZSL problem is considered where the original semantics are from natural language with irrelevant words instead of clean semantics that are widely used in previous work. Specifically, a multi-modal consistent bidirectional generative adversarial network is trained to handle unseen instances by leveraging noise in the natural language. A forward one-to-many mapping from one text description to multiple visual features is coupled with an inverse many-to-one mapping from the visual space to the semantic space. Thus, a multi-modal cycle-consistency loss between the synthesized semantic representations and the ground truth can be learned and leveraged to enforce the generated semantic features to approximate to the real distribution in semantic space. Extensive experiments are conducted to demonstrate that our method consistently outperforms state-of-the-art approaches on natural language-based zero-shot learning tasks.



### Automatic Posture and Movement Tracking of Infants with Wearable Movement Sensors
- **Arxiv ID**: http://arxiv.org/abs/1909.09823v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.09823v2)
- **Published**: 2019-09-21 13:37:28+00:00
- **Updated**: 2019-12-18 13:42:39+00:00
- **Authors**: Manu Airaksinen, Okko Räsänen, Elina Ilén, Taru Häyrinen, Anna Kivi, Viviana Marchi, Anastasia Gallen, Sonja Blom, Anni Varhe, Nico Kaartinen, Leena Haataja, Sampsa Vanhatalo
- **Comment**: 17 pages, 8 figures, preprint of manuscript accepted for publication
  for publication in Nature Scientific Reports
- **Journal**: None
- **Summary**: Infants' spontaneous and voluntary movements mirror developmental integrity of brain networks since they require coordinated activation of multiple sites in the central nervous system. Accordingly, early detection of infants with atypical motor development holds promise for recognizing those infants who are at risk for a wide range of neurodevelopmental disorders (e.g., cerebral palsy, autism spectrum disorders). Previously, novel wearable technology has shown promise for offering efficient, scalable and automated methods for movement assessment in adults. Here, we describe the development of an infant wearable, a multi-sensor smart jumpsuit that allows mobile accelerometer and gyroscope data collection during movements. Using this suit, we first recorded play sessions of 22 typically developing infants of approximately 7 months of age. These data were manually annotated for infant posture and movement based on video recordings of the sessions, and using a novel annotation scheme specifically designed to assess the overall movement pattern of infants in the given age group. A machine learning algorithm, based on deep convolutional neural networks (CNNs) was then trained for automatic detection of posture and movement classes using the data and annotations. Our experiments show that the setup can be used for quantitative tracking of infant movement activities with a human equivalent accuracy, i.e., it meets the human inter-rater agreement levels in infant posture and movement classification. We also quantify the ambiguity of human observers in analyzing infant movements, and propose a method for utilizing this uncertainty for performance improvements in training of the automated classifier. Comparison of different sensor configurations also shows that four-limb recording leads to the best performance in posture and movement classification.



### Invasiveness Prediction of Pulmonary Adenocarcinomas Using Deep Feature Fusion Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.09837v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09837v1)
- **Published**: 2019-09-21 14:54:10+00:00
- **Updated**: 2019-09-21 14:54:10+00:00
- **Authors**: Xiang Li, Jiechao Ma, Hongwei Li
- **Comment**: None
- **Journal**: None
- **Summary**: Early diagnosis of pathological invasiveness of pulmonary adenocarcinomas using computed tomography (CT) imaging would alter the course of treatment of adenocarcinomas and subsequently improve the prognosis. Most of the existing systems use either conventional radiomics features or deep-learning features alone to predict the invasiveness. In this study, we explore the fusion of the two kinds of features and claim that radiomics features can be complementary to deep-learning features. An effective deep feature fusion network is proposed to exploit the complementarity between the two kinds of features, which improves the invasiveness prediction results. We collected a private dataset that contains lung CT scans of 676 patients categorized into four invasiveness types from a collaborating hospital. Evaluations on this dataset demonstrate the effectiveness of our proposal.



### Class Activation Map generation by Multiple Level Class Grouping and Orthogonal Constraint
- **Arxiv ID**: http://arxiv.org/abs/1909.09839v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09839v1)
- **Published**: 2019-09-21 14:59:09+00:00
- **Updated**: 2019-09-21 14:59:09+00:00
- **Authors**: Kaixu Huang, Fanman Meng, Hongliang Li, Shuai Chen, Qingbo Wu, King N. Ngan
- **Comment**: International Conference on Digital Image Computing: Techniques and
  Applications(DICTA) 2019
- **Journal**: None
- **Summary**: Class activation map (CAM) highlights regions of classes based on classification network, which is widely used in weakly supervised tasks. However, it faces the problem that the class activation regions are usually small and local. Although several efforts paid to the second step (the CAM generation step) have partially enhanced the generation, we believe such problem is also caused by the first step (training step), because single classification model trained on the entire classes contains finite discriminate information that limits the object region extraction. To this end, this paper solves CAM generation by using multiple classification models. To form multiple classification networks that carry different discriminative information, we try to capture the semantic relationships between classes to form different semantic levels of classification models. Specifically, hierarchical clustering based on class relationships is used to form hierarchical clustering results, where the clustering levels are treated as semantic levels to form the classification models. Moreover, a new orthogonal module and a two-branch based CAM generation method are proposed to generate class regions that are orthogonal and complementary. We use the PASCAL VOC 2012 dataset to verify the proposed method. Experimental results show that our approach improves the CAM generation.



### Advances in Computer-Aided Diagnosis of Diabetic Retinopathy
- **Arxiv ID**: http://arxiv.org/abs/1909.09853v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09853v1)
- **Published**: 2019-09-21 16:21:15+00:00
- **Updated**: 2019-09-21 16:21:15+00:00
- **Authors**: Saket S. Chaturvedi, Kajol Gupta, Vaishali Ninawe, Prakash S. Prasad
- **Comment**: 6 pages, 1 figures
- **Journal**: None
- **Summary**: Diabetic Retinopathy is a critical health problem influences 100 million individuals worldwide, and these figures are expected to rise, particularly in Asia. Diabetic Retinopathy is a chronic eye disease which can lead to irreversible vision loss. Considering the visual complexity of retinal images, the early-stage diagnosis of Diabetic Retinopathy can be challenging for human experts. However, Early detection of Diabetic Retinopathy can significantly help to avoid permanent vision loss. The capability of computer-aided detection systems to accurately and efficiently detect the diabetic retinopathy had popularized them among researchers. In this review paper, the literature search was conducted on PubMed, Google Scholar, IEEE Explorer with a focus on the computer-aided detection of Diabetic Retinopathy using either of Machine Learning or Deep Learning algorithms. Moreover, this study also explores the typical methodology utilized for the computer-aided diagnosis of Diabetic Retinopathy. This review paper is aimed to direct the researchers about the limitations of current methods and identify the specific areas in the field to boost future research.



### Learning and Segmenting Dense Voxel Embeddings for 3D Neuron Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1909.09872v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09872v2)
- **Published**: 2019-09-21 19:00:27+00:00
- **Updated**: 2021-08-05 23:43:10+00:00
- **Authors**: Kisuk Lee, Ran Lu, Kyle Luther, H. Sebastian Seung
- **Comment**: None
- **Journal**: None
- **Summary**: We show dense voxel embeddings learned via deep metric learning can be employed to produce a highly accurate segmentation of neurons from 3D electron microscopy images. A "metric graph" on a set of edges between voxels is constructed from the dense voxel embeddings generated by a convolutional network. Partitioning the metric graph with long-range edges as repulsive constraints yields an initial segmentation with high precision, with substantial accuracy gain for very thin objects. The convolutional embedding net is reused without any modification to agglomerate the systematic splits caused by complex "self-contact" motifs. Our proposed method achieves state-of-the-art accuracy on the challenging problem of 3D neuron reconstruction from the brain images acquired by serial section electron microscopy. Our alternative, object-centered representation could be more generally useful for other computational tasks in automated neural circuit reconstruction.



### Efficient Surface-Aware Semi-Global Matching with Multi-View Plane-Sweep Sampling
- **Arxiv ID**: http://arxiv.org/abs/1909.09891v1
- **DOI**: 10.5194/isprs-annals-IV-2-W7-137-2019
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09891v1)
- **Published**: 2019-09-21 20:37:55+00:00
- **Updated**: 2019-09-21 20:37:55+00:00
- **Authors**: Boitumelo Ruf, Thomas Pollok, Martin Weinmann
- **Comment**: None
- **Journal**: ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., IV-2/W7,
  137-144, 2019
- **Summary**: Online augmentation of an oblique aerial image sequence with structural information is an essential aspect in the process of 3D scene interpretation and analysis. One key aspect in this is the efficient dense image matching and depth estimation. Here, the Semi-Global Matching (SGM) approach has proven to be one of the most widely used algorithms for efficient depth estimation, providing a good trade-off between accuracy and computational complexity. However, SGM only models a first-order smoothness assumption, thus favoring fronto-parallel surfaces. In this work, we present a hierarchical algorithm that allows for efficient depth and normal map estimation together with confidence measures for each estimate. Our algorithm relies on a plane-sweep multi-image matching followed by an extended SGM optimization that allows to incorporate local surface orientations, thus achieving more consistent and accurate estimates in areasmade up of slanted surfaces, inherent to oblique aerial imagery. We evaluate numerous configurations of our algorithm on two different datasets using an absolute and relative accuracy measure. In our evaluation, we show that the results of our approach are comparable to the ones achieved by refined Structure-from-Motion (SfM) pipelines, such as COLMAP, which are designed for offline processing. In contrast, however, our approach only considers a confined image bundle of an input sequence, thus allowing to perform an online and incremental computation at 1Hz-2Hz.



### Learning a Fixed-Length Fingerprint Representation
- **Arxiv ID**: http://arxiv.org/abs/1909.09901v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09901v2)
- **Published**: 2019-09-21 21:28:28+00:00
- **Updated**: 2019-12-18 16:15:10+00:00
- **Authors**: Joshua J. Engelsma, Kai Cao, Anil K. Jain
- **Comment**: to appear in IEEE Transactions on Pattern Analysis and Machine
  Intelligence
- **Journal**: None
- **Summary**: We present DeepPrint, a deep network, which learns to extract fixed-length fingerprint representations of only 200 bytes. DeepPrint incorporates fingerprint domain knowledge, including alignment and minutiae detection, into the deep network architecture to maximize the discriminative power of its representation. The compact, DeepPrint representation has several advantages over the prevailing variable length minutiae representation which (i) requires computationally expensive graph matching techniques, (ii) is difficult to secure using strong encryption schemes (e.g. homomorphic encryption), and (iii) has low discriminative power in poor quality fingerprints where minutiae extraction is unreliable. We benchmark DeepPrint against two top performing COTS SDKs (Verifinger and Innovatrics) from the NIST and FVC evaluations. Coupled with a re-ranking scheme, the DeepPrint rank-1 search accuracy on the NIST SD4 dataset against a gallery of 1.1 million fingerprints is comparable to the top COTS matcher, but it is significantly faster (DeepPrint: 98.80% in 0.3 seconds vs. COTS A: 98.85% in 27 seconds). To the best of our knowledge, the DeepPrint representation is the most compact and discriminative fixed-length fingerprint representation reported in the academic literature.



