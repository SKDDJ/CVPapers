# Arxiv Papers in cs.CV on 2019-09-26
### Unsupervised Domain Adaptation through Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/1909.11825v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.11825v2)
- **Published**: 2019-09-26 00:21:16+00:00
- **Updated**: 2019-09-29 08:09:29+00:00
- **Authors**: Yu Sun, Eric Tzeng, Trevor Darrell, Alexei A. Efros
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses unsupervised domain adaptation, the setting where labeled training data is available on a source domain, but the goal is to have good performance on a target domain with only unlabeled data. Like much of previous work, we seek to align the learned representations of the source and target domains while preserving discriminability. The way we accomplish alignment is by learning to perform auxiliary self-supervised task(s) on both domains simultaneously. Each self-supervised task brings the two domains closer together along the direction relevant to that task. Training this jointly with the main task classifier on the source domain is shown to successfully generalize to the unlabeled target domain. The presented objective is straightforward to implement and easy to optimize. We achieve state-of-the-art results on four out of seven standard benchmarks, and competitive results on segmentation adaptation. We also demonstrate that our method composes well with another popular pixel-level adaptation method.



### Breast Cancer Diagnosis with Transfer Learning and Global Pooling
- **Arxiv ID**: http://arxiv.org/abs/1909.11839v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.11839v1)
- **Published**: 2019-09-26 01:29:59+00:00
- **Updated**: 2019-09-26 01:29:59+00:00
- **Authors**: Sara Hosseinzadeh Kassani, Peyman Hosseinzadeh Kassani, Michal J. Wesolowski, Kevin A. Schneider, Ralph Deters
- **Comment**: None
- **Journal**: None
- **Summary**: Breast cancer is one of the most common causes of cancer-related death in women worldwide. Early and accurate diagnosis of breast cancer may significantly increase the survival rate of patients. In this study, we aim to develop a fully automatic, deep learning-based, method using descriptor features extracted by Deep Convolutional Neural Network (DCNN) models and pooling operation for the classification of hematoxylin and eosin stain (H&E) histological breast cancer images provided as a part of the International Conference on Image Analysis and Recognition (ICIAR) 2018 Grand Challenge on BreAst Cancer Histology (BACH) Images. Different data augmentation methods are applied to optimize the DCNN performance. We also investigated the efficacy of different stain normalization methods as a pre-processing step. The proposed network architecture using a pre-trained Xception model yields 92.50% average classification accuracy.



### Universal Graph Transformer Self-Attention Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.11855v13
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.11855v13)
- **Published**: 2019-09-26 02:39:59+00:00
- **Updated**: 2022-03-08 12:19:56+00:00
- **Authors**: Dai Quoc Nguyen, Tu Dinh Nguyen, Dinh Phung
- **Comment**: Accepted to The ACM Web Conference 2022 (WWW '22) (Poster and Demo
  Track)
- **Journal**: None
- **Summary**: We introduce a transformer-based GNN model, named UGformer, to learn graph representations. In particular, we present two UGformer variants, wherein the first variant (publicized in September 2019) is to leverage the transformer on a set of sampled neighbors for each input node, while the second (publicized in May 2021) is to leverage the transformer on all input nodes. Experimental results demonstrate that the first UGformer variant achieves state-of-the-art accuracies on benchmark datasets for graph classification in both inductive setting and unsupervised transductive setting; and the second UGformer variant obtains state-of-the-art accuracies for inductive text classification. The code is available at: \url{https://github.com/daiquocnguyen/Graph-Transformer}.



### Lightweight Image Super-Resolution with Information Multi-distillation Network
- **Arxiv ID**: http://arxiv.org/abs/1909.11856v1
- **DOI**: 10.1145/3343031.3351084
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1909.11856v1)
- **Published**: 2019-09-26 02:40:32+00:00
- **Updated**: 2019-09-26 02:40:32+00:00
- **Authors**: Zheng Hui, Xinbo Gao, Yunchu Yang, Xiumei Wang
- **Comment**: To be appear in ACM Multimedia 2019, https://github.com/Zheng222/IMDN
- **Journal**: None
- **Summary**: In recent years, single image super-resolution (SISR) methods using deep convolution neural network (CNN) have achieved impressive results. Thanks to the powerful representation capabilities of the deep networks, numerous previous ways can learn the complex non-linear mapping between low-resolution (LR) image patches and their high-resolution (HR) versions. However, excessive convolutions will limit the application of super-resolution technology in low computing power devices. Besides, super-resolution of any arbitrary scale factor is a critical issue in practical applications, which has not been well solved in the previous approaches. To address these issues, we propose a lightweight information multi-distillation network (IMDN) by constructing the cascaded information multi-distillation blocks (IMDB), which contains distillation and selective fusion parts. Specifically, the distillation module extracts hierarchical features step-by-step, and fusion module aggregates them according to the importance of candidate features, which is evaluated by the proposed contrast-aware channel attention mechanism. To process real images with any sizes, we develop an adaptive cropping strategy (ACS) to super-resolve block-wise image patches using the same well-trained model. Extensive experiments suggest that the proposed method performs favorably against the state-of-the-art SR algorithms in term of visual quality, memory footprint, and inference time. Code is available at \url{https://github.com/Zheng222/IMDN}.



### Convolutional Neural Networks with Dynamic Regularization
- **Arxiv ID**: http://arxiv.org/abs/1909.11862v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.11862v3)
- **Published**: 2019-09-26 03:06:49+00:00
- **Updated**: 2020-12-31 03:14:07+00:00
- **Authors**: Yi Wang, Zhen-Peng Bian, Junhui Hou, Lap-Pui Chau
- **Comment**: 7 pages. Accepted for Publication at IEEE TNNLS
- **Journal**: None
- **Summary**: Regularization is commonly used for alleviating overfitting in machine learning. For convolutional neural networks (CNNs), regularization methods, such as DropBlock and Shake-Shake, have illustrated the improvement in the generalization performance. However, these methods lack a self-adaptive ability throughout training. That is, the regularization strength is fixed to a predefined schedule, and manual adjustments are required to adapt to various network architectures. In this paper, we propose a dynamic regularization method for CNNs. Specifically, we model the regularization strength as a function of the training loss. According to the change of the training loss, our method can dynamically adjust the regularization strength in the training procedure, thereby balancing the underfitting and overfitting of CNNs. With dynamic regularization, a large-scale model is automatically regularized by the strong perturbation, and vice versa. Experimental results show that the proposed method can improve the generalization capability on off-the-shelf network architectures and outperform state-of-the-art regularization methods.



### A Hybrid Deep Learning Architecture for Leukemic B-lymphoblast Classification
- **Arxiv ID**: http://arxiv.org/abs/1909.11866v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.11866v1)
- **Published**: 2019-09-26 03:34:24+00:00
- **Updated**: 2019-09-26 03:34:24+00:00
- **Authors**: Sara Hosseinzadeh Kassani, Peyman Hosseinzadeh kassani, Michal J. Wesolowski, Kevin A. Schneider, Ralph Deters
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic detection of leukemic B-lymphoblast cancer in microscopic images is very challenging due to the complicated nature of histopathological structures. To tackle this issue, an automatic and robust diagnostic system is required for early detection and treatment. In this paper, an automated deep learning-based method is proposed to distinguish between immature leukemic blasts and normal cells. The proposed deep learning based hybrid method, which is enriched by different data augmentation techniques, is able to extract high-level features from input images. Results demonstrate that the proposed model yields better prediction than individual models for Leukemic B-lymphoblast classification with 96.17% overall accuracy, 95.17% sensitivity and 98.58% specificity. Fusing the features extracted from intermediate layers, our approach has the potential to improve the overall classification performance.



### Overcoming Data Limitation in Medical Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1909.11867v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.11867v1)
- **Published**: 2019-09-26 03:40:31+00:00
- **Updated**: 2019-09-26 03:40:31+00:00
- **Authors**: Binh D. Nguyen, Thanh-Toan Do, Binh X. Nguyen, Tuong Do, Erman Tjiputra, Quang D. Tran
- **Comment**: Accepted in MICCAI 2019
- **Journal**: None
- **Summary**: Traditional approaches for Visual Question Answering (VQA) require large amount of labeled data for training. Unfortunately, such large scale data is usually not available for medical domain. In this paper, we propose a novel medical VQA framework that overcomes the labeled data limitation. The proposed framework explores the use of the unsupervised Denoising Auto-Encoder (DAE) and the supervised Meta-Learning. The advantage of DAE is to leverage the large amount of unlabeled images while the advantage of Meta-Learning is to learn meta-weights that quickly adapt to VQA problem with limited labeled data. By leveraging the advantages of these techniques, it allows the proposed framework to be efficiently trained using a small labeled training set. The experimental results show that our proposed method significantly outperforms the state-of-the-art medical VQA.



### Classification of Histopathological Biopsy Images Using Ensemble of Deep Learning Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.11870v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.11870v1)
- **Published**: 2019-09-26 03:57:32+00:00
- **Updated**: 2019-09-26 03:57:32+00:00
- **Authors**: Sara Hosseinzadeh Kassani, Peyman Hosseinzadeh Kassani, Michal J. Wesolowski, Kevin A. Schneider, Ralph Deters
- **Comment**: None
- **Journal**: None
- **Summary**: Breast cancer is one of the leading causes of death across the world in women. Early diagnosis of this type of cancer is critical for treatment and patient care. Computer-aided detection (CAD) systems using convolutional neural networks (CNN) could assist in the classification of abnormalities. In this study, we proposed an ensemble deep learning-based approach for automatic binary classification of breast histology images. The proposed ensemble model adapts three pre-trained CNNs, namely VGG19, MobileNet, and DenseNet. The ensemble model is used for the feature representation and extraction steps. The extracted features are then fed into a multi-layer perceptron classifier to carry out the classification task. Various pre-processing and CNN tuning techniques such as stain-normalization, data augmentation, hyperparameter tuning, and fine-tuning are used to train the model. The proposed method is validated on four publicly available benchmark datasets, i.e., ICIAR, BreakHis, PatchCamelyon, and Bioimaging. The proposed multi-model ensemble method obtains better predictions than single classifiers and machine learning algorithms with accuracies of 98.13%, 95.00%, 94.64% and 83.10% for BreakHis, ICIAR, PatchCamelyon and Bioimaging datasets, respectively.



### Compact Trilinear Interaction for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1909.11874v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.11874v1)
- **Published**: 2019-09-26 04:02:38+00:00
- **Updated**: 2019-09-26 04:02:38+00:00
- **Authors**: Tuong Do, Thanh-Toan Do, Huy Tran, Erman Tjiputra, Quang D. Tran
- **Comment**: Accepted in ICCV 2019
- **Journal**: None
- **Summary**: In Visual Question Answering (VQA), answers have a great correlation with question meaning and visual contents. Thus, to selectively utilize image, question and answer information, we propose a novel trilinear interaction model which simultaneously learns high level associations between these three inputs. In addition, to overcome the interaction complexity, we introduce a multimodal tensor-based PARALIND decomposition which efficiently parameterizes trilinear interaction between the three inputs. Moreover, knowledge distillation is first time applied in Free-form Opened-ended VQA. It is not only for reducing the computational cost and required memory but also for transferring knowledge from trilinear interaction model to bilinear interaction model. The extensive experiments on benchmarking datasets TDIUC, VQA-2.0, and Visual7W show that the proposed compact trilinear interaction model achieves state-of-the-art results when using a single model on all three datasets.



### Resolving Marker Pose Ambiguity by Robust Rotation Averaging with Clique Constraints
- **Arxiv ID**: http://arxiv.org/abs/1909.11888v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.11888v1)
- **Published**: 2019-09-26 04:44:16+00:00
- **Updated**: 2019-09-26 04:44:16+00:00
- **Authors**: Shin-Fang Ch'ng, Naoya Sogi, Pulak Purkait, Tat-Jun Chin, Kazuhiro Fukui
- **Comment**: 7 pages, 4 figures, 4 tables
- **Journal**: None
- **Summary**: Planar markers are useful in robotics and computer vision for mapping and localisation. Given a detected marker in an image, a frequent task is to estimate the 6DOF pose of the marker relative to the camera, which is an instance of planar pose estimation (PPE). Although there are mature techniques, PPE suffers from a fundamental ambiguity problem, in that there can be more than one plausible pose solutions for a PPE instance. Especially when localisation of the marker corners is noisy, it is often difficult to disambiguate the pose solutions based on reprojection error alone. Previous methods choose between the possible solutions using a heuristic criteria, or simply ignore ambiguous markers.   We propose to resolve the ambiguities by examining the consistencies of a set of markers across multiple views. Our specific contributions include a novel rotation averaging formulation that incorporates long-range dependencies between possible marker orientation solutions that arise from PPE ambiguities. We analyse the combinatorial complexity of the problem, and develop a novel lifted algorithm to effectively resolve marker pose ambiguities, without discarding any marker observations. Results on real and synthetic data show that our method is able to handle highly ambiguous inputs, and provides more accurate and/or complete marker-based mapping and localisation.



### Joint-task Self-supervised Learning for Temporal Correspondence
- **Arxiv ID**: http://arxiv.org/abs/1909.11895v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.11895v1)
- **Published**: 2019-09-26 05:11:26+00:00
- **Updated**: 2019-09-26 05:11:26+00:00
- **Authors**: Xueting Li, Sifei Liu, Shalini De Mello, Xiaolong Wang, Jan Kautz, Ming-Hsuan Yang
- **Comment**: NeurIPS 2019
- **Journal**: None
- **Summary**: This paper proposes to learn reliable dense correspondence from videos in a self-supervised manner. Our learning process integrates two highly related tasks: tracking large image regions \emph{and} establishing fine-grained pixel-level associations between consecutive video frames. We exploit the synergy between both tasks through a shared inter-frame affinity matrix, which simultaneously models transitions between video frames at both the region- and pixel-levels. While region-level localization helps reduce ambiguities in fine-grained matching by narrowing down search regions; fine-grained matching provides bottom-up features to facilitate region-level localization. Our method outperforms the state-of-the-art self-supervised methods on a variety of visual correspondence tasks, including video-object and part-segmentation propagation, keypoint tracking, and object tracking. Our self-supervised method even surpasses the fully-supervised affinity feature representation obtained from a ResNet-18 pre-trained on the ImageNet.



### Deep Model Transferability from Attribution Maps
- **Arxiv ID**: http://arxiv.org/abs/1909.11902v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.11902v2)
- **Published**: 2019-09-26 05:36:38+00:00
- **Updated**: 2019-10-14 06:21:00+00:00
- **Authors**: Jie Song, Yixin Chen, Xinchao Wang, Chengchao Shen, Mingli Song
- **Comment**: Accepted by NeurIPS 2019
- **Journal**: None
- **Summary**: Exploring the transferability between heterogeneous tasks sheds light on their intrinsic interconnections, and consequently enables knowledge transfer from one task to another so as to reduce the training effort of the latter. In this paper, we propose an embarrassingly simple yet very efficacious approach to estimating the transferability of deep networks, especially those handling vision tasks. Unlike the seminal work of taskonomy that relies on a large number of annotations as supervision and is thus computationally cumbersome, the proposed approach requires no human annotations and imposes no constraints on the architectures of the networks. This is achieved, specifically, via projecting deep networks into a model space, wherein each network is treated as a point and the distances between two points are measured by deviations of their produced attribution maps. The proposed approach is several-magnitude times faster than taskonomy, and meanwhile preserves a task-wise topological structure highly similar to the one obtained by taskonomy. Code is available at https://github.com/zju-vipa/TransferbilityFromAttributionMaps.



### Segmentation of points of interest during fetal cardiac assesment in the first trimester from color Doppler ultrasound
- **Arxiv ID**: http://arxiv.org/abs/1909.11903v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.11903v1)
- **Published**: 2019-09-26 05:36:58+00:00
- **Updated**: 2019-09-26 05:36:58+00:00
- **Authors**: Ruxandra Stoean, Dominic Iliescu, Catalin Stoean
- **Comment**: None
- **Journal**: None
- **Summary**: The present paper puts forward an incipient study that uses a traditional segmentation method based on Zernike moments for extracting significant features from frames of fetal echocardiograms from first trimester color Doppler examinations. A distance based approach is then used on the obtained indicators to classify frames of three given categories that should be present in a normal heart condition. The computational tool shows promise in supporting the obstetrician in a rapid recognition of heart views during screening.



### Unsupervised Image Translation using Adversarial Networks for Improved Plant Disease Recognition
- **Arxiv ID**: http://arxiv.org/abs/1909.11915v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.11915v1)
- **Published**: 2019-09-26 06:01:42+00:00
- **Updated**: 2019-09-26 06:01:42+00:00
- **Authors**: Haseeb Nazki, Sook Yoon, Alvaro Fuentes, Dong Sun Park
- **Comment**: 20 pages, 11 figures, 3 tables, article under review
- **Journal**: None
- **Summary**: Acquisition of data in task-specific applications of machine learning like plant disease recognition is a costly endeavor owing to the requirements of professional human diligence and time constraints. In this paper, we present a simple pipeline that uses GANs in an unsupervised image translation environment to improve learning with respect to the data distribution in a plant disease dataset, reducing the partiality introduced by acute class imbalance and hence shifting the classification decision boundary towards better performance. The empirical analysis of our method is demonstrated on a limited dataset of 2789 tomato plant disease images, highly corrupted with an imbalance in the 9 disease categories. First, we extend the state of the art for the GAN-based image-to-image translation method by enhancing the perceptual quality of the generated images and preserving the semantics. We introduce AR-GAN, where in addition to the adversarial loss, our synthetic image generator optimizes on Activation Reconstruction loss (ARL) function that optimizes feature activations against the natural image. We present visually more compelling synthetic images in comparison to most prominent existing models and evaluate the performance of our GAN framework in terms of various datasets and metrics. Second, we evaluate the performance of a baseline convolutional neural network classifier for improved recognition using the resulting synthetic samples to augment our training set and compare it with the classical data augmentation scheme. We observe a significant improvement in classification accuracy (+5.2%) using generated synthetic samples as compared to (+0.8%) increase using classic augmentation in an equal class distribution environment.



### Hierarchical Neural Architecture Search via Operator Clustering
- **Arxiv ID**: http://arxiv.org/abs/1909.11926v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.11926v5)
- **Published**: 2019-09-26 06:26:58+00:00
- **Updated**: 2021-01-25 10:03:07+00:00
- **Authors**: Guilin Li, Xing Zhang, Zitong Wang, Matthias Tan, Jiashi Feng, Zhenguo Li, Tong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, the efficiency of automatic neural architecture design has been significantly improved by gradient-based search methods such as DARTS. However, recent literature has brought doubt to the generalization ability of DARTS, arguing that DARTS performs poorly when the search space is changed, i.e, when different set of candidate operators are used. Regularization techniques such as early stopping have been proposed to partially solve this problem. In this paper, we tackle this problem from a different perspective by identifying two contributing factors to the collapse of DARTS when the search space changes: (1) the correlation of similar operators incurs unfavorable competition among them and makes their relative importance score unreliable and (2) the optimization complexity gap between the proxy search stage and the final training. Based on these findings, we propose a new hierarchical search algorithm. With its operator clustering and optimization complexity match, the algorithm can consistently find high-performance architecture across various search spaces. For all the five variants of the popular cell-based search spaces, the proposed algorithm always obtains state-of-the-art architecture with best accuracy on the CIFAR-10, CIFAR-100 and ImageNet over other well-established DARTS-alike algorithms. Code is available at https://github.com/susan0199/StacNAS.



### Adaptive Class Weight based Dual Focal Loss for Improved Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.11932v3
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML, 68T05
- **Links**: [PDF](http://arxiv.org/pdf/1909.11932v3)
- **Published**: 2019-09-26 06:36:21+00:00
- **Updated**: 2020-11-26 05:20:15+00:00
- **Authors**: Md Sazzad Hossain, Andrew P Paplinski, John M Betts
- **Comment**: We, the authors, are withdrawing this preprint due to a number of
  errors pointed out by the reviewers. Based on the reviewers' feedback, the
  paper has gone through an extensive revision, which significantly differs
  from this preprint version by methodologically as well as experimentally. We
  acknowledge the reviewers for their scrutinized review which guided our study
  in the right direction
- **Journal**: None
- **Summary**: In this paper, we propose a Dual Focal Loss (DFL) function, as a replacement for the standard cross entropy (CE) function to achieve a better treatment of the unbalanced classes in a dataset. Our DFL method is an improvement on the recently reported Focal Loss (FL) cross-entropy function, which proposes a scaling method that puts more weight on the examples that are difficult to classify over those that are easy. However, the scaling parameter of FL is empirically set, which is problem-dependent. In addition, like other CE variants, FL only focuses on the loss of true classes. Therefore, no loss feedback is gained from the false classes. Although focusing only on true examples increases probability on true classes and correspondingly reduces probability on false classes due to the nature of the softmax function, it does not achieve the best convergence due to avoidance of the loss on false classes. Our DFL method improves on the simple FL in two ways. Firstly, it takes the idea of FL to focus more on difficult examples than the easy ones, but evaluates loss on both true and negative classes with equal importance. Secondly, the scaling parameter of DFL has been made learnable so that it can tune itself by backpropagation rather than being dependent on manual tuning. In this way, our proposed DFL method offers an auto-tunable loss function that can reduce the class imbalance effect as well as put more focus on both true difficult examples and negative easy examples.



### A Refined Equilibrium Generative Adversarial Network for Retinal Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.11936v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.11936v2)
- **Published**: 2019-09-26 06:50:03+00:00
- **Updated**: 2019-12-18 13:09:50+00:00
- **Authors**: Yukun Zhou, Zailiang Chen, Hailan Shen, Xianxian Zheng, Rongchang Zhao, Xuanchu Duan
- **Comment**: 12 pages, 8 figures, and 9 tables
- **Journal**: None
- **Summary**: Objective: Recognizing retinal vessel abnormity is vital to early diagnosis of ophthalmological diseases and cardiovascular events. However, segmentation results are highly influenced by elusive vessels, especially in low-contrast background and lesion region. In this work, we present an end-to-end synthetic neural network, containing a symmetric equilibrium generative adversarial network (SEGAN), multi-scale features refine blocks (MSFRB), and attention mechanism (AM) to enhance the performance on vessel segmentation. Method: The proposed network is granted powerful multi-scale representation capability to extract detail information. First, SEGAN constructs a symmetric adversarial architecture, which forces generator to produce more realistic images with local details. Second, MSFRB are devised to prevent high-resolution features from being obscured, thereby merging multi-scale features better. Finally, the AM is employed to encourage the network to concentrate on discriminative features. Results: On public dataset DRIVE, STARE, CHASEDB1, and HRF, we evaluate our network quantitatively and compare it with state-of-the-art works. The ablation experiment shows that SEGAN, MSFRB, and AM both contribute to the desirable performance. Conclusion: The proposed network outperforms the mature methods and effectively functions in elusive vessels segmentation, achieving highest scores in Sensitivity, G-Mean, Precision, and F1-Score while maintaining the top level in other metrics. Significance: The appreciable performance and computational efficiency offer great potential in clinical retinal vessel segmentation application. Meanwhile, the network could be utilized to extract detail information in other biomedical issues



### Multi-grained Attention Networks for Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1909.11937v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.11937v2)
- **Published**: 2019-09-26 06:54:00+00:00
- **Updated**: 2019-09-29 09:09:30+00:00
- **Authors**: Huapeng Wu, Zhengxia Zou, Jie Gui, Wen-Jun Zeng, Jieping Ye, Jun Zhang, Hongyi Liu, Zhihui Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (CNN) have drawn great attention in image super-resolution (SR). Recently, visual attention mechanism, which exploits both of the feature importance and contextual cues, has been introduced to image SR and proves to be effective to improve CNN-based SR performance. In this paper, we make a thorough investigation on the attention mechanisms in a SR model and shed light on how simple and effective improvements on these ideas improve the state-of-the-arts. We further propose a unified approach called "multi-grained attention networks (MGAN)" which fully exploits the advantages of multi-scale and attention mechanisms in SR tasks. In our method, the importance of each neuron is computed according to its surrounding regions in a multi-grained fashion and then is used to adaptively re-scale the feature responses. More importantly, the "channel attention" and "spatial attention" strategies in previous methods can be essentially considered as two special cases of our method. We also introduce multi-scale dense connections to extract the image features at multiple scales and capture the features of different layers through dense skip connections. Ablation studies on benchmark datasets demonstrate the effectiveness of our method. In comparison with other state-of-the-art SR methods, our method shows the superiority in terms of both accuracy and model size.



### Multiple Object Forecasting: Predicting Future Object Locations in Diverse Environments
- **Arxiv ID**: http://arxiv.org/abs/1909.11944v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.11944v2)
- **Published**: 2019-09-26 07:11:50+00:00
- **Updated**: 2020-01-07 12:19:53+00:00
- **Authors**: Olly Styles, Tanaya Guha, Victor Sanchez
- **Comment**: WACV 2020. Code & dataset:
  https://github.com/olly-styles/Multiple-Object-Forecasting
- **Journal**: None
- **Summary**: This paper introduces the problem of multiple object forecasting (MOF), in which the goal is to predict future bounding boxes of tracked objects. In contrast to existing works on object trajectory forecasting which primarily consider the problem from a birds-eye perspective, we formulate the problem from an object-level perspective and call for the prediction of full object bounding boxes, rather than trajectories alone. Towards solving this task, we introduce the Citywalks dataset, which consists of over 200k high-resolution video frames. Citywalks comprises of footage recorded in 21 cities from 10 European countries in a variety of weather conditions and over 3.5k unique pedestrian trajectories. For evaluation, we adapt existing trajectory forecasting methods for MOF and confirm cross-dataset generalizability on the MOT-17 dataset without fine-tuning. Finally, we present STED, a novel encoder-decoder architecture for MOF. STED combines visual and temporal features to model both object-motion and ego-motion, and outperforms existing approaches for MOF. Code & dataset link: https://github.com/olly-styles/Multiple-Object-Forecasting



### FoodAI: Food Image Recognition via Deep Learning for Smart Food Logging
- **Arxiv ID**: http://arxiv.org/abs/1909.11946v1
- **DOI**: 10.1145/3292500.3330734
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1909.11946v1)
- **Published**: 2019-09-26 07:15:46+00:00
- **Updated**: 2019-09-26 07:15:46+00:00
- **Authors**: Doyen Sahoo, Wang Hao, Shu Ke, Wu Xiongwei, Hung Le, Palakorn Achananuparp, Ee-Peng Lim, Steven C. H. Hoi
- **Comment**: Published at KDD 2019 (Applied Data Science track). Demo is
  accessible at https://foodai.org/
- **Journal**: None
- **Summary**: An important aspect of health monitoring is effective logging of food consumption. This can help management of diet-related diseases like obesity, diabetes, and even cardiovascular diseases. Moreover, food logging can help fitness enthusiasts, and people who wanting to achieve a target weight. However, food-logging is cumbersome, and requires not only taking additional effort to note down the food item consumed regularly, but also sufficient knowledge of the food item consumed (which is difficult due to the availability of a wide variety of cuisines). With increasing reliance on smart devices, we exploit the convenience offered through the use of smart phones and propose a smart-food logging system: FoodAI, which offers state-of-the-art deep-learning based image recognition capabilities. FoodAI has been developed in Singapore and is particularly focused on food items commonly consumed in Singapore. FoodAI models were trained on a corpus of 400,000 food images from 756 different classes. In this paper we present extensive analysis and insights into the development of this system. FoodAI has been deployed as an API service and is one of the components powering Healthy 365, a mobile app developed by Singapore's Heath Promotion Board. We have over 100 registered organizations (universities, companies, start-ups) subscribing to this service and actively receive several API requests a day. FoodAI has made food logging convenient, aiding smart consumption and a healthy lifestyle.



### Multi-scale Dynamic Feature Encoding Network for Image Demoireing
- **Arxiv ID**: http://arxiv.org/abs/1909.11947v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1909.11947v1)
- **Published**: 2019-09-26 07:23:43+00:00
- **Updated**: 2019-09-26 07:23:43+00:00
- **Authors**: Xi Cheng, Zhenyong Fu, Jian Yang
- **Comment**: Accepted in Advances in Image Manipulation workshop and challenges at
  ICCV 2019
- **Journal**: None
- **Summary**: The prevalence of digital sensors, such as digital cameras and mobile phones, simplifies the acquisition of photos. Digital sensors, however, suffer from producing Moire when photographing objects having complex textures, which deteriorates the quality of photos. Moire spreads across various frequency bands of images and is a dynamic texture with varying colors and shapes, which pose two main challenges in demoireing---an important task in image restoration. In this paper, towards addressing the first challenge, we design a multi-scale network to process images at different spatial resolutions, obtaining features in different frequency bands, and thus our method can jointly remove moire in different frequency bands. Towards solving the second challenge, we propose a dynamic feature encoding module (DFE), embedded in each scale, for dynamic texture. Moire pattern can be eliminated more effectively via DFE.Our proposed method, termed Multi-scale convolutional network with Dynamic feature encoding for image DeMoireing (MDDM), can outperform the state of the arts in fidelity as well as perceptual on benchmarks.



### Hyperspectral Image Classification With Context-Aware Dynamic Graph Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/1909.11953v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.11953v1)
- **Published**: 2019-09-26 07:37:37+00:00
- **Updated**: 2019-09-26 07:37:37+00:00
- **Authors**: Sheng Wan, Chen Gong, Ping Zhong, Shirui Pan, Guangyu Li, Jian Yang
- **Comment**: None
- **Journal**: None
- **Summary**: In hyperspectral image (HSI) classification, spatial context has demonstrated its significance in achieving promising performance. However, conventional spatial context-based methods simply assume that spatially neighboring pixels should correspond to the same land-cover class, so they often fail to correctly discover the contextual relations among pixels in complex situations, and thus leading to imperfect classification results on some irregular or inhomogeneous regions such as class boundaries. To address this deficiency, we develop a new HSI classification method based on the recently proposed Graph Convolutional Network (GCN), as it can flexibly encode the relations among arbitrarily structured non-Euclidean data. Different from traditional GCN, there are two novel strategies adopted by our method to further exploit the contextual relations for accurate HSI classification. First, since the receptive field of traditional GCN is often limited to fairly small neighborhood, we proposed to capture long range contextual relations in HSI by performing successive graph convolutions on a learned region-induced graph which is transformed from the original 2D image grids. Second, we refine the graph edge weight and the connective relationships among image regions by learning the improved adjacency matrix and the 'edge filter', so that the graph can be gradually refined to adapt to the representations generated by each graph convolutional layer. Such updated graph will in turn result in accurate region representations, and vice versa. The experiments carried out on three real-world benchmark datasets demonstrate that the proposed method yields significant improvement in the classification performance when compared with some state-of-the-art approaches.



### Dual-Stream Pyramid Registration Network
- **Arxiv ID**: http://arxiv.org/abs/1909.11966v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.11966v2)
- **Published**: 2019-09-26 08:17:01+00:00
- **Updated**: 2023-04-01 11:28:05+00:00
- **Authors**: Miao Kang, Xiaojun Hu, Weilin Huang, Matthew R. Scott, Mauricio Reyes
- **Comment**: Published in Medical Image Analysis, 2022
- **Journal**: None
- **Summary**: We propose a Dual-Stream Pyramid Registration Network (referred as Dual-PRNet) for unsupervised 3D medical image registration. Unlike recent CNN-based registration approaches, such as VoxelMorph, which explores a single-stream encoder-decoder network to compute a registration fields from a pair of 3D volumes, we design a two-stream architecture able to compute multi-scale registration fields from convolutional feature pyramids. Our contributions are two-fold: (i) we design a two-stream 3D encoder-decoder network which computes two convolutional feature pyramids separately for a pair of input volumes, resulting in strong deep representations that are meaningful for deformation estimation; (ii) we propose a pyramid registration module able to predict multi-scale registration fields directly from the decoding feature pyramids. This allows it to refine the registration fields gradually in a coarse-to-fine manner via sequential warping, and enable the model with the capability for handling significant deformations between two volumes, such as large displacements in spatial domain or slice space. The proposed Dual-PRNet is evaluated on two standard benchmarks for brain MRI registration, where it outperforms the state-of-the-art approaches by a large margin, e.g., having improvements over recent VoxelMorph [2] with 0.683->0.778 on the LPBA40, and 0.511->0.631 on the Mindboggle101, in term of average Dice score. Code is available at: https://github.com/kangmiao15/Dual-Stream-PRNet-Plus.



### Video-Based Convolutional Attention for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1910.04856v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.04856v1)
- **Published**: 2019-09-26 08:23:43+00:00
- **Updated**: 2019-09-26 08:23:43+00:00
- **Authors**: Marco Zamprogno, Marco Passon, Niki Martinel, Giuseppe Serra, Giuseppe Lancioni, Christian Micheloni, Carlo Tasso, Gian Luca Foresti
- **Comment**: 11 pages, 2 figures. Accepted by ICIAP2019, 20th International
  Conference on IMAGE ANALYSIS AND PROCESSING, Trento, Italy, 9-13 September,
  2019
- **Journal**: None
- **Summary**: In this paper we consider the problem of video-based person re-identification, which is the task of associating videos of the same person captured by different and non-overlapping cameras. We propose a Siamese framework in which video frames of the person to re-identify and of the candidate one are processed by two identical networks which produce a similarity score. We introduce an attention mechanisms to capture the relevant information both at frame level (spatial information) and at video level (temporal information given by the importance of a specific frame within the sequence). One of the novelties of our approach is given by a joint concurrent processing of both frame and video levels, providing in such a way a very simple architecture. Despite this fact, our approach achieves better performance than the state-of-the-art on the challenging iLIDS-VID dataset.



### Cut-and-Paste Dataset Generation for Balancing Domain Gaps in Object Instance Detection
- **Arxiv ID**: http://arxiv.org/abs/1909.11972v2
- **DOI**: 10.1109/ACCESS.2021.3051964
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.11972v2)
- **Published**: 2019-09-26 08:30:36+00:00
- **Updated**: 2021-01-27 03:23:28+00:00
- **Authors**: Woo-han Yun, Taewoo Kim, Jaeyeon Lee, Jaehong Kim, Junmo Kim
- **Comment**: 12 pages, 7 figures
- **Journal**: None
- **Summary**: Training an object instance detector where only a few training object images are available is a challenging task. One solution is a cut-and-paste method that generates a training dataset by cutting object areas out of training images and pasting them onto other background images. A detector trained on a dataset generated with a cut-and-paste method suffers from the conventional domain shift problem, which stems from a discrepancy between the source domain (generated training dataset) and the target domain (real test dataset). Though state-of-the-art domain adaptation methods are able to reduce this gap, it is limited because they do not consider the difference of domain gaps of foreground and background. In this study, we present that the conventional domain gap can be divided into two sub-domain gaps for foreground and background. Then, we show that the original cut-and-paste approach suffers from a new domain gap problem, an unbalanced domain gaps, because it has two separate source domains for foreground and background, unlike the conventional domain shift problem. Then, we introduce an advanced cut-and-paste method to balance the unbalanced domain gaps by diversifying the foreground with GAN (generative adversarial network)-generated seed images and simplifying the background using image processing techniques. Experimental results show that our method is effective for balancing domain gaps and improving the accuracy of object instance detection in a cluttered indoor environment using only a few seed images. Furthermore, we show that balancing domain gaps can improve the detection accuracy of state-of-the-art domain adaptation methods.



### Learning Energy-based Spatial-Temporal Generative ConvNets for Dynamic Patterns
- **Arxiv ID**: http://arxiv.org/abs/1909.11975v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.11975v1)
- **Published**: 2019-09-26 08:36:15+00:00
- **Updated**: 2019-09-26 08:36:15+00:00
- **Authors**: Jianwen Xie, Song-Chun Zhu, Ying Nian Wu
- **Comment**: None
- **Journal**: IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
  2019
- **Summary**: Video sequences contain rich dynamic patterns, such as dynamic texture patterns that exhibit stationarity in the temporal domain, and action patterns that are non-stationary in either spatial or temporal domain. We show that an energy-based spatial-temporal generative ConvNet can be used to model and synthesize dynamic patterns. The model defines a probability distribution on the video sequence, and the log probability is defined by a spatial-temporal ConvNet that consists of multiple layers of spatial-temporal filters to capture spatial-temporal patterns of different scales. The model can be learned from the training video sequences by an "analysis by synthesis" learning algorithm that iterates the following two steps. Step 1 synthesizes video sequences from the currently learned model. Step 2 then updates the model parameters based on the difference between the synthesized video sequences and the observed training sequences. We show that the learning algorithm can synthesize realistic dynamic patterns. We also show that it is possible to learn the model from incomplete training sequences with either occluded pixels or missing frames, so that model learning and pattern completion can be accomplished simultaneously.



### Subjective and Objective De-raining Quality Assessment Towards Authentic Rain Image
- **Arxiv ID**: http://arxiv.org/abs/1909.11983v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.11983v3)
- **Published**: 2019-09-26 08:57:34+00:00
- **Updated**: 2019-10-06 03:55:12+00:00
- **Authors**: Qingbo Wu, Lei Wang, King N. Ngan, Hongliang Li, Fanman Meng, Linfeng Xu
- **Comment**: In this revision, we add the comparison with our previous exploration
  towards the de-raining quality assessment in Ref. [16]. Some typos in Tables
  III and IV are corrected, where the missed minus signs are added back for
  some OU metrics
- **Journal**: None
- **Summary**: Images acquired by outdoor vision systems easily suffer poor visibility and annoying interference due to the rainy weather, which brings great challenge for accurately understanding and describing the visual contents. Recent researches have devoted great efforts on the task of rain removal for improving the image visibility. However, there is very few exploration about the quality assessment of de-rained image, even it is crucial for accurately measuring the performance of various de-raining algorithms. In this paper, we first create a de-raining quality assessment (DQA) database that collects 206 authentic rain images and their de-rained versions produced by 6 representative single image rain removal algorithms. Then, a subjective study is conducted on our DQA database, which collects the subject-rated scores of all de-rained images. To quantitatively measure the quality of de-rained image with non-uniform artifacts, we propose a bi-directional feature embedding network (B-FEN) which integrates the features of global perception and local difference together. Experiments confirm that the proposed method significantly outperforms many existing universal blind image quality assessment models. To help the research towards perceptually preferred de-raining algorithm, we will publicly release our DQA database and B-FEN source code on https://github.com/wqb-uestc.



### Admiring the Great Mountain: A Celebration Special Issue in Honor of Stephen Grossbergs 80th Birthday
- **Arxiv ID**: http://arxiv.org/abs/1910.13351v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.13351v1)
- **Published**: 2019-09-26 09:17:01+00:00
- **Updated**: 2019-09-26 09:17:01+00:00
- **Authors**: Donald C. Wunsch
- **Comment**: Editorial for Special Issue of Neural Networks in honor of
  Grossberg's 80th birthday
- **Journal**: None
- **Summary**: This editorial summarizes selected key contributions of Prof. Stephen Grossberg and describes the papers in this 80th birthday special issue in his honor. His productivity, creativity, and vision would each be enough to mark a scientist of the first caliber. In combination, they have resulted in contributions that have changed the entire discipline of neural networks. Grossberg has been tremendously influential in engineering, dynamical systems, and artificial intelligence as well. Indeed, he has been one of the most important mentors and role models in my career, and has done so with extraordinary generosity and encouragement. All authors in this special issue have taken great pleasure in hereby commemorating his extraordinary career and contributions.



### The Stroke Correspondence Problem, Revisited
- **Arxiv ID**: http://arxiv.org/abs/1909.11995v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.11995v3)
- **Published**: 2019-09-26 09:26:12+00:00
- **Updated**: 2021-03-06 22:25:53+00:00
- **Authors**: Dominik Klein
- **Comment**: 16 pages, 2 figures
- **Journal**: None
- **Summary**: We revisit the stroke correspondence problem [13,14]. We optimize this algorithm by 1) evaluating suitable preprocessing (normalization) methods 2) extending the algorithm with an additional distance measure to handle Hiragana, Katakana and Kanji characters with a low number of strokes and c) simplify the stroke linking algorithms. Our contributions are implemented in the free, open-source library ctegaki and in the demo-tools jTegaki and Kanjicanvas.



### CoPhy: Counterfactual Learning of Physical Dynamics
- **Arxiv ID**: http://arxiv.org/abs/1909.12000v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12000v2)
- **Published**: 2019-09-26 09:34:48+00:00
- **Updated**: 2020-04-07 12:48:26+00:00
- **Authors**: Fabien Baradel, Natalia Neverova, Julien Mille, Greg Mori, Christian Wolf
- **Comment**: ICLR 2020 -Spotlight presentation
- **Journal**: None
- **Summary**: Understanding causes and effects in mechanical systems is an essential component of reasoning in the physical world. This work poses a new problem of counterfactual learning of object mechanics from visual input. We develop the CoPhy benchmark to assess the capacity of the state-of-the-art models for causal physical reasoning in a synthetic 3D environment and propose a model for learning the physical dynamics in a counterfactual setting. Having observed a mechanical experiment that involves, for example, a falling tower of blocks, a set of bouncing balls or colliding objects, we learn to predict how its outcome is affected by an arbitrary intervention on its initial conditions, such as displacing one of the objects in the scene. The alternative future is predicted given the altered past and a latent representation of the confounders learned by the model in an end-to-end fashion with no supervision. We compare against feedforward video prediction baselines and show how observing alternative experiences allows the network to capture latent physical properties of the environment, which results in significantly more accurate predictions at the level of super human performance.



### Convex Relaxations for Consensus and Non-Minimal Problems in 3D Vision
- **Arxiv ID**: http://arxiv.org/abs/1909.12034v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12034v1)
- **Published**: 2019-09-26 11:32:02+00:00
- **Updated**: 2019-09-26 11:32:02+00:00
- **Authors**: Thomas Probst, Danda Pani Paudel, Ajad Chhatkuli, Luc Van Gool
- **Comment**: Accepted to ICCV'19
- **Journal**: None
- **Summary**: In this paper, we formulate a generic non-minimal solver using the existing tools of Polynomials Optimization Problems (POP) from computational algebraic geometry. The proposed method exploits the well known Shor's or Lasserre's relaxations, whose theoretical aspects are also discussed. Notably, we further exploit the POP formulation of non-minimal solver also for the generic consensus maximization problems in 3D vision. Our framework is simple and straightforward to implement, which is also supported by three diverse applications in 3D vision, namely rigid body transformation estimation, Non-Rigid Structure-from-Motion (NRSfM), and camera autocalibration. In all three cases, both non-minimal and consensus maximization are tested, which are also compared against the state-of-the-art methods. Our results are competitive to the compared methods, and are also coherent with our theoretical analysis. The main contribution of this paper is the claim that a good approximate solution for many polynomial problems involved in 3D vision can be obtained using the existing theory of numerical computational algebra. This claim leads us to reason about why many relaxed methods in 3D vision behave so well? And also allows us to offer a generic relaxed solver in a rather straightforward way. We further show that the convex relaxation of these polynomials can easily be used for maximizing consensus in a deterministic manner. We support our claim using several experiments for aforementioned three diverse problems in 3D vision.



### Learned Point Cloud Geometry Compression
- **Arxiv ID**: http://arxiv.org/abs/1909.12037v1
- **DOI**: 10.1109/TCSVT.2021.3051377
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.12037v1)
- **Published**: 2019-09-26 11:40:50+00:00
- **Updated**: 2019-09-26 11:40:50+00:00
- **Authors**: Jianqiang Wang, Hao Zhu, Zhan Ma, Tong Chen, Haojie Liu, Qiu Shen
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: This paper presents a novel end-to-end Learned Point Cloud Geometry Compression (a.k.a., Learned-PCGC) framework, to efficiently compress the point cloud geometry (PCG) using deep neural networks (DNN) based variational autoencoders (VAE). In our approach, PCG is first voxelized, scaled and partitioned into non-overlapped 3D cubes, which is then fed into stacked 3D convolutions for compact latent feature and hyperprior generation. Hyperpriors are used to improve the conditional probability modeling of latent features. A weighted binary cross-entropy (WBCE) loss is applied in training while an adaptive thresholding is used in inference to remove unnecessary voxels and reduce the distortion. Objectively, our method exceeds the geometry-based point cloud compression (G-PCC) algorithm standardized by well-known Moving Picture Experts Group (MPEG) with a significant performance margin, e.g., at least 60% BD-Rate (Bjontegaard Delta Rate) gains, using common test datasets. Subjectively, our method has presented better visual quality with smoother surface reconstruction and appealing details, in comparison to all existing MPEG standard compliant PCC methods. Our method requires about 2.5MB parameters in total, which is a fairly small size for practical implementation, even on embedded platform. Additional ablation studies analyze a variety of aspects (e.g., cube size, kernels, etc) to explore the application potentials of our learned-PCGC.



### Function Follows Form: Regression from Complete Thoracic Computed Tomography Scans
- **Arxiv ID**: http://arxiv.org/abs/1909.12047v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.12047v2)
- **Published**: 2019-09-26 12:27:52+00:00
- **Updated**: 2019-09-27 07:13:36+00:00
- **Authors**: Max Argus, Cornelia Schaefer-Prokop, David A. Lynch, Bram van Ginneken
- **Comment**: None
- **Journal**: None
- **Summary**: Chronic Obstructive Pulmonary Disease (COPD) is a leading cause of morbidity and mortality. While COPD diagnosis is based on lung function tests, early stages and progression of different aspects of the disease can be visible and quantitatively assessed on computed tomography (CT) scans. Many studies have been published that quantify imaging biomarkers related to COPD. In this paper we present a convolutional neural network that directly computes visual emphysema scores and predicts the outcome of lung function tests for 195 CT scans from the COPDGene study. Contrary to previous work, the proposed method does not encode any specific prior knowledge about what to quantify, but it is trained end-to-end with a set of 1424 CT scans for which the output parameters were available. The network provided state-of-the-art results for these tasks: Visual emphysema scores are comparable to those assessed by trained human observers; COPD diagnosis from estimated lung function reaches an area under the ROC curve of 0.94, outperforming prior art. The method is easily generalizable to other situations where information from whole scans needs to be summarized in single quantities.



### In-field grape berries counting for yield estimation using dilated CNNs
- **Arxiv ID**: http://arxiv.org/abs/1909.12083v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.12083v1)
- **Published**: 2019-09-26 13:28:53+00:00
- **Updated**: 2019-09-26 13:28:53+00:00
- **Authors**: L. Coviello, M. Cristoforetti, G. Jurman, C. Furlanello
- **Comment**: None
- **Journal**: None
- **Summary**: Digital technologies ignited a revolution in the agrifood domain known as precision agriculture: a main question for enabling precision agriculture at scale is if accurate product quality control can be made available at minimal cost, leveraging existing technologies and agronomists' skills. As a contribution along this direction we demonstrate a tool for accurate fruit yield estimation from smartphone cameras, by adapting Deep Learning algorithms originally developed for crowd counting.



### Two-stage Image Classification Supervised by a Single Teacher Single Student Model
- **Arxiv ID**: http://arxiv.org/abs/1909.12111v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.12111v1)
- **Published**: 2019-09-26 13:59:34+00:00
- **Updated**: 2019-09-26 13:59:34+00:00
- **Authors**: Jianhang Zhou, Shaoning Zeng, Bob Zhang
- **Comment**: Accepted by 30th British Machine Vision Conference (BMVC2019)
- **Journal**: None
- **Summary**: The two-stage strategy has been widely used in image classification. However, these methods barely take the classification criteria of the first stage into consideration in the second prediction stage. In this paper, we propose a novel two-stage representation method (TSR), and convert it to a Single-Teacher Single-Student (STSS) problem in our two-stage image classification framework. We seek the nearest neighbours of the test sample to choose candidate target classes. Meanwhile, the first stage classifier is formulated as the teacher, which holds the classification scores. The samples of the candidate classes are utilized to learn a student classifier based on L2-minimization in the second stage. The student will be supervised by the teacher classifier, which approves the student only if it obtains a higher score. In actuality, the proposed framework generates a stronger classifier by staging two weaker classifiers in a novel way. The experiments conducted on several face and object databases show that our proposed framework is effective and outperforms multiple popular classification methods.



### Balanced Binary Neural Networks with Gated Residual
- **Arxiv ID**: http://arxiv.org/abs/1909.12117v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12117v2)
- **Published**: 2019-09-26 14:03:10+00:00
- **Updated**: 2020-02-11 10:51:56+00:00
- **Authors**: Mingzhu Shen, Xianglong Liu, Ruihao Gong, Kai Han
- **Comment**: Accepted by ICASSP2020
- **Journal**: None
- **Summary**: Binary neural networks have attracted numerous attention in recent years. However, mainly due to the information loss stemming from the biased binarization, how to preserve the accuracy of networks still remains a critical issue. In this paper, we attempt to maintain the information propagated in the forward process and propose a Balanced Binary Neural Networks with Gated Residual (BBG for short). First, a weight balanced binarization is introduced to maximize information entropy of binary weights, and thus the informative binary weights can capture more information contained in the activations. Second, for binary activations, a gated residual is further appended to compensate their information loss during the forward process, with a slight overhead. Both techniques can be wrapped as a generic network module that supports various network architectures for different tasks including classification and detection. We evaluate our BBG on image classification tasks over CIFAR-10/100 and ImageNet and on detection task over Pascal VOC. The experimental results show that BBG-Net performs remarkably well across various network architectures such as VGG, ResNet and SSD with the superior performance over state-of-the-art methods in terms of memory consumption, inference speed and accuracy.



### DISCOMAN: Dataset of Indoor SCenes for Odometry, Mapping And Navigation
- **Arxiv ID**: http://arxiv.org/abs/1909.12146v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12146v1)
- **Published**: 2019-09-26 14:33:31+00:00
- **Updated**: 2019-09-26 14:33:31+00:00
- **Authors**: Pavel Kirsanov, Airat Gaskarov, Filipp Konokhov, Konstantin Sofiiuk, Anna Vorontsova, Igor Slinko, Dmitry Zhukov, Sergey Bykov, Olga Barinova, Anton Konushin
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: We present a novel dataset for training and benchmarking semantic SLAM methods. The dataset consists of 200 long sequences, each one containing 3000-5000 data frames. We generate the sequences using realistic home layouts. For that we sample trajectories that simulate motions of a simple home robot, and then render the frames along the trajectories. Each data frame contains a) RGB images generated using physically-based rendering, b) simulated depth measurements, c) simulated IMU readings and d) ground truth occupancy grid of a house. Our dataset serves a wider range of purposes compared to existing datasets and is the first large-scale benchmark focused on the mapping component of SLAM. The dataset is split into train/validation/test parts sampled from different sets of virtual houses. We present benchmarking results forboth classical geometry-based and recent learning-based SLAM algorithms, a baseline mapping method, semantic segmentation and panoptic segmentation.



### Fast and Effective Adaptation of Facial Action Unit Detection Deep Model
- **Arxiv ID**: http://arxiv.org/abs/1909.12158v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12158v2)
- **Published**: 2019-09-26 14:42:27+00:00
- **Updated**: 2019-11-27 14:13:31+00:00
- **Authors**: Mihee Lee, Ognjen Rudovic, Vladimir Pavlovic, Maja Pantic
- **Comment**: Presented at 2019 IJCAI Affective Computing Workshop
- **Journal**: None
- **Summary**: Detecting facial action units (AU) is one of the fundamental steps in automatic recognition of facial expression of emotions and cognitive states. Though there have been a variety of approaches proposed for this task, most of these models are trained only for the specific target AUs, and as such they fail to easily adapt to the task of recognition of new AUs (i.e., those not initially used to train the target models). In this paper, we propose a deep learning approach for facial AU detection that can easily and in a fast manner adapt to a new AU or target subject by leveraging only a few labeled samples from the new task (either an AU or subject). To this end, we propose a modeling approach based on the notion of the model-agnostic meta-learning, originally proposed for the general image recognition/detection tasks (e.g., the character recognition from the Omniglot dataset). Specifically, each subject and/or AU is treated as a new learning task and the model learns to adapt based on the knowledge of the previous tasks (the AUs and subjects used to pre-train the target models). Thus, given a new subject or AU, this meta-knowledge (that is shared among training and test tasks) is used to adapt the model to the new task using the notion of deep learning and model-agnostic meta-learning. We show on two benchmark datasets (BP4D and DISFA) for facial AU detection that the proposed approach can be easily adapted to new tasks (AUs/subjects). Using only a few labeled examples from these tasks, the model achieves large improvements over the baselines (i.e., non-adapted models).



### Towards neural networks that provably know when they don't know
- **Arxiv ID**: http://arxiv.org/abs/1909.12180v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.12180v2)
- **Published**: 2019-09-26 15:20:08+00:00
- **Updated**: 2020-02-21 09:27:11+00:00
- **Authors**: Alexander Meinke, Matthias Hein
- **Comment**: None
- **Journal**: None
- **Summary**: It has recently been shown that ReLU networks produce arbitrarily over-confident predictions far away from the training data. Thus, ReLU networks do not know when they don't know. However, this is a highly important property in safety critical applications. In the context of out-of-distribution detection (OOD) there have been a number of proposals to mitigate this problem but none of them are able to make any mathematical guarantees. In this paper we propose a new approach to OOD which overcomes both problems. Our approach can be used with ReLU networks and provides provably low confidence predictions far away from the training data as well as the first certificates for low confidence predictions in a neighborhood of an out-distribution point. In the experiments we show that state-of-the-art methods fail in this worst-case setting whereas our model can guarantee its performance while retaining state-of-the-art OOD performance.



### Deep Video Deblurring: The Devil is in the Details
- **Arxiv ID**: http://arxiv.org/abs/1909.12196v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.12196v1)
- **Published**: 2019-09-26 15:35:29+00:00
- **Updated**: 2019-09-26 15:35:29+00:00
- **Authors**: Jochen Gast, Stefan Roth
- **Comment**: To appear at ICCVW 2019
- **Journal**: None
- **Summary**: Video deblurring for hand-held cameras is a challenging task, since the underlying blur is caused by both camera shake and object motion. State-of-the-art deep networks exploit temporal information from neighboring frames, either by means of spatio-temporal transformers or by recurrent architectures. In contrast to these involved models, we found that a simple baseline CNN can perform astonishingly well when particular care is taken w.r.t. the details of model and training procedure. To that end, we conduct a comprehensive study regarding these crucial details, uncovering extreme differences in quantitative and qualitative performance. Exploiting these details allows us to boost the architecture and training procedure of a simple baseline CNN by a staggering 3.15dB, such that it becomes highly competitive w.r.t. cutting-edge networks. This raises the question whether the reported accuracy difference between models is always due to technical contributions or also subject to such orthogonal, but crucial details.



### Adaptive Binary-Ternary Quantization
- **Arxiv ID**: http://arxiv.org/abs/1909.12205v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML, 97R40
- **Links**: [PDF](http://arxiv.org/pdf/1909.12205v3)
- **Published**: 2019-09-26 15:49:08+00:00
- **Updated**: 2021-09-13 18:28:56+00:00
- **Authors**: Ryan Razani, Grégoire Morin, Vahid Partovi Nia, Eyyüb Sari
- **Comment**: None
- **Journal**: CVPR 2021 BiVision Workshop
- **Summary**: Neural network models are resource hungry. It is difficult to deploy such deep networks on devices with limited resources, like smart wearables, cellphones, drones, and autonomous vehicles. Low bit quantization such as binary and ternary quantization is a common approach to alleviate this resource requirements. Ternary quantization provides a more flexible model and outperforms binary quantization in terms of accuracy, however doubles the memory footprint and increases the computational cost. Contrary to these approaches, mixed quantized models allow a trade-off between accuracy and memory footprint. In such models, quantization depth is often chosen manually, or is tuned using a separate optimization routine. The latter requires training a quantized network multiple times. Here, we propose an adaptive combination of binary and ternary quantization, namely Smart Quantization (SQ), in which the quantization depth is modified directly via a regularization function, so that the model is trained only once. Our experimental results show that the proposed method adapts quantization depth successfully while keeping the model accuracy high on MNIST and CIFAR10 benchmarks.



### Implicit Semantic Data Augmentation for Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.12220v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.12220v5)
- **Published**: 2019-09-26 16:17:45+00:00
- **Updated**: 2020-04-25 03:13:03+00:00
- **Authors**: Yulin Wang, Xuran Pan, Shiji Song, Hong Zhang, Cheng Wu, Gao Huang
- **Comment**: Accepted by NeurIPS 2019
- **Journal**: None
- **Summary**: In this paper, we propose a novel implicit semantic data augmentation (ISDA) approach to complement traditional augmentation techniques like flipping, translation or rotation. Our work is motivated by the intriguing property that deep networks are surprisingly good at linearizing features, such that certain directions in the deep feature space correspond to meaningful semantic transformations, e.g., adding sunglasses or changing backgrounds. As a consequence, translating training samples along many semantic directions in the feature space can effectively augment the dataset to improve generalization. To implement this idea effectively and efficiently, we first perform an online estimate of the covariance matrix of deep features for each class, which captures the intra-class semantic variations. Then random vectors are drawn from a zero-mean normal distribution with the estimated covariance to augment the training data in that class. Importantly, instead of augmenting the samples explicitly, we can directly minimize an upper bound of the expected cross-entropy (CE) loss on the augmented training set, leading to a highly efficient algorithm. In fact, we show that the proposed ISDA amounts to minimizing a novel robust CE loss, which adds negligible extra computational cost to a normal training procedure. Although being simple, ISDA consistently improves the generalization performance of popular deep models (ResNets and DenseNets) on a variety of datasets, e.g., CIFAR-10, CIFAR-100 and ImageNet. Code for reproducing our results is available at https://github.com/blackfeather-wang/ISDA-for-Deep-Networks.



### Liquid Warping GAN: A Unified Framework for Human Motion Imitation, Appearance Transfer and Novel View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1909.12224v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.12224v3)
- **Published**: 2019-09-26 16:23:59+00:00
- **Updated**: 2019-10-01 16:42:27+00:00
- **Authors**: Wen Liu, Zhixin Piao, Jie Min, Wenhan Luo, Lin Ma, Shenghua Gao
- **Comment**: accepted by ICCV2019
- **Journal**: None
- **Summary**: We tackle the human motion imitation, appearance transfer, and novel view synthesis within a unified framework, which means that the model once being trained can be used to handle all these tasks. The existing task-specific methods mainly use 2D keypoints (pose) to estimate the human body structure. However, they only expresses the position information with no abilities to characterize the personalized shape of the individual person and model the limbs rotations. In this paper, we propose to use a 3D body mesh recovery module to disentangle the pose and shape, which can not only model the joint location and rotation but also characterize the personalized body shape. To preserve the source information, such as texture, style, color, and face identity, we propose a Liquid Warping GAN with Liquid Warping Block (LWB) that propagates the source information in both image and feature spaces, and synthesizes an image with respect to the reference. Specifically, the source features are extracted by a denoising convolutional auto-encoder for characterizing the source identity well. Furthermore, our proposed method is able to support a more flexible warping from multiple sources. In addition, we build a new dataset, namely Impersonator (iPER) dataset, for the evaluation of human motion imitation, appearance transfer, and novel view synthesis. Extensive experiments demonstrate the effectiveness of our method in several aspects, such as robustness in occlusion case and preserving face identity, shape consistency and clothes details. All codes and datasets are available on https://svip-lab.github.io/project/impersonator.html



### Range Adaptation for 3D Object Detection in LiDAR
- **Arxiv ID**: http://arxiv.org/abs/1909.12249v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12249v1)
- **Published**: 2019-09-26 16:56:59+00:00
- **Updated**: 2019-09-26 16:56:59+00:00
- **Authors**: Ze Wang, Sihao Ding, Ying Li, Minming Zhao, Sohini Roychowdhury, Andreas Wallin, Guillermo Sapiro, Qiang Qiu
- **Comment**: None
- **Journal**: None
- **Summary**: LiDAR-based 3D object detection plays a crucial role in modern autonomous driving systems. LiDAR data often exhibit severe changes in properties across different observation ranges. In this paper, we explore cross-range adaptation for 3D object detection using LiDAR, i.e., far-range observations are adapted to near-range. This way, far-range detection is optimized for similar performance to near-range one. We adopt a bird-eyes view (BEV) detection framework to perform the proposed model adaptation. Our model adaptation consists of an adversarial global adaptation, and a fine-grained local adaptation. The proposed cross range adaptation framework is validated on three state-of-the-art LiDAR based object detection networks, and we consistently observe performance improvement on the far-range objects, without adding any auxiliary parameters to the model. To the best of our knowledge, this paper is the first attempt to study cross-range LiDAR adaptation for object detection in point clouds. To demonstrate the generality of the proposed adaptation framework, experiments on more challenging cross-device adaptation are further conducted, and a new LiDAR dataset with high-quality annotated point clouds is released to promote future research.



### RLBench: The Robot Learning Benchmark & Learning Environment
- **Arxiv ID**: http://arxiv.org/abs/1909.12271v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.12271v1)
- **Published**: 2019-09-26 17:26:18+00:00
- **Updated**: 2019-09-26 17:26:18+00:00
- **Authors**: Stephen James, Zicong Ma, David Rovick Arrojo, Andrew J. Davison
- **Comment**: Videos and code: https://sites.google.com/view/rlbench
- **Journal**: None
- **Summary**: We present a challenging new benchmark and learning-environment for robot learning: RLBench. The benchmark features 100 completely unique, hand-designed tasks ranging in difficulty, from simple target reaching and door opening, to longer multi-stage tasks, such as opening an oven and placing a tray in it. We provide an array of both proprioceptive observations and visual observations, which include rgb, depth, and segmentation masks from an over-the-shoulder stereo camera and an eye-in-hand monocular camera. Uniquely, each task comes with an infinite supply of demos through the use of motion planners operating on a series of waypoints given during task creation time; enabling an exciting flurry of demonstration-based learning. RLBench has been designed with scalability in mind; new tasks, along with their motion-planned demos, can be easily created and then verified by a series of tools, allowing users to submit their own tasks to the RLBench task repository. This large-scale benchmark aims to accelerate progress in a number of vision-guided manipulation research areas, including: reinforcement learning, imitation learning, multi-task learning, geometric computer vision, and in particular, few-shot learning. With the benchmark's breadth of tasks and demonstrations, we propose the first large-scale few-shot challenge in robotics. We hope that the scale and diversity of RLBench offers unparalleled research opportunities in the robot learning community and beyond.



### Energy-Based Models for Deep Probabilistic Regression
- **Arxiv ID**: http://arxiv.org/abs/1909.12297v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.12297v4)
- **Published**: 2019-09-26 17:58:43+00:00
- **Updated**: 2020-07-19 12:47:37+00:00
- **Authors**: Fredrik K. Gustafsson, Martin Danelljan, Goutam Bhat, Thomas B. Schön
- **Comment**: ECCV 2020. Code is available at
  https://github.com/fregu856/ebms_regression
- **Journal**: None
- **Summary**: While deep learning-based classification is generally tackled using standardized approaches, a wide variety of techniques are employed for regression. In computer vision, one particularly popular such technique is that of confidence-based regression, which entails predicting a confidence value for each input-target pair (x,y). While this approach has demonstrated impressive results, it requires important task-dependent design choices, and the predicted confidences lack a natural probabilistic meaning. We address these issues by proposing a general and conceptually simple regression method with a clear probabilistic interpretation. In our proposed approach, we create an energy-based model of the conditional target density p(y|x), using a deep neural network to predict the un-normalized density from (x,y). This model of p(y|x) is trained by directly minimizing the associated negative log-likelihood, approximated using Monte Carlo sampling. We perform comprehensive experiments on four computer vision regression tasks. Our approach outperforms direct regression, as well as other probabilistic and confidence-based methods. Notably, our model achieves a 2.2% AP improvement over Faster-RCNN for object detection on the COCO dataset, and sets a new state-of-the-art on visual tracking when applied for bounding box estimation. In contrast to confidence-based methods, our approach is also shown to be directly applicable to more general tasks such as age and head-pose estimation. Code is available at https://github.com/fregu856/ebms_regression.



### Compressed Sensing Microscopy with Scanning Line Probes
- **Arxiv ID**: http://arxiv.org/abs/1909.12342v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1909.12342v1)
- **Published**: 2019-09-26 19:13:05+00:00
- **Updated**: 2019-09-26 19:13:05+00:00
- **Authors**: Han-Wen Kuo, Anna E. Dorfi, Daniel V. Esposito, John N. Wright
- **Comment**: 15 pages, 13 figures
- **Journal**: None
- **Summary**: In applications of scanning probe microscopy, images are acquired by raster scanning a point probe across a sample. Viewed from the perspective of compressed sensing (CS), this pointwise sampling scheme is inefficient, especially when the target image is structured. While replacing point measurements with delocalized, incoherent measurements has the potential to yield order-of-magnitude improvements in scan time, implementing the delocalized measurements of CS theory is challenging. In this paper we study a partially delocalized probe construction, in which the point probe is replaced with a continuous line, creating a sensor which essentially acquires line integrals of the target image. We show through simulations, rudimentary theoretical analysis, and experiments, that these line measurements can image sparse samples far more efficiently than traditional point measurements, provided the local features in the sample are enough separated. Despite this promise, practical reconstruction from line measurements poses additional difficulties: the measurements are partially coherent, and real measurements exhibit nonidealities. We show how to overcome these limitations using natural strategies (reweighting to cope with coherence, blind calibration for nonidealities), culminating in an end-to-end demonstration.



### Realtime Simulation of Thin-Shell Deformable Materials using CNN-Based Mesh Embedding
- **Arxiv ID**: http://arxiv.org/abs/1909.12354v4
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.12354v4)
- **Published**: 2019-09-26 19:38:58+00:00
- **Updated**: 2020-02-28 04:45:12+00:00
- **Authors**: Qingyang Tan, Zherong Pan, Lin Gao, Dinesh Manocha
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of accelerating thin-shell deformable object simulations by dimension reduction. We present a new algorithm to embed a high-dimensional configuration space of deformable objects in a low-dimensional feature space, where the configurations of objects and feature points have approximate one-to-one mapping. Our key technique is a graph-based convolutional neural network (CNN) defined on meshes with arbitrary topologies and a new mesh embedding approach based on physics-inspired loss term. We have applied our approach to accelerate high-resolution thin shell simulations corresponding to cloth-like materials, where the configuration space has tens of thousands of degrees of freedom. We show that our physics-inspired embedding approach leads to higher accuracy compared with prior mesh embedding methods. Finally, we show that the temporal evolution of the mesh in the feature space can also be learned using a recurrent neural network (RNN) leading to fully learnable physics simulators. After training our learned simulator runs $500-10000\times$ faster and the accuracy is high enough for robot manipulation tasks.



### Can We Trust You? On Calibration of a Probabilistic Object Detector for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1909.12358v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.12358v1)
- **Published**: 2019-09-26 19:48:16+00:00
- **Updated**: 2019-09-26 19:48:16+00:00
- **Authors**: Di Feng, Lars Rosenbaum, Claudius Glaeser, Fabian Timm, Klaus Dietmayer
- **Comment**: To appear in IROS 2019
- **Journal**: None
- **Summary**: Reliable uncertainty estimation is crucial for perception systems in safe autonomous driving. Recently, many methods have been proposed to model uncertainties in deep learning based object detectors. However, the estimated probabilities are often uncalibrated, which may lead to severe problems in safety critical scenarios. In this work, we identify such uncertainty miscalibration problems in a probabilistic LiDAR 3D object detection network, and propose three practical methods to significantly reduce errors in uncertainty calibration. Extensive experiments on several datasets show that our methods produce well-calibrated uncertainties, and generalize well between different datasets.



### Task-Discriminative Domain Alignment for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1909.12366v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12366v1)
- **Published**: 2019-09-26 20:04:50+00:00
- **Updated**: 2019-09-26 20:04:50+00:00
- **Authors**: Behnam Gholami, Pritish Sahu, Minyoung Kim, Vladimir Pavlovic
- **Comment**: This paper is accepted for ORAL presentation at the ICCV 2019 MDALC
  Workshop
- **Journal**: None
- **Summary**: Domain Adaptation (DA), the process of effectively adapting task models learned on one domain, the source, to other related but distinct domains, the targets, with no or minimal retraining, is typically accomplished using the process of source-to-target manifold alignment. However, this process often leads to unsatisfactory adaptation performance, in part because it ignores the task-specific structure of the data. In this paper, we improve the performance of DA by introducing a discriminative discrepancy measure which takes advantage of auxiliary information available in the source and the target domains to better align the source and target distributions. Specifically, we leverage the cohesive clustering structure within individual data manifolds, associated with different tasks, to improve the alignment. This structure is explicit in the source, where the task labels are available, but is implicit in the target, making the problem challenging. We address the challenge by devising a deep DA framework, which combines a new task-driven domain alignment discriminator with domain regularizers that encourage the shared features as task-specific and domain invariant, and prompt the task model to be data structure preserving, guiding its decision boundaries through the low density data regions. We validate our framework on standard benchmarks, including Digits (MNIST, USPS, SVHN, MNIST-M), PACS, and VisDA. Our results show that our proposed model consistently outperforms the state-of-the-art in unsupervised domain adaptation.



### Graph-Preserving Grid Layout: A Simple Graph Drawing Method for Graph Classification using CNNs
- **Arxiv ID**: http://arxiv.org/abs/1909.12383v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.12383v1)
- **Published**: 2019-09-26 20:53:12+00:00
- **Updated**: 2019-09-26 20:53:12+00:00
- **Authors**: Yecheng Lyu, Xinming Huang, Ziming Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Graph convolutional networks (GCNs) suffer from the irregularity of graphs, while more widely-used convolutional neural networks (CNNs) benefit from regular grids. To bridge the gap between GCN and CNN, in contrast to previous works on generalizing the basic operations in CNNs to graph data, in this paper we address the problem of how to project undirected graphs onto the grid in a {\em principled} way where CNNs can be used as backbone for geometric deep learning. To this end, inspired by the literature of graph drawing we propose a novel graph-preserving grid layout (GPGL), an integer programming that minimizes the topological loss on the grid. Technically we propose solving GPGL approximately using a {\em regularized} Kamada-Kawai algorithm, a well-known nonconvex optimization technique in graph drawing, with a vertex separation penalty that improves the rounding performance on top of the solutions from relaxation. Using GPGL we can easily conduct data augmentation as every local minimum will lead to a grid layout for the same graph. Together with the help of multi-scale maxout CNNs, we demonstrate the empirical success of our method for graph classification.



### Optimizing Nondecomposable Data Dependent Regularizers via Lagrangian Reparameterization offers Significant Performance and Efficiency Gains
- **Arxiv ID**: http://arxiv.org/abs/1909.12398v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.12398v1)
- **Published**: 2019-09-26 21:19:30+00:00
- **Updated**: 2019-09-26 21:19:30+00:00
- **Authors**: Sathya N. Ravi, Abhay Venkatesh, Glenn Moo Fung, Vikas Singh
- **Comment**: None
- **Journal**: None
- **Summary**: Data dependent regularization is known to benefit a wide variety of problems in machine learning. Often, these regularizers cannot be easily decomposed into a sum over a finite number of terms, e.g., a sum over individual example-wise terms. The $F_\beta$ measure, Area under the ROC curve (AUCROC) and Precision at a fixed recall (P@R) are some prominent examples that are used in many applications. We find that for most medium to large sized datasets, scalability issues severely limit our ability in leveraging the benefits of such regularizers. Importantly, the key technical impediment despite some recent progress is that, such objectives remain difficult to optimize via backpropapagation procedures. While an efficient general-purpose strategy for this problem still remains elusive, in this paper, we show that for many data-dependent nondecomposable regularizers that are relevant in applications, sizable gains in efficiency are possible with minimal code-level changes; in other words, no specialized tools or numerical schemes are needed. Our procedure involves a reparameterization followed by a partial dualization -- this leads to a formulation that has provably cheap projection operators. We present a detailed analysis of runtime and convergence properties of our algorithm. On the experimental side, we show that a direct use of our scheme significantly improves the state of the art IOU measures reported for MSCOCO Stuff segmentation dataset.



### Markov Decision Process for Video Generation
- **Arxiv ID**: http://arxiv.org/abs/1909.12400v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12400v1)
- **Published**: 2019-09-26 21:23:16+00:00
- **Updated**: 2019-09-26 21:23:16+00:00
- **Authors**: Vladyslav Yushchenko, Nikita Araslanov, Stefan Roth
- **Comment**: To appear at 2019 ICCV Workshop on Large Scale Holistic Video
  Understanding
- **Journal**: None
- **Summary**: We identify two pathological cases of temporal inconsistencies in video generation: video freezing and video looping. To better quantify the temporal diversity, we propose a class of complementary metrics that are effective, easy to implement, data agnostic, and interpretable. Further, we observe that current state-of-the-art models are trained on video samples of fixed length thereby inhibiting long-term modeling. To address this, we reformulate the problem of video generation as a Markov Decision Process (MDP). The underlying idea is to represent motion as a stochastic process with an infinite forecast horizon to overcome the fixed length limitation and to mitigate the presence of temporal artifacts. We show that our formulation is easy to integrate into the state-of-the-art MoCoGAN framework. Our experiments on the Human Actions and UCF-101 datasets demonstrate that our MDP-based model is more memory efficient and improves the video quality both in terms of the new and established metrics.



### A Hierarchical Approach for Visual Storytelling Using Image Description
- **Arxiv ID**: http://arxiv.org/abs/1909.12401v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.12401v1)
- **Published**: 2019-09-26 21:25:41+00:00
- **Updated**: 2019-09-26 21:25:41+00:00
- **Authors**: Md Sultan Al Nahian, Tasmia Tasrin, Sagar Gandhi, Ryan Gaines, Brent Harrison
- **Comment**: Accepted at the 2019 International Conference on Interactive Digital
  Storytelling (ICIDS 2019)
- **Journal**: None
- **Summary**: One of the primary challenges of visual storytelling is developing techniques that can maintain the context of the story over long event sequences to generate human-like stories. In this paper, we propose a hierarchical deep learning architecture based on encoder-decoder networks to address this problem. To better help our network maintain this context while also generating long and diverse sentences, we incorporate natural language image descriptions along with the images themselves to generate each story sentence. We evaluate our system on the Visual Storytelling (VIST) dataset and show that our method outperforms state-of-the-art techniques on a suite of different automatic evaluation metrics. The empirical results from this evaluation demonstrate the necessities of different components of our proposed architecture and shows the effectiveness of the architecture for visual storytelling.



