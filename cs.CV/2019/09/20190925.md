# Arxiv Papers in cs.CV on 2019-09-25
### Learning Propagation for Arbitrarily-structured Data
- **Arxiv ID**: http://arxiv.org/abs/1909.11237v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.11237v1)
- **Published**: 2019-09-25 00:13:10+00:00
- **Updated**: 2019-09-25 00:13:10+00:00
- **Authors**: Sifei Liu, Xueting Li, Varun Jampani, Shalini De Mello, Jan Kautz
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Processing an input signal that contains arbitrary structures, e.g., superpixels and point clouds, remains a big challenge in computer vision. Linear diffusion, an effective model for image processing, has been recently integrated with deep learning algorithms. In this paper, we propose to learn pairwise relations among data points in a global fashion to improve semantic segmentation with arbitrarily-structured data, through spatial generalized propagation networks (SGPN). The network propagates information on a group of graphs, which represent the arbitrarily-structured data, through a learned, linear diffusion process. The module is flexible to be embedded and jointly trained with many types of networks, e.g., CNNs. We experiment with semantic segmentation networks, where we use our propagation module to jointly train on different data -- images, superpixels and point clouds. We show that SGPN consistently improves the performance of both pixel and point cloud segmentation, compared to networks that do not contain this module. Our method suggests an effective way to model the global pairwise relations for arbitrarily-structured data.



### WiderPerson: A Diverse Dataset for Dense Pedestrian Detection in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1909.12118v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12118v1)
- **Published**: 2019-09-25 02:42:07+00:00
- **Updated**: 2019-09-25 02:42:07+00:00
- **Authors**: Shifeng Zhang, Yiliang Xie, Jun Wan, Hansheng Xia, Stan Z. Li, Guodong Guo
- **Comment**: TMM: submitted on 2018.07.17, accepted on 2019.07.01. arXiv admin
  note: text overlap with arXiv:1805.07193, arXiv:1805.00123 by other authors
- **Journal**: None
- **Summary**: Pedestrian detection has achieved significant progress with the availability of existing benchmark datasets. However, there is a gap in the diversity and density between real world requirements and current pedestrian detection benchmarks: 1) most of existing datasets are taken from a vehicle driving through the regular traffic scenario, usually leading to insufficient diversity; 2) crowd scenarios with highly occluded pedestrians are still under represented, resulting in low density. To narrow this gap and facilitate future pedestrian detection research, we introduce a large and diverse dataset named WiderPerson for dense pedestrian detection in the wild. This dataset involves five types of annotations in a wide range of scenarios, no longer limited to the traffic scenario. There are a total of $13,382$ images with $399,786$ annotations, i.e., $29.87$ annotations per image, which means this dataset contains dense pedestrians with various kinds of occlusions. Hence, pedestrians in the proposed dataset are extremely challenging due to large variations in the scenario and occlusion, which is suitable to evaluate pedestrian detectors in the wild. We introduce an improved Faster R-CNN and the vanilla RetinaNet to serve as baselines for the new pedestrian detection benchmark. Several experiments are conducted on previous datasets including Caltech-USA and CityPersons to analyze the generalization capabilities of the proposed dataset and we achieve state-of-the-art performances on these previous datasets without bells and whistles. Finally, we analyze common failure cases and find the classification ability of pedestrian detector needs to be improved to reduce false alarm and miss detection rates. The proposed dataset is available at http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson



### Rescan: Inductive Instance Segmentation for Indoor RGBD Scans
- **Arxiv ID**: http://arxiv.org/abs/1909.11268v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1909.11268v1)
- **Published**: 2019-09-25 03:20:42+00:00
- **Updated**: 2019-09-25 03:20:42+00:00
- **Authors**: Maciej Halber, Yifei Shi, Kai Xu, Thomas Funkhouser
- **Comment**: IEEE International Conference on Computer Vision 2019
- **Journal**: None
- **Summary**: In depth-sensing applications ranging from home robotics to AR/VR, it will be common to acquire 3D scans of interior spaces repeatedly at sparse time intervals (e.g., as part of regular daily use). We propose an algorithm that analyzes these "rescans" to infer a temporal model of a scene with semantic instance information. Our algorithm operates inductively by using the temporal model resulting from past observations to infer an instance segmentation of a new scan, which is then used to update the temporal model. The model contains object instance associations across time and thus can be used to track individual objects, even though there are only sparse observations. During experiments with a new benchmark for the new task, our algorithm outperforms alternate approaches based on state-of-the-art networks for semantic instance segmentation.



### Towards Automated Biometric Identification of Sea Turtles (Chelonia mydas)
- **Arxiv ID**: http://arxiv.org/abs/1909.11277v2
- **DOI**: 10.5614/itbj.ict.res.appl.2018.12.3.4.
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.11277v2)
- **Published**: 2019-09-25 04:02:25+00:00
- **Updated**: 2021-06-23 10:55:49+00:00
- **Authors**: Irwandi Hipiny, Hamimah Ujir, Aazani Mujahid, Nurhartini Kamalia Yahya
- **Comment**: Published in Journal of ICT Research and Applications, [S.l.], v. 12,
  n. 3, p. 256-266, dec. 2018
- **Journal**: Journal of ICT Research and Applications, [S.l.], v. 12, n. 3, p.
  256-266, dec. 2018. ISSN 2338-5499
- **Summary**: Passive biometric identification enables wildlife monitoring with minimal disturbance. Using a motion-activated camera placed at an elevated position and facing downwards, we collected images of sea turtle carapace, each belonging to one of sixteen Chelonia mydas juveniles. We then learned co-variant and robust image descriptors from these images, enabling indexing and retrieval. In this work, we presented several classification results of sea turtle carapaces using the learned image descriptors. We found that a template-based descriptor, i.e., Histogram of Oriented Gradients (HOG) performed exceedingly better during classification than keypoint-based descriptors. For our dataset, a high-dimensional descriptor is a must due to the minimal gradient and color information inside the carapace images. Using HOG, we obtained an average classification accuracy of 65%.



### A Dictionary Approach to Domain-Invariant Learning in Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.11285v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.11285v2)
- **Published**: 2019-09-25 04:35:04+00:00
- **Updated**: 2020-09-28 23:31:44+00:00
- **Authors**: Ze Wang, Xiuyuan Cheng, Guillermo Sapiro, Qiang Qiu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we consider domain-invariant deep learning by explicitly modeling domain shifts with only a small amount of domain-specific parameters in a Convolutional Neural Network (CNN). By exploiting the observation that a convolutional filter can be well approximated as a linear combination of a small set of dictionary atoms, we show for the first time, both empirically and theoretically, that domain shifts can be effectively handled by decomposing a convolutional layer into a domain-specific atom layer and a domain-shared coefficient layer, while both remain convolutional. An input channel will now first convolve spatially only with each respective domain-specific dictionary atom to "absorb" domain variations, and then output channels are linearly combined using common decomposition coefficients trained to promote shared semantics across domains. We use toy examples, rigorous analysis, and real-world examples with diverse datasets and architectures, to show the proposed plug-in framework's effectiveness in cross and joint domain performance and domain adaptation. With the proposed architecture, we need only a small set of dictionary atoms to model each additional domain, which brings a negligible amount of additional parameters, typically a few hundred.



### Stochastic Conditional Generative Networks with Basis Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1909.11286v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.11286v2)
- **Published**: 2019-09-25 04:37:38+00:00
- **Updated**: 2020-02-24 19:35:47+00:00
- **Authors**: Ze Wang, Xiuyuan Cheng, Guillermo Sapiro, Qiang Qiu
- **Comment**: Published as a conference paper at ICLR 2020
- **Journal**: None
- **Summary**: While generative adversarial networks (GANs) have revolutionized machine learning, a number of open questions remain to fully understand them and exploit their power. One of these questions is how to efficiently achieve proper diversity and sampling of the multi-mode data space. To address this, we introduce BasisGAN, a stochastic conditional multi-mode image generator. By exploiting the observation that a convolutional filter can be well approximated as a linear combination of a small set of basis elements, we learn a plug-and-played basis generator to stochastically generate basis elements, with just a few hundred of parameters, to fully embed stochasticity into convolutional filters. By sampling basis elements instead of filters, we dramatically reduce the cost of modeling the parameter space with no sacrifice on either image diversity or fidelity. To illustrate this proposed plug-and-play framework, we construct variants of BasisGAN based on state-of-the-art conditional image generation networks, and train the networks by simply plugging in a basis generator, without additional auxiliary components, hyperparameters, or training objectives. The experimental success is complemented with theoretical results indicating how the perturbations introduced by the proposed sampling of basis elements can propagate to the appearance of generated images.



### Deep learning vessel segmentation and quantification of the foveal avascular zone using commercial and prototype OCT-A platforms
- **Arxiv ID**: http://arxiv.org/abs/1909.11289v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.11289v1)
- **Published**: 2019-09-25 05:04:20+00:00
- **Updated**: 2019-09-25 05:04:20+00:00
- **Authors**: Morgan Heisler, Forson Chan, Zaid Mammo, Chandrakumar Balaratnasingam, Pavle Prentasic, Gavin Docherty, MyeongJin Ju, Sanjeeva Rajapakse, Sieun Lee, Andrew Merkur, Andrew Kirker, David Albiani, David Maberley, K. Bailey Freund, Mirza Faisal Beg, Sven Loncaric, Marinko V. Sarunic, Eduardo V. Navajas
- **Comment**: 22 pages, 5 figures
- **Journal**: None
- **Summary**: Automatic quantification of perifoveal vessel densities in optical coherence tomography angiography (OCT-A) images face challenges such as variable intra- and inter-image signal to noise ratios, projection artefacts from outer vasculature layers, and motion artefacts. This study demonstrates the utility of deep neural networks for automatic quantification of foveal avascular zone (FAZ) parameters and perifoveal vessel density of OCT-A images in healthy and diabetic eyes. OCT-A images of the foveal region were acquired using three OCT-A systems: a 1060nm Swept Source (SS)-OCT prototype, RTVue XR Avanti (Optovue Inc., Fremont, CA), and the ZEISS Angioplex (Carl Zeiss Meditec, Dublin, CA). Automated segmentation was then performed using a deep neural network. Four FAZ morphometric parameters (area, min/max diameter, and eccentricity) and perifoveal vessel density were used as outcome measures. The accuracy, sensitivity and specificity of the DNN vessel segmentations were comparable across all three device platforms. No significant difference between the means of the measurements from automated and manual segmentations were found for any of the outcome measures on any system. The intraclass correlation coefficient (ICC) was also good (> 0.51) for all measurements. Automated deep learning vessel segmentation of OCT-A may be suitable for both commercial and research purposes for better quantification of the retinal circulation.



### Guided Attention Network for Object Detection and Counting on Drones
- **Arxiv ID**: http://arxiv.org/abs/1909.11307v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.11307v1)
- **Published**: 2019-09-25 06:37:49+00:00
- **Updated**: 2019-09-25 06:37:49+00:00
- **Authors**: Yuanqiang Cai, Dawei Du, Libo Zhang, Longyin Wen, Weiqiang Wang, Yanjun Wu, Siwei Lyu
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection and counting are related but challenging problems, especially for drone based scenes with small objects and cluttered background. In this paper, we propose a new Guided Attention Network (GANet) to deal with both object detection and counting tasks based on the feature pyramid. Different from the previous methods relying on unsupervised attention modules, we fuse different scales of feature maps by using the proposed weakly-supervised Background Attention (BA) between the background and objects for more semantic feature representation. Then, the Foreground Attention (FA) module is developed to consider both global and local appearance of the object to facilitate accurate localization. Moreover, the new data argumentation strategy is designed to train a robust model in various complex scenes. Extensive experiments on three challenging benchmarks (i.e., UAVDT, CARPK and PUCPR+) show the state-of-the-art detection and counting performance of the proposed method compared with existing methods.



### Conditional Transferring Features: Scaling GANs to Thousands of Classes with 30% Less High-quality Data for Training
- **Arxiv ID**: http://arxiv.org/abs/1909.11308v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.11308v1)
- **Published**: 2019-09-25 06:45:39+00:00
- **Updated**: 2019-09-25 06:45:39+00:00
- **Authors**: Chunpeng Wu, Wei Wen, Yiran Chen, Hai Li
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial network (GAN) has greatly improved the quality of unsupervised image generation. Previous GAN-based methods often require a large amount of high-quality training data while producing a small number (e.g., tens) of classes. This work aims to scale up GANs to thousands of classes meanwhile reducing the use of high-quality data in training. We propose an image generation method based on conditional transferring features, which can capture pixel-level semantic changes when transforming low-quality images into high-quality ones. Moreover, self-supervision learning is integrated into our GAN architecture to provide more label-free semantic supervisory information observed from the training data. As such, training our GAN architecture requires much fewer high-quality images with a small number of additional low-quality images. The experiments on CIFAR-10 and STL-10 show that even removing 30% high-quality images from the training set, our method can still outperform previous ones. The scalability on object classes has been experimentally validated: our method with 30% fewer high-quality images obtains the best quality in generating 1,000 ImageNet classes, as well as generating all 3,755 classes of CASIA-HWDB1.0 Chinese handwriting characters.



### Cross-View Kernel Similarity Metric Learning Using Pairwise Constraints for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1909.11316v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.11316v1)
- **Published**: 2019-09-25 07:30:23+00:00
- **Updated**: 2019-09-25 07:30:23+00:00
- **Authors**: T M Feroz Ali, Subhasis Chaudhuri
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification is the task of matching pedestrian images across non-overlapping cameras. In this paper, we propose a non-linear cross-view similarity metric learning for handling small size training data in practical re-ID systems. The method employs non-linear mappings combined with cross-view discriminative subspace learning and cross-view distance metric learning based on pairwise similarity constraints. It is a natural extension of XQDA from linear to non-linear mappings using kernels, and learns non-linear transformations for efficiently handling complex non-linearity of person appearance across camera views. Importantly, the proposed method is very computationally efficient. Extensive experiments on four challenging datasets shows that our method attains competitive performance against state-of-the-art methods.



### FALCON: Lightweight and Accurate Convolution
- **Arxiv ID**: http://arxiv.org/abs/1909.11321v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.11321v2)
- **Published**: 2019-09-25 07:48:31+00:00
- **Updated**: 2020-12-29 04:31:56+00:00
- **Authors**: Jun-Gi Jang, Chun Quan, Hyun Dong Lee, U Kang
- **Comment**: None
- **Journal**: None
- **Summary**: How can we efficiently compress Convolutional Neural Network (CNN) while retaining their accuracy on classification tasks? Depthwise Separable Convolution (DSConv), which replaces a standard convolution with a depthwise convolution and a pointwise convolution, has been used for building lightweight architectures. However, previous works based on depthwise separable convolution are limited when compressing a trained CNN model since 1) they are mostly heuristic approaches without a precise understanding of their relations to standard convolution, and 2) their accuracies do not match that of the standard convolution. In this paper, we propose FALCON, an accurate and lightweight method to compress CNN. FALCON uses GEP, our proposed mathematical formulation to approximate the standard convolution kernel, to interpret existing convolution methods based on depthwise separable convolution. By exploiting the knowledge of a trained standard model and carefully determining the order of depthwise separable convolution via GEP, FALCON achieves sufficient accuracy close to that of the trained standard model. Furthermore, this interpretation leads to developing a generalized version rank-k FALCON which performs k independent FALCON operations and sums up the result. Experiments show that FALCON 1) provides higher accuracy than existing methods based on depthwise separable convolution and tensor decomposition, and 2) reduces the number of parameters and FLOPs of standard convolution by up to a factor of 8 while ensuring similar accuracy. We also demonstrate that rank-k FALCON further improves the accuracy while sacrificing a bit of compression and computation reduction rates.



### Balancing Specialization, Generalization, and Compression for Detection and Tracking
- **Arxiv ID**: http://arxiv.org/abs/1909.11348v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.11348v1)
- **Published**: 2019-09-25 08:59:06+00:00
- **Updated**: 2019-09-25 08:59:06+00:00
- **Authors**: Dotan Kaufman, Koby Bibas, Eran Borenstein, Michael Chertok, Tal Hassner
- **Comment**: Accepted to BMVC 2019
- **Journal**: None
- **Summary**: We propose a method for specializing deep detectors and trackers to restricted settings. Our approach is designed with the following goals in mind: (a) Improving accuracy in restricted domains; (b) preventing overfitting to new domains and forgetting of generalized capabilities; (c) aggressive model compression and acceleration. To this end, we propose a novel loss that balances compression and acceleration of a deep learning model vs. loss of generalization capabilities. We apply our method to the existing tracker and detector models. We report detection results on the VIRAT and CAVIAR data sets. These results show our method to offer unprecedented compression rates along with improved detection. We apply our loss for tracker compression at test time, as it processes each video. Our tests on the OTB2015 benchmark show that applying compression during test time actually improves tracking performance.



### Robust Monocular Edge Visual Odometry through Coarse-to-Fine Data Association
- **Arxiv ID**: http://arxiv.org/abs/1909.11362v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.11362v2)
- **Published**: 2019-09-25 09:21:47+00:00
- **Updated**: 2020-03-16 23:28:23+00:00
- **Authors**: Xiaolong Wu, Patricio Vela, Cedric Pradalier
- **Comment**: 6 pages, 7 figures, 2 tables, submitted to iros2020
- **Journal**: None
- **Summary**: In this work, we propose a monocular visual odometry framework, which allows exploiting the best attributes of edge feature for illumination-robust camera tracking, while at the same time ameliorating the performance degradation of edge mapping. In the front-end, an ICP-based edge registration can provide robust motion estimation and coarse data association under lighting changes. In the back-end, a novel edge-guided data association pipeline searches for the best photometrically matched points along geometrically possible edges through template matching, so that the matches can be further refined in later bundle adjustment. The core of our proposed data association strategy lies in a point-to-edge geometric uncertainty analysis, which analytically derives (1) the probabilistic search length formula that significantly reduces the search space for system speed-up and (2) the geometrical confidence metric for mapping degradation detection based on the predicted depth uncertainty. Moreover, match confidence based patch size adaption strategy is integrated into our pipeline, connecting with other components, to reduce the matching ambiguity. We present extensive analysis and evaluation of our proposed system on synthetic and real-world benchmark datasets under the influence of illumination changes and large camera motions, where our proposed system outperforms current state-of-art algorithms.



### Accurate and Compact Convolutional Neural Networks with Trained Binarization
- **Arxiv ID**: http://arxiv.org/abs/1909.11366v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.11366v1)
- **Published**: 2019-09-25 09:29:50+00:00
- **Updated**: 2019-09-25 09:29:50+00:00
- **Authors**: Zhe Xu, Ray C. C. Cheung
- **Comment**: Accepted as an Oral presentation in British Machine Vision Conference
  (BMVC) 2019
- **Journal**: None
- **Summary**: Although convolutional neural networks (CNNs) are now widely used in various computer vision applications, its huge resource demanding on parameter storage and computation makes the deployment on mobile and embedded devices difficult. Recently, binary convolutional neural networks are explored to help alleviate this issue by quantizing both weights and activations with only 1 single bit. However, there may exist a noticeable accuracy degradation when compared with full-precision models. In this paper, we propose an improved training approach towards compact binary CNNs with higher accuracy. Trainable scaling factors for both weights and activations are introduced to increase the value range. These scaling factors will be trained jointly with other parameters via backpropagation. Besides, a specific training algorithm is developed including tight approximation for derivative of discontinuous binarization function and $L_2$ regularization acting on weight scaling factors. With these improvements, the binary CNN achieves 92.3% accuracy on CIFAR-10 with VGG-Small network. On ImageNet, our method also obtains 46.1% top-1 accuracy with AlexNet and 54.2% with Resnet-18 surpassing previous works.



### Attention Convolutional Binary Neural Tree for Fine-Grained Visual Categorization
- **Arxiv ID**: http://arxiv.org/abs/1909.11378v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.11378v2)
- **Published**: 2019-09-25 10:03:48+00:00
- **Updated**: 2020-03-14 04:59:49+00:00
- **Authors**: Ruyi Ji, Longyin Wen, Libo Zhang, Dawei Du, Yanjun Wu, Chen Zhao, Xianglong Liu, Feiyue Huang
- **Comment**: accepted by CVPR 2020
- **Journal**: None
- **Summary**: Fine-grained visual categorization (FGVC) is an important but challenging task due to high intra-class variances and low inter-class variances caused by deformation, occlusion, illumination, etc. An attention convolutional binary neural tree architecture is presented to address those problems for weakly supervised FGVC. Specifically, we incorporate convolutional operations along edges of the tree structure, and use the routing functions in each node to determine the root-to-leaf computational paths within the tree. The final decision is computed as the summation of the predictions from leaf nodes. The deep convolutional operations learn to capture the representations of objects, and the tree structure characterizes the coarse-to-fine hierarchical feature learning process. In addition, we use the attention transformer module to enforce the network to capture discriminative features. The negative log-likelihood loss is used to train the entire network in an end-to-end fashion by SGD with back-propagation. Several experiments on the CUB-200-2011, Stanford Cars and Aircraft datasets demonstrate that the proposed method performs favorably against the state-of-the-arts.



### Beyond image classification: zooplankton identification with deep vector space embeddings
- **Arxiv ID**: http://arxiv.org/abs/1909.11380v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.11380v1)
- **Published**: 2019-09-25 10:12:05+00:00
- **Updated**: 2019-09-25 10:12:05+00:00
- **Authors**: Ketil Malde, Hyeongji Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Zooplankton images, like many other real world data types, have intrinsic properties that make the design of effective classification systems difficult. For instance, the number of classes encountered in practical settings is potentially very large, and classes can be ambiguous or overlap. In addition, the choice of taxonomy often differs between researchers and between institutions. Although high accuracy has been achieved in benchmarks using standard classifier architectures, biases caused by an inflexible classification scheme can have profound effects when the output is used in ecosystem assessments and monitoring.   Here, we propose using a deep convolutional network to construct a vector embedding of zooplankton images. The system maps (embeds) each image into a high-dimensional Euclidean space so that distances between vectors reflect semantic relationships between images. We show that the embedding can be used to derive classifications with comparable accuracy to a specific classifier, but that it simultaneously reveals important structures in the data. Furthermore, we apply the embedding to new classes previously unseen by the system, and evaluate its classification performance in such cases.   Traditional neural network classifiers perform well when the classes are clearly defined a priori and have sufficiently large labeled data sets available. For practical cases in ecology as well as in many other fields this is not the case, and we argue that the vector embedding method presented here is a more appropriate approach.



### Efficient Residual Dense Block Search for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1909.11409v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.11409v3)
- **Published**: 2019-09-25 11:19:49+00:00
- **Updated**: 2019-12-30 08:04:18+00:00
- **Authors**: Dehua Song, Chang Xu, Xu Jia, Yiyi Chen, Chunjing Xu, Yunhe Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Although remarkable progress has been made on single image super-resolution due to the revival of deep convolutional neural networks, deep learning methods are confronted with the challenges of computation and memory consumption in practice, especially for mobile devices. Focusing on this issue, we propose an efficient residual dense block search algorithm with multiple objectives to hunt for fast, lightweight and accurate networks for image super-resolution. Firstly, to accelerate super-resolution network, we exploit the variation of feature scale adequately with the proposed efficient residual dense blocks. In the proposed evolutionary algorithm, the locations of pooling and upsampling operator are searched automatically. Secondly, network architecture is evolved with the guidance of block credits to acquire accurate super-resolution network. The block credit reflects the effect of current block and is earned during model evaluation process. It guides the evolution by weighing the sampling probability of mutation to favor admirable blocks. Extensive experimental results demonstrate the effectiveness of the proposed searching method and the found efficient super-resolution models achieve better performance than the state-of-the-art methods with limited number of parameters and FLOPs.



### Optimal Transport driven CycleGAN for Unsupervised Learning in Inverse Problems
- **Arxiv ID**: http://arxiv.org/abs/1909.12116v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.12116v4)
- **Published**: 2019-09-25 11:28:49+00:00
- **Updated**: 2020-08-30 12:14:48+00:00
- **Authors**: Byeongsu Sim, Gyutaek Oh, Jeongsol Kim, Chanyong Jung, Jong Chul Ye
- **Comment**: accepted for publication in the SIAM Journal on Imaging Sciences
- **Journal**: None
- **Summary**: To improve the performance of classical generative adversarial network (GAN), Wasserstein generative adversarial networks (W-GAN) was developed as a Kantorovich dual formulation of the optimal transport (OT) problem using Wasserstein-1 distance. However, it was not clear how cycleGAN-type generative models can be derived from the optimal transport theory. Here we show that a novel cycleGAN architecture can be derived as a Kantorovich dual OT formulation if a penalized least square (PLS) cost with deep learning-based inverse path penalty is used as a transportation cost. One of the most important advantages of this formulation is that depending on the knowledge of the forward problem, distinct variations of cycleGAN architecture can be derived: for example, one with two pairs of generators and discriminators, and the other with only a single pair of generator and discriminator. Even for the two generator cases, we show that the structural knowledge of the forward operator can lead to a simpler generator architecture which significantly simplifies the neural network training. The new cycleGAN formulation, what we call the OT-cycleGAN, have been applied for various biomedical imaging problems, such as accelerated magnetic resonance imaging (MRI), super-resolution microscopy, and low-dose x-ray computed tomography (CT). Experimental results confirm the efficacy and flexibility of the theory.



### Multi-modal segmentation with missing MR sequences using pre-trained fusion networks
- **Arxiv ID**: http://arxiv.org/abs/1909.11464v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.11464v1)
- **Published**: 2019-09-25 13:04:28+00:00
- **Updated**: 2019-09-25 13:04:28+00:00
- **Authors**: Karin van Garderen, Marion Smits, Stefan Klein
- **Comment**: Accepted at MICCAI MIL3ID workshop 2019
- **Journal**: None
- **Summary**: Missing data is a common problem in machine learning and in retrospective imaging research it is often encountered in the form of missing imaging modalities. We propose to take into account missing modalities in the design and training of neural networks, to ensure that they are capable of providing the best possible prediction even when multiple images are not available. The proposed network combines three modifications to the standard 3D UNet architecture: a training scheme with dropout of modalities, a multi-pathway architecture with fusion layer in the final stage, and the separate pre-training of these pathways. These modifications are evaluated incrementally in terms of performance on full and missing data, using the BraTS multi-modal segmentation challenge. The final model shows significant improvement with respect to the state of the art on missing data and requires less memory during training.



### Towards continuous learning for glioma segmentation with elastic weight consolidation
- **Arxiv ID**: http://arxiv.org/abs/1909.11479v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.11479v1)
- **Published**: 2019-09-25 13:27:23+00:00
- **Updated**: 2019-09-25 13:27:23+00:00
- **Authors**: Karin van Garderen, Sebastian van der Voort, Fatih Incekara, Marion Smits, Stefan Klein
- **Comment**: None
- **Journal**: None
- **Summary**: When finetuning a convolutional neural network (CNN) on data from a new domain, catastrophic forgetting will reduce performance on the original training data. Elastic Weight Consolidation (EWC) is a recent technique to prevent this, which we evaluated while training and re-training a CNN to segment glioma on two different datasets. The network was trained on the public BraTS dataset and finetuned on an in-house dataset with non-enhancing low-grade glioma. EWC was found to decrease catastrophic forgetting in this case, but was also found to restrict adaptation to the new domain.



### CAT: Compression-Aware Training for bandwidth reduction
- **Arxiv ID**: http://arxiv.org/abs/1909.11481v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.11481v1)
- **Published**: 2019-09-25 13:29:58+00:00
- **Updated**: 2019-09-25 13:29:58+00:00
- **Authors**: Chaim Baskin, Brian Chmiel, Evgenii Zheltonozhskii, Ron Banner, Alex M. Bronstein, Avi Mendelson
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have become the dominant neural network architecture for solving visual processing tasks. One of the major obstacles hindering the ubiquitous use of CNNs for inference is their relatively high memory bandwidth requirements, which can be a main energy consumer and throughput bottleneck in hardware accelerators. Accordingly, an efficient feature map compression method can result in substantial performance gains. Inspired by quantization-aware training approaches, we propose a compression-aware training (CAT) method that involves training the model in a way that allows better compression of feature maps during inference. Our method trains the model to achieve low-entropy feature maps, which enables efficient compression at inference time using classical transform coding methods. CAT significantly improves the state-of-the-art results reported for quantization. For example, on ResNet-34 we achieve 73.1% accuracy (0.2% degradation from the baseline) with an average representation of only 1.79 bits per value. Reference implementation accompanies the paper at https://github.com/CAT-teams/CAT



### Non-imaging single-pixel sensing with optimized binary modulation
- **Arxiv ID**: http://arxiv.org/abs/1909.11498v2
- **DOI**: 10.1364/OL.395150
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.11498v2)
- **Published**: 2019-09-25 13:52:06+00:00
- **Updated**: 2019-09-27 15:27:10+00:00
- **Authors**: Hao Fu, Liheng Bian, Jun Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The conventional high-level sensing techniques require high-fidelity images as input to extract target features, which are produced by either complex imaging hardware or high-complexity reconstruction algorithms. In this letter, we propose single-pixel sensing (SPS) that performs high-level sensing directly from coupled measurements of a single-pixel detector, without the conventional image acquisition and reconstruction process. The technique consists of three steps including binary light modulation that can be physically implemented at $\sim$22kHz, single-pixel coupled detection owning wide working spectrum and high signal-to-noise ratio, and end-to-end deep-learning based sensing that reduces both hardware and software complexity. Besides, the binary modulation is trained and optimized together with the sensing network, which ensures least required measurements and optimal sensing accuracy. The effectiveness of SPS is demonstrated on the classification task of handwritten MNIST dataset, and 96.68% classification accuracy at $\sim$1kHz is achieved. The reported single-pixel sensing technique is a novel framework for highly efficient machine intelligence.



### mustGAN: Multi-Stream Generative Adversarial Networks for MR Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1909.11504v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.11504v1)
- **Published**: 2019-09-25 14:11:06+00:00
- **Updated**: 2019-09-25 14:11:06+00:00
- **Authors**: Mahmut Yurt, Salman Ul Hassan Dar, Aykut Erdem, Erkut Erdem, Tolga Çukur
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-contrast MRI protocols increase the level of morphological information available for diagnosis. Yet, the number and quality of contrasts is limited in practice by various factors including scan time and patient motion. Synthesis of missing or corrupted contrasts can alleviate this limitation to improve clinical utility. Common approaches for multi-contrast MRI involve either one-to-one and many-to-one synthesis methods. One-to-one methods take as input a single source contrast, and they learn a latent representation sensitive to unique features of the source. Meanwhile, many-to-one methods receive multiple distinct sources, and they learn a shared latent representation more sensitive to common features across sources. For enhanced image synthesis, here we propose a multi-stream approach that aggregates information across multiple source images via a mixture of multiple one-to-one streams and a joint many-to-one stream. The shared feature maps generated in the many-to-one stream and the complementary feature maps generated in the one-to-one streams are combined with a fusion block. The location of the fusion block is adaptively modified to maximize task-specific performance. Qualitative and quantitative assessments on T1-, T2-, PD-weighted and FLAIR images clearly demonstrate the superior performance of the proposed method compared to previous state-of-the-art one-to-one and many-to-one methods.



### The Good, the Bad and the Ugly: Evaluating Convolutional Neural Networks for Prohibited Item Detection Using Real and Synthetically Composited X-ray Imagery
- **Arxiv ID**: http://arxiv.org/abs/1909.11508v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.11508v1)
- **Published**: 2019-09-25 14:16:16+00:00
- **Updated**: 2019-09-25 14:16:16+00:00
- **Authors**: Neelanjan Bhowmik, Qian Wang, Yona Falinie A. Gaus, Marcin Szarek, Toby P. Breckon
- **Comment**: None
- **Journal**: In Proc. British Machine Vision Conference Workshops, BMVA, 2019
- **Summary**: Detecting prohibited items in X-ray security imagery is pivotal in maintaining border and transport security against a wide range of threat profiles. Convolutional Neural Networks (CNN) with the support of a significant volume of data have brought advancement in such automated prohibited object detection and classification. However, collating such large volumes of X-ray security imagery remains a significant challenge. This work opens up the possibility of using synthetically composed imagery, avoiding the need to collate such large volumes of hand-annotated real-world imagery. Here we investigate the difference in detection performance achieved using real and synthetic X-ray training imagery for CNN architecture detecting three exemplar prohibited items, {Firearm, Firearm Parts, Knives}, within cluttered and complex X-ray security baggage imagery. We achieve 0.88 of mean average precision (mAP) with a Faster R-CNN and ResNet-101 CNN architecture for this 3-class object detection using real X-ray imagery. While the performance is comparable with synthetically composited X-ray imagery (0.78 mAP), our extended evaluation demonstrates both challenge and promise of using synthetically composed images to diversify the X-ray security training imagery for automated detection algorithm training.



### Synthetic Data for Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.11512v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.11512v1)
- **Published**: 2019-09-25 14:20:57+00:00
- **Updated**: 2019-09-25 14:20:57+00:00
- **Authors**: Sergey I. Nikolenko
- **Comment**: 156 pages, 24 figures, 719 references
- **Journal**: None
- **Summary**: Synthetic data is an increasingly popular tool for training deep learning models, especially in computer vision but also in other areas. In this work, we attempt to provide a comprehensive survey of the various directions in the development and application of synthetic data. First, we discuss synthetic datasets for basic computer vision problems, both low-level (e.g., optical flow estimation) and high-level (e.g., semantic segmentation), synthetic environments and datasets for outdoor and urban scenes (autonomous driving), indoor scenes (indoor navigation), aerial navigation, simulation environments for robotics, applications of synthetic data outside computer vision (in neural programming, bioinformatics, NLP, and more); we also survey the work on improving synthetic data development and alternative ways to produce it such as GANs. Second, we discuss in detail the synthetic-to-real domain adaptation problem that inevitably arises in applications of synthetic data, including synthetic-to-real refinement with GAN-based models and domain adaptation at the feature/model level without explicit data transformations. Third, we turn to privacy-related applications of synthetic data and review the work on generating synthetic datasets with differential privacy guarantees. We conclude by highlighting the most promising directions for further work in synthetic data studies.



### Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1909.11515v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.11515v2)
- **Published**: 2019-09-25 14:21:55+00:00
- **Updated**: 2020-02-20 08:54:57+00:00
- **Authors**: Tianyu Pang, Kun Xu, Jun Zhu
- **Comment**: ICLR 2020
- **Journal**: None
- **Summary**: It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally non-linear behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited. Namely, since the locality of the adversarial perturbations, it would be more efficient to actively break the locality via the globality of the model predictions. Inspired by simple geometric intuition, we develop an inference principle, named mixup inference (MI), for mixup-trained models. MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial. Our experiments on CIFAR-10 and CIFAR-100 demonstrate that MI can further improve the adversarial robustness for the models trained by mixup and its variants.



### Gated Channel Transformation for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1909.11519v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.11519v2)
- **Published**: 2019-09-25 14:26:32+00:00
- **Updated**: 2020-03-27 10:08:39+00:00
- **Authors**: Zongxin Yang, Linchao Zhu, Yu Wu, Yi Yang
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: In this work, we propose a generally applicable transformation unit for visual recognition with deep convolutional neural networks. This transformation explicitly models channel relationships with explainable control variables. These variables determine the neuron behaviors of competition or cooperation, and they are jointly optimized with the convolutional weight towards more accurate recognition. In Squeeze-and-Excitation (SE) Networks, the channel relationships are implicitly learned by fully connected layers, and the SE block is integrated at the block-level. We instead introduce a channel normalization layer to reduce the number of parameters and computational complexity. This lightweight layer incorporates a simple l2 normalization, enabling our transformation unit applicable to operator-level without much increase of additional parameters. Extensive experiments demonstrate the effectiveness of our unit with clear margins on many vision tasks, i.e., image classification on ImageNet, object detection and instance segmentation on COCO, video classification on Kinetics.



### Dual Adaptive Pyramid Network for Cross-Stain Histopathology Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.11524v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.11524v1)
- **Published**: 2019-09-25 14:31:02+00:00
- **Updated**: 2019-09-25 14:31:02+00:00
- **Authors**: Xianxu Hou, Jingxin Liu, Bolei Xu, Bozhi Liu, Xin Chen, Mohammad Ilyas, Ian Ellis, Jon Garibaldi, Guoping Qiu
- **Comment**: MICCAI2019
- **Journal**: None
- **Summary**: Supervised semantic segmentation normally assumes the test data being in a similar data domain as the training data. However, in practice, the domain mismatch between the training and unseen data could lead to a significant performance drop. Obtaining accurate pixel-wise label for images in different domains is tedious and labor intensive, especially for histopathology images. In this paper, we propose a dual adaptive pyramid network (DAPNet) for histopathological gland segmentation adapting from one stain domain to another. We tackle the domain adaptation problem on two levels: 1) the image-level considers the differences of image color and style; 2) the feature-level addresses the spatial inconsistency between two domains. The two components are implemented as domain classifiers with adversarial training. We evaluate our new approach using two gland segmentation datasets with H&E and DAB-H stains respectively. The extensive experiments and ablation study demonstrate the effectiveness of our approach on the domain adaptive segmentation task. We show that the proposed approach performs favorably against other state-of-the-art methods.



### Deep Learning for Deepfakes Creation and Detection: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1909.11573v5
- **DOI**: 10.1016/j.cviu.2022.103525
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.11573v5)
- **Published**: 2019-09-25 16:03:45+00:00
- **Updated**: 2022-08-11 09:56:02+00:00
- **Authors**: Thanh Thi Nguyen, Quoc Viet Hung Nguyen, Dung Tien Nguyen, Duc Thanh Nguyen, Thien Huynh-The, Saeid Nahavandi, Thanh Tam Nguyen, Quoc-Viet Pham, Cuong M. Nguyen
- **Comment**: None
- **Journal**: Computer Vision and Image Understanding, 223 (2022) 103525
- **Summary**: Deep learning has been successfully applied to solve various complex problems ranging from big data analytics to computer vision and human-level control. Deep learning advances however have also been employed to create software that can cause threats to privacy, democracy and national security. One of those deep learning-powered applications recently emerged is deepfake. Deepfake algorithms can create fake images and videos that humans cannot distinguish them from authentic ones. The proposal of technologies that can automatically detect and assess the integrity of digital visual media is therefore indispensable. This paper presents a survey of algorithms used to create deepfakes and, more importantly, methods proposed to detect deepfakes in the literature to date. We present extensive discussions on challenges, research trends and directions related to deepfake technologies. By reviewing the background of deepfakes and state-of-the-art deepfake detection methods, this study provides a comprehensive overview of deepfake techniques and facilitates the development of new and more robust methods to deal with the increasingly challenging deepfakes.



### MIC: Mining Interclass Characteristics for Improved Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.11574v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.11574v1)
- **Published**: 2019-09-25 16:04:27+00:00
- **Updated**: 2019-09-25 16:04:27+00:00
- **Authors**: Karsten Roth, Biagio Brattoli, Björn Ommer
- **Comment**: 8 pages, 10 figures, accepted to ICCV 2019
- **Journal**: None
- **Summary**: Metric learning seeks to embed images of objects suchthat class-defined relations are captured by the embeddingspace. However, variability in images is not just due to different depicted object classes, but also depends on other latent characteristics such as viewpoint or illumination. In addition to these structured properties, random noise further obstructs the visual relations of interest. The common approach to metric learning is to enforce a representation that is invariant under all factors but the ones of interest. In contrast, we propose to explicitly learn the latent characteristics that are shared by and go across object classes. We can then directly explain away structured visual variability, rather than assuming it to be unknown random noise. We propose a novel surrogate task to learn visual characteristics shared across classes with a separate encoder. This encoder is trained jointly with the encoder for class information by reducing their mutual information. On five standard image retrieval benchmarks the approach significantly improves upon the state-of-the-art.



### A Closer Look at Domain Shift for Deep Learning in Histopathology
- **Arxiv ID**: http://arxiv.org/abs/1909.11575v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.11575v2)
- **Published**: 2019-09-25 16:06:05+00:00
- **Updated**: 2019-09-26 03:20:50+00:00
- **Authors**: Karin Stacke, Gabriel Eilertsen, Jonas Unger, Claes Lundström
- **Comment**: 8 pages, 4 figures. Accepted to COMPAY2019: Second MICCAI Workshop on
  Computational Pathology
- **Journal**: None
- **Summary**: Domain shift is a significant problem in histopathology. There can be large differences in data characteristics of whole-slide images between medical centers and scanners, making generalization of deep learning to unseen data difficult. To gain a better understanding of the problem, we present a study on convolutional neural networks trained for tumor classification of H&E stained whole-slide images. We analyze how augmentation and normalization strategies affect performance and learned representations, and what features a trained model respond to. Most centrally, we present a novel measure for evaluating the distance between domains in the context of the learned representation of a particular model. This measure can reveal how sensitive a model is to domain variations, and can be used to detect new data that a model will have problems generalizing to. The results show how learning is heavily influenced by the preparation of training data, and that the latent representation used to do classification is sensitive to changes in data distribution, especially when training without augmentation or normalization.



### Communications and Networking Technologies for Intelligent Drone Cruisers
- **Arxiv ID**: http://arxiv.org/abs/1910.05309v1
- **DOI**: 10.1109/GCWkshps45667.2019.9024679
- **Categories**: **cs.NI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.05309v1)
- **Published**: 2019-09-25 16:22:29+00:00
- **Updated**: 2019-09-25 16:22:29+00:00
- **Authors**: Li-Chun Wang, Chuan-Chi Lai, Hong-Han Shuai, Hsin-Piao Lin, Chi-Yu Li, Teng-Hu Cheng, Chiun-Hsun Chen
- **Comment**: 6 pages, 11 figures, accepted by 2019 IEEE Globecom Workshops (GC
  Wkshps): IEEE GLOBECOM 2019 Workshop on Space-Ground Integrated Networks
- **Journal**: None
- **Summary**: Future mobile communication networks require an Aerial Base Station (ABS) with fast mobility and long-term hovering capabilities. At present, unmanned aerial vehicles (UAV) or drones do not have long flight times and are mainly used for monitoring, surveillance, and image post-processing. On the other hand, the traditional airship is too large and not easy to take off and land. Therefore, we propose to develop an "Artificial Intelligence (AI) Drone-Cruiser" base station that can help 5G mobile communication systems and beyond quickly recover the network after a disaster and handle the instant communications by the flash crowd. The drone-cruiser base station can overcome the communications problem for three types of flash crowds, such as in stadiums, parades, and large plaza so that an appropriate number of aerial base stations can be accurately deployed to meet large and dynamic traffic demands. Artificial intelligence can solve these problems by analyzing the collected data, and then adjust the system parameters in the framework of Self-Organizing Network (SON) to achieve the goals of self-configuration, self-optimization, and self-healing. With the help of AI technologies, 5G networks can become more intelligent. This paper aims to provide a new type of service, On-Demand Aerial Base Station as a Service. This work needs to overcome the following five technical challenges: innovative design of drone-cruisers for the long-time hovering, crowd estimation and prediction, rapid 3D wireless channel learning and modeling, 3D placement of aerial base stations and the integration of WiFi front-haul and millimeter wave/WiGig back-haul networks.



### Deep Predictive Motion Tracking in Magnetic Resonance Imaging: Application to Fetal Imaging
- **Arxiv ID**: http://arxiv.org/abs/1909.11625v3
- **DOI**: 10.1109/TMI.2020.2998600
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/1909.11625v3)
- **Published**: 2019-09-25 17:12:40+00:00
- **Updated**: 2020-06-06 23:15:28+00:00
- **Authors**: Ayush Singh, Seyed Sadegh Mohseni Salehi, Ali Gholipour
- **Comment**: The article has been published in IEEE TMI: 14 pages, 11 figures, 2
  tables and 1 supplementary
  https://github.com/bchimagine/DeepPredictiveMotionTracking
- **Journal**: None
- **Summary**: Fetal magnetic resonance imaging (MRI) is challenged by uncontrollable, large, and irregular fetal movements. It is, therefore, performed through visual monitoring of fetal motion and repeated acquisitions to ensure diagnostic-quality images are acquired. Nevertheless, visual monitoring of fetal motion based on displayed slices, and navigation at the level of stacks-of-slices is inefficient. The current process is highly operator-dependent, increases scanner usage and cost, and significantly increases the length of fetal MRI scans which makes them hard to tolerate for pregnant women. To help build automatic MRI motion tracking and navigation systems to overcome the limitations of the current process and improve fetal imaging, we have developed a new real time image-based motion tracking method based on deep learning that learns to predict fetal motion directly from acquired images. Our method is based on a recurrent neural network, composed of spatial and temporal encoder-decoders, that infers motion parameters from anatomical features extracted from sequences of acquired slices. We compared our trained network on held out test sets (including data with different characteristics, e.g. different fetuses scanned at different ages, and motion trajectories recorded from volunteer subjects) with networks designed for estimation as well as methods adopted to make predictions. The results show that our method outperformed alternative techniques, and achieved real-time performance with average errors of 3.5 and 8 degrees for the estimation and prediction tasks, respectively. Our real-time deep predictive motion tracking technique can be used to assess fetal movements, to guide slice acquisitions, and to build navigation systems for fetal MRI.



### Explicitly disentangling image content from translation and rotation with spatial-VAE
- **Arxiv ID**: http://arxiv.org/abs/1909.11663v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1909.11663v1)
- **Published**: 2019-09-25 17:17:30+00:00
- **Updated**: 2019-09-25 17:17:30+00:00
- **Authors**: Tristan Bepler, Ellen D. Zhong, Kotaro Kelley, Edward Brignole, Bonnie Berger
- **Comment**: 11 pages, 6 figures, to appear in the 33rd Conference on Neural
  Information Processing Systems (NeurIPS 2019)
- **Journal**: None
- **Summary**: Given an image dataset, we are often interested in finding data generative factors that encode semantic content independently from pose variables such as rotation and translation. However, current disentanglement approaches do not impose any specific structure on the learned latent representations. We propose a method for explicitly disentangling image rotation and translation from other unstructured latent factors in a variational autoencoder (VAE) framework. By formulating the generative model as a function of the spatial coordinate, we make the reconstruction error differentiable with respect to latent translation and rotation parameters. This formulation allows us to train a neural network to perform approximate inference on these latent variables while explicitly constraining them to only represent rotation and translation. We demonstrate that this framework, termed spatial-VAE, effectively learns latent representations that disentangle image rotation and translation from content and improves reconstruction over standard VAEs on several benchmark datasets, including applications to modeling continuous 2-D views of proteins from single particle electron microscopy and galaxies in astronomical images.



### Non-negative Tensor Patch Dictionary Approaches for Image Compression and Deblurring Applications
- **Arxiv ID**: http://arxiv.org/abs/1910.00993v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA, (primary) 65F22 (secondary) 65F99, 65N20, 65N21
- **Links**: [PDF](http://arxiv.org/pdf/1910.00993v1)
- **Published**: 2019-09-25 18:24:22+00:00
- **Updated**: 2019-09-25 18:24:22+00:00
- **Authors**: Elizabeth Newman, Misha E. Kilmer
- **Comment**: 25 pages, 12 pages
- **Journal**: None
- **Summary**: In recent work (Soltani, Kilmer, Hansen, BIT 2016), an algorithm for non-negative tensor patch dictionary learning in the context of X-ray CT imaging and based on a tensor-tensor product called the $t$-product (Kilmer and Martin, 2011) was presented. Building on that work, in this paper, we use of non-negative tensor patch-based dictionaries trained on other data, such as facial image data, for the purposes of either compression or image deblurring. We begin with an analysis in which we address issues such as suitability of the tensor-based approach relative to a matrix-based approach, dictionary size and patch size to balance computational efficiency and qualitative representations. Next, we develop an algorithm that is capable of recovering non-negative tensor coefficients given a non-negative tensor dictionary. The algorithm is based on a variant of the Modified Residual Norm Steepest Descent method. We show how to augment the algorithm to enforce sparsity in the tensor coefficients, and note that the approach has broader applicability since it can be applied to the matrix case as well. We illustrate the surprising result that dictionaries trained on image data from one class can be successfully used to represent and compress image data from different classes and across different resolutions. Finally, we address the use of non-negative tensor dictionaries in image deblurring. We show that tensor treatment of the deblurring problem coupled with non-negative tensor patch dictionaries can give superior restorations as compared to standard treatment of the non-negativity constrained deblurring problem.



### Deep-learning-based Breast CT for Radiation Dose Reduction
- **Arxiv ID**: http://arxiv.org/abs/1909.11721v1
- **DOI**: 10.1117/12.2530234
- **Categories**: **physics.med-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.11721v1)
- **Published**: 2019-09-25 19:30:08+00:00
- **Updated**: 2019-09-25 19:30:08+00:00
- **Authors**: Wenxiang Cong, Hongming Shan, Xiaohua Zhang, Shaohua Liu, Ruola Ning, Ge Wang
- **Comment**: 7 pages, 4 figures
- **Journal**: None
- **Summary**: Cone-beam breast computed tomography (CT) provides true 3D breast images with isotropic resolution and high-contrast information, detecting calcifications as small as a few hundred microns and revealing subtle tissue differences. However, breast is highly sensitive to x-ray radiation. It is critically important for healthcare to reduce radiation dose. Few-view cone-beam CT only uses a fraction of x-ray projection data acquired by standard cone-beam breast CT, enabling significant reduction of the radiation dose. However, insufficient sampling data would cause severe streak artifacts in CT images reconstructed using conventional methods. In this study, we propose a deep-learning-based method to establish a residual neural network model for the image reconstruction, which is applied for few-view breast CT to produce high quality breast CT images. We respectively evaluate the deep-learning-based image reconstruction using one third and one quarter of x-ray projection views of the standard cone-beam breast CT. Based on clinical breast imaging dataset, we perform a supervised learning to train the neural network from few-view CT images to corresponding full-view CT images. Experimental results show that the deep learning-based image reconstruction method allows few-view breast CT to achieve a radiation dose <6 mGy per cone-beam CT scan, which is a threshold set by FDA for mammographic screening.



### Revisiting Knowledge Distillation via Label Smoothing Regularization
- **Arxiv ID**: http://arxiv.org/abs/1909.11723v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.11723v3)
- **Published**: 2019-09-25 19:33:43+00:00
- **Updated**: 2021-03-04 08:02:53+00:00
- **Authors**: Li Yuan, Francis E. H. Tay, Guilin Li, Tao Wang, Jiashi Feng
- **Comment**: CVPR2020 Oral, codes:
  https://github.com/yuanli2333/Teacher-free-Knowledge-Distillation
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition 2020
- **Summary**: Knowledge Distillation (KD) aims to distill the knowledge of a cumbersome teacher model into a lightweight student model. Its success is generally attributed to the privileged information on similarities among categories provided by the teacher model, and in this sense, only strong teacher models are deployed to teach weaker students in practice. In this work, we challenge this common belief by following experimental observations: 1) beyond the acknowledgment that the teacher can improve the student, the student can also enhance the teacher significantly by reversing the KD procedure; 2) a poorly-trained teacher with much lower accuracy than the student can still improve the latter significantly. To explain these observations, we provide a theoretical analysis of the relationships between KD and label smoothing regularization. We prove that 1) KD is a type of learned label smoothing regularization and 2) label smoothing regularization provides a virtual teacher model for KD. From these results, we argue that the success of KD is not fully due to the similarity information between categories from teachers, but also to the regularization of soft targets, which is equally or even more important.   Based on these analyses, we further propose a novel Teacher-free Knowledge Distillation (Tf-KD) framework, where a student model learns from itself or manuallydesigned regularization distribution. The Tf-KD achieves comparable performance with normal KD from a superior teacher, which is well applied when a stronger teacher model is unavailable. Meanwhile, Tf-KD is generic and can be directly deployed for training deep neural networks. Without any extra computation cost, Tf-KD achieves up to 0.65\% improvement on ImageNet over well-established baseline models, which is superior to label smoothing regularization.



### "Good Robot!": Efficient Reinforcement Learning for Multi-Step Visual Tasks with Sim to Real Transfer
- **Arxiv ID**: http://arxiv.org/abs/1909.11730v4
- **DOI**: 10.1109/LRA.2020.3015448
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.11730v4)
- **Published**: 2019-09-25 19:50:36+00:00
- **Updated**: 2020-08-15 18:10:40+00:00
- **Authors**: Andrew Hundt, Benjamin Killeen, Nicholas Greene, Hongtao Wu, Heeyeon Kwon, Chris Paxton, Gregory D. Hager
- **Comment**: Accepted to the journal IEEE Robotics and Automation Letters (RA-L)
  and to be presented at IROS 2020. This is a minor update to v3. 8 pages, 6
  figures, 3 tables, 1 algorithm. Code is available at
  https://github.com/jhu-lcsr/good_robot and a video overview is at
  https://youtu.be/MbCuEZadkIw
- **Journal**: None
- **Summary**: Current Reinforcement Learning (RL) algorithms struggle with long-horizon tasks where time can be wasted exploring dead ends and task progress may be easily reversed. We develop the SPOT framework, which explores within action safety zones, learns about unsafe regions without exploring them, and prioritizes experiences that reverse earlier progress to learn with remarkable efficiency.   The SPOT framework successfully completes simulated trials of a variety of tasks, improving a baseline trial success rate from 13% to 100% when stacking 4 cubes, from 13% to 99% when creating rows of 4 cubes, and from 84% to 95% when clearing toys arranged in adversarial patterns. Efficiency with respect to actions per trial typically improves by 30% or more, while training takes just 1-20k actions, depending on the task.   Furthermore, we demonstrate direct sim to real transfer. We are able to create real stacks in 100% of trials with 61% efficiency and real rows in 100% of trials with 59% efficiency by directly loading the simulation-trained model on the real robot with no additional real-world fine-tuning. To our knowledge, this is the first instance of reinforcement learning with successful sim to real transfer applied to long term multi-step tasks such as block-stacking and row-making with consideration of progress reversal. Code is available at https://github.com/jhu-lcsr/good_robot .



### Learning Pixel Representations for Generic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.11735v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.11735v1)
- **Published**: 2019-09-25 19:56:29+00:00
- **Updated**: 2019-09-25 19:56:29+00:00
- **Authors**: Oran Shayer, Michael Lindenbaum
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning approaches to generic (non-semantic) segmentation have so far been indirect and relied on edge detection. This is in contrast to semantic segmentation, where DNNs are applied directly. We propose an alternative approach called Deep Generic Segmentation (DGS) and try to follow the path used for semantic segmentation. Our main contribution is a new method for learning a pixel-wise representation that reflects segment relatedness. This representation is combined with a CRF to yield the segmentation algorithm. We show that we are able to learn meaningful representations that improve segmentation quality and that the representations themselves achieve state-of-the-art segment similarity scores. The segmentation results are competitive and promising.



### UNITER: UNiversal Image-TExt Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.11740v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.11740v3)
- **Published**: 2019-09-25 20:02:54+00:00
- **Updated**: 2020-07-17 22:19:59+00:00
- **Authors**: Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Joint image-text embedding is the bedrock for most Vision-and-Language (V+L) tasks, where multimodality inputs are simultaneously processed for joint visual and textual understanding. In this paper, we introduce UNITER, a UNiversal Image-TExt Representation, learned through large-scale pre-training over four image-text datasets (COCO, Visual Genome, Conceptual Captions, and SBU Captions), which can power heterogeneous downstream V+L tasks with joint multimodal embeddings. We design four pre-training tasks: Masked Language Modeling (MLM), Masked Region Modeling (MRM, with three variants), Image-Text Matching (ITM), and Word-Region Alignment (WRA). Different from previous work that applies joint random masking to both modalities, we use conditional masking on pre-training tasks (i.e., masked language/region modeling is conditioned on full observation of image/text). In addition to ITM for global image-text alignment, we also propose WRA via the use of Optimal Transport (OT) to explicitly encourage fine-grained alignment between words and image regions during pre-training. Comprehensive analysis shows that both conditional masking and OT-based WRA contribute to better pre-training. We also conduct a thorough ablation study to find an optimal combination of pre-training tasks. Extensive experiments show that UNITER achieves new state of the art across six V+L tasks (over nine datasets), including Visual Question Answering, Image-Text Retrieval, Referring Expression Comprehension, Visual Commonsense Reasoning, Visual Entailment, and NLVR$^2$. Code is available at https://github.com/ChenRocks/UNITER.



### Data consistency networks for (calibration-less) accelerated parallel MR image reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1909.11795v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.11795v1)
- **Published**: 2019-09-25 22:15:56+00:00
- **Updated**: 2019-09-25 22:15:56+00:00
- **Authors**: Jo Schlemper, Jinming Duan, Cheng Ouyang, Chen Qin, Jose Caballero, Joseph V. Hajnal, Daniel Rueckert
- **Comment**: Presented at ISMRM 27th Annual Meeting & Exhibition (Abstract #4663)
- **Journal**: None
- **Summary**: We present simple reconstruction networks for multi-coil data by extending deep cascade of CNN's and exploiting the data consistency layer. In particular, we propose two variants, where one is inspired by POCSENSE and the other is calibration-less. We show that the proposed approaches are competitive relative to the state of the art both quantitatively and qualitatively.



### Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task Learning and ArcFace
- **Arxiv ID**: http://arxiv.org/abs/1910.04855v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.04855v1)
- **Published**: 2019-09-25 22:45:18+00:00
- **Updated**: 2019-09-25 22:45:18+00:00
- **Authors**: Dimitrios Kollias, Stefanos Zafeiriou
- **Comment**: oral presentation in BMVC 2019
- **Journal**: None
- **Summary**: Affective computing has been largely limited in terms of available data resources. The need to collect and annotate diverse in-the-wild datasets has become apparent with the rise of deep learning models, as the default approach to address any computer vision task. Some in-the-wild databases have been recently proposed. However: i) their size is small, ii) they are not audiovisual, iii) only a small part is manually annotated, iv) they contain a small number of subjects, or v) they are not annotated for all main behavior tasks (valence-arousal estimation, action unit detection and basic expression classification). To address these, we substantially extend the largest available in-the-wild database (Aff-Wild) to study continuous emotions such as valence and arousal. Furthermore, we annotate parts of the database with basic expressions and action units. As a consequence, for the first time, this allows the joint study of all three types of behavior states. We call this database Aff-Wild2. We conduct extensive experiments with CNN and CNN-RNN architectures that use visual and audio modalities; these networks are trained on Aff-Wild2 and their performance is then evaluated on 10 publicly available emotion databases. We show that the networks achieve state-of-the-art performance for the emotion recognition tasks. Additionally, we adapt the ArcFace loss function in the emotion recognition context and use it for training two new networks on Aff-Wild2 and then re-train them in a variety of diverse expression recognition databases. The networks are shown to improve the existing state-of-the-art. The database, emotion recognition models and source code are available at http://ibug.doc.ic.ac.uk/resources/aff-wild2.



### A fast, complete, point cloud based loop closure for LiDAR odometry and mapping
- **Arxiv ID**: http://arxiv.org/abs/1909.11811v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.11811v1)
- **Published**: 2019-09-25 23:25:23+00:00
- **Updated**: 2019-09-25 23:25:23+00:00
- **Authors**: Jiarong Lin, Fu Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a loop closure method to correct the long-term drift in LiDAR odometry and mapping (LOAM). Our proposed method computes the 2D histogram of keyframes, a local map patch, and uses the normalized cross-correlation of the 2D histograms as the similarity metric between the current keyframe and those in the map. We show that this method is fast, invariant to rotation, and produces reliable and accurate loop detection. The proposed method is implemented with careful engineering and integrated into the LOAM algorithm, forming a complete and practical system ready to use. To benefit the community by serving a benchmark for loop closure, the entire system is made open source on Github



### LAVAE: Disentangling Location and Appearance
- **Arxiv ID**: http://arxiv.org/abs/1909.11813v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.11813v2)
- **Published**: 2019-09-25 23:33:14+00:00
- **Updated**: 2019-09-27 00:10:09+00:00
- **Authors**: Andrea Dittadi, Ole Winther
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a probabilistic generative model for unsupervised learning of structured, interpretable, object-based representations of visual scenes. We use amortized variational inference to train the generative model end-to-end. The learned representations of object location and appearance are fully disentangled, and objects are represented independently of each other in the latent space. Unlike previous approaches that disentangle location and appearance, ours generalizes seamlessly to scenes with many more objects than encountered in the training regime. We evaluate the proposed model on multi-MNIST and multi-dSprites data sets.



