# Arxiv Papers in cs.CV on 2019-09-11
### Hybrid Cascaded Neural Network for Liver Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.04797v3
- **DOI**: 10.1109/ISBI45749.2020.9098656
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.04797v3)
- **Published**: 2019-09-11 00:11:14+00:00
- **Updated**: 2019-10-08 05:25:43+00:00
- **Authors**: Raunak Dey, Yi Hong
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic liver lesion segmentation is a challenging task while having a significant impact on assisting medical professionals in the designing of effective treatment and planning proper care. In this paper we propose a cascaded system that combines both 2D and 3D convolutional neural networks to effectively segment hepatic lesions. Our 2D network operates on a slice by slice basis to segment the liver and larger tumors, while we use a 3D network to detect small lesions that are often missed in a 2D segmentation design. We employ this algorithm on the LiTS challenge obtaining a Dice score per case of 68.1%, which performs the best among all non pre-trained models and the second best among published methods. We also perform two-fold cross-validation to reveal the over- and under-segmentation issues in the LiTS annotations.



### Probabilistic framework for solving Visual Dialog
- **Arxiv ID**: http://arxiv.org/abs/1909.04800v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1909.04800v2)
- **Published**: 2019-09-11 00:25:12+00:00
- **Updated**: 2019-10-17 07:30:39+00:00
- **Authors**: Badri N. Patro, Anupriy, Vinay P. Namboodiri
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a probabilistic framework for solving the task of `Visual Dialog'. Solving this task requires reasoning and understanding of visual modality, language modality, and common sense knowledge to answer. Various architectures have been proposed to solve this task by variants of multi-modal deep learning techniques that combine visual and language representations. However, we believe that it is crucial to understand and analyze the sources of uncertainty for solving this task. Our approach allows for estimating uncertainty and also aids a diverse generation of answers. The proposed approach is obtained through a probabilistic representation module that provides us with representations for image, question and conversation history, a module that ensures that diverse latent representations for candidate answers are obtained given the probabilistic representations and an uncertainty representation module that chooses the appropriate answer that minimizes uncertainty. We thoroughly evaluate the model with a detailed ablation analysis, comparison with state of the art and visualization of the uncertainty that aids in the understanding of the method. Using the proposed probabilistic framework, we thus obtain an improved visual dialog system that is also more explainable.



### Variable Rate Deep Image Compression With a Conditional Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1909.04802v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.04802v1)
- **Published**: 2019-09-11 00:33:52+00:00
- **Updated**: 2019-09-11 00:33:52+00:00
- **Authors**: Yoojin Choi, Mostafa El-Khamy, Jungwon Lee
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: In this paper, we propose a novel variable-rate learned image compression framework with a conditional autoencoder. Previous learning-based image compression methods mostly require training separate networks for different compression rates so they can yield compressed images of varying quality. In contrast, we train and deploy only one variable-rate image compression network implemented with a conditional autoencoder. We provide two rate control parameters, i.e., the Lagrange multiplier and the quantization bin size, which are given as conditioning variables to the network. Coarse rate adaptation to a target is performed by changing the Lagrange multiplier, while the rate can be further fine-tuned by adjusting the bin size used in quantizing the encoded representation. Our experimental results show that the proposed scheme provides a better rate-distortion trade-off than the traditional variable-rate image compression codecs such as JPEG2000 and BPG. Our model also shows comparable and sometimes better performance than the state-of-the-art learned image compression models that deploy multiple networks trained for varying rates.



### Adaptive Wasserstein Hourglass for Weakly Supervised Hand Pose Estimation from Monocular RGB
- **Arxiv ID**: http://arxiv.org/abs/1909.05666v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1909.05666v1)
- **Published**: 2019-09-11 01:26:19+00:00
- **Updated**: 2019-09-11 01:26:19+00:00
- **Authors**: Yumeng Zhang, Li Chen, Yufeng Liu, Junhai Yong, Wen Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Insufficient labeled training datasets is one of the bottlenecks of 3D hand pose estimation from monocular RGB images. Synthetic datasets have a large number of images with precise annotations, but the obvious difference with real-world datasets impacts the generalization. Little work has been done to bridge the gap between two domains over their wide difference. In this paper, we propose a domain adaptation method called Adaptive Wasserstein Hourglass (AW Hourglass) for weakly-supervised 3D hand pose estimation, which aims to distinguish the difference and explore the common characteristics (e.g. hand structure) of synthetic and real-world datasets. Learning the common characteristics helps the network focus on pose-related information. The similarity of the characteristics makes it easier to enforce domain-invariant constraints. During training, based on the relation between these common characteristics and 3D pose learned from fully-annotated synthetic datasets, it is beneficial for the network to restore the 3D pose of weakly labeled real-world datasets with the aid of 2D annotations and depth images. While in testing, the network predicts the 3D pose with the input of RGB.



### Antipodal Robotic Grasping using Generative Residual Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1909.04810v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.04810v4)
- **Published**: 2019-09-11 01:44:07+00:00
- **Updated**: 2021-06-10 20:11:46+00:00
- **Authors**: Sulabh Kumra, Shirin Joshi, Ferat Sahin
- **Comment**: 8 pages, 5 figures, IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS) 2020
- **Journal**: None
- **Summary**: In this paper, we present a modular robotic system to tackle the problem of generating and performing antipodal robotic grasps for unknown objects from n-channel image of the scene. We propose a novel Generative Residual Convolutional Neural Network (GR-ConvNet) model that can generate robust antipodal grasps from n-channel input at real-time speeds (~20ms). We evaluate the proposed model architecture on standard datasets and a diverse set of household objects. We achieved state-of-the-art accuracy of 97.7% and 94.6% on Cornell and Jacquard grasping datasets respectively. We also demonstrate a grasp success rate of 95.4% and 93% on household and adversarial objects respectively using a 7 DoF robotic arm.



### Dual-attention Focused Module for Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/1909.04813v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/1909.04813v1)
- **Published**: 2019-09-11 01:49:25+00:00
- **Updated**: 2019-09-11 01:49:25+00:00
- **Authors**: Yukun Zhou, Zailiang Chen, Hailan Shen, Qing Liu, Rongchang Zhao, Yixiong Liang
- **Comment**: 8 pages, 6 figures and 4 tables
- **Journal**: None
- **Summary**: The research on recognizing the most discriminative regions provides referential information for weakly supervised object localization with only image-level annotations. However, the most discriminative regions usually conceal the other parts of the object, thereby impeding entire object recognition and localization. To tackle this problem, the Dual-attention Focused Module (DFM) is proposed to enhance object localization performance. Specifically, we present a dual attention module for information fusion, consisting of a position branch and a channel one. In each branch, the input feature map is deduced into an enhancement map and a mask map, thereby highlighting the most discriminative parts or hiding them. For the position mask map, we introduce a focused matrix to enhance it, which utilizes the principle that the pixels of an object are continuous. Between these two branches, the enhancement map is integrated with the mask map, aiming at partially compensating the lost information and diversifies the features. With the dual-attention module and focused matrix, the entire object region could be precisely recognized with implicit information. We demonstrate outperforming results of DFM in experiments. In particular, DFM achieves state-of-the-art performance in localization accuracy in ILSVRC 2016 and CUB-200-2011.



### Identifying and Resisting Adversarial Videos Using Temporal Consistency
- **Arxiv ID**: http://arxiv.org/abs/1909.04837v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.04837v1)
- **Published**: 2019-09-11 03:11:52+00:00
- **Updated**: 2019-09-11 03:11:52+00:00
- **Authors**: Xiaojun Jia, Xingxing Wei, Xiaochun Cao
- **Comment**: None
- **Journal**: None
- **Summary**: Video classification is a challenging task in computer vision. Although Deep Neural Networks (DNNs) have achieved excellent performance in video classification, recent research shows adding imperceptible perturbations to clean videos can make the well-trained models output wrong labels with high confidence. In this paper, we propose an effective defense framework to characterize and defend adversarial videos. The proposed method contains two phases: (1) adversarial video detection using temporal consistency between adjacent frames, and (2) adversarial perturbation reduction via denoisers in the spatial and temporal domains respectively. Specifically, because of the linear nature of DNNs, the imperceptible perturbations will enlarge with the increasing of DNNs depth, which leads to the inconsistency of DNNs output between adjacent frames. However, the benign video frames often have the same outputs with their neighbor frames owing to the slight changes. Based on this observation, we can distinguish between adversarial videos and benign videos. After that, we utilize different defense strategies against different attacks. We propose the temporal defense, which reconstructs the polluted frames with their temporally neighbor clean frames, to deal with the adversarial videos with sparse polluted frames. For the videos with dense polluted frames, we use an efficient adversarial denoiser to process each frame in the spatial domain, and thus purify the perturbations (we call it as spatial defense). A series of experiments conducted on the UCF-101 dataset demonstrate that the proposed method significantly improves the robustness of video classifiers against adversarial attacks.



### PDA: Progressive Data Augmentation for General Robustness of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.04839v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.04839v3)
- **Published**: 2019-09-11 03:27:54+00:00
- **Updated**: 2020-02-24 11:58:05+00:00
- **Authors**: Hang Yu, Aishan Liu, Xianglong Liu, Gengchao Li, Ping Luo, Ran Cheng, Jichen Yang, Chongzhi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial images are designed to mislead deep neural networks (DNNs), attracting great attention in recent years. Although several defense strategies achieved encouraging robustness against adversarial samples, most of them fail to improve the robustness on common corruptions such as noise, blur, and weather/digital effects (e.g. frost, pixelate). To address this problem, we propose a simple yet effective method, named Progressive Data Augmentation (PDA), which enables general robustness of DNNs by progressively injecting diverse adversarial noises during training. In other words, DNNs trained with PDA are able to obtain more robustness against both adversarial attacks as well as common corruptions than the recent state-of-the-art methods. We also find that PDA is more efficient than prior arts and able to prevent accuracy drop on clean samples without being attacked. Furthermore, we theoretically show that PDA can control the perturbation bound and guarantee better generalization ability than existing work. Extensive experiments on many benchmarks such as CIFAR-10, SVHN, and ImageNet demonstrate that PDA significantly outperforms its counterparts in various experimental setups.



### PDANet: Polarity-consistent Deep Attention Network for Fine-grained Visual Emotion Regression
- **Arxiv ID**: http://arxiv.org/abs/1909.05693v1
- **DOI**: 10.1145/3343031.3351062
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1909.05693v1)
- **Published**: 2019-09-11 05:16:36+00:00
- **Updated**: 2019-09-11 05:16:36+00:00
- **Authors**: Sicheng Zhao, Zizhou Jia, Hui Chen, Leida Li, Guiguang Ding, Kurt Keutzer
- **Comment**: Accepted by ACM Multimedia 2019
- **Journal**: None
- **Summary**: Existing methods on visual emotion analysis mainly focus on coarse-grained emotion classification, i.e. assigning an image with a dominant discrete emotion category. However, these methods cannot well reflect the complexity and subtlety of emotions. In this paper, we study the fine-grained regression problem of visual emotions based on convolutional neural networks (CNNs). Specifically, we develop a Polarity-consistent Deep Attention Network (PDANet), a novel network architecture that integrates attention into a CNN with an emotion polarity constraint. First, we propose to incorporate both spatial and channel-wise attentions into a CNN for visual emotion regression, which jointly considers the local spatial connectivity patterns along each channel and the interdependency between different channels. Second, we design a novel regression loss, i.e. polarity-consistent regression (PCR) loss, based on the weakly supervised emotion polarity to guide the attention generation. By optimizing the PCR loss, PDANet can generate a polarity preserved attention map and thus improve the emotion regression performance. Extensive experiments are conducted on the IAPS, NAPS, and EMOTIC datasets, and the results demonstrate that the proposed PDANet outperforms the state-of-the-art approaches by a large margin for fine-grained visual emotion regression. Our source code is released at: https://github.com/ZizhouJia/PDANet.



### Deep Elastic Networks with Model Selection for Multi-Task Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.04860v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.04860v1)
- **Published**: 2019-09-11 05:46:58+00:00
- **Updated**: 2019-09-11 05:46:58+00:00
- **Authors**: Chanho Ahn, Eunwoo Kim, Songhwai Oh
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: In this work, we consider the problem of instance-wise dynamic network model selection for multi-task learning. To this end, we propose an efficient approach to exploit a compact but accurate model in a backbone architecture for each instance of all tasks. The proposed method consists of an estimator and a selector. The estimator is based on a backbone architecture and structured hierarchically. It can produce multiple different network models of different configurations in a hierarchical structure. The selector chooses a model dynamically from a pool of candidate models given an input instance. The selector is a relatively small-size network consisting of a few layers, which estimates a probability distribution over the candidate models when an input instance of a task is given. Both estimator and selector are jointly trained in a unified learning framework in conjunction with a sampling-based learning strategy, without additional computation steps. We demonstrate the proposed approach for several image classification tasks compared to existing approaches performing model selection or learning multiple tasks. Experimental results show that our approach gives not only outstanding performance compared to other competitors but also the versatility to perform instance-wise model selection for multiple tasks.



### Deep Declarative Networks: A New Hope
- **Arxiv ID**: http://arxiv.org/abs/1909.04866v2
- **DOI**: 10.1109/TPAMI.2021.3059462
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.04866v2)
- **Published**: 2019-09-11 06:19:25+00:00
- **Updated**: 2020-02-27 03:56:39+00:00
- **Authors**: Stephen Gould, Richard Hartley, Dylan Campbell
- **Comment**: None
- **Journal**: None
- **Summary**: We explore a new class of end-to-end learnable models wherein data processing nodes (or network layers) are defined in terms of desired behavior rather than an explicit forward function. Specifically, the forward function is implicitly defined as the solution to a mathematical optimization problem. Consistent with nomenclature in the programming languages community, we name these models deep declarative networks. Importantly, we show that the class of deep declarative networks subsumes current deep learning models. Moreover, invoking the implicit function theorem, we show how gradients can be back-propagated through many declaratively defined data processing nodes thereby enabling end-to-end learning. We show how these declarative processing nodes can be implemented in the popular PyTorch deep learning software library allowing declarative and imperative nodes to co-exist within the same network. We also provide numerous insights and illustrative examples of declarative nodes and demonstrate their application for image and point cloud classification tasks.



### Is Heuristic Sampling Necessary in Training Deep Object Detectors?
- **Arxiv ID**: http://arxiv.org/abs/1909.04868v8
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.04868v8)
- **Published**: 2019-09-11 06:29:58+00:00
- **Updated**: 2021-08-11 10:18:32+00:00
- **Authors**: Joya Chen, Dong Liu, Tong Xu, Shiwei Wu, Yifei Cheng, Enhong Chen
- **Comment**: Accepted by IEEE Transactions on Image Processing (14 pages)
- **Journal**: None
- **Summary**: To train accurate deep object detectors under the extreme foreground-background imbalance, heuristic sampling methods are always necessary, which either re-sample a subset of all training samples (hard sampling methods, \eg biased sampling, OHEM), or use all training samples but re-weight them discriminatively (soft sampling methods, \eg Focal Loss, GHM). In this paper, we challenge the necessity of such hard/soft sampling methods for training accurate deep object detectors. While previous studies have shown that training detectors without heuristic sampling methods would significantly degrade accuracy, we reveal that this degradation comes from an unreasonable classification gradient magnitude caused by the imbalance, rather than a lack of re-sampling/re-weighting. Motivated by our discovery, we propose a simple yet effective \emph{Sampling-Free} mechanism to achieve a reasonable classification gradient magnitude by initialization and loss scaling. Unlike heuristic sampling methods with multiple hyperparameters, our Sampling-Free mechanism is fully data diagnostic, without laborious hyperparameters searching. We verify the effectiveness of our method in training anchor-based and anchor-free object detectors, where our method always achieves higher detection accuracy than heuristic sampling methods on COCO and PASCAL VOC datasets. Our Sampling-Free mechanism provides a new perspective to address the foreground-background imbalance. Our code is released at \url{https://github.com/ChenJoya/sampling-free}.



### Distortion-adaptive Salient Object Detection in 360$^\circ$ Omnidirectional Images
- **Arxiv ID**: http://arxiv.org/abs/1909.04913v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.04913v1)
- **Published**: 2019-09-11 08:33:11+00:00
- **Updated**: 2019-09-11 08:33:11+00:00
- **Authors**: Jia Li, Jinming Su, Changqun Xia, Yonghong Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Image-based salient object detection (SOD) has been extensively explored in the past decades. However, SOD on 360$^\circ$ omnidirectional images is less studied owing to the lack of datasets with pixel-level annotations. Toward this end, this paper proposes a 360$^\circ$ image-based SOD dataset that contains 500 high-resolution equirectangular images. We collect the representative equirectangular images from five mainstream 360$^\circ$ video datasets and manually annotate all objects and regions over these images with precise masks with a free-viewpoint way. To the best of our knowledge, it is the first public available dataset for salient object detection on 360$^\circ$ scenes. By observing this dataset, we find that distortion from projection, large-scale complex scene and small salient objects are the most prominent characteristics. Inspired by these foundings, this paper proposes a baseline model for SOD on equirectangular images. In the proposed approach, we construct a distortion-adaptive module to deal with the distortion caused by the equirectangular projection. In addition, a multi-scale contextual integration block is introduced to perceive and distinguish the rich scenes and objects in omnidirectional scenes. The whole network is organized in a progressively manner with deep supervision. Experimental results show the proposed baseline approach outperforms the top-performanced state-of-the-art methods on 360$^\circ$ SOD dataset. Moreover, benchmarking results of the proposed baseline approach and other methods on 360$^\circ$ SOD dataset show the proposed dataset is very challenging, which also validate the usefulness of the proposed dataset and approach to boost the development of SOD on 360$^\circ$ omnidirectional scenes.



### Multi-Year Vector Dynamic Time Warping Based Crop Mapping
- **Arxiv ID**: http://arxiv.org/abs/1909.04930v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.04930v2)
- **Published**: 2019-09-11 09:05:05+00:00
- **Updated**: 2019-11-27 08:28:22+00:00
- **Authors**: Mustafa Teke, Yasemin Yardımcı
- **Comment**: None
- **Journal**: None
- **Summary**: Recent automated crop mapping via supervised learning-based methods have demonstrated unprecedented improvement over classical techniques. However, most crop mapping studies are limited to same-year crop mapping in which the present year's labeled data is used to predict the same year's crop map. Classification accuracies of these methods degrade considerably in cross-year mapping. Cross-year crop mapping is more useful as it allows the prediction of the following years' crop maps using previously labeled data. We propose Vector Dynamic Time Warping (VDTW), a novel multi-year classification approach based on warping of angular distances between phenological vectors. The results prove that the proposed VDTW method is robust to temporal and spectral variations compensating for different farming practices, climate and atmospheric effects, and measurement errors between years. We also describe a method for determining the most discriminative time window that allows high classification accuracies with limited data. We carried out tests of our approach with Landsat 8 time-series imagery from years 2013 to 2016 for classification of corn and cotton in the Harran Plain, and corn, cotton, and soybean in the Bismil Plain of Southeastern Turkey. In addition, we tested VDTW corn and soybean in Kansas, the US for 2017 and 2018 with the Harmonized Landsat Sentinel data. The VDTW method achieved 99.85% and 99.74% overall accuracies for the same and cross years, respectively with fewer training samples compared to other state-of-the-art approaches, i.e. spectral angle mapper (SAM), dynamic time warping (DTW), time-weighted DTW (TWDTW), random forest (RF), support vector machine (SVM) and deep long short-term memory (LSTM) methods. The proposed method could be expanded for other crop types and/or geographical areas.



### In Defense of LSTMs for Addressing Multiple Instance Learning Problems
- **Arxiv ID**: http://arxiv.org/abs/1909.05690v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1909.05690v5)
- **Published**: 2019-09-11 09:14:08+00:00
- **Updated**: 2021-01-14 09:56:52+00:00
- **Authors**: Kaili Wang, Jose Oramas, Tinne Tuytelaars
- **Comment**: accepted in ACCV 2020 (oral)
- **Journal**: None
- **Summary**: LSTMs have a proven track record in analyzing sequential data. But what about unordered instance bags, as found under a Multiple Instance Learning (MIL) setting? While not often used for this, we show LSTMs excell under this setting too. In addition, we show thatLSTMs are capable of indirectly capturing instance-level information us-ing only bag-level annotations. Thus, they can be used to learn instance-level models in a weakly supervised manner. Our empirical evaluation on both simplified (MNIST) and realistic (Lookbook and Histopathology) datasets shows that LSTMs are competitive with or even surpass state-of-the-art methods specially designed for handling specific MIL problems. Moreover, we show that their performance on instance-level prediction is close to that of fully-supervised methods.



### Multi-Sensor 3D Object Box Refinement for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1909.04942v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.04942v2)
- **Published**: 2019-09-11 09:38:56+00:00
- **Updated**: 2019-11-19 05:36:55+00:00
- **Authors**: Peiliang Li, Siqi Liu, Shaojie Shen
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a 3D object detection system with multi-sensor refinement in the context of autonomous driving. In our framework, the monocular camera serves as the fundamental sensor for 2D object proposal and initial 3D bounding box prediction. While the stereo cameras and LiDAR are treated as adaptive plug-in sensors to refine the 3D box localization performance. For each observed element in the raw measurement domain (e.g., pixels for stereo, 3D points for LiDAR), we model the local geometry as an instance vector representation, which indicates the 3D coordinate of each element respecting to the object frame. Using this unified geometric representation, the 3D object location can be unified refined by the stereo photometric alignment or point cloud alignment. We demonstrate superior 3D detection and localization performance compared to state-of-the-art monocular, stereo methods and competitive performance compared with the baseline LiDAR method on the KITTI object benchmark.



### BERTgrid: Contextualized Embedding for 2D Document Representation and Understanding
- **Arxiv ID**: http://arxiv.org/abs/1909.04948v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.04948v2)
- **Published**: 2019-09-11 09:51:02+00:00
- **Updated**: 2019-10-14 09:55:39+00:00
- **Authors**: Timo I. Denk, Christian Reisswig
- **Comment**: 4 pages, accepted at the "Document Intelligence" workshop of 33rd
  Conference on Neural Information Processing Systems (NeurIPS 2019),
  Vancouver, Canada
- **Journal**: None
- **Summary**: For understanding generic documents, information like font sizes, column layout, and generally the positioning of words may carry semantic information that is crucial for solving a downstream document intelligence task. Our novel BERTgrid, which is based on Chargrid by Katti et al. (2018), represents a document as a grid of contextualized word piece embedding vectors, thereby making its spatial structure and semantics accessible to the processing neural network. The contextualized embedding vectors are retrieved from a BERT language model. We use BERTgrid in combination with a fully convolutional network on a semantic instance segmentation task for extracting fields from invoices. We demonstrate its performance on tabulated line item and document header field extraction.



### AnimalWeb: A Large-Scale Hierarchical Dataset of Annotated Animal Faces
- **Arxiv ID**: http://arxiv.org/abs/1909.04951v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.04951v1)
- **Published**: 2019-09-11 09:55:56+00:00
- **Updated**: 2019-09-11 09:55:56+00:00
- **Authors**: Muhammad Haris Khan, John McDonagh, Salman Khan, Muhammad Shahabuddin, Aditya Arora, Fahad Shahbaz Khan, Ling Shao, Georgios Tzimiropoulos
- **Comment**: 15 pages, 14 figures
- **Journal**: None
- **Summary**: Being heavily reliant on animals, it is our ethical obligation to improve their well-being by understanding their needs. Several studies show that animal needs are often expressed through their faces. Though remarkable progress has been made towards the automatic understanding of human faces, this has regrettably not been the case with animal faces. There exists significant room and appropriate need to develop automatic systems capable of interpreting animal faces. Among many transformative impacts, such a technology will foster better and cheaper animal healthcare, and further advance animal psychology understanding.   We believe the underlying research progress is mainly obstructed by the lack of an adequately annotated dataset of animal faces, covering a wide spectrum of animal species. To this end, we introduce a large-scale, hierarchical annotated dataset of animal faces, featuring 21.9K faces from 334 diverse species and 21 animal orders across biological taxonomy. These faces are captured `in-the-wild' conditions and are consistently annotated with 9 landmarks on key facial features. The proposed dataset is structured and scalable by design; its development underwent four systematic stages involving rigorous, manual annotation effort of over 6K man-hours. We benchmark it for face alignment using the existing art under novel problem settings. Results showcase its challenging nature, unique attributes and present definite prospects for novel, adaptive, and generalized face-oriented CV algorithms. We further benchmark the dataset for face detection and fine-grained recognition tasks, to demonstrate multi-task applications and room for improvement. Experiments indicate that this dataset will push the algorithmic advancements across many related CV tasks and encourage the development of novel systems for animal facial behaviour monitoring. We will make the dataset publicly available.



### WSOD^2: Learning Bottom-up and Top-down Objectness Distillation for Weakly-supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1909.04972v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.04972v1)
- **Published**: 2019-09-11 11:08:55+00:00
- **Updated**: 2019-09-11 11:08:55+00:00
- **Authors**: Zhaoyang Zeng, Bei Liu, Jianlong Fu, Hongyang Chao, Lei Zhang
- **Comment**: Accepted as a ICCV 2019 poster paper
- **Journal**: None
- **Summary**: We study on weakly-supervised object detection (WSOD) which plays a vital role in relieving human involvement from object-level annotations. Predominant works integrate region proposal mechanisms with convolutional neural networks (CNN). Although CNN is proficient in extracting discriminative local features, grand challenges still exist to measure the likelihood of a bounding box containing a complete object (i.e., "objectness"). In this paper, we propose a novel WSOD framework with Objectness Distillation (i.e., WSOD^2) by designing a tailored training mechanism for weakly-supervised object detection. Multiple regression targets are specifically determined by jointly considering bottom-up (BU) and top-down (TD) objectness from low-level measurement and CNN confidences with an adaptive linear combination. As bounding box regression can facilitate a region proposal learning to approach its regression target with high objectness during training, deep objectness representation learned from bottom-up evidences can be gradually distilled into CNN by optimization. We explore different adaptive training curves for BU/TD objectness, and show that the proposed WSOD^2 can achieve state-of-the-art results.



### Monitoring Achilles tendon healing progress in ultrasound imaging with convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1909.04973v1
- **DOI**: 10.1007/978-3-030-32875-7_8
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1909.04973v1)
- **Published**: 2019-09-11 11:09:22+00:00
- **Updated**: 2019-09-11 11:09:22+00:00
- **Authors**: Piotr Woznicki, Przemyslaw Przybyszewski, Norbert Kapinski, Jakub Zielinski, Beata Ciszkowska-Lyson, Bartosz A. Borucki, Tomasz Trzcinski, Krzysztof S. Nowinski
- **Comment**: Paper accepted to MICCAI'19 SUSI workshop
- **Journal**: None
- **Summary**: Achilles tendon rupture is a debilitating injury, which is typically treated with surgical repair and long-term rehabilitation. The recovery, however, is protracted and often incomplete. Diagnosis, as well as healing progress assessment, are largely based on ultrasound and magnetic resonance imaging. In this paper, we propose an automatic method based on deep learning for analysis of Achilles tendon condition and estimation of its healing progress on ultrasound images. We develop custom convolutional neural networks for classification and regression on healing score and feature extraction. Our models are trained and validated on an acquired dataset of over 250.000 sagittal and over 450.000 axial ultrasound slices. The obtained estimates show a high correlation with the assessment of expert radiologists, with respect to all key parameters describing healing progress. We also observe that parameters associated with i.a. intratendinous healing processes are better modeled with sagittal slices. We prove that ultrasound imaging is quantitatively useful for clinical assessment of Achilles tendon healing process and should be viewed as complementary to magnetic resonance imaging.



### Computer-Aided Automated Detection of Gene-Controlled Social Actions of Drosophila
- **Arxiv ID**: http://arxiv.org/abs/1909.04974v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1909.04974v1)
- **Published**: 2019-09-11 11:15:10+00:00
- **Updated**: 2019-09-11 11:15:10+00:00
- **Authors**: Khan Faraz, Ahmed Bouridane, Richard Jiang, Tiancheng Xia, Paul Chazot, Abdel Ennaceur
- **Comment**: published on International Conference on Smart Cities at Cambridge
  2018
- **Journal**: International Conference on Smart Cities at Cambridge 2018
- **Summary**: Gene expression of social actions in Drosophilae has been attracting wide interest from biologists, medical scientists and psychologists. Gene-edited Drosophilae have been used as a test platform for experimental investigation. For example, Parkinson's genes can be embedded into a group of newly bred Drosophilae for research purpose. However, human observation of numerous tiny Drosophilae for a long term is an arduous work, and the dependence on human's acute perception is highly unreliable. As a result, an automated system of social action detection using machine learning has been highly demanded. In this study, we propose to automate the detection and classification of two innate aggressive actions demonstrated by Drosophilae. Robust keypoint detection is achieved using selective spatio-temporal interest points (sSTIP) which are then described using the 3D Scale Invariant Feature Transform (3D-SIFT) descriptors. Dimensionality reduction is performed using Spectral Regression Kernel Discriminant Analysis (SR-KDA) and classification is done using the nearest centre rule. The classification accuracy shown demonstrates the feasibility of the proposed system.



### CARS: Continuous Evolution for Efficient Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1909.04977v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.04977v6)
- **Published**: 2019-09-11 11:17:22+00:00
- **Updated**: 2020-03-09 09:11:53+00:00
- **Authors**: Zhaohui Yang, Yunhe Wang, Xinghao Chen, Boxin Shi, Chao Xu, Chunjing Xu, Qi Tian, Chang Xu
- **Comment**: To be published in CVPR2020
- **Journal**: None
- **Summary**: Searching techniques in most of existing neural architecture search (NAS) algorithms are mainly dominated by differentiable methods for the efficiency reason. In contrast, we develop an efficient continuous evolutionary approach for searching neural networks. Architectures in the population that share parameters within one SuperNet in the latest generation will be tuned over the training dataset with a few epochs. The searching in the next evolution generation will directly inherit both the SuperNet and the population, which accelerates the optimal network generation. The non-dominated sorting strategy is further applied to preserve only results on the Pareto front for accurately updating the SuperNet. Several neural networks with different model sizes and performances will be produced after the continuous search with only 0.4 GPU days. As a result, our framework provides a series of networks with the number of parameters ranging from 3.7M to 5.1M under mobile settings. These networks surpass those produced by the state-of-the-art methods on the benchmark ImageNet dataset.



### Late fusion of deep learning and hand-crafted features for Achilles tendon healing monitoring
- **Arxiv ID**: http://arxiv.org/abs/1909.05687v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.05687v1)
- **Published**: 2019-09-11 11:21:25+00:00
- **Updated**: 2019-09-11 11:21:25+00:00
- **Authors**: Norbert Kapinski, Jedrzej M. Nowosielski, Maciej E. Marchwiany, Jakub Zielinski, Beata Ciszkowska-Lyson, Bartosz A. Borucki, Tomasz Trzcinski, Krzysztof S. Nowinski
- **Comment**: Paper accepted to MICCAI'19 MSKI workshop
- **Journal**: None
- **Summary**: Healing process assessment of the Achilles tendon is usually a complex procedure that relies on a combination of biomechanical and medical imaging tests. As a result, diagnostics remains a tedious and long-lasting task. Recently, a novel method for the automatic assessment of tendon healing based on Magnetic Resonance Imaging and deep learning was introduced. The method assesses six parameters related to the treatment progress utilizing a modified pre-trained network, PCA-reduced space, and linear regression. In this paper, we propose to improve this approach by incorporating hand-crafted features. We first perform a feature selection in order to obtain optimal sets of mixed hand-crafted and deep learning predictors. With the use of approx. 20,000 MRI slices, we then train a meta-regression algorithm that performs the tendon healing assessment. Finally, we evaluate the method against scores given by an experienced radiologist. In comparison with the previous baseline method, our approach significantly improves correlation in all of the six parameters assessed. Furthermore, our method uses only one MRI protocol and saves up to 60\% of the time needed for data acquisition.



### How Old Are You? Face Age Translation with Identity Preservation Using GANs
- **Arxiv ID**: http://arxiv.org/abs/1909.04988v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.04988v1)
- **Published**: 2019-09-11 11:55:23+00:00
- **Updated**: 2019-09-11 11:55:23+00:00
- **Authors**: Zipeng Wang, Zhaoxiang Liu, Jianfeng Huang, Shiguo Lian, Yimin Lin
- **Comment**: 9 pages, 10 figures
- **Journal**: None
- **Summary**: We present a novel framework to generate images of different age while preserving identity information, which is known as face aging. Different from most recent popular face aging networks utilizing Generative Adversarial Networks(GANs) application, our approach do not simply transfer a young face to an old one. Instead, we employ the edge map as intermediate representations, firstly edge maps of young faces are extracted, a CycleGAN-based network is adopted to transfer them into edge maps of old faces, then another pix2pixHD-based network is adopted to transfer the synthesized edge maps, concatenated with identity information, into old faces. In this way, our method can generate more realistic transfered images, simultaneously ensuring that face identity information be preserved well, and the apparent age of the generated image be accurately appropriate. Experimental results demonstrate that our method is feasible for face age translation.



### Domain-Agnostic Few-Shot Classification by Learning Disparate Modulators
- **Arxiv ID**: http://arxiv.org/abs/1909.04999v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.04999v2)
- **Published**: 2019-09-11 12:18:15+00:00
- **Updated**: 2020-09-17 12:09:35+00:00
- **Authors**: Yongseok Choi, Junyoung Park, Subin Yi, Dong-Yeon Cho
- **Comment**: Presented at NeurIPS 2019 Workshop on Meta-Learning (MetaLearn 2019)
- **Journal**: None
- **Summary**: Although few-shot learning research has advanced rapidly with the help of meta-learning, its practical usefulness is still limited because most of them assumed that all meta-training and meta-testing examples came from a single domain. We propose a simple but effective way for few-shot classification in which a task distribution spans multiple domains including ones never seen during meta-training. The key idea is to build a pool of models to cover this wide task distribution and learn to select the best one for a particular task through cross-domain meta-learning. All models in the pool share a base network while each model has a separate modulator to refine the base network in its own way. This framework allows the pool to have representational diversity without losing beneficial domain-invariant features. We verify the effectiveness of the proposed algorithm through experiments on various datasets across diverse domains.



### Human Visual Attention Prediction Boosts Learning & Performance of Autonomous Driving Agents
- **Arxiv ID**: http://arxiv.org/abs/1909.05003v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.05003v1)
- **Published**: 2019-09-11 12:25:22+00:00
- **Updated**: 2019-09-11 12:25:22+00:00
- **Authors**: Alexander Makrigiorgos, Ali Shafti, Alex Harston, Julien Gerard, A. Aldo Faisal
- **Comment**: 7 pages, 6 figures, 2 tables. Submitted to IEEE RA-L with ICRA 2020
- **Journal**: None
- **Summary**: Autonomous driving is a multi-task problem requiring a deep understanding of the visual environment. End-to-end autonomous systems have attracted increasing interest as a method of learning to drive without exhaustively programming behaviours for different driving scenarios. When humans drive, they rely on a finely tuned sensory system which enables them to quickly acquire the information they need while filtering unnecessary details. This ability to identify task-specific high-interest regions within an image could be beneficial to autonomous driving agents and machine learning systems in general. To create a system capable of imitating human gaze patterns and visual attention, we collect eye movement data from human drivers in a virtual reality environment. We use this data to train deep neural networks predicting where humans are most likely to look when driving. We then use the outputs of this trained network to selectively mask driving images using a variety of masking techniques. Finally, autonomous driving agents are trained using these masked images as input. Upon comparison, we found that a dual-branch architecture which processes both raw and attention-masked images substantially outperforms all other models, reducing error in control signal predictions by 25.5\% compared to a standard end-to-end model trained only on raw images.



### Temporally Grounding Language Queries in Videos by Contextual Boundary-aware Prediction
- **Arxiv ID**: http://arxiv.org/abs/1909.05010v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.05010v2)
- **Published**: 2019-09-11 12:38:38+00:00
- **Updated**: 2019-12-18 03:18:44+00:00
- **Authors**: Jingwen Wang, Lin Ma, Wenhao Jiang
- **Comment**: Accepted to AAAI 2020
- **Journal**: None
- **Summary**: The task of temporally grounding language queries in videos is to temporally localize the best matched video segment corresponding to a given language (sentence). It requires certain models to simultaneously perform visual and linguistic understandings. Previous work predominantly ignores the precision of segment localization. Sliding window based methods use predefined search window sizes, which suffer from redundant computation, while existing anchor-based approaches fail to yield precise localization. We address this issue by proposing an end-to-end boundary-aware model, which uses a lightweight branch to predict semantic boundaries corresponding to the given linguistic information. To better detect semantic boundaries, we propose to aggregate contextual information by explicitly modeling the relationship between the current element and its neighbors. The most confident segments are subsequently selected based on both anchor and boundary predictions at the testing stage. The proposed model, dubbed Contextual Boundary-aware Prediction (CBP), outperforms its competitors with a clear margin on three public datasets. All codes are available on https://github.com/JaywongWang/CBP .



### Learning to Propagate for Graph Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.05024v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.05024v2)
- **Published**: 2019-09-11 13:00:24+00:00
- **Updated**: 2019-11-05 08:46:48+00:00
- **Authors**: Lu Liu, Tianyi Zhou, Guodong Long, Jing Jiang, Chengqi Zhang
- **Comment**: Accepted to NeurIPS 2019, code at
  https://github.com/liulu112601/Gated-Propagation-Net, slides at
  https://liulu112601.github.io/resources/GPN-NeurIPS-Slides-revised.pdf,
  Poster at
  https://liulu112601.github.io/resources/Graph-Meta-Learning-Poster-revised.pdf
- **Journal**: None
- **Summary**: Meta-learning extracts common knowledge from learning different tasks and uses it for unseen tasks. It can significantly improve tasks that suffer from insufficient training data, e.g., few shot learning. In most meta-learning methods, tasks are implicitly related by sharing parameters or optimizer. In this paper, we show that a meta-learner that explicitly relates tasks on a graph describing the relations of their output dimensions (e.g., classes) can significantly improve few shot learning. The graph's structure is usually free or cheap to obtain but has rarely been explored in previous works. We develop a novel meta-learner of this type for prototype-based classification, in which a prototype is generated for each class, such that the nearest neighbor search among the prototypes produces an accurate classification. The meta-learner, called "Gated Propagation Network (GPN)", learns to propagate messages between prototypes of different classes on the graph, so that learning the prototype of each class benefits from the data of other related classes. In GPN, an attention mechanism aggregates messages from neighboring classes of each class, with a gate choosing between the aggregated message and the message from the class itself. We train GPN on a sequence of tasks from many-shot to few shot generated by subgraph sampling. During training, it is able to reuse and update previously achieved prototypes from the memory in a life-long learning cycle. In experiments, under different training-test discrepancy and test task generation settings, GPN outperforms recent meta-learning methods on two benchmark datasets. The code of GPN and dataset generation is available at https://github.com/liulu112601/Gated-Propagation-Net.



### Sparse and Imperceivable Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1909.05040v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.05040v1)
- **Published**: 2019-09-11 13:28:44+00:00
- **Updated**: 2019-09-11 13:28:44+00:00
- **Authors**: Francesco Croce, Matthias Hein
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: Neural networks have been proven to be vulnerable to a variety of adversarial attacks. From a safety perspective, highly sparse adversarial attacks are particularly dangerous. On the other hand the pixelwise perturbations of sparse attacks are typically large and thus can be potentially detected. We propose a new black-box technique to craft adversarial examples aiming at minimizing $l_0$-distance to the original image. Extensive experiments show that our attack is better or competitive to the state of the art. Moreover, we can integrate additional bounds on the componentwise perturbation. Allowing pixels to change only in region of high variation and avoiding changes along axis-aligned edges makes our adversarial examples almost non-perceivable. Moreover, we adapt the Projected Gradient Descent attack to the $l_0$-norm integrating componentwise constraints. This allows us to do adversarial training to enhance the robustness of classifiers against sparse and imperceivable adversarial manipulations.



### Tomographic reconstruction to detect evolving structures
- **Arxiv ID**: http://arxiv.org/abs/1909.05686v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.05686v1)
- **Published**: 2019-09-11 13:48:42+00:00
- **Updated**: 2019-09-11 13:48:42+00:00
- **Authors**: Preeti Gopal, Sharat Chandran, Imants Svalbe, Ajit Rajwade
- **Comment**: 33 pages, 18 figures. arXiv admin note: text overlap with
  arXiv:1812.10998
- **Journal**: None
- **Summary**: The need for tomographic reconstruction from sparse measurements arises when the measurement process is potentially harmful, needs to be rapid, or is uneconomical. In such cases, information from previous longitudinal scans of the same object helps to reconstruct the current object while requiring significantly fewer updating measurements. Our work is based on longitudinal data acquisition scenarios where we wish to study new changes that evolve within an object over time, such as in repeated scanning for disease monitoring, or in tomography-guided surgical procedures. While this is easily feasible when measurements are acquired from a large number of projection views, it is challenging when the number of views is limited. If the goal is to track the changes while simultaneously reducing sub-sampling artefacts, we propose (1) acquiring measurements from a small number of views and using a global unweighted prior-based reconstruction. If the goal is to observe details of new changes, we propose (2) acquiring measurements from a moderate number of views and using a more involved reconstruction routine. We show that in the latter case, a weighted technique is necessary in order to prevent the prior from adversely affecting the reconstruction of new structures that are absent in any of the earlier scans. The reconstruction of new regions is safeguarded from the bias of the prior by computing regional weights that moderate the local influence of the priors. We are thus able to effectively reconstruct both the old and the new structures in the test. In addition to testing on simulated data, we have validated the efficacy of our method on real tomographic data. The results demonstrate the use of both unweighted and weighted priors in different scenarios.



### Local block-wise self attention for normal organ segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.05054v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.05054v1)
- **Published**: 2019-09-11 13:50:53+00:00
- **Updated**: 2019-09-11 13:50:53+00:00
- **Authors**: Jue Jiang, Elguindi Sharif, Hyemin Um, Sean Berry, Harini Veeraraghavan
- **Comment**: None
- **Journal**: None
- **Summary**: We developed a new and computationally simple local block-wise self attention based normal structures segmentation approach applied to head and neck computed tomography (CT) images. Our method uses the insight that normal organs exhibit regularity in their spatial location and inter-relation within images, which can be leveraged to simplify the computations required to aggregate feature information. We accomplish this by using local self attention blocks that pass information between each other to derive the attention map. We show that adding additional attention layers increases the contextual field and captures focused attention from relevant structures. We developed our approach using U-net and compared it against multiple state-of-the-art self attention methods. All models were trained on 48 internal headneck CT scans and tested on 48 CT scans from the external public domain database of computational anatomy dataset. Our method achieved the highest Dice similarity coefficient segmentation accuracy of 0.85$\pm$0.04, 0.86$\pm$0.04 for left and right parotid glands, 0.79$\pm$0.07 and 0.77$\pm$0.05 for left and right submandibular glands, 0.93$\pm$0.01 for mandible and 0.88$\pm$0.02 for the brain stem with the lowest increase of 66.7\% computing time per image and 0.15\% increase in model parameters compared with standard U-net. The best state-of-the-art method called point-wise spatial attention, achieved \textcolor{black}{comparable accuracy but with 516.7\% increase in computing time and 8.14\% increase in parameters compared with standard U-net.} Finally, we performed ablation tests and studied the impact of attention block size, overlap of the attention blocks, additional attention layers, and attention block placement on segmentation performance.



### Image Segmentation using Multi-Threshold technique by Histogram Sampling
- **Arxiv ID**: http://arxiv.org/abs/1909.05084v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.05084v1)
- **Published**: 2019-09-11 14:37:47+00:00
- **Updated**: 2019-09-11 14:37:47+00:00
- **Authors**: Amit Gurung, Sangyal Lama Tamang
- **Comment**: 28 pages, 6 figures, 3 tables, unpublished work, in a process to
  submit
- **Journal**: None
- **Summary**: The segmentation of digital images is one of the essential steps in image processing or a computer vision system. It helps in separating the pixels into different regions according to their intensity level. A large number of segmentation techniques have been proposed, and a few of them use complex computational operations. Among all, the most straightforward procedure that can be easily implemented is thresholding. In this paper, we present a unique heuristic approach for image segmentation that automatically determines multilevel thresholds by sampling the histogram of a digital image. Our approach emphasis on selecting a valley as optimal threshold values. We demonstrated that our approach outperforms the popular Otsu's method in terms of CPU computational time. We demonstrated that our approach outperforms the popular Otsu's method in terms of CPU computational time. We observed a maximum speed-up of 35.58x and a minimum speed-up of 10.21x on popular image processing benchmarks. To demonstrate the correctness of our approach in determining threshold values, we compute PSNR, SSIM, and FSIM values to compare with the values obtained by Otsu's method. This evaluation shows that our approach is comparable and better in many cases as compared to well known Otsu's method.



### CEREBRUM: a fast and fully-volumetric Convolutional Encoder-decodeR for weakly-supervised sEgmentation of BRain strUctures from out-of-the-scanner MRI
- **Arxiv ID**: http://arxiv.org/abs/1909.05085v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.05085v2)
- **Published**: 2019-09-11 14:40:30+00:00
- **Updated**: 2019-09-26 13:01:41+00:00
- **Authors**: Dennis Bontempi, Sergio Benini, Alberto Signoroni, Michele Svanera, Lars Muckli
- **Comment**: 15 pages, 6 figures
- **Journal**: None
- **Summary**: Many functional and structural neuroimaging studies call for accurate morphometric segmentation of different brain structures starting from image intensity values of MRI scans. Current automatic (multi-) atlas-based segmentation strategies often lack accuracy on difficult-to-segment brain structures and, since these methods rely on atlas-to-scan alignment, they may take long processing times. Recently, methods deploying solutions based on Convolutional Neural Networks (CNNs) are making the direct analysis of out-of-the-scanner data feasible. However, current CNN-based solutions partition the test volume into 2D or 3D patches, which are processed independently. This entails a loss of global contextual information thereby negatively impacting the segmentation accuracy. In this work, we design and test an optimised end-to-end CNN architecture that makes the exploitation of global spatial information computationally tractable, allowing to process a whole MRI volume at once. We adopt a weakly supervised learning strategy by exploiting a large dataset composed by 947 out-of-the-scanner (3 Tesla T1-weighted 1mm isotropic MP-RAGE 3D sequences) MR Images. The resulting model is able to produce accurate multi-structure segmentation results in only few seconds. Different quantitative measures demonstrate an improved accuracy of our solution when compared to state-of-the-art techniques. Moreover, through a randomised survey involving expert neuroscientists, we show that subjective judgements clearly prefer our solution with respect to the widely adopted atlas-based FreeSurfer software.



### Learning Enhanced Resolution-wise features for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1909.05090v4
- **DOI**: 10.1109/ICIP40778.2020.9191174
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.05090v4)
- **Published**: 2019-09-11 14:46:28+00:00
- **Updated**: 2020-12-13 15:22:41+00:00
- **Authors**: Kun Zhang, Peng He, Ping Yao, Ge Chen, Rui Wu, Min Du, Huimin Li, Li Fu, Tianyao Zheng
- **Comment**: Published on ICIP 2020
- **Journal**: None
- **Summary**: Recently, multi-resolution networks (such as Hourglass, CPN, HRNet, etc.) have achieved significant performance on pose estimation by combining feature maps of various resolutions. In this paper, we propose a Resolution-wise Attention Module (RAM) and Gradual Pyramid Refinement (GPR), to learn enhanced resolution-wise feature maps for precise pose estimation. Specifically, RAM learns a group of weights to represent the different importance of feature maps across resolutions, and the GPR gradually merges every two feature maps from low to high resolutions to regress final human keypoint heatmaps. With the enhanced resolution-wise features learnt by CNN, we obtain more accurate human keypoint locations. The efficacies of our proposed methods are demonstrated on MS-COCO dataset, achieving state-of-the-art performance with average precision of 77.7 on COCO val2017 set and 77.0 on test-dev2017 set without using extra human keypoint training dataset.



### Attention-Aware Age-Agnostic Visual Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/1909.05163v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.05163v1)
- **Published**: 2019-09-11 16:04:42+00:00
- **Updated**: 2019-09-11 16:04:42+00:00
- **Authors**: Ziqi Wang, Jiahui Li, Seyran Khademi, Jan van Gemert
- **Comment**: Presented at ICCV WORKSHOP ON E-HERITAGE 2019, Seoul, South Korea
- **Journal**: None
- **Summary**: A cross-domain visual place recognition (VPR) task is proposed in this work, i.e., matching images of the same architectures depicted in different domains. VPR is commonly treated as an image retrieval task, where a query image from an unknown location is matched with relevant instances from geo-tagged gallery database. Different from conventional VPR settings where the query images and gallery images come from the same domain, we propose a more common but challenging setup where the query images are collected under a new unseen condition. The two domains involved in this work are contemporary street view images of Amsterdam from the Mapillary dataset (source domain) and historical images of the same city from Beeldbank dataset (target domain). We tailored an age-invariant feature learning CNN that can focus on domain invariant objects and learn to match images based on a weakly supervised ranking loss. We propose an attention aggregation module that is robust to domain discrepancy between the train and the test data. Further, a multi-kernel maximum mean discrepancy (MK-MMD) domain adaptation loss is adopted to improve the cross-domain ranking performance. Both attention and adaptation modules are unsupervised while the ranking loss uses weak supervision. Visual inspection shows that the attention module focuses on built forms while the dramatically changing environment are less weighed. Our proposed CNN achieves state of the art results (99% accuracy) on the single-domain VPR task and 20% accuracy at its best on the cross-domain VPR task, revealing the difficulty of age-invariant VPR.



### Comparative Analysis of CNN-based Spatiotemporal Reasoning in Videos
- **Arxiv ID**: http://arxiv.org/abs/1909.05165v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.05165v2)
- **Published**: 2019-09-11 16:10:00+00:00
- **Updated**: 2021-01-11 13:53:46+00:00
- **Authors**: Okan Köpüklü, Fabian Herzog, Gerhard Rigoll
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding actions and gestures in video streams requires temporal reasoning of the spatial content from different time instants, i.e., spatiotemporal (ST) modeling. In this survey paper, we have made a comparative analysis of different ST modeling techniques for action and gecture recognition tasks. Since Convolutional Neural Networks (CNNs) are proved to be an effective tool as a feature extractor for static images, we apply ST modeling techniques on the features of static images from different time instants extracted by CNNs. All techniques are trained end-to-end together with a CNN feature extraction part and evaluated on two publicly available benchmarks: The Jester and the Something-Something datasets. The Jester dataset contains various dynamic and static hand gestures, whereas the Something-Something dataset contains actions of human-object interactions. The common characteristic of these two benchmarks is that the designed architectures need to capture the full temporal content of videos in order to correctly classify actions/gestures. Contrary to expectations, experimental results show that Recurrent Neural Network (RNN) based ST modeling techniques yield inferior results compared to other techniques such as fully convolutional architectures. Codes and pretrained models of this work are publicly available.



### Multi-stage domain adversarial style reconstruction for cytopathological image stain normalization
- **Arxiv ID**: http://arxiv.org/abs/1909.05184v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.05184v1)
- **Published**: 2019-09-11 16:34:25+00:00
- **Updated**: 2019-09-11 16:34:25+00:00
- **Authors**: Xihao Chen, Jingya Yu, Li Chen, Shaoqun Zeng, Xiuli Liu, Shenghua Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: The different stain styles of cytopathological images have a negative effect on the generalization ability of automated image analysis algorithms. This article proposes a new framework that normalizes the stain style for cytopathological images through a stain removal module and a multi-stage domain adversarial style reconstruction module. We convert colorful images into grayscale images with a color-encoding mask. Using the mask, reconstructed images retain their basic color without red and blue mixing, which is important for cytopathological image interpretation. The style reconstruction module consists of per-pixel regression with intradomain adversarial learning, inter-domain adversarial learning, and optional task-based refining. Per-pixel regression with intradomain adversarial learning establishes the generative network from the decolorized input to the reconstructed output. The interdomain adversarial learning further reduces the difference in stain style. The generation network can be optimized by combining it with the task network. Experimental results show that the proposed techniques help to optimize the generation network. The average accuracy increases from 75.41% to 84.79% after the intra-domain adversarial learning, and to 87.00% after interdomain adversarial learning. Under the guidance of the task network, the average accuracy rate reaches 89.58%. The proposed method achieves unsupervised stain normalization of cytopathological images, while preserving the cell structure, texture structure, and cell color properties of the image. This method overcomes the problem of generalizing the task models between different stain styles of cytopathological images.



### Skeleton Image Representation for 3D Action Recognition based on Tree Structure and Reference Joints
- **Arxiv ID**: http://arxiv.org/abs/1909.05704v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.05704v1)
- **Published**: 2019-09-11 16:35:06+00:00
- **Updated**: 2019-09-11 16:35:06+00:00
- **Authors**: Carlos Caetano, François Brémond, William Robson Schwartz
- **Comment**: Conference on Graphics, Patterns and Images (SIBGRAPI2019). arXiv
  admin note: substantial text overlap with arXiv:1907.13025
- **Journal**: None
- **Summary**: In the last years, the computer vision research community has studied on how to model temporal dynamics in videos to employ 3D human action recognition. To that end, two main baseline approaches have been researched: (i) Recurrent Neural Networks (RNNs) with Long-Short Term Memory (LSTM); and (ii) skeleton image representations used as input to a Convolutional Neural Network (CNN). Although RNN approaches present excellent results, such methods lack the ability to efficiently learn the spatial relations between the skeleton joints. On the other hand, the representations used to feed CNN approaches present the advantage of having the natural ability of learning structural information from 2D arrays (i.e., they learn spatial relations from the skeleton joints). To further improve such representations, we introduce the Tree Structure Reference Joints Image (TSRJI), a novel skeleton image representation to be used as input to CNNs. The proposed representation has the advantage of combining the use of reference joints and a tree structure skeleton. While the former incorporates different spatial relationships between the joints, the latter preserves important spatial relations by traversing a skeleton tree with a depth-first order algorithm. Experimental results demonstrate the effectiveness of the proposed representation for 3D action recognition on two datasets achieving state-of-the-art results on the recent NTU RGB+D~120 dataset.



### Reconstructing continuous distributions of 3D protein structure from cryo-EM images
- **Arxiv ID**: http://arxiv.org/abs/1909.05215v3
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.05215v3)
- **Published**: 2019-09-11 17:13:06+00:00
- **Updated**: 2020-02-15 04:31:46+00:00
- **Authors**: Ellen D. Zhong, Tristan Bepler, Joseph H. Davis, Bonnie Berger
- **Comment**: None
- **Journal**: International Conference on Learning Representations (ICLR), 2020
- **Summary**: Cryo-electron microscopy (cryo-EM) is a powerful technique for determining the structure of proteins and other macromolecular complexes at near-atomic resolution. In single particle cryo-EM, the central problem is to reconstruct the three-dimensional structure of a macromolecule from $10^{4-7}$ noisy and randomly oriented two-dimensional projections. However, the imaged protein complexes may exhibit structural variability, which complicates reconstruction and is typically addressed using discrete clustering approaches that fail to capture the full range of protein dynamics. Here, we introduce a novel method for cryo-EM reconstruction that extends naturally to modeling continuous generative factors of structural heterogeneity. This method encodes structures in Fourier space using coordinate-based deep neural networks, and trains these networks from unlabeled 2D cryo-EM images by combining exact inference over image orientation with variational inference for structural heterogeneity. We demonstrate that the proposed method, termed cryoDRGN, can perform ab initio reconstruction of 3D protein complexes from simulated and real 2D cryo-EM image data. To our knowledge, cryoDRGN is the first neural network-based approach for cryo-EM reconstruction and the first end-to-end method for directly reconstructing continuous ensembles of protein structures from cryo-EM images.



### NODE: Extreme Low Light Raw Image Denoising using a Noise Decomposition Network
- **Arxiv ID**: http://arxiv.org/abs/1909.05249v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.05249v1)
- **Published**: 2019-09-11 17:27:34+00:00
- **Updated**: 2019-09-11 17:27:34+00:00
- **Authors**: Hao Guan, Liu Liu, Sean Moran, Fenglong Song, Gregory Slabaugh
- **Comment**: None
- **Journal**: None
- **Summary**: Denoising extreme low light images is a challenging task due to the high noise level. When the illumination is low, digital cameras increase the ISO (electronic gain) to amplify the brightness of captured data. However, this in turn amplifies the noise, arising from read, shot, and defective pixel sources. In the raw domain, read and shot noise are effectively modelled using Gaussian and Poisson distributions respectively, whereas defective pixels can be modeled with impulsive noise. In extreme low light imaging, noise removal becomes a critical challenge to produce a high quality, detailed image with low noise. In this paper, we propose a multi-task deep neural network called Noise Decomposition (NODE) that explicitly and separately estimates defective pixel noise, in conjunction with Gaussian and Poisson noise, to denoise an extreme low light image. Our network is purposely designed to work with raw data, for which the noise is more easily modeled before going through non-linear transformations in the image signal processing (ISP) pipeline. Quantitative and qualitative evaluation show the proposed method to be more effective at denoising real raw images than state-of-the-art techniques.



### SoftTriple Loss: Deep Metric Learning Without Triplet Sampling
- **Arxiv ID**: http://arxiv.org/abs/1909.05235v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.05235v2)
- **Published**: 2019-09-11 17:47:25+00:00
- **Updated**: 2020-04-15 02:17:42+00:00
- **Authors**: Qi Qian, Lei Shang, Baigui Sun, Juhua Hu, Hao Li, Rong Jin
- **Comment**: accepted by ICCV'19
- **Journal**: None
- **Summary**: Distance metric learning (DML) is to learn the embeddings where examples from the same class are closer than examples from different classes. It can be cast as an optimization problem with triplet constraints. Due to the vast number of triplet constraints, a sampling strategy is essential for DML. With the tremendous success of deep learning in classifications, it has been applied for DML. When learning embeddings with deep neural networks (DNNs), only a mini-batch of data is available at each iteration. The set of triplet constraints has to be sampled within the mini-batch. Since a mini-batch cannot capture the neighbors in the original set well, it makes the learned embeddings sub-optimal. On the contrary, optimizing SoftMax loss, which is a classification loss, with DNN shows a superior performance in certain DML tasks. It inspires us to investigate the formulation of SoftMax. Our analysis shows that SoftMax loss is equivalent to a smoothed triplet loss where each class has a single center. In real-world data, one class can contain several local clusters rather than a single one, e.g., birds of different poses. Therefore, we propose the SoftTriple loss to extend the SoftMax loss with multiple centers for each class. Compared with conventional deep metric learning algorithms, optimizing SoftTriple loss can learn the embeddings without the sampling phase by mildly increasing the size of the last fully connected layer. Experiments on the benchmark fine-grained data sets demonstrate the effectiveness of the proposed loss function. Code is available at https://github.com/idstcv/SoftTriple



### Edge-Informed Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1909.05305v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.05305v1)
- **Published**: 2019-09-11 18:54:41+00:00
- **Updated**: 2019-09-11 18:54:41+00:00
- **Authors**: Kamyar Nazeri, Harrish Thasarathan, Mehran Ebrahimi
- **Comment**: None
- **Journal**: None
- **Summary**: The recent increase in the extensive use of digital imaging technologies has brought with it a simultaneous demand for higher-resolution images. We develop a novel edge-informed approach to single image super-resolution (SISR). The SISR problem is reformulated as an image inpainting task. We use a two-stage inpainting model as a baseline for super-resolution and show its effectiveness for different scale factors (x2, x4, x8) compared to basic interpolation schemes. This model is trained using a joint optimization of image contents (texture and color) and structures (edges). Quantitative and qualitative comparisons are included and the proposed model is compared with current state-of-the-art techniques. We show that our method of decoupling structure and texture reconstruction improves the quality of the final reconstructed high-resolution image. Code and models available at: https://github.com/knazeri/edge-informed-sisr



### Distanced LSTM: Time-Distanced Gates in Long Short-Term Memory Models for Lung Cancer Detection
- **Arxiv ID**: http://arxiv.org/abs/1909.05321v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.05321v1)
- **Published**: 2019-09-11 19:25:40+00:00
- **Updated**: 2019-09-11 19:25:40+00:00
- **Authors**: Riqiang Gao, Yuankai Huo, Shunxing Bao, Yucheng Tang, Sanja L. Antic, Emily S. Epstein, Aneri B. Balar, Steve Deppen, Alexis B. Paulson, Kim L. Sandler, Pierre P. Massion, Bennett A. Landman
- **Comment**: This paper is accepted by MLMI (oral), MICCAI workshop
- **Journal**: None
- **Summary**: The field of lung nodule detection and cancer prediction has been rapidly developing with the support of large public data archives. Previous studies have largely focused on cross-sectional (single) CT data. Herein, we consider longitudinal data. The Long Short-Term Memory (LSTM) model addresses learning with regularly spaced time points (i.e., equal temporal intervals). However, clinical imaging follows patient needs with often heterogeneous, irregular acquisitions. To model both regular and irregular longitudinal samples, we generalize the LSTM model with the Distanced LSTM (DLSTM) for temporally varied acquisitions. The DLSTM includes a Temporal Emphasis Model (TEM) that enables learning across regularly and irregularly sampled intervals. Briefly, (1) the time intervals between longitudinal scans are modeled explicitly, (2) temporally adjustable forget and input gates are introduced for irregular temporal sampling; and (3) the latest longitudinal scan has an additional emphasis term. We evaluate the DLSTM framework in three datasets including simulated data, 1794 National Lung Screening Trial (NLST) scans, and 1420 clinically acquired data with heterogeneous and irregular temporal accession. The experiments on the first two datasets demonstrate that our method achieves competitive performance on both simulated and regularly sampled datasets (e.g. improve LSTM from 0.6785 to 0.7085 on F1 score in NLST). In external validation of clinically and irregularly acquired data, the benchmarks achieved 0.8350 (CNN feature) and 0.8380 (LSTM) on the area under the ROC curve (AUC) score, while the proposed DLSTM achieves 0.8905.



### Specifying Object Attributes and Relations in Interactive Scene Generation
- **Arxiv ID**: http://arxiv.org/abs/1909.05379v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.05379v2)
- **Published**: 2019-09-11 21:16:38+00:00
- **Updated**: 2019-11-17 15:33:05+00:00
- **Authors**: Oron Ashual, Lior Wolf
- **Comment**: Best Paper Honorable Mention in ICCV 2019
- **Journal**: The IEEE International Conference on Computer Vision (ICCV), 2019
- **Summary**: We introduce a method for the generation of images from an input scene graph. The method separates between a layout embedding and an appearance embedding. The dual embedding leads to generated images that better match the scene graph, have higher visual quality, and support more complex scene graphs. In addition, the embedding scheme supports multiple and diverse output images per scene graph, which can be further controlled by the user. We demonstrate two modes of per-object control: (i) importing elements from other images, and (ii) navigation in the object space, by selecting an appearance archetype. Our code is publicly available at https://www.github.com/ashual/scene_generation



### A deep learning system for differential diagnosis of skin diseases
- **Arxiv ID**: http://arxiv.org/abs/1909.05382v1
- **DOI**: 10.1038/s41591-020-0842-3
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.05382v1)
- **Published**: 2019-09-11 21:26:42+00:00
- **Updated**: 2019-09-11 21:26:42+00:00
- **Authors**: Yuan Liu, Ayush Jain, Clara Eng, David H. Way, Kang Lee, Peggy Bui, Kimberly Kanada, Guilherme de Oliveira Marinho, Jessica Gallegos, Sara Gabriele, Vishakha Gupta, Nalini Singh, Vivek Natarajan, Rainer Hofmann-Wellenhof, Greg S. Corrado, Lily H. Peng, Dale R. Webster, Dennis Ai, Susan Huang, Yun Liu, R. Carter Dunn, David Coz
- **Comment**: None
- **Journal**: Nature Medicine (2020)
- **Summary**: Skin conditions affect an estimated 1.9 billion people worldwide. A shortage of dermatologists causes long wait times and leads patients to seek dermatologic care from general practitioners. However, the diagnostic accuracy of general practitioners has been reported to be only 0.24-0.70 (compared to 0.77-0.96 for dermatologists), resulting in referral errors, delays in care, and errors in diagnosis and treatment. In this paper, we developed a deep learning system (DLS) to provide a differential diagnosis of skin conditions for clinical cases (skin photographs and associated medical histories). The DLS distinguishes between 26 skin conditions that represent roughly 80% of the volume of skin conditions seen in primary care. The DLS was developed and validated using de-identified cases from a teledermatology practice serving 17 clinical sites via a temporal split: the first 14,021 cases for development and the last 3,756 cases for validation. On the validation set, where a panel of three board-certified dermatologists defined the reference standard for every case, the DLS achieved 0.71 and 0.93 top-1 and top-3 accuracies respectively. For a random subset of the validation set (n=963 cases), 18 clinicians reviewed the cases for comparison. On this subset, the DLS achieved a 0.67 top-1 accuracy, non-inferior to board-certified dermatologists (0.63, p<0.001), and higher than primary care physicians (PCPs, 0.45) and nurse practitioners (NPs, 0.41). The top-3 accuracy showed a similar trend: 0.90 DLS, 0.75 dermatologists, 0.60 PCPs, and 0.55 NPs. These results highlight the potential of the DLS to augment general practitioners to accurately diagnose skin conditions by suggesting differential diagnoses that may not have been considered. Future work will be needed to prospectively assess the clinical impact of using this tool in actual clinical workflows.



### Automated Blood Cell Detection and Counting via Deep Learning for Microfluidic Point-of-Care Medical Devices
- **Arxiv ID**: http://arxiv.org/abs/1909.05393v1
- **DOI**: 10.1088/1757-899X/646/1/012048
- **Categories**: **cs.CV**, cs.AI, cs.ET, cs.LG, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/1909.05393v1)
- **Published**: 2019-09-11 22:14:03+00:00
- **Updated**: 2019-09-11 22:14:03+00:00
- **Authors**: Tiancheng Xia, Richard Jiang, YongQing Fu, Nanlin Jin
- **Comment**: None
- **Journal**: Proceeding of 2019 3rd International Conference on Artificial
  Intelligence Applications and Technologies (AIAAT 2019)
- **Summary**: Automated in-vitro cell detection and counting have been a key theme for artificial and intelligent biological analysis such as biopsy, drug analysis and decease diagnosis. Along with the rapid development of microfluidics and lab-on-chip technologies, in-vitro live cell analysis has been one of the critical tasks for both research and industry communities. However, it is a great challenge to obtain and then predict the precise information of live cells from numerous microscopic videos and images. In this paper, we investigated in-vitro detection of white blood cells using deep neural networks, and discussed how state-of-the-art machine learning techniques could fulfil the needs of medical diagnosis. The approach we used in this study was based on Faster Region-based Convolutional Neural Networks (Faster RCNNs), and a transfer learning process was applied to apply this technique to the microscopic detection of blood cells. Our experimental results demonstrated that fast and efficient analysis of blood cells via automated microscopic imaging can achieve much better accuracy and faster speed than the conventionally applied methods, implying a promising future of this technology to be applied to the microfluidic point-of-care medical devices.



### SuPer: A Surgical Perception Framework for Endoscopic Tissue Manipulation with Surgical Robotics
- **Arxiv ID**: http://arxiv.org/abs/1909.05405v2
- **DOI**: 10.1109/LRA.2020.2970659
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.05405v2)
- **Published**: 2019-09-11 23:57:32+00:00
- **Updated**: 2020-02-14 06:27:34+00:00
- **Authors**: Yang Li, Florian Richter, Jingpei Lu, Emily K. Funk, Ryan K. Orosco, Jianke Zhu, Michael C. Yip
- **Comment**: The first two authors made equal contribution on this paper
- **Journal**: IEEE Robotics and Automation Letters (RA-L), vol. 5, no. 2, pp.
  2294-2301, April 2020
- **Summary**: Traditional control and task automation have been successfully demonstrated in a variety of structured, controlled environments through the use of highly specialized modeled robotic systems in conjunction with multiple sensors. However, the application of autonomy in endoscopic surgery is very challenging, particularly in soft tissue work, due to the lack of high-quality images and the unpredictable, constantly deforming environment. In this work, we propose a novel surgical perception framework, SuPer, for surgical robotic control. This framework continuously collects 3D geometric information that allows for mapping a deformable surgical field while tracking rigid instruments within the field. To achieve this, a model-based tracker is employed to localize the surgical tool with a kinematic prior in conjunction with a model-free tracker to reconstruct the deformable environment and provide an estimated point cloud as a mapping of the environment. The proposed framework was implemented on the da Vinci Surgical System in real-time with an end-effector controller where the target configurations are set and regulated through the framework. Our proposed framework successfully completed soft tissue manipulation tasks with high accuracy. The demonstration of this novel framework is promising for the future of surgical autonomy. In addition, we provide our dataset for further surgical research.



