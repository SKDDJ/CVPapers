# Arxiv Papers in cs.CV on 2019-09-17
### Data-Efficient Classification of Birdcall Through Convolutional Neural Networks Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.07526v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.07526v1)
- **Published**: 2019-09-17 00:16:16+00:00
- **Updated**: 2019-09-17 00:16:16+00:00
- **Authors**: Dina B. Efremova, Mangalam Sankupellay, Dmitry A. Konovalov
- **Comment**: Accepted for IEEE Digital Image Computing: Techniques and
  Applications, 2019 (DICTA 2019), 2-4 December 2019 in Perth, Australia,
  http://dicta2019.dictaconference.org/index.html
- **Journal**: None
- **Summary**: Deep learning Convolutional Neural Network (CNN) models are powerful classification models but require a large amount of training data. In niche domains such as bird acoustics, it is expensive and difficult to obtain a large number of training samples. One method of classifying data with a limited number of training samples is to employ transfer learning. In this research, we evaluated the effectiveness of birdcall classification using transfer learning from a larger base dataset (2814 samples in 46 classes) to a smaller target dataset (351 samples in 10 classes) using the ResNet-50 CNN. We obtained 79% average validation accuracy on the target dataset in 5-fold cross-validation. The methodology of transfer learning from an ImageNet-trained CNN to a project-specific and a much smaller set of classes and images was extended to the domain of spectrogram images, where the base dataset effectively played the role of the ImageNet.



### A*3D Dataset: Towards Autonomous Driving in Challenging Environments
- **Arxiv ID**: http://arxiv.org/abs/1909.07541v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.07541v1)
- **Published**: 2019-09-17 01:19:04+00:00
- **Updated**: 2019-09-17 01:19:04+00:00
- **Authors**: Quang-Hieu Pham, Pierre Sevestre, Ramanpreet Singh Pahwa, Huijing Zhan, Chun Ho Pang, Yuda Chen, Armin Mustafa, Vijay Chandrasekhar, Jie Lin
- **Comment**: A new 3D dataset by I2R, A*STAR for autonomous driving
- **Journal**: None
- **Summary**: With the increasing global popularity of self-driving cars, there is an immediate need for challenging real-world datasets for benchmarking and training various computer vision tasks such as 3D object detection. Existing datasets either represent simple scenarios or provide only day-time data. In this paper, we introduce a new challenging A*3D dataset which consists of RGB images and LiDAR data with significant diversity of scene, time, and weather. The dataset consists of high-density images ($\approx~10$ times more than the pioneering KITTI dataset), heavy occlusions, a large number of night-time frames ($\approx~3$ times the nuScenes dataset), addressing the gaps in the existing datasets to push the boundaries of tasks in autonomous driving research to more challenging highly diverse environments. The dataset contains $39\text{K}$ frames, $7$ classes, and $230\text{K}$ 3D object annotations. An extensive 3D object detection benchmark evaluation on the A*3D dataset for various attributes such as high density, day-time/night-time, gives interesting insights into the advantages and limitations of training and testing 3D object detection in real-world setting.



### Real-Time Variational Fisheye Stereo without Rectification and Undistortion
- **Arxiv ID**: http://arxiv.org/abs/1909.07545v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.07545v1)
- **Published**: 2019-09-17 01:48:06+00:00
- **Updated**: 2019-09-17 01:48:06+00:00
- **Authors**: Menandro Roxas, Takeshi Oishi
- **Comment**: None
- **Journal**: None
- **Summary**: Dense 3D maps from wide-angle cameras is beneficial to robotics applications such as navigation and autonomous driving. In this work, we propose a real-time dense 3D mapping method for fisheye cameras without explicit rectification and undistortion. We extend the conventional variational stereo method by constraining the correspondence search along the epipolar curve using a trajectory field induced by camera motion. We also propose a fast way of generating the trajectory field without increasing the processing time compared to conventional rectified methods. With our implementation, we were able to achieve real-time processing using modern GPUs. Our results show the advantages of our non-rectified dense mapping approach compared to rectified variational methods and non-rectified discrete stereo matching methods.



### STELA: A Real-Time Scene Text Detector with Learned Anchor
- **Arxiv ID**: http://arxiv.org/abs/1909.07549v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07549v2)
- **Published**: 2019-09-17 02:01:30+00:00
- **Updated**: 2019-09-23 01:30:52+00:00
- **Authors**: Linjie Deng, Yanxiang Gong, Xinchen Lu, Yi Lin, Zheng Ma, Mei Xie
- **Comment**: None
- **Journal**: None
- **Summary**: To achieve high coverage of target boxes, a normal strategy of conventional one-stage anchor-based detectors is to utilize multiple priors at each spatial position, especially in scene text detection tasks. In this work, we present a simple and intuitive method for multi-oriented text detection where each location of feature maps only associates with one reference box. The idea is inspired from the twostage R-CNN framework that can estimate the location of objects with any shape by using learned proposals. The aim of our method is to integrate this mechanism into a onestage detector and employ the learned anchor which is obtained through a regression operation to replace the original one into the final predictions. Based on RetinaNet, our method achieves competitive performances on several public benchmarks with a totally real-time efficiency (26:5fps at 800p), which surpasses all of anchor-based scene text detectors. In addition, with less attention on anchor design, we believe our method is easy to be applied on other analogous detection tasks. The code will publicly available at https://github.com/xhzdeng/stela.



### HAD-GAN: A Human-perception Auxiliary Defense GAN to Defend Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1909.07558v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.07558v5)
- **Published**: 2019-09-17 02:46:34+00:00
- **Updated**: 2021-12-22 07:43:50+00:00
- **Authors**: Wanting Yu, Hongyi Yu, Lingyun Jiang, Mengli Zhang, Kai Qiao
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial examples reveal the vulnerability and unexplained nature of neural networks. Studying the defense of adversarial examples is of considerable practical importance. Most adversarial examples that misclassify networks are often undetectable by humans. In this paper, we propose a defense model to train the classifier into a human-perception classification model with shape preference. The proposed model comprising a texture transfer network (TTN) and an auxiliary defense generative adversarial networks (GAN) is called Human-perception Auxiliary Defense GAN (HAD-GAN). The TTN is used to extend the texture samples of a clean image and helps classifiers focus on its shape. GAN is utilized to form a training framework for the model and generate the necessary images. A series of experiments conducted on MNIST, Fashion-MNIST and CIFAR10 show that the proposed model outperforms the state-of-the-art defense methods for network robustness. The model also demonstrates a significant improvement on defense capability of adversarial examples.



### Object-Centric Stereo Matching for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1909.07566v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07566v2)
- **Published**: 2019-09-17 03:07:25+00:00
- **Updated**: 2020-03-10 06:29:22+00:00
- **Authors**: Alex D. Pon, Jason Ku, Chengyao Li, Steven L. Waslander
- **Comment**: Accepted in ICRA 2020
- **Journal**: None
- **Summary**: Safe autonomous driving requires reliable 3D object detection-determining the 6 DoF pose and dimensions of objects of interest. Using stereo cameras to solve this task is a cost-effective alternative to the widely used LiDAR sensor. The current state-of-the-art for stereo 3D object detection takes the existing PSMNet stereo matching network, with no modifications, and converts the estimated disparities into a 3D point cloud, and feeds this point cloud into a LiDAR-based 3D object detector. The issue with existing stereo matching networks is that they are designed for disparity estimation, not 3D object detection; the shape and accuracy of object point clouds are not the focus. Stereo matching networks commonly suffer from inaccurate depth estimates at object boundaries, which we define as streaking, because background and foreground points are jointly estimated. Existing networks also penalize disparity instead of the estimated position of object point clouds in their loss functions. We propose a novel 2D box association and object-centric stereo matching method that only estimates the disparities of the objects of interest to address these two issues. Our method achieves state-of-the-art results on the KITTI 3D and BEV benchmarks.



### A Data-Center FPGA Acceleration Platform for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.07973v1
- **DOI**: 10.1109/FPL.2019.00032
- **Categories**: **cs.CV**, cs.AR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.07973v1)
- **Published**: 2019-09-17 03:32:21+00:00
- **Updated**: 2019-09-17 03:32:21+00:00
- **Authors**: Xiaoyu Yu, Yuwei Wang, Jie Miao, Ephrem Wu, Heng Zhang, Yu Meng, Bo Zhang, Biao Min, Dewei Chen, Jianlin Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Intensive computation is entering data centers with multiple workloads of deep learning. To balance the compute efficiency, performance, and total cost of ownership (TCO), the use of a field-programmable gate array (FPGA) with reconfigurable logic provides an acceptable acceleration capacity and is compatible with diverse computation-sensitive tasks in the cloud. In this paper, we develop an FPGA acceleration platform that leverages a unified framework architecture for general-purpose convolutional neural network (CNN) inference acceleration at a data center. To overcome the computation bound, 4,096 DSPs are assembled and shaped as supertile units (SUs) for different types of convolution, which provide up to 4.2 TOP/s 16-bit fixed-point performance at 500 MHz. The interleaved-task-dispatching method is proposed to map the computation across the SUs, and the memory bound is solved by a dispatching-assembling buffering model and broadcast caches. For various non-convolution operators, a filter processing unit is designed for general-purpose filter-like/pointwise operators. In the experiment, the performances of CNN models running on server-class CPUs, a GPU, and an FPGA are compared. The results show that our design achieves the best FPGA peak performance and a throughput at the same level as that of the state-of-the-art GPU in data centers, with more than 50 times lower latency.



### Is That a Chair? Imagining Affordances Using Simulations of an Articulated Human Body
- **Arxiv ID**: http://arxiv.org/abs/1909.07572v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.07572v2)
- **Published**: 2019-09-17 03:36:32+00:00
- **Updated**: 2020-04-07 21:25:42+00:00
- **Authors**: Hongtao Wu, Deven Misra, Gregory S. Chirikjian
- **Comment**: 7 pages, 6 figures. Accepted to ICRA2020
- **Journal**: None
- **Summary**: For robots to exhibit a high level of intelligence in the real world, they must be able to assess objects for which they have no prior knowledge. Therefore, it is crucial for robots to perceive object affordances by reasoning about physical interactions with the object. In this paper, we propose a novel method to provide robots with an ability to imagine object affordances using physical simulations. The class of chair is chosen here as an initial category of objects to illustrate a more general paradigm. In our method, the robot "imagines" the affordance of an arbitrarily oriented object as a chair by simulating a physical sitting interaction between an articulated human body and the object. This object affordance reasoning is used as a cue for object classification (chair vs non-chair). Moreover, if an object is classified as a chair, the affordance reasoning can also predict the upright pose of the object which allows the sitting interaction to take place. We call this type of poses the functional pose. We demonstrate our method in chair classification on synthetic 3D CAD models. Although our method uses only 30 models for training, it outperforms appearance-based deep learning methods, which require a large amount of training data, when the upright orientation is not assumed to be known a priori. In addition, we showcase that the functional pose predictions of our method align well with human judgments on both synthetic models and real objects scanned by a depth camera.



### Multi-FAN: Multi-Spectral Mosaic Super-Resolution Via Multi-Scale Feature Aggregation Network
- **Arxiv ID**: http://arxiv.org/abs/1909.07577v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.07577v3)
- **Published**: 2019-09-17 03:53:20+00:00
- **Updated**: 2019-11-06 05:52:55+00:00
- **Authors**: Mehrdad Shoeiby, Sadegh Aliakbarian, Saeed Anwar, Lars Petersson
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a novel method to super-resolve multi-spectral images captured by modern real-time single-shot mosaic image sensors, also known as multi-spectral cameras. Our contribution is two-fold. Firstly, we super-resolve multi-spectral images from mosaic images rather than image cubes, which helps to take into account the spatial offset of each wavelength. Secondly, we introduce an external multi-scale feature aggregation network (Multi-FAN) which concatenates the feature maps with different levels of semantic information throughout a super-resolution (SR) network. A cascade of convolutional layers then implicitly selects the most valuable feature maps to generate a mosaic image. This mosaic image is then merged with the mosaic image generated by the SR network to produce a quantitatively superior image. We apply our Multi-FAN to RCAN (Residual Channel Attention Network), which is the state-of-the-art SR algorithm. We show that Multi-FAN improves both quantitative results and well as inference time.



### Radiopathomics: Integration of radiographic and histologic characteristics for prognostication in glioblastoma
- **Arxiv ID**: http://arxiv.org/abs/1909.07581v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.07581v2)
- **Published**: 2019-09-17 04:32:09+00:00
- **Updated**: 2019-09-19 17:58:20+00:00
- **Authors**: Saima Rathore, Muhammad A. Iftikhar, Metin N. Gurcan, Zissimos Mourelatos
- **Comment**: 10 pages, 5 figures, 1 table
- **Journal**: None
- **Summary**: Both radiographic (Rad) imaging, such as multi-parametric magnetic resonance imaging, and digital pathology (Path) images captured from tissue samples are currently acquired as standard clinical practice for glioblastoma tumors. Both these data streams have been separately used for diagnosis and treatment planning, despite the fact that they provide complementary information. In this research work, we aimed to assess the potential of both Rad and Path images in combination and comparison. An extensive set of engineered features was extracted from delineated tumor regions in Rad images, comprising T1, T1-Gd, T2, T2-FLAIR, and 100 random patches extracted from Path images. Specifically, the features comprised descriptors of intensity, histogram, and texture, mainly quantified via gray-level-co-occurrence matrix and gray-level-run-length matrices. Features extracted from images of 107 glioblastoma patients, downloaded from The Cancer Imaging Archive, were run through support vector machine for classification using leave-one-out cross-validation mechanism, and through support vector regression for prediction of continuous survival outcome. The Pearson correlation coefficient was estimated to be 0.75, 0.74, and 0.78 for Rad, Path and RadPath data. The area-under the receiver operating characteristic curve was estimated to be 0.74, 0.76 and 0.80 for Rad, Path and RadPath data, when patients were discretized into long- and short-survival groups based on average survival cutoff. Our results support the notion that synergistically using Rad and Path images may lead to better prognosis at the initial presentation of the disease, thereby facilitating the targeted enrollment of patients into clinical trials.



### Inverse Visual Question Answering with Multi-Level Attentions
- **Arxiv ID**: http://arxiv.org/abs/1909.07583v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.07583v2)
- **Published**: 2019-09-17 04:41:12+00:00
- **Updated**: 2020-12-03 00:13:21+00:00
- **Authors**: Yaser Alwattar, Yuhong Guo
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel deep multi-level attention model to address inverse visual question answering. The proposed model generates regional visual and semantic features at the object level and then enhances them with the answer cue by using attention mechanisms. Two levels of multiple attentions are employed in the model, including the dual attention at the partial question encoding step and the dynamic attention at the next question word generation step. We evaluate the proposed model on the VQA V1 dataset. It demonstrates state-of-the-art performance in terms of multiple commonly used metrics.



### Conformal Prediction based Spectral Clustering
- **Arxiv ID**: http://arxiv.org/abs/1909.07594v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.07594v1)
- **Published**: 2019-09-17 05:09:01+00:00
- **Updated**: 2019-09-17 05:09:01+00:00
- **Authors**: Lalith Srikanth Chintalapati, Raghunatha Sarma Rachakonda
- **Comment**: Under review in a journal
- **Journal**: None
- **Summary**: Spectral Clustering(SC) is a prominent data clustering technique of recent times which has attracted much attention from researchers. It is a highly data-driven method and makes no strict assumptions on the structure of the data to be clustered. One of the central pieces of spectral clustering is the construction of an affinity matrix based on a similarity measure between data points. The way the similarity measure is defined between data points has a direct impact on the performance of the SC technique. Several attempts have been made in the direction of strengthening the pairwise similarity measure to enhance the spectral clustering. In this work, we have defined a novel affinity measure by employing the concept of non-conformity used in Conformal Prediction(CP) framework. The non-conformity based affinity captures the relationship between neighborhoods of data points and has the power to generalize the notion of contextual similarity. We have shown that this formulation of affinity measure gives good results and compares well with the state of the art methods.



### A Guaranteed Convergence Analysis for the Projected Fast Iterative Soft-Thresholding Algorithm in Parallel MRI
- **Arxiv ID**: http://arxiv.org/abs/1909.07600v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, math.OC, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1909.07600v2)
- **Published**: 2019-09-17 05:37:29+00:00
- **Updated**: 2020-08-04 10:17:55+00:00
- **Authors**: Xinlin Zhang, Hengfa Lu, Di Guo, Lijun Bao, Feng Huang, Qin Xu, Xiaobo Qu
- **Comment**: Main text: 13 pages, 10 figures. Supporting material: 5 pages, 5
  figures
- **Journal**: None
- **Summary**: The boom of non-uniform sampling and compressed sensing techniques dramatically alleviates the lengthy data acquisition problem of magnetic resonance imaging. Sparse reconstruction, thanks to its fast computation and promising performance, has attracted researchers to put numerous efforts on it and has been adopted in commercial scanners. To perform sparse reconstruction, choosing a proper algorithm is essential in providing satisfying results and saving time in tuning parameters. The pFISTA, a simple and efficient algorithm for sparse reconstruction, has been successfully extended to parallel imaging. However, its convergence criterion is still an open question. And the existing convergence criterion of single-coil pFISTA cannot be applied to the parallel imaging pFISTA, which, therefore, imposes confusions and difficulties on users about determining the only parameter - step size. In this work, we provide the guaranteed convergence analysis of the parallel imaging version pFISTA to solve the two well-known parallel imaging reconstruction models, SENSE and SPIRiT. Along with the convergence analysis, we provide recommended step size values for SENSE and SPIRiT reconstructions to obtain fast and promising reconstructions. Experiments on in vivo brain images demonstrate the validity of the convergence criterion. Besides, experimental results show that compared to using backtracking and power iteration to determine the step size, our recommended step size achieves more than five times acceleration in reconstruction time in most tested cases.



### Historical and Modern Features for Buddha Statue Classification
- **Arxiv ID**: http://arxiv.org/abs/1909.12921v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1909.12921v2)
- **Published**: 2019-09-17 06:22:32+00:00
- **Updated**: 2019-10-06 14:07:58+00:00
- **Authors**: Benjamin Renoust, Matheus Oliveira Franca, Jacob Chan, Noa Garcia, Van Le, Ayaka Uesaka, Yuta Nakashima, Hajime Nagahara, Jueren Wang, Yutaka Fujioka
- **Comment**: None
- **Journal**: None
- **Summary**: While Buddhism has spread along the Silk Roads, many pieces of art have been displaced. Only a few experts may identify these works, subjectively to their experience. The construction of Buddha statues was taught through the definition of canon rules, but the applications of those rules greatly varies across time and space. Automatic art analysis aims at supporting these challenges. We propose to automatically recover the proportions induced by the construction guidelines, in order to use them and compare between different deep learning features for several classification tasks, in a medium size but rich dataset of Buddha statues, collected with experts of Buddhism art history.



### Improving the Learning of Multi-column Convolutional Neural Network for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/1909.07608v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07608v1)
- **Published**: 2019-09-17 06:34:47+00:00
- **Updated**: 2019-09-17 06:34:47+00:00
- **Authors**: Zhi-Qi Cheng, Jun-Xiu Li, Qi Dai, Xiao Wu, Jun-Yan He, Alexander Hauptmann
- **Comment**: ACM Multimedia 2019
- **Journal**: None
- **Summary**: Tremendous variation in the scale of people/head size is a critical problem for crowd counting. To improve the scale invariance of feature representation, recent works extensively employ Convolutional Neural Networks with multi-column structures to handle different scales and resolutions. However, due to the substantial redundant parameters in columns, existing multi-column networks invariably exhibit almost the same scale features in different columns, which severely affects counting accuracy and leads to overfitting. In this paper, we attack this problem by proposing a novel Multi-column Mutual Learning (McML) strategy. It has two main innovations: 1) A statistical network is incorporated into the multi-column framework to estimate the mutual information between columns, which can approximately indicate the scale correlation between features from different columns. By minimizing the mutual information, each column is guided to learn features with different image scales. 2) We devise a mutual learning scheme that can alternately optimize each column while keeping the other columns fixed on each mini-batch training data. With such asynchronous parameter update process, each column is inclined to learn different feature representation from others, which can efficiently reduce the parameter redundancy and improve generalization ability. More remarkably, McML can be applied to all existing multi-column networks and is end-to-end trainable. Extensive experiments on four challenging benchmarks show that McML can significantly improve the original multi-column networks and outperform the other state-of-the-art approaches.



### BUDA.ART: A Multimodal Content-Based Analysis and Retrieval System for Buddha Statues
- **Arxiv ID**: http://arxiv.org/abs/1909.12932v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1909.12932v1)
- **Published**: 2019-09-17 06:35:24+00:00
- **Updated**: 2019-09-17 06:35:24+00:00
- **Authors**: Benjamin Renoust, Matheus Oliveira Franca, Jacob Chan, Van Le, Ayaka Uesaka, Yuta Nakashima, Hajime Nagahara, Jueren Wang, Yutaka Fujioka
- **Comment**: Demo video at: https://www.youtube.com/watch?v=3XJvLjSWieY
- **Journal**: None
- **Summary**: We introduce BUDA.ART, a system designed to assist researchers in Art History, to explore and analyze an archive of pictures of Buddha statues. The system combines different CBIR and classical retrieval techniques to assemble 2D pictures, 3D statue scans and meta-data, that is focused on the Buddha facial characteristics. We build the system from an archive of 50,000 Buddhism pictures, identify unique Buddha statues, extract contextual information, and provide specific facial embedding to first index the archive. The system allows for mobile, on-site search, and to explore similarities of statues in the archive. In addition, we provide search visualization and 3D analysis of the statues



### Alleviating Feature Confusion for Generative Zero-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.07615v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1909.07615v1)
- **Published**: 2019-09-17 06:59:48+00:00
- **Updated**: 2019-09-17 06:59:48+00:00
- **Authors**: Jingjing Li, Mengmeng Jing, Ke Lu, Lei Zhu, Yang Yang, Zi Huang
- **Comment**: Our codes can be found at github.com/lijin118/AFC-GAN
- **Journal**: ACM Multimedia 2019
- **Summary**: Lately, generative adversarial networks (GANs) have been successfully applied to zero-shot learning (ZSL) and achieved state-of-the-art performance. By synthesizing virtual unseen visual features, GAN-based methods convert the challenging ZSL task into a supervised learning problem. However, GAN-based ZSL methods have to train the generator on the seen categories and further apply it to unseen instances. An inevitable issue of such a paradigm is that the synthesized unseen features are prone to seen references and incapable to reflect the novelty and diversity of real unseen instances. In a nutshell, the synthesized features are confusing. One cannot tell unseen categories from seen ones using the synthesized features. As a result, the synthesized features are too subtle to be classified in generalized zero-shot learning (GZSL) which involves both seen and unseen categories at the test stage. In this paper, we first introduce the feature confusion issue. Then, we propose a new feature generating network, named alleviating feature confusion GAN (AFC-GAN), to challenge the issue. Specifically, we present a boundary loss which maximizes the decision boundary of seen categories and unseen ones. Furthermore, a novel metric named feature confusion score (FCS) is proposed to quantify the feature confusion. Extensive experiments on five widely used datasets verify that our method is able to outperform previous state-of-the-arts under both ZSL and GZSL protocols.



### Cycle-consistent Conditional Adversarial Transfer Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.07618v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1909.07618v1)
- **Published**: 2019-09-17 07:14:26+00:00
- **Updated**: 2019-09-17 07:14:26+00:00
- **Authors**: Jingjing Li, Erpeng Chen, Zhengming Ding, Lei Zhu, Ke Lu, Zi Huang
- **Comment**: Codes at github.com/lijin118/3CATN
- **Journal**: ACM Multimedia 2019
- **Summary**: Domain adaptation investigates the problem of cross-domain knowledge transfer where the labeled source domain and unlabeled target domain have distinctive data distributions. Recently, adversarial training have been successfully applied to domain adaptation and achieved state-of-the-art performance. However, there is still a fatal weakness existing in current adversarial models which is raised from the equilibrium challenge of adversarial training. Specifically, although most of existing methods are able to confuse the domain discriminator, they cannot guarantee that the source domain and target domain are sufficiently similar. In this paper, we propose a novel approach named {\it cycle-consistent conditional adversarial transfer networks} (3CATN) to handle this issue. Our approach takes care of the domain alignment by leveraging adversarial training. Specifically, we condition the adversarial networks with the cross-covariance of learned features and classifier predictions to capture the multimodal structures of data distributions. However, since the classifier predictions are not certainty information, a strong condition with the predictions is risky when the predictions are not accurate. We, therefore, further propose that the truly domain-invariant features should be able to be translated from one domain to the other. To this end, we introduce two feature translation losses and one cycle-consistent loss into the conditional adversarial domain adaptation networks. Extensive experiments on both classical and large-scale datasets verify that our model is able to outperform previous state-of-the-arts with significant improvements.



### Deep End-to-End Alignment and Refinement for Time-of-Flight RGB-D Module
- **Arxiv ID**: http://arxiv.org/abs/1909.07623v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07623v1)
- **Published**: 2019-09-17 07:25:42+00:00
- **Updated**: 2019-09-17 07:25:42+00:00
- **Authors**: Di Qiu, Jiahao Pang, Wenxiu Sun, Chengxi Yang
- **Comment**: ICCV2019
- **Journal**: None
- **Summary**: Recently, it is increasingly popular to equip mobile RGB cameras with Time-of-Flight (ToF) sensors for active depth sensing. However, for off-the-shelf ToF sensors, one must tackle two problems in order to obtain high-quality depth with respect to the RGB camera, namely 1) online calibration and alignment; and 2) complicated error correction for ToF depth sensing. In this work, we propose a framework for jointly alignment and refinement via deep learning. First, a cross-modal optical flow between the RGB image and the ToF amplitude image is estimated for alignment. The aligned depth is then refined via an improved kernel predicting network that performs kernel normalization and applies the bias prior to the dynamic convolution. To enrich our data for end-to-end training, we have also synthesized a dataset using tools from computer graphics. Experimental results demonstrate the effectiveness of our approach, achieving state-of-the-art for ToF refinement.



### Thanks for Nothing: Predicting Zero-Valued Activations with Lightweight Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.07636v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07636v3)
- **Published**: 2019-09-17 07:56:54+00:00
- **Updated**: 2020-07-13 13:07:28+00:00
- **Authors**: Gil Shomron, Ron Banner, Moran Shkolnik, Uri Weiser
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) introduce state-of-the-art results for various tasks with the price of high computational demands. Inspired by the observation that spatial correlation exists in CNN output feature maps (ofms), we propose a method to dynamically predict whether ofm activations are zero-valued or not according to their neighboring activation values, thereby avoiding zero-valued activations and reducing the number of convolution operations. We implement the zero activation predictor (ZAP) with a lightweight CNN, which imposes negligible overheads and is easy to deploy on existing models. ZAPs are trained by mimicking hidden layer ouputs; thereby, enabling a parallel and label-free training. Furthermore, without retraining, each ZAP can be tuned to a different operating point trading accuracy for MAC reduction.



### Progressive Fusion for Unsupervised Binocular Depth Estimation using Cycled Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.07667v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07667v1)
- **Published**: 2019-09-17 09:21:02+00:00
- **Updated**: 2019-09-17 09:21:02+00:00
- **Authors**: Andrea Pilzer, Stéphane Lathuilière, Dan Xu, Mihai Marian Puscas, Elisa Ricci, Nicu Sebe
- **Comment**: Accepted to TPAMI (SI RGB-D Vision), code
  https://github.com/andrea-pilzer/PFN-depth
- **Journal**: None
- **Summary**: Recent deep monocular depth estimation approaches based on supervised regression have achieved remarkable performance. However, they require costly ground truth annotations during training. To cope with this issue, in this paper we present a novel unsupervised deep learning approach for predicting depth maps. We introduce a new network architecture, named Progressive Fusion Network (PFN), that is specifically designed for binocular stereo depth estimation. This network is based on a multi-scale refinement strategy that combines the information provided by both stereo views. In addition, we propose to stack twice this network in order to form a cycle. This cycle approach can be interpreted as a form of data-augmentation since, at training time, the network learns both from the training set images (in the forward half-cycle) but also from the synthesized images (in the backward half-cycle). The architecture is jointly trained with adversarial learning. Extensive experiments on the publicly available datasets KITTI, Cityscapes and ApolloScape demonstrate the effectiveness of the proposed model which is competitive with other unsupervised deep learning methods for depth prediction.



### Spatio-Semantic ConvNet-Based Visual Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/1909.07671v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, I.2.10; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/1909.07671v1)
- **Published**: 2019-09-17 09:30:24+00:00
- **Updated**: 2019-09-17 09:30:24+00:00
- **Authors**: Luis G. Camara, Libor Přeučil
- **Comment**: Accepted in Proceedings of the 2019 European Conference on Mobile
  Robots (ECMR 2019), Prague, Czech Republic, September 4-6, 2019
- **Journal**: None
- **Summary**: We present a Visual Place Recognition system that follows the two-stage format common to image retrieval pipelines. The system encodes images of places by employing the activations of different layers of a pre-trained, off-the-shelf, VGG16 Convolutional Neural Network (CNN) architecture. In the first stage of our method and given a query image of a place, a number of top candidate images is retrieved from a previously stored database of places. In the second stage, we propose an exhaustive comparison of the query image against these candidates by encoding semantic and spatial information in the form of CNN features. Results from our approach outperform by a large margin state-of-the-art visual place recognition methods on five of the most commonly used benchmark datasets. The performance gain is especially remarkable on the most challenging datasets, with more than a twofold recognition improvement with respect to the latest published work.



### Learning to Find Hydrological Corrections
- **Arxiv ID**: http://arxiv.org/abs/1909.07685v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1909.07685v1)
- **Published**: 2019-09-17 09:54:38+00:00
- **Updated**: 2019-09-17 09:54:38+00:00
- **Authors**: Lars Arge, Allan Grønlund, Svend Christian Svendsen, Jonas Tranberg
- **Comment**: 27th ACM SIGSPATIAL International Conference on Advances in
  Geographic Information Systems (ACM SIGSPATIAL 2019)
- **Journal**: None
- **Summary**: High resolution Digital Elevation models, such as the (Big) grid terrain model of Denmark with more than 200 billion measurements, is a basic requirement for water flow modelling and flood risk analysis. However, a large number of modifications often need to be made to even very accurate terrain models, such as the Danish model, before they can be used in realistic flow modeling. These modifications include removal of bridges, which otherwise will act as dams in flow modeling, and inclusion of culverts that transport water underneath roads. In fact, the danish model is accompanied by a detailed set of hydrological corrections for the digital elevation model. However, producing these hydrological corrections is a very slow an expensive process, since it is to a large extent done manually and often with local input. This also means that corrections can be of varying quality. In this paper we propose a new algorithmic apporach based on machine learning and convolutional neural networks for automatically detecting hydrological corrections for such large terrain data. Our model is able to detect most hydrological corrections known for the danish model and quite a few more that should have been included in the original list.



### Multi-Task Learning for Automotive Foggy Scene Understanding via Domain Adaptation to an Illumination-Invariant Representation
- **Arxiv ID**: http://arxiv.org/abs/1909.07697v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.07697v1)
- **Published**: 2019-09-17 10:18:14+00:00
- **Updated**: 2019-09-17 10:18:14+00:00
- **Authors**: Naif Alshammari, Samet Akçay, Toby P. Breckon
- **Comment**: Conference submission
- **Journal**: None
- **Summary**: Joint scene understanding and segmentation for automotive applications is a challenging problem in two key aspects:- (1) classifying every pixel in the entire scene and (2) performing this task under unstable weather and illumination changes (e.g. foggy weather), which results in poor outdoor scene visibility. This poor outdoor scene visibility leads to a non-optimal performance of deep convolutional neural network-based scene understanding and segmentation. In this paper, we propose an efficient end-to-end contemporary automotive semantic scene understanding approach under foggy weather conditions, employing domain adaptation and illumination-invariant image per-transformation. As a multi-task pipeline, our proposed model provides:- (1) transferring images from extreme to clear-weather condition using domain transfer approach and (2) semantically segmenting a scene using a competitive encoder-decoder convolutional neural network (CNN) with dense connectivity, skip connections and fusion-based techniques. We evaluate our approach on challenging foggy datasets, including synthetic dataset (Foggy Cityscapes) as well as real-world datasets (Foggy Zurich and Foggy Driving). By incorporating RGB, depth, and illumination-invariant information, our approach outperforms the state-of-the-art within automotive scene understanding, under foggy weather condition.



### Task-Aware Monocular Depth Estimation for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1909.07701v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07701v2)
- **Published**: 2019-09-17 10:24:17+00:00
- **Updated**: 2019-12-09 10:15:56+00:00
- **Authors**: Xinlong Wang, Wei Yin, Tao Kong, Yuning Jiang, Lei Li, Chunhua Shen
- **Comment**: Accepted by AAAI2020 as Oral
- **Journal**: None
- **Summary**: Monocular depth estimation enables 3D perception from a single 2D image, thus attracting much research attention for years. Almost all methods treat foreground and background regions ("things and stuff") in an image equally. However, not all pixels are equal. Depth of foreground objects plays a crucial role in 3D object recognition and localization. To date how to boost the depth prediction accuracy of foreground objects is rarely discussed. In this paper, we first analyse the data distributions and interaction of foreground and background, then propose the foreground-background separated monocular depth estimation (ForeSeE) method, to estimate the foreground depth and background depth using separate optimization objectives and depth decoders. Our method significantly improves the depth estimation performance on foreground objects. Applying ForeSeE to 3D object detection, we achieve 7.5 AP gains and set new state-of-the-art results among other monocular methods. Code will be available at: https://github.com/WXinlong/ForeSeE.



### re-OBJ: Jointly Learning the Foreground and Background for Object Instance Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1909.07704v2
- **DOI**: 10.1007/978-3-030-30645-8_37
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07704v2)
- **Published**: 2019-09-17 10:36:12+00:00
- **Updated**: 2019-09-23 17:21:55+00:00
- **Authors**: Vaibhav Bansal, Stuart James, Alessio Del Bue
- **Comment**: Accepted to ICIAP 2019 and awarded the Best Student Paper
- **Journal**: None
- **Summary**: Conventional approaches to object instance re-identification rely on matching appearances of the target objects among a set of frames. However, learning appearances of the objects alone might fail when there are multiple objects with similar appearance or multiple instances of same object class present in the scene. This paper proposes that partial observations of the background can be utilized to aid in the object re-identification task for a rigid scene, especially a rigid environment with a lot of reoccurring identical models of objects. Using an extension to the Mask R-CNN architecture, we learn to encode the important and distinct information in the background jointly with the foreground relevant to rigid real-world scenarios such as an indoor environment where objects are static and the camera moves around the scene. We demonstrate the effectiveness of our joint visual feature in the re-identification of objects in the ScanNet dataset and show a relative improvement of around 28.25% in the rank-1 accuracy over the deepSort method.



### DS-PASS: Detail-Sensitive Panoramic Annular Semantic Segmentation through SwaftNet for Surrounding Sensing
- **Arxiv ID**: http://arxiv.org/abs/1909.07721v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1909.07721v2)
- **Published**: 2019-09-17 11:20:54+00:00
- **Updated**: 2020-02-07 16:09:54+00:00
- **Authors**: Kailun Yang, Xinxin Hu, Hao Chen, Kaite Xiang, Kaiwei Wang, Rainer Stiefelhagen
- **Comment**: 8 pages, 10 figures
- **Journal**: None
- **Summary**: Semantically interpreting the traffic scene is crucial for autonomous transportation and robotics systems. However, state-of-the-art semantic segmentation pipelines are dominantly designed to work with pinhole cameras and train with narrow Field-of-View (FoV) images. In this sense, the perception capacity is severely limited to offer higher-level confidence for upstream navigation tasks. In this paper, we propose a network adaptation framework to achieve Panoramic Annular Semantic Segmentation (PASS), which allows to re-use conventional pinhole-view image datasets, enabling modern segmentation networks to comfortably adapt to panoramic images. Specifically, we adapt our proposed SwaftNet to enhance the sensitivity to details by implementing attention-based lateral connections between the detail-critical encoder layers and the context-critical decoder layers. We benchmark the performance of efficient segmenters on panoramic segmentation with our extended PASS dataset, demonstrating that the proposed real-time SwaftNet outperforms state-of-the-art efficient networks. Furthermore, we assess real-world performance when deploying the Detail-Sensitive PASS (DS-PASS) system on a mobile robot and an instrumented vehicle, as well as the benefit of panoramic semantics for visual odometry, showing the robustness and potential to support diverse navigational applications.



### Deep Point-wise Prediction for Action Temporal Proposal
- **Arxiv ID**: http://arxiv.org/abs/1909.07725v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07725v1)
- **Published**: 2019-09-17 11:30:52+00:00
- **Updated**: 2019-09-17 11:30:52+00:00
- **Authors**: Luxuan Li, Tao Kong, Fuchun Sun, Huaping Liu
- **Comment**: accepted by ICONIP2019 oral presentation (International Conference on
  Neural Information Processing)
- **Journal**: None
- **Summary**: Detecting actions in videos is an important yet challenging task. Previous works usually utilize (a) sliding window paradigms, or (b) per-frame action scoring and grouping to enumerate the possible temporal locations. Their performances are also limited to the designs of sliding windows or grouping strategies. In this paper, we present a simple and effective method for temporal action proposal generation, named Deep Point-wise Prediction (DPP). DPP simultaneously predicts the action existing possibility and the corresponding temporal locations, without the utilization of any handcrafted sliding window or grouping. The whole system is end-to-end trained with joint loss of temporal action proposal classification and location prediction. We conduct extensive experiments to verify its effectiveness, generality and robustness on standard THUMOS14 dataset. DPP runs more than 1000 frames per second, which largely satisfies the real-time requirement. The code is available at https://github.com/liluxuan1997/DPP.



### Building Change Detection for Remote Sensing Images Using a Dual Task Constrained Deep Siamese Convolutional Network Model
- **Arxiv ID**: http://arxiv.org/abs/1909.07726v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07726v1)
- **Published**: 2019-09-17 11:34:19+00:00
- **Updated**: 2019-09-17 11:34:19+00:00
- **Authors**: Yi Liu, Chao Pang, Zongqian Zhan, Xiaomeng Zhang, Xue Yang
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, building change detection methods have made great progress by introducing deep learning, but they still suffer from the problem of the extracted features not being discriminative enough, resulting in incomplete regions and irregular boundaries. To tackle this problem, we propose a dual task constrained deep Siamese convolutional network (DTCDSCN) model, which contains three sub-networks: a change detection network and two semantic segmentation networks. DTCDSCN can accomplish both change detection and semantic segmentation at the same time, which can help to learn more discriminative object-level features and obtain a complete change detection map. Furthermore, we introduce a dual attention module (DAM) to exploit the interdependencies between channels and spatial positions, which improves the feature representation. We also improve the focal loss function to suppress the sample imbalance problem. The experimental results obtained with the WHU building dataset show that the proposed method is effective for building change detection and achieves a state-of-the-art performance in terms of four metrics: precision, recall, F1-score, and intersection over union.



### An Image Based Visual Servo Approach with Deep Learning for Robotic Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1909.07727v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07727v1)
- **Published**: 2019-09-17 11:38:09+00:00
- **Updated**: 2019-09-17 11:38:09+00:00
- **Authors**: Jingshu Liu, Yuan Li
- **Comment**: Accepted by The 6th International Workshop on Advanced Computational
  Intelligence and Intelligent Informatics (IWACIII2019)
- **Journal**: None
- **Summary**: Aiming at the difficulty of extracting image features and estimating the Jacobian matrix in image based visual servo, this paper proposes an image based visual servo approach with deep learning. With the powerful learning capabilities of convolutional neural networks(CNN), autonomous learning to extract features from images and fitting the nonlinear relationships from image space to task space is achieved, which can greatly facilitate the image based visual servo procedure. Based on the above ideas a two-stream network based on convolutional neural network is designed and the corresponding control scheme is proposed to realize the four degrees of freedom visual servo of the robot manipulator. Collecting images of observed target under different pose parameters of the manipulator as training samples for CNN, the trained network can be used to estimate the nonlinear relationship from 2D image space to 3D Cartesian space. The two-stream network takes the current image and the desirable image as inputs and makes them equal to guide the manipulator to the desirable pose. The effectiveness of the approach is verified with experimental results.



### ICDAR 2019 Competition on Large-scale Street View Text with Partial Labeling -- RRC-LSVT
- **Arxiv ID**: http://arxiv.org/abs/1909.07741v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1909.07741v1)
- **Published**: 2019-09-17 12:09:33+00:00
- **Updated**: 2019-09-17 12:09:33+00:00
- **Authors**: Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu, Canjie Luo, Chun Chet Ng, Junyu Han, Errui Ding, Jingtuo Liu, Dimosthenis Karatzas, Chee Seng Chan, Lianwen Jin
- **Comment**: ICDAR 2019 Robust Reading Challenge in IAPR International Conference
  on Document Analysis and Recognition (ICDAR)
- **Journal**: None
- **Summary**: Robust text reading from street view images provides valuable information for various applications. Performance improvement of existing methods in such a challenging scenario heavily relies on the amount of fully annotated training data, which is costly and in-efficient to obtain. To scale up the amount of training data while keeping the labeling procedure cost-effective, this competition introduces a new challenge on Large-scale Street View Text with Partial Labeling (LSVT), providing 50, 000 and 400, 000 images in full and weak annotations, respectively. This competition aims to explore the abilities of state-of-the-art methods to detect and recognize text instances from large-scale street view images, closing the gap between research benchmarks and real applications. During the competition period, a total of 41 teams participated in the two proposed tasks with 132 valid submissions, i.e., text detection and end-to-end text spotting. This paper includes dataset descriptions, task definitions, evaluation protocols and results summaries of the ICDAR 2019-LSVT challenge.



### Adversarial Feature Training for Generalizable Robotic Visuomotor Control
- **Arxiv ID**: http://arxiv.org/abs/1909.07745v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.07745v1)
- **Published**: 2019-09-17 12:18:34+00:00
- **Updated**: 2019-09-17 12:18:34+00:00
- **Authors**: Xi Chen, Ali Ghadirzadeh, Mårten Björkman, Patric Jensfelt
- **Comment**: None
- **Journal**: None
- **Summary**: Deep reinforcement learning (RL) has enabled training action-selection policies, end-to-end, by learning a function which maps image pixels to action outputs. However, it's application to visuomotor robotic policy training has been limited because of the challenge of large-scale data collection when working with physical hardware. A suitable visuomotor policy should perform well not just for the task-setup it has been trained for, but also for all varieties of the task, including novel objects at different viewpoints surrounded by task-irrelevant objects. However, it is impractical for a robotic setup to sufficiently collect interactive samples in a RL framework to generalize well to novel aspects of a task. In this work, we demonstrate that by using adversarial training for domain transfer, it is possible to train visuomotor policies based on RL frameworks, and then transfer the acquired policy to other novel task domains. We propose to leverage the deep RL capabilities to learn complex visuomotor skills for uncomplicated task setups, and then exploit transfer learning to generalize to new task domains provided only still images of the task in the target domain. We evaluate our method on two real robotic tasks, picking and pouring, and compare it to a number of prior works, demonstrating its superiority.



### A machine vision meta-algorithm for automated recognition of underwater objects using sidescan sonar imagery
- **Arxiv ID**: http://arxiv.org/abs/1909.07763v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.07763v1)
- **Published**: 2019-09-17 13:15:11+00:00
- **Updated**: 2019-09-17 13:15:11+00:00
- **Authors**: Guillaume Labbe-Morissette, Sylvain Gauthier
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: This paper details a new method to recognize and detect underwater objects in real-time sidescan sonar data imagery streams, with case-studies of applications for underwater archeology, and ghost fishing gear retrieval. We first synthesize images from sidescan data, apply geometric and radiometric corrections, then use 2D feature detection algorithms to identify point clouds of descriptive visual microfeatures such as corners and edges in the sonar images. We then apply a clustering algorithm on the feature point clouds to group feature sets into regions of interest, reject false positives, yielding a georeferenced inventory of objects.



### Single-shot 3D shape reconstruction using deep convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1909.07766v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.07766v1)
- **Published**: 2019-09-17 13:19:31+00:00
- **Updated**: 2019-09-17 13:19:31+00:00
- **Authors**: Hieu Nguyen, Hui Li, Qiang Qiu, Yuzeng Wang, Zhaoyang Wang
- **Comment**: 6 pages, 4 figures, 1 dataset
- **Journal**: None
- **Summary**: A robust single-shot 3D shape reconstruction technique integrating the fringe projection profilometry (FPP) technique with the deep convolutional neural networks (CNNs) is proposed in this letter. The input of the proposed technique is a single FPP image, and the training and validation data sets are prepared by using the conventional multi-frequency FPP technique. Unlike the conventional 3D shape reconstruction methods which involve complex algorithms and intensive computation, the proposed approach uses an end-to-end network architecture to directly carry out the transformation of a 2D images to its corresponding 3D shape. Experiments have been conducted to demonstrate the validity and robustness of the proposed technique. It is capable of satisfying various 3D shape reconstruction demands in scientific research and engineering applications.



### CMTS: Conditional Multiple Trajectory Synthesizer for Generating Safety-critical Driving Scenarios
- **Arxiv ID**: http://arxiv.org/abs/1910.00099v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.00099v2)
- **Published**: 2019-09-17 13:43:14+00:00
- **Updated**: 2019-10-02 18:03:36+00:00
- **Authors**: Wenhao Ding, Mengdi Xu, Ding Zhao
- **Comment**: Submitted to ICRA 2020, 8 pages, 7 figures
- **Journal**: None
- **Summary**: Naturalistic driving trajectories are crucial for the performance of autonomous driving algorithms. However, most of the data is collected in safe scenarios leading to the duplication of trajectories which are easy to be handled by currently developed algorithms. When considering safety, testing algorithms in near-miss scenarios that rarely show up in off-the-shelf datasets is a vital part of the evaluation. As a remedy, we propose a near-miss data synthesizing framework based on Variational Bayesian methods and term it as Conditional Multiple Trajectory Synthesizer (CMTS). We leverage a generative model conditioned on road maps to bridge safe and collision driving data by representing their distribution in the latent space. By sampling from the near-miss distribution, we can synthesize safety-critical data crucial for understanding traffic scenarios but not shown in neither the original dataset nor the collision dataset. Our experimental results demonstrate that the augmented dataset covers more kinds of driving scenarios, especially the near-miss ones, which help improve the trajectory prediction accuracy and the capability of dealing with risky driving scenarios.



### Chinese Street View Text: Large-scale Chinese Text Reading with Partially Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.07808v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1909.07808v2)
- **Published**: 2019-09-17 13:54:24+00:00
- **Updated**: 2020-02-13 04:50:38+00:00
- **Authors**: Yipeng Sun, Jiaming Liu, Wei Liu, Junyu Han, Errui Ding, Jingtuo Liu
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Most existing text reading benchmarks make it difficult to evaluate the performance of more advanced deep learning models in large vocabularies due to the limited amount of training data. To address this issue, we introduce a new large-scale text reading benchmark dataset named Chinese Street View Text (C-SVT) with 430,000 street view images, which is at least 14 times as large as the existing Chinese text reading benchmarks. To recognize Chinese text in the wild while keeping large-scale datasets labeling cost-effective, we propose to annotate one part of the CSVT dataset (30,000 images) in locations and text labels as full annotations and add 400,000 more images, where only the corresponding text-of-interest in the regions is given as weak annotations. To exploit the rich information from the weakly annotated data, we design a text reading network in a partially supervised learning framework, which enables to localize and recognize text, learn from fully and weakly annotated data simultaneously. To localize the best matched text proposals from weakly labeled images, we propose an online proposal matching module incorporated in the whole model, spotting the keyword regions by sharing parameters for end-to-end training. Compared with fully supervised training algorithms, this model can improve the end-to-end recognition performance remarkably by 4.03% in F-score at the same labeling cost. The proposed model can also achieve state-of-the-art results on the ICDAR 2017-RCTW dataset, which demonstrates the effectiveness of the proposed partially supervised learning framework.



### Learn to Segment Organs with a Few Bounding Boxes
- **Arxiv ID**: http://arxiv.org/abs/1909.07809v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07809v1)
- **Published**: 2019-09-17 13:56:37+00:00
- **Updated**: 2019-09-17 13:56:37+00:00
- **Authors**: Abhijeet Parida, Arianne Tran, Nassir Navab, Shadi Albarqouni
- **Comment**: Under Review
- **Journal**: None
- **Summary**: Semantic segmentation is an import task in the medical field to identify the exact extent and orientation of significant structures like organs and pathology. Deep neural networks can perform this task well by leveraging the information from a large well-labeled data-set. This paper aims to present a method that mitigates the necessity of an extensive well-labeled data-set. This method also addresses semi-supervision by enabling segmentation based on bounding box annotations, avoiding the need for full pixel-level annotations. The network presented consists of a single U-Net based unbranched architecture that generates a few-shot segmentation for an unseen human organ using just 4 example annotations of that specific organ. The network is trained by alternately minimizing the nearest neighbor loss for prototype learning and a weighted cross-entropy loss for segmentation learning to perform a fast 3D segmentation with a median score of 54.64%.



### Learning Deformable Point Set Registration with Regularized Dynamic Graph CNNs for Large Lung Motion in COPD Patients
- **Arxiv ID**: http://arxiv.org/abs/1909.07818v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07818v1)
- **Published**: 2019-09-17 13:59:04+00:00
- **Updated**: 2019-09-17 13:59:04+00:00
- **Authors**: Lasse Hansen, Doris Dittmer, Mattias P. Heinrich
- **Comment**: accepted for MICCAI 2019 Workshop Graph Learning in Medical Imaging
- **Journal**: None
- **Summary**: Deformable registration continues to be one of the key challenges in medical image analysis. While iconic registration methods have started to benefit from the recent advances in medical deep learning, the same does not yet apply for the registration of point sets, e.g. registration based on surfaces, keypoints or landmarks. This is mainly due to the restriction of the convolution operator in modern CNNs to densely gridded input. However, with the newly developed methods from the field of geometric deep learning suitable tools are now emerging, which enable powerful analysis of medical data on irregular domains. In this work, we present a new method that enables the learning of regularized feature descriptors with dynamic graph CNNs. By incorporating the learned geometric features as prior probabilities into the well-established coherent point drift (CPD) algorithm, formulated as differentiable network layer, we establish an end-to-end framework for robust registration of two point sets. Our approach is evaluated on the challenging task of aligning keypoints extracted from lung CT scans in inhale and exhale states with large deformations and without any additional intensity information. Our results indicate that the inherent geometric structure of the extracted keypoints is sufficient to establish descriptive point features, which yield a significantly improved performance and robustness of our registration framework.



### Weak Edge Identification Nets for Ocean Front Detection
- **Arxiv ID**: http://arxiv.org/abs/1909.07827v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07827v1)
- **Published**: 2019-09-17 14:10:45+00:00
- **Updated**: 2019-09-17 14:10:45+00:00
- **Authors**: Qingyang Li, Guoqiang Zhong, Cui Xie
- **Comment**: None
- **Journal**: None
- **Summary**: The ocean front has an important impact in many areas, it is meaningful to obtain accurate ocean front positioning, therefore, ocean front detection is a very important task. However, the traditional edge detection algorithm does not detect the weak edge information of the ocean front very well. In response to this problem, we collected relevant ocean front gradient images and found relevant experts to calibrate the ocean front data to obtain groundtruth, and proposed a weak edge identification nets(WEIN) for ocean front detection. Whether it is qualitative or quantitative, our methods perform best. The method uses a welltrained deep learning model to accurately extract the ocean front from the ocean front gradient image. The detection network is divided into multiple stages, and the final output is a multi-stage output image fusion. The method uses the stochastic gradient descent and the correlation loss function to obtain a good ocean front image output.



### AdaptIS: Adaptive Instance Selection Network
- **Arxiv ID**: http://arxiv.org/abs/1909.07829v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07829v1)
- **Published**: 2019-09-17 14:11:42+00:00
- **Updated**: 2019-09-17 14:11:42+00:00
- **Authors**: Konstantin Sofiiuk, Olga Barinova, Anton Konushin
- **Comment**: Accepted at ICCV 2019
- **Journal**: None
- **Summary**: We present Adaptive Instance Selection network architecture for class-agnostic instance segmentation. Given an input image and a point $(x, y)$, it generates a mask for the object located at $(x, y)$. The network adapts to the input point with a help of AdaIN layers, thus producing different masks for different objects on the same image. AdaptIS generates pixel-accurate object masks, therefore it accurately segments objects of complex shape or severely occluded ones. AdaptIS can be easily combined with standard semantic segmentation pipeline to perform panoptic segmentation. To illustrate the idea, we perform experiments on a challenging toy problem with difficult occlusions. Then we extensively evaluate the method on panoptic segmentation benchmarks. We obtain state-of-the-art results on Cityscapes and Mapillary even without pretraining on COCO, and show competitive results on a challenging COCO dataset. The source code of the method and the trained models are available at https://github.com/saic-vul/adaptis.



### Multimodal Multitask Representation Learning for Pathology Biobank Metadata Prediction
- **Arxiv ID**: http://arxiv.org/abs/1909.07846v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.07846v1)
- **Published**: 2019-09-17 14:34:37+00:00
- **Updated**: 2019-09-17 14:34:37+00:00
- **Authors**: Wei-Hung Weng, Yuannan Cai, Angela Lin, Fraser Tan, Po-Hsuan Cameron Chen
- **Comment**: preprint version
- **Journal**: None
- **Summary**: Metadata are general characteristics of the data in a well-curated and condensed format, and have been proven to be useful for decision making, knowledge discovery, and also heterogeneous data organization of biobank. Among all data types in the biobank, pathology is the key component of the biobank and also serves as the gold standard of diagnosis. To maximize the utility of biobank and allow the rapid progress of biomedical science, it is essential to organize the data with well-populated pathology metadata. However, manual annotation of such information is tedious and time-consuming. In the study, we develop a multimodal multitask learning framework to predict four major slide-level metadata of pathology images. The framework learns generalizable representations across tissue slides, pathology reports, and case-level structured data. We demonstrate improved performance across all four tasks with the proposed method compared to a single modal single task baseline on two test sets, one external test set from a distinct data source (TCGA) and one internal held-out test set (TTH). In the test sets, the performance improvements on the averaged area under receiver operating characteristic curve across the four tasks are 16.48% and 9.05% on TCGA and TTH, respectively. Such pathology metadata prediction system may be adopted to mitigate the effort of expert annotation and ultimately accelerate the data-driven research by better utilization of the pathology biobank.



### Character-Centric Storytelling
- **Arxiv ID**: http://arxiv.org/abs/1909.07863v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.07863v3)
- **Published**: 2019-09-17 14:51:59+00:00
- **Updated**: 2020-01-06 10:11:42+00:00
- **Authors**: Aditya Surikuchi, Jorma Laaksonen
- **Comment**: ACL Storytelling (StoryNLP)
- **Journal**: None
- **Summary**: Sequential vision-to-language or visual storytelling has recently been one of the areas of focus in computer vision and language modeling domains. Though existing models generate narratives that read subjectively well, there could be cases when these models miss out on generating stories that account and address all prospective human and animal characters in the image sequences. Considering this scenario, we propose a model that implicitly learns relationships between provided characters and thereby generates stories with respective characters in scope. We use the VIST dataset for this purpose and report numerous statistics on the dataset. Eventually, we describe the model, explain the experiment and discuss our current status and future work.



### TruPercept: Trust Modelling for Autonomous Vehicle Cooperative Perception from Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/1909.07867v1
- **DOI**: None
- **Categories**: **cs.MA**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.07867v1)
- **Published**: 2019-09-17 14:57:45+00:00
- **Updated**: 2019-09-17 14:57:45+00:00
- **Authors**: Braden Hurl, Robin Cohen, Krzysztof Czarnecki, Steven Waslander
- **Comment**: None
- **Journal**: None
- **Summary**: Inter-vehicle communication for autonomous vehicles (AVs) stands to provide significant benefits in terms of perception robustness. We propose a novel approach for AVs to communicate perceptual observations, tempered by trust modelling of peers providing reports. Based on the accuracy of reported object detections as verified locally, communicated messages can be fused to augment perception performance beyond line of sight and at great distance from the ego vehicle. Also presented is a new synthetic dataset which can be used to test cooperative perception. The TruPercept dataset includes unreliable and malicious behaviour scenarios to experiment with some challenges cooperative perception introduces. The TruPercept runtime and evaluation framework allows modular component replacement to facilitate ablation studies as well as the creation of new trust scenarios we are able to show.



### Multi-mapping Image-to-Image Translation via Learning Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/1909.07877v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07877v2)
- **Published**: 2019-09-17 15:10:47+00:00
- **Updated**: 2019-12-26 13:01:10+00:00
- **Authors**: Xiaoming Yu, Yuanqi Chen, Thomas Li, Shan Liu, Ge Li
- **Comment**: Accepted by NeurIPS 2019. Code will be available at
  https://github.com/Xiaoming-Yu/DMIT
- **Journal**: None
- **Summary**: Recent advances of image-to-image translation focus on learning the one-to-many mapping from two aspects: multi-modal translation and multi-domain translation. However, the existing methods only consider one of the two perspectives, which makes them unable to solve each other's problem. To address this issue, we propose a novel unified model, which bridges these two objectives. First, we disentangle the input images into the latent representations by an encoder-decoder architecture with a conditional adversarial training in the feature space. Then, we encourage the generator to learn multi-mappings by a random cross-domain translation. As a result, we can manipulate different parts of the latent representations to perform multi-modal and multi-domain translations simultaneously. Experiments demonstrate that our method outperforms state-of-the-art methods.



### PixelHop: A Successive Subspace Learning (SSL) Method for Object Classification
- **Arxiv ID**: http://arxiv.org/abs/1909.08190v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.08190v1)
- **Published**: 2019-09-17 15:14:19+00:00
- **Updated**: 2019-09-17 15:14:19+00:00
- **Authors**: Yueru Chen, C. -C. Jay Kuo
- **Comment**: 17 pages, 11 figures, 11 tables
- **Journal**: None
- **Summary**: A new machine learning methodology, called successive subspace learning (SSL), is introduced in this work. SSL contains four key ingredients: 1) successive near-to-far neighborhood expansion; 2) unsupervised dimension reduction via subspace approximation; 3) supervised dimension reduction via label-assisted regression (LAG); and 4) feature concatenation and decision making. An image-based object classification method, called PixelHop, is proposed to illustrate the SSL design. It is shown by experimental results that the PixelHop method outperforms the classic CNN model of similar model complexity in three benchmarking datasets (MNIST, Fashion MNIST and CIFAR-10). Although SSL and deep learning (DL) have some high-level concept in common, they are fundamentally different in model formulation, the training process and training complexity. Extensive discussion on the comparison of SSL and DL is made to provide further insights into the potential of SSL.



### Ludwig: a type-based declarative deep learning toolbox
- **Arxiv ID**: http://arxiv.org/abs/1909.07930v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, cs.SE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.07930v1)
- **Published**: 2019-09-17 16:54:29+00:00
- **Updated**: 2019-09-17 16:54:29+00:00
- **Authors**: Piero Molino, Yaroslav Dudin, Sai Sumanth Miryala
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we present Ludwig, a flexible, extensible and easy to use toolbox which allows users to train deep learning models and use them for obtaining predictions without writing code. Ludwig implements a novel approach to deep learning model building based on two main abstractions: data types and declarative configuration files. The data type abstraction allows for easier code and sub-model reuse, and the standardized interfaces imposed by this abstraction allow for encapsulation and make the code easy to extend. Declarative model definition configuration files enable inexperienced users to obtain effective models and increase the productivity of expert users. Alongside these two innovations, Ludwig introduces a general modularized deep learning architecture called Encoder-Combiner-Decoder that can be instantiated to perform a vast amount of machine learning tasks. These innovations make it possible for engineers, scientists from other fields and, in general, a much broader audience to adopt deep learning models for their tasks, concretely helping in its democratization.



### ProtoGAN: Towards Few Shot Learning for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1909.07945v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07945v1)
- **Published**: 2019-09-17 17:28:20+00:00
- **Updated**: 2019-09-17 17:28:20+00:00
- **Authors**: Sai Kumar Dwivedi, Vikram Gupta, Rahul Mitra, Shuaib Ahmed, Arjun Jain
- **Comment**: 9 pages, 5 tables, 2 figures. To appear in the proceedings of ICCV
  Workshop 2019
- **Journal**: None
- **Summary**: Few-shot learning (FSL) for action recognition is a challenging task of recognizing novel action categories which are represented by few instances in the training data. In a more generalized FSL setting (G-FSL), both seen as well as novel action categories need to be recognized. Conventional classifiers suffer due to inadequate data in FSL setting and inherent bias towards seen action categories in G-FSL setting. In this paper, we address this problem by proposing a novel ProtoGAN framework which synthesizes additional examples for novel categories by conditioning a conditional generative adversarial network with class prototype vectors. These class prototype vectors are learnt using a Class Prototype Transfer Network (CPTN) from examples of seen categories. Our synthesized examples for a novel class are semantically similar to real examples belonging to that class and is used to train a model exhibiting better generalization towards novel classes. We support our claim by performing extensive experiments on three datasets: UCF101, HMDB51 and Olympic-Sports. To the best of our knowledge, we are the first to report the results for G-FSL and provide a strong benchmark for future research. We also outperform the state-of-the-art method in FSL for all the aforementioned datasets.



### Semantic Relatedness Based Re-ranker for Text Spotting
- **Arxiv ID**: http://arxiv.org/abs/1909.07950v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.07950v2)
- **Published**: 2019-09-17 17:31:37+00:00
- **Updated**: 2019-09-19 15:29:27+00:00
- **Authors**: Ahmed Sabir, Francesc Moreno-Noguer, Lluís Padró
- **Comment**: Accepted by EMNLP 2019
- **Journal**: None
- **Summary**: Applications such as textual entailment, plagiarism detection or document clustering rely on the notion of semantic similarity, and are usually approached with dimension reduction techniques like LDA or with embedding-based neural approaches. We present a scenario where semantic similarity is not enough, and we devise a neural approach to learn semantic relatedness. The scenario is text spotting in the wild, where a text in an image (e.g. street sign, advertisement or bus destination) must be identified and recognized. Our goal is to improve the performance of vision systems by leveraging semantic information. Our rationale is that the text to be spotted is often related to the image context in which it appears (word pairs such as Delta-airplane, or quarters-parking are not similar, but are clearly related). We show how learning a word-to-word or word-to-sentence relatedness score can improve the performance of text spotting systems up to 2.9 points, outperforming other measures in a benchmark dataset.



### An Internal Learning Approach to Video Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1909.07957v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07957v1)
- **Published**: 2019-09-17 17:47:53+00:00
- **Updated**: 2019-09-17 17:47:53+00:00
- **Authors**: Haotian Zhang, Long Mai, Ning Xu, Zhaowen Wang, John Collomosse, Hailin Jin
- **Comment**: Accepted by ICCV 2019. Website:
  https://cs.stanford.edu/~haotianz/publications/video_inpainting/
- **Journal**: None
- **Summary**: We propose a novel video inpainting algorithm that simultaneously hallucinates missing appearance and motion (optical flow) information, building upon the recent 'Deep Image Prior' (DIP) that exploits convolutional network architectures to enforce plausible texture in static images. In extending DIP to video we make two important contributions. First, we show that coherent video inpainting is possible without a priori training. We take a generative approach to inpainting based on internal (within-video) learning without reliance upon an external corpus of visual data to train a one-size-fits-all model for the large space of general videos. Second, we show that such a framework can jointly generate both appearance and flow, whilst exploiting these complementary modalities to ensure mutual consistency. We show that leveraging appearance statistics specific to each video achieves visually plausible results whilst handling the challenging problem of long-term consistency.



### Towards Object Detection from Motion
- **Arxiv ID**: http://arxiv.org/abs/1909.12950v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.12950v1)
- **Published**: 2019-09-17 18:00:14+00:00
- **Updated**: 2019-09-17 18:00:14+00:00
- **Authors**: Rico Jonschkowski, Austin Stone
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel approach to weakly supervised object detection. Instead of annotated images, our method only requires two short videos to learn to detect a new object: 1) a video of a moving object and 2) one or more "negative" videos of the scene without the object. The key idea of our algorithm is to train the object detector to produce physically plausible object motion when applied to the first video and to not detect anything in the second video. With this approach, our method learns to locate objects without any object location annotations. Once the model is trained, it performs object detection on single images. We evaluate our method in three robotics settings that afford learning objects from motion: observing moving objects, watching demonstrations of object manipulation, and physically interacting with objects (see a video summary at https://youtu.be/BH0Hv3zZG_4).



### Optimizing Through Learned Errors for Accurate Sports Field Registration
- **Arxiv ID**: http://arxiv.org/abs/1909.08034v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08034v2)
- **Published**: 2019-09-17 19:07:00+00:00
- **Updated**: 2020-05-28 04:15:47+00:00
- **Authors**: Wei Jiang, Juan Camilo Gamboa Higuera, Baptiste Angles, Weiwei Sun, Mehrsan Javan, Kwang Moo Yi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an optimization-based framework to register sports field templates onto broadcast videos. For accurate registration we go beyond the prevalent feed-forward paradigm. Instead, we propose to train a deep network that regresses the registration error, and then register images by finding the registration parameters that minimize the regressed error. We demonstrate the effectiveness of our method by applying it to real-world sports broadcast videos, outperforming the state of the art. We further apply our method on a synthetic toy example and demonstrate that our method brings significant gains even when the problem is simplified and unlimited training data is available.



### Masking Salient Object Detection, a Mask Region-based Convolutional Neural Network Analysis for Segmentation of Salient Objects
- **Arxiv ID**: http://arxiv.org/abs/1909.08038v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08038v1)
- **Published**: 2019-09-17 19:14:33+00:00
- **Updated**: 2019-09-17 19:14:33+00:00
- **Authors**: Bruno A. Krinski, Daniel V. Ruiz, Guilherme Z. Machado, Eduardo Todt
- **Comment**: 6 pages, 10 figures, Accepted for presentation at the Conference on
  SBR 2019 7th Brazilian Robotics Symposium/IEEE LARS 2019 16th Latin American
  Robotics Symposium
- **Journal**: None
- **Summary**: In this paper, we propose a broad comparison between Fully Convolutional Networks (FCNs) and Mask Region-based Convolutional Neural Networks (Mask-RCNNs) applied in the Salient Object Detection (SOD) context. Studies in the SOD literature usually explore architectures based in FCNs to detect salient regions and objects in visual scenes. However, besides the promising results achieved, FCNs showed issues in some challenging scenarios. Fairly recently studies in the SOD literature proposed the use of a Mask-RCNN approach to overcome such issues. However, there is no extensive comparison between the two networks in the SOD literature endorsing the effectiveness of Mask-RCNNs over FCN when segmenting salient objects. Aiming to effectively show the superiority of Mask-RCNNs over FCNs in the SOD context, we compare two variations of Mask-RCNNs with two variations of FCNs in eight datasets widely used in the literature and in four metrics. Our findings show that in this context Mask-RCNNs achieved an improvement on the F-measure up to 47% over FCNs.



### Masked-RPCA: Sparse and Low-rank Decomposition Under Overlaying Model and Application to Moving Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1909.08049v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.08049v1)
- **Published**: 2019-09-17 19:39:15+00:00
- **Updated**: 2019-09-17 19:39:15+00:00
- **Authors**: Amirhossein Khalilian-Gourtani, Shervin Minaee, Yao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Foreground detection in a given video sequence is a pivotal step in many computer vision applications such as video surveillance system. Robust Principal Component Analysis (RPCA) performs low-rank and sparse decomposition and accomplishes such a task when the background is stationary and the foreground is dynamic and relatively small. A fundamental issue with RPCA is the assumption that the low-rank and sparse components are added at each element, whereas in reality, the moving foreground is overlaid on the background. We propose the representation via masked decomposition (i.e. an overlaying model) where each element either belongs to the low-rank or the sparse component, decided by a mask. We propose the Masked-RPCA algorithm to recover the mask and the low-rank components simultaneously, utilizing linearizing and alternating direction techniques. We further extend our formulation to be robust to dynamic changes in the background and enforce spatial connectivity in the foreground component. Our study shows significant improvement of the detected mask compared to post-processing on the sparse component obtained by other frameworks.



### Learn to Estimate Labels Uncertainty for Quality Assurance
- **Arxiv ID**: http://arxiv.org/abs/1909.08058v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.08058v1)
- **Published**: 2019-09-17 19:43:57+00:00
- **Updated**: 2019-09-17 19:43:57+00:00
- **Authors**: Agnieszka Tomczack, Nassir Navab, Shadi Albarqouni
- **Comment**: Under Review
- **Journal**: None
- **Summary**: Deep Learning sets the state-of-the-art in many challenging tasks showing outstanding performance in a broad range of applications. Despite its success, it still lacks robustness hindering its adoption in medical applications. Modeling uncertainty, through Bayesian Inference and Monte-Carlo dropout, has been successfully introduced for better understanding the underlying deep learning models. Yet, another important source of uncertainty, coming from the inter-observer variability, has not been thoroughly addressed in the literature. In this paper, we introduce labels uncertainty which better suits medical applications and show that modeling such uncertainty together with epistemic uncertainty is of high interest for quality control and referral systems.



### Ensemble Knowledge Distillation for Learning Improved and Efficient Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.08097v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.08097v3)
- **Published**: 2019-09-17 21:03:19+00:00
- **Updated**: 2020-04-02 03:41:59+00:00
- **Authors**: Umar Asif, Jianbin Tang, Stefan Harrer
- **Comment**: None
- **Journal**: None
- **Summary**: Ensemble models comprising of deep Convolutional Neural Networks (CNN) have shown significant improvements in model generalization but at the cost of large computation and memory requirements. In this paper, we present a framework for learning compact CNN models with improved classification performance and model generalization. For this, we propose a CNN architecture of a compact student model with parallel branches which are trained using ground truth labels and information from high capacity teacher networks in an ensemble learning fashion. Our framework provides two main benefits: i) Distilling knowledge from different teachers into the student network promotes heterogeneity in feature learning at different branches of the student network and enables the network to learn diverse solutions to the target problem. ii) Coupling the branches of the student network through ensembling encourages collaboration and improves the quality of the final predictions by reducing variance in the network outputs. Experiments on the well established CIFAR-10 and CIFAR-100 datasets show that our Ensemble Knowledge Distillation (EKD) improves classification accuracy and model generalization especially in situations with limited training data. Experiments also show that our EKD based compact networks outperform in terms of mean accuracy on the test datasets compared to state-of-the-art knowledge distillation based methods.



### Split Deep Q-Learning for Robust Object Singulation
- **Arxiv ID**: http://arxiv.org/abs/1909.08105v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.08105v2)
- **Published**: 2019-09-17 21:14:06+00:00
- **Updated**: 2020-02-27 12:57:31+00:00
- **Authors**: Iason Sarantopoulos, Marios Kiatos, Zoe Doulgeri, Sotiris Malassiotis
- **Comment**: Accepted for presentation in 2020 International Conference on
  Robotics and Automation (ICRA). Video attachment:
  https://youtu.be/ef1MKgVkN0E
- **Journal**: None
- **Summary**: Extracting a known target object from a pile of other objects in a cluttered environment is a challenging robotic manipulation task encountered in many robotic applications. In such conditions, the target object touches or is covered by adjacent obstacle objects, thus rendering traditional grasping techniques ineffective. In this paper, we propose a pushing policy aiming at singulating the target object from its surrounding clutter, by means of lateral pushing movements of both the neighboring objects and the target object until sufficient 'grasping room' has been achieved. To achieve the above goal we employ reinforcement learning and particularly Deep Q-learning (DQN) to learn optimal push policies by trial and error. A novel Split DQN is proposed to improve the learning rate and increase the modularity of the algorithm. Experiments show that although learning is performed in a simulated environment the transfer of learned policies to a real environment is effective thanks to robust feature selection. Finally, we demonstrate that the modularity of the algorithm allows the addition of extra primitives without retraining the model from scratch.



### Spherical View Synthesis for Self-Supervised 360 Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1909.08112v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.08112v1)
- **Published**: 2019-09-17 21:25:35+00:00
- **Updated**: 2019-09-17 21:25:35+00:00
- **Authors**: Nikolaos Zioulis, Antonis Karakottas, Dimitrios Zarpalas, Federico Alvarez, Petros Daras
- **Comment**: 3DV19, code and data at
  https://vcl3d.github.io/SphericalViewSynthesis/
- **Journal**: None
- **Summary**: Learning based approaches for depth perception are limited by the availability of clean training data. This has led to the utilization of view synthesis as an indirect objective for learning depth estimation using efficient data acquisition procedures. Nonetheless, most research focuses on pinhole based monocular vision, with scarce works presenting results for omnidirectional input. In this work, we explore spherical view synthesis for learning monocular 360 depth in a self-supervised manner and demonstrate its feasibility. Under a purely geometrically derived formulation we present results for horizontal and vertical baselines, as well as for the trinocular case. Further, we show how to better exploit the expressiveness of traditional CNNs when applied to the equirectangular domain in an efficient manner. Finally, given the availability of ground truth depth data, our work is uniquely positioned to compare view synthesis against direct supervision in a consistent and fair manner. The results indicate that alternative research directions might be better suited to enable higher quality depth perception. Our data, models and code are publicly available at https://vcl3d.github.io/SphericalViewSynthesis/.



### Identity-Aware Deep Face Hallucination via Adversarial Face Verification
- **Arxiv ID**: http://arxiv.org/abs/1909.08130v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.08130v1)
- **Published**: 2019-09-17 22:17:58+00:00
- **Updated**: 2019-09-17 22:17:58+00:00
- **Authors**: Hadi Kazemi, Fariborz Taherkhani, Nasser M. Nasrabadi
- **Comment**: BTAS 2019
- **Journal**: None
- **Summary**: In this paper, we address the problem of face hallucination by proposing a novel multi-scale generative adversarial network (GAN) architecture optimized for face verification. First, we propose a multi-scale generator architecture for face hallucination with a high up-scaling ratio factor, which has multiple intermediate outputs at different resolutions. The intermediate outputs have the growing goal of synthesizing small to large images. Second, we incorporate a face verifier with the original GAN discriminator and propose a novel discriminator which learns to discriminate different identities while distinguishing fake generated HR face images from their ground truth images. In particular, the learned generator cares for not only the visual quality of hallucinated face images but also preserving the discriminative features in the hallucination process. In addition, to capture perceptually relevant differences we employ a perceptual similarity loss, instead of similarity in pixel space. We perform a quantitative and qualitative evaluation of our framework on the LFW and CelebA datasets. The experimental results show the advantages of our proposed method against the state-of-the-art methods on the 8x downsampled testing dataset.



### NEMO: Future Object Localization Using Noisy Ego Priors
- **Arxiv ID**: http://arxiv.org/abs/1909.08150v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.08150v3)
- **Published**: 2019-09-17 23:52:17+00:00
- **Updated**: 2020-07-22 18:22:29+00:00
- **Authors**: Srikanth Malla, Isht Dwivedi, Behzad Dariush, Chiho Choi
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting the future trajectory of agents from visual observations is an important problem for realization of safe and effective navigation of autonomous systems in dynamic environments. This paper focuses on two important aspects of future trajectory forecast which are particularly relevant for mobile platforms: 1) modeling uncertainty of the predictions, particularly from egocentric views, where uncertainty in the interactive reactions and behaviors of other agents must consider the uncertainty in the ego-motion, and 2) modeling multi-modality nature of the problem, which are particularly prevalent at junctions in urban traffic scenes. To address these problems in a unified approach, we propose NEMO (Noisy Ego MOtion priors for future object localization) for future forecast of agents in the egocentric view. In the proposed approach, a predictive distribution of future forecast is jointly modeled with the uncertainty of predictions. For this, we divide the problem into two tasks: future ego-motion prediction and future object localization. We first model the multi-modal distribution of future ego-motion with uncertainty estimates. The resulting distribution of ego-behavior is used to sample multiple modes of future ego-motion. Then, each modality is used as a prior to understand the interactions between the ego-vehicle and target agent. We predict the multi-modal future locations of the target from individual modes of the ego-vehicle while modeling the uncertainty of the target's behavior. To this end, we extensively evaluate the proposed framework using the publicly available benchmark dataset (HEV-I) supplemented with odometry data from an Inertial Measurement Unit (IMU).



