# Arxiv Papers in cs.CV on 2019-09-08
### Deep Weakly-Supervised Learning Methods for Classification and Localization in Histology Images: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1909.03354v7
- **DOI**: 10.59275/j.melba.2023-5g54
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.03354v7)
- **Published**: 2019-09-08 00:01:37+00:00
- **Updated**: 2023-03-03 16:04:31+00:00
- **Authors**: Jérôme Rony, Soufiane Belharbi, Jose Dolz, Ismail Ben Ayed, Luke McCaffrey, Eric Granger
- **Comment**: Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org/2023:004
- **Journal**: Machine.Learning.for.Biomedical.Imaging. 2 (2023)
- **Summary**: Using deep learning models to diagnose cancer from histology data presents several challenges. Cancer grading and localization of regions of interest (ROIs) in these images normally relies on both image- and pixel-level labels, the latter requiring a costly annotation process. Deep weakly-supervised object localization (WSOL) methods provide different strategies for low-cost training of deep learning models. Using only image-class annotations, these methods can be trained to classify an image, and yield class activation maps (CAMs) for ROI localization. This paper provides a review of state-of-art DL methods for WSOL. We propose a taxonomy where these methods are divided into bottom-up and top-down methods according to the information flow in models. Although the latter have seen limited progress, recent bottom-up methods are currently driving much progress with deep WSOL methods. Early works focused on designing different spatial pooling functions. However, these methods reached limited localization accuracy, and unveiled a major limitation -- the under-activation of CAMs which leads to high false negative localization. Subsequent works aimed to alleviate this issue and recover complete object. Representative methods from our taxonomy are evaluated and compared in terms of classification and localization accuracy on two challenging histology datasets. Overall, the results indicate poor localization performance, particularly for generic methods that were initially designed to process natural images. Methods designed to address the challenges of histology data yielded good results. However, all methods suffer from high false positive/negative localization. Four key challenges are identified for the application of deep WSOL methods in histology -- under/over activation of CAMs, sensitivity to thresholding, and model selection.



### Episode-based Prototype Generating Network for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.03360v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.03360v2)
- **Published**: 2019-09-08 01:06:15+00:00
- **Updated**: 2020-04-02 03:21:04+00:00
- **Authors**: Yunlong Yu, Zhong Ji, Zhongfei Zhang, Jungong Han
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a simple yet effective episode-based training framework for zero-shot learning (ZSL), where the learning system requires to recognize unseen classes given only the corresponding class semantics. During training, the model is trained within a collection of episodes, each of which is designed to simulate a zero-shot classification task. Through training multiple episodes, the model progressively accumulates ensemble experiences on predicting the mimetic unseen classes, which will generalize well on the real unseen classes. Based on this training framework, we propose a novel generative model that synthesizes visual prototypes conditioned on the class semantic prototypes. The proposed model aligns the visual-semantic interactions by formulating both the visual prototype generation and the class semantic inference into an adversarial framework paired with a parameter-economic Multi-modal Cross-Entropy Loss to capture the discriminative information. Extensive experiments on four datasets under both traditional ZSL and generalized ZSL tasks show that our model outperforms the state-of-the-art approaches by large margins.



### Robust Full-FoV Depth Estimation in Tele-wide Camera System
- **Arxiv ID**: http://arxiv.org/abs/1909.03375v2
- **DOI**: 10.1109/ICASSP40776.2020.9053724
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1909.03375v2)
- **Published**: 2019-09-08 02:50:17+00:00
- **Updated**: 2019-10-18 03:32:22+00:00
- **Authors**: Kai Guo, Seongwook Song, Soonkeun Chang, Tae-ui Kim, Seungmin Han, Irina Kim
- **Comment**: 5 pages, 7 figures, 2 tables
- **Journal**: ICASSP 2020 - 2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)
- **Summary**: Tele-wide camera system with different Field of View (FoV) lenses becomes very popular in recent mobile devices. Usually it is difficult to obtain full-FoV depth based on traditional stereo-matching methods. Pure Deep Neural Network (DNN) based depth estimation methods can obtain full-FoV depth, but have low robustness for scenarios which are not covered by training dataset. In this paper, to address the above problems we propose a hierarchical hourglass network for robust full-FoV depth estimation in tele-wide camera system, which combines the robustness of traditional stereo-matching methods with the accuracy of DNN. More specifically, the proposed network comprises three major modules: single image depth prediction module infers initial depth from input color image, depth propagation module propagates traditional stereo-matching tele-FoV depth to surrounding regions, and depth combination module fuses the initial depth with the propagated depth to generate final output. Each of these modules employs an hourglass model, which is a kind of encoder-decoder structure with skip connections. Experimental results compared with state-of-the-art depth estimation methods demonstrate that our method not only produces robust and better subjective depth quality on wild test images, but also obtains better quantitative results on standard datasets.



### Automatic Image Pixel Clustering based on Mussels Wandering Optimiz
- **Arxiv ID**: http://arxiv.org/abs/1909.03380v1
- **DOI**: 10.1142/S0218001421540057
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.03380v1)
- **Published**: 2019-09-08 03:18:11+00:00
- **Updated**: 2019-09-08 03:18:11+00:00
- **Authors**: Xin Zhong, Frank Y. Shih, Xiwang Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Image segmentation as a clustering problem is to identify pixel groups on an image without any preliminary labels available. It remains a challenge in machine vision because of the variations in size and shape of image segments. Furthermore, determining the segment number in an image is NP-hard without prior knowledge of the image content. This paper presents an automatic color image pixel clustering scheme based on mussels wandering optimization. By applying an activation variable to determine the number of clusters along with the cluster centers optimization, an image is segmented with minimal prior knowledge and human intervention. By revising the within- and between-class sum of squares ratio for random natural image contents, we provide a novel fitness function for image pixel clustering tasks. Comprehensive empirical studies of the proposed scheme against other state-of-the-art competitors on synthetic data and the ASD dataset have demonstrated the promising performance of the proposed scheme.



### A Resource-Efficient Embedded Iris Recognition System Using Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.03385v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.03385v1)
- **Published**: 2019-09-08 04:21:10+00:00
- **Updated**: 2019-09-08 04:21:10+00:00
- **Authors**: Hokchhay Tann, Heng Zhao, Sherief Reda
- **Comment**: None
- **Journal**: None
- **Summary**: Applications of Fully Convolutional Networks (FCN) in iris segmentation have shown promising advances. For mobile and embedded systems, a significant challenge is that the proposed FCN architectures are extremely computationally demanding. In this article, we propose a resource-efficient, end-to-end iris recognition flow, which consists of FCN-based segmentation, contour fitting, followed by Daugman normalization and encoding. To attain accurate and efficient FCN models, we propose a three-step SW/HW co-design methodology consisting of FCN architectural exploration, precision quantization, and hardware acceleration. In our exploration, we propose multiple FCN models, and in comparison to previous works, our best-performing model requires 50X less FLOPs per inference while achieving a new state-of-the-art segmentation accuracy. Next, we select the most efficient set of models and further reduce their computational complexity through weights and activations quantization using 8-bit dynamic fixed-point (DFP) format. Each model is then incorporated into an end-to-end flow for true recognition performance evaluation. A few of our end-to-end pipelines outperform the previous state-of-the-art on two datasets evaluated. Finally, we propose a novel DFP accelerator and fully demonstrate the SW/HW co-design realization of our flow on an embedded FPGA platform. In comparison with the embedded CPU, our hardware acceleration achieves up to 8.3X speedup for the overall pipeline while using less than 15% of the available FPGA resources. We also provide comparisons between the FPGA system and an embedded GPU showing different benefits and drawbacks for the two platforms.



### L_DMI: An Information-theoretic Noise-robust Loss Function
- **Arxiv ID**: http://arxiv.org/abs/1909.03388v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.03388v2)
- **Published**: 2019-09-08 05:09:45+00:00
- **Updated**: 2019-11-04 18:16:37+00:00
- **Authors**: Yilun Xu, Peng Cao, Yuqing Kong, Yizhou Wang
- **Comment**: Accepted by NeurIPS 2019
- **Journal**: None
- **Summary**: Accurately annotating large scale dataset is notoriously expensive both in time and in money. Although acquiring low-quality-annotated dataset can be much cheaper, it often badly damages the performance of trained models when using such dataset without particular treatment. Various methods have been proposed for learning with noisy labels. However, most methods only handle limited kinds of noise patterns, require auxiliary information or steps (e.g. , knowing or estimating the noise transition matrix), or lack theoretical justification. In this paper, we propose a novel information-theoretic loss function, $\mathcal{L}_{DMI}$, for training deep neural networks robust to label noise. The core of $\mathcal{L}_{DMI}$ is a generalized version of mutual information, termed Determinant based Mutual Information (DMI), which is not only information-monotone but also relatively invariant. \emph{To the best of our knowledge, $\mathcal{L}_{DMI}$ is the first loss function that is provably robust to instance-independent label noise, regardless of noise pattern, and it can be applied to any existing classification neural networks straightforwardly without any auxiliary information}. In addition to theoretical justification, we also empirically show that using $\mathcal{L}_{DMI}$ outperforms all other counterparts in the classification task on both image dataset and natural language dataset include Fashion-MNIST, CIFAR-10, Dogs vs. Cats, MR with a variety of synthesized noise patterns and noise amounts, as well as a real-world dataset Clothing1M. Codes are available at https://github.com/Newbeeer/L_DMI .



### Quality Estimation for Image Captions Based on Large-scale Human Evaluations
- **Arxiv ID**: http://arxiv.org/abs/1909.03396v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.03396v2)
- **Published**: 2019-09-08 06:55:53+00:00
- **Updated**: 2021-06-01 19:03:27+00:00
- **Authors**: Tomer Levinboim, Ashish V. Thapliyal, Piyush Sharma, Radu Soricut
- **Comment**: 10 pages, 6 figures, 3 tables. Accepted to NAACL2021.
  https://www.aclweb.org/anthology/2021.naacl-main.253/
- **Journal**: None
- **Summary**: Automatic image captioning has improved significantly over the last few years, but the problem is far from being solved, with state of the art models still often producing low quality captions when used in the wild. In this paper, we focus on the task of Quality Estimation (QE) for image captions, which attempts to model the caption quality from a human perspective and without access to ground-truth references, so that it can be applied at prediction time to detect low-quality captions produced on previously unseen images. For this task, we develop a human evaluation process that collects coarse-grained caption annotations from crowdsourced users, which is then used to collect a large scale dataset spanning more than 600k caption quality ratings. We then carefully validate the quality of the collected ratings and establish baseline models for this new QE task. Finally, we further collect fine-grained caption quality annotations from trained raters, and use them to demonstrate that QE models trained over the coarse ratings can effectively detect and filter out low-quality image captions, thereby improving the user experience from captioning systems.



### Squeeze-and-Attention Networks for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.03402v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.03402v4)
- **Published**: 2019-09-08 08:21:57+00:00
- **Updated**: 2020-04-01 05:50:33+00:00
- **Authors**: Zilong Zhong, Zhong Qiu Lin, Rene Bidart, Xiaodan Hu, Ibrahim Ben Daya, Zhifeng Li, Wei-Shi Zheng, Jonathan Li, Alexander Wong
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: The recent integration of attention mechanisms into segmentation networks improves their representational capabilities through a great emphasis on more informative features. However, these attention mechanisms ignore an implicit sub-task of semantic segmentation and are constrained by the grid structure of convolution kernels. In this paper, we propose a novel squeeze-and-attention network (SANet) architecture that leverages an effective squeeze-and-attention (SA) module to account for two distinctive characteristics of segmentation: i) pixel-group attention, and ii) pixel-wise prediction. Specifically, the proposed SA modules impose pixel-group attention on conventional convolution by introducing an 'attention' convolutional channel, thus taking into account spatial-channel inter-dependencies in an efficient manner. The final segmentation results are produced by merging outputs from four hierarchical stages of a SANet to integrate multi-scale contexts for obtaining an enhanced pixel-wise prediction. Empirical experiments on two challenging public datasets validate the effectiveness of the proposed SANets, which achieves 83.2% mIoU (without COCO pre-training) on PASCAL VOC and a state-of-the-art mIoU of 54.4% on PASCAL Context.



### Open Compound Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1909.03403v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.03403v2)
- **Published**: 2019-09-08 08:41:05+00:00
- **Updated**: 2020-03-29 06:00:41+00:00
- **Authors**: Ziwei Liu, Zhongqi Miao, Xingang Pan, Xiaohang Zhan, Dahua Lin, Stella X. Yu, Boqing Gong
- **Comment**: To appear in CVPR 2020 as an oral presentation. Code, datasets and
  models are available at:
  https://liuziwei7.github.io/projects/CompoundDomain.html
- **Journal**: None
- **Summary**: A typical domain adaptation approach is to adapt models trained on the annotated data in a source domain (e.g., sunny weather) for achieving high performance on the test data in a target domain (e.g., rainy weather). Whether the target contains a single homogeneous domain or multiple heterogeneous domains, existing works always assume that there exist clear distinctions between the domains, which is often not true in practice (e.g., changes in weather). We study an open compound domain adaptation (OCDA) problem, in which the target is a compound of multiple homogeneous domains without domain labels, reflecting realistic data collection from mixed and novel situations. We propose a new approach based on two technical insights into OCDA: 1) a curriculum domain adaptation strategy to bootstrap generalization across domains in a data-driven self-organizing fashion and 2) a memory module to increase the model's agility towards novel domains. Our experiments on digit classification, facial expression recognition, semantic segmentation, and reinforcement learning demonstrate the effectiveness of our approach.



### TorchGAN: A Flexible Framework for GAN Training and Evaluation
- **Arxiv ID**: http://arxiv.org/abs/1909.03410v1
- **DOI**: 10.21105/joss.02606
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.03410v1)
- **Published**: 2019-09-08 09:32:01+00:00
- **Updated**: 2019-09-08 09:32:01+00:00
- **Authors**: Avik Pal, Aniket Das
- **Comment**: None
- **Journal**: Journal of Open Source Software, 6 (2021), 2606
- **Summary**: TorchGAN is a PyTorch based framework for writing succinct and comprehensible code for training and evaluation of Generative Adversarial Networks. The framework's modular design allows effortless customization of the model architecture, loss functions, training paradigms, and evaluation metrics. The key features of TorchGAN are its extensibility, built-in support for a large number of popular models, losses and evaluation metrics, and zero overhead compared to vanilla PyTorch. By using the framework to implement several popular GAN models, we demonstrate its extensibility and ease of use. We also benchmark the training time of our framework for said models against the corresponding baseline PyTorch implementations and observe that TorchGAN's features bear almost zero overhead.



### STA: Adversarial Attacks on Siamese Trackers
- **Arxiv ID**: http://arxiv.org/abs/1909.03413v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.03413v1)
- **Published**: 2019-09-08 09:43:49+00:00
- **Updated**: 2019-09-08 09:43:49+00:00
- **Authors**: Xugang Wu, Xiaoping Wang, Xu Zhou, Songlei Jian
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, the majority of visual trackers adopt Convolutional Neural Network (CNN) as their backbone to achieve high tracking accuracy. However, less attention has been paid to the potential adversarial threats brought by CNN, including Siamese network.   In this paper, we first analyze the existing vulnerabilities in Siamese trackers and propose the requirements for a successful adversarial attack. On this basis, we formulate the adversarial generation problem and propose an end-to-end pipeline to generate a perturbed texture map for the 3D object that causes the trackers to fail. Finally, we conduct thorough experiments to verify the effectiveness of our algorithm. Experiment results show that adversarial examples generated by our algorithm can successfully lower the tracking accuracy of victim trackers and even make them drift off. To the best of our knowledge, this is the first work to generate 3D adversarial examples on visual trackers.



### Pose Estimation for Ground Robots: On Manifold Representation, Integration, Re-Parameterization, and Optimization
- **Arxiv ID**: http://arxiv.org/abs/1909.03423v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.03423v3)
- **Published**: 2019-09-08 10:28:41+00:00
- **Updated**: 2020-10-12 04:53:34+00:00
- **Authors**: Mingming Zhang, Xingxing Zuo, Yiming Chen, Yong Liu, Mingyang Li
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we focus on motion estimation dedicated for non-holonomic ground robots, by probabilistically fusing measurements from the wheel odometer and exteroceptive sensors. For ground robots, the wheel odometer is widely used in pose estimation tasks, especially in applications under planar-scene based environments. However, since the wheel odometer only provides 2D motion estimates, it is extremely challenging to use that for performing accurate full 6D pose (3D position and 3D orientation) estimation. Traditional methods on 6D pose estimation either approximate sensor or motion models, at the cost of accuracy reduction, or rely on other sensors, e.g., inertial measurement unit (IMU), to provide complementary measurements. By contrast, in this paper, we propose a novel method to utilize the wheel odometer for 6D pose estimation, by modeling and utilizing motion manifold for ground robots. Our approach is probabilistically formulated and only requires the wheel odometer and an exteroceptive sensor (e.g., a camera). Specifically, our method i) formulates the motion manifold of ground robots by parametric representation, ii) performs manifold based 6D integration with the wheel odometer measurements only, and iii) re-parameterizes manifold equations periodically for error reduction. To demonstrate the effectiveness and applicability of the proposed algorithmic modules, we integrate that into a sliding-window pose estimator by using measurements from the wheel odometer and a monocular camera. By conducting extensive simulated and real-world experiments, we show that the proposed algorithm outperforms competing state-of-the-art algorithms by a significant margin in pose estimation accuracy, especially when deployed in complex large-scale real-world environments.



### CUDA: Contradistinguisher for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1909.03442v1
- **DOI**: 10.1109/ICDM.2019.00012
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.03442v1)
- **Published**: 2019-09-08 12:16:33+00:00
- **Updated**: 2019-09-08 12:16:33+00:00
- **Authors**: Sourabh Balgi, Ambedkar Dukkipati
- **Comment**: International Conference on Data Mining, ICDM 2019
- **Journal**: None
- **Summary**: In this paper, we propose a simple model referred as Contradistinguisher (CTDR) for unsupervised domain adaptation whose objective is to jointly learn to contradistinguish on unlabeled target domain in a fully unsupervised manner along with prior knowledge acquired by supervised learning on an entirely different domain. Most recent works in domain adaptation rely on an indirect way of first aligning the source and target domain distributions and then learn a classifier on a labeled source domain to classify target domain. This approach of an indirect way of addressing the real task of unlabeled target domain classification has three main drawbacks. (i) The sub-task of obtaining a perfect alignment of the domain in itself might be impossible due to large domain shift (e.g., language domains). (ii) The use of multiple classifiers to align the distributions unnecessarily increases the complexity of the neural networks leading to over-fitting in many cases. (iii) Due to distribution alignment, the domain-specific information is lost as the domains get morphed. In this work, we propose a simple and direct approach that does not require domain alignment. We jointly learn CTDR on both source and target distribution for unsupervised domain adaptation task using contradistinguish loss for the unlabeled target domain in conjunction with a supervised loss for labeled source domain. Our experiments show that avoiding domain alignment by directly addressing the task of unlabeled target domain classification using CTDR achieves state-of-the-art results on eight visual and four language benchmark domain adaptation datasets.



### Dynamic Context Correspondence Network for Semantic Alignment
- **Arxiv ID**: http://arxiv.org/abs/1909.03444v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.03444v1)
- **Published**: 2019-09-08 12:21:08+00:00
- **Updated**: 2019-09-08 12:21:08+00:00
- **Authors**: Shuaiyi Huang, Qiuyue Wang, Songyang Zhang, Shipeng Yan, Xuming He
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Establishing semantic correspondence is a core problem in computer vision and remains challenging due to large intra-class variations and lack of annotated data. In this paper, we aim to incorporate global semantic context in a flexible manner to overcome the limitations of prior work that relies on local semantic representations. To this end, we first propose a context-aware semantic representation that incorporates spatial layout for robust matching against local ambiguities. We then develop a novel dynamic fusion strategy based on attention mechanism to weave the advantages of both local and context features by integrating semantic cues from multiple scales. We instantiate our strategy by designing an end-to-end learnable deep network, named as Dynamic Context Correspondence Network (DCCNet). To train the network, we adopt a multi-auxiliary task loss to improve the efficiency of our weakly-supervised learning procedure. Our approach achieves superior or competitive performance over previous methods on several challenging datasets, including PF-Pascal, PF-Willow, and TSS, demonstrating its effectiveness and generality.



### Imitation Learning for Human Pose Prediction
- **Arxiv ID**: http://arxiv.org/abs/1909.03449v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.03449v1)
- **Published**: 2019-09-08 12:39:31+00:00
- **Updated**: 2019-09-08 12:39:31+00:00
- **Authors**: Borui Wang, Ehsan Adeli, Hsu-kuang Chiu, De-An Huang, Juan Carlos Niebles
- **Comment**: 10 pages, 7 figures, accepted to ICCV 2019
- **Journal**: None
- **Summary**: Modeling and prediction of human motion dynamics has long been a challenging problem in computer vision, and most existing methods rely on the end-to-end supervised training of various architectures of recurrent neural networks. Inspired by the recent success of deep reinforcement learning methods, in this paper we propose a new reinforcement learning formulation for the problem of human pose prediction, and develop an imitation learning algorithm for predicting future poses under this formulation through a combination of behavioral cloning and generative adversarial imitation learning. Our experiments show that our proposed method outperforms all existing state-of-the-art baseline models by large margins on the task of human pose prediction in both short-term predictions and long-term predictions, while also enjoying huge advantage in training speed.



### Blind Geometric Distortion Correction on Images Through Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.03459v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.3; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1909.03459v1)
- **Published**: 2019-09-08 13:13:12+00:00
- **Updated**: 2019-09-08 13:13:12+00:00
- **Authors**: Xiaoyu Li, Bo Zhang, Pedro V. Sander, Jing Liao
- **Comment**: 10 pages, 11 figures, published in CVPR 2019
- **Journal**: None
- **Summary**: We propose the first general framework to automatically correct different types of geometric distortion in a single input image. Our proposed method employs convolutional neural networks (CNNs) trained by using a large synthetic distortion dataset to predict the displacement field between distorted images and corrected images. A model fitting method uses the CNN output to estimate the distortion parameters, achieving a more accurate prediction. The final corrected image is generated based on the predicted flow using an efficient, high-quality resampling method. Experimental results demonstrate that our algorithm outperforms traditional correction methods, and allows for interesting applications such as distortion transfer, distortion exaggeration, and co-occurring distortion correction.



### Deep Workpiece Region Segmentation for Bin Picking
- **Arxiv ID**: http://arxiv.org/abs/1909.03462v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.03462v1)
- **Published**: 2019-09-08 13:29:09+00:00
- **Updated**: 2019-09-08 13:29:09+00:00
- **Authors**: Muhammad Usman Khalid, Janik M. Hager, Werner Kraus, Marco F. Huber, Marc Toussaint
- **Comment**: IEEE CASE 2019
- **Journal**: None
- **Summary**: For most industrial bin picking solutions, the pose of a workpiece is localized by matching a CAD model to point cloud obtained from 3D sensor. Distinguishing flat workpieces from bottom of the bin in point cloud imposes challenges in the localization of workpieces that lead to wrong or phantom detections. In this paper, we propose a framework that solves this problem by automatically segmenting workpiece regions from non-workpiece regions in a point cloud data. It is done in real time by applying a fully convolutional neural network trained on both simulated and real data. The real data has been labelled by our novel technique which automatically generates ground truth labels for real point clouds. Along with real time workpiece segmentation, our framework also helps in improving the number of detected workpieces and estimating the correct object poses. Moreover, it decreases the computation time by approximately 1s due to a reduction of the search space for the object pose estimation.



### Multi-Modal Three-Stream Network for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1909.03466v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.03466v1)
- **Published**: 2019-09-08 13:40:16+00:00
- **Updated**: 2019-09-08 13:40:16+00:00
- **Authors**: Muhammad Usman Khalid, Jie Yu
- **Comment**: Presented in IEEE ICPR 2018
- **Journal**: None
- **Summary**: Human action recognition in video is an active yet challenging research topic due to high variation and complexity of data. In this paper, a novel video based action recognition framework utilizing complementary cues is proposed to handle this complex problem. Inspired by the successful two stream networks for action classification, additional pose features are studied and fused to enhance understanding of human action in a more abstract and semantic way. Towards practices, not only ground truth poses but also noisy estimated poses are incorporated in the framework with our proposed pre-processing module. The whole framework and each cue are evaluated on varied benchmarking datasets as JHMDB, sub-JHMDB and Penn Action. Our results outperform state-of-the-art performance on these datasets and show the strength of complementary cues.



### Learning Geometrically Consistent Mesh Corrections
- **Arxiv ID**: http://arxiv.org/abs/1909.03471v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.03471v1)
- **Published**: 2019-09-08 13:58:05+00:00
- **Updated**: 2019-09-08 13:58:05+00:00
- **Authors**: Ştefan Săftescu, Paul Newman
- **Comment**: None
- **Journal**: None
- **Summary**: Building good 3D maps is a challenging and expensive task, which requires high-quality sensors and careful, time-consuming scanning. We seek to reduce the cost of building good reconstructions by correcting views of existing low-quality ones in a post-hoc fashion using learnt priors over surfaces and appearance. We train a CNN model to predict the difference in inverse-depth from varying viewpoints of two meshes -- one of low quality that we wish to correct, and one of high-quality that we use as a reference.   In contrast to previous work, we pay attention to the problem of excessive smoothing in corrected meshes. We address this with a suitable network architecture, and introduce a loss-weighting mechanism that emphasises edges in the prediction. Furthermore, smooth predictions result in geometrical inconsistencies. To deal with this issue, we present a loss function which penalises re-projection differences that are not due to occlusions. Our model reduces gross errors by 45.3%--77.5%, up to five times more than previous work.



### Autonomous Underwater Vehicle: Electronics and Software Implementation of the Proton AUV
- **Arxiv ID**: http://arxiv.org/abs/1909.03472v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.03472v1)
- **Published**: 2019-09-08 13:58:06+00:00
- **Updated**: 2019-09-08 13:58:06+00:00
- **Authors**: Vivek Mange, Priyam Shah, Vishal Kothari
- **Comment**: 6 pages, 8 figures
- **Journal**: None
- **Summary**: The paper deals with the software and the electronics unit for an autonomous underwater vehicle. The implementation in the electronics unit is the connection and communication between SBC, pixhawk controller and other sensory hardware and actuators. The major implementation of the software unit is the algorithm for object detection based on Convolutional Neural Network (CNN) and its models. The Hyperparameters were tuned according to Odroid Xu4 for various models. The maneuvering algorithm uses the MAVLink protocol of the ArduSub project for movement and its simulation.



### New Graph-based Features For Shape Recognition
- **Arxiv ID**: http://arxiv.org/abs/1909.03482v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.03482v1)
- **Published**: 2019-09-08 15:16:26+00:00
- **Updated**: 2019-09-08 15:16:26+00:00
- **Authors**: Narges Mirehi, Maryam Tahmasbi, Alireza Tavakoli Targhi
- **Comment**: None
- **Journal**: None
- **Summary**: Shape recognition is the main challenging problem in computer vision. Different approaches and tools are used to solve this problem. Most existing approaches to object recognition are based on pixels. Pixel-based methods are dependent on the geometry and nature of the pixels, so the destruction of pixels reduces their performance. In this paper, we study the ability of graphs as shape recognition. We construct a graph that captures the topological and geometrical properties of the object. Then, using the coordinate and relation of its vertices, we extract features that are robust to noise, rotation, scale variation, and articulation. To evaluate our method, we provide different comparisons with state-of-the-art results on various known benchmarks, including Kimia's, Tari56, Tetrapod, and Articulated dataset. We provide an analysis of our method against different variations. The results confirm our performance, especially against noise.



### Anatomy-Aware Self-supervised Fetal MRI Synthesis from Unpaired Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/1909.03483v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.03483v1)
- **Published**: 2019-09-08 15:28:35+00:00
- **Updated**: 2019-09-08 15:28:35+00:00
- **Authors**: Jianbo Jiao, Ana I. L. Namburete, Aris T. Papageorghiou, J. Alison Noble
- **Comment**: MICCAI-MLMI 2019
- **Journal**: None
- **Summary**: Fetal brain magnetic resonance imaging (MRI) offers exquisite images of the developing brain but is not suitable for anomaly screening. For this ultrasound (US) is employed. While expert sonographers are adept at reading US images, MR images are much easier for non-experts to interpret. Hence in this paper we seek to produce images with MRI-like appearance directly from clinical US images. Our own clinical motivation is to seek a way to communicate US findings to patients or clinical professionals unfamiliar with US, but in medical image analysis such a capability is potentially useful, for instance, for US-MRI registration or fusion. Our model is self-supervised and end-to-end trainable. Specifically, based on an assumption that the US and MRI data share a similar anatomical latent space, we first utilise an extractor to determine shared latent features, which are then used for data synthesis. Since paired data was unavailable for our study (and rare in practice), we propose to enforce the distributions to be similar instead of employing pixel-wise constraints, by adversarial learning in both the image domain and latent space. Furthermore, we propose an adversarial structural constraint to regularise the anatomical structures between the two modalities during the synthesis. A cross-modal attention scheme is proposed to leverage non-local spatial correlations. The feasibility of the approach to produce realistic looking MR images is demonstrated quantitatively and with a qualitative evaluation compared to real fetal MR images.



### MULE: Multimodal Universal Language Embedding
- **Arxiv ID**: http://arxiv.org/abs/1909.03493v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1909.03493v2)
- **Published**: 2019-09-08 16:08:04+00:00
- **Updated**: 2019-12-28 21:57:10+00:00
- **Authors**: Donghyun Kim, Kuniaki Saito, Kate Saenko, Stan Sclaroff, Bryan A. Plummer
- **Comment**: Accepted as an oral at AAAI 2020
- **Journal**: None
- **Summary**: Existing vision-language methods typically support two languages at a time at most. In this paper, we present a modular approach which can easily be incorporated into existing vision-language methods in order to support many languages. We accomplish this by learning a single shared Multimodal Universal Language Embedding (MULE) which has been visually-semantically aligned across all languages. Then we learn to relate MULE to visual data as if it were a single language. Our method is not architecture specific, unlike prior work which typically learned separate branches for each language, enabling our approach to easily be adapted to many vision-language methods and tasks. Since MULE learns a single language branch in the multimodal model, we can also scale to support many languages, and languages with fewer annotations can take advantage of the good representation learned from other (more abundant) language data. We demonstrate the effectiveness of MULE on the bidirectional image-sentence retrieval task, supporting up to four languages in a single model. In addition, we show that Machine Translation can be used for data augmentation in multilingual learning, which, combined with MULE, improves mean recall by up to 21.9% on a single-language compared to prior work, with the most significant gains seen on languages with relatively few annotations. Our code is publicly available.



### A New GNG Graph-Based Hand Gesture Recognition Approach
- **Arxiv ID**: http://arxiv.org/abs/1909.03534v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.03534v1)
- **Published**: 2019-09-08 19:27:30+00:00
- **Updated**: 2019-09-08 19:27:30+00:00
- **Authors**: Narges Mirehi, Maryam Tahmasbi
- **Comment**: None
- **Journal**: None
- **Summary**: Hand Gesture Recognition (HGR) is of major importance for Human-Computer Interaction (HCI) applications. In this paper, we present a new hand gesture recognition approach called GNG-IEMD. In this approach, first, we use a Growing Neural Gas (GNG) graph to model the image. Then we extract features from this graph. These features are not geometric or pixel-based, so do not depend on scale, rotation, and articulation. The dissimilarity between hand gestures is measured with a novel Improved Earth Mover\textquotesingle s Distance (IEMD) metric. We evaluate the performance of the proposed approach on challenging public datasets including NTU Hand Digits, HKU, HKU multi-angle, and UESTC-ASL and compare the results with state-of-the-art approaches. The experimental results demonstrate the performance of the proposed approach.



### Cross Domain Image Matching in Presence of Outliers
- **Arxiv ID**: http://arxiv.org/abs/1909.03552v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.03552v1)
- **Published**: 2019-09-08 22:00:30+00:00
- **Updated**: 2019-09-08 22:00:30+00:00
- **Authors**: Xin Liu, Seyran Khademi, Jan C. van Gemert
- **Comment**: ICCV Workshop on Transferring and Adaptive Source Knowledge in
  Computer Vision (TASK-CV) 2019
- **Journal**: None
- **Summary**: Cross domain image matching between image collections from different source and target domains is challenging in times of deep learning due to i) limited variation of image conditions in a training set, ii) lack of paired-image labels during training, iii) the existing of outliers that makes image matching domains not fully overlap. To this end, we propose an end-to-end architecture that can match cross domain images without labels in the target domain and handle non-overlapping domains by outlier detection. We leverage domain adaptation and triplet constraints for training a network capable of learning domain invariant and identity distinguishable representations, and iteratively detecting the outliers with an entropy loss and our proposed weighted MK-MMD. Extensive experimental evidence on Office [17] dataset and our proposed datasets Shape, Pitts-CycleGAN shows that the proposed approach yields state-of-the-art cross domain image matching and outlier detection performance on different benchmarks. The code will be made publicly available.



### AtLoc: Attention Guided Camera Localization
- **Arxiv ID**: http://arxiv.org/abs/1909.03557v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.03557v2)
- **Published**: 2019-09-08 22:33:02+00:00
- **Updated**: 2019-10-28 16:56:07+00:00
- **Authors**: Bing Wang, Changhao Chen, Chris Xiaoxuan Lu, Peijun Zhao, Niki Trigoni, Andrew Markham
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has achieved impressive results in camera localization, but current single-image techniques typically suffer from a lack of robustness, leading to large outliers. To some extent, this has been tackled by sequential (multi-images) or geometry constraint approaches, which can learn to reject dynamic objects and illumination conditions to achieve better performance. In this work, we show that attention can be used to force the network to focus on more geometrically robust objects and features, achieving state-of-the-art performance in common benchmark, even if using only a single image as input. Extensive experimental evidence is provided through public indoor and outdoor datasets. Through visualization of the saliency maps, we demonstrate how the network learns to reject dynamic objects, yielding superior global camera pose regression performance. The source code is avaliable at https://github.com/BingCS/AtLoc.



