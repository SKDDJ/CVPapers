# Arxiv Papers in cs.CV on 2019-07-23
### Light Field Super-resolution via Attention-Guided Fusion of Hybrid Lenses
- **Arxiv ID**: http://arxiv.org/abs/1907.09640v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.09640v2)
- **Published**: 2019-07-23 00:40:39+00:00
- **Updated**: 2020-08-01 03:02:51+00:00
- **Authors**: Jing Jin, Junhui Hou, Jie Chen, Sam Kwong, Jingyi Yu
- **Comment**: This paper was accepted by ACM MM 2020
- **Journal**: None
- **Summary**: This paper explores the problem of reconstructing high-resolution light field (LF) images from hybrid lenses, including a high-resolution camera surrounded by multiple low-resolution cameras. To tackle this challenge, we propose a novel end-to-end learning-based approach, which can comprehensively utilize the specific characteristics of the input from two complementary and parallel perspectives. Specifically, one module regresses a spatially consistent intermediate estimation by learning a deep multidimensional and cross-domain feature representation; the other one constructs another intermediate estimation, which maintains the high-frequency textures, by propagating the information of the high-resolution view. We finally leverage the advantages of the two intermediate estimations via the learned attention maps, leading to the final high-resolution LF image. Extensive experiments demonstrate the significant superiority of our approach over state-of-the-art ones. That is, our method not only improves the PSNR by more than 2 dB, but also preserves the LF structure much better. To the best of our knowledge, this is the first end-to-end deep learning method for reconstructing a high-resolution LF image with a hybrid input. We believe our framework could potentially decrease the cost of high-resolution LF data acquisition and also be beneficial to LF data storage and transmission. The code is available at https://github.com/jingjin25/LFhybridSR-Fusion.



### A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing
- **Arxiv ID**: http://arxiv.org/abs/1907.09642v4
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.09642v4)
- **Published**: 2019-07-23 00:51:21+00:00
- **Updated**: 2019-11-27 04:58:19+00:00
- **Authors**: Wei Liu, Pingping Zhang, Yinjie Lei, Xiaolin Huang, Jie Yang, Ian Reid
- **Comment**: Accepted by AAAI 2020
- **Journal**: None
- **Summary**: Image smoothing is a fundamental procedure in applications of both computer vision and graphics. The required smoothing properties can be different or even contradictive among different tasks. Nevertheless, the inherent smoothing nature of one smoothing operator is usually fixed and thus cannot meet the various requirements of different applications. In this paper, a non-convex non-smooth optimization framework is proposed to achieve diverse smoothing natures where even contradictive smoothing behaviors can be achieved. To this end, we first introduce the truncated Huber penalty function which has seldom been used in image smoothing. A robust framework is then proposed. When combined with the strong flexibility of the truncated Huber penalty function, our framework is capable of a range of applications and can outperform the state-of-the-art approaches in several tasks. In addition, an efficient numerical solution is provided and its convergence is theoretically guaranteed even the optimization framework is non-convex and non-smooth. The effectiveness and superior performance of our approach are validated through comprehensive experimental results in a range of applications.



### Highlight Every Step: Knowledge Distillation via Collaborative Teaching
- **Arxiv ID**: http://arxiv.org/abs/1907.09643v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.09643v1)
- **Published**: 2019-07-23 01:02:39+00:00
- **Updated**: 2019-07-23 01:02:39+00:00
- **Authors**: Haoran Zhao, Xin Sun, Junyu Dong, Changrui Chen, Zihe Dong
- **Comment**: None
- **Journal**: None
- **Summary**: High storage and computational costs obstruct deep neural networks to be deployed on resource-constrained devices. Knowledge distillation aims to train a compact student network by transferring knowledge from a larger pre-trained teacher model. However, most existing methods on knowledge distillation ignore the valuable information among training process associated with training results. In this paper, we provide a new Collaborative Teaching Knowledge Distillation (CTKD) strategy which employs two special teachers. Specifically, one teacher trained from scratch (i.e., scratch teacher) assists the student step by step using its temporary outputs. It forces the student to approach the optimal path towards the final logits with high accuracy. The other pre-trained teacher (i.e., expert teacher) guides the student to focus on a critical region which is more useful for the task. The combination of the knowledge from two special teachers can significantly improve the performance of the student network in knowledge distillation. The results of experiments on CIFAR-10, CIFAR-100, SVHN and Tiny ImageNet datasets verify that the proposed knowledge distillation method is efficient and achieves state-of-the-art performance.



### Few-shot Learning for Domain-specific Fine-grained Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1907.09647v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09647v3)
- **Published**: 2019-07-23 01:21:30+00:00
- **Updated**: 2020-06-01 05:18:20+00:00
- **Authors**: Xin Sun, Hongwei Xv, Junyu Dong, Qiong Li, Changrui Chen
- **Comment**: 11 pages, 11 figures
- **Journal**: None
- **Summary**: Learning to recognize novel visual categories from a few examples is a challenging task for machines in real-world industrial applications. In contrast, humans have the ability to discriminate even similar objects with little supervision. This paper attempts to address the few shot fine-grained image classification problem. We propose a feature fusion model to explore discriminative features by focusing on key regions. The model utilizes the focus area location mechanism to discover the perceptually similar regions among objects. High-order integration is employed to capture the interaction information among intra-parts. We also design a Center Neighbor Loss to form robust embedding space distributions. Furthermore, we build a typical fine-grained and few-shot learning dataset miniPPlankton from the real-world application in the area of marine ecological environments. Extensive experiments are carried out to validate the performance of our method. The results demonstrate that our model achieves competitive performance compared with state-of-the-art models. Our work is a valuable complement to the model domain-specific industrial applications.



### GA-DAN: Geometry-Aware Domain Adaptation Network for Scene Text Detection and Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.09653v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09653v1)
- **Published**: 2019-07-23 01:56:06+00:00
- **Updated**: 2019-07-23 01:56:06+00:00
- **Authors**: Fangneng Zhan, Chuhui Xue, Shijian Lu
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: Recent adversarial learning research has achieved very impressive progress for modelling cross-domain data shifts in appearance space but its counterpart in modelling cross-domain shifts in geometry space lags far behind. This paper presents an innovative Geometry-Aware Domain Adaptation Network (GA-DAN) that is capable of modelling cross-domain shifts concurrently in both geometry space and appearance space and realistically converting images across domains with very different characteristics. In the proposed GA-DAN, a novel multi-modal spatial learning technique is designed which converts a source-domain image into multiple images of different spatial views as in the target domain. A new disentangled cycle-consistency loss is introduced which balances the cycle consistency in appearance and geometry spaces and improves the learning of the whole network greatly. The proposed GA-DAN has been evaluated for the classic scene text detection and recognition tasks, and experiments show that the domain-adapted images achieve superior scene text detection and recognition performance while applied to network training.



### Grasping Using Tactile Sensing and Deep Calibration
- **Arxiv ID**: http://arxiv.org/abs/1907.09656v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.09656v1)
- **Published**: 2019-07-23 02:06:43+00:00
- **Updated**: 2019-07-23 02:06:43+00:00
- **Authors**: Masoud Baghbahari, Aman Behal
- **Comment**: None
- **Journal**: None
- **Summary**: Tactile perception is an essential ability of intelligent robots in interaction with their surrounding environments. This perception as an intermediate level acts between sensation and action and has to be defined properly to generate suitable action in response to sensed data. In this paper, we propose a feedback approach to address robot grasping task using force-torque tactile sensing. While visual perception is an essential part for gross reaching, constant utilization of this sensing modality can negatively affect the grasping process with overwhelming computation. In such case, human being utilizes tactile sensing to interact with objects. Inspired by, the proposed approach is presented and evaluated on a real robot to demonstrate the effectiveness of the suggested framework. Moreover, we utilize a deep learning framework called Deep Calibration in order to eliminate the effect of bias in the collected data from the robot sensors.



### Make Skeleton-based Action Recognition Model Smaller, Faster and Better
- **Arxiv ID**: http://arxiv.org/abs/1907.09658v8
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09658v8)
- **Published**: 2019-07-23 02:06:51+00:00
- **Updated**: 2020-03-18 08:58:07+00:00
- **Authors**: Fan Yang, Sakriani Sakti, Yang Wu, Satoshi Nakamura
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: Although skeleton-based action recognition has achieved great success in recent years, most of the existing methods may suffer from a large model size and slow execution speed. To alleviate this issue, we analyze skeleton sequence properties to propose a Double-feature Double-motion Network (DD-Net) for skeleton-based action recognition. By using a lightweight network structure (i.e., 0.15 million parameters), DD-Net can reach a super fast speed, as 3,500 FPS on one GPU, or, 2,000 FPS on one CPU. By employing robust features, DD-Net achieves the state-of-the-art performance on our experimental datasets: SHREC (i.e., hand actions) and JHMDB (i.e., body actions). Our code will be released with this paper later.



### Enhancing the Discriminative Feature Learning for Visible-Thermal Cross-Modality Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1907.09659v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09659v1)
- **Published**: 2019-07-23 02:09:25+00:00
- **Updated**: 2019-07-23 02:09:25+00:00
- **Authors**: Haijun Liu, Jian Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Existing person re-identification has achieved great progress in the visible domain, capturing all the person images with visible cameras. However, in a 24-hour intelligent surveillance system, the visible cameras may be noneffective at night. In this situation, thermal cameras are the best supplemental components, which capture images without depending on visible light. Therefore, in this paper, we investigate the visible-thermal cross-modality person re-identification (VT Re-ID) problem. In VT Re-ID, there are two knotty problems should be well handled, cross-modality discrepancy and intra-modality variations. To address these two issues, we propose focusing on enhancing the discriminative feature learning (EDFL) with two extreme simple means from two core aspects, (1) skip-connection for mid-level features incorporation to improve the person features with more discriminability and robustness, and (2) dual-modality triplet loss to guide the training procedures by simultaneously considering the cross-modality discrepancy and intra-modality variations. Additionally, the two-stream CNN structure is adopted to learn the multi-modality sharable person features. The experimental results on two datasets show that our proposed EDFL approach distinctly outperforms state-of-the-art methods by large margins, demonstrating the effectiveness of our EDFL to enhance the discriminative feature learning for VT Re-ID.



### Deep Differentiable Random Forests for Age Estimation
- **Arxiv ID**: http://arxiv.org/abs/1907.10665v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10665v2)
- **Published**: 2019-07-23 02:44:38+00:00
- **Updated**: 2019-08-22 15:18:01+00:00
- **Authors**: Wei Shen, Yilu Guo, Yan Wang, Kai Zhao, Bo Wang, Alan Yuille
- **Comment**: Accepted by TPAMI. arXiv admin note: substantial text overlap with
  arXiv:1712.07195
- **Journal**: None
- **Summary**: Age estimation from facial images is typically cast as a label distribution learning or regression problem, since aging is a gradual progress. Its main challenge is the facial feature space w.r.t. ages is inhomogeneous, due to the large variation in facial appearance across different persons of the same age and the non-stationary property of aging. In this paper, we propose two Deep Differentiable Random Forests methods, Deep Label Distribution Learning Forest (DLDLF) and Deep Regression Forest (DRF), for age estimation. Both of them connect split nodes to the top layer of convolutional neural networks (CNNs) and deal with inhomogeneous data by jointly learning input-dependent data partitions at the split nodes and age distributions at the leaf nodes. This joint learning follows an alternating strategy: (1) Fixing the leaf nodes and optimizing the split nodes and the CNN parameters by Back-propagation; (2) Fixing the split nodes and optimizing the leaf nodes by Variational Bounding. Two Deterministic Annealing processes are introduced into the learning of the split and leaf nodes, respectively, to avoid poor local optima and obtain better estimates of tree parameters free of initial values. Experimental results show that DLDLF and DRF achieve state-of-the-art performance on three age estimation datasets.



### Compact Global Descriptor for Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.09665v10
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09665v10)
- **Published**: 2019-07-23 02:51:47+00:00
- **Updated**: 2020-08-05 00:51:57+00:00
- **Authors**: Xiangyu He, Ke Cheng, Qiang Chen, Qinghao Hu, Peisong Wang, Jian Cheng
- **Comment**: This paper will be included in our future works as a subsection
- **Journal**: None
- **Summary**: Long-range dependencies modeling, widely used in capturing spatiotemporal correlation, has shown to be effective in CNN dominated computer vision tasks. Yet neither stacks of convolutional operations to enlarge receptive fields nor recent nonlocal modules is computationally efficient. In this paper, we present a generic family of lightweight global descriptors for modeling the interactions between positions across different dimensions (e.g., channels, frames). This descriptor enables subsequent convolutions to access the informative global features with negligible computational complexity and parameters. Benchmark experiments show that the proposed method can complete state-of-the-art long-range mechanisms with a significant reduction in extra computing cost. Code available at https://github.com/HolmesShuan/Compact-Global-Descriptor.



### Deformable Registration Using Average Geometric Transformations for Brain MR Images
- **Arxiv ID**: http://arxiv.org/abs/1907.09670v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09670v1)
- **Published**: 2019-07-23 03:05:53+00:00
- **Updated**: 2019-07-23 03:05:53+00:00
- **Authors**: Yongpei Zhu, Zicong Zhou, Guojun Liao, Kehong Yuan
- **Comment**: 9 pages,9 figures
- **Journal**: None
- **Summary**: Accurate registration of medical images is vital for doctor's diagnosis and quantitative analysis. In this paper, we propose a new deformable medical image registration method based on average geometric transformations and VoxelMorph CNN architecture. We compute the differential geometric information including Jacobian determinant(JD) and the curl vector(CV) of diffeomorphic registration field and use them as multi-channel of VoxelMorph CNN for second train. In addition, we use the average transformation to construct a standard brain MRI atlas which can be used as fixed image. We verify our method on two datasets including ADNI dataset and MRBrainS18 Challenge dataset, and obtain excellent improvement on MR image registration with average Dice scores and non-negative Jacobian locations compared with MIT's original method. The experimental results show the method can achieve better performance in brain MRI diagnosis.



### Effortless Deep Training for Traffic Sign Detection Using Templates and Arbitrary Natural Images
- **Arxiv ID**: http://arxiv.org/abs/1907.09679v1
- **DOI**: 10.1109/IJCNN.2019.8852086
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09679v1)
- **Published**: 2019-07-23 03:43:00+00:00
- **Updated**: 2019-07-23 03:43:00+00:00
- **Authors**: Lucas Tabelini Torres, Thiago M. Paixão, Rodrigo F. Berriel, Alberto F. De Souza, Claudine Badue, Nicu Sebe, Thiago Oliveira-Santos
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has been successfully applied to several problems related to autonomous driving. Often, these solutions rely on large networks that require databases of real image samples of the problem (i.e., real world) for proper training. The acquisition of such real-world data sets is not always possible in the autonomous driving context, and sometimes their annotation is not feasible (e.g., takes too long or is too expensive). Moreover, in many tasks, there is an intrinsic data imbalance that most learning-based methods struggle to cope with. It turns out that traffic sign detection is a problem in which these three issues are seen altogether. In this work, we propose a novel database generation method that requires only (i) arbitrary natural images, i.e., requires no real image from the domain of interest, and (ii) templates of the traffic signs, i.e., templates synthetically created to illustrate the appearance of the category of a traffic sign. The effortlessly generated training database is shown to be effective for the training of a deep detector (such as Faster R-CNN) on German traffic signs, achieving 95.66% of mAP on average. In addition, the proposed method is able to detect traffic signs with an average precision, recall and F1-score of about 94%, 91% and 93%, respectively. The experiments surprisingly show that detectors can be trained with simple data generation methods and without problem domain data for the background, which is in the opposite direction of the common sense for deep learning.



### Similarity-Preserving Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/1907.09682v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09682v2)
- **Published**: 2019-07-23 03:56:14+00:00
- **Updated**: 2019-08-01 22:48:24+00:00
- **Authors**: Frederick Tung, Greg Mori
- **Comment**: ICCV 2019 camera ready
- **Journal**: None
- **Summary**: Knowledge distillation is a widely applicable technique for training a student neural network under the guidance of a trained teacher network. For example, in neural network compression, a high-capacity teacher is distilled to train a compact student; in privileged learning, a teacher trained with privileged data is distilled to train a student without access to that data. The distillation loss determines how a teacher's knowledge is captured and transferred to the student. In this paper, we propose a new form of knowledge distillation loss that is inspired by the observation that semantically similar inputs tend to elicit similar activation patterns in a trained network. Similarity-preserving knowledge distillation guides the training of a student network such that input pairs that produce similar (dissimilar) activations in the teacher network produce similar (dissimilar) activations in the student network. In contrast to previous distillation methods, the student is not required to mimic the representation space of the teacher, but rather to preserve the pairwise similarities in its own representation space. Experiments on three public datasets demonstrate the potential of our approach.



### Deep-SLAM++: Object-level RGBD SLAM based on class-specific deep shape priors
- **Arxiv ID**: http://arxiv.org/abs/1907.09691v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.09691v2)
- **Published**: 2019-07-23 04:24:25+00:00
- **Updated**: 2019-12-09 13:25:38+00:00
- **Authors**: Lan Hu, Wanting Xu, Kun Huang, Laurent Kneip
- **Comment**: None
- **Journal**: None
- **Summary**: In an effort to increase the capabilities of SLAM systems and produce object-level representations, the community increasingly investigates the imposition of higher-level priors into the estimation process. One such example is given by employing object detectors to load and register full CAD models. Our work extends this idea to environments with unknown objects and imposes object priors by employing modern class-specific neural networks to generate complete model geometry proposals. The difficulty of using such predictions in a real SLAM scenario is that the prediction performance depends on the view-point and measurement quality, with even small changes of the input data sometimes leading to a large variability in the network output. We propose a discrete selection strategy that finds the best among multiple proposals from different registered views by re-enforcing the agreement with the online depth measurements. The result is an effective object-level RGBD SLAM system that produces compact, high-fidelity, and dense 3D maps with semantic annotations. It outperforms traditional fusion strategies in terms of map completeness and resilience against degrading measurement quality.



### Adaptive Compression-based Lifelong Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.09695v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09695v1)
- **Published**: 2019-07-23 04:58:52+00:00
- **Updated**: 2019-07-23 04:58:52+00:00
- **Authors**: Shivangi Srivastava, Maxim Berman, Matthew B. Blaschko, Devis Tuia
- **Comment**: Accepted at BMVC 2019
- **Journal**: None
- **Summary**: The problem of a deep learning model losing performance on a previously learned task when fine-tuned to a new one is a phenomenon known as Catastrophic forgetting. There are two major ways to mitigate this problem: either preserving activations of the initial network during training with a new task; or restricting the new network activations to remain close to the initial ones. The latter approach falls under the denomination of lifelong learning, where the model is updated in a way that it performs well on both old and new tasks, without having access to the old task's training samples anymore. Recently, approaches like pruning networks for freeing network capacity during sequential learning of tasks have been gaining in popularity. Such approaches allow learning small networks while making redundant parameters available for the next tasks. The common problem encountered with these approaches is that the pruning percentage is hard-coded, irrespective of the number of samples, of the complexity of the learning task and of the number of classes in the dataset. We propose a method based on Bayesian optimization to perform adaptive compression/pruning of the network and show its effectiveness in lifelong learning. Our method learns to perform heavy pruning for small and/or simple datasets while using milder compression rates for large and/or complex data. Experiments on classification and semantic segmentation demonstrate the applicability of learning network compression, where we are able to effectively preserve performances along sequences of tasks of varying complexity.



### BMN: Boundary-Matching Network for Temporal Action Proposal Generation
- **Arxiv ID**: http://arxiv.org/abs/1907.09702v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09702v1)
- **Published**: 2019-07-23 05:53:28+00:00
- **Updated**: 2019-07-23 05:53:28+00:00
- **Authors**: Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, Shilei Wen
- **Comment**: This paper is accepted by ICCV 2019
- **Journal**: None
- **Summary**: Temporal action proposal generation is an challenging and promising task which aims to locate temporal regions in real-world videos where action or event may occur. Current bottom-up proposal generation methods can generate proposals with precise boundary, but cannot efficiently generate adequately reliable confidence scores for retrieving proposals. To address these difficulties, we introduce the Boundary-Matching (BM) mechanism to evaluate confidence scores of densely distributed proposals, which denote a proposal as a matching pair of starting and ending boundaries and combine all densely distributed BM pairs into the BM confidence map. Based on BM mechanism, we propose an effective, efficient and end-to-end proposal generation method, named Boundary-Matching Network (BMN), which generates proposals with precise temporal boundaries as well as reliable confidence scores simultaneously. The two-branches of BMN are jointly trained in an unified framework. We conduct experiments on two challenging datasets: THUMOS-14 and ActivityNet-1.3, where BMN shows significant performance improvement with remarkable efficiency and generalizability. Further, combining with existing action classifier, BMN can achieve state-of-the-art temporal action detection performance.



### 2D-CTC for Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.09705v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.09705v1)
- **Published**: 2019-07-23 05:55:28+00:00
- **Updated**: 2019-07-23 05:55:28+00:00
- **Authors**: Zhaoyi Wan, Fengming Xie, Yibo Liu, Xiang Bai, Cong Yao
- **Comment**: None
- **Journal**: None
- **Summary**: Scene text recognition has been an important, active research topic in computer vision for years. Previous approaches mainly consider text as 1D signals and cast scene text recognition as a sequence prediction problem, by feat of CTC or attention based encoder-decoder framework, which is originally designed for speech recognition. However, different from speech voices, which are 1D signals, text instances are essentially distributed in 2D image spaces. To adhere to and make use of the 2D nature of text for higher recognition accuracy, we extend the vanilla CTC model to a second dimension, thus creating 2D-CTC. 2D-CTC can adaptively concentrate on most relevant features while excluding the impact from clutters and noises in the background; It can also naturally handle text instances with various forms (horizontal, oriented and curved) while giving more interpretable intermediate predictions. The experiments on standard benchmarks for scene text recognition, such as IIIT-5K, ICDAR 2015, SVP-Perspective, and CUTE80, demonstrate that the proposed 2D-CTC model outperforms state-of-the-art methods on the text of both regular and irregular shapes. Moreover, 2D-CTC exhibits its superiority over prior art on training and testing speed. Our implementation and models of 2D-CTC will be made publicly available soon later.



### LYTNet: A Convolutional Neural Network for Real-Time Pedestrian Traffic Lights and Zebra Crossing Recognition for the Visually Impaired
- **Arxiv ID**: http://arxiv.org/abs/1907.09706v1
- **DOI**: 10.1007/978-3-030-29888-3_21
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.09706v1)
- **Published**: 2019-07-23 05:56:43+00:00
- **Updated**: 2019-07-23 05:56:43+00:00
- **Authors**: Samuel Yu, Heon Lee, John Kim
- **Comment**: 12 pages, 5 figures, 6 tables, International Conference on Computer
  Analysis of Images and Patterns (CAIP)
- **Journal**: None
- **Summary**: Currently, the visually impaired rely on either a sighted human, guide dog, or white cane to safely navigate. However, the training of guide dogs is extremely expensive, and canes cannot provide essential information regarding the color of traffic lights and direction of crosswalks. In this paper, we propose a deep learning based solution that provides information regarding the traffic light mode and the position of the zebra crossing. Previous solutions that utilize machine learning only provide one piece of information and are mostly binary: only detecting red or green lights. The proposed convolutional neural network, LYTNet, is designed for comprehensiveness, accuracy, and computational efficiency. LYTNet delivers both of the two most important pieces of information for the visually impaired to cross the road. We provide five classes of pedestrian traffic lights rather than the commonly seen three or four, and a direction vector representing the midline of the zebra crossing that is converted from the 2D image plane to real-world positions. We created our own dataset of pedestrian traffic lights containing over 5000 photos taken at hundreds of intersections in Shanghai. The experiments carried out achieve a classification accuracy of 94%, average angle error of 6.35 degrees, with a frame rate of 20 frames per second when testing the network on an iPhone 7 with additional post-processing steps.



### RRNet: Repetition-Reduction Network for Energy Efficient Decoder of Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1907.09707v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.09707v2)
- **Published**: 2019-07-23 05:59:05+00:00
- **Updated**: 2019-07-31 05:26:19+00:00
- **Authors**: Sangyun Oh, Hye-Jin S. Kim, Jongeun Lee, Junmo Kim
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: We introduce Repetition-Reduction network (RRNet) for resource-constrained depth estimation, offering significantly improved efficiency in terms of computation, memory and energy consumption. The proposed method is based on repetition-reduction (RR) blocks. The RR blocks consist of the set of repeated convolutions and the residual connection layer that take place of the pointwise reduction layer with linear connection to the decoder. The RRNet help reduce memory usage and power consumption in the residual connections to the decoder layers. RRNet consumes approximately 3.84 times less energy and 3.06 times less meory and is approaximately 2.21 times faster, without increasing the demand on hardware resource relative to the baseline network (Godard et al, CVPR'17), outperforming current state-of-the-art lightweight architectures such as SqueezeNet, ShuffleNet, MobileNet and PyDNet.



### Invertible Network for Classification and Biomarker Selection for ASD
- **Arxiv ID**: http://arxiv.org/abs/1907.09729v1
- **DOI**: 10.1007/978-3-030-32248-9_78
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09729v1)
- **Published**: 2019-07-23 07:31:39+00:00
- **Updated**: 2019-07-23 07:31:39+00:00
- **Authors**: Juntang Zhuang, Nicha C. Dvornek, Xiaoxiao Li, Pamela Ventola, James S. Duncan
- **Comment**: None
- **Journal**: Medical Image Computing and Computer-Assisted Intervention 2019,
  MICCAI
- **Summary**: Determining biomarkers for autism spectrum disorder (ASD) is crucial to understanding its mechanisms. Recently deep learning methods have achieved success in the classification task of ASD using fMRI data. However, due to the black-box nature of most deep learning models, it's hard to perform biomarker selection and interpret model decisions. The recently proposed invertible networks can accurately reconstruct the input from its output, and have the potential to unravel the black-box representation. Therefore, we propose a novel method to classify ASD and identify biomarkers for ASD using the connectivity matrix calculated from fMRI as the input. Specifically, with invertible networks, we explicitly determine the decision boundary and the projection of data points onto the boundary. Like linear classifiers, the difference between a point and its projection onto the decision boundary can be viewed as the explanation. We then define the importance as the explanation weighted by the gradient of prediction $w.r.t$ the input, and identify biomarkers based on this importance measure. We perform a regression task to further validate our biomarker selection: compared to using all edges in the connectivity matrix, using the top 10\% important edges we generate a lower regression error on 6 different severity scores. Our experiments show that the invertible network is both effective at ASD classification and interpretable, allowing for discovery of reliable biomarkers.



### Variational Registration of Multiple Images with the SVD based SqN Distance Measure
- **Arxiv ID**: http://arxiv.org/abs/1907.09732v1
- **DOI**: 10.1007/978-3-030-22368-7_20
- **Categories**: **eess.IV**, cs.CV, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1907.09732v1)
- **Published**: 2019-07-23 07:41:22+00:00
- **Updated**: 2019-07-23 07:41:22+00:00
- **Authors**: Kai Brehmer, Hari Om Aggrawal, Stefan Heldmann, Jan Modersitzki
- **Comment**: 12 pages, 5 figures, accepted at the conference "Scale Space and
  Variational Methods" in Hofgeismar, Germany 2019
- **Journal**: None
- **Summary**: Image registration, especially the quantification of image similarity, is an important task in image processing. Various approaches for the comparison of two images are discussed in the literature. However, although most of these approaches perform very well in a two image scenario, an extension to a multiple images scenario deserves attention. In this article, we discuss and compare registration methods for multiple images. Our key assumption is, that information about the singular values of a feature matrix of images can be used for alignment. We introduce, discuss and relate three recent approaches from the literature: the Schatten q-norm based SqN distance measure, a rank based approach, and a feature volume based approach. We also present results for typical applications such as dynamic image sequences or stacks of histological sections. Our results indicate that the SqN approach is in fact a suitable distance measure for image registration. Moreover, our examples also indicate that the results obtained by SqN are superior to those obtained by its competitors.



### Shared Generative Latent Representation Learning for Multi-view Clustering
- **Arxiv ID**: http://arxiv.org/abs/1907.09747v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09747v1)
- **Published**: 2019-07-23 08:20:38+00:00
- **Updated**: 2019-07-23 08:20:38+00:00
- **Authors**: Ming Yin, Weitian Huang, Junbin Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Clustering multi-view data has been a fundamental research topic in the computer vision community. It has been shown that a better accuracy can be achieved by integrating information of all the views than just using one view individually. However, the existing methods often struggle with the issues of dealing with the large-scale datasets and the poor performance in reconstructing samples. This paper proposes a novel multi-view clustering method by learning a shared generative latent representation that obeys a mixture of Gaussian distributions. The motivation is based on the fact that the multi-view data share a common latent embedding despite the diversity among the views. Specifically, benefited from the success of the deep generative learning, the proposed model not only can extract the nonlinear features from the views, but render a powerful ability in capturing the correlations among all the views. The extensive experimental results, on several datasets with different scales, demonstrate that the proposed method outperforms the state-of-the-art methods under a range of performance criteria.



### Controlling biases and diversity in diverse image-to-image translation
- **Arxiv ID**: http://arxiv.org/abs/1907.09754v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09754v1)
- **Published**: 2019-07-23 08:35:23+00:00
- **Updated**: 2019-07-23 08:35:23+00:00
- **Authors**: Yaxing Wang, Abel Gonzalez-Garcia, Joost van de Weijer, Luis Herranz
- **Comment**: The paper is under consideration at Computer Vision and Image
  Understanding
- **Journal**: None
- **Summary**: The task of unpaired image-to-image translation is highly challenging due to the lack of explicit cross-domain pairs of instances. We consider here diverse image translation (DIT), an even more challenging setting in which an image can have multiple plausible translations. This is normally achieved by explicitly disentangling content and style in the latent representation and sampling different styles codes while maintaining the image content. Despite the success of current DIT models, they are prone to suffer from bias. In this paper, we study the problem of bias in image-to-image translation. Biased datasets may add undesired changes (e.g. change gender or race in face images) to the output translations as a consequence of the particular underlying visual distribution in the target domain. In order to alleviate the effects of this problem we propose the use of semantic constraints that enforce the preservation of desired image properties. Our proposed model is a step towards unbiased diverse image-to-image translation (UDIT), and results in less unwanted changes in the translated images while still performing the wanted transformation. Experiments on several heavily biased datasets show the effectiveness of the proposed techniques in different domains such as faces, objects, and scenes.



### Not Only Look But Observe: Variational Observation Model of Scene-Level 3D Multi-Object Understanding for Probabilistic SLAM
- **Arxiv ID**: http://arxiv.org/abs/1907.09760v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1907.09760v3)
- **Published**: 2019-07-23 08:45:24+00:00
- **Updated**: 2020-02-04 03:27:35+00:00
- **Authors**: Hyeonwoo Yu
- **Comment**: None
- **Journal**: None
- **Summary**: We present NOLBO, a variational observation model estimation for 3D multi-object from 2D single shot. Previous probabilistic instance-level understandings mainly consider the single-object image, not single shot with multi-object; relations between objects and the entire scene are out of their focus. The objectness of each observation also hardly join their model. Therefore, we propose a method to approximate the Bayesian observation model of scene-level 3D multi-object understanding. By exploiting variational auto-encoder (VAE), we estimate latent variables from the entire scene, which follow tractable distributions and concurrently imply 3D full shape and pose. To perform object-oriented data association and probabilistic simultaneous localization and mapping (SLAM), our observation models can easily be adopted to probabilistic inference by replacing object-oriented features with latent variables.



### Multisensory Learning Framework for Robot Drumming
- **Arxiv ID**: http://arxiv.org/abs/1907.09775v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1907.09775v1)
- **Published**: 2019-07-23 09:09:18+00:00
- **Updated**: 2019-07-23 09:09:18+00:00
- **Authors**: A. Barsky, C. Zito, H. Mori, T. Ogata, J. L. Wyatt
- **Comment**: Extended abstract
- **Journal**: Workshop on Crossmodal Learning for Intelligent Robotics 2nd
  Edition. IEEE/RSJ International Conference on Intelligent Robots and Systems
  (IROS), 2018
- **Summary**: The hype about sensorimotor learning is currently reaching high fever, thanks to the latest advancement in deep learning. In this paper, we present an open-source framework for collecting large-scale, time-synchronised synthetic data from highly disparate sensory modalities, such as audio, video, and proprioception, for learning robot manipulation tasks. We demonstrate the learning of non-linear sensorimotor mappings for a humanoid drumming robot that generates novel motion sequences from desired audio data using cross-modal correspondences. We evaluate our system through the quality of its cross-modal retrieval, for generating suitable motion sequences to match desired unseen audio or video sequences.



### Hallucinating Beyond Observation: Learning to Complete with Partial Observation and Unpaired Prior Knowledge
- **Arxiv ID**: http://arxiv.org/abs/1907.09786v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1907.09786v2)
- **Published**: 2019-07-23 09:40:43+00:00
- **Updated**: 2019-09-06 13:34:21+00:00
- **Authors**: Chenyang Lu, Gijs Dubbelman
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel single-step training strategy that allows convolutional encoder-decoder networks that use skip connections, to complete partially observed data by means of hallucination. This strategy is demonstrated for the task of completing 2-D road layouts as well as 3-D vehicle shapes. As input, it takes data from a partially observed domain, for which no ground truth is available, and data from an unpaired prior knowledge domain and trains the network in an end-to-end manner. Our single-step training strategy is compared against two state-of-the-art baselines, one using a two-step auto-encoder training strategy and one using an adversarial strategy. Our novel strategy achieves an improvement up to +12.2% F-measure on the Cityscapes dataset. The learned network intrinsically generalizes better than the baselines on unseen datasets, which is demonstrated by an improvement up to +23.8% F-measure on the unseen KITTI dataset. Moreover, our approach outperforms the baselines using the same backbone network on the 3-D shape completion benchmark by a margin of 0.006 Hamming distance.



### PointAtrousGraph: Deep Hierarchical Encoder-Decoder with Point Atrous Convolution for Unorganized 3D Points
- **Arxiv ID**: http://arxiv.org/abs/1907.09798v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09798v2)
- **Published**: 2019-07-23 10:16:58+00:00
- **Updated**: 2019-09-13 09:41:25+00:00
- **Authors**: Liang Pan, Chee-Meng Chew, Gim Hee Lee
- **Comment**: 11 pages, 10 figures
- **Journal**: None
- **Summary**: Motivated by the success of encoding multi-scale contextual information for image analysis, we propose our PointAtrousGraph (PAG) - a deep permutation-invariant hierarchical encoder-decoder for efficiently exploiting multi-scale edge features in point clouds. Our PAG is constructed by several novel modules, such as Point Atrous Convolution (PAC), Edge-preserved Pooling (EP) and Edge-preserved Unpooling (EU). Similar with atrous convolution, our PAC can effectively enlarge receptive fields of filters and thus densely learn multi-scale point features. Following the idea of non-overlapping max-pooling operations, we propose our EP to preserve critical edge features during subsampling. Correspondingly, our EU modules gradually recover spatial information for edge features. In addition, we introduce chained skip subsampling/upsampling modules that directly propagate edge features to the final stage. Particularly, our proposed auxiliary loss functions can further improve our performance. Experimental results show that our PAG outperform previous state-of-the-art methods on various 3D semantic perception applications.



### NPSA: Nonorthogonal Principal Skewness Analysis
- **Arxiv ID**: http://arxiv.org/abs/1907.09811v1
- **DOI**: 10.1109/TIP.2020.2984849
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.09811v1)
- **Published**: 2019-07-23 10:48:15+00:00
- **Updated**: 2019-07-23 10:48:15+00:00
- **Authors**: Xiurui Geng, Lei Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Principal skewness analysis (PSA) has been introduced for feature extraction in hyperspectral imagery. As a third-order generalization of principal component analysis (PCA), its solution of searching for the locally maximum skewness direction is transformed into the problem of calculating the eigenpairs (the eigenvalues and the corresponding eigenvectors) of a coskewness tensor. By combining a fixed-point method with an orthogonal constraint, it can prevent the new eigenpairs from converging to the same maxima that has been determined before. However, the eigenvectors of the supersymmetric tensor are not inherently orthogonal in general, which implies that the results obtained by the search strategy used in PSA may unavoidably deviate from the actual eigenpairs. In this paper, we propose a new nonorthogonal search strategy to solve this problem and the new algorithm is named nonorthogonal principal skewness analysis (NPSA). The contribution of NPSA lies in the finding that the search space of the eigenvector to be determined can be enlarged by using the orthogonal complement of the Kronecker product of the previous one, instead of its orthogonal complement space. We give a detailed theoretical proof to illustrate why the new strategy can result in the more accurate eigenpairs. In addition, after some algebraic derivations, the complexity of the presented algorithm is also greatly reduced. Experiments with both simulated data and real multi/hyperspectral imagery demonstrate its validity in feature extraction.



### Bilinear Graph Networks for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1907.09815v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09815v2)
- **Published**: 2019-07-23 10:59:54+00:00
- **Updated**: 2020-02-03 11:26:12+00:00
- **Authors**: Dalu Guo, Chang Xu, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: This paper revisits the bilinear attention networks in the visual question answering task from a graph perspective. The classical bilinear attention networks build a bilinear attention map to extract the joint representation of words in the question and objects in the image but lack fully exploring the relationship between words for complex reasoning. In contrast, we develop bilinear graph networks to model the context of the joint embeddings of words and objects. Two kinds of graphs are investigated, namely image-graph and question-graph. The image-graph transfers features of the detected objects to their related query words, enabling the output nodes to have both semantic and factual information. The question-graph exchanges information between these output nodes from image-graph to amplify the implicit yet important relationship between objects. These two kinds of graphs cooperate with each other, and thus our resulting model can model the relationship and dependency between objects, which leads to the realization of multi-step reasoning. Experimental results on the VQA v2.0 validation dataset demonstrate the ability of our method to handle the complex questions. On the test-std set, our best single model achieves state-of-the-art performance, boosting the overall accuracy to 72.41%.



### From Active Contours to Minimal Geodesic Paths: New Solutions to Active Contours Problems by Eikonal Equations
- **Arxiv ID**: http://arxiv.org/abs/1907.09828v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09828v2)
- **Published**: 2019-07-23 11:44:51+00:00
- **Updated**: 2019-09-27 06:06:07+00:00
- **Authors**: Da Chen, Laurent D. Cohen
- **Comment**: None
- **Journal**: None
- **Summary**: In this chapter, we give an overview of part of our previous work based on the minimal path framework and the Eikonal partial differential equation (PDE). We show that by designing adequate Riemannian and Randers geodesic metrics the minimal paths can be utilized to search for solutions to almost all of the active contour problems and to the Euler-Mumford elastica problem, which allows to blend the advantages from minimal geodesic paths and those original approaches, i.e. the active contours and elastica curves. The proposed minimal path-based models can be applied to deal with a broad variety of image analysis tasks such as boundary detection, image segmentation and tubular structure extraction. The numerical implementations for the computation of minimal paths are known to be quite efficient thanks to the Eikonal solvers such as the Finsler variant of the fast marching method.



### Real-Time Correlation Tracking via Joint Model Compression and Transfer
- **Arxiv ID**: http://arxiv.org/abs/1907.09831v1
- **DOI**: 10.1109/TIP.2020.2989544
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09831v1)
- **Published**: 2019-07-23 11:57:42+00:00
- **Updated**: 2019-07-23 11:57:42+00:00
- **Authors**: Ning Wang, Wengang Zhou, Yibing Song, Chao Ma, Houqiang Li
- **Comment**: 12 pages, 12 figures, submitted to IEEE Transactions on Image
  Processing
- **Journal**: None
- **Summary**: Correlation filters (CF) have received considerable attention in visual tracking because of their computational efficiency. Leveraging deep features via off-the-shelf CNN models (e.g., VGG), CF trackers achieve state-of-the-art performance while consuming a large number of computing resources. This limits deep CF trackers to be deployed to many mobile platforms on which only a single-core CPU is available. In this paper, we propose to jointly compress and transfer off-the-shelf CNN models within a knowledge distillation framework. We formulate a CNN model pretrained from the image classification task as a teacher network, and distill this teacher network into a lightweight student network as the feature extractor to speed up CF trackers. In the distillation process, we propose a fidelity loss to enable the student network to maintain the representation capability of the teacher network. Meanwhile, we design a tracking loss to adapt the objective of the student network from object recognition to visual tracking. The distillation process is performed offline on multiple layers and adaptively updates the student network using a background-aware online learning scheme. Extensive experiments on five challenging datasets demonstrate that the lightweight student network accelerates the speed of state-of-the-art deep CF trackers to real-time on a single-core CPU while maintaining almost the same tracking accuracy.



### ChromaGAN: Adversarial Picture Colorization with Semantic Class Distribution
- **Arxiv ID**: http://arxiv.org/abs/1907.09837v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09837v2)
- **Published**: 2019-07-23 12:20:27+00:00
- **Updated**: 2020-01-20 21:38:56+00:00
- **Authors**: Patricia Vitoria, Lara Raad, Coloma Ballester
- **Comment**: 8 pages + references
- **Journal**: None
- **Summary**: The colorization of grayscale images is an ill-posed problem, with multiple correct solutions. In this paper, we propose an adversarial learning colorization approach coupled with semantic information. A generative network is used to infer the chromaticity of a given grayscale image conditioned to semantic clues. This network is framed in an adversarial model that learns to colorize by incorporating perceptual and semantic understanding of color and class distributions. The model is trained via a fully self-supervised strategy. Qualitative and quantitative results show the capacity of the proposed method to colorize images in a realistic way achieving state-of-the-art results.



### Eye-based Continuous Affect Prediction
- **Arxiv ID**: http://arxiv.org/abs/1907.09896v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.09896v2)
- **Published**: 2019-07-23 14:18:30+00:00
- **Updated**: 2020-01-23 15:50:03+00:00
- **Authors**: Jonny O'Dwyer, Niall Murray, Ronan Flynn
- **Comment**: Accepted paper (pre-print) for 2019 8th International Conference on
  Affective Computing and Intelligent Interaction (ACII)
- **Journal**: None
- **Summary**: Eye-based information channels include the pupils, gaze, saccades, fixational movements, and numerous forms of eye opening and closure. Pupil size variation indicates cognitive load and emotion, while a person's gaze direction is said to be congruent with the motivation to approach or avoid stimuli. The eyelids are involved in facial expressions that can encode basic emotions. Additionally, eye-based cues can have implications for human annotators of emotions or feelings. Despite these facts, the use of eye-based cues in affective computing is in its infancy, however, and this work is intended to start to address this. Eye-based feature sets, incorporating data from all of the aforementioned information channels, that can be estimated from video are proposed. Feature set refinement is provided by way of continuous arousal and valence learning and prediction experiments on the RECOLA validation set. The eye-based features are then combined with a speech feature set to provide confirmation of their usefulness and assess affect prediction performance compared with group-of-humans-level performance on the RECOLA test set. The core contribution of this paper, a refined eye-based feature set, is shown to provide benefits for affect prediction. It is hoped that this work stimulates further research into eye-based affective computing.



### U4D: Unsupervised 4D Dynamic Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/1907.09905v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09905v1)
- **Published**: 2019-07-23 14:25:27+00:00
- **Updated**: 2019-07-23 14:25:27+00:00
- **Authors**: Armin Mustafa, Chris Russell, Adrian Hilton
- **Comment**: To appear in IEEE International Conference in Computer Vision ICCV
  2019
- **Journal**: None
- **Summary**: We introduce the first approach to solve the challenging problem of unsupervised 4D visual scene understanding for complex dynamic scenes with multiple interacting people from multi-view video. Our approach simultaneously estimates a detailed model that includes a per-pixel semantically and temporally coherent reconstruction, together with instance-level segmentation exploiting photo-consistency, semantic and motion information. We further leverage recent advances in 3D pose estimation to constrain the joint semantic instance segmentation and 4D temporally coherent reconstruction. This enables per person semantic instance segmentation of multiple interacting people in complex dynamic scenes. Extensive evaluation of the joint visual scene understanding framework against state-of-the-art methods on challenging indoor and outdoor sequences demonstrates a significant (approx 40%) improvement in semantic segmentation, reconstruction and scene flow accuracy.



### Speech, Head, and Eye-based Cues for Continuous Affect Prediction
- **Arxiv ID**: http://arxiv.org/abs/1907.09919v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1907.09919v2)
- **Published**: 2019-07-23 14:46:51+00:00
- **Updated**: 2020-01-23 15:59:13+00:00
- **Authors**: Jonny O'Dwyer
- **Comment**: Accepted paper (pre-print) for 2019 8th International Conference on
  Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)
- **Journal**: None
- **Summary**: Continuous affect prediction involves the discrete time-continuous regression of affect dimensions. Dimensions to be predicted often include arousal and valence. Continuous affect prediction researchers are now embracing multimodal model input. This provides motivation for researchers to investigate previously unexplored affective cues. Speech-based cues have traditionally received the most attention for affect prediction, however, non-verbal inputs have significant potential to increase the performance of affective computing systems and in addition, allow affect modelling in the absence of speech. However, non-verbal inputs that have received little attention for continuous affect prediction include eye and head-based cues. The eyes are involved in emotion displays and perception while head-based cues have been shown to contribute to emotion conveyance and perception. Additionally, these cues can be estimated non-invasively from video, using modern computer vision tools. This work exploits this gap by comprehensively investigating head and eye-based features and their combination with speech for continuous affect prediction. Hand-crafted, automatically generated and CNN-learned features from these modalities will be investigated for continuous affect prediction. The highest performing feature sets and feature set combinations will answer how effective these features are for the prediction of an individual's affective state.



### Deep Temporal Analysis for Non-Acted Body Affect Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.09945v1
- **DOI**: 10.1109/TAFFC.2020.3003816
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09945v1)
- **Published**: 2019-07-23 15:10:12+00:00
- **Updated**: 2019-07-23 15:10:12+00:00
- **Authors**: Danilo Avola, Luigi Cinque, Alessio Fagioli, Gian Luca Foresti, Cristiano Massaroni
- **Comment**: None
- **Journal**: IEEE Transactions on Affective Computing 2020
- **Summary**: Affective computing is a field of great interest in many computer vision applications, including video surveillance, behaviour analysis, and human-robot interaction. Most of the existing literature has addressed this field by analysing different sets of face features. However, in the last decade, several studies have shown how body movements can play a key role even in emotion recognition. The majority of these experiments on the body are performed by trained actors whose aim is to simulate emotional reactions. These unnatural expressions differ from the more challenging genuine emotions, thus invalidating the obtained results. In this paper, a solution for basic non-acted emotion recognition based on 3D skeleton and Deep Neural Networks (DNNs) is provided. The proposed work introduces three majors contributions. First, unlike the current state-of-the-art in non-acted body affect recognition, where only static or global body features are considered, in this work also temporal local movements performed by subjects in each frame are examined. Second, an original set of global and time-dependent features for body movement description is provided. Third, to the best of out knowledge, this is the first attempt to use deep learning methods for non-acted body affect recognition. Due to the novelty of the topic, only the UCLIC dataset is currently considered the benchmark for comparative tests. On the latter, the proposed method outperforms all the competitors.



### Whole-Sample Mapping of Cancerous and Benign Tissue Properties
- **Arxiv ID**: http://arxiv.org/abs/1907.09974v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.09974v1)
- **Published**: 2019-07-23 16:00:52+00:00
- **Updated**: 2019-07-23 16:00:52+00:00
- **Authors**: Lydia Neary-Zajiczek, Clara Essmann, Neil Clancy, Aiman Haider, Elena Miranda, Michael Shaw, Amir Gander, Brian Davidson, Delmiro Fernandez-Reyes, Vijay Pawar, Danail Stoyanov
- **Comment**: Accepted at MICCAI2019
- **Journal**: None
- **Summary**: Structural and mechanical differences between cancerous and healthy tissue give rise to variations in macroscopic properties such as visual appearance and elastic modulus that show promise as signatures for early cancer detection. Atomic force microscopy (AFM) has been used to measure significant differences in stiffness between cancerous and healthy cells owing to its high force sensitivity and spatial resolution, however due to absorption and scattering of light, it is often challenging to accurately locate where AFM measurements have been made on a bulk tissue sample. In this paper we describe an image registration method that localizes AFM elastic stiffness measurements with high-resolution images of haematoxylin and eosin (H\&E)-stained tissue to within 1.5 microns. Color RGB images are segmented into three structure types (lumen, cells and stroma) by a neural network classifier trained on ground-truth pixel data obtained through k-means clustering in HSV color space. Using the localized stiffness maps and corresponding structural information, a whole-sample stiffness map is generated with a region matching and interpolation algorithm that associates similar structures with measured stiffness values. We present results showing significant differences in stiffness between healthy and cancerous liver tissue and discuss potential applications of this technique.



### Evolutionary Algorithms and Efficient Data Analytics for Image Processing
- **Arxiv ID**: http://arxiv.org/abs/1907.12914v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1907.12914v3)
- **Published**: 2019-07-23 16:13:53+00:00
- **Updated**: 2020-10-19 16:10:16+00:00
- **Authors**: Farid Ghareh Mohammadi, Farzan Shenavarmasouleh, M. Hadi Amini, Hamid R. Arabnia
- **Comment**: 8 pages,5 figures,
- **Journal**: None
- **Summary**: Steganography algorithms facilitate communication between a source and a destination in a secret manner. This is done by embedding messages/text/data into images without impacting the appearance of the resultant images/videos. Steganalysis is the science of determining if an image has secret messages embedded/hidden in it. Because there are numerous steganography algorithms, and since each one of them requires a different type of steganalysis, the steganalysis process is extremely challenging. Thus, researchers aim to develop one universal steganalysis to detect all known and unknown steganography algorithms, ideally in real-time. Universal steganalysis extracts a large number of features to distinguish stego images from cover images. However, the increase in features leads to the problem of the curse of dimensionality (CoD), which is considered to be an NP-hard problem. This COD problem additionally makes real-time steganalysis hard. A large number of features generates large datasets for which machine learning cannot generate an optimal model. Generating a machine learning based model also takes a long time which makes real-time processing appear impossible in any optimization for time-intensive fields such as visual computing. Possible solutions for CoD are deep learning and evolutionary algorithms that overcome the machine learning limitations. In this study, we investigate previously developed evolutionary algorithms for boosting real-time image processing and argue that they provide the most promising solutions for the CoD problem.



### Learning Shape Priors for Robust Cardiac MR Segmentation from Multi-view Images
- **Arxiv ID**: http://arxiv.org/abs/1907.09983v2
- **DOI**: 10.1007/978-3-030-32245-8_58
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.09983v2)
- **Published**: 2019-07-23 16:22:43+00:00
- **Updated**: 2019-12-17 17:06:11+00:00
- **Authors**: Chen Chen, Carlo Biffi, Giacomo Tarroni, Steffen Petersen, Wenjia Bai, Daniel Rueckert
- **Comment**: 11 pages, 5 figures, accepted at MICCAI 2019, Camera-ready version
- **Journal**: None
- **Summary**: Cardiac MR image segmentation is essential for the morphological and functional analysis of the heart. Inspired by how experienced clinicians assess the cardiac morphology and function across multiple standard views (i.e. long- and short-axis views), we propose a novel approach which learns anatomical shape priors across different 2D standard views and leverages these priors to segment the left ventricular (LV) myocardium from short-axis MR image stacks. The proposed segmentation method has the advantage of being a 2D network but at the same time incorporates spatial context from multiple, complementary views that span a 3D space. Our method achieves accurate and robust segmentation of the myocardium across different short-axis slices (from apex to base), outperforming baseline models (e.g. 2D U-Net, 3D U-Net) while achieving higher data efficiency. Compared to the 2D U-Net, the proposed method reduces the mean Hausdorff distance (mm) from 3.24 to 2.49 on the apical slices, from 2.34 to 2.09 on the middle slices and from 3.62 to 2.76 on the basal slices on the test set, when only 10% of the training data was used.



### An Improved Convolutional Neural Network System for Automatically Detecting Rebar in GPR Data
- **Arxiv ID**: http://arxiv.org/abs/1907.09997v1
- **DOI**: 10.1061/9780784482438.054
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09997v1)
- **Published**: 2019-07-23 16:46:50+00:00
- **Updated**: 2019-07-23 16:46:50+00:00
- **Authors**: Zhongming Xiang, Abbas Rashidi, Ge, Ou
- **Comment**: None
- **Journal**: None
- **Summary**: As a mature technology, Ground Penetration Radar (GPR) is now widely employed in detecting rebar and other embedded elements in concrete structures. Manually recognizing rebar from GPR data is a time-consuming and error-prone procedure. Although there are several approaches to automatically detect rebar, it is still challenging to find a high resolution and efficient method for different rebar arrangements, especially for closely spaced rebar meshes. As an improved Convolution Neural Network (CNN), AlexNet shows superiority over traditional methods in image recognition domain. Thus, this paper introduces AlexNet as an alternative solution for automatically detecting rebar within GPR data. In order to show the efficiency of the proposed approach, a traditional CNN is built as the comparative option. Moreover, this research evaluates the impacts of different rebar arrangements and different window sizes on the accuracy of results. The results revealed that: (1) AlexNet outperforms the traditional CNN approach, and its superiority is more notable when the rebar meshes are densely distributed; (2) the detection accuracy significantly varies with changing the size of splitting window, and a proper window should contain enough information about rebar; (3) uniformly and sparsely distributed rebar meshes are more recognizable than densely or unevenly distributed items, due to lower chances of signal interferences.



### A-MAL: Automatic Movement Assessment Learning from Properly Performed Movements in 3D Skeleton Videos
- **Arxiv ID**: http://arxiv.org/abs/1907.10004v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10004v4)
- **Published**: 2019-07-23 16:57:26+00:00
- **Updated**: 2020-07-15 16:47:15+00:00
- **Authors**: Tal Hakim, Ilan Shimshoni
- **Comment**: Accepted and orally presented in CVPM 2019
- **Journal**: None
- **Summary**: The task of assessing movement quality has recently gained high demand in a variety of domains. The ability to automatically assess subject movement in videos that were captured by affordable devices, such as Kinect cameras, is essential for monitoring clinical rehabilitation processes, for improving motor skills and for movement learning tasks. The need to pay attention to low-level details while accurately tracking the movement stages, makes this task very challenging. In this work, we introduce A-MAL, an automatic, strong movement assessment learning algorithm that only learns from properly-performed movement videos without further annotations, powered by a deviation time-segmentation algorithm, a parameter relevance detection algorithm, a novel time-warping algorithm that is based on automatic detection of common temporal points-of-interest and a textual-feedback generation mechanism. We demonstrate our method on movements from the Fugl-Meyer Assessment (FMA) test, which is typically held by occupational therapists in order to monitor patients' recovery processes after strokes.



### Incremental Class Discovery for Semantic Segmentation with RGBD Sensing
- **Arxiv ID**: http://arxiv.org/abs/1907.10008v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1907.10008v1)
- **Published**: 2019-07-23 17:02:46+00:00
- **Updated**: 2019-07-23 17:02:46+00:00
- **Authors**: Yoshikatsu Nakajima, Byeongkeun Kang, Hideo Saito, Kris Kitani
- **Comment**: 10 pages, To appear at IEEE International Conference on Computer
  Vision (ICCV 2019)
- **Journal**: None
- **Summary**: This work addresses the task of open world semantic segmentation using RGBD sensing to discover new semantic classes over time. Although there are many types of objects in the real-word, current semantic segmentation methods make a closed world assumption and are trained only to segment a limited number of object classes. Towards a more open world approach, we propose a novel method that incrementally learns new classes for image segmentation. The proposed system first segments each RGBD frame using both color and geometric information, and then aggregates that information to build a single segmented dense 3D map of the environment. The segmented 3D map representation is a key component of our approach as it is used to discover new object classes by identifying coherent regions in the 3D map that have no semantic label. The use of coherent region in the 3D map as a primitive element, rather than traditional elements such as surfels or voxels, also significantly reduces the computational complexity and memory use of our method. It thus leads to semi-real-time performance at {10.7}Hz when incrementally updating the dense 3D map at every frame. Through experiments on the NYUDv2 dataset, we demonstrate that the proposed method is able to correctly cluster objects of both known and unseen classes. We also show the quantitative comparison with the state-of-the-art supervised methods, the processing time of each step, and the influences of each component.



### Temporally Consistent Horizon Lines
- **Arxiv ID**: http://arxiv.org/abs/1907.10014v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10014v2)
- **Published**: 2019-07-23 17:18:04+00:00
- **Updated**: 2020-01-09 14:09:20+00:00
- **Authors**: Florian Kluger, Hanno Ackermann, Michael Ying Yang, Bodo Rosenhahn
- **Comment**: None
- **Journal**: None
- **Summary**: The horizon line is an important geometric feature for many image processing and scene understanding tasks in computer vision. For instance, in navigation of autonomous vehicles or driver assistance, it can be used to improve 3D reconstruction as well as for semantic interpretation of dynamic environments. While both algorithms and datasets exist for single images, the problem of horizon line estimation from video sequences has not gained attention. In this paper, we show how convolutional neural networks are able to utilise the temporal consistency imposed by video sequences in order to increase the accuracy and reduce the variance of horizon line estimates. A novel CNN architecture with an improved residual convolutional LSTM is presented for temporally consistent horizon line estimation. We propose an adaptive loss function that ensures stable training as well as accurate results. Furthermore, we introduce an extension of the KITTI dataset which contains precise horizon line labels for 43699 images across 72 video sequences. A comprehensive evaluation shows that the proposed approach consistently achieves superior performance compared with existing methods.



### Exploring Semantic Segmentation on the DCT Representation
- **Arxiv ID**: http://arxiv.org/abs/1907.10015v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10015v2)
- **Published**: 2019-07-23 17:18:39+00:00
- **Updated**: 2019-12-29 07:31:45+00:00
- **Authors**: Shao-Yuan Lo, Hsueh-Ming Hang
- **Comment**: Accepted in ACM International Conference on Multimedia in Asia
  (MMAsia) 2019
- **Journal**: None
- **Summary**: Typical convolutional networks are trained and conducted on RGB images. However, images are often compressed for memory savings and efficient transmission in real-world applications. In this paper, we explore methods for performing semantic segmentation on the discrete cosine transform (DCT) representation defined by the JPEG standard. We first rearrange the DCT coefficients to form a preferred input type, then we tailor an existing network to the DCT inputs. The proposed method has an accuracy close to the RGB model at about the same network complexity. Moreover, we investigate the impact of selecting different DCT components on segmentation performance. With a proper selection, one can achieve the same level accuracy using only 36% of the DCT coefficients. We further show the robustness of our method under the quantization errors. To our knowledge, this paper is the first to explore semantic segmentation on the DCT representation.



### Canonical Surface Mapping via Geometric Cycle Consistency
- **Arxiv ID**: http://arxiv.org/abs/1907.10043v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10043v2)
- **Published**: 2019-07-23 17:55:12+00:00
- **Updated**: 2019-08-15 17:42:54+00:00
- **Authors**: Nilesh Kulkarni, Abhinav Gupta, Shubham Tulsiani
- **Comment**: To appear at ICCV 2019. Project page:
  https://nileshkulkarni.github.io/csm/
- **Journal**: None
- **Summary**: We explore the task of Canonical Surface Mapping (CSM). Specifically, given an image, we learn to map pixels on the object to their corresponding locations on an abstract 3D model of the category. But how do we learn such a mapping? A supervised approach would require extensive manual labeling which is not scalable beyond a few hand-picked categories. Our key insight is that the CSM task (pixel to 3D), when combined with 3D projection (3D to pixel), completes a cycle. Hence, we can exploit a geometric cycle consistency loss, thereby allowing us to forgo the dense manual supervision. Our approach allows us to train a CSM model for a diverse set of classes, without sparse or dense keypoint annotation, by leveraging only foreground mask labels for training. We show that our predictions also allow us to infer dense correspondence between two images, and compare the performance of our approach against several methods that predict correspondence by leveraging varying amount of supervision.



### xR-EgoPose: Egocentric 3D Human Pose from an HMD Camera
- **Arxiv ID**: http://arxiv.org/abs/1907.10045v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10045v1)
- **Published**: 2019-07-23 17:58:03+00:00
- **Updated**: 2019-07-23 17:58:03+00:00
- **Authors**: Denis Tome, Patrick Peluse, Lourdes Agapito, Hernan Badino
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: We present a new solution to egocentric 3D body pose estimation from monocular images captured from a downward looking fish-eye camera installed on the rim of a head mounted virtual reality device. This unusual viewpoint, just 2 cm. away from the user's face, leads to images with unique visual appearance, characterized by severe self-occlusions and strong perspective distortions that result in a drastic difference in resolution between lower and upper body. Our contribution is two-fold. Firstly, we propose a new encoder-decoder architecture with a novel dual branch decoder designed specifically to account for the varying uncertainty in the 2D joint locations. Our quantitative evaluation, both on synthetic and real-world datasets, shows that our strategy leads to substantial improvements in accuracy over state of the art egocentric pose estimation approaches. Our second contribution is a new large-scale photorealistic synthetic dataset - xR-EgoPose - offering 383K frames of high quality renderings of people with a diversity of skin tones, body shapes, clothing, in a variety of backgrounds and lighting conditions, performing a range of actions. Our experiments show that the high variability in our new synthetic training corpus leads to good generalization to real world footage and to state of the art results on real world datasets with ground truth. Moreover, an evaluation on the Human3.6M benchmark shows that the performance of our method is on par with top performing approaches on the more classic problem of 3D human pose from a third person viewpoint.



### Trading via Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1907.10046v3
- **DOI**: None
- **Categories**: **cs.CV**, q-fin.CP, q-fin.TR
- **Links**: [PDF](http://arxiv.org/pdf/1907.10046v3)
- **Published**: 2019-07-23 17:58:10+00:00
- **Updated**: 2020-10-26 05:01:18+00:00
- **Authors**: Naftali Cohen, Tucker Balch, Manuela Veloso
- **Comment**: None
- **Journal**: None
- **Summary**: The art of systematic financial trading evolved with an array of approaches, ranging from simple strategies to complex algorithms all relying, primary, on aspects of time-series analysis. Recently, after visiting the trading floor of a leading financial institution, we noticed that traders always execute their trade orders while observing images of financial time-series on their screens. In this work, we built upon the success in image recognition and examine the value in transforming the traditional time-series analysis to that of image classification. We create a large sample of financial time-series images encoded as candlestick (Box and Whisker) charts and label the samples following three algebraically-defined binary trade strategies. Using the images, we train over a dozen machine-learning classification models and find that the algorithms are very efficient in recovering the complicated, multiscale label-generating rules when the data is represented visually. We suggest that the transformation of continuous numeric time-series classification problem to a vision problem is useful for recovering signals typical of technical analysis.



### Multimodal Age and Gender Classification Using Ear and Profile Face Images
- **Arxiv ID**: http://arxiv.org/abs/1907.10081v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10081v1)
- **Published**: 2019-07-23 18:04:16+00:00
- **Updated**: 2019-07-23 18:04:16+00:00
- **Authors**: Dogucan Yaman, Fevziye Irem Eyiokur, Hazım Kemal Ekenel
- **Comment**: 8 pages, 4 figures, accepted for CVPR 2019 - Workshop on Biometrics
- **Journal**: None
- **Summary**: In this paper, we present multimodal deep neural network frameworks for age and gender classification, which take input a profile face image as well as an ear image. Our main objective is to enhance the accuracy of soft biometric trait extraction from profile face images by additionally utilizing a promising biometric modality: ear appearance. For this purpose, we provided end-to-end multimodal deep learning frameworks. We explored different multimodal strategies by employing data, feature, and score level fusion. To increase representation and discrimination capability of the deep neural networks, we benefited from domain adaptation and employed center loss besides softmax loss. We conducted extensive experiments on the UND-F, UND-J2, and FERET datasets. Experimental results indicated that profile face images contain a rich source of information for age and gender classification. We found that the presented multimodal system achieves very high age and gender classification accuracies. Moreover, we attained superior results compared to the state-of-the-art profile face image or ear image-based age and gender classification methods.



### GraphX$^{NET}-$ Chest X-Ray Classification Under Extreme Minimal Supervision
- **Arxiv ID**: http://arxiv.org/abs/1907.10085v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.10085v3)
- **Published**: 2019-07-23 18:10:32+00:00
- **Updated**: 2020-07-03 21:42:19+00:00
- **Authors**: Angelica I. Aviles-Rivero, Nicolas Papadakis, Ruoteng Li, Philip Sellars, Qingnan Fan, Robby T. Tan, Carola-Bibiane Schönlieb
- **Comment**: MICCAI 2019
- **Journal**: None
- **Summary**: The task of classifying X-ray data is a problem of both theoretical and clinical interest. Whilst supervised deep learning methods rely upon huge amounts of labelled data, the critical problem of achieving a good classification accuracy when an extremely small amount of labelled data is available has yet to be tackled. In this work, we introduce a novel semi-supervised framework for X-ray classification which is based on a graph-based optimisation model. To the best of our knowledge, this is the first method that exploits graph-based semi-supervised learning for X-ray data classification. Furthermore, we introduce a new multi-class classification functional with carefully selected class priors which allows for a smooth solution that strengthens the synergy between the limited number of labels and the huge amount of unlabelled data. We demonstrate, through a set of numerical and visual experiments, that our method produces highly competitive results on the ChestX-ray14 data set whilst drastically reducing the need for annotated data.



### Dynamic Facial Expression Generation on Hilbert Hypersphere with Conditional Wasserstein Generative Adversarial Nets
- **Arxiv ID**: http://arxiv.org/abs/1907.10087v2
- **DOI**: 10.1109/TPAMI.2020.3002500
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10087v2)
- **Published**: 2019-07-23 18:22:18+00:00
- **Updated**: 2020-05-28 18:14:42+00:00
- **Authors**: Naima Otberdout, Mohamed Daoudi, Anis Kacem, Lahoucine Ballihi, Stefano Berretti
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a novel approach for generating videos of the six basic facial expressions given a neutral face image. We propose to exploit the face geometry by modeling the facial landmarks motion as curves encoded as points on a hypersphere. By proposing a conditional version of manifold-valued Wasserstein generative adversarial network (GAN) for motion generation on the hypersphere, we learn the distribution of facial expression dynamics of different classes, from which we synthesize new facial expression motions. The resulting motions can be transformed to sequences of landmarks and then to images sequences by editing the texture information using another conditional Generative Adversarial Network. To the best of our knowledge, this is the first work that explores manifold-valued representations with GAN to address the problem of dynamic facial expression generation. We evaluate our proposed approach both quantitatively and qualitatively on two public datasets; Oulu-CASIA and MUG Facial Expression. Our experimental results demonstrate the effectiveness of our approach in generating realistic videos with continuous motion, realistic appearance and identity preservation. We also show the efficiency of our framework for dynamic facial expressions generation, dynamic facial expression transfer and data augmentation for training improved emotion recognition models.



### Exploring Factors for Improving Low Resolution Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.10104v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10104v2)
- **Published**: 2019-07-23 19:16:48+00:00
- **Updated**: 2019-07-25 17:22:08+00:00
- **Authors**: Omid Abdollahi Aghdam, Behzad Bozorgtabar, Hazım Kemal Ekenel, Jean-Philippe Thiran
- **Comment**: CVPR Workshop on Biometrics 2019
- **Journal**: None
- **Summary**: State-of-the-art deep face recognition approaches report near perfect performance on popular benchmarks, e.g., Labeled Faces in the Wild. However, their performance deteriorates significantly when they are applied on low quality images, such as those acquired by surveillance cameras. A further challenge for low resolution face recognition for surveillance applications is the matching of recorded low resolution probe face images with high resolution reference images, which could be the case in watchlist scenarios. In this paper, we have addressed these problems and investigated the factors that would contribute to the identification performance of the state-of-the-art deep face recognition models when they are applied to low resolution face recognition under mismatched conditions. We have observed that the following factors affect performance in a positive way: appearance variety and resolution distribution of the training dataset, resolution matching between the gallery and probe images, and the amount of information included in the probe images. By leveraging this information, we have utilized deep face models trained on MS-Celeb-1M and fine-tuned on VGGFace2 dataset and achieved state-of-the-art accuracies on the SCFace and ICB-RW benchmarks, even without using any training data from the datasets of these benchmarks.



### Lifelong GAN: Continual Learning for Conditional Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1907.10107v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10107v2)
- **Published**: 2019-07-23 19:25:15+00:00
- **Updated**: 2019-08-22 07:44:02+00:00
- **Authors**: Mengyao Zhai, Lei Chen, Fred Tung, Jiawei He, Megha Nawhal, Greg Mori
- **Comment**: accepted to ICCV 2019
- **Journal**: None
- **Summary**: Lifelong learning is challenging for deep neural networks due to their susceptibility to catastrophic forgetting. Catastrophic forgetting occurs when a trained network is not able to maintain its ability to accomplish previously learned tasks when it is trained to perform new tasks. We study the problem of lifelong learning for generative models, extending a trained network to new conditional generation tasks without forgetting previous tasks, while assuming access to the training data for the current task only. In contrast to state-of-the-art memory replay based approaches which are limited to label-conditioned image generation tasks, a more generic framework for continual learning of generative models under different conditional image generation settings is proposed in this paper. Lifelong GAN employs knowledge distillation to transfer learned knowledge from previous networks to the new network. This makes it possible to perform image-conditioned generation tasks in a lifelong learning setting. We validate Lifelong GAN for both image-conditioned and label-conditioned generation tasks, and provide qualitative and quantitative results to show the generality and effectiveness of our method.



### Blind Deblurring using Deep Learning: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1907.10128v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.10128v1)
- **Published**: 2019-07-23 21:04:29+00:00
- **Updated**: 2019-07-23 21:04:29+00:00
- **Authors**: Siddhant Sahu, Manoj Kumar Lenka, Pankaj Kumar Sa
- **Comment**: 9 pages, 10 figures
- **Journal**: None
- **Summary**: We inspect all the deep learning based solutions and provide holistic understanding of various architectures that have evolved over the past few years to solve blind deblurring. The introductory work used deep learning to estimate some features of the blur kernel and then moved onto predicting the blur kernel entirely, which converts the problem into non-blind deblurring. The recent state of the art techniques are end to end, i.e., they don't estimate the blur kernel rather try to estimate the latent sharp image directly from the blurred image. The benchmarking PSNR and SSIM values on standard datasets of GOPRO and Kohler using various architectures are also provided.



### Domain specific cues improve robustness of deep learning based segmentation of ct volumes
- **Arxiv ID**: http://arxiv.org/abs/1907.10132v3
- **DOI**: 10.1038/s41598-020-67544-y
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.10132v3)
- **Published**: 2019-07-23 21:11:22+00:00
- **Updated**: 2020-01-30 09:26:58+00:00
- **Authors**: Marie Kloenne, Sebastian Niehaus, Leonie Lampe, Alberto Merola, Janis Reinelt, Ingo Roeder, Nico Scherf
- **Comment**: None
- **Journal**: Scientific Reports 10, 10712 (2020)
- **Summary**: Machine Learning has considerably improved medical image analysis in the past years. Although data-driven approaches are intrinsically adaptive and thus, generic, they often do not perform the same way on data from different imaging modalities. In particular Computed tomography (CT) data poses many challenges to medical image segmentation based on convolutional neural networks (CNNs), mostly due to the broad dynamic range of intensities and the varying number of recorded slices of CT volumes. In this paper, we address these issues with a framework that combines domain-specific data preprocessing and augmentation with state-of-the-art CNN architectures. The focus is not limited to optimise the score, but also to stabilise the prediction performance since this is a mandatory requirement for use in automated and semi-automated workflows in the clinical environment.   The framework is validated with an architecture comparison to show CNN architecture-independent effects of our framework functionality. We compare a modified U-Net and a modified Mixed-Scale Dense Network (MS-D Net) to compare dilated convolutions for parallel multi-scale processing to the U-Net approach based on traditional scaling operations. Finally, we propose an ensemble model combining the strengths of different individual methods. The framework performs well on a range of tasks such as liver and kidney segmentation, without significant differences in prediction performance on strongly differing volume sizes and varying slice thickness. Thus our framework is an essential step towards performing robust segmentation of unknown real-world samples.



### Reflective-AR Display: An Interaction Methodology for Virtual-Real Alignment in Medical Robotics
- **Arxiv ID**: http://arxiv.org/abs/1907.10138v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.10138v2)
- **Published**: 2019-07-23 21:27:44+00:00
- **Updated**: 2020-03-04 19:16:19+00:00
- **Authors**: Javad Fotouhi, Tianyu Song, Arian Mehrfard, Giacomo Taylor, Qiaochu Wang, Fengfang Xian, Alejandro Martin-Gomez, Bernhard Fuerst, Mehran Armand, Mathias Unberath, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: Robot-assisted minimally invasive surgery has shown to improve patient outcomes, as well as reduce complications and recovery time for several clinical applications. While increasingly configurable robotic arms can maximize reach and avoid collisions in cluttered environments, positioning them appropriately during surgery is complicated because safety regulations prevent automatic driving. We propose a head-mounted display (HMD) based augmented reality (AR) system designed to guide optimal surgical arm set up. The staff equipped with HMD aligns the robot with its planned virtual counterpart. In this user-centric setting, the main challenge is the perspective ambiguities hindering such collaborative robotic solution. To overcome this challenge, we introduce a novel registration concept for intuitive alignment of AR content to its physical counterpart by providing a multi-view AR experience via reflective-AR displays that simultaneously show the augmentations from multiple viewpoints. Using this system, users can visualize different perspectives while actively adjusting the pose to determine the registration transformation that most closely superimposes the virtual onto the real. The experimental results demonstrate improvement in the interactive alignment of a virtual and real robot when using a reflective-AR display. We also present measurements from configuring a robotic manipulator in a simulated trocar placement surgery using the AR guidance methodology.



### Conf-Net: Toward High-Confidence Dense 3D Point-Cloud with Error-Map Prediction
- **Arxiv ID**: http://arxiv.org/abs/1907.10148v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.10148v3)
- **Published**: 2019-07-23 21:41:54+00:00
- **Updated**: 2019-09-19 18:52:58+00:00
- **Authors**: Hamid Hekmatian, Jingfu Jin, Samir Al-Stouhi
- **Comment**: None
- **Journal**: None
- **Summary**: This work proposes a method for depth completion of sparse LiDAR data using a convolutional neural network which can be used to generate semi-dense depth maps and "almost" full 3D point-clouds with significantly lower root mean squared error (RMSE) over state-of-the-art methods. We add an "Error Prediction" unit to our network and present a novel and simple end-to-end method that learns to predict an error-map of depth regression task. An "almost" dense high-confidence/low-variance point-cloud is more valuable for safety-critical applications specifically real-world autonomous driving than a full point-cloud with high error rate and high error variance. Using our predicted error-map, we demonstrate that by up-filling a LiDAR point cloud from 18,000 points to 285,000 points, versus 300,000 points for full depth, we can reduce the RMSE error from 1004 to 399. This error is approximately 60% less than the state-of-the-art and 50% less than the state-of-the-art with RGB guidance (we did not use RGB guidance in our algorithm). In addition to analyzing our results on Kitti depth completion dataset, we also demonstrate the ability of our proposed method to extend to new tasks by deploying our "Error Prediction" unit to improve upon the state-of-the-art for monocular depth estimation. Codes and demo videos are available at http://github.com/hekmak/Conf-net.



### DR Loss: Improving Object Detection by Distributional Ranking
- **Arxiv ID**: http://arxiv.org/abs/1907.10156v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10156v3)
- **Published**: 2019-07-23 22:14:38+00:00
- **Updated**: 2020-04-13 17:14:36+00:00
- **Authors**: Qi Qian, Lei Chen, Hao Li, Rong Jin
- **Comment**: accepted by CVPR'20
- **Journal**: None
- **Summary**: Most of object detection algorithms can be categorized into two classes: two-stage detectors and one-stage detectors. Recently, many efforts have been devoted to one-stage detectors for the simple yet effective architecture. Different from two-stage detectors, one-stage detectors aim to identify foreground objects from all candidates in a single stage. This architecture is efficient but can suffer from the imbalance issue with respect to two aspects: the inter-class imbalance between the number of candidates from foreground and background classes and the intra-class imbalance in the hardness of background candidates, where only a few candidates are hard to be identified. In this work, we propose a novel distributional ranking (DR) loss to handle the challenge. For each image, we convert the classification problem to a ranking problem, which considers pairs of candidates within the image, to address the inter-class imbalance problem. Then, we push the distributions of confidence scores for foreground and background towards the decision boundary. After that, we optimize the rank of the expectations of derived distributions in lieu of original pairs. Our method not only mitigates the intra-class imbalance issue in background candidates but also improves the efficiency for the ranking algorithm. By merely replacing the focal loss in RetinaNet with the developed DR loss and applying ResNet-101 as the backbone, mAP of the single-scale test on COCO can be improved from 39.1% to 41.7% without bells and whistles, which demonstrates the effectiveness of the proposed loss function. Code is available at \url{https://github.com/idstcv/DR_loss}.



### Cap2Det: Learning to Amplify Weak Caption Supervision for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1907.10164v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10164v3)
- **Published**: 2019-07-23 22:39:37+00:00
- **Updated**: 2019-08-16 08:35:36+00:00
- **Authors**: Keren Ye, Mingda Zhang, Adriana Kovashka, Wei Li, Danfeng Qin, Jesse Berent
- **Comment**: To appear in ICCV 2019
- **Journal**: None
- **Summary**: Learning to localize and name object instances is a fundamental problem in vision, but state-of-the-art approaches rely on expensive bounding box supervision. While weakly supervised detection (WSOD) methods relax the need for boxes to that of image-level annotations, even cheaper supervision is naturally available in the form of unstructured textual descriptions that users may freely provide when uploading image content. However, straightforward approaches to using such data for WSOD wastefully discard captions that do not exactly match object names. Instead, we show how to squeeze the most information out of these captions by training a text-only classifier that generalizes beyond dataset boundaries. Our discovery provides an opportunity for learning detection models from noisy but more abundant and freely-available caption data. We also validate our model on three classic object detection benchmarks and achieve state-of-the-art WSOD performance. Our code is available at https://github.com/yekeren/Cap2Det.



### Enhancing Adversarial Example Transferability with an Intermediate Level Attack
- **Arxiv ID**: http://arxiv.org/abs/1907.10823v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.10823v3)
- **Published**: 2019-07-23 23:37:15+00:00
- **Updated**: 2020-02-27 22:43:49+00:00
- **Authors**: Qian Huang, Isay Katsman, Horace He, Zeqi Gu, Serge Belongie, Ser-Nam Lim
- **Comment**: ICCV 2019 camera-ready. Imagenet results are updated after fixing the
  normalization. arXiv admin note: text overlap with arXiv:1811.08458
- **Journal**: None
- **Summary**: Neural networks are vulnerable to adversarial examples, malicious inputs crafted to fool trained models. Adversarial examples often exhibit black-box transfer, meaning that adversarial examples for one model can fool another model. However, adversarial examples are typically overfit to exploit the particular architecture and feature representation of a source model, resulting in sub-optimal black-box transfer attacks to other target models. We introduce the Intermediate Level Attack (ILA), which attempts to fine-tune an existing adversarial example for greater black-box transferability by increasing its perturbation on a pre-specified layer of the source model, improving upon state-of-the-art methods. We show that we can select a layer of the source model to perturb without any knowledge of the target models while achieving high transferability. Additionally, we provide some explanatory insights regarding our method and the effect of optimizing for adversarial examples using intermediate feature maps. Our code is available at https://github.com/CUVL/Intermediate-Level-Attack.



### Analyzing the Variety Loss in the Context of Probabilistic Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/1907.10178v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.10178v1)
- **Published**: 2019-07-23 23:56:02+00:00
- **Updated**: 2019-07-23 23:56:02+00:00
- **Authors**: Luca Anthony Thiede, Pratik Prabhanjan Brahma
- **Comment**: Accepted for publication at ICCV 2019
- **Journal**: None
- **Summary**: Trajectory or behavior prediction of traffic agents is an important component of autonomous driving and robot planning in general. It can be framed as a probabilistic future sequence generation problem and recent literature has studied the applicability of generative models in this context. The variety or Minimum over N (MoN) loss, which tries to minimize the error between the ground truth and the closest of N output predictions, has been used in these recent learning models to improve the diversity of predictions. In this work, we present a proof to show that the MoN loss does not lead to the ground truth probability density function, but approximately to its square root instead. We validate this finding with extensive experiments on both simulated toy as well as real world datasets. We also propose multiple solutions to compensate for the dilation to show improvement of log likelihood of the ground truth samples in the corrected probability density function.



