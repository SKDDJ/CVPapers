# Arxiv Papers in cs.CV on 2019-07-06
### Human detection of machine manipulated media
- **Arxiv ID**: http://arxiv.org/abs/1907.05276v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.05276v2)
- **Published**: 2019-07-06 02:52:42+00:00
- **Updated**: 2019-11-08 15:22:26+00:00
- **Authors**: Matthew Groh, Ziv Epstein, Nick Obradovich, Manuel Cebrian, Iyad Rahwan
- **Comment**: None
- **Journal**: Communications of the ACM 64, no. 10 (2021): 40-47
- **Summary**: Recent advances in neural networks for content generation enable artificial intelligence (AI) models to generate high-quality media manipulations. Here we report on a randomized experiment designed to study the effect of exposure to media manipulations on over 15,000 individuals' ability to discern machine-manipulated media. We engineer a neural network to plausibly and automatically remove objects from images, and we deploy this neural network online with a randomized experiment where participants can guess which image out of a pair of images has been manipulated. The system provides participants feedback on the accuracy of each guess. In the experiment, we randomize the order in which images are presented, allowing causal identification of the learning curve surrounding participants' ability to detect fake content. We find sizable and robust evidence that individuals learn to detect fake content through exposure to manipulated media when provided iterative feedback on their detection attempts. Over a succession of only ten images, participants increase their rating accuracy by over ten percentage points. Our study provides initial evidence that human ability to detect fake, machine-generated content may increase alongside the prevalence of such media online.



### Improving the resolution of microscope by deconvolution after dense scan
- **Arxiv ID**: http://arxiv.org/abs/1907.05275v3
- **DOI**: 10.5281/zenodo.3353772
- **Categories**: **eess.IV**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1907.05275v3)
- **Published**: 2019-07-06 03:35:50+00:00
- **Updated**: 2019-11-19 03:05:19+00:00
- **Authors**: Yaohua Xie
- **Comment**: This work has been patented, thereby is not open-source
- **Journal**: None
- **Summary**: Super-resolution microscopes (such as STED) illuminate samples with a tiny spot, and achieve very high resolution. But structures smaller than the spot cannot be resolved in this way. Therefore, we propose a technique to solve this problem. It is termed "Deconvolution after Dense Scan (DDS)". First, a preprocessing stage is introduced to eliminate the optical uncertainty of the peripheral areas around the sample's ROI (Region of Interest). Then, the ROI is scanned densely together with its peripheral areas. Finally, the high resolution image is recovered by deconvolution. The proposed technique does not need to modify the apparatus much, and is mainly performed by algorithm. Simulation experiments show that the technique can further improve the resolution of super-resolution microscopes.



### Deep Learning for Fine-Grained Image Analysis: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1907.03069v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.03069v1)
- **Published**: 2019-07-06 03:55:02+00:00
- **Updated**: 2019-07-06 03:55:02+00:00
- **Authors**: Xiu-Shen Wei, Jianxin Wu, Quan Cui
- **Comment**: Project page:
  http://www.weixiushen.com/project/Awesome_FGIA/Awesome_FGIA.html. arXiv admin
  note: text overlap with arXiv:1902.06068 by other authors
- **Journal**: None
- **Summary**: Computer vision (CV) is the process of using machines to understand and analyze imagery, which is an integral branch of artificial intelligence. Among various research areas of CV, fine-grained image analysis (FGIA) is a longstanding and fundamental problem, and has become ubiquitous in diverse real-world applications. The task of FGIA targets analyzing visual objects from subordinate categories, \eg, species of birds or models of cars. The small inter-class variations and the large intra-class variations caused by the fine-grained nature makes it a challenging problem. During the booming of deep learning, recent years have witnessed remarkable progress of FGIA using deep learning techniques. In this paper, we aim to give a survey on recent advances of deep learning based FGIA techniques in a systematic way. Specifically, we organize the existing studies of FGIA techniques into three major categories: fine-grained image recognition, fine-grained image retrieval and fine-grained image generation. In addition, we also cover some other important issues of FGIA, such as publicly available benchmark datasets and its related domain specific applications. Finally, we conclude this survey by highlighting several directions and open problems which need be further explored by the community in the future.



### AMD Severity Prediction And Explainability Using Image Registration And Deep Embedded Clustering
- **Arxiv ID**: http://arxiv.org/abs/1907.03075v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.03075v1)
- **Published**: 2019-07-06 04:23:50+00:00
- **Updated**: 2019-07-06 04:23:50+00:00
- **Authors**: Dwarikanath Mahapatra
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method to predict severity of age related macular degeneration (AMD) from input optical coherence tomography (OCT) images. Although there is no standard clinical severity scale for AMD, we leverage deep learning (DL) based image registration and clustering methods to identify diseased cases and predict their severity. Experiments demonstrate our approach's disease classification performance matches state of the art methods. The predicted disease severity performs well on previously unseen data. Registration output provides better explainability than class activation maps regarding label and severity decisions



### Generative Counterfactual Introspection for Explainable Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.03077v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.03077v1)
- **Published**: 2019-07-06 04:30:13+00:00
- **Updated**: 2019-07-06 04:30:13+00:00
- **Authors**: Shusen Liu, Bhavya Kailkhura, Donald Loveland, Yong Han
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose an introspection technique for deep neural networks that relies on a generative model to instigate salient editing of the input image for model interpretation. Such modification provides the fundamental interventional operation that allows us to obtain answers to counterfactual inquiries, i.e., what meaningful change can be made to the input image in order to alter the prediction. We demonstrate how to reveal interesting properties of the given classifiers by utilizing the proposed introspection approach on both the MNIST and the CelebA dataset.



### Affine Disentangled GAN for Interpretable and Robust AV Perception
- **Arxiv ID**: http://arxiv.org/abs/1907.05274v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.05274v1)
- **Published**: 2019-07-06 04:53:49+00:00
- **Updated**: 2019-07-06 04:53:49+00:00
- **Authors**: Letao Liu, Martin Saerbeck, Justin Dauwels
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous vehicles (AV) have progressed rapidly with the advancements in computer vision algorithms. The deep convolutional neural network as the main contributor to this advancement has boosted the classification accuracy dramatically. However, the discovery of adversarial examples reveals the generalization gap between dataset and the real world. Furthermore, affine transformations may also confuse computer vision based object detectors. The degradation of the perception system is undesirable for safety critical systems such as autonomous vehicles. In this paper, a deep learning system is proposed: Affine Disentangled GAN (ADIS-GAN), which is robust against affine transformations and adversarial attacks. It is demonstrated that conventional data augmentation for affine transformation and adversarial attacks are orthogonal, while ADIS-GAN can handle both attacks at the same time. Useful information such as image rotation angle and scaling factor are also generated in ADIS-GAN. On MNIST dataset, ADIS-GAN can achieve over 98 percent classification accuracy within 30 degrees rotation, and over 90 percent classification accuracy against FGSM and PGD adversarial attack.



### Task-Oriented Convex Bilevel Optimization with Latent Feasibility
- **Arxiv ID**: http://arxiv.org/abs/1907.03083v2
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1907.03083v2)
- **Published**: 2019-07-06 06:06:28+00:00
- **Updated**: 2021-12-28 01:38:31+00:00
- **Authors**: Risheng Liu, Long Ma, Xiaoming Yuan, Shangzhi Zeng, Jin Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper firstly proposes a convex bilevel optimization paradigm to formulate and optimize popular learning and vision problems in real-world scenarios. Different from conventional approaches, which directly design their iteration schemes based on given problem formulation, we introduce a task-oriented energy as our latent constraint which integrates richer task information. By explicitly re-characterizing the feasibility, we establish an efficient and flexible algorithmic framework to tackle convex models with both shrunken solution space and powerful auxiliary (based on domain knowledge and data distribution of the task). In theory, we present the convergence analysis of our latent feasibility re-characterization based numerical strategy. We also analyze the stability of the theoretical convergence under computational error perturbation. Extensive numerical experiments are conducted to verify our theoretical findings and evaluate the practical performance of our method on different applications.



### SAN: Scale-Aware Network for Semantic Segmentation of High-Resolution Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/1907.03089v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.03089v1)
- **Published**: 2019-07-06 07:05:13+00:00
- **Updated**: 2019-07-06 07:05:13+00:00
- **Authors**: Jingbo Lin, Weipeng Jing, Houbing Song
- **Comment**: 5 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: High-resolution aerial images have a wide range of applications, such as military exploration, and urban planning. Semantic segmentation is a fundamental method extensively used in the analysis of high-resolution aerial images. However, the ground objects in high-resolution aerial images have the characteristics of inconsistent scales, and this feature usually leads to unexpected predictions. To tackle this issue, we propose a novel scale-aware module (SAM). In SAM, we employ the re-sampling method aimed to make pixels adjust their positions to fit the ground objects with different scales, and it implicitly introduces spatial attention by employing a re-sampling map as the weighted map. As a result, the network with the proposed module named scale-aware network (SANet) has a stronger ability to distinguish the ground objects with inconsistent scale. Other than this, our proposed modules can easily embed in most of the existing network to improve their performance. We evaluate our modules on the International Society for Photogrammetry and Remote Sensing Vaihingen Dataset, and the experimental results and comprehensive analysis demonstrate the effectiveness of our proposed module.



### Regularizing linear inverse problems with convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1907.03100v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IT, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.03100v1)
- **Published**: 2019-07-06 09:30:51+00:00
- **Updated**: 2019-07-06 09:30:51+00:00
- **Authors**: Reinhard Heckel
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks trained on large datsets have emerged as an intriguing alternative for compressing images and solving inverse problems such as denoising and compressive sensing. However, it has only recently been realized that even without training, convolutional networks can function as concise image models, and thus regularize inverse problems. In this paper, we provide further evidence for this finding by studying variations of convolutional neural networks that map few weight parameters to an image. The networks we consider only consist of convolutional operations, with either fixed or parameterized filters followed by ReLU non-linearities. We demonstrate that with both fixed and parameterized convolutional filters those networks enable representing images with few coefficients. What is more, the underparameterization enables regularization of inverse problems, in particular recovering an image from few observations. We show that, similar to standard compressive sensing guarantees, on the order of the number of model parameters many measurements suffice for recovering an image from compressive measurements. Finally, we demonstrate that signal recovery with a un-trained convolutional network outperforms standard l1 and total variation minimization for magnetic resonance imaging (MRI).



### Fast Universal Style Transfer for Artistic and Photorealistic Rendering
- **Arxiv ID**: http://arxiv.org/abs/1907.03118v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.03118v1)
- **Published**: 2019-07-06 11:57:40+00:00
- **Updated**: 2019-07-06 11:57:40+00:00
- **Authors**: Jie An, Haoyi Xiong, Jiebo Luo, Jun Huan, Jinwen Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Universal style transfer is an image editing task that renders an input content image using the visual style of arbitrary reference images, including both artistic and photorealistic stylization. Given a pair of images as the source of content and the reference of style, existing solutions usually first train an auto-encoder (AE) to reconstruct the image using deep features and then embeds pre-defined style transfer modules into the AE reconstruction procedure to transfer the style of the reconstructed image through modifying the deep features. While existing methods typically need multiple rounds of time-consuming AE reconstruction for better stylization, our work intends to design novel neural network architectures on top of AE for fast style transfer with fewer artifacts and distortions all in one pass of end-to-end inference. To this end, we propose two network architectures named ArtNet and PhotoNet to improve artistic and photo-realistic stylization, respectively. Extensive experiments demonstrate that ArtNet generates images with fewer artifacts and distortions against the state-of-the-art artistic transfer algorithms, while PhotoNet improves the photorealistic stylization results by creating sharp images faithfully preserving rich details of the input content. Moreover, ArtNet and PhotoNet can achieve 3X to 100X speed-up over the state-of-the-art algorithms, which is a major advantage for large content images.



### Revisiting Metric Learning for Few-Shot Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1907.03123v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.03123v2)
- **Published**: 2019-07-06 12:19:01+00:00
- **Updated**: 2020-04-16 04:24:48+00:00
- **Authors**: Xiaomeng Li, Lequan Yu, Chi-Wing Fu, Meng Fang, Pheng-Ann Heng
- **Comment**: Accept at Neurocomputing
- **Journal**: None
- **Summary**: The goal of few-shot learning is to recognize new visual concepts with just a few amount of labeled samples in each class. Recent effective metric-based few-shot approaches employ neural networks to learn a feature similarity comparison between query and support examples. However, the importance of feature embedding, i.e., exploring the relationship among training samples, is neglected. In this work, we present a simple yet powerful baseline for few-shot classification by emphasizing the importance of feature embedding. Specifically, we revisit the classical triplet network from deep metric learning, and extend it into a deep K-tuplet network for few-shot learning, utilizing the relationship among the input samples to learn a general representation learning via episode-training. Once trained, our network is able to extract discriminative features for unseen novel categories and can be seamlessly incorporated with a non-linear distance metric function to facilitate the few-shot classification. Our result on the miniImageNet benchmark outperforms other metric-based few-shot classification methods. More importantly, when evaluated on completely different datasets (Caltech-101, CUB-200, Stanford Dogs and Cars) using the model trained with miniImageNet, our method significantly outperforms prior methods, demonstrating its superior capability to generalize to unseen classes.



### Multi-level Wavelet Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.03128v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.03128v1)
- **Published**: 2019-07-06 14:19:03+00:00
- **Updated**: 2019-07-06 14:19:03+00:00
- **Authors**: Pengju Liu, Hongzhi Zhang, Wei Lian, Wangmeng Zuo
- **Comment**: None
- **Journal**: None
- **Summary**: In computer vision, convolutional networks (CNNs) often adopts pooling to enlarge receptive field which has the advantage of low computational complexity. However, pooling can cause information loss and thus is detrimental to further operations such as features extraction and analysis. Recently, dilated filter has been proposed to trade off between receptive field size and efficiency. But the accompanying gridding effect can cause a sparse sampling of input images with checkerboard patterns. To address this problem, in this paper, we propose a novel multi-level wavelet CNN (MWCNN) model to achieve better trade-off between receptive field size and computational efficiency. The core idea is to embed wavelet transform into CNN architecture to reduce the resolution of feature maps while at the same time, increasing receptive field. Specifically, MWCNN for image restoration is based on U-Net architecture, and inverse wavelet transform (IWT) is deployed to reconstruct the high resolution (HR) feature maps. The proposed MWCNN can also be viewed as an improvement of dilated filter and a generalization of average pooling, and can be applied to not only image restoration tasks, but also any CNNs requiring a pooling operation. The experimental results demonstrate effectiveness of the proposed MWCNN for tasks such as image denoising, single image super-resolution, JPEG image artifacts removal and object classification.



### Accurate Congenital Heart Disease Model Generation for 3D Printing
- **Arxiv ID**: http://arxiv.org/abs/1907.05273v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.05273v2)
- **Published**: 2019-07-06 15:15:06+00:00
- **Updated**: 2019-07-12 03:11:44+00:00
- **Authors**: Xiaowei Xu, Tianchen Wang, Dewen Zeng, Yiyu Shi, Qianjun Jia, Haiyun Yuan, Meiping Huang, Jian Zhuang
- **Comment**: 6 figures, 2 tables, accepted by the IEEE International Workshop on
  Signal Processing Systems
- **Journal**: None
- **Summary**: 3D printing has been widely adopted for clinical decision making and interventional planning of Congenital heart disease (CHD), while whole heart and great vessel segmentation is the most significant but time-consuming step in the model generation for 3D printing. While various automatic whole heart and great vessel segmentation frameworks have been developed in the literature, they are ineffective when applied to medical images in CHD, which have significant variations in heart structure and great vessel connections. To address the challenge, we leverage the power of deep learning in processing regular structures and that of graph algorithms in dealing with large variations and propose a framework that combines both for whole heart and great vessel segmentation in CHD. Particularly, we first use deep learning to segment the four chambers and myocardium followed by the blood pool, where variations are usually small. We then extract the connection information and apply graph matching to determine the categories of all the vessels. Experimental results using 683D CT images covering 14 types of CHD show that our method can increase Dice score by 11.9% on average compared with the state-of-the-art whole heart and great vessel segmentation method in normal anatomy. The segmentation results are also printed out using 3D printers for validation.



### AutoCompress: An Automatic DNN Structured Pruning Framework for Ultra-High Compression Rates
- **Arxiv ID**: http://arxiv.org/abs/1907.03141v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.03141v2)
- **Published**: 2019-07-06 15:40:02+00:00
- **Updated**: 2019-09-11 12:15:38+00:00
- **Authors**: Ning Liu, Xiaolong Ma, Zhiyuan Xu, Yanzhi Wang, Jian Tang, Jieping Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Structured weight pruning is a representative model compression technique of DNNs to reduce the storage and computation requirements and accelerate inference. An automatic hyperparameter determination process is necessary due to the large number of flexible hyperparameters. This work proposes AutoCompress, an automatic structured pruning framework with the following key performance improvements: (i) effectively incorporate the combination of structured pruning schemes in the automatic process; (ii) adopt the state-of-art ADMM-based structured weight pruning as the core algorithm, and propose an innovative additional purification step for further weight reduction without accuracy loss; and (iii) develop effective heuristic search method enhanced by experience-based guided search, replacing the prior deep reinforcement learning technique which has underlying incompatibility with the target pruning problem. Extensive experiments on CIFAR-10 and ImageNet datasets demonstrate that AutoCompress is the key to achieve ultra-high pruning rates on the number of weights and FLOPs that cannot be achieved before. As an example, AutoCompress outperforms the prior work on automatic model compression by up to 33x in pruning rate (120x reduction in the actual parameter count) under the same accuracy. Significant inference speedup has been observed from the AutoCompress framework on actual measurements on smartphone. We release all models of this work at anonymous link: http://bit.ly/2VZ63dS.



### Unsupervised cycle-consistent deformation for shape matching
- **Arxiv ID**: http://arxiv.org/abs/1907.03165v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.03165v1)
- **Published**: 2019-07-06 18:43:47+00:00
- **Updated**: 2019-07-06 18:43:47+00:00
- **Authors**: Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, Mathieu Aubry
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a self-supervised approach to deep surface deformation. Given a pair of shapes, our algorithm directly predicts a parametric transformation from one shape to the other respecting correspondences. Our insight is to use cycle-consistency to define a notion of good correspondences in groups of objects and use it as a supervisory signal to train our network. Our method does not rely on a template, assume near isometric deformations or rely on point-correspondence supervision. We demonstrate the efficacy of our approach by using it to transfer segmentation across shapes. We show, on Shapenet, that our approach is competitive with comparable state-of-the-art methods when annotated training data is readily available, but outperforms them by a large margin in the few-shot segmentation scenario.



### Multimodal Fusion with Deep Neural Networks for Audio-Video Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.03196v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.03196v1)
- **Published**: 2019-07-06 22:12:42+00:00
- **Updated**: 2019-07-06 22:12:42+00:00
- **Authors**: Juan D. S. Ortega, Mohammed Senoussaoui, Eric Granger, Marco Pedersoli, Patrick Cardinal, Alessandro L. Koerich
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel deep neural network (DNN) for multimodal fusion of audio, video and text modalities for emotion recognition. The proposed DNN architecture has independent and shared layers which aim to learn the representation for each modality, as well as the best combined representation to achieve the best prediction. Experimental results on the AVEC Sentiment Analysis in the Wild dataset indicate that the proposed DNN can achieve a higher level of Concordance Correlation Coefficient (CCC) than other state-of-the-art systems that perform early fusion of modalities at feature-level (i.e., concatenation) and late fusion at score-level (i.e., weighted average) fusion. The proposed DNN has achieved CCCs of 0.606, 0.534, and 0.170 on the development partition of the dataset for predicting arousal, valence and liking, respectively.



