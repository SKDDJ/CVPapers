# Arxiv Papers in cs.CV on 2019-07-29
### On the Realization and Analysis of Circular Harmonic Transforms for Feature Detection
- **Arxiv ID**: http://arxiv.org/abs/1907.12165v5
- **DOI**: 10.1007/s11554-020-01040-4
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.12165v5)
- **Published**: 2019-07-29 00:20:17+00:00
- **Updated**: 2020-11-06 10:25:09+00:00
- **Authors**: Hugh L Kennedy
- **Comment**: A new section on parallel software implementation (MATLAB, C++ and
  CUDA) was added to this draft version. Manuscript was then accepted for
  publication in Journal of Real-Time Image Processing, special issue on
  Real-Time Statistical Image and Video Processing for Remote Sensing and
  Surveillance Applications
- **Journal**: None
- **Summary**: Circular-harmonic spectra are a compact representation of local image features in two dimensions. It is well known that the computational complexity of such transforms is greatly reduced when polar separability is exploited in steerable filter-banks. Further simplifications are possible when Cartesian separability is incorporated using the radial apodization (i.e. weight, window, or taper) described here, as a consequence of the Laguerre/Hermite correspondence over polar/Cartesian coordinates. The chosen form also mitigates undesirable discretization artefacts due to angular aliasing. The possible utility of circular-harmonic spectra for the description of simple features is illustrated using real data from an airborne electro-optic sensor. The spectrum is deployed in a test-statistic to detect and characterize corners of arbitrary angle and orientation (i.e. wedges). The test-statistic considers uncertainty due to finite sampling and clutter/noise.



### End-to-End Learning Deep CRF models for Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1907.12176v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12176v1)
- **Published**: 2019-07-29 02:27:18+00:00
- **Updated**: 2019-07-29 02:27:18+00:00
- **Authors**: Jun Xiang, Ma Chao, Guohan Xu, Jianhua Hou
- **Comment**: None
- **Journal**: None
- **Summary**: Existing deep multi-object tracking (MOT) approaches first learn a deep representation to describe target objects and then associate detection results by optimizing a linear assignment problem. Despite demonstrated successes, it is challenging to discriminate target objects under mutual occlusion or to reduce identity switches in crowded scenes. In this paper, we propose learning deep conditional random field (CRF) networks, aiming to model the assignment costs as unary potentials and the long-term dependencies among detection results as pairwise potentials. Specifically, we use a bidirectional long short-term memory (LSTM) network to encode the long-term dependencies. We pose the CRF inference as a recurrent neural network learning process using the standard gradient descent algorithm, where unary and pairwise potentials are jointly optimized in an end-to-end manner. Extensive experimental results on the challenging MOT datasets including MOT-2015 and MOT-2016, demonstrate that our approach achieves the state of the art performances in comparison with published works on both benchmarks.



### Solar Image Restoration with the Cycle-GAN Based on Multi-Fractal Properties of Texture Features
- **Arxiv ID**: http://arxiv.org/abs/1907.12192v2
- **DOI**: 10.3847/2041-8213/ab365f
- **Categories**: **astro-ph.IM**, astro-ph.SR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.12192v2)
- **Published**: 2019-07-29 03:09:32+00:00
- **Updated**: 2019-08-11 08:33:44+00:00
- **Authors**: Peng Jia, Yi Huang, Bojun Cai, Dongmei Cai
- **Comment**: Accepted by APJ Letters
- **Journal**: None
- **Summary**: Texture is one of the most obvious characteristics in solar images and it is normally described by texture features. Because textures from solar images of the same wavelength are similar, we assume texture features of solar images are multi-fractals. Based on this assumption, we propose a pure data-based image restoration method: with several high resolution solar images as references, we use the Cycle-Consistent Adversarial Network to restore burred images of the same steady physical process, in the same wavelength obtained by the same telescope. We test our method with simulated and real observation data and find that our method can improve the spatial resolution of solar images, without loss of any frames. Because our method does not need paired training set or additional instruments, it can be used as a post-processing method for solar images obtained by either seeing limited telescopes or telescopes with ground layer adaptive optic system.



### ChaLearn Looking at People: IsoGD and ConGD Large-scale RGB-D Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.12193v1
- **DOI**: 10.1109/TCYB.2020.3012092
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12193v1)
- **Published**: 2019-07-29 03:09:40+00:00
- **Updated**: 2019-07-29 03:09:40+00:00
- **Authors**: Jun Wan, Chi Lin, Longyin Wen, Yunan Li, Qiguang Miao, Sergio Escalera, Gholamreza Anbarjafari, Isabelle Guyon, Guodong Guo, Stan Z. Li
- **Comment**: 14 pages, 8 figures, 6 tables
- **Journal**: IEEE Transactions on Cybernetics 2020
- **Summary**: The ChaLearn large-scale gesture recognition challenge has been run twice in two workshops in conjunction with the International Conference on Pattern Recognition (ICPR) 2016 and International Conference on Computer Vision (ICCV) 2017, attracting more than $200$ teams round the world. This challenge has two tracks, focusing on isolated and continuous gesture recognition, respectively. This paper describes the creation of both benchmark datasets and analyzes the advances in large-scale gesture recognition based on these two datasets. We discuss the challenges of collecting large-scale ground-truth annotations of gesture recognition, and provide a detailed analysis of the current state-of-the-art methods for large-scale isolated and continuous gesture recognition based on RGB-D video sequences. In addition to recognition rate and mean jaccard index (MJI) as evaluation metrics used in our previous challenges, we also introduce the corrected segmentation rate (CSR) metric to evaluate the performance of temporal segmentation for continuous gesture recognition. Furthermore, we propose a bidirectional long short-term memory (Bi-LSTM) baseline method, determining the video division points based on the skeleton points extracted by convolutional pose machine (CPM). Experiments demonstrate that the proposed Bi-LSTM outperforms the state-of-the-art methods with an absolute improvement of $8.1\%$ (from $0.8917$ to $0.9639$) of CSR.



### Seeing Things in Random-Dot Videos
- **Arxiv ID**: http://arxiv.org/abs/1907.12195v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12195v2)
- **Published**: 2019-07-29 03:12:28+00:00
- **Updated**: 2020-01-29 16:39:13+00:00
- **Authors**: Thomas Dag√®s, Michael Lindenbaum, Alfred M. Bruckstein
- **Comment**: Technical report ; Resubmission: Corrected typos. Rephrased some
  sentences
- **Journal**: None
- **Summary**: Humans possess an intricate and powerful visual system in order to perceive and understand the environing world. Human perception can effortlessly detect and correctly group features in visual data and can even interpret random-dot videos induced by imaging natural dynamic scenes with highly noisy sensors such as ultrasound imaging. Remarkably, this happens even if perception completely fails when the same information is presented frame by frame rather than in a video sequence. We study this property of surprising dynamic perception with the first goal of proposing a new detection and spatio-temporal grouping algorithm for such signals when, per frame, the information on objects is both random and sparse and embedded in random noise. The algorithm is based on the succession of temporal integration and spatial statistical tests of unlikeliness, the a contrario framework. The algorithm not only manages to handle such signals but the striking similarity in its performance to the perception by human observers, as witnessed by a series of psychophysical experiments on image and video data, leads us to see in it a simple computational Gestalt model of human perception with only two parameters: the time integration and the visual angle for candidate shapes to be detected.



### Enforcing geometric constraints of virtual normal for depth prediction
- **Arxiv ID**: http://arxiv.org/abs/1907.12209v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12209v2)
- **Published**: 2019-07-29 04:41:53+00:00
- **Updated**: 2019-08-01 06:15:30+00:00
- **Authors**: Wei Yin, Yifan Liu, Chunhua Shen, Youliang Yan
- **Comment**: Fixed typos. Appearing in Proc. Int. Conf. Computer Vision 2019. Code
  is available at: https://tinyurl.com/virtualnormal
- **Journal**: None
- **Summary**: Monocular depth prediction plays a crucial role in understanding 3D scene geometry. Although recent methods have achieved impressive progress in evaluation metrics such as the pixel-wise relative error, most methods neglect the geometric constraints in the 3D space. In this work, we show the importance of the high-order 3D geometric constraints for depth prediction. By designing a loss term that enforces one simple type of geometric constraints, namely, virtual normal directions determined by randomly sampled three points in the reconstructed 3D space, we can considerably improve the depth prediction accuracy. Significantly, the byproduct of this predicted depth being sufficiently accurate is that we are now able to recover good 3D structures of the scene such as the point cloud and surface normal directly from the depth, eliminating the necessity of training new sub-models as was previously done. Experiments on two benchmarks: NYU Depth-V2 and KITTI demonstrate the effectiveness of our method and state-of-the-art performance.



### Automatic Text Line Segmentation Directly in JPEG Compressed Document Images
- **Arxiv ID**: http://arxiv.org/abs/1907.12219v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.12219v1)
- **Published**: 2019-07-29 05:32:31+00:00
- **Updated**: 2019-07-29 05:32:31+00:00
- **Authors**: Bulla Rajesh, Mohammed Javed, P Nagabhushan
- **Comment**: Accepted in GCCE2019, Okinawa, Japan
- **Journal**: None
- **Summary**: JPEG is one of the popular image compression algorithms that provide efficient storage and transmission capabilities in consumer electronics, and hence it is the most preferred image format over the internet world. In the present digital and Big-data era, a huge volume of JPEG compressed document images are being archived and communicated through consumer electronics on daily basis. Though it is advantageous to have data in the compressed form on one side, however, on the other side processing with off-the-shelf methods becomes computationally expensive because it requires decompression and recompression operations. Therefore, it would be novel and efficient, if the compressed data are processed directly in their respective compressed domains of consumer electronics. In the present research paper, we propose to demonstrate this idea taking the case study of printed text line segmentation. Since, JPEG achieves compression by dividing the image into non overlapping 8x8 blocks in the pixel domain and using Discrete Cosine Transform (DCT); it is very likely that the partitioned 8x8 DCT blocks overlap the contents of two adjacent text-lines without leaving any clue for the line separator, thus making text-line segmentation a challenging problem. Two approaches of segmentation have been proposed here using the DC projection profile and AC coefficients of each 8x8 DCT block. The first approach is based on the strategy of partial decompression of selected DCT blocks, and the second approach is with intelligent analysis of F10 and F11 AC coefficients and without using any type of decompression. The proposed methods have been tested with variable font sizes, font style and spacing between lines, and a good performance is reported.



### Multi-Granularity Fusion Network for Proposal and Activity Localization: Submission to ActivityNet Challenge 2019 Task 1 and Task 2
- **Arxiv ID**: http://arxiv.org/abs/1907.12223v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12223v1)
- **Published**: 2019-07-29 06:10:51+00:00
- **Updated**: 2019-07-29 06:10:51+00:00
- **Authors**: Haisheng Su, Xu Zhao, Shuming Liu
- **Comment**: None
- **Journal**: None
- **Summary**: This technical report presents an overview of our solution used in the submission to ActivityNet Challenge 2019 Task 1 (\textbf{temporal action proposal generation}) and Task 2 (\textbf{temporal action localization/detection}). Temporal action proposal indicates the temporal intervals containing the actions and plays an important role in temporal action localization. Top-down and bottom-up methods are the two main categories used for proposal generation in the existing literature. In this paper, we devise a novel Multi-Granularity Fusion Network (MGFN) to combine the proposals generated from different frameworks for complementary filtering and confidence re-ranking. Specifically, we consider the diversity comprehensively from multiple perspectives, e.g. the characteristic aspect, the data aspect, the model aspect and the result aspect. Our MGFN achieves the state-of-the-art performance on the temporal action proposal task with 69.85 AUC score and the temporal action localization task with 38.90 mAP on the challenge testing set.



### KNEEL: Knee Anatomical Landmark Localization Using Hourglass Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.12237v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12237v2)
- **Published**: 2019-07-29 07:18:54+00:00
- **Updated**: 2019-09-06 18:54:20+00:00
- **Authors**: Aleksei Tiulpin, Iaroslav Melekhov, Simo Saarakkala
- **Comment**: Accepted for Publication at ICCV 2019 VRMI Workshop
- **Journal**: None
- **Summary**: This paper addresses the challenge of localization of anatomical landmarks in knee X-ray images at different stages of osteoarthritis (OA). Landmark localization can be viewed as regression problem, where the landmark position is directly predicted by using the region of interest or even full-size images leading to large memory footprint, especially in case of high resolution medical images. In this work, we propose an efficient deep neural networks framework with an hourglass architecture utilizing a soft-argmax layer to directly predict normalized coordinates of the landmark points. We provide an extensive evaluation of different regularization techniques and various loss functions to understand their influence on the localization performance. Furthermore, we introduce the concept of transfer learning from low-budget annotations, and experimentally demonstrate that such approach is improving the accuracy of landmark localization. Compared to the prior methods, we validate our model on two datasets that are independent from the train data and assess the performance of the method for different stages of OA severity. The proposed approach demonstrates better generalization performance compared to the current state-of-the-art.



### A Fine-Grain Error Map Prediction and Segmentation Quality Assessment Framework for Whole-Heart Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.12244v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12244v1)
- **Published**: 2019-07-29 07:35:00+00:00
- **Updated**: 2019-07-29 07:35:00+00:00
- **Authors**: Rongzhao Zhang, Albert C. S. Chung
- **Comment**: 9 pages, accepted by MICCAI'19
- **Journal**: None
- **Summary**: When introducing advanced image computing algorithms, e.g., whole-heart segmentation, into clinical practice, a common suspicion is how reliable the automatically computed results are. In fact, it is important to find out the failure cases and identify the misclassified pixels so that they can be excluded or corrected for the subsequent analysis or diagnosis. However, it is not a trivial problem to predict the errors in a segmentation mask when ground truth (usually annotated by experts) is absent. In this work, we attempt to address the pixel-wise error map prediction problem and the per-case mask quality assessment problem using a unified deep learning (DL) framework. Specifically, we first formalize an error map prediction problem, then we convert it to a segmentation problem and build a DL network to tackle it. We also derive a quality indicator (QI) from a predicted error map to measure the overall quality of a segmentation mask. To evaluate the proposed framework, we perform extensive experiments on a public whole-heart segmentation dataset, i.e., MICCAI 2017 MMWHS. By 5-fold cross validation, we obtain an overall Dice score of 0.626 for the error map prediction task, and observe a high Pearson correlation coefficient (PCC) of 0.972 between QI and the actual segmentation accuracy (Acc), as well as a low mean absolute error (MAE) of 0.0048 between them, which evidences the efficacy of our method in both error map prediction and quality assessment.



### Automatic Registration between Cone-Beam CT and Scanned Surface via Deep-Pose Regression Neural Networks and Clustered Similarities
- **Arxiv ID**: http://arxiv.org/abs/1907.12250v1
- **DOI**: 10.1109/TMI.2020.3007520
- **Categories**: **cs.CV**, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/1907.12250v1)
- **Published**: 2019-07-29 07:46:01+00:00
- **Updated**: 2019-07-29 07:46:01+00:00
- **Authors**: Minyoung Chung, Jingyu Lee, Wisoo Song, Youngchan Song, Il-Hyung Yang, Jeongjin Lee, Yeong-Gil Shin
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Computerized registration between maxillofacial cone-beam computed tomography (CT) images and a scanned dental model is an essential prerequisite in surgical planning for dental implants or orthognathic surgery. We propose a novel method that performs fully automatic registration between a cone-beam CT image and an optically scanned model. To build a robust and automatic initial registration method, our method applies deep-pose regression neural networks in a reduced domain (i.e., 2-dimensional image). Subsequently, fine registration is performed via optimal clusters. Majority voting system achieves globally optimal transformations while each cluster attempts to optimize local transformation parameters. The coherency of clusters determines their candidacy for the optimal cluster set. The outlying regions in the iso-surface are effectively removed based on the consensus among the optimal clusters. The accuracy of registration was evaluated by the Euclidean distance of 10 landmarks on a scanned model which were annotated by the experts in the field. The experiments show that the proposed method's registration accuracy, measured in landmark distance, outperforms other existing methods by 30.77% to 70%. In addition to achieving high accuracy, our proposed method requires neither human-interactions nor priors (e.g., iso-surface extraction). The main significance of our study is twofold: 1) the employment of light-weighted neural networks which indicates the applicability of neural network in extracting pose cues that can be easily obtained and 2) the introduction of an optimal cluster-based registration method that can avoid metal artifacts during the matching procedures.



### Silhouette Guided Point Cloud Reconstruction beyond Occlusion
- **Arxiv ID**: http://arxiv.org/abs/1907.12253v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.12253v1)
- **Published**: 2019-07-29 08:00:08+00:00
- **Updated**: 2019-07-29 08:00:08+00:00
- **Authors**: Chuhang Zou, Derek Hoiem
- **Comment**: None
- **Journal**: None
- **Summary**: One major challenge in 3D reconstruction is to infer the complete shape geometry from partial foreground occlusions. In this paper, we propose a method to reconstruct the complete 3D shape of an object from a single RGB image, with robustness to occlusion. Given the image and a silhouette of the visible region, our approach completes the silhouette of the occluded region and then generates a point cloud. We show improvements for reconstruction of non-occluded and partially occluded objects by providing the predicted complete silhouette as guidance. We also improve state-of-the-art for 3D shape prediction with a 2D reprojection loss from multiple synthetic views and a surface-based smoothing and refinement step. Experiments demonstrate the efficacy of our approach both quantitatively and qualitatively on synthetic and real scene datasets.



### AirFace: Lightweight and Efficient Model for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.12256v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12256v3)
- **Published**: 2019-07-29 08:04:37+00:00
- **Updated**: 2019-09-26 01:04:20+00:00
- **Authors**: Xianyang Li, Feng Wang, Qinghao Hu, Cong Leng
- **Comment**: None
- **Journal**: None
- **Summary**: With the development of convolutional neural network, significant progress has been made in computer vision tasks. However, the commonly used loss function softmax loss and highly efficient network architecture for common visual tasks are not as effective for face recognition. In this paper, we propose a novel loss function named Li-ArcFace based on ArcFace. Li-ArcFace takes the value of the angle through linear function as the target logit rather than through cosine function, which has better convergence and performance on low dimensional embedding feature learning for face recognition. In terms of network architecture, we improved the the perfomance of MobileFaceNet by increasing the network depth, width and adding attention module. Besides, we found some useful training tricks for face recognition. With all the above results, we won the second place in the deepglint-light challenge of LFR2019.



### V-PROM: A Benchmark for Visual Reasoning Using Visual Progressive Matrices
- **Arxiv ID**: http://arxiv.org/abs/1907.12271v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12271v1)
- **Published**: 2019-07-29 08:28:33+00:00
- **Updated**: 2019-07-29 08:28:33+00:00
- **Authors**: Damien Teney, Peng Wang, Jiewei Cao, Lingqiao Liu, Chunhua Shen, Anton van den Hengel
- **Comment**: None
- **Journal**: None
- **Summary**: One of the primary challenges faced by deep learning is the degree to which current methods exploit superficial statistics and dataset bias, rather than learning to generalise over the specific representations they have experienced. This is a critical concern because generalisation enables robust reasoning over unseen data, whereas leveraging superficial statistics is fragile to even small changes in data distribution. To illuminate the issue and drive progress towards a solution, we propose a test that explicitly evaluates abstract reasoning over visual data. We introduce a large-scale benchmark of visual questions that involve operations fundamental to many high-level vision tasks, such as comparisons of counts and logical operations on complex visual properties. The benchmark directly measures a method's ability to infer high-level relationships and to generalise them over image-based concepts. It includes multiple training/test splits that require controlled levels of generalization. We evaluate a range of deep learning architectures, and find that existing models, including those popular for vision-and-language tasks, are unable to solve seemingly-simple instances. Models using relational networks fare better but leave substantial room for improvement.



### Interlaced Sparse Self-Attention for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.12273v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12273v2)
- **Published**: 2019-07-29 08:33:32+00:00
- **Updated**: 2019-07-30 06:33:46+00:00
- **Authors**: Lang Huang, Yuhui Yuan, Jianyuan Guo, Chao Zhang, Xilin Chen, Jingdong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a so-called interlaced sparse self-attention approach to improve the efficiency of the \emph{self-attention} mechanism for semantic segmentation. The main idea is that we factorize the dense affinity matrix as the product of two sparse affinity matrices. There are two successive attention modules each estimating a sparse affinity matrix. The first attention module is used to estimate the affinities within a subset of positions that have long spatial interval distances and the second attention module is used to estimate the affinities within a subset of positions that have short spatial interval distances. These two attention modules are designed so that each position is able to receive the information from all the other positions. In contrast to the original self-attention module, our approach decreases the computation and memory complexity substantially especially when processing high-resolution feature maps. We empirically verify the effectiveness of our approach on six challenging semantic segmentation benchmarks.



### Regularizing Proxies with Multi-Adversarial Training for Unsupervised Domain-Adaptive Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.12282v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12282v1)
- **Published**: 2019-07-29 08:55:22+00:00
- **Updated**: 2019-07-29 08:55:22+00:00
- **Authors**: Tong Shen, Dong Gong, Wei Zhang, Chunhua Shen, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: Training a semantic segmentation model requires a large amount of pixel-level annotation, hampering its application at scale. With computer graphics, we can generate almost unlimited training data with precise annotation. However,a deep model trained with synthetic data usually cannot directly generalize well to realistic images due to domain shift. It has been observed that highly confident labels for the unlabeled real images may be predicted relying on the labeled synthetic data. To tackle the unsupervised domain adaptation problem, we explore the possibilities to generate high-quality labels as proxy labels to supervise the training on target data. Specifically, we propose a novel proxy-based method using multi-adversarial training. We first train the model using synthetic data (source domain). Multiple discriminators are used to align the features be-tween the source and target domain (real images) at different levels. Then we focus on obtaining and selecting high-quality proxy labels by incorporating both the confidence of the class predictor and that from the adversarial discriminators. Our discriminators not only work as a regularizer to encourage feature alignment but also provide an alternative confidence measure for generating proxy labels. Relying on the generated high-quality proxies, our model can be trained in a "supervised manner" on the target do-main. On two major tasks, GTA5->Cityscapes and SYNTHIA->Cityscapes, our method achieves state-of-the-art results, outperforming the previous by a large margin.



### A Two Stage GAN for High Resolution Retinal Image Generation and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.12296v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.12296v1)
- **Published**: 2019-07-29 09:37:18+00:00
- **Updated**: 2019-07-29 09:37:18+00:00
- **Authors**: Paolo Andreini, Simone Bonechi, Monica Bianchini, Alessandro Mecocci, Franco Scarselli, Andrea Sodi
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, the use of deep learning is becoming increasingly popular in computer vision. However, the effective training of deep architectures usually relies on huge sets of annotated data. This is critical in the medical field where it is difficult and expensive to obtain annotated images. In this paper, we use Generative Adversarial Networks (GANs) for synthesizing high quality retinal images, along with the corresponding semantic label-maps, to be used instead of real images during the training process. Differently from other previous proposals, we suggest a two step approach: first, a progressively growing GAN is trained to generate the semantic label-maps, which describe the blood vessel structure (i.e. vasculature); second, an image-to-image translation approach is used to obtain realistic retinal images from the generated vasculature. By using only a handful of training samples, our approach generates realistic high resolution images, that can be effectively used to enlarge small available datasets. Comparable results have been obtained employing the generated images in place of real data during training. The practical viability of the proposed approach has been demonstrated by applying it on two well established benchmark sets for retinal vessel segmentation, both containing a very small number of training samples. Our method obtained better performances with respect to state-of-the-art techniques.



### Multi-Task Attention-Based Semi-Supervised Learning for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.12303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12303v1)
- **Published**: 2019-07-29 09:44:20+00:00
- **Updated**: 2019-07-29 09:44:20+00:00
- **Authors**: Shuai Chen, Gerda Bortsova, Antonio Garcia-Uceda Juarez, Gijs van Tulder, Marleen de Bruijne
- **Comment**: Accepted at MICCAI 2019
- **Journal**: None
- **Summary**: We propose a novel semi-supervised image segmentation method that simultaneously optimizes a supervised segmentation and an unsupervised reconstruction objectives. The reconstruction objective uses an attention mechanism that separates the reconstruction of image areas corresponding to different classes. The proposed approach was evaluated on two applications: brain tumor and white matter hyperintensities segmentation. Our method, trained on unlabeled and a small number of labeled images, outperformed supervised CNNs trained with the same number of images and CNNs pre-trained on unlabeled data. In ablation experiments, we observed that the proposed attention mechanism substantially improves segmentation performance. We explore two multi-task training strategies: joint training and alternating training. Alternating training requires fewer hyperparameters and achieves a better, more stable performance than joint training. Finally, we analyze the features learned by different methods and find that the attention mechanism helps to learn more discriminative features in the deeper layers of encoders.



### EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos
- **Arxiv ID**: http://arxiv.org/abs/1907.12918v2
- **DOI**: 10.1109/TVCG.2019.2934656
- **Categories**: **cs.CV**, cs.HC, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1907.12918v2)
- **Published**: 2019-07-29 10:27:42+00:00
- **Updated**: 2019-10-09 16:46:29+00:00
- **Authors**: Haipeng Zeng, Xingbo Wang, Aoyu Wu, Yong Wang, Quan Li, Alex Endert, Huamin Qu
- **Comment**: 11 pages, 8 figures. Accepted by IEEE VAST 2019
- **Journal**: None
- **Summary**: Emotions play a key role in human communication and public presentations. Human emotions are usually expressed through multiple modalities. Therefore, exploring multimodal emotions and their coherence is of great value for understanding emotional expressions in presentations and improving presentation skills. However, manually watching and studying presentation videos is often tedious and time-consuming. There is a lack of tool support to help conduct an efficient and in-depth multi-level analysis. Thus, in this paper, we introduce EmoCo, an interactive visual analytics system to facilitate efficient analysis of emotion coherence across facial, text, and audio modalities in presentation videos. Our visualization system features a channel coherence view and a sentence clustering view that together enable users to obtain a quick overview of emotion coherence and its temporal evolution. In addition, a detail view and word view enable detailed exploration and comparison from the sentence level and word level, respectively. We thoroughly evaluate the proposed system and visualization techniques through two usage scenarios based on TED Talk videos and interviews with two domain experts. The results demonstrate the effectiveness of our system in gaining insights into emotion coherence in presentations.



### Goal-Driven Sequential Data Abstraction
- **Arxiv ID**: http://arxiv.org/abs/1907.12336v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12336v2)
- **Published**: 2019-07-29 11:24:51+00:00
- **Updated**: 2019-08-08 10:01:40+00:00
- **Authors**: Umar Riaz Muhammad, Yongxin Yang, Timothy M. Hospedales, Tao Xiang, Yi-Zhe Song
- **Comment**: Accepted at ICCV 2019
- **Journal**: None
- **Summary**: Automatic data abstraction is an important capability for both benchmarking machine intelligence and supporting summarization applications. In the former one asks whether a machine can `understand' enough about the meaning of input data to produce a meaningful but more compact abstraction. In the latter this capability is exploited for saving space or human time by summarizing the essence of input data. In this paper we study a general reinforcement learning based framework for learning to abstract sequential data in a goal-driven way. The ability to define different abstraction goals uniquely allows different aspects of the input data to be preserved according to the ultimate purpose of the abstraction. Our reinforcement learning objective does not require human-defined examples of ideal abstraction. Importantly our model processes the input sequence holistically without being constrained by the original input order. Our framework is also domain agnostic -- we demonstrate applications to sketch, video and text data and achieve promising results in all domains.



### Meta Learning for Task-Driven Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/1907.12342v1
- **DOI**: 10.1109/TIE.2019.2931283.
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12342v1)
- **Published**: 2019-07-29 11:35:27+00:00
- **Updated**: 2019-07-29 11:35:27+00:00
- **Authors**: Xuelong Li, Hongli Li, Yongsheng Dong
- **Comment**: 9 pages, 6 figures
- **Journal**: IEEE Transactions on Industrial Electronics, 2019
- **Summary**: Existing video summarization approaches mainly concentrate on sequential or structural characteristic of video data. However, they do not pay enough attention to the video summarization task itself. In this paper, we propose a meta learning method for performing task-driven video summarization, denoted by MetaL-TDVS, to explicitly explore the video summarization mechanism among summarizing processes on different videos. Particularly, MetaL-TDVS aims to excavate the latent mechanism for summarizing video by reformulating video summarization as a meta learning problem and promote generalization ability of the trained model. MetaL-TDVS regards summarizing each video as a single task to make better use of the experience and knowledge learned from processes of summarizing other videos to summarize new ones. Furthermore, MetaL-TDVS updates models via a two-fold back propagation which forces the model optimized on one video to obtain high accuracy on another video in every training step. Extensive experiments on benchmark datasets demonstrate the superiority and better generalization ability of MetaL-TDVS against several state-of-the-art methods.



### FSS-1000: A 1000-Class Dataset for Few-Shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.12347v2
- **DOI**: 10.1109/CVPR42600.2020.00294
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12347v2)
- **Published**: 2019-07-29 11:47:25+00:00
- **Updated**: 2020-04-29 22:35:58+00:00
- **Authors**: Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, Chi-Keung Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Over the past few years, we have witnessed the success of deep learning in image recognition thanks to the availability of large-scale human-annotated datasets such as PASCAL VOC, ImageNet, and COCO. Although these datasets have covered a wide range of object categories, there are still a significant number of objects that are not included. Can we perform the same task without a lot of human annotations? In this paper, we are interested in few-shot object segmentation where the number of annotated training examples are limited to 5 only. To evaluate and validate the performance of our approach, we have built a few-shot segmentation dataset, FSS-1000, which consists of 1000 object classes with pixelwise annotation of ground-truth segmentation. Unique in FSS-1000, our dataset contains significant number of objects that have never been seen or annotated in previous datasets, such as tiny daily objects, merchandise, cartoon characters, logos, etc. We build our baseline model using standard backbone networks such as VGG-16, ResNet-101, and Inception. To our surprise, we found that training our model from scratch using FSS-1000 achieves comparable and even better results than training with weights pre-trained by ImageNet which is more than 100 times larger than FSS-1000. Both our approach and dataset are simple, effective, and easily extensible to learn segmentation of new object classes given very few annotated training examples. Dataset is available at https://github.com/HKUSTCV/FSS-1000.



### Recursive Cascaded Networks for Unsupervised Medical Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1907.12353v3
- **DOI**: 10.1109/ICCV.2019.01070
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12353v3)
- **Published**: 2019-07-29 12:10:03+00:00
- **Updated**: 2019-10-22 12:39:08+00:00
- **Authors**: Shengyu Zhao, Yue Dong, Eric I-Chao Chang, Yan Xu
- **Comment**: Accepted to ICCV 2019
- **Journal**: IEEE International Conference on Computer Vision (ICCV), 2019, pp.
  10600-10610
- **Summary**: We present recursive cascaded networks, a general architecture that enables learning deep cascades, for deformable image registration. The proposed architecture is simple in design and can be built on any base network. The moving image is warped successively by each cascade and finally aligned to the fixed image; this procedure is recursive in a way that every cascade learns to perform a progressive deformation for the current warped image. The entire system is end-to-end and jointly trained in an unsupervised manner. In addition, enabled by the recursive architecture, one cascade can be iteratively applied for multiple times during testing, which approaches a better fit between each of the image pairs. We evaluate our method on 3D medical images, where deformable registration is most commonly applied. We demonstrate that recursive cascaded networks achieve consistent, significant gains and outperform state-of-the-art methods. The performance reveals an increasing trend as long as more cascades are trained, while the limit is not observed. Code is available at https://github.com/microsoft/Recursive-Cascaded-Networks.



### Specular- and Diffuse-reflection-based Face Spoofing Detection for Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/1907.12400v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12400v5)
- **Published**: 2019-07-29 13:03:24+00:00
- **Updated**: 2020-12-16 01:18:22+00:00
- **Authors**: Akinori F. Ebihara, Kazuyuki Sakurai, Hitoshi Imaoka
- **Comment**: International Joint Conference on Biometrics (IJCB) 2020 Google PC
  Chairs Choice Best Paper Award
- **Journal**: None
- **Summary**: In light of the rising demand for biometric-authentication systems, preventing face spoofing attacks is a critical issue for the safe deployment of face recognition systems. Here, we propose an efficient face presentation attack detection (PAD) algorithm that requires minimal hardware and only a small database, making it suitable for resource-constrained devices such as mobile phones. Utilizing one monocular visible light camera, the proposed algorithm takes two facial photos, one taken with a flash, the other without a flash. The proposed $SpecDiff$ descriptor is constructed by leveraging two types of reflection: (i) specular reflections from the iris region that have a specific intensity distribution depending on liveness, and (ii) diffuse reflections from the entire face region that represents the 3D structure of a subject's face. Classifiers trained with $SpecDiff$ descriptor outperforms other flash-based PAD algorithms on both an in-house database and on publicly available NUAA, Replay-Attack, and SiW databases. Moreover, the proposed algorithm achieves statistically significantly better accuracy to that of an end-to-end, deep neural network classifier, while being approximately six-times faster execution speed. The code is publicly available at https://github.com/Akinori-F-Ebihara/SpecDiff-spoofing-detector.



### Consensus Feature Network for Scene Parsing
- **Arxiv ID**: http://arxiv.org/abs/1907.12411v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12411v2)
- **Published**: 2019-07-29 13:22:30+00:00
- **Updated**: 2019-12-01 15:26:05+00:00
- **Authors**: Tianyi Wu, Sheng Tang, Rui Zhang, Guodong Guo, Yongdong Zhang
- **Comment**: 13 pages, 7 figures, 9 tables
- **Journal**: None
- **Summary**: Scene parsing is challenging as it aims to assign one of the semantic categories to each pixel in scene images. Thus, pixel-level features are desired for scene parsing. However, classification networks are dominated by the discriminative portion, so directly applying classification networks to scene parsing will result in inconsistent parsing predictions within one instance and among instances of the same category. To address this problem, we propose two transform units to learn pixel-level consensus features. One is an Instance Consensus Transform (ICT) unit to learn the instance-level consensus features by aggregating features within the same instance. The other is a Category Consensus Transform (CCT) unit to pursue category-level consensus features through keeping the consensus of features among instances of the same category in scene images. The proposed ICT and CCT units are lightweight, data-driven and end-to-end trainable. The features learned by the two units are more coherent in both instance-level and category-level. Furthermore, we present the Consensus Feature Network (CFNet) based on the proposed ICT and CCT units, and demonstrate the effectiveness of each component in our method by performing extensive ablation experiments. Finally, our proposed CFNet achieves competitive performance on four datasets, including Cityscapes, Pascal Context, CamVid, and COCO Stuff.



### Solving the Robot-World Hand-Eye(s) Calibration Problem with Iterative Methods
- **Arxiv ID**: http://arxiv.org/abs/1907.12425v1
- **DOI**: 10.1007/s00138-017-0841-7
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.12425v1)
- **Published**: 2019-07-29 13:42:46+00:00
- **Updated**: 2019-07-29 13:42:46+00:00
- **Authors**: Amy Tabb, Khalil M. Ahmad Yousef
- **Comment**: 25 pages, including Erratum
- **Journal**: Machine Vision and Applications, 2017
- **Summary**: Robot-world, hand-eye calibration is the problem of determining the transformation between the robot end-effector and a camera, as well as the transformation between the robot base and the world coordinate system. This relationship has been modeled as $\mathbf{AX}=\mathbf{ZB}$, where $\mathbf{X}$ and $\mathbf{Z}$ are unknown homogeneous transformation matrices. The successful execution of many robot manipulation tasks depends on determining these matrices accurately, and we are particularly interested in the use of calibration for use in vision tasks. In this work, we describe a collection of methods consisting of two cost function classes, three different parameterizations of rotation components, and separable versus simultaneous formulations. We explore the behavior of this collection of methods on real datasets and simulated datasets, and compare to seven other state-of-the-art methods. Our collection of methods return greater accuracy on many metrics as compared to the state-of-the-art. The collection of methods is extended to the problem of robot-world hand-multiple-eye calibration, and results are shown with two and three cameras mounted on the same robot.



### Learn to Scale: Generating Multipolar Normalized Density Maps for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/1907.12428v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12428v2)
- **Published**: 2019-07-29 13:48:07+00:00
- **Updated**: 2019-08-08 15:56:05+00:00
- **Authors**: Chenfeng Xu, Kai Qiu, Jianlong Fu, Song Bai, Yongchao Xu, Xiang Bai
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: Dense crowd counting aims to predict thousands of human instances from an image, by calculating integrals of a density map over image pixels. Existing approaches mainly suffer from the extreme density variances. Such density pattern shift poses challenges even for multi-scale model ensembling. In this paper, we propose a simple yet effective approach to tackle this problem. First, a patch-level density map is extracted by a density estimation model and further grouped into several density levels which are determined over full datasets. Second, each patch density map is automatically normalized by an online center learning strategy with a multipolar center loss. Such a design can significantly condense the density distribution into several clusters, and enable that the density variance can be learned by a single model. Extensive experiments demonstrate the superiority of the proposed method. Our work outperforms the state-of-the-art by 4.2%, 14.3%, 27.1% and 20.1% in MAE, on ShanghaiTech Part A, ShanghaiTech Part B, UCF_CC_50 and UCF-QNRF datasets, respectively.



### Salient Slices: Improved Neural Network Training and Performance with Image Entropy
- **Arxiv ID**: http://arxiv.org/abs/1907.12436v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.12436v4)
- **Published**: 2019-07-29 13:55:13+00:00
- **Updated**: 2020-05-04 22:06:11+00:00
- **Authors**: Steven J. Frank, Andrea M. Frank
- **Comment**: Final version; article will be published in Neural Computation 32,
  1222-1237 (June 2020)
- **Journal**: None
- **Summary**: As a training and analysis strategy for convolutional neural networks (CNNs), we slice images into tiled segments and use, for training and prediction, segments that both satisfy a criterion of information diversity and contain sufficient content to support classification. In particular, we utilize image entropy as the diversity criterion. This ensures that each tile carries as much information diversity as the original image, and for many applications serves as an indicator of usefulness in classification. To make predictions, a probability aggregation framework is applied to probabilities assigned by the CNN to the input image tiles. This technique facilitates the use of large, high-resolution images that would be impractical to analyze unmodified; provides data augmentation for training, which is particularly valuable when image availability is limited; and the ensemble nature of the input for prediction enhances its accuracy.



### Self-Supervised Learning for Stereo Reconstruction on Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/1907.12446v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12446v1)
- **Published**: 2019-07-29 14:10:26+00:00
- **Updated**: 2019-07-29 14:10:26+00:00
- **Authors**: Patrick Kn√∂belreiter, Christoph Vogel, Thomas Pock
- **Comment**: Symposium Prize Paper Award @IGARSS 2018
- **Journal**: None
- **Summary**: Recent developments established deep learning as an inevitable tool to boost the performance of dense matching and stereo estimation. On the downside, learning these networks requires a substantial amount of training data to be successful. Consequently, the application of these models outside of the laboratory is far from straight forward. In this work we propose a self-supervised training procedure that allows us to adapt our network to the specific (imaging) characteristics of the dataset at hand, without the requirement of external ground truth data. We instead generate interim training data by running our intermediate network on the whole dataset, followed by conservative outlier filtering. Bootstrapped from a pre-trained version of our hybrid CNN-CRF model, we alternate the generation of training data and network training. With this simple concept we are able to lift the completeness and accuracy of the pre-trained version significantly. We also show that our final model compares favorably to other popular stereo estimation algorithms on an aerial dataset.



### Automated Lesion Detection by Regressing Intensity-Based Distance with a Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1907.12452v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.12452v1)
- **Published**: 2019-07-29 14:17:17+00:00
- **Updated**: 2019-07-29 14:17:17+00:00
- **Authors**: Kimberlin M. H. van Wijnen, Florian Dubost, Pinar Yilmaz, M. Arfan Ikram, Wiro J. Niessen, Hieab Adams, Meike W. Vernooij, Marleen de Bruijne
- **Comment**: MICCAI 2019
- **Journal**: None
- **Summary**: Localization of focal vascular lesions on brain MRI is an important component of research on the etiology of neurological disorders. However, manual annotation of lesions can be challenging, time-consuming and subject to observer bias. Automated detection methods often need voxel-wise annotations for training. We propose a novel approach for automated lesion detection that can be trained on scans only annotated with a dot per lesion instead of a full segmentation. From the dot annotations and their corresponding intensity images we compute various distance maps (DMs), indicating the distance to a lesion based on spatial distance, intensity distance, or both. We train a fully convolutional neural network (FCN) to predict these DMs for unseen intensity images. The local optima in the predicted DMs are expected to correspond to lesion locations. We show the potential of this approach to detect enlarged perivascular spaces in white matter on a large brain MRI dataset with an independent test set of 1000 scans. Our method matches the intra-rater performance of the expert rater that was computed on an independent set. We compare the different types of distance maps, showing that incorporating intensity information in the distance maps used to train an FCN greatly improves performance.



### X-LineNet: Detecting Aircraft in Remote Sensing Images by a pair of Intersecting Line Segments
- **Arxiv ID**: http://arxiv.org/abs/1907.12474v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12474v3)
- **Published**: 2019-07-29 15:22:27+00:00
- **Updated**: 2019-12-24 01:39:48+00:00
- **Authors**: Haoran Wei, Yue Zhang, Bing Wang, Yang Yang, Hao Li, Hongqi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Motivated by the development of deep convolution neural networks (DCNNs), tremendous progress has been gained in the field of aircraft detection. These DCNNs based detectors mainly belong to top-down approaches, which first enumerate massive potential locations of objects with the form of rectangular regions, and then identify whether they are objects or not. Compared with these top-down approaches, this paper shows that aircraft detection via bottom-up approach still performs competitively in the era of deep learning. We present a novel one-stage and anchor-free aircraft detection model in a bottom-up manner, which formulates the task as detection of two intersecting line segments inside each target and grouping of them without any rectangular region classification. This model is named as X-LineNet. With simple post-processing, X-LineNet can simultaneously provide multiple representation forms of the detection result: the horizontal bounding box, the rotating bounding box, and the pentagonal mask. The pentagonal mask is a more accurate representation form which has less redundancy and can better represent aircraft than that of rectangular box. Experiments show that X-LineNet outperforms state-of-the-art one-stage object detectors and is competitive compared with advanced two-stage detectors on both UCAS-AOD and NWPU VHR-10 open dataset in the field of aircraft detection.



### Benefiting from Multitask Learning to Improve Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1907.12488v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12488v1)
- **Published**: 2019-07-29 15:37:05+00:00
- **Updated**: 2019-07-29 15:37:05+00:00
- **Authors**: Mohammad Saeed Rad, Behzad Bozorgtabar, Claudiu Musat, Urs-Viktor Marti, Max Basler, Hazim Kemal Ekenel, Jean-Philippe Thiran
- **Comment**: accepted at Neurocomputing (Special Issue on Deep Learning for Image
  Super-Resolution), 2019
- **Journal**: None
- **Summary**: Despite significant progress toward super resolving more realistic images by deeper convolutional neural networks (CNNs), reconstructing fine and natural textures still remains a challenging problem. Recent works on single image super resolution (SISR) are mostly based on optimizing pixel and content wise similarity between recovered and high-resolution (HR) images and do not benefit from recognizability of semantic classes. In this paper, we introduce a novel approach using categorical information to tackle the SISR problem; we present a decoder architecture able to extract and use semantic information to super-resolve a given image by using multitask learning, simultaneously for image super-resolution and semantic segmentation. To explore categorical information during training, the proposed decoder only employs one shared deep network for two task-specific output layers. At run-time only layers resulting HR image are used and no segmentation label is required. Extensive perceptual experiments and a user study on images randomly selected from COCO-Stuff dataset demonstrate the effectiveness of our proposed method and it outperforms the state-of-the-art methods.



### Towards Automatic Screening of Typical and Atypical Behaviors in Children With Autism
- **Arxiv ID**: http://arxiv.org/abs/1907.12537v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12537v2)
- **Published**: 2019-07-29 17:18:11+00:00
- **Updated**: 2019-09-17 14:00:52+00:00
- **Authors**: Andrew Cook, Bappaditya Mandal, Donna Berry, Matthew Johnson
- **Comment**: This paper has been withdrawn by the authors due to insufficient or
  definition error(s) in the ethics approval protocol
- **Journal**: None
- **Summary**: This paper has been withdrawn by the authors due to insufficient or definition error(s) in the ethics approval protocol.   Autism spectrum disorders (ASD) impact the cognitive, social, communicative and behavioral abilities of an individual. The development of new clinical decision support systems is of importance in reducing the delay between presentation of symptoms and an accurate diagnosis. In this work, we contribute a new database consisting of video clips of typical (normal) and atypical (such as hand flapping, spinning or rocking) behaviors, displayed in natural settings, which have been collected from the YouTube video website. We propose a preliminary non-intrusive approach based on skeleton keypoint identification using pretrained deep neural networks on human body video clips to extract features and perform body movement analysis that differentiates typical and atypical behaviors of children. Experimental results on the newly contributed database show that our platform performs best with decision tree as the classifier when compared to other popular methodologies and offers a baseline against which alternate approaches may developed and tested.



### Artistic Domain Generalisation Methods are Limited by their Deep Representations
- **Arxiv ID**: http://arxiv.org/abs/1907.12622v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12622v1)
- **Published**: 2019-07-29 20:06:41+00:00
- **Updated**: 2019-07-29 20:06:41+00:00
- **Authors**: Padraig Boulton, Peter Hall
- **Comment**: 7 pages, 3 figures, 1 Table, 3 Algorithms
- **Journal**: None
- **Summary**: The cross-depiction problem refers to the task of recognising visual objects regardless of their depictions; whether photographed, painted, sketched, {\em etc}. In the past, some researchers considered cross-depiction to be domain adaptation (DA). More recent work considers cross-depiction as domain generalisation (DG), in which algorithms extend recognition from one set of domains (such as photographs and coloured artwork) to another (such as sketches). We show that fixing the last layer of AlexNet to random values provides a performance comparable to state of the art DA and DG algorithms, when tested over the PACS benchmark. With support from background literature, our results lead us to conclude that texture alone is insufficient to support generalisation; rather, higher-order representations such as structure and shape are necessary.



### MoBiNet: A Mobile Binary Network for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1907.12629v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.12629v2)
- **Published**: 2019-07-29 20:31:15+00:00
- **Updated**: 2019-07-31 03:38:10+00:00
- **Authors**: Hai Phan, Dang Huynh, Yihui He, Marios Savvides, Zhiqiang Shen
- **Comment**: None
- **Journal**: None
- **Summary**: MobileNet and Binary Neural Networks are two among the most widely used techniques to construct deep learning models for performing a variety of tasks on mobile and embedded platforms.In this paper, we present a simple yet efficient scheme to exploit MobileNet binarization at activation function and model weights. However, training a binary network from scratch with separable depth-wise and point-wise convolutions in case of MobileNet is not trivial and prone to divergence. To tackle this training issue, we propose a novel neural network architecture, namely MoBiNet - Mobile Binary Network in which skip connections are manipulated to prevent information loss and vanishing gradient, thus facilitate the training process. More importantly, while existing binary neural networks often make use of cumbersome backbones such as Alex-Net, ResNet, VGG-16 with float-type pre-trained weights initialization, our MoBiNet focuses on binarizing the already-compressed neural networks like MobileNet without the need of a pre-trained model to start with. Therefore, our proposal results in an effectively small model while keeping the accuracy comparable to existing ones. Experiments on ImageNet dataset show the potential of the MoBiNet as it achieves 54.40% top-1 accuracy and dramatically reduces the computational cost with binary operators.



### Task Classification Model for Visual Fixation, Exploration, and Search
- **Arxiv ID**: http://arxiv.org/abs/1907.12635v1
- **DOI**: 10.1145/3314111.3323073
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.12635v1)
- **Published**: 2019-07-29 20:50:37+00:00
- **Updated**: 2019-07-29 20:50:37+00:00
- **Authors**: Ayush Kumar, Anjul Tyagi, Michael Burch, Daniel Weiskopf, Klaus Mueller
- **Comment**: 4 pages
- **Journal**: In proceedings of the 11th ACM Symposium on Eye Tracking Research
  and Applications, 2019
- **Summary**: Yarbus' claim to decode the observer's task from eye movements has received mixed reactions. In this paper, we have supported the hypothesis that it is possible to decode the task. We conducted an exploratory analysis on the dataset by projecting features and data points into a scatter plot to visualize the nuance properties for each task. Following this analysis, we eliminated highly correlated features before training an SVM and Ada Boosting classifier to predict the tasks from this filtered eye movements data. We achieve an accuracy of 95.4% on this task classification problem and hence, support the hypothesis that task classification is possible from a user's eye movement data.



### Particle Swarm Optimisation for Evolving Deep Neural Networks for Image Classification by Evolving and Stacking Transferable Blocks
- **Arxiv ID**: http://arxiv.org/abs/1907.12659v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.12659v2)
- **Published**: 2019-07-29 21:30:36+00:00
- **Updated**: 2020-03-21 04:25:01+00:00
- **Authors**: Bin Wang, Bing Xue, Mengjie Zhang
- **Comment**: To appear in ieee wcci 2020
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (CNNs) have been widely used in image classification tasks, but the process of designing CNN architectures is very complex, so Neural Architecture Search (NAS), automatically searching for optimal CNN architectures, has attracted more and more research interests. However, the computational cost of NAS is often too high to apply NAS on real-life applications. In this paper, an efficient particle swarm optimisation method named EPSOCNN is proposed to evolve CNN architectures inspired by the idea of transfer learning. EPSOCNN successfully reduces the computation cost by minimising the search space to a single block and utilising a small subset of the training set to evaluate CNNs during evolutionary process. Meanwhile, EPSOCNN also keeps very competitive classification accuracy by stacking the evolved block multiple times to fit the whole dataset. The proposed EPSOCNN algorithm is evaluated on CIFAR-10 dataset and compared with 13 peer competitors comprised of deep CNNs crafted by hand, learned by reinforcement learning methods and evolved by evolutionary computation approaches, which shows very promising results by outperforming all of the peer competitors with regard to the classification accuracy, number of parameters and the computational cost.



