# Arxiv Papers in cs.CV on 2019-07-19
### A multiscale Laplacian of Gaussian (LoG) filtering approach to pulmonary nodule detection from whole-lung CT scans
- **Arxiv ID**: http://arxiv.org/abs/1907.08328v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.08328v1)
- **Published**: 2019-07-19 01:14:29+00:00
- **Updated**: 2019-07-19 01:14:29+00:00
- **Authors**: Sergei V. Fotin, David F. Yankelevitz, Claudia I. Henschke, Anthony P. Reeves
- **Comment**: 16 pages, 23 figures
- **Journal**: None
- **Summary**: Candidate generation, the first stage for most computer aided detection (CAD) systems, rapidly scans the entire image data for any possible abnormality locations, while the subsequent stages of the CAD system refine the candidates list to determine the most probable or significant of these candidates. The candidate generator creates a list of the locations and provides a size estimate for each candidate. A multiscale scale-normalized Laplacian of Gaussian (LoG) filtering method for detecting pulmonary nodules in whole-lung CT scans, presented in this paper, achieves a high sensitivity for both solid and nonsolid pulmonary nodules. The pulmonary nodule LoG filtering method was validated on a size-enriched database of 706 whole-lung low-dose CT scans containing 499 solid (>= 4 mm) and 107 nonsolid (>= 6 mm) pulmonary nodules. The method achieved a sensitivity of 0.998 (498/499) for solid nodules and a sensitivity of 1.000 (107/107) for nonsolid nodules. Furthermore, compared to radiologist measurements, the method provided low average nodule size estimation error of 0.12 mm for solid and 1.27 mm for nonsolid nodules. The average distance between automatically and manually determined nodule centroids were 1.41 mm and 1.43 mm, respectively.



### Only Time Can Tell: Discovering Temporal Data for Temporal Modeling
- **Arxiv ID**: http://arxiv.org/abs/1907.08340v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.08340v2)
- **Published**: 2019-07-19 02:00:23+00:00
- **Updated**: 2019-10-29 19:20:18+00:00
- **Authors**: Laura Sevilla-Lara, Shengxin Zha, Zhicheng Yan, Vedanuj Goswami, Matt Feiszli, Lorenzo Torresani
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding temporal information and how the visual world changes over time is a fundamental ability of intelligent systems. In video understanding, temporal information is at the core of many current challenges, including compression, efficient inference, motion estimation or summarization. However, in current video datasets it has been observed that action classes can often be recognized without any temporal information from a single frame of video. As a result, both benchmarking and training in these datasets may give an unintentional advantage to models with strong image understanding capabilities, as opposed to those with strong temporal understanding. In this paper we address this problem head on by identifying action classes where temporal information is actually necessary to recognize them and call these "temporal classes". Selecting temporal classes using a computational method would bias the process. Instead, we propose a methodology based on a simple and effective human annotation experiment. We remove just the temporal information by shuffling frames in time and measure if the action can still be recognized. Classes that cannot be recognized when frames are not in order are included in the temporal Dataset. We observe that this set is statistically different from other static classes, and that performance in it correlates with a network's ability to capture temporal information. Thus we use it as a benchmark on current popular networks, which reveals a series of interesting facts. We also explore the effect of training on the temporal dataset, and observe that this leads to better generalization in unseen classes, demonstrating the need for more temporal data. We hope that the proposed dataset of temporal categories will help guide future research in temporal modeling for better video understanding.



### Robust Real-time RGB-D Visual Odometry in Dynamic Environments via Rigid Motion Model
- **Arxiv ID**: http://arxiv.org/abs/1907.08388v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.08388v1)
- **Published**: 2019-07-19 06:53:33+00:00
- **Updated**: 2019-07-19 06:53:33+00:00
- **Authors**: Sangil Lee, Clark Youngdong Son, H. Jin Kim
- **Comment**: None
- **Journal**: None
- **Summary**: In the paper, we propose a robust real-time visual odometry in dynamic environments via rigid-motion model updated by scene flow. The proposed algorithm consists of spatial motion segmentation and temporal motion tracking. The spatial segmentation first generates several motion hypotheses by using a grid-based scene flow and clusters the extracted motion hypotheses, separating objects that move independently of one another. Further, we use a dual-mode motion model to consistently distinguish between the static and dynamic parts in the temporal motion tracking stage. Finally, the proposed algorithm estimates the pose of a camera by taking advantage of the region classified as static parts. In order to evaluate the performance of visual odometry under the existence of dynamic rigid objects, we use self-collected dataset containing RGB-D images and motion capture data for ground-truth. We compare our algorithm with state-of-the-art visual odometry algorithms. The validation results suggest that the proposed algorithm can estimate the pose of a camera robustly and accurately in dynamic environments.



### Artificial Neural Network Algorithm based Skyrmion Material Design of Chiral Crystals
- **Arxiv ID**: http://arxiv.org/abs/1907.09314v1
- **DOI**: None
- **Categories**: **physics.comp-ph**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.09314v1)
- **Published**: 2019-07-19 09:07:09+00:00
- **Updated**: 2019-07-19 09:07:09+00:00
- **Authors**: B. U. V Prashanth, Mohammed Riyaz Ahmed
- **Comment**: 8 Pages, 5 figures
- **Journal**: None
- **Summary**: The model presented in this research predicts ideal chiral crystal and propose a new direction of designing chiral crystals. Skyrmions are topologically protected and structurally assymetric materials with an exotic spin composition. This work presents deep learning method for skyrmion material design of chiral crystals. This paper presents an approach to construct a probabilistic classifier and an Artificial Neural Network(ANN) from a true or false chirality dataset consisting of chiral and achiral compounds with 'A' and 'B' type elements. A quantitative predictor for accuracy of forming the chiral crystals is illustrated. The feasibility of ANN method is tested in a comprehensive manner by comparing with probalistic classifier method. Throughout this manuscript we present deep learnig algorithm design with modelling and simulations of materials. This research work elucidated paves a way to develop sophisticated software tool to make an indicator of crystal design.



### Towards automatic estimation of conversation floors within F-formations
- **Arxiv ID**: http://arxiv.org/abs/1907.10384v2
- **DOI**: None
- **Categories**: **cs.MA**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1907.10384v2)
- **Published**: 2019-07-19 09:16:05+00:00
- **Updated**: 2019-07-31 09:31:16+00:00
- **Authors**: Chirag Raman, Hayley Hung
- **Comment**: 8th International Conference on Affective Computing & Intelligent
  Interaction EMERGent Workshop, 7 pages, 4 Figures, 2 Tables
- **Journal**: None
- **Summary**: The detection of free-standing conversing groups has received significant attention in recent years. In the absence of a formal definition, most studies operationalize the notion of a conversation group either through a spatial or a temporal lens. Spatially, the most commonly used representation is the F-formation, defined by social scientists as the configuration in which people arrange themselves to sustain an interaction. However, the use of this representation is often accompanied with the simplifying assumption that a single conversation occurs within an F-formation. Temporally, various categories have been used to organize conversational units; these include, among others, turn, topic, and floor. Some of these concepts are hard to define objectively by themselves. The present work constitutes an initial exploration into unifying these perspectives by primarily posing the question: can we use the observation of simultaneous speaker turns to infer whether multiple conversation floors exist within an F-formation? We motivate a metric for the existence of distinct conversation floors based on simultaneous speaker turns, and provide an analysis using this metric to characterize conversations across F-formations of varying cardinality. We contribute two key findings: firstly, at the average speaking turn duration of about two seconds for humans, there is evidence for the existence of multiple floors within an F-formation; and secondly, an increase in the cardinality of an F-formation correlates with a decrease in duration of simultaneous speaking turns.



### VRSTC: Occlusion-Free Video Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1907.08427v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.08427v1)
- **Published**: 2019-07-19 09:38:27+00:00
- **Updated**: 2019-07-19 09:38:27+00:00
- **Authors**: Ruibing Hou, Bingpeng Ma, Hong Chang, Xinqian Gu, Shiguang Shan, Xilin Chen
- **Comment**: 10 pages, 6 figures, 5 tables. Accepted by CVPR 2019
- **Journal**: None
- **Summary**: Video person re-identification (re-ID) plays an important role in surveillance video analysis. However, the performance of video re-ID degenerates severely under partial occlusion. In this paper, we propose a novel network, called Spatio-Temporal Completion network (STCnet), to explicitly handle partial occlusion problem. Different from most previous works that discard the occluded frames, STCnet can recover the appearance of the occluded parts. For one thing, the spatial structure of a pedestrian frame can be used to predict the occluded body parts from the unoccluded body parts of this frame. For another, the temporal patterns of pedestrian sequence provide important clues to generate the contents of occluded parts. With the Spatio-temporal information, STCnet can recover the appearance for the occluded parts, which could be leveraged with those unoccluded parts for more accurate video re-ID. By combining a re-ID network with STCnet, a video re-ID framework robust to partial occlusion (VRSTC) is proposed. Experiments on three challenging video re-ID databases demonstrate that the proposed approach outperforms the state-of-the-art.



### Interaction-and-Aggregation Network for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1907.08435v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.08435v1)
- **Published**: 2019-07-19 09:47:58+00:00
- **Updated**: 2019-07-19 09:47:58+00:00
- **Authors**: Ruibing Hou, Bingpeng Ma, Hong Chang, Xinqian Gu, Shiguang Shan, Xilin Chen
- **Comment**: 10 pages, 8 figures, accepted by CVPR 2019
- **Journal**: None
- **Summary**: Person re-identification (reID) benefits greatly from deep convolutional neural networks (CNNs) which learn robust feature embeddings. However, CNNs are inherently limited in modeling the large variations in person pose and scale due to their fixed geometric structures. In this paper, we propose a novel network structure, Interaction-and-Aggregation (IA), to enhance the feature representation capability of CNNs. Firstly, Spatial IA (SIA) module is introduced. It models the interdependencies between spatial features and then aggregates the correlated features corresponding to the same body parts. Unlike CNNs which extract features from fixed rectangle regions, SIA can adaptively determine the receptive fields according to the input person pose and scale. Secondly, we introduce Channel IA (CIA) module which selectively aggregates channel features to enhance the feature representation, especially for smallscale visual cues. Further, IA network can be constructed by inserting IA blocks into CNNs at any depth. We validate the effectiveness of our model for person reID by demonstrating its superiority over state-of-the-art methods on three benchmark datasets.



### Deep Graph-Convolutional Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1907.08448v1
- **DOI**: 10.1109/TIP.2020.3013166
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1907.08448v1)
- **Published**: 2019-07-19 10:27:41+00:00
- **Updated**: 2019-07-19 10:27:41+00:00
- **Authors**: Diego Valsesia, Giulia Fracastoro, Enrico Magli
- **Comment**: None
- **Journal**: None
- **Summary**: Non-local self-similarity is well-known to be an effective prior for the image denoising problem. However, little work has been done to incorporate it in convolutional neural networks, which surpass non-local model-based methods despite only exploiting local information. In this paper, we propose a novel end-to-end trainable neural network architecture employing layers based on graph convolution operations, thereby creating neurons with non-local receptive fields. The graph convolution operation generalizes the classic convolution to arbitrary graphs. In this work, the graph is dynamically computed from similarities among the hidden features of the network, so that the powerful representation learning capabilities of the network are exploited to uncover self-similar patterns. We introduce a lightweight Edge-Conditioned Convolution which addresses vanishing gradient and over-parameterization issues of this particular graph convolution. Extensive experiments show state-of-the-art performance with improved qualitative and quantitative results on both synthetic Gaussian noise and real noise.



### Fast and robust detection of solar modules in electroluminescence images
- **Arxiv ID**: http://arxiv.org/abs/1907.08451v1
- **DOI**: 10.1007/978-3-030-29891-3_46
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.08451v1)
- **Published**: 2019-07-19 10:32:35+00:00
- **Updated**: 2019-07-19 10:32:35+00:00
- **Authors**: Mathis Hoffmann, Bernd Doll, Florian Talkenberg, Christoph J. Brabec, Andreas K. Maier, Vincent Christlein
- **Comment**: None
- **Journal**: None
- **Summary**: Fast, non-destructive and on-site quality control tools, mainly high sensitive imaging techniques, are important to assess the reliability of photovoltaic plants. To minimize the risk of further damages and electrical yield losses, electroluminescence (EL) imaging is used to detect local defects in an early stage, which might cause future electric losses. For an automated defect recognition on EL measurements, a robust detection and rectification of modules, as well as an optional segmentation into cells is required. This paper introduces a method to detect solar modules and crossing points between solar cells in EL images. We only require 1-D image statistics for the detection, resulting in an approach that is computationally efficient. In addition, the method is able to detect the modules under perspective distortion and in scenarios, where multiple modules are visible in the image. We compare our method to the state of the art and show that it is superior in presence of perspective distortion while the performance on images, where the module is roughly coplanar to the detector, is similar to the reference method. Finally, we show that we greatly improve in terms of computational time in comparison to the reference method.



### Slot Based Image Augmentation System for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1907.12900v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.12900v1)
- **Published**: 2019-07-19 13:38:12+00:00
- **Updated**: 2019-07-19 13:38:12+00:00
- **Authors**: Yingwei Zhou
- **Comment**: preprint draft
- **Journal**: None
- **Summary**: Object Detection has been a significant topic in computer vision. As the continuous development of Deep Learning, many advanced academic and industrial outcomes are established on localising and classifying the target objects, such as instance segmentation, video tracking and robotic vision. As the core concept of Deep Learning, Deep Neural Networks (DNNs) and associated training are highly integrated with task-driven modelling, having great effects on accurate detection. The main focus of improving detection performance is proposing DNNs with extra layers and novel topological connections to extract the desired features from input data. However, training these models can be computationally expensive and laborious progress as the complicated model architecture and enormous parameters. Besides, the dataset is another reason causing this issue and low detection accuracy, because of insufficient data samples or difficult instances. To address these training difficulties, this thesis presents two different approaches to improve the detection performance in the relatively light-weight way. As the intrinsic feature of data-driven in deep learning, the first approach is "slot-based image augmentation" to enrich the dataset with extra foreground and background combinations. Instead of the commonly used image flipping method, the proposed system achieved similar mAP improvement with less extra images which decrease training time. This proposed augmentation system has extra flexibility adapting to various scenarios and the performance-driven analysis provides an alternative aspect of conducting image augmentation



### Matrix cofactorization for joint spatial-spectral unmixing of hyperspectral images
- **Arxiv ID**: http://arxiv.org/abs/1907.08511v2
- **DOI**: 10.1109/TGRS.2020.2968541
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.08511v2)
- **Published**: 2019-07-19 13:43:08+00:00
- **Updated**: 2020-02-14 10:10:41+00:00
- **Authors**: Adrien Lagrange, Mathieu Fauvel, Stéphane May, Nicolas Dobigeon
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral unmixing aims at identifying a set of elementary spectra and the corresponding mixture coefficients for each pixel of an image. As the elementary spectra correspond to the reflectance spectra of real materials, they are often very correlated yielding an ill-conditioned problem. To enrich the model and to reduce ambiguity due to the high correlation, it is common to introduce spatial information to complement the spectral information. The most common way to introduce spatial information is to rely on a spatial regularization of the abundance maps. In this paper, instead of considering a simple but limited regularization process, spatial information is directly incorporated through the newly proposed context of spatial unmixing. Contextual features are extracted for each pixel and this additional set of observations is decomposed according to a linear model. Finally the spatial and spectral observations are unmixed jointly through a cofactorization model. In particular, this model introduces a coupling term used to identify clusters of shared spatial and spectral signatures. An evaluation of the proposed method is conducted on synthetic and real data and shows that results are accurate and also very meaningful since they describe both spatially and spectrally the various areas of the scene.



### Predicting Visual Memory Schemas with Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1907.08514v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.08514v1)
- **Published**: 2019-07-19 13:48:21+00:00
- **Updated**: 2019-07-19 13:48:21+00:00
- **Authors**: Cameron Kyle-Davidson, Adrian Bors, Karla Evans
- **Comment**: Accepted to BMVC2019
- **Journal**: None
- **Summary**: Visual memory schema (VMS) maps show which regions of an image cause that image to be remembered or falsely remembered. Previous work has succeeded in generating low resolution VMS maps using convolutional neural networks. We instead approach this problem as an image-to-image translation task making use of a variational autoencoder. This approach allows us to generate higher resolution dual channel images that represent visual memory schemas, allowing us to evaluate predicted true memorability and false memorability separately. We also evaluate the relationship between VMS maps, predicted VMS maps, ground truth memorability scores, and predicted memorability scores.



### Generating fMRI volumes from T1-weighted volumes using 3D CycleGAN
- **Arxiv ID**: http://arxiv.org/abs/1907.08533v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.08533v2)
- **Published**: 2019-07-19 14:47:18+00:00
- **Updated**: 2019-10-18 16:56:06+00:00
- **Authors**: David Abramian, Anders Eklund
- **Comment**: None
- **Journal**: None
- **Summary**: Registration between an fMRI volume and a T1-weighted volume is challenging, since fMRI volumes contain geometric distortions. Here we present preliminary results showing that 3D CycleGAN can be used to synthesize fMRI volumes from T1-weighted volumes, and vice versa, which can facilitate registration.



### VS-Net: Variable splitting network for accelerated parallel MRI reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1907.10033v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.10033v1)
- **Published**: 2019-07-19 21:04:53+00:00
- **Updated**: 2019-07-19 21:04:53+00:00
- **Authors**: Jinming Duan, Jo Schlemper, Chen Qin, Cheng Ouyang, Wenjia Bai, Carlo Biffi, Ghalib Bello, Ben Statton, Declan P O'Regan, Daniel Rueckert
- **Comment**: Accepted by MICCAI 2019
- **Journal**: None
- **Summary**: In this work, we propose a deep learning approach for parallel magnetic resonance imaging (MRI) reconstruction, termed a variable splitting network (VS-Net), for an efficient, high-quality reconstruction of undersampled multi-coil MR data. We formulate the generalized parallel compressed sensing reconstruction as an energy minimization problem, for which a variable splitting optimization method is derived. Based on this formulation we propose a novel, end-to-end trainable deep neural network architecture by unrolling the resulting iterative process of such variable splitting scheme. VS-Net is evaluated on complex valued multi-coil knee images for 4-fold and 6-fold acceleration factors. We show that VS-Net outperforms state-of-the-art deep learning reconstruction algorithms, in terms of reconstruction accuracy and perceptual quality. Our code is publicly available at https://github.com/j-duan/VS-Net.



### A Multi-Scale Mapping Approach Based on a Deep Learning CNN Model for Reconstructing High-Resolution Urban DEMs
- **Arxiv ID**: http://arxiv.org/abs/1907.12898v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.12898v2)
- **Published**: 2019-07-19 21:20:49+00:00
- **Updated**: 2019-10-08 09:21:58+00:00
- **Authors**: Ling Jiang, Yang Hu, Xilin Xia, Qiuhua Liang, Andrea Soltoggio
- **Comment**: None
- **Journal**: None
- **Summary**: The shortage of high-resolution urban digital elevation model (DEM) datasets has been a challenge for modelling urban flood and managing its risk. A solution is to develop effective approaches to reconstruct high-resolution DEMs from their low-resolution equivalents that are more widely available. However, the current high-resolution DEM reconstruction approaches mainly focus on natural topography. Few attempts have been made for urban topography which is typically an integration of complex man-made and natural features. This study proposes a novel multi-scale mapping approach based on convolutional neural network (CNN) to deal with the complex characteristics of urban topography and reconstruct high-resolution urban DEMs. The proposed multi-scale CNN model is firstly trained using urban DEMs that contain topographic features at different resolutions, and then used to reconstruct the urban DEM at a specified (high) resolution from a low-resolution equivalent. A two-level accuracy assessment approach is also designed to evaluate the performance of the proposed urban DEM reconstruction method, in terms of numerical accuracy and morphological accuracy. The proposed DEM reconstruction approach is applied to a 121 km2 urbanized area in London, UK. Compared with other commonly used methods, the current CNN based approach produces superior results, providing a cost-effective innovative method to acquire high-resolution DEMs in other data-scarce environments.



### Cross-Domain Car Detection Using Unsupervised Image-to-Image Translation: From Day to Night
- **Arxiv ID**: http://arxiv.org/abs/1907.08719v1
- **DOI**: 10.1109/IJCNN.2019.8852008
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.08719v1)
- **Published**: 2019-07-19 22:37:28+00:00
- **Updated**: 2019-07-19 22:37:28+00:00
- **Authors**: Vinicius F. Arruda, Thiago M. Paixão, Rodrigo F. Berriel, Alberto F. De Souza, Claudine Badue, Nicu Sebe, Thiago Oliveira-Santos
- **Comment**: 8 pages, 8 figures,
  https://github.com/viniciusarruda/cross-domain-car-detection and accepted at
  IJCNN 2019
- **Journal**: None
- **Summary**: Deep learning techniques have enabled the emergence of state-of-the-art models to address object detection tasks. However, these techniques are data-driven, delegating the accuracy to the training dataset which must resemble the images in the target task. The acquisition of a dataset involves annotating images, an arduous and expensive process, generally requiring time and manual effort. Thus, a challenging scenario arises when the target domain of application has no annotated dataset available, making tasks in such situation to lean on a training dataset of a different domain. Sharing this issue, object detection is a vital task for autonomous vehicles where the large amount of driving scenarios yields several domains of application requiring annotated data for the training process. In this work, a method for training a car detection system with annotated data from a source domain (day images) without requiring the image annotations of the target domain (night images) is presented. For that, a model based on Generative Adversarial Networks (GANs) is explored to enable the generation of an artificial dataset with its respective annotations. The artificial dataset (fake dataset) is created translating images from day-time domain to night-time domain. The fake dataset, which comprises annotated images of only the target domain (night images), is then used to train the car detector model. Experimental results showed that the proposed method achieved significant and consistent improvements, including the increasing by more than 10% of the detection performance when compared to the training with only the available annotated data (i.e., day images).



