# Arxiv Papers in cs.CV on 2019-07-20
### A Retina-inspired Sampling Method for Visual Texture Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1907.08769v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1907.08769v1)
- **Published**: 2019-07-20 06:54:14+00:00
- **Updated**: 2019-07-20 06:54:14+00:00
- **Authors**: Lin Zhu, Siwei Dong, Tiejun Huang, Yonghong Tian
- **Comment**: Published in ICME 2019
- **Journal**: None
- **Summary**: Conventional frame-based camera is not able to meet the demand of rapid reaction for real-time applications, while the emerging dynamic vision sensor (DVS) can realize high speed capturing for moving objects. However, to achieve visual texture reconstruction, DVS need extra information apart from the output spikes. This paper introduces a fovea-like sampling method inspired by the neuron signal processing in retina, which aims at visual texture reconstruction only taking advantage of the properties of spikes. In the proposed method, the pixels independently respond to the luminance changes with temporal asynchronous spikes. Analyzing the arrivals of spikes makes it possible to restore the luminance information, enabling reconstructing the natural scene for visualization. Three decoding methods of spike stream for texture reconstruction are proposed for high-speed motion and stationary scenes. Compared to conventional frame-based camera and DVS, our model can achieve better image quality and higher flexibility, which is capable of changing the way that demanding machine vision applications are built.



### Inferring Occluded Geometry Improves Performance when Retrieving an Object from Dense Clutter
- **Arxiv ID**: http://arxiv.org/abs/1907.08770v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.08770v2)
- **Published**: 2019-07-20 06:57:52+00:00
- **Updated**: 2019-09-04 20:59:28+00:00
- **Authors**: Andrew Price, Linyi Jin, Dmitry Berenson
- **Comment**: None
- **Journal**: None
- **Summary**: Object search -- the problem of finding a target object in a cluttered scene -- is essential to solve for many robotics applications in warehouse and household environments. However, cluttered environments entail that objects often occlude one another, making it difficult to segment objects and infer their shapes and properties. Instead of relying on the availability of CAD or other explicit models of scene objects, we augment a manipulation planner for cluttered environments with a state-of-the-art deep neural network for shape completion as well as a volumetric memory system, allowing the robot to reason about what may be contained in occluded areas. We test the system in a variety of tabletop manipulation scenes composed of household items, highlighting its applicability to realistic domains. Our results suggest that incorporating both components into a manipulation planning framework significantly reduces the number of actions needed to find a hidden object in dense clutter.



### Evaluation of Distance Measures for Feature based Image Registration using AlexNet
- **Arxiv ID**: http://arxiv.org/abs/1907.12921v1
- **DOI**: 10.14569/IJACSA.2018.091034
- **Categories**: **cs.CV**, cs.DC, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.12921v1)
- **Published**: 2019-07-20 09:36:50+00:00
- **Updated**: 2019-07-20 09:36:50+00:00
- **Authors**: K. Kavitha, B. Thirumala Rao
- **Comment**: None
- **Journal**: None
- **Summary**: Image registration is a classic problem of computer vision with several applications across areas like defence, remote sensing, medicine etc. Feature based image registration methods traditionally used hand-crafted feature extraction algorithms, which detect key points in an image and describe them using a region around the point. Such features are matched using a threshold either on distances or ratio of distances computed between the feature descriptors. Evolution of deep learning, in particular convolution neural networks, has enabled researchers to address several problems of vision such as recognition, tracking, localization etc. Outputs of convolution layers or fully connected layers of CNN which has been trained for applications like visual recognition are proved to be effective when used as features in other applications such as retrieval. In this work, a deep CNN, AlexNet, is used in the place of handcrafted features for feature extraction in the first stage of image registration. However, there is a need to identify a suitable distance measure and a matching method for effective results. Several distance metrics have been evaluated in the framework of nearest neighbour and nearest neighbour ratio matching methods using benchmark dataset. Evaluation is done by comparing matching and registration performance using metrics computed from ground truth.   Keywords: Distance measures; deep learning; feature detection; feature descriptor; image matching



### Unsupervised Separation of Dynamics from Pixels
- **Arxiv ID**: http://arxiv.org/abs/1907.12906v1
- **DOI**: 10.1007/s40300-019-00155-4
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.12906v1)
- **Published**: 2019-07-20 10:22:14+00:00
- **Updated**: 2019-07-20 10:22:14+00:00
- **Authors**: Silvia Chiappa, Ulrich Paquet
- **Comment**: None
- **Journal**: METRON, Springer, 2019
- **Summary**: We present an approach to learn the dynamics of multiple objects from image sequences in an unsupervised way. We introduce a probabilistic model that first generate noisy positions for each object through a separate linear state-space model, and then renders the positions of all objects in the same image through a highly non-linear process. Such a linear representation of the dynamics enables us to propose an inference method that uses exact and efficient inference tools and that can be deployed to query the model in different ways without retraining.



### Pan-tilt-zoom SLAM for Sports Videos
- **Arxiv ID**: http://arxiv.org/abs/1907.08816v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.08816v1)
- **Published**: 2019-07-20 14:25:16+00:00
- **Updated**: 2019-07-20 14:25:16+00:00
- **Authors**: Jikai Lu, Jianhui Chen, James J. Little
- **Comment**: 10+3 pages, BMVC 2019 accepted
- **Journal**: None
- **Summary**: We present an online SLAM system specifically designed to track pan-tilt-zoom (PTZ) cameras in highly dynamic sports such as basketball and soccer games. In these games, PTZ cameras rotate very fast and players cover large image areas. To overcome these challenges, we propose to use a novel camera model for tracking and to use rays as landmarks in mapping. Rays overcome the missing depth in pure-rotation cameras. We also develop an online pan-tilt forest for mapping and introduce moving objects (players) detection to mitigate negative impacts from foreground objects. We test our method on both synthetic and real datasets. The experimental results show the superior performance of our method over previous methods for online PTZ camera pose estimation.



### PH-GCN: Person Re-identification with Part-based Hierarchical Graph Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/1907.08822v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.08822v1)
- **Published**: 2019-07-20 15:18:39+00:00
- **Updated**: 2019-07-20 15:18:39+00:00
- **Authors**: Bo Jiang, Xixi Wang, Bin Luo
- **Comment**: None
- **Journal**: None
- **Summary**: The person re-identification (Re-ID) task requires to robustly extract feature representations for person images. Recently, part-based representation models have been widely studied for extracting the more compact and robust feature representations for person images to improve person Re-ID results. However, existing part-based representation models mostly extract the features of different parts independently which ignore the relationship information between different parts. To overcome this limitation, in this paper we propose a novel deep learning framework, named Part-based Hierarchical Graph Convolutional Network (PH-GCN) for person Re-ID problem. Given a person image, PH-GCN first constructs a hierarchical graph to represent the pairwise relationships among different parts. Then, both local and global feature learning are performed by the messages passing in PH-GCN, which takes other nodes information into account for part feature representation. Finally, a perceptron layer is adopted for the final person part label prediction and re-identification. The proposed framework provides a general solution that integrates local, global and structural feature learning simultaneously in a unified end-to-end network. Extensive experiments on several benchmark datasets demonstrate the effectiveness of the proposed PH-GCN based Re-ID approach.



### Automated Surgical Activity Recognition with One Labeled Sequence
- **Arxiv ID**: http://arxiv.org/abs/1907.08825v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.08825v1)
- **Published**: 2019-07-20 15:26:51+00:00
- **Updated**: 2019-07-20 15:26:51+00:00
- **Authors**: Robert DiPietro, Gregory D. Hager
- **Comment**: Accepted for publication at MICCAI 2019
- **Journal**: None
- **Summary**: Prior work has demonstrated the feasibility of automated activity recognition in robot-assisted surgery from motion data. However, these efforts have assumed the availability of a large number of densely-annotated sequences, which must be provided manually by experts. This process is tedious, expensive, and error-prone. In this paper, we present the first analysis under the assumption of scarce annotations, where as little as one annotated sequence is available for training. We demonstrate feasibility of automated recognition in this challenging setting, and we show that learning representations in an unsupervised fashion, before the recognition phase, leads to significant gains in performance. In addition, our paper poses a new challenge to the community: how much further can we push performance in this important yet relatively unexplored regime?



### Recurrent Connections Aid Occluded Object Recognition by Discounting Occluders
- **Arxiv ID**: http://arxiv.org/abs/1907.08831v2
- **DOI**: 10.1007/978-3-030-30508-6_24
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.08831v2)
- **Published**: 2019-07-20 16:10:04+00:00
- **Updated**: 2019-09-11 09:36:29+00:00
- **Authors**: Markus Roland Ernst, Jochen Triesch, Thomas Burwick
- **Comment**: 13 pages, 5 figures, accepted at the 28th International Conference on
  Artificial Neural Networks, published in Springer Lecture Notes in Computer
  Science vol 11729
- **Journal**: In: Tetko, I. V. et al. (eds.) ICANN 2019. LNCS, vol 11729.
  Springer, Cham, pp 294-305
- **Summary**: Recurrent connections in the visual cortex are thought to aid object recognition when part of the stimulus is occluded. Here we investigate if and how recurrent connections in artificial neural networks similarly aid object recognition. We systematically test and compare architectures comprised of bottom-up (B), lateral (L) and top-down (T) connections. Performance is evaluated on a novel stereoscopic occluded object recognition dataset. The task consists of recognizing one target digit occluded by multiple occluder digits in a pseudo-3D environment. We find that recurrent models perform significantly better than their feedforward counterparts, which were matched in parametric complexity. Furthermore, we analyze how the network's representation of the stimuli evolves over time due to recurrent connections. We show that the recurrent connections tend to move the network's representation of an occluded digit towards its un-occluded version. Our results suggest that both the brain and artificial neural networks can exploit recurrent connectivity to aid occluded object recognition.



### Direct Quantification for Coronary Artery Stenosis Using Multiview Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.10032v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.10032v3)
- **Published**: 2019-07-20 16:12:18+00:00
- **Updated**: 2020-08-08 02:51:47+00:00
- **Authors**: Dong Zhang, Guang Yang, Shu Zhao, Yanping Zhang, Heye Zhang, Shuo Li
- **Comment**: Refine some text in paper
- **Journal**: None
- **Summary**: The quantification of the coronary artery stenosis is of significant clinical importance in coronary artery disease diagnosis and intervention treatment. It aims to quantify the morphological indices of the coronary artery lesions such as minimum lumen diameter, reference vessel diameter, lesion length, and these indices are the reference of the interventional stent placement. In this study, we propose a direct multiview quantitative coronary angiography (DMQCA) model as an automatic clinical tool to quantify the coronary artery stenosis from X-ray coronary angiography images. The proposed DMQCA model consists of a multiview module with two attention mechanisms, a key-frame module, and a regression module, to achieve direct accurate multiple-index estimation. The multi-view module comprehensively learns the Spatio-temporal features of coronary arteries through a three-dimensional convolution. The attention mechanisms of each view focus on the subtle feature of the lesion region and capture the important context information. The key-frame module learns the subtle features of the stenosis through successive dilated residual blocks. The regression module finally generates the indices estimation from multiple features.



### Order Matters: Shuffling Sequence Generation for Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/1907.08845v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.08845v1)
- **Published**: 2019-07-20 17:25:25+00:00
- **Updated**: 2019-07-20 17:25:25+00:00
- **Authors**: Junyan Wang, Bingzhang Hu, Yang Long, Yu Guan
- **Comment**: This manuscript has been accepted at BMVC 2019. See the project at
  https://github.com/andrewjywang/SEENet
- **Journal**: None
- **Summary**: Predicting future frames in natural video sequences is a new challenge that is receiving increasing attention in the computer vision community. However, existing models suffer from severe loss of temporal information when the predicted sequence is long. Compared to previous methods focusing on generating more realistic contents, this paper extensively studies the importance of sequential order information for video generation. A novel Shuffling sEquence gEneration network (SEE-Net) is proposed that can learn to discriminate unnatural sequential orders by shuffling the video frames and comparing them to the real video sequence. Systematic experiments on three datasets with both synthetic and real-world videos manifest the effectiveness of shuffling sequence generation for video prediction in our proposed model and demonstrate state-of-the-art performance by both qualitative and quantitative evaluations. The source code is available at https://github.com/andrewjywang/SEENet.



### Unsupervised Segmentation of Hyperspectral Images Using 3D Convolutional Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1907.08870v1
- **DOI**: 10.1109/LGRS.2019.2960945
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.08870v1)
- **Published**: 2019-07-20 22:17:10+00:00
- **Updated**: 2019-07-20 22:17:10+00:00
- **Authors**: Jakub Nalepa, Michal Myller, Yasuteru Imai, Ken-ichi Honda, Tomomi Takeda, Marek Antoniak
- **Comment**: Submitted to IEEE Geoscience and Remote Sensing Letters
- **Journal**: None
- **Summary**: Hyperspectral image analysis has become an important topic widely researched by the remote sensing community. Classification and segmentation of such imagery help understand the underlying materials within a scanned scene, since hyperspectral images convey a detailed information captured in a number of spectral bands. Although deep learning has established the state of the art in the field, it still remains challenging to train well-generalizing models due to the lack of ground-truth data. In this letter, we tackle this problem and propose an end-to-end approach to segment hyperspectral images in a fully unsupervised way. We introduce a new deep architecture which couples 3D convolutional autoencoders with clustering. Our multi-faceted experimental study---performed over benchmark and real-life data---revealed that our approach delivers high-quality segmentation without any prior class labels.



### Construct Dynamic Graphs for Hand Gesture Recognition via Spatial-Temporal Attention
- **Arxiv ID**: http://arxiv.org/abs/1907.08871v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.08871v1)
- **Published**: 2019-07-20 22:24:01+00:00
- **Updated**: 2019-07-20 22:24:01+00:00
- **Authors**: Yuxiao Chen, Long Zhao, Xi Peng, Jianbo Yuan, Dimitris N. Metaxas
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a Dynamic Graph-Based Spatial-Temporal Attention (DG-STA) method for hand gesture recognition. The key idea is to first construct a fully-connected graph from a hand skeleton, where the node features and edges are then automatically learned via a self-attention mechanism that performs in both spatial and temporal domains. We further propose to leverage the spatial-temporal cues of joint positions to guarantee robust recognition in challenging conditions. In addition, a novel spatial-temporal mask is applied to significantly cut down the computational cost by 99%. We carry out extensive experiments on benchmarks (DHG-14/28 and SHREC'17) and prove the superior performance of our method compared with the state-of-the-art methods. The source code can be found at https://github.com/yuxiaochen1103/DG-STA.



### Human Extraction and Scene Transition utilizing Mask R-CNN
- **Arxiv ID**: http://arxiv.org/abs/1907.08884v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1907.08884v2)
- **Published**: 2019-07-20 23:57:46+00:00
- **Updated**: 2019-10-26 14:31:25+00:00
- **Authors**: Asati Minkesh, Kraittipong Worranitta, Miyachi Taizo
- **Comment**: 6 pages, 10 figures
- **Journal**: None
- **Summary**: Object detection is a trendy branch of computer vision, especially on human recognition and pedestrian detection. Recognizing the complete body of a person has always been a difficult problem. Over the years, researchers proposed various methods, and recently, Mask R-CNN has made a breakthrough for instance segmentation. Based on Faster R-CNN, Mask R-CNN has been able to generate a segmentation mask for each instance. We propose an application to extracts multiple persons from images and videos for pleasant life scenes to grouping happy moments of people such as family or friends and a community for QOL (Quality Of Life). We likewise propose a methodology to put extracted images of persons into the new background. This enables a user to make a pleasant collection of happy facial expressions and actions of his/her family and friends in his/her life. Mask R-CNN detects all types of object masks from images. Then our algorithm considers only the target person and extracts a person only without obstacles, such as dogs in front of the person, and the user also can select multiple persons as their expectations. Our algorithm is effective for both an image and a video irrespective of the length of it. Our algorithm does not add any overhead to Mask R-CNN, running at 5 fps. We show examples of yoga-person in an image and a dancer in a dance-video frame. We hope our simple and effective approach would serve as a baseline for replacing the image background and help ease future research.



