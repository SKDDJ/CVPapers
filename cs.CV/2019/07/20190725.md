# Arxiv Papers in cs.CV on 2019-07-25
### Min-max Entropy for Weakly Supervised Pointwise Localization
- **Arxiv ID**: http://arxiv.org/abs/1907.12934v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.12934v5)
- **Published**: 2019-07-25 00:51:18+00:00
- **Updated**: 2021-01-21 04:08:10+00:00
- **Authors**: Soufiane Belharbi, Jérôme Rony, Jose Dolz, Ismail Ben Ayed, Luke McCaffrey, Eric Granger
- **Comment**: 27 pages, 15 figures
- **Journal**: None
- **Summary**: Pointwise localization allows more precise localization and accurate interpretability, compared to bounding box, in applications where objects are highly unstructured such as in medical domain. In this work, we focus on weakly supervised localization (WSL) where a model is trained to classify an image and localize regions of interest at pixel-level using only global image annotation. Typical convolutional attentions maps are prune to high false positive regions. To alleviate this issue, we propose a new deep learning method for WSL, composed of a localizer and a classifier, where the localizer is constrained to determine relevant and irrelevant regions using conditional entropy (CE) with the aim to reduce false positive regions. Experimental results on a public medical dataset and two natural datasets, using Dice index, show that, compared to state of the art WSL methods, our proposal can provide significant improvements in terms of image-level classification and pixel-level localization (low false positive) with robustness to overfitting. A public reproducible PyTorch implementation is provided in: https://github.com/sbelharbi/wsol-min-max-entropy-interpretability .



### Interpreting the Latent Space of GANs for Semantic Face Editing
- **Arxiv ID**: http://arxiv.org/abs/1907.10786v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10786v3)
- **Published**: 2019-07-25 01:30:16+00:00
- **Updated**: 2020-03-31 10:24:42+00:00
- **Authors**: Yujun Shen, Jinjin Gu, Xiaoou Tang, Bolei Zhou
- **Comment**: CVPR2020 camera-ready
- **Journal**: None
- **Summary**: Despite the recent advance of Generative Adversarial Networks (GANs) in high-fidelity image synthesis, there lacks enough understanding of how GANs are able to map a latent code sampled from a random distribution to a photo-realistic image. Previous work assumes the latent space learned by GANs follows a distributed representation but observes the vector arithmetic phenomenon. In this work, we propose a novel framework, called InterFaceGAN, for semantic face editing by interpreting the latent semantics learned by GANs. In this framework, we conduct a detailed study on how different semantics are encoded in the latent space of GANs for face synthesis. We find that the latent code of well-trained generative models actually learns a disentangled representation after linear transformations. We explore the disentanglement between various semantics and manage to decouple some entangled semantics with subspace projection, leading to more precise control of facial attributes. Besides manipulating gender, age, expression, and the presence of eyeglasses, we can even vary the face pose as well as fix the artifacts accidentally generated by GAN models. The proposed method is further applied to achieve real image manipulation when combined with GAN inversion methods or some encoder-involved models. Extensive results suggest that learning to synthesize faces spontaneously brings a disentangled and controllable facial attribute representation.



### Composition-Aware Image Aesthetics Assessment
- **Arxiv ID**: http://arxiv.org/abs/1907.10801v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10801v1)
- **Published**: 2019-07-25 02:22:16+00:00
- **Updated**: 2019-07-25 02:22:16+00:00
- **Authors**: Dong Liu, Rohit Puri, Nagendra Kamath, Subhabrata Bhattachary
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic image aesthetics assessment is important for a wide variety of applications such as on-line photo suggestion, photo album management and image retrieval. Previous methods have focused on mapping the holistic image content to a high or low aesthetics rating. However, the composition information of an image characterizes the harmony of its visual elements according to the principles of art, and provides richer information for learning aesthetics. In this work, we propose to model the image composition information as the mutual dependency of its local regions, and design a novel architecture to leverage such information to boost the performance of aesthetics assessment. To achieve this, we densely partition an image into local regions and compute aesthetics-preserving features over the regions to characterize the aesthetics properties of image content. With the feature representation of local regions, we build a region composition graph in which each node denotes one region and any two nodes are connected by an edge weighted by the similarity of the region features. We perform reasoning on this graph via graph convolution, in which the activation of each node is determined by its highly correlated neighbors. Our method naturally uncovers the mutual dependency of local regions in the network training procedure, and achieves the state-of-the-art performance on the benchmark visual aesthetics datasets.



### Co-Evolutionary Compression for Unpaired Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1907.10804v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.10804v1)
- **Published**: 2019-07-25 02:26:14+00:00
- **Updated**: 2019-07-25 02:26:14+00:00
- **Authors**: Han Shu, Yunhe Wang, Xu Jia, Kai Han, Hanting Chen, Chunjing Xu, Qi Tian, Chang Xu
- **Comment**: Accepted by ICCV 2019
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) have been successfully used for considerable computer vision tasks, especially the image-to-image translation. However, generators in these networks are of complicated architectures with large number of parameters and huge computational complexities. Existing methods are mainly designed for compressing and speeding-up deep neural networks in the classification task, and cannot be directly applied on GANs for image translation, due to their different objectives and training procedures. To this end, we develop a novel co-evolutionary approach for reducing their memory usage and FLOPs simultaneously. In practice, generators for two image domains are encoded as two populations and synergistically optimized for investigating the most important convolution filters iteratively. Fitness of each individual is calculated using the number of parameters, a discriminator-aware regularization, and the cycle consistency. Extensive experiments conducted on benchmark datasets demonstrate the effectiveness of the proposed method for obtaining compact and effective generators.



### Self-Supervised Adaptation of High-Fidelity Face Models for Monocular Performance Tracking
- **Arxiv ID**: http://arxiv.org/abs/1907.10815v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10815v1)
- **Published**: 2019-07-25 03:20:21+00:00
- **Updated**: 2019-07-25 03:20:21+00:00
- **Authors**: Jae Shin Yoon, Takaaki Shiratori, Shoou-I Yu, Hyun Soo Park
- **Comment**: This work is accepted by CVPR 2019
- **Journal**: None
- **Summary**: Improvements in data-capture and face modeling techniques have enabled us to create high-fidelity realistic face models. However, driving these realistic face models requires special input data, e.g. 3D meshes and unwrapped textures. Also, these face models expect clean input data taken under controlled lab environments, which is very different from data collected in the wild. All these constraints make it challenging to use the high-fidelity models in tracking for commodity cameras. In this paper, we propose a self-supervised domain adaptation approach to enable the animation of high-fidelity face models from a commodity camera. Our approach first circumvents the requirement for special input data by training a new network that can directly drive a face model just from a single 2D image. Then, we overcome the domain mismatch between lab and uncontrolled environments by performing self-supervised domain adaptation based on "consecutive frame texture consistency" based on the assumption that the appearance of the face is consistent over consecutive frames, avoiding the necessity of modeling the new environment such as lighting or background. Experiments show that we are able to drive a high-fidelity face model to perform complex facial motion from a cellphone camera without requiring any labeled data from the new domain.



### U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1907.10830v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.10830v4)
- **Published**: 2019-07-25 04:17:25+00:00
- **Updated**: 2020-04-08 15:36:10+00:00
- **Authors**: Junho Kim, Minjae Kim, Hyeonwoo Kang, Kwanghee Lee
- **Comment**: Accepted to ICLR 2020
- **Journal**: None
- **Summary**: We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters. Our code and datasets are available at https://github.com/taki0112/UGATIT or https://github.com/znxlwm/UGATIT-pytorch.



### Safe Feature Elimination for Non-Negativity Constrained Convex Optimization
- **Arxiv ID**: http://arxiv.org/abs/1907.10831v2
- **DOI**: 10.1007/s10957-019-01612-w
- **Categories**: **math.OC**, cs.CV, 49N15, 90C25, 90C46
- **Links**: [PDF](http://arxiv.org/pdf/1907.10831v2)
- **Published**: 2019-07-25 04:24:20+00:00
- **Updated**: 2019-11-17 18:43:20+00:00
- **Authors**: James Folberth, Stephen Becker
- **Comment**: v2: added pointer to version to appear in Journal of Optimization
  Theory and Applications
- **Journal**: None
- **Summary**: Inspired by recent work on safe feature elimination for $1$-norm regularized least-squares, we develop strategies to eliminate features from convex optimization problems with non-negativity constraints. Our strategy is safe in the sense that it will only remove features/coordinates from the problem when they are guaranteed to be zero at a solution. To perform feature elimination we use an accurate, but not optimal, primal-dual feasible pair, making our methods robust and able to be used on ill-conditioned problems. We supplement our feature elimination problem with a method to construct an accurate dual feasible point from an accurate primal feasible point; this allows us to use a first-order method to find an accurate primal feasible point, then use that point to construct an accurate dual feasible point and perform feature elimination. Under reasonable conditions, our feature elimination strategy will eventually eliminate all zero features from the problem. As an application of our methods we show how safe feature elimination can be used to robustly certify the uniqueness of non-negative least-squares (NNLS) problems. We give numerical examples on a well-conditioned synthetic NNLS problem and a on set of 40000 extremely ill-conditioned NNLS problems arising in a microscopy application.



### Submission to ActivityNet Challenge 2019: Task B Spatio-temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/1907.10837v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.10837v1)
- **Published**: 2019-07-25 04:48:10+00:00
- **Updated**: 2019-07-25 04:48:10+00:00
- **Authors**: Chunfei Ma, Joonhyang Choi, Byeongwon Lee, Seungji Yang
- **Comment**: 4 pages, 2 fighures
- **Journal**: None
- **Summary**: This technical report present an overview of our system proposed for the spatio-temporal action localization(SAL) task in ActivityNet Challenge 2019. Unlike previous two-streams-based works, we focus on exploring the end-to-end trainable architecture using only RGB sequential images. To this end, we employ a previously proposed simple yet effective two-branches network called SlowFast Networks which is capable of capturing both short- and long-term spatiotemporal features. Moreover, to handle the severe class imbalance and overfitting problems, we propose a correlation-preserving data augmentation method and a random label subsampling method which have been proven to be able to reduce overfitting and improve the performance.



### A Fine-Grained Facial Expression Database for End-to-End Multi-Pose Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.10838v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10838v1)
- **Published**: 2019-07-25 04:58:33+00:00
- **Updated**: 2019-07-25 04:58:33+00:00
- **Authors**: Wenxuan Wang, Qiang Sun, Tao Chen, Chenjie Cao, Ziqi Zheng, Guoqiang Xu, Han Qiu, Yanwei Fu
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: The recent research of facial expression recognition has made a lot of progress due to the development of deep learning technologies, but some typical challenging problems such as the variety of rich facial expressions and poses are still not resolved. To solve these problems, we develop a new Facial Expression Recognition (FER) framework by involving the facial poses into our image synthesizing and classification process. There are two major novelties in this work. First, we create a new facial expression dataset of more than 200k images with 119 persons, 4 poses and 54 expressions. To our knowledge this is the first dataset to label faces with subtle emotion changes for expression recognition purpose. It is also the first dataset that is large enough to validate the FER task on unbalanced poses, expressions, and zero-shot subject IDs. Second, we propose a facial pose generative adversarial network (FaPE-GAN) to synthesize new facial expression images to augment the data set for training purpose, and then learn a LightCNN based Fa-Net model for expression classification. Finally, we advocate four novel learning tasks on this dataset. The experimental results well validate the effectiveness of the proposed approach.



### Hard-Aware Fashion Attribute Classification
- **Arxiv ID**: http://arxiv.org/abs/1907.10839v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.10839v1)
- **Published**: 2019-07-25 05:02:02+00:00
- **Updated**: 2019-07-25 05:02:02+00:00
- **Authors**: Yun Ye, Yixin Li, Bo Wu, Wei Zhang, Lingyu Duan, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: Fashion attribute classification is of great importance to many high-level tasks such as fashion item search, fashion trend analysis, fashion recommendation, etc. The task is challenging due to the extremely imbalanced data distribution, particularly the attributes with only a few positive samples. In this paper, we introduce a hard-aware pipeline to make full use of "hard" samples/attributes. We first propose Hard-Aware BackPropagation (HABP) to efficiently and adaptively focus on training "hard" data. Then for the identified hard labels, we propose to synthesize more complementary samples for training. To stabilize training, we extend semi-supervised GAN by directly deactivating outputs for synthetic complementary samples (Deact). In general, our method is more effective in addressing "hard" cases. HABP weights more on "hard" samples. For "hard" attributes with insufficient training data, Deact brings more stable synthetic samples for training and further improve the performance. Our method is verified on large scale fashion dataset, outperforming other state-of-the-art without any additional supervisions.



### Learning Resolution-Invariant Deep Representations for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1907.10843v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.10843v1)
- **Published**: 2019-07-25 05:24:05+00:00
- **Updated**: 2019-07-25 05:24:05+00:00
- **Authors**: Yun-Chun Chen, Yu-Jhe Li, Xiaofei Du, Yu-Chiang Frank Wang
- **Comment**: Accepted to AAAI 2019 (Oral)
- **Journal**: None
- **Summary**: Person re-identification (re-ID) solves the task of matching images across cameras and is among the research topics in vision community. Since query images in real-world scenarios might suffer from resolution loss, how to solve the resolution mismatch problem during person re-ID becomes a practical problem. Instead of applying separate image super-resolution models, we propose a novel network architecture of Resolution Adaptation and re-Identification Network (RAIN) to solve cross-resolution person re-ID. Advancing the strategy of adversarial learning, we aim at extracting resolution-invariant representations for re-ID, while the proposed model is learned in an end-to-end training fashion. Our experiments confirm that the use of our model can recognize low-resolution query images, even if the resolution is not seen during training. Moreover, the extension of our model for semi-supervised re-ID further confirms the scalability of our proposed method for real-world scenarios and applications.



### PU-GAN: a Point Cloud Upsampling Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1907.10844v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10844v1)
- **Published**: 2019-07-25 05:28:56+00:00
- **Updated**: 2019-07-25 05:28:56+00:00
- **Authors**: Ruihui Li, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, Pheng-Ann Heng
- **Comment**: accepted by ICCV2019, project page at
  https://liruihui.github.io/publication/PU-GAN
- **Journal**: None
- **Summary**: Point clouds acquired from range scans are often sparse, noisy, and non-uniform. This paper presents a new point cloud upsampling network called PU-GAN, which is formulated based on a generative adversarial network (GAN), to learn a rich variety of point distributions from the latent space and upsample points over patches on object surfaces. To realize a working GAN network, we construct an up-down-up expansion unit in the generator for upsampling point features with error feedback and self-correction, and formulate a self-attention unit to enhance the feature integration. Further, we design a compound loss with adversarial, uniform and reconstruction terms, to encourage the discriminator to learn more latent patterns and enhance the output point distribution uniformity. Qualitative and quantitative evaluations demonstrate the quality of our results over the state-of-the-arts in terms of distribution uniformity, proximity-to-surface, and 3D reconstruction quality.



### A Compact Light Field Camera for Real-Time Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1907.10880v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10880v1)
- **Published**: 2019-07-25 07:48:11+00:00
- **Updated**: 2019-07-25 07:48:11+00:00
- **Authors**: Yuriy Anisimov, Oliver Wasenmüller, Didier Stricker
- **Comment**: The 18th International Conference on Computer Analysis of Images and
  Patterns, Salerno, 2-6 September, 2019
- **Journal**: None
- **Summary**: Depth cameras are utilized in many applications. Recently light field approaches are increasingly being used for depth computation. While these approaches demonstrate the technical feasibility, they can not be brought into real-world application, since they have both a high computation time as well as a large design. Exactly these two drawbacks are overcome in this paper. For the first time, we present a depth camera based on the light field principle, which provides real-time depth information as well as a compact design.



### Interpretability Beyond Classification Output: Semantic Bottleneck Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.10882v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.10882v2)
- **Published**: 2019-07-25 07:53:00+00:00
- **Updated**: 2019-07-28 11:40:03+00:00
- **Authors**: Max Losch, Mario Fritz, Bernt Schiele
- **Comment**: Correct figures in appendix
- **Journal**: None
- **Summary**: Today's deep learning systems deliver high performance based on end-to-end training. While they deliver strong performance, these systems are hard to interpret. To address this issue, we propose Semantic Bottleneck Networks (SBN): deep networks with semantically interpretable intermediate layers that all downstream results are based on. As a consequence, the analysis on what the final prediction is based on is transparent to the engineer and failure cases and modes can be analyzed and avoided by high-level reasoning. We present a case study on street scene segmentation to demonstrate the feasibility and power of SBN. In particular, we start from a well performing classic deep network which we adapt to house a SB-Layer containing task related semantic concepts (such as object-parts and materials). Importantly, we can recover state of the art performance despite a drastic dimensionality reduction from 1000s (non-semantic feature) to 10s (semantic concept) channels. Additionally we show how the activations of the SB-Layer can be used for both the interpretation of failure cases of the network as well as for confidence prediction of the resulting output. For the first time, e.g., we show interpretable segmentation results for most predictions at over 99% accuracy.



### Performance Evaluation of Two-layer lossless HDR Coding using Histogram Packing Technique under Various Tone-mapping Operators
- **Arxiv ID**: http://arxiv.org/abs/1907.10889v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.10889v1)
- **Published**: 2019-07-25 08:08:13+00:00
- **Updated**: 2019-07-25 08:08:13+00:00
- **Authors**: Hiroyuki Kobayashi, Hitoshi Kiya
- **Comment**: to appear in 2019 IEEE 8th Global Conference on Consumer Electronics
- **Journal**: None
- **Summary**: We proposed a lossless two-layer HDR coding method using a histogram packing technique. The proposed method was demonstrated to outperform the normative JPEG XT encoder, under the use of the default tone-mapping operator. However, the performance under various tone-mapping operators has not been discussed. In this paper, we aim to compare the performance of the proposed method with that of the JPEG XT encoder under the use of various tone-mapping operators to clearly show the characteristic difference between them.



### Simultaneous multi-view instance detection with learned geometric soft-constraints
- **Arxiv ID**: http://arxiv.org/abs/1907.10892v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.10892v1)
- **Published**: 2019-07-25 08:11:22+00:00
- **Updated**: 2019-07-25 08:11:22+00:00
- **Authors**: Ahmed Samy Nassar, Sebastien Lefevre, Jan D. Wegner
- **Comment**: Internationcal Conference on Computer Vision 2019 (ICCV 19)
- **Journal**: None
- **Summary**: We propose to jointly learn multi-view geometry and warping between views of the same object instances for robust cross-view object detection. What makes multi-view object instance detection difficult are strong changes in viewpoint, lighting conditions, high similarity of neighbouring objects, and strong variability in scale. By turning object detection and instance re-identification in different views into a joint learning task, we are able to incorporate both image appearance and geometric soft constraints into a single, multi-view detection process that is learnable end-to-end. We validate our method on a new, large data set of street-level panoramas of urban objects and show superior performance compared to various baselines. Our contribution is threefold: a large-scale, publicly available data set for multi-view instance detection and re-identification; an annotation tool custom-tailored for multi-view instance detection; and a novel, holistic multi-view instance detection and re-identification method that jointly models geometry and appearance across views.



### How to Manipulate CNNs to Make Them Lie: the GradCAM Case
- **Arxiv ID**: http://arxiv.org/abs/1907.10901v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.10901v2)
- **Published**: 2019-07-25 08:51:08+00:00
- **Updated**: 2019-08-16 14:12:21+00:00
- **Authors**: Tom Viering, Ziqi Wang, Marco Loog, Elmar Eisemann
- **Comment**: Presented at BMVC 2019: Workshop on Interpretable and Explainable
  Machine Vision, Cardiff, UK. Updated to BMVC template
- **Journal**: None
- **Summary**: Recently many methods have been introduced to explain CNN decisions. However, it has been shown that some methods can be sensitive to manipulation of the input. We continue this line of work and investigate the explanation method GradCAM. Instead of manipulating the input, we consider an adversary that manipulates the model itself to attack the explanation. By changing weights and architecture, we demonstrate that it is possible to generate any desired explanation, while leaving the model's accuracy essentially unchanged. This illustrates that GradCAM cannot explain the decision of every CNN and provides a proof of concept showing that it is possible to obfuscate the inner workings of a CNN. Finally, we combine input and model manipulation. To this end we put a backdoor in the network: the explanation is correct unless there is a specific pattern present in the input, which triggers a malicious explanation. Our work raises new security concerns, especially in settings where explanations of models may be used to make decisions, such as in the medical domain.



### Self-supervised Domain Adaptation for Computer Vision Tasks
- **Arxiv ID**: http://arxiv.org/abs/1907.10915v3
- **DOI**: 10.1109/ACCESS.2019.2949697
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.10915v3)
- **Published**: 2019-07-25 09:20:29+00:00
- **Updated**: 2019-12-10 23:55:34+00:00
- **Authors**: Jiaolong Xu, Liang Xiao, Antonio M. Lopez
- **Comment**: Accepted by IEEE Access
- **Journal**: IEEE Access. 7 (2019) 156694-156706
- **Summary**: Recent progress of self-supervised visual representation learning has achieved remarkable success on many challenging computer vision benchmarks. However, whether these techniques can be used for domain adaptation has not been explored. In this work, we propose a generic method for self-supervised domain adaptation, using object recognition and semantic segmentation of urban scenes as use cases. Focusing on simple pretext/auxiliary tasks (e.g. image rotation prediction), we assess different learning strategies to improve domain adaptation effectiveness by self-supervision. Additionally, we propose two complementary strategies to further boost the domain adaptation accuracy on semantic segmentation within our method, consisting of prediction layer alignment and batch normalization calibration. The experimental results show adaptation levels comparable to most studied domain adaptation methods, thus, bringing self-supervision as a new alternative for reaching domain adaptation. The code is available at https://github.com/Jiaolong/self-supervised-da.



### Closing the Gap between Deep and Conventional Image Registration using Probabilistic Dense Displacement Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.10931v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.10931v1)
- **Published**: 2019-07-25 09:44:12+00:00
- **Updated**: 2019-07-25 09:44:12+00:00
- **Authors**: Mattias P. Heinrich
- **Comment**: accepted for publication at MICCAI 2019, open source code available
  at https://github.com/multimodallearning/pdd_net
- **Journal**: None
- **Summary**: Nonlinear image registration continues to be a fundamentally important tool in medical image analysis. Diagnostic tasks, image-guided surgery and radiotherapy as well as motion analysis all rely heavily on accurate intra-patient alignment. Furthermore, inter-patient registration enables atlas-based segmentation or landmark localisation and shape analysis. When labelled scans are scarce and anatomical differences large, conventional registration has often remained superior to deep learning methods that have so far mainly dealt with relatively small or low-complexity deformations. We address this shortcoming by leveraging ideas from probabilistic dense displacement optimisation that has excelled in many registration tasks with large deformations. We propose to design a network with approximate min-convolutions and mean field inference for differentiable displacement regularisation within a discrete weakly-supervised registration setting. By employing these meaningful and theoretically proven constraints, our learnable registration algorithm contains very few trainable weights (primarily for feature extraction) and is easier to train with few labelled scans. It is very fast in training and inference and achieves state-of-the-art accuracies for the challenging inter-patient registration of abdominal CT outperforming previous deep learning approaches by 15% Dice overlap.



### Convolutional Neural Networks on Randomized Data
- **Arxiv ID**: http://arxiv.org/abs/1907.10935v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10935v1)
- **Published**: 2019-07-25 09:59:27+00:00
- **Updated**: 2019-07-25 09:59:27+00:00
- **Authors**: Cristian Ivan
- **Comment**: 8 pages, 17 figures, presented at Deep-Vision workshop, CVPR 2019
- **Journal**: Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition Workshops, pp. 1-8. 2019
- **Summary**: Convolutional Neural Networks (CNNs) are build specifically for computer vision tasks for which it is known that the input data is a hierarchical structure based on locally correlated elements. The question that naturally arises is what happens with the performance of CNNs if one of the basic properties of the data is removed, e.g. what happens if the image pixels are randomly permuted? Intuitively one expects that the convolutional network performs poorly in these circumstances in contrast to a multilayer perceptron (MLPs) whose classification accuracy should not be affected by the pixel randomization. This work shows that by randomizing image pixels the hierarchical structure of the data is destroyed and long range correlations are introduced which standard CNNs are not able to capture. We show that their classification accuracy is heavily dependent on the class similarities as well as the pixel randomization process. We also indicate that dilated convolutions are able to recover some of the pixel correlations and improve the performance.



### ET-Net: A Generic Edge-aTtention Guidance Network for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.10936v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10936v1)
- **Published**: 2019-07-25 10:00:08+00:00
- **Updated**: 2019-07-25 10:00:08+00:00
- **Authors**: Zhijie Zhang, Huazhu Fu, Hang Dai, Jianbing Shen, Yanwei Pang, Ling Shao
- **Comment**: MICCAI 2019
- **Journal**: None
- **Summary**: Segmentation is a fundamental task in medical image analysis. However, most existing methods focus on primary region extraction and ignore edge information, which is useful for obtaining accurate segmentation. In this paper, we propose a generic medical segmentation method, called Edge-aTtention guidance Network (ET-Net), which embeds edge-attention representations to guide the segmentation network. Specifically, an edge guidance module is utilized to learn the edge-attention representations in the early encoding layers, which are then transferred to the multi-scale decoding layers, fused using a weighted aggregation module. The experimental results on four segmentation tasks (i.e., optic disc/cup and vessel segmentation in retinal images, and lung segmentation in chest X-Ray and CT images) demonstrate that preserving edge-attention representations contributes to the final segmentation accuracy, and our proposed method outperforms current state-of-the-art segmentation methods. The source code of our method is available at https://github.com/ZzzJzzZ/ETNet.



### Y-Autoencoders: disentangling latent representations via sequential-encoding
- **Arxiv ID**: http://arxiv.org/abs/1907.10949v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.10949v1)
- **Published**: 2019-07-25 10:28:15+00:00
- **Updated**: 2019-07-25 10:28:15+00:00
- **Authors**: Massimiliano Patacchiola, Patrick Fox-Roberts, Edward Rosten
- **Comment**: None
- **Journal**: None
- **Summary**: In the last few years there have been important advancements in generative models with the two dominant approaches being Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). However, standard Autoencoders (AEs) and closely related structures have remained popular because they are easy to train and adapt to different tasks. An interesting question is if we can achieve state-of-the-art performance with AEs while retaining their good properties. We propose an answer to this question by introducing a new model called Y-Autoencoder (Y-AE). The structure and training procedure of a Y-AE enclose a representation into an implicit and an explicit part. The implicit part is similar to the output of an autoencoder and the explicit part is strongly correlated with labels in the training set. The two parts are separated in the latent space by splitting the output of the encoder into two paths (forming a Y shape) before decoding and re-encoding. We then impose a number of losses, such as reconstruction loss, and a loss on dependence between the implicit and explicit parts. Additionally, the projection in the explicit manifold is monitored by a predictor, that is embedded in the encoder and trained end-to-end with no adversarial losses. We provide significant experimental results on various domains, such as separation of style and content, image-to-image translation, and inverse graphics.



### Cross Attention Network for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.10958v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10958v1)
- **Published**: 2019-07-25 10:46:03+00:00
- **Updated**: 2019-07-25 10:46:03+00:00
- **Authors**: Mengyu Liu, Hujun Yin
- **Comment**: Accepted in ICIP 2019
- **Journal**: None
- **Summary**: In this paper, we address the semantic segmentation task with a deep network that combines contextual features and spatial information. The proposed Cross Attention Network is composed of two branches and a Feature Cross Attention (FCA) module. Specifically, a shallow branch is used to preserve low-level spatial information and a deep branch is employed to extract high-level contextual features. Then the FCA module is introduced to combine these two branches. Different from most existing attention mechanisms, the FCA module obtains spatial attention map and channel attention map from two branches separately, and then fuses them. The contextual features are used to provide global contextual guidance in fused feature maps, and spatial features are used to refine localizations. The proposed network outperforms other real-time methods with improved speed on the Cityscapes and CamVid datasets with lightweight backbones, and achieves state-of-the-art performance with a deep backbone.



### Is Texture Predictive for Age and Sex in Brain MRI?
- **Arxiv ID**: http://arxiv.org/abs/1907.10961v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.10961v1)
- **Published**: 2019-07-25 10:53:28+00:00
- **Updated**: 2019-07-25 10:53:28+00:00
- **Authors**: Nick Pawlowski, Ben Glocker
- **Comment**: MIDL 2019 [arXiv:1907.08612]
- **Journal**: None
- **Summary**: Deep learning builds the foundation for many medical image analysis tasks where neuralnetworks are often designed to have a large receptive field to incorporate long spatialdependencies. Recent work has shown that large receptive fields are not always necessaryfor computer vision tasks on natural images. We explore whether this translates to certainmedical imaging tasks such as age and sex prediction from a T1-weighted brain MRI scans.



### Learning Transparent Object Matting
- **Arxiv ID**: http://arxiv.org/abs/1907.11544v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.11544v1)
- **Published**: 2019-07-25 11:14:46+00:00
- **Updated**: 2019-07-25 11:14:46+00:00
- **Authors**: Guanying Chen, Kai Han, Kwan-Yee K. Wong
- **Comment**: To appear in International Journal of Computer Vision, Project Page:
  https://guanyingc.github.io/TOM-Net. arXiv admin note: substantial text
  overlap with arXiv:1803.04636
- **Journal**: None
- **Summary**: This paper addresses the problem of image matting for transparent objects. Existing approaches often require tedious capturing procedures and long processing time, which limit their practical use. In this paper, we formulate transparent object matting as a refractive flow estimation problem, and propose a deep learning framework, called TOM-Net, for learning the refractive flow. Our framework comprises two parts, namely a multi-scale encoder-decoder network for producing a coarse prediction, and a residual network for refinement. At test time, TOM-Net takes a single image as input, and outputs a matte (consisting of an object mask, an attenuation mask and a refractive flow field) in a fast feed-forward pass. As no off-the-shelf dataset is available for transparent object matting, we create a large-scale synthetic dataset consisting of $178K$ images of transparent objects rendered in front of images sampled from the Microsoft COCO dataset. We also capture a real dataset consisting of $876$ samples using $14$ transparent objects and $60$ background images. Besides, we show that our method can be easily extended to handle the cases where a trimap or a background image is available.Promising experimental results have been achieved on both synthetic and real data, which clearly demonstrate the effectiveness of our approach.



### Overfitting of neural nets under class imbalance: Analysis and improvements for segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.10982v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.10982v2)
- **Published**: 2019-07-25 11:47:12+00:00
- **Updated**: 2019-10-01 13:22:58+00:00
- **Authors**: Zeju Li, Konstantinos Kamnitsas, Ben Glocker
- **Comment**: Accepted at MICCAI 2019; typo corrected in Table 1
- **Journal**: None
- **Summary**: Overfitting in deep learning has been the focus of a number of recent works, yet its exact impact on the behavior of neural networks is not well understood. This study analyzes overfitting by examining how the distribution of logits alters in relation to how much the model overfits. Specifically, we find that when training with few data samples, the distribution of logit activations when processing unseen test samples of an under-represented class tends to shift towards and even across the decision boundary, while the over-represented class seems unaffected. In image segmentation, foreground samples are often heavily under-represented. We observe that sensitivity of the model drops as a result of overfitting, while precision remains mostly stable. Based on our analysis, we derive asymmetric modifications of existing loss functions and regularizers including a large margin loss, focal loss, adversarial training and mixup, which specifically aim at reducing the shift observed when embedding unseen samples of the under-represented class. We study the case of binary segmentation of brain tumor core and show that our proposed simple modifications lead to significantly improved segmentation performance over the symmetric variants.



### Enhancing Underexposed Photos using Perceptually Bidirectional Similarity
- **Arxiv ID**: http://arxiv.org/abs/1907.10992v3
- **DOI**: 10.1109/TMM.2020.2982045
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10992v3)
- **Published**: 2019-07-25 12:01:59+00:00
- **Updated**: 2020-07-08 12:56:44+00:00
- **Authors**: Qing Zhang, Yongwei Nie, Lei Zhu, Chunxia Xiao, Wei-Shi Zheng
- **Comment**: Aceepted to IEEE Transactions on Multimedia (TMM)
- **Journal**: None
- **Summary**: Although remarkable progress has been made, existing methods for enhancing underexposed photos tend to produce visually unpleasing results due to the existence of visual artifacts (e.g., color distortion, loss of details and uneven exposure). We observed that this is because they fail to ensure the perceptual consistency of visual information between the source underexposed image and its enhanced output. To obtain high-quality results free of these artifacts, we present a novel underexposed photo enhancement approach that is able to maintain the perceptual consistency. We achieve this by proposing an effective criterion, referred to as perceptually bidirectional similarity, which explicitly describes how to ensure the perceptual consistency. Particularly, we adopt the Retinex theory and cast the enhancement problem as a constrained illumination estimation optimization, where we formulate perceptually bidirectional similarity as constraints on illumination and solve for the illumination which can recover the desired artifact-free enhancement results. In addition, we describe a video enhancement framework that adopts the presented illumination estimation for handling underexposed videos. To this end, a probabilistic approach is introduced to propagate illuminations of sampled keyframes to the entire video by tackling a Bayesian Maximum A Posteriori problem. Extensive experiments demonstrate the superiority of our method over the state-of-the-art methods.



### Weakly Supervised Recognition of Surgical Gestures
- **Arxiv ID**: http://arxiv.org/abs/1907.10993v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, eess.IV, 68 Computer science
- **Links**: [PDF](http://arxiv.org/pdf/1907.10993v1)
- **Published**: 2019-07-25 12:07:33+00:00
- **Updated**: 2019-07-25 12:07:33+00:00
- **Authors**: Beatrice van Amsterdam, Hirenkumar Nakawala, Elena De Momi, Danail Stoyanov
- **Comment**: 2019 IEEE International Conference on Robotics and Automation (ICRA)
- **Journal**: None
- **Summary**: Kinematic trajectories recorded from surgical robots contain information about surgical gestures and potentially encode cues about surgeon's skill levels. Automatic segmentation of these trajectories into meaningful action units could help to develop new metrics for surgical skill assessment as well as to simplify surgical automation. State-of-the-art methods for action recognition relied on manual labelling of large datasets, which is time consuming and error prone. Unsupervised methods have been developed to overcome these limitations. However, they often rely on tedious parameter tuning and perform less well than supervised approaches, especially on data with high variability such as surgical trajectories. Hence, the potential of weak supervision could be to improve unsupervised learning while avoiding manual annotation of large datasets. In this paper, we used at a minimum one expert demonstration and its ground truth annotations to generate an appropriate initialization for a GMM-based algorithm for gesture recognition. We showed on real surgical demonstrations that the latter significantly outperforms standard task-agnostic initialization methods. We also demonstrated how to improve the recognition accuracy further by redefining the actions and optimising the inputs.



### Don't Worry About the Weather: Unsupervised Condition-Dependent Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1907.11004v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.11004v1)
- **Published**: 2019-07-25 12:28:35+00:00
- **Updated**: 2019-07-25 12:28:35+00:00
- **Authors**: Horia Porav, Tom Bruls, Paul Newman
- **Comment**: Presented at ITSC2019
- **Journal**: None
- **Summary**: Modern models that perform system-critical tasks such as segmentation and localization exhibit good performance and robustness under ideal conditions (i.e. daytime, overcast) but performance degrades quickly and often catastrophically when input conditions change. In this work, we present a domain adaptation system that uses light-weight input adapters to pre-processes input images, irrespective of their appearance, in a way that makes them compatible with off-the-shelf computer vision tasks that are trained only on inputs with ideal conditions. No fine-tuning is performed on the off-the-shelf models, and the system is capable of incrementally training new input adapters in a self-supervised fashion, using the computer vision tasks as supervisors, when the input domain differs significantly from previously seen domains. We report large improvements in semantic segmentation and topological localization performance on two popular datasets, RobotCar and BDD.



### Towards Generalizing Sensorimotor Control Across Weather Conditions
- **Arxiv ID**: http://arxiv.org/abs/1907.11025v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.11025v1)
- **Published**: 2019-07-25 13:22:05+00:00
- **Updated**: 2019-07-25 13:22:05+00:00
- **Authors**: Qadeer Khan, Patrick Wenzel, Daniel Cremers, Laura Leal-Taixé
- **Comment**: Accepted for publication in 2019 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS)
- **Journal**: None
- **Summary**: The ability of deep learning models to generalize well across different scenarios depends primarily on the quality and quantity of annotated data. Labeling large amounts of data for all possible scenarios that a model may encounter would not be feasible; if even possible. We propose a framework to deal with limited labeled training data and demonstrate it on the application of vision-based vehicle control. We show how limited steering angle data available for only one condition can be transferred to multiple different weather scenarios. This is done by leveraging unlabeled images in a teacher-student learning paradigm complemented with an image-to-image translation network. The translation network transfers the images to a new domain, whereas the teacher provides soft supervised targets to train the student on this domain. Furthermore, we demonstrate how utilization of auxiliary networks can reduce the size of a model at inference time, without affecting the accuracy. The experiments show that our approach generalizes well across multiple different weather conditions using only ground truth labels from one domain.



### Attention Guided Network for Retinal Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.12930v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.12930v3)
- **Published**: 2019-07-25 13:37:23+00:00
- **Updated**: 2019-10-23 07:48:48+00:00
- **Authors**: Shihao Zhang, Huazhu Fu, Yuguang Yan, Yubing Zhang, Qingyao Wu, Ming Yang, Mingkui Tan, Yanwu Xu
- **Comment**: Accepted to MICCAI 2019. Project page:
  (https://github.com/HzFu/AGNet)
- **Journal**: None
- **Summary**: Learning structural information is critical for producing an ideal result in retinal image segmentation. Recently, convolutional neural networks have shown a powerful ability to extract effective representations. However, convolutional and pooling operations filter out some useful structural information. In this paper, we propose an Attention Guided Network (AG-Net) to preserve the structural information and guide the expanding operation. In our AG-Net, the guided filter is exploited as a structure sensitive expanding path to transfer structural information from previous feature maps, and an attention block is introduced to exclude the noise and reduce the negative influence of background further. The extensive experiments on two retinal image segmentation tasks (i.e., blood vessel segmentation, optic disc and cup segmentation) demonstrate the effectiveness of our proposed method.



### Importance-Aware Semantic Segmentation with Efficient Pyramidal Context Network for Navigational Assistant Systems
- **Arxiv ID**: http://arxiv.org/abs/1907.11066v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11066v2)
- **Published**: 2019-07-25 14:04:22+00:00
- **Updated**: 2019-07-27 12:08:24+00:00
- **Authors**: Kaite Xiang, Kaiwei Wang, Kailun Yang
- **Comment**: 7 pages, 22 figures, IEEE Intelligent Transportation Systems
  Conference - ITSC 2019
- **Journal**: None
- **Summary**: Semantic Segmentation (SS) is a task to assign semantic label to each pixel of the images, which is of immense significance for autonomous vehicles, robotics and assisted navigation of vulnerable road users. It is obvious that in different application scenarios, different objects possess hierarchical importance and safety-relevance, but conventional loss functions like cross entropy have not taken the different levels of importance of diverse traffic elements into consideration. To address this dilemma, we leverage and re-design an importance-aware loss function, throwing insightful hints on how importance of semantics are assigned for real-world applications. To customize semantic segmentation networks for different navigational tasks, we extend ERF-PSPNet, a real-time segmenter designed for wearable device aiding visually impaired pedestrians, and propose BiERF-PSPNet, which can yield high-quality segmentation maps with finer spatial details exceptionally suitable for autonomous vehicles. A comprehensive variety of experiments with these efficient pyramidal context networks on CamVid and Cityscapes datasets demonstrates the effectiveness of our proposal to support diverse navigational assistant systems.



### SlimYOLOv3: Narrower, Faster and Better for Real-Time UAV Applications
- **Arxiv ID**: http://arxiv.org/abs/1907.11093v1
- **DOI**: 10.1109/ICCVW.2019.00011
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11093v1)
- **Published**: 2019-07-25 14:22:43+00:00
- **Updated**: 2019-07-25 14:22:43+00:00
- **Authors**: Pengyi Zhang, Yunxin Zhong, Xiaoqiong Li
- **Comment**: None
- **Journal**: None
- **Summary**: Drones or general Unmanned Aerial Vehicles (UAVs), endowed with computer vision function by on-board cameras and embedded systems, have become popular in a wide range of applications. However, real-time scene parsing through object detection running on a UAV platform is very challenging, due to limited memory and computing power of embedded devices. To deal with these challenges, in this paper we propose to learn efficient deep object detectors through channel pruning of convolutional layers. To this end, we enforce channel-level sparsity of convolutional layers by imposing L1 regularization on channel scaling factors and prune less informative feature channels to obtain "slim" object detectors. Based on such approach, we present SlimYOLOv3 with fewer trainable parameters and floating point operations (FLOPs) in comparison of original YOLOv3 (Joseph Redmon et al., 2018) as a promising solution for real-time object detection on UAVs. We evaluate SlimYOLOv3 on VisDrone2018-Det benchmark dataset; compelling results are achieved by SlimYOLOv3 in comparison of unpruned counterpart, including ~90.8% decrease of FLOPs, ~92.0% decline of parameter size, running ~2 times faster and comparable detection accuracy as YOLOv3. Experimental results with different pruning ratios consistently verify that proposed SlimYOLOv3 with narrower structure are more efficient, faster and better than YOLOv3, and thus are more suitable for real-time object detection on UAVs. Our codes are made publicly available at https://github.com/PengyiZhang/SlimYOLOv3.



### How far are we from quantifying visual attention in mobile HCI?
- **Arxiv ID**: http://arxiv.org/abs/1907.11106v1
- **DOI**: 10.1109/MPRV.2020.2967736
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.11106v1)
- **Published**: 2019-07-25 14:38:52+00:00
- **Updated**: 2019-07-25 14:38:52+00:00
- **Authors**: Mihai Bâce, Sander Staal, Andreas Bulling
- **Comment**: 7 pages, 4 figures
- **Journal**: IEEE Pervasive Computing, April-June 2020
- **Summary**: With an ever-increasing number of mobile devices competing for our attention, quantifying when, how often, or for how long users visually attend to their devices has emerged as a core challenge in mobile human-computer interaction. Encouraged by recent advances in automatic eye contact detection using machine learning and device-integrated cameras, we provide a fundamental investigation into the feasibility of quantifying visual attention during everyday mobile interactions. We identify core challenges and sources of errors associated with sensing attention on mobile devices in the wild, including the impact of face and eye visibility, the importance of robust head pose estimation, and the need for accurate gaze estimation. Based on this analysis, we propose future research directions and discuss how eye contact detection represents the foundation for exciting new applications towards next-generation pervasive attentive user interfaces.



### MultiDepth: Single-Image Depth Estimation via Multi-Task Regression and Classification
- **Arxiv ID**: http://arxiv.org/abs/1907.11111v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11111v1)
- **Published**: 2019-07-25 14:43:31+00:00
- **Updated**: 2019-07-25 14:43:31+00:00
- **Authors**: Lukas Liebel, Marco Körner
- **Comment**: Accepted for presentation at the IEEE Intelligent Transportation
  Systems Conference (ITSC) 2019
- **Journal**: None
- **Summary**: We introduce MultiDepth, a novel training strategy and convolutional neural network (CNN) architecture that allows approaching single-image depth estimation (SIDE) as a multi-task problem. SIDE is an important part of road scene understanding. It, thus, plays a vital role in advanced driver assistance systems and autonomous vehicles. Best results for the SIDE task so far have been achieved using deep CNNs. However, optimization of regression problems, such as estimating depth, is still a challenging task. For the related tasks of image classification and semantic segmentation, numerous CNN-based methods with robust training behavior have been proposed. Hence, in order to overcome the notorious instability and slow convergence of depth value regression during training, MultiDepth makes use of depth interval classification as an auxiliary task. The auxiliary task can be disabled at test-time to predict continuous depth values using the main regression branch more efficiently. We applied MultiDepth to road scenes and present results on the KITTI depth prediction dataset. In experiments, we were able to show that end-to-end multi-task learning with both, regression and classification, is able to considerably improve training and yield more accurate results.



### Accurate and Robust Eye Contact Detection During Everyday Mobile Device Interactions
- **Arxiv ID**: http://arxiv.org/abs/1907.11115v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.11115v1)
- **Published**: 2019-07-25 14:55:16+00:00
- **Updated**: 2019-07-25 14:55:16+00:00
- **Authors**: Mihai Bâce, Sander Staal, Andreas Bulling
- **Comment**: 12 pages, 8 figures
- **Journal**: None
- **Summary**: Quantification of human attention is key to several tasks in mobile human-computer interaction (HCI), such as predicting user interruptibility, estimating noticeability of user interface content, or measuring user engagement. Previous works to study mobile attentive behaviour required special-purpose eye tracking equipment or constrained users' mobility. We propose a novel method to sense and analyse visual attention on mobile devices during everyday interactions. We demonstrate the capabilities of our method on the sample task of eye contact detection that has recently attracted increasing research interest in mobile HCI. Our method builds on a state-of-the-art method for unsupervised eye contact detection and extends it to address challenges specific to mobile interactive scenarios. Through evaluation on two current datasets, we demonstrate significant performance improvements for eye contact detection across mobile devices, users, or environmental conditions. Moreover, we discuss how our method enables the calculation of additional attention metrics that, for the first time, enable researchers from different domains to study and quantify attention allocation during mobile interactions in the wild.



### Learning Visual Actions Using Multiple Verb-Only Labels
- **Arxiv ID**: http://arxiv.org/abs/1907.11117v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11117v2)
- **Published**: 2019-07-25 14:58:34+00:00
- **Updated**: 2019-08-01 14:13:08+00:00
- **Authors**: Michael Wray, Dima Damen
- **Comment**: Accepted at BMVC 2019. More information can be found at
  https://mwray.github.io/MVOL/. Annotations can be found at
  https://github.com/mwray/Multi-Verb-Labels
- **Journal**: None
- **Summary**: This work introduces verb-only representations for both recognition and retrieval of visual actions, in video. Current methods neglect legitimate semantic ambiguities between verbs, instead choosing unambiguous subsets of verbs along with objects to disambiguate the actions. We instead propose multiple verb-only labels, which we learn through hard or soft assignment as a regression. This enables learning a much larger vocabulary of verbs, including contextual overlaps of these verbs. We collect multi-verb annotations for three action video datasets and evaluate the verb-only labelling representations for action recognition and cross-modal retrieval (video-to-text and text-to-video). We demonstrate that multi-label verb-only representations outperform conventional single verb labels. We also explore other benefits of a multi-verb representation including cross-dataset retrieval and verb type manner and result verb types) retrieval.



### Hetero-Modal Variational Encoder-Decoder for Joint Modality Completion and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.11150v1
- **DOI**: 10.1007/978-3-030-32245-8_9
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.11150v1)
- **Published**: 2019-07-25 15:49:11+00:00
- **Updated**: 2019-07-25 15:49:11+00:00
- **Authors**: Reuben Dorent, Samuel Joutard, Marc Modat, Sébastien Ourselin, Tom Vercauteren
- **Comment**: Accepted at MICCAI 2019
- **Journal**: None
- **Summary**: We propose a new deep learning method for tumour segmentation when dealing with missing imaging modalities. Instead of producing one network for each possible subset of observed modalities or using arithmetic operations to combine feature maps, our hetero-modal variational 3D encoder-decoder independently embeds all observed modalities into a shared latent representation. Missing data and tumour segmentation can be then generated from this embedding. In our scenario, the input is a random subset of modalities. We demonstrate that the optimisation problem can be seen as a mixture sampling. In addition to this, we introduce a new network architecture building upon both the 3D U-Net and the Multi-Modal Variational Auto-Encoder (MVAE). Finally, we evaluate our method on BraTS2018 using subsets of the imaging modalities as input. Our model outperforms the current state-of-the-art method for dealing with missing modalities and achieves similar performance to the subset-specific equivalent networks.



### Unsupervised Domain Adaptation via Calibrating Uncertainties
- **Arxiv ID**: http://arxiv.org/abs/1907.11202v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.11202v1)
- **Published**: 2019-07-25 17:02:51+00:00
- **Updated**: 2019-07-25 17:02:51+00:00
- **Authors**: Ligong Han, Yang Zou, Ruijiang Gao, Lezi Wang, Dimitris Metaxas
- **Comment**: 4 pages
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR) Workshops, 2019, pp. 99-102
- **Summary**: Unsupervised domain adaptation (UDA) aims at inferring class labels for unlabeled target domain given a related labeled source dataset. Intuitively, a model trained on source domain normally produces higher uncertainties for unseen data. In this work, we build on this assumption and propose to adapt from source to target domain via calibrating their predictive uncertainties. The uncertainty is quantified as the Renyi entropy, from which we propose a general Renyi entropy regularization (RER) framework. We further employ variational Bayes learning for reliable uncertainty estimation. In addition, calibrating the sample variance of network parameters serves as a plug-in regularizer for training. We discuss the theoretical properties of the proposed method and demonstrate its effectiveness on three domain-adaptation tasks.



### Minimal Solvers for Rectifying from Radially-Distorted Scales and Change of Scales
- **Arxiv ID**: http://arxiv.org/abs/1907.11539v2
- **DOI**: 10.1007/s11263-019-01216-x
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11539v2)
- **Published**: 2019-07-25 17:16:32+00:00
- **Updated**: 2020-04-09 23:34:52+00:00
- **Authors**: James Pritts, Zuzana Kukelova, Viktor Larsson, Yaroslava Lochman, Ondřej Chum
- **Comment**: arXiv admin note: text overlap with arXiv:1807.06110
- **Journal**: International Journal of Computer Vision (2020) 1-19
- **Summary**: This paper introduces the first minimal solvers that jointly estimate lens distortion and affine rectification from the image of rigidly-transformed coplanar features. The solvers work on scenes without straight lines and, in general, relax strong assumptions about scene content made by the state of the art. The proposed solvers use the affine invariant that coplanar repeats have the same scale in rectified space. The solvers are separated into two groups that differ by how the equal scale invariant of rectified space is used to place constraints on the lens undistortion and rectification parameters. We demonstrate a principled approach for generating stable minimal solvers by the Gr\"obner basis method, which is accomplished by sampling feasible monomial bases to maximize numerical stability. Synthetic and real-image experiments confirm that the proposed solvers demonstrate superior robustness to noise compared to the state of the art. Accurate rectifications on imagery taken with narrow to fisheye field-of-view lenses demonstrate the wide applicability of the proposed method. The method is fully automatic.



### HUGE2: a Highly Untangled Generative-model Engine for Edge-computing
- **Arxiv ID**: http://arxiv.org/abs/1907.11210v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.11210v2)
- **Published**: 2019-07-25 17:21:52+00:00
- **Updated**: 2021-05-08 17:07:28+00:00
- **Authors**: Feng Shi, Ziheng Xu, Tao Yuan, Song-Chun Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: As a type of prominent studies in deep learning, generative models have been widely investigated in research recently. Two research branches of the deep learning models, the Generative Networks (GANs, VAE) and the Semantic Segmentation, rely highly on the upsampling operations, especially the transposed convolution and the dilated convolution. However, these two types of convolutions are intrinsically different from standard convolution regarding the insertion of zeros in input feature maps or in kernels respectively. This distinct nature severely degrades the performance of the existing deep learning engine or frameworks, such as Darknet, Tensorflow, and PyTorch, which are mainly developed for the standard convolution. Another trend in deep learning realm is to deploy the model onto edge/ embedded devices, in which the memory resource is scarce. In this work, we propose a Highly Untangled Generative-model Engine for Edge-computing or HUGE2 for accelerating these two special convolutions on the edge-computing platform by decomposing the kernels and untangling these smaller convolutions by performing basic matrix multiplications. The methods we propose use much smaller memory footprint, hence much fewer memory accesses, and the data access patterns also dramatically increase the reusability of the data already fetched in caches, hence increasing the localities of caches. Our engine achieves a speedup of nearly 5x on embedded CPUs, and around 10x on embedded GPUs, and more than 50% reduction of memory access.



### A Novel Approach for Robust Multi Human Action Recognition and Summarization based on 3D Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.11272v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11272v4)
- **Published**: 2019-07-25 18:48:59+00:00
- **Updated**: 2021-03-15 08:56:57+00:00
- **Authors**: Noor Almaadeed, Omar Elharrouss, Somaya Al-Maadeed, Ahmed Bouridane, Azeddine Beghdadi
- **Comment**: None
- **Journal**: None
- **Summary**: Human actions in videos are 3D signals. However, there are a few methods available for multiple human action recognition. For long videos, it's difficult to search within a video for a specific action and/or person. For that, this paper proposes a new technic for multiple human action recognition and summarization for surveillance videos. The proposed approach proposes a new representation of the data by extracting the sequence of each person from the scene. This is followed by an analysis of each sequence to detect and recognize the corresponding actions using 3D convolutional neural networks (3DCNNs). Action-based video summarization is performed by saving each person's action at each time of the video. Results of this work revealed that the proposed method provides accurate multi human action recognition that easily used for summarization of any action. Further, for other videos that can be collected from the internet, which are complex and not built for surveillance applications, the proposed model was evaluated on some datasets like UCF101 and YouTube without any preprocessing. For this category of videos, the summarization is performed on the video sequences by summarizing the actions in each subsequence. The results obtained demonstrate its efficiency compared to state-of-the-art methods.



### Accurate and Robust Pulmonary Nodule Detection by 3D Feature Pyramid Network with Self-supervised Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.11704v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.11704v1)
- **Published**: 2019-07-25 21:00:29+00:00
- **Updated**: 2019-07-25 21:00:29+00:00
- **Authors**: Jingya Liu, Liangliang Cao, Oguz Akin, Yingli Tian
- **Comment**: 15 pages, 8 figures, 5 tables, under review by Medical Image
  Analysis. arXiv admin note: substantial text overlap with arXiv:1906.03467
- **Journal**: None
- **Summary**: Accurate detection of pulmonary nodules with high sensitivity and specificity is essential for automatic lung cancer diagnosis from CT scans. Although many deep learning-based algorithms make great progress for improving the accuracy of nodule detection, the high false positive rate is still a challenging problem which limits the automatic diagnosis in routine clinical practice. Moreover, the CT scans collected from multiple manufacturers may affect the robustness of Computer-aided diagnosis (CAD) due to the differences in intensity scales and machine noises. In this paper, we propose a novel self-supervised learning assisted pulmonary nodule detection framework based on a 3D Feature Pyramid Network (3DFPN) to improve the sensitivity of nodule detection by employing multi-scale features to increase the resolution of nodules, as well as a parallel top-down path to transit the high-level semantic features to complement low-level general features. Furthermore, a High Sensitivity and Specificity (HS2) network is introduced to eliminate the false positive nodule candidates by tracking the appearance changes in continuous CT slices of each nodule candidate on Location History Images (LHI). In addition, in order to improve the performance consistency of the proposed framework across data captured by different CT scanners without using additional annotations, an effective self-supervised learning schema is applied to learn spatiotemporal features of CT scans from large-scale unlabeled data. The performance and robustness of our method are evaluated on several publicly available datasets with significant performance improvements. The proposed framework is able to accurately detect pulmonary nodules with high sensitivity and specificity and achieves 90.6% sensitivity with 1/8 false positive per scan which outperforms the state-of-the-art results 15.8% on LUNA16 dataset.



### SceneGraphNet: Neural Message Passing for 3D Indoor Scene Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.11308v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1907.11308v1)
- **Published**: 2019-07-25 21:03:15+00:00
- **Updated**: 2019-07-25 21:03:15+00:00
- **Authors**: Yang Zhou, Zachary While, Evangelos Kalogerakis
- **Comment**: 8 pages, 8 figures, to appear in ICCV 2019
- **Journal**: None
- **Summary**: In this paper we propose a neural message passing approach to augment an input 3D indoor scene with new objects matching their surroundings. Given an input, potentially incomplete, 3D scene and a query location, our method predicts a probability distribution over object types that fit well in that location. Our distribution is predicted though passing learned messages in a dense graph whose nodes represent objects in the input scene and edges represent spatial and structural relationships. By weighting messages through an attention mechanism, our method learns to focus on the most relevant surrounding scene context to predict new scene objects. We found that our method significantly outperforms state-of-the-art approaches in terms of correctly predicting objects missing in a scene based on our experiments in the SUNCG dataset. We also demonstrate other applications of our method, including context-based 3D object recognition and iterative scene generation.



### NoduleNet: Decoupled False Positive Reductionfor Pulmonary Nodule Detection and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.11320v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11320v1)
- **Published**: 2019-07-25 22:05:21+00:00
- **Updated**: 2019-07-25 22:05:21+00:00
- **Authors**: Hao Tang, Chupeng Zhang, Xiaohui Xie
- **Comment**: Accepted to MICCAI 2019
- **Journal**: None
- **Summary**: Pulmonary nodule detection, false positive reduction and segmentation represent three of the most common tasks in the computeraided analysis of chest CT images. Methods have been proposed for eachtask with deep learning based methods heavily favored recently. However training deep learning models to solve each task separately may be sub-optimal - resource intensive and without the benefit of feature sharing. Here, we propose a new end-to-end 3D deep convolutional neural net (DCNN), called NoduleNet, to solve nodule detection, false positive reduction and nodule segmentation jointly in a multi-task fashion. To avoid friction between different tasks and encourage feature diversification, we incorporate two major design tricks: 1) decoupled feature maps for nodule detection and false positive reduction, and 2) a segmentation refinement subnet for increasing the precision of nodule segmentation. Extensive experiments on the large-scale LIDC dataset demonstrate that the multi-task training is highly beneficial, improving the nodule detection accuracy by 10.27%, compared to the baseline model trained to only solve the nodule detection task. We also carry out systematic ablation studies to highlight contributions from each of the added components. Code is available at https://github.com/uci-cbcl/NoduleNet.



### Object as Distribution
- **Arxiv ID**: http://arxiv.org/abs/1907.12929v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.12929v1)
- **Published**: 2019-07-25 23:10:21+00:00
- **Updated**: 2019-07-25 23:10:21+00:00
- **Authors**: Li Ding, Lex Fridman
- **Comment**: NeurIPS 2019
- **Journal**: None
- **Summary**: Object detection is a critical part of visual scene understanding. The representation of the object in the detection task has important implications on the efficiency and feasibility of annotation, robustness to occlusion, pose, lighting, and other visual sources of semantic uncertainty, and effectiveness in real-world applications (e.g., autonomous driving). Popular object representations include 2D and 3D bounding boxes, polygons, splines, pixels, and voxels. Each have their strengths and weakness. In this work, we propose a new representation of objects based on the bivariate normal distribution. This distribution-based representation has the benefit of robust detection of highly-overlapping objects and the potential for improved downstream tracking and instance segmentation tasks due to the statistical representation of object edges. We provide qualitative evaluation of this representation for the object detection task and quantitative evaluation of its use in a baseline algorithm for the instance segmentation task.



