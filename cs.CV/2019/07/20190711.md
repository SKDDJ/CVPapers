# Arxiv Papers in cs.CV on 2019-07-11
### A General Decoupled Learning Framework for Parameterized Image Operators
- **Arxiv ID**: http://arxiv.org/abs/1907.05852v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.05852v1)
- **Published**: 2019-07-11 00:22:39+00:00
- **Updated**: 2019-07-11 00:22:39+00:00
- **Authors**: Qingnan Fan, Dongdong Chen, Lu Yuan, Gang Hua, Nenghai Yu, Baoquan Chen
- **Comment**: Published in TPAMI 2019. arXiv admin note: substantial text overlap
  with arXiv:1807.08186
- **Journal**: None
- **Summary**: Many different deep networks have been used to approximate, accelerate or improve traditional image operators. Among these traditional operators, many contain parameters which need to be tweaked to obtain the satisfactory results, which we refer to as parameterized image operators. However, most existing deep networks trained for these operators are only designed for one specific parameter configuration, which does not meet the needs of real scenarios that usually require flexible parameters settings. To overcome this limitation, we propose a new decoupled learning algorithm to learn from the operator parameters to dynamically adjust the weights of a deep network for image operators, denoted as the base network. The learned algorithm is formed as another network, namely the weight learning network, which can be end-to-end jointly trained with the base network. Experiments demonstrate that the proposed framework can be successfully applied to many traditional parameterized image operators. To accelerate the parameter tuning for practical scenarios, the proposed framework can be further extended to dynamically change the weights of only one single layer of the base network while sharing most computation cost. We demonstrate that this cheap parameter-tuning extension of the proposed decoupled learning framework even outperforms the state-of-the-art alternative approaches.



### Diverse Trajectory Forecasting with Determinantal Point Processes
- **Arxiv ID**: http://arxiv.org/abs/1907.04967v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1907.04967v2)
- **Published**: 2019-07-11 00:59:22+00:00
- **Updated**: 2019-12-23 10:23:40+00:00
- **Authors**: Ye Yuan, Kris Kitani
- **Comment**: ICLR 2020
- **Journal**: None
- **Summary**: The ability to forecast a set of likely yet diverse possible future behaviors of an agent (e.g., future trajectories of a pedestrian) is essential for safety-critical perception systems (e.g., autonomous vehicles). In particular, a set of possible future behaviors generated by the system must be diverse to account for all possible outcomes in order to take necessary safety precautions. It is not sufficient to maintain a set of the most likely future outcomes because the set may only contain perturbations of a single outcome. While generative models such as variational autoencoders (VAEs) have been shown to be a powerful tool for learning a distribution over future trajectories, randomly drawn samples from the learned implicit likelihood model may not be diverse -- the likelihood model is derived from the training data distribution and the samples will concentrate around the major mode that has most data. In this work, we propose to learn a diversity sampling function (DSF) that generates a diverse and likely set of future trajectories. The DSF maps forecasting context features to a set of latent codes which can be decoded by a generative model (e.g., VAE) into a set of diverse trajectory samples. Concretely, the process of identifying the diverse set of samples is posed as a parameter estimation of the DSF. To learn the parameters of the DSF, the diversity of the trajectory samples is evaluated by a diversity loss based on a determinantal point process (DPP). Gradient descent is performed over the DSF parameters, which in turn move the latent codes of the sample set to find an optimal diverse and likely set of trajectories. Our method is a novel application of DPPs to optimize a set of items (trajectories) in continuous space. We demonstrate the diversity of the trajectories produced by our approach on both low-dimensional 2D trajectory data and high-dimensional human motion data.



### My lips are concealed: Audio-visual speech enhancement through obstructions
- **Arxiv ID**: http://arxiv.org/abs/1907.04975v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1907.04975v1)
- **Published**: 2019-07-11 02:05:48+00:00
- **Updated**: 2019-07-11 02:05:48+00:00
- **Authors**: Triantafyllos Afouras, Joon Son Chung, Andrew Zisserman
- **Comment**: Accepted to Interspeech 2019
- **Journal**: None
- **Summary**: Our objective is an audio-visual model for separating a single speaker from a mixture of sounds such as other speakers and background noise. Moreover, we wish to hear the speaker even when the visual cues are temporarily absent due to occlusion. To this end we introduce a deep audio-visual speech enhancement network that is able to separate a speaker's voice by conditioning on both the speaker's lip movements and/or a representation of their voice. The voice representation can be obtained by either (i) enrollment, or (ii) by self-enrollment -- learning the representation on-the-fly given sufficient unobstructed visual input. The model is trained by blending audios, and by introducing artificial occlusions around the mouth region that prevent the visual modality from dominating. The method is speaker-independent, and we demonstrate it on real examples of speakers unheard (and unseen) during training. The method also improves over previous models in particular for cases of occlusion in the visual modality.



### Agile Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1907.04978v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.04978v1)
- **Published**: 2019-07-11 02:51:27+00:00
- **Updated**: 2019-07-11 02:51:27+00:00
- **Authors**: Jingjing Li, Mengmeng Jing, Yue Xie, Ke Lu, Zi Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Domain adaptation investigates the problem of leveraging knowledge from a well-labeled source domain to an unlabeled target domain, where the two domains are drawn from different data distributions. Because of the distribution shifts, different target samples have distinct degrees of difficulty in adaptation. However, existing domain adaptation approaches overwhelmingly neglect the degrees of difficulty and deploy exactly the same framework for all of the target samples. Generally, a simple or shadow framework is fast but rough. A sophisticated or deep framework, on the contrary, is accurate but slow. In this paper, we aim to challenge the fundamental contradiction between the accuracy and speed in domain adaptation tasks. We propose a novel approach, named {\it agile domain adaptation}, which agilely applies optimal frameworks to different target samples and classifies the target samples according to their adaptation difficulties. Specifically, we propose a paradigm which performs several early detections before the final classification. If a sample can be classified at one of the early stage with enough confidence, the sample would exit without the subsequent processes. Notably, the proposed method can significantly reduce the running cost of domain adaptation approaches, which can extend the application scenarios of domain adaptation to even mobile devices and real-time systems. Extensive experiments on two open benchmarks verify the effectiveness and efficiency of the proposed method.



### Aesthetic Attributes Assessment of Images
- **Arxiv ID**: http://arxiv.org/abs/1907.04983v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.04983v2)
- **Published**: 2019-07-11 03:25:47+00:00
- **Updated**: 2019-07-29 16:30:49+00:00
- **Authors**: Xin Jin, Le Wu, Geng Zhao, Xiaodong Li, Xiaokun Zhang, Shiming Ge, Dongqing Zou, Bin Zhou, Xinghui Zhou
- **Comment**: to appear in ACM MM 2019, camera ready version
- **Journal**: None
- **Summary**: Image aesthetic quality assessment has been a relatively hot topic during the last decade. Most recently, comments type assessment (aesthetic captions) has been proposed to describe the general aesthetic impression of an image using text. In this paper, we propose Aesthetic Attributes Assessment of Images, which means the aesthetic attributes captioning. This is a new formula of image aesthetic assessment, which predicts aesthetic attributes captions together with the aesthetic score of each attribute. We introduce a new dataset named \emph{DPC-Captions} which contains comments of up to 5 aesthetic attributes of one image through knowledge transfer from a full-annotated small-scale dataset. Then, we propose Aesthetic Multi-Attribute Network (AMAN), which is trained on a mixture of fully-annotated small-scale PCCD dataset and weakly-annotated large-scale DPC-Captions dataset. Our AMAN makes full use of transfer learning and attention model in a single framework. The experimental results on our DPC-Captions and PCCD dataset reveal that our method can predict captions of 5 aesthetic attributes together with numerical score assessment of each attribute. We use the evaluation criteria used in image captions to prove that our specially designed AMAN model outperforms traditional CNN-LSTM model and modern SCA-CNN model of image captions.



### Object Detection in Video with Spatial-temporal Context Aggregation
- **Arxiv ID**: http://arxiv.org/abs/1907.04988v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.04988v1)
- **Published**: 2019-07-11 03:49:55+00:00
- **Updated**: 2019-07-11 03:49:55+00:00
- **Authors**: Hao Luo, Lichao Huang, Han Shen, Yuan Li, Chang Huang, Xinggang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent cutting-edge feature aggregation paradigms for video object detection rely on inferring feature correspondence. The feature correspondence estimation problem is fundamentally difficult due to poor image quality, motion blur, etc, and the results of feature correspondence estimation are unstable. To avoid the problem, we propose a simple but effective feature aggregation framework which operates on the object proposal-level. It learns to enhance each proposal's feature via modeling semantic and spatio-temporal relationships among object proposals from both within a frame and across adjacent frames. Experiments are carried out on the ImageNet VID dataset. Without any bells and whistles, our method obtains 80.3\% mAP on the ImageNet VID dataset, which is superior over the previous state-of-the-arts. The proposed feature aggregation mechanism improves the single frame Faster RCNN baseline by 5.8% mAP. Besides, under the setting of no temporal post-processing, our method outperforms the previous state-of-the-art by 1.4% mAP.



### Camera Exposure Control for Robust Robot Vision with Noise-Aware Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/1907.12646v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1907.12646v1)
- **Published**: 2019-07-11 04:31:59+00:00
- **Updated**: 2019-07-11 04:31:59+00:00
- **Authors**: Ukcheol Shin, Jinsun Park, Gyumin Shim, Francois Rameau, In So Kweon
- **Comment**: 8 pages,6 figures, accepted in IROS2019
- **Journal**: None
- **Summary**: In this paper, we propose a noise-aware exposure control algorithm for robust robot vision. Our method aims to capture the best-exposed image which can boost the performance of various computer vision and robotics tasks. For this purpose, we carefully design an image quality metric which captures complementary quality attributes and ensures light-weight computation. Specifically, our metric consists of a combination of image gradient, entropy, and noise metrics. The synergy of these measures allows preserving sharp edge and rich texture in the image while maintaining a low noise level. Using this novel metric, we propose a real-time and fully automatic exposure and gain control technique based on the Nelder-Mead method. To illustrate the effectiveness of our technique, a large set of experimental results demonstrates higher qualitative and quantitative performances when compared with conventional approaches.



### A Survey of Deep Learning-based Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1907.09408v2
- **DOI**: 10.1109/ACCESS.2019.2939201
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09408v2)
- **Published**: 2019-07-11 04:53:09+00:00
- **Updated**: 2019-10-10 17:09:59+00:00
- **Authors**: Licheng Jiao, Fan Zhang, Fang Liu, Shuyuan Yang, Lingling Li, Zhixi Feng, Rong Qu
- **Comment**: 30 pages,12 figures
- **Journal**: 05 September 2019
- **Summary**: Object detection is one of the most important and challenging branches of computer vision, which has been widely applied in peoples life, such as monitoring security, autonomous driving and so on, with the purpose of locating instances of semantic objects of a certain class. With the rapid development of deep learning networks for detection tasks, the performance of object detectors has been greatly improved. In order to understand the main development status of object detection pipeline, thoroughly and deeply, in this survey, we first analyze the methods of existing typical detection models and describe the benchmark datasets. Afterwards and primarily, we provide a comprehensive overview of a variety of object detection methods in a systematic manner, covering the one-stage and two-stage detectors. Moreover, we list the traditional and new applications. Some representative branches of object detection are analyzed as well. Finally, we discuss the architecture of exploiting these object detection methods to build an effective and efficient system and point out a set of development trends to better follow the state-of-the-art algorithms and further research.



### Two-stream Spatiotemporal Feature for Video QA Task
- **Arxiv ID**: http://arxiv.org/abs/1907.05006v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.05006v1)
- **Published**: 2019-07-11 05:51:39+00:00
- **Updated**: 2019-07-11 05:51:39+00:00
- **Authors**: Chiwan Song, Woobin Im, Sung-eui Yoon
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Understanding the content of videos is one of the core techniques for developing various helpful applications in the real world, such as recognizing various human actions for surveillance systems or customer behavior analysis in an autonomous shop. However, understanding the content or story of the video still remains a challenging problem due to its sheer amount of data and temporal structure. In this paper, we propose a multi-channel neural network structure that adopts a two-stream network structure, which has been shown high performance in human action recognition field, and use it as a spatiotemporal video feature extractor for solving video question and answering task. We also adopt a squeeze-and-excitation structure to two-stream network structure for achieving a channel-wise attended spatiotemporal feature. For jointly modeling the spatiotemporal features from video and the textual features from the question, we design a context matching module with a level adjusting layer to remove the gap of information between visual and textual features by applying attention mechanism on joint modeling. Finally, we adopt a scoring mechanism and smoothed ranking loss objective function for selecting the correct answer from answer candidates. We evaluate our model with TVQA dataset, and our approach shows the improved result in textual only setting, but the result with visual feature shows the limitation and possibility of our approach.



### Semi-supervised Feature-Level Attribute Manipulation for Fashion Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1907.05007v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.05007v1)
- **Published**: 2019-07-11 05:51:49+00:00
- **Updated**: 2019-07-11 05:51:49+00:00
- **Authors**: Minchul Shin, Sanghyuk Park, Taeksoo Kim
- **Comment**: Accepted to BMVC 2019
- **Journal**: None
- **Summary**: With a growing demand for the search by image, many works have studied the task of fashion instance-level image retrieval (FIR). Furthermore, the recent works introduce a concept of fashion attribute manipulation (FAM) which manipulates a specific attribute (e.g color) of a fashion item while maintaining the rest of the attributes (e.g shape, and pattern). In this way, users can search not only "the same" items but also "similar" items with the desired attributes. FAM is a challenging task in that the attributes are hard to define, and the unique characteristics of a query are hard to be preserved. Although both FIR and FAM are important in real-life applications, most of the previous studies have focused on only one of these problem. In this study, we aim to achieve competitive performance on both FIR and FAM. To do so, we propose a novel method that converts a query into a representation with the desired attributes. We introduce a new idea of attribute manipulation at the feature level, by matching the distribution of manipulated features with real features. In this fashion, the attribute manipulation can be done independently from learning a representation from the image. By introducing the feature-level attribute manipulation, the previous methods for FIR can perform attribute manipulation without sacrificing their retrieval performance.



### Optimal Feature Transport for Cross-View Image Geo-Localization
- **Arxiv ID**: http://arxiv.org/abs/1907.05021v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.05021v3)
- **Published**: 2019-07-11 06:56:40+00:00
- **Updated**: 2019-11-27 05:53:04+00:00
- **Authors**: Yujiao Shi, Xin Yu, Liu Liu, Tong Zhang, Hongdong Li
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of cross-view image geo-localization, where the geographic location of a ground-level street-view query image is estimated by matching it against a large scale aerial map (e.g., a high-resolution satellite image). State-of-the-art deep-learning based methods tackle this problem as deep metric learning which aims to learn global feature representations of the scene seen by the two different views. Despite promising results are obtained by such deep metric learning methods, they, however, fail to exploit a crucial cue relevant for localization, namely, the spatial layout of local features. Moreover, little attention is paid to the obvious domain gap (between aerial view and ground view) in the context of cross-view localization. This paper proposes a novel Cross-View Feature Transport (CVFT) technique to explicitly establish cross-view domain transfer that facilitates feature alignment between ground and aerial images. Specifically, we implement the CVFT as network layers, which transports features from one domain to the other, leading to more meaningful feature similarity comparison. Our model is differentiable and can be learned end-to-end. Experiments on large-scale datasets have demonstrated that our method has remarkably boosted the state-of-the-art cross-view localization performance, e.g., on the CVUSA dataset, with significant improvements for top-1 recall from 40.79% to 61.43%, and for top-10 from 76.36% to 90.49%. We expect the key insight of the paper (i.e., explicitly handling domain difference via domain transport) will prove to be useful for other similar problems in computer vision as well.



### Micro-expression Action Unit Detection with Spatio-temporal Adaptive Pooling
- **Arxiv ID**: http://arxiv.org/abs/1907.05023v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1907.05023v2)
- **Published**: 2019-07-11 07:00:47+00:00
- **Updated**: 2020-04-10 06:13:38+00:00
- **Authors**: Yante Li, Xiaohua Huang, Guoying Zhao
- **Comment**: There is a bug in the method. The results are not correct
- **Journal**: None
- **Summary**: Action Unit (AU) detection plays an important role for facial expression recognition. To the best of our knowledge, there is little research about AU analysis for micro-expressions. In this paper, we focus on AU detection in micro-expressions. Microexpression AU detection is challenging due to the small quantity of micro-expression databases, low intensity, short duration of facial muscle change, and class imbalance. In order to alleviate the problems, we propose a novel Spatio-Temporal Adaptive Pooling (STAP) network for AU detection in micro-expressions. Firstly, STAP is aggregated by a series of convolutional filters of different sizes. In this way, STAP can obtain multi-scale information on spatial and temporal domains. On the other hand, STAP contains less parameters, thus it has less computational cost and is suitable for micro-expression AU detection on very small databases. Furthermore, STAP module is designed to pool discriminative information for micro-expression AUs on spatial and temporal domains.Finally, Focal loss is employed to prevent the vast number of negatives from overwhelming the microexpression AU detector. In experiments, we firstly polish the AU annotations on three commonly used databases. We conduct intensive experiments on three micro-expression databases, and provide several baseline results on micro-expression AU detection. The results show that our proposed approach outperforms the basic Inflated inception-v1 (I3D) in terms of an average of F1- score. We also evaluate the performance of our proposed method on cross-database protocol. It demonstrates that our proposed approach is feasible for cross-database micro-expression AU detection. Importantly, the results on three micro-expression databases and cross-database protocol provide extensive baseline results for future research on micro-expression AU detection.



### Extension of Sinkhorn Method: Optimal Movement Estimation of Agents Moving at Constant Velocity
- **Arxiv ID**: http://arxiv.org/abs/1907.05036v1
- **DOI**: 10.1527/tjsai.D-J13
- **Categories**: **eess.IV**, cs.CV, q-bio.CB, 92C55
- **Links**: [PDF](http://arxiv.org/pdf/1907.05036v1)
- **Published**: 2019-07-11 08:01:56+00:00
- **Updated**: 2019-07-11 08:01:56+00:00
- **Authors**: Daigo Okada, Naotoshi Nakamura, Takuya Wada, Ayako Iwasaki, Ryo Yamada
- **Comment**: 12 pages, 7 figures, 2 tables
- **Journal**: Transactions of the Japanese Society for Artificial Intelligence
  34.5 (2019) D-J13_1-7
- **Summary**: In the field of bioimaging, an important part of analyzing the motion of objects is tracking. We propose a method that applies the Sinkhorn distance for solving the optimal transport problem to track objects. The advantage of this method is that it can flexibly incorporate various assumptions in tracking as a cost matrix. First, we extend the Sinkhorn distance from two dimensions to three dimensions. Using this three-dimensional distance, we compare the performance of two types of tracking technique, namely tracking that associates objects that are close to each other, which conventionally uses the nearest-neighbor method, and tracking that assumes that the object is moving at constant velocity, using three types of simulation data. The results suggest that when tracking objects moving at constant velocity, our method is superior to conventional nearest-neighbor tracking as long as the added noise is not excessively large. We show that the Sinkhorn method can be applied effectively to object tracking. Our simulation data analysis suggests that when objects are moving at constant velocity, our method, which sets acceleration as a cost, outperforms the traditional nearest-neighbor method in terms of tracking objects. To apply the proposed method to real bioimaging data, it is necessary to set an appropriate cost indicator based on the movement features.



### BlazeFace: Sub-millisecond Neural Face Detection on Mobile GPUs
- **Arxiv ID**: http://arxiv.org/abs/1907.05047v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.05047v2)
- **Published**: 2019-07-11 08:40:08+00:00
- **Updated**: 2019-07-14 11:12:28+00:00
- **Authors**: Valentin Bazarevsky, Yury Kartynnik, Andrey Vakunov, Karthik Raveendran, Matthias Grundmann
- **Comment**: 4 pages, 3 figures; CVPR Workshop on Computer Vision for Augmented
  and Virtual Reality, Long Beach, CA, USA, 2019
- **Journal**: None
- **Summary**: We present BlazeFace, a lightweight and well-performing face detector tailored for mobile GPU inference. It runs at a speed of 200-1000+ FPS on flagship devices. This super-realtime performance enables it to be applied to any augmented reality pipeline that requires an accurate facial region of interest as an input for task-specific models, such as 2D/3D facial keypoint or geometry estimation, facial features or expression classification, and face region segmentation. Our contributions include a lightweight feature extraction network inspired by, but distinct from MobileNetV1/V2, a GPU-friendly anchor scheme modified from Single Shot MultiBox Detector (SSD), and an improved tie resolution strategy alternative to non-maximum suppression.



### FIRE: Unsupervised bi-directional inter-modality registration using deep networks
- **Arxiv ID**: http://arxiv.org/abs/1907.05062v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.05062v1)
- **Published**: 2019-07-11 09:00:54+00:00
- **Updated**: 2019-07-11 09:00:54+00:00
- **Authors**: Chengjia Wang, Giorgos Papanastasiou, Agisilaos Chartsias, Grzegorz Jacenkow, Sotirios A. Tsaftaris, Heye Zhang
- **Comment**: We submitted this paper to a top medical imaging conference,
  srebuttal responded by the meta-reviewer. We were told that this work is not
  important and will not have big impact as the "reviewers were not
  enthusiastic". Here I publish the paper online for an open discussion. I will
  publish the code, the pre-trained model, the results, especially the reviews,
  and the meta reviews on github
- **Journal**: None
- **Summary**: Inter-modality image registration is an critical preprocessing step for many applications within the routine clinical pathway. This paper presents an unsupervised deep inter-modality registration network that can learn the optimal affine and non-rigid transformations simultaneously. Inverse-consistency is an important property commonly ignored in recent deep learning based inter-modality registration algorithms. We address this issue through the proposed multi-task architecture and the new comprehensive transformation network. Specifically, the proposed model learns a modality-independent latent representation to perform cycle-consistent cross-modality synthesis, and use an inverse-consistent loss to learn a pair of transformations to align the synthesized image with the target. We name this proposed framework as FIRE due to the shape of its structure. Our method shows comparable and better performances with the popular baseline method in experiments on multi-sequence brain MR data and intra-modality 4D cardiac Cine-MR data.



### MeetUp! A Corpus of Joint Activity Dialogues in a Visual Environment
- **Arxiv ID**: http://arxiv.org/abs/1907.05084v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.05084v1)
- **Published**: 2019-07-11 10:06:20+00:00
- **Updated**: 2019-07-11 10:06:20+00:00
- **Authors**: Nikolai Ilinykh, Sina Zarrieß, David Schlangen
- **Comment**: In Proceedings of the 23rd Workshop on the Semantics and Pragmatics
  of Dialogue (semdial / LondonLogue), London, September 2019
- **Journal**: None
- **Summary**: Building computer systems that can converse about their visual environment is one of the oldest concerns of research in Artificial Intelligence and Computational Linguistics (see, for example, Winograd's 1972 SHRDLU system). Only recently, however, have methods from computer vision and natural language processing become powerful enough to make this vision seem more attainable. Pushed especially by developments in computer vision, many data sets and collection environments have recently been published that bring together verbal interaction and visual processing. Here, we argue that these datasets tend to oversimplify the dialogue part, and we propose a task---MeetUp!---that requires both visual and conversational grounding, and that makes stronger demands on representations of the discourse. MeetUp! is a two-player coordination game where players move in a visual environment, with the objective of finding each other. To do so, they must talk about what they see, and achieve mutual understanding. We describe a data collection and show that the resulting dialogues indeed exhibit the dialogue phenomena of interest, while also challenging the language & vision aspect.



### Deep-Learning for Tidemark Segmentation in Human Osteochondral Tissues Imaged with Micro-computed Tomography
- **Arxiv ID**: http://arxiv.org/abs/1907.05089v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.05089v1)
- **Published**: 2019-07-11 10:20:50+00:00
- **Updated**: 2019-07-11 10:20:50+00:00
- **Authors**: Aleksei Tiulpin, Mikko Finnilä, Petri Lehenkari, Heikki J. Nieminen, Simo Saarakkala
- **Comment**: None
- **Journal**: None
- **Summary**: Three-dimensional (3D) semi-quantitative grading of pathological features in articular cartilage (AC) offers significant improvements in basic research of osteoarthritis (OA). We have earlier developed the 3D protocol for imaging of AC and its structures which includes staining of the sample with a contrast agent (phosphotungstic acid, PTA) and a consequent scanning with micro-computed tomography. Such a protocol was designed to provide X-ray attenuation contrast to visualize AC structure. However, at the same time, this protocol has one major disadvantage: the loss of contrast at the tidemark (calcified cartilage interface, CCI). An accurate segmentation of CCI can be very important for understanding the etiology of OA and ex-vivo evaluation of tidemark condition at early OA stages. In this paper, we present the first application of Deep Learning to PTA-stained osteochondral samples that allows to perform tidemark segmentation in a fully-automatic manner. Our method is based on U-Net trained using a combination of binary cross-entropy and soft Jaccard loss. On cross-validation, this approach yielded intersection over the union of 0.59, 0.70, 0.79, 0.83 and 0.86 within 15 {\mu}m, 30 {\mu}m, 45 {\mu}m, 60 {\mu}m and 75 {\mu}m padded zones around the tidemark, respectively. Our codes and the dataset that consisted of 35 PTA-stained human AC samples are made publicly available together with the segmentation masks to facilitate the development of biomedical image segmentation methods.



### Efficient Semantic Scene Completion Network with Spatial Group Convolution
- **Arxiv ID**: http://arxiv.org/abs/1907.05091v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.05091v1)
- **Published**: 2019-07-11 10:29:03+00:00
- **Updated**: 2019-07-11 10:29:03+00:00
- **Authors**: Jiahui Zhang, Hao Zhao, Anbang Yao, Yurong Chen, Li Zhang, Hongen Liao
- **Comment**: An oral paper in ECCV 2018, and the code is available at
  https://github.com/zjhthu/SGC-Release.git
- **Journal**: None
- **Summary**: We introduce Spatial Group Convolution (SGC) for accelerating the computation of 3D dense prediction tasks. SGC is orthogonal to group convolution, which works on spatial dimensions rather than feature channel dimension. It divides input voxels into different groups, then conducts 3D sparse convolution on these separated groups. As only valid voxels are considered when performing convolution, computation can be significantly reduced with a slight loss of accuracy. The proposed operations are validated on semantic scene completion task, which aims to predict a complete 3D volume with semantic labels from a single depth image. With SGC, we further present an efficient 3D sparse convolutional network, which harnesses a multiscale architecture and a coarse-to-fine prediction strategy. Evaluations are conducted on the SUNCG dataset, achieving state-of-the-art performance and fast speed. Code is available at https://github.com/zjhthu/SGC-Release.git



### Activitynet 2019 Task 3: Exploring Contexts for Dense Captioning Events in Videos
- **Arxiv ID**: http://arxiv.org/abs/1907.05092v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.05092v1)
- **Published**: 2019-07-11 10:29:04+00:00
- **Updated**: 2019-07-11 10:29:04+00:00
- **Authors**: Shizhe Chen, Yuqing Song, Yida Zhao, Qin Jin, Zhaoyang Zeng, Bei Liu, Jianlong Fu, Alexander Hauptmann
- **Comment**: Winner solution in CVPR 2019 Activitynet Dense Video Captioning
  challenge
- **Journal**: None
- **Summary**: Contextual reasoning is essential to understand events in long untrimmed videos. In this work, we systematically explore different captioning models with various contexts for the dense-captioning events in video task, which aims to generate captions for different events in the untrimmed video. We propose five types of contexts as well as two categories of event captioning models, and evaluate their contributions for event captioning from both accuracy and diversity aspects. The proposed captioning models are plugged into our pipeline system for the dense video captioning challenge. The overall system achieves the state-of-the-art performance on the dense-captioning events in video task with 9.91 METEOR score on the challenge testing set.



### Multifaceted Analysis of Fine-Tuning in Deep Model for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.05099v1
- **DOI**: 10.1145/3319500
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.05099v1)
- **Published**: 2019-07-11 10:47:49+00:00
- **Updated**: 2019-07-11 10:47:49+00:00
- **Authors**: Xiangyang Li, Luis Herranz, Shuqiang Jiang
- **Comment**: Accepted by ACM Transactions on Data Science
- **Journal**: None
- **Summary**: In recent years, convolutional neural networks (CNNs) have achieved impressive performance for various visual recognition scenarios. CNNs trained on large labeled datasets can not only obtain significant performance on most challenging benchmarks but also provide powerful representations, which can be used to a wide range of other tasks. However, the requirement of massive amounts of data to train deep neural networks is a major drawback of these models, as the data available is usually limited or imbalanced. Fine-tuning (FT) is an effective way to transfer knowledge learned in a source dataset to a target task. In this paper, we introduce and systematically investigate several factors that influence the performance of fine-tuning for visual recognition. These factors include parameters for the retraining procedure (e.g., the initial learning rate of fine-tuning), the distribution of the source and target data (e.g., the number of categories in the source dataset, the distance between the source and target datasets) and so on. We quantitatively and qualitatively analyze these factors, evaluate their influence, and present many empirical observations. The results reveal insights into what fine-tuning changes CNN parameters and provide useful and evidence-backed intuitions about how to implement fine-tuning for computer vision tasks.



### Image-Based Size Analysis of Agglomerated and Partially Sintered Particles via Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.05112v3
- **DOI**: 10.1016/j.powtec.2019.10.020
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.05112v3)
- **Published**: 2019-07-11 11:12:17+00:00
- **Updated**: 2019-11-18 22:42:13+00:00
- **Authors**: Max Frei, Frank Einar Kruis
- **Comment**: 17 pages, 20 figures, 4 tables; supplementary materials
- **Journal**: None
- **Summary**: There is a high demand for fully automated methods for the analysis of primary particle size distributions of agglomerated, sintered or occluded primary particles, due to their impact on material properties. Therefore, a novel, deep learning-based, method for the detection of such primary particles was proposed and tested, which renders a manual tuning of analysis parameters unnecessary. As a specialty, the training of the utilized convolutional neural networks was carried out using only synthetic images, thereby avoiding the laborious task of manual annotation and increasing the ground truth quality. Nevertheless, the proposed method performs excellent on real world samples of sintered silica nanoparticles with various sintering degrees and varying image conditions. In a direct comparison, the proposed method clearly outperforms two state-of-the-art methods for automated image-based particle size analysis (Hough transformation and the ImageJ ParticleSizer plug-in), thereby attaining human-like performance.



### Deep Active Learning for Axon-Myelin Segmentation on Histology Data
- **Arxiv ID**: http://arxiv.org/abs/1907.05143v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.05143v1)
- **Published**: 2019-07-11 12:31:30+00:00
- **Updated**: 2019-07-11 12:31:30+00:00
- **Authors**: Melanie Lubrano di Scandalea, Christian S. Perone, Mathieu Boudreau, Julien Cohen-Adad
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation is a crucial task in biomedical image processing, which recent breakthroughs in deep learning have allowed to improve. However, deep learning methods in general are not yet widely used in practice since they require large amount of data for training complex models. This is particularly challenging for biomedical images, because data and ground truths are a scarce resource. Annotation efforts for biomedical images come with a real cost, since experts have to manually label images at pixel-level on samples usually containing many instances of the target anatomy (e.g. in histology samples: neurons, astrocytes, mitochondria, etc.). In this paper we provide a framework for Deep Active Learning applied to a real-world scenario. Our framework relies on the U-Net architecture and overall uncertainty measure to suggest which sample to annotate. It takes advantage of the uncertainty measure obtained by taking Monte Carlo samples while using Dropout regularization scheme. Experiments were done on spinal cord and brain microscopic histology samples to perform a myelin segmentation task. Two realistic small datasets of 14 and 24 images were used, from different acquisition settings (Serial Block-Face Electron Microscopy and Transmitting Electron Microscopy) and showed that our method reached a maximum Dice value after adding 3 uncertainty-selected samples to the initial training set, versus 15 randomly-selected samples, thereby significantly reducing the annotation effort. We focused on a plausible scenario and showed evidence that this straightforward implementation achieves a high segmentation performance with very few labelled samples. We believe our framework may benefit any biomedical researcher willing to obtain fast and accurate image segmentation on their own dataset. The code is freely available at https://github.com/neuropoly/deep-active-learning.



### Disease classification of macular Optical Coherence Tomography scans using deep learning software: validation on independent, multi-centre data
- **Arxiv ID**: http://arxiv.org/abs/1907.05164v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.05164v1)
- **Published**: 2019-07-11 12:53:17+00:00
- **Updated**: 2019-07-11 12:53:17+00:00
- **Authors**: Kanwal K. Bhatia, Mark S. Graham, Louise Terry, Ashley Wood, Paris Tranos, Sameer Trikha, Nicolas Jaccard
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: To evaluate Pegasus-OCT, a clinical decision support software for the identification of features of retinal disease from macula OCT scans, across heterogenous populations involving varying patient demographics, device manufacturers, acquisition sites and operators.   Methods: 5,588 normal and anomalous macular OCT volumes (162,721 B-scans), acquired at independent centres in five countries, were processed using the software. Results were evaluated against ground truth provided by the dataset owners.   Results: Pegasus-OCT performed with AUROCs of at least 98% for all datasets in the detection of general macular anomalies. For scans of sufficient quality, the AUROCs for general AMD and DME detection were found to be at least 99% and 98%, respectively.   Conclusions: The ability of a clinical decision support system to cater for different populations is key to its adoption. Pegasus-OCT was shown to be able to detect AMD, DME and general anomalies in OCT volumes acquired across multiple independent sites with high performance. Its use thus offers substantial promise, with the potential to alleviate the burden of growing demand in eye care services caused by retinal disease.



### Edge Heuristic GAN for Non-uniform Blind Deblurring
- **Arxiv ID**: http://arxiv.org/abs/1907.05185v1
- **DOI**: 10.1109/LSP.2019.2939752
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.05185v1)
- **Published**: 2019-07-11 13:33:34+00:00
- **Updated**: 2019-07-11 13:33:34+00:00
- **Authors**: Shuai Zheng, Zhenfeng Zhu, Jian Cheng, Yandong Guo, Yao Zhao
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: Non-uniform blur, mainly caused by camera shake and motions of multiple objects, is one of the most common causes of image quality degradation. However, the traditional blind deblurring methods based on blur kernel estimation do not perform well on complicated non-uniform motion blurs. Recent studies show that GAN-based approaches achieve impressive performance on deblurring tasks. In this letter, to further improve the performance of GAN-based methods on deblurring tasks, we propose an edge heuristic multi-scale generative adversarial network(GAN), which uses the "coarse-to-fine" scheme to restore clear images in an end-to-end manner. In particular, an edge-enhanced network is designed to generate sharp edges as auxiliary information to guide the deblurring process. Furthermore, We propose a hierarchical content loss function for deblurring tasks. Extensive experiments on different datasets show that our method achieves state-of-the-art performance in dynamic scene deblurring.



### Cross-Domain Complementary Learning Using Pose for Multi-Person Part Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.05193v2
- **DOI**: 10.1109/TCSVT.2020.2995122
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.05193v2)
- **Published**: 2019-07-11 13:47:18+00:00
- **Updated**: 2020-05-14 00:22:29+00:00
- **Authors**: Kevin Lin, Lijuan Wang, Kun Luo, Yinpeng Chen, Zicheng Liu, Ming-Ting Sun
- **Comment**: To appear in IEEE Transactions on Circuits and Systems for Video
  Technology; Presented at ICCV 2019 Demonstration
- **Journal**: None
- **Summary**: Supervised deep learning with pixel-wise training labels has great successes on multi-person part segmentation. However, data labeling at pixel-level is very expensive. To solve the problem, people have been exploring to use synthetic data to avoid the data labeling. Although it is easy to generate labels for synthetic data, the results are much worse compared to those using real data and manual labeling. The degradation of the performance is mainly due to the domain gap, i.e., the discrepancy of the pixel value statistics between real and synthetic data. In this paper, we observe that real and synthetic humans both have a skeleton (pose) representation. We found that the skeletons can effectively bridge the synthetic and real domains during the training. Our proposed approach takes advantage of the rich and realistic variations of the real data and the easily obtainable labels of the synthetic data to learn multi-person part segmentation on real images without any human-annotated labels. Through experiments, we show that without any human labeling, our method performs comparably to several state-of-the-art approaches which require human labeling on Pascal-Person-Parts and COCO-DensePose datasets. On the other hand, if part labels are also available in the real-images during training, our method outperforms the supervised state-of-the-art methods by a large margin. We further demonstrate the generalizability of our method on predicting novel keypoints in real images where no real data labels are available for the novel keypoints detection. Code and pre-trained models are available at https://github.com/kevinlin311tw/CDCL-human-part-segmentation



### Aerial Animal Biometrics: Individual Friesian Cattle Recovery and Visual Identification via an Autonomous UAV with Onboard Deep Inference
- **Arxiv ID**: http://arxiv.org/abs/1907.05310v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.05310v1)
- **Published**: 2019-07-11 15:37:58+00:00
- **Updated**: 2019-07-11 15:37:58+00:00
- **Authors**: William Andrew, Colin Greatwood, Tilo Burghardt
- **Comment**: Accepted 7 page manuscript to be presented at IROS 2019
- **Journal**: None
- **Summary**: This paper describes a computationally-enhanced M100 UAV platform with an onboard deep learning inference system for integrated computer vision and navigation able to autonomously find and visually identify by coat pattern individual Holstein Friesian cattle in freely moving herds. We propose an approach that utilises three deep convolutional neural network architectures running live onboard the aircraft; that is, a YoloV2-based species detector, a dual-stream CNN delivering exploratory agency and an InceptionV3-based biometric LRCN for individual animal identification. We evaluate the performance of each of the components offline, and also online via real-world field tests comprising 146.7 minutes of autonomous low altitude flight in a farm environment over a dispersed herd of 17 heifer dairy cows. We report error-free identification performance on this online experiment. The presented proof-of-concept system is the first of its kind and a successful step towards autonomous biometric identification of individual animals from the air in open pasture environments for tag-less AI support in farming and ecology.



### Graph Neural Based End-to-end Data Association Framework for Online Multiple-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1907.05315v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.05315v1)
- **Published**: 2019-07-11 15:43:38+00:00
- **Updated**: 2019-07-11 15:43:38+00:00
- **Authors**: Xiaolong Jiang, Peizhao Li, Yanjing Li, Xiantong Zhen
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present an end-to-end framework to settle data association in online Multiple-Object Tracking (MOT). Given detection responses, we formulate the frame-by-frame data association as Maximum Weighted Bipartite Matching problem, whose solution is learned using a neural network. The network incorporates an affinity learning module, wherein both appearance and motion cues are investigated to encode object feature representation and compute pairwise affinities. Employing the computed affinities as edge weights, the following matching problem on a bipartite graph is resolved by the optimization module, which leverages a graph neural network to adapt with the varying cardinalities of the association problem and solve the combinatorial hardness with favorable scalability and compatibility. To facilitate effective training of the proposed tracking network, we design a multi-level matrix loss in conjunction with the assembled supervision methodology. Being trained end-to-end, all modules in the tracker can co-adapt and co-operate collaboratively, resulting in improved model adaptiveness and less parameter-tuning efforts. Experiment results on the MOT benchmarks demonstrate the efficacy of the proposed approach.



### Robust GPU-based Virtual Reality Simulation of Radio Frequency Ablations for Various Needle Geometries and Locations
- **Arxiv ID**: http://arxiv.org/abs/1907.05709v1
- **DOI**: 10.1007/s11548-019-02033-w
- **Categories**: **physics.med-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.05709v1)
- **Published**: 2019-07-11 15:53:36+00:00
- **Updated**: 2019-07-11 15:53:36+00:00
- **Authors**: Niclas Kath, Heinz Handels, Andre Mastmeyer
- **Comment**: 18 pages, 14 figures, 1 table, 2 algorithms, 2 movies
- **Journal**: International Journal of Computer Assisted Radiology and Surgery,
  2019
- **Summary**: Purpose: Radio-frequency ablations play an important role in the therapy of malignant liver lesions. The navigation of a needle to the lesion poses a challenge for both the trainees and intervening physicians. Methods: This publication presents a new GPU-based, accurate method for the simulation of radio-frequency ablations for lesions at the needle tip in general and for an existing visuo-haptic 4D VR simulator. The method is implemented real-time capable with Nvidia CUDA. Results: It performs better than a literature method concerning the theoretical characteristic of monotonic convergence of the bioheat PDE and a in vitro gold standard with significant improvements (p < 0.05) in terms of Pearson correlations. It shows no failure modes or theoretically inconsistent individual simulation results after the initial phase of 10 seconds. On the Nvidia 1080 Ti GPU it achieves a very high frame rendering performance of >480 Hz. Conclusion: Our method provides a more robust and safer real-time ablation planning and intraoperative guidance technique, especially avoiding the over-estimation of the ablated tissue death zone, which is risky for the patient in terms of tumor recurrence. Future in vitro measurements and optimization shall further improve the conservative estimate.



### Online Inference and Detection of Curbs in Partially Occluded Scenes with Sparse LIDAR
- **Arxiv ID**: http://arxiv.org/abs/1907.05375v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.05375v1)
- **Published**: 2019-07-11 16:50:38+00:00
- **Updated**: 2019-07-11 16:50:38+00:00
- **Authors**: Tarlan Suleymanov, Lars Kunze, Paul Newman
- **Comment**: Accepted at the 22nd IEEE Intelligent Transportation Systems
  Conference (ITSC19), October, 2019, Auckland, New Zealand
- **Journal**: None
- **Summary**: Road boundaries, or curbs, provide autonomous vehicles with essential information when interpreting road scenes and generating behaviour plans. Although curbs convey important information, they are difficult to detect in complex urban environments (in particular in comparison to other elements of the road such as traffic signs and road markings). These difficulties arise from occlusions by other traffic participants as well as changing lighting and/or weather conditions. Moreover, road boundaries have various shapes, colours and structures while motion planning algorithms require accurate and precise metric information in real-time to generate their plans.   In this paper, we present a real-time LIDAR-based approach for accurate curb detection around the vehicle (360 degree). Our approach deals with both occlusions from traffic and changing environmental conditions. To this end, we project 3D LIDAR pointcloud data into 2D bird's-eye view images (akin to Inverse Perspective Mapping). These images are then processed by trained deep networks to infer both visible and occluded road boundaries. Finally, a post-processing step filters detected curb segments and tracks them over time. Experimental results demonstrate the effectiveness of the proposed approach on real-world driving data. Hence, we believe that our LIDAR-based approach provides an efficient and effective way to detect visible and occluded curbs around the vehicles in challenging driving scenarios.



### Monocular 3D Sway Tracking for Assessing Postural Instability in Cerebral Hypoperfusion During Quiet Standing
- **Arxiv ID**: http://arxiv.org/abs/1907.05376v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1907.05376v2)
- **Published**: 2019-07-11 16:51:39+00:00
- **Updated**: 2019-11-05 21:50:11+00:00
- **Authors**: Robert Amelard, Kevin R Murray, Eric T Hedge, Taylor W Cleworth, Mamiko Noguchi, Andrew Laing, Richard L Hughson
- **Comment**: None
- **Journal**: None
- **Summary**: Postural instability is prevalent in aging and neurodegenerative disease, decreasing quality of life and independence. Quantitatively monitoring balance control is important for assessing treatment efficacy and rehabilitation progress. However, existing technologies for assessing postural sway are complex and expensive, limiting their widespread utility. Here, we propose a monocular imaging system capable of assessing sub-millimeter 3D sway dynamics during quiet standing. Two anatomical targets with known feature geometries were placed on the lumbar and shoulder. Upper and lower trunk 3D kinematic motion was automatically assessed from a set of 2D frames through geometric feature tracking and an inverse motion model. Sway was tracked in 3D and compared between control and hypoperfusion conditions in 14 healthy young adults. The proposed system demonstrated high agreement with a commercial motion capture system (error $1.5 \times 10^{-4}~\text{mm}$, [$-0.52$, $0.52$]). Between-condition differences in sway dynamics were observed in anterior-posterior sway during early and mid stance, and medial-lateral sway during mid stance commensurate with decreased cerebral perfusion, followed by recovered sway dynamics during late stance with cerebral perfusion recovery. This inexpensive single-camera system enables quantitative 3D sway monitoring for assessing neuromuscular balance control in weakly constrained environments.



### Single Image Super-Resolution via CNN Architectures and TV-TV Minimization
- **Arxiv ID**: http://arxiv.org/abs/1907.05380v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1907.05380v2)
- **Published**: 2019-07-11 16:57:59+00:00
- **Updated**: 2019-11-27 21:32:39+00:00
- **Authors**: Marija Vella, João F. C. Mota
- **Comment**: Accepted to BMVC 2019; v2 contains updated results and minor bug
  fixes
- **Journal**: None
- **Summary**: Super-resolution (SR) is a technique that allows increasing the resolution of a given image. Having applications in many areas, from medical imaging to consumer electronics, several SR methods have been proposed. Currently, the best performing methods are based on convolutional neural networks (CNNs) and require extensive datasets for training. However, at test time, they fail to impose consistency between the super-resolved image and the given low-resolution image, a property that classic reconstruction-based algorithms naturally enforce in spite of having poorer performance. Motivated by this observation, we propose a new framework that joins both approaches and produces images with superior quality than any of the prior methods. Although our framework requires additional computation, our experiments on Set5, Set14, and BSD100 show that it systematically produces images with better peak signal to noise ratio (PSNR) and structural similarity (SSIM) than the current state-of-the-art CNN architectures for SR.



### On the Evaluation of Conditional GANs
- **Arxiv ID**: http://arxiv.org/abs/1907.08175v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.08175v3)
- **Published**: 2019-07-11 17:41:57+00:00
- **Updated**: 2019-12-24 02:53:54+00:00
- **Authors**: Terrance DeVries, Adriana Romero, Luis Pineda, Graham W. Taylor, Michal Drozdzal
- **Comment**: None
- **Journal**: None
- **Summary**: Conditional Generative Adversarial Networks (cGANs) are finding increasingly widespread use in many application domains. Despite outstanding progress, quantitative evaluation of such models often involves multiple distinct metrics to assess different desirable properties, such as image quality, conditional consistency, and intra-conditioning diversity. In this setting, model benchmarking becomes a challenge, as each metric may indicate a different "best" model. In this paper, we propose the Frechet Joint Distance (FJD), which is defined as the Frechet distance between joint distributions of images and conditioning, allowing it to implicitly capture the aforementioned properties in a single metric. We conduct proof-of-concept experiments on a controllable synthetic dataset, which consistently highlight the benefits of FJD when compared to currently established metrics. Moreover, we use the newly introduced metric to compare existing cGAN-based models for a variety of conditioning modalities (e.g. class labels, object masks, bounding boxes, images, and text captions). We show that FJD can be used as a promising single metric for cGAN benchmarking and model selection. Code can be found at https://github.com/facebookresearch/fjd.



### Adversarial Objects Against LiDAR-Based Autonomous Driving Systems
- **Arxiv ID**: http://arxiv.org/abs/1907.05418v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.05418v1)
- **Published**: 2019-07-11 17:59:13+00:00
- **Updated**: 2019-07-11 17:59:13+00:00
- **Authors**: Yulong Cao, Chaowei Xiao, Dawei Yang, Jing Fang, Ruigang Yang, Mingyan Liu, Bo Li
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are found to be vulnerable against adversarial examples, which are carefully crafted inputs with a small magnitude of perturbation aiming to induce arbitrarily incorrect predictions. Recent studies show that adversarial examples can pose a threat to real-world security-critical applications: a "physical adversarial Stop Sign" can be synthesized such that the autonomous driving cars will misrecognize it as others (e.g., a speed limit sign). However, these image-space adversarial examples cannot easily alter 3D scans of widely equipped LiDAR or radar on autonomous vehicles. In this paper, we reveal the potential vulnerabilities of LiDAR-based autonomous driving detection systems, by proposing an optimization based approach LiDAR-Adv to generate adversarial objects that can evade the LiDAR-based detection system under various conditions. We first show the vulnerabilities using a blackbox evolution-based algorithm, and then explore how much a strong adversary can do, using our gradient-based approach LiDAR-Adv. We test the generated adversarial objects on the Baidu Apollo autonomous driving platform and show that such physical systems are indeed vulnerable to the proposed attacks. We also 3D-print our adversarial objects and perform physical experiments to illustrate that such vulnerability exists in the real world. Please find more visualizations and results on the anonymous website: https://sites.google.com/view/lidar-adv.



### Hybrid Residual Attention Network for Single Image Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/1907.05514v1
- **DOI**: 10.1109/ACCESS.2019.2942346
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.05514v1)
- **Published**: 2019-07-11 22:48:23+00:00
- **Updated**: 2019-07-11 22:48:23+00:00
- **Authors**: Abdul Muqeet, Md Tauhid Bin Iqbal, Sung-Ho Bae
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: The extraction and proper utilization of convolution neural network (CNN) features have a significant impact on the performance of image super-resolution (SR). Although CNN features contain both the spatial and channel information, current deep techniques on SR often suffer to maximize performance due to using either the spatial or channel information. Moreover, they integrate such information within a deep or wide network rather than exploiting all the available features, eventually resulting in high computational complexity. To address these issues, we present a binarized feature fusion (BFF) structure that utilizes the extracted features from residual groups (RG) in an effective way. Each residual group (RG) consists of multiple hybrid residual attention blocks (HRAB) that effectively integrates the multiscale feature extraction module and channel attention mechanism in a single block. Furthermore, we use dilated convolutions with different dilation factors to extract multiscale features. We also propose to adopt global, short and long skip connections and residual groups (RG) structure to ease the flow of information without losing important features details. In the paper, we call this overall network architecture as hybrid residual attention network (HRAN). In the experiment, we have observed the efficacy of our method against the state-of-the-art methods for both the quantitative and qualitative comparisons.



### Graph-Structured Visual Imitation
- **Arxiv ID**: http://arxiv.org/abs/1907.05518v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.05518v2)
- **Published**: 2019-07-11 23:06:16+00:00
- **Updated**: 2020-03-05 01:33:45+00:00
- **Authors**: Maximilian Sieb, Zhou Xian, Audrey Huang, Oliver Kroemer, Katerina Fragkiadaki
- **Comment**: 8 pages, 3 figures, 1 table
- **Journal**: None
- **Summary**: We cast visual imitation as a visual correspondence problem. Our robotic agent is rewarded when its actions result in better matching of relative spatial configurations for corresponding visual entities detected in its workspace and teacher's demonstration. We build upon recent advances in Computer Vision,such as human finger keypoint detectors, object detectors trained on-the-fly with synthetic augmentations, and point detectors supervised by viewpoint changes and learn multiple visual entity detectors for each demonstration without human annotations or robot interactions. We empirically show the proposed factorized visual representations of entities and their spatial arrangements drive successful imitation of a variety of manipulation skills within minutes, using a single demonstration and without any environment instrumentation. It is robust to background clutter and can effectively generalize across environment variations between demonstrator and imitator, greatly outperforming unstructured non-factorized full-frame CNN encodings of previous works.



