# Arxiv Papers in cs.CV on 2019-07-08
### Travel Time Estimation without Road Networks: An Urban Morphological Layout Representation Approach
- **Arxiv ID**: http://arxiv.org/abs/1907.03381v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.03381v1)
- **Published**: 2019-07-08 01:52:35+00:00
- **Updated**: 2019-07-08 01:52:35+00:00
- **Authors**: Wuwei Lan, Yanyan Xu, Bin Zhao
- **Comment**: Accepted at IJCAI 2019
- **Journal**: None
- **Summary**: Travel time estimation is a crucial task for not only personal travel scheduling but also city planning. Previous methods focus on modeling toward road segments or sub-paths, then summing up for a final prediction, which have been recently replaced by deep neural models with end-to-end training. Usually, these methods are based on explicit feature representations, including spatio-temporal features, traffic states, etc. Here, we argue that the local traffic condition is closely tied up with the land-use and built environment, i.e., metro stations, arterial roads, intersections, commercial area, residential area, and etc, yet the relation is time-varying and too complicated to model explicitly and efficiently. Thus, this paper proposes an end-to-end multi-task deep neural model, named Deep Image to Time (DeepI2T), to learn the travel time mainly from the built environment images, a.k.a. the morphological layout images, and showoff the new state-of-the-art performance on real-world datasets in two cities. Moreover, our model is designed to tackle both path-aware and path-blind scenarios in the testing phase. This work opens up new opportunities of using the publicly available morphological layout images as considerable information in multiple geography-related smart city applications.



### Learning Structural Graph Layouts and 3D Shapes for Long Span Bridges 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1907.03387v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1907.03387v2)
- **Published**: 2019-07-08 02:39:04+00:00
- **Updated**: 2020-05-21 12:42:03+00:00
- **Authors**: Fangqiao Hu, Jin Zhao, Yong Huang, Hui Li
- **Comment**: None
- **Journal**: None
- **Summary**: A learning-based 3D reconstruction method for long-span bridges is proposed in this paper. 3D reconstruction generates a 3D computer model of a real object or scene from images, it involves many stages and open problems. Existing point-based methods focus on generating 3D point clouds and their reconstructed polygonal mesh or fitting-based geometrical models in urban scenes civil structures reconstruction within Manhattan world constrains and have made great achievements. Difficulties arise when an attempt is made to transfer these systems to structures with complex topology and part relations like steel trusses and long-span bridges, this could be attributed to point clouds are often unevenly distributed with noise and suffer from occlusions and incompletion, recovering a satisfactory 3D model from these highly unstructured point clouds in a bottom-up pattern while preserving the geometrical and topological properties makes enormous challenge to existing algorithms. Considering the prior human knowledge that these structures are in conformity to regular spatial layouts in terms of components, a learning-based topology-aware 3D reconstruction method which can obtain high-level structural graph layouts and low-level 3D shapes from images is proposed in this paper. We demonstrate the feasibility of this method by testing on two real long-span steel truss cable-stayed bridges.



### Facial Makeup Transfer Combining Illumination Transfer
- **Arxiv ID**: http://arxiv.org/abs/1907.03398v1
- **DOI**: 10.1109/ACCESS.2019.2923116
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.03398v1)
- **Published**: 2019-07-08 04:12:25+00:00
- **Updated**: 2019-07-08 04:12:25+00:00
- **Authors**: Xin Jin, Rui Han, Ning Ning, Xiaodong Li, Xiaokun Zhang
- **Comment**: IEEE Access, conference short version: ISAIR2019
- **Journal**: in IEEE Access, vol. 7, pp. 80928-80936, 2019
- **Summary**: To meet the women appearance needs, we present a novel virtual experience approach of facial makeup transfer, developed into windows platform application software. The makeup effects could present on the user's input image in real time, with an only single reference image. The input image and reference image are divided into three layers by facial feature points landmarked: facial structure layer, facial color layer, and facial detail layer. Except for the above layers are processed by different algorithms to generate output image, we also add illumination transfer, so that the illumination effect of the reference image is automatically transferred to the input image. Our approach has the following three advantages: (1) Black or dark and white facial makeup could be effectively transferred by introducing illumination transfer; (2) Efficiently transfer facial makeup within seconds compared to those methods based on deep learning frameworks; (3) Reference images with the air-bangs could transfer makeup perfectly.



### Distill-2MD-MTL: Data Distillation based on Multi-Dataset Multi-Domain Multi-Task Frame Work to Solve Face Related Tasksks, Multi Task Learning, Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.03402v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.03402v2)
- **Published**: 2019-07-08 04:36:32+00:00
- **Updated**: 2019-07-09 02:57:13+00:00
- **Authors**: Sepidehsadat Hosseini, Mohammad Amin Shabani, Nam Ik Cho
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new semi-supervised learning method on face-related tasks based on Multi-Task Learning (MTL) and data distillation. The proposed method exploits multiple datasets with different labels for different-but-related tasks such as simultaneous age, gender, race, facial expression estimation. Specifically, when there are only a few well-labeled data for a specific task among the multiple related ones, we exploit the labels of other related tasks in different domains. Our approach is composed of (1) a new MTL method which can deal with weakly labeled datasets and perform several tasks simultaneously, and (2) an MTL-based data distillation framework which enables network generalization for the training and test data from different domains. Experiments show that the proposed multi-task system performs each task better than the baseline single task. It is also demonstrated that using different domain datasets along with the main dataset can enhance network generalization and overcome the domain differences between datasets. Also, comparing data distillation both on the baseline and MTL framework, the latter shows more accurate predictions on unlabeled data from different domains. Furthermore, by proposing a new learning-rate optimization method, our proposed network is able to dynamically tune its learning rate.



### Bootstrap Model Ensemble and Rank Loss for Engagement Intensity Regression
- **Arxiv ID**: http://arxiv.org/abs/1907.03422v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.03422v1)
- **Published**: 2019-07-08 06:54:47+00:00
- **Updated**: 2019-07-08 06:54:47+00:00
- **Authors**: Kai Wang, Jianfei Yang, Da Guo, Kaipeng Zhang, Xiaojiang Peng, Yu Qiao
- **Comment**: This paper is about EmotiW 2019 engagement intensity regression
  challenge
- **Journal**: None
- **Summary**: This paper presents our approach for the engagement intensity regression task of EmotiW 2019. The task is to predict the engagement intensity value of a student when he or she is watching an online MOOCs video in various conditions. Based on our winner solution last year, we mainly explore head features and body features with a bootstrap strategy and two novel loss functions in this paper. We maintain the framework of multi-instance learning with long short-term memory (LSTM) network, and make three contributions. First, besides of the gaze and head pose features, we explore facial landmark features in our framework. Second, inspired by the fact that engagement intensity can be ranked in values, we design a rank loss as a regularization which enforces a distance margin between the features of distant category pairs and adjacent category pairs. Third, we use the classical bootstrap aggregation method to perform model ensemble which randomly samples a certain training data by several times and then averages the model predictions. We evaluate the performance of our method and discuss the influence of each part on the validation dataset. Our methods finally win 3rd place with MSE of 0.0626 on the testing set.



### Segway DRIVE Benchmark: Place Recognition and SLAM Data Collected by A Fleet of Delivery Robots
- **Arxiv ID**: http://arxiv.org/abs/1907.03424v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.03424v1)
- **Published**: 2019-07-08 07:06:14+00:00
- **Updated**: 2019-07-08 07:06:14+00:00
- **Authors**: Jianzhu Huai, Yusen Qin, Fumin Pang, Zichong Chen
- **Comment**: 8 pages, 4 figures, 4 tables, conference
- **Journal**: None
- **Summary**: Visual place recognition and simultaneous localization and mapping (SLAM) have recently begun to be used in real-world autonomous navigation tasks like food delivery. Existing datasets for SLAM research are often not representative of in situ operations, leaving a gap between academic research and real-world deployment. In response, this paper presents the Segway DRIVE benchmark, a novel and challenging dataset suite collected by a fleet of Segway delivery robots. Each robot is equipped with a global-shutter fisheye camera, a consumer-grade IMU synced to the camera on chip, two low-cost wheel encoders, and a removable high-precision lidar for generating reference solutions. As they routinely carry out tasks in office buildings and shopping malls while collecting data, the dataset spanning a year is characterized by planar motions, moving pedestrians in scenes, and changing environment and lighting. Such factors typically pose severe challenges and may lead to failures for SLAM algorithms. Moreover, several metrics are proposed to evaluate metric place recognition algorithms. With these metrics, sample SLAM and metric place recognition methods were evaluated on this benchmark.   The first release of our benchmark has hundreds of sequences, covering more than 50 km of indoor floors. More data will be added as the robot fleet continues to operate in real life. The benchmark is available at http://drive.segwayrobotics.com/#/dataset/download.



### Perceptual representations of structural information in images: application to quality assessment of synthesized view in FTV scenario
- **Arxiv ID**: http://arxiv.org/abs/1907.03448v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.03448v1)
- **Published**: 2019-07-08 08:23:31+00:00
- **Updated**: 2019-07-08 08:23:31+00:00
- **Authors**: Ling suiyi, Li Jing, Le Callet Patrick, Wang Junle
- **Comment**: None
- **Journal**: None
- **Summary**: As the immersive multimedia techniques like Free-viewpoint TV (FTV) develop at an astonishing rate, user's demand for high-quality immersive contents increases dramatically. Unlike traditional uniform artifacts, the distortions within immersive contents could be non-uniform structure-related and thus are challenging for commonly used quality metrics. Recent studies have demonstrated that the representation of visual features can be extracted from multiple levels of the hierarchy. Inspired by the hierarchical representation mechanism in the human visual system (HVS), in this paper, we explore to adopt structural representations to quantitatively measure the impact of such structure-related distortion on perceived quality in FTV scenario. More specifically, a bio-inspired full reference image quality metric is proposed based on 1) low-level contour descriptor; 2) mid-level contour category descriptor; and 3) task-oriented non-natural structure descriptor. The experimental results show that the proposed model outperforms significantly the state-of-the-art metrics.



### A unified neural network for object detection, multiple object tracking and vehicle re-identification
- **Arxiv ID**: http://arxiv.org/abs/1907.03465v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.03465v1)
- **Published**: 2019-07-08 09:07:22+00:00
- **Updated**: 2019-07-08 09:07:22+00:00
- **Authors**: Yuhao Xu, Jiakui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep SORT\cite{wojke2017simple} is a tracking-by-detetion approach to multiple object tracking with a detector and a RE-ID model.   Both separately training and inference with the two model is time-comsuming.   In this paper, we unify the detector and RE-ID model into an end-to-end network, by adding an additional track branch for tracking in Faster RCNN architecture. With a unified network, we are able to train the whole model end-to-end with multi loss, which has shown much benefit in other recent works.   The RE-ID model in Deep SORT needs to use deep CNNs to extract feature map from detected object images, However, track branch in our proposed network straight make use of   the RoI feature vector in Faster RCNN baseline, which reduced the amount of calculation.   Since the single image lacks the same object which is necessary when we use the triplet loss to optimizer the track branch, we concatenate the neighbouring frames in a video to construct our training dataset.   We have trained and evaluated our model on AIC19 vehicle tracking dataset, experiment shows that our model with resnet101 backbone can achieve 57.79 \% mAP and track vehicle well.



### A Deep Learning Approach for Real-Time 3D Human Action Recognition from Skeletal Data
- **Arxiv ID**: http://arxiv.org/abs/1907.03520v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.03520v2)
- **Published**: 2019-07-08 11:22:41+00:00
- **Updated**: 2022-08-10 04:43:35+00:00
- **Authors**: Huy Hieu Pham, Houssam Salmane, Louahdi Khoudour, Alain Crouzil, Pablo Zegers, Sergio A Velastin
- **Comment**: Accepted for publication by the 16th International Conference on
  Image Analysis and Recognition (ICIAR 2019)
- **Journal**: None
- **Summary**: We present a new deep learning approach for real-time 3D human action recognition from skeletal data and apply it to develop a vision-based intelligent surveillance system. Given a skeleton sequence, we propose to encode skeleton poses and their motions into a single RGB image. An Adaptive Histogram Equalization (AHE) algorithm is then applied on the color images to enhance their local patterns and generate more discriminative features. For learning and classification tasks, we design Deep Neural Networks based on the Densely Connected Convolutional Architecture (DenseNet) to extract features from enhanced-color images and classify them into classes. Experimental results on two challenging datasets show that the proposed method reaches state-of-the-art accuracy, whilst requiring low computational time for training and inference. This paper also introduces CEMEST, a new RGB-D dataset depicting passenger behaviors in public transport. It consists of 203 untrimmed real-world surveillance videos of realistic normal and anomalous events. We achieve promising results on real conditions of this dataset with the support of data augmentation and transfer learning techniques. This enables the construction of real-world applications based on deep learning for enhancing monitoring and security in public transport.



### Linking Art through Human Poses
- **Arxiv ID**: http://arxiv.org/abs/1907.03537v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.03537v1)
- **Published**: 2019-07-08 12:06:33+00:00
- **Updated**: 2019-07-08 12:06:33+00:00
- **Authors**: Tomas Jenicek, Ondřej Chum
- **Comment**: None
- **Journal**: None
- **Summary**: We address the discovery of composition transfer in artworks based on their visual content. Automated analysis of large art collections, which are growing as a result of art digitization among museums and galleries, is an important tool for art history and assists cultural heritage preservation. Modern image retrieval systems offer good performance on visually similar artworks, but fail in the cases of more abstract composition transfer. The proposed approach links artworks through a pose similarity of human figures depicted in images. Human figures are the subject of a large fraction of visual art from middle ages to modernity and their distinctive poses were often a source of inspiration among artists. The method consists of two steps -- fast pose matching and robust spatial verification. We experimentally show that explicit human pose matching is superior to standard content-based image retrieval methods on a manually annotated art composition transfer dataset.



### Unified Attentional Generative Adversarial Network for Brain Tumor Segmentation From Multimodal Unpaired Images
- **Arxiv ID**: http://arxiv.org/abs/1907.03548v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.03548v1)
- **Published**: 2019-07-08 12:12:59+00:00
- **Updated**: 2019-07-08 12:12:59+00:00
- **Authors**: Wenguang Yuan, Jia Wei, Jiabing Wang, Qianli Ma, Tolga Tasdizen
- **Comment**: 9 pages, 4 figures, Accepted by MICCAI2019
- **Journal**: None
- **Summary**: In medical applications, the same anatomical structures may be observed in multiple modalities despite the different image characteristics. Currently, most deep models for multimodal segmentation rely on paired registered images. However, multimodal paired registered images are difficult to obtain in many cases. Therefore, developing a model that can segment the target objects from different modalities with unpaired images is significant for many clinical applications. In this work, we propose a novel two-stream translation and segmentation unified attentional generative adversarial network (UAGAN), which can perform any-to-any image modality translation and segment the target objects simultaneously in the case where two or more modalities are available. The translation stream is used to capture modality-invariant features of the target anatomical structures. In addition, to focus on segmentation-related features, we add attentional blocks to extract valuable features from the translation stream. Experiments on three-modality brain tumor segmentation indicate that UAGAN outperforms the existing methods in most cases.



### Variational Context: Exploiting Visual and Textual Context for Grounding Referring Expressions
- **Arxiv ID**: http://arxiv.org/abs/1907.03609v1
- **DOI**: 10.1109/TPAMI.2019.2926266
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.03609v1)
- **Published**: 2019-07-08 13:37:48+00:00
- **Updated**: 2019-07-08 13:37:48+00:00
- **Authors**: Yulei Niu, Hanwang Zhang, Zhiwu Lu, Shih-Fu Chang
- **Comment**: Accepted as regular paper in IEEE Transactions on Pattern Analysis
  and Machine Intelligence (TPAMI). Substantial text overlap with
  arXiv:1712.01892
- **Journal**: None
- **Summary**: We focus on grounding (i.e., localizing or linking) referring expressions in images, e.g., ``largest elephant standing behind baby elephant''. This is a general yet challenging vision-language task since it does not only require the localization of objects, but also the multimodal comprehension of context -- visual attributes (e.g., ``largest'', ``baby'') and relationships (e.g., ``behind'') that help to distinguish the referent from other objects, especially those of the same category. Due to the exponential complexity involved in modeling the context associated with multiple image regions, existing work oversimplifies this task to pairwise region modeling by multiple instance learning. In this paper, we propose a variational Bayesian method, called Variational Context, to solve the problem of complex context modeling in referring expression grounding. Specifically, our framework exploits the reciprocal relation between the referent and context, i.e., either of them influences estimation of the posterior distribution of the other, and thereby the search space of context can be greatly reduced. In addition to reciprocity, our framework considers the semantic information of context, i.e., the referring expression can be reproduced based on the estimated context. We also extend the model to unsupervised setting where no annotation for the referent is available. Extensive experiments on various benchmarks show consistent improvement over state-of-the-art methods in both supervised and unsupervised settings.



### Unsupervised Domain Alignment to Mitigate Low Level Dataset Biases
- **Arxiv ID**: http://arxiv.org/abs/1907.03644v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.03644v2)
- **Published**: 2019-07-08 14:22:54+00:00
- **Updated**: 2019-07-12 00:35:13+00:00
- **Authors**: Kirthi Shankar Sivamani
- **Comment**: 10 pages, 4 figures, 6 tables, submitted to ICAAI 2019
- **Journal**: None
- **Summary**: Dataset bias is a well-known problem in the field of computer vision. The presence of implicit bias in any image collection hinders a model trained and validated on a particular dataset to yield similar accuracies when tested on other datasets. In this paper, we propose a novel debiasing technique to reduce the effects of a biased training dataset. Our goal is to augment the training data using a generative network by learning a non-linear mapping from the source domain (training set) to the target domain (testing set) while retaining training set labels. The cycle consistency loss and adversarial loss for generative adversarial networks are used to learn the mapping. A structured similarity index (SSIM) loss is used to enforce label retention while augmenting the training set. Our methods and hypotheses are supported by quantitative comparisons with prior debiasing techniques. These comparisons showcase the superiority of our method and its potential to mitigate the effects of dataset bias during the inference stage.



### Introduction to Camera Pose Estimation with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.05272v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.05272v3)
- **Published**: 2019-07-08 14:43:06+00:00
- **Updated**: 2019-07-16 04:18:26+00:00
- **Authors**: Yoli Shavit, Ron Ferens
- **Comment**: None
- **Journal**: None
- **Summary**: Over the last two decades, deep learning has transformed the field of computer vision. Deep convolutional networks were successfully applied to learn different vision tasks such as image classification, image segmentation, object detection and many more. By transferring the knowledge learned by deep models on large generic datasets, researchers were further able to create fine-tuned models for other more specific tasks. Recently this idea was applied for regressing the absolute camera pose from an RGB image. Although the resulting accuracy was sub-optimal, compared to classic feature-based solutions, this effort led to a surge of learning-based pose estimation methods. Here, we review deep learning approaches for camera pose estimation. We describe key methods in the field and identify trends aiming at improving the original deep pose regression solution. We further provide an extensive cross-comparison of existing learning-based pose estimators, together with practical notes on their execution for reproducibility purposes. Finally, we discuss emerging solutions and potential future research directions.



### From Points to Parts: 3D Object Detection from Point Cloud with Part-aware and Part-aggregation Network
- **Arxiv ID**: http://arxiv.org/abs/1907.03670v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.03670v3)
- **Published**: 2019-07-08 15:19:48+00:00
- **Updated**: 2020-03-16 04:33:20+00:00
- **Authors**: Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang, Hongsheng Li
- **Comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence 2020, code is available at
  https://github.com/sshaoshuai/PointCloudDet3D
- **Journal**: None
- **Summary**: 3D object detection from LiDAR point cloud is a challenging problem in 3D scene understanding and has many practical applications. In this paper, we extend our preliminary work PointRCNN to a novel and strong point-cloud-based 3D object detection framework, the part-aware and aggregation neural network (Part-$A^2$ net). The whole framework consists of the part-aware stage and the part-aggregation stage. Firstly, the part-aware stage for the first time fully utilizes free-of-charge part supervisions derived from 3D ground-truth boxes to simultaneously predict high quality 3D proposals and accurate intra-object part locations. The predicted intra-object part locations within the same proposal are grouped by our new-designed RoI-aware point cloud pooling module, which results in an effective representation to encode the geometry-specific features of each 3D proposal. Then the part-aggregation stage learns to re-score the box and refine the box location by exploring the spatial relationship of the pooled intra-object part locations. Extensive experiments are conducted to demonstrate the performance improvements from each component of our proposed framework. Our Part-$A^2$ net outperforms all existing 3D detection methods and achieves new state-of-the-art on KITTI 3D object detection dataset by utilizing only the LiDAR point cloud data. Code is available at https://github.com/sshaoshuai/PointCloudDet3D.



### TrackNet: A Deep Learning Network for Tracking High-speed and Tiny Objects in Sports Applications
- **Arxiv ID**: http://arxiv.org/abs/1907.03698v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.MM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.03698v1)
- **Published**: 2019-07-08 16:08:43+00:00
- **Updated**: 2019-07-08 16:08:43+00:00
- **Authors**: Yu-Chuan Huang, I-No Liao, Ching-Hsuan Chen, Tsì-Uí İk, Wen-Chih Peng
- **Comment**: None
- **Journal**: None
- **Summary**: Ball trajectory data are one of the most fundamental and useful information in the evaluation of players' performance and analysis of game strategies. Although vision-based object tracking techniques have been developed to analyze sport competition videos, it is still challenging to recognize and position a high-speed and tiny ball accurately. In this paper, we develop a deep learning network, called TrackNet, to track the tennis ball from broadcast videos in which the ball images are small, blurry, and sometimes with afterimage tracks or even invisible. The proposed heatmap-based deep learning network is trained to not only recognize the ball image from a single frame but also learn flying patterns from consecutive frames. TrackNet takes images with a size of $640\times360$ to generate a detection heatmap from either a single frame or several consecutive frames to position the ball and can achieve high precision even on public domain videos. The network is evaluated on the video of the men's singles final at the 2017 Summer Universiade, which is available on YouTube. The precision, recall, and F1-measure of TrackNet reach $99.7\%$, $97.3\%$, and $98.5\%$, respectively. To prevent overfitting, 9 additional videos are partially labeled together with a subset from the previous dataset to implement 10-fold cross-validation, and the precision, recall, and F1-measure are $95.3\%$, $75.7\%$, and $84.3\%$, respectively. A conventional image processing algorithm is also implemented to compare with TrackNet. Our experiments indicate that TrackNet outperforms conventional method by a big margin and achieves exceptional ball tracking performance. The dataset and demo video are available at https://nol.cs.nctu.edu.tw/ndo3je6av9/.



### A Comparison of Super-Resolution and Nearest Neighbors Interpolation Applied to Object Detection on Satellite Data
- **Arxiv ID**: http://arxiv.org/abs/1907.05283v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.05283v1)
- **Published**: 2019-07-08 17:03:12+00:00
- **Updated**: 2019-07-08 17:03:12+00:00
- **Authors**: Evan Koester, Cem Safak Sahin
- **Comment**: None
- **Journal**: None
- **Summary**: As Super-Resolution (SR) has matured as a research topic, it has been applied to additional topics beyond image reconstruction. In particular, combining classification or object detection tasks with a super-resolution preprocessing stage has yielded improvements in accuracy especially with objects that are small relative to the scene. While SR has shown promise, a study comparing SR and naive upscaling methods such as Nearest Neighbors (NN) interpolation when applied as a preprocessing step for object detection has not been performed. We apply the topic to satellite data and compare the Multi-scale Deep Super-Resolution (MDSR) system to NN on the xView challenge dataset. To do so, we propose a pipeline for processing satellite data that combines multi-stage image tiling and upscaling, the YOLOv2 object detection architecture, and label stitching. We compare the effects of training models using an upscaling factor of 4, upscaling images from 30cm Ground Sample Distance (GSD) to an effective GSD of 7.5cm. Upscaling by this factor significantly improves detection results, increasing Average Precision (AP) of a generalized vehicle class by 23 percent. We demonstrate that while SR produces upscaled images that are more visually pleasing than their NN counterparts, object detection networks see little difference in accuracy with images upsampled using NN obtaining nearly identical results to the MDSRx4 enhanced images with a difference of 0.0002 AP between the two methods.



### Correlation via synthesis: end-to-end nodule image generation and radiogenomic map learning based on generative adversarial network
- **Arxiv ID**: http://arxiv.org/abs/1907.03728v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.03728v1)
- **Published**: 2019-07-08 17:17:18+00:00
- **Updated**: 2019-07-08 17:17:18+00:00
- **Authors**: Ziyue Xu, Xiaosong Wang, Hoo-Chang Shin, Dong Yang, Holger Roth, Fausto Milletari, Ling Zhang, Daguang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Radiogenomic map linking image features and gene expression profiles is useful for noninvasively identifying molecular properties of a particular type of disease. Conventionally, such map is produced in three separate steps: 1) gene-clustering to "metagenes", 2) image feature extraction, and 3) statistical correlation between metagenes and image features. Each step is independently performed and relies on arbitrary measurements. In this work, we investigate the potential of an end-to-end method fusing gene data with image features to generate synthetic image and learn radiogenomic map simultaneously. To achieve this goal, we develop a generative adversarial network (GAN) conditioned on both background images and gene expression profiles, synthesizing the corresponding image. Image and gene features are fused at different scales to ensure the realism and quality of the synthesized image. We tested our method on non-small cell lung cancer (NSCLC) dataset. Results demonstrate that the proposed method produces realistic synthetic images, and provides a promising way to find gene-image relationship in a holistic end-to-end manner.



### Point-Voxel CNN for Efficient 3D Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.03739v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.03739v2)
- **Published**: 2019-07-08 17:48:45+00:00
- **Updated**: 2019-12-09 20:46:48+00:00
- **Authors**: Zhijian Liu, Haotian Tang, Yujun Lin, Song Han
- **Comment**: NeurIPS 2019. The first two authors contributed equally to this work.
  Project page: http://pvcnn.mit.edu/
- **Journal**: None
- **Summary**: We present Point-Voxel CNN (PVCNN) for efficient, fast 3D deep learning. Previous work processes 3D data using either voxel-based or point-based NN models. However, both approaches are computationally inefficient. The computation cost and memory footprints of the voxel-based models grow cubically with the input resolution, making it memory-prohibitive to scale up the resolution. As for point-based networks, up to 80% of the time is wasted on structuring the sparse data which have rather poor memory locality, not on the actual feature extraction. In this paper, we propose PVCNN that represents the 3D input data in points to reduce the memory consumption, while performing the convolutions in voxels to reduce the irregular, sparse data access and improve the locality. Our PVCNN model is both memory and computation efficient. Evaluated on semantic and part segmentation datasets, it achieves much higher accuracy than the voxel-based baseline with 10x GPU memory reduction; it also outperforms the state-of-the-art point-based models with 7x measured speedup on average. Remarkably, the narrower version of PVCNN achieves 2x speedup over PointNet (an extremely efficient model) on part and scene segmentation benchmarks with much higher accuracy. We validate the general effectiveness of PVCNN on 3D object detection: by replacing the primitives in Frustrum PointNet with PVConv, it outperforms Frustrum PointNet++ by 2.4% mAP on average with 1.5x measured speedup and GPU memory reduction.



### Fully Convolutional Network for Removing DCT Artefacts From Images
- **Arxiv ID**: http://arxiv.org/abs/1907.03798v2
- **DOI**: 10.1109/IJCNN48605.2020.9207249
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.03798v2)
- **Published**: 2019-07-08 18:31:11+00:00
- **Updated**: 2021-05-24 10:51:13+00:00
- **Authors**: Patryk Najgebauer, Rafal Scherer, Leszek Rutkowski
- **Comment**: None
- **Journal**: 2020 International Joint Conference on Neural Networks (IJCNN),
  2020, pp. 1-8
- **Summary**: Image compression is one of the essential methods of image processing. Its most prominent advantage is the significant reduction of image size allowing for more efficient storage and transfer. However, lossy compression is associated with the loss of some image details in favor of reducing its size. In compressed images, the deficiencies are manifested by noticeable defects in the form of artifacts; the most common are block artifacts, ringing effect, or blur. In this article, we propose three models of fully convolutional networks with different configurations and examine their abilities in reducing compression artifacts. In the experiments, we research the extent to which the results are improved for models that will process the image in a similar way to the compression algorithm, and whether the initialization with predefined filters would allow for better image reconstruction than developed solely during learning.



### Rehearsal-Free Continual Learning over Small Non-I.I.D. Batches
- **Arxiv ID**: http://arxiv.org/abs/1907.03799v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.03799v3)
- **Published**: 2019-07-08 18:32:25+00:00
- **Updated**: 2020-04-21 16:13:12+00:00
- **Authors**: Vincenzo Lomonaco, Davide Maltoni, Lorenzo Pellegrini
- **Comment**: Accepted in the CLVision Workshop at CVPR2020: 12 pages, 7 figures, 5
  tables, 3 algorithms
- **Journal**: None
- **Summary**: Robotic vision is a field where continual learning can play a significant role. An embodied agent operating in a complex environment subject to frequent and unpredictable changes is required to learn and adapt continuously. In the context of object recognition, for example, a robot should be able to learn (without forgetting) objects of never before seen classes as well as improving its recognition capabilities as new instances of already known classes are discovered. Ideally, continual learning should be triggered by the availability of short videos of single objects and performed on-line on on-board hardware with fine-grained updates. In this paper, we introduce a novel continual learning protocol based on the CORe50 benchmark and propose two rehearsal-free continual learning techniques, CWR* and AR1*, that can learn effectively even in the challenging case of nearly 400 small non-i.i.d. incremental batches. In particular, our experiments show that AR1* can outperform other state-of-the-art rehearsal-free techniques by more than 15% accuracy in some cases, with a very light and constant computational and memory overhead across training batches.



### Personalised aesthetics with residual adapters
- **Arxiv ID**: http://arxiv.org/abs/1907.03802v1
- **DOI**: 10.1007/978-3-030-31332-6_44
- **Categories**: **cs.CV**, cs.LG, stat.ML, 68T10 (Primary), 68T45 (Secondary), I.2.10; I.5.4; I.4.3
- **Links**: [PDF](http://arxiv.org/pdf/1907.03802v1)
- **Published**: 2019-07-08 18:40:16+00:00
- **Updated**: 2019-07-08 18:40:16+00:00
- **Authors**: Carlos Rodríguez - Pardo, Hakan Bilen
- **Comment**: 12 pages, 4 figures. In Iberian Conference on Pattern Recognition and
  Image Analysis proceedings
- **Journal**: None
- **Summary**: The use of computational methods to evaluate aesthetics in photography has gained interest in recent years due to the popularization of convolutional neural networks and the availability of new annotated datasets. Most studies in this area have focused on designing models that do not take into account individual preferences for the prediction of the aesthetic value of pictures. We propose a model based on residual learning that is capable of learning subjective, user specific preferences over aesthetics in photography, while surpassing the state-of-the-art methods and keeping a limited number of user-specific parameters in the model. Our model can also be used for picture enhancement, and it is suitable for content-based or hybrid recommender systems in which the amount of computational resources is limited.



### Barriers towards no-reference metrics application to compressed video quality analysis: on the example of no-reference metric NIQE
- **Arxiv ID**: http://arxiv.org/abs/1907.03842v2
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.GR, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/1907.03842v2)
- **Published**: 2019-07-08 20:07:16+00:00
- **Updated**: 2019-08-29 14:30:22+00:00
- **Authors**: Anastasia Zvezdakova, Dmitriy Kulikov, Denis Kondranin, Dmitriy Vatolin
- **Comment**: None
- **Journal**: None
- **Summary**: This paper analyses the application of no-reference metric NIQE to the task of video-codec comparison. A number of issues in the metric behaviour on videos was detected and described. The metric has outlying scores on black and solid-coloured frames. The proposed averaging technique for metric quality scores helped to improve the results in some cases. Also, NIQE has low-quality scores for videos with detailed textures and higher scores for videos of lower bitrates due to the blurring of these textures after compression. Although NIQE showed natural results for many tested videos, it is not universal and currently can not be used for video-codec comparisons.



### Brain Tissues Segmentation on MR Perfusion Images Using CUSUM Filter for Boundary Pixels
- **Arxiv ID**: http://arxiv.org/abs/1907.03865v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.03865v1)
- **Published**: 2019-07-08 20:46:36+00:00
- **Updated**: 2019-07-08 20:46:36+00:00
- **Authors**: S. M. Alkhimova, A. P. Krenevych
- **Comment**: None
- **Journal**: International Journal of Computing (2019), V. 18, N 2., P. 127-134
- **Summary**: The fully automated and relatively accurate method of brain tissues segmentation on T2-weighted magnetic resonance perfusion images is proposed. Segmentation with this method provides a possibility to obtain perfusion region of interest on images with abnormal brain anatomy that is very important for perfusion analysis. In the proposed method the result is presented as a binary mask, which marks two regions: brain tissues pixels with unity values and skull, extracranial soft tissue and background pixels with zero values. The binary mask is produced based on the location of boundary between two studied regions. Each boundary point is detected with CUSUM filter as a change point for iteratively accumulated points at time of moving on a sinusoidal-like path along the boundary from one region to another. The evaluation results for 20 clinical cases showed that proposed segmentation method could significantly reduce the time and efforts required to obtain desirable results for perfusion region of interest detection on T2-weighted magnetic resonance perfusion images with abnormal brain anatomy.



### Fast Visual Object Tracking with Rotated Bounding Boxes
- **Arxiv ID**: http://arxiv.org/abs/1907.03892v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.03892v5)
- **Published**: 2019-07-08 22:06:39+00:00
- **Updated**: 2019-09-12 06:26:54+00:00
- **Authors**: Bao Xin Chen, John K. Tsotsos
- **Comment**: None
- **Journal**: 2019 IEEE/CVF International Conference on Computer Vision (ICCV)
  Workshop
- **Summary**: In this paper, we demonstrate a novel algorithm that uses ellipse fitting to estimate the bounding box rotation angle and size with the segmentation(mask) on the target for online and real-time visual object tracking. Our method, SiamMask_E, improves the bounding box fitting procedure of the state-of-the-art object tracking algorithm SiamMask and still retains a fast-tracking frame rate (80 fps) on a system equipped with GPU (GeForce GTX 1080 Ti or higher). We tested our approach on the visual object tracking datasets (VOT2016, VOT2018, and VOT2019) that were labeled with rotated bounding boxes. By comparing with the original SiamMask, we achieved an improved Accuracy of 0.652 and 0.309 EAO on VOT2019, which is 0.056 and 0.026 higher than the original SiamMask. The implementation is available on GitHub: https://github.com/baoxinchen/siammask_e.



