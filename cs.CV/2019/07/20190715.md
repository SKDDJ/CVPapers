# Arxiv Papers in cs.CV on 2019-07-15
### Exploring Deep Anomaly Detection Methods Based on Capsule Net
- **Arxiv ID**: http://arxiv.org/abs/1907.06312v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.06312v1)
- **Published**: 2019-07-15 02:15:58+00:00
- **Updated**: 2019-07-15 02:15:58+00:00
- **Authors**: Xiaoyan Li, Iluju Kiringa, Tet Yeap, Xiaodan Zhu, Yifeng Li
- **Comment**: Presented in the "ICML 2019 Workshop on Uncertainty & Robustness in
  Deep Learning", June 14, Long Beach, California, USA
- **Journal**: None
- **Summary**: In this paper, we develop and explore deep anomaly detection techniques based on the capsule network (CapsNet) for image data. Being able to encoding intrinsic spatial relationship between parts and a whole, CapsNet has been applied as both a classifier and deep autoencoder. This inspires us to design a prediction-probability-based and a reconstruction-error-based normality score functions for evaluating the "outlierness" of unseen images. Our results on three datasets demonstrate that the prediction-probability-based method performs consistently well, while the reconstruction-error-based approach is relatively sensitive to the similarity between labeled and unlabeled images. Furthermore, both of the CapsNet-based methods outperform the principled benchmark methods in many cases.



### Enabling Multi-Shell b-Value Generalizability of Data-Driven Diffusion Models with Deep SHORE
- **Arxiv ID**: http://arxiv.org/abs/1907.06319v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.06319v3)
- **Published**: 2019-07-15 03:05:00+00:00
- **Updated**: 2020-02-22 15:42:29+00:00
- **Authors**: Vishwesh Nath, Ilwoo Lyu, Kurt G. Schilling, Prasanna Parvathaneni, Colin B. Hansen, Yucheng Tang, Yuankai Huo, Vaibhav A. Janve, Yurui Gao, Iwona Stepniewska, Adam W. Anderson, Bennett A. Landman
- **Comment**: None
- **Journal**: None
- **Summary**: Intra-voxel models of the diffusion signal are essential for interpreting organization of the tissue environment at micrometer level with data at millimeter resolution. Recent advances in data driven methods have enabled direct compari-son and optimization of methods for in-vivo data with externally validated histological sections with both 2-D and 3-D histology. Yet, all existing methods make limiting assumptions of either (1) model-based linkages between b-values or (2) limited associations with single shell data. We generalize prior deep learning models that used single shell spherical harmonic transforms to integrate the re-cently developed simple harmonic oscillator reconstruction (SHORE) basis. To enable learning on the SHORE manifold, we present an alternative formulation of the fiber orientation distribution (FOD) object using the SHORE basis while rep-resenting the observed diffusion weighted data in the SHORE basis. To ensure consistency of hyper-parameter optimization for SHORE, we present our Deep SHORE approach to learn on a data-optimized manifold. Deep SHORE is evalu-ated with eight-fold cross-validation of a preclinical MRI-histology data with four b-values. Generalizability of in-vivo human data is evaluated on two separate 3T MRI scanners. Specificity in terms of angular correlation (ACC) with the preclinical data improved on single shell: 0.78 relative to 0.73 and 0.73, multi-shell: 0.80 relative to 0.74 (p < 0.001). In the in-vivo human data, Deep SHORE was more consistent across scanners with 0.63 relative to other multi-shell methods 0.39, 0.52 and 0.57 in terms of ACC. In conclusion, Deep SHORE is a promising method to enable data driven learning with DW-MRI under conditions with varying b-values, number of diffusion shells, and gradient directions per shell.



### FastV2C-HandNet: Fast Voxel to Coordinate Hand Pose Estimation with 3D Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.06327v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.06327v3)
- **Published**: 2019-07-15 04:04:01+00:00
- **Updated**: 2020-02-20 14:31:45+00:00
- **Authors**: Rohan Lekhwani, Bhupendra Singh
- **Comment**: 13 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: Hand pose estimation from monocular depth images has been an important and challenging problem in the Computer Vision community. In this paper, we present a novel approach to estimate 3D hand joint locations from 2D depth images. Unlike most of the previous methods, our model captures the 3D spatial information from a depth image thereby giving it a greater understanding of the input. We voxelize the input depth map to capture the 3D features of the input and perform 3D data augmentations to make our network robust to real-world images. Our network is trained in an end-to-end manner which reduces time and space complexity significantly when compared to other methods. Through extensive experiments, we show that our model outperforms state-of-the-art methods with respect to the time it takes to train and predict 3D hand joint locations. This makes our method more suitable for real-world hand pose estimation scenarios.



### Boosting Resolution and Recovering Texture of micro-CT Images with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.07131v3
- **DOI**: 10.1029/2019WR026052
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.07131v3)
- **Published**: 2019-07-15 04:32:50+00:00
- **Updated**: 2019-07-27 01:37:33+00:00
- **Authors**: Ying Da Wang, Ryan T. Armstrong, Peyman Mostaghimi
- **Comment**: \keywords{Digital Rock Imaging \and Super Resolution \and
  Convolutional Neural Networks \and Generative Adversarial Networks}
- **Journal**: None
- **Summary**: Digital Rock Imaging is constrained by detector hardware, and a trade-off between the image field of view (FOV) and the image resolution must be made. This can be compensated for with super resolution (SR) techniques that take a wide FOV, low resolution (LR) image, and super resolve a high resolution (HR), high FOV image. The Enhanced Deep Super Resolution Generative Adversarial Network (EDSRGAN) is trained on the Deep Learning Digital Rock Super Resolution Dataset, a diverse compilation 12000 of raw and processed uCT images. The network shows comparable performance of 50% to 70% reduction in relative error over bicubic interpolation. GAN performance in recovering texture shows superior visual similarity compared to SRCNN and other methods. Difference maps indicate that the SRCNN section of the SRGAN network recovers large scale edge (grain boundaries) features while the GAN network regenerates perceptually indistinguishable high frequency texture. Network performance is generalised with augmentation, showing high adaptability to noise and blur. HR images are fed into the network, generating HR-SR images to extrapolate network performance to sub-resolution features present in the HR images themselves. Results show that under-resolution features such as dissolved minerals and thin fractures are regenerated despite the network operating outside of trained specifications. Comparison with Scanning Electron Microscope images shows details are consistent with the underlying geometry of the sample. Recovery of textures benefits the characterisation of digital rocks with a high proportion of under-resolution micro-porous features, such as carbonate and coal samples. Images that are normally constrained by the mineralogy of the rock (coal), by fast transient imaging (waterflooding), or by the energy of the source (microporosity), can be super resolved accurately for further analysis downstream.



### DA-RefineNet:A Dual Input Whole Slide Image Segmentation Algorithm Based on Attention
- **Arxiv ID**: http://arxiv.org/abs/1907.06358v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.06358v3)
- **Published**: 2019-07-15 08:15:48+00:00
- **Updated**: 2020-12-17 03:38:14+00:00
- **Authors**: Ziqiang Li, Rentuo Tao, Qianrun Wu, Bin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic medical image segmentation has wide applications for disease diagnosing. However, it is much more challenging than natural optical image segmentation due to the high-resolution of medical images and the corresponding huge computation cost. The sliding window is a commonly used technique for whole slide image (WSI) segmentation, however, for these methods based on the sliding window, the main drawback is lacking global contextual information for supervision. In this paper, we propose a dual-inputs attention network (denoted as DA-RefineNet) for WSI segmentation, where both local fine-grained information and global coarse information can be efficiently utilized. Sufficient comparative experiments are conducted to evaluate the effectiveness of the proposed method, the results prove that the proposed method can achieve better performance on WSI segmentation compared to methods relying on single-input.



### Multimodal deep networks for text and image-based document classification
- **Arxiv ID**: http://arxiv.org/abs/1907.06370v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06370v1)
- **Published**: 2019-07-15 08:43:49+00:00
- **Updated**: 2019-07-15 08:43:49+00:00
- **Authors**: Nicolas Audebert, Catherine Herold, Kuider Slimani, CÃ©dric Vidal
- **Comment**: None
- **Journal**: None
- **Summary**: Classification of document images is a critical step for archival of old manuscripts, online subscription and administrative procedures. Computer vision and deep learning have been suggested as a first solution to classify documents based on their visual appearance. However, achieving the fine-grained classification that is required in real-world setting cannot be achieved by visual analysis alone. Often, the relevant information is in the actual text content of the document. We design a multimodal neural network that is able to learn from word embeddings, computed on text extracted by OCR, and from the image. We show that this approach boosts pure image accuracy by 3% on Tobacco3482 and RVL-CDIP augmented by our new QS-OCR text dataset (https://github.com/Quicksign/ocrized-text-dataset), even without clean text information.



### Mitigating the Hubness Problem for Zero-Shot Learning of 3D Objects
- **Arxiv ID**: http://arxiv.org/abs/1907.06371v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06371v1)
- **Published**: 2019-07-15 08:47:14+00:00
- **Updated**: 2019-07-15 08:47:14+00:00
- **Authors**: Ali Cheraghian, Shafin Rahman, Dylan Campbell, Lars Petersson
- **Comment**: BMVC 2019
- **Journal**: None
- **Summary**: The development of advanced 3D sensors has enabled many objects to be captured in the wild at a large scale, and a 3D object recognition system may therefore encounter many objects for which the system has received no training. Zero-Shot Learning (ZSL) approaches can assist such systems in recognizing previously unseen objects. Applying ZSL to 3D point cloud objects is an emerging topic in the area of 3D vision, however, a significant problem that ZSL often suffers from is the so-called hubness problem, which is when a model is biased to predict only a few particular labels for most of the test instances. We observe that this hubness problem is even more severe for 3D recognition than for 2D recognition. One reason for this is that in 2D one can use pre-trained networks trained on large datasets like ImageNet, which produces high-quality features. However, in the 3D case there are no such large-scale, labelled datasets available for pre-training which means that the extracted 3D features are of poorer quality which, in turn, exacerbates the hubness problem. In this paper, we therefore propose a loss to specifically address the hubness problem. Our proposed method is effective for both Zero-Shot and Generalized Zero-Shot Learning, and we perform extensive evaluations on the challenging datasets ModelNet40, ModelNet10, McGill and SHREC2015. A new state-of-the-art result for both zero-shot tasks in the 3D case is established.



### Sequence Level Semantics Aggregation for Video Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1907.06390v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06390v2)
- **Published**: 2019-07-15 09:37:40+00:00
- **Updated**: 2019-08-20 02:38:11+00:00
- **Authors**: Haiping Wu, Yuntao Chen, Naiyan Wang, Zhaoxiang Zhang
- **Comment**: ICCV 2019 camera ready
- **Journal**: None
- **Summary**: Video objection detection (VID) has been a rising research direction in recent years. A central issue of VID is the appearance degradation of video frames caused by fast motion. This problem is essentially ill-posed for a single frame. Therefore, aggregating features from other frames becomes a natural choice. Existing methods rely heavily on optical flow or recurrent neural networks for feature aggregation. However, these methods emphasize more on the temporally nearby frames. In this work, we argue that aggregating features in the full-sequence level will lead to more discriminative and robust features for video object detection. To achieve this goal, we devise a novel Sequence Level Semantics Aggregation (SELSA) module. We further demonstrate the close relationship between the proposed method and the classic spectral clustering method, providing a novel view for understanding the VID problem. We test the proposed method on the ImageNet VID and the EPIC KITCHENS dataset and achieve new state-of-the-art results. Our method does not need complicated postprocessing methods such as Seq-NMS or Tubelet rescoring, which keeps the pipeline simple and clean.



### Improving the Harmony of the Composite Image by Spatial-Separated Attention Module
- **Arxiv ID**: http://arxiv.org/abs/1907.06406v3
- **DOI**: 10.1109/TIP.2020.2975979
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06406v3)
- **Published**: 2019-07-15 10:06:42+00:00
- **Updated**: 2020-02-22 09:09:38+00:00
- **Authors**: Xiaodong Cun, Chi-Man Pun
- **Comment**: Accepted by IEEE Transactions on Image Processing (TIP) 2020
- **Journal**: None
- **Summary**: Image composition is one of the most important applications in image processing. However, the inharmonious appearance between the spliced region and background degrade the quality of the image. Thus, we address the problem of Image Harmonization: Given a spliced image and the mask of the spliced region, we try to harmonize the "style" of the pasted region with the background (non-spliced region). Previous approaches have been focusing on learning directly by the neural network. In this work, we start from an empirical observation: the differences can only be found in the spliced region between the spliced image and the harmonized result while they share the same semantic information and the appearance in the non-spliced region. Thus, in order to learn the feature map in the masked region and the others individually, we propose a novel attention module named Spatial-Separated Attention Module (S2AM). Furthermore, we design a novel image harmonization framework by inserting the S2AM in the coarser low-level features of the Unet structure in two different ways. Besides image harmonization, we make a big step for harmonizing the composite image without the specific mask under previous observation. The experiments show that the proposed S2AM performs better than other state-of-the-art attention modules in our task. Moreover, we demonstrate the advantages of our model against other state-of-the-art image harmonization methods via criteria from multiple points of view. Code is available at https://github.com/vinthony/s2am



### Quick, Stat!: A Statistical Analysis of the Quick, Draw! Dataset
- **Arxiv ID**: http://arxiv.org/abs/1907.06417v2
- **DOI**: 10.11128/arep.58
- **Categories**: **cs.CV**, cs.DB, eess.IV, 68T30, I.2.6; I.5.1; I.3.4
- **Links**: [PDF](http://arxiv.org/pdf/1907.06417v2)
- **Published**: 2019-07-15 10:28:34+00:00
- **Updated**: 2019-10-23 09:07:23+00:00
- **Authors**: Raul Fernandez-Fernandez, Juan G. Victores, David Estevez, Carlos Balaguer
- **Comment**: 12 pages, Eurosim 2019
- **Journal**: None
- **Summary**: The Quick, Draw! Dataset is a Google dataset with a collection of 50 million drawings, divided in 345 categories, collected from the users of the game Quick, Draw!. In contrast with most of the existing image datasets, in the Quick, Draw! Dataset, drawings are stored as time series of pencil positions instead of a bitmap matrix composed by pixels. This aspect makes this dataset the largest doodle dataset available at the time. The Quick, Draw! Dataset is presented as a great opportunity to researchers for developing and studying machine learning techniques. Due to the size of this dataset and the nature of its source, there is a scarce of information about the quality of the drawings contained. In this paper, a statistical analysis of three of the classes contained in the Quick, Draw! Dataset is depicted: mountain, book and whale. The goal is to give to the reader a first impression of the data collected in this dataset. For the analysis of the quality of the drawings, a Classification Neural Network was trained to obtain a classification score. Using this classification score and the parameters provided by the dataset, a statistical analysis of the quality and nature of the drawings contained in this dataset is provided.



### A Short Note on the Kinetics-700 Human Action Dataset
- **Arxiv ID**: http://arxiv.org/abs/1907.06987v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06987v2)
- **Published**: 2019-07-15 12:58:21+00:00
- **Updated**: 2022-10-17 19:58:40+00:00
- **Authors**: Joao Carreira, Eric Noland, Chloe Hillier, Andrew Zisserman
- **Comment**: added note about dangers of training on k700 and evaluating on
  k400/k600. arXiv admin note: text overlap with arXiv:1808.01340
- **Journal**: None
- **Summary**: We describe an extension of the DeepMind Kinetics human action dataset from 600 classes to 700 classes, where for each class there are at least 600 video clips from different YouTube videos. This paper details the changes introduced for this new release of the dataset, and includes a comprehensive set of statistics as well as baseline results using the I3D neural network architecture.



### Color Cerberus
- **Arxiv ID**: http://arxiv.org/abs/1907.06483v1
- **DOI**: 10.1109/ISPA.2019.8868425
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.06483v1)
- **Published**: 2019-07-15 13:04:31+00:00
- **Updated**: 2019-07-15 13:04:31+00:00
- **Authors**: A. ~Savchik, E. ~Ershov, S. ~Karpenko
- **Comment**: None
- **Journal**: None
- **Summary**: Simple convolutional neural network was able to win ISISPA color constancy competition. Partial reimplementation of (Bianco, 2017) neural architecture would have shown even better results in this setup.



### An Efficient Framework for Visible-Infrared Cross Modality Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1907.06498v2
- **DOI**: 10.1016/j.image.2020.115933
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06498v2)
- **Published**: 2019-07-15 13:32:15+00:00
- **Updated**: 2020-08-02 03:41:05+00:00
- **Authors**: Emrah Basaran, Muhittin Gokmen, Mustafa E. Kamasak
- **Comment**: None
- **Journal**: None
- **Summary**: Visible-infrared cross-modality person re-identification (VI-ReId) is an essential task for video surveillance in poorly illuminated or dark environments. Despite many recent studies on person re-identification in the visible domain (ReId), there are few studies dealing specifically with VI-ReId. Besides challenges that are common for both ReId and VI-ReId such as pose/illumination variations, background clutter and occlusion, VI-ReId has additional challenges as color information is not available in infrared images. As a result, the performance of VI-ReId systems is typically lower than that of ReId systems. In this work, we propose a four-stream framework to improve VI-ReId performance. We train a separate deep convolutional neural network in each stream using different representations of input images. We expect that different and complementary features can be learned from each stream. In our framework, grayscale and infrared input images are used to train the ResNet in the first stream. In the second stream, RGB and three-channel infrared images (created by repeating the infrared channel) are used. In the remaining two streams, we use local pattern maps as input images. These maps are generated utilizing local Zernike moments transformation. Local pattern maps are obtained from grayscale and infrared images in the third stream and from RGB and three-channel infrared images in the last stream. We improve the performance of the proposed framework by employing a re-ranking algorithm for post-processing. Our results indicate that the proposed framework outperforms current state-of-the-art with a large margin by improving Rank-1/mAP by 29.79%/30.91% on SYSU-MM01 dataset, and by 9.73%/16.36% on RegDB dataset.



### Detecting and Simulating Artifacts in GAN Fake Images
- **Arxiv ID**: http://arxiv.org/abs/1907.06515v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.06515v2)
- **Published**: 2019-07-15 14:22:34+00:00
- **Updated**: 2019-10-15 22:53:46+00:00
- **Authors**: Xu Zhang, Svebor Karaman, Shih-Fu Chang
- **Comment**: This is an extended version of our original AutoGAN paper which will
  be appeared in WIFS 2019
- **Journal**: None
- **Summary**: To detect GAN generated images, conventional supervised machine learning algorithms require collection of a number of real and fake images from the targeted GAN model. However, the specific model used by the attacker is often unavailable. To address this, we propose a GAN simulator, AutoGAN, which can simulate the artifacts produced by the common pipeline shared by several popular GAN models. Additionally, we identify a unique artifact caused by the up-sampling component included in the common GAN pipeline. We show theoretically such artifacts are manifested as replications of spectra in the frequency domain and thus propose a classifier model based on the spectrum input, rather than the pixel input. By using the simulated images to train a spectrum based classifier, even without seeing the fake images produced by the targeted GAN model during training, our approach achieves state-of-the-art performances on detecting fake images generated by popular GAN models such as CycleGAN.



### Deep Sequential Mosaicking of Fetoscopic Videos
- **Arxiv ID**: http://arxiv.org/abs/1907.06543v1
- **DOI**: 10.1007/978-3-030-32239-7_35
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.06543v1)
- **Published**: 2019-07-15 15:11:09+00:00
- **Updated**: 2019-07-15 15:11:09+00:00
- **Authors**: Sophia Bano, Francisco Vasconcelos, Marcel Tella Amo, George Dwyer, Caspar Gruijthuijsen, Jan Deprest, Sebastien Ourselin, Emmanuel Vander Poorten, Tom Vercauteren, Danail Stoyanov
- **Comment**: Accepted at MICCAI 2019
- **Journal**: None
- **Summary**: Twin-to-twin transfusion syndrome treatment requires fetoscopic laser photocoagulation of placental vascular anastomoses to regulate blood flow to both fetuses. Limited field-of-view (FoV) and low visual quality during fetoscopy make it challenging to identify all vascular connections. Mosaicking can align multiple overlapping images to generate an image with increased FoV, however, existing techniques apply poorly to fetoscopy due to the low visual quality, texture paucity, and hence fail in longer sequences due to the drift accumulated over time. Deep learning techniques can facilitate in overcoming these challenges. Therefore, we present a new generalized Deep Sequential Mosaicking (DSM) framework for fetoscopic videos captured from different settings such as simulation, phantom, and real environments. DSM extends an existing deep image-based homography model to sequential data by proposing controlled data augmentation and outlier rejection methods. Unlike existing methods, DSM can handle visual variations due to specular highlights and reflection across adjacent frames, hence reducing the accumulated drift. We perform experimental validation and comparison using 5 diverse fetoscopic videos to demonstrate the robustness of our framework.



### Recovery Guarantees for Compressible Signals with Adversarial Noise
- **Arxiv ID**: http://arxiv.org/abs/1907.06565v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.DS, cs.LG, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.06565v3)
- **Published**: 2019-07-15 16:15:12+00:00
- **Updated**: 2019-08-07 16:53:34+00:00
- **Authors**: Jasjeet Dhaliwal, Kyle Hambrook
- **Comment**: Theorem 1 updated, \ell_\infty defense added, Lemma 9 added, comp.
  section updated, abstract updated, and other minor writing edits
- **Journal**: None
- **Summary**: We provide recovery guarantees for compressible signals that have been corrupted with noise and extend the framework introduced in \cite{bafna2018thwarting} to defend neural networks against $\ell_0$-norm, $\ell_2$-norm, and $\ell_{\infty}$-norm attacks. Our results are general as they can be applied to most unitary transforms used in practice and hold for $\ell_0$-norm, $\ell_2$-norm, and $\ell_\infty$-norm bounded noise. In the case of $\ell_0$-norm noise, we prove recovery guarantees for Iterative Hard Thresholding (IHT) and Basis Pursuit (BP). For $\ell_2$-norm bounded noise, we provide recovery guarantees for BP and for the case of $\ell_\infty$-norm bounded noise, we provide recovery guarantees for Dantzig Selector (DS). These guarantees theoretically bolster the defense framework introduced in \cite{bafna2018thwarting} for defending neural networks against adversarial inputs. Finally, we experimentally demonstrate the effectiveness of this defense framework against an array of $\ell_0$, $\ell_2$ and $\ell_\infty$ norm attacks.



### Adversarial Video Generation on Complex Datasets
- **Arxiv ID**: http://arxiv.org/abs/1907.06571v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.06571v2)
- **Published**: 2019-07-15 16:27:04+00:00
- **Updated**: 2019-09-25 16:37:55+00:00
- **Authors**: Aidan Clark, Jeff Donahue, Karen Simonyan
- **Comment**: None
- **Journal**: None
- **Summary**: Generative models of natural images have progressed towards high fidelity samples by the strong leveraging of scale. We attempt to carry this success to the field of video modeling by showing that large Generative Adversarial Networks trained on the complex Kinetics-600 dataset are able to produce video samples of substantially higher complexity and fidelity than previous work. Our proposed model, Dual Video Discriminator GAN (DVD-GAN), scales to longer and higher resolution videos by leveraging a computationally efficient decomposition of its discriminator. We evaluate on the related tasks of video synthesis and video prediction, and achieve new state-of-the-art Fr\'echet Inception Distance for prediction for Kinetics-600, as well as state-of-the-art Inception Score for synthesis on the UCF-101 dataset, alongside establishing a strong baseline for synthesis on Kinetics-600.



### Multi-scale Graph-based Grading for Alzheimer's Disease Prediction
- **Arxiv ID**: http://arxiv.org/abs/1907.06625v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.06625v1)
- **Published**: 2019-07-15 17:57:17+00:00
- **Updated**: 2019-07-15 17:57:17+00:00
- **Authors**: Kilian Hett, Vinh-Thong Ta, JosÃ© V. ManjÃ³n, Pierrick CoupÃ©
- **Comment**: None
- **Journal**: None
- **Summary**: The prediction of subjects with mild cognitive impairment (MCI) who will progress to Alzheimer's disease (AD) is clinically relevant, and may above all have a significant impact on accelerate the development of new treatments. In this paper, we present a new MRI-based biomarker that enables us to predict conversion of MCI subjects to AD accurately. In order to better capture the AD signature, we introduce two main contributions. First, we present a new graph-based grading framework to combine inter-subject similarity features and intra-subject variability features. This framework involves patch-based grading of anatomical structures and graph-based modeling of structure alteration relationships. Second, we propose an innovative multiscale brain analysis to capture alterations caused by AD at different anatomical levels. Based on a cascade of classifiers, this multiscale approach enables the analysis of alterations of whole brain structures and hippocampus subfields at the same time. During our experiments using the ADNI-1 dataset, the proposed multiscale graph-based grading method obtained an area under the curve (AUC) of 81% to predict conversion of MCI subjects to AD within three years. Moreover, when combined with cognitive scores, the proposed method obtained 85% of AUC. These results are competitive in comparison to state-of-the-art methods evaluated on the same dataset.



### Batch-Shaping for Learning Conditional Channel Gated Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.06627v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.06627v4)
- **Published**: 2019-07-15 17:58:04+00:00
- **Updated**: 2020-04-03 08:42:24+00:00
- **Authors**: Babak Ehteshami Bejnordi, Tijmen Blankevoort, Max Welling
- **Comment**: Published as a conference paper at ICLR 2020
- **Journal**: None
- **Summary**: We present a method that trains large capacity neural networks with significantly improved accuracy and lower dynamic computational cost. We achieve this by gating the deep-learning architecture on a fine-grained-level. Individual convolutional maps are turned on/off conditionally on features in the network. To achieve this, we introduce a new residual block architecture that gates convolutional channels in a fine-grained manner. We also introduce a generally applicable tool $batch$-$shaping$ that matches the marginal aggregate posteriors of features in a neural network to a pre-specified prior distribution. We use this novel technique to force gates to be more conditional on the data. We present results on CIFAR-10 and ImageNet datasets for image classification, and Cityscapes for semantic segmentation. Our results show that our method can slim down large architectures conditionally, such that the average computational cost on the data is on par with a smaller architecture, but with higher accuracy. In particular, on ImageNet, our ResNet50 and ResNet34 gated networks obtain 74.60% and 72.55% top-1 accuracy compared to the 69.76% accuracy of the baseline ResNet18 model, for similar complexity. We also show that the resulting networks automatically learn to use more features for difficult examples and fewer features for simple examples.



### Slow Feature Analysis for Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.06670v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06670v1)
- **Published**: 2019-07-15 18:05:37+00:00
- **Updated**: 2019-07-15 18:05:37+00:00
- **Authors**: Zhang Zhang, Dacheng Tao
- **Comment**: None
- **Journal**: IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
  VOL. 34, NO. 3, MARCH 2012
- **Summary**: Slow Feature Analysis (SFA) extracts slowly varying features from a quickly varying input signal. It has been successfully applied to modeling the visual receptive fields of the cortical neurons. Sufficient experimental results in neuroscience suggest that the temporal slowness principle is a general learning principle in visual perception. In this paper, we introduce the SFA framework to the problem of human action recognition by incorporating the discriminative information with SFA learning and considering the spatial relationship of body parts. In particular, we consider four kinds of SFA learning strategies, including the original unsupervised SFA (U-SFA), the supervised SFA (S-SFA), the discriminative SFA (D-SFA), and the spatial discriminative SFA (SD-SFA), to extract slow feature functions from a large amount of training cuboids which are obtained by random sampling in motion boundaries. Afterward, to represent action sequences, the squared first order temporal derivatives are accumulated over all transformed cuboids into one feature vector, which is termed the Accumulated Squared Derivative (ASD) feature. The ASD feature encodes the statistical distribution of slow features in an action sequence. Finally, a linear support vector machine (SVM) is trained to classify actions represented by ASD features. We conduct extensive experiments, including two sets of control experiments, two sets of large scale experiments on the KTH and Weizmann databases, and two sets of experiments on the CASIA and UT-interaction databases, to demonstrate the effectiveness of SFA for human action recognition.



### MaskPlus: Improving Mask Generation for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.06713v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06713v3)
- **Published**: 2019-07-15 19:36:54+00:00
- **Updated**: 2019-09-27 08:34:29+00:00
- **Authors**: Shichao Xu, Shuyue Lan, Qi Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Instance segmentation is a promising yet challenging topic in computer vision. Recent approaches such as Mask R-CNN typically divide this problem into two parts -- a detection component and a mask generation branch, and mostly focus on the improvement of the detection part. In this paper, we present an approach that extends Mask R-CNN with five novel optimization techniques for improving the mask generation branch and reducing the conflicts between the mask branch and the detection component in training. These five techniques are independent to each other and can be flexibly utilized in building various instance segmentation architectures for increasing the overall accuracy. We demonstrate the effectiveness of our approach with tests on the COCO dataset.



### Real-time Facial Surface Geometry from Monocular Video on Mobile GPUs
- **Arxiv ID**: http://arxiv.org/abs/1907.06724v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06724v1)
- **Published**: 2019-07-15 20:08:17+00:00
- **Updated**: 2019-07-15 20:08:17+00:00
- **Authors**: Yury Kartynnik, Artsiom Ablavatski, Ivan Grishchenko, Matthias Grundmann
- **Comment**: 4 pages, 4 figures; CVPR Workshop on Computer Vision for Augmented
  and Virtual Reality, Long Beach, CA, USA, 2019
- **Journal**: None
- **Summary**: We present an end-to-end neural network-based model for inferring an approximate 3D mesh representation of a human face from single camera input for AR applications. The relatively dense mesh model of 468 vertices is well-suited for face-based AR effects. The proposed model demonstrates super-realtime inference speed on mobile GPUs (100-1000+ FPS, depending on the device and model variant) and a high prediction quality that is comparable to the variance in manual annotations of the same image.



### Deep learning-based color holographic microscopy
- **Arxiv ID**: http://arxiv.org/abs/1907.06727v1
- **DOI**: 10.1002/jbio.201900107
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.optics, 68T01, 68T05, 68U10, 62M45, 78M32, 92C55, 94A08, I.2; I.2.1; I.2.6; I.2.10; I.4.5; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/1907.06727v1)
- **Published**: 2019-07-15 20:15:21+00:00
- **Updated**: 2019-07-15 20:15:21+00:00
- **Authors**: Tairan Liu, Zhensong Wei, Yair Rivenson, Kevin de Haan, Yibo Zhang, Yichen Wu, Aydogan Ozcan
- **Comment**: 25 pages, 8 Figures, 2 Tables
- **Journal**: Journal of Biophotonics (2019)
- **Summary**: We report a framework based on a generative adversarial network (GAN) that performs high-fidelity color image reconstruction using a single hologram of a sample that is illuminated simultaneously by light at three different wavelengths. The trained network learns to eliminate missing-phase-related artifacts, and generates an accurate color transformation for the reconstructed image. Our framework is experimentally demonstrated using lung and prostate tissue sections that are labeled with different histological stains. This framework is envisaged to be applicable to point-of-care histopathology, and presents a significant improvement in the throughput of coherent microscopy systems given that only a single hologram of the specimen is required for accurate color imaging.



### Mapping road safety features from streetview imagery: A deep learning approach
- **Arxiv ID**: http://arxiv.org/abs/1907.12647v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.12647v1)
- **Published**: 2019-07-15 20:38:33+00:00
- **Updated**: 2019-07-15 20:38:33+00:00
- **Authors**: Arpan Sainju, Zhe Jiang
- **Comment**: 17 pages, 16 figures, 3 tables
- **Journal**: None
- **Summary**: Each year, around 6 million car accidents occur in the U.S. on average. Road safety features (e.g., concrete barriers, metal crash barriers, rumble strips) play an important role in preventing or mitigating vehicle crashes. Accurate maps of road safety features is an important component of safety management systems for federal or state transportation agencies, helping traffic engineers identify locations to invest on safety infrastructure. In current practice, mapping road safety features is largely done manually (e.g., observations on the road or visual interpretation of streetview imagery), which is both expensive and time consuming. In this paper, we propose a deep learning approach to automatically map road safety features from streetview imagery. Unlike existing Convolutional Neural Networks (CNNs) that classify each image individually, we propose to further add Recurrent Neural Network (Long Short Term Memory) to capture geographic context of images (spatial autocorrelation effect along linear road network paths). Evaluations on real world streetview imagery show that our proposed model outperforms several baseline methods.



### Real-time Hair Segmentation and Recoloring on Mobile GPUs
- **Arxiv ID**: http://arxiv.org/abs/1907.06740v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06740v1)
- **Published**: 2019-07-15 20:39:15+00:00
- **Updated**: 2019-07-15 20:39:15+00:00
- **Authors**: Andrei Tkachenka, Gregory Karpiak, Andrey Vakunov, Yury Kartynnik, Artsiom Ablavatski, Valentin Bazarevsky, Siargey Pisarchyk
- **Comment**: 4 pages, 5 figures; CVPR Workshop on Computer Vision for Augmented
  and Virtual Reality, Long Beach, CA, USA, 2019
- **Journal**: None
- **Summary**: We present a novel approach for neural network-based hair segmentation from a single camera input specifically designed for real-time, mobile application. Our relatively small neural network produces a high-quality hair segmentation mask that is well suited for AR effects, e.g. virtual hair recoloring. The proposed model achieves real-time inference speed on mobile GPUs (30-100+ FPS, depending on the device) with high accuracy. We also propose a very realistic hair recoloring scheme. Our method has been deployed in major AR application and is used by millions of users.



### AugLabel: Exploiting Word Representations to Augment Labels for Face Attribute Classification
- **Arxiv ID**: http://arxiv.org/abs/1907.06757v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06757v1)
- **Published**: 2019-07-15 21:15:09+00:00
- **Updated**: 2019-07-15 21:15:09+00:00
- **Authors**: Binod Bhattarai, Rumeysa Bodur, Tae-Kyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Augmenting data in image space (eg. flipping, cropping etc) and activation space (eg. dropout) are being widely used to regularise deep neural networks and have been successfully applied on several computer vision tasks. Unlike previous works, which are mostly focused on doing augmentation in the aforementioned domains, we propose to do augmentation in label space. In this paper, we present a novel method to generate fixed dimensional labels with continuous values for images by exploiting the word2vec representations of the existing categorical labels. We then append these representations with existing categorical labels and train the model. We validated our idea on two challenging face attribute classification data sets viz. CelebA and LFWA. Our extensive experiments show that the augmented labels improve the performance of the competitive deep learning baseline and reduce the need of annotated real data up to 50%, while attaining a performance similar to the state-of-the-art methods.



### The iWildCam 2019 Challenge Dataset
- **Arxiv ID**: http://arxiv.org/abs/1907.07617v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.07617v1)
- **Published**: 2019-07-15 21:57:00+00:00
- **Updated**: 2019-07-15 21:57:00+00:00
- **Authors**: Sara Beery, Dan Morris, Pietro Perona
- **Comment**: From the Sixth Fine-Grained Visual Categorization Workshop at CVPR19.
  arXiv admin note: text overlap with arXiv:1904.05986
- **Journal**: None
- **Summary**: Camera Traps (or Wild Cams) enable the automatic collection of large quantities of image data. Biologists all over the world use camera traps to monitor biodiversity and population density of animal species. The computer vision community has been making strides towards automating the species classification challenge in camera traps, but as we try to expand the scope of these models from specific regions where we have collected training data to different areas we are faced with an interesting problem: how do you classify a species in a new region that you may not have seen in previous training data?   In order to tackle this problem, we have prepared a dataset and challenge where the training data and test data are from different regions, namely The American Southwest and the American Northwest. We use the Caltech Camera Traps dataset, collected from the American Southwest, as training data. We add a new dataset from the American Northwest, curated from data provided by the Idaho Department of Fish and Game (IDFG), as our test dataset. The test data has some class overlap with the training data, some species are found in both datasets, but there are both species seen during training that are not seen during test and vice versa. To help fill the gaps in the training species, we allow competitors to utilize transfer learning from two alternate domains: human-curated images from iNaturalist and synthetic images from Microsoft's TrapCam-AirSim simulation environment.



### Efficient Pipeline for Camera Trap Image Review
- **Arxiv ID**: http://arxiv.org/abs/1907.06772v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06772v1)
- **Published**: 2019-07-15 22:15:00+00:00
- **Updated**: 2019-07-15 22:15:00+00:00
- **Authors**: Sara Beery, Dan Morris, Siyu Yang
- **Comment**: From the Data Mining and AI for Conservation Workshop at KDD19
- **Journal**: None
- **Summary**: Biologists all over the world use camera traps to monitor biodiversity and wildlife population density. The computer vision community has been making strides towards automating the species classification challenge in camera traps, but it has proven difficult to to apply models trained in one region to images collected in different geographic areas. In some cases, accuracy falls off catastrophically in new region, due to both changes in background and the presence of previously-unseen species. We propose a pipeline that takes advantage of a pre-trained general animal detector and a smaller set of labeled images to train a classification model that can efficiently achieve accurate results in a new region.



### Improving 3D Object Detection for Pedestrians with Virtual Multi-View Synthesis Orientation Estimation
- **Arxiv ID**: http://arxiv.org/abs/1907.06777v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06777v1)
- **Published**: 2019-07-15 22:27:16+00:00
- **Updated**: 2019-07-15 22:27:16+00:00
- **Authors**: Jason Ku, Alex D. Pon, Sean Walsh, Steven L. Waslander
- **Comment**: Accepted in IROS 2019
- **Journal**: None
- **Summary**: Accurately estimating the orientation of pedestrians is an important and challenging task for autonomous driving because this information is essential for tracking and predicting pedestrian behavior. This paper presents a flexible Virtual Multi-View Synthesis module that can be adopted into 3D object detection methods to improve orientation estimation. The module uses a multi-step process to acquire the fine-grained semantic information required for accurate orientation estimation. First, the scene's point cloud is densified using a structure preserving depth completion algorithm and each point is colorized using its corresponding RGB pixel. Next, virtual cameras are placed around each object in the densified point cloud to generate novel viewpoints, which preserve the object's appearance. We show that this module greatly improves the orientation estimation on the challenging pedestrian class on the KITTI benchmark. When used with the open-source 3D detector AVOD-FPN, we outperform all other published methods on the pedestrian Orientation, 3D, and Bird's Eye View benchmarks.



### Rethinking RGB-D Salient Object Detection: Models, Data Sets, and Large-Scale Benchmarks
- **Arxiv ID**: http://arxiv.org/abs/1907.06781v2
- **DOI**: 10.1109/TNNLS.2020.2996406
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06781v2)
- **Published**: 2019-07-15 22:43:20+00:00
- **Updated**: 2020-07-11 17:32:35+00:00
- **Authors**: Deng-Ping Fan, Zheng Lin, Jia-Xing Zhao, Yun Liu, Zhao Zhang, Qibin Hou, Menglong Zhu, Ming-Ming Cheng
- **Comment**: Accepted in TNNLS20. 15 pages, 12 figures. Code:
  https://github.com/DengPingFan/D3NetBenchmark
- **Journal**: None
- **Summary**: The use of RGB-D information for salient object detection has been extensively explored in recent years. However, relatively few efforts have been put towards modeling salient object detection in real-world human activity scenes with RGBD. In this work, we fill the gap by making the following contributions to RGB-D salient object detection. (1) We carefully collect a new SIP (salient person) dataset, which consists of ~1K high-resolution images that cover diverse real-world scenes from various viewpoints, poses, occlusions, illuminations, and backgrounds. (2) We conduct a large-scale (and, so far, the most comprehensive) benchmark comparing contemporary methods, which has long been missing in the field and can serve as a baseline for future research. We systematically summarize 32 popular models and evaluate 18 parts of 32 models on seven datasets containing a total of about 97K images. (3) We propose a simple general architecture, called Deep Depth-Depurator Network (D3Net). It consists of a depth depurator unit (DDU) and a three-stream feature learning module (FLM), which performs low-quality depth map filtering and cross-modal feature learning respectively. These components form a nested structure and are elaborately designed to be learned jointly. D3Net exceeds the performance of any prior contenders across all five metrics under consideration, thus serving as a strong model to advance research in this field. We also demonstrate that D3Net can be used to efficiently extract salient object masks from real scenes, enabling effective background changing application with a speed of 65fps on a single GPU. All the saliency maps, our new SIP dataset, the D3Net model, and the evaluation tools are publicly available at https://github.com/DengPingFan/D3NetBenchmark.



