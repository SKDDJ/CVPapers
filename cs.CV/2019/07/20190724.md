# Arxiv Papers in cs.CV on 2019-07-24
### Pose-variant 3D Facial Attribute Generation
- **Arxiv ID**: http://arxiv.org/abs/1907.10202v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10202v1)
- **Published**: 2019-07-24 01:52:33+00:00
- **Updated**: 2019-07-24 01:52:33+00:00
- **Authors**: Feng-Ju Chang, Xiang Yu, Ram Nevatia, Manmohan Chandraker
- **Comment**: None
- **Journal**: None
- **Summary**: We address the challenging problem of generating facial attributes using a single image in an unconstrained pose. In contrast to prior works that largely consider generation on 2D near-frontal images, we propose a GAN-based framework to generate attributes directly on a dense 3D representation given by UV texture and position maps, resulting in photorealistic, geometrically-consistent and identity-preserving outputs. Starting from a self-occluded UV texture map obtained by applying an off-the-shelf 3D reconstruction method, we propose two novel components. First, a texture completion generative adversarial network (TC-GAN) completes the partial UV texture map. Second, a 3D attribute generation GAN (3DA-GAN) synthesizes the target attribute while obtaining an appearance consistent with 3D face geometry and preserving identity. Extensive experiments on CelebA, LFW and IJB-A show that our method achieves consistently better attribute generation accuracy than prior methods, a higher degree of qualitative photorealism and preserves face identity information.



### Mixed-Supervised Dual-Network for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.10209v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10209v2)
- **Published**: 2019-07-24 02:31:45+00:00
- **Updated**: 2019-08-26 05:45:09+00:00
- **Authors**: Duo Wang, Ming Li, Nir Ben-Shlomo, C. Eduardo Corrales, Yu Cheng, Tao Zhang, Jagadeesan Jayender
- **Comment**: To appear in MICCAI 2019
- **Journal**: None
- **Summary**: Deep learning based medical image segmentation models usually require large datasets with high-quality dense segmentations to train, which are very time-consuming and expensive to prepare. One way to tackle this challenge is by using the mixed-supervised learning framework, in which only a part of data is densely annotated with segmentation label and the rest is weakly labeled with bounding boxes. The model is trained jointly in a multi-task learning setting. In this paper, we propose Mixed-Supervised Dual-Network (MSDN), a novel architecture which consists of two separate networks for the detection and segmentation tasks respectively, and a series of connection modules between the layers of the two networks. These connection modules are used to transfer useful information from the auxiliary detection task to help the segmentation task. We propose to use a recent technique called "Squeeze and Excitation" in the connection module to boost the transfer. We conduct experiments on two medical image segmentation datasets. The proposed MSDN model outperforms multiple baselines.



### A CNN-based tool for automatic tongue contour tracking in ultrasound images
- **Arxiv ID**: http://arxiv.org/abs/1907.10210v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.10210v1)
- **Published**: 2019-07-24 02:34:58+00:00
- **Updated**: 2019-07-24 02:34:58+00:00
- **Authors**: Jian Zhu, Will Styler, Ian Calloway
- **Comment**: None
- **Journal**: None
- **Summary**: For speech research, ultrasound tongue imaging provides a non-invasive means for visualizing tongue position and movement during articulation. Extracting tongue contours from ultrasound images is a basic step in analyzing ultrasound data but this task often requires non-trivial manual annotation. This study presents an open source tool for fully automatic tracking of tongue contours in ultrasound frames using neural network based methods. We have implemented and systematically compared two convolutional neural networks, U-Net and DenseU-Net, under different conditions. Though both models can perform automatic contour tracking with comparable accuracy, Dense U-Net architecture seems more generalizable across test datasets while U-Net has faster extraction speed. Our comparison also shows that the choice of loss function and data augmentation have a greater effect on tracking performance in this task. This public available segmentation tool shows considerable promise for the automated tongue contour annotation of ultrasound images in speech research.



### Motion-Aware Feature for Improved Video Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/1907.10211v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.10211v1)
- **Published**: 2019-07-24 02:36:28+00:00
- **Updated**: 2019-07-24 02:36:28+00:00
- **Authors**: Yi Zhu, Shawn Newsam
- **Comment**: BMVC 2019
- **Journal**: None
- **Summary**: Motivated by our observation that motion information is the key to good anomaly detection performance in video, we propose a temporal augmented network to learn a motion-aware feature. This feature alone can achieve competitive performance with previous state-of-the-art methods, and when combined with them, can achieve significant performance improvements. Furthermore, we incorporate temporal context into the Multiple Instance Learning (MIL) ranking model by using an attention block. The learned attention weights can help to differentiate between anomalous and normal video segments better. With the proposed motion-aware feature and the temporal MIL ranking model, we outperform previous approaches by a large margin on both anomaly detection and anomalous action recognition tasks in the UCF Crime dataset.



### Image Super-Resolution Using a Wavelet-based Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1907.10213v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.10213v1)
- **Published**: 2019-07-24 02:44:41+00:00
- **Updated**: 2019-07-24 02:44:41+00:00
- **Authors**: Qi Zhang, Huafeng Wang, Sichen Yang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we consider the problem of super-resolution recons-truction. This is a hot topic because super-resolution reconstruction has a wide range of applications in the medical field, remote sensing monitoring, and criminal investigation. Compared with traditional algorithms, the current super-resolution reconstruction algorithm based on deep learning greatly improves the clarity of reconstructed pictures. Existing work like Super-Resolution Using a Generative Adversarial Network (SRGAN) can effectively restore the texture details of the image. However, experimentally verified that the texture details of the image recovered by the SRGAN are not robust. In order to get super-resolution reconstructed images with richer high-frequency details, we improve the network structure and propose a super-resolution reconstruction algorithm combining wavelet transform and Generative Adversarial Network. The proposed algorithm can efficiently reconstruct high-resolution images with rich global information and local texture details. We have trained our model by PyTorch framework and VOC2012 dataset, and tested it by Set5, Set14, BSD100 and Urban100 test datasets.



### Efficient Circle-Based Camera Pose Tracking Free of PnP
- **Arxiv ID**: http://arxiv.org/abs/1907.10219v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10219v1)
- **Published**: 2019-07-24 03:07:45+00:00
- **Updated**: 2019-07-24 03:07:45+00:00
- **Authors**: Fulin Tang, Yihong Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Camera pose tracking attracts much interest both from academic and industrial communities, of which the methods based on planar markers are easy to be implemented. However, most of the existing methods need to identify multiple points in the marker images for matching to space points. Then, PnP and RANSAC are used to compute the camera pose. If cameras move fast or are far away from markers, matching is easy to generate errors and even RANSAC cannot remove incorrect matching. Then, the result by PnP cannot have good performance. To solve this problem, we design circular markers and represent 6D camera pose analytically and unifiedly as very concise forms from each of the marker by projective invariance. Afterwards, the pose is further optimized by a proposed nonlinear cost function based on a polar-n-direction geometric distance. The method is from imaged circle edges and without PnP/RANSAC, making pose tracking robust and accurate. Experimental results show that the proposed method outperforms the state of the arts in terms of noise, blur, and distance from camera to marker. Simultaneously, it can still run at about 100 FPS on a consumer computer with only CPU.



### Movement science needs different pose tracking algorithms
- **Arxiv ID**: http://arxiv.org/abs/1907.10226v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1907.10226v1)
- **Published**: 2019-07-24 03:54:22+00:00
- **Updated**: 2019-07-24 03:54:22+00:00
- **Authors**: Nidhi Seethapathi, Shaofei Wang, Rachit Saluja, Gunnar Blohm, Konrad P. Kording
- **Comment**: 13 pages, 2 figures, 1 table
- **Journal**: None
- **Summary**: Over the last decade, computer science has made progress towards extracting body pose from single camera photographs or videos. This promises to enable movement science to detect disease, quantify movement performance, and take the science out of the lab into the real world. However, current pose tracking algorithms fall short of the needs of movement science; the types of movement data that matter are poorly estimated. For instance, the metrics currently used for evaluating pose tracking algorithms use noisy hand-labeled ground truth data and do not prioritize precision of relevant variables like three-dimensional position, velocity, acceleration, and forces which are crucial for movement science. Here, we introduce the scientific disciplines that use movement data, the types of data they need, and discuss the changes needed to make pose tracking truly transformative for movement science.



### Stochastic trajectory prediction with social graph network
- **Arxiv ID**: http://arxiv.org/abs/1907.10233v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10233v1)
- **Published**: 2019-07-24 04:44:34+00:00
- **Updated**: 2019-07-24 04:44:34+00:00
- **Authors**: Lidan Zhang, Qi She, Ping Guo
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Pedestrian trajectory prediction is a challenging task because of the complexity of real-world human social behaviors and uncertainty of the future motion. For the first issue, existing methods adopt fully connected topology for modeling the social behaviors, while ignoring non-symmetric pairwise relationships. To effectively capture social behaviors of relevant pedestrians, we utilize a directed social graph which is dynamically constructed on timely location and speed direction. Based on the social graph, we further propose a network to collect social effects and accumulate with individual representation, in order to generate destination-oriented and social-aware representations. For the second issue, instead of modeling the uncertainty of the entire future as a whole, we utilize a temporal stochastic method for sequentially learning a prior model of uncertainty during social interactions. The prediction on the next step is then generated by sampling on the prior model and progressively decoding with a hierarchical LSTMs. Experimental results on two public datasets show the effectiveness of our method, especially when predicting trajectories in very crowded scenes.



### AdaCoF: Adaptive Collaboration of Flows for Video Frame Interpolation
- **Arxiv ID**: http://arxiv.org/abs/1907.10244v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10244v3)
- **Published**: 2019-07-24 05:40:53+00:00
- **Updated**: 2020-03-08 13:19:57+00:00
- **Authors**: Hyeongmin Lee, Taeoh Kim, Tae-young Chung, Daehyun Pak, Yuseok Ban, Sangyoun Lee
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Video frame interpolation is one of the most challenging tasks in video processing research. Recently, many studies based on deep learning have been suggested. Most of these methods focus on finding locations with useful information to estimate each output pixel using their own frame warping operations. However, many of them have Degrees of Freedom (DoF) limitations and fail to deal with the complex motions found in real world videos. To solve this problem, we propose a new warping module named Adaptive Collaboration of Flows (AdaCoF). Our method estimates both kernel weights and offset vectors for each target pixel to synthesize the output frame. AdaCoF is one of the most generalized warping modules compared to other approaches, and covers most of them as special cases of it. Therefore, it can deal with a significantly wide domain of complex motions. To further improve our framework and synthesize more realistic outputs, we introduce dual-frame adversarial loss which is applicable only to video frame interpolation tasks. The experimental results show that our method outperforms the state-of-the-art methods for both fixed training set environments and the Middlebury benchmark.



### Learning Embedding of 3D models with Quadric Loss
- **Arxiv ID**: http://arxiv.org/abs/1907.10250v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.10250v1)
- **Published**: 2019-07-24 05:57:27+00:00
- **Updated**: 2019-07-24 05:57:27+00:00
- **Authors**: Nitin Agarwal, Sung-eui Yoon, M Gopi
- **Comment**: Accepted to BMVC 2019 for Oral Presentation
- **Journal**: None
- **Summary**: Sharp features such as edges and corners play an important role in the perception of 3D models. In order to capture them better, we propose quadric loss, a point-surface loss function, which minimizes the quadric error between the reconstructed points and the input surface. Computation of Quadric loss is easy, efficient since the quadric matrices can be computed apriori, and is fully differentiable, making quadric loss suitable for training point and mesh based architectures. Through extensive experiments we show the merits and demerits of quadric loss. When combined with Chamfer loss, quadric loss achieves better reconstruction results as compared to any one of them or other point-surface loss functions.



### HA-CCN: Hierarchical Attention-based Crowd Counting Network
- **Arxiv ID**: http://arxiv.org/abs/1907.10255v1
- **DOI**: 10.1109/TIP.2019.2928634
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10255v1)
- **Published**: 2019-07-24 06:20:14+00:00
- **Updated**: 2019-07-24 06:20:14+00:00
- **Authors**: Vishwanath A. Sindagi, Vishal M. Patel
- **Comment**: Accepted for publication at IEEE Transactions on Image Processing
  (TIP) 2019
- **Journal**: None
- **Summary**: Single image-based crowd counting has recently witnessed increased focus, but many leading methods are far from optimal, especially in highly congested scenes. In this paper, we present Hierarchical Attention-based Crowd Counting Network (HA-CCN) that employs attention mechanisms at various levels to selectively enhance the features of the network. The proposed method, which is based on the VGG16 network, consists of a spatial attention module (SAM) and a set of global attention modules (GAM). SAM enhances low-level features in the network by infusing spatial segmentation information, whereas the GAM focuses on enhancing channel-wise information in the higher level layers. The proposed method is a single-step training framework, simple to implement and achieves state-of-the-art results on different datasets.   Furthermore, we extend the proposed counting network by introducing a novel set-up to adapt the network to different scenes and datasets via weak supervision using image-level labels. This new set up reduces the burden of acquiring labour intensive point-wise annotations for new datasets while improving the cross-dataset performance.



### Adaptive and Compressive Beamforming Using Deep Learning for Medical Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/1907.10257v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.10257v3)
- **Published**: 2019-07-24 06:30:30+00:00
- **Updated**: 2020-02-23 16:15:20+00:00
- **Authors**: Shujaat Khan, Jaeyoung Huh, Jong Chul Ye
- **Comment**: This is a significantly extended version of the original paper in
  arXiv:1901.01706. This paper is accepted for IEEE Transactions on
  Ultrasonics, Ferroelectrics, and Frequency Control
- **Journal**: None
- **Summary**: In ultrasound (US) imaging, various types of adaptive beamforming techniques have been investigated to improve the resolution and contrast-to-noise ratio of the delay and sum (DAS) beamformers. Unfortunately, the performance of these adaptive beamforming approaches degrade when the underlying model is not sufficiently accurate and the number of channels decreases. To address this problem, here we propose a deep learning-based beamformer to generate significantly improved images over widely varying measurement conditions and channel subsampling patterns. In particular, our deep neural network is designed to directly process full or sub-sampled radio-frequency (RF) data acquired at various subsampling rates and detector configurations so that it can generate high quality ultrasound images using a single beamformer. The origin of such input-dependent adaptivity is also theoretically analyzed. Experimental results using B-mode focused ultrasound confirm the efficacy of the proposed methods.



### Discriminative Consistent Domain Generation for Semi-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.10267v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.10267v1)
- **Published**: 2019-07-24 06:59:23+00:00
- **Updated**: 2019-07-24 06:59:23+00:00
- **Authors**: Jun Chen, Heye Zhang, Yanping Zhang, Shu Zhao, Raad Mohiaddin, Tom Wong, David Firmin, Guang Yang, Jennifer Keegan
- **Comment**: MICCAI 2019
- **Journal**: None
- **Summary**: Deep learning based task systems normally rely on a large amount of manually labeled training data, which is expensive to obtain and subject to operator variations. Moreover, it does not always hold that the manually labeled data and the unlabeled data are sitting in the same distribution. In this paper, we alleviate these problems by proposing a discriminative consistent domain generation (DCDG) approach to achieve a semi-supervised learning. The discriminative consistent domain is achieved by a double-sided domain adaptation. The double-sided domain adaptation aims to make a fusion of the feature spaces of labeled data and unlabeled data. In this way, we can fit the differences of various distributions between labeled data and unlabeled data. In order to keep the discriminativeness of generated consistent domain for the task learning, we apply an indirect learning for the double-sided domain adaptation. Based on the generated discriminative consistent domain, we can use the unlabeled data to learn the task model along with the labeled data via a consistent image generation. We demonstrate the performance of our proposed DCDG on the late gadolinium enhancement cardiac MRI (LGE-CMRI) images acquired from patients with atrial fibrillation in two clinical centers for the segmentation of the left atrium anatomy (LA) and proximal pulmonary veins (PVs). The experiments show that our semi-supervised approach achieves compelling segmentation results, which can prove the robustness of DCDG for the semi-supervised learning using the unlabeled data along with labeled data acquired from a single center or multicenter studies.



### Hyperspectral City V1.0 Dataset and Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1907.10270v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10270v4)
- **Published**: 2019-07-24 07:13:27+00:00
- **Updated**: 2020-02-26 12:50:23+00:00
- **Authors**: Shaodi You, Erqi Huang, Shuaizhe Liang, Yongrong Zheng, Yunxiang Li, Fan Wang, Sen Lin, Qiu Shen, Xun Cao, Diming Zhang, Yuanjiang Li, Yu Li, Ying Fu, Boxin Shi, Feng Lu, Yinqiang Zheng, Robby T. Tan
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: This document introduces the background and the usage of the Hyperspectral City Dataset and the benchmark. The documentation first starts with the background and motivation of the dataset. Follow it, we briefly describe the method of collecting the dataset and the processing method from raw dataset to the final release dataset, specifically, the version 1.0. We also provide the detailed usage of the dataset and the evaluation metric for submitted the result for the 2019 Hyperspectral City Challenge.



### Non-Local Representation based Mutual Affine-Transfer Network for Photorealistic Stylization
- **Arxiv ID**: http://arxiv.org/abs/1907.10274v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10274v2)
- **Published**: 2019-07-24 07:21:44+00:00
- **Updated**: 2021-07-06 03:02:07+00:00
- **Authors**: Ying Qu, Zhenzhou Shao, Hairong Qi
- **Comment**: Accepted by IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE
  INTELLIGENCE
- **Journal**: None
- **Summary**: Photorealistic stylization aims to transfer the style of a reference photo onto a content photo in a natural fashion, such that the stylized image looks like a real photo taken by a camera. State-of-the-art methods stylize the image locally within each matched semantic region and are prone to global color inconsistency across semantic objects/parts, making the stylized image less photorealistic. To tackle the challenging issues, we propose a non-local representation scheme, constrained with a mutual affine-transfer network (NL-MAT). Through a dictionary-based decomposition, NL-MAT is able to successfully decouple matched non-local representations and color information of the image pair, such that the context correspondence between the image pair is incorporated naturally, which largely facilitates local style transfer in a global-consistent fashion. To the best of our knowledge, this is the first attempt to address the photorealistic stylization problem with a non-local representation scheme, such that no additional models or steps for semantic matching are required during stylization. Experimental results demonstrate that the proposed method is able to generate photorealistic results with local style transfer while preserving both the spatial structure and global color consistency of the content image.



### Backward-Forward Algorithm: An Improvement towards Extreme Learning Machine
- **Arxiv ID**: http://arxiv.org/abs/1907.10282v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.10282v4)
- **Published**: 2019-07-24 07:52:57+00:00
- **Updated**: 2019-10-07 08:37:20+00:00
- **Authors**: Dibyasundar Das, Deepak Ranjan Nayak, Ratnakar Dash, Banshidhar Majhi
- **Comment**: 12 Pages, 11 figures, to be submitted to journal
- **Journal**: None
- **Summary**: The extreme learning machine needs a large number of hidden nodes to generalize a single hidden layer neural network for a given training data-set. The need for more number of hidden nodes suggests that the neural-network is memorizing rather than generalizing the model. Hence, a supervised learning method is described here that uses Moore-Penrose approximation to determine both input-weight and output-weight in two epochs, namely, backward-pass and forward-pass. The proposed technique has an advantage over the back-propagation method in terms of iterations required and is superior to the extreme learning machine in terms of the number of hidden units necessary for generalization.



### StableNet: Semi-Online, Multi-Scale Deep Video Stabilization
- **Arxiv ID**: http://arxiv.org/abs/1907.10283v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.10283v1)
- **Published**: 2019-07-24 07:54:45+00:00
- **Updated**: 2019-07-24 07:54:45+00:00
- **Authors**: Chia-Hung Huang, Hang Yin, Yu-Wing Tai, Chi-Keung Tang
- **Comment**: Chia-Hung and Hang have equal contribution
- **Journal**: None
- **Summary**: Video stabilization algorithms are of greater importance nowadays with the prevalence of hand-held devices which unavoidably produce videos with undesirable shaky motions. In this paper we propose a data-driven online video stabilization method along with a paired dataset for deep learning. The network processes each unsteady frame progressively in a multi-scale manner, from low resolution to high resolution, and then outputs an affine transformation to stabilize the frame. Different from conventional methods which require explicit feature tracking or optical flow estimation, the underlying stabilization process is learned implicitly from the training data, and the stabilization process can be done online. Since there are limited public video stabilization datasets available, we synthesized unstable videos with different extent of shake that simulate real-life camera movement. Experiments show that our method is able to outperform other stabilization methods in several unstable samples while remaining comparable in general. Also, our method is tested on complex contents and found robust enough to dampen these samples to some extent even it was not explicitly trained in the contents.



### Zero-Shot Sign Language Recognition: Can Textual Data Uncover Sign Languages?
- **Arxiv ID**: http://arxiv.org/abs/1907.10292v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10292v1)
- **Published**: 2019-07-24 08:12:41+00:00
- **Updated**: 2019-07-24 08:12:41+00:00
- **Authors**: Yunus Can Bilge, Nazli Ikizler-Cinbis, Ramazan Gokberk Cinbis
- **Comment**: To appear in British Machine Vision Conference (BMVC) 2019
- **Journal**: None
- **Summary**: We introduce the problem of zero-shot sign language recognition (ZSSLR), where the goal is to leverage models learned over the seen sign class examples to recognize the instances of unseen signs. To this end, we propose to utilize the readily available descriptions in sign language dictionaries as an intermediate-level semantic representation for knowledge transfer. We introduce a new benchmark dataset called ASL-Text that consists of 250 sign language classes and their accompanying textual descriptions. Compared to the ZSL datasets in other domains (such as object recognition), our dataset consists of limited number of training examples for a large number of classes, which imposes a significant challenge. We propose a framework that operates over the body and hand regions by means of 3D-CNNs, and models longer temporal relationships via bidirectional LSTMs. By leveraging the descriptive text embeddings along with these spatio-temporal representations within a zero-shot learning framework, we show that textual data can indeed be useful in uncovering sign languages. We anticipate that the introduced approach and the accompanying dataset will provide a basis for further exploration of this new zero-shot learning problem.



### Segmenting Objects in Day and Night:Edge-Conditioned CNN for Thermal Image Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.10303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10303v1)
- **Published**: 2019-07-24 08:49:27+00:00
- **Updated**: 2019-07-24 08:49:27+00:00
- **Authors**: Chenglong Li, Wei Xia, Yan Yan, Bin Luo, Jin Tang
- **Comment**: Submitted it to IEEE TNNLS. arXiv admin note: text overlap with
  arXiv:1806.01013 by other authors
- **Journal**: None
- **Summary**: Despite much research progress in image semantic segmentation, it remains challenging under adverse environmental conditions caused by imaging limitations of visible spectrum. While thermal infrared cameras have several advantages over cameras for the visible spectrum, such as operating in total darkness, insensitive to illumination variations, robust to shadow effects and strong ability to penetrate haze and smog. These advantages of thermal infrared cameras make the segmentation of semantic objects in day and night. In this paper, we propose a novel network architecture, called edge-conditioned convolutional neural network (EC-CNN), for thermal image semantic segmentation. Particularly, we elaborately design a gated feature-wise transform layer in EC-CNN to adaptively incorporate edge prior knowledge. The whole EC-CNN is end-to-end trained, and can generate high-quality segmentation results with the edge guidance. Meanwhile, we also introduce a new benchmark dataset named "Segment Objects in Day And night"(SODA) for comprehensive evaluations in thermal image semantic segmentation. SODA contains over 7,168 manually annotated and synthetically generated thermal images with 20 semantic region labels and from a broad range of viewpoints and scene complexities. Extensive experiments on SODA demonstrate the effectiveness of the proposed EC-CNN against the state-of-the-art methods.



### Towards Adversarially Robust Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1907.10310v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.10310v1)
- **Published**: 2019-07-24 09:04:23+00:00
- **Updated**: 2019-07-24 09:04:23+00:00
- **Authors**: Haichao Zhang, Jianyu Wang
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Object detection is an important vision task and has emerged as an indispensable component in many vision system, rendering its robustness as an increasingly important performance factor for practical applications. While object detection models have been demonstrated to be vulnerable against adversarial attacks by many recent works, very few efforts have been devoted to improving their robustness. In this work, we take an initial attempt towards this direction. We first revisit and systematically analyze object detectors and many recently developed attacks from the perspective of model robustness. We then present a multi-task learning perspective of object detection and identify an asymmetric role of task losses. We further develop an adversarial training approach which can leverage the multiple sources of attacks for improving the robustness of detection models. Extensive experiments on PASCAL-VOC and MS-COCO verified the effectiveness of the proposed approach.



### From Big to Small: Multi-Scale Local Planar Guidance for Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1907.10326v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10326v6)
- **Published**: 2019-07-24 09:31:24+00:00
- **Updated**: 2021-09-23 10:23:51+00:00
- **Authors**: Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, Il Hong Suh
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating accurate depth from a single image is challenging because it is an ill-posed problem as infinitely many 3D scenes can be projected to the same 2D scene. However, recent works based on deep convolutional neural networks show great progress with plausible results. The convolutional neural networks are generally composed of two parts: an encoder for dense feature extraction and a decoder for predicting the desired depth. In the encoder-decoder schemes, repeated strided convolution and spatial pooling layers lower the spatial resolution of transitional outputs, and several techniques such as skip connections or multi-layer deconvolutional networks are adopted to recover the original resolution for effective dense prediction. In this paper, for more effective guidance of densely encoded features to the desired depth prediction, we propose a network architecture that utilizes novel local planar guidance layers located at multiple stages in the decoding phase. We show that the proposed method outperforms the state-of-the-art works with significant margin evaluating on challenging benchmarks. We also provide results from an ablation study to validate the effectiveness of the proposed method.



### Multi-adversarial Faster-RCNN for Unrestricted Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1907.10343v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10343v2)
- **Published**: 2019-07-24 10:12:36+00:00
- **Updated**: 2019-09-07 02:08:35+00:00
- **Authors**: Zhenwei He, Lei Zhang
- **Comment**: This paper is accepted in ICCV2019
- **Journal**: None
- **Summary**: Conventional object detection methods essentially suppose that the training and testing data are collected from a restricted target domain with expensive labeling cost. For alleviating the problem of domain dependency and cumbersome labeling, this paper proposes to detect objects in an unrestricted environment by leveraging domain knowledge trained from an auxiliary source domain with sufficient labels. Specifically, we propose a multi-adversarial Faster-RCNN (MAF) framework for unrestricted object detection, which inherently addresses domain disparity minimization for domain adaptation in feature representation. The paper merits are in three-fold: 1) With the idea that object detectors often becomes domain incompatible when image distribution resulted domain disparity appears, we propose a hierarchical domain feature alignment module, in which multiple adversarial domain classifier submodules for layer-wise domain feature confusion are designed; 2) An information invariant scale reduction module (SRM) for hierarchical feature map resizing is proposed for promoting the training efficiency of adversarial domain adaptation; 3) In order to improve the domain adaptability, the aggregated proposal features with detection results are feed into a proposed weighted gradient reversal layer (WGRL) for characterizing hard confused domain samples. We evaluate our MAF on unrestricted tasks, including Cityscapes, KITTI, Sim10k, etc. and the experiments show the state-of-the-art performance over the existing detectors.



### Delving Deep into Liver Focal Lesion Detection: A Preliminary Study
- **Arxiv ID**: http://arxiv.org/abs/1907.10346v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10346v1)
- **Published**: 2019-07-24 10:24:41+00:00
- **Updated**: 2019-07-24 10:24:41+00:00
- **Authors**: Jiechao Ma, Yingqian Chen, Yu Chen, Fengkai Wan, Sumin Xue, Ziping Li, Shiting Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Hepatocellular carcinoma (HCC) is the second most frequent cause of malignancy-related death and is one of the diseases with the highest incidence in the world. Because the liver is the only organ in the human body that is supplied by two major vessels: the hepatic artery and the portal vein, various types of malignant tumors can spread from other organs to the liver. And due to the liver masses' heterogeneous and diffusive shape, the tumor lesions are very difficult to be recognized, thus automatic lesion detection is necessary for the doctors with huge workloads. To assist doctors, this work uses the existing large-scale annotation medical image data to delve deep into liver lesion detection from multiple directions. To solve technical difficulties, such as the image-recognition task, traditional deep learning with convolution neural networks (CNNs) has been widely applied in recent years. However, this kind of neural network, such as Faster Regions with CNN features (R-CNN), cannot leverage the spatial information because it is applied in natural images (2D) rather than medical images (3D), such as computed tomography (CT) images. To address this issue, we propose a novel algorithm that is appropriate for liver CT imaging. Furthermore, according to radiologists' experience in clinical diagnosis and the characteristics of CT images of liver cancer, a liver cancer-detection framework with CNN, including image processing, feature extraction, region proposal, image registration, and classification recognition, was proposed to facilitate the effective detection of liver lesions.



### Computer Aided Detection of Deep Inferior Epigastric Perforators in Computed Tomography Angiography scans
- **Arxiv ID**: http://arxiv.org/abs/1907.10354v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1907.10354v1)
- **Published**: 2019-07-24 10:44:30+00:00
- **Updated**: 2019-07-24 10:44:30+00:00
- **Authors**: Ricardo J. Araújo, Vera Garrido, Catarina A. Baraças, Maria A. Vasconcelos, Carlos Mavioso, João C. Anacleto, Maria J. Cardoso, Hélder P. Oliveira
- **Comment**: 26 pages, 2 tables, 9 figures, submitted to Computerized Medical
  Imaging and Graphics journal
- **Journal**: None
- **Summary**: The deep inferior epigastric artery perforator (DIEAP) flap is the most common free flap used for breast reconstruction after a mastectomy. It makes use of the skin and fat of the lower abdomen to build a new breast mound either at the same time of the mastectomy or in a second surgery. This operation requires preoperative imaging studies to evaluate the branches - the perforators - that irrigate the tissue that will be used to reconstruct the breast mound. These branches will support tissue viability after the microsurgical ligation of the inferior epigastric vessels to the receptor vessels in the thorax. Usually through a Computed Tomography Angiography (CTA), each perforator, diameter and direction is manually identified by the imaging team, who will subsequently draw a map for the identification of the best vascular support for the reconstruction. In the current work we propose a semi-automatic methodology that aims at reducing the time and subjectivity inherent to the manual annotation. In 21 CTAs from patients proposed for breast reconstruction with DIEAP flaps, the subcutaneous region of each perforator was extracted, by means of a tracking procedure, whereas the intramuscular portion was detected through a minimum cost approach. Both were subsequently compared with the radiologist manual annotation. Results showed that the semi-automatic procedure was able to correctly detect the course of the DIEAPs with a minimum error (average error of 0.64 mm and 0.50 mm regarding the extraction of subcutaneous and intramuscular paths, respectively). The objective methodology is a promising tool in the automatic detection of perforators in CTA and can contribute to spare human resources and reduce subjectivity in the aforementioned task.



### DispVoxNets: Non-Rigid Point Set Alignment with Supervised Learning Proxies
- **Arxiv ID**: http://arxiv.org/abs/1907.10367v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/1907.10367v2)
- **Published**: 2019-07-24 11:14:10+00:00
- **Updated**: 2019-08-06 07:11:55+00:00
- **Authors**: Soshi Shimada, Vladislav Golyanik, Edgar Tretschk, Didier Stricker, Christian Theobalt
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a supervised-learning framework for non-rigid point set alignment of a new kind - Displacements on Voxels Networks (DispVoxNets) - which abstracts away from the point set representation and regresses 3D displacement fields on regularly sampled proxy 3D voxel grids. Thanks to recently released collections of deformable objects with known intra-state correspondences, DispVoxNets learn a deformation model and further priors (e.g., weak point topology preservation) for different object categories such as cloths, human bodies and faces. DispVoxNets cope with large deformations, noise and clustered outliers more robustly than the state-of-the-art. At test time, our approach runs orders of magnitude faster than previous techniques. All properties of DispVoxNets are ascertained numerically and qualitatively in extensive experiments and comparisons to several previous methods.



### Self-attention based BiLSTM-CNN classifier for the prediction of ischemic and non-ischemic cardiomyopathy
- **Arxiv ID**: http://arxiv.org/abs/1907.10370v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.10370v3)
- **Published**: 2019-07-24 11:34:44+00:00
- **Updated**: 2019-07-29 06:46:33+00:00
- **Authors**: Kavita Dubey, Anant Agarwal, Astitwa Sarthak Lathe, Ranjeet Kumar, Vishal Srivastava
- **Comment**: None
- **Journal**: None
- **Summary**: Heart Failure is a major component of healthcare expenditure and a leading cause of mortality worldwide. Despite higher inter-rater variability, endomyocardial biopsy (EMB) is still regarded as the standard technique, used to identify the cause (e.g. ischemic or non-ischemic cardiomyopathy, coronary artery disease, myocardial infarction etc.) of unexplained heart failure. In this paper, we focus on identifying cardiomyopathy as ischemic or non-ischemic. For this, we propose and implement a new unified architecture comprising CNN (inception-V3 model) and bidirectional LSTM (BiLSTM) with self-attention mechanism to predict the ischemic or non-ischemic to classify cardiomyopathy using histopathological images. The proposed model is based on self-attention that implicitly focuses on the information outputted from the hidden layers of BiLSTM. Through our results we demonstrate that this framework carries a high learning capacity and is able to improve the classification performance.



### Higher-Order Function Networks for Learning Composable 3D Object Representations
- **Arxiv ID**: http://arxiv.org/abs/1907.10388v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.10388v2)
- **Published**: 2019-07-24 12:31:16+00:00
- **Updated**: 2020-04-06 05:18:09+00:00
- **Authors**: Eric Mitchell, Selim Engin, Volkan Isler, Daniel D Lee
- **Comment**: To be published in International Conference on Learning
  Representations (ICLR 2020) [https://openreview.net/forum?id=HJgfDREKDB]; 19
  pages
- **Journal**: None
- **Summary**: We present a new approach to 3D object representation where a neural network encodes the geometry of an object directly into the weights and biases of a second 'mapping' network. This mapping network can be used to reconstruct an object by applying its encoded transformation to points randomly sampled from a simple geometric space, such as the unit sphere. We study the effectiveness of our method through various experiments on subsets of the ShapeNet dataset. We find that the proposed approach can reconstruct encoded objects with accuracy equal to or exceeding state-of-the-art methods with orders of magnitude fewer parameters. Our smallest mapping network has only about 7000 parameters and shows reconstruction quality on par with state-of-the-art object decoder architectures with millions of parameters. Further experiments on feature mixing through the composition of learned functions show that the encoding captures a meaningful subspace of objects.



### Progressive Perception-Oriented Network for Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1907.10399v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.10399v2)
- **Published**: 2019-07-24 12:43:35+00:00
- **Updated**: 2020-09-11 15:07:03+00:00
- **Authors**: Zheng Hui, Jie Li, Xinbo Gao, Xiumei Wang
- **Comment**: Information Sciences 2020
- **Journal**: None
- **Summary**: Recently, it has been demonstrated that deep neural networks can significantly improve the performance of single image super-resolution (SISR). Numerous studies have concentrated on raising the quantitative quality of super-resolved (SR) images. However, these methods that target PSNR maximization usually produce blurred images at large upscaling factor. The introduction of generative adversarial networks (GANs) can mitigate this issue and show impressive results with synthetic high-frequency textures. Nevertheless, these GAN-based approaches always have a tendency to add fake textures and even artifacts to make the SR image of visually higher-resolution. In this paper, we propose a novel perceptual image super-resolution method that progressively generates visually high-quality results by constructing a stage-wise network. Specifically, the first phase concentrates on minimizing pixel-wise error, and the second stage utilizes the features extracted by the previous stage to pursue results with better structural retention. The final stage employs fine structure features distilled by the second phase to produce more realistic results. In this way, we can maintain the pixel, and structural level information in the perceptual image as much as possible. It is useful to note that the proposed method can build three types of images in a feed-forward process. Also, we explore a new generator that adopts multi-scale hierarchical features fusion. Extensive experiments on benchmark datasets show that our approach is superior to the state-of-the-art methods. Code is available at https://github.com/Zheng222/PPON.



### Investigating Correlations of Inter-coder Agreement and Machine Annotation Performance for Historical Video Data
- **Arxiv ID**: http://arxiv.org/abs/1907.10450v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DL, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1907.10450v1)
- **Published**: 2019-07-24 13:50:20+00:00
- **Updated**: 2019-07-24 13:50:20+00:00
- **Authors**: Kader Pustu-Iren, Markus Mühling, Nikolaus Korfhage, Joanna Bars, Sabrina Bernhöft, Angelika Hörth, Bernd Freisleben, Ralph Ewerth
- **Comment**: None
- **Journal**: None
- **Summary**: Video indexing approaches such as visual concept classification and person recognition are essential to enable fine-grained semantic search in large-scale video archives such as the historical video collection of former German Democratic Republic (GDR) maintained by the German Broadcasting Archive (DRA). Typically, a lexicon of visual concepts has to be defined for semantic search. However, the definition of visual concepts can be more or less subjective due to individually differing judgments of annotators, which may have an impact on annotation quality and subsequently training of supervised machine learning methods. In this paper, we analyze the inter-coder agreement for historical TV data of the former GDR for visual concept classification and person recognition. The inter-coder agreement is evaluated for a group of expert as well as non-expert annotators in order to determine differences in annotation homogeneity. Furthermore, correlations between visual recognition performance and inter-annotator agreement are measured. In this context, information about image quantity and agreement are used to predict average precision for concept classification. Finally, the influence of expert vs. non-expert annotations acquired in the study are used to evaluate person recognition.



### Dense Feature Aggregation and Pruning for RGBT Tracking
- **Arxiv ID**: http://arxiv.org/abs/1907.10451v1
- **DOI**: 10.1145/3343031.3350928
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10451v1)
- **Published**: 2019-07-24 13:51:24+00:00
- **Updated**: 2019-07-24 13:51:24+00:00
- **Authors**: Yabin Zhu, Chenglong Li, Bin Luo, Jin Tang, Xiao Wang
- **Comment**: arXiv admin note: text overlap with arXiv:1811.09855
- **Journal**: ACM International Conference on Multimedia.2019
- **Summary**: How to perform effective information fusion of different modalities is a core factor in boosting the performance of RGBT tracking. This paper presents a novel deep fusion algorithm based on the representations from an end-to-end trained convolutional neural network. To deploy the complementarity of features of all layers, we propose a recursive strategy to densely aggregate these features that yield robust representations of target objects in each modality. In different modalities, we propose to prune the densely aggregated features of all modalities in a collaborative way. In a specific, we employ the operations of global average pooling and weighted random selection to perform channel scoring and selection, which could remove redundant and noisy features to achieve more robust feature representation. Experimental results on two RGBT tracking benchmark datasets suggest that our tracker achieves clear state-of-the-art against other RGB and RGBT tracking methods.



### Understanding Adversarial Attacks on Deep Learning Based Medical Image Analysis Systems
- **Arxiv ID**: http://arxiv.org/abs/1907.10456v2
- **DOI**: 10.1016/j.patcog.2020.107332
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.4.9; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1907.10456v2)
- **Published**: 2019-07-24 14:04:13+00:00
- **Updated**: 2020-03-13 15:57:31+00:00
- **Authors**: Xingjun Ma, Yuhao Niu, Lin Gu, Yisen Wang, Yitian Zhao, James Bailey, Feng Lu
- **Comment**: 15 pages, 11 figures, to appear in Pattern Recognition
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have become popular for medical image analysis tasks like cancer diagnosis and lesion detection. However, a recent study demonstrates that medical deep learning systems can be compromised by carefully-engineered adversarial examples/attacks with small imperceptible perturbations. This raises safety concerns about the deployment of these systems in clinical settings. In this paper, we provide a deeper understanding of adversarial examples in the context of medical images. We find that medical DNN models can be more vulnerable to adversarial attacks compared to models for natural images, according to two different viewpoints. Surprisingly, we also find that medical adversarial attacks can be easily detected, i.e., simple detectors can achieve over 98% detection AUC against state-of-the-art attacks, due to fundamental feature differences compared to normal examples. We believe these findings may be a useful basis to approach the design of more explainable and secure medical deep learning systems.



### Multi-task Localization and Segmentation for X-ray Guided Planning in Knee Surgery
- **Arxiv ID**: http://arxiv.org/abs/1907.10465v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.10465v1)
- **Published**: 2019-07-24 14:32:10+00:00
- **Updated**: 2019-07-24 14:32:10+00:00
- **Authors**: Florian Kordon, Peter Fischer, Maxim Privalov, Benedict Swartman, Marc Schnetzke, Jochen Franke, Ruxandra Lasowski, Andreas Maier, Holger Kunze
- **Comment**: Accepted for MICCAI 2019
- **Journal**: None
- **Summary**: X-ray based measurement and guidance are commonly used tools in orthopaedic surgery to facilitate a minimally invasive workflow. Typically, a surgical planning is first performed using knowledge of bone morphology and anatomical landmarks. Information about bone location then serves as a prior for registration during overlay of the planning on intra-operative X-ray images. Performing these steps manually however is prone to intra-rater/inter-rater variability and increases task complexity for the surgeon. To remedy these issues, we propose an automatic framework for planning and subsequent overlay. We evaluate it on the example of femoral drill site planning for medial patellofemoral ligament reconstruction surgery. A deep multi-task stacked hourglass network is trained on 149 conventional lateral X-ray images to jointly localize two femoral landmarks, to predict a region of interest for the posterior femoral cortex tangent line, and to perform semantic segmentation of the femur, patella, tibia, and fibula with adaptive task complexity weighting. On 38 clinical test images the framework achieves a median localization error of 1.50 mm for the femoral drill site and mean IOU scores of 0.99, 0.97, 0.98, and 0.96 for the femur, patella, tibia, and fibula respectively. The demonstrated approach consistently performs surgical planning at expert-level precision without the need for manual correction.



### Recurrent Aggregation Learning for Multi-View Echocardiographic Sequences Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.11292v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.11292v1)
- **Published**: 2019-07-24 14:43:18+00:00
- **Updated**: 2019-07-24 14:43:18+00:00
- **Authors**: Ming Li, Weiwei Zhang, Guang Yang, Chengjia Wang, Heye Zhang, Huafeng Liu, Wei Zheng, Shuo Li
- **Comment**: MICCAI 2019
- **Journal**: None
- **Summary**: Multi-view echocardiographic sequences segmentation is crucial for clinical diagnosis. However, this task is challenging due to limited labeled data, huge noise, and large gaps across views. Here we propose a recurrent aggregation learning method to tackle this challenging task. By pyramid ConvBlocks, multi-level and multi-scale features are extracted efficiently. Hierarchical ConvLSTMs next fuse these features and capture spatial-temporal information in multi-level and multi-scale space. We further introduce a double-branch aggregation mechanism for segmentation and classification which are mutually promoted by deep aggregation of multi-level and multi-scale features. The segmentation branch provides information to guide the classification while the classification branch affords multi-view regularization to refine segmentations and further lessen gaps across views. Our method is built as an end-to-end framework for segmentation and classification. Adequate experiments on our multi-view dataset (9000 labeled images) and the CAMUS dataset (1800 labeled images) corroborate that our method achieves not only superior segmentation and classification accuracy but also prominent temporal stability.



### A Convolutional Forward and Back-Projection Model for Fan-Beam Geometry
- **Arxiv ID**: http://arxiv.org/abs/1907.10526v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1907.10526v1)
- **Published**: 2019-07-24 15:39:41+00:00
- **Updated**: 2019-07-24 15:39:41+00:00
- **Authors**: Kai Zhang, Alireza Entezari
- **Comment**: This paper was submitted to IEEE-TMI, and it's an extension of our
  ISBI paper (https://ieeexplore.ieee.org/abstract/document/8759285)
- **Journal**: None
- **Summary**: Iterative methods for tomographic image reconstruction have great potential for enabling high quality imaging from low-dose projection data. The computational burden of iterative reconstruction algorithms, however, has been an impediment in their adoption in practical CT reconstruction problems. We present an approach for highly efficient and accurate computation of forward model for image reconstruction in fan-beam geometry in X-ray CT. The efficiency of computations makes this approach suitable for large-scale optimization algorithms with on-the-fly, memory-less, computations of the forward and back-projection. Our experiments demonstrate the improvements in accuracy as well as efficiency of our model, specifically for first-order box splines (i.e., pixel-basis) compared to recently developed methods for this purpose, namely Look-up Table-based Ray Integration (LTRI) and Separable Footprints (SF) in 2-D.



### CvxPnPL: A Unified Convex Solution to the Absolute Pose Estimation Problem from Point and Line Correspondences
- **Arxiv ID**: http://arxiv.org/abs/1907.10545v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1907.10545v2)
- **Published**: 2019-07-24 16:28:52+00:00
- **Updated**: 2019-08-09 07:42:09+00:00
- **Authors**: Sérgio Agostinho, João Gomes, Alessio Del Bue
- **Comment**: Main paper and supplemental material included. References added and
  minor change to fig 1
- **Journal**: None
- **Summary**: We present a new convex method to estimate 3D pose from mixed combinations of 2D-3D point and line correspondences, the Perspective-n-Points-and-Lines problem (PnPL). We merge the contributions of each point and line into a unified Quadratic Constrained Quadratic Problem (QCQP) and then relax it into a Semi Definite Program (SDP) through Shor's relaxation. This makes it possible to gracefully handle mixed configurations of points and lines. Furthermore, the proposed relaxation allows us to recover a finite number of solutions under ambiguous configurations. In such cases, the 3D pose candidates are found by further enforcing geometric constraints on the solution space and then retrieving such poses from the intersections of multiple quadrics. Experiments provide results in line with the best performing state of the art methods while providing the flexibility of solving for an arbitrary number of points and lines.



### Maximum likelihood estimation for disk image parameters
- **Arxiv ID**: http://arxiv.org/abs/1907.10557v2
- **DOI**: 10.1109/LSP.2020.3017371
- **Categories**: **eess.IV**, astro-ph.IM, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.10557v2)
- **Published**: 2019-07-24 16:55:29+00:00
- **Updated**: 2020-03-18 10:02:54+00:00
- **Authors**: Matwey V. Kornilov
- **Comment**: 13 pages, 4 figures. in IEEE Signal Processing Letters
- **Journal**: None
- **Summary**: We present a novel technique for estimating disk parameters (the centre and the radius) from its 2D image. It is based on the maximal likelihood approach utilising both edge pixels coordinates and the image intensity gradients. We emphasise the following advantages of our likelihood model. It has closed-form formulae for parameter estimating, requiring less computational resources than iterative algorithms therefore. The likelihood model naturally distinguishes the outer and inner annulus edges. The proposed technique was evaluated on both synthetic and real data.



### QRMODA and BRMODA: Novel Models for Face Recognition Accuracy in Computer Vision Systems with Adapted Video Streams
- **Arxiv ID**: http://arxiv.org/abs/1907.10559v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1907.10559v1)
- **Published**: 2019-07-24 16:59:56+00:00
- **Updated**: 2019-07-24 16:59:56+00:00
- **Authors**: Hayder Hamandi, Nabil Sarhan
- **Comment**: None
- **Journal**: None
- **Summary**: A major challenge facing Computer Vision systems is providing the ability to accurately detect threats and recognize subjects and/or objects under dynamically changing network conditions. We propose two novel models that characterize the face recognition accuracy in terms of video encoding parameters. Specifically, we model the accuracy in terms of video resolution, quantization, and actual bit rate. We validate the models using two distinct video datasets and a large image dataset by conducting 1, 668 experiments that involve simultaneously varying combinations of encoding parameters. We show that both models hold true for the deep learning and statistical based face recognition. Furthermore, we show that the models can be used to capture different accuracy metrics, specifically the recall, precision, and F1-score. Ultimately, we provide meaningful insights on the factors affecting the constants of each proposed model.



### Distilled Siamese Networks for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1907.10586v3
- **DOI**: 10.1109/TPAMI.2021.3127492
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10586v3)
- **Published**: 2019-07-24 17:46:44+00:00
- **Updated**: 2022-11-28 20:04:38+00:00
- **Authors**: Jianbing Shen, Yuanpei Liu, Xingping Dong, Xiankai Lu, Fahad Shahbaz Khan, Steven Hoi
- **Comment**: Accepted by IEEE T-PAMI
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  44(12), pp.8896-8909, 1 Dec. 2022
- **Summary**: In recent years, Siamese network based trackers have significantly advanced the state-of-the-art in real-time tracking. Despite their success, Siamese trackers tend to suffer from high memory costs, which restrict their applicability to mobile devices with tight memory budgets. To address this issue, we propose a distilled Siamese tracking framework to learn small, fast and accurate trackers (students), which capture critical knowledge from large Siamese trackers (teachers) by a teacher-students knowledge distillation model. This model is intuitively inspired by the one teacher vs. multiple students learning method typically employed in schools. In particular, our model contains a single teacher-student distillation module and a student-student knowledge sharing mechanism. The former is designed using a tracking-specific distillation strategy to transfer knowledge from a teacher to students. The latter is utilized for mutual learning between students to enable in-depth knowledge understanding. Extensive empirical evaluations on several popular Siamese trackers demonstrate the generality and effectiveness of our framework. Moreover, the results on five tracking benchmarks show that the proposed distilled trackers achieve compression rates of up to 18$\times$ and frame-rates of $265$ FPS, while obtaining comparable tracking accuracy compared to base models.



### Curriculum based Dropout Discriminator for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1907.10628v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.10628v2)
- **Published**: 2019-07-24 18:00:12+00:00
- **Updated**: 2019-10-19 19:43:26+00:00
- **Authors**: Vinod Kumar Kurmi, Vipul Bajaj, Venkatesh K Subramanian, Vinay P Namboodiri
- **Comment**: BMVC 2019 Accepted, Project Page:
  https://delta-lab-iitk.github.io/CD3A/
- **Journal**: None
- **Summary**: Domain adaptation is essential to enable wide usage of deep learning based networks trained using large labeled datasets. Adversarial learning based techniques have shown their utility towards solving this problem using a discriminator that ensures source and target distributions are close. However, here we suggest that rather than using a point estimate, it would be useful if a distribution based discriminator could be used to bridge this gap. This could be achieved using multiple classifiers or using traditional ensemble methods. In contrast, we suggest that a Monte Carlo dropout based ensemble discriminator could suffice to obtain the distribution based discriminator. Specifically, we propose a curriculum based dropout discriminator that gradually increases the variance of the sample based distribution and the corresponding reverse gradients are used to align the source and target feature representations. The detailed results and thorough ablation analysis show that our model outperforms state-of-the-art results.



### Warp and Learn: Novel Views Generation for Vehicles and Other Objects
- **Arxiv ID**: http://arxiv.org/abs/1907.10634v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.10634v3)
- **Published**: 2019-07-24 18:01:51+00:00
- **Updated**: 2020-10-10 15:21:58+00:00
- **Authors**: Andrea Palazzi, Luca Bergamini, Simone Calderara, Rita Cucchiara
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence
- **Journal**: None
- **Summary**: In this work we introduce a new self-supervised, semi-parametric approach for synthesizing novel views of a vehicle starting from a single monocular image. Differently from parametric (i.e. entirely learning-based) methods, we show how a-priori geometric knowledge about the object and the 3D world can be successfully integrated into a deep learning based image generation framework. As this geometric component is not learnt, we call our approach semi-parametric. In particular, we exploit man-made object symmetry and piece-wise planarity to integrate rich a-priori visual information into the novel viewpoint synthesis process. An Image Completion Network (ICN) is then trained to generate a realistic image starting from this geometric guidance. This careful blend between parametric and non-parametric components allows us to i) operate in a real-world scenario, ii) preserve high-frequency visual information such as textures, iii) handle truly arbitrary 3D roto-translations of the input and iv) perform shape transfer to completely different 3D models. Eventually, we show that our approach can be easily complemented with synthetic data and extended to other rigid objects with completely different topology, even in presence of concave structures and holes (e.g. chairs). A comprehensive experimental analysis against state-of-the-art competitors shows the efficacy of our method both from a quantitative and a perceptive point of view. Supplementary material, animated results, code and data are available at: https://github.com/ndrplz/semiparametric



### Synthetic Augmentation and Feature-based Filtering for Improved Cervical Histopathology Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1907.10655v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.10655v1)
- **Published**: 2019-07-24 18:54:11+00:00
- **Updated**: 2019-07-24 18:54:11+00:00
- **Authors**: Yuan Xue, Qianying Zhou, Jiarong Ye, L. Rodney Long, Sameer Antani, Carl Cornwell, Zhiyun Xue, Xiaolei Huang
- **Comment**: MICCAI 2019
- **Journal**: None
- **Summary**: Cervical intraepithelial neoplasia (CIN) grade of histopathology images is a crucial indicator in cervical biopsy results. Accurate CIN grading of epithelium regions helps pathologists with precancerous lesion diagnosis and treatment planning. Although an automated CIN grading system has been desired, supervised training of such a system would require a large amount of expert annotations, which are expensive and time-consuming to collect. In this paper, we investigate the CIN grade classification problem on segmented epithelium patches. We propose to use conditional Generative Adversarial Networks (cGANs) to expand the limited training dataset, by synthesizing realistic cervical histopathology images. While the synthetic images are visually appealing, they are not guaranteed to contain meaningful features for data augmentation. To tackle this issue, we propose a synthetic-image filtering mechanism based on the divergence in feature space between generated images and class centroids in order to control the feature quality of selected synthetic images for data augmentation. Our models are evaluated on a cervical histopathology image dataset with a limited number of patch-level CIN grade annotations. Extensive experimental results show a significant improvement of classification accuracy from 66.3% to 71.7% using the same ResNet18 baseline classifier after leveraging our cGAN generated images with feature-based filtering, which demonstrates the effectiveness of our models.



### SDNet: Semantically Guided Depth Estimation Network
- **Arxiv ID**: http://arxiv.org/abs/1907.10659v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10659v1)
- **Published**: 2019-07-24 19:08:31+00:00
- **Updated**: 2019-07-24 19:08:31+00:00
- **Authors**: Matthias Ochs, Adrian Kretz, Rudolf Mester
- **Comment**: Paper is accepted at German Conference on Pattern Recognition (GCPR),
  Dortmund, Germany, September 2019
- **Journal**: None
- **Summary**: Autonomous vehicles and robots require a full scene understanding of the environment to interact with it. Such a perception typically incorporates pixel-wise knowledge of the depths and semantic labels for each image from a video sensor. Recent learning-based methods estimate both types of information independently using two separate CNNs. In this paper, we propose a model that is able to predict both outputs simultaneously, which leads to improved results and even reduced computational costs compared to independent estimation of depth and semantics. We also empirically prove that the CNN is capable of learning more meaningful and semantically richer features. Furthermore, our SDNet estimates the depth based on ordinal classification. On the basis of these two enhancements, our proposed method achieves state-of-the-art results in semantic segmentation and depth estimation from single monocular input images on two challenging datasets.



### Dual Grid Net: hand mesh vertex regression from single depth maps
- **Arxiv ID**: http://arxiv.org/abs/1907.10695v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10695v1)
- **Published**: 2019-07-24 20:07:58+00:00
- **Updated**: 2019-07-24 20:07:58+00:00
- **Authors**: Chengde Wan, Thomas Probst, Luc Van Gool, Angela Yao
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for recovering the dense 3D surface of the hand by regressing the vertex coordinates of a mesh model from a single depth map. To this end, we use a two-stage 2D fully convolutional network architecture. In the first stage, the network estimates a dense correspondence field for every pixel on the depth map or image grid to the mesh grid. In the second stage, we design a differentiable operator to map features learned from the previous stage and regress a 3D coordinate map on the mesh grid. Finally, we sample from the mesh grid to recover the mesh vertices, and fit it an articulated template mesh in closed form. During inference, the network can predict all the mesh vertices, transformation matrices for every joint and the joint coordinates in a single forward pass. When given supervision on the sparse key-point coordinates, our method achieves state-of-the-art accuracy on NYU dataset for key point localization while recovering mesh vertices and a dense correspondence map. Our framework can also be learned through self-supervision by minimizing a set of data fitting and kinematic prior terms. With multi-camera rig during training to resolve self-occlusion, it can perform competitively with strongly supervised methods Without any human annotation.



### Improving Social Awareness Through DANTE: A Deep Affinity Network for Clustering Conversational Interactants
- **Arxiv ID**: http://arxiv.org/abs/1907.12910v4
- **DOI**: 10.1145/3392824
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12910v4)
- **Published**: 2019-07-24 20:09:03+00:00
- **Updated**: 2020-01-15 05:03:36+00:00
- **Authors**: Mason Swofford, John Charles Peruzzi, Nathan Tsoi, Sydney Thompson, Roberto Martín-Martín, Silvio Savarese, Marynel Vázquez
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a data-driven approach to detect conversational groups by identifying spatial arrangements typical of these focused social encounters. Our approach uses a novel Deep Affinity Network (DANTE) to predict the likelihood that two individuals in a scene are part of the same conversational group, considering their social context. The predicted pair-wise affinities are then used in a graph clustering framework to identify both small (e.g., dyads) and large groups. The results from our evaluation on multiple, established benchmarks suggest that combining powerful deep learning methods with classical clustering techniques can improve the detection of conversational groups in comparison to prior approaches. Finally, we demonstrate the practicality of our approach in a human-robot interaction scenario. Our efforts show that our work advances group detection not only in theory, but also in practice.



### Uncalibrated Deflectometry with a Mobile Device on Extended Specular Surfaces
- **Arxiv ID**: http://arxiv.org/abs/1907.10700v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10700v1)
- **Published**: 2019-07-24 20:17:32+00:00
- **Updated**: 2019-07-24 20:17:32+00:00
- **Authors**: Florian Willomitzer, Chia-Kai Yeh, Vikas Gupta, William Spies, Florian Schiffers, Marc Walton, Oliver Cossairt
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a system and methods for the three-dimensional measurement of extended specular surfaces with high surface normal variations. Our system consists only of a mobile hand held device and exploits screen and front camera for Deflectometry-based surface measurements. We demonstrate high quality measurements without the need for an offline calibration procedure. In addition, we develop a multi-view technique to compensate for the small screen of a mobile device so that large surfaces can be densely reconstructed in their entirety. This work is a first step towards developing a self-calibrating Deflectometry procedure capable of taking 3D surface measurements of specular objects in the wild and accessible to users with little to no technical imaging experience.



### RNN-based Online Handwritten Character Recognition Using Accelerometer and Gyroscope Data
- **Arxiv ID**: http://arxiv.org/abs/1907.12935v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.12935v1)
- **Published**: 2019-07-24 20:44:00+00:00
- **Updated**: 2019-07-24 20:44:00+00:00
- **Authors**: Davit Soselia, Shota Amashukeli, Irakli Koberidze, Levan Shugliashvili
- **Comment**: None
- **Journal**: None
- **Summary**: This abstract explores an RNN-based approach to online handwritten recognition problem. Our method uses data from an accelerometer and a gyroscope mounted on a handheld pen-like device to train and run a character pre-diction model. We have built a dataset of timestamped gyroscope and accelerometer data gathered during the manual process of handwriting Latin characters, labeled with the character being written; in total, the dataset con-sists of 1500 gyroscope and accelerometer data sequenc-es for 8 characters of the Latin alphabet from 6 different people, and 20 characters, each 1500 samples from Georgian alphabet from 5 different people. with each sequence containing the gyroscope and accelerometer data captured during the writing of a particular character sampled once every 10ms. We train an RNN-based neural network architecture on this dataset to predict the character being written. The model is optimized with categorical cross-entropy loss and RMSprop optimizer and achieves high accuracy on test data.



### LayoutVAE: Stochastic Scene Layout Generation From a Label Set
- **Arxiv ID**: http://arxiv.org/abs/1907.10719v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10719v3)
- **Published**: 2019-07-24 20:53:55+00:00
- **Updated**: 2021-06-01 06:25:20+00:00
- **Authors**: Akash Abdu Jyothi, Thibaut Durand, Jiawei He, Leonid Sigal, Greg Mori
- **Comment**: 20 pages, 24 figures, accepted in ICCV 2019
- **Journal**: None
- **Summary**: Recently there is an increasing interest in scene generation within the research community. However, models used for generating scene layouts from textual description largely ignore plausible visual variations within the structure dictated by the text. We propose LayoutVAE, a variational autoencoder based framework for generating stochastic scene layouts. LayoutVAE is a versatile modeling framework that allows for generating full image layouts given a label set, or per label layouts for an existing image given a new label. In addition, it is also capable of detecting unusual layouts, potentially providing a way to evaluate layout generation problem. Extensive experiments on MNIST-Layouts and challenging COCO 2017 Panoptic dataset verifies the effectiveness of our proposed framework.



### Joint Adversarial Training: Incorporating both Spatial and Pixel Attacks
- **Arxiv ID**: http://arxiv.org/abs/1907.10737v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.10737v2)
- **Published**: 2019-07-24 21:36:27+00:00
- **Updated**: 2019-07-31 05:28:27+00:00
- **Authors**: Haichao Zhang, Jianyu Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Conventional adversarial training methods using attacks that manipulate the pixel value directly and individually, leading to models that are less robust in face of spatial transformation-based attacks. In this paper, we propose a joint adversarial training method that incorporates both spatial transformation-based and pixel-value based attacks for improving model robustness. We introduce a spatial transformation-based attack with an explicit notion of budget and develop an algorithm for spatial attack generation. We further integrate both pixel and spatial attacks into one generation model and show how to leverage the complementary strengths of each other in training for improving the overall model robustness. Extensive experimental results on different benchmark datasets compared with state-of-the-art methods verified the effectiveness of the proposed method.



### One-stage Shape Instantiation from a Single 2D Image to 3D Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/1907.10763v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.10763v1)
- **Published**: 2019-07-24 22:41:46+00:00
- **Updated**: 2019-07-24 22:41:46+00:00
- **Authors**: Xiao-Yun Zhou, Zhao-Yang Wang, Peichao Li, Jian-Qing Zheng, Guang-Zhong Yang
- **Comment**: 8.5 pages, 5 figures, MICCAI 2019
- **Journal**: None
- **Summary**: Shape instantiation which predicts the 3D shape of a dynamic target from one or more 2D images is important for real-time intra-operative navigation. Previously, a general shape instantiation framework was proposed with manual image segmentation to generate a 2D Statistical Shape Model (SSM) and with Kernel Partial Least Square Regression (KPLSR) to learn the relationship between the 2D and 3D SSM for 3D shape prediction. In this paper, the two-stage shape instantiation is improved to be one-stage. PointOutNet with 19 convolutional layers and three fully-connected layers is used as the network structure and Chamfer distance is used as the loss function to predict the 3D target point cloud from a single 2D image. With the proposed one-stage shape instantiation algorithm, a spontaneous image-to-point cloud training and inference can be achieved. A dataset from 27 Right Ventricle (RV) subjects, indicating 609 experiments, were used to validate the proposed one-stage shape instantiation algorithm. An average point cloud-to-point cloud (PC-to-PC) error of 1.72mm has been achieved, which is comparable to the PLSR-based (1.42mm) and KPLSR-based (1.31mm) two-stage shape instantiation algorithm.



### Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1907.10764v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.10764v4)
- **Published**: 2019-07-24 22:43:55+00:00
- **Updated**: 2019-11-21 21:51:21+00:00
- **Authors**: Haichao Zhang, Jianyu Wang
- **Comment**: Published at NeurIPS 2019
- **Journal**: None
- **Summary**: We introduce a feature scattering-based adversarial training approach for improving model robustness against adversarial attacks. Conventional adversarial training approaches leverage a supervised scheme (either targeted or non-targeted) in generating attacks for training, which typically suffer from issues such as label leaking as noted in recent works. Differently, the proposed approach generates adversarial images for training through feature scattering in the latent space, which is unsupervised in nature and avoids label leaking. More importantly, this new approach generates perturbed images in a collaborative fashion, taking the inter-sample relationships into consideration. We conduct analysis on model robustness and demonstrate the effectiveness of the proposed approach through extensively experiments on different datasets compared with state-of-the-art approaches.



