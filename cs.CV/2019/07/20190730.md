# Arxiv Papers in cs.CV on 2019-07-30
### Deep Non-Rigid Structure from Motion
- **Arxiv ID**: http://arxiv.org/abs/1908.00052v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.00052v2)
- **Published**: 2019-07-30 00:26:07+00:00
- **Updated**: 2019-08-11 19:07:59+00:00
- **Authors**: Chen Kong, Simon Lucey
- **Comment**: Oral Paper in ICCV 2019. arXiv admin note: substantial text overlap
  with arXiv:1902.10840, arXiv:1907.13123
- **Journal**: None
- **Summary**: Current non-rigid structure from motion (NRSfM) algorithms are mainly limited with respect to: (i) the number of images, and (ii) the type of shape variability they can handle. This has hampered the practical utility of NRSfM for many applications within vision. In this paper we propose a novel deep neural network to recover camera poses and 3D points solely from an ensemble of 2D image coordinates. The proposed neural network is mathematically interpretable as a multi-layer block sparse dictionary learning problem, and can handle problems of unprecedented scale and shape complexity. Extensive experiments demonstrate the impressive performance of our approach where we exhibit superior precision and robustness against all available state-of-the-art works in the order of magnitude. We further propose a quality measure (based on the network weights) which circumvents the need for 3D ground-truth to ascertain the confidence we have in the reconstruction.



### Deep Non-Rigid Structure from Motion with Missing Data
- **Arxiv ID**: http://arxiv.org/abs/1907.13123v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.13123v2)
- **Published**: 2019-07-30 00:35:15+00:00
- **Updated**: 2019-09-06 18:46:07+00:00
- **Authors**: Chen Kong, Simon Lucey
- **Comment**: Submission to PAMI
- **Journal**: None
- **Summary**: Non-Rigid Structure from Motion (NRSfM) refers to the problem of reconstructing cameras and the 3D point cloud of a non-rigid object from an ensemble of images with 2D correspondences. Current NRSfM algorithms are limited from two perspectives: (i) the number of images, and (ii) the type of shape variability they can handle. These difficulties stem from the inherent conflict between the condition of the system and the degrees of freedom needing to be modeled -- which has hampered its practical utility for many applications within vision. In this paper we propose a novel hierarchical sparse coding model for NRSFM which can overcome (i) and (ii) to such an extent, that NRSFM can be applied to problems in vision previously thought too ill posed. Our approach is realized in practice as the training of an unsupervised deep neural network (DNN) auto-encoder with a unique architecture that is able to disentangle pose from 3D structure. Using modern deep learning computational platforms allows us to solve NRSfM problems at an unprecedented scale and shape complexity. Our approach has no 3D supervision, relying solely on 2D point correspondences. Further, our approach is also able to handle missing/occluded 2D points without the need for matrix completion. Extensive experiments demonstrate the impressive performance of our approach where we exhibit superior precision and robustness against all available state-of-the-art works in some instances by an order of magnitude. We further propose a new quality measure (based on the network weights) which circumvents the need for 3D ground-truth to ascertain the confidence we have in the reconstructability. We believe our work to be a significant advance over state-of-the-art in NRSFM.



### Multi-Angle Point Cloud-VAE: Unsupervised Feature Learning for 3D Point Clouds from Multiple Angles by Joint Self-Reconstruction and Half-to-Half Prediction
- **Arxiv ID**: http://arxiv.org/abs/1907.12704v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12704v1)
- **Published**: 2019-07-30 02:17:12+00:00
- **Updated**: 2019-07-30 02:17:12+00:00
- **Authors**: Zhizhong Han, Xiyang Wang, Yu-Shen Liu, Matthias Zwicker
- **Comment**: To appear at ICCV 2019
- **Journal**: None
- **Summary**: Unsupervised feature learning for point clouds has been vital for large-scale point cloud understanding. Recent deep learning based methods depend on learning global geometry from self-reconstruction. However, these methods are still suffering from ineffective learning of local geometry, which significantly limits the discriminability of learned features. To resolve this issue, we propose MAP-VAE to enable the learning of global and local geometry by jointly leveraging global and local self-supervision. To enable effective local self-supervision, we introduce multi-angle analysis for point clouds. In a multi-angle scenario, we first split a point cloud into a front half and a back half from each angle, and then, train MAP-VAE to learn to predict a back half sequence from the corresponding front half sequence. MAP-VAE performs this half-to-half prediction using RNN to simultaneously learn each local geometry and the spatial relationship among them. In addition, MAP-VAE also learns global geometry via self-reconstruction, where we employ a variational constraint to facilitate novel shape generation. The outperforming results in four shape analysis tasks show that MAP-VAE can learn more discriminative global or local features than the state-of-the-art methods.



### Exploring large scale public medical image datasets
- **Arxiv ID**: http://arxiv.org/abs/1907.12720v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.12720v1)
- **Published**: 2019-07-30 03:09:27+00:00
- **Updated**: 2019-07-30 03:09:27+00:00
- **Authors**: Luke Oakden-Rayner
- **Comment**: 9 pages, 5 tables
- **Journal**: None
- **Summary**: Rationale and Objectives: Medical artificial intelligence systems are dependent on well characterised large scale datasets. Recently released public datasets have been of great interest to the field, but pose specific challenges due to the disconnect they cause between data generation and data usage, potentially limiting the utility of these datasets.   Materials and Methods: We visually explore two large public datasets, to determine how accurate the provided labels are and whether other subtle problems exist. The ChestXray14 dataset contains 112,120 frontal chest films, and the MURA dataset contains 40,561 upper limb radiographs. A subset of around 700 images from both datasets was reviewed by a board-certified radiologist, and the quality of the original labels was determined.   Results: The ChestXray14 labels did not accurately reflect the visual content of the images, with positive predictive values mostly between 10% and 30% lower than the values presented in the original documentation. There were other significant problems, with examples of hidden stratification and label disambiguation failure. The MURA labels were more accurate, but the original normal/abnormal labels were inaccurate for the subset of cases with degenerative joint disease, with a sensitivity of 60% and a specificity of 82%.   Conclusion: Visual inspection of images is a necessary component of understanding large image datasets. We recommend that teams producing public datasets should perform this important quality control procedure and include a thorough description of their findings, along with an explanation of the data generating procedures and labelling rules, in the documentation for their datasets.



### Confounder-Aware Visualization of ConvNets
- **Arxiv ID**: http://arxiv.org/abs/1907.12727v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.12727v2)
- **Published**: 2019-07-30 03:54:08+00:00
- **Updated**: 2021-06-26 01:06:42+00:00
- **Authors**: Qingyu Zhao, Ehsan Adeli, Adolf Pfefferbaum, Edith V. Sullivan, Kilian M. Pohl
- **Comment**: None
- **Journal**: None
- **Summary**: With recent advances in deep learning, neuroimaging studies increasingly rely on convolutional networks (ConvNets) to predict diagnosis based on MR images. To gain a better understanding of how a disease impacts the brain, the studies visualize the salience maps of the ConvNet highlighting voxels within the brain majorly contributing to the prediction. However, these salience maps are generally confounded, i.e., some salient regions are more predictive of confounding variables (such as age) than the diagnosis. To avoid such misinterpretation, we propose in this paper an approach that aims to visualize confounder-free saliency maps that only highlight voxels predictive of the diagnosis. The approach incorporates univariate statistical tests to identify confounding effects within the intermediate features learned by ConvNet. The influence from the subset of confounded features is then removed by a novel partial back-propagation procedure. We use this two-step approach to visualize confounder-free saliency maps extracted from synthetic and two real datasets. These experiments reveal the potential of our visualization in producing unbiased model-interpretation.



### Propose-and-Attend Single Shot Detector
- **Arxiv ID**: http://arxiv.org/abs/1907.12736v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.12736v1)
- **Published**: 2019-07-30 04:56:25+00:00
- **Updated**: 2019-07-30 04:56:25+00:00
- **Authors**: Ho-Deok Jang, Sanghyun Woo, Philipp Benz, Jinsun Park, In So Kweon
- **Comment**: 8 pages, 2 figures, 7 tables
- **Journal**: None
- **Summary**: We present a simple yet effective prediction module for a one-stage detector. The main process is conducted in a coarse-to-fine manner. First, the module roughly adjusts the default boxes to well capture the extent of target objects in an image. Second, given the adjusted boxes, the module aligns the receptive field of the convolution filters accordingly, not requiring any embedding layers. Both steps build a propose-and-attend mechanism, mimicking two-stage detectors in a highly efficient manner. To verify its effectiveness, we apply the proposed module to a basic one-stage detector SSD. Our final model achieves an accuracy comparable to that of state-of-the-art detectors while using a fraction of their model parameters and computational overheads. Moreover, we found that the proposed module has two strong applications. 1) The module can be successfully integrated into a lightweight backbone, further pushing the efficiency of the one-stage detector. 2) The module also allows train-from-scratch without relying on any sophisticated base networks as previous methods do.



### Temporal Attentive Alignment for Large-Scale Video Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1907.12743v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.12743v6)
- **Published**: 2019-07-30 05:43:55+00:00
- **Updated**: 2019-09-15 00:48:41+00:00
- **Authors**: Min-Hung Chen, Zsolt Kira, Ghassan AlRegib, Jaekwon Yoo, Ruxin Chen, Jian Zheng
- **Comment**: ICCV 2019 (Oral) camera-ready + supplementary. Code and data:
  http://github.com/cmhungsteve/TA3N
- **Journal**: None
- **Summary**: Although various image-based domain adaptation (DA) techniques have been proposed in recent years, domain shift in videos is still not well-explored. Most previous works only evaluate performance on small-scale datasets which are saturated. Therefore, we first propose two large-scale video DA datasets with much larger domain discrepancy: UCF-HMDB_full and Kinetics-Gameplay. Second, we investigate different DA integration methods for videos, and show that simultaneously aligning and learning temporal dynamics achieves effective alignment even without sophisticated DA methods. Finally, we propose Temporal Attentive Adversarial Adaptation Network (TA3N), which explicitly attends to the temporal dynamics using domain discrepancy for more effective domain alignment, achieving state-of-the-art performance on four video DA datasets (e.g. 7.9% accuracy gain over "Source only" from 73.9% to 81.8% on "HMDB --> UCF", and 10.3% gain on "Kinetics --> Gameplay"). The code and data are released at http://github.com/cmhungsteve/TA3N.



### Not All Adversarial Examples Require a Complex Defense: Identifying Over-optimized Adversarial Examples with IQR-based Logit Thresholding
- **Arxiv ID**: http://arxiv.org/abs/1907.12744v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.12744v1)
- **Published**: 2019-07-30 05:46:49+00:00
- **Updated**: 2019-07-30 05:46:49+00:00
- **Authors**: Utku Ozbulak, Arnout Van Messem, Wesley De Neve
- **Comment**: Accepted for the 2019 International Joint Conference on Neural
  Networks (IJCNN-19)
- **Journal**: None
- **Summary**: Detecting adversarial examples currently stands as one of the biggest challenges in the field of deep learning. Adversarial attacks, which produce adversarial examples, increase the prediction likelihood of a target class for a particular data point. During this process, the adversarial example can be further optimized, even when it has already been wrongly classified with 100% confidence, thus making the adversarial example even more difficult to detect. For this kind of adversarial examples, which we refer to as over-optimized adversarial examples, we discovered that the logits of the model provide solid clues on whether the data point at hand is adversarial or genuine. In this context, we first discuss the masking effect of the softmax function for the prediction made and explain why the logits of the model are more useful in detecting over-optimized adversarial examples. To identify this type of adversarial examples in practice, we propose a non-parametric and computationally efficient method which relies on interquartile range, with this method becoming more effective as the image resolution increases. We support our observations throughout the paper with detailed experiments for different datasets (MNIST, CIFAR-10, and ImageNet) and several architectures.



### Deep Multi-Kernel Convolutional LSTM Networks and an Attention-Based Mechanism for Videos
- **Arxiv ID**: http://arxiv.org/abs/1908.08990v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.08990v1)
- **Published**: 2019-07-30 05:51:20+00:00
- **Updated**: 2019-07-30 05:51:20+00:00
- **Authors**: Sebastian Agethen, Winston H. Hsu
- **Comment**: None
- **Journal**: None
- **Summary**: Action recognition greatly benefits motion understanding in video analysis. Recurrent networks such as long short-term memory (LSTM) networks are a popular choice for motion-aware sequence learning tasks. Recently, a convolutional extension of LSTM was proposed, in which input-to-hidden and hidden-to-hidden transitions are modeled through convolution with a single kernel. This implies an unavoidable trade-off between effectiveness and efficiency. Herein, we propose a new enhancement to convolutional LSTM networks that supports accommodation of multiple convolutional kernels and layers. This resembles a Network-in-LSTM approach, which improves upon the aforementioned concern. In addition, we propose an attention-based mechanism that is specifically designed for our multi-kernel extension. We evaluated our proposed extensions in a supervised classification setting on the UCF-101 and Sports-1M datasets, with the findings showing that our enhancements improve accuracy. We also undertook qualitative analysis to reveal the characteristics of our system and the convolutional LSTM baseline.



### Impact of Adversarial Examples on Deep Learning Models for Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.13124v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CR, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.13124v1)
- **Published**: 2019-07-30 06:03:57+00:00
- **Updated**: 2019-07-30 06:03:57+00:00
- **Authors**: Utku Ozbulak, Arnout Van Messem, Wesley De Neve
- **Comment**: Accepted for the 22nd International Conference on Medical Image
  Computing and Computer Assisted Intervention (MICCAI-19)
- **Journal**: None
- **Summary**: Deep learning models, which are increasingly being used in the field of medical image analysis, come with a major security risk, namely, their vulnerability to adversarial examples. Adversarial examples are carefully crafted samples that force machine learning models to make mistakes during testing time. These malicious samples have been shown to be highly effective in misguiding classification tasks. However, research on the influence of adversarial examples on segmentation is significantly lacking. Given that a large portion of medical imaging problems are effectively segmentation problems, we analyze the impact of adversarial examples on deep learning-based image segmentation models. Specifically, we expose the vulnerability of these models to adversarial examples by proposing the Adaptive Segmentation Mask Attack (ASMA). This novel algorithm makes it possible to craft targeted adversarial examples that come with (1) high intersection-over-union rates between the target adversarial mask and the prediction and (2) with perturbation that is, for the most part, invisible to the bare eye. We lay out experimental and visual evidence by showing results obtained for the ISIC skin lesion segmentation challenge and the problem of glaucoma optic disc segmentation. An implementation of this algorithm and additional examples can be found at https://github.com/utkuozbulak/adaptive-segmentation-mask-attack.



### Finding Moments in Video Collections Using Natural Language
- **Arxiv ID**: http://arxiv.org/abs/1907.12763v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1907.12763v2)
- **Published**: 2019-07-30 07:31:02+00:00
- **Updated**: 2022-02-23 12:44:54+00:00
- **Authors**: Victor Escorcia, Mattia Soldan, Josef Sivic, Bernard Ghanem, Bryan Russell
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce the task of retrieving relevant video moments from a large corpus of untrimmed, unsegmented videos given a natural language query. Our task poses unique challenges as a system must efficiently identify both the relevant videos and localize the relevant moments in the videos. To address these challenges, we propose SpatioTemporal Alignment with Language (STAL), a model that represents a video moment as a set of regions within a series of short video clips and aligns a natural language query to the moment's regions. Our alignment cost compares variable-length language and video features using symmetric squared Chamfer distance, which allows for efficient indexing and retrieval of the video moments. Moreover, aligning language features to regions within a video moment allows for finer alignment compared to methods that extract only an aggregate feature from the entire video moment. We evaluate our approach on two recently proposed datasets for temporal localization of moments in video with natural language (DiDeMo and Charades-STA) extended to our video corpus moment retrieval setting. We show that our STAL re-ranking model outperforms the recently proposed Moment Context Network on all criteria across all datasets on our proposed task, obtaining relative gains of 37% - 118% for average recall and up to 30% for median rank. Moreover, our approach achieves more than 130x faster retrieval and 8x smaller index size with a 1M video corpus in an approximate setting.



### PointHop: An Explainable Machine Learning Method for Point Cloud Classification
- **Arxiv ID**: http://arxiv.org/abs/1907.12766v2
- **DOI**: 10.1109/TMM.2019.2963592
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.12766v2)
- **Published**: 2019-07-30 07:39:40+00:00
- **Updated**: 2019-12-16 01:37:28+00:00
- **Authors**: Min Zhang, Haoxuan You, Pranav Kadam, Shan Liu, C. -C. Jay Kuo
- **Comment**: 13 pages with 9 figures
- **Journal**: None
- **Summary**: An explainable machine learning method for point cloud classification, called the PointHop method, is proposed in this work. The PointHop method consists of two stages: 1) local-to-global attribute building through iterative one-hop information exchange, and 2) classification and ensembles. In the attribute building stage, we address the problem of unordered point cloud data using a space partitioning procedure and developing a robust descriptor that characterizes the relationship between a point and its one-hop neighbor in a PointHop unit. When we put multiple PointHop units in cascade, the attributes of a point will grow by taking its relationship with one-hop neighbor points into account iteratively. Furthermore, to control the rapid dimension growth of the attribute vector associated with a point, we use the Saab transform to reduce the attribute dimension in each PointHop unit. In the classification and ensemble stage, we feed the feature vector obtained from multiple PointHop units to a classifier. We explore ensemble methods to improve the classification performance furthermore. It is shown by experimental results that the PointHop method offers classification performance that is comparable with state-of-the-art methods while demanding much lower training complexity.



### An Empirical Study of Propagation-based Methods for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.12769v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12769v1)
- **Published**: 2019-07-30 08:00:47+00:00
- **Updated**: 2019-07-30 08:00:47+00:00
- **Authors**: Hengkai Guo, Wenji Wang, Guanjun Guo, Huaxia Li, Jiachen Liu, Qian He, Xuefeng Xiao
- **Comment**: The 2019 DAVIS Challenge on Video Object Segmentation - CVPR
  Workshops
- **Journal**: None
- **Summary**: While propagation-based approaches have achieved state-of-the-art performance for video object segmentation, the literature lacks a fair comparison of different methods using the same settings. In this paper, we carry out an empirical study for propagation-based methods. We view these approaches from a unified perspective and conduct detailed ablation study for core methods, input cues, multi-object combination and training strategies. With careful designs, our improved end-to-end memory networks achieve a global mean of 76.1 on DAVIS 2017 val set.



### Towards Pure End-to-End Learning for Recognizing Multiple Text Sequences from an Image
- **Arxiv ID**: http://arxiv.org/abs/1907.12791v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12791v1)
- **Published**: 2019-07-30 09:07:28+00:00
- **Updated**: 2019-07-30 09:07:28+00:00
- **Authors**: Xu Zhenlong, Zhou shuigeng, Cheng zhanzhan, Bai fan, Niu yi, Pu shiliang
- **Comment**: None
- **Journal**: None
- **Summary**: Here we address a challenging problem: recognizing multiple text sequences from an image by pure end-to-end learning. It is twofold: 1) Multiple text sequences recognition. Each image may contain multiple text sequences of different content, location and orientation, and we try to recognize all the text sequences contained in the image. 2) Pure end-to-end (PEE) learning.We solve the problem in a pure end-to-end learning way where each training image is labeled by only text transcripts of all contained sequences, without any geometric annotations. Most existing works recognize multiple text sequences from an image in a non-end-to-end (NEE) or quasi-end-to-end (QEE) way, in which each image is trained with both text transcripts and text locations.Only recently, a PEE method was proposed to recognize text sequences from an image where the text sequence was split to several lines in the image. However, it cannot be directly applied to recognizing multiple text sequences from an image. So in this paper, we propose a pure end-to-end learning method to recognize multiple text sequences from an image. Our method directly learns multiple sequences of probability distribution conditioned on each input image, and outputs multiple text transcripts with a well-designed decoding strategy.To evaluate the proposed method, we constructed several datasets mainly based on an existing public dataset andtwo real application scenarios. Experimental results show that the proposed method can effectively recognize multiple text sequences from images, and outperforms CTC-based and attention-based baseline methods.



### EdgeNet: A novel approach for Arabic numeral classification
- **Arxiv ID**: http://arxiv.org/abs/1908.02254v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.02254v1)
- **Published**: 2019-07-30 10:17:43+00:00
- **Updated**: 2019-07-30 10:17:43+00:00
- **Authors**: S. M. A. Sharif, Ghulam Mujtaba, S. M. Nadim Uddin
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the importance of handwritten numeral classification, a robust and effective method for a widely used language like Arabic is still due. This study focuses to overcome two major limitations of existing works: data diversity and effective learning method. Hence, the existing Arabic numeral datasets have been merged into a single dataset and augmented to introduce data diversity. Moreover, a novel deep model has been proposed to exploit diverse data samples of unified dataset. The proposed deep model utilizes the low-level edge features by propagating them through residual connection. To make a fair comparison with the proposed model, the existing works have been studied under the unified dataset. The comparison experiments illustrate that the unified dataset accelerates the performance of the existing works. Moreover, the proposed model outperforms the existing state-of-the-art Arabic handwritten numeral classification methods and obtain an accuracy of 99.59% in the validation phase. Apart from that, different state-of-the-art classification models have studied with the same dataset to reveal their feasibility for the Arabic numeral classification. Code available at http://github.com/sharif-apu/EdgeNet.



### Challenge of Spatial Cognition for Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.04396v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04396v2)
- **Published**: 2019-07-30 11:35:40+00:00
- **Updated**: 2020-05-12 15:51:21+00:00
- **Authors**: Xi Zhang, Xiaolin Wu, Jun Du
- **Comment**: None
- **Journal**: None
- **Summary**: Given the success of the deep convolutional neural networks (DCNNs) in applications of visual recognition and classification, it would be tantalizing to test if DCNNs can also learn spatial concepts, such as straightness, convexity, left/right, front/back, relative size, aspect ratio, polygons, etc., from varied visual examples of these concepts that are simple and yet vital for spatial reasoning. Much to our dismay, extensive experiments of the type of cognitive psychology demonstrate that the data-driven deep learning (DL) cannot see through superficial variations in visual representations and grasp the spatial concept in abstraction. The root cause of failure turns out to be the learning methodology, not the computational model of the neural network itself. By incorporating task-specific convolutional kernels, we are able to construct DCNNs for spatial cognition tasks that can generalize to input images not drawn from the same distribution of the training set. This work raises a precaution that without manually-incorporated priors or features DCCNs may fail spatial cognitive tasks at rudimentary level.



### Orientation-aware Semantic Segmentation on Icosahedron Spheres
- **Arxiv ID**: http://arxiv.org/abs/1907.12849v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12849v1)
- **Published**: 2019-07-30 11:59:24+00:00
- **Updated**: 2019-07-30 11:59:24+00:00
- **Authors**: Chao Zhang, Stephan Liwicki, William Smith, Roberto Cipolla
- **Comment**: 9 pages, accepted to iccv 2019
- **Journal**: None
- **Summary**: We address semantic segmentation on omnidirectional images, to leverage a holistic understanding of the surrounding scene for applications like autonomous driving systems. For the spherical domain, several methods recently adopt an icosahedron mesh, but systems are typically rotation invariant or require significant memory and parameters, thus enabling execution only at very low resolutions. In our work, we propose an orientation-aware CNN framework for the icosahedron mesh. Our representation allows for fast network operations, as our design simplifies to standard network operations of classical CNNs, but under consideration of north-aligned kernel convolutions for features on the sphere. We implement our representation and demonstrate its memory efficiency up-to a level-8 resolution mesh (equivalent to 640 x 1024 equirectangular images). Finally, since our kernels operate on the tangent of the sphere, standard feature weights, pretrained on perspective data, can be directly transferred with only small need for weight refinement. In our evaluation our orientation-aware CNN becomes a new state of the art for the recent 2D3DS dataset, and our Omni-SYNTHIA version of SYNTHIA. Rotation invariant classification and segmentation tasks are additionally presented for comparison to prior art.



### ColorMapGAN: Unsupervised Domain Adaptation for Semantic Segmentation Using Color Mapping Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.12859v2
- **DOI**: 10.1109/TGRS.2020.2980417
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12859v2)
- **Published**: 2019-07-30 12:30:32+00:00
- **Updated**: 2019-09-29 19:56:56+00:00
- **Authors**: Onur Tasar, S L Happy, Yuliya Tarabalka, Pierre Alliez
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the various reasons such as atmospheric effects and differences in acquisition, it is often the case that there exists a large difference between spectral bands of satellite images collected from different geographic locations. The large shift between spectral distributions of training and test data causes the current state of the art supervised learning approaches to output unsatisfactory maps. We present a novel semantic segmentation framework that is robust to such shift. The key component of the proposed framework is Color Mapping Generative Adversarial Networks (ColorMapGAN), which can generate fake training images that are semantically exactly the same as training images, but whose spectral distribution is similar to the distribution of the test images. We then use the fake images and the ground-truth for the training images to fine-tune the already trained classifier. Contrary to the existing Generative Adversarial Networks (GANs), the generator in ColorMapGAN does not have any convolutional or pooling layers. It learns to transform the colors of the training data to the colors of the test data by performing only one element-wise matrix multiplication and one matrix addition operations. Thanks to the architecturally simple but powerful design of ColorMapGAN, the proposed framework outperforms the existing approaches with a large margin in terms of both accuracy and computational complexity.



### LEAF-QA: Locate, Encode & Attend for Figure Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1907.12861v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12861v1)
- **Published**: 2019-07-30 12:39:06+00:00
- **Updated**: 2019-07-30 12:39:06+00:00
- **Authors**: Ritwick Chaudhry, Sumit Shekhar, Utkarsh Gupta, Pranav Maneriker, Prann Bansal, Ajay Joshi
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce LEAF-QA, a comprehensive dataset of $250,000$ densely annotated figures/charts, constructed from real-world open data sources, along with ~2 million question-answer (QA) pairs querying the structure and semantics of these charts. LEAF-QA highlights the problem of multimodal QA, which is notably different from conventional visual QA (VQA), and has recently gained interest in the community. Furthermore, LEAF-QA is significantly more complex than previous attempts at chart QA, viz. FigureQA and DVQA, which present only limited variations in chart data. LEAF-QA being constructed from real-world sources, requires a novel architecture to enable question answering. To this end, LEAF-Net, a deep architecture involving chart element localization, question and answer encoding in terms of chart elements, and an attention network is proposed. Different experiments are conducted to demonstrate the challenges of QA on LEAF-QA. The proposed architecture, LEAF-Net also considerably advances the current state-of-the-art on FigureQA and DVQA.



### Open Set Domain Adaptation for Image and Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.12865v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12865v1)
- **Published**: 2019-07-30 12:50:17+00:00
- **Updated**: 2019-07-30 12:50:17+00:00
- **Authors**: Pau Panareda Busto, Ahsan Iqbal, Juergen Gall
- **Comment**: None
- **Journal**: None
- **Summary**: Since annotating and curating large datasets is very expensive, there is a need to transfer the knowledge from existing annotated datasets to unlabelled data. Data that is relevant for a specific application, however, usually differs from publicly available datasets since it is sampled from a different domain. While domain adaptation methods compensate for such a domain shift, they assume that all categories in the target domain are known and match the categories in the source domain. Since this assumption is violated under real-world conditions, we propose an approach for open set domain adaptation where the target domain contains instances of categories that are not present in the source domain. The proposed approach achieves state-of-the-art results on various datasets for image classification and action recognition. Since the approach can be used for open set and closed set domain adaptation, as well as unsupervised and semi-supervised domain adaptation, it is a versatile tool for many applications.



### 2D and 3D Segmentation of uncertain local collagen fiber orientations in SHG microscopy
- **Arxiv ID**: http://arxiv.org/abs/1907.12868v1
- **DOI**: 10.1007/978-3-030-33676-9_26
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.12868v1)
- **Published**: 2019-07-30 12:56:01+00:00
- **Updated**: 2019-07-30 12:56:01+00:00
- **Authors**: Lars Schmarje, Claudius Zelenka, Ulf Geisen, Claus-C. Glüer, Reinhard Koch
- **Comment**: None
- **Journal**: DAGM GCPR 2019
- **Summary**: Collagen fiber orientations in bones, visible with Second Harmonic Generation (SHG) microscopy, represent the inner structure and its alteration due to influences like cancer. While analyses of these orientations are valuable for medical research, it is not feasible to analyze the needed large amounts of local orientations manually. Since we have uncertain borders for these local orientations only rough regions can be segmented instead of a pixel-wise segmentation. We analyze the effect of these uncertain borders on human performance by a user study. Furthermore, we compare a variety of 2D and 3D methods such as classical approaches like Fourier analysis with state-of-the-art deep neural networks for the classification of local fiber orientations. We present a general way to use pretrained 2D weights in 3D neural networks, such as Inception-ResNet-3D a 3D extension of Inception-ResNet-v2. In a 10 fold cross-validation our two stage segmentation based on Inception-ResNet-3D and transferred 2D ImageNet weights achieves a human comparable accuracy.



### 4X4 Census Transform
- **Arxiv ID**: http://arxiv.org/abs/1907.12891v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.12891v1)
- **Published**: 2019-07-30 13:30:45+00:00
- **Updated**: 2019-07-30 13:30:45+00:00
- **Authors**: Olivier Rukundo
- **Comment**: 3 pages, 9 figures, 2 tables
- **Journal**: None
- **Summary**: This paper proposes a 4X4 Census Transform (4X4CT) to encourage further research in computer vision and visual computing. Unlike the traditional 3X3 CT which uses a nine pixels kernel, the proposed 4X4CT uses a sixteen pixels kernel with four overlapped groups of 3X3 kernel size. In each overlapping group, a reference input pixel profits from its nearest eight pixels to produce an eight bits binary string convertible to a grayscale integer of the 4X4CT's output pixel. Preliminary experiments demonstrated more image textural crispness and contrast than the CT as well as alternativeness to enable meaningful solutions to be achieved.



### Increasing Shape Bias in ImageNet-Trained Networks Using Transfer Learning and Domain-Adversarial Methods
- **Arxiv ID**: http://arxiv.org/abs/1907.12892v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.12892v1)
- **Published**: 2019-07-30 13:30:46+00:00
- **Updated**: 2019-07-30 13:30:46+00:00
- **Authors**: Francis Brochu
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have become the state-of-the-art method to learn from image data. However, recent research shows that they may include a texture and colour bias in their representation, contrary to the intuition that they learn the shapes of the image content and to human biological learning. Thus, recent works have attempted to increase the shape bias in CNNs in order to train more robust and accurate networks on tasks. One such approach uses style-transfer in order to remove texture clues from the data. This work reproduces this methodology on four image classification datasets, as well as extends the method to use domain-adversarial training in order to further increase the shape bias in the learned representation. The results show the proposed method increases the robustness and shape bias of the CNNs, while it does not provide a gain in accuracy.



### Safe Augmentation: Learning Task-Specific Transformations from Data
- **Arxiv ID**: http://arxiv.org/abs/1907.12896v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12896v1)
- **Published**: 2019-07-30 13:32:42+00:00
- **Updated**: 2019-07-30 13:32:42+00:00
- **Authors**: Irynei Baran, Orest Kupyn, Arseny Kravchenko
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation is widely used as a part of the training process applied to deep learning models, especially in the computer vision domain. Currently, common data augmentation techniques are designed manually. Therefore they require expert knowledge and time. Moreover, augmentations are dataset-specific, and the optimal augmentations set on a specific dataset has limited transferability to others. We present a simple and explainable method called $\textbf{Safe Augmentation}$ that can learn task-specific data augmentation techniques that do not change the data distribution and improve the generalization of the model. We propose to use safe augmentation in two ways: for model fine-tuning and along with other augmentation techniques. Our method is model-agnostic, easy to implement, and achieves better accuracy on CIFAR-10, CIFAR-100, SVHN, Tiny ImageNet, and Cityscapes datasets comparing to baseline augmentation techniques. The code is available at $\href{https://github.com/Irynei/SafeAugmentation}{https://github.com/Irynei/SafeAugmentation}$.



### Deep Learning architectures for generalized immunofluorescence based nuclear image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.12975v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/1907.12975v1)
- **Published**: 2019-07-30 14:23:29+00:00
- **Updated**: 2019-07-30 14:23:29+00:00
- **Authors**: Florian Kromp, Lukas Fischer, Eva Bozsaky, Inge Ambros, Wolfgang Doerr, Sabine Taschner-Mandl, Peter Ambros, Allan Hanbury
- **Comment**: 10 pages + 3 supplementary pages
- **Journal**: None
- **Summary**: Separating and labeling each instance of a nucleus (instance-aware segmentation) is the key challenge in segmenting single cell nuclei on fluorescence microscopy images. Deep Neural Networks can learn the implicit transformation of a nuclear image into a probability map indicating the class membership of each pixel (nucleus or background), but the use of post-processing steps to turn the probability map into a labeled object mask is error-prone. This especially accounts for nuclear images of tissue sections and nuclear images across varying tissue preparations. In this work, we aim to evaluate the performance of state-of-the-art deep learning architectures to segment nuclei in fluorescence images of various tissue origins and sample preparation types without post-processing. We compare architectures that operate on pixel to pixel translation and an architecture that operates on object detection and subsequent locally applied segmentation. In addition, we propose a novel strategy to create artificial images to extend the training set. We evaluate the influence of ground truth annotation quality, image scale and segmentation complexity on segmentation performance. Results show that three out of four deep learning architectures (U-Net, U-Net with ResNet34 backbone, Mask R-CNN) can segment fluorescent nuclear images on most of the sample preparation types and tissue origins with satisfactory segmentation performance. Mask R-CNN, an architecture designed to address instance aware segmentation tasks, outperforms other architectures. Equal nuclear mean size, consistent nuclear annotations and the use of artificially generated images result in overall acceptable precision and recall across different tissues and sample preparation types.



### Bilateral Operators for Functional Maps
- **Arxiv ID**: http://arxiv.org/abs/1907.12993v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12993v1)
- **Published**: 2019-07-30 14:55:23+00:00
- **Updated**: 2019-07-30 14:55:23+00:00
- **Authors**: Gautam Pai, Mor Joseph-Rivlin, Ron Kimmel
- **Comment**: None
- **Journal**: None
- **Summary**: A majority of shape correspondence frameworks are based on devising pointwise and pairwise constraints on the correspondence map. The functional maps framework allows for formulating these constraints in the spectral domain. In this paper, we develop a functional map framework for the shape correspondence problem by constructing pairwise constraints using point-wise descriptors. Our core observation is that, every point-wise descriptor allows for the construction a pairwise kernel operator whose low frequency eigenfunctions depict regions of similar descriptor values at various scales of frequency. By aggregating the pairwise information from the descriptor and the intrinsic geometry of the surface encoded in the heat kernel, we construct a hybrid kernel and call it the bilateral operator. Analogous to the edge preserving bilateral filter in image processing, the action of the bilateral operator on a function defined over the manifold yields a descriptor dependent local smoothing of that function. By forcing the correspondence map to commute with the Bilateral operator, we show that we can maximally exploit the information from a given set of pointwise descriptors in a functional map framework.



### Synthesis and Inpainting-Based MR-CT Registration for Image-Guided Thermal Ablation of Liver Tumors
- **Arxiv ID**: http://arxiv.org/abs/1907.13020v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.13020v1)
- **Published**: 2019-07-30 15:37:16+00:00
- **Updated**: 2019-07-30 15:37:16+00:00
- **Authors**: Dongming Wei, Sahar Ahmad, Jiayu Huo, Wen Peng, Yunhao Ge, Zhong Xue, Pew-Thian Yap, Wentao Li, Dinggang Shen, Qian Wang
- **Comment**: Accepted in MICCAI 2019
- **Journal**: None
- **Summary**: Thermal ablation is a minimally invasive procedure for treat-ing small or unresectable tumors. Although CT is widely used for guiding ablation procedures, the contrast of tumors against surrounding normal tissues in CT images is often poor, aggravating the difficulty in accurate thermal ablation. In this paper, we propose a fast MR-CT image registration method to overlay a pre-procedural MR (pMR) image onto an intra-procedural CT (iCT) image for guiding the thermal ablation of liver tumors. By first using a Cycle-GAN model with mutual information constraint to generate synthesized CT (sCT) image from the cor-responding pMR, pre-procedural MR-CT image registration is carried out through traditional mono-modality CT-CT image registration. At the intra-procedural stage, a partial-convolution-based network is first used to inpaint the probe and its artifacts in the iCT image. Then, an unsupervised registration network is used to efficiently align the pre-procedural CT (pCT) with the inpainted iCT (inpCT) image. The final transformation from pMR to iCT is obtained by combining the two estimated transformations,i.e., (1) from the pMR image space to the pCT image space (through sCT) and (2) from the pCT image space to the iCT image space (through inpCT). Experimental results confirm that the proposed method achieves high registration accuracy with a very fast computational speed.



### SkeleMotion: A New Representation of Skeleton Joint Sequences Based on Motion Information for 3D Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.13025v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.13025v1)
- **Published**: 2019-07-30 15:40:07+00:00
- **Updated**: 2019-07-30 15:40:07+00:00
- **Authors**: Carlos Caetano, Jessica Sena, François Brémond, Jefersson A. dos Santos, William Robson Schwartz
- **Comment**: 16-th IEEE International Conference on Advanced Video and
  Signal-based Surveillance (AVSS2019)
- **Journal**: None
- **Summary**: Due to the availability of large-scale skeleton datasets, 3D human action recognition has recently called the attention of computer vision community. Many works have focused on encoding skeleton data as skeleton image representations based on spatial structure of the skeleton joints, in which the temporal dynamics of the sequence is encoded as variations in columns and the spatial structure of each frame is represented as rows of a matrix. To further improve such representations, we introduce a novel skeleton image representation to be used as input of Convolutional Neural Networks (CNNs), named SkeleMotion. The proposed approach encodes the temporal dynamics by explicitly computing the magnitude and orientation values of the skeleton joints. Different temporal scales are employed to compute motion values to aggregate more temporal dynamics to the representation making it able to capture longrange joint interactions involved in actions as well as filtering noisy motion values. Experimental results demonstrate the effectiveness of the proposed representation on 3D action recognition outperforming the state-of-the-art on NTU RGB+D 120 dataset.



### Lung image segmentation by generative adversarial networks
- **Arxiv ID**: http://arxiv.org/abs/1907.13033v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.13033v1)
- **Published**: 2019-07-30 15:47:02+00:00
- **Updated**: 2019-07-30 15:47:02+00:00
- **Authors**: Jiaxin Cai, Hongfeng Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Lung image segmentation plays an important role in computer-aid pulmonary diseases diagnosis and treatment. This paper proposed a lung image segmentation method by generative adversarial networks. We employed a variety of generative adversarial networks and use its capability of image translation to perform image segmentation. The generative adversarial networks was employed to translate the original lung image to the segmented image. The generative adversarial networks based segmentation method was test on real lung image data set. Experimental results shows that the proposed method is effective and outperform state-of-the art method.



### Efficient Method for Categorize Animals in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1907.13037v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.13037v1)
- **Published**: 2019-07-30 15:51:56+00:00
- **Updated**: 2019-07-30 15:51:56+00:00
- **Authors**: Abulikemu Abuduweili, Xin Wu, Xingchen Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic species classification in camera traps would greatly help the biodiversity monitoring and species analysis in the earth. In order to accelerate the development of automatic species classification task, "Microsoft AI for Earth" have prepared a challenge in FGVC6 workshop at CVPR 2019, which called "iWildCam 2019 competition". In this work, we propose an efficient method for categorizing animals in the wild. We transfer the state-of-the-art ImagaNet pretrained models to the problem. To improve the generalization and robustness of the model, we utilize efficient image augmentation and regularization strategies, like cutout, mixup and label-smoothing. Finally, we use ensemble learning to increase the performance of the model. Thanks to advanced regularization strategies and ensemble learning, we got top 7/336 places in the final leaderboard. Source code of this work is available at https://github.com/Walleclipse/iWildCam_2019_FGVC6



### Weakly Supervised Body Part Segmentation with Pose based Part Priors
- **Arxiv ID**: http://arxiv.org/abs/1907.13051v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.13051v2)
- **Published**: 2019-07-30 16:21:11+00:00
- **Updated**: 2020-10-30 18:53:06+00:00
- **Authors**: Zhengyuan Yang, Yuncheng Li, Linjie Yang, Ning Zhang, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Human body part segmentation refers to the task of predicting the semantic segmentation mask for each body part. Fully supervised body part segmentation methods achieve good performances but require an enormous amount of effort to annotate part masks for training. In contrast to high annotation costs needed for a limited number of part mask annotations, a large number of weak labels such as poses and full body masks already exist and contain relevant information. Motivated by the possibility of using existing weak labels, we propose the first weakly supervised body part segmentation framework. The core idea is first converting the sparse weak labels such as keypoints to the initial estimate of body part masks, and then iteratively refine the part mask predictions. We name the initial part masks estimated from poses the "part priors." With sufficient extra weak labels, our weakly supervised framework achieves a comparable performance (62.0% mIoU) to the fully supervised method (63.6% mIoU) on the Pascal-Person-Part dataset. Furthermore, in the extended semi-supervised setting, the proposed framework outperforms the state-of-art methods. Moreover, we extend our proposed framework to other keypoint-supervised part segmentation tasks such as face parsing.



### GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations
- **Arxiv ID**: http://arxiv.org/abs/1907.13052v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.13052v4)
- **Published**: 2019-07-30 16:22:39+00:00
- **Updated**: 2020-11-23 10:31:22+00:00
- **Authors**: Martin Engelcke, Adam R. Kosiorek, Oiwi Parker Jones, Ingmar Posner
- **Comment**: Published at the International Conference on Learning Representations
  (ICLR) 2020
- **Journal**: None
- **Summary**: Generative latent-variable models are emerging as promising tools in robotics and reinforcement learning. Yet, even though tasks in these domains typically involve distinct objects, most state-of-the-art generative models do not explicitly capture the compositional nature of visual scenes. Two recent exceptions, MONet and IODINE, decompose scenes into objects in an unsupervised fashion. Their underlying generative processes, however, do not account for component interactions. Hence, neither of them allows for principled sampling of novel scenes. Here we present GENESIS, the first object-centric generative model of 3D visual scenes capable of both decomposing and generating scenes by capturing relationships between scene components. GENESIS parameterises a spatial GMM over images which is decoded from a set of object-centric latent variables that are either inferred sequentially in an amortised fashion or sampled from an autoregressive prior. We train GENESIS on several publicly available datasets and evaluate its performance on scene generation, decomposition, and semi-supervised learning.



### Grid Saliency for Context Explanations of Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.13054v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.13054v2)
- **Published**: 2019-07-30 16:25:52+00:00
- **Updated**: 2019-11-07 12:29:44+00:00
- **Authors**: Lukas Hoyer, Mauricio Munoz, Prateek Katiyar, Anna Khoreva, Volker Fischer
- **Comment**: 33rd Conference on Neural Information Processing Systems (NeurIPS
  2019)
- **Journal**: None
- **Summary**: Recently, there has been a growing interest in developing saliency methods that provide visual explanations of network predictions. Still, the usability of existing methods is limited to image classification models. To overcome this limitation, we extend the existing approaches to generate grid saliencies, which provide spatially coherent visual explanations for (pixel-level) dense prediction networks. As the proposed grid saliency allows to spatially disentangle the object and its context, we specifically explore its potential to produce context explanations for semantic segmentation networks, discovering which context most influences the class predictions inside a target object area. We investigate the effectiveness of grid saliency on a synthetic dataset with an artificially induced bias between objects and their context as well as on the real-world Cityscapes dataset using state-of-the-art segmentation networks. Our results show that grid saliency can be successfully used to provide easily interpretable context explanations and, moreover, can be employed for detecting and localizing contextual biases present in the data.



### Screening Mammogram Classification with Prior Exams
- **Arxiv ID**: http://arxiv.org/abs/1907.13057v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.13057v1)
- **Published**: 2019-07-30 16:34:53+00:00
- **Updated**: 2019-07-30 16:34:53+00:00
- **Authors**: Jungkyu Park, Jason Phang, Yiqiu Shen, Nan Wu, S. Gene Kim, Linda Moy, Kyunghyun Cho, Krzysztof J. Geras
- **Comment**: MIDL 2019 [arXiv:1907.08612]
- **Journal**: None
- **Summary**: Radiologists typically compare a patient's most recent breast cancer screening exam to their previous ones in making informed diagnoses. To reflect this practice, we propose new neural network models that compare pairs of screening mammograms from the same patient. We train and evaluate our proposed models on over 665,000 pairs of images (over 166,000 pairs of exams). Our best model achieves an AUC of 0.866 in predicting malignancy in patients who underwent breast cancer screening, reducing the error rate of the corresponding baseline.



### Pay attention to the activations: a modular attention mechanism for fine-grained image recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.13075v1
- **DOI**: 10.1109/TMM.2019.2928494
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.13075v1)
- **Published**: 2019-07-30 17:00:15+00:00
- **Updated**: 2019-07-30 17:00:15+00:00
- **Authors**: Pau Rodríguez López, Diego Velazquez Dorta, Guillem Cucurull Preixens, Josep M. Gonfaus, F. Xavier Roca Marva, Jordi Gonzàlez Sabaté
- **Comment**: IEEE Transactions on Multimedia, ECCV extension
- **Journal**: None
- **Summary**: Fine-grained image recognition is central to many multimedia tasks such as search, retrieval and captioning. Unfortunately, these tasks are still challenging since the appearance of samples of the same class can be more different than those from different classes. Attention has been typically implemented in neural networks by selecting the most informative regions of the image that improve classification. In contrast, in this paper, attention is not applied at the image level but to the convolutional feature activations. In essence, with our approach, the neural model learns to attend to lower-level feature activations without requiring part annotations and uses those activations to update and rectify the output likelihood distribution. The proposed mechanism is modular, architecture-independent and efficient in terms of both parameters and computation required. Experiments demonstrate that well-known networks such as Wide Residual Networks and ResNeXt, when augmented with our approach, systematically improve their classification accuracy and become more robust to changes in deformation and pose and to the presence of clutter. As a result, our proposal reaches state-of-the-art classification accuracies in CIFAR-10, the Adience gender recognition task, Stanford Dogs, and UEC-Food100 while obtaining competitive performance in ImageNet, CIFAR-100, CUB200 Birds, and Stanford Cars. In addition, we analyze the different components of our model, showing that the proposed attention modules succeed in finding the most discriminative regions of the image. Finally, as a proof of concept, we demonstrate that with only local predictions, an augmented neural network can successfully classify an image before reaching any fully connected layer, thus reducing the computational amount up to 10%.



### Deformable Filter Convolution for Point Cloud Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1907.13079v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.13079v1)
- **Published**: 2019-07-30 17:09:12+00:00
- **Updated**: 2019-07-30 17:09:12+00:00
- **Authors**: Yuwen Xiong, Mengye Ren, Renjie Liao, Kelvin Wong, Raquel Urtasun
- **Comment**: None
- **Journal**: None
- **Summary**: Point clouds are the native output of many real-world 3D sensors. To borrow the success of 2D convolutional network architectures, a majority of popular 3D perception models voxelize the points, which can result in a loss of local geometric details that cannot be recovered. In this paper, we propose a novel learnable convolution layer for processing 3D point cloud data directly. Instead of discretizing points into fixed voxels, we deform our learnable 3D filters to match with the point cloud shape. We propose to combine voxelized backbone networks with our deformable filter layer at 1) the network input stream and 2) the output prediction layers to enhance point level reasoning. We obtain state-of-the-art results on LiDAR semantic segmentation and producing a significant gain in performance on LiDAR object detection.



### Deblurring Face Images using Uncertainty Guided Multi-Stream Semantic Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.13106v2
- **DOI**: 10.1109/TIP.2020.2990354
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.13106v2)
- **Published**: 2019-07-30 17:41:41+00:00
- **Updated**: 2020-04-20 21:08:03+00:00
- **Authors**: Rajeev Yasarla, Federico Perazzi, Vishal M. Patel
- **Comment**: Accepted at TIP 2020
- **Journal**: None
- **Summary**: We propose a novel multi-stream architecture and training methodology that exploits semantic labels for facial image deblurring. The proposed Uncertainty Guided Multi- Stream Semantic Network (UMSN) processes regions belonging to each semantic class independently and learns to combine their outputs into the final deblurred result. Pixel-wise semantic labels are obtained using a segmentation network. A predicted confidence measure is used during training to guide the network towards the challenging regions of the human face such as the eyes and nose. The entire network is trained in an end- to-end fashion. Comprehensive experiments on three different face datasets demonstrate that the proposed method achieves significant improvements over the recent state-of-the-art face deblurring methods. Code is available at: https://github.com/ rajeevyasarla/UMSN-Face-Deblurring



### Degeneracy in Self-Calibration Revisited and a Deep Learning Solution for Uncalibrated SLAM
- **Arxiv ID**: http://arxiv.org/abs/1907.13185v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.13185v1)
- **Published**: 2019-07-30 19:07:13+00:00
- **Updated**: 2019-07-30 19:07:13+00:00
- **Authors**: Bingbing Zhuang, Quoc-Huy Tran, Pan Ji, Gim Hee Lee, Loong Fah Cheong, Manmohan Chandraker
- **Comment**: To appear at IROS 2019
- **Journal**: None
- **Summary**: Self-calibration of camera intrinsics and radial distortion has a long history of research in the computer vision community. However, it remains rare to see real applications of such techniques to modern Simultaneous Localization And Mapping (SLAM) systems, especially in driving scenarios. In this paper, we revisit the geometric approach to this problem, and provide a theoretical proof that explicitly shows the ambiguity between radial distortion and scene depth when two-view geometry is used to self-calibrate the radial distortion. In view of such geometric degeneracy, we propose a learning approach that trains a convolutional neural network (CNN) on a large amount of synthetic data. We demonstrate the utility of our proposed method by applying it as a checkerboard-free calibration tool for SLAM, achieving comparable or superior performance to previous learning and hand-crafted methods.



### The Best of Both Modes: Separately Leveraging RGB and Depth for Unseen Object Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.13236v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1907.13236v2)
- **Published**: 2019-07-30 21:35:33+00:00
- **Updated**: 2020-07-16 02:05:00+00:00
- **Authors**: Christopher Xie, Yu Xiang, Arsalan Mousavian, Dieter Fox
- **Comment**: None
- **Journal**: None
- **Summary**: In order to function in unstructured environments, robots need the ability to recognize unseen novel objects. We take a step in this direction by tackling the problem of segmenting unseen object instances in tabletop environments. However, the type of large-scale real-world dataset required for this task typically does not exist for most robotic settings, which motivates the use of synthetic data. We propose a novel method that separately leverages synthetic RGB and synthetic depth for unseen object instance segmentation. Our method is comprised of two stages where the first stage operates only on depth to produce rough initial masks, and the second stage refines these masks with RGB. Surprisingly, our framework is able to learn from synthetic RGB-D data where the RGB is non-photorealistic. To train our method, we introduce a large-scale synthetic dataset of random objects on tabletops. We show that our method, trained on this dataset, can produce sharp and accurate masks, outperforming state-of-the-art methods on unseen object instance segmentation. We also show that our method can segment unseen objects for robot grasping. Code, models and video can be found at https://rse-lab.cs.washington.edu/projects/unseen-object-instance-segmentation/.



### Joint Group Feature Selection and Discriminative Filter Learning for Robust Visual Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1907.13242v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.13242v2)
- **Published**: 2019-07-30 21:41:59+00:00
- **Updated**: 2019-08-02 13:38:05+00:00
- **Authors**: Tianyang Xu, Zhen-Hua Feng, Xiao-Jun Wu, Josef Kittler
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new Group Feature Selection method for Discriminative Correlation Filters (GFS-DCF) based visual object tracking. The key innovation of the proposed method is to perform group feature selection across both channel and spatial dimensions, thus to pinpoint the structural relevance of multi-channel features to the filtering system. In contrast to the widely used spatial regularisation or feature selection methods, to the best of our knowledge, this is the first time that channel selection has been advocated for DCF-based tracking. We demonstrate that our GFS-DCF method is able to significantly improve the performance of a DCF tracker equipped with deep neural network features. In addition, our GFS-DCF enables joint feature selection and filter learning, achieving enhanced discrimination and interpretability of the learned filters.   To further improve the performance, we adaptively integrate historical information by constraining filters to be smooth across temporal frames, using an efficient low-rank approximation. By design, specific temporal-spatial-channel configurations are dynamically learned in the tracking process, highlighting the relevant features, and alleviating the performance degrading impact of less discriminative representations and reducing information redundancy. The experimental results obtained on OTB2013, OTB2015, VOT2017, VOT2018 and TrackingNet demonstrate the merits of our GFS-DCF and its superiority over the state-of-the-art trackers. The code is publicly available at https://github.com/XU-TIANYANG/GFS-DCF.



### Landmark Detection in Low Resolution Faces with Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.13255v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.13255v1)
- **Published**: 2019-07-30 23:12:01+00:00
- **Updated**: 2019-07-30 23:12:01+00:00
- **Authors**: Amit Kumar, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: Landmark detection algorithms trained on high resolution images perform poorly on datasets containing low resolution images. This deters the performance of algorithms relying on quality landmarks, for example, face recognition. To the best of our knowledge, there does not exist any dataset consisting of low resolution face images along with their annotated landmarks, making supervised training infeasible. In this paper, we present a semi-supervised approach to predict landmarks on low resolution images by learning them from labeled high resolution images. The objective of this work is to show that predicting landmarks directly on low resolution images is more effective than the current practice of aligning images after rescaling or superresolution. In a two-step process, the proposed approach first learns to generate low resolution images by modeling the distribution of target low resolution images. In the second stage, the roles of generated images and real low resolution images are switched and the model learns to predict landmarks for real low resolution images from generated low resolution images. With extensive experimentation, we study the impact of each of the design choices and also show that prediction of landmarks directly on low resolution images improves the performance of important tasks such as face recognition in low resolution images.



### Robust Autocalibrated Structured Low-Rank EPI Ghost Correction
- **Arxiv ID**: http://arxiv.org/abs/1907.13261v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.13261v3)
- **Published**: 2019-07-30 23:40:15+00:00
- **Updated**: 2020-10-01 23:04:24+00:00
- **Authors**: Rodrigo A. Lobos, W. Scott Hoge, Ahsan Javed, Congyu Liao, Kawin Setsompop, Krishna S. Nayak, Justin P. Haldar
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: We propose and evaluate a new structured low-rank method for EPI ghost correction called Robust Autocalibrated LORAKS (RAC-LORAKS). The method can be used to suppress EPI ghosts arising from the differences between different readout gradient polarities and/or the differences between different shots. It does not require conventional EPI navigator signals, and is robust to imperfect autocalibration data.   Methods: Autocalibrated LORAKS is a previous structured low-rank method for EPI ghost correction that uses GRAPPA-type autocalibration data to enable high-quality ghost correction. This method works well when the autocalibration data is pristine, but performance degrades substantially when the autocalibration information is imperfect. RAC-LORAKS generalizes Autocalibrated LORAKS in two ways. First, it does not completely trust the information from autocalibration data, and instead considers the autocalibration and EPI data simultaneously when estimating low-rank matrix structure. And second, it uses complementary information from the autocalibration data to improve EPI reconstruction in a multi-contrast joint reconstruction framework. RAC-LORAKS is evaluated using simulations and in vivo data, including comparisons to state-of-the-art methods.   Results: RAC-LORAKS is demonstrated to have good ghost elimination performance compared to state-of-the-art methods in several complicated EPI acquisition scenarios (including gradient-echo brain imaging, diffusion-encoded brain imaging, and cardiac imaging).   Conclusion: RAC-LORAKS provides effective suppression of EPI ghosts and is robust to imperfect autocalibration data.



