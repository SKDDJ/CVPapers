# Arxiv Papers in cs.CV on 2019-07-01
### Learning to Blindly Assess Image Quality in the Laboratory and Wild
- **Arxiv ID**: http://arxiv.org/abs/1907.00516v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.00516v3)
- **Published**: 2019-07-01 02:31:07+00:00
- **Updated**: 2020-05-20 05:51:24+00:00
- **Authors**: Weixia Zhang, Kede Ma, Guangtao Zhai, Xiaokang Yang
- **Comment**: Accepted by ICIP2020
- **Journal**: None
- **Summary**: Computational models for blind image quality assessment (BIQA) are typically trained in well-controlled laboratory environments with limited generalizability to realistically distorted images. Similarly, BIQA models optimized for images captured in the wild cannot adequately handle synthetically distorted images. To face the cross-distortion-scenario challenge, we develop a BIQA model and an approach of training it on multiple IQA databases (of different distortion scenarios) simultaneously. A key step in our approach is to create and combine image pairs within individual databases as the training set, which effectively bypasses the issue of perceptual scale realignment. We compute a continuous quality annotation for each pair from the corresponding human opinions, indicating the probability of one image having better perceptual quality. We train a deep neural network for BIQA over the training set of massive image pairs by minimizing the fidelity loss. Experiments on six IQA databases demonstrate that the optimized model by the proposed training strategy is effective in blindly assessing image quality in the laboratory and wild, outperforming previous BIQA methods by a large margin.



### Cross-view Relation Networks for Mammogram Mass Detection
- **Arxiv ID**: http://arxiv.org/abs/1907.00528v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.00528v1)
- **Published**: 2019-07-01 03:27:09+00:00
- **Updated**: 2019-07-01 03:27:09+00:00
- **Authors**: Jiechao Ma, Sen Liang, Xiang Li, Hongwei Li, Bjoern H Menze, Rongguo Zhang, Wei-Shi Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Mammogram is the most effective imaging modality for the mass lesion detection of breast cancer at the early stage. The information from the two paired views (i.e., medio-lateral oblique and cranio-caudal) are highly relational and complementary, and this is crucial for doctors' decisions in clinical practice. However, existing mass detection methods do not consider jointly learning effective features from the two relational views. To address this issue, this paper proposes a novel mammogram mass detection framework, termed Cross-View Relation Region-based Convolutional Neural Networks (CVR-RCNN). The proposed CVR-RCNN is expected to capture the latent relation information between the corresponding mass region of interests (ROIs) from the two paired views. Evaluations on a new large-scale private dataset and a public mammogram dataset show that the proposed CVR-RCNN outperforms existing state-of-the-art mass detection methods. Meanwhile, our experimental results suggest that incorporating the relation information across two views helps to train a superior detection model, which is a promising avenue for mammogram mass detection.



### Large Area 3D Human Pose Detection Via Stereo Reconstruction in Panoramic Cameras
- **Arxiv ID**: http://arxiv.org/abs/1907.00534v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.00534v1)
- **Published**: 2019-07-01 04:09:23+00:00
- **Updated**: 2019-07-01 04:09:23+00:00
- **Authors**: Christoph Heindl, Thomas Pönitz, Andreas Pichler, Josef Scharinger
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel 3D human pose detector using two panoramic cameras. We show that transforming fisheye perspectives to rectilinear views allows a direct application of two-dimensional deep-learning pose estimation methods, without the explicit need for a costly re-training step to compensate for fisheye image distortions. By utilizing panoramic cameras, our method is capable of accurately estimating human poses over a large field of view. This renders our method suitable for ergonomic analyses and other pose based assessments.



### Spatio-thermal depth correction of RGB-D sensors based on Gaussian Processes in real-time
- **Arxiv ID**: http://arxiv.org/abs/1907.00549v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.00549v1)
- **Published**: 2019-07-01 05:07:33+00:00
- **Updated**: 2019-07-01 05:07:33+00:00
- **Authors**: Christoph Heindl, Thomas Pönitz, Gernot Stübl, Andreas Pichler, Josef Scharinger
- **Comment**: None
- **Journal**: None
- **Summary**: Commodity RGB-D sensors capture color images along with dense pixel-wise depth information in real-time. Typical RGB-D sensors are provided with a factory calibration and exhibit erratic depth readings due to coarse calibration values, ageing and thermal influence effects. This limits their applicability in computer vision and robotics. We propose a novel method to accurately calibrate depth considering spatial and thermal influences jointly. Our work is based on Gaussian Process Regression in a four dimensional Cartesian and thermal domain. We propose to leverage modern GPUs for dense depth map correction in real-time. For reproducibility we make our dataset and source code publicly available.



### Learning to Approximate Directional Fields Defined over 2D Planes
- **Arxiv ID**: http://arxiv.org/abs/1907.00559v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.00559v1)
- **Published**: 2019-07-01 05:58:46+00:00
- **Updated**: 2019-07-01 05:58:46+00:00
- **Authors**: Maria Taktasheva, Albert Matveev, Alexey Artemov, Evgeny Burnaev
- **Comment**: 7 pages, 5 figures
- **Journal**: Proc. of AIST, 2019
- **Summary**: Reconstruction of directional fields is a need in many geometry processing tasks, such as image tracing, extraction of 3D geometric features, and finding principal surface directions. A common approach to the construction of directional fields from data relies on complex optimization procedures, which are usually poorly formalizable, require a considerable computational effort, and do not transfer across applications. In this work, we propose a deep learning-based approach and study the expressive power and generalization ability.



### Weight Normalization based Quantization for Deep Neural Network Compression
- **Arxiv ID**: http://arxiv.org/abs/1907.00593v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.00593v1)
- **Published**: 2019-07-01 07:59:39+00:00
- **Updated**: 2019-07-01 07:59:39+00:00
- **Authors**: Wen-Pu Cai, Wu-Jun Li
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: With the development of deep neural networks, the size of network models becomes larger and larger. Model compression has become an urgent need for deploying these network models to mobile or embedded devices. Model quantization is a representative model compression technique. Although a lot of quantization methods have been proposed, many of them suffer from a high quantization error caused by a long-tail distribution of network weights. In this paper, we propose a novel quantization method, called weight normalization based quantization (WNQ), for model compression. WNQ adopts weight normalization to avoid the long-tail distribution of network weights and subsequently reduces the quantization error. Experiments on CIFAR-100 and ImageNet show that WNQ can outperform other baselines to achieve state-of-the-art performance.



### One Network for Multi-Domains: Domain Adaptive Hashing with Intersectant Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1907.00612v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.00612v1)
- **Published**: 2019-07-01 08:48:39+00:00
- **Updated**: 2019-07-01 08:48:39+00:00
- **Authors**: Tao He, Yuan-Fang Li, Lianli Gao, Dongxiang Zhang, Jingkuan Song
- **Comment**: Accepted by IJCAI 2019
- **Journal**: None
- **Summary**: With the recent explosive increase of digital data, image recognition and retrieval become a critical practical application. Hashing is an effective solution to this problem, due to its low storage requirement and high query speed. However, most of past works focus on hashing in a single (source) domain. Thus, the learned hash function may not adapt well in a new (target) domain that has a large distributional difference with the source domain. In this paper, we explore an end-to-end domain adaptive learning framework that simultaneously and precisely generates discriminative hash codes and classifies target domain images. Our method encodes two domains images into a semantic common space, followed by two independent generative adversarial networks arming at crosswise reconstructing two domains' images, reducing domain disparity and improving alignment in the shared space. We evaluate our framework on {four} public benchmark datasets, all of which show that our method is superior to the other state-of-the-art methods on the tasks of object recognition and image retrieval.



### CDTB: A Color and Depth Visual Object Tracking Dataset and Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1907.00618v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.00618v1)
- **Published**: 2019-07-01 09:02:40+00:00
- **Updated**: 2019-07-01 09:02:40+00:00
- **Authors**: Alan Lukežič, Ugur Kart, Jani Käpylä, Ahmed Durmush, Joni-Kristian Kämäräinen, Jiří Matas, Matej Kristan
- **Comment**: None
- **Journal**: None
- **Summary**: A long-term visual object tracking performance evaluation methodology and a benchmark are proposed. Performance measures are designed by following a long-term tracking definition to maximize the analysis probing strength. The new measures outperform existing ones in interpretation potential and in better distinguishing between different tracking behaviors. We show that these measures generalize the short-term performance measures, thus linking the two tracking problems. Furthermore, the new measures are highly robust to temporal annotation sparsity and allow annotation of sequences hundreds of times longer than in the current datasets without increasing manual annotation labor. A new challenging dataset of carefully selected sequences with many target disappearances is proposed. A new tracking taxonomy is proposed to position trackers on the short-term/long-term spectrum. The benchmark contains an extensive evaluation of the largest number of long-term tackers and comparison to state-of-the-art short-term trackers. We analyze the influence of tracking architecture implementations to long-term performance and explore various re-detection strategies as well as influence of visual model update strategies to long-term tracking drift. The methodology is integrated in the VOT toolkit to automate experimental analysis and benchmarking and to facilitate future development of long-term trackers.



### Permutohedral Attention Module for Efficient Non-Local Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.00641v2
- **DOI**: 10.1007/978-3-030-32226-7_44
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.00641v2)
- **Published**: 2019-07-01 10:18:37+00:00
- **Updated**: 2019-08-27 14:30:25+00:00
- **Authors**: Samuel Joutard, Reuben Dorent, Amanda Isaac, Sebastien Ourselin, Tom Vercauteren, Marc Modat
- **Comment**: Accepted at MICCAI-2019
- **Journal**: None
- **Summary**: Medical image processing tasks such as segmentation often require capturing non-local information. As organs, bones, and tissues share common characteristics such as intensity, shape, and texture, the contextual information plays a critical role in correctly labeling them. Segmentation and labeling is now typically done with convolutional neural networks (CNNs) but the context of the CNN is limited by the receptive field which itself is limited by memory requirements and other properties. In this paper, we propose a new attention module, that we call Permutohedral Attention Module (PAM), to efficiently capture non-local characteristics of the image. The proposed method is both memory and computationally efficient. We provide a GPU implementation of this module suitable for 3D medical imaging problems. We demonstrate the efficiency and scalability of our module with the challenging task of vertebrae segmentation and labeling where context plays a crucial role because of the very similar appearance of different vertebrae.



### Self-supervised Hyperspectral Image Restoration using Separable Image Prior
- **Arxiv ID**: http://arxiv.org/abs/1907.00651v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.00651v1)
- **Published**: 2019-07-01 10:51:57+00:00
- **Updated**: 2019-07-01 10:51:57+00:00
- **Authors**: Ryuji Imamura, Tatsuki Itasaka, Masahiro Okuda
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised learning with a convolutional neural network is recognized as a powerful means of image restoration. However, most such methods have been designed for application to grayscale and/or color images; therefore, they have limited success when applied to hyperspectral image restoration. This is partially owing to large datasets being difficult to collect, and also the heavy computational load associated with the restoration of an image with many spectral bands. To address this difficulty, we propose a novel self-supervised learning strategy for application to hyperspectral image restoration. Our method automatically creates a training dataset from a single degraded image and trains a denoising network without any clear images. Another notable feature of our method is the use of a separable convolutional layer. We undertake experiments to prove that the use of a separable network allows us to acquire the prior of a hyperspectral image and to realize efficient restoration. We demonstrate the validity of our method through extensive experiments and show that our method has better characteristics than those that are currently regarded as state-of-the-art.



### Avoiding Implementation Pitfalls of "Matrix Capsules with EM Routing" by Hinton et al
- **Arxiv ID**: http://arxiv.org/abs/1907.00652v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.00652v1)
- **Published**: 2019-07-01 10:51:58+00:00
- **Updated**: 2019-07-01 10:51:58+00:00
- **Authors**: Ashley Daniel Gritzman
- **Comment**: None
- **Journal**: None
- **Summary**: The recent progress on capsule networks by Hinton et al. has generated considerable excitement in the machine learning community. The idea behind a capsule is inspired by a cortical minicolumn in the brain, whereby a vertically organised group of around 100 neurons receive common inputs, have common outputs, are interconnected, and may well constitute a fundamental computation unit of the cerebral cortex. However, Hinton's paper on "Matrix Capsule with EM Routing'" was unfortunately not accompanied by a release of source code, which left interested researchers attempting to implement the architecture and reproduce the benchmarks on their own. This has certainly slowed the progress of research building on this work. While writing our own implementation, we noticed several common mistakes in other open source implementations that we came across. In this paper we share some of these learnings, specifically focusing on three implementation pitfalls and how to avoid them: (1) parent capsules with only one child; (2) normalising the amount of data assigned to parent capsules; (3) parent capsules at different positions compete for child capsules. While our implementation is a considerable improvement over currently available implementations, it still falls slightly short of the performance reported by Hinton et al. (2018). The source code for this implementation is available on GitHub at the following URL: https://github.com/IBM/matrix-capsules-with-em-routing.



### The Resale Price Prediction of Secondhand Jewelry Items Using a Multi-modal Deep Model with Iterative Co-Attention
- **Arxiv ID**: http://arxiv.org/abs/1907.00661v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.00661v1)
- **Published**: 2019-07-01 11:17:33+00:00
- **Updated**: 2019-07-01 11:17:33+00:00
- **Authors**: Yusuke Yamaura, Nobuya Kanemaki, Yukihiro Tsuboshita
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: The resale price assessment of secondhand jewelry items relies heavily on the individual knowledge and skill of domain experts. In this paper, we propose a methodology for reconstructing an AI system that autonomously assesses the resale prices of secondhand jewelry items without the need for professional knowledge. As shown in recent studies on fashion items, multimodal approaches combining specifications and visual information of items have succeeded in obtaining fine-grained representations of fashion items, although they generally apply simple vector operations through a multimodal fusion. We similarly build a multimodal model using images and attributes of the product and further employ state-of-the-art multimodal deep neural networks applied in computer vision to achieve a practical performance level. In addition, we model the pricing procedure of an expert using iterative co-attention networks in which the appearance and attributes of the product are carefully and iteratively observed. Herein, we demonstrate the effectiveness of our model using a large dataset of secondhand no brand jewelry items received from a collaborating fashion retailer, and show that the iterative co-attention process operates effectively in the context of resale price prediction. Our model architecture is widely applicable to other fashion items where appearance and specifications are important aspects.



### Speaker-independent classification of phonetic segments from raw ultrasound in child speech
- **Arxiv ID**: http://arxiv.org/abs/1907.01413v1
- **DOI**: 10.1109/ICASSP.2019.8683564
- **Categories**: **eess.AS**, cs.CL, cs.CV, cs.LG, cs.SD, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.01413v1)
- **Published**: 2019-07-01 12:04:13+00:00
- **Updated**: 2019-07-01 12:04:13+00:00
- **Authors**: Manuel Sam Ribeiro, Aciel Eshky, Korin Richmond, Steve Renals
- **Comment**: 5 pages, 4 figures, published in ICASSP2019 (IEEE International
  Conference on Acoustics, Speech and Signal Processing, 2019)
- **Journal**: None
- **Summary**: Ultrasound tongue imaging (UTI) provides a convenient way to visualize the vocal tract during speech production. UTI is increasingly being used for speech therapy, making it important to develop automatic methods to assist various time-consuming manual tasks currently performed by speech therapists. A key challenge is to generalize the automatic processing of ultrasound tongue images to previously unseen speakers. In this work, we investigate the classification of phonetic segments (tongue shapes) from raw ultrasound recordings under several training scenarios: speaker-dependent, multi-speaker, speaker-independent, and speaker-adapted. We observe that models underperform when applied to data from speakers not seen at training time. However, when provided with minimal additional speaker information, such as the mean ultrasound frame, the models generalize better to unseen speakers.



### Multi-atlas image registration of clinical data with automated quality assessment using ventricle segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.00695v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.00695v2)
- **Published**: 2019-07-01 12:22:39+00:00
- **Updated**: 2019-12-26 11:40:41+00:00
- **Authors**: Florian Dubost, Marleen de Bruijne, Marco Nardin, Adrian V. Dalca, Kathleen L. Donahue, Anne-Katrin Giese, Mark R. Etherton, Ona Wu, Marius de Groot, Wiro Niessen, Meike Vernooij, Natalia S. Rost, Markus D. Schirmer
- **Comment**: None
- **Journal**: None
- **Summary**: Registration is a core component of many imaging pipelines. In case of clinical scans, with lower resolution and sometimes substantial motion artifacts, registration can produce poor results. Visual assessment of registration quality in large clinical datasets is inefficient. In this work, we propose to automatically assess the quality of registration to an atlas in clinical FLAIR MRI scans of the brain. The method consists of automatically segmenting the ventricles of a given scan using a neural network, and comparing the segmentation to the atlas' ventricles propagated to image space. We used the proposed method to improve clinical image registration to a general atlas by computing multiple registrations and then selecting the registration that yielded the highest ventricle overlap. Methods were evaluated in a single-site dataset of more than 1000 scans, as well as a multi-center dataset comprising 142 clinical scans from 12 sites. The automated ventricle segmentation reached a Dice coefficient with manual annotations of 0.89 in the single-site dataset, and 0.83 in the multi-center dataset. Registration via age-specific atlases could improve ventricle overlap compared to a direct registration to the general atlas (Dice similarity coefficient increase up to 0.15). Experiments also showed that selecting scans with the registration quality assessment method could improve the quality of average maps of white matter hyperintensity burden, instead of using all scans for the computation of the white matter hyperintensity map. In this work, we demonstrated the utility of an automated tool for assessing image registration quality in clinical scans. This image quality assessment step could ultimately assist in the translation of automated neuroimaging pipelines to the clinic.



### Learning Objectness from Sonar Images for Class-Independent Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1907.00734v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.00734v1)
- **Published**: 2019-07-01 12:46:08+00:00
- **Updated**: 2019-07-01 12:46:08+00:00
- **Authors**: Matias Valdenegro-Toro
- **Comment**: European Conference on Mobile Robots 2019
- **Journal**: None
- **Summary**: Detecting novel objects without class information is not trivial, as it is difficult to generalize from a small training set. This is an interesting problem for underwater robotics, as modeling marine objects is inherently more difficult in sonar images, and training data might not be available apriori. Detection proposals algorithms can be used for this purpose but usually requires a large amount of output bounding boxes. In this paper we propose the use of a fully convolutional neural network that regresses an objectness value directly from a Forward-Looking sonar image. By ranking objectness, we can produce high recall (96 %) with only 100 proposals per image. In comparison, EdgeBoxes requires 5000 proposals to achieve a slightly better recall of 97 %, while Selective Search requires 2000 proposals to achieve 95 % recall. We also show that our method outperforms a template matching baseline by a considerable margin, and is able to generalize to completely new objects. We expect that this kind of technique can be used in the field to find lost objects under the sea.



### Synchronising audio and ultrasound by learning cross-modal embeddings
- **Arxiv ID**: http://arxiv.org/abs/1907.00758v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.00758v2)
- **Published**: 2019-07-01 13:22:48+00:00
- **Updated**: 2019-11-27 11:24:26+00:00
- **Authors**: Aciel Eshky, Manuel Sam Ribeiro, Korin Richmond, Steve Renals
- **Comment**: 5 pages, 1 figure, 4 tables; Interspeech 2019 with the following
  edits: 1) Loss and accuracy upon convergence were accidentally reported from
  an older model. Now updated with model described throughout the paper. All
  other results remain unchanged. 2) Max true offset in the training data
  corrected from 179ms to 1789ms. 3) Detectability "boundary/range" renamed to
  detectability "thresholds"
- **Journal**: None
- **Summary**: Audiovisual synchronisation is the task of determining the time offset between speech audio and a video recording of the articulators. In child speech therapy, audio and ultrasound videos of the tongue are captured using instruments which rely on hardware to synchronise the two modalities at recording time. Hardware synchronisation can fail in practice, and no mechanism exists to synchronise the signals post hoc. To address this problem, we employ a two-stream neural network which exploits the correlation between the two modalities to find the offset. We train our model on recordings from 69 speakers, and show that it correctly synchronises 82.9% of test utterances from unseen therapy sessions and unseen speakers, thus considerably reducing the number of utterances to be manually synchronised. An analysis of model performance on the test utterances shows that directed phone articulations are more difficult to automatically synchronise compared to utterances containing natural variation in speech such as words, sentences, or conversations.



### FastDVDnet: Towards Real-Time Deep Video Denoising Without Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/1907.01361v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.01361v2)
- **Published**: 2019-07-01 14:10:34+00:00
- **Updated**: 2020-04-29 18:29:49+00:00
- **Authors**: Matias Tassano, Julie Delon, Thomas Veit
- **Comment**: Code for this algorithm and results can be found in
  https://github.com/m-tassano/fastdvdnet. arXiv admin note: text overlap with
  arXiv:1906.11890
- **Journal**: None
- **Summary**: In this paper, we propose a state-of-the-art video denoising algorithm based on a convolutional neural network architecture. Until recently, video denoising with neural networks had been a largely under explored domain, and existing methods could not compete with the performance of the best patch-based methods. The approach we introduce in this paper, called FastDVDnet, shows similar or better performance than other state-of-the-art competitors with significantly lower computing times. In contrast to other existing neural network denoisers, our algorithm exhibits several desirable properties such as fast runtimes, and the ability to handle a wide range of noise levels with a single network model. The characteristics of its architecture make it possible to avoid using a costly motion compensation stage while achieving excellent performance. The combination between its denoising performance and lower computational load makes this algorithm attractive for practical denoising applications. We compare our method with different state-of-art algorithms, both visually and with respect to objective quality metrics.



### Online Multiple Pedestrians Tracking using Deep Temporal Appearance Matching Association
- **Arxiv ID**: http://arxiv.org/abs/1907.00831v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.00831v4)
- **Published**: 2019-07-01 14:44:41+00:00
- **Updated**: 2020-10-09 14:32:42+00:00
- **Authors**: Young-Chul Yoon, Du Yong Kim, Young-min Song, Kwangjin Yoon, Moongu Jeon
- **Comment**: Accepted in Information Sciences, Elsevier. 3rd Prize on 4th BMTT
  MOTChallenge Workshop held in CVPR2019
- **Journal**: None
- **Summary**: In online multi-target tracking, modeling of appearance and geometric similarities between pedestrians visual scenes is of great importance. The higher dimension of inherent information in the appearance model compared to the geometric model is problematic in many ways. However, due to the recent success of deep-learning-based methods, handling of high-dimensional appearance information becomes feasible. Among many deep neural networks, Siamese network with triplet loss has been widely adopted as an effective appearance feature extractor. Since the Siamese network can extract the features of each input independently, one can update and maintain target-specific features. However, it is not suitable for multi-target settings that require comparison with other inputs. To address this issue, we propose a novel track appearance model based on the joint-inference network. The proposed method enables a comparison of two inputs to be used for adaptive appearance modeling and contributes to the disambiguation of target-observation matching and to the consolidation of identity consistency. Diverse experimental results support the effectiveness of our method. Our work was recognized as the 3rd-best tracker in BMTT MOTChallenge 2019, held at CVPR2019. The code is available at https://github.com/yyc9268/Deep-TAMA.



### UltraSuite: A Repository of Ultrasound and Acoustic Data from Child Speech Therapy Sessions
- **Arxiv ID**: http://arxiv.org/abs/1907.00835v1
- **DOI**: 10.21437/Interspeech.2018-1736
- **Categories**: **cs.CL**, cs.CV, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.00835v1)
- **Published**: 2019-07-01 14:54:31+00:00
- **Updated**: 2019-07-01 14:54:31+00:00
- **Authors**: Aciel Eshky, Manuel Sam Ribeiro, Joanne Cleland, Korin Richmond, Zoe Roxburgh, James Scobbie, Alan Wrench
- **Comment**: 5 pages, 1 figure, 3 tables; accepted to Interspeech 2018: 19th
  Annual Conference of the International Speech Communication Association
  (ISCA)
- **Journal**: None
- **Summary**: We introduce UltraSuite, a curated repository of ultrasound and acoustic data, collected from recordings of child speech therapy sessions. This release includes three data collections, one from typically developing children and two from children with speech sound disorders. In addition, it includes a set of annotations, some manual and some automatically produced, and software tools to process, transform and visualise the data.



### XNect: Real-time Multi-Person 3D Motion Capture with a Single RGB Camera
- **Arxiv ID**: http://arxiv.org/abs/1907.00837v2
- **DOI**: 10.1145/3386569.3392410
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1907.00837v2)
- **Published**: 2019-07-01 14:59:02+00:00
- **Updated**: 2020-04-30 09:55:59+00:00
- **Authors**: Dushyant Mehta, Oleksandr Sotnychenko, Franziska Mueller, Weipeng Xu, Mohamed Elgharib, Pascal Fua, Hans-Peter Seidel, Helge Rhodin, Gerard Pons-Moll, Christian Theobalt
- **Comment**: To appear in ACM Transactions on Graphics (SIGGRAPH) 2020
- **Journal**: None
- **Summary**: We present a real-time approach for multi-person 3D motion capture at over 30 fps using a single RGB camera. It operates successfully in generic scenes which may contain occlusions by objects and by other people. Our method operates in subsequent stages. The first stage is a convolutional neural network (CNN) that estimates 2D and 3D pose features along with identity assignments for all visible joints of all individuals.We contribute a new architecture for this CNN, called SelecSLS Net, that uses novel selective long and short range skip connections to improve the information flow allowing for a drastically faster network without compromising accuracy. In the second stage, a fully connected neural network turns the possibly partial (on account of occlusion) 2Dpose and 3Dpose features for each subject into a complete 3Dpose estimate per individual. The third stage applies space-time skeletal model fitting to the predicted 2D and 3D pose per subject to further reconcile the 2D and 3D pose, and enforce temporal coherence. Our method returns the full skeletal pose in joint angles for each subject. This is a further key distinction from previous work that do not produce joint angle results of a coherent skeleton in real time for multi-person scenes. The proposed system runs on consumer hardware at a previously unseen speed of more than 30 fps given 512x320 images as input while achieving state-of-the-art accuracy, which we will demonstrate on a range of challenging real-world scenes.



### SLSNet: Skin lesion segmentation using a lightweight generative adversarial network
- **Arxiv ID**: http://arxiv.org/abs/1907.00856v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.00856v3)
- **Published**: 2019-07-01 15:21:18+00:00
- **Updated**: 2021-06-17 19:35:47+00:00
- **Authors**: Md. Mostafa Kamal Sarker, Hatem A. Rashwan, Farhan Akram, Vivek Kumar Singh, Syeda Furruka Banu, Forhad U H Chowdhury, Kabir Ahmed Choudhury, Sylvie Chambon, Petia Radeva, Domenec Puig, Mohamed Abdel-Nasser
- **Comment**: Accepted in Expert Systems with Applications
- **Journal**: None
- **Summary**: The determination of precise skin lesion boundaries in dermoscopic images using automated methods faces many challenges, most importantly, the presence of hair, inconspicuous lesion edges and low contrast in dermoscopic images, and variability in the color, texture and shapes of skin lesions. Existing deep learning-based skin lesion segmentation algorithms are expensive in terms of computational time and memory. Consequently, running such segmentation algorithms requires a powerful GPU and high bandwidth memory, which are not available in dermoscopy devices. Thus, this article aims to achieve precise skin lesion segmentation with minimum resources: a lightweight, efficient generative adversarial network (GAN) model called SLSNet, which combines 1-D kernel factorized networks, position and channel attention, and multiscale aggregation mechanisms with a GAN model. The 1-D kernel factorized network reduces the computational cost of 2D filtering. The position and channel attention modules enhance the discriminative ability between the lesion and non-lesion feature representations in spatial and channel dimensions, respectively. A multiscale block is also used to aggregate the coarse-to-fine features of input skin images and reduce the effect of the artifacts. SLSNet is evaluated on two publicly available datasets: ISBI 2017 and the ISIC 2018. Although SLSNet has only 2.35 million parameters, the experimental results demonstrate that it achieves segmentation results on a par with the state-of-the-art skin lesion segmentation methods with an accuracy of 97.61%, and Dice and Jaccard similarity coefficients of 90.63% and 81.98%, respectively. SLSNet can run at more than 110 frames per second (FPS) in a single GTX1080Ti GPU, which is faster than well-known deep learning-based image segmentation models, such as FCN. Therefore, SLSNet can be used for practical dermoscopic applications.



### An Efficient Solution for Breast Tumor Segmentation and Classification in Ultrasound Images Using Deep Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.00887v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.00887v1)
- **Published**: 2019-07-01 16:00:49+00:00
- **Updated**: 2019-07-01 16:00:49+00:00
- **Authors**: Vivek Kumar Singh, Hatem A. Rashwan, Mohamed Abdel-Nasser, Md. Mostafa Kamal Sarker, Farhan Akram, Nidhi Pandey, Santiago Romani, Domenec Puig
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: This paper proposes an efficient solution for tumor segmentation and classification in breast ultrasound (BUS) images. We propose to add an atrous convolution layer to the conditional generative adversarial network (cGAN) segmentation model to learn tumor features at different resolutions of BUS images. To automatically re-balance the relative impact of each of the highest level encoded features, we also propose to add a channel-wise weighting block in the network. In addition, the SSIM and L1-norm loss with the typical adversarial loss are used as a loss function to train the model. Our model outperforms the state-of-the-art segmentation models in terms of the Dice and IoU metrics, achieving top scores of 93.76% and 88.82%, respectively. In the classification stage, we show that few statistics features extracted from the shape of the boundaries of the predicted masks can properly discriminate between benign and malignant tumors with an accuracy of 85%$



### Nature Inspired Dimensional Reduction Technique for Fast and Invariant Visual Feature Extraction
- **Arxiv ID**: http://arxiv.org/abs/1907.01102v1
- **DOI**: 10.30534/ijatcse/2019/57832019
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1907.01102v1)
- **Published**: 2019-07-01 16:58:45+00:00
- **Updated**: 2019-07-01 16:58:45+00:00
- **Authors**: Ravimal Bandara, Lochandaka Ranathunga, Nor Aniza Abdullah
- **Comment**: 11 pages, 11 figures, IJTCSE
- **Journal**: International Journal of Advanced Trends in Computer Science and
  Engineering (IJATCSE) Vol.8 No.3 (2019) 696-706
- **Summary**: Fast and invariant feature extraction is crucial in certain computer vision applications where the computation time is constrained in both training and testing phases of the classifier. In this paper, we propose a nature-inspired dimensionality reduction technique for fast and invariant visual feature extraction. The human brain can exchange the spatial and spectral resolution to reconstruct missing colors in visual perception. The phenomenon is widely used in the printing industry to reduce the number of colors used to print, through a technique, called color dithering. In this work, we adopt a fast error-diffusion color dithering algorithm to reduce the spectral resolution and extract salient features by employing novel Hessian matrix analysis technique, which is then described by a spatial-chromatic histogram. The computation time, descriptor dimensionality and classification performance of the proposed feature are assessed under drastic variances in orientation, viewing angle and illumination of objects comparing with several different state-of-the-art handcrafted and deep-learned features. Extensive experiments on two publicly available object datasets, coil-100 and ALOI carried on both a desktop PC and a Raspberry Pi device show multiple advantages of using the proposed approach, such as the lower computation time, high robustness, and comparable classification accuracy under weakly supervised environment. Further, it showed the capability of operating solely inside a conventional SoC device utilizing a small fraction of the available hardware resources.



### A Framework For Identifying Group Behavior Of Wild Animals
- **Arxiv ID**: http://arxiv.org/abs/1907.00932v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.00932v1)
- **Published**: 2019-07-01 17:12:32+00:00
- **Updated**: 2019-07-01 17:12:32+00:00
- **Authors**: Guido Muscioni, Riccardo Pressiani, Matteo Foglio, Margaret C. Crofoot, Marco D. Santambrogio, Tanya Berger-Wolf
- **Comment**: KDD19 Workshop on Data Mining and AI for Conservation, Earth Day (5
  August 2019), Anchorage, AL
- **Journal**: None
- **Summary**: Activity recognition and, more generally, behavior inference tasks are gaining a lot of interest. Much of it is work in the context of human behavior. New available tracking technologies for wild animals are generating datasets that indirectly may provide information about animal behavior. In this work, we propose a method for classifying these data into behavioral annotation, particularly collective behavior of a social group. Our method is based on sequence analysis with a direct encoding of the interactions of a group of wild animals. We evaluate our approach on a real world dataset, showing significant accuracy improvements over baseline methods.



### Pano Popups: Indoor 3D Reconstruction with a Plane-Aware Network
- **Arxiv ID**: http://arxiv.org/abs/1907.00939v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.00939v2)
- **Published**: 2019-07-01 17:24:18+00:00
- **Updated**: 2020-02-24 18:59:47+00:00
- **Authors**: Marc Eder, Pierre Moulon, Li Guan
- **Comment**: 2019 International Conference on 3D Vision (3DV). IEEE, 2019
- **Journal**: None
- **Summary**: In this work we present a method to train a plane-aware convolutional neural network for dense depth and surface normal estimation as well as plane boundaries from a single indoor $360^\circ$ image. Using our proposed loss function, our network outperforms existing methods for single-view, indoor, omnidirectional depth estimation and provides an initial benchmark for surface normal prediction from $360^\circ$ images. Our improvements are due to the use of a novel plane-aware loss that leverages principal curvature as an indicator of planar boundaries. We also show that including geodesic coordinate maps as network priors provides a significant boost in surface normal prediction accuracy. Finally, we demonstrate how we can combine our network's outputs to generate high quality 3D "pop-up" models of indoor scenes.



### Estimating brain age based on a healthy population with deep learning and structural MRI
- **Arxiv ID**: http://arxiv.org/abs/1907.00943v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1907.00943v1)
- **Published**: 2019-07-01 17:30:18+00:00
- **Updated**: 2019-07-01 17:30:18+00:00
- **Authors**: Xinyang Feng, Zachary C. Lipton, Jie Yang, Scott A. Small, Frank A. Provenzano
- **Comment**: 32 pages, 9 figures, 6 tables
- **Journal**: None
- **Summary**: Numerous studies have established that estimated brain age, as derived from statistical models trained on healthy populations, constitutes a valuable biomarker that is predictive of cognitive decline and various neurological diseases. In this work, we curate a large-scale heterogeneous dataset (N = 10,158, age range 18 - 97) of structural brain MRIs in a healthy population from multiple publicly-available sources, upon which we train a deep learning model for brain age estimation. The availability of the large-scale dataset enables a more uniform age distribution across adult life-span for effective age estimation with no bias toward certain age groups. We demonstrate that the age estimation accuracy, evaluated with mean absolute error (MAE) and correlation coefficient (r), outperforms previously reported methods in both a hold-out test set reflective of the custom population (MAE = 4.06 years, r = 0.970) and an independent life-span evaluation dataset (MAE = 4.21 years, r = 0.960) on which a previous study has evaluated. We further demonstrate the utility of the estimated age in life-span aging analysis of cognitive functions. Furthermore, we conduct extensive ablation tests and employ feature-attribution techniques to analyze which regions contribute the most predictive value, demonstrating the prominence of the frontal lobe as well as pattern shift across life-span. In summary, we achieve superior age estimation performance confirming the efficacy of deep learning and the added utility of training with data both in larger number and more uniformly distributed than in previous studies. We demonstrate the regional contribution to our brain age predictions through multiple routes and confirm the association of divergence between estimated and chronological brain age with neuropsychological measures.



### ICDAR2019 Robust Reading Challenge on Multi-lingual Scene Text Detection and Recognition -- RRC-MLT-2019
- **Arxiv ID**: http://arxiv.org/abs/1907.00945v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.00945v1)
- **Published**: 2019-07-01 17:30:31+00:00
- **Updated**: 2019-07-01 17:30:31+00:00
- **Authors**: Nibal Nayef, Yash Patel, Michal Busta, Pinaki Nath Chowdhury, Dimosthenis Karatzas, Wafa Khlif, Jiri Matas, Umapada Pal, Jean-Christophe Burie, Cheng-lin Liu, Jean-Marc Ogier
- **Comment**: ICDAR'19 camera-ready version. Competition available at
  https://rrc.cvc.uab.es/?ch=15. The first two authors contributed equally
- **Journal**: None
- **Summary**: With the growing cosmopolitan culture of modern cities, the need of robust Multi-Lingual scene Text (MLT) detection and recognition systems has never been more immense. With the goal to systematically benchmark and push the state-of-the-art forward, the proposed competition builds on top of the RRC-MLT-2017 with an additional end-to-end task, an additional language in the real images dataset, a large scale multi-lingual synthetic dataset to assist the training, and a baseline End-to-End recognition method. The real dataset consists of 20,000 images containing text from 10 languages. The challenge has 4 tasks covering various aspects of multi-lingual scene text: (a) text detection, (b) cropped word script classification, (c) joint text detection and script classification and (d) end-to-end detection and recognition. In total, the competition received 60 submissions from the research and industrial communities. This paper presents the dataset, the tasks and the findings of the presented RRC-MLT-2019 challenge.



### Bayesian Optimization on Large Graphs via a Graph Convolutional Generative Model: Application in Cardiac Model Personalization
- **Arxiv ID**: http://arxiv.org/abs/1907.01406v2
- **DOI**: 10.1007/978-3-030-32245-8_51
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.01406v2)
- **Published**: 2019-07-01 17:47:21+00:00
- **Updated**: 2020-05-18 16:01:35+00:00
- **Authors**: Jwala Dhamala, Sandesh Ghimire, John L. Sapp, B. Milan Horacek, Linwei Wang
- **Comment**: 9 pages, 5 figures, MICCAI
- **Journal**: None
- **Summary**: Personalization of cardiac models involves the optimization of organ tissue properties that vary spatially over the non-Euclidean geometry model of the heart. To represent the high-dimensional (HD) unknown of tissue properties, most existing works rely on a low-dimensional (LD) partitioning of the geometrical model. While this exploits the geometry of the heart, it is of limited expressiveness to allow partitioning that is small enough for effective optimization. Recently, a variational auto-encoder (VAE) was utilized as a more expressive generative model to embed the HD optimization into the LD latent space. Its Euclidean nature, however, neglects the rich geometrical information in the heart. In this paper, we present a novel graph convolutional VAE to allow generative modeling of non-Euclidean data, and utilize it to embed Bayesian optimization of large graphs into a small latent space. This approach bridges the gap of previous works by introducing an expressive generative model that is able to incorporate the knowledge of spatial proximity and hierarchical compositionality of the underlying geometry. It further allows transferring of the learned features across different geometries, which was not possible with a regular VAE. We demonstrate these benefits of the presented method in synthetic and real data experiments of estimating tissue excitability in a cardiac electrophysiological model.



### Single-Path Mobile AutoML: Efficient ConvNet Design and NAS Hyperparameter Optimization
- **Arxiv ID**: http://arxiv.org/abs/1907.00959v1
- **DOI**: 10.1109/JSTSP.2020.2971421
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.00959v1)
- **Published**: 2019-07-01 17:52:55+00:00
- **Updated**: 2019-07-01 17:52:55+00:00
- **Authors**: Dimitrios Stamoulis, Ruizhou Ding, Di Wang, Dimitrios Lymberopoulos, Bodhi Priyantha, Jie Liu, Diana Marculescu
- **Comment**: Detailed extension (journal) of the Single-Path NAS ECMLPKDD'19 paper
  (arXiv:1904.02877)
- **Journal**: None
- **Summary**: Can we reduce the search cost of Neural Architecture Search (NAS) from days down to only few hours? NAS methods automate the design of Convolutional Networks (ConvNets) under hardware constraints and they have emerged as key components of AutoML frameworks. However, the NAS problem remains challenging due to the combinatorially large design space and the significant search time (at least 200 GPU-hours). In this work, we alleviate the NAS search cost down to less than 3 hours, while achieving state-of-the-art image classification results under mobile latency constraints. We propose a novel differentiable NAS formulation, namely Single-Path NAS, that uses one single-path over-parameterized ConvNet to encode all architectural decisions based on shared convolutional kernel parameters, hence drastically decreasing the search overhead. Single-Path NAS achieves state-of-the-art top-1 ImageNet accuracy (75.62%), hence outperforming existing mobile NAS methods in similar latency settings (~80ms). In particular, we enhance the accuracy-runtime trade-off in differentiable NAS by treating the Squeeze-and-Excitation path as a fully searchable operation with our novel single-path encoding. Our method has an overall cost of only 8 epochs (24 TPU-hours), which is up to 5,000x faster compared to prior work. Moreover, we study how different NAS formulation choices affect the performance of the designed ConvNets. Furthermore, we exploit the efficiency of our method to answer an interesting question: instead of empirically tuning the hyperparameters of the NAS solver (as in prior work), can we automatically find the hyperparameter values that yield the desired accuracy-runtime trade-off? We open-source our entire codebase at: https://github.com/dstamoulis/single-path-nas.



### Going Deeper with Lean Point Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.00960v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, 68T45, I.2.10; I.3.0; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1907.00960v2)
- **Published**: 2019-07-01 17:53:20+00:00
- **Updated**: 2020-06-16 17:01:43+00:00
- **Authors**: Eric-Tuan Le, Iasonas Kokkinos, Niloy J. Mitra
- **Comment**: 16 pages, 11 figures, 9 tables
- **Journal**: None
- **Summary**: In this work we introduce Lean Point Networks (LPNs) to train deeper and more accurate point processing networks by relying on three novel point processing blocks that improve memory consumption, inference time, and accuracy: a convolution-type block for point sets that blends neighborhood information in a memory-efficient manner; a crosslink block that efficiently shares information across low- and high-resolution processing branches; and a multiresolution point cloud processing block for faster diffusion of information. By combining these blocks, we design wider and deeper point-based architectures. We report systematic accuracy and memory consumption improvements on multiple publicly available segmentation tasks by using our generic modules as drop-in replacements for the blocks of multiple architectures (PointNet++, DGCNN, SpiderNet, PointCNN).



### Accurate, reliable and fast robustness evaluation
- **Arxiv ID**: http://arxiv.org/abs/1907.01003v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CR, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1907.01003v2)
- **Published**: 2019-07-01 18:18:10+00:00
- **Updated**: 2019-12-12 18:32:51+00:00
- **Authors**: Wieland Brendel, Jonas Rauber, Matthias Kümmerer, Ivan Ustyuzhaninov, Matthias Bethge
- **Comment**: Accepted at the 2019 Conference on Neural Information Processing
  Systems
- **Journal**: None
- **Summary**: Throughout the past five years, the susceptibility of neural networks to minimal adversarial perturbations has moved from a peculiar phenomenon to a core issue in Deep Learning. Despite much attention, however, progress towards more robust models is significantly impaired by the difficulty of evaluating the robustness of neural network models. Today's methods are either fast but brittle (gradient-based attacks), or they are fairly reliable but slow (score- and decision-based attacks). We here develop a new set of gradient-based adversarial attacks which (a) are more reliable in the face of gradient-masking than other gradient-based attacks, (b) perform better and are more query efficient than current state-of-the-art gradient-based attacks, (c) can be flexibly adapted to a wide range of adversarial criteria and (d) require virtually no hyperparameter tuning. These findings are carefully validated across a diverse set of six different models and hold for L0, L1, L2 and Linf in both targeted as well as untargeted scenarios. Implementations will soon be available in all major toolboxes (Foolbox, CleverHans and ART). We hope that this class of attacks will make robustness evaluations easier and more reliable, thus contributing to more signal in the search for more robust machine learning models.



### Symmetry Detection and Classification in Drawings of Graphs
- **Arxiv ID**: http://arxiv.org/abs/1907.01004v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.01004v3)
- **Published**: 2019-07-01 18:20:03+00:00
- **Updated**: 2019-08-27 03:55:14+00:00
- **Authors**: Felice De Luca, Md Iqbal Hossain, Stephen Kobourov
- **Comment**: Appears in the Proceedings of the 27th International Symposium on
  Graph Drawing and Network Visualization (GD 2019)
- **Journal**: None
- **Summary**: Symmetry is a key feature observed in nature (from flowers and leaves, to butterflies and birds) and in human-made objects (from paintings and sculptures, to manufactured objects and architectural design). Rotational, translational, and especially reflectional symmetries, are also important in drawings of graphs. Detecting and classifying symmetries can be very useful in algorithms that aim to create symmetric graph drawings and in this paper we present a machine learning approach for these tasks. Specifically, we show that deep neural networks can be used to detect reflectional symmetries with 92% accuracy. We also build a multi-class classifier to distinguish between reflectional horizontal, reflectional vertical, rotational, and translational symmetries. Finally, we make available a collection of images of graph drawings with specific symmetric features that can be used in machine learning systems for training, testing and validation purposes. Our datasets, best trained ML models, source code are available online.



### Diminishing the Effect of Adversarial Perturbations via Refining Feature Representation
- **Arxiv ID**: http://arxiv.org/abs/1907.01023v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.01023v2)
- **Published**: 2019-07-01 19:21:22+00:00
- **Updated**: 2019-10-01 17:43:49+00:00
- **Authors**: Nader Asadi, AmirMohammad Sarfi, Mehrdad Hosseinzadeh, Sahba Tahsini, Mahdi Eftekhari
- **Comment**: Accepted at NeuralIPS 2019 workshop on Safety and Robustness in
  Decision Making
- **Journal**: None
- **Summary**: Deep neural networks are highly vulnerable to adversarial examples, which imposes severe security issues for these state-of-the-art models. Many defense methods have been proposed to mitigate this problem. However, a lot of them depend on modification or additional training of the target model. In this work, we analytically investigate each layer's representation of non-perturbed and perturbed images and show the effect of perturbations on each of these representations. Accordingly, a method based on whitening coloring transform is proposed in order to diminish the misrepresentation of any desirable layer caused by adversaries. Our method can be applied to any layer of any arbitrary model without the need of any modification or additional training. Due to the fact that the full whitening of the layer's representation is not easily differentiable, our proposed method is superbly robust against white-box attacks. Furthermore, we demonstrate the strength of our method against some state-of-the-art black-box attacks.



### Learning to aggregate feature representations
- **Arxiv ID**: http://arxiv.org/abs/1907.01034v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1907.01034v3)
- **Published**: 2019-07-01 19:35:17+00:00
- **Updated**: 2019-07-04 06:34:12+00:00
- **Authors**: Guy Gaziv
- **Comment**: Report for Algonauts2019 challenge. 4 pages, 2 figures
- **Journal**: None
- **Summary**: The Algonauts challenge requires to construct a multi-subject encoder of images to brain activity. Deep networks such as ResNet-50 and AlexNet trained for image classification are known to produce feature representations along their intermediate stages which closely mimic the visual hierarchy. However the challenges introduced in the Algonauts project, including combining data from multiple subjects, relying on very few similarity data points, solving for various ROIs, and multi-modality, require devising a flexible framework which can efficiently accommodate them. Here we build upon a recent state-of-the-art classification network (SE-ResNeXt-50) and construct an adaptive combination of its intermediate representations. While the pretrained network serves as a backbone of our model, we learn how to aggregate feature representations along five stages of the network. During learning, our method enables to modulate and screen outputs from each stage along the network as governed by the optimized objective. We applied our method to the Algonauts2019 fMRI and MEG challenges. Using the combined fMRI and MEG data, our approach was rated among the leading five for both challenges. Surprisingly we find that for both the lower and higher order areas (EVC and IT) the adaptive aggregation favors features stemming at later stages of the network.



### Associative Embedding for Game-Agnostic Team Discrimination
- **Arxiv ID**: http://arxiv.org/abs/1907.01058v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01058v1)
- **Published**: 2019-07-01 20:20:12+00:00
- **Updated**: 2019-07-01 20:20:12+00:00
- **Authors**: Maxime Istasse, Julien Moreau, Christophe De Vleeschouwer
- **Comment**: Published in CVPR 2019 workshop Computer Vision in Sports, under the
  name "Associative Embedding for Team Discrimination"
  (http://openaccess.thecvf.com/content_CVPRW_2019/html/CVSports/Istasse_Associative_Embedding_for_Team_Discrimination_CVPRW_2019_paper.html)
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR) Workshops, 2019
- **Summary**: Assigning team labels to players in a sport game is not a trivial task when no prior is known about the visual appearance of each team. Our work builds on a Convolutional Neural Network (CNN) to learn a descriptor, namely a pixel-wise embedding vector, that is similar for pixels depicting players from the same team, and dissimilar when pixels correspond to distinct teams. The advantage of this idea is that no per-game learning is needed, allowing efficient team discrimination as soon as the game starts. In principle, the approach follows the associative embedding framework introduced in arXiv:1611.05424 to differentiate instances of objects. Our work is however different in that it derives the embeddings from a lightweight segmentation network and, more fundamentally, because it considers the assignment of the same embedding to unconnected pixels, as required by pixels of distinct players from the same team. Excellent results, both in terms of team labelling accuracy and generalization to new games/arenas, have been achieved on panoramic views of a large variety of basketball games involving players interactions and occlusions. This makes our method a good candidate to integrate team separation in many CNN-based sport analytics pipelines.



### DeepTEGINN: Deep Learning Based Tools to Extract Graphs from Images of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.01062v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1907.01062v1)
- **Published**: 2019-07-01 20:33:08+00:00
- **Updated**: 2019-07-01 20:33:08+00:00
- **Authors**: Gustavo Borges Moreno e Mello, Vibeke Devold Valderhaug, Sidney Pontes-Filho, Evi Zouganeli, Ioanna Sandvig, Stefano Nichele
- **Comment**: 5 pages, 2 figures, ICDL IEEE conference 2019
- **Journal**: None
- **Summary**: In the brain, the structure of a network of neurons defines how these neurons implement the computations that underlie the mind and the behavior of animals and humans. Provided that we can describe the network of neurons as a graph, we can employ methods from graph theory to investigate its structure or use cellular automata to mathematically assess its function. Although, software for the analysis of graphs and cellular automata are widely available. Graph extraction from the image of networks of brain cells remains difficult. Nervous tissue is heterogeneous, and differences in anatomy may reflect relevant differences in function. Here we introduce a deep learning based toolbox to extracts graphs from images of brain tissue. This toolbox provides an easy-to-use framework allowing system neuroscientists to generate graphs based on images of brain tissue by combining methods from image processing, deep learning, and graph theory. The goals are to simplify the training and usage of deep learning methods for computer vision and facilitate its integration into graph extraction pipelines. In this way, the toolbox provides an alternative to the required laborious manual process of tracing, sorting and classifying. We expect to democratize the machine learning methods to a wider community of users beyond the computer vision experts and improve the time-efficiency of graph extraction from large brain image datasets, which may lead to further understanding of the human mind.



### Multiview Aggregation for Learning Category-Specific Shape Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1907.01085v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.01085v2)
- **Published**: 2019-07-01 22:01:37+00:00
- **Updated**: 2019-12-08 22:01:21+00:00
- **Authors**: Srinath Sridhar, Davis Rempe, Julien Valentin, Sofien Bouaziz, Leonidas J. Guibas
- **Comment**: In Proceedings of Advances in Neural Information Processing Systems
  (NeurIPS) 2019
- **Journal**: None
- **Summary**: We investigate the problem of learning category-specific 3D shape reconstruction from a variable number of RGB views of previously unobserved object instances. Most approaches for multiview shape reconstruction operate on sparse shape representations, or assume a fixed number of views. We present a method that can estimate dense 3D shape, and aggregate shape across multiple and varying number of input views. Given a single input view of an object instance, we propose a representation that encodes the dense shape of the visible object surface as well as the surface behind line of sight occluded by the visible surface. When multiple input views are available, the shape representation is designed to be aggregated into a single 3D shape using an inexpensive union operation. We train a 2D CNN to learn to predict this representation from a variable number of views (1 or more). We further aggregate multiview information by using permutation equivariant layers that promote order-agnostic view information exchange at the feature level. Experiments show that our approach is able to produce dense 3D reconstructions of objects that improve in quality as more views are added.



