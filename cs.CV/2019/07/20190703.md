# Arxiv Papers in cs.CV on 2019-07-03
### A Semi-Supervised Framework for Automatic Pixel-Wise Breast Cancer Grading of Histological Images
- **Arxiv ID**: http://arxiv.org/abs/1907.01696v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01696v2)
- **Published**: 2019-07-03 01:30:06+00:00
- **Updated**: 2022-03-08 06:53:31+00:00
- **Authors**: Yanyuet Man, Xiangyun Ding, Xingcheng Yao, Han Bao
- **Comment**: The author list and contents of this paper is not complete. Other
  authors request to withdraw this paper
- **Journal**: None
- **Summary**: Throughout the world, breast cancer is one of the leading causes of female death. Recently, deep learning methods are developed to automatically grade breast cancer of histological slides. However, the performance of existing deep learning models is limited due to the lack of large annotated biomedical datasets. One promising way to relieve the annotating burden is to leverage the unannotated datasets to enhance the trained model. In this paper, we first apply active learning method in breast cancer grading, and propose a semi-supervised framework based on expectation maximization (EM) model. The proposed EM approach is based on the collaborative filtering among the annotated and unannotated datasets. The collaborative filtering method effectively extracts useful and credible datasets from the unannotated images. Results of pixel-wise prediction of whole-slide images (WSI) demonstrate that the proposed method not only outperforms state-of-art methods, but also significantly reduces the annotation cost by over 70%.



### Compositional Structure Learning for Sequential Video Data
- **Arxiv ID**: http://arxiv.org/abs/1907.01709v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.01709v1)
- **Published**: 2019-07-03 02:28:48+00:00
- **Updated**: 2019-07-03 02:28:48+00:00
- **Authors**: Kyoung-Woon On, Eun-Sol Kim, Yu-Jung Heo, Byoung-Tak Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Conventional sequential learning methods such as Recurrent Neural Networks (RNNs) focus on interactions between consecutive inputs, i.e. first-order Markovian dependency. However, most of sequential data, as seen with videos, have complex temporal dependencies that imply variable-length semantic flows and their compositions, and those are hard to be captured by conventional methods. Here, we propose Temporal Dependency Networks (TDNs) for learning video data by discovering these complex structures of the videos. The TDNs represent video as a graph whose nodes and edges correspond to frames of the video and their dependencies respectively. Via a parameterized kernel with graph-cut and graph convolutions, the TDNs find compositional temporal dependencies of the data in multilevel graph forms. We evaluate the proposed method on the large-scale video dataset Youtube-8M. The experimental results show that our model efficiently learns the complex semantic structure of video data.



### Mask Embedding in conditional GAN for Guided Synthesis of High Resolution Images
- **Arxiv ID**: http://arxiv.org/abs/1907.01710v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.01710v1)
- **Published**: 2019-07-03 02:30:36+00:00
- **Updated**: 2019-07-03 02:30:36+00:00
- **Authors**: Yinhao Ren, Zhe Zhu, Yingzhou Li, Joseph Lo
- **Comment**: 11 pages,6 figures
- **Journal**: None
- **Summary**: Recent advancements in conditional Generative Adversarial Networks (cGANs) have shown promises in label guided image synthesis. Semantic masks, such as sketches and label maps, are another intuitive and effective form of guidance in image synthesis. Directly incorporating the semantic masks as constraints dramatically reduces the variability and quality of the synthesized results. We observe this is caused by the incompatibility of features from different inputs (such as mask image and latent vector) of the generator. To use semantic masks as guidance whilst providing realistic synthesized results with fine details, we propose to use mask embedding mechanism to allow for a more efficient initial feature projection in the generator. We validate the effectiveness of our approach by training a mask guided face generator using CELEBA-HQ dataset. We can generate realistic and high resolution facial images up to the resolution of 512*512 with a mask guidance. Our code is publicly available.



### A Deep Image Compression Framework for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.01714v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01714v1)
- **Published**: 2019-07-03 02:44:29+00:00
- **Updated**: 2019-07-03 02:44:29+00:00
- **Authors**: Nai Bian, Feng Liang, Haisheng Fu, Bo Lei
- **Comment**: None
- **Journal**: None
- **Summary**: Face recognition technology has advanced rapidly and has been widely used in various applications. Due to the extremely huge amount of data of face images and the large computing resources required correspondingly in large-scale face recognition tasks, there is a requirement for a face image compression approach that is highly suitable for face recognition tasks. In this paper, we propose a deep convolutional autoencoder compression network for face recognition tasks. In the compression process, deep features are extracted from the original image by the convolutional neural networks to produce a compact representation of the original image, which is then encoded and saved by existing codec such as PNG. This compact representation is utilized by the reconstruction network to generate a reconstructed image of the original one. In order to improve the face recognition accuracy when the compression framework is used in a face recognition system, we combine this compression framework with a existing face recognition network for joint optimization. We test the proposed scheme and find that after joint training, the Labeled Faces in the Wild (LFW) dataset compressed by our compression framework has higher face verification accuracy than that compressed by JPEG2000, and is much higher than that compressed by JPEG.



### Unsupervised Anomalous Trajectory Detection for Crowded Scenes
- **Arxiv ID**: http://arxiv.org/abs/1907.01717v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.01717v1)
- **Published**: 2019-07-03 02:56:47+00:00
- **Updated**: 2019-07-03 02:56:47+00:00
- **Authors**: Deepan Das, Deepak Mishra
- **Comment**: None
- **Journal**: None
- **Summary**: We present an improved clustering based, unsupervised anomalous trajectory detection algorithm for crowded scenes. The proposed work is based on four major steps, namely, extraction of trajectories from crowded scene video, extraction of several features from these trajectories, independent mean-shift clustering and anomaly detection. First, the trajectories of all moving objects in a crowd are extracted using a multi feature video object tracker. These trajectories are then transformed into a set of feature spaces. Mean shift clustering is applied on these feature matrices to obtain distinct clusters, while a Shannon Entropy based anomaly detector identifies corresponding anomalies. In the final step, a voting mechanism identifies the trajectories that exhibit anomalous characteristics. The algorithm is tested on crowd scene videos from datasets. The videos represent various possible crowd scenes with different motion patterns and the method performs well to detect the expected anomalous trajectories from the scene.



### Deep Attentive Features for Prostate Segmentation in 3D Transrectal Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/1907.01743v1
- **DOI**: 10.1109/TMI.2019.2913184
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.01743v1)
- **Published**: 2019-07-03 05:21:52+00:00
- **Updated**: 2019-07-03 05:21:52+00:00
- **Authors**: Yi Wang, Haoran Dou, Xiaowei Hu, Lei Zhu, Xin Yang, Ming Xu, Jing Qin, Pheng-Ann Heng, Tianfu Wang, Dong Ni
- **Comment**: 11 pages, 10 figures, 2 tables. Accepted by IEEE transactions on
  Medical Imaging
- **Journal**: None
- **Summary**: Automatic prostate segmentation in transrectal ultrasound (TRUS) images is of essential importance for image-guided prostate interventions and treatment planning. However, developing such automatic solutions remains very challenging due to the missing/ambiguous boundary and inhomogeneous intensity distribution of the prostate in TRUS, as well as the large variability in prostate shapes. This paper develops a novel 3D deep neural network equipped with attention modules for better prostate segmentation in TRUS by fully exploiting the complementary information encoded in different layers of the convolutional neural network (CNN). Our attention module utilizes the attention mechanism to selectively leverage the multilevel features integrated from different layers to refine the features at each individual layer, suppressing the non-prostate noise at shallow layers of the CNN and increasing more prostate details into features at deep layers. Experimental results on challenging 3D TRUS volumes show that our method attains satisfactory segmentation performance. The proposed attention mechanism is a general strategy to aggregate multi-level deep features and has the potential to be used for other medical image segmentation tasks. The code is publicly available at https://github.com/wulalago/DAF3D.



### Region-Manipulated Fusion Networks for Pancreatitis Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.01744v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.01744v1)
- **Published**: 2019-07-03 05:37:04+00:00
- **Updated**: 2019-07-03 05:37:04+00:00
- **Authors**: Jian Wang, Xiaoyao Li, Xiangbo Shu, Weiqin Li
- **Comment**: None
- **Journal**: None
- **Summary**: This work first attempts to automatically recognize pancreatitis on CT scan images. However, different form the traditional object recognition, such pancreatitis recognition is challenging due to the fine-grained and non-rigid appearance variability of the local diseased regions. To this end, we propose a customized Region-Manipulated Fusion Networks (RMFN) to capture the key characteristics of local lesion for pancreatitis recognition. Specifically, to effectively highlight the imperceptible lesion regions, a novel region-manipulated scheme in RMFN is proposed to force the lesion regions while weaken the non-lesion regions by ceaselessly aggregating the multi-scale local information onto feature maps. The proposed scheme can be flexibly equipped into the existing neural networks, such as AlexNet and VGG. To evaluate the performance of the propose method, a real CT image database about pancreatitis is collected from hospitals \footnote{The database is available later}. And experimental results on such database well demonstrate the effectiveness of the proposed method for pancreatitis recognition.



### Attention routing between capsules
- **Arxiv ID**: http://arxiv.org/abs/1907.01750v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01750v4)
- **Published**: 2019-07-03 06:01:16+00:00
- **Updated**: 2019-11-13 08:02:00+00:00
- **Authors**: Jaewoong Choi, Hyun Seo, Suii Im, Myungjoo Kang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a new capsule network architecture called Attention Routing CapsuleNet (AR CapsNet). We replace the dynamic routing and squash activation function of the capsule network with dynamic routing (CapsuleNet) with the attention routing and capsule activation. The attention routing is a routing between capsules through an attention module. The attention routing is a fast forward-pass while keeping spatial information. On the other hand, the intuitive interpretation of the dynamic routing is finding a centroid of the prediction capsules. Thus, the squash activation function and its variant focus on preserving a vector orientation. However, the capsule activation focuses on performing a capsule-scale activation function.   We evaluate our proposed model on the MNIST, affNIST, and CIFAR-10 classification tasks. The proposed model achieves higher accuracy with fewer parameters (x0.65 in the MNIST, x0.82 in the CIFAR-10) and less training time than CapsuleNet (x0.19 in the MNIST, x0.35 in the CIFAR-10). These results validate that designing a capsule-scale operation is a key factor to implement the capsule concept.   Also, our experiment shows that our proposed model is transformation equivariant as CapsuleNet. As we perturb each element of the output capsule, the decoder attached to the output capsules shows global variations. Further experiments show that the difference in the capsule features caused by applying affine transformations on an input image is significantly aligned in one direction.



### Calibration of fisheye camera using entrance pupil
- **Arxiv ID**: http://arxiv.org/abs/1907.01759v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.01759v1)
- **Published**: 2019-07-03 06:45:02+00:00
- **Updated**: 2019-07-03 06:45:02+00:00
- **Authors**: Peter Fasogbon, Emre Aksu
- **Comment**: 5 pages, 4 figures, Accepted for publication at ICIP 2019
- **Journal**: None
- **Summary**: Most conventional camera calibration algorithms assume that the imaging device has a Single Viewpoint (SVP). This is not necessarily true for special imaging device such as fisheye lenses. As a consequence, the intrinsic camera calibration result is not always reliable. In this paper, we propose a new formation model that tends to relax this assumption so that a Non-Single Viewpoint (NSVP) system is corrected to always maintain a SVP, by taking into account the variation of the Entrance Pupil (EP) using thin lens modeling. In addition, we present a calibration procedure for the image formation to estimate these EP parameters using non linear optimization procedure with bundle adjustment. From experiments, we are able to obtain slightly better re-projection error than traditional methods, and the camera parameters are better estimated. The proposed calibration procedure is simple and can easily be integrated to any other thin lens image formation model.



### Accelerating Generative Neural Networks on Unmodified Deep Learning Processors -- A Software Approach
- **Arxiv ID**: http://arxiv.org/abs/1907.01773v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DS, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1907.01773v3)
- **Published**: 2019-07-03 07:18:57+00:00
- **Updated**: 2020-04-29 02:50:01+00:00
- **Authors**: Dawen Xu, Ying Wang, Kaijie Tu, Cheng Liu, Bingsheng He, Lei Zhang
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Generative neural network is a new category of neural networks and it has been widely utilized in applications such as content generation, unsupervised learning, segmentation and pose estimation. It typically involves massive computing-intensive deconvolution operations that cannot be fitted to conventional neural network processors directly. However, prior works mainly investigated specialized hardware architectures through intensive hardware modifications to the existing deep learning processors to accelerate deconvolution together with the convolution. In contrast, this work proposes a novel deconvolution implementation with a software approach and enables fast and efficient deconvolution execution on the legacy deep learning processors. Our proposed method reorganizes the computation of deconvolution and allows the deep learning processors to treat it as the standard convolution by splitting the original deconvolution filters into multiple small filters. Compared to prior acceleration schemes, the implemented acceleration scheme achieves 2.41x - 4.34x performance speedup and reduces the energy consumption by 27.7% - 54.5% on a set of realistic benchmarks. In addition, we also applied the deconvolution computing approach to the off-the-shelf commodity deep learning processors. The performance of deconvolution also exhibits significant performance speedup over prior deconvolution implementations.



### Image Super-Resolution Using Attention Based DenseNet with Residual Deconvolution
- **Arxiv ID**: http://arxiv.org/abs/1907.05282v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.05282v1)
- **Published**: 2019-07-03 08:28:13+00:00
- **Updated**: 2019-07-03 08:28:13+00:00
- **Authors**: Zhuangzi Li
- **Comment**: None
- **Journal**: None
- **Summary**: Image super-resolution is a challenging task and has attracted increasing attention in research and industrial communities. In this paper, we propose a novel end-to-end Attention-based DenseNet with Residual Deconvolution named as ADRD. In our ADRD, a weighted dense block, in which the current layer receives weighted features from all previous levels, is proposed to capture valuable features rely in dense layers adaptively. And a novel spatial attention module is presented to generate a group of attentive maps for emphasizing informative regions. In addition, we design an innovative strategy to upsample residual information via the deconvolution layer, so that the high-frequency details can be accurately upsampled. Extensive experiments conducted on publicly available datasets demonstrate the promising performance of the proposed ADRD against the state-of-the-arts, both quantitatively and qualitatively.



### Benchmarking unsupervised near-duplicate image detection
- **Arxiv ID**: http://arxiv.org/abs/1907.02821v1
- **DOI**: 10.1016/j.eswa.2019.05.002
- **Categories**: **cs.CV**, cs.CR, cs.LG, cs.MM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.02821v1)
- **Published**: 2019-07-03 09:08:47+00:00
- **Updated**: 2019-07-03 09:08:47+00:00
- **Authors**: Lia Morra, Fabrizio Lamberti
- **Comment**: Accepted for publication in Expert Systems with Applications
- **Journal**: Expert Systems with Applications, online first, 2019
- **Summary**: Unsupervised near-duplicate detection has many practical applications ranging from social media analysis and web-scale retrieval, to digital image forensics. It entails running a threshold-limited query on a set of descriptors extracted from the images, with the goal of identifying all possible near-duplicates, while limiting the false positives due to visually similar images. Since the rate of false alarms grows with the dataset size, a very high specificity is thus required, up to $1 - 10^{-9}$ for realistic use cases; this important requirement, however, is often overlooked in literature. In recent years, descriptors based on deep convolutional neural networks have matched or surpassed traditional feature extraction methods in content-based image retrieval tasks. To the best of our knowledge, ours is the first attempt to establish the performance range of deep learning-based descriptors for unsupervised near-duplicate detection on a range of datasets, encompassing a broad spectrum of near-duplicate definitions. We leverage both established and new benchmarks, such as the Mir-Flick Near-Duplicate (MFND) dataset, in which a known ground truth is provided for all possible pairs over a general, large scale image collection. To compare the specificity of different descriptors, we reduce the problem of unsupervised detection to that of binary classification of near-duplicate vs. not-near-duplicate images. The latter can be conveniently characterized using Receiver Operating Curve (ROC). Our findings in general favor the choice of fine-tuning deep convolutional networks, as opposed to using off-the-shelf features, but differences at high specificity settings depend on the dataset and are often small. The best performance was observed on the MFND benchmark, achieving 96\% sensitivity at a false positive rate of $1.43 \times 10^{-6}$.



### Tracking system of Mine Patrol Robot for Low Illumination Environment
- **Arxiv ID**: http://arxiv.org/abs/1907.01806v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01806v3)
- **Published**: 2019-07-03 09:14:37+00:00
- **Updated**: 2019-10-22 03:07:51+00:00
- **Authors**: Shaoze You, Hua Zhu, Menggang Li, Lei Wang, Chaoquan Tang
- **Comment**: 13 pages, 7 figures, 1 table, 27 conference
- **Journal**: None
- **Summary**: Computer vision has received a significant attention in recent years, which is one of the important parts for robots to apperceive external environment. Discriminative Correlation Filter (DCF) based trackers gained more popularity due to their efficiency, however, tracking in low-illumination environments is a challenging problem, not yet successfully addressed in the literature. In this work, we tackle the problems by introducing Low-Illumination Long-term Correlation Tracker (LLCT). First, fused features only including HOG and Color Names are employed to boost the tracking efficiency. Second, we used the standard PCA to reduction scheme in the translation and scale estimation phase for accelerating. Third, we learned a long-term correlation filter to keep the long-term memory ability. Finally, update memory templates with interval updates, then re-match existing and initial templates every few frames to maintain template accuracy. The extensive experiments on popular Object Tracking Benchmark OTB-50 datasets have demonstrated that the proposed tracker outperforms the state-of-the-art trackers significantly achieves a high real-time (33FPS) performance. In addition, the proposed approach can be integrated easily in robot system and the running speed performed well. The experimental results show that the novel tracker performance in low-illumination environment is better than that of general trackers.



### Super-Resolution of PROBA-V Images Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.01821v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.01821v1)
- **Published**: 2019-07-03 09:53:05+00:00
- **Updated**: 2019-07-03 09:53:05+00:00
- **Authors**: Marcus Märtens, Dario Izzo, Andrej Krzic, Daniël Cox
- **Comment**: To appear in Special Issue on Applications of Artificial Intelligence
  in Aerospace Engineering in the Journal "Astrodynamics"
- **Journal**: None
- **Summary**: ESA's PROBA-V Earth observation satellite enables us to monitor our planet at a large scale, studying the interaction between vegetation and climate and provides guidance for important decisions on our common global future. However, the interval at which high resolution images are recorded spans over several days, in contrast to the availability of lower resolution images which is often daily. We collect an extensive dataset of both, high and low resolution images taken by PROBA-V instruments during monthly periods to investigate Multi Image Super-resolution, a technique to merge several low resolution images to one image of higher quality. We propose a convolutional neural network that is able to cope with changes in illumination, cloud coverage and landscape features which are challenges introduced by the fact that the different images are taken over successive satellite passages over the same region. Given a bicubic upscaling of low resolution images taken under optimal conditions, we find the Peak Signal to Noise Ratio of the reconstructed image of the network to be higher for a large majority of different scenes. This shows that applied machine learning has the potential to enhance large amounts of previously collected earth observation data during multiple satellite passes.



### Cascade Attention Guided Residue Learning GAN for Cross-Modal Translation
- **Arxiv ID**: http://arxiv.org/abs/1907.01826v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01826v3)
- **Published**: 2019-07-03 10:04:54+00:00
- **Updated**: 2021-12-10 18:52:25+00:00
- **Authors**: Bin Duan, Wei Wang, Hao Tang, Hugo Latapie, Yan Yan
- **Comment**: 9 pages, 6 figures, update template
- **Journal**: None
- **Summary**: Since we were babies, we intuitively develop the ability to correlate the input from different cognitive sensors such as vision, audio, and text. However, in machine learning, this cross-modal learning is a nontrivial task because different modalities have no homogeneous properties. Previous works discover that there should be bridges among different modalities. From neurology and psychology perspective, humans have the capacity to link one modality with another one, e.g., associating a picture of a bird with the only hearing of its singing and vice versa. Is it possible for machine learning algorithms to recover the scene given the audio signal? In this paper, we propose a novel Cascade Attention-Guided Residue GAN (CAR-GAN), aiming at reconstructing the scenes given the corresponding audio signals. Particularly, we present a residue module to mitigate the gap between different modalities progressively. Moreover, a cascade attention guided network with a novel classification loss function is designed to tackle the cross-modal learning task. Our model keeps the consistency in high-level semantic label domain and is able to balance two different modalities. The experimental results demonstrate that our model achieves the state-of-the-art cross-modal audio-visual generation on the challenging Sub-URMP dataset. Code will be available at https://github.com/tuffr5/CAR-GAN.



### Intrinsic Calibration of Depth Cameras for Mobile Robots using a Radial Laser Scanner
- **Arxiv ID**: http://arxiv.org/abs/1907.01839v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.01839v1)
- **Published**: 2019-07-03 10:35:19+00:00
- **Updated**: 2019-07-03 10:35:19+00:00
- **Authors**: David Zuñiga-Noël, Jose-Raul Ruiz-Sarmiento, Javier Gonzalez-Jimenez
- **Comment**: Submitted to the 18th International Conference on Computer Analysis
  of Images and Patterns. Project webpage (code):
  http://github.com/dzunigan/depth_calibration
- **Journal**: None
- **Summary**: Depth cameras, typically in RGB-D configurations, are common devices in mobile robotic platforms given their appealing features: high frequency and resolution, low price and power requirements, among others. These sensors may come with significant, non-linear errors in the depth measurements that jeopardize robot tasks, like free-space detection, environment reconstruction or visual robot-human interaction. This paper presents a method to calibrate such systematic errors with the help of a second, more precise range sensor, in our case a radial laser scanner. In contrast to what it may seem at first, this does not mean a serious limitation in practice since these two sensors are often mounted jointly in many mobile robotic platforms, as they complement well each other. Moreover, the laser scanner can be used just for the calibration process and get rid of it after that. The main contributions of the paper are: i) the calibration is formulated from a probabilistic perspective through a Maximum Likelihood Estimation problem, and ii) the proposed method can be easily executed automatically by mobile robotic platforms. To validate the proposed approach we evaluated for both, local distortion of 3D planar reconstructions and global shifts in the measurements, obtaining considerably more accurate results. A C++ open-source implementation of the presented method has been released for the benefit of the community.



### Semi-supervised Image Attribute Editing using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.01841v2
- **DOI**: 10.1016/j.neucom.2020.03.071
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.01841v2)
- **Published**: 2019-07-03 10:38:56+00:00
- **Updated**: 2020-04-13 17:18:48+00:00
- **Authors**: Yahya Dogan, Hacer Yalim Keles
- **Comment**: This paper is the preprint of the accepted manuscript in
  Neurocomputing Journal. To visualize the Figures in the manuscript in high
  quality, please check the version at this URL:
  https://github.com/yahyadogan72/CRG
- **Journal**: None
- **Summary**: Image attribute editing is a challenging problem that has been recently studied by many researchers using generative networks. The challenge is in the manipulation of selected attributes of images while preserving the other details. The method to achieve this goal is to find an accurate latent vector representation of an image and a direction corresponding to the attribute. Almost all the works in the literature use labeled datasets in a supervised setting for this purpose. In this study, we introduce an architecture called Cyclic Reverse Generator (CRG), which allows learning the inverse function of the generator accurately via an encoder in an unsupervised setting by utilizing cyclic cost minimization. Attribute editing is then performed using the CRG models for finding desired attribute representations in the latent space. In this work, we use two arbitrary reference images, with and without desired attributes, to compute an attribute direction for editing. We show that the proposed approach performs better in terms of image reconstruction compared to the existing end-to-end generative models both quantitatively and qualitatively. We demonstrate state-of-the-art results on both real images and generated images in CelebA dataset.



### FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1907.01845v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.01845v5)
- **Published**: 2019-07-03 10:50:38+00:00
- **Updated**: 2021-07-28 10:19:08+00:00
- **Authors**: Xiangxiang Chu, Bo Zhang, Ruijun Xu
- **Comment**: Accepted to ICCV21
- **Journal**: None
- **Summary**: One of the most critical problems in weight-sharing neural architecture search is the evaluation of candidate models within a predefined search space. In practice, a one-shot supernet is trained to serve as an evaluator. A faithful ranking certainly leads to more accurate searching results. However, current methods are prone to making misjudgments. In this paper, we prove that their biased evaluation is due to inherent unfairness in the supernet training. In view of this, we propose two levels of constraints: expectation fairness and strict fairness. Particularly, strict fairness ensures equal optimization opportunities for all choice blocks throughout the training, which neither overestimates nor underestimates their capacity. We demonstrate that this is crucial for improving the confidence of models' ranking. Incorporating the one-shot supernet trained under the proposed fairness constraints with a multi-objective evolutionary search algorithm, we obtain various state-of-the-art models, e.g., FairNAS-A attains 77.5% top-1 validation accuracy on ImageNet. The models and their evaluation codes are made publicly available online http://github.com/fairnas/FairNAS .



### Deformable Tube Network for Action Detection in Videos
- **Arxiv ID**: http://arxiv.org/abs/1907.01847v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.01847v1)
- **Published**: 2019-07-03 10:55:35+00:00
- **Updated**: 2019-07-03 10:55:35+00:00
- **Authors**: Wei Li, Zehuan Yuan, Dashan Guo, Lei Huang, Xiangzhong Fang, Changhu Wang
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of spatio-temporal action detection in videos. Existing methods commonly either ignore temporal context in action recognition and localization, or lack the modelling of flexible shapes of action tubes. In this paper, we propose a two-stage action detector called Deformable Tube Network (DTN), which is composed of a Deformation Tube Proposal Network (DTPN) and a Deformable Tube Recognition Network (DTRN) similar to the Faster R-CNN architecture. In DTPN, a fast proposal linking algorithm (FTL) is introduced to connect region proposals across frames to generate multiple deformable action tube proposals. To perform action detection, we design a 3D convolution network with skip connections for tube classification and regression. Modelling action proposals as deformable tubes explicitly considers the shape of action tubes compared to 3D cuboids. Moreover, 3D convolution based recognition network can learn temporal dynamics sufficiently for action detection. Our experimental results show that we significantly outperform the methods with 3D cuboids and obtain the state-of-the-art results on both UCF-Sports and AVA datasets.



### City-GAN: Learning architectural styles using a custom Conditional GAN architecture
- **Arxiv ID**: http://arxiv.org/abs/1907.05280v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.05280v2)
- **Published**: 2019-07-03 11:43:36+00:00
- **Updated**: 2020-05-26 20:19:30+00:00
- **Authors**: Maximilian Bachl, Daniel C. Ferreira
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) are a well-known technique that is trained on samples (e.g. pictures of fruits) and which after training is able to generate realistic new samples. Conditional GANs (CGANs) additionally provide label information for subclasses (e.g. apple, orange, pear) which enables the GAN to learn more easily and increase the quality of its output samples. We use GANs to learn architectural features of major cities and to generate images of buildings which do not exist. We show that currently available GAN and CGAN architectures are unsuited for this task and propose a custom architecture and demonstrate that our architecture has superior performance for this task and verify its capabilities with extensive experiments.



### Simple vs complex temporal recurrences for video saliency prediction
- **Arxiv ID**: http://arxiv.org/abs/1907.01869v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.01869v4)
- **Published**: 2019-07-03 12:02:05+00:00
- **Updated**: 2019-07-16 13:13:46+00:00
- **Authors**: Panagiotis Linardos, Eva Mohedano, Juan Jose Nieto, Noel E. O'Connor, Xavier Giro-i-Nieto, Kevin McGuinness
- **Comment**: Accepted at BMVC 2019
- **Journal**: None
- **Summary**: This paper investigates modifying an existing neural network architecture for static saliency prediction using two types of recurrences that integrate information from the temporal domain. The first modification is the addition of a ConvLSTM within the architecture, while the second is a conceptually simple exponential moving average of an internal convolutional state. We use weights pre-trained on the SALICON dataset and fine-tune our model on DHF1K. Our results show that both modifications achieve state-of-the-art results and produce similar saliency maps. Source code is available at https://git.io/fjPiB.



### Robust Cochlear Modiolar Axis Detection in CT
- **Arxiv ID**: http://arxiv.org/abs/1907.01870v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.01870v1)
- **Published**: 2019-07-03 12:03:01+00:00
- **Updated**: 2019-07-03 12:03:01+00:00
- **Authors**: Wilhelm Wimmer, Clair Vandersteen, Nicolas Guevara, Marco Caversaccio, Hervé Delingette
- **Comment**: Accepted for MICCAI 2019
- **Journal**: None
- **Summary**: The cochlea, the auditory part of the inner ear, is a spiral-shaped organ with large morphological variability. An individualized assessment of its shape is essential for clinical applications related to tonotopy and cochlear implantation. To unambiguously reference morphological parameters, reliable recognition of the cochlear modiolar axis in computed tomography (CT) images is required. The conventional method introduces measurement uncertainties, as it is based on manually selected and difficult to identify landmarks. Herein, we present an algorithm for robust modiolar axis detection in clinical CT images. We define the modiolar axis as the rotation component of the kinematic spiral motion inherent in the cochlear shape. For surface fitting, we use a compact shape representation in a 7-dimensional kinematic parameter space based on extended Pl\"ucker coordinates. It is the first time such a kinematic representation is used for shape analysis in medical images. Robust surface fitting is achieved with an adapted approximate maximum likelihood method assuming a Student-t distribution, enabling axis detection even in partially available surface data. We verify the algorithm performance on a synthetic data set with cochlear surface subsets. In addition, we perform an experimental study with four experts in 23 human cochlea CT data sets to compare the automated detection with the manually found axes. Axes found from co-registered high resolution micro-CT scans are used for reference. Our experiments show that the algorithm reduces the alignment error providing more reliable modiolar axis detection for clinical and research applications.



### Learning to Predict Robot Keypoints Using Artificially Generated Images
- **Arxiv ID**: http://arxiv.org/abs/1907.01879v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1907.01879v1)
- **Published**: 2019-07-03 12:20:16+00:00
- **Updated**: 2019-07-03 12:20:16+00:00
- **Authors**: Christoph Heindl, Sebastian Zambal, Josef Scharinger
- **Comment**: Work in progress
- **Journal**: None
- **Summary**: This work considers robot keypoint estimation on color images as a supervised machine learning task. We propose the use of probabilistically created renderings to overcome the lack of labeled real images. Rather than sampling from stationary distributions, our approach introduces a feedback mechanism that constantly adapts probability distributions according to current training progress. Initial results show, our approach achieves near-human-level accuracy on real images. Additionally, we demonstrate that feedback leads to fewer required training steps, while maintaining the same model quality on synthetic data sets.



### Probabilistic Multilayer Regularization Network for Unsupervised 3D Brain Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1907.01922v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01922v1)
- **Published**: 2019-07-03 13:12:03+00:00
- **Updated**: 2019-07-03 13:12:03+00:00
- **Authors**: Lihao Liu, Xiaowei Hu, Lei Zhu, Pheng-Ann Heng
- **Comment**: Accepted at MICCAI 2019
- **Journal**: None
- **Summary**: Brain image registration transforms a pair of images into one system with the matched imaging contents, which is of essential importance for brain image analysis. This paper presents a novel framework for unsupervised 3D brain image registration by capturing the feature-level transformation relationships between the unaligned image and reference image. To achieve this, we develop a feature-level probabilistic model to provide the direct regularization to the hidden layers of two deep convolutional neural networks, which are constructed from two input images. This model design is developed into multiple layers of these two networks to capture the transformation relationships at different levels. We employ two common benchmark datasets for 3D brain image registration and perform various experiments to evaluate our method. Experimental results show that our method clearly outperforms state-of-the-art methods on both benchmark datasets by a large margin.



### Supervised Uncertainty Quantification for Segmentation with Multiple Annotations
- **Arxiv ID**: http://arxiv.org/abs/1907.01949v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.01949v2)
- **Published**: 2019-07-03 13:53:54+00:00
- **Updated**: 2022-05-27 18:54:52+00:00
- **Authors**: Shi Hu, Daniel Worrall, Stefan Knegt, Bas Veeling, Henkjan Huisman, Max Welling
- **Comment**: MICCAI 2019. Fixed a few typos
- **Journal**: None
- **Summary**: The accurate estimation of predictive uncertainty carries importance in medical scenarios such as lung node segmentation. Unfortunately, most existing works on predictive uncertainty do not return calibrated uncertainty estimates, which could be used in practice. In this work we exploit multi-grader annotation variability as a source of 'groundtruth' aleatoric uncertainty, which can be treated as a target in a supervised learning problem. We combine this groundtruth uncertainty with a Probabilistic U-Net and test on the LIDC-IDRI lung nodule CT dataset and MICCAI2012 prostate MRI dataset. We find that we are able to improve predictive uncertainty estimates. We also find that we can improve sample accuracy and sample diversity. In real-world applications, our method could inform doctors about the confidence of the segmentation results.



### Intrinsic Image Popularity Assessment
- **Arxiv ID**: http://arxiv.org/abs/1907.01985v2
- **DOI**: 10.1145/3343031.3351007
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.01985v2)
- **Published**: 2019-07-03 15:15:21+00:00
- **Updated**: 2019-07-04 15:38:50+00:00
- **Authors**: Keyan Ding, Kede Ma, Shiqi Wang
- **Comment**: Accepted by ACM Multimedia 2019
- **Journal**: Proceedings of the 27th ACM International Conference on
  Multimedia, 2019
- **Summary**: The goal of research in automatic image popularity assessment (IPA) is to develop computational models that can accurately predict the potential of a social image to go viral on the Internet. Here, we aim to single out the contribution of visual content to image popularity, i.e., intrinsic image popularity. Specifically, we first describe a probabilistic method to generate massive popularity-discriminable image pairs, based on which the first large-scale image database for intrinsic IPA (I$^2$PA) is established. We then develop computational models for I$^2$PA based on deep neural networks, optimizing for ranking consistency with millions of popularity-discriminable image pairs. Experiments on Instagram and other social platforms demonstrate that the optimized model performs favorably against existing methods, exhibits reasonable generalizability on different databases, and even surpasses human-level performance on Instagram. In addition, we conduct a psychophysical experiment to analyze various aspects of human behavior in I$^2$PA.



### Measuring the Data Efficiency of Deep Learning Methods
- **Arxiv ID**: http://arxiv.org/abs/1907.02549v1
- **DOI**: 10.5220/0007456306910698
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.02549v1)
- **Published**: 2019-07-03 15:22:23+00:00
- **Updated**: 2019-07-03 15:22:23+00:00
- **Authors**: Hlynur Davíð Hlynsson, Alberto N. Escalante-B., Laurenz Wiskott
- **Comment**: 8 pages
- **Journal**: In Proceedings of the 8th International Conference on Pattern
  Recognition Applications and Methods - Volume 1: ICPRAM (2019) pages 691-698
- **Summary**: In this paper, we propose a new experimental protocol and use it to benchmark the data efficiency --- performance as a function of training set size --- of two deep learning algorithms, convolutional neural networks (CNNs) and hierarchical information-preserving graph-based slow feature analysis (HiGSFA), for tasks in classification and transfer learning scenarios. The algorithms are trained on different-sized subsets of the MNIST and Omniglot data sets. HiGSFA outperforms standard CNN networks when the models are trained on 50 and 200 samples per class for MNIST classification. In other cases, the CNNs perform better. The results suggest that there are cases where greedy, locally optimal bottom-up learning is equally or more powerful than global gradient-based learning.



### On-Device Neural Net Inference with Mobile GPUs
- **Arxiv ID**: http://arxiv.org/abs/1907.01989v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.01989v1)
- **Published**: 2019-07-03 15:23:20+00:00
- **Updated**: 2019-07-03 15:23:20+00:00
- **Authors**: Juhyun Lee, Nikolay Chirkov, Ekaterina Ignasheva, Yury Pisarchyk, Mogan Shieh, Fabio Riccardi, Raman Sarokin, Andrei Kulik, Matthias Grundmann
- **Comment**: Computer Vision and Pattern Recognition Workshop: Efficient Deep
  Learning for Computer Vision 2019
- **Journal**: None
- **Summary**: On-device inference of machine learning models for mobile phones is desirable due to its lower latency and increased privacy. Running such a compute-intensive task solely on the mobile CPU, however, can be difficult due to limited computing power, thermal constraints, and energy consumption. App developers and researchers have begun exploiting hardware accelerators to overcome these challenges. Recently, device manufacturers are adding neural processing units into high-end phones for on-device inference, but these account for only a small fraction of hand-held devices. In this paper, we present how we leverage the mobile GPU, a ubiquitous hardware accelerator on virtually every phone, to run inference of deep neural networks in real-time for both Android and iOS devices. By describing our architecture, we also discuss how to design networks that are mobile GPU-friendly. Our state-of-the-art mobile GPU inference engine is integrated into the open-source project TensorFlow Lite and publicly available at https://tensorflow.org/lite.



### Learning with Known Operators reduces Maximum Training Error Bounds
- **Arxiv ID**: http://arxiv.org/abs/1907.01992v1
- **DOI**: 10.1038/s42256-019-0077-5
- **Categories**: **cs.LG**, cs.CV, physics.med-ph, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.01992v1)
- **Published**: 2019-07-03 15:35:16+00:00
- **Updated**: 2019-07-03 15:35:16+00:00
- **Authors**: Andreas K. Maier, Christopher Syben, Bernhard Stimpel, Tobias Würfl, Mathis Hoffmann, Frank Schebesch, Weilin Fu, Leonid Mill, Lasse Kling, Silke Christiansen
- **Comment**: Paper conditionally accepted in Nature Machine Intelligence
- **Journal**: Nature Machine Intelligence 1, 373-380, 2019
- **Summary**: We describe an approach for incorporating prior knowledge into machine learning algorithms. We aim at applications in physics and signal processing in which we know that certain operations must be embedded into the algorithm. Any operation that allows computation of a gradient or sub-gradient towards its inputs is suited for our framework. We derive a maximal error bound for deep nets that demonstrates that inclusion of prior knowledge results in its reduction. Furthermore, we also show experimentally that known operators reduce the number of free parameters. We apply this approach to various tasks ranging from CT image reconstruction over vessel segmentation to the derivation of previously unknown imaging algorithms. As such the concept is widely applicable for many researchers in physics, imaging, and signal processing. We assume that our analysis will support further investigation of known operators in other fields of physics, imaging, and signal processing.



### Robust Synthesis of Adversarial Visual Examples Using a Deep Image Prior
- **Arxiv ID**: http://arxiv.org/abs/1907.01996v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01996v1)
- **Published**: 2019-07-03 15:40:05+00:00
- **Updated**: 2019-07-03 15:40:05+00:00
- **Authors**: Thomas Gittings, Steve Schneider, John Collomosse
- **Comment**: Accepted to BMVC 2019
- **Journal**: None
- **Summary**: We present a novel method for generating robust adversarial image examples building upon the recent `deep image prior' (DIP) that exploits convolutional network architectures to enforce plausible texture in image synthesis. Adversarial images are commonly generated by perturbing images to introduce high frequency noise that induces image misclassification, but that is fragile to subsequent digital manipulation of the image. We show that using DIP to reconstruct an image under adversarial constraint induces perturbations that are more robust to affine deformation, whilst remaining visually imperceptible. Furthermore we show that our DIP approach can also be adapted to produce local adversarial patches (`adversarial stickers'). We demonstrate robust adversarial examples over a broad gamut of images and object classes drawn from the ImageNet dataset.



### Anatomically Consistent Segmentation of Organs at Risk in MRI with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.02003v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.02003v1)
- **Published**: 2019-07-03 15:55:43+00:00
- **Updated**: 2019-07-03 15:55:43+00:00
- **Authors**: Pawel Mlynarski, Hervé Delingette, Hamza Alghamdi, Pierre-Yves Bondiau, Nicholas Ayache
- **Comment**: Submitted to Computerized Medical Imaging and Graphics
- **Journal**: None
- **Summary**: Planning of radiotherapy involves accurate segmentation of a large number of organs at risk, i.e. organs for which irradiation doses should be minimized to avoid important side effects of the therapy. We propose a deep learning method for segmentation of organs at risk inside the brain region, from Magnetic Resonance (MR) images. Our system performs segmentation of eight structures: eye, lens, optic nerve, optic chiasm, pituitary gland, hippocampus, brainstem and brain. We propose an efficient algorithm to train neural networks for an end-to-end segmentation of multiple and non-exclusive classes, addressing problems related to computational costs and missing ground truth segmentations for a subset of classes. We enforce anatomical consistency of the result in a postprocessing step, in particular we introduce a graph-based algorithm for segmentation of the optic nerves, enforcing the connectivity between the eyes and the optic chiasm. We report cross-validated quantitative results on a database of 44 contrast-enhanced T1-weighted MRIs with provided segmentations of the considered organs at risk, which were originally used for radiotherapy planning. In addition, the segmentations produced by our model on an independent test set of 50 MRIs are evaluated by an experienced radiotherapist in order to qualitatively assess their accuracy. The mean distances between produced segmentations and the ground truth ranged from 0.1 mm to 0.7 mm across different organs. A vast majority (96 %) of the produced segmentations were found acceptable for radiotherapy planning.



### Chasing Ghosts: Instruction Following as Bayesian State Tracking
- **Arxiv ID**: http://arxiv.org/abs/1907.02022v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1907.02022v2)
- **Published**: 2019-07-03 16:39:05+00:00
- **Updated**: 2019-11-26 18:52:33+00:00
- **Authors**: Peter Anderson, Ayush Shrivastava, Devi Parikh, Dhruv Batra, Stefan Lee
- **Comment**: NeurIPS 2019
- **Journal**: None
- **Summary**: A visually-grounded navigation instruction can be interpreted as a sequence of expected observations and actions an agent following the correct trajectory would encounter and perform. Based on this intuition, we formulate the problem of finding the goal location in Vision-and-Language Navigation (VLN) within the framework of Bayesian state tracking - learning observation and motion models conditioned on these expectable events. Together with a mapper that constructs a semantic spatial map on-the-fly during navigation, we formulate an end-to-end differentiable Bayes filter and train it to identify the goal by predicting the most likely trajectory through the map according to the instructions. The resulting navigation policy constitutes a new approach to instruction following that explicitly models a probability distribution over states, encoding strong geometric and algorithmic priors while enabling greater explainability. Our experiments show that our approach outperforms a strong LingUNet baseline when predicting the goal location on the map. On the full VLN task, i.e. navigating to the goal location, our approach achieves promising results with less reliance on navigation constraints.



### Using Deep Learning to Count Albatrosses from Space
- **Arxiv ID**: http://arxiv.org/abs/1907.02040v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.02040v1)
- **Published**: 2019-07-03 17:09:25+00:00
- **Updated**: 2019-07-03 17:09:25+00:00
- **Authors**: Ellen Bowler, Peter T. Fretwell, Geoffrey French, Michal Mackiewicz
- **Comment**: 4 pages, 5 figures, to be presented at IEEE 2019 International
  Geoscience & Remote Sensing Symposium (IGARSS 2019), scheduled for July 28 -
  August 2, 2019
- **Journal**: None
- **Summary**: In this paper we test the use of a deep learning approach to automatically count Wandering Albatrosses in Very High Resolution (VHR) satellite imagery. We use a dataset of manually labelled imagery provided by the British Antarctic Survey to train and develop our methods. We employ a U-Net architecture, designed for image segmentation, to simultaneously classify and localise potential albatrosses. We aid training with the use of the Focal Loss criterion, to deal with extreme class imbalance in the dataset. Initial results achieve peak precision and recall values of approximately 80%. Finally we assess the model's performance in relation to inter-observer variation, by comparing errors against an image labelled by multiple observers. We conclude model accuracy falls within the range of human counters. We hope that the methods will streamline the analysis of VHR satellite images, enabling more frequent monitoring of a species which is of high conservation concern.



### Minimally distorted Adversarial Examples with a Fast Adaptive Boundary Attack
- **Arxiv ID**: http://arxiv.org/abs/1907.02044v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.02044v2)
- **Published**: 2019-07-03 17:22:05+00:00
- **Updated**: 2020-07-20 15:18:47+00:00
- **Authors**: Francesco Croce, Matthias Hein
- **Comment**: None
- **Journal**: None
- **Summary**: The evaluation of robustness against adversarial manipulation of neural networks-based classifiers is mainly tested with empirical attacks as methods for the exact computation, even when available, do not scale to large networks. We propose in this paper a new white-box adversarial attack wrt the $l_p$-norms for $p \in \{1,2,\infty\}$ aiming at finding the minimal perturbation necessary to change the class of a given input. It has an intuitive geometric meaning, yields quickly high quality results, minimizes the size of the perturbation (so that it returns the robust accuracy at every threshold with a single run). It performs better or similar to state-of-the-art attacks which are partially specialized to one $l_p$-norm, and is robust to the phenomenon of gradient masking.



### Self-supervised Learning of Interpretable Keypoints from Unlabelled Videos
- **Arxiv ID**: http://arxiv.org/abs/1907.02055v2
- **DOI**: 10.1109/CVPR42600.2020.00881
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.02055v2)
- **Published**: 2019-07-03 17:47:08+00:00
- **Updated**: 2020-12-23 18:59:02+00:00
- **Authors**: Tomas Jakab, Ankush Gupta, Hakan Bilen, Andrea Vedaldi
- **Comment**: CVPR 2020 (oral). Project page:
  http://www.robots.ox.ac.uk/~vgg/research/unsupervised_pose/
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR), 2020, pp. 8787-8797
- **Summary**: We propose KeypointGAN, a new method for recognizing the pose of objects from a single image that for learning uses only unlabelled videos and a weak empirical prior on the object poses. Video frames differ primarily in the pose of the objects they contain, so our method distils the pose information by analyzing the differences between frames. The distillation uses a new dual representation of the geometry of objects as a set of 2D keypoints, and as a pictorial representation, i.e. a skeleton image. This has three benefits: (1) it provides a tight `geometric bottleneck' which disentangles pose from appearance, (2) it can leverage powerful image-to-image translation networks to map between photometry and geometry, and (3) it allows to incorporate empirical pose priors in the learning process. The pose priors are obtained from unpaired data, such as from a different dataset or modality such as mocap, such that no annotated image is ever used in learning the pose recognition network. In standard benchmarks for pose recognition for humans and faces, our method achieves state-of-the-art performance among methods that do not require any labelled images for training.



### Novel evaluation of surgical activity recognition models using task-based efficiency metrics
- **Arxiv ID**: http://arxiv.org/abs/1907.02060v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.02060v1)
- **Published**: 2019-07-03 17:55:31+00:00
- **Updated**: 2019-07-03 17:55:31+00:00
- **Authors**: Aneeq Zia, Liheng Guo, Linlin Zhou, Irfan Essa, Anthony Jarc
- **Comment**: None
- **Journal**: International Journal of Computer Assisted Radiology and Surgery
  (IJCARS) 2019
- **Summary**: Purpose: Surgical task-based metrics (rather than entire procedure metrics) can be used to improve surgeon training and, ultimately, patient care through focused training interventions. Machine learning models to automatically recognize individual tasks or activities are needed to overcome the otherwise manual effort of video review. Traditionally, these models have been evaluated using frame-level accuracy. Here, we propose evaluating surgical activity recognition models by their effect on task-based efficiency metrics. In this way, we can determine when models have achieved adequate performance for providing surgeon feedback via metrics from individual tasks.   Methods: We propose a new CNN-LSTM model, RP-Net-V2, to recognize the 12 steps of robotic-assisted radical prostatectomies (RARP). We evaluated our model both in terms of conventional methods (e.g. Jaccard Index, task boundary accuracy) as well as novel ways, such as the accuracy of efficiency metrics computed from instrument movements and system events.   Results: Our proposed model achieves a Jaccard Index of 0.85 thereby outperforming previous models on robotic-assisted radical prostatectomies. Additionally, we show that metrics computed from tasks automatically identified using RP-Net-V2 correlate well with metrics from tasks labeled by clinical experts.   Conclusions: We demonstrate that metrics-based evaluation of surgical activity recognition models is a viable approach to determine when models can be used to quantify surgical efficiencies. We believe this approach and our results illustrate the potential for fully automated, post-operative efficiency reports.



### A comprehensive evaluation of full-reference image quality assessment algorithms on KADID-10k
- **Arxiv ID**: http://arxiv.org/abs/1907.02096v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.02096v1)
- **Published**: 2019-07-03 18:46:36+00:00
- **Updated**: 2019-07-03 18:46:36+00:00
- **Authors**: Domonkos Varga
- **Comment**: None
- **Journal**: None
- **Summary**: Significant progress has been made in the past decade for full-reference image quality assessment (FR-IQA). However, new large scale image quality databases have been released for evaluating image quality assessment algorithms. In this study, our goal is to give a comprehensive evaluation of state-of-the-art FR-IQA metrics using the recently published KADID-10k database which is largest available one at the moment. Our evaluation results and the associated discussions is very helpful to obtain a clear understanding about the status of state-of-the-art FR-IQA metrics.



### Tranquil Clouds: Neural Networks for Learning Temporally Coherent Features in Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1907.05279v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.05279v2)
- **Published**: 2019-07-03 18:54:02+00:00
- **Updated**: 2020-01-29 10:55:16+00:00
- **Authors**: Lukas Prantl, Nuttapong Chentanez, Stefan Jeschke, Nils Thuerey
- **Comment**: Further information and videos at
  https://ge.in.tum.de/publications/2020-iclr-prantl/
- **Journal**: None
- **Summary**: Point clouds, as a form of Lagrangian representation, allow for powerful and flexible applications in a large number of computational disciplines. We propose a novel deep-learning method to learn stable and temporally coherent feature spaces for points clouds that change over time. We identify a set of inherent problems with these approaches: without knowledge of the time dimension, the inferred solutions can exhibit strong flickering, and easy solutions to suppress this flickering can result in undesirable local minima that manifest themselves as halo structures. We propose a novel temporal loss function that takes into account higher time derivatives of the point positions, and encourages mingling, i.e., to prevent the aforementioned halos. We combine these techniques in a super-resolution method with a truncation approach to flexibly adapt the size of the generated positions. We show that our method works for large, deforming point sets from different sources to demonstrate the flexibility of our approach.



### DeepMRSeg: A convolutional deep neural network for anatomy and abnormality segmentation on MR images
- **Arxiv ID**: http://arxiv.org/abs/1907.02110v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.02110v1)
- **Published**: 2019-07-03 19:10:37+00:00
- **Updated**: 2019-07-03 19:10:37+00:00
- **Authors**: Jimit Doshi, Guray Erus, Mohamad Habes, Christos Davatzikos
- **Comment**: 18 pages, 8 figures, 3 tables
- **Journal**: None
- **Summary**: Segmentation has been a major task in neuroimaging. A large number of automated methods have been developed for segmenting healthy and diseased brain tissues. In recent years, deep learning techniques have attracted a lot of attention as a result of their high accuracy in different segmentation problems. We present a new deep learning based segmentation method, DeepMRSeg, that can be applied in a generic way to a variety of segmentation tasks. The proposed architecture combines recent advances in the field of biomedical image segmentation and computer vision. We use a modified UNet architecture that takes advantage of multiple convolution filter sizes to achieve multi-scale feature extraction adaptive to the desired segmentation task. Importantly, our method operates on minimally processed raw MRI scan. We validated our method on a wide range of segmentation tasks, including white matter lesion segmentation, segmentation of deep brain structures and hippocampus segmentation. We provide code and pre-trained models to allow researchers apply our method on their own datasets.



### Primate Face Identification in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1907.02642v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.02642v1)
- **Published**: 2019-07-03 19:26:09+00:00
- **Updated**: 2019-07-03 19:26:09+00:00
- **Authors**: Ankita Shukla, Gullal Singh Cheema, Saket Anand, Qamar Qureshi, Yadvendradev Jhala
- **Comment**: arXiv admin note: text overlap with arXiv:1811.00743
- **Journal**: PRICAI 2019, The 16th Pacific Rim International Conference on
  Artificial Intelligence
- **Summary**: Ecological imbalance owing to rapid urbanization and deforestation has adversely affected the population of several wild animals. This loss of habitat has skewed the population of several non-human primate species like chimpanzees and macaques and has constrained them to co-exist in close proximity of human settlements, often leading to human-wildlife conflicts while competing for resources. For effective wildlife conservation and conflict management, regular monitoring of population and of conflicted regions is necessary. However, existing approaches like field visits for data collection and manual analysis by experts is resource intensive, tedious and time consuming, thus necessitating an automated, non-invasive, more efficient alternative like image based facial recognition. The challenge in individual identification arises due to unrelated factors like pose, lighting variations and occlusions due to the uncontrolled environments, that is further exacerbated by limited training data. Inspired by human perception, we propose to learn representations that are robust to such nuisance factors and capture the notion of similarity over the individual identity sub-manifolds. The proposed approach, Primate Face Identification (PFID), achieves this by training the network to distinguish between positive and negative pairs of images. The PFID loss augments the standard cross entropy loss with a pairwise loss to learn more discriminative and generalizable features, thus making it appropriate for other related identification tasks like open-set, closed set and verification. We report state-of-the-art accuracy on facial recognition of two primate species, rhesus macaques and chimpanzees under the four protocols of classification, verification, closed-set identification and open-set recognition.



### Non-Structured DNN Weight Pruning -- Is It Beneficial in Any Platform?
- **Arxiv ID**: http://arxiv.org/abs/1907.02124v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.02124v2)
- **Published**: 2019-07-03 20:27:51+00:00
- **Updated**: 2020-01-07 19:43:16+00:00
- **Authors**: Xiaolong Ma, Sheng Lin, Shaokai Ye, Zhezhi He, Linfeng Zhang, Geng Yuan, Sia Huat Tan, Zhengang Li, Deliang Fan, Xuehai Qian, Xue Lin, Kaisheng Ma, Yanzhi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Large deep neural network (DNN) models pose the key challenge to energy efficiency due to the significantly higher energy consumption of off-chip DRAM accesses than arithmetic or SRAM operations. It motivates the intensive research on model compression with two main approaches. Weight pruning leverages the redundancy in the number of weights and can be performed in a non-structured, which has higher flexibility and pruning rate but incurs index accesses due to irregular weights, or structured manner, which preserves the full matrix structure with lower pruning rate. Weight quantization leverages the redundancy in the number of bits in weights. Compared to pruning, quantization is much more hardware-friendly, and has become a "must-do" step for FPGA and ASIC implementations. This paper provides a definitive answer to the question for the first time. First, we build ADMM-NN-S by extending and enhancing ADMM-NN, a recently proposed joint weight pruning and quantization framework. Second, we develop a methodology for fair and fundamental comparison of non-structured and structured pruning in terms of both storage and computation efficiency. Our results show that ADMM-NN-S consistently outperforms the prior art: (i) it achieves 348x, 36x, and 8x overall weight pruning on LeNet-5, AlexNet, and ResNet-50, respectively, with (almost) zero accuracy loss; (ii) we demonstrate the first fully binarized (for all layers) DNNs can be lossless in accuracy in many cases. These results provide a strong baseline and credibility of our study. Based on the proposed comparison framework, with the same accuracy and quantization, the results show that non-structrued pruning is not competitive in terms of both storage and computation efficiency. Thus, we conclude that non-structured pruning is considered harmful. We urge the community not to continue the DNN inference acceleration for non-structured sparsity.



### The Indirect Convolution Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1907.02129v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1907.02129v1)
- **Published**: 2019-07-03 20:51:18+00:00
- **Updated**: 2019-07-03 20:51:18+00:00
- **Authors**: Marat Dukhan
- **Comment**: Presented on Efficient Deep Learning for Computer Vision workshop at
  CVPR 2019
- **Journal**: None
- **Summary**: Deep learning frameworks commonly implement convolution operators with GEMM-based algorithms. In these algorithms, convolution is implemented on top of matrix-matrix multiplication (GEMM) functions, provided by highly optimized BLAS libraries. Convolutions with 1x1 kernels can be directly represented as a GEMM call, but convolutions with larger kernels require a special memory layout transformation - im2col or im2row - to fit into GEMM interface.   The Indirect Convolution algorithm provides the efficiency of the GEMM primitive without the overhead of im2col transformation. In contrast to GEMM-based algorithms, the Indirect Convolution does not reshuffle the data to fit into the GEMM primitive but introduces an indirection buffer - a buffer of pointers to the start of each row of image pixels. This broadens the application of our modified GEMM function to convolutions with arbitrary kernel size, padding, stride, and dilation.   The Indirect Convolution algorithm reduces memory overhead proportionally to the number of input channels and outperforms the GEMM-based algorithm by up to 62% on convolution parameters which involve im2col transformations in GEMM-based algorithms. This, however, comes at cost of minor performance reduction on 1x1 stride-1 convolutions.



### Analyzing the Cross-Sensor Portability of Neural Network Architectures for LiDAR-based Semantic Labeling
- **Arxiv ID**: http://arxiv.org/abs/1907.02149v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.02149v1)
- **Published**: 2019-07-03 22:19:37+00:00
- **Updated**: 2019-07-03 22:19:37+00:00
- **Authors**: Florian Piewak, Peter Pinggera, Marius Zöllner
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art approaches for the semantic labeling of LiDAR point clouds heavily rely on the use of deep Convolutional Neural Networks (CNNs). However, transferring network architectures across different LiDAR sensor types represents a significant challenge, especially due to sensor specific design choices with regard to network architecture as well as data representation. In this paper we propose a new CNN architecture for the point-wise semantic labeling of LiDAR data which achieves state-of-the-art results while increasing portability across sensor types. This represents a significant advantage given the fast-paced development of LiDAR hardware technology. We perform a thorough quantitative cross-sensor analysis of semantic labeling performance in comparison to a state-of-the-art reference method. Our evaluation shows that the proposed architecture is indeed highly portable, yielding an improvement of 10 percentage points in the Intersection-over-Union (IoU) score when compared to the reference approach. Further, the results indicate that the proposed network architecture can provide an efficient way for the automated generation of large-scale training data for novel LiDAR sensor types without the need for extensive manual annotation or multi-modal label transfer.



### Slim-CNN: A Light-Weight CNN for Face Attribute Prediction
- **Arxiv ID**: http://arxiv.org/abs/1907.02157v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.02157v1)
- **Published**: 2019-07-03 23:02:44+00:00
- **Updated**: 2019-07-03 23:02:44+00:00
- **Authors**: Ankit Sharma, Hassan Foroosh
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a computationally-efficient CNN micro-architecture Slim Module to design a lightweight deep neural network Slim-Net for face attribute prediction. Slim Modules are constructed by assembling depthwise separable convolutions with pointwise convolution to produce a computationally efficient module. The problem of facial attribute prediction is challenging because of the large variations in pose, background, illumination, and dataset imbalance. We stack these Slim Modules to devise a compact CNN which still maintains very high accuracy. Additionally, the neural network has a very low memory footprint which makes it suitable for mobile and embedded applications. Experiments on the CelebA dataset show that Slim-Net achieves an accuracy of 91.24% with at least 25 times fewer parameters than comparably performing methods, which reduces the memory storage requirement of Slim-net by at least 87%.



### Deep Learning-Based Semantic Segmentation of Microscale Objects
- **Arxiv ID**: http://arxiv.org/abs/1907.03576v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.03576v1)
- **Published**: 2019-07-03 23:07:01+00:00
- **Updated**: 2019-07-03 23:07:01+00:00
- **Authors**: Ekta U. Samani, Wei Guo, Ashis G. Banerjee
- **Comment**: A condensed version of the paper is published in the Proceedings of
  the 2019 International Conference on Manipulation, Automation and Robotics at
  Small Scales
- **Journal**: None
- **Summary**: Accurate estimation of the positions and shapes of microscale objects is crucial for automated imaging-guided manipulation using a non-contact technique such as optical tweezers. Perception methods that use traditional computer vision algorithms tend to fail when the manipulation environments are crowded. In this paper, we present a deep learning model for semantic segmentation of the images representing such environments. Our model successfully performs segmentation with a high mean Intersection Over Union score of 0.91.



### Seeing Under the Cover: A Physics Guided Learning Approach for In-Bed Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1907.02161v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.02161v3)
- **Published**: 2019-07-03 23:41:23+00:00
- **Updated**: 2019-09-20 15:00:51+00:00
- **Authors**: Shuangjun Liu, Sarah Ostadabbas
- **Comment**: None
- **Journal**: None
- **Summary**: Human in-bed pose estimation has huge practical values in medical and healthcare applications yet still mainly relies on expensive pressure mapping (PM) solutions. In this paper, we introduce our novel physics inspired vision-based approach that addresses the challenging issues associated with the in-bed pose estimation problem including monitoring a fully covered person in complete darkness. We reformulated this problem using our proposed Under the Cover Imaging via Thermal Diffusion (UCITD) method to capture the high resolution pose information of the body even when it is fully covered by using a long wavelength IR technique. We proposed a physical hyperparameter concept through which we achieved high quality groundtruth pose labels in different modalities. A fully annotated in-bed pose dataset called Simultaneously-collected multimodal Lying Pose (SLP) is also formed/released with the same order of magnitude as most existing large-scale human pose datasets to support complex models' training and evaluation. A network trained from scratch on it and tested on two diverse settings, one in a living room and the other in a hospital room showed pose estimation performance of 99.5% and 95.7% in PCK0.2 standard, respectively. Moreover, in a multi-factor comparison with a state-of-the art in-bed pose monitoring solution based on PM, our solution showed significant superiority in all practical aspects by being 60 times cheaper, 300 times smaller, while having higher pose recognition granularity and accuracy.



