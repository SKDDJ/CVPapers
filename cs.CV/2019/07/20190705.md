# Arxiv Papers in cs.CV on 2019-07-05
### Automated Non-Destructive Inspection of Fused Filament Fabrication Components Using Thermographic Signal Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1907.02634v1
- **DOI**: 10.1016/j.addma.2019.100923
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.02634v1)
- **Published**: 2019-07-05 01:39:40+00:00
- **Updated**: 2019-07-05 01:39:40+00:00
- **Authors**: Joshua E. Siegel, Maria F. Beemer, Steven M. Shepard
- **Comment**: None
- **Journal**: None
- **Summary**: Manufacturers struggle to produce low-cost, robust and complex components at manufacturing lot-size one. Additive processes like Fused Filament Fabrication (FFF) inexpensively produce complex geometries, but defects limit viability in critical applications. We present an approach to high-accuracy, high-throughput and low-cost automated non-destructive testing (NDT) for FFF interlayer delamination using Flash Thermography (FT) data processed with Thermographic Signal Reconstruction (TSR) and Artificial Intelligence (AI). A Deep Neural Network (DNN) attains 95.4% per-pixel accuracy when differentiating four delamination thicknesses 5mm subsurface in PolyLactic Acid (PLA) widgets, and 98.6% accuracy in differentiating acceptable from unacceptable condition for the same components. Automated inspection enables time- and cost-efficient 100% inspection for delamination defects, supporting FFF's use in critical and small-batch applications.



### Evaluating the distribution learning capabilities of GANs
- **Arxiv ID**: http://arxiv.org/abs/1907.02662v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.02662v1)
- **Published**: 2019-07-05 02:59:40+00:00
- **Updated**: 2019-07-05 02:59:40+00:00
- **Authors**: Amit Rege, Claire Monteleoni
- **Comment**: None
- **Journal**: None
- **Summary**: We evaluate the distribution learning capabilities of generative adversarial networks by testing them on synthetic datasets. The datasets include common distributions of points in $R^n$ space and images containing polygons of various shapes and sizes. We find that by and large GANs fail to faithfully recreate point datasets which contain discontinous support or sharp bends with noise. Additionally, on image datasets, we find that GANs do not seem to learn to count the number of objects of the same kind in an image. We also highlight the apparent tension between generalization and learning in GANs.



### Blind Image Quality Assessment Using A Deep Bilinear Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1907.02665v1
- **DOI**: 10.1109/TCSVT.2018.2886771
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1907.02665v1)
- **Published**: 2019-07-05 03:35:35+00:00
- **Updated**: 2019-07-05 03:35:35+00:00
- **Authors**: Weixia Zhang, Kede Ma, Jia Yan, Dexiang Deng, Zhou Wang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a deep bilinear model for blind image quality assessment (BIQA) that handles both synthetic and authentic distortions. Our model consists of two convolutional neural networks (CNN), each of which specializes in one distortion scenario. For synthetic distortions, we pre-train a CNN to classify image distortion type and level, where we enjoy large-scale training data. For authentic distortions, we adopt a pre-trained CNN for image classification. The features from the two CNNs are pooled bilinearly into a unified representation for final quality prediction. We then fine-tune the entire model on target subject-rated databases using a variant of stochastic gradient descent. Extensive experiments demonstrate that the proposed model achieves superior performance on both synthetic and authentic databases. Furthermore, we verify the generalizability of our method on the Waterloo Exploration Database using the group maximum differentiation competition.



### Extraction and Analysis of Fictional Character Networks: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1907.02704v5
- **DOI**: 10.1145/3344548
- **Categories**: **cs.SI**, cs.CL, cs.CV, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1907.02704v5)
- **Published**: 2019-07-05 07:27:31+00:00
- **Updated**: 2022-03-31 12:21:03+00:00
- **Authors**: Vincent Labatut, Xavier Bost
- **Comment**: None
- **Journal**: ACM Computing Surveys, Association for Computing Machinery, 2019,
  52 (5), pp.89
- **Summary**: A character network is a graph extracted from a narrative, in which vertices represent characters and edges correspond to interactions between them. A number of narrative-related problems can be addressed automatically through the analysis of character networks, such as summarization, classification, or role detection. Character networks are particularly relevant when considering works of fictions (e.g. novels, plays, movies, TV series), as their exploitation allows developing information retrieval and recommendation systems. However, works of fiction possess specific properties making these tasks harder. This survey aims at presenting and organizing the scientific literature related to the extraction of character networks from works of fiction, as well as their analysis. We first describe the extraction process in a generic way, and explain how its constituting steps are implemented in practice, depending on the medium of the narrative, the goal of the network analysis, and other factors. We then review the descriptive tools used to characterize character networks, with a focus on the way they are interpreted in this context. We illustrate the relevance of character networks by also providing a review of applications derived from their analysis. Finally, we identify the limitations of the existing approaches, and the most promising perspectives.



### Prior Activation Distribution (PAD): A Versatile Representation to Utilize DNN Hidden Units
- **Arxiv ID**: http://arxiv.org/abs/1907.02711v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.02711v1)
- **Published**: 2019-07-05 07:55:09+00:00
- **Updated**: 2019-07-05 07:55:09+00:00
- **Authors**: Lakmal Meegahapola, Vengateswaran Subramaniam, Lance Kaplan, Archan Misra
- **Comment**: Submitted to NeurIPS 2019
- **Journal**: None
- **Summary**: In this paper, we introduce the concept of Prior Activation Distribution (PAD) as a versatile and general technique to capture the typical activation patterns of hidden layer units of a Deep Neural Network used for classification tasks. We show that the combined neural activations of such a hidden layer have class-specific distributional properties, and then define multiple statistical measures to compute how far a test sample's activations deviate from such distributions. Using a variety of benchmark datasets (including MNIST, CIFAR10, Fashion-MNIST & notMNIST), we show how such PAD-based measures can be used, independent of any training technique, to (a) derive fine-grained uncertainty estimates for inferences; (b) provide inferencing accuracy competitive with alternatives that require execution of the full pipeline, and (c) reliably isolate out-of-distribution test samples.



### C^3 Framework: An Open-source PyTorch Code for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/1907.02724v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.02724v1)
- **Published**: 2019-07-05 08:40:35+00:00
- **Updated**: 2019-07-05 08:40:35+00:00
- **Authors**: Junyu Gao, Wei Lin, Bin Zhao, Dong Wang, Chenyu Gao, Jun Wen
- **Comment**: Technical report
- **Journal**: None
- **Summary**: This technical report attempts to provide efficient and solid kits addressed on the field of crowd counting, which is denoted as Crowd Counting Code Framework (C$^3$F). The contributions of C$^3$F are in three folds: 1) Some solid baseline networks are presented, which have achieved the state-of-the-arts. 2) Some flexible parameter setting strategies are provided to further promote the performance. 3) A powerful log system is developed to record the experiment process, which can enhance the reproducibility of each experiment. Our code is made publicly available at \url{https://github.com/gjy3035/C-3-Framework}. Furthermore, we also post a Chinese blog\footnote{\url{https://zhuanlan.zhihu.com/p/65650998}} to describe the details and insights of crowd counting.



### A 3D Convolutional Approach to Spectral Object Segmentation in Space and Time
- **Arxiv ID**: http://arxiv.org/abs/1907.02731v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.02731v5)
- **Published**: 2019-07-05 09:07:19+00:00
- **Updated**: 2020-04-27 19:35:48+00:00
- **Authors**: Elena Burceanu, Marius Leordeanu
- **Comment**: accepted at International Joint Conference on Artificial Intelligence
  2020 (IJCAI-2020)
- **Journal**: None
- **Summary**: We formulate object segmentation in video as a graph partitioning problem in space and time, in which nodes are pixels and their relations form local neighborhoods. We claim that the strongest cluster in this pixel-level graph represents the salient object segmentation. We compute the main cluster using a novel and fast 3D filtering technique that finds the spectral clustering solution, namely the principal eigenvector of the graph's adjacency matrix, without building the matrix explicitly - which would be intractable. Our method is based on the power iteration for finding the principal eigenvector of a matrix, which we prove is equivalent to performing a specific set of 3D convolutions in the space-time feature volume. This allows us to avoid creating the matrix and have a fast parallel implementation on GPU. We show that our method is much faster than classical power iteration applied directly on the adjacency matrix. Different from other works, ours is dedicated to preserving object consistency in space and time at the level of pixels. For that, it requires powerful pixel-wise features at the frame level. This makes it perfectly suitable for incorporating the output of a backbone network or other methods and fast-improving over their solution without supervision. In experiments, we obtain consistent improvement, with the same set of hyper-parameters, over the top state of the art methods on DAVIS-2016 dataset, both in unsupervised and semi-supervised tasks. We also achieve top results on the well-known SegTrackv2 dataset.



### Adversarial Learning with Multiscale Features and Kernel Factorization for Retinal Blood Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.02742v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.02742v1)
- **Published**: 2019-07-05 09:35:38+00:00
- **Updated**: 2019-07-05 09:35:38+00:00
- **Authors**: Farhan Akram, Vivek Kumar Singh, Hatem A. Rashwan, Mohamed Abdel-Nasser, Md. Mostafa Kamal Sarker, Nidhi Pandey, Domenec Puig
- **Comment**: 9 pages, 4 figures
- **Journal**: None
- **Summary**: In this paper, we propose an efficient blood vessel segmentation method for the eye fundus images using adversarial learning with multiscale features and kernel factorization. In the generator network of the adversarial framework, spatial pyramid pooling, kernel factorization and squeeze excitation block are employed to enhance the feature representation in spatial domain on different scales with reduced computational complexity. In turn, the discriminator network of the adversarial framework is formulated by combining convolutional layers with an additional squeeze excitation block to differentiate the generated segmentation mask from its respective ground truth. Before feeding the images to the network, we pre-processed them by using edge sharpening and Gaussian regularization to reach an optimized solution for vessel segmentation. The output of the trained model is post-processed using morphological operations to remove the small speckles of noise. The proposed method qualitatively and quantitatively outperforms state-of-the-art vessel segmentation methods using DRIVE and STARE datasets.



### Self-Supervised Learning for Cardiac MR Image Segmentation by Anatomical Position Prediction
- **Arxiv ID**: http://arxiv.org/abs/1907.02757v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.02757v1)
- **Published**: 2019-07-05 10:27:38+00:00
- **Updated**: 2019-07-05 10:27:38+00:00
- **Authors**: Wenjia Bai, Chen Chen, Giacomo Tarroni, Jinming Duan, Florian Guitton, Steffen E. Petersen, Yike Guo, Paul M. Matthews, Daniel Rueckert
- **Comment**: Accepted by MICCAI 2019
- **Journal**: None
- **Summary**: In the recent years, convolutional neural networks have transformed the field of medical image analysis due to their capacity to learn discriminative image features for a variety of classification and regression tasks. However, successfully learning these features requires a large amount of manually annotated data, which is expensive to acquire and limited by the available resources of expert image analysts. Therefore, unsupervised, weakly-supervised and self-supervised feature learning techniques receive a lot of attention, which aim to utilise the vast amount of available data, while at the same time avoid or substantially reduce the effort of manual annotation. In this paper, we propose a novel way for training a cardiac MR image segmentation network, in which features are learnt in a self-supervised manner by predicting anatomical positions. The anatomical positions serve as a supervisory signal and do not require extra manual annotation. We demonstrate that this seemingly simple task provides a strong signal for feature learning and with self-supervised learning, we achieve a high segmentation accuracy that is better than or comparable to a U-net trained from scratch, especially at a small data setting. When only five annotated subjects are available, the proposed method improves the mean Dice metric from 0.811 to 0.852 for short-axis image segmentation, compared to the baseline U-net.



### Data Efficient Unsupervised Domain Adaptation for Cross-Modality Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.02766v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.02766v2)
- **Published**: 2019-07-05 10:47:39+00:00
- **Updated**: 2019-08-12 19:58:10+00:00
- **Authors**: Cheng Ouyang, Konstantinos Kamnitsas, Carlo Biffi, Jinming Duan, Daniel Rueckert
- **Comment**: Accepted by MICCAI 2019
- **Journal**: None
- **Summary**: Deep learning models trained on medical images from a source domain (e.g. imaging modality) often fail when deployed on images from a different target domain, despite imaging common anatomical structures. Deep unsupervised domain adaptation (UDA) aims to improve the performance of a deep neural network model on a target domain, using solely unlabelled target domain data and labelled source domain data. However, current state-of-the-art methods exhibit reduced performance when target data is scarce. In this work, we introduce a new data efficient UDA method for multi-domain medical image segmentation. The proposed method combines a novel VAE-based feature prior matching, which is data-efficient, and domain adversarial training to learn a shared domain-invariant latent space which is exploited during segmentation. Our method is evaluated on a public multi-modality cardiac image segmentation dataset by adapting from the labelled source domain (3D MRI) to the unlabelled target domain (3D CT). We show that by using only one single unlabelled 3D CT scan, the proposed architecture outperforms the state-of-the-art in the same setting. Finally, we perform ablation studies on prior matching and domain adversarial training to shed light on the theoretical grounding of the proposed method.



### Degenerative Adversarial NeuroImage Nets: Generating Images that Mimic Disease Progression
- **Arxiv ID**: http://arxiv.org/abs/1907.02787v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.02787v2)
- **Published**: 2019-07-05 12:11:01+00:00
- **Updated**: 2019-08-30 18:56:35+00:00
- **Authors**: Daniele Ravi, Daniel C. Alexander, Neil P. Oxtoby
- **Comment**: Paper accepted for MICCAI 2019
- **Journal**: None
- **Summary**: Simulating images representative of neurodegenerative diseases is important for predicting patient outcomes and for validation of computational models of disease progression. This capability is valuable for secondary prevention clinical trials where outcomes and screening criteria involve neuroimaging. Traditional computational methods are limited by imposing a parametric model for atrophy and are extremely resource-demanding. Recent advances in deep learning have yielded data-driven models for longitudinal studies (e.g., face ageing) that are capable of generating synthetic images in real-time. Similar solutions can be used to model trajectories of atrophy in the brain, although new challenges need to be addressed to ensure accurate disease progression modelling. Here we propose Degenerative Adversarial NeuroImage Net (DaniNet) --- a new deep learning approach that learns to emulate the effect of neurodegeneration on MRI by simulating atrophy as a function of ages, and disease progression. DaniNet uses an underlying set of Support Vector Regressors (SVRs) trained to capture the patterns of regional intensity changes that accompany disease progression. DaniNet produces whole output images, consisting of 2D-MRI slices that are constrained to match regional predictions from the SVRs. DaniNet is also able to maintain the unique brain morphology of individuals. Adversarial training ensures realistic brain images and smooth temporal progression. We train our model using 9652 T1-weighted (longitudinal) MRI extracted from the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. We perform quantitative and qualitative evaluations on a separate test set of 1283 images (also from ADNI) demonstrating the ability of DaniNet to produce accurate and convincing synthetic images that emulate disease progression.



### Incremental Concept Learning via Online Generative Memory Recall
- **Arxiv ID**: http://arxiv.org/abs/1907.02788v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.02788v1)
- **Published**: 2019-07-05 12:13:46+00:00
- **Updated**: 2019-07-05 12:13:46+00:00
- **Authors**: Huaiyu Li, Weiming Dong, Bao-Gang Hu
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to learn more and more concepts over time from incrementally arriving data is essential for the development of a life-long learning system. However, deep neural networks often suffer from forgetting previously learned concepts when continually learning new concepts, which is known as catastrophic forgetting problem. The main reason for catastrophic forgetting is that the past concept data is not available and neural weights are changed during incrementally learning new concepts. In this paper, we propose a pseudo-rehearsal based class incremental learning approach to make neural networks capable of continually learning new concepts. We use a conditional generative adversarial network to consolidate old concepts memory and recall pseudo samples during learning new concepts and a balanced online memory recall strategy is to maximally maintain old memories. And we design a comprehensible incremental concept learning network as well as a concept contrastive loss to alleviate the magnitude of neural weights change. We evaluate the proposed approach on MNIST, Fashion-MNIST and SVHN datasets and compare with other rehearsal based approaches. The extensive experiments demonstrate the effectiveness of our approach.



### AI-based evaluation of the SDGs: The case of crop detection with earth observation data
- **Arxiv ID**: http://arxiv.org/abs/1907.02813v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.02813v1)
- **Published**: 2019-07-05 13:26:33+00:00
- **Updated**: 2019-07-05 13:26:33+00:00
- **Authors**: Natalia Efremova, Dennis West, Dmitry Zausaev
- **Comment**: ICLR workshop "AI for Social Good"
- **Journal**: None
- **Summary**: The framework of the seventeen sustainable development goals is a challenge for developers and researchers applying artificial intelligence (AI). AI and earth observations (EO) can provide reliable and disaggregated data for better monitoring of the sustainable development goals (SDGs). In this paper, we present an overview of SDG targets, which can be effectively measured with AI tools. We identify indicators with the most significant contribution from the AI and EO and describe an application of state-of-the-art machine learning models to one of the indicators. We describe an application of U-net with SE blocks for efficient segmentation of satellite imagery for crop detection. Finally, we demonstrate how AI can be more effectively applied in solutions directly contributing towards specific SDGs and propose further research on an AI-based evaluative infrastructure for SDGs.



### Visual Appearance Analysis of Forest Scenes for Monocular SLAM
- **Arxiv ID**: http://arxiv.org/abs/1907.02824v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1907.02824v1)
- **Published**: 2019-07-05 13:38:08+00:00
- **Updated**: 2019-07-05 13:38:08+00:00
- **Authors**: James Garforth, Barbara Webb
- **Comment**: Accepted to ICRA 2019
- **Journal**: None
- **Summary**: Monocular simultaneous localisation and mapping (SLAM) is a cheap and energy efficient way to enable Unmanned Aerial Vehicles (UAVs) to safely navigate managed forests and gather data crucial for monitoring tree health. SLAM research, however, has mostly been conducted in structured human environments, and as such is poorly adapted to unstructured forests. In this paper, we compare the performance of state of the art monocular SLAM systems on forest data and use visual appearance statistics to characterise the differences between forests and other environments, including a photorealistic simulated forest. We find that SLAM systems struggle with all but the most straightforward forest terrain and identify key attributes (lighting changes and in-scene motion) which distinguish forest scenes from "classic" urban datasets. These differences offer an insight into what makes forests harder to map and open the way for targeted improvements. We also demonstrate that even simulations that look impressive to the human eye can fail to properly reflect the difficult attributes of the environment they simulate, and provide suggestions for more closely mimicking natural scenes.



### Depth Restoration: A fast low-rank matrix completion via dual-graph regularization
- **Arxiv ID**: http://arxiv.org/abs/1907.02841v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.02841v4)
- **Published**: 2019-07-05 14:09:31+00:00
- **Updated**: 2020-01-08 09:29:44+00:00
- **Authors**: Wenxiang Zuo, Qiang Li, Xianming Liu
- **Comment**: The paper will be added more experiments. The main idea of the paper
  needs to be revamped. Please withdraw the paper
- **Journal**: None
- **Summary**: As a real scenes sensing approach, depth information obtains the widespread applications. However, resulting from the restriction of depth sensing technology, the depth map captured in practice usually suffers terrible noise and missing values at plenty of pixels. In this paper, we propose a fast low-rank matrix completion via dual-graph regularization for depth restoration. Specifically, the depth restoration can be transformed into a low-rank matrix completion problem. In order to complete the low-rank matrix and restore it to the depth map, the proposed dual-graph method containing the local and non-local graph regularizations exploits the local similarity of depth maps and the gradient consistency of depth-color counterparts respectively. In addition, the proposed approach achieves the high speed depth restoration due to closed-form solution. Experimental results demonstrate that the proposed method outperforms the state-of-the-art methods with respect to both objective and subjective quality evaluations, especially for serious depth degeneration.



### Distilling with Residual Network for Single Image Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/1907.02843v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.02843v1)
- **Published**: 2019-07-05 14:14:04+00:00
- **Updated**: 2019-07-05 14:14:04+00:00
- **Authors**: Xiaopeng Sun, Wen Lu, Rui Wang, Furui Bai
- **Comment**: 6 pages; Accepted to ICME2019
- **Journal**: None
- **Summary**: Recently, the deep convolutional neural network (CNN) has made remarkable progress in single image super resolution(SISR). However, blindly using the residual structure and dense structure to extract features from LR images, can cause the network to be bloated and difficult to train. To address these problems, we propose a simple and efficient distilling with residual network(DRN) for SISR. In detail, we propose residual distilling block(RDB) containing two branches, while one branch performs a residual operation and the other branch distills effective information. To further improve efficiency, we design residual distilling group(RDG) by stacking some RDBs and one long skip connection, which can effectively extract local features and fuse them with global features. These efficient features beneficially contribute to image reconstruction. Experiments on benchmark datasets demonstrate that our DRN is superior to the state-of-the-art methods, specifically has a better trade-off between performance and model size.



### Cardiac MRI Segmentation with Strong Anatomical Guarantees
- **Arxiv ID**: http://arxiv.org/abs/1907.02865v2
- **DOI**: 10.1007/978-3-030-32245-8_70
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.02865v2)
- **Published**: 2019-07-05 14:45:21+00:00
- **Updated**: 2019-08-26 16:19:41+00:00
- **Authors**: Nathan Painchaud, Youssef Skandarani, Thierry Judge, Olivier Bernard, Alain Lalande, Pierre-Marc Jodoin
- **Comment**: 9 pages, accepted for MICCAI 2019; camera ready corrections,
  acknowledgments
- **Journal**: in Medical Image Computing and Computer Assisted Intervention -
  MICCAI 2019, 2019, pp. 632-640
- **Summary**: Recent publications have shown that the segmentation accuracy of modern-day convolutional neural networks (CNN) applied on cardiac MRI can reach the inter-expert variability, a great achievement in this area of research. However, despite these successes, CNNs still produce anatomically inaccurate segmentations as they provide no guarantee on the anatomical plausibility of their outcome, even when using a shape prior. In this paper, we propose a cardiac MRI segmentation method which always produces anatomically plausible results. At the core of the method is an adversarial variational autoencoder (aVAE) whose latent space encodes a smooth manifold on which lies a large spectrum of valid cardiac shapes. This aVAE is used to automatically warp anatomically inaccurate cardiac shapes towards a close but correct shape. Our method can accommodate any cardiac segmentation method and convert its anatomically implausible results to plausible ones without affecting its overall geometric and clinical metrics. With our method, CNNs can now produce results that are both within the inter-expert variability and always anatomically plausible.



### A new method for determining the filled point of the tooth by Bit-Plane Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1907.02873v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.02873v1)
- **Published**: 2019-07-05 14:54:41+00:00
- **Updated**: 2019-07-05 14:54:41+00:00
- **Authors**: Zahra Alidousti, Maryam Taghizadeh Dehkordi
- **Comment**: 2019 IEEE 4th Conference on Technology In Electrical and Computer
  Engineering (ETECH 2019) Information and Communication Technology (ICT)
  Tehran, Iran
- **Journal**: None
- **Summary**: Up to now, researchers have applied segmentation techniques in their studies on teeth images, with construction on tooth root length and depth. In this paper, a new approach to the exact identification of the filled points of the tooth is proposed. In this method, the filled teeth are detection by applying the Bit-Plane algorithm on the OPG images. The novelty of the proposed method is that we can use it in medicine for the detection of dental filling and we calculate and present the area of the filled points which may help dentists to assess the filled point of the tooth. The experimental results, confirmed by the dentists, clearly indicate that this method is able to separate the filled points from the rest of healthy teeth completely.



### Generating large labeled data sets for laparoscopic image processing tasks using unpaired image-to-image translation
- **Arxiv ID**: http://arxiv.org/abs/1907.02882v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.02882v1)
- **Published**: 2019-07-05 15:10:20+00:00
- **Updated**: 2019-07-05 15:10:20+00:00
- **Authors**: Micha Pfeiffer, Isabel Funke, Maria R. Robu, Sebastian Bodenstedt, Leon Strenger, Sandy Engelhardt, Tobias Roß, Matthew J. Clarkson, Kurinchi Gurusamy, Brian R. Davidson, Lena Maier-Hein, Carina Riediger, Thilo Welsch, Jürgen Weitz, Stefanie Speidel
- **Comment**: Accepted at MICCAI 2019
- **Journal**: None
- **Summary**: In the medical domain, the lack of large training data sets and benchmarks is often a limiting factor for training deep neural networks. In contrast to expensive manual labeling, computer simulations can generate large and fully labeled data sets with a minimum of manual effort. However, models that are trained on simulated data usually do not translate well to real scenarios. To bridge the domain gap between simulated and real laparoscopic images, we exploit recent advances in unpaired image-to-image translation. We extent an image-to-image translation method to generate a diverse multitude of realistically looking synthetic images based on images from a simple laparoscopy simulation. By incorporating means to ensure that the image content is preserved during the translation process, we ensure that the labels given for the simulated images remain valid for their realistically looking translations. This way, we are able to generate a large, fully labeled synthetic data set of laparoscopic images with realistic appearance. We show that this data set can be used to train models for the task of liver segmentation of laparoscopic images. We achieve average dice scores of up to 0.89 in some patients without manually labeling a single laparoscopic image and show that using our synthetic data to pre-train models can greatly improve their performance. The synthetic data set will be made publicly available, fully labeled with segmentation maps, depth maps, normal maps, and positions of tools and camera (http://opencas.dkfz.de/image2image).



### A Performance Evaluation of Correspondence Grouping Methods for 3D Rigid Data Matching
- **Arxiv ID**: http://arxiv.org/abs/1907.02890v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.02890v1)
- **Published**: 2019-07-05 15:22:24+00:00
- **Updated**: 2019-07-05 15:22:24+00:00
- **Authors**: Jiaqi Yang, Ke Xian, Peng Wang, Yanning Zhang
- **Comment**: Extension of 3DV 2017 paper
- **Journal**: None
- **Summary**: Seeking consistent point-to-point correspondences between 3D rigid data (point clouds, meshes, or depth maps) is a fundamental problem in 3D computer vision. While a number of correspondence selection methods have been proposed in recent years, their advantages and shortcomings remain unclear regarding different applications and perturbations. To fill this gap, this paper gives a comprehensive evaluation of nine state-of-the-art 3D correspondence grouping methods. A good correspondence grouping algorithm is expected to retrieve as many as inliers from initial feature matches, giving a rise in both precision and recall as well as facilitating accurate transformation estimation. Toward this rule, we deploy experiments on three benchmarks with different application contexts including shape retrieval, 3D object recognition, and point cloud registration together with various perturbations such as noise, point density variation, clutter, occlusion, partial overlap, different scales of initial correspondences, and different combinations of keypoint detectors and descriptors. The rich variety of application scenarios and nuisances result in different spatial distributions and inlier ratios of initial feature correspondences, thus enabling a thorough evaluation. Based on the outcomes, we give a summary of the traits, merits, and demerits of evaluated approaches and indicate some potential future research directions.



### Improved local search for graph edit distance
- **Arxiv ID**: http://arxiv.org/abs/1907.02929v2
- **DOI**: 10.1016/j.patrec.2019.10.028
- **Categories**: **cs.DS**, cs.CV, cs.LG, 05C60, 68W25, 65D15, 68T20
- **Links**: [PDF](http://arxiv.org/pdf/1907.02929v2)
- **Published**: 2019-07-05 16:52:40+00:00
- **Updated**: 2019-11-26 11:55:25+00:00
- **Authors**: Nicolas Boria, David B. Blumenthal, Sébastien Bougleux, Luc Brun
- **Comment**: None
- **Journal**: Pattern Recognition Letters 129, pages 19-25, 2020
- **Summary**: The graph edit distance (GED) measures the dissimilarity between two graphs as the minimal cost of a sequence of elementary operations transforming one graph into another. This measure is fundamental in many areas such as structural pattern recognition or classification. However, exactly computing GED is NP-hard. Among different classes of heuristic algorithms that were proposed to compute approximate solutions, local search based algorithms provide the tightest upper bounds for GED. In this paper, we present K-REFINE and RANDPOST. K-REFINE generalizes and improves an existing local search algorithm and performs particularly well on small graphs. RANDPOST is a general warm start framework that stochastically generates promising initial solutions to be used by any local search based GED algorithm. It is particularly efficient on large graphs. An extensive empirical evaluation demonstrates that both K-REFINE and RANDPOST perform excellently in practice.



### Visualizing Uncertainty and Saliency Maps of Deep Convolutional Neural Networks for Medical Imaging Applications
- **Arxiv ID**: http://arxiv.org/abs/1907.02940v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.02940v1)
- **Published**: 2019-07-05 17:23:04+00:00
- **Updated**: 2019-07-05 17:23:04+00:00
- **Authors**: Jae Duk Seo
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models are now used in many different industries, while in certain domains safety is not a critical issue in the medical field it is a huge concern. Not only, we want the models to generalize well but we also want to know the models confidence respect to its decision and which features matter the most. Our team aims to develop a full pipeline in which not only displays the uncertainty of the models decision but also, the saliency map to show which sets of pixels of the input image contribute most to the predictions.



### A Novel Deep Learning Pipeline for Retinal Vessel Detection in Fluorescein Angiography
- **Arxiv ID**: http://arxiv.org/abs/1907.02946v2
- **DOI**: 10.1109/TIP.2020.2991530
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.02946v2)
- **Published**: 2019-07-05 17:27:44+00:00
- **Updated**: 2020-05-10 18:35:54+00:00
- **Authors**: Li Ding, Mohammad H. Bawany, Ajay E. Kuriyan, Rajeev S. Ramchandran, Charles C. Wykoff, Gaurav Sharma
- **Comment**: A paper based on this pre-print has been published (after revisions)
  in IEEE Trans. Image Processing. See the first footnote on front page of the
  article for details
- **Journal**: IEEE Trans. Image Proc., 29(1), 2020
- **Summary**: While recent advances in deep learning have significantly advanced the state of the art for vessel detection in color fundus (CF) images, the success for detecting vessels in fluorescein angiography (FA) has been stymied due to the lack of labeled ground truth datasets. We propose a novel pipeline to detect retinal vessels in FA images using deep neural networks that reduces the effort required for generating labeled ground truth data by combining two key components: cross-modality transfer and human-in-the-loop learning. The cross-modality transfer exploits concurrently captured CF and fundus FA images. Binary vessels maps are first detected from CF images with a pre-trained neural network and then are geometrically registered with and transferred to FA images via robust parametric chamfer alignment to a preliminary FA vessel detection obtained with an unsupervised technique. Using the transferred vessels as initial ground truth labels for deep learning, the human-in-the-loop approach progressively improves the quality of the ground truth labeling by iterating between deep-learning and labeling. The approach significantly reduces manual labeling effort while increasing engagement. We highlight several important considerations for the proposed methodology and validate the performance on three datasets. Experimental results demonstrate that the proposed pipeline significantly reduces the annotation effort and the resulting deep learning methods outperform prior existing FA vessel detection methods by a significant margin. A new public dataset, RECOVERY-FA19, is introduced that includes high-resolution ultra-widefield images and accurately labeled ground truth binary vessel maps.



### Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions
- **Arxiv ID**: http://arxiv.org/abs/1907.02957v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.02957v2)
- **Published**: 2019-07-05 17:57:57+00:00
- **Updated**: 2020-02-18 05:05:45+00:00
- **Authors**: Yao Qin, Nicholas Frosst, Sara Sabour, Colin Raffel, Garrison Cottrell, Geoffrey Hinton
- **Comment**: None
- **Journal**: ICLR 2020
- **Summary**: Adversarial examples raise questions about whether neural network models are sensitive to the same visual features as humans. In this paper, we first detect adversarial examples or otherwise corrupted images based on a class-conditional reconstruction of the input. To specifically attack our detection mechanism, we propose the Reconstructive Attack which seeks both to cause a misclassification and a low reconstruction error. This reconstructive attack produces undetected adversarial examples but with much smaller success rate. Among all these attacks, we find that CapsNets always perform better than convolutional networks. Then, we diagnose the adversarial examples for CapsNets and find that the success of the reconstructive attack is highly related to the visual similarity between the source and target class. Additionally, the resulting perturbations can cause the input image to appear visually more like the target class and hence become non-adversarial. This suggests that CapsNets use features that are more aligned with human perception and have the potential to address the central issue raised by adversarial examples.



### High-throughput Onboard Hyperspectral Image Compression with Ground-based CNN Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1907.02959v1
- **DOI**: 10.1109/TGRS.2019.2927434
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.02959v1)
- **Published**: 2019-07-05 17:59:25+00:00
- **Updated**: 2019-07-05 17:59:25+00:00
- **Authors**: Diego Valsesia, Enrico Magli
- **Comment**: None
- **Journal**: None
- **Summary**: Compression of hyperspectral images onboard of spacecrafts is a tradeoff between the limited computational resources and the ever-growing spatial and spectral resolution of the optical instruments. As such, it requires low-complexity algorithms with good rate-distortion performance and high throughput. In recent years, the Consultative Committee for Space Data Systems (CCSDS) has focused on lossless and near-lossless compression approaches based on predictive coding, resulting in the recently published CCSDS 123.0-B-2 recommended standard. While the in-loop reconstruction of quantized prediction residuals provides excellent rate-distortion performance for the near-lossless operating mode, it significantly constrains the achievable throughput due to data dependencies. In this paper, we study the performance of a faster method based on prequantization of the image followed by a lossless predictive compressor. While this is well known to be suboptimal, one can exploit powerful signal models to reconstruct the image at the ground segment, recovering part of the suboptimality. In particular, we show that convolutional neural networks can be used for this task and that they can recover the whole SNR drop incurred at a bitrate of 2 bits per pixel.



### Embodied Vision-and-Language Navigation with Dynamic Convolutional Filters
- **Arxiv ID**: http://arxiv.org/abs/1907.02985v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.02985v2)
- **Published**: 2019-07-05 18:02:30+00:00
- **Updated**: 2019-09-25 17:12:21+00:00
- **Authors**: Federico Landi, Lorenzo Baraldi, Massimiliano Corsini, Rita Cucchiara
- **Comment**: BMVC 2019 (Oral). Code is available at
  https://github.com/aimagelab/DynamicConv-agent
- **Journal**: None
- **Summary**: In Vision-and-Language Navigation (VLN), an embodied agent needs to reach a target destination with the only guidance of a natural language instruction. To explore the environment and progress towards the target location, the agent must perform a series of low-level actions, such as rotate, before stepping ahead. In this paper, we propose to exploit dynamic convolutional filters to encode the visual information and the lingual description in an efficient way. Differently from some previous works that abstract from the agent perspective and use high-level navigation spaces, we design a policy which decodes the information provided by dynamic convolution into a series of low-level, agent friendly actions. Results show that our model exploiting dynamic filters performs better than other architectures with traditional convolution, being the new state of the art for embodied VLN in the low-level action space. Additionally, we attempt to categorize recent work on VLN depending on their architectural choices and distinguish two main groups: we call them low-level actions and high-level actions models. To the best of our knowledge, we are the first to propose this analysis and categorization for VLN.



### Blind Universal Bayesian Image Denoising with Gaussian Noise Level Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.03029v2
- **DOI**: 10.1109/TIP.2020.2976814
- **Categories**: **cs.CV**, eess.IV, I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/1907.03029v2)
- **Published**: 2019-07-05 21:39:50+00:00
- **Updated**: 2020-03-07 18:32:11+00:00
- **Authors**: Majed El Helou, Sabine Süsstrunk
- **Comment**: Final uncompressed TIP version available online in open access (DOI
  attached)
- **Journal**: IEEE Transactions on Image Processing, vol. 29, pp. 4885-4897,
  2020
- **Summary**: Blind and universal image denoising consists of using a unique model that denoises images with any level of noise. It is especially practical as noise levels do not need to be known when the model is developed or at test time. We propose a theoretically-grounded blind and universal deep learning image denoiser for additive Gaussian noise removal. Our network is based on an optimal denoising solution, which we call fusion denoising. It is derived theoretically with a Gaussian image prior assumption. Synthetic experiments show our network's generalization strength to unseen additive noise levels. We also adapt the fusion denoising network architecture for image denoising on real images. Our approach improves real-world grayscale additive image denoising PSNR results for training noise levels and further on noise levels not seen during training. It also improves state-of-the-art color image denoising performance on every single noise level, by an average of 0.1dB, whether trained on or not.



### Dependency-aware Attention Control for Unconstrained Face Recognition with Image Sets
- **Arxiv ID**: http://arxiv.org/abs/1907.03030v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1907.03030v1)
- **Published**: 2019-07-05 21:40:56+00:00
- **Updated**: 2019-07-05 21:40:56+00:00
- **Authors**: Xiaofeng Liu, B. V. K Vijaya Kumar, Chao Yang, Qingming Tang, Jane You
- **Comment**: Fixed the unreadable code in CVF version. arXiv admin note: text
  overlap with arXiv:1707.00130 by other authors
- **Journal**: None
- **Summary**: This paper targets the problem of image set-based face verification and identification. Unlike traditional single media (an image or video) setting, we encounter a set of heterogeneous contents containing orderless images and videos. The importance of each image is usually considered either equal or based on their independent quality assessment. How to model the relationship of orderless images within a set remains a challenge. We address this problem by formulating it as a Markov Decision Process (MDP) in the latent space. Specifically, we first present a dependency-aware attention control (DAC) network, which resorts to actor-critic reinforcement learning for sequential attention decision of each image embedding to fully exploit the rich correlation cues among the unordered images. Moreover, we introduce its sample-efficient variant with off-policy experience replay to speed up the learning process. The pose-guided representation scheme can further boost the performance at the extremes of the pose variation.



### Video Question Generation via Cross-Modal Self-Attention Networks Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.03049v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1907.03049v3)
- **Published**: 2019-07-05 23:47:04+00:00
- **Updated**: 2020-02-16 21:11:03+00:00
- **Authors**: Yu-Siang Wang, Hung-Ting Su, Chen-Hsi Chang, Zhe-Yu Liu, Winston H. Hsu
- **Comment**: Accepted by ICASSP 2020
- **Journal**: None
- **Summary**: We introduce a novel task, Video Question Generation (Video QG). A Video QG model automatically generates questions given a video clip and its corresponding dialogues. Video QG requires a range of skills -- sentence comprehension, temporal relation, the interplay between vision and language, and the ability to ask meaningful questions. To address this, we propose a novel semantic rich cross-modal self-attention (SRCMSA) network to aggregate the multi-modal and diverse features. To be more precise, we enhance the video frames semantic by integrating the object-level information, and we jointly consider the cross-modal attention for the video question generation task. Excitingly, our proposed model remarkably improves the baseline from 7.58 to 14.48 in the BLEU-4 score on the TVQA dataset. Most of all, we arguably pave a novel path toward understanding the challenging video input and we provide detailed analysis in terms of diversity, which ushers the avenues for future investigations.



