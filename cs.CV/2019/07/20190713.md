# Arxiv Papers in cs.CV on 2019-07-13
### Learning Complex Basis Functions for Invariant Representations of Audio
- **Arxiv ID**: http://arxiv.org/abs/1907.05982v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1907.05982v1)
- **Published**: 2019-07-13 00:23:26+00:00
- **Updated**: 2019-07-13 00:23:26+00:00
- **Authors**: Stefan Lattner, Monika Dörfler, Andreas Arzt
- **Comment**: Paper accepted at the 20th International Society for Music
  Information Retrieval Conference, ISMIR 2019, Delft, The Netherlands,
  November 4-8; 8 pages, 4 figures, 4 tables
- **Journal**: None
- **Summary**: Learning features from data has shown to be more successful than using hand-crafted features for many machine learning tasks. In music information retrieval (MIR), features learned from windowed spectrograms are highly variant to transformations like transposition or time-shift. Such variances are undesirable when they are irrelevant for the respective MIR task. We propose an architecture called Complex Autoencoder (CAE) which learns features invariant to orthogonal transformations. Mapping signals onto complex basis functions learned by the CAE results in a transformation-invariant "magnitude space" and a transformation-variant "phase space". The phase space is useful to infer transformations between data pairs. When exploiting the invariance-property of the magnitude space, we achieve state-of-the-art results in audio-to-score alignment and repeated section discovery for audio. A PyTorch implementation of the CAE, including the repeated section discovery method, is available online.



### SynthText3D: Synthesizing Scene Text Images from 3D Virtual Worlds
- **Arxiv ID**: http://arxiv.org/abs/1907.06007v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06007v2)
- **Published**: 2019-07-13 04:18:04+00:00
- **Updated**: 2019-12-09 12:17:41+00:00
- **Authors**: Minghui Liao, Boyu Song, Shangbang Long, Minghang He, Cong Yao, Xiang Bai
- **Comment**: Accepted by SCIS
- **Journal**: None
- **Summary**: With the development of deep neural networks, the demand for a significant amount of annotated training data becomes the performance bottlenecks in many fields of research and applications. Image synthesis can generate annotated images automatically and freely, which gains increasing attention recently. In this paper, we propose to synthesize scene text images from the 3D virtual worlds, where the precise descriptions of scenes, editable illumination/visibility, and realistic physics are provided. Different from the previous methods which paste the rendered text on static 2D images, our method can render the 3D virtual scene and text instances as an entirety. In this way, real-world variations, including complex perspective transformations, various illuminations, and occlusions, can be realized in our synthesized scene text images. Moreover, the same text instances with various viewpoints can be produced by randomly moving and rotating the virtual camera, which acts as human eyes. The experiments on the standard scene text detection benchmarks using the generated synthetic data demonstrate the effectiveness and superiority of the proposed method. The code and synthetic data is available at: https://github.com/MhLiao/SynthText3D



### A Cost Effective Solution for Road Crack Inspection using Cameras and Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.06014v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.06014v2)
- **Published**: 2019-07-13 05:43:06+00:00
- **Updated**: 2019-10-22 04:29:26+00:00
- **Authors**: Qipei Mei, Mustafa Gül
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic crack detection on pavement surfaces is an important research field in the scope of developing an intelligent transportation infrastructure system. In this paper, a cost effective solution for road crack inspection by mounting commercial grade sport camera, GoPro, on the rear of the moving vehicle is introduced. Also, a novel method called ConnCrack combining conditional Wasserstein generative adversarial network and connectivity maps is proposed for road crack detection. In this method, a 121-layer densely connected neural network with deconvolution layers for multi-level feature fusion is used as generator, and a 5-layer fully convolutional network is used as discriminator. To overcome the scattered output issue related to deconvolution layers, connectivity maps are introduced to represent the crack information within the proposed ConnCrack. The proposed method is tested on a publicly available dataset as well our collected data. The results show that the proposed method achieves state-of-the-art performance compared with other existing methods in terms of precision, recall and F1 score.



### Structure-Aware Residual Pyramid Network for Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1907.06023v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06023v1)
- **Published**: 2019-07-13 07:31:24+00:00
- **Updated**: 2019-07-13 07:31:24+00:00
- **Authors**: Xiaotian Chen, Xuejin Chen, Zheng-Jun Zha
- **Comment**: 7pages, 7figures, Accepted by the 28th International Joint Conference
  on Artificial Intelligence (IJCAI-2019)
- **Journal**: None
- **Summary**: Monocular depth estimation is an essential task for scene understanding. The underlying structure of objects and stuff in a complex scene is critical to recovering accurate and visually-pleasing depth maps. Global structure conveys scene layouts, while local structure reflects shape details. Recently developed approaches based on convolutional neural networks (CNNs) significantly improve the performance of depth estimation. However, few of them take into account multi-scale structures in complex scenes. In this paper, we propose a Structure-Aware Residual Pyramid Network (SARPN) to exploit multi-scale structures for accurate depth prediction. We propose a Residual Pyramid Decoder (RPD) which expresses global scene structure in upper levels to represent layouts, and local structure in lower levels to present shape details. At each level, we propose Residual Refinement Modules (RRM) that predict residual maps to progressively add finer structures on the coarser structure predicted at the upper level. In order to fully exploit multi-scale image features, an Adaptive Dense Feature Fusion (ADFF) module, which adaptively fuses effective features from all scales for inferring structures of each scale, is introduced. Experiment results on the challenging NYU-Depth v2 dataset demonstrate that our proposed approach achieves state-of-the-art performance in both qualitative and quantitative evaluation. The code is available at https://github.com/Xt-Chen/SARPN.



### M3D-RPN: Monocular 3D Region Proposal Network for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1907.06038v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06038v2)
- **Published**: 2019-07-13 09:40:22+00:00
- **Updated**: 2019-08-11 15:08:13+00:00
- **Authors**: Garrick Brazil, Xiaoming Liu
- **Comment**: To appear in ICCV 2019 as an oral presentation
- **Journal**: None
- **Summary**: Understanding the world in 3D is a critical component of urban autonomous driving. Generally, the combination of expensive LiDAR sensors and stereo RGB imaging has been paramount for successful 3D object detection algorithms, whereas monocular image-only methods experience drastically reduced performance. We propose to reduce the gap by reformulating the monocular 3D detection problem as a standalone 3D region proposal network. We leverage the geometric relationship of 2D and 3D perspectives, allowing 3D boxes to utilize well-known and powerful convolutional features generated in the image-space. To help address the strenuous 3D parameter estimations, we further design depth-aware convolutional layers which enable location specific feature development and in consequence improved 3D scene understanding. Compared to prior work in monocular 3D detection, our method consists of only the proposed 3D region proposal network rather than relying on external networks, data, or multiple stages. M3D-RPN is able to significantly improve the performance of both monocular 3D Object Detection and Bird's Eye View tasks within the KITTI urban autonomous driving dataset, while efficiently using a shared multi-class model.



### Learning better generative models for dexterous, single-view grasping of novel objects
- **Arxiv ID**: http://arxiv.org/abs/1907.06053v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, I.2.9; I.2.10; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1907.06053v1)
- **Published**: 2019-07-13 11:37:32+00:00
- **Updated**: 2019-07-13 11:37:32+00:00
- **Authors**: Marek Kopicki, Dominik Belter, Jeremy L. Wyatt
- **Comment**: 19 pages, 15 figures, 7 tables
- **Journal**: None
- **Summary**: This paper concerns the problem of how to learn to grasp dexterously, so as to be able to then grasp novel objects seen only from a single view-point. Recently, progress has been made in data-efficient learning of generative grasp models which transfer well to novel objects. These generative grasp models are learned from demonstration (LfD). One weakness is that, as this paper shall show, grasp transfer under challenging single view conditions is unreliable. Second, the number of generative model elements rises linearly in the number of training examples. This, in turn, limits the potential of these generative models for generalisation and continual improvement. In this paper, it is shown how to address these problems. Several technical contributions are made: (i) a view-based model of a grasp; (ii) a method for combining and compressing multiple grasp models; (iii) a new way of evaluating contacts that is used both to generate and to score grasps. These, together, improve both grasp performance and reduce the number of models learned for grasp transfer. These advances, in turn, also allow the introduction of autonomous training, in which the robot learns from self-generated grasps. Evaluation on a challenging test set shows that, with innovations (i)-(iii) deployed, grasp transfer success rises from 55.1% to 81.6%. By adding autonomous training this rises to 87.8%. These differences are statistically significant. In total, across all experiments, 539 test grasps were executed on real objects.



### Using dynamic routing to extract intermediate features for developing scalable capsule networks
- **Arxiv ID**: http://arxiv.org/abs/1907.06062v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1907.06062v1)
- **Published**: 2019-07-13 12:12:36+00:00
- **Updated**: 2019-07-13 12:12:36+00:00
- **Authors**: Bodhisatwa Mandal, Swarnendu Ghosh, Ritesh Sarkhel, Nibaran Das, Mita Nasipuri
- **Comment**: Second International Conference on Advanced Computational and
  Communication Paradigms held at Sikkim Manipal Institute of Technology,
  Sikkim, India during February 25 - 28 , 2019
- **Journal**: None
- **Summary**: Capsule networks have gained a lot of popularity in short time due to its unique approach to model equivariant class specific properties as capsules from images. However the dynamic routing algorithm comes with a steep computational complexity. In the proposed approach we aim to create scalable versions of the capsule networks that are much faster and provide better accuracy in problems with higher number of classes. By using dynamic routing to extract intermediate features instead of generating output class specific capsules, a large increase in the computational speed has been observed. Moreover, by extracting equivariant feature capsules instead of class specific capsules, the generalization capability of the network has also increased as a result of which there is a boost in accuracy.



### Image Evolution Trajectory Prediction and Classification from Baseline using Learning-based Patch Atlas Selection for Early Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/1907.06064v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.06064v1)
- **Published**: 2019-07-13 12:17:00+00:00
- **Updated**: 2019-07-13 12:17:00+00:00
- **Authors**: Can Gafuroglu, Islem Rekik
- **Comment**: None
- **Journal**: None
- **Summary**: Patients initially diagnosed with early mild cognitive impairment (eMCI) are known to be a clinically heterogeneous group with very subtle patterns of brain atrophy. To examine the boarders between normal controls (NC) and eMCI, Magnetic Resonance Imaging (MRI) was extensively used as a non-invasive imaging modality to pin-down subtle changes in brain images of MCI patients. However, eMCI research remains limited by the number of available MRI acquisition timepoints. Ideally, one would learn how to diagnose MCI patients in an early stage from MRI data acquired at a single timepoint, while leveraging 'non-existing' follow-up observations. To this aim, we propose novel supervised and unsupervised frameworks that learn how to jointly predict and label the evolution trajectory of intensity patches, each seeded at a specific brain landmark, from a baseline intensity patch. Specifically, both strategies aim to identify the best training atlas patches at baseline timepoint to predict and classify the evolution trajectory of a given testing baseline patch. The supervised technique learns how to select the best atlas patches by training bidirectional mappings from the space of pairwise patch similarities to their corresponding prediction errors -when one patch was used to predict the other. On the other hand, the unsupervised technique learns a manifold of baseline atlas and testing patches using multiple kernels to well capture patch distributions at multiple scales. Once the best baseline atlas patches are selected, we retrieve their evolution trajectories and average them to predict the evolution trajectory of the testing baseline patch. Next, we input the predicted trajectories to an ensemble of linear classifiers, each trained at a specific landmark. Our classification accuracy increased by up to 10% points in comparison to single timepoint-based classification methods.



### ALFA: Agglomerative Late Fusion Algorithm for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1907.06067v1
- **DOI**: 10.1109/ICPR.2018.8545182
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06067v1)
- **Published**: 2019-07-13 12:30:37+00:00
- **Updated**: 2019-07-13 12:30:37+00:00
- **Authors**: Evgenii Razinkov, Iuliia Saveleva, Jiři Matas
- **Comment**: E. Razinkov, I. Saveleva and J. Matas, "ALFA: Agglomerative Late
  Fusion Algorithm for Object Detection," 2018 24th International Conference on
  Pattern Recognition (ICPR), Beijing, 2018, pp. 2594-2599
- **Journal**: None
- **Summary**: We propose ALFA - a novel late fusion algorithm for object detection. ALFA is based on agglomerative clustering of object detector predictions taking into consideration both the bounding box locations and the class scores. Each cluster represents a single object hypothesis whose location is a weighted combination of the clustered bounding boxes.   ALFA was evaluated using combinations of a pair (SSD and DeNet) and a triplet (SSD, DeNet and Faster R-CNN) of recent object detectors that are close to the state-of-the-art. ALFA achieves state of the art results on PASCAL VOC 2007 and PASCAL VOC 2012, outperforming the individual detectors as well as baseline combination strategies, achieving up to 32% lower error than the best individual detectors and up to 6% lower error than the reference fusion algorithm DBF - Dynamic Belief Fusion.



### S&CNet: Monocular Depth Completion for Autonomous Systems and 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1907.06071v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.06071v2)
- **Published**: 2019-07-13 13:07:31+00:00
- **Updated**: 2019-08-29 09:32:59+00:00
- **Authors**: Lei Zhang, Weihai Chen, Chao Hu, Xingming Wu, Zhengguo Li
- **Comment**: 10 pages,8 figures
- **Journal**: None
- **Summary**: Dense depth completion is essential for autonomous systems and 3D reconstruction. In this paper, a lightweight yet efficient network (S\&CNet) is proposed to obtain a good trade-off between efficiency and accuracy for the dense depth completion. A dual-stream attention module (S\&C enhancer) is introduced to measure both spatial-wise and the channel-wise global-range relationship of extracted features so as to improve the performance. A coarse-to-fine network is designed and the proposed S\&C enhancer is plugged into the coarse estimation network between its encoder and decoder network. Experimental results demonstrate that our approach achieves competitive performance with existing works on KITTI dataset but almost four times faster. The proposed S\&C enhancer can be plugged into other existing works and boost their performance significantly with a negligible additional computational cost.



### Adaptive Context Encoding Module for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.06082v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06082v1)
- **Published**: 2019-07-13 14:02:21+00:00
- **Updated**: 2019-07-13 14:02:21+00:00
- **Authors**: Congcong Wang, Faouzi Alaya Cheikh, Azeddine Beghdadi, Ole Jakob Elle
- **Comment**: None
- **Journal**: None
- **Summary**: The object sizes in images are diverse, therefore, capturing multiple scale context information is essential for semantic segmentation. Existing context aggregation methods such as pyramid pooling module (PPM) and atrous spatial pyramid pooling (ASPP) design different pooling size or atrous rate, such that multiple scale information is captured. However, the pooling sizes and atrous rates are chosen manually and empirically. In order to capture object context information adaptively, in this paper, we propose an adaptive context encoding (ACE) module based on deformable convolution operation to argument multiple scale information. Our ACE module can be embedded into other Convolutional Neural Networks (CNN) easily for context aggregation. The effectiveness of the proposed module is demonstrated on Pascal-Context and ADE20K datasets. Although our proposed ACE only consists of three deformable convolution blocks, it outperforms PPM and ASPP in terms of mean Intersection of Union (mIoU) on both datasets. All the experiment study confirms that our proposed module is effective as compared to the state-of-the-art methods.



### Motion Segmentation Using Locally Affine Atom Voting
- **Arxiv ID**: http://arxiv.org/abs/1907.06091v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06091v1)
- **Published**: 2019-07-13 14:55:32+00:00
- **Updated**: 2019-07-13 14:55:32+00:00
- **Authors**: Erez Posner, Rami Hagege
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: We present a novel method for motion segmentation called LAAV (Locally Affine Atom Voting). Our model's main novelty is using sets of features to segment motion for all features in the scene. LAAV acts as a pre-processing pipeline stage for features in the image, followed by a fine-tuned version of the state-of-the-art Random Voting (RV) method. Unlike standard approaches, LAAV segments motion using feature-set affinities instead of pair-wise affinities between all features; therefore, it significantly simplifies complex scenarios and reduces the computational cost without a loss of accuracy. We describe how the challenges encountered by using previously suggested approaches are addressed using our model. We then compare our algorithm with several state-of-the-art methods. Experiments shows that our approach achieves the most accurate motion segmentation results and, in the presence of measurement noise, achieves comparable results to the other algorithms.



### Multi-Task Recurrent Convolutional Network with Correlation Loss for Surgical Video Analysis
- **Arxiv ID**: http://arxiv.org/abs/1907.06099v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.06099v1)
- **Published**: 2019-07-13 15:49:00+00:00
- **Updated**: 2019-07-13 15:49:00+00:00
- **Authors**: Yueming Jin, Huaxia Li, Qi Dou, Hao Chen, Jing Qin, Chi-Wing Fu, Pheng-Ann Heng
- **Comment**: Minor Revision at Medical Image Analysis
- **Journal**: None
- **Summary**: Surgical tool presence detection and surgical phase recognition are two fundamental yet challenging tasks in surgical video analysis and also very essential components in various applications in modern operating rooms. While these two analysis tasks are highly correlated in clinical practice as the surgical process is well-defined, most previous methods tackled them separately, without making full use of their relatedness. In this paper, we present a novel method by developing a multi-task recurrent convolutional network with correlation loss (MTRCNet-CL) to exploit their relatedness to simultaneously boost the performance of both tasks. Specifically, our proposed MTRCNet-CL model has an end-to-end architecture with two branches, which share earlier feature encoders to extract general visual features while holding respective higher layers targeting for specific tasks. Given that temporal information is crucial for phase recognition, long-short term memory (LSTM) is explored to model the sequential dependencies in the phase recognition branch. More importantly, a novel and effective correlation loss is designed to model the relatedness between tool presence and phase identification of each video frame, by minimizing the divergence of predictions from the two branches. Mutually leveraging both low-level feature sharing and high-level prediction correlating, our MTRCNet-CL method can encourage the interactions between the two tasks to a large extent, and hence can bring about benefits to each other. Extensive experiments on a large surgical video dataset (Cholec80) demonstrate outstanding performance of our proposed method, consistently exceeding the state-of-the-art methods by a large margin (e.g., 89.1% v.s. 81.0% for the mAP in tool presence detection and 87.4% v.s. 84.5% for F1 score in phase recognition). The code can be found on our project website.



### Detecting Spoofing Attacks Using VGG and SincNet: BUT-Omilia Submission to ASVspoof 2019 Challenge
- **Arxiv ID**: http://arxiv.org/abs/1907.12908v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1907.12908v1)
- **Published**: 2019-07-13 17:27:40+00:00
- **Updated**: 2019-07-13 17:27:40+00:00
- **Authors**: Hossein Zeinali, Themos Stafylakis, Georgia Athanasopoulou, Johan Rohdin, Ioannis Gkinis, Lukáš Burget, Jan "Honza'' Černocký
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present the system description of the joint efforts of Brno University of Technology (BUT) and Omilia -- Conversational Intelligence for the ASVSpoof2019 Spoofing and Countermeasures Challenge. The primary submission for Physical access (PA) is a fusion of two VGG networks, trained on single and two-channels features. For Logical access (LA), our primary system is a fusion of VGG and the recently introduced SincNet architecture. The results on PA show that the proposed networks yield very competitive performance in all conditions and achieved 86\:\% relative improvement compared to the official baseline. On the other hand, the results on LA showed that although the proposed architecture and training strategy performs very well on certain spoofing attacks, it fails to generalize to certain attacks that are unseen during training.



### Understanding Deep Learning Techniques for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.06119v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1907.06119v1)
- **Published**: 2019-07-13 19:23:42+00:00
- **Updated**: 2019-07-13 19:23:42+00:00
- **Authors**: Swarnendu Ghosh, Nibaran Das, Ishita Das, Ujjwal Maulik
- **Comment**: None
- **Journal**: None
- **Summary**: The machine learning community has been overwhelmed by a plethora of deep learning based approaches. Many challenging computer vision tasks such as detection, localization, recognition and segmentation of objects in unconstrained environment are being efficiently addressed by various types of deep neural networks like convolutional neural networks, recurrent networks, adversarial networks, autoencoders and so on. While there have been plenty of analytical studies regarding the object detection or recognition domain, many new deep learning techniques have surfaced with respect to image segmentation techniques. This paper approaches these various deep learning techniques of image segmentation from an analytical perspective. The main goal of this work is to provide an intuitive understanding of the major techniques that has made significant contribution to the image segmentation domain. Starting from some of the traditional image segmentation approaches, the paper progresses describing the effect deep learning had on the image segmentation domain. Thereafter, most of the major segmentation algorithms have been logically categorized with paragraphs dedicated to their unique contribution. With an ample amount of intuitive explanations, the reader is expected to have an improved ability to visualize the internal dynamics of these processes.



### FMRI data augmentation via synthesis
- **Arxiv ID**: http://arxiv.org/abs/1907.06134v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.06134v1)
- **Published**: 2019-07-13 21:30:41+00:00
- **Updated**: 2019-07-13 21:30:41+00:00
- **Authors**: Peiye Zhuang, Alexander G. Schwing, Sanmi Koyejo
- **Comment**: None
- **Journal**: None
- **Summary**: We present an empirical evaluation of fMRI data augmentation via synthesis. For synthesis we use generative mod-els trained on real neuroimaging data to produce novel task-dependent functional brain images. Analyzed generative mod-els include classic approaches such as the Gaussian mixture model (GMM), and modern implicit generative models such as the generative adversarial network (GAN) and the variational auto-encoder (VAE). In particular, the proposed GAN and VAE models utilize 3-dimensional convolutions, which enables modeling of high-dimensional brain image tensors with structured spatial correlations. The synthesized datasets are then used to augment classifiers designed to predict cognitive and behavioural outcomes. Our results suggest that the proposed models are able to generate high-quality synthetic brain images which are diverse and task-dependent. Perhaps most importantly, the performance improvements of data aug-mentation via synthesis are shown to be complementary to the choice of the predictive model. Thus, our results suggest that data augmentation via synthesis is a promising approach to address the limited availability of fMRI data, and to improve the quality of predictive fMRI models.



### Neural Embedding for Physical Manipulations
- **Arxiv ID**: http://arxiv.org/abs/1907.06143v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.06143v1)
- **Published**: 2019-07-13 22:57:23+00:00
- **Updated**: 2019-07-13 22:57:23+00:00
- **Authors**: Lingzhi Zhang, Andong Cao, Rui Li, Jianbo Shi
- **Comment**: None
- **Journal**: None
- **Summary**: In common real-world robotic operations, action and state spaces can be vast and sometimes unknown, and observations are often relatively sparse. How do we learn the full topology of action and state spaces when given only few and sparse observations? Inspired by the properties of grid cells in mammalian brains, we build a generative model that enforces a normalized pairwise distance constraint between the latent space and output space to achieve data-efficient discovery of output spaces. This method achieves substantially better results than prior generative models, such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs). Prior models have the common issue of mode collapse and thus fail to explore the full topology of output space. We demonstrate the effectiveness of our model on various datasets both qualitatively and quantitatively.



### ThirdEye: Triplet Based Iris Recognition without Normalization
- **Arxiv ID**: http://arxiv.org/abs/1907.06147v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06147v1)
- **Published**: 2019-07-13 23:27:24+00:00
- **Updated**: 2019-07-13 23:27:24+00:00
- **Authors**: Sohaib Ahmad, Benjamin Fuller
- **Comment**: None
- **Journal**: None
- **Summary**: Most iris recognition pipelines involve three stages: segmenting into iris/non-iris pixels, normalization the iris region to a fixed area, and extracting relevant features for comparison. Given recent advances in deep learning it is prudent to ask which stages are required for accurate iris recognition. Lojez et al. (IWBF 2019) recently concluded that the segmentation stage is still crucial for good accuracy.We ask if normalization is beneficial? Towards answering this question, we develop a new iris recognition system called ThirdEye based on triplet convolutional neural networks (Schroff et al., ICCV 2015). ThirdEye directly uses segmented images without normalization. We observe equal error rates of 1.32%, 9.20%, and 0.59% on the ND-0405, UbirisV2, and IITD datasets respectively. For IITD, the most constrained dataset, this improves on the best prior work. However, for ND-0405 and UbirisV2,our equal error rate is slightly worse than prior systems. Our concluding hypothesis is that normalization is more important for less constrained environments.



