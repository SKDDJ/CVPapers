# Arxiv Papers in cs.CV on 2019-07-31
### EMPNet: Neural Localisation and Mapping Using Embedded Memory Points
- **Arxiv ID**: http://arxiv.org/abs/1907.13268v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.13268v2)
- **Published**: 2019-07-31 01:04:13+00:00
- **Updated**: 2019-08-02 02:41:01+00:00
- **Authors**: Gil Avraham, Yan Zuo, Thanuja Dharmasiri, Tom Drummond
- **Comment**: Accepted at ICCV 2019
- **Journal**: None
- **Summary**: Continuously estimating an agent's state space and a representation of its surroundings has proven vital towards full autonomy. A shared common ground among systems which successfully achieve this feat is the integration of previously encountered observations into the current state being estimated. This necessitates the use of a memory module for incorporating previously visited states whilst simultaneously offering an internal representation of the observed environment. In this work we develop a memory module which contains rigidly aligned point-embeddings that represent a coherent scene structure acquired from an RGB-D sequence of observations. The point-embeddings are extracted using modern convolutional neural network architectures, and alignment is performed by computing a dense correspondence matrix between a new observation and the current embeddings residing in the memory module. The whole framework is end-to-end trainable, resulting in a recurrent joint optimisation of the point-embeddings contained in the memory. This process amplifies the shared information across states, providing increased robustness and accuracy. We show significant improvement of our method across a set of experiments performed on the synthetic VIZDoom environment and a real world Active Vision Dataset.



### I-Keyboard: Fully Imaginary Keyboard on Touch Devices Empowered by Deep Neural Decoder
- **Arxiv ID**: http://arxiv.org/abs/1907.13285v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.13285v1)
- **Published**: 2019-07-31 02:22:49+00:00
- **Updated**: 2019-07-31 02:22:49+00:00
- **Authors**: Ue-Hwan Kim, Sahng-Min Yoo, Jong-Hwan Kim
- **Comment**: Submitted to IEEE TRANSACTIONS ON CYBERNETICS
- **Journal**: None
- **Summary**: Text-entry aims to provide an effective and efficient pathway for humans to deliver their messages to computers. With the advent of mobile computing, the recent focus of text-entry research has moved from physical keyboards to soft keyboards. Current soft keyboards, however, increase the typo rate due to lack of tactile feedback and degrade the usability of mobile devices due to their large portion on screens. To tackle these limitations, we propose a fully imaginary keyboard (I-Keyboard) with a deep neural decoder (DND). The invisibility of I-Keyboard maximizes the usability of mobile devices and DND empowered by a deep neural architecture allows users to start typing from any position on the touch screens at any angle. To the best of our knowledge, the eyes-free ten-finger typing scenario of I-Keyboard which does not necessitate both a calibration step and a predefined region for typing is first explored in this work. For the purpose of training DND, we collected the largest user data in the process of developing I-Keyboard. We verified the performance of the proposed I-Keyboard and DND by conducting a series of comprehensive simulations and experiments under various conditions. I-Keyboard showed 18.95% and 4.06% increases in typing speed (45.57 WPM) and accuracy (95.84%), respectively over the baseline.



### Self-training with progressive augmentation for unsupervised cross-domain person re-identification
- **Arxiv ID**: http://arxiv.org/abs/1907.13315v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.13315v1)
- **Published**: 2019-07-31 05:51:38+00:00
- **Updated**: 2019-07-31 05:51:38+00:00
- **Authors**: Xinyu Zhang, Jiewei Cao, Chunhua Shen, Mingyu You
- **Comment**: Accepted to Proc. Int. Conf. Computer Vision, 2019. Code is available
  at: https://tinyurl.com/PASTReID
- **Journal**: None
- **Summary**: Person re-identification (Re-ID) has achieved great improvement with deep learning and a large amount of labelled training data. However, it remains a challenging task for adapting a model trained in a source domain of labelled data to a target domain of only unlabelled data available. In this work, we develop a self-training method with progressive augmentation framework (PAST) to promote the model performance progressively on the target dataset. Specially, our PAST framework consists of two stages, namely, conservative stage and promoting stage. The conservative stage captures the local structure of target-domain data points with triplet-based loss functions, leading to improved feature representations. The promoting stage continuously optimizes the network by appending a changeable classification layer to the last layer of the model, enabling the use of global information about the data distribution. Importantly, we propose a new self-training strategy that progressively augments the model capability by adopting conservative and promoting stages alternately. Furthermore, to improve the reliability of selected triplet samples, we introduce a ranking-based triplet loss in the conservative stage, which is a label-free objective function basing on the similarities between data pairs. Experiments demonstrate that the proposed method achieves state-of-the-art person Re-ID performance under the unsupervised cross-domain setting. Code is available at: https://tinyurl.com/PASTReID



### Overcoming Catastrophic Forgetting by Neuron-level Plasticity Control
- **Arxiv ID**: http://arxiv.org/abs/1907.13322v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.13322v1)
- **Published**: 2019-07-31 06:22:00+00:00
- **Updated**: 2019-07-31 06:22:00+00:00
- **Authors**: Inyoung Paik, Sangjun Oh, Tae-Yeong Kwak, Injung Kim
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: To address the issue of catastrophic forgetting in neural networks, we propose a novel, simple, and effective solution called neuron-level plasticity control (NPC). While learning a new task, the proposed method preserves the knowledge for the previous tasks by controlling the plasticity of the network at the neuron level. NPC estimates the importance value of each neuron and consolidates important \textit{neurons} by applying lower learning rates, rather than restricting individual connection weights to stay close to certain values. The experimental results on the incremental MNIST (iMNIST) and incremental CIFAR100 (iCIFAR100) datasets show that neuron-level consolidation is substantially more effective compared to the connection-level consolidation approaches.



### Capsule Networks Need an Improved Routing Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1907.13327v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.13327v1)
- **Published**: 2019-07-31 06:39:58+00:00
- **Updated**: 2019-07-31 06:39:58+00:00
- **Authors**: Inyoung Paik, Taeyeong Kwak, Injung Kim
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: In capsule networks, the routing algorithm connects capsules in consecutive layers, enabling the upper-level capsules to learn higher-level concepts by combining the concepts of the lower-level capsules. Capsule networks are known to have a few advantages over conventional neural networks, including robustness to 3D viewpoint changes and generalization capability. However, some studies have reported negative experimental results. Nevertheless, the reason for this phenomenon has not been analyzed yet. We empirically analyzed the effect of five different routing algorithms. The experimental results show that the routing algorithms do not behave as expected and often produce results that are worse than simple baseline algorithms that assign the connection strengths uniformly or randomly. We also show that, in most cases, the routing algorithms do not change the classification result but polarize the link strengths, and the polarization can be extreme when they continue to repeat without stopping. In order to realize the true potential of the capsule network, it is essential to develop an improved routing algorithm.



### Adversarial Test on Learnable Image Encryption
- **Arxiv ID**: http://arxiv.org/abs/1907.13342v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.13342v1)
- **Published**: 2019-07-31 07:24:57+00:00
- **Updated**: 2019-07-31 07:24:57+00:00
- **Authors**: MaungMaung AprilPyone, Warit Sirichotedumrong, Hitoshi Kiya
- **Comment**: To be appeared in 2019 IEEE 8th Global Conference on Consumer
  Electronics (GCCE 2019)
- **Journal**: None
- **Summary**: Data for deep learning should be protected for privacy preserving. Researchers have come up with the notion of learnable image encryption to satisfy the requirement. However, existing privacy preserving approaches have never considered the threat of adversarial attacks. In this paper, we ran an adversarial test on learnable image encryption in five different scenarios. The results show different behaviors of the network in the variable key scenarios and suggest learnable image encryption provides certain level of adversarial robustness.



### Online Multi-Object Tracking Framework with the GMPHD Filter and Occlusion Group Management
- **Arxiv ID**: http://arxiv.org/abs/1907.13347v1
- **DOI**: 10.1109/ACCESS.2019.2953276
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1907.13347v1)
- **Published**: 2019-07-31 07:36:09+00:00
- **Updated**: 2019-07-31 07:36:09+00:00
- **Authors**: Young-min Song, Kwangjin Yoon, Young-Chul Yoon, Kin-Choong Yow, Moongu Jeon
- **Comment**: This paper includes 15 pages and 9 figures, and has been prepared for
  a journal (not yet submitted anywhere)
- **Journal**: None
- **Summary**: In this paper, we propose an efficient online multi-object tracking framework based on the GMPHD filter and occlusion group management scheme where the GMPHD filter utilizes hierarchical data association to reduce the false negatives caused by miss detection. The hierarchical data association consists of two steps: detection-to-track and track-to-track associations, which can recover the lost tracks and their switched IDs. In addition, the proposed framework is equipped with an object grouping management scheme which handles occlusion problems with two main parts. The first part is "track merging" which can merge the false positive tracks caused by false positive detections from occlusions, where the false positive tracks are usually occluded with a measure. The measure is the occlusion ratio between visual objects, sum-of-intersection-over-area (SIOA) we defined instead of the IOU metric. The second part is "occlusion group energy minimization (OGEM)" which prevents the occluded true positive tracks from false "track merging". We define each group of the occluded objects as an energy function and find an optimal hypothesis which makes the energy minimal. We evaluate the proposed tracker in benchmark datasets such as MOT15 and MOT17 which are built for multi-person tracking. An ablation study in training dataset shows that not only "track merging" and "OGEM" complement each other but also the proposed tracking method has more robust performance and less sensitive to parameters than baseline methods. Also, SIOA works better than IOU for various sizes of false positives. Experimental results show that the proposed tracker efficiently handles occlusion situations and achieves competitive performance compared to the state-of-the-art methods. Especially, our method shows the best multi-object tracking accuracy among the online and real-time executable methods.



### Competing Ratio Loss for Discriminative Multi-class Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1907.13349v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.13349v2)
- **Published**: 2019-07-31 07:38:04+00:00
- **Updated**: 2019-08-01 05:42:45+00:00
- **Authors**: Ke Zhang, Xinsheng Wang, Yurong Guo, Zhenbing Zhao, Zhanyu Ma, Tony X. Han
- **Comment**: The method proposed in this paper has major technical shortcomings,
  so we want to withdraw
- **Journal**: None
- **Summary**: The development of deep convolutional neural network architecture is critical to the improvement of image classification task performance. A lot of studies of image classification based on deep convolutional neural network focus on the network structure to improve the image classification performance. Contrary to these studies, we focus on the loss function. Cross-entropy Loss (CEL) is widely used for training a multi-class classification deep convolutional neural network. While CEL has been successfully implemented in image classification tasks, it only focuses on the posterior probability of correct class when the labels of training images are one-hot. It cannot be discriminated against the classes not belong to correct class (wrong classes) directly. In order to solve the problem of CEL, we propose Competing Ratio Loss (CRL), which calculates the posterior probability ratio between the correct class and competing wrong classes to better discriminate the correct class from competing wrong classes, increasing the difference between the negative log likelihood of the correct class and the negative log likelihood of competing wrong classes, widening the difference between the probability of the correct class and the probabilities of wrong classes. To demonstrate the effectiveness of our loss function, we perform some sets of experiments on different types of image classification datasets, including CIFAR, SVHN, CUB200- 2011, Adience and ImageNet datasets. The experimental results show the effectiveness and robustness of our loss function on different deep convolutional neural network architectures and different image classification tasks, such as fine-grained image classification, hard face age estimation and large-scale image classification.



### Towards Digital Retina in Smart Cities: A Model Generation, Utilization and Communication Paradigm
- **Arxiv ID**: http://arxiv.org/abs/1907.13368v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.13368v1)
- **Published**: 2019-07-31 08:50:49+00:00
- **Updated**: 2019-07-31 08:50:49+00:00
- **Authors**: Yihang Lou, Ling-Yu Duan, Yong Luo, Ziqian Chen, Tongliang Liu, Shiqi Wang, Wen Gao
- **Comment**: 6 pages, 6 figures
- **Journal**: None
- **Summary**: The digital retina in smart cities is to select what the City Eye tells the City Brain, and convert the acquired visual data from front-end visual sensors to features in an intelligent sensing manner. By deploying deep learning and/or handcrafted models in front-end devices, the compact features can be extracted and subsequently delivered to back-end cloud for search and advanced analytics. In this context, we propose a model generation, utilization, and communication paradigm, aiming to address a set of unique challenges for better artificial intelligence services in smart cities. In particular, we present an integrated multiple deep learning models reuse and prediction strategy, which greatly increases the feasibility of the digital retina in processing and analyzing the large-scale visual data in smart cities. The promise of the proposed paradigm is demonstrated through a set of experiments.



### Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.13369v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.13369v2)
- **Published**: 2019-07-31 08:51:31+00:00
- **Updated**: 2019-08-02 11:13:13+00:00
- **Authors**: Wenhao Wu, Dongliang He, Xiao Tan, Shifeng Chen, Shilei Wen
- **Comment**: Accepted by ICCV 2019 (oral)
- **Journal**: None
- **Summary**: Video Recognition has drawn great research interest and great progress has been made. A suitable frame sampling strategy can improve the accuracy and efficiency of recognition. However, mainstream solutions generally adopt hand-crafted frame sampling strategies for recognition. It could degrade the performance, especially in untrimmed videos, due to the variation of frame-level saliency. To this end, we concentrate on improving untrimmed video classification via developing a learning-based frame sampling strategy. We intuitively formulate the frame sampling procedure as multiple parallel Markov decision processes, each of which aims at picking out a frame/clip by gradually adjusting an initial sampling. Then we propose to solve the problems with multi-agent reinforcement learning (MARL). Our MARL framework is composed of a novel RNN-based context-aware observation network which jointly models context information among nearby agents and historical states of a specific agent, a policy network which generates the probability distribution over a predefined action space at each step and a classification network for reward calculation as well as final recognition. Extensive experimental results show that our MARL-based scheme remarkably outperforms hand-crafted strategies with various 2D and 3D baseline methods. Our single RGB model achieves a comparable performance of ActivityNet v1.3 champion submission with multi-modal multi-model fusion and new state-of-the-art results on YouTube Birds and YouTube Cars.



### Incremental Learning Techniques for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.13372v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.13372v4)
- **Published**: 2019-07-31 09:00:15+00:00
- **Updated**: 2019-09-17 07:33:25+00:00
- **Authors**: Umberto Michieli, Pietro Zanuttigh
- **Comment**: 8 pages, 3 figures, 4 tables
- **Journal**: International Conference on Computer Vision (ICCV), Workshop on
  Transferring and Adapting Source Knowledge in Computer Vision (TASK-CV) 2019
- **Summary**: Deep learning architectures exhibit a critical drop of performance due to catastrophic forgetting when they are required to incrementally learn new tasks. Contemporary incremental learning frameworks focus on image classification and object detection while in this work we formally introduce the incremental learning problem for semantic segmentation in which a pixel-wise labeling is considered. To tackle this task we propose to distill the knowledge of the previous model to retain the information about previously learned classes, whilst updating the current model to learn the new ones. We propose various approaches working both on the output logits and on intermediate features. In opposition to some recent frameworks, we do not store any image from previously learned classes and only the last model is needed to preserve high accuracy on these classes. The experimental evaluation on the Pascal VOC2012 dataset shows the effectiveness of the proposed approaches.



### Learned Collaborative Stereo Refinement
- **Arxiv ID**: http://arxiv.org/abs/1907.13391v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.13391v1)
- **Published**: 2019-07-31 09:41:36+00:00
- **Updated**: 2019-07-31 09:41:36+00:00
- **Authors**: Patrick Knöbelreiter, Thomas Pock
- **Comment**: @German Conference on Pattern Recognition 2019
- **Journal**: None
- **Summary**: In this work, we propose a learning-based method to denoise and refine disparity maps of a given stereo method. The proposed variational network arises naturally from unrolling the iterates of a proximal gradient method applied to a variational energy defined in a joint disparity, color, and confidence image space. Our method allows to learn a robust collaborative regularizer leveraging the joint statistics of the color image, the confidence map and the disparity map. Due to the variational structure of our method, the individual steps can be easily visualized, thus enabling interpretability of the method. We can therefore provide interesting insights into how our method refines and denoises disparity maps. The efficiency of our method is demonstrated by the publicly available stereo benchmarks Middlebury 2014 and Kitti 2015.



### Cartoon Face Recognition: A Benchmark Dataset
- **Arxiv ID**: http://arxiv.org/abs/1907.13394v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.13394v3)
- **Published**: 2019-07-31 09:48:41+00:00
- **Updated**: 2020-06-27 03:43:33+00:00
- **Authors**: Yi Zheng, Yifan Zhao, Mengyuan Ren, He Yan, Xiangju Lu, Junhui Liu, Jia Li
- **Comment**: 9 papers, 6 figures
- **Journal**: None
- **Summary**: Recent years have witnessed increasing attention in cartoon media, powered by the strong demands of industrial applications. As the first step to understand this media, cartoon face recognition is a crucial but less-explored task with few datasets proposed. In this work, we first present a new challenging benchmark dataset, consisting of 389,678 images of 5,013 cartoon characters annotated with identity, bounding box, pose, and other auxiliary attributes. The dataset, named iCartoonFace, is currently the largest-scale, high-quality, richannotated, and spanning multiple occurrences in the field of image recognition, including near-duplications, occlusions, and appearance changes. In addition, we provide two types of annotations for cartoon media, i.e., face recognition, and face detection, with the help of a semi-automatic labeling algorithm. To further investigate this challenging dataset, we propose a multi-task domain adaptation approach that jointly utilizes the human and cartoon domain knowledge with three discriminative regularizations. We hence perform a benchmark analysis of the proposed dataset and verify the superiority of the proposed approach in the cartoon face recognition task. We believe this public availability will attract more research attention in broad practical application scenarios.



### Uncertainty Quantification in Deep Learning for Safer Neuroimage Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1907.13418v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.13418v1)
- **Published**: 2019-07-31 11:17:45+00:00
- **Updated**: 2019-07-31 11:17:45+00:00
- **Authors**: Ryutaro Tanno, Daniel Worrall, Enrico Kaden, Aurobrata Ghosh, Francesco Grussu, Alberto Bizzi, Stamatios N. Sotiropoulos, Antonio Criminisi, Daniel C. Alexander
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning (DL) has shown great potential in medical image enhancement problems, such as super-resolution or image synthesis. However, to date, little consideration has been given to uncertainty quantification over the output image. Here we introduce methods to characterise different components of uncertainty in such problems and demonstrate the ideas using diffusion MRI super-resolution. Specifically, we propose to account for $intrinsic$ uncertainty through a heteroscedastic noise model and for $parameter$ uncertainty through approximate Bayesian inference, and integrate the two to quantify $predictive$ uncertainty over the output image. Moreover, we introduce a method to propagate the predictive uncertainty on a multi-channelled image to derived scalar parameters, and separately quantify the effects of intrinsic and parameter uncertainty therein. The methods are evaluated for super-resolution of two different signal representations of diffusion MR images---DTIs and Mean Apparent Propagator MRI---and their derived quantities such as MD and FA, on multiple datasets of both healthy and pathological human brains. Results highlight three key benefits of uncertainty modelling for improving the safety of DL-based image enhancement systems. Firstly, incorporating uncertainty improves the predictive performance even when test data departs from training data. Secondly, the predictive uncertainty highly correlates with errors, and is therefore capable of detecting predictive "failures". Results demonstrate that such an uncertainty measure enables subject-specific and voxel-wise risk assessment of the output images. Thirdly, we show that the method for decomposing predictive uncertainty into its independent sources provides high-level "explanations" for the performance by quantifying how much uncertainty arises from the inherent difficulty of the task or the limited training examples.



### Expectation-Maximization Attention Networks for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.13426v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.13426v2)
- **Published**: 2019-07-31 11:35:54+00:00
- **Updated**: 2019-08-16 17:55:50+00:00
- **Authors**: Xia Li, Zhisheng Zhong, Jianlong Wu, Yibo Yang, Zhouchen Lin, Hong Liu
- **Comment**: In Proceedings of International Conference in Computer Vision (ICCV),
  2019. Oral
- **Journal**: None
- **Summary**: Self-attention mechanism has been widely used for various tasks. It is designed to compute the representation of each position by a weighted sum of the features at all positions. Thus, it can capture long-range relations for computer vision tasks. However, it is computationally consuming. Since the attention maps are computed w.r.t all other positions. In this paper, we formulate the attention mechanism into an expectation-maximization manner and iteratively estimate a much more compact set of bases upon which the attention maps are computed. By a weighted summation upon these bases, the resulting representation is low-rank and deprecates noisy information from the input. The proposed Expectation-Maximization Attention (EMA) module is robust to the variance of input and is also friendly in memory and computation. Moreover, we set up the bases maintenance and normalization methods to stabilize its training procedure. We conduct extensive experiments on popular semantic segmentation benchmarks including PASCAL VOC, PASCAL Context and COCO Stuff, on which we set new records.



### Rapid Light Field Depth Estimation with Semi-Global Matching
- **Arxiv ID**: http://arxiv.org/abs/1907.13449v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.13449v1)
- **Published**: 2019-07-31 12:27:29+00:00
- **Updated**: 2019-07-31 12:27:29+00:00
- **Authors**: Yuriy Anisimov, Oliver Wasenmüller, Didier Stricker
- **Comment**: IEEE 15th International Conference on Intelligent Computer
  Communication and Processing, Cluj-Napoca, September 5-7, 2019
- **Journal**: None
- **Summary**: Running time of the light field depth estimation algorithms is typically high. This assessment is based on the computational complexity of existing methods and the large amounts of data involved. The aim of our work is to develop a simple and fast algorithm for accurate depth computation. In this context, we propose an approach, which involves Semi-Global Matching for the processing of light field images. It forms on comparison of pixels' correspondences with different metrics in the substantially bounded light field space. We show that our method is suitable for the fast production of a proper result in a variety of light field configurations



### Use What You Have: Video Retrieval Using Representations From Collaborative Experts
- **Arxiv ID**: http://arxiv.org/abs/1907.13487v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.13487v2)
- **Published**: 2019-07-31 13:19:37+00:00
- **Updated**: 2020-02-14 04:32:35+00:00
- **Authors**: Yang Liu, Samuel Albanie, Arsha Nagrani, Andrew Zisserman
- **Comment**: This update contains a correction to previously reported results
- **Journal**: None
- **Summary**: The rapid growth of video on the internet has made searching for video content using natural language queries a significant challenge. Human-generated queries for video datasets `in the wild' vary a lot in terms of degree of specificity, with some queries describing specific details such as the names of famous identities, content from speech, or text available on the screen. Our goal is to condense the multi-modal, extremely high dimensional information from videos into a single, compact video representation for the task of video retrieval using free-form text queries, where the degree of specificity is open-ended.   For this we exploit existing knowledge in the form of pre-trained semantic embeddings which include 'general' features such as motion, appearance, and scene features from visual content. We also explore the use of more 'specific' cues from ASR and OCR which are intermittently available for videos and find that these signals remain challenging to use effectively for retrieval. We propose a collaborative experts model to aggregate information from these different pre-trained experts and assess our approach empirically on five retrieval benchmarks: MSR-VTT, LSMDC, MSVD, DiDeMo, and ActivityNet. Code and data can be found at www.robots.ox.ac.uk/~vgg/research/collaborative-experts/. This paper contains a correction to results reported in the previous version.



### On the difficulty of learning and predicting the long-term dynamics of bouncing objects
- **Arxiv ID**: http://arxiv.org/abs/1907.13494v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.13494v1)
- **Published**: 2019-07-31 13:29:34+00:00
- **Updated**: 2019-07-31 13:29:34+00:00
- **Authors**: Alberto Cenzato, Alberto Testolin, Marco Zorzi
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to accurately predict the surrounding environment is a foundational principle of intelligence in biological and artificial agents. In recent years, a variety of approaches have been proposed for learning to predict the physical dynamics of objects interacting in a visual scene. Here we conduct a systematic empirical evaluation of several state-of-the-art unsupervised deep learning models that are considered capable of learning the spatio-temporal structure of a popular dataset composed by synthetic videos of bouncing objects. We show that most of the models indeed obtain high accuracy on the standard benchmark of predicting the next frame of a sequence, and one of them even achieves state-of-the-art performance. However, all models fall short when probed with the more challenging task of generating multiple successive frames. Our results show that the ability to perform short-term predictions does not imply that the model has captured the underlying structure and dynamics of the visual environment, thereby calling for a careful rethinking of the metrics commonly adopted for evaluating temporal models. We also investigate whether the learning outcome could be affected by the use of curriculum-based teaching.



### Probabilistic Motion Modeling from Medical Image Sequences: Application to Cardiac Cine-MRI
- **Arxiv ID**: http://arxiv.org/abs/1907.13524v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.13524v2)
- **Published**: 2019-07-31 14:25:47+00:00
- **Updated**: 2019-09-23 12:57:40+00:00
- **Authors**: Julian Krebs, Tommaso Mansi, Nicholas Ayache, Hervé Delingette
- **Comment**: Probabilistic Motion Model, Motion Tracking, Temporal
  Super-Resolution, Diffeomorphic Registration, Temporal Variational
  Autoencoder (Final version)
- **Journal**: None
- **Summary**: We propose to learn a probabilistic motion model from a sequence of images. Besides spatio-temporal registration, our method offers to predict motion from a limited number of frames, useful for temporal super-resolution. The model is based on a probabilistic latent space and a novel temporal dropout training scheme. This enables simulation and interpolation of realistic motion patterns given only one or any subset of frames of a sequence. The encoded motion also allows to be transported from one subject to another without the need of inter-subject registration. An unsupervised generative deformation model is applied within a temporal convolutional network which leads to a diffeomorphic motion model, encoded as a low-dimensional motion matrix. Applied to cardiac cine-MRI sequences, we show improved registration accuracy and spatio-temporally smoother deformations compared to three state-of-the-art registration algorithms. Besides, we demonstrate the model's applicability to motion transport by simulating a pathology in a healthy case. Furthermore, we show an improved motion reconstruction from incomplete sequences compared to linear and cubic interpolation.



### What's in the box? Explaining the black-box model through an evaluation of its interpretable features
- **Arxiv ID**: http://arxiv.org/abs/1908.04348v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, 68T30, 68U10, I.2
- **Links**: [PDF](http://arxiv.org/pdf/1908.04348v1)
- **Published**: 2019-07-31 15:05:06+00:00
- **Updated**: 2019-07-31 15:05:06+00:00
- **Authors**: Francesco Ventura, Tania Cerquitelli
- **Comment**: 5 pages, 5 images
- **Journal**: None
- **Summary**: Algorithms are powerful and necessary tools behind a large part of the information we use every day. However, they may introduce new sources of bias, discrimination and other unfair practices that affect people who are unaware of it. Greater algorithm transparency is indispensable to provide more credible and reliable services. Moreover, requiring developers to design transparent algorithm-driven applications allows them to keep the model accessible and human understandable, increasing the trust of end users. In this paper we present EBAnO, a new engine able to produce prediction-local explanations for a black-box model exploiting interpretable feature perturbations. EBAnO exploits the hypercolumns representation together with the cluster analysis to identify a set of interpretable features of images. Furthermore two indices have been proposed to measure the influence of input features on the final prediction made by a CNN model. EBAnO has been preliminarily tested on a set of heterogeneous images. The results highlight the effectiveness of EBAnO in explaining the CNN classification through the evaluation of interpretable features influence.



### Deep Sensor Fusion for Real-Time Odometry Estimation
- **Arxiv ID**: http://arxiv.org/abs/1908.00524v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.00524v1)
- **Published**: 2019-07-31 15:29:15+00:00
- **Updated**: 2019-07-31 15:29:15+00:00
- **Authors**: Michelle Valente, Cyril Joly, Arnaud de La Fortelle
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1902.08536
- **Journal**: None
- **Summary**: Cameras and 2D laser scanners, in combination, are able to provide low-cost, light-weight and accurate solutions, which make their fusion well-suited for many robot navigation tasks. However, correct data fusion depends on precise calibration of the rigid body transform between the sensors. In this paper we present the first framework that makes use of Convolutional Neural Networks (CNNs) for odometry estimation fusing 2D laser scanners and mono-cameras. The use of CNNs provides the tools to not only extract the features from the two sensors, but also to fuse and match them without needing a calibration between the sensors. We transform the odometry estimation into an ordinal classification problem in order to find accurate rotation and translation values between consecutive frames. Results on a real road dataset show that the fusion network runs in real-time and is able to improve the odometry estimation of a single sensor alone by learning how to fuse two different types of data information.



### An Elastic Energy Minimization Framework for Mean Surface Calculation
- **Arxiv ID**: http://arxiv.org/abs/1907.13557v1
- **DOI**: None
- **Categories**: **math.MG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.13557v1)
- **Published**: 2019-07-31 15:33:19+00:00
- **Updated**: 2019-07-31 15:33:19+00:00
- **Authors**: Jozsef Molnar, Peter Horvath
- **Comment**: None
- **Journal**: None
- **Summary**: As the continuation of the contour mean calculation - designed for averaging the manual delineations of 3D layer stack images - in this paper, the most important equations: a) the reparameterization equations to determine the minimizing diffeomorphism and b) the proper centroid calculation for the surface mean calculation are presented. The chosen representation space: escaled Position by Square root Normal (RPSN) is a real valued vector space, invariant under the action of the reparameterization group and the imposed L2 metric (used to define the distance function) has well defined meaning: the sum of the central second moments of the coordinate functions. For comparision purpose, the reparameterization equations for elastic surface matching, using the Square Root Normal Function (SRNF) are also provided. The reparameterization equations for these cases have formal similarity, albeit the targeted applications differ: SRNF representation suitable for shape analysis purpose whereas RPSN is more fit for the cases where all contextual information - including the relative translation between the constituent surfaces - are to be retained (but the sake of theoretical completeness, the possibility of the consistent relative displacement removal in the RPSN case is also addressed).



### Synthetic Image Augmentation for Improved Classification using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.13576v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.13576v1)
- **Published**: 2019-07-31 16:05:52+00:00
- **Updated**: 2019-07-31 16:05:52+00:00
- **Authors**: Keval Doshi
- **Comment**: State Recognition Symposium
- **Journal**: None
- **Summary**: Object detection and recognition has been an ongoing research topic for a long time in the field of computer vision. Even in robotics, detecting the state of an object by a robot still remains a challenging task. Also, collecting data for each possible state is also not feasible. In this literature, we use a deep convolutional neural network with SVM as a classifier to help with recognizing the state of a cooking object. We also study how a generative adversarial network can be used for synthetic data augmentation and improving the classification accuracy. The main motivation behind this work is to estimate how well a robot could recognize the current state of an object



### Auto-labelling of Markers in Optical Motion Capture by Permutation Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.13580v1
- **DOI**: 10.1007/978-3-030-22514-8_14
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.13580v1)
- **Published**: 2019-07-31 16:25:34+00:00
- **Updated**: 2019-07-31 16:25:34+00:00
- **Authors**: Saeed Ghorbani, Ali Etemad, Nikolaus F. Troje
- **Comment**: None
- **Journal**: Computer Graphics International Conference, pp. 167-178. Springer,
  Cham, 2019
- **Summary**: Optical marker-based motion capture is a vital tool in applications such as motion and behavioural analysis, animation, and biomechanics. Labelling, that is, assigning optical markers to the pre-defined positions on the body is a time consuming and labour intensive postprocessing part of current motion capture pipelines. The problem can be considered as a ranking process in which markers shuffled by an unknown permutation matrix are sorted to recover the correct order. In this paper, we present a framework for automatic marker labelling which first estimates a permutation matrix for each individual frame using a differentiable permutation learning model and then utilizes temporal consistency to identify and correct remaining labelling errors. Experiments conducted on the test data show the effectiveness of our framework.



### Unsupervised Domain Adaptation via Disentangled Representations: Application to Cross-Modality Liver Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.13590v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.13590v2)
- **Published**: 2019-07-31 16:45:19+00:00
- **Updated**: 2019-08-29 02:47:32+00:00
- **Authors**: Junlin Yang, Nicha C. Dvornek, Fan Zhang, Julius Chapiro, MingDe Lin, James S. Duncan
- **Comment**: None
- **Journal**: None
- **Summary**: A deep learning model trained on some labeled data from a certain source domain generally performs poorly on data from different target domains due to domain shifts. Unsupervised domain adaptation methods address this problem by alleviating the domain shift between the labeled source data and the unlabeled target data. In this work, we achieve cross-modality domain adaptation, i.e. between CT and MRI images, via disentangled representations. Compared to learning a one-to-one mapping as the state-of-art CycleGAN, our model recovers a many-to-many mapping between domains to capture the complex cross-domain relations. It preserves semantic feature-level information by finding a shared content space instead of a direct pixelwise style transfer. Domain adaptation is achieved in two steps. First, images from each domain are embedded into two spaces, a shared domain-invariant content space and a domain-specific style space. Next, the representation in the content space is extracted to perform a task. We validated our method on a cross-modality liver segmentation task, to train a liver segmentation model on CT images that also performs well on MRI. Our method achieved Dice Similarity Coefficient (DSC) of 0.81, outperforming a CycleGAN-based method of 0.72. Moreover, our model achieved good generalization to joint-domain learning, in which unpaired data from different modalities are jointly learned to improve the segmentation performance on each individual modality. Lastly, under a multi-modal target domain with significant diversity, our approach exhibited the potential for diverse image generation and remained effective with DSC of 0.74 on multi-phasic MRI while the CycleGAN-based method performed poorly with a DSC of only 0.52.



### Learning to Dress 3D People in Generative Clothing
- **Arxiv ID**: http://arxiv.org/abs/1907.13615v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1907.13615v3)
- **Published**: 2019-07-31 17:30:54+00:00
- **Updated**: 2020-05-22 17:21:49+00:00
- **Authors**: Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Gerard Pons-Moll, Siyu Tang, Michael J. Black
- **Comment**: CVPR-2020 camera ready. Code and data are available at
  https://cape.is.tue.mpg.de
- **Journal**: None
- **Summary**: Three-dimensional human body models are widely used in the analysis of human pose and motion. Existing models, however, are learned from minimally-clothed 3D scans and thus do not generalize to the complexity of dressed people in common images and videos. Additionally, current models lack the expressive power needed to represent the complex non-linear geometry of pose-dependent clothing shapes. To address this, we learn a generative 3D mesh model of clothed people from 3D scans with varying pose and clothing. Specifically, we train a conditional Mesh-VAE-GAN to learn the clothing deformation from the SMPL body model, making clothing an additional term in SMPL. Our model is conditioned on both pose and clothing type, giving the ability to draw samples of clothing to dress different body shapes in a variety of styles and poses. To preserve wrinkle detail, our Mesh-VAE-GAN extends patchwise discriminators to 3D meshes. Our model, named CAPE, represents global shape and fine local structure, effectively extending the SMPL body model to clothing. To our knowledge, this is the first generative model that directly dresses 3D human body meshes and generalizes to different poses. The model, code and data are available for research purposes at https://cape.is.tue.mpg.de.



### Video Stitching for Linear Camera Arrays
- **Arxiv ID**: http://arxiv.org/abs/1907.13622v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.13622v1)
- **Published**: 2019-07-31 17:42:38+00:00
- **Updated**: 2019-07-31 17:42:38+00:00
- **Authors**: Wei-Sheng Lai, Orazio Gallo, Jinwei Gu, Deqing Sun, Ming-Hsuan Yang, Jan Kautz
- **Comment**: This work is accepted in BMVC 2019. Project website:
  http://vllab.ucmerced.edu/wlai24/video_stitching/
- **Journal**: None
- **Summary**: Despite the long history of image and video stitching research, existing academic and commercial solutions still produce strong artifacts. In this work, we propose a wide-baseline video stitching algorithm for linear camera arrays that is temporally stable and tolerant to strong parallax. Our key insight is that stitching can be cast as a problem of learning a smooth spatial interpolation between the input videos. To solve this problem, inspired by pushbroom cameras, we introduce a fast pushbroom interpolation layer and propose a novel pushbroom stitching network, which learns a dense flow field to smoothly align the multiple input videos for spatial interpolation. Our approach outperforms the state-of-the-art by a significant margin, as we show with a user study, and has immediate applications in many areas such as virtual reality, immersive telepresence, autonomous driving, and video surveillance.



### DROGON: A Trajectory Prediction Model based on Intention-Conditioned Behavior Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1908.00024v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1908.00024v3)
- **Published**: 2019-07-31 18:04:28+00:00
- **Updated**: 2020-11-06 09:59:58+00:00
- **Authors**: Chiho Choi, Srikanth Malla, Abhishek Patil, Joon Hee Choi
- **Comment**: Conference on Robot Learning (CoRL) 2020
- **Journal**: None
- **Summary**: We propose a Deep RObust Goal-Oriented trajectory prediction Network (DROGON) for accurate vehicle trajectory prediction by considering behavioral intentions of vehicles in traffic scenes. Our main insight is that the behavior (i.e., motion) of drivers can be reasoned from their high level possible goals (i.e., intention) on the road. To succeed in such behavior reasoning, we build a conditional prediction model to forecast goal-oriented trajectories with the following stages: (i) relational inference where we encode relational interactions of vehicles using the perceptual context; (ii) intention estimation to compute the probability distributions of intentional goals based on the inferred relations; and (iii) behavior reasoning where we reason about the behaviors of vehicles as trajectories conditioned on the intentions. To this end, we extend the proposed framework to the pedestrian trajectory prediction task, showing the potential applicability toward general trajectory prediction.



### Image Captioning with Unseen Objects
- **Arxiv ID**: http://arxiv.org/abs/1908.00047v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.00047v1)
- **Published**: 2019-07-31 18:48:52+00:00
- **Updated**: 2019-07-31 18:48:52+00:00
- **Authors**: Berkan Demirel, Ramazan Gokberk Cinbis, Nazli Ikizler-Cinbis
- **Comment**: To appear in British Machine Vision Conference (BMVC) 2019
- **Journal**: None
- **Summary**: Image caption generation is a long standing and challenging problem at the intersection of computer vision and natural language processing. A number of recently proposed approaches utilize a fully supervised object recognition model within the captioning approach. Such models, however, tend to generate sentences which only consist of objects predicted by the recognition models, excluding instances of the classes without labelled training examples. In this paper, we propose a new challenging scenario that targets the image captioning problem in a fully zero-shot learning setting, where the goal is to be able to generate captions of test images containing objects that are not seen during training. The proposed approach jointly uses a novel zero-shot object detection model and a template-based sentence generator. Our experiments show promising results on the COCO dataset.



### An Empirical Study of Batch Normalization and Group Normalization in Conditional Computation
- **Arxiv ID**: http://arxiv.org/abs/1908.00061v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.00061v1)
- **Published**: 2019-07-31 19:37:16+00:00
- **Updated**: 2019-07-31 19:37:16+00:00
- **Authors**: Vincent Michalski, Vikram Voleti, Samira Ebrahimi Kahou, Anthony Ortiz, Pascal Vincent, Chris Pal, Doina Precup
- **Comment**: None
- **Journal**: None
- **Summary**: Batch normalization has been widely used to improve optimization in deep neural networks. While the uncertainty in batch statistics can act as a regularizer, using these dataset statistics specific to the training set impairs generalization in certain tasks. Recently, alternative methods for normalizing feature activations in neural networks have been proposed. Among them, group normalization has been shown to yield similar, in some domains even superior performance to batch normalization. All these methods utilize a learned affine transformation after the normalization operation to increase representational power. Methods used in conditional computation define the parameters of these transformations as learnable functions of conditioning information. In this work, we study whether and where the conditional formulation of group normalization can improve generalization compared to conditional batch normalization. We evaluate performances on the tasks of visual question answering, few-shot learning, and conditional image generation.



### Simultaneous Iris and Periocular Region Detection Using Coarse Annotations
- **Arxiv ID**: http://arxiv.org/abs/1908.00069v1
- **DOI**: 10.1109/SIBGRAPI.2019.00032
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.00069v1)
- **Published**: 2019-07-31 20:09:48+00:00
- **Updated**: 2019-07-31 20:09:48+00:00
- **Authors**: Diego R. Lucio, Rayson Laroca, Luiz A. Zanlorensi, Gladston Moreira, David Menotti
- **Comment**: Accepted for presentation at the Conference on Graphics, Patterns and
  Images (SIBGRAPI) 2019
- **Journal**: None
- **Summary**: In this work, we propose to detect the iris and periocular regions simultaneously using coarse annotations and two well-known object detectors: YOLOv2 and Faster R-CNN. We believe coarse annotations can be used in recognition systems based on the iris and periocular regions, given the much smaller engineering effort required to manually annotate the training images. We manually made coarse annotations of the iris and periocular regions (122K images from the visible (VIS) spectrum and 38K images from the near-infrared (NIR) spectrum). The iris annotations in the NIR databases were generated semi-automatically by first applying an iris segmentation CNN and then performing a manual inspection. These annotations were made for 11 well-known public databases (3 NIR and 8 VIS) designed for the iris-based recognition problem and are publicly available to the research community. Experimenting our proposal on these databases, we highlight two results. First, the Faster R-CNN + Feature Pyramid Network (FPN) model reported an Intersection over Union (IoU) higher than YOLOv2 (91.86% vs 85.30%). Second, the detection of the iris and periocular regions being performed simultaneously is as accurate as performed separately, but with a lower computational cost, i.e., two tasks were carried out at the cost of one.



### Machine Learning at the Network Edge: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1908.00080v4
- **DOI**: 10.1145/3469029
- **Categories**: **cs.LG**, cs.CV, cs.NI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.00080v4)
- **Published**: 2019-07-31 20:23:00+00:00
- **Updated**: 2021-05-23 19:52:16+00:00
- **Authors**: M. G. Sarwar Murshed, Christopher Murphy, Daqing Hou, Nazar Khan, Ganesh Ananthanarayanan, Faraz Hussain
- **Comment**: 35 pages, 4 figures; restructured text to combine ML/DL into a single
  section; updated tables/figures; added a new table summarizing major ML edge
  applications, fixed typos
- **Journal**: ACM Comput. Surv. 54, 8, Article 170 (November 2022)
- **Summary**: Resource-constrained IoT devices, such as sensors and actuators, have become ubiquitous in recent years. This has led to the generation of large quantities of data in real-time, which is an appealing target for AI systems. However, deploying machine learning models on such end-devices is nearly impossible. A typical solution involves offloading data to external computing systems (such as cloud servers) for further processing but this worsens latency, leads to increased communication costs, and adds to privacy concerns. To address this issue, efforts have been made to place additional computing devices at the edge of the network, i.e close to the IoT devices where the data is generated. Deploying machine learning systems on such edge computing devices alleviates the above issues by allowing computations to be performed close to the data sources. This survey describes major research efforts where machine learning systems have been deployed at the edge of computer networks, focusing on the operational aspects including compression techniques, tools, frameworks, and hardware used in successful applications of intelligent edge systems.



### Interior Object Detection and Color Harmonization
- **Arxiv ID**: http://arxiv.org/abs/1908.04344v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1908.04344v2)
- **Published**: 2019-07-31 21:02:12+00:00
- **Updated**: 2021-03-18 13:45:07+00:00
- **Authors**: Sharmin Pathan
- **Comment**: None
- **Journal**: Frontiers in Artificial Intelligence and Machine Learning 2019
- **Summary**: Confused about renovating your space? Choosing the perfect color for your walls is always a challenging task. One does rounds of color consultation and several patch tests. This paper proposes an AI tool to pitch paint based on attributes of your room and other furniture, and visualize it on your walls. It makes the color selection process easy. It takes in images of a room, detects furniture objects using YOLO object detection. Once these objects have been detected, the tool picks out color of the object. Later this object specific information gets appended to the room attributes (room_type, room_size, preferred_tone, etc) and a deep neural net is trained to make predictions for color/texture/wallpaper for the walls. Finally, these predictions are visualized on the walls from the images provided. The idea is to take the knowledge of a color consultant and pitch colors that suit the walls and provide a good contrast with the furniture and harmonize with different colors in the room. Transfer learning for YOLO object detection from the COCO dataset was used as a starting point and the weights were later fine-tuned by training on additional images. The model was trained on 1000 records listing the room and furniture attributes, to predict colors. Given the room image, this method finds the best color scheme for the walls. These predictions are then visualized on the walls in the image using image segmentation. The results are visually appealing and automatically enhance the color look-and-feel.



### OCT Fingerprints: Resilience to Presentation Attacks
- **Arxiv ID**: http://arxiv.org/abs/1908.00102v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.00102v1)
- **Published**: 2019-07-31 21:18:34+00:00
- **Updated**: 2019-07-31 21:18:34+00:00
- **Authors**: Tarang Chugh, Anil K. Jain
- **Comment**: Fingerprint presentation attack detection, OCT scanner; 9 pages, 8
  figures
- **Journal**: None
- **Summary**: Optical coherent tomography (OCT) fingerprint technology provides rich depth information, including internal fingerprint (papillary junction) and sweat (eccrine) glands, in addition to imaging any fake layers (presentation attacks) placed over finger skin. Unlike 2D surface fingerprint scans, additional depth information provided by the cross-sectional OCT depth profile scans are purported to thwart fingerprint presentation attacks. We develop and evaluate a presentation attack detector (PAD) based on deep convolutional neural network (CNN). Input data to CNN are local patches extracted from the cross-sectional OCT depth profile scans captured using THORLabs Telesto series spectral-domain fingerprint reader. The proposed approach achieves a TDR of 99.73% @ FDR of 0.2% on a database of 3,413 bonafide and 357 PA OCT scans, fabricated using 8 different PA materials. By employing a visualization technique, known as CNN-Fixations, we are able to identify the regions in the OCT scan patches that are crucial for fingerprint PAD detection.



### Few-Shot Meta-Denoising
- **Arxiv ID**: http://arxiv.org/abs/1908.00111v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.00111v2)
- **Published**: 2019-07-31 21:40:44+00:00
- **Updated**: 2019-11-25 21:19:46+00:00
- **Authors**: Leslie Casas, Attila Klimmek, Gustavo Carneiro, Nassir Navab, Vasileios Belagiannis
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of few-shot learning-based denoising where the training set contains just a handful of clean and noisy samples. A solution to mitigate the small training set issue is to pre-train a denoising model with small training sets containing pairs of clean and synthesized noisy signals, produced from empirical noise priors, and fine-tune on the available small training set. While such transfer learning seems effective, it may not generalize well because of the limited amount of training data. In this work, we propose a new meta-learning training approach for few-shot learning-based denoising problems. Our model is meta-trained using known synthetic noise models, and then fine-tuned with the small training set, with the real noise, as a few-shot learning task. Meta-learning from small training sets of synthetically generated data during meta-training enables us to not only generate an infinite number of training tasks, but also train a model to learn with small training sets -- both advantages have the potential to improve the generalisation of the denoising model. Our approach is empirically shown to produce more accurate denoising results than supervised learning and transfer learning in three denoising evaluations for images and 1-D signals. Interestingly, our study provides strong indications that meta-learning has the potential to become the main learning algorithm for denoising.



### 3D Virtual Garment Modeling from RGB Images
- **Arxiv ID**: http://arxiv.org/abs/1908.00114v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.00114v1)
- **Published**: 2019-07-31 21:47:52+00:00
- **Updated**: 2019-07-31 21:47:52+00:00
- **Authors**: Yi Xu, Shanglin Yang, Wei Sun, Li Tan, Kefeng Li, Hui Zhou
- **Comment**: 9 pages; 9 figures; accepted to IEEE International Symposium on Mixed
  and Augmented Reality 2019
- **Journal**: None
- **Summary**: We present a novel approach that constructs 3D virtual garment models from photos. Unlike previous methods that require photos of a garment on a human model or a mannequin, our approach can work with various states of the garment: on a model, on a mannequin, or on a flat surface. To construct a complete 3D virtual model, our approach only requires two images as input, one front view and one back view. We first apply a multi-task learning network called JFNet that jointly predicts fashion landmarks and parses a garment image into semantic parts. The predicted landmarks are used for estimating sizing information of the garment. Then, a template garment mesh is deformed based on the sizing information to generate the final 3D model. The semantic parts are utilized for extracting color textures from input images. The results of our approach can be used in various Virtual Reality and Mixed Reality applications.



### ShapeCaptioner: Generative Caption Network for 3D Shapes by Learning a Mapping from Parts Detected in Multiple Views to Sentences
- **Arxiv ID**: http://arxiv.org/abs/1908.00120v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.00120v1)
- **Published**: 2019-07-31 22:17:31+00:00
- **Updated**: 2019-07-31 22:17:31+00:00
- **Authors**: Zhizhong Han, Chao Chen, Yu-Shen Liu, Matthias Zwicker
- **Comment**: None
- **Journal**: None
- **Summary**: 3D shape captioning is a challenging application in 3D shape understanding. Captions from recent multi-view based methods reveal that they cannot capture part-level characteristics of 3D shapes. This leads to a lack of detailed part-level description in captions, which human tend to focus on. To resolve this issue, we propose ShapeCaptioner, a generative caption network, to perform 3D shape captioning from semantic parts detected in multiple views. Our novelty lies in learning the knowledge of part detection in multiple views from 3D shape segmentations and transferring this knowledge to facilitate learning the mapping from 3D shapes to sentences. Specifically, ShapeCaptioner aggregates the parts detected in multiple colored views using our novel part class specific aggregation to represent a 3D shape, and then, employs a sequence to sequence model to generate the caption. Our outperforming results show that ShapeCaptioner can learn 3D shape features with more detailed part characteristics to facilitate better 3D shape captioning than previous work.



### "Sliced" Subwindow Search: a Sublinear-complexity Solution to the Maximum Rectangle Problem
- **Arxiv ID**: http://arxiv.org/abs/1908.00140v2
- **DOI**: None
- **Categories**: **cs.DS**, cs.CC, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.00140v2)
- **Published**: 2019-07-31 23:21:52+00:00
- **Updated**: 2023-04-09 21:48:47+00:00
- **Authors**: Max Reuter, Gheorghe-Teodor Bercea, Liana Fong
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: Considering a 2D matrix of positive and negative numbers, how might one draw a rectangle within it whose contents sum higher than all other rectangles'? This fundamental problem, commonly known the maximum rectangle problem or subwindow search, spans many computational domains. Yet, the problem has not been solved without demanding computational resources at least linearly proportional to the size of the matrix. In this work, we present a new approach to the problem which achieves sublinear time and memory complexities by interpolating between a small amount of equidistant sections of the matrix. Applied to natural images, our solution outperforms the state-of-the-art by achieving an 11x increase in speed and memory efficiency at 99% comparative accuracy. In general, our solution outperforms existing solutions when matrices are sufficiently large and a marginal decrease in accuracy is acceptable, such as in many problems involving natural images. As such, it is well-suited for real-time application and in a variety of computationally hard instances of the maximum rectangle problem.



