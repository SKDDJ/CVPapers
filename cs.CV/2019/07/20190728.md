# Arxiv Papers in cs.CV on 2019-07-28
### Learning Wear Patterns on Footwear Outsoles Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.12005v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12005v1)
- **Published**: 2019-07-28 03:42:23+00:00
- **Updated**: 2019-07-28 03:42:23+00:00
- **Authors**: Xavier Francis, Hamid Sharifzadeh, Angus Newton, Nilufar Baghaei, Soheil Varastehpour
- **Comment**: None
- **Journal**: None
- **Summary**: Footwear outsoles acquire characteristics unique to the individual wearing them over time. Forensic scientists largely rely on their skills and knowledge, gained through years of experience, to analyse such characteristics on a shoeprint. In this work, we present a convolutional neural network model that can predict the wear pattern on a unique dataset of shoeprints that captures the life and wear of a pair of shoes. We present an additional architecture able to reconstruct the outsole back to its original state on a given week, and provide empirical evaluations of the performance of both models.



### ROAM: Recurrently Optimizing Tracking Model
- **Arxiv ID**: http://arxiv.org/abs/1907.12006v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12006v3)
- **Published**: 2019-07-28 03:50:05+00:00
- **Updated**: 2020-03-24 06:28:26+00:00
- **Authors**: Tianyu Yang, Pengfei Xu, Runbo Hu, Hua Chai, Antoni B. Chan
- **Comment**: CVPR2020 camera ready
- **Journal**: None
- **Summary**: In this paper, we design a tracking model consisting of response generation and bounding box regression, where the first component produces a heat map to indicate the presence of the object at different positions and the second part regresses the relative bounding box shifts to anchors mounted on sliding-window locations. Thanks to the resizable convolutional filters used in both components to adapt to the shape changes of objects, our tracking model does not need to enumerate different sized anchors, thus saving model parameters. To effectively adapt the model to appearance variations, we propose to offline train a recurrent neural optimizer to update tracking model in a meta-learning setting, which can converge the model in a few gradient steps. This improves the convergence speed of updating the tracking model while achieving better performance. We extensively evaluate our trackers, ROAM and ROAM++, on the OTB, VOT, LaSOT, GOT-10K and TrackingNet benchmark and our methods perform favorably against state-of-the-art algorithms.



### Temporal Interpolation of Geostationary Satellite Imagery with Task Specific Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1907.12013v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12013v3)
- **Published**: 2019-07-28 04:44:50+00:00
- **Updated**: 2020-03-01 01:34:09+00:00
- **Authors**: Thomas Vandal, Ramakrishna Nemani
- **Comment**: None
- **Journal**: None
- **Summary**: Applications of satellite data in areas such as weather tracking and modeling, ecosystem monitoring, wildfire detection, and land-cover change are heavily dependent on the trade-offs to spatial, spectral and temporal resolutions of observations. In weather tracking, high-frequency temporal observations are critical and used to improve forecasts, study severe events, and extract atmospheric motion, among others. However, while the current generation of geostationary satellites have hemispheric coverage at 10-15 minute intervals, higher temporal frequency observations are ideal for studying mesoscale severe weather events. In this work, we apply a task specific optical flow approach to temporal up-sampling using deep convolutional neural networks. We apply this technique to 16-bands of GOES-R/Advanced Baseline Imager mesoscale dataset to temporally enhance full disk hemispheric snapshots of different spatial resolutions from 15 minutes to 1 minute. Experiments show the effectiveness of task specific optical flow and multi-scale blocks for interpolating high-frequency severe weather events relative to bilinear and global optical flow baselines. Lastly, we demonstrate strong performance in capturing variability during a convective precipitation events.



### Fairest of Them All: Establishing a Strong Baseline for Cross-Domain Person ReID
- **Arxiv ID**: http://arxiv.org/abs/1907.12016v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1907.12016v2)
- **Published**: 2019-07-28 05:20:34+00:00
- **Updated**: 2019-12-27 18:25:34+00:00
- **Authors**: Devinder Kumar, Parthipan Siva, Paul Marchwica, Alexander Wong
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification (ReID) remains a very difficult challenge in computer vision, and critical for large-scale video surveillance scenarios where an individual could appear in different camera views at different times. There has been recent interest in tackling this challenge using cross-domain approaches, which leverages data from source domains that are different than the target domain. Such approaches are more practical for real-world widespread deployment given that they don't require on-site training (as with unsupervised or domain transfer approaches) or on-site manual annotation and training (as with supervised approaches). In this study, we take a systematic approach to establishing a large baseline source domain and target domain for cross-domain person ReID. We accomplish this by conducting a comprehensive analysis to study the similarities between source domains proposed in literature, and studying the effects of incrementally increasing the size of the source domain. This allows us to establish a balanced source domain and target domain split that promotes variety in both source and target domains. Furthermore, using lessons learned from the state-of-the-art supervised person re-identification methods, we establish a strong baseline method for cross-domain person ReID. Experiments show that a source domain composed of two of the largest person ReID domains (SYSU and MSMT) performs well across six commonly-used target domains. Furthermore, we show that, surprisingly, two of the recent commonly-used domains (PRID and GRID) have too few query images to provide meaningful insights. As such, based on our findings, we propose the following balanced baseline for cross-domain person ReID consisting of: i) a fixed multi-source domain consisting of SYSU, MSMT, Airport and 3DPeS, and ii) a multi-target domain consisting of Market-1501, DukeMTMC-reID, CUHK03, PRID, GRID and VIPeR.



### What Should I Ask? Using Conversationally Informative Rewards for Goal-Oriented Visual Dialog
- **Arxiv ID**: http://arxiv.org/abs/1907.12021v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1907.12021v1)
- **Published**: 2019-07-28 06:15:35+00:00
- **Updated**: 2019-07-28 06:15:35+00:00
- **Authors**: Pushkar Shukla, Carlos Elmadjian, Richika Sharan, Vivek Kulkarni, Matthew Turk, William Yang Wang
- **Comment**: Accepted to ACL 2019
- **Journal**: None
- **Summary**: The ability to engage in goal-oriented conversations has allowed humans to gain knowledge, reduce uncertainty, and perform tasks more efficiently. Artificial agents, however, are still far behind humans in having goal-driven conversations. In this work, we focus on the task of goal-oriented visual dialogue, aiming to automatically generate a series of questions about an image with a single objective. This task is challenging since these questions must not only be consistent with a strategy to achieve a goal, but also consider the contextual information in the image. We propose an end-to-end goal-oriented visual dialogue system, that combines reinforcement learning with regularized information gain. Unlike previous approaches that have been proposed for the task, our work is motivated by the Rational Speech Act framework, which models the process of human inquiry to reach a goal. We test the two versions of our model on the GuessWhat?! dataset, obtaining significant results that outperform the current state-of-the-art models in the task of generating questions to find an undisclosed object in an image.



### DAR-Net: Dynamic Aggregation Network for Semantic Scene Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.12022v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1907.12022v2)
- **Published**: 2019-07-28 06:23:19+00:00
- **Updated**: 2019-12-26 03:13:59+00:00
- **Authors**: Zongyue Zhao, Min Liu, Karthik Ramani
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional grid/neighbor-based static pooling has become a constraint for point cloud geometry analysis. In this paper, we propose DAR-Net, a novel network architecture that focuses on dynamic feature aggregation. The central idea of DAR-Net is generating a self-adaptive pooling skeleton that considers both scene complexity and local geometry features. Providing variable semi-local receptive fields and weights, the skeleton serves as a bridge that connect local convolutional feature extractors and a global recurrent feature integrator. Experimental results on indoor scene datasets show advantages of the proposed approach compared to state-of-the-art architectures that adopt static pooling methods.



### Two-Stream CNN with Loose Pair Training for Multi-modal AMD Categorization
- **Arxiv ID**: http://arxiv.org/abs/1907.12023v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.12023v1)
- **Published**: 2019-07-28 06:27:01+00:00
- **Updated**: 2019-07-28 06:27:01+00:00
- **Authors**: Weisen Wang, Zhiyan Xu, Weihong Yu, Jianchun Zhao, Jingyuan Yang, Feng He, Zhikun Yang, Di Chen, Dayong Ding, Youxin Chen, Xirong Li
- **Comment**: accepted by MICCAI 2019
- **Journal**: None
- **Summary**: This paper studies automated categorization of age-related macular degeneration (AMD) given a multi-modal input, which consists of a color fundus image and an optical coherence tomography (OCT) image from a specific eye. Previous work uses a traditional method, comprised of feature extraction and classifier training that cannot be optimized jointly. By contrast, we propose a two-stream convolutional neural network (CNN) that is end-to-end. The CNN's fusion layer is tailored to the need of fusing information from the fundus and OCT streams. For generating more multi-modal training instances, we introduce Loose Pair training, where a fundus image and an OCT image are paired based on class labels rather than eyes. Moreover, for a visual interpretation of how the individual modalities make contributions, we extend the class activation mapping technique to the multi-modal scenario. Experiments on a real-world dataset collected from an outpatient clinic justify the viability of our proposal for multi-modal AMD categorization.



### Dilated Point Convolutions: On the Receptive Field Size of Point Convolutions on 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1907.12046v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12046v3)
- **Published**: 2019-07-28 08:52:41+00:00
- **Updated**: 2020-05-23 16:11:30+00:00
- **Authors**: Francis Engelmann, Theodora Kontogianni, Bastian Leibe
- **Comment**: ICRA 2020 Video https://www.youtube.com/watch?v=JDfFmuOvMkM Project
  https://francisengelmann.github.io/DPC/
- **Journal**: None
- **Summary**: In this work, we propose Dilated Point Convolutions (DPC). In a thorough ablation study, we show that the receptive field size is directly related to the performance of 3D point cloud processing tasks, including semantic segmentation and object classification. Point convolutions are widely used to efficiently process 3D data representations such as point clouds or graphs. However, we observe that the receptive field size of recent point convolutional networks is inherently limited. Our dilated point convolutions alleviate this issue, they significantly increase the receptive field size of point convolutions. Importantly, our dilation mechanism can easily be integrated into most existing point convolutional networks. To evaluate the resulting network architectures, we visualize the receptive field and report competitive scores on popular point cloud benchmarks.



### FocusNet: Imbalanced Large and Small Organ Segmentation with an End-to-End Deep Neural Network for Head and Neck CT Images
- **Arxiv ID**: http://arxiv.org/abs/1907.12056v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.12056v1)
- **Published**: 2019-07-28 09:45:19+00:00
- **Updated**: 2019-07-28 09:45:19+00:00
- **Authors**: Yunhe Gao, Rui Huang, Ming Chen, Zhe Wang, Jincheng Deng, Yuanyuan Chen, Yiwei Yang, Jie Zhang, Chanjuan Tao, Hongsheng Li
- **Comment**: MICCAI 2019
- **Journal**: None
- **Summary**: In this paper, we propose an end-to-end deep neural network for solving the problem of imbalanced large and small organ segmentation in head and neck (HaN) CT images. To conduct radiotherapy planning for nasopharyngeal cancer, more than 10 organs-at-risk (normal organs) need to be precisely segmented in advance. However, the size ratio between large and small organs in the head could reach hundreds. Directly using such imbalanced organ annotations to train deep neural networks generally leads to inaccurate small-organ label maps. We propose a novel end-to-end deep neural network to solve this challenging problem by automatically locating, ROI-pooling, and segmenting small organs with specifically designed small-organ sub-networks while maintaining the accuracy of large organ segmentation. A strong main network with densely connected atrous spatial pyramid pooling and squeeze-and-excitation modules is used for segmenting large organs, where large organs' label maps are directly output. For small organs, their probabilistic locations instead of label maps are estimated by the main network. High-resolution and multi-scale feature volumes for each small organ are ROI-pooled according to their locations and are fed into small-organ networks for accurate segmenting small organs. Our proposed network is extensively tested on both collected real data and the \emph{MICCAI Head and Neck Auto Segmentation Challenge 2015} dataset, and shows superior performance compared with state-of-the-art segmentation methods.



### Charting the Right Manifold: Manifold Mixup for Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.12087v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.12087v4)
- **Published**: 2019-07-28 14:14:55+00:00
- **Updated**: 2020-01-18 20:01:55+00:00
- **Authors**: Puneet Mangla, Mayank Singh, Abhishek Sinha, Nupur Kumari, Vineeth N Balasubramanian, Balaji Krishnamurthy
- **Comment**: WACV 2020, Code: https://github.com/nupurkmr9/S2M2_fewshot
- **Journal**: None
- **Summary**: Few-shot learning algorithms aim to learn model parameters capable of adapting to unseen classes with the help of only a few labeled examples. A recent regularization technique - Manifold Mixup focuses on learning a general-purpose representation, robust to small changes in the data distribution. Since the goal of few-shot learning is closely linked to robust representation learning, we study Manifold Mixup in this problem setting. Self-supervised learning is another technique that learns semantically meaningful features, using only the inherent structure of the data. This work investigates the role of learning relevant feature manifold for few-shot tasks using self-supervision and regularization techniques. We observe that regularizing the feature manifold, enriched via self-supervised techniques, with Manifold Mixup significantly improves few-shot learning performance. We show that our proposed method S2M2 beats the current state-of-the-art accuracy on standard few-shot learning datasets like CIFAR-FS, CUB, mini-ImageNet and tiered-ImageNet by 3-8 %. Through extensive experimentation, we show that the features learned using our approach generalize to complex few-shot evaluation tasks, cross-domain scenarios and are robust against slight changes to data distribution.



### Real-time Tracking-by-Detection of Human Motion in RGB-D Camera Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.12112v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12112v1)
- **Published**: 2019-07-28 17:23:50+00:00
- **Updated**: 2019-07-28 17:23:50+00:00
- **Authors**: Alessandro Malaguti, Marco Carraro, Mattia Guidolin, Luca Tagliapietra, Emanuele Menegatti, Stefano Ghidoni
- **Comment**: Accepted to IEEE SMC 2019
- **Journal**: None
- **Summary**: This paper presents a novel real-time tracking system capable of improving body pose estimation algorithms in distributed camera networks. The first stage of our approach introduces a linear Kalman filter operating at the body joints level, used to fuse single-view body poses coming from different detection nodes of the network and to ensure temporal consistency between them. The second stage, instead, refines the Kalman filter estimates by fitting a hierarchical model of the human body having constrained link sizes in order to ensure the physical consistency of the tracking. The effectiveness of the proposed approach is demonstrated through a broad experimental validation, performed on a set of sequences whose ground truth references are generated by a commercial marker-based motion capture system. The obtained results show how the proposed system outperforms the considered state-of-the-art approaches, granting accurate and reliable estimates. Moreover, the developed methodology constrains neither the number of persons to track, nor the number, position, synchronization, frame-rate, and manufacturer of the RGB-D cameras used. Finally, the real-time performances of the system are of paramount importance for a large number of real-world applications.



### It's All About The Scale -- Efficient Text Detection Using Adaptive Scaling
- **Arxiv ID**: http://arxiv.org/abs/1907.12122v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12122v1)
- **Published**: 2019-07-28 18:32:08+00:00
- **Updated**: 2019-07-28 18:32:08+00:00
- **Authors**: Elad Richardson, Yaniv Azar, Or Avioz, Niv Geron, Tomer Ronen, Zach Avraham, Stav Shapiro
- **Comment**: None
- **Journal**: None
- **Summary**: "Text can appear anywhere". This property requires us to carefully process all the pixels in an image in order to accurately localize all text instances. In particular, for the more difficult task of localizing small text regions, many methods use an enlarged image or even several rescaled ones as their input. This significantly increases the processing time of the entire image and needlessly enlarges background regions. If we were to have a prior telling us the coarse location of text instances in the image and their approximate scale, we could have adaptively chosen which regions to process and how to rescale them, thus significantly reducing the processing time. To estimate this prior we propose a segmentation-based network with an additional "scale predictor", an output channel that predicts the scale of each text segment. The network is applied on a scaled down image to efficiently approximate the desired prior, without processing all the pixels of the original image. The approximated prior is then used to create a compact image containing only text regions, resized to a canonical scale, which is fed again to the segmentation network for fine-grained detection. We show that our approach offers a powerful alternative to fixed scaling schemes, achieving an equivalent accuracy to larger input scales while processing far fewer pixels. Qualitative and quantitative results are presented on the ICDAR15 and ICDAR17 MLT benchmarks to validate our approach.



### An Empirical Study on Leveraging Scene Graphs for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1907.12133v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1907.12133v1)
- **Published**: 2019-07-28 19:59:20+00:00
- **Updated**: 2019-07-28 19:59:20+00:00
- **Authors**: Cheng Zhang, Wei-Lun Chao, Dong Xuan
- **Comment**: Accepted as oral presentation at BMVC 2019
- **Journal**: None
- **Summary**: Visual question answering (Visual QA) has attracted significant attention these years. While a variety of algorithms have been proposed, most of them are built upon different combinations of image and language features as well as multi-modal attention and fusion. In this paper, we investigate an alternative approach inspired by conventional QA systems that operate on knowledge graphs. Specifically, we investigate the use of scene graphs derived from images for Visual QA: an image is abstractly represented by a graph with nodes corresponding to object entities and edges to object relationships. We adapt the recently proposed graph network (GN) to encode the scene graph and perform structured reasoning according to the input question. Our empirical studies demonstrate that scene graphs can already capture essential information of images and graph networks have the potential to outperform state-of-the-art Visual QA algorithms but with a much cleaner architecture. By analyzing the features generated by GNs we can further interpret the reasoning process, suggesting a promising direction towards explainable Visual QA.



### FingerNet: Pushing The Limits of Fingerprint Recognition Using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1907.12956v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.12956v1)
- **Published**: 2019-07-28 21:00:56+00:00
- **Updated**: 2019-07-28 21:00:56+00:00
- **Authors**: Shervin Minaee, Elham Azimi, Amirali Abdolrashidi
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1907.09380
- **Journal**: None
- **Summary**: Fingerprint recognition has been utilized for cellphone authentication, airport security and beyond. Many different features and algorithms have been proposed to improve fingerprint recognition. In this paper, we propose an end-to-end deep learning framework for fingerprint recognition using convolutional neural networks (CNNs) which can jointly learn the feature representation and perform recognition. We train our model on a large-scale fingerprint recognition dataset, and improve over previous approaches in terms of accuracy. Our proposed model is able to achieve a very high recognition accuracy on a well-known fingerprint dataset. We believe this framework can be widely used for biometrics recognition tasks, making more scalable and accurate systems possible. We have also used a visualization technique to highlight the important areas in an input fingerprint image, that mostly impact the recognition results.



### Iris Recognition for Personal Identification using LAMSTAR neural network
- **Arxiv ID**: http://arxiv.org/abs/1907.12145v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12145v1)
- **Published**: 2019-07-28 22:04:48+00:00
- **Updated**: 2019-07-28 22:04:48+00:00
- **Authors**: Shideh Homayon, Mahdi Salarian
- **Comment**: None
- **Journal**: None
- **Summary**: Iris recognition is one of the most important biometric recognition method. This is because the iris texture provides many features such as freckles, coronas, stripes, furrows, crypts, etc. Those features are unique for different people and distinguishable. Such unique features in the anatomical structure of the iris make it possible the differentiation among individuals. So during last years huge number of people have been trying to improve its performance. In this article first different common steps for the Iris recognition system is explained. Then a special type of neural network is used for recognition part. Experimental results show high accuracy can be obtained especially when the primary steps are done well.



