# Arxiv Papers in cs.CV on 2019-07-21
### An Efficient 3D CNN for Action/Object Segmentation in Video
- **Arxiv ID**: http://arxiv.org/abs/1907.08895v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.08895v1)
- **Published**: 2019-07-21 02:05:38+00:00
- **Updated**: 2019-07-21 02:05:38+00:00
- **Authors**: Rui Hou, Chen Chen, Rahul Sukthankar, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Network (CNN) based image segmentation has made great progress in recent years. However, video object segmentation remains a challenging task due to its high computational complexity. Most of the previous methods employ a two-stream CNN framework to handle spatial and motion features separately. In this paper, we propose an end-to-end encoder-decoder style 3D CNN to aggregate spatial and temporal information simultaneously for video object segmentation. To efficiently process video, we propose 3D separable convolution for the pyramid pooling module and decoder, which dramatically reduces the number of operations while maintaining the performance. Moreover, we also extend our framework to video action segmentation by adding an extra classifier to predict the action label for actors in videos. Extensive experiments on several video datasets demonstrate the superior performance of the proposed approach for action and object segmentation compared to the state-of-the-art.



### Automated Muscle Segmentation from Clinical CT using Bayesian U-Net for Personalized Musculoskeletal Modeling
- **Arxiv ID**: http://arxiv.org/abs/1907.08915v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.08915v2)
- **Published**: 2019-07-21 05:04:16+00:00
- **Updated**: 2019-12-09 12:00:17+00:00
- **Authors**: Yuta Hiasa, Yoshito Otake, Masaki Takao, Takeshi Ogawa, Nobuhiko Sugano, Yoshinobu Sato
- **Comment**: 11 pages, 10 figures, and supplementary materials
- **Journal**: None
- **Summary**: We propose a method for automatic segmentation of individual muscles from a clinical CT. The method uses Bayesian convolutional neural networks with the U-Net architecture, using Monte Carlo dropout that infers an uncertainty metric in addition to the segmentation label. We evaluated the performance of the proposed method using two data sets: 20 fully annotated CTs of the hip and thigh regions and 18 partially annotated CTs that are publicly available from The Cancer Imaging Archive (TCIA) database. The experiments showed a Dice coefficient (DC) of 0.891 +/- 0.016 (mean +/- std) and an average symmetric surface distance (ASD) of 0.994 +/- 0.230 mm over 19 muscles in the set of 20 CTs. These results were statistically significant improvements compared to the state-of-the-art hierarchical multi-atlas method which resulted in 0.845 +/- 0.031 DC and 1.556 +/- 0.444 mm ASD. We evaluated validity of the uncertainty metric in the multi-class organ segmentation problem and demonstrated a correlation between the pixels with high uncertainty and the segmentation failure. One application of the uncertainty metric in active-learning is demonstrated, and the proposed query pixel selection method considerably reduced the manual annotation cost for expanding the training data set. The proposed method allows an accurate patient-specific analysis of individual muscle shapes in a clinical routine. This would open up various applications including personalization of biomechanical simulation and quantitative evaluation of muscle atrophy.



### Validation of Modulation Transfer Functions and Noise Power Spectra from Natural Scenes
- **Arxiv ID**: http://arxiv.org/abs/1907.08924v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.08924v1)
- **Published**: 2019-07-21 07:01:55+00:00
- **Updated**: 2019-07-21 07:01:55+00:00
- **Authors**: Edward W. S. Fry, Sophie Triantaphillidou, Robin B. Jenkin, John R. Jarvis, Ralph E. Jacobson
- **Comment**: None
- **Journal**: None
- **Summary**: The Modulation Transfer Function (MTF) and the Noise Power Spectrum (NPS) characterize imaging system sharpness/resolution and noise, respectively. Both measures are based on linear system theory but are applied routinely to systems employing non-linear, content-aware image processing. For such systems, MTFs/NPSs are derived inaccurately from traditional test charts containing edges, sinusoids, noise or uniform tone signals, which are unrepresentative of natural scene signals. The dead leaves test chart delivers improved measurements, but still has limitations when describing the performance of scene-dependent systems. In this paper, we validate several novel scene-and-process-dependent MTF (SPD-MTF) and NPS (SPD-NPS) measures that characterize, either: i) system performance concerning one scene, or ii) average real-world performance concerning many scenes, or iii) the level of system scene-dependency. We also derive novel SPD-NPS and SPD-MTF measures using the dead leaves chart. We demonstrate that all the proposed measures are robust and preferable for scene-dependent systems than current measures.



### Scene-and-Process-Dependent Spatial Image Quality Metrics
- **Arxiv ID**: http://arxiv.org/abs/1907.08926v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.08926v1)
- **Published**: 2019-07-21 07:13:43+00:00
- **Updated**: 2019-07-21 07:13:43+00:00
- **Authors**: Edward W. S. Fry, Sophie Triantaphillidou, Robin B. Jenkin, Ralph E. Jacobson, John R. Jarvis
- **Comment**: None
- **Journal**: None
- **Summary**: Spatial image quality metrics designed for camera systems generally employ the Modulation Transfer Function (MTF), the Noise Power Spectrum (NPS), and a visual contrast detection model. Prior art indicates that scene-dependent characteristics of non-linear, content-aware image processing are unaccounted for by MTFs and NPSs measured using traditional methods. We present two novel metrics: the log Noise Equivalent Quanta (log NEQ) and Visual log NEQ. They both employ scene-and-process-dependent MTF (SPD-MTF) and NPS (SPD-NPS) measures, which account for signal-transfer and noise scene-dependency, respectively. We also investigate implementing contrast detection and discrimination models that account for scene-dependent visual masking. Also, three leading camera metrics are revised that use the above scene-dependent measures. All metrics are validated by examining correlations with the perceived quality of images produced by simulated camera pipelines. Metric accuracy improved consistently when the SPD-MTFs and SPD-NPSs were implemented. The novel metrics outperformed existing metrics of the same genre.



### Watch It Twice: Video Captioning with a Refocused Video Encoder
- **Arxiv ID**: http://arxiv.org/abs/1907.12905v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12905v1)
- **Published**: 2019-07-21 08:53:42+00:00
- **Updated**: 2019-07-21 08:53:42+00:00
- **Authors**: Xiangxi Shi, Jianfei Cai, Shafiq Joty, Jiuxiang Gu
- **Comment**: None
- **Journal**: None
- **Summary**: With the rapid growth of video data and the increasing demands of various applications such as intelligent video search and assistance toward visually-impaired people, video captioning task has received a lot of attention recently in computer vision and natural language processing fields. The state-of-the-art video captioning methods focus more on encoding the temporal information, while lack of effective ways to remove irrelevant temporal information and also neglecting the spatial details. However, the current RNN encoding module in single time order can be influenced by the irrelevant temporal information, especially the irrelevant temporal information is at the beginning of the encoding. In addition, neglecting spatial information will lead to the relationship confusion of the words and detailed loss. Therefore, in this paper, we propose a novel recurrent video encoding method and a novel visual spatial feature for the video captioning task. The recurrent encoding module encodes the video twice with the predicted key frame to avoid the irrelevant temporal information often occurring at the beginning and the end of a video. The novel spatial features represent the spatial information in different regions of a video and enrich the details of a caption. Experiments on two benchmark datasets show superior performance of the proposed method.



### An Interpretable Compression and Classification System: Theory and Applications
- **Arxiv ID**: http://arxiv.org/abs/1907.08952v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1907.08952v2)
- **Published**: 2019-07-21 10:16:56+00:00
- **Updated**: 2020-04-14 13:13:31+00:00
- **Authors**: Tzu-Wei Tseng, Kai-Jiun Yang, C. -C. Jay Kuo, Shang-Ho, Tsai
- **Comment**: 12 pages, 12 figures and 5 tables
- **Journal**: None
- **Summary**: This study proposes a low-complexity interpretable classification system. The proposed system contains three main modules including feature extraction, feature reduction, and classification. All of them are linear. Thanks to the linear property, the extracted and reduced features can be inversed to original data, like a linear transform such as Fourier transform, so that one can quantify and visualize the contribution of individual features towards the original data. Also, the reduced features and reversibility naturally endure the proposed system ability of data compression. This system can significantly compress data with a small percent deviation between the compressed and the original data. At the same time, when the compressed data is used for classification, it still achieves high testing accuracy. Furthermore, we observe that the extracted features of the proposed system can be approximated to uncorrelated Gaussian random variables. Hence, classical theory in estimation and detection can be applied for classification. This motivates us to propose using a MAP (maximum a posteriori) based classification method. As a result, the extracted features and the corresponding performance have statistical meaning and mathematically interpretable. Simulation results show that the proposed classification system not only enjoys significant reduced training and testing time but also high testing accuracy compared to the conventional schemes.



### Tracking Holistic Object Representations
- **Arxiv ID**: http://arxiv.org/abs/1907.12920v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, stat.ML, 68T45, 62H30, 62H35, 51N99, 68T05, 68T20, 68T40, 68W25, 15B99, I.4.10; I.2.10; I.2.6; I.2.9
- **Links**: [PDF](http://arxiv.org/pdf/1907.12920v2)
- **Published**: 2019-07-21 10:51:21+00:00
- **Updated**: 2019-08-06 09:27:19+00:00
- **Authors**: Axel Sauer, Elie Aljalbout, Sami Haddadin
- **Comment**: Accepted for oral presentation at BMVC 2019
- **Journal**: None
- **Summary**: Recent advances in visual tracking are based on siamese feature extractors and template matching. For this category of trackers, latest research focuses on better feature embeddings and similarity measures. In this work, we focus on building holistic object representations for tracking. We propose a framework that is designed to be used on top of previous trackers without any need for further training of the siamese network. The framework leverages the idea of obtaining additional object templates during the tracking process. Since the number of stored templates is limited, our method only keeps the most diverse ones. We achieve this by providing a new diversity measure in the space of siamese features. The obtained representation contains information beyond the ground truth object location provided to the system. It is then useful for tracking itself but also for further tasks which require a visual understanding of objects. Strong empirical results on tracking benchmarks indicate that our method can improve the performance and robustness of the underlying trackers while barely reducing their speed. In addition, our method is able to match current state-of-the-art results, while using a simpler and older network architecture and running three times faster.



### Image Classification with Hierarchical Multigraph Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.09000v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.09000v1)
- **Published**: 2019-07-21 16:30:32+00:00
- **Updated**: 2019-07-21 16:30:32+00:00
- **Authors**: Boris Knyazev, Xiao Lin, Mohamed R. Amer, Graham W. Taylor
- **Comment**: 13 pages, BMVC 2019
- **Journal**: None
- **Summary**: Graph Convolutional Networks (GCNs) are a class of general models that can learn from graph structured data. Despite being general, GCNs are admittedly inferior to convolutional neural networks (CNNs) when applied to vision tasks, mainly due to the lack of domain knowledge that is hardcoded into CNNs, such as spatially oriented translation invariant filters. However, a great advantage of GCNs is the ability to work on irregular inputs, such as superpixels of images. This could significantly reduce the computational cost of image reasoning tasks. Another key advantage inherent to GCNs is the natural ability to model multirelational data. Building upon these two promising properties, in this work, we show best practices for designing GCNs for image classification; in some cases even outperforming CNNs on the MNIST, CIFAR-10 and PASCAL image datasets.



### Attention Filtering for Multi-person Spatiotemporal Action Detection on Deep Two-Stream CNN Architectures
- **Arxiv ID**: http://arxiv.org/abs/1907.12919v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.12919v1)
- **Published**: 2019-07-21 16:53:43+00:00
- **Updated**: 2019-07-21 16:53:43+00:00
- **Authors**: João Antunes, Pedro Abreu, Alexandre Bernardino, Asim Smailagic, Daniel Siewiorek
- **Comment**: None
- **Journal**: None
- **Summary**: Action detection and recognition tasks have been the target of much focus in the computer vision community due to their many applications, namely, security, robotics and recommendation systems. Recently, datasets like AVA, provide multi-person, multi-label, spatiotemporal action detection and recognition challenges. Being unable to discern which portions of the input to use for classification is a limitation of two-stream CNN approaches, once the vision task involves several people with several labels. We address this limitation and improve the state-of-the-art performance of two-stream CNNs. In this paper we present four contributions: our fovea attention filtering that highlights targets for classification without discarding background; a generalized binary loss function designed for the AVA dataset; miniAVA, a partition of AVA that maintains temporal continuity and class distribution with only one tenth of the dataset size; and ablation studies on alternative attention filters. Our method, using fovea attention filtering and our generalized binary loss, achieves a relative video mAP improvement of 20% over the two-stream baseline in AVA, and is competitive with the state-of-the-art in the UCF101-24. We also show a relative video mAP improvement of 12.6% when using our generalized binary loss over the standard sum-of-sigmoids.



### signADAM: Learning Confidences for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.09008v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.09008v1)
- **Published**: 2019-07-21 17:08:50+00:00
- **Updated**: 2019-07-21 17:08:50+00:00
- **Authors**: Dong Wang, Yicheng Liu, Wenwo Tang, Fanhua Shang, Hongying Liu, Qigong Sun, Licheng Jiao
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: In this paper, we propose a new first-order gradient-based algorithm to train deep neural networks. We first introduce the sign operation of stochastic gradients (as in sign-based methods, e.g., SIGN-SGD) into ADAM, which is called as signADAM. Moreover, in order to make the rate of fitting each feature closer, we define a confidence function to distinguish different components of gradients and apply it to our algorithm. It can generate more sparse gradients than existing algorithms do. We call this new algorithm signADAM++. In particular, both our algorithms are easy to implement and can speed up training of various deep neural networks. The motivation of signADAM++ is preferably learning features from the most different samples by updating large and useful gradients regardless of useless information in stochastic gradients. We also establish theoretical convergence guarantees for our algorithms. Empirical results on various datasets and models show that our algorithms yield much better performance than many state-of-the-art algorithms including SIGN-SGD, SIGNUM and ADAM. We also analyze the performance from multiple perspectives including the loss landscape and develop an adaptive method to further improve generalization. The source code is available at https://github.com/DongWanginxdu/signADAM-Learn-by-Confidence.



### ImageNet-trained deep neural network exhibits illusion-like response to the Scintillating Grid
- **Arxiv ID**: http://arxiv.org/abs/1907.09019v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1907.09019v2)
- **Published**: 2019-07-21 19:14:47+00:00
- **Updated**: 2019-08-05 02:13:38+00:00
- **Authors**: Eric D. Sun, Ron Dekel
- **Comment**: Supplementary material at end of document
- **Journal**: None
- **Summary**: Deep neural network (DNN) models for computer vision are now capable of human-level object recognition. Consequently, similarities in the performance and vulnerabilities of DNN and human vision are of great interest. Here we characterize the response of the VGG-19 DNN to images of the Scintillating Grid visual illusion, in which white dots are perceived to be partially black. We observed a significant deviation from the expected monotonic relation between VGG-19 representational dissimilarity and dot whiteness in the Scintillating Grid. That is, a linear increase in dot whiteness leads to a non-linear increase and then, remarkably, a decrease (non-monotonicity) in representational dissimilarity. In control images, mostly monotonic relations between representational dissimilarity and dot whiteness were observed. Furthermore, the dot whiteness level corresponding to the maximal representational dissimilarity (i.e. onset of non-monotonic dissimilarity) matched closely with that corresponding to the onset of illusion perception in human observers. As such, the non-monotonic response in the DNN is a potential model correlate for human illusion perception.



### TARN: Temporal Attentive Relation Network for Few-Shot and Zero-Shot Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.09021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09021v1)
- **Published**: 2019-07-21 19:52:24+00:00
- **Updated**: 2019-07-21 19:52:24+00:00
- **Authors**: Mina Bishay, Georgios Zoumpourlis, Ioannis Patras
- **Comment**: 14 pages, IEEE Transactions on Affective Computing
- **Journal**: None
- **Summary**: In this paper we propose a novel Temporal Attentive Relation Network (TARN) for the problems of few-shot and zero-shot action recognition. At the heart of our network is a meta-learning approach that learns to compare representations of variable temporal length, that is, either two videos of different length (in the case of few-shot action recognition) or a video and a semantic representation such as word vector (in the case of zero-shot action recognition). By contrast to other works in few-shot and zero-shot action recognition, we a) utilise attention mechanisms so as to perform temporal alignment, and b) learn a deep-distance measure on the aligned representations at video segment level. We adopt an episode-based training scheme and train our network in an end-to-end manner. The proposed method does not require any fine-tuning in the target domain or maintaining additional representations as is the case of memory networks. Experimental results show that the proposed architecture outperforms the state of the art in few-shot action recognition, and achieves competitive results in zero-shot action recognition.



### Biometric Blockchain: A Better Solution for the Security and Trust of Food Logistics
- **Arxiv ID**: http://arxiv.org/abs/1907.10589v2
- **DOI**: 10.1088/1757-899X/646/1/012009
- **Categories**: **cs.CR**, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1907.10589v2)
- **Published**: 2019-07-21 23:07:58+00:00
- **Updated**: 2019-09-11 22:18:11+00:00
- **Authors**: Bing Xu, Tobechukwu Agbele, Richard Jiang
- **Comment**: None
- **Journal**: 2019 3rd International Conference on Artificial Intelligence
  Applications and Technologies (AIAAT 2019)
- **Summary**: Blockchain has been emerging as a promising technology that could totally change the landscape of data security in the coming years, particularly for data access over Internet-of-Things and cloud servers. However, blockchain itself, though secured by its protocol, does not identify who owns the data and who uses the data. Other than simply encrypting data into keys, in this paper, we proposed a protocol called Biometric Blockchain (BBC) that explicitly incorporate the biometric cues of individuals to unambiguously identify the creators and users in a blockchain-based system, particularly to address the increasing needs to secure the food logistics, following the recently widely reported incident on wrongly labelled foods that caused the death of a customer on a flight. The advantage of using BBC in the food logistics is clear: it can not only identify if the data or labels are authentic, but also clearly record who is responsible for the secured data or labels. As a result, such a BBC-based solution can great ease the difficulty to control the risks accompanying the food logistics, such as faked foods or wrong gradient labels.



### Shallow Unorganized Neural Networks using Smart Neuron Model for Visual Perception
- **Arxiv ID**: http://arxiv.org/abs/1907.09050v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.09050v2)
- **Published**: 2019-07-21 23:09:35+00:00
- **Updated**: 2019-10-30 23:50:49+00:00
- **Authors**: Richard Jiang, Danny Crookes
- **Comment**: None
- **Journal**: None
- **Summary**: The recent success of Deep Neural Networks (DNNs) has revealed the significant capability of neural computing in many challenging applications. Although DNNs are derived from emulating biological neurons, there still exist doubts over whether or not DNNs are the final and best model to emulate the mechanism of human intelligence. In particular, there are two discrepancies between computational DNN models and the observed facts of biological neurons. First, human neurons are interconnected randomly, while DNNs need carefully-designed architectures to work properly. Second, human neurons usually have a long spiking latency (~100ms) which implies that not many layers can be involved in making a decision, while DNNs could have hundreds of layers to guarantee high accuracy. In this paper, we propose a new computational model, namely shallow unorganized neural networks (SUNNs), in contrast to ANNs/DNNs. The proposed SUNNs differ from standard ANNs or DNNs in three fundamental aspects: 1) SUNNs are based on an adaptive neuron cell model, Smart Neurons, that allows each artificial neuron cell to adaptively respond to its inputs rather than carrying out a fixed weighted-sum operation like the classic neuron model in ANNs/DNNs; 2) SUNNs can cope with computational tasks with very shallow architectures; 3) SUNNs have a natural topology with random interconnections, as the human brain does, and as proposed by Turing's B-type unorganized machines. We implemented the proposed SUNN architecture and tested it on a number of unsupervised early stage visual perception tasks. Surprisingly, such simple shallow architectures achieved very good results in our experiments. The success of our new computational model makes it the first workable example of Turing's B-Type unorganized machine that can achieve comparable or better performance against the state-of-the-art algorithms.



