# Arxiv Papers in cs.CV on 2019-07-27
### Quadtree Generating Networks: Efficient Hierarchical Scene Parsing with Sparse Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1907.11821v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11821v2)
- **Published**: 2019-07-27 00:20:12+00:00
- **Updated**: 2019-09-17 18:58:50+00:00
- **Authors**: Kashyap Chitta, Jose M. Alvarez, Martial Hebert
- **Comment**: Accepted for IEEE Winter Conference on Applications of Computer
  Vision (WACV) 2020
- **Journal**: None
- **Summary**: Semantic segmentation with Convolutional Neural Networks is a memory-intensive task due to the high spatial resolution of feature maps and output predictions. In this paper, we present Quadtree Generating Networks (QGNs), a novel approach able to drastically reduce the memory footprint of modern semantic segmentation networks. The key idea is to use quadtrees to represent the predictions and target segmentation masks instead of dense pixel grids. Our quadtree representation enables hierarchical processing of an input image, with the most computationally demanding layers only being used at regions in the image containing boundaries between classes. In addition, given a trained model, our representation enables flexible inference schemes to trade-off accuracy and computational cost, allowing the network to adapt in constrained situations such as embedded devices. We demonstrate the benefits of our approach on the Cityscapes, SUN-RGBD and ADE20k datasets. On Cityscapes, we obtain an relative 3% mIoU improvement compared to a dilated network with similar memory consumption; and only receive a 3% relative mIoU drop compared to a large dilated network, while reducing memory consumption by over 4$\times$.



### Forced Spatial Attention for Driver Foot Activity Classification
- **Arxiv ID**: http://arxiv.org/abs/1907.11824v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11824v3)
- **Published**: 2019-07-27 01:36:09+00:00
- **Updated**: 2019-10-20 20:53:43+00:00
- **Authors**: Akshay Rangesh, Mohan M. Trivedi
- **Comment**: None
- **Journal**: None
- **Summary**: This paper provides a simple solution for reliably solving image classification tasks tied to spatial locations of salient objects in the scene. Unlike conventional image classification approaches that are designed to be invariant to translations of objects in the scene, we focus on tasks where the output classes vary with respect to where an object of interest is situated within an image. To handle this variant of the image classification task, we propose augmenting the standard cross-entropy (classification) loss with a domain dependent Forced Spatial Attention (FSA) loss, which in essence compels the network to attend to specific regions in the image associated with the desired output class. To demonstrate the utility of this loss function, we consider the task of driver foot activity classification - where each activity is strongly correlated with where the driver's foot is in the scene. Training with our proposed loss function results in significantly improved accuracies, better generalization, and robustness against noise, while obviating the need for very large datasets.



### Reprojection R-CNN: A Fast and Accurate Object Detector for 360Â° Images
- **Arxiv ID**: http://arxiv.org/abs/1907.11830v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11830v1)
- **Published**: 2019-07-27 02:12:41+00:00
- **Updated**: 2019-07-27 02:12:41+00:00
- **Authors**: Pengyu Zhao, Ansheng You, Yuanxing Zhang, Jiaying Liu, Kaigui Bian, Yunhai Tong
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: 360{\deg} images are usually represented in either equirectangular projection (ERP) or multiple perspective projections. Different from the flat 2D images, the detection task is challenging for 360{\deg} images due to the distortion of ERP and the inefficiency of perspective projections. However, existing methods mostly focus on one of the above representations instead of both, leading to limited detection performance. Moreover, the lack of appropriate bounding-box annotations as well as the annotated datasets further increases the difficulties of the detection task. In this paper, we present a standard object detection framework for 360{\deg} images. Specifically, we adapt the terminologies of the traditional object detection task to the omnidirectional scenarios, and propose a novel two-stage object detector, i.e., Reprojection R-CNN by combining both ERP and perspective projection. Owing to the omnidirectional field-of-view of ERP, Reprojection R-CNN first generates coarse region proposals efficiently by a distortion-aware spherical region proposal network. Then, it leverages the distortion-free perspective projection and refines the proposed regions by a novel reprojection network. We construct two novel synthetic datasets for training and evaluation. Experiments reveal that Reprojection R-CNN outperforms the previous state-of-the-art methods on the mAP metric. In addition, the proposed detector could run at 178ms per image in the panoramic datasets, which implies its practicability in real-world applications.



### Hybrid-Attention based Decoupled Metric Learning for Zero-Shot Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1907.11832v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11832v1)
- **Published**: 2019-07-27 02:27:10+00:00
- **Updated**: 2019-07-27 02:27:10+00:00
- **Authors**: Binghui Chen, Weihong Deng
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: In zero-shot image retrieval (ZSIR) task, embedding learning becomes more attractive, however, many methods follow the traditional metric learning idea and omit the problems behind zero-shot settings. In this paper, we first emphasize the importance of learning visual discriminative metric and preventing the partial/selective learning behavior of learner in ZSIR, and then propose the Decoupled Metric Learning (DeML) framework to achieve these individually. Instead of coarsely optimizing an unified metric, we decouple it into multiple attention-specific parts so as to recurrently induce the discrimination and explicitly enhance the generalization. And they are mainly achieved by our object-attention module based on random walk graph propagation and the channel-attention module based on the adversary constraint, respectively. We demonstrate the necessity of addressing the vital problems in ZSIR on the popular benchmarks, outperforming the state-of-theart methods by a significant margin. Code is available at http://www.bhchen.cn



### Pick-and-Learn: Automatic Quality Evaluation for Noisy-Labeled Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.11835v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.11835v1)
- **Published**: 2019-07-27 02:34:30+00:00
- **Updated**: 2019-07-27 02:34:30+00:00
- **Authors**: Haidong Zhu, Jialin Shi, Ji Wu
- **Comment**: Accepted for MICCAI2019
- **Journal**: None
- **Summary**: Deep learning methods have achieved promising performance in many areas, but they are still struggling with noisy-labeled images during the training process. Considering that the annotation quality indispensably relies on great expertise, the problem is even more crucial in the medical image domain. How to eliminate the disturbance from noisy labels for segmentation tasks without further annotations is still a significant challenge. In this paper, we introduce our label quality evaluation strategy for deep neural networks automatically assessing the quality of each label, which is not explicitly provided, and training on clean-annotated ones. We propose a solution for network automatically evaluating the relative quality of the labels in the training set and using good ones to tune the network parameters. We also design an overfitting control module to let the network maximally learn from the precise annotations during the training process. Experiments on the public biomedical image segmentation dataset have proved the method outperforms baseline methods and retains both high accuracy and good generalization at different noise levels.



### Attribute Aware Pooling for Pedestrian Attribute Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.11837v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.11837v1)
- **Published**: 2019-07-27 02:45:32+00:00
- **Updated**: 2019-07-27 02:45:32+00:00
- **Authors**: Kai Han, Yunhe Wang, Han Shu, Chuanjian Liu, Chunjing Xu, Chang Xu
- **Comment**: Accepted by IJCAI 2019
- **Journal**: None
- **Summary**: This paper expands the strength of deep convolutional neural networks (CNNs) to the pedestrian attribute recognition problem by devising a novel attribute aware pooling algorithm. Existing vanilla CNNs cannot be straightforwardly applied to handle multi-attribute data because of the larger label space as well as the attribute entanglement and correlations. We tackle these challenges that hampers the development of CNNs for multi-attribute classification by fully exploiting the correlation between different attributes. The multi-branch architecture is adopted for fucusing on attributes at different regions. Besides the prediction based on each branch itself, context information of each branch are employed for decision as well. The attribute aware pooling is developed to integrate both kinds of information. Therefore, attributes which are indistinct or tangled with others can be accurately recognized by exploiting the context information. Experiments on benchmark datasets demonstrate that the proposed pooling method appropriately explores and exploits the correlations between attributes for the pedestrian attribute recognition.



### Inertial nonconvex alternating minimizations for the image deblurring
- **Arxiv ID**: http://arxiv.org/abs/1907.12945v1
- **DOI**: 10.1109/TIP.2019.2924339
- **Categories**: **math.OC**, cs.CV, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.12945v1)
- **Published**: 2019-07-27 02:50:45+00:00
- **Updated**: 2019-07-27 02:50:45+00:00
- **Authors**: Tao Sun, Roberto Barrio, Marcos Rodriguez, Hao Jiang
- **Comment**: Transactions on Image Processing
- **Journal**: None
- **Summary**: In image processing, Total Variation (TV) regularization models are commonly used to recover blurred images. One of the most efficient and popular methods to solve the convex TV problem is the Alternating Direction Method of Multipliers (ADMM) algorithm, recently extended using the inertial proximal point method. Although all the classical studies focus on only a convex formulation, recent articles are paying increasing attention to the nonconvex methodology due to its good numerical performance and properties. In this paper, we propose to extend the classical formulation with a novel nonconvex Alternating Direction Method of Multipliers with the Inertial technique (IADMM). Under certain assumptions on the parameters, we prove the convergence of the algorithm with the help of the Kurdyka-{\L}ojasiewicz property. We also present numerical simulations on classical TV image reconstruction problems to illustrate the efficiency of the new algorithm and its behavior compared with the well established ADMM method.



### Learning Instance-wise Sparsity for Accelerating Deep Models
- **Arxiv ID**: http://arxiv.org/abs/1907.11840v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.11840v1)
- **Published**: 2019-07-27 02:59:38+00:00
- **Updated**: 2019-07-27 02:59:38+00:00
- **Authors**: Chuanjian Liu, Yunhe Wang, Kai Han, Chunjing Xu, Chang Xu
- **Comment**: Accepted by IJCAI 2019
- **Journal**: None
- **Summary**: Exploring deep convolutional neural networks of high efficiency and low memory usage is very essential for a wide variety of machine learning tasks. Most of existing approaches used to accelerate deep models by manipulating parameters or filters without data, e.g., pruning and decomposition. In contrast, we study this problem from a different perspective by respecting the difference between data. An instance-wise feature pruning is developed by identifying informative features for different instances. Specifically, by investigating a feature decay regularization, we expect intermediate feature maps of each instance in deep neural networks to be sparse while preserving the overall network performance. During online inference, subtle features of input images extracted by intermediate layers of a well-trained neural network can be eliminated to accelerate the subsequent calculations. We further take coefficient of variation as a measure to select the layers that are appropriate for acceleration. Extensive experiments conducted on benchmark datasets and networks demonstrate the effectiveness of the proposed method.



### Generative Adversarial Network for Handwritten Text
- **Arxiv ID**: http://arxiv.org/abs/1907.11845v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.11845v3)
- **Published**: 2019-07-27 04:15:10+00:00
- **Updated**: 2020-02-27 07:17:16+00:00
- **Authors**: Bo Ji, Tianyi Chen
- **Comment**: 12 pages, 7 figures, submitted for WACV 2020
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) have proven hugely successful in variety of applications of image processing. However, generative adversarial networks for handwriting is relatively rare somehow because of difficulty of handling sequential handwriting data by Convolutional Neural Network (CNN). In this paper, we propose a handwriting generative adversarial network framework (HWGANs) for synthesizing handwritten stroke data. The main features of the new framework include: (i) A discriminator consists of an integrated CNN-Long-Short-Term- Memory (LSTM) based feature extraction with Path Signature Features (PSF) as input and a Feedforward Neural Network (FNN) based binary classifier; (ii) A recurrent latent variable model as generator for synthesizing sequential handwritten data. The numerical experiments show the effectivity of the new model. Moreover, comparing with sole handwriting generator, the HWGANs synthesize more natural and realistic handwritten text.



### Genetic Deep Learning for Lung Cancer Screening
- **Arxiv ID**: http://arxiv.org/abs/1907.11849v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11849v1)
- **Published**: 2019-07-27 04:47:35+00:00
- **Updated**: 2019-07-27 04:47:35+00:00
- **Authors**: Hunter Park, Connor Monahan
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have shown great promise in improving computer aided detection (CADe). From classifying tumors found via mammography as benign or malignant to automated detection of colorectal polyps in CT colonography, these advances have helped reduce the need for further evaluation with invasive testing and prevent errors from missed diagnoses by acting as a second observer in today's fast paced and high volume clinical environment. CADe methods have become faster and more precise thanks to innovations in deep learning over the past several years. With advancements such as the inception module and utilization of residual connections, the approach to designing CNN architectures has become an art. It is customary to use proven models and fine tune them for particular tasks given a dataset, often requiring tedious work. We investigated using a genetic algorithm (GA) to conduct a neural architectural search (NAS) to generate a novel CNN architecture to find early stage lung cancer in chest x-rays (CXR). Using a dataset of over twelve thousand biopsy proven cases of lung cancer, the trained classification model achieved an accuracy of 97.15% with a PPV of 99.88% and a NPV of 94.81%, beating models such as Inception-V3 and ResNet-152 while simultaneously reducing the number of parameters a factor of 4 and 14, respectively.



### A Benchmark on Tricks for Large-scale Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1907.11854v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.11854v2)
- **Published**: 2019-07-27 05:58:00+00:00
- **Updated**: 2020-04-23 06:29:25+00:00
- **Authors**: Byungsoo Ko, Minchul Shin, Geonmo Gu, HeeJae Jun, Tae Kwan Lee, Youngjoon Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Many studies have been performed on metric learning, which has become a key ingredient in top-performing methods of instance-level image retrieval. Meanwhile, less attention has been paid to pre-processing and post-processing tricks that can significantly boost performance. Furthermore, we found that most previous studies used small scale datasets to simplify processing. Because the behavior of a feature representation in a deep learning model depends on both domain and data, it is important to understand how model behave in large-scale environments when a proper combination of retrieval tricks is used. In this paper, we extensively analyze the effect of well-known pre-processing, post-processing tricks, and their combination for large-scale image retrieval. We found that proper use of these tricks can significantly improve model performance without necessitating complex architecture or introducing loss, as confirmed by achieving a competitive result on the Google Landmark Retrieval Challenge 2019.



### Blind Deblurring Using GANs
- **Arxiv ID**: http://arxiv.org/abs/1907.11880v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.11880v1)
- **Published**: 2019-07-27 09:30:21+00:00
- **Updated**: 2019-07-27 09:30:21+00:00
- **Authors**: Manoj Kumar Lenka, Anubha Pandey, Anurag Mittal
- **Comment**: 15 Pages (including reference and appendices). 6 Figures. 5 Tables
  (including appendices). Work done as a part of the Summer Research Fellowship
  Program by the Indian Academy of Sciences
- **Journal**: None
- **Summary**: Deblurring is the task of restoring a blurred image to a sharp one, retrieving the information lost due to the blur. In blind deblurring we have no information regarding the blur kernel. As deblurring can be considered as an image to image translation task, deep learning based solutions, including the ones which use GAN (Generative Adversarial Network), have been proven effective for deblurring. Most of them have an encoder-decoder structure. Our objective is to try different GAN structures and improve its performance through various modifications to the existing structure for supervised deblurring. In supervised deblurring we have pairs of blurred and their corresponding sharp images, while in the unsupervised case we have a set of blurred and sharp images but their is no correspondence between them. Modifications to the structures is done to improve the global perception of the model. As blur is non-uniform in nature, for deblurring we require global information of the entire image, whereas convolution used in CNN is able to provide only local perception. Deep models can be used to improve global perception but due to large number of parameters it becomes difficult for it to converge and inference time increases, to solve this we propose the use of attention module (non-local block) which was previously used in language translation and other image to image translation tasks in deblurring. Use of residual connection also improves the performance of deblurring as features from the lower layers are added to the upper layers of the model. It has been found that classical losses like L1, L2, and perceptual loss also help in training of GANs when added together with adversarial loss. We also concatenate edge information of the image to observe its effects on deblurring. We also use feedback modules to retain long term dependencies



### Context Model for Pedestrian Intention Prediction using Factored Latent-Dynamic Conditional Random Fields
- **Arxiv ID**: http://arxiv.org/abs/1907.11881v4
- **DOI**: 10.1109/TITS.2020.2995166
- **Categories**: **cs.CV**, cs.RO, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.11881v4)
- **Published**: 2019-07-27 09:34:12+00:00
- **Updated**: 2020-09-15 11:19:23+00:00
- **Authors**: Satyajit Neogi, Michael Hoy, Kang Dang, Hang Yu, Justin Dauwels
- **Comment**: Accepted by IEEE Transactions on Intelligent Transportation Systems
- **Journal**: None
- **Summary**: Smooth handling of pedestrian interactions is a key requirement for Autonomous Vehicles (AV) and Advanced Driver Assistance Systems (ADAS). Such systems call for early and accurate prediction of a pedestrian's crossing/not-crossing behaviour in front of the vehicle. Existing approaches to pedestrian behaviour prediction make use of pedestrian motion, his/her location in a scene and static context variables such as traffic lights, zebra crossings etc. We stress on the necessity of early prediction for smooth operation of such systems. We introduce the influence of vehicle interactions on pedestrian intention for this purpose. In this paper, we show a discernible advance in prediction time aided by the inclusion of such vehicle interaction context. We apply our methods to two different datasets, one in-house collected - NTU dataset and another public real-life benchmark - JAAD dataset. We also propose a generic graphical model Factored Latent-Dynamic Conditional Random Fields (FLDCRF) for single and multi-label sequence prediction as well as joint interaction modeling tasks. FLDCRF outperforms Long Short-Term Memory (LSTM) networks across the datasets ($\sim$100 sequences per dataset) over identical time-series features. While the existing best system predicts pedestrian stopping behaviour with 70\% accuracy 0.38 seconds before the actual events, our system achieves such accuracy at least 0.9 seconds on an average before the actual events across datasets.



### Effective and efficient ROI-wise visual encoding using an end-to-end CNN regression model and selective optimization
- **Arxiv ID**: http://arxiv.org/abs/1907.11885v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.11885v1)
- **Published**: 2019-07-27 10:09:05+00:00
- **Updated**: 2019-07-27 10:09:05+00:00
- **Authors**: Kai Qiao, Chi Zhang, Jian Chen, Linyuan Wang, Li Tong, Bin Yan
- **Comment**: under review in Computational Intelligence and Neuroscience
- **Journal**: None
- **Summary**: Recently, visual encoding based on functional magnetic resonance imaging (fMRI) have realized many achievements with the rapid development of deep network computation. Visual encoding model is aimed at predicting brain activity in response to presented image stimuli. Currently, visual encoding is accomplished mainly by firstly extracting image features through convolutional neural network (CNN) model pre-trained on computer vision task, and secondly training a linear regression model to map specific layer of CNN features to each voxel, namely voxel-wise encoding. However, the two-step manner model, essentially, is hard to determine which kind of well features are well linearly matched for beforehand unknown fMRI data with little understanding of human visual representation. Analogizing computer vision mostly related human vision, we proposed the end-to-end convolution regression model (ETECRM) in the region of interest (ROI)-wise manner to accomplish effective and efficient visual encoding. The end-to-end manner was introduced to make the model automatically learn better matching features to improve encoding performance. The ROI-wise manner was used to improve the encoding efficiency for many voxels. In addition, we designed the selective optimization including self-adapting weight learning and weighted correlation loss, noise regularization to avoid interfering of ineffective voxels in ROI-wise encoding. Experiment demonstrated that the proposed model obtained better predicting accuracy than the two-step manner of encoding models. Comparative analysis implied that end-to-end manner and large volume of fMRI data may drive the future development of visual encoding.



### Deep learning-based prediction of kinetic parameters from myocardial perfusion MRI
- **Arxiv ID**: http://arxiv.org/abs/1907.11899v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1907.11899v1)
- **Published**: 2019-07-27 11:58:43+00:00
- **Updated**: 2019-07-27 11:58:43+00:00
- **Authors**: Cian M. Scannell, Piet van den Bosch, Amedeo Chiribiri, Jack Lee, Marcel Breeuwer, Mitko Veta
- **Comment**: Medical Imaging with Deep Learning: MIDL 2019 Extended Abstract
  Track. MIDL 2019 [arXiv:1907.08612]
- **Journal**: None
- **Summary**: The quantification of myocardial perfusion MRI has the potential to provide a fast, automated and user-independent assessment of myocardial ischaemia. However, due to the relatively high noise level and low temporal resolution of the acquired data and the complexity of the tracer-kinetic models, the model fitting can yield unreliable parameter estimates. A solution to this problem is the use of Bayesian inference which can incorporate prior knowledge and improve the reliability of the parameter estimation. This, however, uses Markov chain Monte Carlo sampling to approximate the posterior distribution of the kinetic parameters which is extremely time intensive. This work proposes training convolutional networks to directly predict the kinetic parameters from the signal-intensity curves that are trained using estimates obtained from the Bayesian inference. This allows fast estimation of the kinetic parameters with a similar performance to the Bayesian inference.



### Semantic Guided Single Image Reflection Removal
- **Arxiv ID**: http://arxiv.org/abs/1907.11912v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11912v3)
- **Published**: 2019-07-27 13:52:00+00:00
- **Updated**: 2022-01-18 09:29:24+00:00
- **Authors**: Yunfei Liu, Yu Li, Shaodi You, Feng Lu
- **Comment**: Accepted by TOMM
- **Journal**: None
- **Summary**: Reflection is common in images capturing scenes behind a glass window, which is not only a disturbance visually but also influence the performance of other computer vision algorithms. Single image reflection removal is an ill-posed problem because the color at each pixel needs to be separated into two values, i.e., the desired clear background and the reflection. To solve it, existing methods propose priors such as smoothness, color consistency. However, the low-level priors are not reliable in complex scenes, for instance, when capturing a real outdoor scene through a window, both the foreground and background contain both smooth and sharp area and a variety of color. In this paper, inspired by the fact that human can separate the two layers easily by recognizing the objects, we use the object semantic as guidance to force the same semantic object belong to the same layer. Extensive experiments on different datasets show that adding the semantic information offers a significant improvement to reflection separation. We also demonstrate the applications of the proposed method to other computer vision tasks.



### Rethinking Classification and Localization for Cascade R-CNN
- **Arxiv ID**: http://arxiv.org/abs/1907.11914v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11914v1)
- **Published**: 2019-07-27 13:57:05+00:00
- **Updated**: 2019-07-27 13:57:05+00:00
- **Authors**: Ang Li, Xue Yang, Chongyang Zhang
- **Comment**: BMVC 2019 Camera Ready
- **Journal**: None
- **Summary**: We extend the state-of-the-art Cascade R-CNN with a simple feature sharing mechanism. Our approach focuses on the performance increases on high IoU but decreases on low IoU thresholds--a key problem this detector suffers from. Feature sharing is extremely helpful, our results show that given this mechanism embedded into all stages, we can easily narrow the gap between the last stage and preceding stages on low IoU thresholds without resorting to the commonly used testing ensemble but the network itself. We also observe obvious improvements on all IoU thresholds benefited from feature sharing, and the resulting cascade structure can easily match or exceed its counterparts, only with negligible extra parameters introduced. To push the envelope, we demonstrate 43.2 AP on COCO object detection without any bells and whistles including testing ensemble, surpassing previous Cascade R-CNN by a large margin. Our framework is easy to implement and we hope it can serve as a general and strong baseline for future research.



### Triangulation: Why Optimize?
- **Arxiv ID**: http://arxiv.org/abs/1907.11917v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11917v2)
- **Published**: 2019-07-27 14:08:09+00:00
- **Updated**: 2019-08-23 10:28:20+00:00
- **Authors**: Seong Hun Lee, Javier Civera
- **Comment**: Accepted to BMVC2019 (oral presentation)
- **Journal**: None
- **Summary**: For decades, it has been widely accepted that the gold standard for two-view triangulation is to minimize the cost based on reprojection errors. In this work, we challenge this idea. We propose a novel alternative to the classic midpoint method that leads to significantly lower 2D errors and parallax errors. It provides a numerically stable closed-form solution based solely on a pair of backprojected rays. Since our solution is rotationally invariant, it can also be applied for fisheye and omnidirectional cameras. We show that for small parallax angles, our method outperforms the state-of-the-art in terms of combined 2D, 3D and parallax accuracy, while achieving comparable speed.



### Remote Heart Rate Measurement from Highly Compressed Facial Videos: an End-to-end Deep Learning Solution with Video Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1907.11921v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.11921v1)
- **Published**: 2019-07-27 14:22:57+00:00
- **Updated**: 2019-07-27 14:22:57+00:00
- **Authors**: Zitong Yu, Wei Peng, Xiaobai Li, Xiaopeng Hong, Guoying Zhao
- **Comment**: IEEE ICCV2019, accepted
- **Journal**: None
- **Summary**: Remote photoplethysmography (rPPG), which aims at measuring heart activities without any contact, has great potential in many applications (e.g., remote healthcare). Existing rPPG approaches rely on analyzing very fine details of facial videos, which are prone to be affected by video compression. Here we propose a two-stage, end-to-end method using hidden rPPG information enhancement and attention networks, which is the first attempt to counter video compression loss and recover rPPG signals from highly compressed videos. The method includes two parts: 1) a Spatio-Temporal Video Enhancement Network (STVEN) for video enhancement, and 2) an rPPG network (rPPGNet) for rPPG signal recovery. The rPPGNet can work on its own for robust rPPG measurement, and the STVEN network can be added and jointly trained to further boost the performance especially on highly compressed videos. Comprehensive experiments are performed on two benchmark datasets to show that, 1) the proposed method not only achieves superior performance on compressed videos with high-quality videos pair, 2) it also generalizes well on novel data with only compressed videos available, which implies the promising potential for real world applications.



### MaskGAN: Towards Diverse and Interactive Facial Image Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1907.11922v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.11922v2)
- **Published**: 2019-07-27 14:23:19+00:00
- **Updated**: 2020-04-01 05:34:29+00:00
- **Authors**: Cheng-Han Lee, Ziwei Liu, Lingyun Wu, Ping Luo
- **Comment**: To appear in IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR), 2020. The code, models and dataset are available at:
  https://github.com/switchablenorms/CelebAMask-HQ
- **Journal**: None
- **Summary**: Facial image manipulation has achieved great progress in recent years. However, previous methods either operate on a predefined set of face attributes or leave users little freedom to interactively manipulate images. To overcome these drawbacks, we propose a novel framework termed MaskGAN, enabling diverse and interactive face manipulation. Our key insight is that semantic masks serve as a suitable intermediate representation for flexible face manipulation with fidelity preservation. MaskGAN has two main components: 1) Dense Mapping Network (DMN) and 2) Editing Behavior Simulated Training (EBST). Specifically, DMN learns style mapping between a free-form user modified mask and a target image, enabling diverse generation results. EBST models the user editing behavior on the source mask, making the overall framework more robust to various manipulated inputs. Specifically, it introduces dual-editing consistency as the auxiliary supervision signal. To facilitate extensive studies, we construct a large-scale high-resolution face dataset with fine-grained mask annotations named CelebAMask-HQ. MaskGAN is comprehensively evaluated on two challenging tasks: attribute transfer and style copy, demonstrating superior performance over other state-of-the-art methods. The code, models, and dataset are available at https://github.com/switchablenorms/CelebAMask-HQ.



### Segmenting Hyperspectral Images Using Spectral-Spatial Convolutional Neural Networks With Training-Time Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.11935v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11935v1)
- **Published**: 2019-07-27 15:32:10+00:00
- **Updated**: 2019-07-27 15:32:10+00:00
- **Authors**: Jakub Nalepa, Lukasz Tulczyjew, Michal Myller, Michal Kawulok
- **Comment**: Submitted to IEEE Geoscience and Remote Sensing Letters
- **Journal**: None
- **Summary**: Hyperspectral imaging provides detailed information about the scanned objects, as it captures their spectral characteristics within a large number of wavelength bands. Classification of such data has become an active research topic due to its wide applicability in a variety of fields. Deep learning has established the state of the art in the area, and it constitutes the current research mainstream. In this letter, we introduce a new spectral-spatial convolutional neural network, benefitting from a battery of data augmentation techniques which help deal with a real-life problem of lacking ground-truth training data. Our rigorous experiments showed that the proposed method outperforms other spectral-spatial techniques from the literature, and delivers precise hyperspectral classification in real time.



### Learnable Parameter Similarity
- **Arxiv ID**: http://arxiv.org/abs/1907.11943v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.11943v1)
- **Published**: 2019-07-27 16:14:08+00:00
- **Updated**: 2019-07-27 16:14:08+00:00
- **Authors**: Guangcong Wang, Jianhuang Lai, Wenqi Liang, Guangrun Wang
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Most of the existing approaches focus on specific visual tasks while ignoring the relations between them. Estimating task relation sheds light on the learning of high-order semantic concepts, e.g., transfer learning. How to reveal the underlying relations between different visual tasks remains largely unexplored. In this paper, we propose a novel \textbf{L}earnable \textbf{P}arameter \textbf{S}imilarity (\textbf{LPS}) method that learns an effective metric to measure the similarity of second-order semantics hidden in trained models. LPS is achieved by using a second-order neural network to align high-dimensional model parameters and learning second-order similarity in an end-to-end way. In addition, we create a model set called ModelSet500 as a parameter similarity learning benchmark that contains 500 trained models. Extensive experiments on ModelSet500 validate the effectiveness of the proposed method. Code will be released at \url{https://github.com/Wanggcong/learnable-parameter-similarity}.



### Learning Body Shape and Pose from Dense Correspondences
- **Arxiv ID**: http://arxiv.org/abs/1907.11955v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11955v1)
- **Published**: 2019-07-27 17:42:46+00:00
- **Updated**: 2019-07-27 17:42:46+00:00
- **Authors**: Yusuke Yoshiyasu, Lucas Gamez
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the problem of learning 3D human pose and body shape from 2D image dataset, without having to use 3D dataset (body shape and pose). The idea is to use dense correspondences between image points and a body surface, which can be annotated on in-the wild 2D images, and extract and aggregate 3D information from them. To do so, we propose a training strategy called ``deform-and-learn" where we alternate deformable surface registration and training of deep convolutional neural networks (ConvNets). Unlike previous approaches, our method does not require 3D pose annotations from a motion capture (MoCap) system or human intervention to validate 3D pose annotations.



### Attribute-Guided Deep Polarimetric Thermal-to-visible Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.11980v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11980v1)
- **Published**: 2019-07-27 21:14:30+00:00
- **Updated**: 2019-07-27 21:14:30+00:00
- **Authors**: Seyed Mehdi Iranmanesh, Nasser M. Nasrabadi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present an attribute-guided deep coupled learning framework to address the problem of matching polarimetric thermal face photos against a gallery of visible faces. The coupled framework contains two sub-networks, one dedicated to the visible spectrum and the second sub-network dedicated to the polarimetric thermal spectrum. Each sub-network is made of a generative adversarial network (GAN) architecture. We propose a novel Attribute-Guided Coupled Generative Adversarial Network (AGC-GAN) architecture which utilizes facial attributes to improve the thermal-to-visible face recognition performance. The proposed AGC-GAN exploits the facial attributes and leverages multiple loss functions in order to learn rich discriminative features in a common embedding subspace. To achieve a realistic photo reconstruction while preserving the discriminative information, we also add a perceptual loss term to the coupling loss function. An ablation study is performed to show the effectiveness of different loss functions for optimizing the proposed method. Moreover, the superiority of the model compared to the state-of-the-art models is demonstrated using polarimetric dataset.



