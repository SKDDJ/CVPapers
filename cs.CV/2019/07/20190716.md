# Arxiv Papers in cs.CV on 2019-07-16
### 2nd Place Solution to the GQA Challenge 2019
- **Arxiv ID**: http://arxiv.org/abs/1907.06794v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06794v2)
- **Published**: 2019-07-16 00:09:09+00:00
- **Updated**: 2019-08-16 22:04:53+00:00
- **Authors**: Shijie Geng, Ji Zhang, Hang Zhang, Ahmed Elgammal, Dimitris N. Metaxas
- **Comment**: None
- **Journal**: None
- **Summary**: We present a simple method that achieves unexpectedly superior performance for Complex Reasoning involved Visual Question Answering. Our solution collects statistical features from high-frequency words of all the questions asked about an image and use them as accurate knowledge for answering further questions of the same image. We are fully aware that this setting is not ubiquitously applicable, and in a more common setting one should assume the questions are asked separately and they cannot be gathered to obtain a knowledge base. Nonetheless, we use this method as an evidence to demonstrate our observation that the bottleneck effect is more severe on the feature extraction part than it is on the knowledge reasoning part. We show significant gaps when using the same reasoning model with 1) ground-truth features; 2) statistical features; 3) detected features from completely learned detectors, and analyze what these gaps mean to researches on visual reasoning topics. Our model with the statistical features achieves the 2nd place in the GQA Challenge 2019.



### Instant Motion Tracking and Its Applications to Augmented Reality
- **Arxiv ID**: http://arxiv.org/abs/1907.06796v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06796v1)
- **Published**: 2019-07-16 00:13:09+00:00
- **Updated**: 2019-07-16 00:13:09+00:00
- **Authors**: Jianing Wei, Genzhi Ye, Tyler Mullen, Matthias Grundmann, Adel Ahmadyan, Tingbo Hou
- **Comment**: CVPR Workshop on Computer Vision for Augmented and Virtual Reality,
  Long Beach, CA, 2019
- **Journal**: CVPR Workshop on Computer Vision for Augmented and Virtual
  Reality, Long Beach, CA, 2019
- **Summary**: Augmented Reality (AR) brings immersive experiences to users. With recent advances in computer vision and mobile computing, AR has scaled across platforms, and has increased adoption in major products. One of the key challenges in enabling AR features is proper anchoring of the virtual content to the real world, a process referred to as tracking. In this paper, we present a system for motion tracking, which is capable of robustly tracking planar targets and performing relative-scale 6DoF tracking without calibration. Our system runs in real-time on mobile phones and has been deployed in multiple major products on hundreds of millions of devices.



### Stereo-based terrain traversability analysis using normal-based segmentation and superpixel surface analysis
- **Arxiv ID**: http://arxiv.org/abs/1907.06823v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.06823v1)
- **Published**: 2019-07-16 03:48:18+00:00
- **Updated**: 2019-07-16 03:48:18+00:00
- **Authors**: Aras R. Dargazany
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, an stereo-based traversability analysis approach for all terrains in off-road mobile robotics, e.g. Unmanned Ground Vehicles (UGVs) is proposed. This approach reformulates the problem of terrain traversability analysis into two main problems: (1) 3D terrain reconstruction and (2) terrain all surfaces detection and analysis. The proposed approach is using stereo camera for perception and 3D reconstruction of the terrain. In order to detect all the existing surfaces in the 3D reconstructed terrain as superpixel surfaces (i.e. segments), an image segmentation technique is applied using geometry-based features (pixel-based surface normals). Having detected all the surfaces, Superpixel Surface Traversability Analysis approach (SSTA) is applied on all of the detected surfaces (superpixel segments) in order to classify them based on their traversability index. The proposed SSTA approach is based on: (1) Superpixel surface normal and plane estimation, (2) Traversability analysis using superpixel surface planes. Having analyzed all the superpixel surfaces based on their traversability, these surfaces are finally classified into five main categories as following: traversable, semi-traversable, non-traversable, unknown and undecided.



### Adversarial Sensor Attack on LiDAR-based Perception in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1907.06826v2
- **DOI**: 10.1145/3319535.3339815
- **Categories**: **cs.CR**, cs.CV, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.06826v2)
- **Published**: 2019-07-16 04:00:56+00:00
- **Updated**: 2019-08-20 13:26:03+00:00
- **Authors**: Yulong Cao, Chaowei Xiao, Benjamin Cyr, Yimeng Zhou, Won Park, Sara Rampazzi, Qi Alfred Chen, Kevin Fu, Z. Morley Mao
- **Comment**: Accepted at the ACM Conference on Computer and Communications
  Security (CCS), 2019
- **Journal**: None
- **Summary**: In Autonomous Vehicles (AVs), one fundamental pillar is perception, which leverages sensors like cameras and LiDARs (Light Detection and Ranging) to understand the driving environment. Due to its direct impact on road safety, multiple prior efforts have been made to study its the security of perception systems. In contrast to prior work that concentrates on camera-based perception, in this work we perform the first security study of LiDAR-based perception in AV settings, which is highly important but unexplored. We consider LiDAR spoofing attacks as the threat model and set the attack goal as spoofing obstacles close to the front of a victim AV. We find that blindly applying LiDAR spoofing is insufficient to achieve this goal due to the machine learning-based object detection process. Thus, we then explore the possibility of strategically controlling the spoofed attack to fool the machine learning model. We formulate this task as an optimization problem and design modeling methods for the input perturbation function and the objective function. We also identify the inherent limitations of directly solving the problem using optimization and design an algorithm that combines optimization and global sampling, which improves the attack success rates to around 75%. As a case study to understand the attack impact at the AV driving decision level, we construct and evaluate two attack scenarios that may damage road safety and mobility. We also discuss defense directions at the AV system, sensor, and machine learning model levels.



### An Inter-Layer Weight Prediction and Quantization for Deep Neural Networks based on a Smoothly Varying Weight Hypothesis
- **Arxiv ID**: http://arxiv.org/abs/1907.06835v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.06835v2)
- **Published**: 2019-07-16 04:44:59+00:00
- **Updated**: 2020-08-20 02:32:12+00:00
- **Authors**: Kang-Ho Lee, JoonHyun Jeong, Sung-Ho Bae
- **Comment**: 12 pages, 7 figures
- **Journal**: None
- **Summary**: Due to a resource-constrained environment, network compression has become an important part of deep neural networks research. In this paper, we propose a new compression method, \textit{Inter-Layer Weight Prediction} (ILWP) and quantization method which quantize the predicted residuals between the weights in all convolution layers based on an inter-frame prediction method in conventional video coding schemes. Furthermore, we found a phenomenon \textit{Smoothly Varying Weight Hypothesis} (SVWH) which is that the weights in adjacent convolution layers share strong similarity in shapes and values, i.e., the weights tend to vary smoothly along with the layers. Based on SVWH, we propose a second ILWP and quantization method which quantize the predicted residuals between the weights in adjacent convolution layers. Since the predicted weight residuals tend to follow Laplace distributions with very low variance, the weight quantization can more effectively be applied, thus producing more zero weights and enhancing the weight compression ratio. In addition, we propose a new \textit{inter-layer loss} for eliminating non-texture bits, which enabled us to more effectively store only texture bits. That is, the proposed loss regularizes the weights such that the collocated weights between the adjacent two layers have the same values. Finally, we propose an ILWP with an inter-layer loss and quantization method. Our comprehensive experiments show that the proposed method achieves a much higher weight compression rate at the same accuracy level compared with the previous quantization-based compression methods in deep neural networks.



### Improved Reinforcement Learning through Imitation Learning Pretraining Towards Image-based Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1907.06838v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.06838v1)
- **Published**: 2019-07-16 04:48:52+00:00
- **Updated**: 2019-07-16 04:48:52+00:00
- **Authors**: Tianqi Wang, Dong Eui Chang
- **Comment**: 5 pages, 2019 19th International Conference on Control, Automation
  and Systems (ICCAS 2019)
- **Journal**: None
- **Summary**: We present a training pipeline for the autonomous driving task given the current camera image and vehicle speed as the input to produce the throttle, brake, and steering control output. The simulator Airsim's convenient weather and lighting API provides a sufficient diversity during training which can be very helpful to increase the trained policy's robustness. In order to not limit the possible policy's performance, we use a continuous and deterministic control policy setting. We utilize ResNet-34 as our actor and critic networks with some slight changes in the fully connected layers. Considering human's mastery of this task and the high-complexity nature of this task, we first use imitation learning to mimic the given human policy and leverage the trained policy and its weights to the reinforcement learning phase for which we use DDPG. This combination shows a considerable performance boost comparing to both pure imitation learning and pure DDPG for the autonomous driving task.



### Deep inspection: an electrical distribution pole parts study via deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/1907.06844v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.06844v1)
- **Published**: 2019-07-16 05:10:38+00:00
- **Updated**: 2019-07-16 05:10:38+00:00
- **Authors**: Liangchen Liu, Teng Zhang, Kun Zhao, Arnold Wiliem, Kieren Astin-Walmsley, Brian Lovell
- **Comment**: electrical distribution pole inspection, integrated inspection
  system, object detection, imbalanced data classification, To appear in
  Proceeding of ICIP 2019
- **Journal**: None
- **Summary**: Electrical distribution poles are important assets in electricity supply. These poles need to be maintained in good condition to ensure they protect community safety, maintain reliability of supply, and meet legislative obligations. However, maintaining such a large volumes of assets is an expensive and challenging task. To address this, recent approaches utilise imagery data captured from helicopter and/or drone inspections. Whilst reducing the cost for manual inspection, manual analysis on each image is still required. As such, several image-based automated inspection systems have been proposed. In this paper, we target two major challenges: tiny object detection and extremely imbalanced datasets, which currently hinder the wide deployment of the automatic inspection. We propose a novel two-stage zoom-in detection method to gradually focus on the object of interest. To address the imbalanced dataset problem, we propose the resampling as well as reweighting schemes to iteratively adapt the model to the large intra-class variation of major class and balance the contributions to the loss from each class. Finally, we integrate these components together and devise a novel automatic inspection framework. Extensive experiments demonstrate that our proposed approaches are effective and can boost the performance compared to the baseline methods.



### AirwayNet: A Voxel-Connectivity Aware Approach for Accurate Airway Segmentation Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.06852v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.06852v1)
- **Published**: 2019-07-16 05:54:22+00:00
- **Updated**: 2019-07-16 05:54:22+00:00
- **Authors**: Yulei Qin, Mingjian Chen, Hao Zheng, Yun Gu, Mali Shen, Jie Yang, Xiaolin Huang, Yue-Min Zhu, Guang-Zhong Yang
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: Airway segmentation on CT scans is critical for pulmonary disease diagnosis and endobronchial navigation. Manual extraction of airway requires strenuous efforts due to the complicated structure and various appearance of airway. For automatic airway extraction, convolutional neural networks (CNNs) based methods have recently become the state-of-the-art approach. However, there still remains a challenge for CNNs to perceive the tree-like pattern and comprehend the connectivity of airway. To address this, we propose a voxel-connectivity aware approach named AirwayNet for accurate airway segmentation. By connectivity modeling, conventional binary segmentation task is transformed into 26 tasks of connectivity prediction. Thus, our AirwayNet learns both airway structure and relationship between neighboring voxels. To take advantage of context knowledge, lung distance map and voxel coordinates are fed into AirwayNet as additional semantic information. Compared to existing approaches, AirwayNet achieved superior performance, demonstrating the effectiveness of the network's awareness of voxel connectivity.



### Separable Convolutional LSTMs for Faster Video Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.06876v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.06876v1)
- **Published**: 2019-07-16 07:52:52+00:00
- **Updated**: 2019-07-16 07:52:52+00:00
- **Authors**: Andreas Pfeuffer, Klaus Dietmayer
- **Comment**: None
- **Journal**: 2019 22st International Conference on Intelligent Transportation
  Systems (ITSC)
- **Summary**: Semantic Segmentation is an important module for autonomous robots such as self-driving cars. The advantage of video segmentation approaches compared to single image segmentation is that temporal image information is considered, and their performance increases due to this. Hence, single image segmentation approaches are extended by recurrent units such as convolutional LSTM (convLSTM) cells, which are placed at suitable positions in the basic network architecture. However, a major critique of video segmentation approaches based on recurrent neural networks is their large parameter count and their computational complexity, and so, their inference time of one video frame takes up to 66 percent longer than their basic version. Inspired by the success of the spatial and depthwise separable convolutional neural networks, we generalize these techniques for convLSTMs in this work, so that the number of parameters and the required FLOPs are reduced significantly. Experiments on different datasets show that the segmentation approaches using the proposed, modified convLSTM cells achieve similar or slightly worse accuracy, but are up to 15 percent faster on a GPU than the ones using the standard convLSTM cells. Furthermore, a new evaluation metric is introduced, which measures the amount of flickering pixels in the segmented video sequence.



### Cascade RetinaNet: Maintaining Consistency for Single-Stage Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1907.06881v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06881v1)
- **Published**: 2019-07-16 08:01:56+00:00
- **Updated**: 2019-07-16 08:01:56+00:00
- **Authors**: Hongkai Zhang, Hong Chang, Bingpeng Ma, Shiguang Shan, Xilin Chen
- **Comment**: BMVC 2019
- **Journal**: None
- **Summary**: Recent researches attempt to improve the detection performance by adopting the idea of cascade for single-stage detectors. In this paper, we analyze and discover that inconsistency is the major factor limiting the performance. The refined anchors are associated with the feature extracted from the previous location and the classifier is confused by misaligned classification and localization. Further, we point out two main designing rules for the cascade manner: improving consistency between classification confidence and localization performance, and maintaining feature consistency between different stages. A multistage object detector named Cas-RetinaNet, is then proposed for reducing the misalignments. It consists of sequential stages trained with increasing IoU thresholds for improving the correlation, and a novel Feature Consistency Module for mitigating the feature inconsistency. Experiments show that our proposed Cas-RetinaNet achieves stable performance gains across different models and input scales. Specifically, our method improves RetinaNet from 39.1 AP to 41.1 AP on the challenging MS COCO dataset without any bells or whistles.



### Learning Depth from Monocular Videos Using Synthetic Data: A Temporally-Consistent Domain Adaptation Approach
- **Arxiv ID**: http://arxiv.org/abs/1907.06882v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06882v2)
- **Published**: 2019-07-16 08:05:26+00:00
- **Updated**: 2019-11-26 18:04:56+00:00
- **Authors**: Yipeng Mou, Mingming Gong, Huan Fu, Kayhan Batmanghelich, Kun Zhang, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Majority of state-of-the-art monocular depth estimation methods are supervised learning approaches. The success of such approaches heavily depends on the high-quality depth labels which are expensive to obtain. Some recent methods try to learn depth networks by leveraging unsupervised cues from monocular videos which are easier to acquire but less reliable. In this paper, we propose to resolve this dilemma by transferring knowledge from synthetic videos with easily obtainable ground-truth depth labels. Due to the stylish difference between synthetic and real images, we propose a temporally-consistent domain adaptation (TCDA) approach that simultaneously explores labels in the synthetic domain and temporal constraints in the videos to improve style transfer and depth prediction. Furthermore, we make use of the ground-truth optical flow and pose information in the synthetic data to learn moving mask and pose prediction networks. The learned moving masks can filter out moving regions that produces erroneous temporal constraints and the estimated poses provide better initializations for estimating temporal constraints. Experimental results demonstrate the effectiveness of our method and comparable performance against state-of-the-art.



### A General Framework for Uncertainty Estimation in Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.06890v4
- **DOI**: 10.1109/LRA.2020.2974682
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.06890v4)
- **Published**: 2019-07-16 08:46:03+00:00
- **Updated**: 2020-02-07 12:10:56+00:00
- **Authors**: Antonio Loquercio, Mattia Segù, Davide Scaramuzza
- **Comment**: Accepted for publication in the Robotics and Automation Letters 2020,
  and for presentation at the International Conference on Robotics and
  Automation (ICRA) 2020
- **Journal**: IEEE Robotics and Automation Letters 2020
- **Summary**: Neural networks predictions are unreliable when the input sample is out of the training distribution or corrupted by noise. Being able to detect such failures automatically is fundamental to integrate deep learning algorithms into robotics. Current approaches for uncertainty estimation of neural networks require changes to the network and optimization process, typically ignore prior knowledge about the data, and tend to make over-simplifying assumptions which underestimate uncertainty. To address these limitations, we propose a novel framework for uncertainty estimation. Based on Bayesian belief networks and Monte-Carlo sampling, our framework not only fully models the different sources of prediction uncertainty, but also incorporates prior data information, e.g. sensor noise. We show theoretically that this gives us the ability to capture uncertainty better than existing methods. In addition, our framework has several desirable properties: (i) it is agnostic to the network architecture and task; (ii) it does not require changes in the optimization process; (iii) it can be applied to already trained architectures. We thoroughly validate the proposed framework through extensive experiments on both computer vision and control tasks, where we outperform previous methods by up to 23% in accuracy.



### Mango Tree Net -- A fully convolutional network for semantic segmentation and individual crown detection of mango trees
- **Arxiv ID**: http://arxiv.org/abs/1907.06915v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06915v1)
- **Published**: 2019-07-16 09:41:13+00:00
- **Updated**: 2019-07-16 09:41:13+00:00
- **Authors**: Vikas Agaradahalli Gurumurthy, Ramesh Kestur, Omkar Narasipura
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents a method for semantic segmentation of mango trees in high resolution aerial imagery, and, a novel method for individual crown detection of mango trees using segmentation output. Mango Tree Net, a fully convolutional neural network (FCN), is trained using supervised learning to perform semantic segmentation of mango trees in imagery acquired using an unmanned aerial vehicle (UAV). The proposed network is retrained to separate touching/overlapping tree crowns in segmentation output. Contour based connected object detection is performed on the segmentation output from retrained network. Bounding boxes are drawn on the original images using coordinates of connected objects to achieve individual crown detection. The training dataset consists of 8,824 image patches of size 240 x 240. The approach is tested for performance on segmentation and individual crown detection tasks using test datasets containing 36 and 4 images respectively. The performance is analyzed using standard metrics precision, recall, f1-score and accuracy. Results obtained demonstrate the robustness of the proposed methods despite variations in factors such as scale, occlusion, lighting conditions and surrounding vegetation.



### Single-bit-per-weight deep convolutional neural networks without batch-normalization layers for embedded systems
- **Arxiv ID**: http://arxiv.org/abs/1907.06916v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.06916v2)
- **Published**: 2019-07-16 09:42:02+00:00
- **Updated**: 2019-07-22 13:04:27+00:00
- **Authors**: Mark D. McDonnell, Hesham Mostafa, Runchun Wang, Andre van Schaik
- **Comment**: 8 pages, published IEEE conference paper
- **Journal**: None
- **Summary**: Batch-normalization (BN) layers are thought to be an integrally important layer type in today's state-of-the-art deep convolutional neural networks for computer vision tasks such as classification and detection. However, BN layers introduce complexity and computational overheads that are highly undesirable for training and/or inference on low-power custom hardware implementations of real-time embedded vision systems such as UAVs, robots and Internet of Things (IoT) devices. They are also problematic when batch sizes need to be very small during training, and innovations such as residual connections introduced more recently than BN layers could potentially have lessened their impact. In this paper we aim to quantify the benefits BN layers offer in image classification networks, in comparison with alternative choices. In particular, we study networks that use shifted-ReLU layers instead of BN layers. We found, following experiments with wide residual networks applied to the ImageNet, CIFAR 10 and CIFAR 100 image classification datasets, that BN layers do not consistently offer a significant advantage. We found that the accuracy margin offered by BN layers depends on the data set, the network size, and the bit-depth of weights. We conclude that in situations where BN layers are undesirable due to speed, memory or complexity costs, that using shifted-ReLU layers instead should be considered; we found they can offer advantages in all these areas, and often do not impose a significant accuracy cost.



### Human Pose Estimation for Real-World Crowded Scenarios
- **Arxiv ID**: http://arxiv.org/abs/1907.06922v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06922v1)
- **Published**: 2019-07-16 09:53:27+00:00
- **Updated**: 2019-07-16 09:53:27+00:00
- **Authors**: Thomas Golda, Tobias Kalb, Arne Schumann, Jürgen Beyerer
- **Comment**: Accepted for the 16th IEEE International Conference on Advanced Video
  and Signal-based Surveillance (AVSS)
- **Journal**: None
- **Summary**: Human pose estimation has recently made significant progress with the adoption of deep convolutional neural networks. Its many applications have attracted tremendous interest in recent years. However, many practical applications require pose estimation for human crowds, which still is a rarely addressed problem. In this work, we explore methods to optimize pose estimation for human crowds, focusing on challenges introduced with dense crowds, such as occlusions, people in close proximity to each other, and partial visibility of people. In order to address these challenges, we evaluate three aspects of a pose detection approach: i) a data augmentation method to introduce robustness to occlusions, ii) the explicit detection of occluded body parts, and iii) the use of the synthetic generated datasets. The first approach to improve the accuracy in crowded scenarios is to generate occlusions at training time using person and object cutouts from the object recognition dataset COCO (Common Objects in Context). Furthermore, the synthetically generated dataset JTA (Joint Track Auto) is evaluated for the use in real-world crowd applications. In order to overcome the transfer gap of JTA originating from a low pose variety and less dense crowds, an extension dataset is created to ease the use for real-world applications. Additionally, the occlusion flags provided with JTA are utilized to train a model, which explicitly distinguishes between occluded and visible body parts in two distinct branches. The combination of the proposed additions to the baseline method help to improve the overall accuracy by 4.7% AP and thereby provide comparable results to current state-of-the-art approaches on the respective dataset.



### Deep Learning for Pneumothorax Detection and Localization in Chest Radiographs
- **Arxiv ID**: http://arxiv.org/abs/1907.07324v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.07324v1)
- **Published**: 2019-07-16 11:06:48+00:00
- **Updated**: 2019-07-16 11:06:48+00:00
- **Authors**: André Gooßen, Hrishikesh Deshpande, Tim Harder, Evan Schwab, Ivo Baltruschat, Thusitha Mabotuwana, Nathan Cross, Axel Saalbach
- **Comment**: MIDL 2019 [arXiv:1907.08612]
- **Journal**: None
- **Summary**: Pneumothorax is a critical condition that requires timely communication and immediate action. In order to prevent significant morbidity or patient death, early detection is crucial. For the task of pneumothorax detection, we study the characteristics of three different deep learning techniques: (i) convolutional neural networks, (ii) multiple-instance learning, and (iii) fully convolutional networks. We perform a five-fold cross-validation on a dataset consisting of 1003 chest X-ray images. ROC analysis yields AUCs of 0.96, 0.93, and 0.92 for the three methods, respectively. We review the classification and localization performance of these approaches as well as an ensemble of the three aforementioned techniques.



### Semi-supervised Breast Lesion Detection in Ultrasound Video Based on Temporal Coherence
- **Arxiv ID**: http://arxiv.org/abs/1907.06941v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06941v1)
- **Published**: 2019-07-16 11:16:52+00:00
- **Updated**: 2019-07-16 11:16:52+00:00
- **Authors**: Sihong Chen, Weiping Yu, Kai Ma, Xinlong Sun, Xiaona Lin, Desheng Sun, Yefeng Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Breast lesion detection in ultrasound video is critical for computer-aided diagnosis. However, detecting lesion in video is quite challenging due to the blurred lesion boundary, high similarity to soft tissue and lack of video annotations. In this paper, we propose a semi-supervised breast lesion detection method based on temporal coherence which can detect the lesion more accurately. We aggregate features extracted from the historical key frames with adaptive key-frame scheduling strategy. Our proposed method accomplishes the unlabeled videos detection task by leveraging the supervision information from a different set of labeled images. In addition, a new WarpNet is designed to replace both the traditional spatial warping and feature aggregation operation, leading to a tremendous increase in speed. Experiments on 1,060 2D ultrasound sequences demonstrate that our proposed method achieves state-of-the-art video detection result as 91.3% in mean average precision and 19 ms per frame on GPU, compared to a RetinaNet based detection method in 86.6% and 32 ms.



### Fused Detection of Retinal Biomarkers in OCT Volumes
- **Arxiv ID**: http://arxiv.org/abs/1907.06955v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.06955v1)
- **Published**: 2019-07-16 12:12:57+00:00
- **Updated**: 2019-07-16 12:12:57+00:00
- **Authors**: Thomas Kurmann, Pablo Márquez-Neila, Siqing Yu, Marion Munk, Sebastian Wolf, Raphael Sznitman
- **Comment**: None
- **Journal**: None
- **Summary**: Optical Coherence Tomography (OCT) is the primary imaging modality for detecting pathological biomarkers associated to retinal diseases such as Age-Related Macular Degeneration. In practice, clinical diagnosis and treatment strategies are closely linked to biomarkers visible in OCT volumes and the ability to identify these plays an important role in the development of ophthalmic pharmaceutical products. In this context, we present a method that automatically predicts the presence of biomarkers in OCT cross-sections by incorporating information from the entire volume. We do so by adding a bidirectional LSTM to fuse the outputs of a Convolutional Neural Network that predicts individual biomarkers. We thus avoid the need to use pixel-wise annotations to train our method, and instead provide fine-grained biomarker information regardless. On a dataset of 416 volumes, we show that our approach imposes coherence between biomarker predictions across volume slices and our predictions are superior to several existing approaches.



### A Unified Deep Framework for Joint 3D Pose Estimation and Action Recognition from a Single RGB Camera
- **Arxiv ID**: http://arxiv.org/abs/1907.06968v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06968v1)
- **Published**: 2019-07-16 12:50:42+00:00
- **Updated**: 2019-07-16 12:50:42+00:00
- **Authors**: Huy Hieu Pham, Houssam Salmane, Louahdi Khoudour, Alain Crouzil, Pablo Zegers, Sergio A Velastin
- **Comment**: None
- **Journal**: None
- **Summary**: We present a deep learning-based multitask framework for joint 3D human pose estimation and action recognition from RGB video sequences. Our approach proceeds along two stages. In the first, we run a real-time 2D pose detector to determine the precise pixel location of important keypoints of the body. A two-stream neural network is then designed and trained to map detected 2D keypoints into 3D poses. In the second, we deploy the Efficient Neural Architecture Search (ENAS) algorithm to find an optimal network architecture that is used for modeling the spatio-temporal evolution of the estimated 3D poses via an image-based intermediate representation and performing action recognition. Experiments on Human3.6M, MSR Action3D and SBU Kinect Interaction datasets verify the effectiveness of the proposed method on the targeted tasks. Moreover, we show that our method requires a low computational budget for training and inference.



### Speed estimation evaluation on the KITTI benchmark based on motion and monocular depth information
- **Arxiv ID**: http://arxiv.org/abs/1907.06989v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.06989v1)
- **Published**: 2019-07-16 13:36:25+00:00
- **Updated**: 2019-07-16 13:36:25+00:00
- **Authors**: Róbert-Adrian Rill
- **Comment**: technical report with 16 pages, 3 figures, 7 tables
- **Journal**: None
- **Summary**: In this technical report we investigate speed estimation of the ego-vehicle on the KITTI benchmark using state-of-the-art deep neural network based optical flow and single-view depth prediction methods. Using a straightforward intuitive approach and approximating a single scale factor, we evaluate several application schemes of the deep networks and formulate meaningful conclusions such as: combining depth information with optical flow improves speed estimation accuracy as opposed to using optical flow alone; the quality of the deep neural network methods influences speed estimation performance; using the depth and optical flow results from smaller crops of wide images degrades performance. With these observations in mind, we achieve a RMSE of less than 1 m/s for vehicle speed estimation using monocular images as input from recordings of the KITTI benchmark. Limitations and possible future directions are discussed as well.



### Perception of visual numerosity in humans and machines
- **Arxiv ID**: http://arxiv.org/abs/1907.06996v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1907.06996v1)
- **Published**: 2019-07-16 13:45:18+00:00
- **Updated**: 2019-07-16 13:45:18+00:00
- **Authors**: Alberto Testolin, Serena Dolfi, Mathijs Rochus, Marco Zorzi
- **Comment**: 27 pages, 7 figures
- **Journal**: None
- **Summary**: Numerosity perception is foundational to mathematical learning, but its computational bases are strongly debated. Some investigators argue that humans are endowed with a specialized system supporting numerical representation; others argue that visual numerosity is estimated using continuous magnitudes, such as density or area, which usually co-vary with number. Here we reconcile these contrasting perspectives by testing deep networks on the same numerosity comparison task that was administered to humans, using a stimulus space that allows to measure the contribution of non-numerical features. Our model accurately simulated the psychophysics of numerosity perception and the associated developmental changes: discrimination was driven by numerosity information, but non-numerical features had a significant impact, especially early during development. Representational similarity analysis further highlighted that both numerosity and continuous magnitudes were spontaneously encoded even when no task had to be carried out, demonstrating that numerosity is a major, salient property of our visual environment.



### X-Net: Brain Stroke Lesion Segmentation Based on Depthwise Separable Convolution and Long-range Dependencies
- **Arxiv ID**: http://arxiv.org/abs/1907.07000v2
- **DOI**: 10.1007/978-3-030-32248-9_28
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.07000v2)
- **Published**: 2019-07-16 13:48:41+00:00
- **Updated**: 2019-12-30 06:52:15+00:00
- **Authors**: Kehan Qi, Hao Yang, Cheng Li, Zaiyi Liu, Meiyun Wang, Qiegen Liu, Shanshan Wang
- **Comment**: MICCAI 2019
- **Journal**: None
- **Summary**: The morbidity of brain stroke increased rapidly in the past few years. To help specialists in lesion measurements and treatment planning, automatic segmentation methods are critically required for clinical practices. Recently, approaches based on deep learning and methods for contextual information extraction have served in many image segmentation tasks. However, their performances are limited due to the insufficient training of a large number of parameters, which sometimes fail in capturing long-range dependencies. To address these issues, we propose a depthwise separable convolution based X-Net that designs a nonlocal operation namely Feature Similarity Module (FSM) to capture long-range dependencies. The adopted depthwise convolution allows to reduce the network size, while the developed FSM provides a more effective, dense contextual information extraction and thus facilitates better segmentation. The effectiveness of X-Net was evaluated on an open dataset Anatomical Tracings of Lesions After Stroke (ATLAS) with superior performance achieved compared to other six state-of-the-art approaches. We make our code and models available at https://github.com/Andrewsher/X-Net.



### CLCI-Net: Cross-Level fusion and Context Inference Networks for Lesion Segmentation of Chronic Stroke
- **Arxiv ID**: http://arxiv.org/abs/1907.07008v2
- **DOI**: 10.1007/978-3-030-32248-9_30
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.07008v2)
- **Published**: 2019-07-16 13:54:50+00:00
- **Updated**: 2019-07-17 02:26:04+00:00
- **Authors**: Hao Yang, Weijian Huang, Kehan Qi, Cheng Li, Xinfeng Liu, Meiyun Wang, Hairong Zheng, Shanshan Wang
- **Comment**: None
- **Journal**: Medical Image Computing and Computer Assisted Intervention 2019:
  266-274
- **Summary**: Segmenting stroke lesions from T1-weighted MR images is of great value for large-scale stroke rehabilitation neuroimaging analyses. Nevertheless, there are great challenges with this task, such as large range of stroke lesion scales and the tissue intensity similarity. The famous encoder-decoder convolutional neural network, which although has made great achievements in medical image segmentation areas, may fail to address these challenges due to the insufficient uses of multi-scale features and context information. To address these challenges, this paper proposes a Cross-Level fusion and Context Inference Network (CLCI-Net) for the chronic stroke lesion segmentation from T1-weighted MR images. Specifically, a Cross-Level feature Fusion (CLF) strategy was developed to make full use of different scale features across different levels; Extending Atrous Spatial Pyramid Pooling (ASPP) with CLF, we have enriched multi-scale features to handle the different lesion sizes; In addition, convolutional long short-term memory (ConvLSTM) is employed to infer context information and thus capture fine structures to address the intensity similarity issue. The proposed approach was evaluated on an open-source dataset, the Anatomical Tracings of Lesions After Stroke (ATLAS) with the results showing that our network outperforms five state-of-the-art methods. We make our code and models available at https://github.com/YH0517/CLCI_Net.



### Improving Semantic Segmentation via Dilated Affinity
- **Arxiv ID**: http://arxiv.org/abs/1907.07011v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.07011v2)
- **Published**: 2019-07-16 13:59:22+00:00
- **Updated**: 2019-07-27 02:00:03+00:00
- **Authors**: Boxi Wu, Shuai Zhao, Wenqing Chu, Zheng Yang, Deng Cai
- **Comment**: 10 pages, 5 figures, under review of NIPS2019
- **Journal**: None
- **Summary**: Introducing explicit constraints on the structural predictions has been an effective way to improve the performance of semantic segmentation models. Existing methods are mainly based on insufficient hand-crafted rules that only partially capture the image structure, and some methods can also suffer from the efficiency issue. As a result, most of the state-of-the-art fully convolutional networks did not adopt these techniques. In this work, we propose a simple, fast yet effective method that exploits structural information through direct supervision with minor additional expense. To be specific, our method explicitly requires the network to predict semantic segmentation as well as dilated affinity, which is a sparse version of pair-wise pixel affinity. The capability of telling the relationships between pixels are directly built into the model and enhance the quality of segmentation in two stages. 1) Joint training with dilated affinity can provide robust feature representations and thus lead to finer segmentation results. 2) The extra output of affinity information can be further utilized to refine the original segmentation with a fast propagation process. Consistent improvements are observed on various benchmark datasets when applying our framework to the existing state-of-the-art model. Codes will be released soon.



### Data Selection for training Semantic Segmentation CNNs with cross-dataset weak supervision
- **Arxiv ID**: http://arxiv.org/abs/1907.07023v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.07023v1)
- **Published**: 2019-07-16 14:17:06+00:00
- **Updated**: 2019-07-16 14:17:06+00:00
- **Authors**: Panagiotis Meletis, Rob Romijnders, Gijs Dubbelman
- **Comment**: IEEE ITSC 2019
- **Journal**: None
- **Summary**: Training convolutional networks for semantic segmentation with strong (per-pixel) and weak (per-bounding-box) supervision requires a large amount of weakly labeled data. We propose two methods for selecting the most relevant data with weak supervision. The first method is designed for finding visually similar images without the need of labels and is based on modeling image representations with a Gaussian Mixture Model (GMM). As a byproduct of GMM modeling, we present useful insights on characterizing the data generating distribution. The second method aims at finding images with high object diversity and requires only the bounding box labels. Both methods are developed in the context of automated driving and experimentation is conducted on Cityscapes and Open Images datasets. We demonstrate performance gains by reducing the amount of employed weakly labeled images up to 100 times for Open Images and up to 20 times for Cityscapes.



### Uncertainty-aware Self-ensembling Model for Semi-supervised 3D Left Atrium Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.07034v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.07034v1)
- **Published**: 2019-07-16 14:34:11+00:00
- **Updated**: 2019-07-16 14:34:11+00:00
- **Authors**: Lequan Yu, Shujun Wang, Xiaomeng Li, Chi-Wing Fu, Pheng-Ann Heng
- **Comment**: Accepted by MICCAI2019; Code is available in
  https://github.com/yulequan/UA-MT
- **Journal**: None
- **Summary**: Training deep convolutional neural networks usually requires a large amount of labeled data. However, it is expensive and time-consuming to annotate data for medical image segmentation tasks. In this paper, we present a novel uncertainty-aware semi-supervised framework for left atrium segmentation from 3D MR images. Our framework can effectively leverage the unlabeled data by encouraging consistent predictions of the same input under different perturbations. Concretely, the framework consists of a student model and a teacher model, and the student model learns from the teacher model by minimizing a segmentation loss and a consistency loss with respect to the targets of the teacher model. We design a novel uncertainty-aware scheme to enable the student model to gradually learn from the meaningful and reliable targets by exploiting the uncertainty information. Experiments show that our method achieves high performance gains by incorporating the unlabeled data. Our method outperforms the state-of-the-art semi-supervised methods, demonstrating the potential of our framework for the challenging semi-supervised problems.



### Pedestrian Tracking by Probabilistic Data Association and Correspondence Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1907.07045v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1907.07045v1)
- **Published**: 2019-07-16 14:58:37+00:00
- **Updated**: 2019-07-16 14:58:37+00:00
- **Authors**: Borna Bićanić, Marin Oršić, Ivan Marković, Siniša Šegvić, Ivan Petrović
- **Comment**: None
- **Journal**: 22nd International Conference on Information Fusion (FUSION)
  (2019)
- **Summary**: This paper studies the interplay between kinematics (position and velocity) and appearance cues for establishing correspondences in multi-target pedestrian tracking. We investigate tracking-by-detection approaches based on a deep learning detector, joint integrated probabilistic data association (JIPDA), and appearance-based tracking of deep correspondence embeddings. We first addressed the fixed-camera setup by fine-tuning a convolutional detector for accurate pedestrian detection and combining it with kinematic-only JIPDA. The resulting submission ranked first on the 3DMOT2015 benchmark. However, in sequences with a moving camera and unknown ego-motion, we achieved the best results by replacing kinematic cues with global nearest neighbor tracking of deep correspondence embeddings. We trained the embeddings by fine-tuning features from the second block of ResNet-18 using angular loss extended by a margin term. We note that integrating deep correspondence embeddings directly in JIPDA did not bring significant improvement. It appears that geometry of deep correspondence embeddings for soft data association needs further investigation in order to obtain the best from both worlds.



### How much real data do we actually need: Analyzing object detection performance using synthetic and real data
- **Arxiv ID**: http://arxiv.org/abs/1907.07061v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.07061v1)
- **Published**: 2019-07-16 15:21:38+00:00
- **Updated**: 2019-07-16 15:21:38+00:00
- **Authors**: Farzan Erlik Nowruzi, Prince Kapoor, Dhanvin Kolhatkar, Fahed Al Hassanat, Robert Laganiere, Julien Rebut
- **Comment**: Accepted in International Conference on Machine Learning (ICML 2019)
  Workshop on AI for Autonomous Driving
- **Journal**: None
- **Summary**: In recent years, deep learning models have resulted in a huge amount of progress in various areas, including computer vision. By nature, the supervised training of deep models requires a large amount of data to be available. This ideal case is usually not tractable as the data annotation is a tremendously exhausting and costly task to perform. An alternative is to use synthetic data. In this paper, we take a comprehensive look into the effects of replacing real data with synthetic data. We further analyze the effects of having a limited amount of real data. We use multiple synthetic and real datasets along with a simulation tool to create large amounts of cheaply annotated synthetic data. We analyze the domain similarity of each of these datasets. We provide insights about designing a methodological procedure for training deep networks using these datasets.



### Anatomically-Informed Multiple Linear Assignment Problems for White Matter Bundle Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.07077v1
- **DOI**: 10.1109/ISBI.2019.8759174
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.07077v1)
- **Published**: 2019-07-16 15:42:21+00:00
- **Updated**: 2019-07-16 15:42:21+00:00
- **Authors**: Giulia Bertò, Paolo Avesani, Franco Pestilli, Daniel Bullock, Bradley Caron, Emanuele Olivetti
- **Comment**: None
- **Journal**: 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI
  2019)
- **Summary**: Segmenting white matter bundles from human tractograms is a task of interest for several applications. Current methods for bundle segmentation consider either only prior knowledge about the relative anatomical position of a bundle, or only its geometrical properties. Our aim is to improve the results of segmentation by proposing a method that takes into account information about both the underlying anatomy and the geometry of bundles at the same time. To achieve this goal, we extend a state-of-the-art example-based method based on the Linear Assignment Problem (LAP) by including prior anatomical information within the optimization process. The proposed method shows a significant improvement with respect to the original method, in particular on small bundles.



### Efficient Segmentation: Learning Downsampling Near Semantic Boundaries
- **Arxiv ID**: http://arxiv.org/abs/1907.07156v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.07156v1)
- **Published**: 2019-07-16 17:27:21+00:00
- **Updated**: 2019-07-16 17:27:21+00:00
- **Authors**: Dmitrii Marin, Zijian He, Peter Vajda, Priyam Chatterjee, Sam Tsai, Fei Yang, Yuri Boykov
- **Comment**: None
- **Journal**: None
- **Summary**: Many automated processes such as auto-piloting rely on a good semantic segmentation as a critical component. To speed up performance, it is common to downsample the input frame. However, this comes at the cost of missed small objects and reduced accuracy at semantic boundaries. To address this problem, we propose a new content-adaptive downsampling technique that learns to favor sampling locations near semantic boundaries of target classes. Cost-performance analysis shows that our method consistently outperforms the uniform sampling improving balance between accuracy and computational efficiency. Our adaptive sampling gives segmentation with better quality of boundaries and more reliable support for smaller-size objects.



### EnforceNet: Monocular Camera Localization in Large Scale Indoor Sparse LiDAR Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/1907.07160v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.07160v1)
- **Published**: 2019-07-16 17:35:53+00:00
- **Updated**: 2019-07-16 17:35:53+00:00
- **Authors**: Yu Chen, Guan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Pose estimation is a fundamental building block for robotic applications such as autonomous vehicles, UAV, and large scale augmented reality. It is also a prohibitive factor for those applications to be in mass production, since the state-of-the-art, centimeter-level pose estimation often requires long mapping procedures and expensive localization sensors, e.g. LiDAR and high precision GPS/IMU, etc. To overcome the cost barrier, we propose a neural network based solution to localize a consumer degree RGB camera within a prior sparse LiDAR map with comparable centimeter-level precision. We achieved it by introducing a novel network module, which we call resistor module, to enforce the network generalize better, predicts more accurately, and converge faster. Such results are benchmarked by several datasets we collected in the large scale indoor parking garage scenes. We plan to open both the data and the code for the community to join the effort to advance this field.



### Predicting Next-Season Designs on High Fashion Runway
- **Arxiv ID**: http://arxiv.org/abs/1907.07161v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.07161v1)
- **Published**: 2019-07-16 17:38:45+00:00
- **Updated**: 2019-07-16 17:38:45+00:00
- **Authors**: Yusan Lin, Hao Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Fashion is a large and fast-changing industry. Foreseeing the upcoming fashion trends is beneficial for fashion designers, consumers, and retailers. However, fashion trends are often perceived as unpredictable due to the enormous amount of factors involved into designers' subjectivity. In this paper, we propose a fashion trend prediction framework and design neural network models to leverage structured fashion runway show data, learn the fashion collection embedding, and further train RNN/LSTM models to capture the designers' style evolution. Our proposed framework consists of (1) a runway embedding learning model that uses fashion runway images to learn every season's collection embedding, and (2) a next-season fashion design prediction model that leverage the concept of designer style and trend to predict next-season design given designers. Through experiments on a collected dataset across 32 years of fashion shows, our framework can achieve the best performance of 78.42% AUC on average and 95% for an individual designer when predicting the next season's design.



### Explaining Classifiers with Causal Concept Effect (CaCE)
- **Arxiv ID**: http://arxiv.org/abs/1907.07165v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.07165v2)
- **Published**: 2019-07-16 17:47:43+00:00
- **Updated**: 2020-02-28 18:56:14+00:00
- **Authors**: Yash Goyal, Amir Feder, Uri Shalit, Been Kim
- **Comment**: None
- **Journal**: None
- **Summary**: How can we understand classification decisions made by deep neural networks? Many existing explainability methods rely solely on correlations and fail to account for confounding, which may result in potentially misleading explanations. To overcome this problem, we define the Causal Concept Effect (CaCE) as the causal effect of (the presence or absence of) a human-interpretable concept on a deep neural net's predictions. We show that the CaCE measure can avoid errors stemming from confounding. Estimating CaCE is difficult in situations where we cannot easily simulate the do-operator. To mitigate this problem, we use a generative model, specifically a Variational AutoEncoder (VAE), to measure VAE-CaCE. In an extensive experimental analysis, we show that the VAE-CaCE is able to estimate the true concept causal effect, compared to baselines for a number of datasets including high dimensional images.



### On the "steerability" of generative adversarial networks
- **Arxiv ID**: http://arxiv.org/abs/1907.07171v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.07171v4)
- **Published**: 2019-07-16 17:55:07+00:00
- **Updated**: 2020-02-17 01:13:18+00:00
- **Authors**: Ali Jahanian, Lucy Chai, Phillip Isola
- **Comment**: None
- **Journal**: None
- **Summary**: An open secret in contemporary machine learning is that many models work beautifully on standard benchmarks but fail to generalize outside the lab. This has been attributed to biased training data, which provide poor coverage over real world events. Generative models are no exception, but recent advances in generative adversarial networks (GANs) suggest otherwise - these models can now synthesize strikingly realistic and diverse images. Is generative modeling of photos a solved problem? We show that although current GANs can fit standard datasets very well, they still fall short of being comprehensive models of the visual manifold. In particular, we study their ability to fit simple transformations such as camera movements and color changes. We find that the models reflect the biases of the datasets on which they are trained (e.g., centered objects), but that they also exhibit some capacity for generalization: by "steering" in latent space, we can shift the distribution while still creating realistic images. We hypothesize that the degree of distributional shift is related to the breadth of the training data distribution. Thus, we conduct experiments to quantify the limits of GAN transformations and introduce techniques to mitigate the problem. Code is released on our project page: https://ali-design.github.io/gan_steerability/



### Natural Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1907.07174v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.07174v4)
- **Published**: 2019-07-16 17:56:30+00:00
- **Updated**: 2021-03-04 21:56:19+00:00
- **Authors**: Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, Dawn Song
- **Comment**: CVPR 2021; dataset and code available at
  https://github.com/hendrycks/natural-adv-examples
- **Journal**: None
- **Summary**: We introduce two challenging datasets that reliably cause machine learning model performance to substantially degrade. The datasets are collected with a simple adversarial filtration technique to create datasets with limited spurious cues. Our datasets' real-world, unmodified examples transfer to various unseen models reliably, demonstrating that computer vision models have shared weaknesses. The first dataset is called ImageNet-A and is like the ImageNet test set, but it is far more challenging for existing models. We also curate an adversarial out-of-distribution detection dataset called ImageNet-O, which is the first out-of-distribution detection dataset created for ImageNet models. On ImageNet-A a DenseNet-121 obtains around 2% accuracy, an accuracy drop of approximately 90%, and its out-of-distribution detection performance on ImageNet-O is near random chance levels. We find that existing data augmentation techniques hardly boost performance, and using other public training datasets provides improvements that are limited. However, we find that improvements to computer vision architectures provide a promising path towards robust models.



### RayTracer.jl: A Differentiable Renderer that supports Parameter Optimization for Scene Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1907.07198v3
- **DOI**: 10.5281/zenodo.1442780
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.07198v3)
- **Published**: 2019-07-16 18:01:44+00:00
- **Updated**: 2019-11-01 15:06:21+00:00
- **Authors**: Avik Pal
- **Comment**: Proceedings of the JuliaCon Conferences 2019
- **Journal**: JuliaCon Proceedings, 1 (2020), 37
- **Summary**: In this paper, we present RayTracer.jl, a renderer in Julia that is fully differentiable using source-to-source Automatic Differentiation (AD). This means that RayTracer not only renders 2D images from 3D scene parameters, but it can be used to optimize for model parameters that generate a target image in a Differentiable Programming (DP) pipeline. We interface our renderer with the deep learning library Flux for use in combination with neural networks. We demonstrate the use of this differentiable renderer in rendering tasks and in solving inverse graphics problems.



### Real-time Vision-based Depth Reconstruction with NVidia Jetson
- **Arxiv ID**: http://arxiv.org/abs/1907.07210v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1907.07210v1)
- **Published**: 2019-07-16 18:33:20+00:00
- **Updated**: 2019-07-16 18:33:20+00:00
- **Authors**: Andrey Bokovoy, Kirill Muravyev, Konstantin Yakovlev
- **Comment**: Camera-ready version as submitted to ECMR 2019
- **Journal**: None
- **Summary**: Vision-based depth reconstruction is a challenging problem extensively studied in computer vision but still lacking universal solution. Reconstructing depth from single image is particularly valuable to mobile robotics as it can be embedded to the modern vision-based simultaneous localization and mapping (vSLAM) methods providing them with the metric information needed to construct accurate maps in real scale. Typically, depth reconstruction is done nowadays via fully-convolutional neural networks (FCNNs). In this work we experiment with several FCNN architectures and introduce a few enhancements aimed at increasing both the effectiveness and the efficiency of the inference. We experimentally determine the solution that provides the best performance/accuracy tradeoff and is able to run on NVidia Jetson with the framerates exceeding 16FPS for 320 x 240 input. We also evaluate the suggested models by conducting monocular vSLAM of unknown indoor environment on NVidia Jetson TX2 in real-time. Open-source implementation of the models and the inference node for Robot Operating System (ROS) are available at https://github.com/CnnDepth/tx2_fcnn_node.



### Exact Recovery of Tensor Robust Principal Component Analysis under Linear Transforms
- **Arxiv ID**: http://arxiv.org/abs/1907.08288v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.08288v1)
- **Published**: 2019-07-16 19:05:15+00:00
- **Updated**: 2019-07-16 19:05:15+00:00
- **Authors**: Canyi Lu, Pan Zhou
- **Comment**: arXiv admin note: text overlap with arXiv:1804.03728; text overlap
  with arXiv:1311.6182 by other authors
- **Journal**: None
- **Summary**: This work studies the Tensor Robust Principal Component Analysis (TRPCA) problem, which aims to exactly recover the low-rank and sparse components from their sum. Our model is motivated by the recently proposed linear transforms based tensor-tensor product and tensor SVD. We define a new transforms depended tensor rank and the corresponding tensor nuclear norm. Then we solve the TRPCA problem by convex optimization whose objective is a weighted combination of the new tensor nuclear norm and the $\ell_1$-norm. In theory, we show that under certain incoherence conditions, the convex program exactly recovers the underlying low-rank and sparse components. It is of great interest that our new TRPCA model generalizes existing works. In particular, if the studied tensor reduces to a matrix, our TRPCA model reduces to the known matrix RPCA. Our new TRPCA which is allowed to use general linear transforms can be regarded as an extension of our former TRPCA work which uses the discrete Fourier transform. But their proof of the recovery guarantee is different. Numerical experiments verify our results and the application on image recovery demonstrates the superiority of our method.



### Learning Multimodal Fixed-Point Weights using Gradient Descent
- **Arxiv ID**: http://arxiv.org/abs/1907.07220v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.07220v1)
- **Published**: 2019-07-16 19:11:01+00:00
- **Updated**: 2019-07-16 19:11:01+00:00
- **Authors**: Lukas Enderich, Fabian Timm, Lars Rosenbaum, Wolfram Burgard
- **Comment**: presented at ESANN 2019 (European Symposium on Artificial Neural
  Networks, Computational Intelligence and Machine Learning)
- **Journal**: https://www.elen.ucl.ac.be/esann/proceedings/papers.php?ann=2019
- **Summary**: Due to their high computational complexity, deep neural networks are still limited to powerful processing units. To promote a reduced model complexity by dint of low-bit fixed-point quantization, we propose a gradient-based optimization strategy to generate a symmetric mixture of Gaussian modes (SGM) where each mode belongs to a particular quantization stage. We achieve 2-bit state-of-the-art performance and illustrate the model's ability for self-dependent weight adaptation during training.



### Scene Motion Decomposition for Learnable Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/1907.07227v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.07227v1)
- **Published**: 2019-07-16 19:38:32+00:00
- **Updated**: 2019-07-16 19:38:32+00:00
- **Authors**: Igor Slinko, Anna Vorontsova, Filipp Konokhov, Olga Barinova, Anton Konushin
- **Comment**: None
- **Journal**: CVPR 2019 Workshop
- **Summary**: Optical Flow (OF) and depth are commonly used for visual odometry since they provide sufficient information about camera ego-motion in a rigid scene. We reformulate the problem of ego-motion estimation as a problem of motion estimation of a 3D-scene with respect to a static camera. The entire scene motion can be represented as a combination of motions of its visible points. Using OF and depth we estimate a motion of each point in terms of 6DoF and represent results in the form of motion maps, each one addressing single degree of freedom. In this work we provide motion maps as inputs to a deep neural network that predicts 6DoF of scene motion. Through our evaluation on outdoor and indoor datasets we show that utilizing motion maps leads to accuracy improvement in comparison with naive stacking of depth and OF. Another contribution of our work is a novel network architecture that efficiently exploits motion maps and outperforms learnable RGB/RGB-D baselines.



### Autonomous Driving in the Lung using Deep Learning for Localization
- **Arxiv ID**: http://arxiv.org/abs/1907.08136v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1907.08136v1)
- **Published**: 2019-07-16 20:07:13+00:00
- **Updated**: 2019-07-16 20:07:13+00:00
- **Authors**: Jake Sganga, David Eng, Chauncey Graetzel, David B. Camarillo
- **Comment**: 10 pages, 11 figures. arXiv admin note: text overlap with
  arXiv:1903.10554
- **Journal**: None
- **Summary**: Lung cancer is the leading cause of cancer-related death worldwide, and early diagnosis is critical to improving patient outcomes. To diagnose cancer, a highly trained pulmonologist must navigate a flexible bronchoscope deep into the branched structure of the lung for biopsy. The biopsy fails to sample the target tissue in 26-33% of cases largely because of poor registration with the preoperative CT map. To improve intraoperative registration, we develop two deep learning approaches to localize the bronchoscope in the preoperative CT map based on the bronchoscopic video in real-time, called AirwayNet and BifurcationNet. The networks are trained entirely on simulated images derived from the patient-specific CT. When evaluated on recorded bronchoscopy videos in a phantom lung, AirwayNet outperforms other deep learning localization algorithms with an area under the precision-recall curve of 0.97. Using AirwayNet, we demonstrate autonomous driving in the phantom lung based on video feedback alone. The robot reaches four targets in the left and right lungs in 95% of the trials. On recorded videos in eight human cadaver lungs, AirwayNet achieves areas under the precision-recall curve ranging from 0.82 to 0.997.



### Style Transfer Applied to Face Liveness Detection with User-Centered Models
- **Arxiv ID**: http://arxiv.org/abs/1907.07270v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.07270v1)
- **Published**: 2019-07-16 21:40:29+00:00
- **Updated**: 2019-07-16 21:40:29+00:00
- **Authors**: Israel A. Laurensi R., Luciana T. Menon, Manoel Camillo O. Penna N., Alessandro L. Koerich, Alceu S. Britto Jr
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a face anti-spoofing user-centered model (FAS-UCM). The major difficulty, in this case, is obtaining fraudulent images from all users to train the models. To overcome this problem, the proposed method is divided in three main parts: generation of new spoof images, based on style transfer and spoof image representation models; training of a Convolutional Neural Network (CNN) for liveness detection; evaluation of the live and spoof testing images for each subject. The generalization of the CNN to perform style transfer has shown promising qualitative results. Preliminary results have shown that the proposed method is capable of distinguishing between live and spoof images on the SiW database, with an average classification error rate of 0.22.



### Relation Network for Multi-label Aerial Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1907.07274v3
- **DOI**: 10.1109/TGRS.2019.2963364
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.07274v3)
- **Published**: 2019-07-16 22:00:47+00:00
- **Updated**: 2020-02-10 03:44:21+00:00
- **Authors**: Yuansheng Hua, Lichao Mou, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-label classification plays a momentous role in perceiving intricate contents of an aerial image and triggers several related studies over the last years. However, most of them deploy few efforts in exploiting label relations, while such dependencies are crucial for making accurate predictions. Although an LSTM layer can be introduced to modeling such label dependencies in a chain propagation manner, the efficiency might be questioned when certain labels are improperly inferred. To address this, we propose a novel aerial image multi-label classification network, attention-aware label relational reasoning network. Particularly, our network consists of three elemental modules: 1) a label-wise feature parcel learning module, 2) an attentional region extraction module, and 3) a label relational inference module. To be more specific, the label-wise feature parcel learning module is designed for extracting high-level label-specific features. The attentional region extraction module aims at localizing discriminative regions in these features and yielding attentional label-specific features. The label relational inference module finally predicts label existences using label relations reasoned from outputs of the previous module. The proposed network is characterized by its capacities of extracting discriminative label-wise features in a proposal-free way and reasoning about label relations naturally and interpretably. In our experiments, we evaluate the proposed model on the UCM multi-label dataset and a newly produced dataset, AID multi-label dataset. Quantitative and qualitative results on these two datasets demonstrate the effectiveness of our model. To facilitate progress in the multi-label aerial image classification, the AID multi-label dataset will be made publicly available.



### Towards Understanding Generalization in Gradient-Based Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.07287v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.07287v1)
- **Published**: 2019-07-16 23:22:14+00:00
- **Updated**: 2019-07-16 23:22:14+00:00
- **Authors**: Simon Guiroy, Vikas Verma, Christopher Pal
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we study generalization of neural networks in gradient-based meta-learning by analyzing various properties of the objective landscapes. We experimentally demonstrate that as meta-training progresses, the meta-test solutions, obtained after adapting the meta-train solution of the model, to new tasks via few steps of gradient-based fine-tuning, become flatter, lower in loss, and further away from the meta-train solution. We also show that those meta-test solutions become flatter even as generalization starts to degrade, thus providing an experimental evidence against the correlation between generalization and flat minima in the paradigm of gradient-based meta-leaning. Furthermore, we provide empirical evidence that generalization to new tasks is correlated with the coherence between their adaptation trajectories in parameter space, measured by the average cosine similarity between task-specific trajectory directions, starting from a same meta-train solution. We also show that coherence of meta-test gradients, measured by the average inner product between the task-specific gradient vectors evaluated at meta-train solution, is also correlated with generalization. Based on these observations, we propose a novel regularizer for MAML and provide experimental evidence for its effectiveness.



