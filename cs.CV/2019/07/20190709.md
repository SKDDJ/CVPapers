# Arxiv Papers in cs.CV on 2019-07-09
### Learning by Abstraction: The Neural State Machine
- **Arxiv ID**: http://arxiv.org/abs/1907.03950v4
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.03950v4)
- **Published**: 2019-07-09 03:08:41+00:00
- **Updated**: 2019-11-25 10:02:05+00:00
- **Authors**: Drew A. Hudson, Christopher D. Manning
- **Comment**: Published as a conference paper at NeurIPS 2019 (spotlight)
- **Journal**: None
- **Summary**: We introduce the Neural State Machine, seeking to bridge the gap between the neural and symbolic views of AI and integrate their complementary strengths for the task of visual reasoning. Given an image, we first predict a probabilistic graph that represents its underlying semantics and serves as a structured world model. Then, we perform sequential reasoning over the graph, iteratively traversing its nodes to answer a given question or draw a new inference. In contrast to most neural architectures that are designed to closely interact with the raw sensory data, our model operates instead in an abstract latent space, by transforming both the visual and linguistic modalities into semantic concept-based representations, thereby achieving enhanced transparency and modularity. We evaluate our model on VQA-CP and GQA, two recent VQA datasets that involve compositionality, multi-step inference and diverse reasoning skills, achieving state-of-the-art results in both cases. We provide further experiments that illustrate the model's strong generalization capacity across multiple dimensions, including novel compositions of concepts, changes in the answer distribution, and unseen linguistic structures, demonstrating the qualities and efficacy of our approach.



### Accurate Nuclear Segmentation with Center Vector Encoding
- **Arxiv ID**: http://arxiv.org/abs/1907.03951v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.03951v2)
- **Published**: 2019-07-09 03:10:23+00:00
- **Updated**: 2019-07-10 02:05:43+00:00
- **Authors**: Jiahui Li, Zhiqiang Hu, Shuang Yang
- **Comment**: Published in The 26th international conference on Information
  Processing in Medical Imaging (IPMI)
- **Journal**: None
- **Summary**: Nuclear segmentation is important and frequently demanded for pathology image analysis, yet is also challenging due to nuclear crowdedness and possible occlusion. In this paper, we present a novel bottom-up method for nuclear segmentation. The concepts of Center Mask and Center Vector are introduced to better depict the relationship between pixels and nuclear instances. The instance differentiation process are thus largely simplified and easier to understand. Experiments demonstrate the effectiveness of Center Vector Encoding, where our method outperforms state-of-the-arts by a clear margin.



### Signet Ring Cell Detection With a Semi-supervised Learning Framework
- **Arxiv ID**: http://arxiv.org/abs/1907.03954v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.03954v1)
- **Published**: 2019-07-09 03:14:29+00:00
- **Updated**: 2019-07-09 03:14:29+00:00
- **Authors**: Jiahui Li, Shuang Yang, Xiaodi Huang, Qian Da, Xiaoqun Yang, Zhiqiang Hu, Qi Duan, Chaofu Wang, Hongsheng Li
- **Comment**: Published in The 26th international conference on Information
  Processing in Medical Imaging (IPMI)
- **Journal**: None
- **Summary**: Signet ring cell carcinoma is a type of rare adenocarcinoma with poor prognosis. Early detection leads to huge improvement of patients' survival rate. However, pathologists can only visually detect signet ring cells under the microscope. This procedure is not only laborious but also prone to omission. An automatic and accurate signet ring cell detection solution is thus important but has not been investigated before. In this paper, we take the first step to present a semi-supervised learning framework for the signet ring cell detection problem. Self-training is proposed to deal with the challenge of incomplete annotations, and cooperative-training is adapted to explore the unlabeled regions. Combining the two techniques, our semi-supervised learning framework can make better use of both labeled and unlabeled data. Experiments on large real clinical data demonstrate the effectiveness of our design. Our framework achieves accurate signet ring cell detection and can be readily applied in the clinical trails. The dataset will be released soon to facilitate the development of the area.



### Attentive CT Lesion Detection Using Deep Pyramid Inference with Multi-Scale Booster
- **Arxiv ID**: http://arxiv.org/abs/1907.03958v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.03958v1)
- **Published**: 2019-07-09 03:23:32+00:00
- **Updated**: 2019-07-09 03:23:32+00:00
- **Authors**: Qingbin Shao, Lijun Gong, Kai Ma, Hualuo Liu, Yefeng Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate lesion detection in computer tomography (CT) slices benefits pathologic organ analysis in the medical diagnosis process. More recently, it has been tackled as an object detection problem using the Convolutional Neural Networks (CNNs). Despite the achievements from off-the-shelf CNN models, the current detection accuracy is limited by the inability of CNNs on lesions at vastly different scales. In this paper, we propose a Multi-Scale Booster (MSB) with channel and spatial attention integrated into the backbone Feature Pyramid Network (FPN). In each pyramid level, the proposed MSB captures fine-grained scale variations by using Hierarchically Dilated Convolutions (HDC). Meanwhile, the proposed channel and spatial attention modules increase the network's capability of selecting relevant features response for lesion detection. Extensive experiments on the DeepLesion benchmark dataset demonstrate that the proposed method performs superiorly against state-of-the-art approaches.



### Learning from Thresholds: Fully Automated Classification of Tumor Infiltrating Lymphocytes for Multiple Cancer Types
- **Arxiv ID**: http://arxiv.org/abs/1907.03960v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.03960v1)
- **Published**: 2019-07-09 03:25:50+00:00
- **Updated**: 2019-07-09 03:25:50+00:00
- **Authors**: Shahira Abousamra, Le Hou, Rajarsi Gupta, Chao Chen, Dimitris Samaras, Tahsin Kurc, Rebecca Batiste, Tianhao Zhao, Shroyer Kenneth, Joel Saltz
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning classifiers for characterization of whole slide tissue morphology require large volumes of annotated data to learn variations across different tissue and cancer types. As is well known, manual generation of digital pathology training data is time consuming and expensive. In this paper, we propose a semi-automated method for annotating a group of similar instances at once, instead of collecting only per-instance manual annotations. This allows for a much larger training set, that reflects visual variability across multiple cancer types and thus training of a single network which can be automatically applied to each cancer type without human adjustment. We apply our method to the important task of classifying Tumor Infiltrating Lymphocytes (TILs) in H&E images. Prior approaches were trained for individual cancer types, with smaller training sets and human-in-the-loop threshold adjustment. We utilize these thresholded results as large scale "semi-automatic" annotations. Combined with existing manual annotations, our trained deep networks are able to automatically produce better TIL prediction results in 12 cancer types, compared to the human-in-the-loop approach.



### 3D Multi-Object Tracking: A Baseline and New Evaluation Metrics
- **Arxiv ID**: http://arxiv.org/abs/1907.03961v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1907.03961v5)
- **Published**: 2019-07-09 03:26:21+00:00
- **Updated**: 2020-07-22 03:22:51+00:00
- **Authors**: Xinshuo Weng, Jianren Wang, David Held, Kris Kitani
- **Comment**: Accepted at IROS 2020
- **Journal**: None
- **Summary**: 3D multi-object tracking (MOT) is an essential component for many applications such as autonomous driving and assistive robotics. Recent work on 3D MOT focuses on developing accurate systems giving less attention to practical considerations such as computational cost and system complexity. In contrast, this work proposes a simple real-time 3D MOT system. Our system first obtains 3D detections from a LiDAR point cloud. Then, a straightforward combination of a 3D Kalman filter and the Hungarian algorithm is used for state estimation and data association. Additionally, 3D MOT datasets such as KITTI evaluate MOT methods in the 2D space and standardized 3D MOT evaluation tools are missing for a fair comparison of 3D MOT methods. Therefore, we propose a new 3D MOT evaluation tool along with three new metrics to comprehensively evaluate 3D MOT methods. We show that, although our system employs a combination of classical MOT modules, we achieve state-of-the-art 3D MOT performance on two 3D MOT benchmarks (KITTI and nuScenes). Surprisingly, although our system does not use any 2D data as inputs, we achieve competitive performance on the KITTI 2D MOT leaderboard. Our proposed system runs at a rate of $207.4$ FPS on the KITTI dataset, achieving the fastest speed among all modern MOT systems. To encourage standardized 3D MOT evaluation, our system and evaluation code are made publicly available at https://github.com/xinshuoweng/AB3DMOT.



### Sparse-to-Dense Hypercolumn Matching for Long-Term Visual Localization
- **Arxiv ID**: http://arxiv.org/abs/1907.03965v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.03965v2)
- **Published**: 2019-07-09 03:32:23+00:00
- **Updated**: 2019-08-21 03:44:33+00:00
- **Authors**: Hugo Germain, Guillaume Bourmaud, Vincent Lepetit
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel approach to feature point matching, suitable for robust and accurate outdoor visual localization in long-term scenarios. Given a query image, we first match it against a database of registered reference images, using recent retrieval techniques. This gives us a first estimate of the camera pose. To refine this estimate, like previous approaches, we match 2D points across the query image and the retrieved reference image. This step, however, is prone to fail as it is still very difficult to detect and match sparse feature points across images captured in potentially very different conditions. Our key contribution is to show that we need to extract sparse feature points only in the retrieved reference image: We then search for the corresponding 2D locations in the query image exhaustively. This search can be performed efficiently using convolutional operations, and robustly by using hypercolumn descriptors, i.e. image features computed for retrieval. We refer to this method as Sparse-to-Dense Hypercolumn Matching. Because we know the 3D locations of the sparse feature points in the reference images thanks to an offline reconstruction stage, it is then possible to accurately estimate the camera pose from these matches. Our experiments show that this method allows us to outperform the state-of-the-art on several challenging outdoor datasets.



### On the Exact Recovery Conditions of 3D Human Motion from 2D Landmark Motion with Sparse Articulated Motion
- **Arxiv ID**: http://arxiv.org/abs/1907.03967v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1907.03967v1)
- **Published**: 2019-07-09 03:34:20+00:00
- **Updated**: 2019-07-09 03:34:20+00:00
- **Authors**: Abed Malti
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the problem of exact recovery condition in retrieving 3D human motion from 2D landmark motion. We use a skeletal kinematic model to represent the 3D human motion as a vector of angular articulation motion. We address this problem based on the observation that at high tracking rate, regardless of the global rigid motion, only few angular articulations have non-zero motion. We propose a first ideal formulation with $\ell_0$-norm to minimize the cardinal of non-zero angular articulation motion given an equality constraint on the time-differentiation of the reprojection error. The second relaxed formulation relies on an $\ell_1$-norm to minimize the sum of absolute values of the angular articulation motion. This formulation has the advantage of being able to provide 3D motion even in the under-determined case when twice the number of 2D landmarks is smaller than the number of angular articulations. We define a specific property which is the Projective Kinematic Space Property (PKSP) that takes into account the reprojection constraint and the kinematic model. We prove that for the relaxed formulation we are able to recover the exact 3D human motion from 2D landmarks if and only if the PKSP property is verified. We further demonstrate that solving the relaxed formulation provides the same ground-truth solution as the ideal formulation if and only if the PKSP condition is filled. Results with simulated sparse skeletal angular motion show the ability of the proposed method to recover exact location of angular motion. We provide results on publicly available real data (HUMAN3.6M, PANOPTIC and MPI-I3DHP).



### A Targeted Acceleration and Compression Framework for Low bit Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.05271v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.05271v1)
- **Published**: 2019-07-09 04:09:55+00:00
- **Updated**: 2019-07-09 04:09:55+00:00
- **Authors**: Biao Qian, Yang Wang
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: 1 bit deep neural networks (DNNs), of which both the activations and weights are binarized , are attracting more and more attention due to their high computational efficiency and low memory requirement . However, the drawback of large accuracy dropping also restrict s its application. In this paper, we propose a novel Targeted Acceleration and Compression (TAC) framework to improve the performance of 1 bit deep neural networks W e consider that the acceleration and compression effects of binarizing fully connected layer s are not sufficient to compensate for the accuracy loss caused by it In the proposed framework, t he convolutional and fully connected layer are separated and optimized i ndividually . F or the convolutional layer s , both the activations and weights are binarized. For the fully connected layer s, the binarization operation is re placed by network pruning and low bit quantization. The proposed framework is implemented on the CIFAR 10, CIFAR 100 and ImageNet ( ILSVRC 12 ) datasets , and experimental results show that the proposed TAC can significantly improve the accuracy of 1 bit deep neural networks and outperforms the state of the art by more than 6 percentage points .



### UnsuperPoint: End-to-end Unsupervised Interest Point Detector and Descriptor
- **Arxiv ID**: http://arxiv.org/abs/1907.04011v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.04011v1)
- **Published**: 2019-07-09 06:50:13+00:00
- **Updated**: 2019-07-09 06:50:13+00:00
- **Authors**: Peter Hviid Christiansen, Mikkel Fly Kragh, Yury Brodskiy, Henrik Karstoft
- **Comment**: None
- **Journal**: None
- **Summary**: It is hard to create consistent ground truth data for interest points in natural images, since interest points are hard to define clearly and consistently for a human annotator. This makes interest point detectors non-trivial to build. In this work, we introduce an unsupervised deep learning-based interest point detector and descriptor. Using a self-supervised approach, we utilize a siamese network and a novel loss function that enables interest point scores and positions to be learned automatically. The resulting interest point detector and descriptor is UnsuperPoint. We use regression of point positions to 1) make UnsuperPoint end-to-end trainable and 2) to incorporate non-maximum suppression in the model. Unlike most trainable detectors, it requires no generation of pseudo ground truth points, no structure-from-motion-generated representations and the model is learned from only one round of training. Furthermore, we introduce a novel loss function to regularize network predictions to be uniformly distributed. UnsuperPoint runs in real-time with 323 frames per second (fps) at a resolution of $224\times320$ and 90 fps at $480\times640$. It is comparable or better than state-of-the-art performance when measured for speed, repeatability, localization, matching score and homography estimation on the HPatch dataset.



### BADAM: A Public Dataset for Baseline Detection in Arabic-script Manuscripts
- **Arxiv ID**: http://arxiv.org/abs/1907.04041v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.04041v1)
- **Published**: 2019-07-09 08:30:56+00:00
- **Updated**: 2019-07-09 08:30:56+00:00
- **Authors**: Benjamin Kiessling, Daniel Stökl Ben Ezra, Matthew Thomas Miller
- **Comment**: None
- **Journal**: None
- **Summary**: The application of handwritten text recognition to historical works is highly dependant on accurate text line retrieval. A number of systems utilizing a robust baseline detection paradigm have emerged recently but the advancement of layout analysis methods for challenging scripts is held back by the lack of well-established datasets including works in non-Latin scripts. We present a dataset of 400 annotated document images from different domains and time periods. A short elaboration on the particular challenges posed by handwriting in Arabic script for layout analysis and subsequent processing steps is given. Lastly, we propose a method based on a fully convolutional encoder-decoder network to extract arbitrarily shaped text line images from manuscripts.



### Deep Pixel-wise Binary Supervision for Face Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/1907.04047v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1907.04047v1)
- **Published**: 2019-07-09 08:45:33+00:00
- **Updated**: 2019-07-09 08:45:33+00:00
- **Authors**: Anjith George, Sebastien Marcel
- **Comment**: 8 pages, 5 figures, To appear in : International Conference on
  Biometrics, ICB 2019
- **Journal**: None
- **Summary**: Face recognition has evolved as a prominent biometric authentication modality. However, vulnerability to presentation attacks curtails its reliable deployment. Automatic detection of presentation attacks is essential for secure use of face recognition technology in unattended scenarios. In this work, we introduce a Convolutional Neural Network (CNN) based framework for presentation attack detection, with deep pixel-wise supervision. The framework uses only frame level information making it suitable for deployment in smart devices with minimal computational and time overhead. We demonstrate the effectiveness of the proposed approach in public datasets for both intra as well as cross-dataset experiments. The proposed approach achieves an HTER of 0% in Replay Mobile dataset and an ACER of 0.42% in Protocol-1 of OULU dataset outperforming state of the art methods.



### Domain Adaptation in Multi-Channel Autoencoder based Features for Robust Face Anti-Spoofing
- **Arxiv ID**: http://arxiv.org/abs/1907.04048v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.04048v1)
- **Published**: 2019-07-09 08:53:32+00:00
- **Updated**: 2019-07-09 08:53:32+00:00
- **Authors**: Olegs Nikisins, Anjith George, Sebastien Marcel
- **Comment**: 8 pages, 7 figures, To appear in International Conference on
  Biometrics, ICB 2019
- **Journal**: None
- **Summary**: While the performance of face recognition systems has improved significantly in the last decade, they are proved to be highly vulnerable to presentation attacks (spoofing). Most of the research in the field of face presentation attack detection (PAD), was focused on boosting the performance of the systems within a single database. Face PAD datasets are usually captured with RGB cameras, and have very limited number of both bona-fide samples and presentation attack instruments. Training face PAD systems on such data leads to poor performance, even in the closed-set scenario, especially when sophisticated attacks are involved.   We explore two paths to boost the performance of the face PAD system against challenging attacks. First, by using multi-channel (RGB, Depth and NIR) data, which is still easily accessible in a number of mass production devices. Second, we develop a novel Autoencoders + MLP based face PAD algorithm. Moreover, instead of collecting more data for training of the proposed deep architecture, the domain adaptation technique is proposed, transferring the knowledge of facial appearance from RGB to multi-channel domain. We also demonstrate, that learning the features of individual facial regions, is more discriminative than the features learned from an entire face. The proposed system is tested on a very recent publicly available multi-channel PAD database with a wide variety of presentation attacks.



### Improving Deep Lesion Detection Using 3D Contextual and Spatial Attention
- **Arxiv ID**: http://arxiv.org/abs/1907.04052v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.04052v1)
- **Published**: 2019-07-09 09:19:51+00:00
- **Updated**: 2019-07-09 09:19:51+00:00
- **Authors**: Qingyi Tao, Zongyuan Ge, Jianfei Cai, Jianxiong Yin, Simon See
- **Comment**: Accepted by MICCAI 2019
- **Journal**: None
- **Summary**: Lesion detection from computed tomography (CT) scans is challenging compared to natural object detection because of two major reasons: small lesion size and small inter-class variation. Firstly, the lesions usually only occupy a small region in the CT image. The feature of such small region may not be able to provide sufficient information due to its limited spatial feature resolution. Secondly, in CT scans, the lesions are often indistinguishable from the background since the lesion and non-lesion areas may have very similar appearances. To tackle both problems, we need to enrich the feature representation and improve the feature discriminativeness. Therefore, we introduce a dual-attention mechanism to the 3D contextual lesion detection framework, including the cross-slice contextual attention to selectively aggregate the information from different slices through a soft re-sampling process. Moreover, we propose intra-slice spatial attention to focus the feature learning in the most prominent regions. Our method can be easily trained end-to-end without adding heavy overhead on the base detection network. We use DeepLesion dataset and train a universal lesion detector to detect all kinds of lesions such as liver tumors, lung nodules, and so on. The results show that our model can significantly boost the results of the baseline lesion detector (with 3D contextual information) but using much fewer slices.



### Lidar-based Object Classification with Explicit Occlusion Modeling
- **Arxiv ID**: http://arxiv.org/abs/1907.04057v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.04057v2)
- **Published**: 2019-07-09 09:46:27+00:00
- **Updated**: 2019-07-10 00:56:47+00:00
- **Authors**: Xiaoxiang Zhang, Hao Fu, Bin Dai
- **Comment**: None
- **Journal**: None
- **Summary**: LIDAR is one of the most important sensors for Unmanned Ground Vehicles (UGV). Object detection and classification based on lidar point cloud is a key technology for UGV. In object detection and classification, the mutual occlusion between neighboring objects is an important factor affecting the accuracy. In this paper, we consider occlusion as an intrinsic property of the point cloud data. We propose a novel approach that explicitly model the occlusion. The occlusion property is then taken into account in the subsequent classification step. We perform experiments on the KITTI dataset. Experimental results indicate that by utilizing the occlusion property that we modeled, the classifier obtains much better performance.



### Depth from Small Motion using Rank-1 Initialization
- **Arxiv ID**: http://arxiv.org/abs/1907.04058v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.04058v1)
- **Published**: 2019-07-09 09:50:04+00:00
- **Updated**: 2019-07-09 09:50:04+00:00
- **Authors**: Peter O. Fasogbon
- **Comment**: 8 pages, 6 figures
- **Journal**: 14th International Conference on Computer Vision Theory and
  Applications, February 2019
- **Summary**: Depth from Small Motion (DfSM) (Ha et al., 2016) is particularly interesting for commercial handheld devices because it allows the possibility to get depth information with minimal user effort and cooperation. Due to speed and memory issue on these devices, the self calibration optimization of the method using Bundle Adjustment (BA) need as little as 10-15 images. Therefore, the optimization tends to take many iterations to converge or may not converge at all in some cases. This work propose a robust initialization for the bundle adjustment using the rank-1 factorization method (Tomasi and Kanade, 1992), (Aguiar and Moura, 1999a). We create a constraint matrix that is rank-1 in a noiseless situation, then use SVD to compute the inverse depth values and the camera motion. We only need about quarter fraction of the bundle adjustment iteration to converge. We also propose grided feature extraction technique so that only important and small features are tracked all over the image frames. This also ensure speedup in the full execution time on the mobile device. For the experiments, we have documented the execution time with the proposed Rank-1 initialization on two mobile device platforms using optimized accelerations with CPU-GPU co-processing. The combination of Rank 1-BA generates more robust depth-map and is significantly faster than using BA alone.



### A Light weight and Hybrid Deep Learning Model based Online Signature Verification
- **Arxiv ID**: http://arxiv.org/abs/1907.04061v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.04061v1)
- **Published**: 2019-07-09 09:55:38+00:00
- **Updated**: 2019-07-09 09:55:38+00:00
- **Authors**: Chandra Sekhar V., Anoushka Doctor, Prerana Mukherjee, Viswanath Pulabaigiri
- **Comment**: accepted in ICDAR-WML: The 2nd International Workshop on Machine
  Learning 2019
- **Journal**: None
- **Summary**: The augmented usage of deep learning-based models for various AI related problems are as a result of modern architectures of deeper length and the availability of voluminous interpreted datasets. The models based on these architectures require huge training and storage cost, which makes them inefficient to use in critical applications like online signature verification (OSV) and to deploy in resource constraint devices. As a solution, in this work, our contribution is two-fold. 1) An efficient dimensionality reduction technique, to reduce the number of features to be considered and 2) a state-of-the-art model CNN-LSTM based hybrid architecture for online signature verification. Thorough experiments on the publicly available datasets MCYT, SUSIG, SVC confirms that the proposed model achieves better accuracy even with as low as one training sample. The proposed models yield state-of-the-art performance in various categories of all the three datasets.



### Deep Probabilistic Modeling of Glioma Growth
- **Arxiv ID**: http://arxiv.org/abs/1907.04064v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.04064v1)
- **Published**: 2019-07-09 10:00:33+00:00
- **Updated**: 2019-07-09 10:00:33+00:00
- **Authors**: Jens Petersen, Paul F. Jäger, Fabian Isensee, Simon A. A. Kohl, Ulf Neuberger, Wolfgang Wick, Jürgen Debus, Sabine Heiland, Martin Bendszus, Philipp Kickingereder, Klaus H. Maier-Hein
- **Comment**: MICCAI 2019
- **Journal**: None
- **Summary**: Existing approaches to modeling the dynamics of brain tumor growth, specifically glioma, employ biologically inspired models of cell diffusion, using image data to estimate the associated parameters. In this work, we propose an alternative approach based on recent advances in probabilistic segmentation and representation learning that implicitly learns growth dynamics directly from data without an underlying explicit model. We present evidence that our approach is able to learn a distribution of plausible future tumor appearances conditioned on past observations of the same tumor.



### RinQ Fingerprinting: Recurrence-informed Quantile Networks for Magnetic Resonance Fingerprinting
- **Arxiv ID**: http://arxiv.org/abs/1907.05277v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.05277v2)
- **Published**: 2019-07-09 10:29:55+00:00
- **Updated**: 2019-07-21 13:06:51+00:00
- **Authors**: Elisabeth Hoppe, Florian Thamm, Gregor Körzdörfer, Christopher Syben, Franziska Schirrmacher, Mathias Nittka, Josef Pfeuffer, Heiko Meyer, Andreas Maier
- **Comment**: Accepted for MICCAI 2019
- **Journal**: None
- **Summary**: Recently, Magnetic Resonance Fingerprinting (MRF) was proposed as a quantitative imaging technique for the simultaneous acquisition of tissue parameters such as relaxation times $T_1$ and $T_2$. Although the acquisition is highly accelerated, the state-of-the-art reconstruction suffers from long computation times: Template matching methods are used to find the most similar signal to the measured one by comparing it to pre-simulated signals of possible parameter combinations in a discretized dictionary. Deep learning approaches can overcome this limitation, by providing the direct mapping from the measured signal to the underlying parameters by one forward pass through a network. In this work, we propose a Recurrent Neural Network (RNN) architecture in combination with a novel quantile layer. RNNs are well suited for the processing of time-dependent signals and the quantile layer helps to overcome the noisy outliers by considering the spatial neighbors of the signal. We evaluate our approach using in-vivo data from multiple brain slices and several volunteers, running various experiments. We show that the RNN approach with small patches of complex-valued input signals in combination with a quantile layer outperforms other architectures, e.g. previously proposed CNNs for the MRF reconstruction reducing the error in $T_1$ and $T_2$ by more than 80%.



### Template-Based Posit Multiplication for Training and Inferring in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.04091v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.04091v1)
- **Published**: 2019-07-09 11:36:19+00:00
- **Updated**: 2019-07-09 11:36:19+00:00
- **Authors**: Raúl Murillo Montero, Alberto A. Del Barrio, Guillermo Botella
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: The posit number system is arguably the most promising and discussed topic in Arithmetic nowadays. The recent breakthroughs claimed by the format proposed by John L. Gustafson have put posits in the spotlight. In this work, we first describe an algorithm for multiplying two posit numbers, even when the number of exponent bits is zero. This configuration, scarcely tackled in literature, is particularly interesting because it allows the deployment of a fast sigmoid function. The proposed multiplication algorithm is then integrated as a template into the well-known FloPoCo framework. Synthesis results are shown to compare with the floating point multiplication offered by FloPoCo as well. Second, the performance of posits is studied in the scenario of Neural Networks in both training and inference stages. To the best of our knowledge, this is the first time that training is done with posit format, achieving promising results for a binary classification problem even with reduced posit configurations. In the inference stage, 8-bit posits are as good as floating point when dealing with the MNIST dataset, but lose some accuracy with CIFAR-10.



### Efficient Pose Selection for Interactive Camera Calibration
- **Arxiv ID**: http://arxiv.org/abs/1907.04096v1
- **DOI**: 10.1109/ISMAR.2018.00026
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.04096v1)
- **Published**: 2019-07-09 11:44:40+00:00
- **Updated**: 2019-07-09 11:44:40+00:00
- **Authors**: Pavel Rojtberg, Arjan Kuijper
- **Comment**: None
- **Journal**: None
- **Summary**: The choice of poses for camera calibration with planar patterns is only rarely considered - yet the calibration precision heavily depends on it. This work presents a pose selection method that finds a compact and robust set of calibration poses and is suitable for interactive calibration. Consequently, singular poses that would lead to an unreliable solution are avoided explicitly, while poses reducing the uncertainty of the calibration are favoured. For this, we use uncertainty propagation.   Our method takes advantage of a self-identifying calibration pattern to track the camera pose in real-time. This allows to iteratively guide the user to the target poses, until the desired quality level is reached. Therefore, only a sparse set of key-frames is needed for calibration.   The method is evaluated on separate training and testing sets, as well as on synthetic data. Our approach performs better than comparable solutions while requiring 30% less calibration frames.



### calibDB: enabling web based computer vision through on-the-fly camera calibration
- **Arxiv ID**: http://arxiv.org/abs/1907.04100v2
- **DOI**: 10.1145/3329714.3338132
- **Categories**: **cs.CV**, cs.NI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.04100v2)
- **Published**: 2019-07-09 11:51:44+00:00
- **Updated**: 2019-08-15 12:30:15+00:00
- **Authors**: Pavel Rojtberg, Felix Gorschlüter
- **Comment**: None
- **Journal**: None
- **Summary**: For many computer vision applications, the availability of camera calibration data is crucial as overall quality heavily depends on it. While calibration data is available on some devices through Augmented Reality (AR) frameworks like ARCore and ARKit, for most cameras this information is not available. Therefore, we propose a web based calibration service that not only aggregates calibration data, but also allows calibrating new cameras on-the-fly. We build upon a novel camera calibration framework that enables even novice users to perform a precise camera calibration in about 2 minutes. This allows general deployment of computer vision algorithms on the web, which was previously not possible due to lack of calibration data.



### Quantifying Confounding Bias in Neuroimaging Datasets with Causal Inference
- **Arxiv ID**: http://arxiv.org/abs/1907.04102v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.04102v1)
- **Published**: 2019-07-09 11:57:22+00:00
- **Updated**: 2019-07-09 11:57:22+00:00
- **Authors**: Christian Wachinger, Benjamin Gutierrez Becker, Anna Rieckmann, Sebastian Pölsterl
- **Comment**: MICCAI 2019
- **Journal**: None
- **Summary**: Neuroimaging datasets keep growing in size to address increasingly complex medical questions. However, even the largest datasets today alone are too small for training complex machine learning models. A potential solution is to increase sample size by pooling scans from several datasets. In this work, we combine 12,207 MRI scans from 15 studies and show that simple pooling is often ill-advised due to introducing various types of biases in the training data. First, we systematically define these biases. Second, we detect bias by experimentally showing that scans can be correctly assigned to their respective dataset with 73.3% accuracy. Finally, we propose to tell causal from confounding factors by quantifying the extent of confounding and causality in a single dataset using causal inference. We achieve this by finding the simplest graphical model in terms of Kolmogorov complexity. As Kolmogorov complexity is not directly computable, we employ the minimum description length to approximate it. We empirically show that our approach is able to estimate plausible causal relationships from real neuroimaging data.



### 3D pavement surface reconstruction using an RGB-D sensor
- **Arxiv ID**: http://arxiv.org/abs/1907.04124v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.04124v2)
- **Published**: 2019-07-09 13:02:27+00:00
- **Updated**: 2019-07-12 02:59:13+00:00
- **Authors**: Ahmadreza Mahmoudzadeh, Sayna Firoozi Yeganeh, Amir Golroo
- **Comment**: 5 pages, 7 Figures
- **Journal**: None
- **Summary**: A core procedure of pavement management systems is data collection. The modern technologies which are used for this purpose, such as point-based lasers and laser scanners, are too expensive to purchase, operate, and maintain. Thus, it is rarely feasible for city officials in developing countries to conduct data collection using these devices. This paper aims to introduce a cost-effective technology which can be used for pavement distress data collection and 3D pavement surface reconstruction. The applied technology in this research is the Kinect sensor which is not only cost-effective but also sufficiently precise. The Kinect sensor can register both depth and color images simultaneously. A cart is designed to mount an array of Kinect sensors. The cameras are calibrated and the slopes of collected surfaces are corrected via the Singular Value Decomposition (SVD) algorithm. Then, a procedure is proposed for stitching the RGB-D (Red Green Blue Depth) images using SURF (Speeded-up Robust Features) and MSAC (M-estimator SAmple Consensus) algorithms in order to create a 3D-structure of the pavement surface. Finally, transverse profiles are extracted and some field experiments are conducted to evaluate the reliability of the proposed approach for detecting pavement surface defects.



### A Deep Neural Network for Finger Counting and Numerosity Estimation
- **Arxiv ID**: http://arxiv.org/abs/1907.05270v2
- **DOI**: 10.1109/SSCI44817.2019.9002694
- **Categories**: **cs.CV**, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.05270v2)
- **Published**: 2019-07-09 13:10:28+00:00
- **Updated**: 2020-01-14 17:33:53+00:00
- **Authors**: Leszek Pecyna, Angelo Cangelosi, Alessandro Di Nuovo
- **Comment**: 8 pages, accepted and presented on a conference. In Proceedings of
  the 2019 IEEE Symposium Series on Computational Intelligence (SSCI)
- **Journal**: 2019 IEEE Symposium Series on Computational Intelligence
- **Summary**: In this paper, we present neuro-robotics models with a deep artificial neural network capable of generating finger counting positions and number estimation. We first train the model in an unsupervised manner where each layer is treated as a Restricted Boltzmann Machine or an autoencoder. Such a model is further trained in a supervised way. This type of pre-training is tested on our baseline model and two methods of pre-training are compared. The network is extended to produce finger counting positions. The performance in number estimation of such an extended model is evaluated. We test the hypothesis if the subitizing process can be obtained by one single model used also for estimation of higher numerosities. The results confirm the importance of unsupervised training in our enumeration task and show some similarities to human behaviour in the case of subitizing.



### Nonnegative Matrix Factorization with Local Similarity Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.04150v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.04150v1)
- **Published**: 2019-07-09 13:25:50+00:00
- **Updated**: 2019-07-09 13:25:50+00:00
- **Authors**: Chong Peng, Zhao Kang, Chenglizhao Chen, Qiang Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Existing nonnegative matrix factorization methods focus on learning global structure of the data to construct basis and coefficient matrices, which ignores the local structure that commonly exists among data. In this paper, we propose a new type of nonnegative matrix factorization method, which learns local similarity and clustering in a mutually enhancing way. The learned new representation is more representative in that it better reveals inherent geometric property of the data. Nonlinear expansion is given and efficient multiplicative updates are developed with theoretical convergence guarantees. Extensive experimental results have confirmed the effectiveness of the proposed model.



### Learning in Competitive Network with Haeusslers Equation adapted using FIREFLY algorithm
- **Arxiv ID**: http://arxiv.org/abs/1907.04160v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.04160v1)
- **Published**: 2019-07-09 13:40:14+00:00
- **Updated**: 2019-07-09 13:40:14+00:00
- **Authors**: N. Joshi
- **Comment**: None
- **Journal**: None
- **Summary**: Many of the competitive neural network consists of spatially arranged neurons. The weigh matrix that connects cells represents local excitation and long-range inhibition. They are known as soft-winner-take-all networks and shown to exhibit desirable information-processing. The local excitatory connections are many times predefined hand-wired based depending on spatial arrangement which is chosen using the previous knowledge of data. Here we present learning in recurrent network through Haeusslers equation and modified wiring scheme based on biologically based Firefly algorithm. Following results show learning in such network from input patterns without hand-wiring with fixed topology.



### Influence of Pointing on Learning to Count: A Neuro-Robotics Model
- **Arxiv ID**: http://arxiv.org/abs/1907.05269v1
- **DOI**: 10.1109/SSCI.2018.8628811
- **Categories**: **cs.CV**, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.05269v1)
- **Published**: 2019-07-09 13:59:36+00:00
- **Updated**: 2019-07-09 13:59:36+00:00
- **Authors**: Leszek Pecyna, Angelo Cangelosi
- **Comment**: 8 pages, 5 figures. In Proceedings of the 2018 IEEE Symposium Series
  on Computational Intelligence (SSCI) (pp. 358-365). IEEE
- **Journal**: None
- **Summary**: In this paper a neuro-robotics model capable of counting using gestures is introduced. The contribution of gestures to learning to count is tested with various model and training conditions. Two studies were presented in this article. In the first, we combine different modalities of the robot's neural network, in the second, a novel training procedure for it is proposed. The model is trained with pointing data from an iCub robot simulator. The behaviour of the model is in line with that of human children in terms of performance change depending on gesture production.



### Adaptive Exploration for Unsupervised Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1907.04194v2
- **DOI**: 10.1145/3369393
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.04194v2)
- **Published**: 2019-07-09 14:36:08+00:00
- **Updated**: 2020-04-10 05:25:45+00:00
- **Authors**: Yuhang Ding, Hehe Fan, Mingliang Xu, Yi Yang
- **Comment**: ACM Transactions on Multimedia Computing, Communications and
  Application (TOMCCAP)
- **Journal**: None
- **Summary**: Due to domain bias, directly deploying a deep person re-identification (re-ID) model trained on one dataset often achieves considerably poor accuracy on another dataset. In this paper, we propose an Adaptive Exploration (AE) method to address the domain-shift problem for re-ID in an unsupervised manner. Specifically, in the target domain, the re-ID model is inducted to 1) maximize distances between all person images and 2) minimize distances between similar person images. In the first case, by treating each person image as an individual class, a non-parametric classifier with a feature memory is exploited to encourage person images to move far away from each other. In the second case, according to a similarity threshold, our method adaptively selects neighborhoods for each person image in the feature space. By treating these similar person images as the same class, the non-parametric classifier forces them to stay closer. However, a problem of the adaptive selection is that, when an image has too many neighborhoods, it is more likely to attract other images as its neighborhoods. As a result, a minority of images may select a large number of neighborhoods while a majority of images have only a few neighborhoods. To address this issue, we additionally integrate a balance strategy into the adaptive selection. We evaluate our methods with two protocols. The first one is called "target-only re-ID", in which only the unlabeled target data is used for training. The second one is called "domain adaptive re-ID", in which both the source data and the target data are used during training. Experimental results on large-scale re-ID datasets demonstrate the effectiveness of our method. Our code has been released at https://github.com/dyh127/Adaptive-Exploration-for-Unsupervised-Person-Re-Identification.



### Gated Multiple Feedback Network for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1907.04253v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.04253v2)
- **Published**: 2019-07-09 15:35:04+00:00
- **Updated**: 2019-07-10 10:58:34+00:00
- **Authors**: Qilei Li, Zhen Li, Lu Lu, Gwanggil Jeon, Kai Liu, Xiaomin Yang
- **Comment**: Accepted to BMVC2019
- **Journal**: None
- **Summary**: The rapid development of deep learning (DL) has driven single image super-resolution (SR) into a new era. However, in most existing DL based image SR networks, the information flows are solely feedforward, and the high-level features cannot be fully explored. In this paper, we propose the gated multiple feedback network (GMFN) for accurate image SR, in which the representation of low-level features are efficiently enriched by rerouting multiple high-level features. We cascade multiple residual dense blocks (RDBs) and recurrently unfolds them across time. The multiple feedback connections between two adjacent time steps in the proposed GMFN exploits multiple high-level features captured under large receptive fields to refine the low-level features lacking enough contextual information. The elaborately designed gated feedback module (GFM) efficiently selects and further enhances useful information from multiple rerouted high-level features, and then refine the low-level features with the enhanced high-level information. Extensive experiments demonstrate the superiority of our proposed GMFN against state-of-the-art SR methods in terms of both quantitative metrics and visual quality. Code is available at https://github.com/liqilei/GMFN.



### Image based Eye Gaze Tracking and its Applications
- **Arxiv ID**: http://arxiv.org/abs/1907.04325v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.04325v1)
- **Published**: 2019-07-09 15:54:49+00:00
- **Updated**: 2019-07-09 15:54:49+00:00
- **Authors**: Anjith George
- **Comment**: 177 pages, PhD Thesis
- **Journal**: None
- **Summary**: Eye movements play a vital role in perceiving the world. Eye gaze can give a direct indication of the users point of attention, which can be useful in improving human-computer interaction. Gaze estimation in a non-intrusive manner can make human-computer interaction more natural. Eye tracking can be used for several applications such as fatigue detection, biometric authentication, disease diagnosis, activity recognition, alertness level estimation, gaze-contingent display, human-computer interaction, etc. Even though eye-tracking technology has been around for many decades, it has not found much use in consumer applications. The main reasons are the high cost of eye tracking hardware and lack of consumer level applications. In this work, we attempt to address these two issues. In the first part of this work, image-based algorithms are developed for gaze tracking which includes a new two-stage iris center localization algorithm. We have developed a new algorithm which works in challenging conditions such as motion blur, glint, and varying illumination levels. A person independent gaze direction classification framework using a convolutional neural network is also developed which eliminates the requirement of user-specific calibration.   In the second part of this work, we have developed two applications which can benefit from eye tracking data. A new framework for biometric identification based on eye movement parameters is developed. A framework for activity recognition, using gaze data from a head-mounted eye tracker is also developed. The information from gaze data, ego-motion, and visual features are integrated to classify the activities.



### Deep Learning for Spacecraft Pose Estimation from Photorealistic Rendering
- **Arxiv ID**: http://arxiv.org/abs/1907.04298v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1907.04298v2)
- **Published**: 2019-07-09 17:32:38+00:00
- **Updated**: 2019-08-29 16:03:11+00:00
- **Authors**: Pedro F. Proenca, Yang Gao
- **Comment**: * Adding more related work and references
- **Journal**: None
- **Summary**: On-orbit proximity operations in space rendezvous, docking and debris removal require precise and robust 6D pose estimation under a wide range of lighting conditions and against highly textured background, i.e., the Earth. This paper investigates leveraging deep learning and photorealistic rendering for monocular pose estimation of known uncooperative spacecrafts. We first present a simulator built on Unreal Engine 4, named URSO, to generate labeled images of spacecrafts orbiting the Earth, which can be used to train and evaluate neural networks. Secondly, we propose a deep learning framework for pose estimation based on orientation soft classification, which allows modelling orientation ambiguity as a mixture of Gaussians. This framework was evaluated both on URSO datasets and the ESA pose estimation challenge. In this competition, our best model achieved 3rd place on the synthetic test set and 2nd place on the real test set. Moreover, our results show the impact of several architectural and training aspects, and we demonstrate qualitatively how models learned on URSO datasets can perform on real images from space.



### DSNet: Automatic Dermoscopic Skin Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.04305v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.04305v2)
- **Published**: 2019-07-09 17:42:51+00:00
- **Updated**: 2020-01-23 22:06:47+00:00
- **Authors**: Md. Kamrul Hasan, Lavsen Dahal, Prasad N. Samarakoon, Fakrul Islam Tushar, Robert Marti Marly
- **Comment**: 25 pages
- **Journal**: None
- **Summary**: Automatic segmentation of skin lesion is considered a crucial step in Computer Aided Diagnosis (CAD) for melanoma diagnosis. Despite its significance, skin lesion segmentation remains a challenging task due to their diverse color, texture, and indistinguishable boundaries and forms an open problem. Through this study, we present a new and automatic semantic segmentation network for robust skin lesion segmentation named Dermoscopic Skin Network (DSNet). In order to reduce the number of parameters to make the network lightweight, we used depth-wise separable convolution in lieu of standard convolution to project the learned discriminating features onto the pixel space at different stages of the encoder. Additionally, we implemented U-Net and Fully Convolutional Network (FCN8s) to compare against the proposed DSNet. We evaluate our proposed model on two publicly available datasets, namely ISIC-2017 and PH2. The obtained mean Intersection over Union (mIoU) is 77.5 % and 87.0 % respectively for ISIC-2017 and PH2 datasets which outperformed the ISIC-2017 challenge winner by 1.0 % with respect to mIoU. Our proposed network also outperformed U-Net and FCN8s respectively by 3.6 % and 6.8 % with respect to mIoU on the ISIC-2017 dataset. Our network for skin lesion segmentation outperforms other methods and can provide better segmented masks on two different test datasets which can lead to better performance in melanoma detection. Our trained model along with the source code and predicted masks are made publicly available.



### Positional Normalization
- **Arxiv ID**: http://arxiv.org/abs/1907.04312v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.04312v2)
- **Published**: 2019-07-09 17:52:01+00:00
- **Updated**: 2019-12-19 18:58:04+00:00
- **Authors**: Boyi Li, Felix Wu, Kilian Q. Weinberger, Serge Belongie
- **Comment**: Accepted to NeurIPS 2019 (spotlight)
- **Journal**: None
- **Summary**: A popular method to reduce the training time of deep neural networks is to normalize activations at each layer. Although various normalization schemes have been proposed, they all follow a common theme: normalize across spatial dimensions and discard the extracted statistics. In this paper, we propose an alternative normalization method that noticeably departs from this convention and normalizes exclusively across channels. We argue that the channel dimension is naturally appealing as it allows us to extract the first and second moments of features extracted at a particular image position. These moments capture structural information about the input image and extracted features, which opens a new avenue along which a network can benefit from feature normalization: Instead of disregarding the normalization constants, we propose to re-inject them into later layers to preserve or transfer structural information in generative networks. Codes are available at https://github.com/Boyiliee/PONO.



### Hybrid system identification using switching density networks
- **Arxiv ID**: http://arxiv.org/abs/1907.04360v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.04360v4)
- **Published**: 2019-07-09 18:31:51+00:00
- **Updated**: 2019-09-18 10:04:09+00:00
- **Authors**: Michael Burke, Yordan Hristov, Subramanian Ramamoorthy
- **Comment**: None
- **Journal**: Conference on Robot Learning (CoRL 2019)
- **Summary**: Behaviour cloning is a commonly used strategy for imitation learning and can be extremely effective in constrained domains. However, in cases where the dynamics of an environment may be state dependent and varying, behaviour cloning places a burden on model capacity and the number of demonstrations required. This paper introduces switching density networks, which rely on a categorical reparametrisation for hybrid system identification. This results in a network comprising a classification layer that is followed by a regression layer. We use switching density networks to predict the parameters of hybrid control laws, which are toggled by a switching layer to produce different controller outputs, when conditioned on an input state. This work shows how switching density networks can be used for hybrid system identification in a variety of tasks, successfully identifying the key joint angle goals that make up manipulation tasks, while simultaneously learning image-based goal classifiers and regression networks that predict joint angles from images. We also show that they can cluster the phase space of an inverted pendulum, identifying the balance, spin and pump controllers required to solve this task. Switching density networks can be difficult to train, but we introduce a cross entropy regularisation loss that stabilises training.



### Estimating Pedestrian Moving State Based on Single 2D Body Pose
- **Arxiv ID**: http://arxiv.org/abs/1907.04361v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1907.04361v3)
- **Published**: 2019-07-09 18:32:21+00:00
- **Updated**: 2019-09-16 14:44:20+00:00
- **Authors**: Zixing Wang, Nikolaos Papanikolopoulos
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: The Crossing or Not-Crossing (C/NC) problem is important to autonomous vehicles (AVs) for safe vehicle/pedestrian interactions. However, this problem setup often ignores pedestrians walking along the direction of the vehicles' movement (LONG). To enhance the AVs' awareness of pedestrians behavior, we make the first step towards extending the C/NC to the C/NC/LONG problem and recognize them based on single body pose. In contrast, previous C/NC state classifiers depend on multiple poses or contextual information. Our proposed shallow neural network classifier aims to recognize these three states swiftly. We tested it on the JAAD dataset and reported an average 81.23% accuracy. Furthermore, this model can be integrated with different sensors and algorithms that provide 2D pedestrian body pose so that it is able to function across multiple light and weather conditions.



### BASN -- Learning Steganography with Binary Attention Mechanism
- **Arxiv ID**: http://arxiv.org/abs/1907.04362v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1907.04362v1)
- **Published**: 2019-07-09 18:33:51+00:00
- **Updated**: 2019-07-09 18:33:51+00:00
- **Authors**: Yang Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Secret information sharing through image carrier has aroused much research attention in recent years with images' growing domination on the Internet and mobile applications. However, with the booming trend of convolutional neural networks, image steganography is facing a more significant challenge from neural-network-automated tasks. To improve the security of image steganography and minimize task result distortion, models must maintain the feature maps generated by task-specific networks being irrelative to any hidden information embedded in the carrier. This paper introduces a binary attention mechanism into image steganography to help alleviate the security issue, and in the meanwhile, increase embedding payload capacity. The experimental results show that our method has the advantage of high payload capacity with little feature map distortion and still resist detection by state-of-the-art image steganalysis algorithms.



### M3D-GAN: Multi-Modal Multi-Domain Translation with Universal Attention
- **Arxiv ID**: http://arxiv.org/abs/1907.04378v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.04378v1)
- **Published**: 2019-07-09 19:33:01+00:00
- **Updated**: 2019-07-09 19:33:01+00:00
- **Authors**: Shuang Ma, Daniel McDuff, Yale Song
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial networks have led to significant advances in cross-modal/domain translation. However, typically these networks are designed for a specific task (e.g., dialogue generation or image synthesis, but not both). We present a unified model, M3D-GAN, that can translate across a wide range of modalities (e.g., text, image, and speech) and domains (e.g., attributes in images or emotions in speech). Our model consists of modality subnets that convert data from different modalities into unified representations, and a unified computing body where data from different modalities share the same network architecture. We introduce a universal attention module that is jointly trained with the whole network and learns to encode a large range of domain information into a highly structured latent space. We use this to control synthesis in novel ways, such as producing diverse realistic pictures from a sketch or varying the emotion of synthesized speech. We evaluate our approach on extensive benchmark tasks, including image-to-image, text-to-image, image captioning, text-to-speech, speech recognition, and machine translation. Our results show state-of-the-art performance on some of the tasks.



### A New Stereo Benchmarking Dataset for Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/1907.04404v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.04404v1)
- **Published**: 2019-07-09 20:41:08+00:00
- **Updated**: 2019-07-09 20:41:08+00:00
- **Authors**: Sonali Patil, Bharath Comandur, Tanmay Prakash, Avinash C. Kak
- **Comment**: None
- **Journal**: None
- **Summary**: In order to facilitate further research in stereo reconstruction with multi-date satellite images, the goal of this paper is to provide a set of stereo-rectified images and the associated groundtruthed disparities for 10 AOIs (Area of Interest) drawn from two sources: 8 AOIs from IARPA's MVS Challenge dataset and 2 AOIs from the CORE3D-Public dataset. The disparities were groundtruthed by first constructing a fused DSM from the stereo pairs and by aligning 30 cm LiDAR with the fused DSM. Unlike the existing benckmarking datasets, we have also carried out a quantitative evaluation of our groundtruthed disparities using human annotated points in two of the AOIs. Additionally, the rectification accuracy in our dataset is comparable to the same in the existing state-of-the-art stereo datasets. In general, we have used the WorldView-3 (WV3) images for the dataset, the exception being the UCSD area for which we have used both WV3 and WorldView-2 (WV2) images. All of the dataset images are now in the public domain. Since multi-date satellite images frequently include images acquired in different seasons (which creates challenges in finding corresponding pairs of pixels for stereo), our dataset also includes for each image a building mask over which the disparities estimated by stereo should prove reliable. Additional metadata included in the dataset includes information about each image's acquisition date and time, the azimuth and elevation angles of the camera, and the intersection angles for the two views in a stereo pair. Also included in the dataset are both quantitative and qualitative analyses of the accuracy of the groundtruthed disparity maps. Our dataset is available for download at \url{https://engineering.purdue.edu/RVL/Database/SatStereo/index.html}



### Global Optimality Guarantees for Nonconvex Unsupervised Video Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.04409v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.04409v2)
- **Published**: 2019-07-09 20:53:13+00:00
- **Updated**: 2020-02-22 21:45:47+00:00
- **Authors**: Brendon G. Anderson, Somayeh Sojoudi
- **Comment**: Proceedings of the 57th Annual Allerton Conference on Communication,
  Control, and Computing, 2019; added funding source information and notation
  definitions
- **Journal**: Proceedings of the 57th Annual Allerton Conference on
  Communication, Control, and Computing, pp. 965--972, 2019
- **Summary**: In this paper, we consider the problem of unsupervised video object segmentation via background subtraction. Specifically, we pose the nonsemantic extraction of a video's moving objects as a nonconvex optimization problem via a sum of sparse and low-rank matrices. The resulting formulation, a nonnegative variant of robust principal component analysis, is more computationally tractable than its commonly employed convex relaxation, although not generally solvable to global optimality. In spite of this limitation, we derive intuitive and interpretable conditions on the video data under which the uniqueness and global optimality of the object segmentation are guaranteed using local search methods. We illustrate these novel optimality criteria through example segmentations using real video data.



### StrokeSave: A Novel, High-Performance Mobile Application for Stroke Diagnosis using Deep Learning and Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1907.05358v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.05358v1)
- **Published**: 2019-07-09 21:01:58+00:00
- **Updated**: 2019-07-09 21:01:58+00:00
- **Authors**: Ankit Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: According to the WHO, Cerebrovascular Stroke, or CS, is the second largest cause of death worldwide. Current diagnosis of CS relies on labor and cost intensive neuroimaging techniques, unsuitable for areas with inadequate access to quality medical facilities. Thus, there is a great need for an efficient diagnosis alternative. StrokeSave is a platform for users to self-diagnose for prevalence to stroke. The mobile app is continuously updated with heart rate, blood pressure, and blood oxygen data from sensors on the patient wrist. Once these measurements reach a threshold for possible stroke, the patient takes facial images and vocal recordings to screen for paralysis attributed to stroke. A custom designed lens attached to a phone's camera then takes retinal images for the deep learning model to classify based on presence of retinopathy and sends a comprehensive diagnosis. The deep learning model, which consists of a RNN trained on 100 voice slurred audio files, a SVM trained on 410 vascular data points, and a CNN trained on 520 retinopathy images, achieved a holistic accuracy of 95.0 percent when validated on 327 samples. This value exceeds that of clinical examination accuracy, which is around 40 to 89 percent, further demonstrating the vital utility of such a medical device. Through this automated platform, users receive efficient, highly accurate diagnosis without professional medical assistance, revolutionizing medical diagnosis of CS and potentially saving millions of lives.



### Automatic Mass Detection in Breast Using Deep Convolutional Neural Network and SVM Classifier
- **Arxiv ID**: http://arxiv.org/abs/1907.04424v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.04424v1)
- **Published**: 2019-07-09 21:39:23+00:00
- **Updated**: 2019-07-09 21:39:23+00:00
- **Authors**: Md. Kamrul Hasan, Tajwar Abrar Aleef
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Mammography is the most widely used gold standard for screening breast cancer, where, mass detection is considered as the prominent step. Detecting mass in the breast is, however, an arduous problem as they usually have large variations between them in terms of shape, size, boundary, and texture. In this literature, the process of mass detection is automated with the use of transfer learning techniques of Deep Convolutional Neural Networks (DCNN). Pre-trained VGG19 network is used to extract features which are then followed by bagged decision tree for features selection and then a Support Vector Machine (SVM) classifier is trained and used for classifying between the mass and non-mass. Area Under ROC Curve (AUC) is chosen as the performance metric, which is then maximized during classifier selection and hyper-parameter tuning. The robustness of the two selected type of classifiers, C-SVM, and \u{psion}-SVM, are investigated with extensive experiments before selecting the best performing classifier. All experiments in this paper were conducted using the INbreast dataset. The best AUC obtained from the experimental results is 0.994 +/- 0.003 i.e. [0.991, 0.997]. Our results conclude that by using pre-trained VGG19 network, high-level distinctive features can be extracted from Mammograms which when used with the proposed SVM classifier is able to robustly distinguish between the mass and non-mass present in breast.



### GluonCV and GluonNLP: Deep Learning in Computer Vision and Natural Language Processing
- **Arxiv ID**: http://arxiv.org/abs/1907.04433v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.04433v2)
- **Published**: 2019-07-09 21:59:44+00:00
- **Updated**: 2020-02-13 00:54:42+00:00
- **Authors**: Jian Guo, He He, Tong He, Leonard Lausen, Mu Li, Haibin Lin, Xingjian Shi, Chenguang Wang, Junyuan Xie, Sheng Zha, Aston Zhang, Hang Zhang, Zhi Zhang, Zhongyue Zhang, Shuai Zheng, Yi Zhu
- **Comment**: None
- **Journal**: Journal of Machine Learning Research 21 (2020) 1-7
- **Summary**: We present GluonCV and GluonNLP, the deep learning toolkits for computer vision and natural language processing based on Apache MXNet (incubating). These toolkits provide state-of-the-art pre-trained models, training scripts, and training logs, to facilitate rapid prototyping and promote reproducible research. We also provide modular APIs with flexible building blocks to enable efficient customization. Leveraging the MXNet ecosystem, the deep learning models in GluonCV and GluonNLP can be deployed onto a variety of platforms with different programming languages. The Apache 2.0 license has been adopted by GluonCV and GluonNLP to allow for software distribution, modification, and usage.



### A review on deep learning techniques for 3D sensed data classification
- **Arxiv ID**: http://arxiv.org/abs/1907.04444v1
- **DOI**: 10.3390/rs11121499
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.04444v1)
- **Published**: 2019-07-09 22:30:03+00:00
- **Updated**: 2019-07-09 22:30:03+00:00
- **Authors**: David Griffiths, Jan Boehm
- **Comment**: 25 pages, 9 figures. Review paper
- **Journal**: None
- **Summary**: Over the past decade deep learning has driven progress in 2D image understanding. Despite these advancements, techniques for automatic 3D sensed data understanding, such as point clouds, is comparatively immature. However, with a range of important applications from indoor robotics navigation to national scale remote sensing there is a high demand for algorithms that can learn to automatically understand and classify 3D sensed data. In this paper we review the current state-of-the-art deep learning architectures for processing unstructured Euclidean data. We begin by addressing the background concepts and traditional methodologies. We review the current main approaches including; RGB-D, multi-view, volumetric and fully end-to-end architecture designs. Datasets for each category are documented and explained. Finally, we give a detailed discussion about the future of deep learning for 3D sensed data, using literature to justify the areas where future research would be most valuable.



### PhysGAN: Generating Physical-World-Resilient Adversarial Examples for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1907.04449v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.04449v3)
- **Published**: 2019-07-09 22:46:10+00:00
- **Updated**: 2021-05-11 20:56:32+00:00
- **Authors**: Zelun Kong, Junfeng Guo, Ang Li, Cong Liu
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Although Deep neural networks (DNNs) are being pervasively used in vision-based autonomous driving systems, they are found vulnerable to adversarial attacks where small-magnitude perturbations into the inputs during test time cause dramatic changes to the outputs. While most of the recent attack methods target at digital-world adversarial scenarios, it is unclear how they perform in the physical world, and more importantly, the generated perturbations under such methods would cover a whole driving scene including those fixed background imagery such as the sky, making them inapplicable to physical world implementation. We present PhysGAN, which generates physical-world-resilient adversarial examples for mislead-ing autonomous driving systems in a continuous manner. We show the effectiveness and robustness of PhysGAN via extensive digital and real-world evaluations. Digital experiments show that PhysGAN is effective for various steer-ing models and scenes, which misleads the average steer-ing angle by up to 23.06 degrees under various scenarios. The real-world studies further demonstrate that PhysGAN is sufficiently resilient in practice, which misleads the average steering angle by up to 19.17 degrees. We compare PhysGAN with a set of state-of-the-art baseline methods including several of our self-designed ones, which further demonstrate the robustness and efficacy of our approach. We also show that PhysGAN outperforms state-of-the-art baseline methods To the best of our knowledge, PhysGANis probably the first technique of generating realistic and physical-world-resilient adversarial examples for attacking common autonomous driving scenarios.



