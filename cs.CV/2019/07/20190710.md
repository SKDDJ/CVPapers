# Arxiv Papers in cs.CV on 2019-07-10
### Coarse Graining of Data via Inhomogeneous Diffusion Condensation
- **Arxiv ID**: http://arxiv.org/abs/1907.04463v3
- **DOI**: 10.1109/BigData47090.2019.9006013
- **Categories**: **cs.HC**, cs.CV, cs.LG, q-bio.QM, I.5.3
- **Links**: [PDF](http://arxiv.org/pdf/1907.04463v3)
- **Published**: 2019-07-10 00:08:07+00:00
- **Updated**: 2020-03-09 20:12:26+00:00
- **Authors**: Nathan Brugnone, Alex Gonopolskiy, Mark W. Moyle, Manik Kuchroo, David van Dijk, Kevin R. Moon, Daniel Colon-Ramos, Guy Wolf, Matthew J. Hirn, Smita Krishnaswamy
- **Comment**: 14 pages, 7 figures
- **Journal**: Proceedings of the 2019 IEEE International Conference on Big Data,
  pages 2624-2633, 2019
- **Summary**: Big data often has emergent structure that exists at multiple levels of abstraction, which are useful for characterizing complex interactions and dynamics of the observations. Here, we consider multiple levels of abstraction via a multiresolution geometry of data points at different granularities. To construct this geometry we define a time-inhomogeneous diffusion process that effectively condenses data points together to uncover nested groupings at larger and larger granularities. This inhomogeneous process creates a deep cascade of intrinsic low pass filters on the data affinity graph that are applied in sequence to gradually eliminate local variability while adjusting the learned data geometry to increasingly coarser resolutions. We provide visualizations to exhibit our method as a continuously-hierarchical clustering with directions of eliminated variation highlighted at each step. The utility of our algorithm is demonstrated via neuronal data condensation, where the constructed multiresolution data geometry uncovers the organization, grouping, and connectivity between neurons.



### A New Benchmark and Approach for Fine-grained Cross-media Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1907.04476v2
- **DOI**: 10.1145/3343031.3350974
- **Categories**: **cs.IR**, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1907.04476v2)
- **Published**: 2019-07-10 01:15:22+00:00
- **Updated**: 2019-10-31 06:37:53+00:00
- **Authors**: Xiangteng He, Yuxin Peng, Liu Xie
- **Comment**: 9 pages, ACM MM 2019
- **Journal**: None
- **Summary**: Cross-media retrieval is to return the results of various media types corresponding to the query of any media type. Existing researches generally focus on coarse-grained cross-media retrieval. When users submit an image of "Slaty-backed Gull" as a query, coarse-grained cross-media retrieval treats it as "Bird", so that users can only get the results of "Bird", which may include other bird species with similar appearance (image and video), descriptions (text) or sounds (audio), such as "Herring Gull". Such coarse-grained cross-media retrieval is not consistent with human lifestyle, where we generally have the fine-grained requirement of returning the exactly relevant results of "Slaty-backed Gull" instead of "Herring Gull". However, few researches focus on fine-grained cross-media retrieval, which is a highly challenging and practical task. Therefore, in this paper, we first construct a new benchmark for fine-grained cross-media retrieval, which consists of 200 fine-grained subcategories of the "Bird", and contains 4 media types, including image, text, video and audio. To the best of our knowledge, it is the first benchmark with 4 media types for fine-grained cross-media retrieval. Then, we propose a uniform deep model, namely FGCrossNet, which simultaneously learns 4 types of media without discriminative treatments. We jointly consider three constraints for better common representation learning: classification constraint ensures the learning of discriminative features, center constraint ensures the compactness characteristic of the features of the same subcategory, and ranking constraint ensures the sparsity characteristic of the features of different subcategories. Extensive experiments verify the usefulness of the new benchmark and the effectiveness of our FGCrossNet. They will be made available at https://github.com/PKU-ICST-MIPL/FGCrossNet_ACMMM2019.



### Fetal Pose Estimation in Volumetric MRI using a 3D Convolution Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1907.04500v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.04500v1)
- **Published**: 2019-07-10 03:56:02+00:00
- **Updated**: 2019-07-10 03:56:02+00:00
- **Authors**: Junshen Xu, Molin Zhang, Esra Abaci Turk, Larry Zhang, Ellen Grant, Kui Ying, Polina Golland, Elfar Adalsteinsson
- **Comment**: MICCAI 2019
- **Journal**: None
- **Summary**: The performance and diagnostic utility of magnetic resonance imaging (MRI) in pregnancy is fundamentally constrained by fetal motion. Motion of the fetus, which is unpredictable and rapid on the scale of conventional imaging times, limits the set of viable acquisition techniques to single-shot imaging with severe compromises in signal-to-noise ratio and diagnostic contrast, and frequently results in unacceptable image quality. Surprisingly little is known about the characteristics of fetal motion during MRI and here we propose and demonstrate methods that exploit a growing repository of MRI observations of the gravid abdomen that are acquired at low spatial resolution but relatively high temporal resolution and over long durations (10-30 minutes). We estimate fetal pose per frame in MRI volumes of the pregnant abdomen via deep learning algorithms that detect key fetal landmarks. Evaluation of the proposed method shows that our framework achieves quantitatively an average error of 4.47 mm and 96.4\% accuracy (with error less than 10 mm). Fetal pose estimation in MRI time series yields novel means of quantifying fetal movements in health and disease, and enables the learning of kinematic models that may enhance prospective mitigation of fetal motion artifacts during MRI acquisition.



### Purifying Real Images with an Attention-guided Style Transfer Network for Gaze Estimation
- **Arxiv ID**: http://arxiv.org/abs/2002.06145v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.06145v1)
- **Published**: 2019-07-10 04:29:32+00:00
- **Updated**: 2019-07-10 04:29:32+00:00
- **Authors**: Yuxiao Yan, Yang Yan, Jinjia Peng, Huibing Wang, Xianping Fu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1903.05820,
  arXiv:1903.08152; and text overlap with arXiv:1603.08155 by other authors
- **Journal**: None
- **Summary**: Recently, the progress of learning-by-synthesis has proposed a training model for synthetic images, which can effectively reduce the cost of human and material resources. However, due to the different distribution of synthetic images compared to real images, the desired performance cannot be achieved. Real images consist of multiple forms of light orientation, while synthetic images consist of a uniform light orientation. These features are considered to be characteristic of outdoor and indoor scenes, respectively. To solve this problem, the previous method learned a model to improve the realism of the synthetic image. Different from the previous methods, this paper try to purify real image by extracting discriminative and robust features to convert outdoor real images to indoor synthetic images. In this paper, we first introduce the segmentation masks to construct RGB-mask pairs as inputs, then we design a attention-guided style transfer network to learn style features separately from the attention and bkgd(background) region , learn content features from full and attention region. Moreover, we propose a novel region-level task-guided loss to restrain the features learnt from style and content. Experiments were performed using mixed studies (qualitative and quantitative) methods to demonstrate the possibility of purifying real images in complex directions. We evaluate the proposed method on three public datasets, including LPW, COCO and MPIIGaze. Extensive experimental results show that the proposed method is effective and achieves the state-of-the-art results.



### Restoring Images with Unknown Degradation Factors by Recurrent Use of a Multi-branch Network
- **Arxiv ID**: http://arxiv.org/abs/1907.04508v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.04508v2)
- **Published**: 2019-07-10 04:58:10+00:00
- **Updated**: 2020-01-21 07:00:11+00:00
- **Authors**: Xing Liu, Masanori Suganuma, Xiyang Luo, Takayuki Okatani
- **Comment**: None
- **Journal**: None
- **Summary**: The employment of convolutional neural networks has achieved unprecedented performance in the task of image restoration for a variety of degradation factors. However, high-performance networks have been specifically designed for a single degradation factor. In this paper, we tackle a harder problem, restoring a clean image from its degraded version with an unknown degradation factor, subject to the condition that it is one of the known factors. Toward this end, we design a network having multiple pairs of input and output branches and use it in a recurrent fashion such that a different branch pair is used at each of the recurrent paths. We reinforce the shared part of the network with improved components so that it can handle different degradation factors. We also propose a two-step training method for the network, which consists of multi-task learning and finetuning. The experimental results show that the proposed network yields at least comparable or sometimes even better performance on four degradation factors as compared with the best dedicated network for each of the four. We also test it on a further harder task where the input image contains multiple degradation factors that are mixed with unknown mixture ratios, showing that it achieves better performance than the previous state-of-the-art method designed for the task.



### Preferences Prediction using a Gallery of Mobile Device based on Scene Recognition and Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1907.04519v2
- **DOI**: 10.1016/j.patcog.2021.108248
- **Categories**: **cs.CV**, cs.LG, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1907.04519v2)
- **Published**: 2019-07-10 05:45:08+00:00
- **Updated**: 2021-04-18 13:33:11+00:00
- **Authors**: A. V. Savchenko, K. V. Demochkin, I. S. Grechikhin
- **Comment**: 19 pages; 9 figures, preprint submitter to Pattern Recognition
  journal
- **Journal**: None
- **Summary**: In this paper user modeling task is examined by processing a gallery of photos and videos on a mobile device. We propose novel engine for user preference prediction based on scene recognition, object detection and facial analysis. At first, all faces in a gallery are clustered and all private photos and videos with faces from large clusters are processed on the embedded system in offline mode. Other photos may be sent to the remote server to be analyzed by very deep models. The visual features of each photo are obtained from scene recognition and object detection models. These features are aggregated into a single user descriptor in the neural attention block. The proposed pipeline is implemented for the Android mobile platform. Experimental results with a subset of Photo Event Collection, Web Image Dataset for Event Recognition and Amazon Fashion datasets demonstrate the possibility to process images very efficiently without significant accuracy degradation. The source code of Android mobile application is publicly available at https://github.com/HSE-asavchenko/mobile-visual-preferences.



### Regularizing Neural Networks for Future Trajectory Prediction via Inverse Reinforcement Learning Framework
- **Arxiv ID**: http://arxiv.org/abs/1907.04525v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.04525v2)
- **Published**: 2019-07-10 06:00:51+00:00
- **Updated**: 2019-12-26 00:32:53+00:00
- **Authors**: Dooseop Choi, Kyoungwook Min, Jeongdan Choi
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting distant future trajectories of agents in a dynamic scene is not an easy problem because the future trajectory of an agent is affected by not only his/her past trajectory but also the scene contexts. To tackle this problem, we propose a model based on recurrent neural networks (RNNs) and a novel method for training the model. The proposed model is based on an encoder-decoder architecture where the encoder encodes inputs (past trajectories and scene context information) while the decoder produces a trajectory from the context vector given by the encoder. We train the networks of the proposed model to produce a future trajectory, which is the closest to the true trajectory, while maximizing a reward from a reward function. The reward function is also trained at the same time to maximize the margin between the rewards from the ground-truth trajectory and its estimate. The reward function plays the role of a regularizer for the proposed model so the trained networks are able to better utilize the scene context information for the prediction task. We evaluated the proposed model on several public datasets. Experimental results show that the prediction performance of the proposed model is much improved by the regularization, which outperforms the-state-of-the-arts in terms of accuracy. The implementation codes are available at https://github.com/d1024choi/traj-pred-irl/.



### Neural Reasoning, Fast and Slow, for Video Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1907.04553v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.04553v2)
- **Published**: 2019-07-10 07:53:17+00:00
- **Updated**: 2020-04-11 00:30:34+00:00
- **Authors**: Thao Minh Le, Vuong Le, Svetha Venkatesh, Truyen Tran
- **Comment**: None
- **Journal**: International Joint Conference on Neural Networks (IJCNN) 2020
- **Summary**: What does it take to design a machine that learns to answer natural questions about a video? A Video QA system must simultaneously understand language, represent visual content over space-time, and iteratively transform these representations in response to lingual content in the query, and finally arriving at a sensible answer. While recent advances in lingual and visual question answering have enabled sophisticated representations and neural reasoning mechanisms, major challenges in Video QA remain on dynamic grounding of concepts, relations and actions to support the reasoning process. Inspired by the dual-process account of human reasoning, we design a dual process neural architecture, which is composed of a question-guided video processing module (System 1, fast and reactive) followed by a generic reasoning module (System 2, slow and deliberative). System 1 is a hierarchical model that encodes visual patterns about objects, actions and relations in space-time given the textual cues from the question. The encoded representation is a set of high-level visual features, which are then passed to System 2. Here multi-step inference follows to iteratively chain visual elements as instructed by the textual elements. The system is evaluated on the SVQA (synthetic) and TGIF-QA datasets (real), demonstrating competitive results, with a large margin in the case of multi-step reasoning.



### Deep Multi Label Classification in Affine Subspaces
- **Arxiv ID**: http://arxiv.org/abs/1907.04563v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.04563v1)
- **Published**: 2019-07-10 08:16:27+00:00
- **Updated**: 2019-07-10 08:16:27+00:00
- **Authors**: Thomas Kurmann, Pablo Marquez Neila, Sebastian Wolf, Raphael Sznitman
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-label classification (MLC) problems are becoming increasingly popular in the context of medical imaging. This has in part been driven by the fact that acquiring annotations for MLC is far less burdensome than for semantic segmentation and yet provides more expressiveness than multi-class classification. However, to train MLCs, most methods have resorted to similar objective functions as with traditional multi-class classification settings. We show in this work that such approaches are not optimal and instead propose a novel deep MLC classification method in affine subspace. At its core, the method attempts to pull features of class-labels towards different affine subspaces while maximizing the distance between them. We evaluate the method using two MLC medical imaging datasets and show a large performance increase compared to previous multi-label frameworks. This method can be seen as a plug-in replacement loss function and is trainable in an end-to-end fashion.



### Progressive Wasserstein Barycenters of Persistence Diagrams
- **Arxiv ID**: http://arxiv.org/abs/1907.04565v2
- **DOI**: 10.1109/TVCG.2019.2934256
- **Categories**: **cs.GR**, cs.CG, cs.CV, cs.DS
- **Links**: [PDF](http://arxiv.org/pdf/1907.04565v2)
- **Published**: 2019-07-10 08:24:11+00:00
- **Updated**: 2019-10-09 16:36:24+00:00
- **Authors**: Jules Vidal, Joseph Budin, Julien Tierny
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents an efficient algorithm for the progressive approximation of Wasserstein barycenters of persistence diagrams, with applications to the visual analysis of ensemble data. Given a set of scalar fields, our approach enables the computation of a persistence diagram which is representative of the set, and which visually conveys the number, data ranges and saliences of the main features of interest found in the set. Such representative diagrams are obtained by computing explicitly the discrete Wasserstein barycenter of the set of persistence diagrams, a notoriously computationally intensive task. In particular, we revisit efficient algorithms for Wasserstein distance approximation [12,51] to extend previous work on barycenter estimation [94]. We present a new fast algorithm, which progressively approximates the barycenter by iteratively increasing the computation accuracy as well as the number of persistent features in the output diagram. Such a progressivity drastically improves convergence in practice and allows to design an interruptible algorithm, capable of respecting computation time constraints. This enables the approximation of Wasserstein barycenters within interactive times. We present an application to ensemble clustering where we revisit the k-means algorithm to exploit our barycenters and compute, within execution time constraints, meaningful clusters of ensemble data along with their barycenter diagram. Extensive experiments on synthetic and real-life data sets report that our algorithm converges to barycenters that are qualitatively meaningful with regard to the applications, and quantitatively comparable to previous techniques, while offering an order of magnitude speedup when run until convergence (without time constraint). Our algorithm can be trivially parallelized to provide additional speedups in practice on standard workstations. [...]



### Generating All the Roads to Rome: Road Layout Randomization for Improved Road Marking Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.04569v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.04569v1)
- **Published**: 2019-07-10 08:27:59+00:00
- **Updated**: 2019-07-10 08:27:59+00:00
- **Authors**: Tom Bruls, Horia Porav, Lars Kunze, Paul Newman
- **Comment**: presented at ITSC 2019
- **Journal**: None
- **Summary**: Road markings provide guidance to traffic participants and enforce safe driving behaviour, understanding their semantic meaning is therefore paramount in (automated) driving. However, producing the vast quantities of road marking labels required for training state-of-the-art deep networks is costly, time-consuming, and simply infeasible for every domain and condition. In addition, training data retrieved from virtual worlds often lack the richness and complexity of the real world and consequently cannot be used directly. In this paper, we provide an alternative approach in which new road marking training pairs are automatically generated. To this end, we apply principles of domain randomization to the road layout and synthesize new images from altered semantic labels. We demonstrate that training on these synthetic pairs improves mIoU of the segmentation of rare road marking classes during real-world deployment in complex urban environments by more than 12 percentage points, while performance for other classes is retained. This framework can easily be scaled to all domains and conditions to generate large-scale road marking datasets, while avoiding manual labelling effort.



### Out-of-Distribution Detection Using Neural Rendering Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1907.04572v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.04572v1)
- **Published**: 2019-07-10 08:32:53+00:00
- **Updated**: 2019-07-10 08:32:53+00:00
- **Authors**: Yujia Huang, Sihui Dai, Tan Nguyen, Richard G. Baraniuk, Anima Anandkumar
- **Comment**: None
- **Journal**: None
- **Summary**: Out-of-distribution (OoD) detection is a natural downstream task for deep generative models, due to their ability to learn the input probability distribution. There are mainly two classes of approaches for OoD detection using deep generative models, viz., based on likelihood measure and the reconstruction loss. However, both approaches are unable to carry out OoD detection effectively, especially when the OoD samples have smaller variance than the training samples. For instance, both flow based and VAE models assign higher likelihood to images from SVHN when trained on CIFAR-10 images. We use a recently proposed generative model known as neural rendering model (NRM) and derive metrics for OoD. We show that NRM unifies both approaches since it provides a likelihood estimate and also carries out reconstruction in each layer of the neural network. Among various measures, we found the joint likelihood of latent variables to be the most effective one for OoD detection. Our results show that when trained on CIFAR-10, lower likelihood (of latent variables) is assigned to SVHN images. Additionally, we show that this metric is consistent across other OoD datasets. To the best of our knowledge, this is the first work to show consistently lower likelihood for OoD data with smaller variance with deep generative models.



### Dunhuang Grottoes Painting Dataset and Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1907.04589v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.04589v2)
- **Published**: 2019-07-10 09:35:08+00:00
- **Updated**: 2019-07-11 06:46:18+00:00
- **Authors**: Tianxiu Yu, Shijie Zhang, Cong Lin, Shaodi You, Jian Wu, Jiawan Zhang, Xiaohong Ding, Huili An
- **Comment**: 8 pages, 1 column
- **Journal**: None
- **Summary**: This document introduces the background and the usage of the Dunhuang Grottoes Dataset and the benchmark. The documentation first starts with the background of the Dunhuang Grotto, which is widely recognised as an priceless heritage. Given that digital method is the modern trend for heritage protection and restoration. Follow the trend, we release the first public dataset for Dunhuang Grotto Painting restoration. The rest of the documentation details the painting data generation. To enable a data driven fashion, this dataset provided a large number of training and testing example which is sufficient for a deep learning approach. The detailed usage of the dataset as well as the benchmark is described.



### Evaluation of Retinal Image Quality Assessment Networks in Different Color-spaces
- **Arxiv ID**: http://arxiv.org/abs/1907.05345v4
- **DOI**: 10.1007/978-3-030-32239-7_6
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.05345v4)
- **Published**: 2019-07-10 10:47:36+00:00
- **Updated**: 2020-01-09 10:49:13+00:00
- **Authors**: Huazhu Fu, Boyang Wang, Jianbing Shen, Shanshan Cui, Yanwu Xu, Jiang Liu, Ling Shao
- **Comment**: Accepted by MICCAI 2019. Corrected two typos in Table 1 as: (1) in
  training set, the number of "Usable + All" should be '1,876'; (2) In testing
  set, the number of "Total + DR-0" should be '11,362'. Project page:
  https://github.com/hzfu/EyeQ
- **Journal**: None
- **Summary**: Retinal image quality assessment (RIQA) is essential for controlling the quality of retinal imaging and guaranteeing the reliability of diagnoses by ophthalmologists or automated analysis systems. Existing RIQA methods focus on the RGB color-space and are developed based on small datasets with binary quality labels (i.e., `Accept' and `Reject'). In this paper, we first re-annotate an Eye-Quality (EyeQ) dataset with 28,792 retinal images from the EyePACS dataset, based on a three-level quality grading system (i.e., `Good', `Usable' and `Reject') for evaluating RIQA methods. Our RIQA dataset is characterized by its large-scale size, multi-level grading, and multi-modality. Then, we analyze the influences on RIQA of different color-spaces, and propose a simple yet efficient deep network, named Multiple Color-space Fusion Network (MCF-Net), which integrates the different color-space representations at both a feature-level and prediction-level to predict image quality grades. Experiments on our EyeQ dataset show that our MCF-Net obtains a state-of-the-art performance, outperforming the other deep learning methods. Furthermore, we also evaluate diabetic retinopathy (DR) detection methods on images of different quality, and demonstrate that the performances of automated diagnostic systems are highly dependent on image quality.



### Video Action Recognition Via Neural Architecture Searching
- **Arxiv ID**: http://arxiv.org/abs/1907.04632v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.04632v1)
- **Published**: 2019-07-10 11:44:28+00:00
- **Updated**: 2019-07-10 11:44:28+00:00
- **Authors**: Wei Peng, Xiaopeng Hong, Guoying Zhao
- **Comment**: Accepted by IEEE ICIP2019
- **Journal**: None
- **Summary**: Deep neural networks have achieved great success for video analysis and understanding. However, designing a high-performance neural architecture requires substantial efforts and expertise. In this paper, we make the first attempt to let algorithm automatically design neural networks for video action recognition tasks. Specifically, a spatio-temporal network is developed in a differentiable space modeled by a directed acyclic graph, thus a gradient-based strategy can be performed to search an optimal architecture. Nonetheless, it is computationally expensive, since the computational burden to evaluate each architecture candidate is still heavy. To alleviate this issue, we, for the video input, introduce a temporal segment approach to reduce the computational cost without losing global video information. For the architecture, we explore in an efficient search space by introducing pseudo 3D operators. Experiments show that, our architecture outperforms popular neural architectures, under the training from scratch protocol, on the challenging UCF101 dataset, surprisingly, with only around one percentage of parameters of its manual-design counterparts.



### Multi-Person tracking by multi-scale detection in Basketball scenarios
- **Arxiv ID**: http://arxiv.org/abs/1907.04637v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.04637v1)
- **Published**: 2019-07-10 11:56:35+00:00
- **Updated**: 2019-07-10 11:56:35+00:00
- **Authors**: Adrià Arbués-Sangüesa, Gloria Haro, Coloma Ballester
- **Comment**: Accepted in IMVIP 2019
- **Journal**: None
- **Summary**: Tracking data is a powerful tool for basketball teams in order to extract advanced semantic information and statistics that might lead to a performance boost. However, multi-person tracking is a challenging task to solve in single-camera video sequences, given the frequent occlusions and cluttering that occur in a restricted scenario. In this paper, a novel multi-scale detection method is presented, which is later used to extract geometric and content features, resulting in a multi-person video tracking system. Having built a dataset from scratch together with its ground truth (more than 10k bounding boxes), standard metrics are evaluated, obtaining notable results both in terms of detection (F1-score) and tracking (MOTA). The presented system could be used as a source of data gathering in order to extract useful statistics and semantic analyses a posteriori.



### One Shot Learning for Deformable Medical Image Registration and Periodic Motion Tracking
- **Arxiv ID**: http://arxiv.org/abs/1907.04641v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.04641v3)
- **Published**: 2019-07-10 12:05:19+00:00
- **Updated**: 2020-02-10 09:36:03+00:00
- **Authors**: Tobias Fechter, Dimos Baltas
- **Comment**: None
- **Journal**: None
- **Summary**: Deformable image registration is a very important field of research in medical imaging. Recently multiple deep learning approaches were published in this area showing promising results. However, drawbacks of deep learning methods are the need for a large amount of training datasets and their inability to register unseen images different from the training datasets. One shot learning comes without the need of large training datasets and has already been proven to be applicable to 3D data. In this work we present a one shot registration approach for periodic motion tracking in 3D and 4D datasets. When applied to 3D dataset the algorithm calculates the inverse of a registration vector field simultaneously. For registration we employed a U-Net combined with a coarse to fine approach and a differential spatial transformer module. The algorithm was thoroughly tested with multiple 4D and 3D datasets publicly available. The results show that the presented approach is able to track periodic motion and to yield a competitive registration accuracy. Possible applications are the use as a stand-alone algorithm for 3D and 4D motion tracking or in the beginning of studies until enough datasets for a separate training phase are available.



### A Projectional Ansatz to Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1907.04675v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.FA, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.04675v2)
- **Published**: 2019-07-10 12:49:07+00:00
- **Updated**: 2019-08-06 11:36:52+00:00
- **Authors**: Sören Dittmer, Peter Maass
- **Comment**: None
- **Journal**: None
- **Summary**: Recently the field of inverse problems has seen a growing usage of mathematically only partially understood learned and non-learned priors. Based on first principles, we develop a projectional approach to inverse problems that addresses the incorporation of these priors, while still guaranteeing data consistency. We implement this projectional method (PM) on the one hand via very general Plug-and-Play priors and on the other hand, via an end-to-end training approach. To this end, we introduce a novel alternating neural architecture, allowing for the incorporation of highly customized priors from data in a principled manner. We also show how the recent success of Regularization by Denoising (RED) can, at least to some extent, be explained as an approximation of the PM. Furthermore, we demonstrate how the idea can be applied to stop the degradation of Deep Image Prior (DIP) reconstructions over time.



### Domain Adaptation-based Augmentation for Weakly Supervised Nuclei Detection
- **Arxiv ID**: http://arxiv.org/abs/1907.04681v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.04681v1)
- **Published**: 2019-07-10 12:52:46+00:00
- **Updated**: 2019-07-10 12:52:46+00:00
- **Authors**: Nicolas Brieu, Armin Meier, Ansh Kapil, Ralf Schoenmeyer, Christos G. Gavriel, Peter D. Caie, Günter Schmidt
- **Comment**: None
- **Journal**: None
- **Summary**: The detection of nuclei is one of the most fundamental components of computational pathology. Current state-of-the-art methods are based on deep learning, with the prerequisite that extensive labeled datasets are available. The increasing number of patient cohorts to be analyzed, the diversity of tissue stains and indications, as well as the cost of dataset labeling motivates the development of novel methods to reduce labeling effort across domains. We introduce in this work a weakly supervised 'inter-domain' approach that (i) performs stain normalization and unpaired image-to-image translation to transform labeled images on a source domain to synthetic labeled images on an unlabeled target domain and (ii) uses the resulting synthetic labeled images to train a detection network on the target domain. Extensive experiments show the superiority of the proposed approach against the state-of-the-art 'intra-domain' detection based on fully-supervised learning.



### AVEC 2019 Workshop and Challenge: State-of-Mind, Detecting Depression with AI, and Cross-Cultural Affect Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.11510v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.IR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.11510v1)
- **Published**: 2019-07-10 13:41:42+00:00
- **Updated**: 2019-07-10 13:41:42+00:00
- **Authors**: Fabien Ringeval, Björn Schuller, Michel Valstar, NIcholas Cummins, Roddy Cowie, Leili Tavabi, Maximilian Schmitt, Sina Alisamir, Shahin Amiriparian, Eva-Maria Messner, Siyang Song, Shuo Liu, Ziping Zhao, Adria Mallol-Ragolta, Zhao Ren, Mohammad Soleymani, Maja Pantic
- **Comment**: None
- **Journal**: None
- **Summary**: The Audio/Visual Emotion Challenge and Workshop (AVEC 2019) "State-of-Mind, Detecting Depression with AI, and Cross-cultural Affect Recognition" is the ninth competition event aimed at the comparison of multimedia processing and machine learning methods for automatic audiovisual health and emotion analysis, with all participants competing strictly under the same conditions. The goal of the Challenge is to provide a common benchmark test set for multimodal information processing and to bring together the health and emotion recognition communities, as well as the audiovisual processing communities, to compare the relative merits of various approaches to health and emotion recognition from real-life data. This paper presents the major novelties introduced this year, the challenge guidelines, the data used, and the performance of the baseline systems on the three proposed tasks: state-of-mind recognition, depression assessment with AI, and cross-cultural affect sensing, respectively.



### Utilizing Eye Gaze to Enhance the Generalization of Imitation Networks to Unseen Environments
- **Arxiv ID**: http://arxiv.org/abs/1907.04728v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1907.04728v2)
- **Published**: 2019-07-10 13:55:05+00:00
- **Updated**: 2019-08-27 03:51:20+00:00
- **Authors**: Congcong Liu, Yuying Chen, Lei Tai, Ming Liu, Bertram Shi
- **Comment**: 4 pages, 3 figures, accepted by ICML 2019 Workshop on Understanding
  and Improving Generalization in Deep Learning
- **Journal**: None
- **Summary**: Vision-based autonomous driving through imitation learning mimics the behaviors of human drivers by training on pairs of data of raw driver-view images and actions. However, there are other cues, e.g. gaze behavior, available from human drivers that have yet to be exploited. Previous research has shown that novice human learners can benefit from observing experts' gaze patterns. We show here that deep neural networks can also benefit from this. We demonstrate different approaches to integrating gaze information into imitation networks. Our results show that the integration of gaze information improves the generalization performance of networks to unseen environments.



### SynthCity: A large scale synthetic point cloud
- **Arxiv ID**: http://arxiv.org/abs/1907.04758v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.04758v1)
- **Published**: 2019-07-10 14:43:56+00:00
- **Updated**: 2019-07-10 14:43:56+00:00
- **Authors**: David Griffiths, Jan Boehm
- **Comment**: 6 pages, 4 figures, dataset white paper
- **Journal**: None
- **Summary**: With deep learning becoming a more prominent approach for automatic classification of three-dimensional point cloud data, a key bottleneck is the amount of high quality training data, especially when compared to that available for two-dimensional images. One potential solution is the use of synthetic data for pre-training networks, however the ability for models to generalise from synthetic data to real world data has been poorly studied for point clouds. Despite this, a huge wealth of 3D virtual environments exist which, if proved effective can be exploited. We therefore argue that research in this domain would be of significant use. In this paper we present SynthCity an open dataset to help aid research. SynthCity is a 367.9M point synthetic full colour Mobile Laser Scanning point cloud. Every point is assigned a label from one of nine categories. We generate our point cloud in a typical Urban/Suburban environment using the Blensor plugin for Blender.



### Toward a Procedural Fruit Tree Rendering Framework for Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1907.04759v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1907.04759v1)
- **Published**: 2019-07-10 14:45:14+00:00
- **Updated**: 2019-07-10 14:45:14+00:00
- **Authors**: Thomas Duboudin, Maxime Petit, Liming Chen
- **Comment**: None
- **Journal**: 7th International Workshop on Image Analysis Methods in the Plant
  Sciences, Jul 2019, Lyon, France. pp.4 - 5
- **Summary**: We propose a procedural fruit tree rendering framework, based on Blender and Python scripts allowing to generate quickly labeled dataset (i.e. including ground truth semantic segmentation). It is designed to train image analysis deep learning methods (e.g. in a robotic fruit harvesting context), where real labeled training datasets are usually scarce and existing synthetic ones are too specialized. Moreover, the framework includes the possibility to introduce parametrized variations in the model (e.g. lightning conditions, background), producing a dataset with embedded Domain Randomization aspect.



### Towards Affordance Prediction with Vision via Task Oriented Grasp Quality Metrics
- **Arxiv ID**: http://arxiv.org/abs/1907.04761v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.04761v1)
- **Published**: 2019-07-10 14:45:37+00:00
- **Updated**: 2019-07-10 14:45:37+00:00
- **Authors**: Luca Cavalli, Gianpaolo Di Pietro, Matteo Matteucci
- **Comment**: 8 pages, presented at the Second International Workshop on
  Computational Models of Affordance in Robotics
- **Journal**: None
- **Summary**: While many quality metrics exist to evaluate the quality of a grasp by itself, no clear quantification of the quality of a grasp relatively to the task the grasp is used for has been defined yet. In this paper we propose a framework to extend the concept of grasp quality metric to task-oriented grasping by defining affordance functions via basic grasp metrics for an open set of task affordances. We evaluate both the effectivity of the proposed task oriented metrics and their practical applicability by learning to infer them from vision. Indeed, we assess the validity of our novel framework both in the context of perfect information, i.e., known object model, and in the partial information context, i.e., inferring task oriented metrics from vision, underlining advantages and limitations of both situations. In the former, physical metrics of grasp hypotheses on an object are defined and computed in known object model simulation, in the latter deep models are trained to infer such properties from partial information in the form of synthesized range images.



### Metamorphic Detection of Adversarial Examples in Deep Learning Models With Affine Transformations
- **Arxiv ID**: http://arxiv.org/abs/1907.04774v1
- **DOI**: 10.1109/MET.2019.00016
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.04774v1)
- **Published**: 2019-07-10 15:04:18+00:00
- **Updated**: 2019-07-10 15:04:18+00:00
- **Authors**: Rohan Reddy Mekala, Gudjon Einar Magnusson, Adam Porter, Mikael Lindvall, Madeline Diep
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial attacks are small, carefully crafted perturbations, imperceptible to the naked eye; that when added to an image cause deep learning models to misclassify the image with potentially detrimental outcomes. With the rise of artificial intelligence models in consumer safety and security intensive industries such as self-driving cars, camera surveillance and face recognition, there is a growing need for guarding against adversarial attacks. In this paper, we present an approach that uses metamorphic testing principles to automatically detect such adversarial attacks. The approach can detect image manipulations that are so small, that they are impossible to detect by a human through visual inspection. By applying metamorphic relations based on distance ratio preserving affine image transformations which compare the behavior of the original and transformed image; we show that our proposed approach can determine whether or not the input image is adversarial with a high degree of accuracy.



### Improving Prognostic Performance in Resectable Pancreatic Ductal Adenocarcinoma using Radiomics and Deep Learning Features Fusion in CT Images
- **Arxiv ID**: http://arxiv.org/abs/1907.04822v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.04822v1)
- **Published**: 2019-07-10 16:43:50+00:00
- **Updated**: 2019-07-10 16:43:50+00:00
- **Authors**: Yucheng Zhang, Edrise M. Lobo-Mueller, Paul Karanicolas, Steven Gallinger, Masoom A. Haider, Farzad Khalvati
- **Comment**: None
- **Journal**: None
- **Summary**: As an analytic pipeline for quantitative imaging feature extraction and analysis, radiomics has grown rapidly in the past a few years. Recent studies in radiomics aim to investigate the relationship between tumors imaging features and clinical outcomes. Open source radiomics feature banks enable the extraction and analysis of thousands of predefined features. On the other hand, recent advances in deep learning have shown significant potential in the quantitative medical imaging field, raising the research question of whether predefined radiomics features have predictive information in addition to deep learning features. In this study, we propose a feature fusion method and investigate whether a combined feature bank of deep learning and predefined radiomics features can improve the prognostics performance. CT images from resectable Pancreatic Adenocarcinoma (PDAC) patients were used to compare the prognosis performance of common feature reduction and fusion methods and the proposed risk-score based feature fusion method for overall survival. It was shown that the proposed feature fusion method significantly improves the prognosis performance for overall survival in resectable PDAC cohorts, elevating the area under ROC curve by 51% compared to predefined radiomics features alone, by 16% compared to deep learning features alone, and by 32% compared to existing feature fusion and reduction methods for a combination of deep learning and predefined radiomics features.



### Barnes-Hut Approximation for Point SetGeodesic Shooting
- **Arxiv ID**: http://arxiv.org/abs/1907.04834v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.04834v1)
- **Published**: 2019-07-10 17:32:07+00:00
- **Updated**: 2019-07-10 17:32:07+00:00
- **Authors**: Jiancong Wang, Long Xie, Paul Yushkevich, James Gee
- **Comment**: None
- **Journal**: None
- **Summary**: Geodesic shooting has been successfully applied to diffeo-morphic registration of point sets. Exact computation of the geodesicshooting between point sets, however, requiresO(N2) calculations each time step on the number of points in the point set. We proposean approximation approach based on the Barnes-Hut algorithm to speedup point set geodesic shooting. This approximation can reduce the al-gorithm complexity toO(N b+N logN). The evaluation of the proposedmethod in both simulated images and the medial temporal lobe thick-ness analysis demonstrates a comparable accuracy to the exact point set geodesic shooting while offering up to 3-fold speed up. This improvementopens up a range of clinical research studies and practical problems towhich the method can be effectively applied.



### Enhanced generative adversarial network for 3D brain MRI super-resolution
- **Arxiv ID**: http://arxiv.org/abs/1907.04835v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.04835v2)
- **Published**: 2019-07-10 17:32:28+00:00
- **Updated**: 2019-07-15 20:05:57+00:00
- **Authors**: Jiancong Wang, Yuhua Chen, Yifan Wu, Jianbo Shi, James Gee
- **Comment**: None
- **Journal**: None
- **Summary**: Single image super-resolution (SISR) reconstruction for magnetic resonance imaging (MRI) has generated significant interest because of its potential to not only speed up imaging but to improve quantitative processing and analysis of available image data. Generative Adversarial Networks (GAN) have proven to perform well in recovering image texture detail, and many variants have therefore been proposed for SISR. In this work, we develop an enhancement to tackle GAN-based 3D SISR by introducing a new residual-in-residual dense block (RRDG) generator that is both memory efficient and achieves state-of-the-art performance in terms of PSNR (Peak Signal to Noise Ratio), SSIM (Structural Similarity) and NRMSE (Normalized Root Mean Squared Error) metrics. We also introduce a patch GAN discriminator with improved convergence behavior to better model brain image texture. We proposed a novel the anatomical fidelity evaluation of the results using a pre-trained brain parcellation network. Finally, these developments are combined through a simple and efficient method to balance etween image and texture quality in the final output.



### Fast geodesic shooting for landmark matching using CUDA
- **Arxiv ID**: http://arxiv.org/abs/1907.04839v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.04839v1)
- **Published**: 2019-07-10 17:36:57+00:00
- **Updated**: 2019-07-10 17:36:57+00:00
- **Authors**: Jiancong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Landmark matching via geodesic shooting is a prerequisite task for numerous registration based applications in biomedicine. Geodesic shooting has been developed as one solution approach and formulates the diffeomorphic registration as an optimal control problem under the Hamiltonian framework. In this framework, with landmark positions q0 fixed, the problem solely depends on the initial momentum p0 and evolves through time steps according to a set of constraint equations. Given an initial p0, the algorithm flows q and p forward through time steps, calculates a loss based on point-set mismatch and kinetic energy, back-propagate through time to calculate gradient on p0 and update it. In the forward and backward pass, a pair-wise kernel on landmark points K and additional intermediate terms have to be calculated and marginalized, leading to O(N2) computational complexity, N being the number of points to be registered. For medical image applications, N maybe in the range of thousands, rendering this operation computationally expensive. In this work we ropose a CUDA implementation based on shared memory reduction. Our implementation achieves nearly 2 orders magnitude speed up compared to a naive CPU-based implementation, in addition to improved numerical accuracy as well as better registration results.



### Fully Convolutional Networks for Handwriting Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.04888v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.04888v1)
- **Published**: 2019-07-10 18:54:27+00:00
- **Updated**: 2019-07-10 18:54:27+00:00
- **Authors**: Felipe Petroski Such, Dheeraj Peri, Frank Brockler, Paul Hutkowski, Raymond Ptucha
- **Comment**: Published at International Conference on Frontiers in Handwriting
  Recognition
- **Journal**: None
- **Summary**: Handwritten text recognition is challenging because of the virtually infinite ways a human can write the same message. Our fully convolutional handwriting model takes in a handwriting sample of unknown length and outputs an arbitrary stream of symbols. Our dual stream architecture uses both local and global context and mitigates the need for heavy preprocessing steps such as symbol alignment correction as well as complex post processing steps such as connectionist temporal classification, dictionary matching or language models. Using over 100 unique symbols, our model is agnostic to Latin-based languages, and is shown to be quite competitive with state of the art dictionary based methods on the popular IAM and RIMES datasets. When a dictionary is known, we further allow a probabilistic character error rate to correct errant word blocks. Finally, we introduce an attention based mechanism which can automatically target variants of handwriting, such as slant, stroke width, or noise.



### Vision-and-Dialog Navigation
- **Arxiv ID**: http://arxiv.org/abs/1907.04957v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1907.04957v3)
- **Published**: 2019-07-10 23:41:46+00:00
- **Updated**: 2019-10-13 02:09:00+00:00
- **Authors**: Jesse Thomason, Michael Murray, Maya Cakmak, Luke Zettlemoyer
- **Comment**: Conference on Robot Learning (CoRL) 2019
- **Journal**: None
- **Summary**: Robots navigating in human environments should use language to ask for assistance and be able to understand human responses. To study this challenge, we introduce Cooperative Vision-and-Dialog Navigation, a dataset of over 2k embodied, human-human dialogs situated in simulated, photorealistic home environments. The Navigator asks questions to their partner, the Oracle, who has privileged access to the best next steps the Navigator should take according to a shortest path planner. To train agents that search an environment for a goal location, we define the Navigation from Dialog History task. An agent, given a target object and a dialog history between humans cooperating to find that object, must infer navigation actions towards the goal in unexplored environments. We establish an initial, multi-modal sequence-to-sequence model and demonstrate that looking farther back in the dialog history improves performance. Sourcecode and a live interface demo can be found at https://cvdn.dev/



