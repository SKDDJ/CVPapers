# Arxiv Papers in cs.CV on 2019-07-26
### Image Enhancement by Recurrently-trained Super-resolution Network
- **Arxiv ID**: http://arxiv.org/abs/1907.11341v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.11341v1)
- **Published**: 2019-07-26 00:30:36+00:00
- **Updated**: 2019-07-26 00:30:36+00:00
- **Authors**: Saem Park, Nojun Kwak
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a new learning strategy for image enhancement by recurrently training the same simple superresolution (SR) network multiple times. After initially training an SR network by using pairs of a corrupted low resolution (LR) image and an original image, the proposed method makes use of the trained SR network to generate new high resolution (HR) images with a doubled resolution from the original uncorrupted images. Then, the new HR images are downscaled to the original resolution, which work as target images for the SR network in the next stage. The newly generated HR images by the repeatedly trained SR network show better image quality and this strategy of training LR to mimic new HR can lead to a more efficient SR network. Up to a certain point, by repeating this process multiple times, better and better images are obtained. This recurrent leaning strategy for SR can be a good solution for downsizing convolution networks and making a more efficient SR network. To measure the enhanced image quality, for the first time in this area of super-resolution and image enhancement, we use VIQET MOS score which reflects human visual quality more accurately than the conventional MSE measure.



### Camera Distance-aware Top-down Approach for 3D Multi-person Pose Estimation from a Single RGB Image
- **Arxiv ID**: http://arxiv.org/abs/1907.11346v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11346v2)
- **Published**: 2019-07-26 01:02:45+00:00
- **Updated**: 2019-08-17 06:39:36+00:00
- **Authors**: Gyeongsik Moon, Ju Yong Chang, Kyoung Mu Lee
- **Comment**: Published at ICCV 2019
- **Journal**: None
- **Summary**: Although significant improvement has been achieved recently in 3D human pose estimation, most of the previous methods only treat a single-person case. In this work, we firstly propose a fully learning-based, camera distance-aware top-down approach for 3D multi-person pose estimation from a single RGB image. The pipeline of the proposed system consists of human detection, absolute 3D human root localization, and root-relative 3D single-person pose estimation modules. Our system achieves comparable results with the state-of-the-art 3D single-person pose estimation models without any groundtruth information and significantly outperforms previous 3D multi-person pose estimation methods on publicly available datasets.   The code is available in https://github.com/mks0601/3DMPPE_ROOTNET_RELEASE , https://github.com/mks0601/3DMPPE_POSENET_RELEASE.



### Learning Quintuplet Loss for Large-scale Visual Geo-Localization
- **Arxiv ID**: http://arxiv.org/abs/1907.11350v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11350v2)
- **Published**: 2019-07-26 01:08:30+00:00
- **Updated**: 2020-10-09 02:42:02+00:00
- **Authors**: Qiang Zhai
- **Comment**: None
- **Journal**: None
- **Summary**: With the maturity of Artificial Intelligence (AI) technology, Large Scale Visual Geo-Localization (LSVGL) is increasingly important in urban computing, where the task is to accurately and efficiently recognize the geo-location of a given query image. The main challenge of LSVGL faced by many experiments due to the appearance of real-word places may differ in various ways. While perspective deviation almost inevitably exists between training images and query images because of the arbitrary perspective. To cope with this situation, in this paper, we in-depth analyze the limitation of triplet loss which is the most commonly used metric learning loss in state-of-the-art LSVGL framework, and propose a new QUInTuplet Loss (QUITLoss) by embedding all the potential positive samples to the primitive triplet loss. Extensive experiments have been conducted to verify the effectiveness of the proposed approach and the results demonstrate that our new loss can enhance various LSVGL methods.



### Unifying Structure Analysis and Surrogate-driven Function Regression for Glaucoma OCT Image Screening
- **Arxiv ID**: http://arxiv.org/abs/1907.12927v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12927v1)
- **Published**: 2019-07-26 01:45:43+00:00
- **Updated**: 2019-07-26 01:45:43+00:00
- **Authors**: Xi Wang, Hao Chen, Luyang Luo, An-ran Ran, Poemen P. Chan, Clement C. Tham, Carol Y. Cheung, Pheng-Ann Heng
- **Comment**: 9 pages, 2 figures, MICCAI2019
- **Journal**: None
- **Summary**: Optical Coherence Tomography (OCT) imaging plays an important role in glaucoma diagnosis in clinical practice. Early detection and timely treatment can prevent glaucoma patients from permanent vision loss. However, only a dearth of automated methods has been developed based on OCT images for glaucoma study. In this paper, we present a novel framework to effectively classify glaucoma OCT images from normal ones. A semi-supervised learning strategy with smoothness assumption is applied for surrogate assignment of missing function regression labels. Besides, the proposed multi-task learning network is capable of exploring the structure and function relationship from the OCT image and visual field measurement simultaneously, which contributes to classification performance boosting. Essentially, we are the first to unify the structure analysis and function regression for glaucoma screening. It is also worth noting that we build the largest glaucoma OCT image dataset involving 4877 volumes to develop and evaluate the proposed method. Extensive experiments demonstrate that our framework outperforms the baseline methods and two glaucoma experts by a large margin, achieving 93.2%, 93.2% and 97.8% on accuracy, F1 score and AUC, respectively.



### DABNet: Depth-wise Asymmetric Bottleneck for Real-time Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.11357v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11357v2)
- **Published**: 2019-07-26 01:50:31+00:00
- **Updated**: 2019-10-01 01:29:58+00:00
- **Authors**: Gen Li, Inyoung Yun, Jonghyun Kim, Joongkyu Kim
- **Comment**: Accepted to BMVC 2019
- **Journal**: None
- **Summary**: As a pixel-level prediction task, semantic segmentation needs large computational cost with enormous parameters to obtain high performance. Recently, due to the increasing demand for autonomous systems and robots, it is significant to make a tradeoff between accuracy and inference speed. In this paper, we propose a novel Depthwise Asymmetric Bottleneck (DAB) module to address this dilemma, which efficiently adopts depth-wise asymmetric convolution and dilated convolution to build a bottleneck structure. Based on the DAB module, we design a Depth-wise Asymmetric Bottleneck Network (DABNet) especially for real-time semantic segmentation, which creates sufficient receptive field and densely utilizes the contextual information. Experiments on Cityscapes and CamVid datasets demonstrate that the proposed DABNet achieves a balance between speed and precision. Specifically, without any pretrained model and postprocessing, it achieves 70.1% Mean IoU on the Cityscapes test dataset with only 0.76 million parameters and a speed of 104 FPS on a single GTX 1080Ti card.



### Improved Super-Resolution Convolution Neural Network for Large Images
- **Arxiv ID**: http://arxiv.org/abs/1907.12928v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.12928v1)
- **Published**: 2019-07-26 02:44:07+00:00
- **Updated**: 2019-07-26 02:44:07+00:00
- **Authors**: Junyu, Wang, Rong Song
- **Comment**: None
- **Journal**: None
- **Summary**: Single image super-resolution (SISR) is a very popular topic nowadays, which has both research value and practical value. In daily life, we crop a large image into sub-images to do super-resolution and then merge them together. Although convolution neural network performs very well in the research field, if we use it to do super-resolution, we can easily observe cutting lines from merged pictures. To address these problems, in this paper, we propose a refined architecture of SRCNN with 'Symmetric padding', 'Random learning' and 'Residual learning'. Moreover, we have done a lot of experiments to prove our model performs best among a lot of the state-of-art methods.



### MVB: A Large-Scale Dataset for Baggage Re-Identification and Merged Siamese Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.11366v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11366v1)
- **Published**: 2019-07-26 02:47:43+00:00
- **Updated**: 2019-07-26 02:47:43+00:00
- **Authors**: Zhulin Zhang, Dong Li, Jinhua Wu, Yunda Sun, Li Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel dataset named MVB (Multi View Baggage) for baggage ReID task which has some essential differences from person ReID. The features of MVB are three-fold. First, MVB is the first publicly released large-scale dataset that contains 4519 baggage identities and 22660 annotated baggage images as well as its surface material labels. Second, all baggage images are captured by specially-designed multi-view camera system to handle pose variation and occlusion, in order to obtain the 3D information of baggage surface as complete as possible. Third, MVB has remarkable inter-class similarity and intra-class dissimilarity, considering the fact that baggage might have very similar appearance while the data is collected in two real airport environments, where imaging factors varies significantly from each other. Moreover, we proposed a merged Siamese network as baseline model and evaluated its performance. Experiments and case study are conducted on MVB.



### BSUV-Net: A Fully-Convolutional Neural Network for Background Subtraction of Unseen Videos
- **Arxiv ID**: http://arxiv.org/abs/1907.11371v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11371v2)
- **Published**: 2019-07-26 03:05:00+00:00
- **Updated**: 2020-01-14 16:30:38+00:00
- **Authors**: M. Ozan Tezcan, Prakash Ishwar, Janusz Konrad
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Background subtraction is a basic task in computer vision and video processing often applied as a pre-processing step for object tracking, people recognition, etc. Recently, a number of successful background-subtraction algorithms have been proposed, however nearly all of the top-performing ones are supervised. Crucially, their success relies upon the availability of some annotated frames of the test video during training. Consequently, their performance on completely "unseen" videos is undocumented in the literature. In this work, we propose a new, supervised, background-subtraction algorithm for unseen videos (BSUV-Net) based on a fully-convolutional neural network. The input to our network consists of the current frame and two background frames captured at different time scales along with their semantic segmentation maps. In order to reduce the chance of overfitting, we also introduce a new data-augmentation technique which mitigates the impact of illumination difference between the background frames and the current frame. On the CDNet-2014 dataset, BSUV-Net outperforms state-of-the-art algorithms evaluated on unseen videos in terms of several metrics including F-measure, recall and precision.



### Unsupervised Learning Framework of Interest Point Via Properties Optimization
- **Arxiv ID**: http://arxiv.org/abs/1907.11375v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11375v1)
- **Published**: 2019-07-26 03:31:27+00:00
- **Updated**: 2019-07-26 03:31:27+00:00
- **Authors**: Pei Yan, Yihua Tan, Yuan Xiao, Yuan Tai, Cai Wen
- **Comment**: 16 pages, 6 figures, 5 tables
- **Journal**: None
- **Summary**: This paper presents an entirely unsupervised interest point training framework by jointly learning detector and descriptor, which takes an image as input and outputs a probability and a description for every image point. The objective of the training framework is formulated as joint probability distribution of the properties of the extracted points. The essential properties are selected as sparsity, repeatability and discriminability which are formulated by the probabilities. To maximize the objective efficiently, latent variable is introduced to represent the probability of that a point satisfies the required properties. Therefore, original maximization can be optimized with Expectation Maximization algorithm (EM). Considering high computation cost of EM on large scale image set, we implement the optimization process with an efficient strategy as Mini-Batch approximation of EM (MBEM). In the experiments both detector and descriptor are instantiated with fully convolutional network which is named as Property Network (PN). The experiments demonstrate that PN outperforms state-of-the-art methods on a number of image matching benchmarks without need of retraining. PN also reveals that the proposed training framework has high flexibility to adapt to diverse types of scenes.



### Distill-to-Label: Weakly Supervised Instance Labeling Using Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/1907.12926v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.12926v1)
- **Published**: 2019-07-26 04:39:17+00:00
- **Updated**: 2019-07-26 04:39:17+00:00
- **Authors**: Jayaraman J. Thiagarajan, Satyananda Kashyap, Alexandros Karagyris
- **Comment**: None
- **Journal**: None
- **Summary**: Weakly supervised instance labeling using only image-level labels, in lieu of expensive fine-grained pixel annotations, is crucial in several applications including medical image analysis. In contrast to conventional instance segmentation scenarios in computer vision, the problems that we consider are characterized by a small number of training images and non-local patterns that lead to the diagnosis. In this paper, we explore the use of multiple instance learning (MIL) to design an instance label generator under this weakly supervised setting. Motivated by the observation that an MIL model can handle bags of varying sizes, we propose to repurpose an MIL model originally trained for bag-level classification to produce reliable predictions for single instances, i.e., bags of size $1$. To this end, we introduce a novel regularization strategy based on virtual adversarial training for improving MIL training, and subsequently develop a knowledge distillation technique for repurposing the trained MIL model. Using empirical studies on colon cancer and breast cancer detection from histopathological images, we show that the proposed approach produces high-quality instance-level prediction and significantly outperforms state-of-the MIL methods.



### Product Image Recognition with Guidance Learning and Noisy Supervision
- **Arxiv ID**: http://arxiv.org/abs/1907.11384v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11384v1)
- **Published**: 2019-07-26 05:13:13+00:00
- **Updated**: 2019-07-26 05:13:13+00:00
- **Authors**: Qing Li, Xiaojiang Peng, Liangliang Cao, Wenbin Du, Hao Xing, Yu Qiao
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: This paper considers recognizing products from daily photos, which is an important problem in real-world applications but also challenging due to background clutters, category diversities, noisy labels, etc. We address this problem by two contributions. First, we introduce a novel large-scale product image dataset, termed as Product-90. Instead of collecting product images by labor-and time-intensive image capturing, we take advantage of the web and download images from the reviews of several e-commerce websites where the images are casually captured by consumers. Labels are assigned automatically by the categories of e-commerce websites. Totally the Product-90 consists of more than 140K images with 90 categories. Due to the fact that consumers may upload unrelated images, it is inevitable that our Product-90 introduces noisy labels. As the second contribution, we develop a simple yet efficient \textit{guidance learning} (GL) method for training convolutional neural networks (CNNs) with noisy supervision. The GL method first trains an initial teacher network with the full noisy dataset, and then trains a target/student network with both large-scale noisy set and small manually-verified clean set in a multi-task manner. Specifically, in the stage of student network training, the large-scale noisy data is supervised by its guidance knowledge which is the combination of its given noisy label and the soften label from the teacher network. We conduct extensive experiments on our Products-90 and public datasets, namely Food101, Food-101N, and Clothing1M. Our guidance learning method achieves performance superior to state-of-the-art methods on these datasets.



### Automatic Calcium Scoring in Cardiac and Chest CT Using DenseRAUnet
- **Arxiv ID**: http://arxiv.org/abs/1907.11392v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.11392v1)
- **Published**: 2019-07-26 06:30:44+00:00
- **Updated**: 2019-07-26 06:30:44+00:00
- **Authors**: Jiechao Ma, Rongguo Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Cardiovascular disease (CVD) is a common and strong threat to human beings, featuring high prevalence, disability and mortality. The amount of coronary artery calcification (CAC) is an effective factor for CVD risk evaluation. Conventionally, CAC is quantified using ECG-synchronized cardiac CT but rarely from general chest CT scans. However, compared with ECG-synchronized cardiac CT, chest CT is more prevalent and economical in clinical practice. To address this, we propose an automatic method based on Dense U-Net to segment coronary calcium pixels on both types of CT scans. Our contribution is two-fold. First, we propose a novel network called DenseRAUnet, which takes advantage of Dense U-net, ResNet and atrous convolutions. We prove the robustness and generalizability of our model by training it exclusively on chest CT while test on both types of CT scans. Second, we design a loss function combining bootstrap with IoU function to balance foreground and background classes. DenseRAUnet is trained in a 2.5D fashion and tested on a private dataset consisting of 144 scans. Results show an F1-score of 0.75, with 0.83 accuracy of predicting cardiovascular disease risk.



### A Comparative Study of High-Recall Real-Time Semantic Segmentation Based on Swift Factorized Network
- **Arxiv ID**: http://arxiv.org/abs/1907.11394v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11394v1)
- **Published**: 2019-07-26 06:36:18+00:00
- **Updated**: 2019-07-26 06:36:18+00:00
- **Authors**: Kaite Xiang, Kaiwei Wang, Kailun Yang
- **Comment**: 14 pages, 11figures, SPIE Security + Defence 2019
- **Journal**: None
- **Summary**: Semantic Segmentation (SS) is the task to assign a semantic label to each pixel of the observed images, which is of crucial significance for autonomous vehicles, navigation assistance systems for the visually impaired, and augmented reality devices. However, there is still a long way for SS to be put into practice as there are two essential challenges that need to be addressed: efficiency and evaluation criterions for practical application. For specific application scenarios, different criterions need to be adopted. Recall rate is an important criterion for many tasks like autonomous vehicles. For autonomous vehicles, we need to focus on the detection of the traffic objects like cars, buses, and pedestrians, which should be detected with high recall rates. In other words, it is preferable to detect it wrongly than miss it, because the other traffic objects will be dangerous if the algorithm miss them and segment them as safe roadways. In this paper, our main goal is to explore possible methods to attain high recall rate. Firstly, we propose a real-time SS network named Swift Factorized Network (SFN). The proposed network is adapted from SwiftNet, whose structure is a typical U-shape structure with lateral connections. Inspired by ERFNet and Global convolution Networks (GCNet), we propose two different blocks to enlarge valid receptive field. They do not take up too much calculation resources, but significantly enhance the performance compared with the baseline network. Secondly, we explore three ways to achieve higher recall rate, i.e. loss function, classifier and decision rules. We perform a comprehensive set of experiments on state-of-the-art datasets including CamVid and Cityscapes. We demonstrate that our SS convolutional neural networks reach excellent performance. Furthermore, we make a detailed analysis and comparison of the three proposed methods on the promotion of recall rate.



### Improving Generalization via Attribute Selection on Out-of-the-box Data
- **Arxiv ID**: http://arxiv.org/abs/1907.11397v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11397v2)
- **Published**: 2019-07-26 06:57:27+00:00
- **Updated**: 2019-10-28 05:28:53+00:00
- **Authors**: Xiaofeng Xu, Ivor W. Tsang, Chuancai Liu
- **Comment**: Accepted by Neural Computation
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) aims to recognize unseen objects (test classes) given some other seen objects (training classes), by sharing information of attributes between different objects. Attributes are artificially annotated for objects and treated equally in recent ZSL tasks. However, some inferior attributes with poor predictability or poor discriminability may have negative impacts on the ZSL system performance. This paper first derives a generalization error bound for ZSL tasks. Our theoretical analysis verifies that selecting the subset of key attributes can improve the generalization performance of the original ZSL model, which utilizes all the attributes. Unfortunately, previous attribute selection methods are conducted based on the seen data, and their selected attributes have poor generalization capability to the unseen data, which is unavailable in the training stage of ZSL tasks. Inspired by learning from pseudo relevance feedback, this paper introduces the out-of-the-box data, which is pseudo data generated by an attribute-guided generative model, to mimic the unseen data. After that, we present an iterative attribute selection (IAS) strategy which iteratively selects key attributes based on the out-of-the-box data. Since the distribution of the generated out-of-the-box data is similar to the test data, the key attributes selected by IAS can be effectively generalized to test data. Extensive experiments demonstrate that IAS can significantly improve existing attribute-based ZSL methods and achieve state-of-the-art performance.



### UGAN: Untraceable GAN for Multi-Domain Face Translation
- **Arxiv ID**: http://arxiv.org/abs/1907.11418v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11418v2)
- **Published**: 2019-07-26 08:05:13+00:00
- **Updated**: 2019-09-12 02:23:45+00:00
- **Authors**: Defa Zhu, Si Liu, Wentao Jiang, Chen Gao, Tianyi Wu, Qaingchang Wang, Guodong Guo
- **Comment**: None
- **Journal**: None
- **Summary**: The multi-domain image-to-image translation is a challenging task where the goal is to translate an image into multiple different domains. The target-only characteristics are desired for translated images, while the source-only characteristics should be erased. However, recent methods often suffer from retaining the characteristics of the source domain, which are incompatible with the target domain. To address this issue, we propose a method called Untraceable GAN, which has a novel source classifier to differentiate which domain an image is translated from, and determines whether the translated image still retains the characteristics of the source domain. Furthermore, we take the prototype of the target domain as the guidance for the translator to effectively synthesize the target-only characteristics. The translator is learned to synthesize the target-only characteristics and make the source domain untraceable for the discriminator, so that the source-only characteristics are erased. Finally, extensive experiments on three face editing tasks, including face aging, makeup, and expression editing, show that the proposed UGAN can produce superior results over the state-of-the-art models. The source code will be released.



### Deep MRI Reconstruction: Unrolled Optimization Algorithms Meet Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.11711v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, eess.SP, physics.med-ph, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.11711v1)
- **Published**: 2019-07-26 08:22:53+00:00
- **Updated**: 2019-07-26 08:22:53+00:00
- **Authors**: Dong Liang, Jing Cheng, Ziwen Ke, Leslie Ying
- **Comment**: a review paper on deep learning MR reconstruction
- **Journal**: None
- **Summary**: Image reconstruction from undersampled k-space data has been playing an important role for fast MRI. Recently, deep learning has demonstrated tremendous success in various fields and also shown potential to significantly speed up MR reconstruction with reduced measurements. This article gives an overview of deep learning-based image reconstruction methods for MRI. Three types of deep learning-based approaches are reviewed, the data-driven, model-driven and integrated approaches. The main structure of each network in three approaches is explained and the analysis of common parts of reviewed networks and differences in-between are highlighted. Based on the review, a number of signal processing issues are discussed for maximizing the potential of deep reconstruction for fast MRI. the discussion may facilitate further development of "optimal" network and performance analysis from a theoretical point of view.



### Look Further to Recognize Better: Learning Shared Topics and Category-Specific Dictionaries for Open-Ended 3D Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.12924v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1907.12924v1)
- **Published**: 2019-07-26 08:28:49+00:00
- **Updated**: 2019-07-26 08:28:49+00:00
- **Authors**: S. Hamidreza Kasaei
- **Comment**: arXiv admin note: text overlap with arXiv:1902.03057
- **Journal**: None
- **Summary**: Service robots are expected to operate effectively in human-centric environments for long periods of time. In such realistic scenarios, fine-grained object categorization is as important as basic-level object categorization. We tackle this problem by proposing an open-ended object recognition approach which concurrently learns both the object categories and the local features for encoding objects. In this work, each object is represented using a set of general latent visual topics and category-specific dictionaries. The general topics encode the common patterns of all categories, while the category-specific dictionary describes the content of each category in details. The proposed approach discovers both sets of general and specific representations in an unsupervised fashion and updates them incrementally using new object views. Experimental results show that our approach yields significant improvements over the previous state-of-the-art approaches concerning scalability and object classification performance. Moreover, our approach demonstrates the capability of learning from very few training examples in a real-world setting. Regarding computation time, the best result was obtained with a Bag-of-Words method followed by a variant of the Latent Dirichlet Allocation approach.



### Exploiting the Redundancy in Convolutional Filters for Parameter Reduction
- **Arxiv ID**: http://arxiv.org/abs/1907.11432v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11432v3)
- **Published**: 2019-07-26 08:39:31+00:00
- **Updated**: 2020-08-10 16:53:03+00:00
- **Authors**: Kumara Kahatapitiya, Ranga Rodrigo
- **Comment**: Accepted to be published in Proceedings of IEEE Winter Conference on
  Applications of Computer Vision (WACV) 2021
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance in many computer vision tasks over the years. However, this comes at the cost of heavy computation and memory intensive network designs, suggesting potential improvements in efficiency. Convolutional layers of CNNs partly account for such an inefficiency, as they are known to learn redundant features. In this work, we exploit this redundancy, observing it as the correlation between convolutional filters of a layer, and propose an alternative approach to reproduce it efficiently. The proposed 'LinearConv' layer learns a set of orthogonal filters, and a set of coefficients that linearly combines them to introduce a controlled redundancy. We introduce a correlation-based regularization loss to achieve such flexibility over redundancy, and control the number of parameters in turn. This is designed as a plug-and-play layer to conveniently replace a conventional convolutional layer, without any additional changes required in the network architecture or the hyperparameter settings. Our experiments verify that LinearConv models achieve a performance on-par with their counterparts, with almost a 50% reduction in parameters on average, and the same computational requirement and speed at inference.



### Semantic Deep Intermodal Feature Transfer: Transferring Feature Descriptors Between Imaging Modalities
- **Arxiv ID**: http://arxiv.org/abs/1907.11436v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.11436v1)
- **Published**: 2019-07-26 08:42:38+00:00
- **Updated**: 2019-07-26 08:42:38+00:00
- **Authors**: Sebastian P. Kleinschmidt, Bernardo Wagner
- **Comment**: None
- **Journal**: None
- **Summary**: Under difficult environmental conditions, the view of RGB cameras may be restricted by fog, dust or difficult lighting situations. Because thermal cameras visualize thermal radiation, they are not subject to the same limitations as RGB cameras. However, because RGB and thermal imaging differ significantly in appearance, common, state-of-the-art feature descriptors are unsuitable for intermodal feature matching between these imaging modalities. As a consequence, visual maps created with an RGB camera can currently not be used for localization using a thermal camera. In this paper, we introduce the Semantic Deep Intermodal Feature Transfer (Se-DIFT), an approach for transferring image feature descriptors from the visual to the thermal spectrum and vice versa. For this purpose, we predict potential feature appearance in varying imaging modalities using a deep convolutional encoder-decoder architecture in combination with a global feature vector. Since the representation of a thermal image is not only affected by features which can be extracted from an RGB image, we introduce the global feature vector which augments the auto encoder's coding. The global feature vector contains additional information about the thermal history of a scene which is automatically extracted from external data sources. By augmenting the encoder's coding, we decrease the L1 error of the prediction by more than 7% compared to the prediction of a traditional U-Net architecture. To evaluate our approach, we match image feature descriptors detected in RGB and thermal images using Se-DIFT. Subsequently, we make a competitive comparison on the intermodal transferability of SIFT, SURF, and ORB features using our approach.



### Universal Pooling -- A New Pooling Method for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.11440v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11440v1)
- **Published**: 2019-07-26 09:00:00+00:00
- **Updated**: 2019-07-26 09:00:00+00:00
- **Authors**: Junhyuk Hyun, Hongje Seong, Euntai Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Pooling is one of the main elements in convolutional neural networks. The pooling reduces the size of the feature map, enabling training and testing with a limited amount of computation. This paper proposes a new pooling method named universal pooling. Unlike the existing pooling methods such as average pooling, max pooling, and stride pooling with fixed pooling function, universal pooling generates any pooling function, depending on a given problem and dataset. Universal pooling was inspired by attention methods and can be considered as a channel-wise form of local spatial attention. Universal pooling is trained jointly with the main network and it is shown that it includes the existing pooling methods. Finally, when applied to two benchmark problems, the proposed method outperformed the existing pooling methods and performed with the expected diversity, adapting to the given problem.



### A bisector line field approach to interpolation of orientation fields
- **Arxiv ID**: http://arxiv.org/abs/1907.11449v4
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.NA
- **Links**: [PDF](http://arxiv.org/pdf/1907.11449v4)
- **Published**: 2019-07-26 09:18:36+00:00
- **Updated**: 2020-09-21 08:23:48+00:00
- **Authors**: Nicolas Boizot, Ludovic Sacchelli
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an approach to the problem of global reconstruction of an orientation field. The method is based on a geometric model called "bisector line fields", which maps a pair of vector fields to an orientation field, effectively generalizing the notion of doubling phase vector fields. Endowed with a well chosen energy minimization problem, we provide a polynomial interpolation of a target orientation field while bypassing the doubling phase step. The procedure is then illustrated with examples from fingerprint analysis.



### Using 3D Convolutional Neural Networks to Learn Spatiotemporal Features for Automatic Surgical Gesture Recognition in Video
- **Arxiv ID**: http://arxiv.org/abs/1907.11454v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.11454v1)
- **Published**: 2019-07-26 09:34:09+00:00
- **Updated**: 2019-07-26 09:34:09+00:00
- **Authors**: Isabel Funke, Sebastian Bodenstedt, Florian Oehme, Felix von Bechtolsheim, Jürgen Weitz, Stefanie Speidel
- **Comment**: Accepted at MICCAI 2019. Source code will be made available
- **Journal**: None
- **Summary**: Automatically recognizing surgical gestures is a crucial step towards a thorough understanding of surgical skill. Possible areas of application include automatic skill assessment, intra-operative monitoring of critical surgical steps, and semi-automation of surgical tasks. Solutions that rely only on the laparoscopic video and do not require additional sensor hardware are especially attractive as they can be implemented at low cost in many scenarios. However, surgical gesture recognition based only on video is a challenging problem that requires effective means to extract both visual and temporal information from the video. Previous approaches mainly rely on frame-wise feature extractors, either handcrafted or learned, which fail to capture the dynamics in surgical video. To address this issue, we propose to use a 3D Convolutional Neural Network (CNN) to learn spatiotemporal features from consecutive video frames. We evaluate our approach on recordings of robot-assisted suturing on a bench-top model, which are taken from the publicly available JIGSAWS dataset. Our approach achieves high frame-wise surgical gesture recognition accuracies of more than 84%, outperforming comparable models that either extract only spatial features or model spatial and low-level temporal information separately. For the first time, these results demonstrate the benefit of spatiotemporal CNNs for video-based surgical gesture recognition.



### Multiple Human Association between Top and Horizontal Views by Matching Subjects' Spatial Distributions
- **Arxiv ID**: http://arxiv.org/abs/1907.11458v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.11458v1)
- **Published**: 2019-07-26 09:47:17+00:00
- **Updated**: 2019-07-26 09:47:17+00:00
- **Authors**: Ruize Han, Yujun Zhang, Wei Feng, Chenxing Gong, Xiaoyu Zhang, Jiewen Zhao, Liang Wan, Song Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Video surveillance can be significantly enhanced by using both top-view data, e.g., those from drone-mounted cameras in the air, and horizontal-view data, e.g., those from wearable cameras on the ground. Collaborative analysis of different-view data can facilitate various kinds of applications, such as human tracking, person identification, and human activity recognition. However, for such collaborative analysis, the first step is to associate people, referred to as subjects in this paper, across these two views. This is a very challenging problem due to large human-appearance difference between top and horizontal views. In this paper, we present a new approach to address this problem by exploring and matching the subjects' spatial distributions between the two views. More specifically, on the top-view image, we model and match subjects' relative positions to the horizontal-view camera in both views and define a matching cost to decide the actual location of horizontal-view camera and its view angle in the top-view image. We collect a new dataset consisting of top-view and horizontal-view image pairs for performance evaluation and the experimental results show the effectiveness of the proposed method.



### Preterm infants' limb-pose estimation from depth images using convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1907.12949v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12949v1)
- **Published**: 2019-07-26 10:15:57+00:00
- **Updated**: 2019-07-26 10:15:57+00:00
- **Authors**: Sara Moccia, Lucia Migliorelli, Rocco Pietrini, Emanuele Frontoni
- **Comment**: None
- **Journal**: None
- **Summary**: Preterm infants' limb-pose estimation is a crucial but challenging task, which may improve patients' care and facilitate clinicians in infant's movements monitoring. Work in the literature either provides approaches to whole-body segmentation and tracking, which, however, has poor clinical value, or retrieve a posteriori limb pose from limb segmentation, increasing computational costs and introducing inaccuracy sources. In this paper, we address the problem of limb-pose estimation under a different point of view. We proposed a 2D fully-convolutional neural network for roughly detecting limb joints and joint connections, followed by a regression convolutional neural network for accurate joint and joint-connection position estimation. Joints from the same limb are then connected with a maximum bipartite matching approach. Our analysis does not require any prior modeling of infants' body structure, neither any manual interventions. For developing and testing the proposed approach, we built a dataset of four videos (video length = 90 s) recorded with a depth sensor in a neonatal intensive care unit (NICU) during the actual clinical practice, achieving median root mean square distance [pixels] of 10.790 (right arm), 10.542 (left arm), 8.294 (right leg), 11.270 (left leg) with respect to the ground-truth limb pose. The idea of estimating limb pose directly from depth images may represent a future paradigm for addressing the problem of preterm-infants' movement monitoring and offer all possible support to clinicians in NICUs.



### Context-Integrated and Feature-Refined Network for Lightweight Object Parsing
- **Arxiv ID**: http://arxiv.org/abs/1907.11474v3
- **DOI**: 10.1109/TIP.2020.2978583
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11474v3)
- **Published**: 2019-07-26 10:50:30+00:00
- **Updated**: 2020-02-25 04:12:16+00:00
- **Authors**: Bin Jiang, Wenxuan Tu, Chao Yang, Junsong Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation for lightweight object parsing is a very challenging task, because both accuracy and efficiency (e.g., execution speed, memory footprint or computational complexity) should all be taken into account. However, most previous works pay too much attention to one-sided perspective, either accuracy or speed, and ignore others, which poses a great limitation to actual demands of intelligent devices. To tackle this dilemma, we propose a novel lightweight architecture named Context-Integrated and Feature-Refined Network (CIFReNet). The core components of CIFReNet are the Long-skip Refinement Module (LRM) and the Multi-scale Context Integration Module (MCIM). The LRM is designed to ease the propagation of spatial information between low-level and high-level stages. Furthermore, channel attention mechanism is introduced into the process of long-skip learning to boost the quality of low-level feature refinement. Meanwhile, the MCIM consists of three cascaded Dense Semantic Pyramid (DSP) blocks with image-level features, which is presented to encode multiple context information and enlarge the field of view. Specifically, the proposed DSP block exploits a dense feature sampling strategy to enhance the information representations without significantly increasing the computation cost. Comprehensive experiments are conducted on three benchmark datasets for object parsing including Cityscapes, CamVid, and Helen. As indicated, the proposed method reaches a better trade-off between accuracy and efficiency compared with the other state-of-the-art methods.



### Single Level Feature-to-Feature Forecasting with Deformable Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1907.11475v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11475v1)
- **Published**: 2019-07-26 10:51:24+00:00
- **Updated**: 2019-07-26 10:51:24+00:00
- **Authors**: Josip Šarić, Marin Oršić, Tonći Antunović, Sacha Vražić, Siniša Šegvić
- **Comment**: Accepted to German Conference on Pattern Recognition 2019. 19 pages,
  8 figures, 7 tables
- **Journal**: None
- **Summary**: Future anticipation is of vital importance in autonomous driving and other decision-making systems. We present a method to anticipate semantic segmentation of future frames in driving scenarios based on feature-to-feature forecasting. Our method is based on a semantic segmentation model without lateral connections within the upsampling path. Such design ensures that the forecasting addresses only the most abstract features on a very coarse resolution. We further propose to express feature-to-feature forecasting with deformable convolutions. This increases the modelling power due to being able to represent different motion patterns within a single feature map. Experiments show that our models with deformable convolutions outperform their regular and dilated counterparts while minimally increasing the number of parameters. Our method achieves state of the art performance on the Cityscapes validation set when forecasting nine timesteps into the future.



### Annotation-Free Cardiac Vessel Segmentation via Knowledge Transfer from Retinal Images
- **Arxiv ID**: http://arxiv.org/abs/1907.11483v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.11483v1)
- **Published**: 2019-07-26 11:08:32+00:00
- **Updated**: 2019-07-26 11:08:32+00:00
- **Authors**: Fei Yu, Jie Zhao, Yanjun Gong, Zhi Wang, Yuxi Li, Fan Yang, Bin Dong, Quanzheng Li, Li Zhang
- **Comment**: Accepted at MICCAI 2019
- **Journal**: None
- **Summary**: Segmenting coronary arteries is challenging, as classic unsupervised methods fail to produce satisfactory results and modern supervised learning (deep learning) requires manual annotation which is often time-consuming and can some time be infeasible. To solve this problem, we propose a knowledge transfer based shape-consistent generative adversarial network (SC-GAN), which is an annotation-free approach that uses the knowledge from publicly available annotated fundus dataset to segment coronary arteries. The proposed network is trained in an end-to-end fashion, generating and segmenting synthetic images that maintain the background of coronary angiography and preserve the vascular structures of retinal vessels and coronary arteries. We train and evaluate the proposed model on a dataset of 1092 digital subtraction angiography images, and experiments demonstrate the supreme accuracy of the proposed method on coronary arteries segmentation.



### Multi-level Domain Adaptive learning for Cross-Domain Detection
- **Arxiv ID**: http://arxiv.org/abs/1907.11484v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11484v2)
- **Published**: 2019-07-26 11:09:24+00:00
- **Updated**: 2019-08-09 05:32:19+00:00
- **Authors**: Rongchang Xie, Fei Yu, Jiachao Wang, Yizhou Wang, Li Zhang
- **Comment**: Accepted to the TASK-CV workshop at ICCV 2019
- **Journal**: None
- **Summary**: In recent years, object detection has shown impressive results using supervised deep learning, but it remains challenging in a cross-domain environment. The variations of illumination, style, scale, and appearance in different domains can seriously affect the performance of detection models. Previous works use adversarial training to align global features across the domain shift and to achieve image information transfer. However, such methods do not effectively match the distribution of local features, resulting in limited improvement in cross-domain object detection. To solve this problem, we propose a multi-level domain adaptive model to simultaneously align the distributions of local-level features and global-level features. We evaluate our method with multiple experiments, including adverse weather adaptation, synthetic data adaptation, and cross camera adaptation. In most object categories, the proposed method achieves superior performance against state-of-the-art techniques, which demonstrates the effectiveness and robustness of our method.



### Outfit Compatibility Prediction and Diagnosis with Multi-Layered Comparison Network
- **Arxiv ID**: http://arxiv.org/abs/1907.11496v2
- **DOI**: 10.1145/3343031.3350909
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1907.11496v2)
- **Published**: 2019-07-26 11:39:15+00:00
- **Updated**: 2019-08-22 03:56:30+00:00
- **Authors**: Xin Wang, Bo Wu, Yun Ye, Yueqi Zhong
- **Comment**: 9 pages, 6 figures, Proceedings of the 27th ACM International
  Conference on Multimedia
- **Journal**: None
- **Summary**: Existing works about fashion outfit compatibility focus on predicting the overall compatibility of a set of fashion items with their information from different modalities. However, there are few works explore how to explain the prediction, which limits the persuasiveness and effectiveness of the model. In this work, we propose an approach to not only predict but also diagnose the outfit compatibility. We introduce an end-to-end framework for this goal, which features for: (1) The overall compatibility is learned from all type-specified pairwise similarities between items, and the backpropagation gradients are used to diagnose the incompatible factors. (2) We leverage the hierarchy of CNN and compare the features at different layers to take into account the compatibilities of different aspects from the low level (such as color, texture) to the high level (such as style). To support the proposed method, we build a new type-specified outfit dataset named Polyvore-T based on Polyvore dataset. We compare our method with the prior state-of-the-art in two tasks: outfit compatibility prediction and fill-in-the-blank. Experiments show that our approach has advantages in both prediction performance and diagnosis ability.



### DCT-CompCNN: A Novel Image Classification Network Using JPEG Compressed DCT Coefficients
- **Arxiv ID**: http://arxiv.org/abs/1907.11503v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.11503v1)
- **Published**: 2019-07-26 12:01:21+00:00
- **Updated**: 2019-07-26 12:01:21+00:00
- **Authors**: Bulla Rajesh, Mohammed Javed, Ratnesh, Shubham Srivastava
- **Comment**: None
- **Journal**: None
- **Summary**: The popularity of Convolutional Neural Network (CNN) in the field of Image Processing and Computer Vision has motivated researchers and industrialist experts across the globe to solve different challenges with high accuracy. The simplest way to train a CNN classifier is to directly feed the original RGB pixels images into the network. However, if we intend to classify images directly with its compressed data, the same approach may not work better, like in case of JPEG compressed images. This research paper investigates the issues of modifying the input representation of the JPEG compressed data, and then feeding into the CNN. The architecture is termed as DCT-CompCNN. This novel approach has shown that CNNs can also be trained with JPEG compressed DCT coefficients, and subsequently can produce a better performance in comparison with the conventional CNN approach. The efficiency of the modified input representation is tested with the existing ResNet-50 architecture and the proposed DCT-CompCNN architecture on a public image classification datasets like Dog Vs Cat and CIFAR-10 datasets, reporting a better performance



### Context-Aware Multipath Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.11519v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.11519v1)
- **Published**: 2019-07-26 12:34:31+00:00
- **Updated**: 2019-07-26 12:34:31+00:00
- **Authors**: Dumindu Tissera, Kumara Kahatapitiya, Rukshan Wijesinghe, Subha Fernando, Ranga Rodrigo
- **Comment**: None
- **Journal**: None
- **Summary**: Making a single network effectively address diverse contexts---learning the variations within a dataset or multiple datasets---is an intriguing step towards achieving generalized intelligence. Existing approaches of deepening, widening, and assembling networks are not cost effective in general. In view of this, networks which can allocate resources according to the context of the input and regulate flow of information across the network are effective. In this paper, we present Context-Aware Multipath Network (CAMNet), a multi-path neural network with data-dependant routing between parallel tensors. We show that our model performs as a generalized model capturing variations in individual datasets and multiple different datasets, both simultaneously and sequentially. CAMNet surpasses the performance of classification and pixel-labeling tasks in comparison with the equivalent single-path, multi-path, and deeper single-path networks, considering datasets individually, sequentially, and in combination. The data-dependent routing between tensors in CAMNet enables the model to control the flow of information end-to-end, deciding which resources to be common or domain-specific.



### Report on UG^2+ Challenge Track 1: Assessing Algorithms to Improve Video Object Detection and Classification from Unconstrained Mobility Platforms
- **Arxiv ID**: http://arxiv.org/abs/1907.11529v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11529v4)
- **Published**: 2019-07-26 12:40:51+00:00
- **Updated**: 2020-11-20 04:16:10+00:00
- **Authors**: Sreya Banerjee, Rosaura G. VidalMata, Zhangyang Wang, Walter J. Scheirer
- **Comment**: Supplemental material: http://bit.ly/UG2Supp
- **Journal**: None
- **Summary**: How can we effectively engineer a computer vision system that is able to interpret videos from unconstrained mobility platforms like UAVs? One promising option is to make use of image restoration and enhancement algorithms from the area of computational photography to improve the quality of the underlying frames in a way that also improves automatic visual recognition. Along these lines, exploratory work is needed to find out which image pre-processing algorithms, in combination with the strongest features and supervised machine learning approaches, are good candidates for difficult scenarios like motion blur, weather, and mis-focus -- all common artifacts in UAV acquired images. This paper summarizes the protocols and results of Track 1 of the UG^2+ Challenge held in conjunction with IEEE/CVF CVPR 2019. The challenge looked at two separate problems: (1) object detection improvement in video, and (2) object classification improvement in video. The challenge made use of the UG^2 (UAV, Glider, Ground) dataset, which is an established benchmark for assessing the interplay between image restoration and enhancement and visual recognition. 16 algorithms were submitted by academic and corporate teams, and a detailed analysis of how they performed on each challenge problem is reported here.



### Bayesian Volumetric Autoregressive generative models for better semisupervised learning
- **Arxiv ID**: http://arxiv.org/abs/1907.11559v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.CO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.11559v1)
- **Published**: 2019-07-26 13:08:36+00:00
- **Updated**: 2019-07-26 13:08:36+00:00
- **Authors**: Guilherme Pombo, Robert Gray, Tom Varsavsky, John Ashburner, Parashkev Nachev
- **Comment**: None
- **Journal**: None
- **Summary**: Deep generative models are rapidly gaining traction in medical imaging. Nonetheless, most generative architectures struggle to capture the underlying probability distributions of volumetric data, exhibit convergence problems, and offer no robust indices of model uncertainty. By comparison, the autoregressive generative model PixelCNN can be extended to volumetric data with relative ease, it readily attempts to learn the true underlying probability distribution and it still admits a Bayesian reformulation that provides a principled framework for reasoning about model uncertainty. Our contributions in this paper are two fold: first, we extend PixelCNN to work with volumetric brain magnetic resonance imaging data. Second, we show that reformulating this model to approximate a deep Gaussian process yields a measure of uncertainty that improves the performance of semi-supervised learning, in particular classification performance in settings where the proportion of labelled data is low. We quantify this improvement across classification, regression, and semantic segmentation tasks, training and testing on clinical magnetic resonance brain imaging data comprising T1-weighted and diffusion-weighted sequences.



### Deep Learning for Classification and Severity Estimation of Coffee Leaf Biotic Stress
- **Arxiv ID**: http://arxiv.org/abs/1907.11561v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.11561v1)
- **Published**: 2019-07-26 13:12:44+00:00
- **Updated**: 2019-07-26 13:12:44+00:00
- **Authors**: J. G. M. Esgario, R. A. Krohling, J. A. Ventura
- **Comment**: None
- **Journal**: None
- **Summary**: Biotic stress consists of damage to plants through other living organisms. Efficient control of biotic agents such as pests and pathogens (viruses, fungi, bacteria, etc.) is closely related to the concept of agricultural sustainability. Agricultural sustainability promotes the development of new technologies that allow the reduction of environmental impacts, greater accessibility to farmers and, consequently, increase on productivity. The use of computer vision with deep learning methods allows the early and correct identification of the stress-causing agent. So, corrective measures can be applied as soon as possible to mitigate the problem. The objective of this work is to design an effective and practical system capable of identifying and estimating the stress severity caused by biotic agents on coffee leaves. The proposed approach consists of a multi-task system based on convolutional neural networks. In addition, we have explored the use of data augmentation techniques to make the system more robust and accurate. The experimental results obtained for classification as well as for severity estimation indicate that the proposed system might be a suitable tool to assist both experts and farmers in the identification and quantification of biotic stresses in coffee plantations.



### Cooperative image captioning
- **Arxiv ID**: http://arxiv.org/abs/1907.11565v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11565v1)
- **Published**: 2019-07-26 13:18:56+00:00
- **Updated**: 2019-07-26 13:18:56+00:00
- **Authors**: Gilad Vered, Gal Oren, Yuval Atzmon, Gal Chechik
- **Comment**: None
- **Journal**: None
- **Summary**: When describing images with natural language, the descriptions can be made more informative if tuned using downstream tasks. This is often achieved by training two networks: a "speaker network" that generates sentences given an image, and a "listener network" that uses them to perform a task. Unfortunately, training multiple networks jointly to communicate to achieve a joint task, faces two major challenges. First, the descriptions generated by a speaker network are discrete and stochastic, making optimization very hard and inefficient. Second, joint training usually causes the vocabulary used during communication to drift and diverge from natural language.   We describe an approach that addresses both challenges. We first develop a new effective optimization based on partial-sampling from a multinomial distribution combined with straight-through gradient updates, which we name PSST for Partial-Sampling Straight-Through. Second, we show that the generated descriptions can be kept close to natural by constraining them to be similar to human descriptions. Together, this approach creates descriptions that are both more discriminative and more natural than previous approaches. Evaluations on the standard COCO benchmark show that PSST Multinomial dramatically improve the recall@10 from 60% to 86% maintaining comparable language naturalness, and human evaluations show that it also increases naturalness while keeping the discriminative power of generated captions.



### Self-Adaptive 2D-3D Ensemble of Fully Convolutional Networks for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.11587v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.11587v1)
- **Published**: 2019-07-26 14:14:53+00:00
- **Updated**: 2019-07-26 14:14:53+00:00
- **Authors**: Maria G. Baldeon Calisto, Susana K. Lai-Yuen
- **Comment**: None
- **Journal**: None
- **Summary**: Segmentation is a critical step in medical image analysis. Fully Convolutional Networks (FCNs) have emerged as powerful segmentation models achieving state-of-the-art results in various medical image datasets. Network architectures are usually designed manually for a specific segmentation task so applying them to other medical datasets requires extensive experience and time. Moreover, the segmentation requires handling large volumetric data that results in big and complex architectures. Recently, methods that automatically design neural networks for medical image segmentation have been presented; however, most approaches either do not fully consider volumetric information or do not optimize the size of the network. In this paper, we propose a novel self-adaptive 2D-3D ensemble of FCNs for medical image segmentation that incorporates volumetric information and optimizes both the model's performance and size. The model is composed of an ensemble of a 2D FCN that extracts intra-slice information, and a 3D FCN that exploits inter-slice information. The architectures of the 2D and 3D FCNs are automatically adapted to a medical image dataset using a multiobjective evolutionary based algorithm that minimizes both the segmentation error and number of parameters in the network. The proposed 2D-3D FCN ensemble was tested on the task of prostate segmentation on the image dataset from the PROMISE12 Grand Challenge. The resulting network is ranked in the top 10 submissions, surpassing the performance of other automatically-designed architectures while being considerably smaller in size.



### Unsupervised Learning for Optical Flow Estimation Using Pyramid Convolution LSTM
- **Arxiv ID**: http://arxiv.org/abs/1907.11628v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11628v1)
- **Published**: 2019-07-26 15:28:59+00:00
- **Updated**: 2019-07-26 15:28:59+00:00
- **Authors**: Shuosen Guan, Haoxin Li, Wei-Shi Zheng
- **Comment**: IEEE International Conference on Multimedia and Expo(ICME). 2019
- **Journal**: None
- **Summary**: Most of current Convolution Neural Network (CNN) based methods for optical flow estimation focus on learning optical flow on synthetic datasets with groundtruth, which is not practical. In this paper, we propose an unsupervised optical flow estimation framework named PCLNet. It uses pyramid Convolution LSTM (ConvLSTM) with the constraint of adjacent frame reconstruction, which allows flexibly estimating multi-frame optical flows from any video clip. Besides, by decoupling motion feature learning and optical flow representation, our method avoids complex short-cut connections used in existing frameworks while improving accuracy of optical flow estimation. Moreover, different from those methods using specialized CNN architectures for capturing motion, our framework directly learns optical flow from the features of generic CNNs and thus can be easily embedded in any CNN based frameworks for other tasks. Extensive experiments have verified that our method not only estimates optical flow effectively and accurately, but also obtains comparable performance on action recognition.



### Multi-Stage Prediction Networks for Data Harmonization
- **Arxiv ID**: http://arxiv.org/abs/1907.11629v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.11629v1)
- **Published**: 2019-07-26 15:29:46+00:00
- **Updated**: 2019-07-26 15:29:46+00:00
- **Authors**: Stefano B. Blumberg, Marco Palombo, Can Son Khoo, Chantal M. W. Tax, Ryutaro Tanno, Daniel C. Alexander
- **Comment**: Accepted In Medical Image Computing and Computer Assisted
  Intervention (MICCAI) 2019
- **Journal**: None
- **Summary**: In this paper, we introduce multi-task learning (MTL) to data harmonization (DH); where we aim to harmonize images across different acquisition platforms and sites. This allows us to integrate information from multiple acquisitions and improve the predictive performance and learning efficiency of the harmonization model. Specifically, we introduce the Multi Stage Prediction (MSP) Network, a MTL framework that incorporates neural networks of potentially disparate architectures, trained for different individual acquisition platforms, into a larger architecture that is refined in unison. The MSP utilizes high-level features of single networks for individual tasks, as inputs of additional neural networks to inform the final prediction, therefore exploiting redundancy across tasks to make the most of limited training data. We validate our methods on a dMRI harmonization challenge dataset, where we predict three modern platform types, from one obtained from an old scanner. We show how MTL architectures, such as the MSP, produce around 20\% improvement of patch-based mean-squared error over current state-of-the-art methods and that our MSP outperforms off-the-shelf MTL networks. Our code is available https://github.com/sbb-gh/ .



### Differential Scene Flow from Light Field Gradients
- **Arxiv ID**: http://arxiv.org/abs/1907.11637v2
- **DOI**: 10.1007/s11263-019-01230-z
- **Categories**: **cs.CV**, eess.IV, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1907.11637v2)
- **Published**: 2019-07-26 15:47:14+00:00
- **Updated**: 2019-07-29 18:01:08+00:00
- **Authors**: Sizhuo Ma, Brandon M. Smith, Mohit Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents novel techniques for recovering 3D dense scene flow, based on differential analysis of 4D light fields. The key enabling result is a per-ray linear equation, called the ray flow equation, that relates 3D scene flow to 4D light field gradients. The ray flow equation is invariant to 3D scene structure and applicable to a general class of scenes, but is under-constrained (3 unknowns per equation). Thus, additional constraints must be imposed to recover motion. We develop two families of scene flow algorithms by leveraging the structural similarity between ray flow and optical flow equations: local 'Lucas-Kanade' ray flow and global 'Horn-Schunck' ray flow, inspired by corresponding optical flow methods. We also develop a combined local-global method by utilizing the correspondence structure in the light fields. We demonstrate high precision 3D scene flow recovery for a wide range of scenarios, including rotation and non-rigid motion. We analyze the theoretical and practical performance limits of the proposed techniques via the light field structure tensor, a 3x3 matrix that encodes the local structure of light fields. We envision that the proposed analysis and algorithms will lead to design of future light-field cameras that are optimized for motion sensing, in addition to depth sensing.



### On the Design of Black-box Adversarial Examples by Leveraging Gradient-free Optimization and Operator Splitting Method
- **Arxiv ID**: http://arxiv.org/abs/1907.11684v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.11684v4)
- **Published**: 2019-07-26 17:29:52+00:00
- **Updated**: 2019-12-04 21:05:19+00:00
- **Authors**: Pu Zhao, Sijia Liu, Pin-Yu Chen, Nghia Hoang, Kaidi Xu, Bhavya Kailkhura, Xue Lin
- **Comment**: accepted by ICCV 2019
- **Journal**: None
- **Summary**: Robust machine learning is currently one of the most prominent topics which could potentially help shaping a future of advanced AI platforms that not only perform well in average cases but also in worst cases or adverse situations. Despite the long-term vision, however, existing studies on black-box adversarial attacks are still restricted to very specific settings of threat models (e.g., single distortion metric and restrictive assumption on target model's feedback to queries) and/or suffer from prohibitively high query complexity. To push for further advances in this field, we introduce a general framework based on an operator splitting method, the alternating direction method of multipliers (ADMM) to devise efficient, robust black-box attacks that work with various distortion metrics and feedback settings without incurring high query complexity. Due to the black-box nature of the threat model, the proposed ADMM solution framework is integrated with zeroth-order (ZO) optimization and Bayesian optimization (BO), and thus is applicable to the gradient-free regime. This results in two new black-box adversarial attack generation methods, ZO-ADMM and BO-ADMM. Our empirical evaluations on image classification datasets show that our proposed approaches have much lower function query complexities compared to state-of-the-art attack methods, but achieve very competitive attack success rates.



### Real-time Visual Object Tracking with Natural Language Description
- **Arxiv ID**: http://arxiv.org/abs/1907.11751v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11751v3)
- **Published**: 2019-07-26 18:44:17+00:00
- **Updated**: 2019-12-03 22:08:53+00:00
- **Authors**: Qi Feng, Vitaly Ablavsky, Qinxun Bai, Guorong Li, Stan Sclaroff
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, deep-learning-based visual object trackers have been studied thoroughly, but handling occlusions and/or rapid motion of the target remains challenging. In this work, we argue that conditioning on the natural language (NL) description of a target provides information for longer-term invariance, and thus helps cope with typical tracking challenges. However, deriving a formulation to combine the strengths of appearance-based tracking with the language modality is not straightforward. We propose a novel deep tracking-by-detection formulation that can take advantage of NL descriptions. Regions that are related to the given NL description are generated by a proposal network during the detection phase of the tracker. Our LSTM based tracker then predicts the update of the target from regions proposed by the NL based detection phase. In benchmarks, our method is competitive with state of the art trackers, while it outperforms all other trackers on targets with unambiguous and precise language annotations. It also beats the state-of-the-art NL tracker when initializing without a bounding box. Our method runs at over 30 fps on a single GPU.



### To Learn or Not to Learn: Analyzing the Role of Learning for Navigation in Virtual Environments
- **Arxiv ID**: http://arxiv.org/abs/1907.11770v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11770v1)
- **Published**: 2019-07-26 19:45:23+00:00
- **Updated**: 2019-07-26 19:45:23+00:00
- **Authors**: Noriyuki Kojima, Jia Deng
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we compare learning-based methods and classical methods for navigation in virtual environments. We construct classical navigation agents and demonstrate that they outperform state-of-the-art learning-based agents on two standard benchmarks: MINOS and Stanford Large-Scale 3D Indoor Spaces. We perform detailed analysis to study the strengths and weaknesses of learned agents and classical agents, as well as how characteristics of the virtual environment impact navigation performance. Our results show that learned agents have inferior collision avoidance and memory management, but are superior in handling ambiguity and noise. These results can inform future design of navigation agents.



### Memory- and Communication-Aware Model Compression for Distributed Deep Learning Inference on IoT
- **Arxiv ID**: http://arxiv.org/abs/1907.11804v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.DC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.11804v1)
- **Published**: 2019-07-26 22:17:42+00:00
- **Updated**: 2019-07-26 22:17:42+00:00
- **Authors**: Kartikeya Bhardwaj, Chingyi Lin, Anderson Sartor, Radu Marculescu
- **Comment**: This preprint is for personal use only. The official article will
  appear as part of the ESWEEK-TECS special issue and will be presented in the
  International Conference on Hardware/Software Codesign and System Synthesis
  (CODES+ISSS), 2019
- **Journal**: None
- **Summary**: Model compression has emerged as an important area of research for deploying deep learning models on Internet-of-Things (IoT). However, for extremely memory-constrained scenarios, even the compressed models cannot fit within the memory of a single device and, as a result, must be distributed across multiple devices. This leads to a distributed inference paradigm in which memory and communication costs represent a major bottleneck. Yet, existing model compression techniques are not communication-aware. Therefore, we propose Network of Neural Networks (NoNN), a new distributed IoT learning paradigm that compresses a large pretrained 'teacher' deep network into several disjoint and highly-compressed 'student' modules, without loss of accuracy. Moreover, we propose a network science-based knowledge partitioning algorithm for the teacher model, and then train individual students on the resulting disjoint partitions. Extensive experimentation on five image classification datasets, for user-defined memory/performance budgets, show that NoNN achieves higher accuracy than several baselines and similar accuracy as the teacher model, while using minimal communication among students. Finally, as a case study, we deploy the proposed model for CIFAR-10 dataset on edge devices and demonstrate significant improvements in memory footprint (up to 24x), performance (up to 12x), and energy per node (up to 14x) compared to the large teacher model. We further show that for distributed inference on multiple edge devices, our proposed NoNN model results in up to 33x reduction in total latency w.r.t. a state-of-the-art model compression baseline.



### VITAL: A Visual Interpretation on Text with Adversarial Learning for Image Labeling
- **Arxiv ID**: http://arxiv.org/abs/1907.11811v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11811v2)
- **Published**: 2019-07-26 22:52:45+00:00
- **Updated**: 2019-08-01 18:57:53+00:00
- **Authors**: Tao Hu, Chengjiang Long, Leheng Zhang, Chunxia Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel way to interpret text information by extracting visual feature presentation from multiple high-resolution and photo-realistic synthetic images generated by Text-to-image Generative Adversarial Network (GAN) to improve the performance of image labeling. Firstly, we design a stacked Generative Multi-Adversarial Network (GMAN), StackGMAN++, a modified version of the current state-of-the-art Text-to-image GAN, StackGAN++, to generate multiple synthetic images with various prior noises conditioned on a text. And then we extract deep visual features from the generated synthetic images to explore the underlying visual concepts for text. Finally, we combine image-level visual feature, text-level feature and visual features based on synthetic images together to predict labels for images. We conduct experiments on two benchmark datasets and the experimental results clearly demonstrate the efficacy of our proposed approach.



### Momentum-Net: Fast and convergent iterative neural network for inverse problems
- **Arxiv ID**: http://arxiv.org/abs/1907.11818v3
- **DOI**: 10.1109/TPAMI.2020.3012955
- **Categories**: **eess.IV**, cs.CV, cs.LG, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1907.11818v3)
- **Published**: 2019-07-26 23:42:37+00:00
- **Updated**: 2020-06-20 09:42:48+00:00
- **Authors**: Il Yong Chun, Zhengyu Huang, Hongki Lim, Jeffrey A. Fessler
- **Comment**: 28 pages, 13 figures, 3 algorithms, 4 tables, submitted revision to
  IEEE T-PAMI
- **Journal**: IEEE Trans. Pattern Anal. Mach. Intell., 45(5):4915-4931, Apr.
  2023
- **Summary**: Iterative neural networks (INN) are rapidly gaining attention for solving inverse problems in imaging, image processing, and computer vision. INNs combine regression NNs and an iterative model-based image reconstruction (MBIR) algorithm, often leading to both good generalization capability and outperforming reconstruction quality over existing MBIR optimization models. This paper proposes the first fast and convergent INN architecture, Momentum-Net, by generalizing a block-wise MBIR algorithm that uses momentum and majorizers with regression NNs. For fast MBIR, Momentum-Net uses momentum terms in extrapolation modules, and noniterative MBIR modules at each iteration by using majorizers, where each iteration of Momentum-Net consists of three core modules: image refining, extrapolation, and MBIR. Momentum-Net guarantees convergence to a fixed-point for general differentiable (non)convex MBIR functions (or data-fit terms) and convex feasible sets, under two asymptomatic conditions. To consider data-fit variations across training and testing samples, we also propose a regularization parameter selection scheme based on the "spectral spread" of majorization matrices. Numerical experiments for light-field photography using a focal stack and sparse-view computational tomography demonstrate that, given identical regression NN architectures, Momentum-Net significantly improves MBIR speed and accuracy over several existing INNs; it significantly improves reconstruction quality compared to a state-of-the-art MBIR method in each application.



### Grape detection, segmentation and tracking using deep neural networks and three-dimensional association
- **Arxiv ID**: http://arxiv.org/abs/1907.11819v3
- **DOI**: 10.1016/J.COMPAG.2020.105247
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.11819v3)
- **Published**: 2019-07-26 23:45:51+00:00
- **Updated**: 2020-02-07 11:36:18+00:00
- **Authors**: Thiago T. Santos, Leonardo L. de Souza, Andreza A. dos Santos, Sandra Avila
- **Comment**: None
- **Journal**: Computers and Electronics in Agriculture, 170, 105-247 (2020)
- **Summary**: Agricultural applications such as yield prediction, precision agriculture and automated harvesting need systems able to infer the crop state from low-cost sensing devices. Proximal sensing using affordable cameras combined with computer vision has seen a promising alternative, strengthened after the advent of convolutional neural networks (CNNs) as an alternative for challenging pattern recognition problems in natural images. Considering fruit growing monitoring and automation, a fundamental problem is the detection, segmentation and counting of individual fruits in orchards. Here we show that for wine grapes, a crop presenting large variability in shape, color, size and compactness, grape clusters can be successfully detected, segmented and tracked using state-of-the-art CNNs. In a test set containing 408 grape clusters from images taken on a trellis-system based vineyard, we have reached an F 1 -score up to 0.91 for instance segmentation, a fine separation of each cluster from other structures in the image that allows a more accurate assessment of fruit size and shape. We have also shown as clusters can be identified and tracked along video sequences recording orchard rows. We also present a public dataset containing grape clusters properly annotated in 300 images and a novel annotation methodology for segmentation of complex objects in natural images. The presented pipeline for annotation, training, evaluation and tracking of agricultural patterns in images can be replicated for different crops and production systems. It can be employed in the development of sensing components for several agricultural and environmental applications.



