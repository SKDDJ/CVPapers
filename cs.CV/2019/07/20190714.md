# Arxiv Papers in cs.CV on 2019-07-14
### Smile, Be Happy :) Emoji Embedding for Visual Sentiment Analysis
- **Arxiv ID**: http://arxiv.org/abs/1907.06160v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06160v3)
- **Published**: 2019-07-14 03:29:02+00:00
- **Updated**: 2020-08-09 02:48:03+00:00
- **Authors**: Ziad Al-Halah, Andrew Aitken, Wenzhe Shi, Jose Caballero
- **Comment**: International Conference on Computer Vision (ICCV 2019) Workshops.
  Project page and the Visual Smiley Dataset:
  https://www.cs.utexas.edu/~ziad/emoji_visual_sentiment.html
- **Journal**: None
- **Summary**: Due to the lack of large-scale datasets, the prevailing approach in visual sentiment analysis is to leverage models trained for object classification in large datasets like ImageNet. However, objects are sentiment neutral which hinders the expected gain of transfer learning for such tasks. In this work, we propose to overcome this problem by learning a novel sentiment-aligned image embedding that is better suited for subsequent visual sentiment analysis. Our embedding leverages the intricate relation between emojis and images in large-scale and readily available data from social media. Emojis are language-agnostic, consistent, and carry a clear sentiment signal which make them an excellent proxy to learn a sentiment aligned embedding. Hence, we construct a novel dataset of 4 million images collected from Twitter with their associated emojis. We train a deep neural model for image embedding using emoji prediction task as a proxy. Our evaluation demonstrates that the proposed embedding outperforms the popular object-based counterpart consistently across several sentiment analysis benchmarks. Furthermore, without bell and whistles, our compact, effective and simple embedding outperforms the more elaborate and customized state-of-the-art deep models on these public benchmarks. Additionally, we introduce a novel emoji representation based on their visual emotional response which supports a deeper understanding of the emoji modality and their usage on social media.



### FoodX-251: A Dataset for Fine-grained Food Classification
- **Arxiv ID**: http://arxiv.org/abs/1907.06167v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06167v1)
- **Published**: 2019-07-14 05:01:31+00:00
- **Updated**: 2019-07-14 05:01:31+00:00
- **Authors**: Parneet Kaur, Karan Sikka, Weijun Wang, Serge Belongie, Ajay Divakaran
- **Comment**: Published at Fine-Grained Visual Categorization Workshop, CVPR19
- **Journal**: None
- **Summary**: Food classification is a challenging problem due to the large number of categories, high visual similarity between different foods, as well as the lack of datasets for training state-of-the-art deep models. Solving this problem will require advances in both computer vision models as well as datasets for evaluating these models. In this paper we focus on the second aspect and introduce FoodX-251, a dataset of 251 fine-grained food categories with 158k images collected from the web. We use 118k images as a training set and provide human verified labels for 40k images that can be used for validation and testing. In this work, we outline the procedure of creating this dataset and provide relevant baselines with deep learning models. The FoodX-251 dataset has been used for organizing iFood-2019 challenge in the Fine-Grained Visual Categorization workshop (FGVC6 at CVPR 2019) and is available for download.



### A Divide-and-Conquer Approach towards Understanding Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.06194v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.06194v1)
- **Published**: 2019-07-14 09:50:45+00:00
- **Updated**: 2019-07-14 09:50:45+00:00
- **Authors**: Weilin Fu, Katharina Breininger, Roman Schaffert, Nishant Ravikumar, Andreas Maier
- **Comment**: This paper is accepted in MICCAI 2019
- **Journal**: None
- **Summary**: Deep neural networks have achieved tremendous success in various fields including medical image segmentation. However, they have long been criticized for being a black-box, in that interpretation, understanding and correcting architectures is difficult as there is no general theory for deep neural network design. Previously, precision learning was proposed to fuse deep architectures and traditional approaches. Deep networks constructed in this way benefit from the original known operator, have fewer parameters, and improved interpretability. However, they do not yield state-of-the-art performance in all applications. In this paper, we propose to analyze deep networks using known operators, by adopting a divide-and-conquer strategy to replace network components, whilst retaining its performance. The task of retinal vessel segmentation is investigated for this purpose. We start with a high-performance U-Net and show by step-by-step conversion that we are able to divide the network into modules of known operators. The results indicate that a combination of a trainable guided filter and a trainable version of the Frangi filter yields a performance at the level of U-Net (AUC 0.974 vs. 0.972) with a tremendous reduction in parameters (111,536 vs. 9,575). In addition, the trained layers can be mapped back into their original algorithmic interpretation and analyzed using standard tools of signal processing.



### Unsupervised Automatic Building Extraction Using Active Contour Model on Unregistered Optical Imagery and Airborne LiDAR Data
- **Arxiv ID**: http://arxiv.org/abs/1907.06206v1
- **DOI**: 10.5194/isprs-archives-XLII-2-W16-181-2019
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.06206v1)
- **Published**: 2019-07-14 11:18:56+00:00
- **Updated**: 2019-07-14 11:18:56+00:00
- **Authors**: Thanh Huy Nguyen, Sylvie Daniel, Didier Gueriot, Christophe Sintes, Jean-Marc Le Caillec
- **Comment**: PIA19 - Photogrammetric Image Analysis 2019 which will be held in
  conjunction with MRSS19 - Munich Remote Sensing Symposium 2019 on September
  18th-20th, 2019 in Munich, Germany. Proceeding: The International Archives of
  the Photogrammetry, Remote Sensing and Spatial Information Sciences
- **Journal**: Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci. XLII-2/W16
  (2019) 181-188
- **Summary**: Automatic extraction of buildings in urban scenes has become a subject of growing interest in the domain of photogrammetry and remote sensing, particularly with the emergence of LiDAR systems since mid-1990s. However, in reality, this task is still very challenging due to the complexity of building size and shapes, as well as its surrounding environment. Active contour model, colloquially called snake model, which has been extensively used in many applications in computer vision and image processing, is also applied to extract buildings from aerial/satellite imagery. Motivated by the limitations of existing snake models addressing to the building extraction, this paper presents an unsupervised and fully automatic snake model to extract buildings using optical imagery and an unregistered airborne LiDAR dataset, without manual initial points or training data. The proposed method is shown to be capable of extracting buildings with varying color from complex environments, and yielding high overall accuracy.



### State Estimation in Visual Inertial Autonomous Helicopter Landing Using Optimisation on Manifold
- **Arxiv ID**: http://arxiv.org/abs/1907.06247v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1907.06247v1)
- **Published**: 2019-07-14 17:01:50+00:00
- **Updated**: 2019-07-14 17:01:50+00:00
- **Authors**: Thinh Hoang Dinh, Hieu Le Thi Hong, Tri Ngo Dinh
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous helicopter landing is a challenging task that requires precise information about the aircraft states regarding the helicopters position, attitude, as well as position of the helipad. To this end, we propose a solution that fuses data from an Inertial Measurement Unit (IMU) and a monocular camera which is capable of detecting helipads position in the image plane. The algorithm utilises manifold based nonlinear optimisation over preintegrated IMU measurements and reprojection error in temporally uniformly distributed keyframes, exhibiting good performance in terms of accuracy and being computationally feasible. Our contributions of this paper are the formal address of the landmarks Jacobian expressions and the adaptation of equality constrained Gauss-Newton method to this specific problem. Numerical simulations on MATLAB/Simulink confirm the validity of given claims.



### Autoencoding sensory substitution
- **Arxiv ID**: http://arxiv.org/abs/1907.06286v1
- **DOI**: 10.13140/RG.2.2.10576.87048
- **Categories**: **q-bio.NC**, cs.CV, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1907.06286v1)
- **Published**: 2019-07-14 21:58:10+00:00
- **Updated**: 2019-07-14 21:58:10+00:00
- **Authors**: Viktor TÃ³th, Lauri Parkkonen
- **Comment**: None
- **Journal**: None
- **Summary**: Tens of millions of people live blind, and their number is ever increasing. Visual-to-auditory sensory substitution (SS) encompasses a family of cheap, generic solutions to assist the visually impaired by conveying visual information through sound. The required SS training is lengthy: months of effort is necessary to reach a practical level of adaptation. There are two reasons for the tedious training process: the elongated substituting audio signal, and the disregard for the compressive characteristics of the human hearing system. To overcome these obstacles, we developed a novel class of SS methods, by training deep recurrent autoencoders for image-to-sound conversion. We successfully trained deep learning models on different datasets to execute visual-to-auditory stimulus conversion. By constraining the visual space, we demonstrated the viability of shortened substituting audio signals, while proposing mechanisms, such as the integration of computational hearing models, to optimally convey visual features in the substituting stimulus as perceptually discernible auditory components. We tested our approach in two separate cases. In the first experiment, the author went blindfolded for 5 days, while performing SS training on hand posture discrimination. The second experiment assessed the accuracy of reaching movements towards objects on a table. In both test cases, above-chance-level accuracy was attained after a few hours of training. Our novel SS architecture broadens the horizon of rehabilitation methods engineered for the visually impaired. Further improvements on the proposed model shall yield hastened rehabilitation of the blind and a wider adaptation of SS devices as a consequence.



### Measuring the Transferability of Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1907.06291v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.06291v1)
- **Published**: 2019-07-14 22:20:58+00:00
- **Updated**: 2019-07-14 22:20:58+00:00
- **Authors**: Deyan Petrov, Timothy M. Hospedales
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial examples are of wide concern due to their impact on the reliability of contemporary machine learning systems. Effective adversarial examples are mostly found via white-box attacks. However, in some cases they can be transferred across models, thus enabling them to attack black-box models. In this work we evaluate the transferability of three adversarial attacks - the Fast Gradient Sign Method, the Basic Iterative Method, and the Carlini & Wagner method, across two classes of models - the VGG class(using VGG16, VGG19 and an ensemble of VGG16 and VGG19), and the Inception class(Inception V3, Xception, Inception Resnet V2, and an ensemble of the three). We also outline the problems with the assessment of transferability in the current body of research and attempt to amend them by picking specific "strong" parameters for the attacks, and by using a L-Infinity clipping technique and the SSIM metric for the final evaluation of the attack transferability.



### Perceptually Motivated Method for Image Inpainting Comparison
- **Arxiv ID**: http://arxiv.org/abs/1907.06296v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.06296v1)
- **Published**: 2019-07-14 23:50:42+00:00
- **Updated**: 2019-07-14 23:50:42+00:00
- **Authors**: Ivan Molodetskikh, Mikhail Erofeev, Dmitry Vatolin
- **Comment**: 8 pages, 9 figures
- **Journal**: None
- **Summary**: The field of automatic image inpainting has progressed rapidly in recent years, but no one has yet proposed a standard method of evaluating algorithms. This absence is due to the problem's challenging nature: image-inpainting algorithms strive for realism in the resulting images, but realism is a subjective concept intrinsic to human perception. Existing objective image-quality metrics provide a poor approximation of what humans consider more or less realistic.   To improve the situation and to better organize both prior and future research in this field, we conducted a subjective comparison of nine state-of-the-art inpainting algorithms and propose objective quality metrics that exhibit high correlation with the results of our comparison.



