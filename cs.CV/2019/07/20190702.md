# Arxiv Papers in cs.CV on 2019-07-02
### Visualizing and Describing Fine-grained Categories as Textures
- **Arxiv ID**: http://arxiv.org/abs/1907.05288v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.05288v1)
- **Published**: 2019-07-02 00:19:37+00:00
- **Updated**: 2019-07-02 00:19:37+00:00
- **Authors**: Tsung-Yu Lin, Mikayla Timm, Chenyun Wu, Subhransu Maji
- **Comment**: None
- **Journal**: None
- **Summary**: We analyze how categories from recent FGVC challenges can be described by their textural content. The motivation is that subtle differences between species of birds or butterflies can often be described in terms of the texture associated with them and that several top-performing networks are inspired by texture-based representations. These representations are characterized by orderless pooling of second-order filter activations such as in bilinear CNNs and the winner of the iNaturalist 2018 challenge. Concretely, for each category we (i) visualize the "maximal images" by obtaining inputs x that maximize the probability of the particular class according to a texture-based deep network, and (ii) automatically describe the maximal images using a set of texture attributes. The models for texture captioning were trained on our ongoing efforts on collecting a dataset of describable textures building on the DTD dataset. These visualizations indicate what aspects of the texture is most discriminative for each category while the descriptions provide a language-based explanation of the same.



### Language2Pose: Natural Language Grounded Pose Forecasting
- **Arxiv ID**: http://arxiv.org/abs/1907.01108v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1907.01108v2)
- **Published**: 2019-07-02 00:38:44+00:00
- **Updated**: 2019-11-27 19:06:42+00:00
- **Authors**: Chaitanya Ahuja, Louis-Philippe Morency
- **Comment**: None
- **Journal**: None
- **Summary**: Generating animations from natural language sentences finds its applications in a a number of domains such as movie script visualization, virtual human animation and, robot motion planning. These sentences can describe different kinds of actions, speeds and direction of these actions, and possibly a target destination. The core modeling challenge in this language-to-pose application is how to map linguistic concepts to motion animations.   In this paper, we address this multimodal problem by introducing a neural architecture called Joint Language to Pose (or JL2P), which learns a joint embedding of language and pose. This joint embedding space is learned end-to-end using a curriculum learning approach which emphasizes shorter and easier sequences first before moving to longer and harder ones. We evaluate our proposed model on a publicly available corpus of 3D pose data and human-annotated sentences. Both objective metrics and human judgment evaluation confirm that our proposed approach is able to generate more accurate animations and are deemed visually more representative by humans than other data driven approaches.



### Robust Tensor Completion Using Transformed Tensor SVD
- **Arxiv ID**: http://arxiv.org/abs/1907.01113v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.01113v1)
- **Published**: 2019-07-02 00:50:31+00:00
- **Updated**: 2019-07-02 00:50:31+00:00
- **Authors**: Guangjing Song, Michael K. Ng, Xiongjun Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study robust tensor completion by using transformed tensor singular value decomposition (SVD), which employs unitary transform matrices instead of discrete Fourier transform matrix that is used in the traditional tensor SVD. The main motivation is that a lower tubal rank tensor can be obtained by using other unitary transform matrices than that by using discrete Fourier transform matrix. This would be more effective for robust tensor completion. Experimental results for hyperspectral, video and face datasets have shown that the recovery performance for the robust tensor completion problem by using transformed tensor SVD is better in PSNR than that by using Fourier transform and other robust tensor completion methods.



### Vision-based Pedestrian Alert Safety System (PASS) for Signalized Intersections
- **Arxiv ID**: http://arxiv.org/abs/1907.05284v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.05284v1)
- **Published**: 2019-07-02 02:17:55+00:00
- **Updated**: 2019-07-02 02:17:55+00:00
- **Authors**: Mhafuzul Islam, Mizanur Rahman, Mashrur Chowdhury, Gurcan Comert, Eshaa Deepak Sood, Amy Apon
- **Comment**: 23 pages, 8 figures
- **Journal**: None
- **Summary**: Although Vehicle-to-Pedestrian (V2P) communication can significantly improve pedestrian safety at a signalized intersection, this safety is hindered as pedestrians often do not carry hand-held devices (e.g., Dedicated short-range communication (DSRC) and 5G enabled cell phone) to communicate with connected vehicles nearby. To overcome this limitation, in this study, traffic cameras at a signalized intersection were used to accurately detect and locate pedestrians via a vision-based deep learning technique to generate safety alerts in real-time about possible conflicts between vehicles and pedestrians. The contribution of this paper lies in the development of a system using a vision-based deep learning model that is able to generate personal safety messages (PSMs) in real-time (every 100 milliseconds). We develop a pedestrian alert safety system (PASS) to generate a safety alert of an imminent pedestrian-vehicle crash using generated PSMs to improve pedestrian safety at a signalized intersection. Our approach estimates the location and velocity of a pedestrian more accurately than existing DSRC-enabled pedestrian hand-held devices. A connected vehicle application, the Pedestrian in Signalized Crosswalk Warning (PSCW), was developed to evaluate the vision-based PASS. Numerical analyses show that our vision-based PASS is able to satisfy the accuracy and latency requirements of pedestrian safety applications in a connected vehicle environment.



### Learnable Gated Temporal Shift Module for Deep Video Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1907.01131v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01131v2)
- **Published**: 2019-07-02 02:38:24+00:00
- **Updated**: 2019-07-09 07:16:48+00:00
- **Authors**: Ya-Liang Chang, Zhe Yu Liu, Kuan-Ying Lee, Winston Hsu
- **Comment**: Accepted to BMVC 2019
- **Journal**: None
- **Summary**: How to efficiently utilize temporal information to recover videos in a consistent way is the main issue for video inpainting problems. Conventional 2D CNNs have achieved good performance on image inpainting but often lead to temporally inconsistent results where frames will flicker when applied to videos (see https://www.youtube.com/watch?v=87Vh1HDBjD0&list=PLPoVtv-xp_dL5uckIzz1PKwNjg1yI0I94&index=1); 3D CNNs can capture temporal information but are computationally intensive and hard to train. In this paper, we present a novel component termed Learnable Gated Temporal Shift Module (LGTSM) for video inpainting models that could effectively tackle arbitrary video masks without additional parameters from 3D convolutions. LGTSM is designed to let 2D convolutions make use of neighboring frames more efficiently, which is crucial for video inpainting. Specifically, in each layer, LGTSM learns to shift some channels to its temporal neighbors so that 2D convolutions could be enhanced to handle temporal information. Meanwhile, a gated convolution is applied to the layer to identify the masked areas that are poisoning for conventional convolutions. On the FaceForensics and Free-form Video Inpainting (FVI) dataset, our model achieves state-of-the-art results with simply 33% of parameters and inference time.



### High-speed Railway Fastener Detection and Localization Method based on convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/1907.01141v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01141v2)
- **Published**: 2019-07-02 03:11:57+00:00
- **Updated**: 2019-07-31 06:51:58+00:00
- **Authors**: Qing Song, Yao Guo, Jianan Jiang, Chun Liu, Mengjie Hu
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: Railway transportation is the artery of China's national economy and plays an important role in the development of today's society. Due to the late start of China's railway security inspection technology, the current railway security inspection tasks mainly rely on manual inspection, but the manual inspection efficiency is low, and a lot of manpower and material resources are needed. In this paper, we establish a steel rail fastener detection image dataset, which contains 4,000 rail fastener pictures about 4 types. We use the regional suggestion network to generate the region of interest, extracts the features using the convolutional neural network, and fuses the classifier into the detection network. With online hard sample mining to improve the accuracy of the model, we optimize the Faster RCNN detection framework by reducing the number of regions of interest. Finally, the model accuracy reaches 99% and the speed reaches 35FPS in the deployment environment of TITAN X GPU.



### Disentangled Makeup Transfer with Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1907.01144v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01144v1)
- **Published**: 2019-07-02 03:19:07+00:00
- **Updated**: 2019-07-02 03:19:07+00:00
- **Authors**: Honglun Zhang, Wenqing Chen, Hao He, Yaohui Jin
- **Comment**: None
- **Journal**: None
- **Summary**: Facial makeup transfer is a widely-used technology that aims to transfer the makeup style from a reference face image to a non-makeup face. Existing literature leverage the adversarial loss so that the generated faces are of high quality and realistic as real ones, but are only able to produce fixed outputs. Inspired by recent advances in disentangled representation, in this paper we propose DMT (Disentangled Makeup Transfer), a unified generative adversarial network to achieve different scenarios of makeup transfer. Our model contains an identity encoder as well as a makeup encoder to disentangle the personal identity and the makeup style for arbitrary face images. Based on the outputs of the two encoders, a decoder is employed to reconstruct the original faces. We also apply a discriminator to distinguish real faces from fake ones. As a result, our model can not only transfer the makeup styles from one or more reference face images to a non-makeup face with controllable strength, but also produce various outputs with styles sampled from a prior distribution. Extensive experiments demonstrate that our model is superior to existing literature by generating high-quality results for different scenarios of makeup transfer.



### Multi-scale Template Matching with Scalable Diversity Similarity in an Unconstrained Environment
- **Arxiv ID**: http://arxiv.org/abs/1907.01150v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01150v1)
- **Published**: 2019-07-02 03:49:05+00:00
- **Updated**: 2019-07-02 03:49:05+00:00
- **Authors**: Yi Zhang, Chao Zhang, Takuya Akashi
- **Comment**: British Machine Vision Conference (BMVC2019)
- **Journal**: None
- **Summary**: We propose a novel multi-scale template matching method which is robust against both scaling and rotation in unconstrained environments. The key component behind is a similarity measure referred to as scalable diversity similarity (SDS). Specifically, SDS exploits bidirectional diversity of the nearest neighbor (NN) matches between two sets of points. To address the scale-robustness of the similarity measure, local appearance and rank information are jointly used for the NN search. Furthermore, by introducing penalty term on the scale change, and polar radius term into the similarity measure, SDS is shown to be a well-performing similarity measure against overall size and rotation changes, as well as non-rigid geometric deformations, background clutter, and occlusions. The properties of SDS are statistically justified, and experiments on both synthetic and real-world data show that SDS can significantly outperform state-of-the-art methods.



### Procedure Planning in Instructional Videos
- **Arxiv ID**: http://arxiv.org/abs/1907.01172v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01172v3)
- **Published**: 2019-07-02 05:17:44+00:00
- **Updated**: 2020-04-13 05:49:55+00:00
- **Authors**: Chien-Yi Chang, De-An Huang, Danfei Xu, Ehsan Adeli, Li Fei-Fei, Juan Carlos Niebles
- **Comment**: 14 pages, 7 figures
- **Journal**: None
- **Summary**: In this paper, we study the problem of procedure planning in instructional videos, which can be seen as a step towards enabling autonomous agents to plan for complex tasks in everyday settings such as cooking. Given the current visual observation of the world and a visual goal, we ask the question "What actions need to be taken in order to achieve the goal?". The key technical challenge is to learn structured and plannable state and action spaces directly from unstructured videos. We address this challenge by proposing Dual Dynamics Networks (DDN), a framework that explicitly leverages the structured priors imposed by the conjugate relationships between states and actions in a learned plannable latent space. We evaluate our method on real-world instructional videos. Our experiments show that DDN learns plannable representations that lead to better planning performance compared to existing planning approaches and neural network policies.



### Multi-Cue Vehicle Detection for Semantic Video Compression In Georegistered Aerial Videos
- **Arxiv ID**: http://arxiv.org/abs/1907.01176v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01176v1)
- **Published**: 2019-07-02 05:40:02+00:00
- **Updated**: 2019-07-02 05:40:02+00:00
- **Authors**: Noor Al-Shakarji, Filiz Bunyak, Hadi Aliakbarpour, Guna Seetharaman, Kannappan Palaniappan
- **Comment**: CVPR2019 Workshop on Detecting Objects in Aerial Images
- **Journal**: None
- **Summary**: Detection of moving objects such as vehicles in videos acquired from an airborne camera is very useful for video analytics applications. Using fast low power algorithms for onboard moving object detection would also provide region of interest-based semantic information for scene content aware image compression. This would enable more efficient and flexible communication link utilization in lowbandwidth airborne cloud computing networks. Despite recent advances in both UAV or drone platforms and imaging sensor technologies, vehicle detection from aerial video remains challenging due to small object sizes, platform motion and camera jitter, obscurations, scene complexity and degraded imaging conditions. This paper proposes an efficient moving vehicle detection pipeline which synergistically fuses both appearance and motion-based detections in a complementary manner using deep learning combined with flux tensor spatio-temporal filtering. Our proposed multi-cue pipeline is able to detect moving vehicles with high precision and recall, while filtering out false positives such as parked vehicles, through intelligent fusion. Experimental results show that incorporating contextual information of moving vehicles enables high semantic compression ratios of over 100:1 with high image fidelity, for better utilization of limited bandwidth air-to-ground network links.



### Generative Guiding Block: Synthesizing Realistic Looking Variants Capable of Even Large Change Demands
- **Arxiv ID**: http://arxiv.org/abs/1907.01187v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01187v1)
- **Published**: 2019-07-02 06:24:21+00:00
- **Updated**: 2019-07-02 06:24:21+00:00
- **Authors**: Minho Park, Hak Gu Kim, Yong Man Ro
- **Comment**: This work is accepted in ICIP 2019
- **Journal**: None
- **Summary**: Realistic image synthesis is to generate an image that is perceptually indistinguishable from an actual image. Generating realistic looking images with large variations (e.g., large spatial deformations and large pose change), however, is very challenging. Handing large variations as well as preserving appearance needs to be taken into account in the realistic looking image generation. In this paper, we propose a novel realistic looking image synthesis method, especially in large change demands. To do that, we devise generative guiding blocks. The proposed generative guiding block includes realistic appearance preserving discriminator and naturalistic variation transforming discriminator. By taking the proposed generative guiding blocks into generative model, the latent features at the layer of generative model are enhanced to synthesize both realistic looking- and target variation- image. With qualitative and quantitative evaluation in experiments, we demonstrated the effectiveness of the proposed generative guiding blocks, compared to the state-of-the-arts.



### Inverse Attention Guided Deep Crowd Counting Network
- **Arxiv ID**: http://arxiv.org/abs/1907.01193v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01193v2)
- **Published**: 2019-07-02 06:48:18+00:00
- **Updated**: 2019-07-22 03:25:42+00:00
- **Authors**: Vishwanath A. Sindagi, Vishal M. Patel
- **Comment**: Accepted at 16th IEEE International Conference on Advanced Video and
  Signal-based Surveillance (AVSS) 2019
- **Journal**: None
- **Summary**: In this paper, we address the challenging problem of crowd counting in congested scenes. Specifically, we present Inverse Attention Guided Deep Crowd Counting Network (IA-DCCN) that efficiently infuses segmentation information through an inverse attention mechanism into the counting network, resulting in significant improvements. The proposed method, which is based on VGG-16, is a single-step training framework and is simple to implement. The use of segmentation information results in minimal computational overhead and does not require any additional annotations. We demonstrate the significance of segmentation guided inverse attention through a detailed analysis and ablation study. Furthermore, the proposed method is evaluated on three challenging crowd counting datasets and is shown to achieve significant improvements over several recent methods.



### Kite: Automatic speech recognition for unmanned aerial vehicles
- **Arxiv ID**: http://arxiv.org/abs/1907.01195v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1907.01195v1)
- **Published**: 2019-07-02 06:50:24+00:00
- **Updated**: 2019-07-02 06:50:24+00:00
- **Authors**: Dan Oneata, Horia Cucu
- **Comment**: 5 pages, accepted at Interspeech 2019
- **Journal**: None
- **Summary**: This paper addresses the problem of building a speech recognition system attuned to the control of unmanned aerial vehicles (UAVs). Even though UAVs are becoming widespread, the task of creating voice interfaces for them is largely unaddressed. To this end, we introduce a multi-modal evaluation dataset for UAV control, consisting of spoken commands and associated images, which represent the visual context of what the UAV "sees" when the pilot utters the command. We provide baseline results and address two research directions: (i) how robust the language models are, given an incomplete list of commands at train time; (ii) how to incorporate visual information in the language model. We find that recurrent neural networks (RNNs) are a solution to both tasks: they can be successfully adapted using a small number of commands and they can be extended to use visual cues. Our results show that the image-based RNN outperforms its text-only counterpart even if the command-image training associations are automatically generated and inherently imperfect. The dataset and our code are available at http://kite.speed.pub.ro.



### Proposal, Tracking and Segmentation (PTS): A Cascaded Network for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.01203v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01203v2)
- **Published**: 2019-07-02 07:22:58+00:00
- **Updated**: 2019-07-04 05:03:38+00:00
- **Authors**: Qiang Zhou, Zilong Huang, Lichao Huang, Yongchao Gong, Han Shen, Chang Huang, Wenyu Liu, Xinggang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Video object segmentation (VOS) aims at pixel-level object tracking given only the annotations in the first frame. Due to the large visual variations of objects in video and the lack of training samples, it remains a difficult task despite the upsurging development of deep learning. Toward solving the VOS problem, we bring in several new insights by the proposed unified framework consisting of object proposal, tracking and segmentation components. The object proposal network transfers objectness information as generic knowledge into VOS; the tracking network identifies the target object from the proposals; and the segmentation network is performed based on the tracking results with a novel dynamic-reference based model adaptation scheme. Extensive experiments have been conducted on the DAVIS'17 dataset and the YouTube-VOS dataset, our method achieves the state-of-the-art performance on several video object segmentation benchmarks. We make the code publicly available at https://github.com/sydney0zq/PTSNet.



### TedEval: A Fair Evaluation Metric for Scene Text Detectors
- **Arxiv ID**: http://arxiv.org/abs/1907.01227v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01227v1)
- **Published**: 2019-07-02 08:19:20+00:00
- **Updated**: 2019-07-02 08:19:20+00:00
- **Authors**: Chae Young Lee, Youngmin Baek, Hwalsuk Lee
- **Comment**: 7 pages, 10 figures, Accepted by Workshop on Industrial Applications
  of Document Analysis and Recognition 2019
- **Journal**: None
- **Summary**: Despite the recent success of scene text detection methods, common evaluation metrics fail to provide a fair and reliable comparison among detectors. They have obvious drawbacks in reflecting the inherent characteristic of text detection tasks, unable to address issues such as granularity, multiline, and character incompleteness. In this paper, we propose a novel evaluation protocol called TedEval (Text detector Evaluation), which evaluates text detections by an instance-level matching and a character-level scoring. Based on a firm standard rewarding behaviors that result in successful recognition, TedEval can act as a reliable standard for comparing and quantizing the detection quality throughout all difficulty levels. In this regard, we believe that TedEval can play a key role in developing state-of-the-art scene text detectors. The code is publicly available at https://github.com/clovaai/TedEval.



### Dual Network Architecture for Few-view CT -- Trained on ImageNet Data and Transferred for Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/1907.01262v6
- **DOI**: 10.1117/12.2531198
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.01262v6)
- **Published**: 2019-07-02 09:46:27+00:00
- **Updated**: 2019-09-12 12:49:18+00:00
- **Authors**: Huidong Xie, Hongming Shan, Wenxiang Cong, Xiaohua Zhang, Shaohua Liu, Ruola Ning, Ge Wang
- **Comment**: 11 pages, 5 figures, 2019 SPIE Optical Engineering + Applications
- **Journal**: None
- **Summary**: X-ray computed tomography (CT) reconstructs cross-sectional images from projection data. However, ionizing X-ray radiation associated with CT scanning might induce cancer and genetic damage. Therefore, the reduction of radiation dose has attracted major attention. Few-view CT image reconstruction is an important topic to reduce the radiation dose. Recently, data-driven algorithms have shown great potential to solve the few-view CT problem. In this paper, we develop a dual network architecture (DNA) for reconstructing images directly from sinograms. In the proposed DNA method, a point-based fully-connected layer learns the backprojection process requesting significantly less memory than the prior arts do. Proposed method uses O(C*N*N_c) parameters where N and N_c denote the dimension of reconstructed images and number of projections respectively. C is an adjustable parameter that can be set as low as 1. Our experimental results demonstrate that DNA produces a competitive performance over the other state-of-the-art methods. Interestingly, natural images can be used to pre-train DNA to avoid overfitting when the amount of real patient images is limited.



### Improving the generalizability of convolutional neural network-based segmentation on CMR images
- **Arxiv ID**: http://arxiv.org/abs/1907.01268v2
- **DOI**: 10.3389/fcvm.2020.00105
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.01268v2)
- **Published**: 2019-07-02 09:57:15+00:00
- **Updated**: 2019-07-03 10:15:20+00:00
- **Authors**: Chen Chen, Wenjia Bai, Rhodri H. Davies, Anish N. Bhuva, Charlotte Manisty, James C. Moon, Nay Aung, Aaron M. Lee, Mihir M. Sanghvi, Kenneth Fung, Jose Miguel Paiva, Steffen E. Petersen, Elena Lukaschuk, Stefan K. Piechnik, Stefan Neubauer, Daniel Rueckert
- **Comment**: 15 pages, 8 figures
- **Journal**: None
- **Summary**: Convolutional neural network (CNN) based segmentation methods provide an efficient and automated way for clinicians to assess the structure and function of the heart in cardiac MR images. While CNNs can generally perform the segmentation tasks with high accuracy when training and test images come from the same domain (e.g. same scanner or site), their performance often degrades dramatically on images from different scanners or clinical sites. We propose a simple yet effective way for improving the network generalization ability by carefully designing data normalization and augmentation strategies to accommodate common scenarios in multi-site, multi-scanner clinical imaging data sets. We demonstrate that a neural network trained on a single-site single-scanner dataset from the UK Biobank can be successfully applied to segmenting cardiac MR images across different sites and different scanners without substantial loss of accuracy. Specifically, the method was trained on a large set of 3,975 subjects from the UK Biobank. It was then directly tested on 600 different subjects from the UK Biobank for intra-domain testing and two other sets for cross-domain testing: the ACDC dataset (100 subjects, 1 site, 2 scanners) and the BSCMR-AS dataset (599 subjects, 6 sites, 9 scanners). The proposed method produces promising segmentation results on the UK Biobank test set which are comparable to previously reported values in the literature, while also performing well on cross-domain test sets, achieving a mean Dice metric of 0.90 for the left ventricle, 0.81 for the myocardium and 0.82 for the right ventricle on the ACDC dataset; and 0.89 for the left ventricle, 0.83 for the myocardium on the BSCMR-AS dataset. The proposed method offers a potential solution to improve CNN-based model generalizability for the cross-scanner and cross-site cardiac MR image segmentation task.



### An Analysis of Deep Neural Networks with Attention for Action Recognition from a Neurophysiological Perspective
- **Arxiv ID**: http://arxiv.org/abs/1907.01273v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01273v1)
- **Published**: 2019-07-02 10:07:06+00:00
- **Updated**: 2019-07-02 10:07:06+00:00
- **Authors**: Swathikiran Sudhakaran, Oswald Lanz
- **Comment**: Presented as an extended abstract in the Mutual benefits of cognitive
  and computer vision (MBCCV) workshop, CVPR 2019
- **Journal**: None
- **Summary**: We review three recent deep learning based methods for action recognition and present a brief comparative analysis of the methods from a neurophyisiological point of view. We posit that there are some analogy between the three presented deep learning based methods and some of the existing hypotheses regarding the functioning of human brain.



### Semi-Bagging Based Deep Neural Architecture to Extract Text from High Entropy Images
- **Arxiv ID**: http://arxiv.org/abs/1907.01284v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.01284v1)
- **Published**: 2019-07-02 10:26:14+00:00
- **Updated**: 2019-07-02 10:26:14+00:00
- **Authors**: Pranay Dugar, Anirban Chatterjee, Rajesh Shreedhar Bhat, Saswata Sahoo
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Extracting texts of various size and shape from images containing multiple objects is an important problem in many contexts, especially, in connection to e-commerce, augmented reality assistance system in natural scene, etc. The existing works (based on only CNN) often perform sub-optimally when the image contains regions of high entropy having multiple objects. This paper presents an end-to-end text detection strategy combining a segmentation algorithm and an ensemble of multiple text detectors of different types to detect text in every individual image segments independently. The proposed strategy involves a super-pixel based image segmenter which splits an image into multiple regions. A convolutional deep neural architecture is developed which works on each of the segments and detects texts of multiple shapes, sizes, and structures. It outperforms the competing methods in terms of coverage in detecting texts in images especially the ones where the text of various types and sizes are compacted in a small region along with various other objects. Furthermore, the proposed text detection method along with a text recognizer outperforms the existing state-of-the-art approaches in extracting text from high entropy images. We validate the results on a dataset consisting of product images on an e-commerce website.



### Lane Detection and Classification using Cascaded CNNs
- **Arxiv ID**: http://arxiv.org/abs/1907.01294v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01294v2)
- **Published**: 2019-07-02 10:54:06+00:00
- **Updated**: 2019-07-17 18:52:21+00:00
- **Authors**: Fabio Pizzati, Marco Allodi, Alejandro Barrera, Fernando García
- **Comment**: Presented at Eurocast 2019
- **Journal**: None
- **Summary**: Lane detection is extremely important for autonomous vehicles. For this reason, many approaches use lane boundary information to locate the vehicle inside the street, or to integrate GPS-based localization. As many other computer vision based tasks, convolutional neural networks (CNNs) represent the state-of-the-art technology to indentify lane boundaries. However, the position of the lane boundaries w.r.t. the vehicle may not suffice for a reliable positioning, as for path planning or localization information regarding lane types may also be needed. In this work, we present an end-to-end system for lane boundary identification, clustering and classification, based on two cascaded neural networks, that runs in real-time. To build the system, 14336 lane boundaries instances of the TuSimple dataset for lane detection have been labelled using 8 different classes. Our dataset and the code for inference are available online.



### Dynamic Face Video Segmentation via Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.01296v4
- **DOI**: 10.1109/CVPR42600.2020.00699
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01296v4)
- **Published**: 2019-07-02 11:07:26+00:00
- **Updated**: 2021-02-27 17:27:53+00:00
- **Authors**: Yujiang Wang, Mingzhi Dong, Jie Shen, Yang Wu, Shiyang Cheng, Maja Pantic
- **Comment**: CVPR 2020. 300VW with segmentation labels is available at:
  https://github.com/mapleandfire/300VW-Mask
- **Journal**: None
- **Summary**: For real-time semantic video segmentation, most recent works utilised a dynamic framework with a key scheduler to make online key/non-key decisions. Some works used a fixed key scheduling policy, while others proposed adaptive key scheduling methods based on heuristic strategies, both of which may lead to suboptimal global performance. To overcome this limitation, we model the online key decision process in dynamic video segmentation as a deep reinforcement learning problem and learn an efficient and effective scheduling policy from expert information about decision history and from the process of maximising global return. Moreover, we study the application of dynamic video segmentation on face videos, a field that has not been investigated before. By evaluating on the 300VW dataset, we show that the performance of our reinforcement key scheduler outperforms that of various baselines in terms of both effective key selections and running speed. Further results on the Cityscapes dataset demonstrate that our proposed method can also generalise to other scenarios. To the best of our knowledge, this is the first work to use reinforcement learning for online key-frame decision in dynamic video segmentation, and also the first work on its application on face videos.



### An Integrated Image Filter for Enhancing Change Detection Results
- **Arxiv ID**: http://arxiv.org/abs/1907.01301v1
- **DOI**: 10.1109/ACCESS.2019.2927255
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01301v1)
- **Published**: 2019-07-02 11:32:35+00:00
- **Updated**: 2019-07-02 11:32:35+00:00
- **Authors**: Dawei Li, Siyuan Yan, Xin Cai, Yan Cao, Sifan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Change detection is a fundamental task in computer vision. Despite significant advances have been made, most of the change detection methods fail to work well in challenging scenes due to ubiquitous noise and interferences. Nowadays, post-processing methods (e.g. MRF, and CRF) aiming to enhance the binary change detection results still fall short of the requirements on universality for distinctive scenes, applicability for different types of detection methods, accuracy, and real-time performance. Inspired by the nature of image filtering, which separates noise from pixel observations and recovers the real structure of patches, we consider utilizing image filters to enhance the detection masks. In this paper, we present an integrated filter which comprises a weighted local guided image filter and a weighted spatiotemporal tree filter. The spatiotemporal tree filter leverages the global spatiotemporal information of adjacent video frames and meanwhile the guided filter carries out local window filtering of pixels, for enhancing the coarse change detection masks. The main contributions are three: (i) the proposed filter can make full use of the information of the same object in consecutive frames to improve its current detection mask by computations on a spatiotemporal minimum spanning tree; (ii) the integrated filter possesses both advantages of local filtering and global filtering; it not only has good edge-preserving property but also can handle heavily textured and colorful foreground regions; and (iii) Unlike some popular enhancement methods (MRF, and CRF) that require either a priori background probabilities or a posteriori foreground probabilities for every pixel to improve the coarse detection masks, our method is a versatile enhancement filter that can be applied after many different types of change detection methods, and is particularly suitable for video sequences.



### Brno Mobile OCR Dataset
- **Arxiv ID**: http://arxiv.org/abs/1907.01307v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01307v1)
- **Published**: 2019-07-02 11:53:42+00:00
- **Updated**: 2019-07-02 11:53:42+00:00
- **Authors**: Martin Kišš, Michal Hradiš, Oldřich Kodym
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce the Brno Mobile OCR Dataset (B-MOD) for document Optical Character Recognition from low-quality images captured by handheld mobile devices. While OCR of high-quality scanned documents is a mature field where many commercial tools are available, and large datasets of text in the wild exist, no existing datasets can be used to develop and test document OCR methods robust to non-uniform lighting, image blur, strong noise, built-in denoising, sharpening, compression and other artifacts present in many photographs from mobile devices.   This dataset contains 2 113 unique pages from random scientific papers, which were photographed by multiple people using 23 different mobile devices. The resulting 19 728 photographs of various visual quality are accompanied by precise positions and text annotations of 500k text lines. We further provide an evaluation methodology, including an evaluation server and a testset with non-public annotations.   We provide a state-of-the-art text recognition baseline build on convolutional and recurrent neural networks trained with Connectionist Temporal Classification loss. This baseline achieves 2 %, 22 % and 73 % word error rates on easy, medium and hard parts of the dataset, respectively, confirming that the dataset is challenging.   The presented dataset will enable future development and evaluation of document analysis for low-quality images. It is primarily intended for line-level text recognition, and can be further used for line localization, layout analysis, image restoration and text binarization.



### Unsupervised Deformable Image Registration Using Cycle-Consistent CNN
- **Arxiv ID**: http://arxiv.org/abs/1907.01319v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.01319v1)
- **Published**: 2019-07-02 12:29:02+00:00
- **Updated**: 2019-07-02 12:29:02+00:00
- **Authors**: Boah Kim, Jieun Kim, June-Goo Lee, Dong Hwan Kim, Seong Ho Park, Jong Chul Ye
- **Comment**: accepted for MICCAI 2019
- **Journal**: None
- **Summary**: Medical image registration is one of the key processing steps for biomedical image analysis such as cancer diagnosis. Recently, deep learning based supervised and unsupervised image registration methods have been extensively studied due to its excellent performance in spite of ultra-fast computational time compared to the classical approaches. In this paper, we present a novel unsupervised medical image registration method that trains deep neural network for deformable registration of 3D volumes using a cycle-consistency. Thanks to the cycle consistency, the proposed deep neural networks can take diverse pair of image data with severe deformation for accurate registration. Experimental results using multiphase liver CT images demonstrate that our method provides very precise 3D image registration within a few seconds, resulting in more accurate cancer size estimation.



### Human Body Parts Tracking: Applications to Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.05281v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.05281v1)
- **Published**: 2019-07-02 12:40:13+00:00
- **Updated**: 2019-07-02 12:40:13+00:00
- **Authors**: Aras R. Dargazany
- **Comment**: None
- **Journal**: None
- **Summary**: As cameras and computers became popular, the applications of computer vision techniques attracted attention enormously. One of the most important applications in the computer vision community is human activity recognition. In order to recognize human activities, we propose a human body parts tracking system that tracks human body parts such as head, torso, arms and legs in order to perform activity recognition tasks in real time. This thesis presents a real-time human body parts tracking system (i.e. HBPT) from video sequences. Our body parts model is mostly represented by body components such as legs, head, torso and arms. The body components are modeled using torso location and size which are obtained by a torso tracking method in each frame. In order to track the torso, we are using a blob tracking module to find the approximate location and size of the torso in each frame. By tracking the torso, we will be able to track other body parts based on their location with respect to the torso on the detected silhouette. In the proposed method for human body part tracking, we are also using a refining module to improve the detected silhouette by refining the foreground mask (i.e. obtained by background subtraction) in order to detect the body parts with respect to torso location and size. Having found the torso size and location, the region of each human body part on the silhouette will be modeled by a 2D-Gaussian blob in each frame in order to show its location, size and pose. The proposed approach described in this thesis tracks accurately the body parts in different illumination conditions and in the presence of partial occlusions. The proposed approach is applied to activity recognition tasks such as approaching an object, carrying an object and opening a box or suitcase.



### Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer
- **Arxiv ID**: http://arxiv.org/abs/1907.01341v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01341v3)
- **Published**: 2019-07-02 13:16:52+00:00
- **Updated**: 2020-08-25 09:37:24+00:00
- **Authors**: René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, Vladlen Koltun
- **Comment**: To appear in TPAMI (accepted August 2020)
- **Journal**: None
- **Summary**: The success of monocular depth estimation relies on large and diverse training sets. Due to the challenges associated with acquiring dense ground-truth depth across different environments at scale, a number of datasets with distinct characteristics and biases have emerged. We develop tools that enable mixing multiple datasets during training, even if their annotations are incompatible. In particular, we propose a robust training objective that is invariant to changes in depth range and scale, advocate the use of principled multi-objective learning to combine data from different sources, and highlight the importance of pretraining encoders on auxiliary tasks. Armed with these tools, we experiment with five diverse training datasets, including a new, massive data source: 3D films. To demonstrate the generalization power of our approach we use zero-shot cross-dataset transfer}, i.e. we evaluate on datasets that were not seen during training. The experiments confirm that mixing data from complementary sources greatly improves monocular depth estimation. Our approach clearly outperforms competing methods across diverse datasets, setting a new state of the art for monocular depth estimation. Some results are shown in the supplementary video at https://youtu.be/D46FzVyL9I8



### The Ethical Dilemma when (not) Setting up Cost-based Decision Rules in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.01342v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, 62-07, 62C05
- **Links**: [PDF](http://arxiv.org/pdf/1907.01342v1)
- **Published**: 2019-07-02 13:17:14+00:00
- **Updated**: 2019-07-02 13:17:14+00:00
- **Authors**: Robin Chan, Matthias Rottmann, Radin Dardashti, Fabian Hüger, Peter Schlicht, Hanno Gottschalk
- **Comment**: None
- **Journal**: CVPR-W SAIAD 2019
- **Summary**: Neural networks for semantic segmentation can be seen as statistical models that provide for each pixel of one image a probability distribution on predefined classes. The predicted class is then usually obtained by the maximum a-posteriori probability (MAP) which is known as Bayes rule in decision theory. From decision theory we also know that the Bayes rule is optimal regarding the simple symmetric cost function. Therefore, it weights each type of confusion between two different classes equally, e.g., given images of urban street scenes there is no distinction in the cost function if the network confuses a person with a street or a building with a tree. Intuitively, there might be confusions of classes that are more important to avoid than others. In this work, we want to raise awareness of the possibility of explicitly defining confusion costs and the associated ethical difficulties if it comes down to providing numbers. We define two cost functions from different extreme perspectives, an egoistic and an altruistic one, and show how safety relevant quantities like precision / recall and (segment-wise) false positive / negative rate change when interpolating between MAP, egoistic and altruistic decision rules.



### Pathologist-Level Grading of Prostate Biopsies with Artificial Intelligence
- **Arxiv ID**: http://arxiv.org/abs/1907.01368v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.01368v1)
- **Published**: 2019-07-02 13:52:02+00:00
- **Updated**: 2019-07-02 13:52:02+00:00
- **Authors**: Peter Ström, Kimmo Kartasalo, Henrik Olsson, Leslie Solorzano, Brett Delahunt, Daniel M. Berney, David G. Bostwick, Andrew J. Evans, David J. Grignon, Peter A. Humphrey, Kenneth A. Iczkowski, James G. Kench, Glen Kristiansen, Theodorus H. van der Kwast, Katia R. M. Leite, Jesse K. McKenney, Jon Oxley, Chin-Chen Pan, Hemamali Samaratunga, John R. Srigley, Hiroyuki Takahashi, Toyonori Tsuzuki, Murali Varma, Ming Zhou, Johan Lindberg, Cecilia Bergström, Pekka Ruusuvuori, Carolina Wählby, Henrik Grönberg, Mattias Rantalainen, Lars Egevad, Martin Eklund
- **Comment**: 45 pages, 11 figures
- **Journal**: None
- **Summary**: Background: An increasing volume of prostate biopsies and a world-wide shortage of uro-pathologists puts a strain on pathology departments. Additionally, the high intra- and inter-observer variability in grading can result in over- and undertreatment of prostate cancer. Artificial intelligence (AI) methods may alleviate these problems by assisting pathologists to reduce workload and harmonize grading.   Methods: We digitized 6,682 needle biopsies from 976 participants in the population based STHLM3 diagnostic study to train deep neural networks for assessing prostate biopsies. The networks were evaluated by predicting the presence, extent, and Gleason grade of malignant tissue for an independent test set comprising 1,631 biopsies from 245 men. We additionally evaluated grading performance on 87 biopsies individually graded by 23 experienced urological pathologists from the International Society of Urological Pathology. We assessed discriminatory performance by receiver operating characteristics (ROC) and tumor extent predictions by correlating predicted millimeter cancer length against measurements by the reporting pathologist. We quantified the concordance between grades assigned by the AI and the expert urological pathologists using Cohen's kappa.   Results: The performance of the AI to detect and grade cancer in prostate needle biopsy samples was comparable to that of international experts in prostate pathology. The AI achieved an area under the ROC curve of 0.997 for distinguishing between benign and malignant biopsy cores, and 0.999 for distinguishing between men with or without prostate cancer. The correlation between millimeter cancer predicted by the AI and assigned by the reporting pathologist was 0.96. For assigning Gleason grades, the AI achieved an average pairwise kappa of 0.62. This was within the range of the corresponding values for the expert pathologists (0.60 to 0.73).



### Multi-scale GANs for Memory-efficient Generation of High Resolution Medical Images
- **Arxiv ID**: http://arxiv.org/abs/1907.01376v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.01376v2)
- **Published**: 2019-07-02 13:59:24+00:00
- **Updated**: 2019-07-08 09:09:17+00:00
- **Authors**: Hristina Uzunova, Jan Ehrhardt, Fabian Jacob, Alex Frydrychowicz, Heinz Handels
- **Comment**: Accepted at MICCAI 2019
- **Journal**: None
- **Summary**: Currently generative adversarial networks (GANs) are rarely applied to medical images of large sizes, especially 3D volumes, due to their large computational demand. We propose a novel multi-scale patch-based GAN approach to generate large high resolution 2D and 3D images. Our key idea is to first learn a low-resolution version of the image and then generate patches of successively growing resolutions conditioned on previous scales. In a domain translation use-case scenario, 3D thorax CTs of size 512x512x512 and thorax X-rays of size 2048x2048 are generated and we show that, due to the constant GPU memory demand of our method, arbitrarily large images of high resolution can be generated. Moreover, compared to common patch-based approaches, our multi-resolution scheme enables better image quality and prevents patch artifacts.



### Training Auto-encoder-based Optimizers for Terahertz Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1907.01377v2
- **DOI**: 10.1007/978-3-030-33676-9_7
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.01377v2)
- **Published**: 2019-07-02 14:01:35+00:00
- **Updated**: 2019-10-29 11:56:05+00:00
- **Authors**: Tak Ming Wong, Matthias Kahl, Peter Haring Bolívar, Andreas Kolb, Michael Möller
- **Comment**: This is a pre-print of a conference paper published in German
  Conference on Pattern Recognition (GCPR) 2019
- **Journal**: Pattern Recognition. DAGM GCPR 2019. Lecture Notes in Computer
  Science, vol 11824. Springer, Cham
- **Summary**: Terahertz (THz) sensing is a promising imaging technology for a wide variety of different applications. Extracting the interpretable and physically meaningful parameters for such applications, however, requires solving an inverse problem in which a model function determined by these parameters needs to be fitted to the measured data. Since the underlying optimization problem is nonconvex and very costly to solve, we propose learning the prediction of suitable parameters from the measured data directly. More precisely, we develop a model-based autoencoder in which the encoder network predicts suitable parameters and the decoder is fixed to a physically meaningful model function, such that we can train the encoding network in an unsupervised way. We illustrate numerically that the resulting network is more than 140 times faster than classical optimization techniques while making predictions with only slightly higher objective values. Using such predictions as starting points of local optimization techniques allows us to converge to better local minima about twice as fast as optimization without the network-based initialization.



### CSSegNet: Fine-Grained Cardiac Structures Segmentation Using Dilated Pyramid Pooling in U-net
- **Arxiv ID**: http://arxiv.org/abs/1907.01390v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1907.01390v1)
- **Published**: 2019-07-02 14:17:31+00:00
- **Updated**: 2019-07-02 14:17:31+00:00
- **Authors**: Fei Feng, Jiajia Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Cardiac structure segmentation plays an important role in medical analysis procedures. Images' blurred boundaries issue always limits the segmentation performance. To address this difficult problem, we presented a novel network structure which embedded dilated pyramid pooling block in the skip connections between networks' encoding and decoding stage. A dilated pyramid pooling block is made up of convolutions and pooling operations with different vision scopes. Equipped the model with such module, it could be endowed with multi-scales vision ability. Together combining with other techniques, it included a multi-scales initial features extraction and a multi-resolutions' prediction aggregation module. As for backbone feature extraction network, we referred to the basic idea of Xception network which benefited from separable convolutions. Evaluated on the Post 2017 MICCAI-ACDC challenge phase data, our proposed model could achieve state-of-the-art performance in left ventricle (LVC) cavities and right ventricle cavities (RVC) segmentation tasks. Results revealed that our method has advantages on both geometrical (Dice coefficient, Hausdorff distance) and clinical evaluation (Ejection Fraction, Volume), which represent closer boundaries and more statistically significant separately.



### A Single Video Super-Resolution GAN for Multiple Downsampling Operators based on Pseudo-Inverse Image Formation Models
- **Arxiv ID**: http://arxiv.org/abs/1907.01399v1
- **DOI**: 10.1016/j.dsp.2020.102801
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.01399v1)
- **Published**: 2019-07-02 14:28:27+00:00
- **Updated**: 2019-07-02 14:28:27+00:00
- **Authors**: Santiago López-Tapia, Alice Lucas, Rafael Molina, Aggelos K. Katsaggelos
- **Comment**: None
- **Journal**: None
- **Summary**: The popularity of high and ultra-high definition displays has led to the need for methods to improve the quality of videos already obtained at much lower resolutions. Current Video Super-Resolution methods are not robust to mismatch between training and testing degradation models since they are trained against a single degradation model (usually bicubic downsampling). This causes their performance to deteriorate in real-life applications. At the same time, the use of only the Mean Squared Error during learning causes the resulting images to be too smooth. In this work we propose a new Convolutional Neural Network for video super resolution which is robust to multiple degradation models. During training, which is performed on a large dataset of scenes with slow and fast motions, it uses the pseudo-inverse image formation model as part of the network architecture in conjunction with perceptual losses, in addition to a smoothness constraint that eliminates the artifacts originating from these perceptual losses. The experimental validation shows that our approach outperforms current state-of-the-art methods and is robust to multiple degradations.



### A Closest Point Proposal for MCMC-based Probabilistic Surface Registration
- **Arxiv ID**: http://arxiv.org/abs/1907.01414v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01414v2)
- **Published**: 2019-07-02 14:43:22+00:00
- **Updated**: 2020-07-17 13:31:04+00:00
- **Authors**: Dennis Madsen, Andreas Morel-Forster, Patrick Kahr, Dana Rahbani, Thomas Vetter, Marcel Lüthi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose to view non-rigid surface registration as a probabilistic inference problem. Given a target surface, we estimate the posterior distribution of surface registrations. We demonstrate how the posterior distribution can be used to build shape models that generalize better and show how to visualize the uncertainty in the established correspondence. Furthermore, in a reconstruction task, we show how to estimate the posterior distribution of missing data without assuming a fixed point-to-point correspondence.   We introduce the closest-point proposal for the Metropolis-Hastings algorithm. Our proposal overcomes the limitation of slow convergence compared to a random-walk strategy. As the algorithm decouples inference from modeling the posterior using a propose-and-verify scheme, we show how to choose different distance measures for the likelihood model.   All presented results are fully reproducible using publicly available data and our open-source implementation of the registration framework.



### Landmark Assisted CycleGAN for Cartoon Face Generation
- **Arxiv ID**: http://arxiv.org/abs/1907.01424v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01424v1)
- **Published**: 2019-07-02 15:00:50+00:00
- **Updated**: 2019-07-02 15:00:50+00:00
- **Authors**: Ruizheng Wu, Xiaodong Gu, Xin Tao, Xiaoyong Shen, Yu-Wing Tai, J iaya Jia
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we are interested in generating an cartoon face of a person by using unpaired training data between real faces and cartoon ones. A major challenge of this task is that the structures of real and cartoon faces are in two different domains, whose appearance differs greatly from each other. Without explicit correspondence, it is difficult to generate a high quality cartoon face that captures the essential facial features of a person. In order to solve this problem, we propose landmark assisted CycleGAN, which utilizes face landmarks to define landmark consistency loss and to guide the training of local discriminator in CycleGAN. To enforce structural consistency in landmarks, we utilize the conditional generator and discriminator. Our approach is capable to generate high-quality cartoon faces even indistinguishable from those drawn by artists and largely improves state-of-the-art.



### Improving Borderline Adulthood Facial Age Estimation through Ensemble Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.01427v1
- **DOI**: 10.1145/3339252.3341491
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.01427v1)
- **Published**: 2019-07-02 15:05:24+00:00
- **Updated**: 2019-07-02 15:05:24+00:00
- **Authors**: Felix Anda, David Lillis, Aikaterini Kanta, Brett A. Becker, Elias Bou-Harb, Nhien-An Le-Khac, Mark Scanlon
- **Comment**: None
- **Journal**: 14th International Conference on Availability, Reliability and
  Security (ARES 2019), Canterbury, UK, August 2019
- **Summary**: Achieving high performance for facial age estimation with subjects in the borderline between adulthood and non-adulthood has always been a challenge. Several studies have used different approaches from the age of a baby to an elder adult and different datasets have been employed to measure the mean absolute error (MAE) ranging between 1.47 to 8 years. The weakness of the algorithms specifically in the borderline has been a motivation for this paper. In our approach, we have developed an ensemble technique that improves the accuracy of underage estimation in conjunction with our deep learning model (DS13K) that has been fine-tuned on the Deep Expectation (DEX) model. We have achieved an accuracy of 68% for the age group 16 to 17 years old, which is 4 times better than the DEX accuracy for such age range. We also present an evaluation of existing cloud-based and offline facial age prediction services, such as Amazon Rekognition, Microsoft Azure Cognitive Services, How-Old.net and DEX.



### Where are the Masks: Instance Segmentation with Image-level Supervision
- **Arxiv ID**: http://arxiv.org/abs/1907.01430v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.01430v1)
- **Published**: 2019-07-02 15:12:20+00:00
- **Updated**: 2019-07-02 15:12:20+00:00
- **Authors**: Issam H. Laradji, David Vazquez, Mark Schmidt
- **Comment**: Accepted at BMVC2019
- **Journal**: None
- **Summary**: A major obstacle in instance segmentation is that existing methods often need many per-pixel labels in order to be effective. These labels require large human effort and for certain applications, such labels are not readily available. To address this limitation, we propose a novel framework that can effectively train with image-level labels, which are significantly cheaper to acquire. For instance, one can do an internet search for the term "car" and obtain many images where a car is present with minimal effort. Our framework consists of two stages: (1) train a classifier to generate pseudo masks for the objects of interest; (2) train a fully supervised Mask R-CNN on these pseudo masks. Our two main contribution are proposing a pipeline that is simple to implement and is amenable to different segmentation methods; and achieves new state-of-the-art results for this problem setup. Our results are based on evaluating our method on PASCAL VOC 2012, a standard dataset for weakly supervised methods, where we demonstrate major performance gains compared to existing methods with respect to mean average precision.



### An End-to-End Neural Network for Image Cropping by Learning Composition from Aesthetic Photos
- **Arxiv ID**: http://arxiv.org/abs/1907.01432v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01432v3)
- **Published**: 2019-07-02 15:18:33+00:00
- **Updated**: 2019-08-22 16:25:26+00:00
- **Authors**: Peng Lu, Hao Zhang, Xujun Peng, Xiaofu Jin
- **Comment**: None
- **Journal**: None
- **Summary**: As one of the fundamental techniques for image editing, image cropping discards unrelevant contents and remains the pleasing portions of the image to enhance the overall composition and achieve better visual/aesthetic perception. In this paper, we primarily focus on improving the accuracy of automatic image cropping, and on further exploring its potential in public datasets with high efficiency. From this respect, we propose a deep learning based framework to learn the objects composition from photos with high aesthetic qualities, where an anchor region is detected through a convolutional neural network (CNN) with the Gaussian kernel to maintain the interested objects' integrity. This initial detected anchor area is then fed into a light weighted regression network to obtain the final cropping result. Unlike the conventional methods that multiple candidates are proposed and evaluated iteratively, only a single anchor region is produced in our model, which is mapped to the final output directly. Thus, low computational resources are required for the proposed approach. The experimental results on the public datasets show that both cropping accuracy and efficiency achieve the state-ofthe-art performances.



### Attribute-Driven Spontaneous Motion in Unpaired Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1907.01452v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01452v2)
- **Published**: 2019-07-02 15:32:25+00:00
- **Updated**: 2019-10-11 09:25:20+00:00
- **Authors**: Ruizheng Wu, Xin Tao, Xiaodong Gu, Xiaoyong Shen, Jiaya Jia
- **Comment**: Accepted by ICCV 2019
- **Journal**: None
- **Summary**: Current image translation methods, albeit effective to produce high-quality results in various applications, still do not consider much geometric transform. We in this paper propose the spontaneous motion estimation module, along with a refinement part, to learn attribute-driven deformation between source and target domains. Extensive experiments and visualization demonstrate effectiveness of these modules. We achieve promising results in unpaired-image translation tasks, and enable interesting applications based on spontaneous motion.



### Obj-GloVe: Scene-Based Contextual Object Embedding
- **Arxiv ID**: http://arxiv.org/abs/1907.01478v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.01478v1)
- **Published**: 2019-07-02 16:29:02+00:00
- **Updated**: 2019-07-02 16:29:02+00:00
- **Authors**: Canwen Xu, Zhenzhong Chen, Chenliang Li
- **Comment**: 14 pages; not the final version
- **Journal**: None
- **Summary**: Recently, with the prevalence of large-scale image dataset, the co-occurrence information among classes becomes rich, calling for a new way to exploit it to facilitate inference. In this paper, we propose Obj-GloVe, a generic scene-based contextual embedding for common visual objects, where we adopt the word embedding method GloVe to exploit the co-occurrence between entities. We train the embedding on pre-processed Open Images V4 dataset and provide extensive visualization and analysis by dimensionality reduction and projecting the vectors along a specific semantic axis, and showcasing the nearest neighbors of the most common objects. Furthermore, we reveal the potential applications of Obj-GloVe on object detection and text-to-image synthesis, then verify its effectiveness on these two applications respectively.



### HOnnotate: A method for 3D Annotation of Hand and Object Poses
- **Arxiv ID**: http://arxiv.org/abs/1907.01481v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.01481v6)
- **Published**: 2019-07-02 16:39:05+00:00
- **Updated**: 2020-05-30 20:37:29+00:00
- **Authors**: Shreyas Hampali, Mahdi Rad, Markus Oberweger, Vincent Lepetit
- **Comment**: Accepted to CVPR2020
- **Journal**: None
- **Summary**: We propose a method for annotating images of a hand manipulating an object with the 3D poses of both the hand and the object, together with a dataset created using this method. Our motivation is the current lack of annotated real images for this problem, as estimating the 3D poses is challenging, mostly because of the mutual occlusions between the hand and the object. To tackle this challenge, we capture sequences with one or several RGB-D cameras and jointly optimize the 3D hand and object poses over all the frames simultaneously. This method allows us to automatically annotate each frame with accurate estimates of the poses, despite large mutual occlusions. With this method, we created HO-3D, the first markerless dataset of color images with 3D annotations for both the hand and object. This dataset is currently made of 77,558 frames, 68 sequences, 10 persons, and 10 objects. Using our dataset, we develop a single RGB image-based method to predict the hand pose when interacting with objects under severe occlusions and show it generalizes to objects not seen in the dataset.



### Hyper-Molecules: on the Representation and Recovery of Dynamical Structures, with Application to Flexible Macro-Molecular Structures in Cryo-EM
- **Arxiv ID**: http://arxiv.org/abs/1907.01589v1
- **DOI**: 10.1088/1361-6420/ab5ede
- **Categories**: **cs.CV**, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1907.01589v1)
- **Published**: 2019-07-02 19:16:12+00:00
- **Updated**: 2019-07-02 19:16:12+00:00
- **Authors**: Roy R. Lederman, Joakim Andén, Amit Singer
- **Comment**: None
- **Journal**: None
- **Summary**: Cryo-electron microscopy (cryo-EM), the subject of the 2017 Nobel Prize in Chemistry, is a technology for determining the 3-D structure of macromolecules from many noisy 2-D projections of instances of these macromolecules, whose orientations and positions are unknown. The molecular structures are not rigid objects, but flexible objects involved in dynamical processes. The different conformations are exhibited by different instances of the macromolecule observed in a cryo-EM experiment, each of which is recorded as a particle image. The range of conformations and the conformation of each particle are not known a priori; one of the great promises of cryo-EM is to map this conformation space. Remarkable progress has been made in determining rigid structures from homogeneous samples of molecules in spite of the unknown orientation of each particle image and significant progress has been made in recovering a few distinct states from mixtures of rather distinct conformations, but more complex heterogeneous samples remain a major challenge. We introduce the ``hyper-molecule'' framework for modeling structures across different states of heterogeneous molecules, including continuums of states. The key idea behind this framework is representing heterogeneous macromolecules as high-dimensional objects, with the additional dimensions representing the conformation space. This idea is then refined to model properties such as localized heterogeneity. In addition, we introduce an algorithmic framework for recovering such maps of heterogeneous objects from experimental data using a Bayesian formulation of the problem and Markov chain Monte Carlo (MCMC) algorithms to address the computational challenges in recovering these high dimensional hyper-molecules. We demonstrate these ideas in a prototype applied to synthetic data.



### Estimation of Absolute States of Human Skeletal Muscle via Standard B-Mode Ultrasound Imaging and Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.01649v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.01649v1)
- **Published**: 2019-07-02 21:09:27+00:00
- **Updated**: 2019-07-02 21:09:27+00:00
- **Authors**: Ryan J. Cunningham, Ian D. Loram
- **Comment**: 10 pages, 5 figures, 2 tables. This paper is currently under review
- **Journal**: None
- **Summary**: Objective: To test automated in vivo estimation of active and passive skeletal muscle states using ultrasonic imaging. Background: Current technology (electromyography, dynamometry, shear wave imaging) provides no general, non-invasive method for online estimation of skeletal intramuscular states. Ultrasound (US) allows non-invasive imaging of muscle, yet current computational approaches have never achieved simultaneous extraction nor generalisation of independently varying, active and passive states. We use deep learning to investigate the generalizable content of 2D US muscle images. Method: US data synchronized with electromyography of the calf muscles, with measures of joint moment/angle were recorded from 32 healthy participants (7 female, ages: 27.5, 19-65). We extracted a region of interest of medial gastrocnemius and soleus using our prior developed accurate segmentation algorithm. From the segmented images, a deep convolutional neural network was trained to predict three absolute, drift-free, components of the neurobiomechanical state (activity, joint angle, joint moment) during experimentally designed, simultaneous, independent variation of passive (joint angle) and active (electromyography) inputs. Results: For all 32 held-out participants (16-fold cross-validation) the ankle joint angle, electromyography, and joint moment were estimated to accuracy 55+-8%, 57+-11%, and 46+-9% respectively. Significance: With 2D US imaging, deep neural networks can encode in generalizable form, the activity-length-tension state relationship of muscle. Observation only, low power, 2D US imaging can provide a new category of technology for non-invasive estimation of neural output, length and tension in skeletal muscle. This proof of principle has value for personalised muscle diagnosis in pain, injury, neurological conditions, neuropathies, myopathies and ageing.



### Automated Detection and Type Classification of Central Venous Catheters in Chest X-Rays
- **Arxiv ID**: http://arxiv.org/abs/1907.01656v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.01656v3)
- **Published**: 2019-07-02 21:31:20+00:00
- **Updated**: 2019-07-25 17:33:48+00:00
- **Authors**: Vaishnavi Subramanian, Hongzhi Wang, Joy T. Wu, Ken C. L. Wong, Arjun Sharma, Tanveer Syeda-Mahmood
- **Comment**: Accepted to Medical Image Computing and Computer Assisted
  Intervention (MICCAI) 2019; Data available: ML-CDS Challenge, MICCAI2019
  (http://www.mcbr-cds.org/challenge/challenge-description.html)
- **Journal**: None
- **Summary**: Central venous catheters (CVCs) are commonly used in critical care settings for monitoring body functions and administering medications. They are often described in radiology reports by referring to their presence, identity and placement. In this paper, we address the problem of automatic detection of their presence and identity through automated segmentation using deep learning networks and classification based on their intersection with previously learned shape priors from clinician annotations of CVCs. The results not only outperform existing methods of catheter detection achieving 85.2% accuracy at 91.6% precision, but also enable high precision (95.2%) classification of catheter types on a large dataset of over 10,000 chest X-rays, presenting a robust and practical solution to this problem.



### Graph Neural Network for Interpreting Task-fMRI Biomarkers
- **Arxiv ID**: http://arxiv.org/abs/1907.01661v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.01661v2)
- **Published**: 2019-07-02 21:43:43+00:00
- **Updated**: 2019-07-12 02:40:08+00:00
- **Authors**: Xiaoxiao Li, Nicha C. Dvornek, Yuan Zhou, Juntang Zhuang, Pamela Ventola, James S. Duncan
- **Comment**: None
- **Journal**: Medical Image Computing and Computer-Assisted Intervention 2019
- **Summary**: Finding the biomarkers associated with ASD is helpful for understanding the underlying roots of the disorder and can lead to earlier diagnosis and more targeted treatment. A promising approach to identify biomarkers is using Graph Neural Networks (GNNs), which can be used to analyze graph structured data, i.e. brain networks constructed by fMRI. One way to interpret important features is through looking at how the classification probability changes if the features are occluded or replaced. The major limitation of this approach is that replacing values may change the distribution of the data and lead to serious errors. Therefore, we develop a 2-stage pipeline to eliminate the need to replace features for reliable biomarker interpretation. Specifically, we propose an inductive GNN to embed the graphs containing different properties of task-fMRI for identifying ASD and then discover the brain regions/sub-graphs used as evidence for the GNN classifier. We first show GNN can achieve high accuracy in identifying ASD. Next, we calculate the feature importance scores using GNN and compare the interpretation ability with Random Forest. Finally, we run with different atlases and parameters, proving the robustness of the proposed method. The detected biomarkers reveal their association with social behaviors. We also show the potential of discovering new informative biomarkers. Our pipeline can be generalized to other graph feature importance interpretation problems.



### Neural Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1907.02065v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.02065v1)
- **Published**: 2019-07-02 22:49:25+00:00
- **Updated**: 2019-07-02 22:49:25+00:00
- **Authors**: Elaina Tan, Lakshay Sharma
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, the biggest advances in major Computer Vision tasks, such as object recognition, handwritten-digit identification, facial recognition, and many others., have all come through the use of Convolutional Neural Networks (CNNs). Similarly, in the domain of Natural Language Processing, Recurrent Neural Networks (RNNs), and Long Short Term Memory networks (LSTMs) in particular, have been crucial to some of the biggest breakthroughs in performance for tasks such as machine translation, part-of-speech tagging, sentiment analysis, and many others. These individual advances have greatly benefited tasks even at the intersection of NLP and Computer Vision, and inspired by this success, we studied some existing neural image captioning models that have proven to work well. In this work, we study some existing captioning models that provide near state-of-the-art performances, and try to enhance one such model. We also present a simple image captioning model that makes use of a CNN, an LSTM, and the beam search1 algorithm, and study its performance based on various qualitative and quantitative metrics.



### Structure fusion based on graph convolutional networks for semi-supervised classification
- **Arxiv ID**: http://arxiv.org/abs/1907.02586v1
- **DOI**: 10.3390/electronics9030432
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.02586v1)
- **Published**: 2019-07-02 23:43:05+00:00
- **Updated**: 2019-07-02 23:43:05+00:00
- **Authors**: Guangfeng Lin, Jing Wang, Kaiyang Liao, Fan Zhao, Wanjun Chen
- **Comment**: None
- **Journal**: Electronics,2020
- **Summary**: Suffering from the multi-view data diversity and complexity for semi-supervised classification, most of existing graph convolutional networks focus on the networks architecture construction or the salient graph structure preservation, and ignore the the complete graph structure for semi-supervised classification contribution. To mine the more complete distribution structure from multi-view data with the consideration of the specificity and the commonality, we propose structure fusion based on graph convolutional networks (SF-GCN) for improving the performance of semi-supervised classification. SF-GCN can not only retain the special characteristic of each view data by spectral embedding, but also capture the common style of multi-view data by distance metric between multi-graph structures. Suppose the linear relationship between multi-graph structures, we can construct the optimization function of structure fusion model by balancing the specificity loss and the commonality loss. By solving this function, we can simultaneously obtain the fusion spectral embedding from the multi-view data and the fusion structure as adjacent matrix to input graph convolutional networks for semi-supervised classification. Experiments demonstrate that the performance of SF-GCN outperforms that of the state of the arts on three challenging datasets, which are Cora,Citeseer and Pubmed in citation networks.



### SkeletonNet: Shape Pixel to Skeleton Pixel
- **Arxiv ID**: http://arxiv.org/abs/1907.01683v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.01683v1)
- **Published**: 2019-07-02 23:44:05+00:00
- **Updated**: 2019-07-02 23:44:05+00:00
- **Authors**: Sabari Nathan, Priya Kansal
- **Comment**: Published in CVPRw 2019
- **Journal**: IEEE/CVPRw 2019
- **Summary**: Deep Learning for Geometric Shape Understating has organized a challenge for extracting different kinds of skeletons from the images of different objects. This competition is organized in association with CVPR 2019. There are three different tracks of this competition. The present manuscript describes the method used to train the model for the dataset provided in the first track. The first track aims to extract skeleton pixels from the shape pixels of 89 different objects. For the purpose of extracting the skeleton, a U-net model which is comprised of an encoder-decoder structure has been used. In our proposed architecture, unlike the plain decoder in the traditional Unet, we have designed the decoder in the format of HED architecture, wherein we have introduced 4 side layers and fused them to one dilation convolutional layer to connect the broken links of the skeleton. Our proposed architecture achieved the F1 score of 0.77 on test data.



