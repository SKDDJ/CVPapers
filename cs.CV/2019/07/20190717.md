# Arxiv Papers in cs.CV on 2019-07-17
### A General Framework of Learning Multi-Vehicle Interaction Patterns from Videos
- **Arxiv ID**: http://arxiv.org/abs/1907.07315v1
- **DOI**: 10.1109/ITSC.2019.8917212
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.07315v1)
- **Published**: 2019-07-17 03:41:51+00:00
- **Updated**: 2019-07-17 03:41:51+00:00
- **Authors**: Chengyuan Zhang, Jiacheng Zhu, Wenshuo Wang, Ding Zhao
- **Comment**: 2019 IEEE Intelligent Transportation Systems Conference (ITSC)
- **Journal**: None
- **Summary**: Semantic learning and understanding of multi-vehicle interaction patterns in a cluttered driving environment are essential but challenging for autonomous vehicles to make proper decisions. This paper presents a general framework to gain insights into intricate multi-vehicle interaction patterns from bird's-eye view traffic videos. We adopt a Gaussian velocity field to describe the time-varying multi-vehicle interaction behaviors and then use deep autoencoders to learn associated latent representations for each temporal frame. Then, we utilize a hidden semi-Markov model with a hierarchical Dirichlet process as a prior to segment these sequential representations into granular components, also called traffic primitives, corresponding to interaction patterns. Experimental results demonstrate that our proposed framework can extract traffic primitives from videos, thus providing a semantic way to analyze multi-vehicle interaction patterns, even for cluttered driving scenarios that are far messier than human beings can cope with.



### Half a Percent of Labels is Enough: Efficient Animal Detection in UAV Imagery using Deep CNNs and Active Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.07319v1
- **DOI**: 10.1109/TGRS.2019.2927393
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.07319v1)
- **Published**: 2019-07-17 04:06:17+00:00
- **Updated**: 2019-07-17 04:06:17+00:00
- **Authors**: Benjamin Kellenberger, Diego Marcos, Sylvain Lobry, Devis Tuia
- **Comment**: In press at IEEE Transactions on Geoscience and Remote Sensing (TGRS)
- **Journal**: None
- **Summary**: We present an Active Learning (AL) strategy for re-using a deep Convolutional Neural Network (CNN)-based object detector on a new dataset. This is of particular interest for wildlife conservation: given a set of images acquired with an Unmanned Aerial Vehicle (UAV) and manually labeled gound truth, our goal is to train an animal detector that can be re-used for repeated acquisitions, e.g. in follow-up years. Domain shifts between datasets typically prevent such a direct model application. We thus propose to bridge this gap using AL and introduce a new criterion called Transfer Sampling (TS). TS uses Optimal Transport to find corresponding regions between the source and the target datasets in the space of CNN activations. The CNN scores in the source dataset are used to rank the samples according to their likelihood of being animals, and this ranking is transferred to the target dataset. Unlike conventional AL criteria that exploit model uncertainty, TS focuses on very confident samples, thus allowing a quick retrieval of true positives in the target dataset, where positives are typically extremely rare and difficult to find by visual inspection. We extend TS with a new window cropping strategy that further accelerates sample retrieval. Our experiments show that with both strategies combined, less than half a percent of oracle-provided labels are enough to find almost 80% of the animals in challenging sets of UAV images, beating all baselines by a margin.



### Towards Data-Driven Automatic Video Editing
- **Arxiv ID**: http://arxiv.org/abs/1907.07345v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.07345v1)
- **Published**: 2019-07-17 05:54:37+00:00
- **Updated**: 2019-07-17 05:54:37+00:00
- **Authors**: Sergey Podlesnyy
- **Comment**: 2019 15th International Conference on Natural Computation, Fuzzy
  Systems and Knowledge Discovery, Kunming, China
- **Journal**: None
- **Summary**: Automatic video editing involving at least the steps of selecting the most valuable footage from points of view of visual quality and the importance of action filmed; and cutting the footage into a brief and coherent visual story that would be interesting to watch is implemented in a purely data-driven manner. Visual semantic and aesthetic features are extracted by the ImageNet-trained convolutional neural network, and the editing controller is trained by an imitation learning algorithm. As a result, at test time the controller shows the signs of observing basic cinematography editing rules learned from the corpus of motion pictures masterpieces.



### Diving Deeper into Underwater Image Enhancement: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1907.07863v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.07863v1)
- **Published**: 2019-07-17 06:45:25+00:00
- **Updated**: 2019-07-17 06:45:25+00:00
- **Authors**: Saeed Anwar, Chongyi Li
- **Comment**: None
- **Journal**: None
- **Summary**: The powerful representation capacity of deep learning has made it inevitable for the underwater image enhancement community to employ its potential. The exploration of deep underwater image enhancement networks is increasing over time, and hence; a comprehensive survey is the need of the hour. In this paper, our main aim is two-fold, 1): to provide a comprehensive and in-depth survey of the deep learning-based underwater image enhancement, which covers various perspectives ranging from algorithms to open issues, and 2): to conduct a qualitative and quantitative comparison of the deep algorithms on diverse datasets to serve as a benchmark, which has been barely explored before. To be specific, we first introduce the underwater image formation models, which are the base of training data synthesis and design of deep networks, and also helpful for understanding the process of underwater image degradation. Then, we review deep underwater image enhancement algorithms, and a glimpse of some of the aspects of the current networks is presented including network architecture, network parameters, training data, loss function, and training configurations. We also summarize the evaluation metrics and underwater image datasets. Following that, a systematically experimental comparison is carried out to analyze the robustness and effectiveness of deep algorithms. Meanwhile, we point out the shortcomings of current benchmark datasets and evaluation metrics. Finally, we discuss several unsolved open issues and suggest possible research directions. We hope that all efforts done in this paper might serve as a comprehensive reference for future research and call for the development of deep learning-based underwater image enhancement.



### Towards Markerless Grasp Capture
- **Arxiv ID**: http://arxiv.org/abs/1907.07388v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.07388v1)
- **Published**: 2019-07-17 08:41:21+00:00
- **Updated**: 2019-07-17 08:41:21+00:00
- **Authors**: Samarth Brahmbhatt, Charles C. Kemp, James Hays
- **Comment**: Third Workshop on Computer Vision for AR/VR, CVPR 2019
- **Journal**: None
- **Summary**: Humans excel at grasping objects and manipulating them. Capturing human grasps is important for understanding grasping behavior and reconstructing it realistically in Virtual Reality (VR). However, grasp capture - capturing the pose of a hand grasping an object, and orienting it w.r.t. the object - is difficult because of the complexity and diversity of the human hand, and occlusion. Reflective markers and magnetic trackers traditionally used to mitigate this difficulty introduce undesirable artifacts in images and can interfere with natural grasping behavior. We present preliminary work on a completely marker-less algorithm for grasp capture from a video depicting a grasp. We show how recent advances in 2D hand pose estimation can be used with well-established optimization techniques. Uniquely, our algorithm can also capture hand-object contact in detail and integrate it in the grasp capture process. This is work in progress, find more details at https://contactdb. cc.gatech.edu/grasp_capture.html.



### Underexposed Image Correction via Hybrid Priors Navigated Deep Propagation
- **Arxiv ID**: http://arxiv.org/abs/1907.07408v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.07408v1)
- **Published**: 2019-07-17 09:27:05+00:00
- **Updated**: 2019-07-17 09:27:05+00:00
- **Authors**: Risheng Liu, Long Ma, Yuxi Zhang, Xin Fan, Zhongxuan Luo
- **Comment**: Submitted to IEEE Transactions on Neural Networks and Learning
  Systems (TNNLS). Project page: http://dutmedia.org/HPNDP/
- **Journal**: None
- **Summary**: Enhancing visual qualities for underexposed images is an extensively concerned task that plays important roles in various areas of multimedia and computer vision. Most existing methods often fail to generate high-quality results with appropriate luminance and abundant details. To address these issues, we in this work develop a novel framework, integrating both knowledge from physical principles and implicit distributions from data to solve the underexposed image correction task. More concretely, we propose a new perspective to formulate this task as an energy-inspired model with advanced hybrid priors. A propagation procedure navigated by the hybrid priors is well designed for simultaneously propagating the reflectance and illumination toward desired results. We conduct extensive experiments to verify the necessity of integrating both underlying principles (i.e., with knowledge) and distributions (i.e., from data) as navigated deep propagation. Plenty of experimental results of underexposed image correction demonstrate that our proposed method performs favorably against the state-of-the-art methods on both subjective and objective assessments. Additionally, we execute the task of face detection to further verify the naturalness and practical value of underexposed image correction. What's more, we employ our method to single image haze removal whose experimental results further demonstrate its superiorities.



### Lung Nodules Detection and Segmentation Using 3D Mask-RCNN
- **Arxiv ID**: http://arxiv.org/abs/1907.07676v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.07676v1)
- **Published**: 2019-07-17 09:51:11+00:00
- **Updated**: 2019-07-17 09:51:11+00:00
- **Authors**: Evi Kopelowitz, Guy Engelhard
- **Comment**: MIDL 2019 [arXiv:1907.08612]
- **Journal**: None
- **Summary**: Accurate assessment of Lung nodules is a time consuming and error prone ingredient of the radiologist interpretation work. Automating 3D volume detection and segmentation can improve workflow as well as patient care. Previous works have focused either on detecting lung nodules from a full CT scan or on segmenting them from a small ROI. We adapt the state of the art architecture for 2D object detection and segmentation, MaskRCNN, to handle 3D images and employ it to detect and segment lung nodules from CT scans. We report on competitive results for the lung nodule detection on LUNA16 data set. The added value of our method is that in addition to lung nodule detection, our framework produces 3D segmentations of the detected nodules.



### CU-Net: Cascaded U-Net with Loss Weighted Sampling for Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.07677v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.07677v1)
- **Published**: 2019-07-17 10:16:04+00:00
- **Updated**: 2019-07-17 10:16:04+00:00
- **Authors**: Hongying Liu, Xiongjie Shen, Fanhua Shang, Fei Wang
- **Comment**: 9 pages, 4 figures
- **Journal**: None
- **Summary**: This paper proposes a novel cascaded U-Net for brain tumor segmentation. Inspired by the distinct hierarchical structure of brain tumor, we design a cascaded deep network framework, in which the whole tumor is segmented firstly and then the tumor internal substructures are further segmented. Considering that the increase of the network depth brought by cascade structures leads to a loss of accurate localization information in deeper layers, we construct many skip connections to link features at the same resolution and transmit detailed information from shallow layers to the deeper layers. Then we present a loss weighted sampling (LWS) scheme to eliminate the issue of imbalanced data during training the network. Experimental results on BraTS 2017 data show that our architecture framework outperforms the state-of-the-art segmentation algorithms, especially in terms of segmentation sensitivity.



### OGNet: Salient Object Detection with Output-guided Attention Module
- **Arxiv ID**: http://arxiv.org/abs/1907.07449v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.07449v1)
- **Published**: 2019-07-17 11:36:37+00:00
- **Updated**: 2019-07-17 11:36:37+00:00
- **Authors**: Shiping Zhu, Lanyun Zhu
- **Comment**: submitted to IEEE Transactions on Circuits and Systems for Video
  Technology
- **Journal**: None
- **Summary**: Attention mechanisms are widely used in salient object detection models based on deep learning, which can effectively promote the extraction and utilization of useful information by neural networks. However, most of the existing attention modules used in salient object detection are input with the processed feature map itself, which easily leads to the problem of `blind overconfidence'. In this paper, instead of applying the widely used self-attention module, we present an output-guided attention module built with multi-scale outputs to overcome the problem of `blind overconfidence'. We also construct a new loss function, the intractable area F-measure loss function, which is based on the F-measure of the hard-to-handle area to improve the detection effect of the model in the edge areas and confusing areas of an image. Extensive experiments and abundant ablation studies are conducted to evaluate the effect of our methods and to explore the most suitable structure for the model. Tests on several data sets show that our model performs very well, even though it is very lightweight.



### AVDNet: A Small-Sized Vehicle Detection Network for Aerial Visual Data
- **Arxiv ID**: http://arxiv.org/abs/1907.07477v1
- **DOI**: 10.1109/LGRS.2019.2923564
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.07477v1)
- **Published**: 2019-07-17 12:42:55+00:00
- **Updated**: 2019-07-17 12:42:55+00:00
- **Authors**: Murari Mandal, Manal Shah, Prashant Meena, Sanhita Devi, Santosh Kumar Vipparthi
- **Comment**: IEEE Geoscience and Remote Sensing Letters, doi:
  10.1109/LGRS.2019.2923564
- **Journal**: IEEE Geoscience and Remote Sensing Letters, doi:
  10.1109/LGRS.2019.2923564
- **Summary**: Detection of small-sized targets in aerial views is a challenging task due to the smallness of vehicle size, complex background, and monotonic object appearances. In this letter, we propose a one-stage vehicle detection network (AVDNet) to robustly detect small-sized vehicles in aerial scenes. In AVDNet, we introduced ConvRes residual blocks at multiple scales to alleviate the problem of vanishing features for smaller objects caused because of the inclusion of deeper convolutional layers. These residual blocks, along with enlarged output feature map, ensure the robust representation of the salient features for small sized objects. Furthermore, we proposed a recurrent-feature aware visualization (RFAV) technique to analyze the network behavior. We also created a new airborne image data set (ABD) by annotating 1396 new objects in 79 aerial images for our experiments. The effectiveness of AVDNet is validated on VEDAI, DLR- 3K, DOTA, and the combined (VEDAI, DLR-3K, DOTA, and ABD) data set. Experimental results demonstrate the significant performance improvement of the proposed method over state-of-the-art detection techniques in terms of mAP, computation, and space complexity.



### Benchmarking Robustness in Object Detection: Autonomous Driving when Winter is Coming
- **Arxiv ID**: http://arxiv.org/abs/1907.07484v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.07484v2)
- **Published**: 2019-07-17 12:51:10+00:00
- **Updated**: 2020-03-31 08:42:46+00:00
- **Authors**: Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak, Oliver Bringmann, Alexander S. Ecker, Matthias Bethge, Wieland Brendel
- **Comment**: 21 pages, 10 figures, 1 dragon
- **Journal**: None
- **Summary**: The ability to detect objects regardless of image distortions or weather conditions is crucial for real-world applications of deep learning like autonomous driving. We here provide an easy-to-use benchmark to assess how object detection models perform when image quality degrades. The three resulting benchmark datasets, termed Pascal-C, Coco-C and Cityscapes-C, contain a large variety of image corruptions. We show that a range of standard object detection models suffer a severe performance loss on corrupted images (down to 30--60\% of the original performance). However, a simple data augmentation trick---stylizing the training images---leads to a substantial increase in robustness across corruption type, severity and dataset. We envision our comprehensive benchmark to track future progress towards building robust object detection models. Benchmark, code and data are publicly available.



### Multi-Adapter RGBT Tracking
- **Arxiv ID**: http://arxiv.org/abs/1907.07485v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.07485v1)
- **Published**: 2019-07-17 12:51:37+00:00
- **Updated**: 2019-07-17 12:51:37+00:00
- **Authors**: Chenglong Li, Andong Lu, Aihua Zheng, Zhengzheng Tu, Jin Tang
- **Comment**: Code site: https://github.com/Alexadlu/MANet
- **Journal**: None
- **Summary**: The task of RGBT tracking aims to take the complementary advantages from visible spectrum and thermal infrared data to achieve robust visual tracking, and receives more and more attention in recent years. Existing works focus on modality-specific information integration by introducing modality weights to achieve adaptive fusion or learning robust feature representations of different modalities. Although these methods could effectively deploy the modality-specific properties, they ignore the potential values of modality-shared cues as well as instance-aware information, which are crucial for effective fusion of different modalities in RGBT tracking. In this paper, we propose a novel Multi-Adapter convolutional Network (MANet) to jointly perform modality-shared, modality-specific and instance-aware feature learning in an end-to-end trained deep framework for RGBT tracking. We design three kinds of adapters within our network. In a specific, the generality adapter is to extract shared object representations, the modality adapter aims at encoding modality-specific information to deploy their complementary advantages, and the instance adapter is to model the appearance properties and temporal variations of a certain object. Moreover, to reduce computational complexity for real-time demand of visual tracking, we design a parallel structure of generic adapter and modality adapter. Extensive experiments on two RGBT tracking benchmark datasets demonstrate the outstanding performance of the proposed tracker against other state-of-the-art RGB and RGBT tracking algorithms.



### A Link Between the Multiplicative and Additive Functional Asplund's Metrics
- **Arxiv ID**: http://arxiv.org/abs/1907.07509v1
- **DOI**: 10.1007/978-3-030-20867-7_4
- **Categories**: **cs.CV**, math.FA
- **Links**: [PDF](http://arxiv.org/pdf/1907.07509v1)
- **Published**: 2019-07-17 13:32:58+00:00
- **Updated**: 2019-07-17 13:32:58+00:00
- **Authors**: Guillaume Noyel
- **Comment**: None
- **Journal**: 14th International Symposium on Mathematical Morphology, Saarland
  University, Jul 2019, Saarbr\"ucken, Germany. pp.41-53
- **Summary**: Functional Asplund's metrics were recently introduced to perform pattern matching robust to lighting changes thanks to double-sided probing in the Logarithmic Image Processing (LIP) framework. Two metrics were defined, namely the LIP-multiplicative Asplund's metric which is robust to variations of object thickness (or opacity) and the LIP-additive Asplund's metric which is robust to variations of camera exposure-time (or light intensity). Maps of distances-i.e. maps of these metric values-were also computed between a reference template and an image. Recently, it was proven that the map of LIP-multiplicative As-plund's distances corresponds to mathematical morphology operations. In this paper, the link between both metrics and between their corresponding distance maps will be demonstrated. It will be shown that the map of LIP-additive Asplund's distances of an image can be computed from the map of the LIP-multiplicative Asplund's distance of a transform of this image and vice-versa. Both maps will be related by the LIP isomorphism which will allow to pass from the image space of the LIP-additive distance map to the positive real function space of the LIP-multiplicative distance map. Experiments will illustrate this relation and the robustness of the LIP-additive Asplund's metric to lighting changes.



### Stereo Event Lifetime and Disparity Estimation for Dynamic Vision Sensors
- **Arxiv ID**: http://arxiv.org/abs/1907.07518v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.07518v1)
- **Published**: 2019-07-17 13:46:22+00:00
- **Updated**: 2019-07-17 13:46:22+00:00
- **Authors**: Antea Hadviger, Ivan Marković, Ivan Petrović
- **Comment**: Accepted to European Conference on Mobile Robots (ECMR) 2019
- **Journal**: None
- **Summary**: Event-based cameras are biologically inspired sensors that output asynchronous pixel-wise brightness changes in the scene called events. They have a high dynamic range and temporal resolution of a microsecond, opposed to standard cameras that output frames at fixed frame rates and suffer from motion blur. Forming stereo pairs of such cameras can open novel application possibilities, since for each event depth can be readily estimated; however, to fully exploit asynchronous nature of the sensor and avoid fixed time interval event accumulation, stereo event lifetime estimation should be employed. In this paper, we propose a novel method for event lifetime estimation of stereo event-cameras, allowing generation of sharp gradient images of events that serve as input to disparity estimation methods. Since a single brightness change triggers events in both event-camera sensors, we propose a method for single shot event lifetime and disparity estimation, with association via stereo matching. The proposed method is approximately twice as fast and more accurate than if lifetimes were estimated separately for each sensor and then stereo matched. Results are validated on real-world data through multiple stereo event-camera experiments.



### FOSNet: An End-to-End Trainable Deep Neural Network for Scene Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.07570v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.07570v2)
- **Published**: 2019-07-17 15:10:24+00:00
- **Updated**: 2019-07-18 10:15:06+00:00
- **Authors**: Hongje Seong, Junhyuk Hyun, Euntai Kim
- **Comment**: 2019 IEEE. Personal use of this material is permitted. Permission
  from IEEE must be obtained for all other uses, in any current or future
  media, including reprinting/republishing this material for advertising or
  promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: None
- **Summary**: Scene recognition is an image recognition problem aimed at predicting the category of the place at which the image is taken. In this paper, a new scene recognition method using the convolutional neural network (CNN) is proposed. The proposed method is based on the fusion of the object and the scene information in the given image and the CNN framework is named as FOS (fusion of object and scene) Net. In addition, a new loss named scene coherence loss (SCL) is developed to train the FOSNet and to improve the scene recognition performance. The proposed SCL is based on the unique traits of the scene that the 'sceneness' spreads and the scene class does not change all over the image. The proposed FOSNet was experimented with three most popular scene recognition datasets, and their state-of-the-art performance is obtained in two sets: 60.14% on Places 2 and 90.37% on MIT indoor 67. The second highest performance of 77.28% is obtained on SUN 397.



### AquaSight: Automatic Water Impurity Detection Utilizing Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.07573v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.07573v1)
- **Published**: 2019-07-17 15:17:21+00:00
- **Updated**: 2019-07-17 15:17:21+00:00
- **Authors**: Ankit Gupta, Elliott Ruebush
- **Comment**: None
- **Journal**: None
- **Summary**: According to the United Nations World Water Assessment Programme, every day, 2 million tons of sewage and industrial and agricultural waste are discharged into the worlds water. In order to address this pervasive issue of increasing water pollution, while ensuring that the global population has an efficient, accurate, and low cost method to assess whether the water they drink is contaminated, we propose AquaSight, a novel mobile application that utilizes deep learning methods, specifically Convolutional Neural Networks, for automated water impurity detection. After comprehensive training with a dataset of 105 images representing varying magnitudes of contamination, the deep learning algorithm achieved a 96 percent accuracy and loss of 0.108. Furthermore, the machine learning model uses efficient analysis of the turbidity and transparency levels of water to estimate a particular sample of waters level of contamination. When deployed, the AquaSight system will provide an efficient way for individuals to secure an estimation of water quality, alerting local and national government to take action and potentially saving millions of lives worldwide.



### News Cover Assessment via Multi-task Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.07581v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.07581v2)
- **Published**: 2019-07-17 15:24:01+00:00
- **Updated**: 2019-07-18 02:26:58+00:00
- **Authors**: Zixun Sun, Shuang Zhao, Chengwei Zhu, Xiao Chen
- **Comment**: 6 pages, 9 figures
- **Journal**: None
- **Summary**: Online personalized news product needs a suitable cover for the article. The news cover demands to be with high image quality, and draw readers' attention at same time, which is extraordinary challenging due to the subjectivity of the task. In this paper, we assess the news cover from image clarity and object salience perspective. We propose an end-to-end multi-task learning network for image clarity assessment and semantic segmentation simultaneously, the results of which can be guided for news cover assessment. The proposed network is based on a modified DeepLabv3+ model. The network backbone is used for multiple scale spatial features exaction, followed by two branches for image clarity assessment and semantic segmentation, respectively. The experiment results show that the proposed model is able to capture important content in images and performs better than single-task learning baselines on our proposed game content based CIA dataset.



### Deep Metric Learning with Alternating Projections onto Feasible Sets
- **Arxiv ID**: http://arxiv.org/abs/1907.07585v3
- **DOI**: 10.1109/ICIP42928.2021.9506317
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.07585v3)
- **Published**: 2019-07-17 15:29:19+00:00
- **Updated**: 2021-12-15 21:10:44+00:00
- **Authors**: Oğul Can, Yeti Ziya Gürbüz, A. Aydın Alatan
- **Comment**: 10 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: During the training of networks for distance metric learning, minimizers of the typical loss functions can be considered as "feasible points" satisfying a set of constraints imposed by the training data. To this end, we reformulate distance metric learning problem as finding a feasible point of a constraint set where the embedding vectors of the training data satisfy desired intra-class and inter-class proximity. The feasible set induced by the constraint set is expressed as the intersection of the relaxed feasible sets which enforce the proximity constraints only for particular samples (a sample from each class) of the training data. Then, the feasible point problem is to be approximately solved by performing alternating projections onto those feasible sets. Such an approach introduces a regularization term and results in minimizing a typical loss function with a systematic batch set construction where these batches are constrained to contain the same sample from each class for a certain number of iterations. Moreover, these particular samples can be considered as the class representatives, allowing efficient utilization of hard class mining during batch construction. The proposed technique is applied with the well-accepted losses and evaluated on Stanford Online Products, CAR196 and CUB200-2011 datasets for image retrieval and clustering. Outperforming state-of-the-art, the proposed approach consistently improves the performance of the integrated loss functions with no additional computational cost and boosts the performance further by hard negative class mining.



### Robustness properties of Facebook's ResNeXt WSL models
- **Arxiv ID**: http://arxiv.org/abs/1907.07640v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.07640v5)
- **Published**: 2019-07-17 17:03:52+00:00
- **Updated**: 2019-12-09 16:28:47+00:00
- **Authors**: A. Emin Orhan
- **Comment**: 10 pages, 4 figures, 4 tables; v5 corrects the ImageNet-A results and
  revises the discussion accordingly
- **Journal**: None
- **Summary**: We investigate the robustness properties of ResNeXt class image recognition models trained with billion scale weakly supervised data (ResNeXt WSL models). These models, recently made public by Facebook AI, were trained with ~1B images from Instagram and fine-tuned on ImageNet. We show that these models display an unprecedented degree of robustness against common image corruptions and perturbations, as measured by the ImageNet-C and ImageNet-P benchmarks. They also achieve substantially improved accuracies on the recently introduced "natural adversarial examples" benchmark (ImageNet-A). The largest of the released models, in particular, achieves state-of-the-art results on ImageNet-C, ImageNet-P, and ImageNet-A by a large margin. The gains on ImageNet-C, ImageNet-P, and ImageNet-A far outpace the gains on ImageNet validation accuracy, suggesting the former as more useful benchmarks to measure further progress in image recognition. Remarkably, the ResNeXt WSL models even achieve a limited degree of adversarial robustness against state-of-the-art white-box attacks (10-step PGD attacks). However, in contrast to adversarially trained models, the robustness of the ResNeXt WSL models rapidly declines with the number of PGD steps, suggesting that these models do not achieve genuine adversarial robustness. Visualization of the learned features also confirms this conclusion. Finally, we show that although the ResNeXt WSL models are more shape-biased than comparable ImageNet-trained models in a shape-texture cue conflict experiment, they still remain much more texture-biased than humans, suggesting that they share some of the underlying characteristics of ImageNet-trained models that make this benchmark challenging.



### Truck Traffic Monitoring with Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/1907.07660v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.07660v1)
- **Published**: 2019-07-17 17:45:40+00:00
- **Updated**: 2019-07-17 17:45:40+00:00
- **Authors**: Lynn H. Kaack, George H. Chen, M. Granger Morgan
- **Comment**: 31 pages, 15 figures, to be published in ACM SIGCAS Conference on
  Computing and Sustainable Societies (COMPASS) 2019
- **Journal**: None
- **Summary**: The road freight sector is responsible for a large and growing share of greenhouse gas emissions, but reliable data on the amount of freight that is moved on roads in many parts of the world are scarce. Many low- and middle-income countries have limited ground-based traffic monitoring and freight surveying activities. In this proof of concept, we show that we can use an object detection network to count trucks in satellite images and predict average annual daily truck traffic from those counts. We describe a complete model, test the uncertainty of the estimation, and discuss the transfer to developing countries.



### Deep Multi-View Learning via Task-Optimal CCA
- **Arxiv ID**: http://arxiv.org/abs/1907.07739v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.07739v1)
- **Published**: 2019-07-17 20:06:47+00:00
- **Updated**: 2019-07-17 20:06:47+00:00
- **Authors**: Heather D. Couture, Roland Kwitt, J. S. Marron, Melissa Troester, Charles M. Perou, Marc Niethammer
- **Comment**: None
- **Journal**: None
- **Summary**: Canonical Correlation Analysis (CCA) is widely used for multimodal data analysis and, more recently, for discriminative tasks such as multi-view learning; however, it makes no use of class labels. Recent CCA methods have started to address this weakness but are limited in that they do not simultaneously optimize the CCA projection for discrimination and the CCA projection itself, or they are linear only. We address these deficiencies by simultaneously optimizing a CCA-based and a task objective in an end-to-end manner. Together, these two objectives learn a non-linear CCA projection to a shared latent space that is highly correlated and discriminative. Our method shows a significant improvement over previous state-of-the-art (including deep supervised approaches) for cross-view classification, regularization with a second view, and semi-supervised learning on real data.



### Real-Time Highly Accurate Dense Depth on a Power Budget using an FPGA-CPU Hybrid SoC
- **Arxiv ID**: http://arxiv.org/abs/1907.07745v1
- **DOI**: 10.1109/TCSII.2019.2909169
- **Categories**: **cs.CV**, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1907.07745v1)
- **Published**: 2019-07-17 20:22:47+00:00
- **Updated**: 2019-07-17 20:22:47+00:00
- **Authors**: Oscar Rahnama, Tommaso Cavallari, Stuart Golodetz, Alessio Tonioni, Thomas Joy, Luigi Di Stefano, Simon Walker, Philip H. S. Torr
- **Comment**: 6 pages, 7 figures, 2 tables, journal
- **Journal**: IEEE Transactions on Circuits and Systems II: Express Briefs, vol.
  66, no. 5, pp. 773-777, May 2019
- **Summary**: Obtaining highly accurate depth from stereo images in real time has many applications across computer vision and robotics, but in some contexts, upper bounds on power consumption constrain the feasible hardware to embedded platforms such as FPGAs. Whilst various stereo algorithms have been deployed on these platforms, usually cut down to better match the embedded architecture, certain key parts of the more advanced algorithms, e.g. those that rely on unpredictable access to memory or are highly iterative in nature, are difficult to deploy efficiently on FPGAs, and thus the depth quality that can be achieved is limited. In this paper, we leverage a FPGA-CPU chip to propose a novel, sophisticated, stereo approach that combines the best features of SGM and ELAS-based methods to compute highly accurate dense depth in real time. Our approach achieves an 8.7% error rate on the challenging KITTI 2015 dataset at over 50 FPS, with a power consumption of only 5W.



### End-to-end sensor modeling for LiDAR Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/1907.07748v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.07748v1)
- **Published**: 2019-07-17 20:34:14+00:00
- **Updated**: 2019-07-17 20:34:14+00:00
- **Authors**: Khaled Elmadawi, Moemen Abdelrazek, Mohamed Elsobky, Hesham M. Eraqi, Mohamed Zahran
- **Comment**: Accepted in IEEE Intelligent Transportation Systems Conference - ITSC
  2019
- **Journal**: None
- **Summary**: Advanced sensors are a key to enable self-driving cars technology. Laser scanner sensors (LiDAR, Light Detection And Ranging) became a fundamental choice due to its long-range and robustness to low light driving conditions. The problem of designing a control software for self-driving cars is a complex task to explicitly formulate in rule-based systems, thus recent approaches rely on machine learning that can learn those rules from data. The major problem with such approaches is that the amount of training data required for generalizing a machine learning model is big, and on the other hand LiDAR data annotation is very costly compared to other car sensors. An accurate LiDAR sensor model can cope with such problem. Moreover, its value goes beyond this because existing LiDAR development, validation, and evaluation platforms and processes are very costly, and virtual testing and development environments are still immature in terms of physical properties representation. In this work we propose a novel Deep Learning-based LiDAR sensor model. This method models the sensor echos, using a Deep Neural Network to model echo pulse widths learned from real data using Polar Grid Maps (PGM). We benchmark our model performance against comprehensive real sensor data and very promising results are achieved that sets a baseline for future works.



### Patient-specific Conditional Joint Models of Shape, Image Features and Clinical Indicators
- **Arxiv ID**: http://arxiv.org/abs/1907.07783v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CG, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.07783v1)
- **Published**: 2019-07-17 21:49:29+00:00
- **Updated**: 2019-07-17 21:49:29+00:00
- **Authors**: Bernhard Egger, Markus D. Schirmer, Florian Dubost, Marco J. Nardin, Natalia S. Rost, Polina Golland
- **Comment**: Supplementary material: https://www.youtube.com/watch?v=gPoHP_iFQIA
- **Journal**: MICCAI 2019, the 22nd International Conference on Medical Image
  Computing and Computer Assisted Intervention, in Shenzhen, China
- **Summary**: We propose and demonstrate a joint model of anatomical shapes, image features and clinical indicators for statistical shape modeling and medical image analysis. The key idea is to employ a copula model to separate the joint dependency structure from the marginal distributions of variables of interest. This separation provides flexibility on the assumptions made during the modeling process. The proposed method can handle binary, discrete, ordinal and continuous variables. We demonstrate a simple and efficient way to include binary, discrete and ordinal variables into the modeling. We build Bayesian conditional models based on observed partial clinical indicators, features or shape based on Gaussian processes capturing the dependency structure. We apply the proposed method on a stroke dataset to jointly model the shape of the lateral ventricles, the spatial distribution of the white matter hyperintensity associated with periventricular white matter disease, and clinical indicators. The proposed method yields interpretable joint models for data exploration and patient-specific statistical shape models for medical image analysis.



### Data augmentation with Symbolic-to-Real Image Translation GANs for Traffic Sign Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.12902v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.12902v1)
- **Published**: 2019-07-17 21:52:01+00:00
- **Updated**: 2019-07-17 21:52:01+00:00
- **Authors**: Nour Soufi, Matias Valdenegro-Toro
- **Comment**: 6 pages, 10 figures
- **Journal**: None
- **Summary**: Traffic sign recognition is an important component of many advanced driving assistance systems, and it is required for full autonomous driving. Computational performance is usually the bottleneck in using large scale neural networks for this purpose. SqueezeNet is a good candidate for efficient image classification of traffic signs, but in our experiments it does not reach high accuracy, and we believe this is due to lack of data, requiring data augmentation. Generative adversarial networks can learn the high dimensional distribution of empirical data, allowing the generation of new data points. In this paper we apply pix2pix GANs architecture to generate new traffic sign images and evaluate the use of these images in data augmentation. We were motivated to use pix2pix to translate symbolic sign images to real ones due to the mode collapse in Conditional GANs. Through our experiments we found that data augmentation using GAN can increase classification accuracy for circular traffic signs from 92.1% to 94.0%, and for triangular traffic signs from 93.8% to 95.3%, producing an overall improvement of 2%. However some traditional augmentation techniques can outperform GAN data augmentation, for example contrast variation in circular traffic signs (95.5%) and displacement on triangular traffic signs (96.7 %). Our negative results shows that while GANs can be naively used for data augmentation, they are not always the best choice, depending on the problem and variability in the data.



### Product Aesthetic Design: A Machine Learning Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.07786v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, econ.EM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.07786v2)
- **Published**: 2019-07-17 21:56:55+00:00
- **Updated**: 2022-11-15 05:00:09+00:00
- **Authors**: Alex Burnap, John R. Hauser, Artem Timoshenko
- **Comment**: None
- **Journal**: None
- **Summary**: Aesthetics are critically important to market acceptance. In the automotive industry, an improved aesthetic design can boost sales by 30% or more. Firms invest heavily in designing and testing aesthetics. A single automotive "theme clinic" can cost over $100,000, and hundreds are conducted annually. We propose a model to augment the commonly-used aesthetic design process by predicting aesthetic scores and automatically generating innovative and appealing product designs. The model combines a probabilistic variational autoencoder (VAE) with adversarial components from generative adversarial networks (GAN) and a supervised learning component. We train and evaluate the model with data from an automotive partner-images of 203 SUVs evaluated by targeted consumers and 180,000 high-quality unrated images. Our model predicts well the appeal of new aesthetic designs-43.5% improvement relative to a uniform baseline and substantial improvement over conventional machine learning models and pretrained deep neural networks. New automotive designs are generated in a controllable manner for use by design teams. We empirically verify that automatically generated designs are (1) appealing to consumers and (2) resemble designs which were introduced to the market five years after our data were collected. We provide an additional proof-of-concept application using opensource images of dining room chairs.



### GRIP++: Enhanced Graph-based Interaction-aware Trajectory Prediction for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1907.07792v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1907.07792v2)
- **Published**: 2019-07-17 22:10:16+00:00
- **Updated**: 2020-05-19 23:27:53+00:00
- **Authors**: Xin Li, Xiaowen Ying, Mooi Choo Chuah
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the advancement in the technology of autonomous driving cars, the safety of a self-driving car is still a challenging problem that has not been well studied. Motion prediction is one of the core functions of an autonomous driving car. Previously, we propose a novel scheme called GRIP which is designed to predict trajectories for traffic agents around an autonomous car efficiently. GRIP uses a graph to represent the interactions of close objects, applies several graph convolutional blocks to extract features, and subsequently uses an encoder-decoder long short-term memory (LSTM) model to make predictions. Even though our experimental results show that GRIP improves the prediction accuracy of the state-of-the-art solution by 30%, GRIP still has some limitations. GRIP uses a fixed graph to describe the relationships between different traffic agents and hence may suffer some performance degradations when it is being used in urban traffic scenarios. Hence, in this paper, we describe an improved scheme called GRIP++ where we use both fixed and dynamic graphs for trajectory predictions of different types of traffic agents. Such an improvement can help autonomous driving cars avoid many traffic accidents. Our evaluations using a recently released urban traffic dataset, namely ApolloScape showed that GRIP++ achieves better prediction accuracy than state-of-the-art schemes. GRIP++ ranked #1 on the leaderboard of the ApolloScape trajectory competition in October 2019. In addition, GRIP++ runs 21.7 times faster than a state-of-the-art scheme, CS-LSTM.



### OmniNet: A unified architecture for multi-modal multi-task learning
- **Arxiv ID**: http://arxiv.org/abs/1907.07804v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.07804v2)
- **Published**: 2019-07-17 22:59:56+00:00
- **Updated**: 2020-07-03 09:59:06+00:00
- **Authors**: Subhojeet Pramanik, Priyanka Agrawal, Aman Hussain
- **Comment**: Source code available at: https://github.com/subho406/OmniNet
- **Journal**: None
- **Summary**: Transformer is a popularly used neural network architecture, especially for language understanding. We introduce an extended and unified architecture that can be used for tasks involving a variety of modalities like image, text, videos, etc. We propose a spatio-temporal cache mechanism that enables learning spatial dimension of the input in addition to the hidden states corresponding to the temporal input sequence. The proposed architecture further enables a single model to support tasks with multiple input modalities as well as asynchronous multi-task learning, thus we refer to it as OmniNet. For example, a single instance of OmniNet can concurrently learn to perform the tasks of part-of-speech tagging, image captioning, visual question answering and video activity recognition. We demonstrate that training these four tasks together results in about three times compressed model while retaining the performance in comparison to training them individually. We also show that using this neural network pre-trained on some modalities assists in learning unseen tasks such as video captioning and video question answering. This illustrates the generalization capacity of the self-attention mechanism on the spatio-temporal cache present in OmniNet.



### A fully 3D multi-path convolutional neural network with feature fusion and feature weighting for automatic lesion identification in brain MRI images
- **Arxiv ID**: http://arxiv.org/abs/1907.07807v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.07807v2)
- **Published**: 2019-07-17 23:21:42+00:00
- **Updated**: 2019-11-16 18:30:20+00:00
- **Authors**: Yunzhe Xue, Meiyan Xie, Fadi G. Farhat, Olga Boukrina, A. M. Barrett, Jeffrey R. Binder, Usman W. Roshan, William W. Graves
- **Comment**: Machine Learning for Health (ML4H) at NeurIPS 2019 - Extended
  Abstract
- **Journal**: None
- **Summary**: We propose a fully 3D multi-path convolutional network to predict stroke lesions from 3D brain MRI images. Our multi-path model has independent encoders for different modalities containing residual convolutional blocks, weighted multi-path feature fusion from different modalities, and weighted fusion modules to combine encoder and decoder features. Compared to existing 3D CNNs like DeepMedic, 3D U-Net, and AnatomyNet, our networks achieves the highest statistically significant cross-validation accuracy of 60.5% on the large ATLAS benchmark of 220 patients. We also test our model on multi-modal images from the Kessler Foundation and Medical College Wisconsin and achieve a statistically significant cross-validation accuracy of 65%, significantly outperforming the multi-modal 3D U-Net and DeepMedic. Overall our model offers a principled, extensible multi-path approach that outperforms multi-channel alternatives and achieves high Dice accuracies on existing benchmarks.



### Unsupervised Task Design to Meta-Train Medical Image Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1907.07816v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.07816v1)
- **Published**: 2019-07-17 23:51:24+00:00
- **Updated**: 2019-07-17 23:51:24+00:00
- **Authors**: Gabriel Maicas, Cuong Nguyen, Farbod Motlagh, Jacinto C. Nascimento, Gustavo Carneiro
- **Comment**: None
- **Journal**: None
- **Summary**: Meta-training has been empirically demonstrated to be the most effective pre-training method for few-shot learning of medical image classifiers (i.e., classifiers modeled with small training sets). However, the effectiveness of meta-training relies on the availability of a reasonable number of hand-designed classification tasks, which are costly to obtain, and consequently rarely available. In this paper, we propose a new method to unsupervisedly design a large number of classification tasks to meta-train medical image classifiers. We evaluate our method on a breast dynamically contrast enhanced magnetic resonance imaging (DCE-MRI) data set that has been used to benchmark few-shot training methods of medical image classifiers. Our results show that the proposed unsupervised task design to meta-train medical image classifiers builds a pre-trained model that, after fine-tuning, produces better classification results than other unsupervised and supervised pre-training methods, and competitive results with respect to meta-training that relies on hand-designed classification tasks.



