# Arxiv Papers in cs.CV on 2019-05-29
### Learning Robust Global Representations by Penalizing Local Predictive Power
- **Arxiv ID**: http://arxiv.org/abs/1905.13549v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.13549v2)
- **Published**: 2019-05-29 00:27:54+00:00
- **Updated**: 2019-11-05 03:56:06+00:00
- **Authors**: Haohan Wang, Songwei Ge, Eric P. Xing, Zachary C. Lipton
- **Comment**: None
- **Journal**: NeurIPS 2019
- **Summary**: Despite their renowned predictive power on i.i.d. data, convolutional neural networks are known to rely more on high-frequency patterns that humans deem superficial than on low-frequency patterns that agree better with intuitions about what constitutes category membership. This paper proposes a method for training robust convolutional networks by penalizing the predictive power of the local representations learned by earlier layers. Intuitively, our networks are forced to discard predictive signals such as color and texture that can be gleaned from local receptive fields and to rely instead on the global structures of the image. Across a battery of synthetic and benchmark domain adaptation tasks, our method confers improved generalization out of the domain. Also, to evaluate cross-domain transfer, we introduce ImageNet-Sketch, a new dataset consisting of sketch-like images, that matches the ImageNet classification validation set in categories and scale.



### Vehicle Detection in Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.13390v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.13390v1)
- **Published**: 2019-05-29 00:33:28+00:00
- **Updated**: 2019-05-29 00:33:28+00:00
- **Authors**: Yao Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision is developing rapidly with the support of deep learning techniques. This thesis proposes an advanced vehicle-detection model based on an improvement to classical convolutional neural networks. The advanced model was applied against a vehicle detection benchmark and was built to detect on-road objects. First, we propose a high-level architecture for our advanced model, which utilizes different state-of-the-art deep learning techniques. Then, we utilize the residual neural networks and region proposal network to achieve competitive performance according to the vehicle detection benchmark. Lastly, we describe the developing trend of vehicle detection techniques and the future direction of research.



### Towards Real Scene Super-Resolution with Raw Images
- **Arxiv ID**: http://arxiv.org/abs/1905.12156v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.12156v1)
- **Published**: 2019-05-29 01:18:44+00:00
- **Updated**: 2019-05-29 01:18:44+00:00
- **Authors**: Xiangyu Xu, Yongrui Ma, Wenxiu Sun
- **Comment**: Accepted in CVPR 2019, project page:
  https://sites.google.com/view/xiangyuxu/rawsr_cvpr19
- **Journal**: None
- **Summary**: Most existing super-resolution methods do not perform well in real scenarios due to lack of realistic training data and information loss of the model input. To solve the first problem, we propose a new pipeline to generate realistic training data by simulating the imaging process of digital cameras. And to remedy the information loss of the input, we develop a dual convolutional neural network to exploit the originally captured radiance information in raw images. In addition, we propose to learn a spatially-variant color transformation which helps more effective color corrections. Extensive experiments demonstrate that super-resolution with raw data helps recover fine details and clear structures, and more importantly, the proposed network and data generation pipeline achieve superior results for single image super-resolution in real scenarios.



### Volumetric Capture of Humans with a Single RGBD Camera via Semi-Parametric Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.12162v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.12162v1)
- **Published**: 2019-05-29 01:29:51+00:00
- **Updated**: 2019-05-29 01:29:51+00:00
- **Authors**: Rohit Pandey, Anastasia Tkach, Shuoran Yang, Pavel Pidlypenskyi, Jonathan Taylor, Ricardo Martin-Brualla, Andrea Tagliasacchi, George Papandreou, Philip Davidson, Cem Keskin, Shahram Izadi, Sean Fanello
- **Comment**: None
- **Journal**: None
- **Summary**: Volumetric (4D) performance capture is fundamental for AR/VR content generation. Whereas previous work in 4D performance capture has shown impressive results in studio settings, the technology is still far from being accessible to a typical consumer who, at best, might own a single RGBD sensor. Thus, in this work, we propose a method to synthesize free viewpoint renderings using a single RGBD camera. The key insight is to leverage previously seen "calibration" images of a given user to extrapolate what should be rendered in a novel viewpoint from the data available in the sensor. Given these past observations from multiple viewpoints, and the current RGBD image from a fixed view, we propose an end-to-end framework that fuses both these data sources to generate novel renderings of the performer. We demonstrate that the method can produce high fidelity images, and handle extreme changes in subject pose and camera viewpoints. We also show that the system generalizes to performers not seen in the training data. We run exhaustive experiments demonstrating the effectiveness of the proposed semi-parametric model (i.e. calibration images available to the neural network) compared to other state of the art machine learned solutions. Further, we compare the method with more traditional pipelines that employ multi-view capture. We show that our framework is able to achieve compelling results, with substantially less infrastructure than previously required.



### Closed-Loop Adaptation for Weakly-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.12190v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.12190v1)
- **Published**: 2019-05-29 02:59:02+00:00
- **Updated**: 2019-05-29 02:59:02+00:00
- **Authors**: Zhengqiang Zhang, Shujian Yu, Shi Yin, Qinmu Peng, Xinge You
- **Comment**: None
- **Journal**: None
- **Summary**: Weakly-supervised semantic segmentation aims to assign each pixel a semantic category under weak supervisions, such as image-level tags. Most of existing weakly-supervised semantic segmentation methods do not use any feedback from segmentation output and can be considered as open-loop systems. They are prone to accumulated errors because of the static seeds and the sensitive structure information. In this paper, we propose a generic self-adaptation mechanism for existing weakly-supervised semantic segmentation methods by introducing two feedback chains, thus constituting a closed-loop system. Specifically, the first chain iteratively produces dynamic seeds by incorporating cross-image structure information, whereas the second chain further expands seed regions by a customized random walk process to reconcile inner-image structure information characterized by superpixels. Experiments on PASCAL VOC 2012 suggest that our network outperforms state-of-the-art methods with significantly less computational and memory burden.



### NPTC-net: Narrow-Band Parallel Transport Convolutional Neural Network on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1905.12218v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.12218v3)
- **Published**: 2019-05-29 05:07:26+00:00
- **Updated**: 2021-02-21 13:18:10+00:00
- **Authors**: Pengfei Jin, Tianhao Lai, Rongjie Lai, Bin Dong
- **Comment**: 18 pages, 6 figures
- **Journal**: None
- **Summary**: Convolution plays a crucial role in various applications in signal and image processing, analysis, and recognition. It is also the main building block of convolution neural networks (CNNs). Designing appropriate convolution neural networks on manifold-structured point clouds can inherit and empower recent advances of CNNs to analyzing and processing point cloud data. However, one of the major challenges is to define a proper way to "sweep" filters through the point cloud as a natural generalization of the planar convolution and to reflect the point cloud's geometry at the same time. In this paper, we consider generalizing convolution by adapting parallel transport on the point cloud. Inspired by a triangulated surface-based method [Stefan C. Schonsheck, Bin Dong, and Rongjie Lai, arXiv:1805.07857.], we propose the Narrow-Band Parallel Transport Convolution (NPTC) using a specifically defined connection on a voxel-based narrow-band approximation of point cloud data. With that, we further propose a deep convolutional neural network based on NPTC (called NPTC-net) for point cloud classification and segmentation. Comprehensive experiments show that the proposed NPTC-net achieves similar or better results than current state-of-the-art methods on point cloud classification and segmentation.



### Kernel-Induced Label Propagation by Mapping for Semi-Supervised Classification
- **Arxiv ID**: http://arxiv.org/abs/1905.12236v2
- **DOI**: 10.1109/TBDATA.2018.2797977
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.12236v2)
- **Published**: 2019-05-29 06:23:49+00:00
- **Updated**: 2019-05-31 01:53:35+00:00
- **Authors**: Zhao Zhang, Lei Jia, Mingbo Zhao, Guangcan Liu, Meng Wang, Shuicheng Yan
- **Comment**: Accepted by IEEE TBD
- **Journal**: None
- **Summary**: Kernel methods have been successfully applied to the areas of pattern recognition and data mining. In this paper, we mainly discuss the issue of propagating labels in kernel space. A Kernel-Induced Label Propagation (Kernel-LP) framework by mapping is proposed for high-dimensional data classification using the most informative patterns of data in kernel space. The essence of Kernel-LP is to perform joint label propagation and adaptive weight learning in a transformed kernel space. That is, our Kernel-LP changes the task of label propagation from the commonly-used Euclidean space in most existing work to kernel space. The motivation of our Kernel-LP to propagate labels and learn the adaptive weights jointly by the assumption of an inner product space of inputs, i.e., the original linearly inseparable inputs may be mapped to be separable in kernel space. Kernel-LP is based on existing positive and negative LP model, i.e., the effects of negative label information are integrated to improve the label prediction power. Also, Kernel-LP performs adaptive weight construction over the same kernel space, so it can avoid the tricky process of choosing the optimal neighborhood size suffered in traditional criteria. Two novel and efficient out-of-sample approaches for our Kernel-LP to involve new test data are also presented, i.e., (1) direct kernel mapping and (2) kernel mapping-induced label reconstruction, both of which purely depend on the kernel matrix between training set and testing set. Owing to the kernel trick, our algorithms will be applicable to handle the high-dimensional real data. Extensive results on real datasets demonstrate the effectiveness of our approach.



### Vision-to-Language Tasks Based on Attributes and Attention Mechanism
- **Arxiv ID**: http://arxiv.org/abs/1905.12243v1
- **DOI**: 10.1109/TCYB.2019.2914351
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.12243v1)
- **Published**: 2019-05-29 06:55:23+00:00
- **Updated**: 2019-05-29 06:55:23+00:00
- **Authors**: Xuelong Li, Aihong Yuan, Xiaoqiang Lu
- **Comment**: 15 pages, 6 figures, 50 references
- **Journal**: None
- **Summary**: Vision-to-language tasks aim to integrate computer vision and natural language processing together, which has attracted the attention of many researchers. For typical approaches, they encode image into feature representations and decode it into natural language sentences. While they neglect high-level semantic concepts and subtle relationships between image regions and natural language elements. To make full use of these information, this paper attempt to exploit the text guided attention and semantic-guided attention (SA) to find the more correlated spatial information and reduce the semantic gap between vision and language. Our method includes two level attention networks. One is the text-guided attention network which is used to select the text-related regions. The other is SA network which is used to highlight the concept-related regions and the region-related concepts. At last, all these information are incorporated to generate captions or answers. Practically, image captioning and visual question answering experiments have been carried out, and the experimental results have shown the excellent performance of the proposed approach.



### Learning Multilingual Word Embeddings Using Image-Text Data
- **Arxiv ID**: http://arxiv.org/abs/1905.12260v1
- **DOI**: 10.18653/v1/W19-1807
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.12260v1)
- **Published**: 2019-05-29 07:55:17+00:00
- **Updated**: 2019-05-29 07:55:17+00:00
- **Authors**: Karan Singhal, Karthik Raman, Balder ten Cate
- **Comment**: None
- **Journal**: None
- **Summary**: There has been significant interest recently in learning multilingual word embeddings -- in which semantically similar words across languages have similar embeddings. State-of-the-art approaches have relied on expensive labeled data, which is unavailable for low-resource languages, or have involved post-hoc unification of monolingual embeddings. In the present paper, we investigate the efficacy of multilingual embeddings learned from weakly-supervised image-text data. In particular, we propose methods for learning multilingual embeddings using image-text data, by enforcing similarity between the representations of the image and that of the text. Our experiments reveal that even without using any expensive labeled data, a bag-of-words-based embedding model trained on image-text data achieves performance comparable to the state-of-the-art on crosslingual semantic similarity tasks.



### KG-GAN: Knowledge-Guided Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.12261v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.12261v2)
- **Published**: 2019-05-29 07:55:46+00:00
- **Updated**: 2019-09-23 09:48:33+00:00
- **Authors**: Che-Han Chang, Chun-Hsien Yu, Szu-Ying Chen, Edward Y. Chang
- **Comment**: None
- **Journal**: None
- **Summary**: Can generative adversarial networks (GANs) generate roses of various colors given only roses of red petals as input? The answer is negative, since GANs' discriminator would reject all roses of unseen petal colors. In this study, we propose knowledge-guided GAN (KG-GAN) to fuse domain knowledge with the GAN framework. KG-GAN trains two generators; one learns from data whereas the other learns from knowledge with a constraint function. Experimental results demonstrate the effectiveness of KG-GAN in generating unseen flower categories from seen categories given textual descriptions of the unseen ones.



### Exploiting Epistemic Uncertainty of Anatomy Segmentation for Anomaly Detection in Retinal OCT
- **Arxiv ID**: http://arxiv.org/abs/1905.12806v1
- **DOI**: 10.1109/TMI.2019.2919951
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.12806v1)
- **Published**: 2019-05-29 08:39:42+00:00
- **Updated**: 2019-05-29 08:39:42+00:00
- **Authors**: Philipp Seeböck, José Ignacio Orlando, Thomas Schlegl, Sebastian M. Waldstein, Hrvoje Bogunović, Sophie Klimscha, Georg Langs, Ursula Schmidt-Erfurth
- **Comment**: Accepted for publication in IEEE Transactions on Medical Imaging,
  2019
- **Journal**: None
- **Summary**: Diagnosis and treatment guidance are aided by detecting relevant biomarkers in medical images. Although supervised deep learning can perform accurate segmentation of pathological areas, it is limited by requiring a-priori definitions of these regions, large-scale annotations, and a representative patient cohort in the training set. In contrast, anomaly detection is not limited to specific definitions of pathologies and allows for training on healthy samples without annotation. Anomalous regions can then serve as candidates for biomarker discovery. Knowledge about normal anatomical structure brings implicit information for detecting anomalies. We propose to take advantage of this property using bayesian deep learning, based on the assumption that epistemic uncertainties will correlate with anatomical deviations from a normal training set. A Bayesian U-Net is trained on a well-defined healthy environment using weak labels of healthy anatomy produced by existing methods. At test time, we capture epistemic uncertainty estimates of our model using Monte Carlo dropout. A novel post-processing technique is then applied to exploit these estimates and transfer their layered appearance to smooth blob-shaped segmentations of the anomalies. We experimentally validated this approach in retinal optical coherence tomography (OCT) images, using weak labels of retinal layers. Our method achieved a Dice index of 0.789 in an independent anomaly test set of age-related macular degeneration (AMD) cases. The resulting segmentations allowed very high accuracy for separating healthy and diseased cases with late wet AMD, dry geographic atrophy (GA), diabetic macular edema (DME) and retinal vein occlusion (RVO). Finally, we qualitatively observed that our approach can also detect other deviations in normal scans such as cut edge artifacts.



### Image Denoising with Graph-Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.12281v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.12281v1)
- **Published**: 2019-05-29 09:17:21+00:00
- **Updated**: 2019-05-29 09:17:21+00:00
- **Authors**: Diego Valsesia, Giulia Fracastoro, Enrico Magli
- **Comment**: IEEE International Conference on Image Processing (ICIP) 2019
- **Journal**: None
- **Summary**: Recovering an image from a noisy observation is a key problem in signal processing. Recently, it has been shown that data-driven approaches employing convolutional neural networks can outperform classical model-based techniques, because they can capture more powerful and discriminative features. However, since these methods are based on convolutional operations, they are only capable of exploiting local similarities without taking into account non-local self-similarities. In this paper we propose a convolutional neural network that employs graph-convolutional layers in order to exploit both local and non-local similarities. The graph-convolutional layers dynamically construct neighborhoods in the feature space to detect latent correlations in the feature maps produced by the hidden layers. The experimental results show that the proposed architecture outperforms classical convolutional neural networks for the denoising task.



### Complex-valued neural networks for machine learning on non-stationary physical data
- **Arxiv ID**: http://arxiv.org/abs/1905.12321v2
- **DOI**: 10.1016/j.cageo.2020.104643
- **Categories**: **cs.LG**, cs.CV, physics.comp-ph, physics.geo-ph, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.12321v2)
- **Published**: 2019-05-29 10:47:42+00:00
- **Updated**: 2019-11-26 09:11:41+00:00
- **Authors**: Jesper Sören Dramsch, Mikael Lüthje, Anders Nymark Christensen
- **Comment**: 17 pages total, 15 pages, 2 pages references, paper, 11 figures, 28
  networks
- **Journal**: None
- **Summary**: Deep learning has become an area of interest in most scientific areas, including physical sciences. Modern networks apply real-valued transformations on the data. Particularly, convolutions in convolutional neural networks discard phase information entirely. Many deterministic signals, such as seismic data or electrical signals, contain significant information in the phase of the signal. We explore complex-valued deep convolutional networks to leverage non-linear feature maps. Seismic data commonly has a lowcut filter applied, to attenuate noise from ocean waves and similar long wavelength contributions. Discarding the phase information leads to low-frequency aliasing analogous to the Nyquist-Shannon theorem for high frequencies. In non-stationary data, the phase content can stabilize training and improve the generalizability of neural networks. While it has been shown that phase content can be restored in deep neural networks, we show how including phase information in feature maps improves both training and inference from deterministic physical data. Furthermore, we show that the reduction of parameters in a complex network outperforms larger real-valued networks.



### Learning the Non-linearity in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.12337v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.12337v1)
- **Published**: 2019-05-29 11:32:06+00:00
- **Updated**: 2019-05-29 11:32:06+00:00
- **Authors**: Gavneet Singh Chadha, Andreas Schwung
- **Comment**: None
- **Journal**: None
- **Summary**: We propose the introduction of nonlinear operation into the feature generation process in convolutional neural networks. This nonlinearity can be implemented in various ways. First we discuss the use of nonlinearities in the process of data augmentation to increase the robustness of the neural networks recognition capacity. To this end, we randomly disturb the input data set by applying exponents within a certain numerical range to individual data points of the input space. Second we propose nonlinear convolutional neural networks where we apply the exponential operation to each element of the receptive field. To this end, we define an additional weight matrix of the same dimension as the standard kernel weight matrix. The weights of this matrix then constitute the exponents of the corresponding components of the receptive field. In the basic setting, we keep the weight parameters fixed during training by defining suitable parameters. Alternatively, we make the exponential weight parameters end-to-end trainable using a suitable parameterization. The network architecture is applied to time series analysis data set showing a considerable increase in the classification performance compared to baseline networks.



### Super Interaction Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1905.12349v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.12349v1)
- **Published**: 2019-05-29 11:54:07+00:00
- **Updated**: 2019-05-29 11:54:07+00:00
- **Authors**: Yang Yao, Xu Zhang, Baile Xu, Furao Shen, Jian Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies have demonstrated that the convolutional networks heavily rely on the quality and quantity of generated features. However, in lightweight networks, there are limited available feature information because these networks tend to be shallower and thinner due to the efficiency consideration. For farther improving the performance and accuracy of lightweight networks, we develop Super Interaction Neural Networks (SINet) model from a novel point of view: enhancing the information interaction in neural networks. In order to achieve information interaction along the width of the deep network, we propose Exchange Shortcut Connection, which can integrate the information from different convolution groups without any extra computation cost. And then, in order to achieve information interaction along the depth of the network, we proposed Dense Funnel Layer and Attention based Hierarchical Joint Decision, which are able to make full use of middle layer features. Our experiments show that the superior performance of SINet over other state-of-the-art lightweight models in ImageNet dataset. Furthermore, we also exhibit the effectiveness and universality of our proposed components by ablation studies.



### Disentangling Monocular 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1905.12365v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.12365v1)
- **Published**: 2019-05-29 12:13:53+00:00
- **Updated**: 2019-05-29 12:13:53+00:00
- **Authors**: Andrea Simonelli, Samuel Rota Rota Bulò, Lorenzo Porzi, Manuel López-Antequera, Peter Kontschieder
- **Comment**: Project website at
  https://research.mapillary.com/publication/MonoDIS/
- **Journal**: None
- **Summary**: In this paper we propose an approach for monocular 3D object detection from a single RGB image, which leverages a novel disentangling transformation for 2D and 3D detection losses and a novel, self-supervised confidence score for 3D bounding boxes. Our proposed loss disentanglement has the twofold advantage of simplifying the training dynamics in the presence of losses with complex interactions of parameters, and sidestepping the issue of balancing independent regression terms. Our solution overcomes these issues by isolating the contribution made by groups of parameters to a given loss, without changing its nature. We further apply loss disentanglement to another novel, signed Intersection-over-Union criterion-driven loss for improving 2D detection results. Besides our methodological innovations, we critically review the AP metric used in KITTI3D, which emerged as the most important dataset for comparing 3D detection results. We identify and resolve a flaw in the 11-point interpolated AP metric, affecting all previously published detection results and particularly biases the results of monocular 3D detection. We provide extensive experimental evaluations and ablation studies on the KITTI3D and nuScenes datasets, setting new state-of-the-art results on object category car by large margins.



### Functional Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1906.00001v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.00001v2)
- **Published**: 2019-05-29 12:26:12+00:00
- **Updated**: 2019-10-29 14:52:57+00:00
- **Authors**: Cassidy Laidlaw, Soheil Feizi
- **Comment**: Accepted to NeurIPS 2019
- **Journal**: None
- **Summary**: We propose functional adversarial attacks, a novel class of threat models for crafting adversarial examples to fool machine learning models. Unlike a standard $\ell_p$-ball threat model, a functional adversarial threat model allows only a single function to be used to perturb input features to produce an adversarial example. For example, a functional adversarial attack applied on colors of an image can change all red pixels simultaneously to light red. Such global uniform changes in images can be less perceptible than perturbing pixels of the image individually. For simplicity, we refer to functional adversarial attacks on image colors as ReColorAdv, which is the main focus of our experiments. We show that functional threat models can be combined with existing additive ($\ell_p$) threat models to generate stronger threat models that allow both small, individual perturbations and large, uniform changes to an input. Moreover, we prove that such combinations encompass perturbations that would not be allowed in either constituent threat model. In practice, ReColorAdv can significantly reduce the accuracy of a ResNet-32 trained on CIFAR-10. Furthermore, to the best of our knowledge, combining ReColorAdv with other attacks leads to the strongest existing attack even after adversarial training. An implementation of ReColorAdv is available at https://github.com/cassidylaidlaw/ReColorAdv .



### Coherent Semantic Attention for Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1905.12384v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.12384v3)
- **Published**: 2019-05-29 12:46:53+00:00
- **Updated**: 2019-07-04 04:57:29+00:00
- **Authors**: Hongyu Liu, Bin Jiang, Yi Xiao, Chao Yang
- **Comment**: None
- **Journal**: None
- **Summary**: The latest deep learning-based approaches have shown promising results for the challenging task of inpainting missing regions of an image. However, the existing methods often generate contents with blurry textures and distorted structures due to the discontinuity of the local pixels. From a semantic-level perspective, the local pixel discontinuity is mainly because these methods ignore the semantic relevance and feature continuity of hole regions. To handle this problem, we investigate the human behavior in repairing pictures and propose a fined deep generative model-based approach with a novel coherent semantic attention (CSA) layer, which can not only preserve contextual structure but also make more effective predictions of missing parts by modeling the semantic relevance between the holes features. The task is divided into rough, refinement as two steps and model each step with a neural network under the U-Net architecture, where the CSA layer is embedded into the encoder of refinement step. To stabilize the network training process and promote the CSA layer to learn more effective parameters, we propose a consistency loss to enforce the both the CSA layer and the corresponding layer of the CSA in decoder to be close to the VGG feature layer of a ground truth image simultaneously. The experiments on CelebA, Places2, and Paris StreetView datasets have validated the effectiveness of our proposed methods in image inpainting tasks and can obtain images with a higher quality as compared with the existing state-of-the-art approaches.



### Instance-Aware Representation Learning and Association for Online Multi-Person Tracking
- **Arxiv ID**: http://arxiv.org/abs/1905.12409v1
- **DOI**: 10.1016/j.patcog.2019.04.018
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.12409v1)
- **Published**: 2019-05-29 13:16:12+00:00
- **Updated**: 2019-05-29 13:16:12+00:00
- **Authors**: Hefeng Wu, Yafei Hu, Keze Wang, Hanhui Li, Lin Nie, Hui Cheng
- **Comment**: accepted by Pattern Recognition
- **Journal**: None
- **Summary**: Multi-Person Tracking (MPT) is often addressed within the detection-to-association paradigm. In such approaches, human detections are first extracted in every frame and person trajectories are then recovered by a procedure of data association (usually offline). However, their performances usually degenerate in presence of detection errors, mutual interactions and occlusions. In this paper, we present a deep learning based MPT approach that learns instance-aware representations of tracked persons and robustly online infers states of the tracked persons. Specifically, we design a multi-branch neural network (MBN), which predicts the classification confidences and locations of all targets by taking a batch of candidate regions as input. In our MBN architecture, each branch (instance-subnet) corresponds to an individual to be tracked and new branches can be dynamically created for handling newly appearing persons. Then based on the output of MBN, we construct a joint association matrix that represents meaningful states of tracked persons (e.g., being tracked or disappearing from the scene) and solve it by using the efficient Hungarian algorithm. Moreover, we allow the instance-subnets to be updated during tracking by online mining hard examples, accounting to person appearance variations over time. We comprehensively evaluate our framework on a popular MPT benchmark, demonstrating its excellent performance in comparison with recent online MPT methods.



### Uncertainty Based Detection and Relabeling of Noisy Image Labels
- **Arxiv ID**: http://arxiv.org/abs/1906.11876v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.11876v1)
- **Published**: 2019-05-29 13:44:38+00:00
- **Updated**: 2019-05-29 13:44:38+00:00
- **Authors**: Jan M. Köhler, Maximilian Autenrieth, William H. Beluch
- **Comment**: Uncertainty and Robustness in Deep Visual Learning Workshop at CVPR
  2019
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are powerful tools in computer vision tasks. However, in many realistic scenarios label noise is prevalent in the training images, and overfitting to these noisy labels can significantly harm the generalization performance of DNNs. We propose a novel technique to identify data with noisy labels based on the different distributions of the predictive uncertainties from a DNN over the clean and noisy data. Additionally, the behavior of the uncertainty over the course of training helps to identify the network weights which best can be used to relabel the noisy labels. Data with noisy labels can therefore be cleaned in an iterative process. Our proposed method can be easily implemented, and shows promising performance on the task of noisy label detection on CIFAR-10 and CIFAR-100.



### Hierarchical Feature Aggregation Networks for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1905.12462v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.12462v1)
- **Published**: 2019-05-29 13:58:37+00:00
- **Updated**: 2019-05-29 13:58:37+00:00
- **Authors**: Swathikiran Sudhakaran, Sergio Escalera, Oswald Lanz
- **Comment**: None
- **Journal**: None
- **Summary**: Most action recognition methods base on a) a late aggregation of frame level CNN features using average pooling, max pooling, or RNN, among others, or b) spatio-temporal aggregation via 3D convolutions. The first assume independence among frame features up to a certain level of abstraction and then perform higher-level aggregation, while the second extracts spatio-temporal features from grouped frames as early fusion. In this paper we explore the space in between these two, by letting adjacent feature branches interact as they develop into the higher level representation. The interaction happens between feature differencing and averaging at each level of the hierarchy, and it has convolutional structure that learns to select the appropriate mode locally in contrast to previous works that impose one of the modes globally (e.g. feature differencing) as a design choice. We further constrain this interaction to be conservative, e.g. a local feature subtraction in one branch is compensated by the addition on another, such that the total feature flow is preserved. We evaluate the performance of our proposal on a number of existing models, i.e. TSN, TRN and ECO, to show its flexibility and effectiveness in improving action recognition performance.



### Food for thought: Ethical considerations of user trust in computer vision
- **Arxiv ID**: http://arxiv.org/abs/1905.12487v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1905.12487v1)
- **Published**: 2019-05-29 14:25:43+00:00
- **Updated**: 2019-05-29 14:25:43+00:00
- **Authors**: Kaylen J. Pfisterer, Jennifer Boger, Alexander Wong
- **Comment**: Accepted to CVPR2019: Fairness Accountability Transparency and Ethics
  in Computer Vision Workshop
- **Journal**: None
- **Summary**: In computer vision research, especially when novel applications of tools are developed, ethical implications around user perceptions of trust in the underlying technology should be considered and supported. Here, we describe an example of the incorporation of such considerations within the long-term care sector for tracking resident food and fluid intake. We highlight our recent user study conducted to develop a Goldilocks quality horizontal prototype designed to support trust cues in which perceived trust in our horizontal prototype was higher than the existing system in place. We discuss the importance and need for user engagement as part of ongoing computer vision-driven technology development and describe several important factors related to trust that are relevant to developing decision-making tools.



### Image-to-Image Translation with Multi-Path Consistency Regularization
- **Arxiv ID**: http://arxiv.org/abs/1905.12498v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.12498v1)
- **Published**: 2019-05-29 14:36:03+00:00
- **Updated**: 2019-05-29 14:36:03+00:00
- **Authors**: Jianxin Lin, Yingce Xia, Yijun Wang, Tao Qin, Zhibo Chen
- **Comment**: 8 pages, 6 figures. Accepted by the 28th International Joint
  Conference on Artificial Intelligence (IJCAI-2019)
- **Journal**: None
- **Summary**: Image translation across different domains has attracted much attention in both machine learning and computer vision communities. Taking the translation from source domain $\mathcal{D}_s$ to target domain $\mathcal{D}_t$ as an example, existing algorithms mainly rely on two kinds of loss for training: One is the discrimination loss, which is used to differentiate images generated by the models and natural images; the other is the reconstruction loss, which measures the difference between an original image and the reconstructed version through $\mathcal{D}_s\to\mathcal{D}_t\to\mathcal{D}_s$ translation. In this work, we introduce a new kind of loss, multi-path consistency loss, which evaluates the differences between direct translation $\mathcal{D}_s\to\mathcal{D}_t$ and indirect translation $\mathcal{D}_s\to\mathcal{D}_a\to\mathcal{D}_t$ with $\mathcal{D}_a$ as an auxiliary domain, to regularize training. For multi-domain translation (at least, three) which focuses on building translation models between any two domains, at each training iteration, we randomly select three domains, set them respectively as the source, auxiliary and target domains, build the multi-path consistency loss and optimize the network. For two-domain translation, we need to introduce an additional auxiliary domain and construct the multi-path consistency loss. We conduct various experiments to demonstrate the effectiveness of our proposed methods, including face-to-face translation, paint-to-photo translation, and de-raining/de-noising translation.



### GlyphGAN: Style-Consistent Font Generation Based on Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.12502v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.12502v2)
- **Published**: 2019-05-29 14:44:23+00:00
- **Updated**: 2019-05-30 07:30:43+00:00
- **Authors**: Hideaki Hayashi, Kohtaro Abe, Seiichi Uchida
- **Comment**: To appear in Knowledge-Based Systems
- **Journal**: None
- **Summary**: In this paper, we propose GlyphGAN: style-consistent font generation based on generative adversarial networks (GANs). GANs are a framework for learning a generative model using a system of two neural networks competing with each other. One network generates synthetic images from random input vectors, and the other discriminates between synthetic and real images. The motivation of this study is to create new fonts using the GAN framework while maintaining style consistency over all characters. In GlyphGAN, the input vector for the generator network consists of two vectors: character class vector and style vector. The former is a one-hot vector and is associated with the character class of each sample image during training. The latter is a uniform random vector without supervised information. In this way, GlyphGAN can generate an infinite variety of fonts with the character and style independently controlled. Experimental results showed that fonts generated by GlyphGAN have style consistency and diversity different from the training images without losing their legibility.



### Are Disentangled Representations Helpful for Abstract Visual Reasoning?
- **Arxiv ID**: http://arxiv.org/abs/1905.12506v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1905.12506v3)
- **Published**: 2019-05-29 14:52:32+00:00
- **Updated**: 2020-01-07 14:36:07+00:00
- **Authors**: Sjoerd van Steenkiste, Francesco Locatello, Jürgen Schmidhuber, Olivier Bachem
- **Comment**: Accepted to NeurIPS 2019
- **Journal**: None
- **Summary**: A disentangled representation encodes information about the salient factors of variation in the data independently. Although it is often argued that this representational format is useful in learning to solve many real-world down-stream tasks, there is little empirical evidence that supports this claim. In this paper, we conduct a large-scale study that investigates whether disentangled representations are more suitable for abstract reasoning tasks. Using two new tasks similar to Raven's Progressive Matrices, we evaluate the usefulness of the representations learned by 360 state-of-the-art unsupervised disentanglement models. Based on these representations, we train 3600 abstract reasoning models and observe that disentangled representations do in fact lead to better down-stream performance. In particular, they enable quicker learning using fewer samples.



### Smooth Shells: Multi-Scale Shape Registration with Functional Maps
- **Arxiv ID**: http://arxiv.org/abs/1905.12512v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1905.12512v2)
- **Published**: 2019-05-29 15:01:10+00:00
- **Updated**: 2019-12-02 16:33:20+00:00
- **Authors**: Marvin Eisenberger, Zorah Lähner, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel 3D shape correspondence method based on the iterative alignment of so-called smooth shells. Smooth shells define a series of coarse-to-fine shape approximations designed to work well with multiscale algorithms. The main idea is to first align rough approximations of the geometry and then add more and more details to refine the correspondence. We fuse classical shape registration with Functional Maps by embedding the input shapes into an intrinsic-extrinsic product space. Moreover, we disambiguate intrinsic symmetries by applying a surrogate based Markov chain Monte Carlo initialization. Our method naturally handles various types of noise that commonly occur in real scans, like non-isometry or incompatible meshing. Finally, we demonstrate state-of-the-art quantitative results on several datasets and show that our pipeline produces smoother, more realistic results than other automatic matching methods in real world applications.



### Stabilizing GANs with Soft Octave Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1905.12534v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.12534v3)
- **Published**: 2019-05-29 15:28:54+00:00
- **Updated**: 2020-12-17 15:49:57+00:00
- **Authors**: Ricard Durall, Franz-Josef Pfreundt, Janis Keuper
- **Comment**: None
- **Journal**: None
- **Summary**: Motivated by recently published methods using frequency decompositions of convolutions (e.g. Octave Convolutions), we propose a novel convolution scheme to stabilize the training and reduce the likelihood of a mode collapse. The basic idea of our approach is to split convolutional filters into additive high and low frequency parts, while shifting weight updates from low to high during the training. Intuitively, this method forces GANs to learn low frequency coarse image structures before descending into fine (high frequency) details. We also show, that the use of the proposed soft octave convolutions reduces common artifacts in the frequency domain of generated images. Our approach is orthogonal and complementary to existing stabilization methods and can simply be plugged into any CNN based GAN architecture. Experiments on the CelebA dataset show the effectiveness of the proposed method.



### A Quaternion-based Certifiably Optimal Solution to the Wahba Problem with Outliers
- **Arxiv ID**: http://arxiv.org/abs/1905.12536v4
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.RO, 68T40, 74Pxx, 46N10, 65D19, I.2.9; G.1.6; I.4.5; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1905.12536v4)
- **Published**: 2019-05-29 15:36:40+00:00
- **Updated**: 2019-09-22 14:54:34+00:00
- **Authors**: Heng Yang, Luca Carlone
- **Comment**: 21 pages, accepted for Oral Presentation at ICCV 2019
- **Journal**: None
- **Summary**: The Wahba problem, also known as rotation search, seeks to find the best rotation to align two sets of vector observations given putative correspondences, and is a fundamental routine in many computer vision and robotics applications. This work proposes the first polynomial-time certifiably optimal approach for solving the Wahba problem when a large number of vector observations are outliers. Our first contribution is to formulate the Wahba problem using a Truncated Least Squares (TLS) cost that is insensitive to a large fraction of spurious correspondences. The second contribution is to rewrite the problem using unit quaternions and show that the TLS cost can be framed as a Quadratically-Constrained Quadratic Program (QCQP). Since the resulting optimization is still highly non-convex and hard to solve globally, our third contribution is to develop a convex Semidefinite Programming (SDP) relaxation. We show that while a naive relaxation performs poorly in general, our relaxation is tight even in the presence of large noise and outliers. We validate the proposed algorithm, named QUASAR (QUAternion-based Semidefinite relAxation for Robust alignment), in both synthetic and real datasets showing that the algorithm outperforms RANSAC, robust local optimization techniques, global outlier-removal procedures, and Branch-and-Bound methods. QUASAR is able to compute certifiably optimal solutions (i.e. the relaxation is exact) even in the case when 95% of the correspondences are outliers.



### Application of Different Simulated Spectral Data and Machine Learning to Estimate the Chlorophyll a Concentration of Several Inland Waters
- **Arxiv ID**: http://arxiv.org/abs/1905.12563v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.12563v2)
- **Published**: 2019-05-29 16:14:52+00:00
- **Updated**: 2019-08-21 08:43:01+00:00
- **Authors**: Philipp M. Maier, Sina Keller
- **Comment**: This contribution was accepted for the IEEE Whispers 2019 in
  Amsterdam
- **Journal**: None
- **Summary**: Water quality is of great importance for humans and for the environment and has to be monitored continuously. It is determinable through proxies such as the chlorophyll a concentration, which can be monitored by remote sensing techniques. This study focuses on the trade-off between the spatial and the spectral resolution of six simulated satellite-based data sets when estimating the chlorophyll a concentration with supervised machine learning models. The initial dataset for the spectral simulation of the satellite missions contains spectrometer data and measured chlorophyll a concentration of 13 different inland waters. Focusing on the regression performance, it appears that the machine learning models achieve almost as good results with the simulated Sentinel data as with the simulated hyperspectral data. Regarding the applicability, the Sentinel 2 mission is the best choice for small inland waters due to its high spatial and temporal resolution in combination with a suitable spectral resolution.



### Flat2Layout: Flat Representation for Estimating Layout of General Room Types
- **Arxiv ID**: http://arxiv.org/abs/1905.12571v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.12571v1)
- **Published**: 2019-05-29 16:26:43+00:00
- **Updated**: 2019-05-29 16:26:43+00:00
- **Authors**: Chi-Wei Hsiao, Cheng Sun, Min Sun, Hwann-Tzong Chen
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a new approach, Flat2Layout, for estimating general indoor room layout from a single-view RGB image whereas existing methods can only produce layout topologies captured from the box-shaped room. The proposed flat representation encodes the layout information into row vectors which are treated as the training target of the deep model. A dynamic programming based postprocessing is employed to decode the estimated flat output from the deep model into the final room layout. Flat2Layout achieves state-of-the-art performance on existing room layout benchmark. This paper also constructs a benchmark for validating the performance on general layout topologies, where Flat2Layout achieves good performance on general room types. Flat2Layout is applicable on more scenario for layout estimation and would have an impact on applications of Scene Modeling, Robotics, and Augmented Reality.



### Segmentation of blood vessels in retinal fundus images
- **Arxiv ID**: http://arxiv.org/abs/1905.12596v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.12596v1)
- **Published**: 2019-05-29 17:17:45+00:00
- **Updated**: 2019-05-29 17:17:45+00:00
- **Authors**: Michiel Straat, Jorrit Oosterhof
- **Comment**: Conference: SC@RUG 2017, 6 pages
- **Journal**: None
- **Summary**: In recent years, several automatic segmentation methods have been proposed for blood vessels in retinal fundus images, ranging from using cheap and fast trainable filters to complicated neural networks and even deep learning. One example of a filted-based segmentation method is B-COSFIRE. In this approach the image filter is trained with example prototype patterns, to which the filter becomes selective by finding points in a Difference of Gaussian response on circles around the center with large intensity variation. In this paper we discuss and evaluate several of these vessel segmentation methods. We take a closer look at B-COSFIRE and study the performance of B-COSFIRE on the recently published IOSTAR dataset by experiments and we examine how the parameter values affect the performance. In the experiment we manage to reach a segmentation accuracy of 0.9419. Based on our findings we discuss when B-COSFIRE is the preferred method to use and in which circumstances it could be beneficial to use a more (computationally) complex segmentation method. We also shortly discuss areas beyond blood vessel segmentation where these methods can be used to segment elongated structures, such as rivers in satellite images or nerves of a leaf.



### Learning Navigation Subroutines from Egocentric Videos
- **Arxiv ID**: http://arxiv.org/abs/1905.12612v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.12612v2)
- **Published**: 2019-05-29 17:50:19+00:00
- **Updated**: 2019-10-15 08:10:25+00:00
- **Authors**: Ashish Kumar, Saurabh Gupta, Jitendra Malik
- **Comment**: None
- **Journal**: None
- **Summary**: Planning at a higher level of abstraction instead of low level torques improves the sample efficiency in reinforcement learning, and computational efficiency in classical planning. We propose a method to learn such hierarchical abstractions, or subroutines from egocentric video data of experts performing tasks. We learn a self-supervised inverse model on small amounts of random interaction data to pseudo-label the expert egocentric videos with agent actions. Visuomotor subroutines are acquired from these pseudo-labeled videos by learning a latent intent-conditioned policy that predicts the inferred pseudo-actions from the corresponding image observations. We demonstrate our proposed approach in context of navigation, and show that we can successfully learn consistent and diverse visuomotor subroutines from passive egocentric videos. We demonstrate the utility of our acquired visuomotor subroutines by using them as is for exploration, and as sub-policies in a hierarchical RL framework for reaching point goals and semantic goals. We also demonstrate behavior of our subroutines in the real world, by deploying them on a real robotic platform. Project website: https://ashishkumar1993.github.io/subroutines/.



### Provably scale-covariant continuous hierarchical networks based on scale-normalized differential expressions coupled in cascade
- **Arxiv ID**: http://arxiv.org/abs/1905.13555v3
- **DOI**: 10.1007/s10851-019-00915-x
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.13555v3)
- **Published**: 2019-05-29 18:02:48+00:00
- **Updated**: 2019-10-28 11:55:43+00:00
- **Authors**: Tony Lindeberg
- **Comment**: 29 pages, 16 figures, 3 tables. arXiv admin note: substantial text
  overlap with arXiv:1903.00289
- **Journal**: Journal of Mathematical Imaging and Vision, 62(1): 120-148, 2020
- **Summary**: This article presents a theory for constructing hierarchical networks in such a way that the networks are guaranteed to be provably scale covariant. We first present a general sufficiency argument for obtaining scale covariance, which holds for a wide class of networks defined from linear and non-linear differential expressions expressed in terms of scale-normalized scale-space derivatives. Then, we present a more detailed development of one example of such a network constructed from a combination of mathematically derived models of receptive fields and biologically inspired computations. Based on a functional model of complex cells in terms of an oriented quasi quadrature combination of first- and second-order directional Gaussian derivatives, we couple such primitive computations in cascade over combinatorial expansions over image orientations. Scale-space properties of the computational primitives are analysed and we give explicit proofs of how the resulting representation allows for scale and rotation covariance. A prototype application to texture analysis is developed and it is demonstrated that a simplified mean-reduced representation of the resulting QuasiQuadNet leads to promising experimental results on three texture datasets.



### Emergence of Object Segmentation in Perturbed Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1905.12663v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.12663v2)
- **Published**: 2019-05-29 18:17:39+00:00
- **Updated**: 2019-11-02 17:46:33+00:00
- **Authors**: Adam Bielski, Paolo Favaro
- **Comment**: 33rd Conference on Neural Information Processing Systems (NeurIPS
  2019), Spotlight presentation
- **Journal**: None
- **Summary**: We introduce a novel framework to build a model that can learn how to segment objects from a collection of images without any human annotation. Our method builds on the observation that the location of object segments can be perturbed locally relative to a given background without affecting the realism of a scene. Our approach is to first train a generative model of a layered scene. The layered representation consists of a background image, a foreground image and the mask of the foreground. A composite image is then obtained by overlaying the masked foreground image onto the background. The generative model is trained in an adversarial fashion against a discriminator, which forces the generative model to produce realistic composite images. To force the generator to learn a representation where the foreground layer corresponds to an object, we perturb the output of the generative model by introducing a random shift of both the foreground image and mask relative to the background. Because the generator is unaware of the shift before computing its output, it must produce layered representations that are realistic for any such random perturbation. Finally, we learn to segment an image by defining an autoencoder consisting of an encoder, which we train, and the pre-trained generator as the decoder, which we freeze. The encoder maps an image to a feature vector, which is fed as input to the generator to give a composite image matching the original input image. Because the generator outputs an explicit layered representation of the scene, the encoder learns to detect and segment objects. We demonstrate this framework on real images of several object categories.



### Entropic Regularisation of Robust Optimal Transport
- **Arxiv ID**: http://arxiv.org/abs/1905.12678v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.12678v1)
- **Published**: 2019-05-29 18:51:11+00:00
- **Updated**: 2019-05-29 18:51:11+00:00
- **Authors**: Rozenn Dahyot, Hana Alghamdi, Mairead Grogan
- **Comment**: 8 pages
- **Journal**: Proceeding of Irish Machine Vision and Image Processing conference
  IMVIP 2019
- **Summary**: Grogan et al [11,12] have recently proposed a solution to colour transfer by minimising the Euclidean distance L2 between two probability density functions capturing the colour distributions of two images (palette and target). It was shown to be very competitive to alternative solutions based on Optimal Transport for colour transfer. We show that in fact Grogan et al's formulation can also be understood as a new robust Optimal Transport based framework with entropy regularisation over marginals.



### What Makes Training Multi-Modal Classification Networks Hard?
- **Arxiv ID**: http://arxiv.org/abs/1905.12681v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.12681v5)
- **Published**: 2019-05-29 19:10:06+00:00
- **Updated**: 2020-04-03 00:36:42+00:00
- **Authors**: Weiyao Wang, Du Tran, Matt Feiszli
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Consider end-to-end training of a multi-modal vs. a single-modal network on a task with multiple input modalities: the multi-modal network receives more information, so it should match or outperform its single-modal counterpart. In our experiments, however, we observe the opposite: the best single-modal network always outperforms the multi-modal network. This observation is consistent across different combinations of modalities and on different tasks and benchmarks.   This paper identifies two main causes for this performance drop: first, multi-modal networks are often prone to overfitting due to increased capacity. Second, different modalities overfit and generalize at different rates, so training them jointly with a single optimization strategy is sub-optimal. We address these two problems with a technique we call Gradient Blending, which computes an optimal blend of modalities based on their overfitting behavior. We demonstrate that Gradient Blending outperforms widely-used baselines for avoiding overfitting and achieves state-of-the-art accuracy on various tasks including human action recognition, ego-centric action recognition, and acoustic event detection.



### A survey of Object Classification and Detection based on 2D/3D data
- **Arxiv ID**: http://arxiv.org/abs/1905.12683v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.12683v2)
- **Published**: 2019-05-29 19:15:01+00:00
- **Updated**: 2022-01-22 04:57:44+00:00
- **Authors**: Xiaoke Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, by using deep neural network based algorithms, object classification, detection and semantic segmentation solutions are significantly improved. However, one challenge for 2D image-based systems is that they cannot provide accurate 3D location information. This is critical for location sensitive applications such as autonomous driving and robot navigation. On the other hand, 3D methods, such as RGB-D and RGB-LiDAR based systems, can provide solutions that significantly improve the RGB only approaches. That is why this is an interesting research area for both industry and academia. Compared with 2D image-based systems, 3D-based systems are more complicated due to the following five reasons: 1) Data representation itself is more complicated. 3D images can be represented by point clouds, meshes, volumes. 2D images have pixel grid representations. 2) The computation and memory resource requirement is higher as an extra dimension is added. 3) Different distribution of the objects and difference in scene areas between indoor and outdoor make one unified framework hard to achieve. 4) 3D data, especially for the outdoor scenario, is sparse compared with the dense 2D images which makes the detection task more challenging. Finally, large size labelled datasets, which are extremely important for supervised based algorithms, are still under construction compared with well-built 2D datasets such as ImageNet. Based on challenges listed above, the described systems are organized by application scenarios, data representation methods and main tasks addressed. At the same time, critical 2D based systems which greatly influence the 3D ones are also introduced to show the connection between them.



### Dynamic Traffic Scene Classification with Space-Time Coherence
- **Arxiv ID**: http://arxiv.org/abs/1905.12708v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.12708v1)
- **Published**: 2019-05-29 20:28:49+00:00
- **Updated**: 2019-05-29 20:28:49+00:00
- **Authors**: Athma Narayanan, Isht Dwivedi, Behzad Dariush
- **Comment**: accpeted in (International Conference on Robotics and Automation)ICRA
  2019
- **Journal**: None
- **Summary**: This paper examines the problem of dynamic traffic scene classification under space-time variations in viewpoint that arise from video captured on-board a moving vehicle. Solutions to this problem are important for realization of effective driving assistance technologies required to interpret or predict road user behavior. Currently, dynamic traffic scene classification has not been adequately addressed due to a lack of benchmark datasets that consider spatiotemporal evolution of traffic scenes resulting from a vehicle's ego-motion. This paper has three main contributions. First, an annotated dataset is released to enable dynamic scene classification that includes 80 hours of diverse high quality driving video data clips collected in the San Francisco Bay area. The dataset includes temporal annotations for road places, road types, weather, and road surface conditions. Second, we introduce novel and baseline algorithms that utilize semantic context and temporal nature of the dataset for dynamic classification of road scenes. Finally, we showcase algorithms and experimental results that highlight how extracted features from scene classification serve as strong priors and help with tactical driver behavior understanding. The results show significant improvement from previously reported driving behavior detection baselines in the literature.



### Extending Monocular Visual Odometry to Stereo Camera Systems by Scale Optimization
- **Arxiv ID**: http://arxiv.org/abs/1905.12723v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.12723v3)
- **Published**: 2019-05-29 21:05:46+00:00
- **Updated**: 2019-09-17 04:05:06+00:00
- **Authors**: Jiawei Mo, Junaed Sattar
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel approach for extending monocular visual odometry to a stereo camera system. The proposed method uses an additional camera to accurately estimate and optimize the scale of the monocular visual odometry, rather than triangulating 3D points from stereo matching. Specifically, the 3D points generated by the monocular visual odometry are projected onto the other camera of the stereo pair, and the scale is recovered and optimized by directly minimizing the photometric error. It is computationally efficient, adding minimal overhead to the stereo vision system compared to straightforward stereo matching, and is robust to repetitive texture. Additionally, direct scale optimization enables stereo visual odometry to be purely based on the direct method. Extensive evaluation on public datasets (e.g., KITTI), and outdoor environments (both terrestrial and underwater) demonstrates the accuracy and efficiency of a stereo visual odometry approach extended by scale optimization, and its robustness in environments with challenging textures.



### Zeroth-Order Stochastic Alternating Direction Method of Multipliers for Nonconvex Nonsmooth Optimization
- **Arxiv ID**: http://arxiv.org/abs/1905.12729v2
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.12729v2)
- **Published**: 2019-05-29 21:10:14+00:00
- **Updated**: 2019-07-30 02:09:00+00:00
- **Authors**: Feihu Huang, Shangqian Gao, Songcan Chen, Heng Huang
- **Comment**: To Appear in IJCAI 2019. Supplementary materials are added
- **Journal**: None
- **Summary**: Alternating direction method of multipliers (ADMM) is a popular optimization tool for the composite and constrained problems in machine learning. However, in many machine learning problems such as black-box attacks and bandit feedback, ADMM could fail because the explicit gradients of these problems are difficult or infeasible to obtain. Zeroth-order (gradient-free) methods can effectively solve these problems due to that the objective function values are only required in the optimization. Recently, though there exist a few zeroth-order ADMM methods, they build on the convexity of objective function. Clearly, these existing zeroth-order methods are limited in many applications. In the paper, thus, we propose a class of fast zeroth-order stochastic ADMM methods (i.e., ZO-SVRG-ADMM and ZO-SAGA-ADMM) for solving nonconvex problems with multiple nonsmooth penalties, based on the coordinate smoothing gradient estimator. Moreover, we prove that both the ZO-SVRG-ADMM and ZO-SAGA-ADMM have convergence rate of $O(1/T)$, where $T$ denotes the number of iterations. In particular, our methods not only reach the best convergence rate $O(1/T)$ for the nonconvex optimization, but also are able to effectively solve many complex machine learning problems with multiple regularized penalties and constraints. Finally, we conduct the experiments of black-box binary classification and structured adversarial attack on black-box deep neural network to validate the efficiency of our algorithms.



### Distant Pedestrian Detection in the Wild using Single Shot Detector with Deep Convolutional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.12759v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.12759v1)
- **Published**: 2019-05-29 22:42:49+00:00
- **Updated**: 2019-05-29 22:42:49+00:00
- **Authors**: Ranjith Dinakaran, Philip Easom, Li Zhang, Ahmed Bouridane, Richard Jiang, Eran Edirisinghe
- **Comment**: arXiv admin note: text overlap with arXiv:1711.08174,
  arXiv:1511.06434, arXiv:1706.05274 by other authors
- **Journal**: The 2019 International Joint Conference on Neural Networks (IJCNN)
- **Summary**: In this work, we examine the feasibility of applying Deep Convolutional Generative Adversarial Networks (DCGANs) with Single Shot Detector (SSD) as data-processing technique to handle with the challenge of pedestrian detection in the wild. Specifically, we attempted to use in-fill completion (where a portion of the image is masked) to generate random transformations of images with portions missing to expand existing labelled datasets. In our work, GAN has been trained intensively on low resolution images, in order to neutralize the challenges of the pedestrian detection in the wild, and considered humans, and few other classes for detection in smart cities. The object detector experiment performed by training GAN model along with SSD provided a substantial improvement in the results. This approach presents a very interesting overview in the current state of art on GAN networks for object detection. We used Canadian Institute for Advanced Research (CIFAR), Caltech, KITTI data set for training and testing the network under different resolutions and the experimental results with comparison been showedbetween DCGAN cascaded with SSD and SSD itself.



### Batch weight for domain adaptation with mass shift
- **Arxiv ID**: http://arxiv.org/abs/1905.12760v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.12760v1)
- **Published**: 2019-05-29 22:43:29+00:00
- **Updated**: 2019-05-29 22:43:29+00:00
- **Authors**: Mikołaj Bińkowski, R Devon Hjelm, Aaron Courville
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain transfer is the task of transferring or translating samples from a source distribution to a different target distribution. Current solutions unsupervised domain transfer often operate on data on which the modes of the distribution are well-matched, for instance have the same frequencies of classes between source and target distributions. However, these models do not perform well when the modes are not well-matched, as would be the case when samples are drawn independently from two different, but related, domains. This mode imbalance is problematic as generative adversarial networks (GANs), a successful approach in this setting, are sensitive to mode frequency, which results in a mismatch of semantics between source samples and generated samples of the target distribution. We propose a principled method of re-weighting training samples to correct for such mass shift between the transferred distributions, which we call batch-weight. We also provide rigorous probabilistic setting for domain transfer and new simplified objective for training transfer networks, an alternative to complex, multi-component loss functions used in the current state-of-the art image-to-image translation models. The new objective stems from the discrimination of joint distributions and enforces cycle-consistency in an abstract, high-level, rather than pixel-wise, sense. Lastly, we experimentally show the effectiveness of the proposed methods in several image-to-image translation tasks.



### $d$-SNE: Domain Adaptation using Stochastic Neighborhood Embedding
- **Arxiv ID**: http://arxiv.org/abs/1905.12775v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.12775v1)
- **Published**: 2019-05-29 23:16:51+00:00
- **Updated**: 2019-05-29 23:16:51+00:00
- **Authors**: Xiang Xu, Xiong Zhou, Ragav Venkatesan, Gurumurthy Swaminathan, Orchid Majumder
- **Comment**: Accepted as Oral at CVPR 2019
- **Journal**: None
- **Summary**: Deep neural networks often require copious amount of labeled-data to train their scads of parameters. Training larger and deeper networks is hard without appropriate regularization, particularly while using a small dataset. Laterally, collecting well-annotated data is expensive, time-consuming and often infeasible. A popular way to regularize these networks is to simply train the network with more data from an alternate representative dataset. This can lead to adverse effects if the statistics of the representative dataset are dissimilar to our target. This predicament is due to the problem of domain shift. Data from a shifted domain might not produce bespoke features when a feature extractor from the representative domain is used. In this paper, we propose a new technique ($d$-SNE) of domain adaptation that cleverly uses stochastic neighborhood embedding techniques and a novel modified-Hausdorff distance. The proposed technique is learnable end-to-end and is therefore, ideally suited to train neural networks. Extensive experiments demonstrate that $d$-SNE outperforms the current states-of-the-art and is robust to the variances in different datasets, even in the one-shot and semi-supervised learning settings. $d$-SNE also demonstrates the ability to generalize to multiple domains concurrently.



