# Arxiv Papers in cs.CV on 2019-05-15
### DARNet: Deep Active Ray Network for Building Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.05889v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05889v1)
- **Published**: 2019-05-15 00:00:21+00:00
- **Updated**: 2019-05-15 00:00:21+00:00
- **Authors**: Dominic Cheng, Renjie Liao, Sanja Fidler, Raquel Urtasun
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: In this paper, we propose a Deep Active Ray Network (DARNet) for automatic building segmentation. Taking an image as input, it first exploits a deep convolutional neural network (CNN) as the backbone to predict energy maps, which are further utilized to construct an energy function. A polygon-based contour is then evolved via minimizing the energy function, of which the minimum defines the final segmentation. Instead of parameterizing the contour using Euclidean coordinates, we adopt polar coordinates, i.e., rays, which not only prevents self-intersection but also simplifies the design of the energy function. Moreover, we propose a loss function that directly encourages the contours to match building boundaries. Our DARNet is trained end-to-end by back-propagating through the energy minimization and the backbone CNN, which makes the CNN adapt to the dynamics of the contour evolution. Experiments on three building instance segmentation datasets demonstrate our DARNet achieves either state-of-the-art or comparable performances to other competitors.



### Crowd Density Estimation using Novel Feature Descriptor
- **Arxiv ID**: http://arxiv.org/abs/1905.05891v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05891v1)
- **Published**: 2019-05-15 00:00:59+00:00
- **Updated**: 2019-05-15 00:00:59+00:00
- **Authors**: Adwan Alownie Alanazi, Muhammad Bilal
- **Comment**: None
- **Journal**: International Research Journal of Engineering and Technology,
  Volume 05, Issue 10, Oct 2018
- **Summary**: Crowd density estimation is an important task for crowd monitoring. Many efforts have been done to automate the process of estimating crowd density from images and videos. Despite series of efforts, it remains a challenging task. In this paper, we proposes a new texture feature-based approach for the estimation of crowd density based on Completed Local Binary Pattern (CLBP). We first divide the image into blocks and then re-divide the blocks into cells. For each cell, we compute CLBP and then concatenate them to describe the texture of the corresponding block. We then train a multi-class Support Vector Machine (SVM) classifier, which classifies each block of image into one of four categories, i.e. Very Low, Low, Medium, and High. We evaluate our technique on the PETS 2009 dataset, and from the experiments, we show to achieve 95% accuracy for the proposed descriptor. We also compare other state-of-the-art texture descriptors and from the experimental results, we show that our proposed method outperforms other state-of-the-art methods.



### Addressing the Loss-Metric Mismatch with Adaptive Loss Alignment
- **Arxiv ID**: http://arxiv.org/abs/1905.05895v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.05895v1)
- **Published**: 2019-05-15 00:12:07+00:00
- **Updated**: 2019-05-15 00:12:07+00:00
- **Authors**: Chen Huang, Shuangfei Zhai, Walter Talbott, Miguel Angel Bautista, Shih-Yu Sun, Carlos Guestrin, Josh Susskind
- **Comment**: Accepted to ICML 2019
- **Journal**: None
- **Summary**: In most machine learning training paradigms a fixed, often handcrafted, loss function is assumed to be a good proxy for an underlying evaluation metric. In this work we assess this assumption by meta-learning an adaptive loss function to directly optimize the evaluation metric. We propose a sample efficient reinforcement learning approach for adapting the loss dynamically during training. We empirically show how this formulation improves performance by simultaneously optimizing the evaluation metric and smoothing the loss landscape. We verify our method in metric learning and classification scenarios, showing considerable improvements over the state-of-the-art on a diverse set of tasks. Importantly, our method is applicable to a wide range of loss functions and evaluation metrics. Furthermore, the learned policies are transferable across tasks and data, demonstrating the versatility of the method.



### Task-Driven Modular Networks for Zero-Shot Compositional Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.05908v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05908v1)
- **Published**: 2019-05-15 01:29:08+00:00
- **Updated**: 2019-05-15 01:29:08+00:00
- **Authors**: Senthil Purushwalkam, Maximilian Nickel, Abhinav Gupta, Marc'Aurelio Ranzato
- **Comment**: http://www.cs.cmu.edu/~spurushw/projects/compositional.html
- **Journal**: None
- **Summary**: One of the hallmarks of human intelligence is the ability to compose learned knowledge into novel concepts which can be recognized without a single training example. In contrast, current state-of-the-art methods require hundreds of training examples for each possible category to build reliable and accurate classifiers. To alleviate this striking difference in efficiency, we propose a task-driven modular architecture for compositional reasoning and sample efficient learning. Our architecture consists of a set of neural network modules, which are small fully connected layers operating in semantic concept space. These modules are configured through a gating function conditioned on the task to produce features representing the compatibility between the input image and the concept under consideration. This enables us to express tasks as a combination of sub-tasks and to generalize to unseen categories by reweighting a set of small modules. Furthermore, the network can be trained efficiently as it is fully differentiable and its modules operate on small sub-spaces. We focus our study on the problem of compositional zero-shot classification of object-attribute categories. We show in our experiments that current evaluation metrics are flawed as they only consider unseen object-attribute pairs. When extending the evaluation to the generalized setting which accounts also for pairs seen during training, we discover that naive baseline methods perform similarly or better than current approaches. However, our modular network is able to outperform all existing approaches on two widely-used benchmark datasets.



### A Learning based Branch and Bound for Maximum Common Subgraph Problems
- **Arxiv ID**: http://arxiv.org/abs/1905.05840v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML, I.5.2; F.2.2
- **Links**: [PDF](http://arxiv.org/pdf/1905.05840v2)
- **Published**: 2019-05-15 01:37:02+00:00
- **Updated**: 2019-05-22 01:40:04+00:00
- **Authors**: Yan-li Liu, Chu-min Li, Hua Jiang, Kun He
- **Comment**: 6 pages, 4 figures, uses ijcai19.sty
- **Journal**: None
- **Summary**: Branch-and-bound (BnB) algorithms are widely used to solve combinatorial problems, and the performance crucially depends on its branching heuristic.In this work, we consider a typical problem of maximum common subgraph (MCS), and propose a branching heuristic inspired from reinforcement learning with a goal of reaching a tree leaf as early as possible to greatly reduce the search tree size.Extensive experiments show that our method is beneficial and outperforms current best BnB algorithm for the MCS.



### Unsupervised Deep Contrast Enhancement with Power Constraint for OLED Displays
- **Arxiv ID**: http://arxiv.org/abs/1905.05916v5
- **DOI**: 10.1109/TIP.2019.2953352
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.05916v5)
- **Published**: 2019-05-15 02:15:35+00:00
- **Updated**: 2019-12-10 01:52:59+00:00
- **Authors**: Yong-Goo Shin, Seung Park, Yoon-Jae Yeo, Min-Jae Yoo, Sung-Jea Ko
- **Comment**: Accepted to IEEE transactions on Image Processing. To be published
- **Journal**: None
- **Summary**: Various power-constrained contrast enhancement (PCCE) techniques have been applied to an organic light emitting diode (OLED) display for reducing the power demands of the display while preserving the image quality. In this paper, we propose a new deep learning-based PCCE scheme that constrains the power consumption of the OLED displays while enhancing the contrast of the displayed image. In the proposed method, the power consumption is constrained by simply reducing the brightness a certain ratio, whereas the perceived visual quality is preserved as much as possible by enhancing the contrast of the image using a convolutional neural network (CNN). Furthermore, our CNN can learn the PCCE technique without a reference image by unsupervised learning. Experimental results show that the proposed method is superior to conventional ones in terms of image quality assessment metrics such as a visual saliency-induced index (VSI) and a measure of enhancement (EME).



### Approximating the Ideal Observer and Hotelling Observer for binary signal detection tasks by use of supervised learning methods
- **Arxiv ID**: http://arxiv.org/abs/1905.06330v1
- **DOI**: 10.1109/TMI.2019.2911211
- **Categories**: **eess.SP**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.06330v1)
- **Published**: 2019-05-15 03:19:29+00:00
- **Updated**: 2019-05-15 03:19:29+00:00
- **Authors**: Weimin Zhou, Hua Li, Mark A. Anastasio
- **Comment**: IEEE Transactions on Medical Imaging (Early Access), 2019
- **Journal**: None
- **Summary**: It is widely accepted that optimization of medical imaging system performance should be guided by task-based measures of image quality (IQ). Task-based measures of IQ quantify the ability of an observer to perform a specific task such as detection or estimation of a signal (e.g., a tumor). For binary signal detection tasks, the Bayesian Ideal Observer (IO) sets an upper limit of observer performance and has been advocated for use in optimizing medical imaging systems and data-acquisition designs. Except in special cases, determination of the IO test statistic is analytically intractable. Markov-chain Monte Carlo (MCMC) techniques can be employed to approximate IO detection performance, but their reported applications have been limited to relatively simple object models. In cases where the IO test statistic is difficult to compute, the Hotelling Observer (HO) can be employed. To compute the HO test statistic, potentially large covariance matrices must be accurately estimated and subsequently inverted, which can present computational challenges. This work investigates supervised learning-based methodologies for approximating the IO and HO test statistics. Convolutional neural networks (CNNs) and single-layer neural networks (SLNNs) are employed to approximate the IO and HO test statistics, respectively. Numerical simulations were conducted for both signal-known-exactly (SKE) and signal-known-statistically (SKS) signal detection tasks. The performances of the supervised learning methods are assessed via receiver operating characteristic (ROC) analysis and the results are compared to those produced by use of traditional numerical methods or analytical calculations when feasible. The potential advantages of the proposed supervised learning approaches for approximating the IO and HO test statistics are discussed.



### Game Theoretic Optimization via Gradient-based Nikaido-Isoda Function
- **Arxiv ID**: http://arxiv.org/abs/1905.05927v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.05927v1)
- **Published**: 2019-05-15 03:20:45+00:00
- **Updated**: 2019-05-15 03:20:45+00:00
- **Authors**: Arvind U. Raghunathan, Anoop Cherian, Devesh K. Jha
- **Comment**: Accepted at International Conference on Machine Learning (ICML), 2019
- **Journal**: None
- **Summary**: Computing Nash equilibrium (NE) of multi-player games has witnessed renewed interest due to recent advances in generative adversarial networks. However, computing equilibrium efficiently is challenging. To this end, we introduce the Gradient-based Nikaido-Isoda (GNI) function which serves: (i) as a merit function, vanishing only at the first-order stationary points of each player's optimization problem, and (ii) provides error bounds to a stationary Nash point. Gradient descent is shown to converge sublinearly to a first-order stationary point of the GNI function. For the particular case of bilinear min-max games and multi-player quadratic games, the GNI function is convex. Hence, the application of gradient descent in this case yields linear convergence to an NE (when one exists). In our numerical experiments, we observe that the GNI formulation always converges to the first-order stationary point of each player's optimization problem.



### Orthogonal Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.05929v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.05929v2)
- **Published**: 2019-05-15 03:34:10+00:00
- **Updated**: 2019-10-15 13:14:20+00:00
- **Authors**: Kui Jia, Shuai Li, Yuxin Wen, Tongliang Liu, Dacheng Tao
- **Comment**: To Appear in IEEE Transactions on Pattern Analysis and Machine
  Intelligence
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2019
- **Summary**: In this paper, we introduce the algorithms of Orthogonal Deep Neural Networks (OrthDNNs) to connect with recent interest of spectrally regularized deep learning methods. OrthDNNs are theoretically motivated by generalization analysis of modern DNNs, with the aim to find solution properties of network weights that guarantee better generalization. To this end, we first prove that DNNs are of local isometry on data distributions of practical interest; by using a new covering of the sample space and introducing the local isometry property of DNNs into generalization analysis, we establish a new generalization error bound that is both scale- and range-sensitive to singular value spectrum of each of networks' weight matrices. We prove that the optimal bound w.r.t. the degree of isometry is attained when each weight matrix has a spectrum of equal singular values, among which orthogonal weight matrix or a non-square one with orthonormal rows or columns is the most straightforward choice, suggesting the algorithms of OrthDNNs. We present both algorithms of strict and approximate OrthDNNs, and for the later ones we propose a simple yet effective algorithm called Singular Value Bounding (SVB), which performs as well as strict OrthDNNs, but at a much lower computational cost. We also propose Bounded Batch Normalization (BBN) to make compatible use of batch normalization with OrthDNNs. We conduct extensive comparative studies by using modern architectures on benchmark image classification. Experiments show the efficacy of OrthDNNs.



### Unsupervised Depth Completion from Visual Inertial Odometry
- **Arxiv ID**: http://arxiv.org/abs/1905.08616v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.08616v4)
- **Published**: 2019-05-15 03:47:18+00:00
- **Updated**: 2021-07-21 11:21:08+00:00
- **Authors**: Alex Wong, Xiaohan Fei, Stephanie Tsuei, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: We describe a method to infer dense depth from camera motion and sparse depth as estimated using a visual-inertial odometry system. Unlike other scenarios using point clouds from lidar or structured light sensors, we have few hundreds to few thousand points, insufficient to inform the topology of the scene. Our method first constructs a piecewise planar scaffolding of the scene, and then uses it to infer dense depth using the image along with the sparse points. We use a predictive cross-modal criterion, akin to `self-supervision,' measuring photometric consistency across time, forward-backward pose consistency, and geometric compatibility with the sparse point cloud. We also launch the first visual-inertial + depth dataset, which we hope will foster additional exploration into combining the complementary strengths of visual and inertial sensors. To compare our method to prior work, we adopt the unsupervised KITTI depth completion benchmark, and show state-of-the-art performance on it. Code available at: https://github.com/alexklwong/unsupervised-depth-completion-visual-inertial-odometry.



### ROI Regularization for Semi-supervised and Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.08615v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.08615v1)
- **Published**: 2019-05-15 03:54:24+00:00
- **Updated**: 2019-05-15 03:54:24+00:00
- **Authors**: Hiroshi Kaizuka, Yasuhiro Nagasaki, Ryo Sako
- **Comment**: 14 pages, 7 tables, 2 figures
- **Journal**: None
- **Summary**: We propose ROI regularization (ROIreg) as a semi-supervised learning method for image classification. ROIreg focuses on the maximum probability of a posterior probability distribution g(x) obtained when inputting an unlabeled data sample x into a convolutional neural network (CNN). ROIreg divides the pixel set of x into multiple blocks and evaluates, for each block, its contribution to the maximum probability. A masked data sample x_ROI is generated by replacing blocks with relatively small degrees of contribution with random images. Then, ROIreg trains CNN so that g(x_ROI ) does not change as much as possible from g(x). Therefore, ROIreg can be said to refine the classification ability of CNN more. On the other hand, Virtual Adverserial Training (VAT), which is an excellent semi-supervised learning method, generates data sample x_VAT by perturbing x in the direction in which g(x) changes most. Then, VAT trains CNN so that g(x_VAT ) does not change from g(x) as much as possible. Therefore, VAT can be said to be a method to improve CNN's weakness. Thus, ROIreg and VAT have complementary training effects. In fact, the combination of VAT and ROIreg improves the results obtained when using VAT or ROIreg alone. This combination also improves the state-of-the-art on "SVHN with and without data augmentation" and "CIFAR-10 without data augmentation". We also propose a method called ROI augmentation (ROIaug) as a method to apply ROIreg to data augmentation in supervised learning. However, the evaluation function used there is different from the standard cross-entropy. ROIaug improves the performance of supervised learning for both SVHN and CIFAR-10. Finally, we investigate the performance degradation of VAT and VAT+ROIreg when data samples not belonging to classification classes are included in unlabeled data.



### Constrained low-tubal-rank tensor recovery for hyperspectral images mixed noise removal by bilateral random projections
- **Arxiv ID**: http://arxiv.org/abs/1905.05941v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05941v1)
- **Published**: 2019-05-15 04:20:12+00:00
- **Updated**: 2019-05-15 04:20:12+00:00
- **Authors**: Hao Zhang, Xi-Le Zhao, Tai-Xiang Jiang, Michael Kwok-Po Ng
- **Comment**: Accepted by IGARSS 2019
- **Journal**: None
- **Summary**: In this paper, we propose a novel low-tubal-rank tensor recovery model, which directly constrains the tubal rank prior for effectively removing the mixed Gaussian and sparse noise in hyperspectral images. The constraints of tubal-rank and sparsity can govern the solution of the denoised tensor in the recovery procedure. To solve the constrained low-tubal-rank model, we develop an iterative algorithm based on bilateral random projections to efficiently solve the proposed model. The advantage of random projections is that the approximation of the low-tubal-rank tensor can be obtained quite accurately in an inexpensive manner. Experimental examples for hyperspectral image denoising are presented to demonstrate the effectiveness and efficiency of the proposed method.



### Depth map estimation methodology for detecting free-obstacle navigation areas
- **Arxiv ID**: http://arxiv.org/abs/1905.05946v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/1905.05946v1)
- **Published**: 2019-05-15 04:57:53+00:00
- **Updated**: 2019-05-15 04:57:53+00:00
- **Authors**: Sergio Trejo, Karla Martinez, Gerardo Flores
- **Comment**: None
- **Journal**: ICUAS'19 The 2019 International Conference on Unmanned Aircraft
  Systems
- **Summary**: This paper presents a vision-based methodology which makes use of a stereo camera rig and a one dimension LiDAR to estimate free obstacle areas for quadrotor navigation. The presented approach fuses information provided by a depth map from a stereo camera rig, and the sensing distance of the 1D-LiDAR. Once the depth map is filtered with a Weighted Least Squares filter (WLS), the information is fused through a Kalman filter algorithm. To determine if there is a free space large enough for the quadrotor to pass through, our approach marks an area inside the disparity map by using the Kalman Filter output information. The whole process is implemented in an embedded computer Jetson TX2 and coded in the Robotic Operating System (ROS). Experiments demonstrate the effectiveness of our approach.



### Joint haze image synthesis and dehazing with mmd-vae losses
- **Arxiv ID**: http://arxiv.org/abs/1905.05947v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05947v1)
- **Published**: 2019-05-15 05:12:21+00:00
- **Updated**: 2019-05-15 05:12:21+00:00
- **Authors**: Zongliang Li, Chi Zhang, Gaofeng Meng, Yuehu Liu
- **Comment**: Preprinted version on arxiv, May-05-2019
- **Journal**: None
- **Summary**: Fog and haze are weathers with low visibility which are adversarial to the driving safety of intelligent vehicles equipped with optical sensors like cameras and LiDARs. Therefore image dehazing for perception enhancement and haze image synthesis for testing perception abilities are equivalently important in the development of such autonomous driving systems. From the view of image translation, these two problems are essentially dual with each other, which have the potentiality to be solved jointly. In this paper, we propose an unsupervised Image-to-Image Translation framework based on Variational Autoencoders (VAE) and Generative Adversarial Nets (GAN) to handle haze image synthesis and haze removal simultaneously. Since the KL divergence in the VAE objectives could not guarantee the optimal mapping under imbalanced and unpaired training samples with limited size, Maximum mean discrepancy (MMD) based VAE is utilized to ensure the translating consistency in both directions. The comprehensive analysis on both synthesis and dehazing performance of our method demonstrate the feasibility and practicability of the proposed method.



### An Efficient Pre-processing Method to Eliminate Adversarial Effects
- **Arxiv ID**: http://arxiv.org/abs/1905.08614v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08614v2)
- **Published**: 2019-05-15 05:38:29+00:00
- **Updated**: 2019-12-30 12:53:27+00:00
- **Authors**: Hua Wang, Jie Wang, Zhaoxia Yin
- **Comment**: in Chinese
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) are vulnerable to adversarial examples generated by imposing subtle perturbations to inputs that lead a model to predict incorrect outputs. Currently, a large number of researches on defending adversarial examples pay little attention to the real-world applications, either with high computational complexity or poor defensive effects. Motivated by this observation, we develop an efficient preprocessing method to defend adversarial images. Specifically, before an adversarial example is fed into the model, we perform two image transformations: WebP compression, which is utilized to remove the small adversarial noises. Flip operation, which flips the image once along one side of the image to destroy the specific structure of adversarial perturbations. Finally, a de-perturbed sample is obtained and can be correctly classified by DNNs. Experimental results on ImageNet show that our method outperforms the state-of-the-art defense methods. It can effectively defend adversarial attacks while ensure only very small accuracy drop on normal images.



### Demographic Inference and Representative Population Estimates from Multilingual Social Media Data
- **Arxiv ID**: http://arxiv.org/abs/1905.05961v1
- **DOI**: 10.1145/3308558.3313684
- **Categories**: **cs.CY**, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.05961v1)
- **Published**: 2019-05-15 06:15:16+00:00
- **Updated**: 2019-05-15 06:15:16+00:00
- **Authors**: Zijian Wang, Scott A. Hale, David Adelani, Przemyslaw A. Grabowicz, Timo Hartmann, Fabian Flöck, David Jurgens
- **Comment**: 12 pages, 10 figures, Proceedings of the 2019 World Wide Web
  Conference (WWW '19)
- **Journal**: Proceedings of the 2019 World Wide Web Conference (WWW '19), May
  13--17, 2019, San Francisco, CA, USA
- **Summary**: Social media provide access to behavioural data at an unprecedented scale and granularity. However, using these data to understand phenomena in a broader population is difficult due to their non-representativeness and the bias of statistical inference tools towards dominant languages and groups. While demographic attribute inference could be used to mitigate such bias, current techniques are almost entirely monolingual and fail to work in a global environment. We address these challenges by combining multilingual demographic inference with post-stratification to create a more representative population sample. To learn demographic attributes, we create a new multimodal deep neural architecture for joint classification of age, gender, and organization-status of social media users that operates in 32 languages. This method substantially outperforms current state of the art while also reducing algorithmic bias. To correct for sampling biases, we propose fully interpretable multilevel regression methods that estimate inclusion probabilities from inferred joint population counts and ground-truth population counts. In a large experiment over multilingual heterogeneous European regions, we show that our demographic inference and bias correction together allow for more accurate estimates of populations and make a significant step towards representative social sensing in downstream applications with multilingual social media.



### Deep Kinship Verification via Appearance-shape Joint Prediction and Adaptation-based Approach
- **Arxiv ID**: http://arxiv.org/abs/1905.05964v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05964v1)
- **Published**: 2019-05-15 06:17:09+00:00
- **Updated**: 2019-05-15 06:17:09+00:00
- **Authors**: Heming Zhang, Xiaolong Wang, C. -C. Jay Kuo
- **Comment**: ICIP 2019
- **Journal**: None
- **Summary**: Kinship verification aims to identify the kin relation between two given face images. It is a very challenging problem due to the lack of training data and facial similarity variations between kinship pairs. In this work, we build a novel appearance and shape based deep learning pipeline. First we adopt the knowledge learned from general face recognition network to learn general facial features. Afterwards, we learn kinship oriented appearance and shape features from kinship pairs and combine them for the final prediction. We have evaluated the model performance on a widely used popular benchmark and demonstrated the superiority over the state-of-the-art.



### Arbitrary Shape Scene Text Detection with Adaptive Text Region Representation
- **Arxiv ID**: http://arxiv.org/abs/1905.05980v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05980v1)
- **Published**: 2019-05-15 07:06:27+00:00
- **Updated**: 2019-05-15 07:06:27+00:00
- **Authors**: Xiaobing Wang, Yingying Jiang, Zhenbo Luo, Cheng-Lin Liu, Hyunsoo Choi, Sungjin Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Scene text detection attracts much attention in computer vision, because it can be widely used in many applications such as real-time text translation, automatic information entry, blind person assistance, robot sensing and so on. Though many methods have been proposed for horizontal and oriented texts, detecting irregular shape texts such as curved texts is still a challenging problem. To solve the problem, we propose a robust scene text detection method with adaptive text region representation. Given an input image, a text region proposal network is first used for extracting text proposals. Then, these proposals are verified and refined with a refinement network. Here, recurrent neural network based adaptive text region representation is proposed for text region refinement, where a pair of boundary points are predicted each time step until no new points are found. In this way, text regions of arbitrary shapes are detected and represented with adaptive number of boundary points. This gives more accurate description of text regions. Experimental results on five benchmarks, namely, CTW1500, TotalText, ICDAR2013, ICDAR2015 and MSRATD500, show that the proposed method achieves state-of-the-art in scene text detection.



### Dilated Spatial Generative Adversarial Networks for Ergodic Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1905.08613v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.08613v1)
- **Published**: 2019-05-15 08:12:53+00:00
- **Updated**: 2019-05-15 08:12:53+00:00
- **Authors**: Cyprien Ruffino, Romain Hérault, Eric Laloy, Gilles Gasso
- **Comment**: None
- **Journal**: Conf{\'e}rence sur l'Apprentissage Automatique, Jun 2018, Rouen,
  France
- **Summary**: Generative models have recently received renewed attention as a result of adversarial learning. Generative adversarial networks consist of samples generation model and a discrimination model able to distinguish between genuine and synthetic samples. In combination with convolutional (for the discriminator) and de-convolutional (for the generator) layers, they are particularly suitable for image generation, especially of natural scenes. However, the presence of fully connected layers adds global dependencies in the generated images. This may lead to high and global variations in the generated sample for small local variations in the input noise. In this work we propose to use architec-tures based on fully convolutional networks (including among others dilated layers), architectures specifically designed to generate globally ergodic images, that is images without global dependencies. Conducted experiments reveal that these architectures are well suited for generating natural textures such as geologic structures .



### User profiles matching for different social networks based on faces embeddings
- **Arxiv ID**: http://arxiv.org/abs/1905.06081v1
- **DOI**: 10.1007/978-3-030-29859-3_47
- **Categories**: **cs.CV**, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1905.06081v1)
- **Published**: 2019-05-15 10:50:51+00:00
- **Updated**: 2019-05-15 10:50:51+00:00
- **Authors**: Timur Sokhin, Nikolay Butakov, Denis Nasonov
- **Comment**: Submitted to HAIS 2019 conference
- **Journal**: None
- **Summary**: It is common practice nowadays to use multiple social networks for different social roles. Although this, these networks assume differences in content type, communications and style of speech. If we intend to understand human behaviour as a key-feature for recommender systems, banking risk assessments or sociological researches, this is better to achieve using a combination of the data from different social media. In this paper, we propose a new approach for user profiles matching across social media based on embeddings of publicly available users' face photos and conduct an experimental study of its efficiency. Our approach is stable to changes in content and style for certain social media.



### Vehicle Shape and Color Classification Using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1905.08612v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.08612v1)
- **Published**: 2019-05-15 11:10:18+00:00
- **Updated**: 2019-05-15 11:10:18+00:00
- **Authors**: Mohamed Nafzi, Michael Brauckmann, Tobias Glasmachers
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a module of vehicle reidentification based on make/model and color classification. It could be used by the Automated Vehicular Surveillance (AVS) or by the fast analysis of video data. Many of problems, that are related to this topic, had to be addressed. In order to facilitate and accelerate the progress in this subject, we will present our way to collect and to label a large scale data set. We used deeper neural networks in our training. They showed a good classification accuracy. We show the results of make/model and color classification on controlled and video data set. We demonstrate with the help of a developed application the re-identification of vehicles on video images based on make/model and color classification. This work was partially funded under the grant.



### Human Motion Trajectory Prediction: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1905.06113v3
- **DOI**: 10.1177/0278364920917446
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.06113v3)
- **Published**: 2019-05-15 12:09:55+00:00
- **Updated**: 2019-12-17 09:27:25+00:00
- **Authors**: Andrey Rudenko, Luigi Palmieri, Michael Herman, Kris M. Kitani, Dariu M. Gavrila, Kai O. Arras
- **Comment**: Submitted to the International Journal of Robotics Research (IJRR),
  37 pages
- **Journal**: None
- **Summary**: With growing numbers of intelligent autonomous systems in human environments, the ability of such systems to perceive, understand and anticipate human behavior becomes increasingly important. Specifically, predicting future positions of dynamic agents and planning considering such predictions are key tasks for self-driving vehicles, service robots and advanced surveillance systems. This paper provides a survey of human motion trajectory prediction. We review, analyze and structure a large selection of work from different communities and propose a taxonomy that categorizes existing methods based on the motion modeling approach and level of contextual information used. We provide an overview of the existing datasets and performance metrics. We discuss limitations of the state of the art and outline directions for further research.



### Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations
- **Arxiv ID**: http://arxiv.org/abs/1905.06139v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.06139v3)
- **Published**: 2019-05-15 12:39:49+00:00
- **Updated**: 2019-11-04 17:10:36+00:00
- **Authors**: Fenglin Liu, Yuanxin Liu, Xuancheng Ren, Xiaodong He, Xu Sun
- **Comment**: Accepted by NeurIPS 2019
- **Journal**: None
- **Summary**: In vision-and-language grounding problems, fine-grained representations of the image are considered to be of paramount importance. Most of the current systems incorporate visual features and textual concepts as a sketch of an image. However, plainly inferred representations are usually undesirable in that they are composed of separate components, the relations of which are elusive. In this work, we aim at representing an image with a set of integrated visual regions and corresponding textual concepts, reflecting certain semantics. To this end, we build the Mutual Iterative Attention (MIA) module, which integrates correlated visual features and textual concepts, respectively, by aligning the two modalities. We evaluate the proposed approach on two representative vision-and-language grounding tasks, i.e., image captioning and visual question answering. In both tasks, the semantic-grounded image representations consistently boost the performance of the baseline models under all metrics across the board. The results demonstrate that our approach is effective and generalizes well to a wide range of models for image-related applications. (The code is available at https://github.com/fenglinliu98/MIA)



### VICSOM: VIsual Clues from SOcial Media for psychological assessment
- **Arxiv ID**: http://arxiv.org/abs/1905.06203v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1905.06203v1)
- **Published**: 2019-05-15 14:18:01+00:00
- **Updated**: 2019-05-15 14:18:01+00:00
- **Authors**: Mohammad Mahdi Dehshibi, Gerard Pons, Bita Baiani, David Masip
- **Comment**: None
- **Journal**: None
- **Summary**: Sharing multimodal information (typically images, videos or text) in Social Network Sites (SNS) occupies a relevant part of our time. The particular way how users expose themselves in SNS can provide useful information to infer human behaviors. This paper proposes to use multimodal data gathered from Instagram accounts to predict the perceived prototypical needs described in Glasser's choice theory. The contribution is two-fold: (i) we provide a large multimodal database from Instagram public profiles (more than 30,000 images and text captions) annotated by expert Psychologists on each perceived behavior according to Glasser's theory, and (ii) we propose to automate the recognition of the (unconsciously) perceived needs by the users. Particularly, we propose a baseline using three different feature sets: visual descriptors based on pixel images (SURF and Visual Bag of Words), a high-level descriptor based on the automated scene description using Convolutional Neural Networks, and a text-based descriptor (Word2vec) obtained from processing the captions provided by the users. Finally, we propose a multimodal fusion of these descriptors obtaining promising results in the multi-label classification problem.



### Significance of parallel computing on the performance of Digital Image Correlation algorithms in MATLAB
- **Arxiv ID**: http://arxiv.org/abs/1905.06228v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.PF, G.1.6; I.5.5; J.2
- **Links**: [PDF](http://arxiv.org/pdf/1905.06228v1)
- **Published**: 2019-05-15 15:05:15+00:00
- **Updated**: 2019-05-15 15:05:15+00:00
- **Authors**: Andreas Thoma, Sridhar Ravi
- **Comment**: 17 pages, 5 figures, 6 tables
- **Journal**: None
- **Summary**: Digital Image Correlation (DIC) is a powerful tool used to evaluate displacements and deformations in a non-intrusive manner. By comparing two images, one of the undeformed reference state of a specimen and another of the deformed target state, the relative displacement between those two states is determined. DIC is well known and often used for post-processing analysis of in-plane displacements and deformation of specimen. Increasing the analysis speed to enable real-time DIC analysis will be beneficial and extend the field of use of this technique. Here we tested several combinations of the most common DIC methods in combination with different parallelization approaches in MATLAB and evaluated their performance to determine whether real-time analysis is possible with these methods. To reflect improvements in computing technology different hardware settings were also analysed. We found that implementation problems can reduce the efficiency of a theoretically superior algorithm such that it becomes practically slower than a sub-optimal algorithm. The Newton-Raphson algorithm in combination with a modified Particle Swarm algorithm in parallel image computation was found to be most effective. This is contrary to theory, suggesting that the inverse-compositional Gauss-Newton algorithm is superior. As expected, the Brute Force Search algorithm is the least effective method. We also found that the correct choice of parallelization tasks is crucial to achieve improvements in computing speed. A poorly chosen parallelisation approach with high parallel overhead leads to inferior performance. Finally, irrespective of the computing mode the correct choice of combinations of integer-pixel and sub-pixel search algorithms is decisive for an efficient analysis. Using currently available hardware real-time analysis at high framerates remains an aspiration.



### 3D Semantic Scene Completion from a Single Depth Image using Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1905.06231v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.06231v1)
- **Published**: 2019-05-15 15:12:41+00:00
- **Updated**: 2019-05-15 15:12:41+00:00
- **Authors**: Yueh-Tung Chen, Martin Garbade, Juergen Gall
- **Comment**: None
- **Journal**: None
- **Summary**: We address the task of 3D semantic scene completion, i.e. , given a single depth image, we predict the semantic labels and occupancy of voxels in a 3D grid representing the scene. In light of the recently introduced generative adversarial networks (GAN), our goal is to explore the potential of this model and the efficiency of various important design choices. Our results show that using conditional GANs outperforms the vanilla GAN setup. We evaluate these architecture designs on several datasets. Based on our experiments, we demonstrate that GANs are able to outperform the performance of a baseline 3D CNN in case of clean annotations, but they suffer from poorly aligned annotations.



### Budget-Aware Adapters for Multi-Domain Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.06242v3
- **DOI**: 10.1109/ICCV.2019.00047
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.06242v3)
- **Published**: 2019-05-15 15:25:04+00:00
- **Updated**: 2020-12-08 14:18:06+00:00
- **Authors**: Rodrigo Berriel, Stéphane Lathuilière, Moin Nabi, Tassilo Klein, Thiago Oliveira-Santos, Nicu Sebe, Elisa Ricci
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Multi-Domain Learning (MDL) refers to the problem of learning a set of models derived from a common deep architecture, each one specialized to perform a task in a certain domain (e.g., photos, sketches, paintings). This paper tackles MDL with a particular interest in obtaining domain-specific models with an adjustable budget in terms of the number of network parameters and computational complexity. Our intuition is that, as in real applications the number of domains and tasks can be very large, an effective MDL approach should not only focus on accuracy but also on having as few parameters as possible. To implement this idea we derive specialized deep models for each domain by adapting a pre-trained architecture but, differently from other methods, we propose a novel strategy to automatically adjust the computational complexity of the network. To this aim, we introduce Budget-Aware Adapters that select the most relevant feature channels to better handle data from a novel domain. Some constraints on the number of active switches are imposed in order to obtain a network respecting the desired complexity budget. Experimentally, we show that our approach leads to recognition accuracy competitive with state-of-the-art approaches but with much lighter networks both in terms of storage and computation.



### Regularized Evolutionary Algorithm for Dynamic Neural Topology Search
- **Arxiv ID**: http://arxiv.org/abs/1905.06252v2
- **DOI**: 10.1007/978-3-030-30642-7_20
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.06252v2)
- **Published**: 2019-05-15 15:36:56+00:00
- **Updated**: 2019-08-19 08:49:44+00:00
- **Authors**: Cristiano Saltori, Subhankar Roy, Nicu Sebe, Giovanni Iacca
- **Comment**: None
- **Journal**: None
- **Summary**: Designing neural networks for object recognition requires considerable architecture engineering. As a remedy, neuro-evolutionary network architecture search, which automatically searches for optimal network architectures using evolutionary algorithms, has recently become very popular. Although very effective, evolutionary algorithms rely heavily on having a large population of individuals (i.e., network architectures) and is therefore memory expensive. In this work, we propose a Regularized Evolutionary Algorithm with low memory footprint to evolve a dynamic image classifier. In details, we introduce novel custom operators that regularize the evolutionary process of a micro-population of 10 individuals. We conduct experiments on three different digits datasets (MNIST, USPS, SVHN) and show that our evolutionary method obtains competitive results with the current state-of-the-art.



### Automatic Long-Term Deception Detection in Group Interaction Videos
- **Arxiv ID**: http://arxiv.org/abs/1905.08617v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.08617v2)
- **Published**: 2019-05-15 15:45:02+00:00
- **Updated**: 2019-06-15 16:36:53+00:00
- **Authors**: Chongyang Bai, Maksim Bolonkin, Judee Burgoon, Chao Chen, Norah Dunbar, Bharat Singh, V. S. Subrahmanian, Zhe Wu
- **Comment**: ICME 2019
- **Journal**: None
- **Summary**: Most work on automated deception detection (ADD) in video has two restrictions: (i) it focuses on a video of one person, and (ii) it focuses on a single act of deception in a one or two minute video. In this paper, we propose a new ADD framework which captures long term deception in a group setting. We study deception in the well-known Resistance game (like Mafia and Werewolf) which consists of 5-8 players of whom 2-3 are spies. Spies are deceptive throughout the game (typically 30-65 minutes) to keep their identity hidden. We develop an ensemble predictive model to identify spies in Resistance videos. We show that features from low-level and high-level video analysis are insufficient, but when combined with a new class of features that we call LiarRank, produce the best results. We achieve AUCs of over 0.70 in a fully automated setting. Our demo can be found at http://home.cs.dartmouth.edu/~mbolonkin/scan/demo/



### Machine learning approach for segmenting glands in colon histology images using local intensity and texture features
- **Arxiv ID**: http://arxiv.org/abs/1905.08611v1
- **DOI**: 10.1109/IADCC.2018.8692135
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.08611v1)
- **Published**: 2019-05-15 16:40:14+00:00
- **Updated**: 2019-05-15 16:40:14+00:00
- **Authors**: Rupali Khatun, Soumick Chatterjee
- **Comment**: None
- **Journal**: 8th International Advance Computing Conference (IACC), 2018
- **Summary**: Colon Cancer is one of the most common types of cancer. The treatment is planned to depend on the grade or stage of cancer. One of the preconditions for grading of colon cancer is to segment the glandular structures of tissues. Manual segmentation method is very time-consuming, and it leads to life risk for the patients. The principal objective of this project is to assist the pathologist to accurate detection of colon cancer. In this paper, the authors have proposed an algorithm for an automatic segmentation of glands in colon histology using local intensity and texture features. Here the dataset images are cropped into patches with different window sizes and taken the intensity of those patches, and also calculated texture-based features. Random forest classifier has been used to classify this patch into different labels. A multilevel random forest technique in a hierarchical way is proposed. This solution is fast, accurate and it is very much applicable in a clinical setup.



### 3D Point Cloud Generative Adversarial Network Based on Tree Structured Graph Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1905.06292v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.06292v2)
- **Published**: 2019-05-15 16:51:18+00:00
- **Updated**: 2019-05-16 02:26:59+00:00
- **Authors**: Dong Wook Shu, Sung Woo Park, Junseok Kwon
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: In this paper, we propose a novel generative adversarial network (GAN) for 3D point clouds generation, which is called tree-GAN. To achieve state-of-the-art performance for multi-class 3D point cloud generation, a tree-structured graph convolution network (TreeGCN) is introduced as a generator for tree-GAN. Because TreeGCN performs graph convolutions within a tree, it can use ancestor information to boost the representation power for features. To evaluate GANs for 3D point clouds accurately, we develop a novel evaluation metric called Frechet point cloud distance (FPD). Experimental results demonstrate that the proposed tree-GAN outperforms state-of-the-art GANs in terms of both conventional metrics and FPD, and can generate point clouds for different semantic parts without prior knowledge.



### BiRA-Net: Bilinear Attention Net for Diabetic Retinopathy Grading
- **Arxiv ID**: http://arxiv.org/abs/1905.06312v2
- **DOI**: 10.1109/ICIP.2019.8803074
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.06312v2)
- **Published**: 2019-05-15 17:38:52+00:00
- **Updated**: 2019-07-01 04:48:50+00:00
- **Authors**: Ziyuan Zhao, Kerui Zhang, Xuejie Hao, Jing Tian, Matthew Chin Heng Chua, Li Chen, Xin Xu
- **Comment**: Accepted at ICIP 2019
- **Journal**: 2019 IEEE International Conference on Image Processing (ICIP)
- **Summary**: Diabetic retinopathy (DR) is a common retinal disease that leads to blindness. For diagnosis purposes, DR image grading aims to provide automatic DR grade classification, which is not addressed in conventional research methods of binary DR image classification. Small objects in the eye images, like lesions and microaneurysms, are essential to DR grading in medical imaging, but they could easily be influenced by other objects. To address these challenges, we propose a new deep learning architecture, called BiRA-Net, which combines the attention model for feature extraction and bilinear model for fine-grained classification. Furthermore, in considering the distance between different grades of different DR categories, we propose a new loss function, called grading loss, which leads to improved training convergence of the proposed approach. Experimental results are provided to demonstrate the superior performance of the proposed approach.



### Synthetic Defocus and Look-Ahead Autofocus for Casual Videography
- **Arxiv ID**: http://arxiv.org/abs/1905.06326v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1905.06326v3)
- **Published**: 2019-05-15 17:59:05+00:00
- **Updated**: 2019-05-21 13:26:02+00:00
- **Authors**: Xuaner Zhang, Kevin Matzen, Vivien Nguyen, Dillon Yao, You Zhang, Ren Ng
- **Comment**: (V2 author name corrected) SIGGRAPH 2019; project website:
  https://ceciliavision.github.io/vid-auto-focus/
- **Journal**: None
- **Summary**: In cinema, large camera lenses create beautiful shallow depth of field (DOF), but make focusing difficult and expensive. Accurate cinema focus usually relies on a script and a person to control focus in realtime. Casual videographers often crave cinematic focus, but fail to achieve it. We either sacrifice shallow DOF, as in smartphone videos; or we struggle to deliver accurate focus, as in videos from larger cameras. This paper is about a new approach in the pursuit of cinematic focus for casual videography. We present a system that synthetically renders refocusable video from a deep DOF video shot with a smartphone, and analyzes future video frames to deliver context-aware autofocus for the current frame. To create refocusable video, we extend recent machine learning methods designed for still photography, contributing a new dataset for machine training, a rendering model better suited to cinema focus, and a filtering solution for temporal coherence. To choose focus accurately for each frame, we demonstrate autofocus that looks at upcoming video frames and applies AI-assist modules such as motion, face, audio and saliency detection. We also show that autofocus benefits from machine learning and a large-scale video dataset with focus annotation, where we use our RVR-LAAF GUI to create this sizable dataset efficiently. We deliver, for example, a shallow DOF video where the autofocus transitions onto each person before she begins to speak. This is impossible for conventional camera autofocus because it would require seeing into the future.



### Local Features and Visual Words Emerge in Activations
- **Arxiv ID**: http://arxiv.org/abs/1905.06358v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.06358v1)
- **Published**: 2019-05-15 18:04:10+00:00
- **Updated**: 2019-05-15 18:04:10+00:00
- **Authors**: Oriane Siméoni, Yannis Avrithis, Ondrej Chum
- **Comment**: None
- **Journal**: CVPR 2019
- **Summary**: We propose a novel method of deep spatial matching (DSM) for image retrieval. Initial ranking is based on image descriptors extracted from convolutional neural network activations by global pooling, as in recent state-of-the-art work. However, the same sparse 3D activation tensor is also approximated by a collection of local features. These local features are then robustly matched to approximate the optimal alignment of the tensors. This happens without any network modification, additional layers or training. No local feature detection happens on the original image. No local feature descriptors and no visual vocabulary are needed throughout the whole process.   We experimentally show that the proposed method achieves the state-of-the-art performance on standard benchmarks across different network architectures and different global pooling methods. The highest gain in performance is achieved when diffusion on the nearest-neighbor graph of global descriptors is initiated from spatially verified images.



### Multi-task Learning for Chest X-ray Abnormality Classification on Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/1905.06362v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.06362v1)
- **Published**: 2019-05-15 18:09:40+00:00
- **Updated**: 2019-05-15 18:09:40+00:00
- **Authors**: Sebastian Guendel, Florin C. Ghesu, Sasa Grbic, Eli Gibson, Bogdan Georgescu, Andreas Maier, Dorin Comaniciu
- **Comment**: None
- **Journal**: None
- **Summary**: Chest X-ray (CXR) is the most common X-ray examination performed in daily clinical practice for the diagnosis of various heart and lung abnormalities. The large amount of data to be read and reported, with 100+ studies per day for a single radiologist, poses a challenge in maintaining consistently high interpretation accuracy. In this work, we propose a method for the classification of different abnormalities based on CXR scans of the human body. The system is based on a novel multi-task deep learning architecture that in addition to the abnormality classification, supports the segmentation of the lungs and heart and classification of regions where the abnormality is located. We demonstrate that by training these tasks concurrently, one can increase the classification performance of the model. Experiments were performed on an extensive collection of 297,541 chest X-ray images from 86,876 patients, leading to a state-of-the-art performance level of 0.883 AUC on average for 12 different abnormalities. We also conducted a detailed performance analysis and compared the accuracy of our system with 3 board-certified radiologists. In this context, we highlight the high level of label noise inherent to this problem. On a reduced subset containing only cases with high confidence reference labels based on the consensus of the 3 radiologists, our system reached an average AUC of 0.945.



### Collaborative Global-Local Networks for Memory-Efficient Segmentation of Ultra-High Resolution Images
- **Arxiv ID**: http://arxiv.org/abs/1905.06368v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.06368v3)
- **Published**: 2019-05-15 18:22:06+00:00
- **Updated**: 2021-03-03 17:35:25+00:00
- **Authors**: Wuyang Chen, Ziyu Jiang, Zhangyang Wang, Kexin Cui, Xiaoning Qian
- **Comment**: CVPR2019 oral
- **Journal**: None
- **Summary**: Segmentation of ultra-high resolution images is increasingly demanded, yet poses significant challenges for algorithm efficiency, in particular considering the (GPU) memory limits. Current approaches either downsample an ultra-high resolution image or crop it into small patches for separate processing. In either way, the loss of local fine details or global contextual information results in limited segmentation accuracy. We propose collaborative Global-Local Networks (GLNet) to effectively preserve both global and local information in a highly memory-efficient manner. GLNet is composed of a global branch and a local branch, taking the downsampled entire image and its cropped local patches as respective inputs. For segmentation, GLNet deeply fuses feature maps from two branches, capturing both the high-resolution fine structures from zoomed-in local patches and the contextual dependency from the downsampled input. To further resolve the potential class imbalance problem between background and foreground regions, we present a coarse-to-fine variant of GLNet, also being memory-efficient. Extensive experiments and analyses have been performed on three real-world ultra-high aerial and medical image datasets (resolution up to 30 million pixels). With only one single 1080Ti GPU and less than 2GB memory used, our GLNet yields high-quality segmentation results and achieves much more competitive accuracy-memory usage trade-offs compared to state-of-the-arts.



### Contrast Optimization And Local Adaptation (COALA) for HDR Compression
- **Arxiv ID**: http://arxiv.org/abs/1905.06372v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.06372v1)
- **Published**: 2019-05-15 18:22:54+00:00
- **Updated**: 2019-05-15 18:22:54+00:00
- **Authors**: Shay Maymon, Hila Barel
- **Comment**: None
- **Journal**: None
- **Summary**: This paper develops a novel approach for high dynamic-range compression. It relies on the widely accepted assumption that the human visual system is not very sensitive to absolute luminance reaching the retina, but rather responds to relative luminance ratios. Dynamic-range compression is then formulated as a regularized optimization in which the image dynamic range is reduced while the local contrast of the original scene is preserved. Our method is shown to be capable of drastic dynamic-range compression, while preserving fine details and avoiding common artifacts such as halos, gradient reversals, or loss of local contrast.



### Tracking in Urban Traffic Scenes from Background Subtraction and Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1905.06381v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.06381v1)
- **Published**: 2019-05-15 18:46:01+00:00
- **Updated**: 2019-05-15 18:46:01+00:00
- **Authors**: Hui-Lee Ooi, Guillaume-Alexandre Bilodeau, Nicolas Saunier
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose to combine detections from background subtraction and from a multiclass object detector for multiple object tracking (MOT) in urban traffic scenes. These objects are associated across frames using spatial, colour and class label information, and trajectory prediction is evaluated to yield the final MOT outputs. The proposed method was tested on the Urban tracker dataset and shows competitive performances compared to state-of-the-art approaches. Results show that the integration of different detection inputs remains a challenging task that greatly affects the MOT performance.



### Dynamic Neural Network Channel Execution for Efficient Training
- **Arxiv ID**: http://arxiv.org/abs/1905.06435v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.06435v1)
- **Published**: 2019-05-15 21:10:28+00:00
- **Updated**: 2019-05-15 21:10:28+00:00
- **Authors**: Simeon E. Spasov, Pietro Lio
- **Comment**: None
- **Journal**: None
- **Summary**: Existing methods for reducing the computational burden of neural networks at run-time, such as parameter pruning or dynamic computational path selection, focus solely on improving computational efficiency during inference. On the other hand, in this work, we propose a novel method which reduces the memory footprint and number of computing operations required for training and inference. Our framework efficiently integrates pruning as part of the training procedure by exploring and tracking the relative importance of convolutional channels. At each training step, we select only a subset of highly salient channels to execute according to the combinatorial upper confidence bound algorithm, and run a forward and backward pass only on these activated channels, hence learning their parameters. Consequently, we enable the efficient discovery of compact models. We validate our approach empirically on state-of-the-art CNNs - VGGNet, ResNet and DenseNet, and on several image classification datasets. Results demonstrate our framework for dynamic channel execution reduces computational cost up to 4x and parameter count up to 9x, thus reducing the memory and computational demands for discovering and training compact neural network models.



### Fluorescence Image Histology Pattern Transformation using Image Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1905.06442v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.06442v1)
- **Published**: 2019-05-15 21:33:07+00:00
- **Updated**: 2019-05-15 21:33:07+00:00
- **Authors**: Mohammadhassan Izadyyazdanabadi, Evgenii Belykh, Xiaochun Zhao, Leandro Borba Moreira, Sirin Gandhi, Claudio Cavallo, Jennifer Eschbacher, Peter Nakaji, Mark C. Preul, Yezhou Yang
- **Comment**: Submitted to Frontiers in Oncology
- **Journal**: None
- **Summary**: Confocal laser endomicroscopy (CLE) allow on-the-fly in vivo intraoperative imaging in a discreet field of view, especially for brain tumors, rather than extracting tissue for examination ex vivo with conventional light microscopy. Fluorescein sodium-driven CLE imaging is more interactive, rapid, and portable than conventional hematoxylin and eosin (H&E)-staining. However, it has several limitations: CLE images may be contaminated with artifacts (motion, red blood cells, noise), and neuropathologists are mainly trained on colorful stained histology slides like H&E while the CLE images are gray. To improve the diagnostic quality of CLE, we used a micrograph of an H&E slide from a glioma tumor biopsy and image style transfer, a neural network method for integrating the content and style of two images. This was done through minimizing the deviation of the target image from both the content (CLE) and style (H&E) images. The style transferred images were assessed and compared to conventional H&E histology by neurosurgeons and a neuropathologist who then validated the quality enhancement in 100 pairs of original and transformed images. Average reviewers' score on test images showed 84 out of 100 transformed images had fewer artifacts and more noticeable critical structures compared to their original CLE form. By providing images that are more interpretable than the original CLE images and more rapidly acquired than H&E slides, the style transfer method allows a real-time, cellular-level tissue examination using CLE technology that closely resembles the conventional appearance of H&E staining and may yield better diagnostic recognition than original CLE grayscale images.



### An interdisciplinary survey of network similarity methods
- **Arxiv ID**: http://arxiv.org/abs/1905.06457v1
- **DOI**: None
- **Categories**: **cs.SI**, cs.CV, q-bio.MN
- **Links**: [PDF](http://arxiv.org/pdf/1905.06457v1)
- **Published**: 2019-05-15 22:16:48+00:00
- **Updated**: 2019-05-15 22:16:48+00:00
- **Authors**: Emily Evans, Marissa Graham
- **Comment**: None
- **Journal**: None
- **Summary**: Comparative graph and network analysis play an important role in both systems biology and pattern recognition, but existing surveys on the topic have historically ignored or underserved one or the other of these fields. We present an integrative introduction to the key objectives and methods of graph and network comparison in each field, with the intent of remaining accessible to relative novices in order to mitigate the barrier to interdisciplinary idea crossover.   To guide our investigation, and to quantitatively justify our assertions about what the key objectives and methods of each field are, we have constructed a citation network containing 5,793 vertices from the full reference lists of over two hundred relevant papers, which we collected by searching Google Scholar for ten different network comparison-related search terms. We investigate its basic statistics and community structure, and frame our presentation around the papers found to have high importance according to five different standard centrality measures.



### Relaxed 2-D Principal Component Analysis by $L_p$ Norm for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1905.06458v1
- **DOI**: 10.1007/978-3-030-26763-6_19
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.06458v1)
- **Published**: 2019-05-15 22:23:09+00:00
- **Updated**: 2019-05-15 22:23:09+00:00
- **Authors**: Xiao Chen, Zhi-Gang Jia, Yunfeng Cai, Mei-Xiang Zhao
- **Comment**: 19 pages, 11 figures
- **Journal**: In: Huang DS., Bevilacqua V., Premaratne P. (eds) Intelligent
  Computing Theories and Application. ICIC 2019. Lecture Notes in Computer
  Science, vol 11643. Springer, Cham
- **Summary**: A relaxed two dimensional principal component analysis (R2DPCA) approach is proposed for face recognition. Different to the 2DPCA, 2DPCA-$L_1$ and G2DPCA, the R2DPCA utilizes the label information (if known) of training samples to calculate a relaxation vector and presents a weight to each subset of training data. A new relaxed scatter matrix is defined and the computed projection axes are able to increase the accuracy of face recognition. The optimal $L_p$-norms are selected in a reasonable range. Numerical experiments on practical face databased indicate that the R2DPCA has high generalization ability and can achieve a higher recognition rate than state-of-the-art methods.



