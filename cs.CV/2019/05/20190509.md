# Arxiv Papers in cs.CV on 2019-05-09
### Anomaly Detection in Images
- **Arxiv ID**: http://arxiv.org/abs/1905.13147v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.13147v1)
- **Published**: 2019-05-09 00:44:19+00:00
- **Updated**: 2019-05-09 00:44:19+00:00
- **Authors**: Manpreet Singh Minhas, John Zelek
- **Comment**: None
- **Journal**: None
- **Summary**: Visual defect assessment is a form of anomaly detection. This is very relevant in finding faults such as cracks and markings in various surface inspection tasks like pavement and automotive parts. The task involves detection of deviation/divergence of anomalous samples from the normal ones. Two of the major challenges in supervised anomaly detection are the lack of labelled training data and the low availability of anomaly instances. Semi-supervised methods which learn the underlying distribution of the normal samples and then measure the deviation/divergence from the estimated model as the anomaly score have limitations in their overall ability to detect anomalies. This paper proposes the application of network-based deep transfer learning using convolutional neural networks (CNNs) for the task of anomaly detection. Single class SVMs have been used in the past with some success, however we hypothesize that deeper networks for single class classification should perform better. Results obtained on established anomaly detection benchmarks as well as on a real-world dataset, show that the proposed method clearly outperforms the existing state-of-the-art methods, by achieving a staggering average area under the receiver operating characteristic curve value of 0.99 for the tested data-sets which is an average improvement of 41% on the CIFAR10, 20% on MNIST and 16% on Cement Crack data-sets.



### A Dual-Path Model With Adaptive Attention For Vehicle Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1905.03397v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03397v3)
- **Published**: 2019-05-09 00:52:19+00:00
- **Updated**: 2019-09-24 16:16:35+00:00
- **Authors**: Pirazh Khorramshahi, Amit Kumar, Neehar Peri, Sai Saketh Rambhatla, Jun-Cheng Chen, Rama Chellappa
- **Comment**: This work has been accepted for oral presentation in ICCV 2019
- **Journal**: None
- **Summary**: In recent years, attention models have been extensively used for person and vehicle re-identification. Most re-identification methods are designed to focus attention on key-point locations. However, depending on the orientation, the contribution of each key-point varies. In this paper, we present a novel dual-path adaptive attention model for vehicle re-identification (AAVER). The global appearance path captures macroscopic vehicle features while the orientation conditioned part appearance path learns to capture localized discriminative features by focusing attention on the most informative key-points. Through extensive experimentation, we show that the proposed AAVER method is able to accurately re-identify vehicles in unconstrained scenarios, yielding state of the art results on the challenging dataset VeRi-776. As a byproduct, the proposed system is also able to accurately predict vehicle key-points and shows an improvement of more than 7% over state of the art. The code for key-point estimation model is available at https://github.com/Pirazh/Vehicle_Key_Point_Orientation_Estimation.



### PPGNet: Learning Point-Pair Graph for Line Segment Detection
- **Arxiv ID**: http://arxiv.org/abs/1905.03415v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03415v2)
- **Published**: 2019-05-09 02:22:06+00:00
- **Updated**: 2019-05-16 04:00:25+00:00
- **Authors**: Ziheng Zhang, Zhengxin Li, Ning Bi, Jia Zheng, Jinlei Wang, Kun Huang, Weixin Luo, Yanyu Xu, Shenghua Gao
- **Comment**: To appear in CVPR 2019
- **Journal**: None
- **Summary**: In this paper, we present a novel framework to detect line segments in man-made environments. Specifically, we propose to describe junctions, line segments and relationships between them with a simple graph, which is more structured and informative than end-point representation used in existing line segment detection methods. In order to extract a line segment graph from an image, we further introduce the PPGNet, a convolutional neural network that directly infers a graph from an image. We evaluate our method on published benchmarks including York Urban and Wireframe datasets. The results demonstrate that our method achieves satisfactory performance and generalizes well on all the benchmarks. The source code of our work is available at \url{https://github.com/svip-lab/PPGNet}.



### Deep Learning Acceleration Techniques for Real Time Mobile Vision Applications
- **Arxiv ID**: http://arxiv.org/abs/1905.03418v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1905.03418v2)
- **Published**: 2019-05-09 02:39:37+00:00
- **Updated**: 2019-06-07 15:31:48+00:00
- **Authors**: Gael Kamdem De Teyou
- **Comment**: This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources
- **Journal**: None
- **Summary**: Deep Learning (DL) has become a crucial technology for Artificial Intelligence (AI). It is a powerful technique to automatically extract high-level features from complex data which can be exploited for applications such as computer vision, natural language processing, cybersecurity, communications, and so on. For the particular case of computer vision, several algorithms like object detection in real time videos have been proposed and they work well on Desktop GPUs and distributed computing platforms. However these algorithms are still heavy for mobile and embedded visual applications. The rapid spreading of smart portable devices and the emerging 5G network are introducing new smart multimedia applications in mobile environments. As a consequence, the possibility of implementing deep neural networks to mobile environments has attracted a lot of researchers. This paper presents emerging deep learning acceleration techniques that can enable the delivery of real time visual recognition into the hands of end users, anytime and anywhere.



### Adversarial Image Translation: Unrestricted Adversarial Examples in Face Recognition Systems
- **Arxiv ID**: http://arxiv.org/abs/1905.03421v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.03421v3)
- **Published**: 2019-05-09 02:58:45+00:00
- **Updated**: 2020-01-28 06:36:40+00:00
- **Authors**: Kazuya Kakizaki, Kosuke Yoshida
- **Comment**: Kazuya Kakizaki and Kosuke Yoshida share equal contributions.
  Accepted at AAAI Workshop on Artificial Intelligence Safety (2020)
- **Journal**: None
- **Summary**: Thanks to recent advances in deep neural networks (DNNs), face recognition systems have become highly accurate in classifying a large number of face images. However, recent studies have found that DNNs could be vulnerable to adversarial examples, raising concerns about the robustness of such systems. Adversarial examples that are not restricted to small perturbations could be more serious since conventional certified defenses might be ineffective against them. To shed light on the vulnerability to such adversarial examples, we propose a flexible and efficient method for generating unrestricted adversarial examples using image translation techniques. Our method enables us to translate a source image into any desired facial appearance with large perturbations to deceive target face recognition systems. Our experimental results indicate that our method achieved about $90$ and $80\%$ attack success rates under white- and black-box settings, respectively, and that the translated images are perceptually realistic and maintain the identifiability of the individual while the perturbations are large enough to bypass certified defenses.



### Frustratingly Easy Person Re-Identification: Generalizing Person Re-ID in Practice
- **Arxiv ID**: http://arxiv.org/abs/1905.03422v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03422v3)
- **Published**: 2019-05-09 03:00:13+00:00
- **Updated**: 2019-07-22 04:16:02+00:00
- **Authors**: Jieru Jia, Qiuqi Ruan, Timothy M. Hospedales
- **Comment**: 14 pages,2 figures
- **Journal**: None
- **Summary**: Contemporary person re-identification (\reid) methods usually require access to data from the deployment camera network during training in order to perform well. This is because contemporary \reid{} models trained on one dataset do not generalise to other camera networks due to the domain-shift between datasets. This requirement is often the bottleneck for deploying \reid{} systems in practical security or commercial applications, as it may be impossible to collect this data in advance or prohibitively costly to annotate it. This paper alleviates this issue by proposing a simple baseline for domain generalizable~(DG) person re-identification. That is, to learn a \reid{} model from a set of source domains that is suitable for application to unseen datasets out-of-the-box, without any model updating. Specifically, we observe that the domain discrepancy in \reid{} is due to style and content variance across datasets and demonstrate appropriate Instance and Feature Normalization alleviates much of the resulting domain-shift in Deep \reid{} models. Instance Normalization~(IN) in early layers filters out style statistic variations and Feature Normalization~(FN) in deep layers is able to further eliminate disparity in content statistics. Compared to contemporary alternatives, this approach is extremely simple to implement, while being faster to train and test, thus making it an extremely valuable baseline for implementing \reid{} in practice. With a few lines of code, it increases the rank 1 \reid{} accuracy by {11.8\%, 33.2\%, 12.8\% and 8.5\%} on the VIPeR, PRID, GRID, and i-LIDS benchmarks respectively. Source codes are available at \url{https://github.com/BJTUJia/person_reID_DualNorm}.



### MAP Inference via L2-Sphere Linear Program Reformulation
- **Arxiv ID**: http://arxiv.org/abs/1905.03433v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1905.03433v3)
- **Published**: 2019-05-09 03:47:15+00:00
- **Updated**: 2020-01-20 03:09:50+00:00
- **Authors**: Baoyuan Wu, Li Shen, Tong Zhang, Bernard Ghanem
- **Comment**: Accepted to International Journal of Computer Vision
- **Journal**: None
- **Summary**: Maximum a posteriori (MAP) inference is an important task for graphical models. Due to complex dependencies among variables in realistic model, finding an exact solution for MAP inference is often intractable. Thus, many approximation methods have been developed, among which the linear programming (LP) relaxation based methods show promising performance. However, one major drawback of LP relaxation is that it is possible to give fractional solutions. Instead of presenting a tighter relaxation, in this work we propose a continuous but equivalent reformulation of the original MAP inference problem, called LS-LP. We add the L2-sphere constraint onto the original LP relaxation, leading to an intersected space with the local marginal polytope that is equivalent to the space of all valid integer label configurations. Thus, LS-LP is equivalent to the original MAP inference problem. We propose a perturbed alternating direction method of multipliers (ADMM) algorithm to optimize the LS-LP problem, by adding a sufficiently small perturbation epsilon onto the objective function and constraints. We prove that the perturbed ADMM algorithm globally converges to the epsilon-Karush-Kuhn-Tucker (epsilon-KKT) point of the LS-LP problem. The convergence rate will also be analyzed. Experiments on several benchmark datasets from Probabilistic Inference Challenge (PIC 2011) and OpenGM 2 show competitive performance of our proposed method against state-of-the-art MAP inference methods.



### ROSA: Robust Salient Object Detection against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1905.03434v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03434v1)
- **Published**: 2019-05-09 03:56:32+00:00
- **Updated**: 2019-05-09 03:56:32+00:00
- **Authors**: Haofeng Li, Guanbin Li, Yizhou Yu
- **Comment**: To be published in IEEE Transactions on Cybernetics
- **Journal**: None
- **Summary**: Recently salient object detection has witnessed remarkable improvement owing to the deep convolutional neural networks which can harvest powerful features for images. In particular, state-of-the-art salient object detection methods enjoy high accuracy and efficiency from fully convolutional network (FCN) based frameworks which are trained from end to end and predict pixel-wise labels. However, such framework suffers from adversarial attacks which confuse neural networks via adding quasi-imperceptible noises to input images without changing the ground truth annotated by human subjects. To our knowledge, this paper is the first one that mounts successful adversarial attacks on salient object detection models and verifies that adversarial samples are effective on a wide range of existing methods. Furthermore, this paper proposes a novel end-to-end trainable framework to enhance the robustness for arbitrary FCN-based salient object detection models against adversarial attacks. The proposed framework adopts a novel idea that first introduces some new generic noise to destroy adversarial perturbations, and then learns to predict saliency maps for input images with the introduced noise. Specifically, our proposed method consists of a segment-wise shielding component, which preserves boundaries and destroys delicate adversarial noise patterns and a context-aware restoration component, which refines saliency maps through global contrast modeling. Experimental results suggest that our proposed framework improves the performance significantly for state-of-the-art models on a series of datasets.



### Two-Stage Convolutional Neural Network Architecture for Lung Nodule Detection
- **Arxiv ID**: http://arxiv.org/abs/1905.03445v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.03445v1)
- **Published**: 2019-05-09 05:21:40+00:00
- **Updated**: 2019-05-09 05:21:40+00:00
- **Authors**: Haichao Cao, Hong Liu, Enmin Song, Guangzhi Ma, Xiangyang Xu, Renchao Jin, Tengying Liu, Chih-Cheng Hung
- **Comment**: 29 pages, 10 figures
- **Journal**: None
- **Summary**: Early detection of lung cancer is an effective way to improve the survival rate of patients. It is a critical step to have accurate detection of lung nodules in computed tomography (CT) images for the diagnosis of lung cancer. However, due to the heterogeneity of the lung nodules and the complexity of the surrounding environment, robust nodule detection has been a challenging task. In this study, we propose a two-stage convolutional neural network (TSCNN) architecture for lung nodule detection. The CNN architecture in the first stage is based on the improved UNet segmentation network to establish an initial detection of lung nodules. Simultaneously, in order to obtain a high recall rate without introducing excessive false positive nodules, we propose a novel sampling strategy, and use the offline hard mining idea for training and prediction according to the proposed cascaded prediction method. The CNN architecture in the second stage is based on the proposed dual pooling structure, which is built into three 3D CNN classification networks for false positive reduction. Since the network training requires a significant amount of training data, we adopt a data augmentation method based on random mask. Furthermore, we have improved the generalization ability of the false positive reduction model by means of ensemble learning. The proposed method has been experimentally verified on the LUNA dataset. Experimental results show that the proposed TSCNN architecture can obtain competitive detection performance.



### DistillHash: Unsupervised Deep Hashing by Distilling Data Pairs
- **Arxiv ID**: http://arxiv.org/abs/1905.03465v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03465v1)
- **Published**: 2019-05-09 07:10:12+00:00
- **Updated**: 2019-05-09 07:10:12+00:00
- **Authors**: Erkun Yang, Tongliang Liu, Cheng Deng, Wei Liu, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the high storage and search efficiency, hashing has become prevalent for large-scale similarity search. Particularly, deep hashing methods have greatly improved the search performance under supervised scenarios. In contrast, unsupervised deep hashing models can hardly achieve satisfactory performance due to the lack of reliable supervisory similarity signals. To address this issue, we propose a novel deep unsupervised hashing model, dubbed DistillHash, which can learn a distilled data set consisted of data pairs, which have confidence similarity signals. Specifically, we investigate the relationship between the initial noisy similarity signals learned from local structures and the semantic similarity labels assigned by a Bayes optimal classifier. We show that under a mild assumption, some data pairs, of which labels are consistent with those assigned by the Bayes optimal classifier, can be potentially distilled. Inspired by this fact, we design a simple yet effective strategy to distill data pairs automatically and further adopt a Bayesian learning framework to learn hash functions from the distilled data set. Extensive experimental results on three widely used benchmark datasets show that the proposed DistillHash consistently accomplishes the state-of-the-art search performance.



### Multi-Person Pose Estimation with Enhanced Channel-wise and Spatial Information
- **Arxiv ID**: http://arxiv.org/abs/1905.03466v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03466v1)
- **Published**: 2019-05-09 07:12:40+00:00
- **Updated**: 2019-05-09 07:12:40+00:00
- **Authors**: Kai Su, Dongdong Yu, Zhenqi Xu, Xin Geng, Changhu Wang
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: Multi-person pose estimation is an important but challenging problem in computer vision. Although current approaches have achieved significant progress by fusing the multi-scale feature maps, they pay little attention to enhancing the channel-wise and spatial information of the feature maps. In this paper, we propose two novel modules to perform the enhancement of the information for the multi-person pose estimation. First, a Channel Shuffle Module (CSM) is proposed to adopt the channel shuffle operation on the feature maps with different levels, promoting cross-channel information communication among the pyramid feature maps. Second, a Spatial, Channel-wise Attention Residual Bottleneck (SCARB) is designed to boost the original residual unit with attention mechanism, adaptively highlighting the information of the feature maps both in the spatial and channel-wise context. The effectiveness of our proposed modules is evaluated on the COCO keypoint benchmark, and experimental results show that our approach achieves the state-of-the-art results.



### Grand Challenge of 106-Point Facial Landmark Localization
- **Arxiv ID**: http://arxiv.org/abs/1905.03469v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03469v3)
- **Published**: 2019-05-09 07:18:57+00:00
- **Updated**: 2019-07-24 04:49:28+00:00
- **Authors**: Yinglu Liu, Hao Shen, Yue Si, Xiaobo Wang, Xiangyu Zhu, Hailin Shi, Zhibin Hong, Hanqi Guo, Ziyuan Guo, Yanqin Chen, Bi Li, Teng Xi, Jun Yu, Haonian Xie, Guochen Xie, Mengyan Li, Qing Lu, Zengfu Wang, Shenqi Lai, Zhenhua Chai, Xiaoming Wei
- **Comment**: This paper is accepted at ICME2019 Grand Challenge. The JD-landmark
  dataset has been released and can be downloaded from
  https://sites.google.com/view/hailin-shi
- **Journal**: None
- **Summary**: Facial landmark localization is a very crucial step in numerous face related applications, such as face recognition, facial pose estimation, face image synthesis, etc. However, previous competitions on facial landmark localization (i.e., the 300-W, 300-VW and Menpo challenges) aim to predict 68-point landmarks, which are incompetent to depict the structure of facial components. In order to overcome this problem, we construct a challenging dataset, named JD-landmark. Each image is manually annotated with 106-point landmarks. This dataset covers large variations on pose and expression, which brings a lot of difficulties to predict accurate landmarks. We hold a 106-point facial landmark localization competition1 on this dataset in conjunction with IEEE International Conference on Multimedia and Expo (ICME) 2019. The purpose of this competition is to discover effective and robust facial landmark localization approaches.



### Two-layer Near-lossless HDR Coding with Backward Compatibility to JPEG
- **Arxiv ID**: http://arxiv.org/abs/1905.04129v1
- **DOI**: 10.1587/transfun.E102.A.1842
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.04129v1)
- **Published**: 2019-05-09 09:12:41+00:00
- **Updated**: 2019-05-09 09:12:41+00:00
- **Authors**: Hiroyuki Kobayashi, Osamu Watanabe, Hitoshi Kiya
- **Comment**: To appear in IEEE International Conference on Image Processing 2019,
  Taipei, Taiwan
- **Journal**: None
- **Summary**: We propose an efficient two-layer near-lossless coding method using an extended histogram packing technique with backward compatibility to the legacy JPEG standard. The JPEG XT, which is the international standard to compress HDR images, adopts a two-layer coding method for backward compatibility to the legacy JPEG standard. However, there are two problems with this two-layer coding method. One is that it does not exhibit better near-lossless performance than other methods for HDR image compression with single-layer structure. The other problem is that the determining the appropriate values of the coding parameters may be required for each input image to achieve good compression performance of near-lossless compression with the two-layer coding method of the JPEG XT. To solve these problems, we focus on a histogram-packing technique that takes into account the histogram sparseness of HDR images. We used zero-skip quantization, which is an extension of the histogram-packing technique proposed for lossless coding, for implementing the proposed near-lossless coding method. The experimental results indicate that the proposed method exhibits not only a better near-lossless compression performance than that of the two-layer coding method of the JPEG XT, but also there are no issue regarding the combination of parameter values without losing backward compatibility to the JPEG standard.



### Sketch2code: Generating a website from a paper mockup
- **Arxiv ID**: http://arxiv.org/abs/1905.13750v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.13750v1)
- **Published**: 2019-05-09 10:15:13+00:00
- **Updated**: 2019-05-09 10:15:13+00:00
- **Authors**: Alex Robinson
- **Comment**: None
- **Journal**: None
- **Summary**: An early stage of developing user-facing applications is creating a wireframe to layout the interface. Once a wireframe has been created it is given to a developer to implement in code. Developing boiler plate user interface code is time consuming work but still requires an experienced developer. In this dissertation we present two approaches which automates this process, one using classical computer vision techniques, and another using a novel application of deep semantic segmentation networks. We release a dataset of websites which can be used to train and evaluate these approaches. Further, we have designed a novel evaluation framework which allows empirical evaluation by creating synthetic sketches. Our evaluation illustrates that our deep learning approach outperforms our classical computer vision approach and we conclude that deep learning is the most promising direction for future research.



### Embedding Human Knowledge into Deep Neural Network via Attention Map
- **Arxiv ID**: http://arxiv.org/abs/1905.03540v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03540v4)
- **Published**: 2019-05-09 11:32:44+00:00
- **Updated**: 2019-12-19 06:16:51+00:00
- **Authors**: Masahiro Mitsuhara, Hiroshi Fukui, Yusuke Sakashita, Takanori Ogata, Tsubasa Hirakawa, Takayoshi Yamashita, Hironobu Fujiyoshi
- **Comment**: 10 pages, 10 figures
- **Journal**: None
- **Summary**: In this work, we aim to realize a method for embedding human knowledge into deep neural networks. While the conventional method to embed human knowledge has been applied for non-deep machine learning, it is challenging to apply it for deep learning models due to the enormous number of model parameters. To tackle this problem, we focus on the attention mechanism of an attention branch network (ABN). In this paper, we propose a fine-tuning method that utilizes a single-channel attention map which is manually edited by a human expert. Our fine-tuning method can train a network so that the output attention map corresponds to the edited ones. As a result, the fine-tuned network can output an attention map that takes into account human knowledge. Experimental results with ImageNet, CUB-200-2010, and IDRiD demonstrate that it is possible to obtain a clear attention map for a visual explanation and improve the classification performance. Our findings can be a novel framework for optimizing networks through human intuitive editing via a visual interface and suggest new possibilities for human-machine cooperation in addition to the improvement of visual explanations.



### A Novel Adaptive Kernel for the RBF Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.03546v1
- **DOI**: 10.1007/s00034-016-0375-7
- **Categories**: **stat.ML**, cs.CV, cs.LG, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1905.03546v1)
- **Published**: 2019-05-09 11:38:57+00:00
- **Updated**: 2019-05-09 11:38:57+00:00
- **Authors**: Shujaat Khan, Imran Naseem, Roberto Togneri, Mohammed Bennamoun
- **Comment**: None
- **Journal**: Circuits, Systems, and Signal Processing, vol. 36, no. 4, pp.
  1639-1653, 2017
- **Summary**: In this paper, we propose a novel adaptive kernel for the radial basis function (RBF) neural networks. The proposed kernel adaptively fuses the Euclidean and cosine distance measures to exploit the reciprocating properties of the two. The proposed framework dynamically adapts the weights of the participating kernels using the gradient descent method thereby alleviating the need for predetermined weights. The proposed method is shown to outperform the manual fusion of the kernels on three major problems of estimation namely nonlinear system identification, pattern classification and function approximation.



### Cycle-IR: Deep Cyclic Image Retargeting
- **Arxiv ID**: http://arxiv.org/abs/1905.03556v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03556v1)
- **Published**: 2019-05-09 12:05:12+00:00
- **Updated**: 2019-05-09 12:05:12+00:00
- **Authors**: Weimin Tan, Bo Yan, Chumin Lin, Xuejing Niu
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Supervised deep learning techniques have achieved great success in various fields due to getting rid of the limitation of handcrafted representations. However, most previous image retargeting algorithms still employ fixed design principles such as using gradient map or handcrafted features to compute saliency map, which inevitably restricts its generality. Deep learning techniques may help to address this issue, but the challenging problem is that we need to build a large-scale image retargeting dataset for the training of deep retargeting models. However, building such a dataset requires huge human efforts.   In this paper, we propose a novel deep cyclic image retargeting approach, called Cycle-IR, to firstly implement image retargeting with a single deep model, without relying on any explicit user annotations. Our idea is built on the reverse mapping from the retargeted images to the given input images. If the retargeted image has serious distortion or excessive loss of important visual information, the reverse mapping is unlikely to restore the input image well. We constrain this forward-reverse consistency by introducing a cyclic perception coherence loss. In addition, we propose a simple yet effective image retargeting network (IRNet) to implement the image retargeting process. Our IRNet contains a spatial and channel attention layer, which is able to discriminate visually important regions of input images effectively, especially in cluttered images. Given arbitrary sizes of input images and desired aspect ratios, our Cycle-IR can produce visually pleasing target images directly. Extensive experiments on the standard RetargetMe dataset show the superiority of our Cycle-IR. In addition, our Cycle-IR outperforms the Multiop method and obtains the best result in the user study. Code is available at https://github.com/mintanwei/Cycle-IR.



### D2-Net: A Trainable CNN for Joint Detection and Description of Local Features
- **Arxiv ID**: http://arxiv.org/abs/1905.03561v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03561v1)
- **Published**: 2019-05-09 12:12:14+00:00
- **Updated**: 2019-05-09 12:12:14+00:00
- **Authors**: Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Pollefeys, Josef Sivic, Akihiko Torii, Torsten Sattler
- **Comment**: Accepted at CVPR 2019
- **Journal**: None
- **Summary**: In this work we address the problem of finding reliable pixel-level correspondences under difficult imaging conditions. We propose an approach where a single convolutional neural network plays a dual role: It is simultaneously a dense feature descriptor and a feature detector. By postponing the detection to a later stage, the obtained keypoints are more stable than their traditional counterparts based on early detection of low-level structures. We show that this model can be trained using pixel correspondences extracted from readily available large-scale SfM reconstructions, without any further annotations. The proposed method obtains state-of-the-art performance on both the difficult Aachen Day-Night localization dataset and the InLoc indoor localization benchmark, as well as competitive performance on other benchmarks for image matching and 3D reconstruction.



### Spatial-Spectral Feature Extraction via Deep ConvLSTM Neural Networks for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1905.03577v2
- **DOI**: 10.1109/TGRS.2019.2961947
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03577v2)
- **Published**: 2019-05-09 12:43:11+00:00
- **Updated**: 2022-04-11 03:41:57+00:00
- **Authors**: Wen-Shuai Hu, Heng-Chao Li, Lei Pan, Wei Li, Ran Tao, Qian Du
- **Comment**: 14 pages, 8 figures
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, vol. 58, no.
  6, pp. 4237-4250, June 2020
- **Summary**: In recent years, deep learning has presented a great advance in hyperspectral image (HSI) classification. Particularly, long short-term memory (LSTM), as a special deep learning structure, has shown great ability in modeling long-term dependencies in the time dimension of video or the spectral dimension of HSIs. However, the loss of spatial information makes it quite difficult to obtain the better performance. In order to address this problem, two novel deep models are proposed to extract more discriminative spatial-spectral features by exploiting the Convolutional LSTM (ConvLSTM). By taking the data patch in a local sliding window as the input of each memory cell band by band, the 2-D extended architecture of LSTM is considered for building the spatial-spectral ConvLSTM 2-D Neural Network (SSCL2DNN) to model long-range dependencies in the spectral domain. To better preserve the intrinsic structure information of the hyperspectral data, the spatial-spectral ConvLSTM 3-D Neural Network (SSCL3DNN) is proposed by extending LSTM to 3-D version for further improving the classification performance. The experiments, conducted on three commonly used HSI data sets, demonstrate that the proposed deep models have certain competitive advantages and can provide better classification performance than other state-of-the-art approaches.



### Learning Representations for Predicting Future Activities
- **Arxiv ID**: http://arxiv.org/abs/1905.03578v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IT, cs.RO, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1905.03578v1)
- **Published**: 2019-05-09 12:45:01+00:00
- **Updated**: 2019-05-09 12:45:01+00:00
- **Authors**: Mohammadreza Zolfaghari, Özgün Çiçek, Syed Mohsin Ali, Farzaneh Mahdisoltani, Can Zhang, Thomas Brox
- **Comment**: 14 pages, ICCV 2019 submission, Code and Models:
  https://github.com/lmb-freiburg/PreFAct
- **Journal**: None
- **Summary**: Foreseeing the future is one of the key factors of intelligence. It involves understanding of the past and current environment as well as decent experience of its possible dynamics. In this work, we address future prediction at the abstract level of activities. We propose a network module for learning embeddings of the environment's dynamics in a self-supervised way. To take the ambiguities and high variances in the future activities into account, we use a multi-hypotheses scheme that can represent multiple futures. We demonstrate the approach by classifying future activities on the Epic-Kitchens and Breakfast datasets. Moreover, we generate captions that describe the future activities



### Fast and Efficient Zero-Learning Image Fusion
- **Arxiv ID**: http://arxiv.org/abs/1905.03590v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03590v1)
- **Published**: 2019-05-09 13:11:26+00:00
- **Updated**: 2019-05-09 13:11:26+00:00
- **Authors**: Fayez Lahoud, Sabine Süsstrunk
- **Comment**: 13 pages, 10 figures
- **Journal**: None
- **Summary**: We propose a real-time image fusion method using pre-trained neural networks. Our method generates a single image containing features from multiple sources. We first decompose images into a base layer representing large scale intensity variations, and a detail layer containing small scale changes. We use visual saliency to fuse the base layers, and deep feature maps extracted from a pre-trained neural network to fuse the detail layers. We conduct ablation studies to analyze our method's parameters such as decomposition filters, weight construction methods, and network depth and architecture. Then, we validate its effectiveness and speed on thermal, medical, and multi-focus fusion. We also apply it to multiple image inputs such as multi-exposure sequences. The experimental results demonstrate that our technique achieves state-of-the-art performance in visual quality, objective assessment, and runtime efficiency.



### Intra-frame Object Tracking by Deblatting
- **Arxiv ID**: http://arxiv.org/abs/1905.03633v2
- **DOI**: 10.1109/ICCVW.2019.00283
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03633v2)
- **Published**: 2019-05-09 13:48:01+00:00
- **Updated**: 2019-06-20 11:17:14+00:00
- **Authors**: Jan Kotera, Denys Rozumnyi, Filip Šroubek, Jiří Matas
- **Comment**: None
- **Journal**: 2019 IEEE/CVF International Conference on Computer Vision Workshop
  (ICCVW)
- **Summary**: Objects moving at high speed along complex trajectories often appear in videos, especially videos of sports. Such objects elapse non-negligible distance during exposure time of a single frame and therefore their position in the frame is not well defined. They appear as semi-transparent streaks due to the motion blur and cannot be reliably tracked by standard trackers. We propose a novel approach called Tracking by Deblatting based on the observation that motion blur is directly related to the intra-frame trajectory of an object. Blur is estimated by solving two intertwined inverse problems, blind deblurring and image matting, which we call deblatting. The trajectory is then estimated by fitting a piecewise quadratic curve, which models physically justifiable trajectories. As a result, tracked objects are precisely localized with higher temporal resolution than by conventional trackers. The proposed TbD tracker was evaluated on a newly created dataset of videos with ground truth obtained by a high-speed camera using a novel Trajectory-IoU metric that generalizes the traditional Intersection over Union and measures the accuracy of the intra-frame trajectory. The proposed method outperforms baseline both in recall and trajectory accuracy.



### Liver Lesion Segmentation with slice-wise 2D Tiramisu and Tversky loss function
- **Arxiv ID**: http://arxiv.org/abs/1905.03639v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.03639v1)
- **Published**: 2019-05-09 13:58:21+00:00
- **Updated**: 2019-05-09 13:58:21+00:00
- **Authors**: Karsten Roth, Tomasz Konopczyński, Jürgen Hesser
- **Comment**: None
- **Journal**: None
- **Summary**: At present, lesion segmentation is still performed manually (or semi-automatically) by medical experts. To facilitate this process, we contribute a fully-automatic lesion segmentation pipeline. This work proposes a method as a part of the LiTS (Liver Tumor Segmentation Challenge) competition for ISBI 17 and MICCAI 17 comparing methods for automatics egmentation of liver lesions in CT scans. By utilizing cascaded, densely connected 2D U-Nets and a Tversky-coefficient based loss function, our framework achieves very good shape extractions with high detection sensitivity, with competitive scores at time of publication. In addition, adjusting hyperparameters in our Tversky-loss allows to tune the network towards higher sensitivity or robustness.



### Improving Discrete Latent Representations With Differentiable Approximation Bridges
- **Arxiv ID**: http://arxiv.org/abs/1905.03658v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.03658v3)
- **Published**: 2019-05-09 14:31:59+00:00
- **Updated**: 2019-10-26 01:41:50+00:00
- **Authors**: Jason Ramapuram, Russ Webb
- **Comment**: None
- **Journal**: None
- **Summary**: Modern neural network training relies on piece-wise (sub-)differentiable functions in order to use backpropagation to update model parameters. In this work, we introduce a novel method to allow simple non-differentiable functions at intermediary layers of deep neural networks. We do so by training with a differentiable approximation bridge (DAB) neural network which approximates the non-differentiable forward function and provides gradient updates during backpropagation. We present strong empirical results (performing over 600 experiments) in four different domains: unsupervised (image) representation learning, variational (image) density estimation, image classification, and sequence sorting to demonstrate that our proposed method improves state of the art performance. We demonstrate that training with DAB aided discrete non-differentiable functions improves image reconstruction quality and posterior linear separability by 10% against the Gumbel-Softmax relaxed estimator [37, 26] as well as providing a 9% improvement in the test variational lower bound in comparison to the state of the art RELAX [16] discrete estimator. We also observe an accuracy improvement of 77% in neural sequence sorting and a 25% improvement against the straight-through estimator [5] in an image classification setting. The DAB network is not used for inference and expands the class of functions that are usable in neural networks.



### S4L: Self-Supervised Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.03670v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.03670v2)
- **Published**: 2019-05-09 14:55:56+00:00
- **Updated**: 2019-07-23 13:31:21+00:00
- **Authors**: Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, Lucas Beyer
- **Comment**: All four authors contributed equally
- **Journal**: None
- **Summary**: This work tackles the problem of semi-supervised learning of image classifiers. Our main insight is that the field of semi-supervised learning can benefit from the quickly advancing field of self-supervised visual representation learning. Unifying these two approaches, we propose the framework of self-supervised semi-supervised learning and use it to derive two novel semi-supervised image classification methods. We demonstrate the effectiveness of these methods in comparison to both carefully tuned baselines, and existing semi-supervised learning methods. We then show that our approach and existing semi-supervised methods can be jointly trained, yielding a new state-of-the-art result on semi-supervised ILSVRC-2012 with 10% of labels.



### Seesaw-Net: Convolution Neural Network With Uneven Group Convolution
- **Arxiv ID**: http://arxiv.org/abs/1905.03672v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03672v5)
- **Published**: 2019-05-09 14:56:59+00:00
- **Updated**: 2019-12-01 12:48:35+00:00
- **Authors**: Jintao Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we are interested in boosting the representation capability of convolution neural networks which utilizing the inverted residual structure. Based on the success of Inverted Residual structure[Sandler et al. 2018] and Interleaved Low-Rank Group Convolutions[Sun et al. 2018], we rethink this two pattern of neural network structure, rather than NAS(Neural architecture search) method[Zoph and Le 2017; Pham et al. 2018; Liu et al. 2018b], we introduce uneven point-wise group convolution, which provide a novel search space for designing basic blocks to obtain better trade-off between representation capability and computational cost. Meanwhile, we propose two novel information flow patterns that will enable cross-group information flow for multiple group convolution layers with and without any channel permute/shuffle operation. Dense experiments on image classification task show that our proposed model, named Seesaw-Net, achieves state-of-the-art(SOTA) performance with limited computation and memory cost. Our code will be open-source and available together with pre-trained models.



### Learning Loss for Active Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.03677v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.03677v1)
- **Published**: 2019-05-09 15:03:48+00:00
- **Updated**: 2019-05-09 15:03:48+00:00
- **Authors**: Donggeun Yoo, In So Kweon
- **Comment**: Accepted to CVPR 2019 (Oral)
- **Journal**: None
- **Summary**: The performance of deep neural networks improves with more annotated data. The problem is that the budget for annotation is limited. One solution to this is active learning, where a model asks human to annotate data that it perceived as uncertain. A variety of recent methods have been proposed to apply active learning to deep networks but most of them are either designed specific for their target tasks or computationally inefficient for large networks. In this paper, we propose a novel active learning method that is simple but task-agnostic, and works efficiently with the deep networks. We attach a small parametric module, named "loss prediction module," to a target network, and learn it to predict target losses of unlabeled inputs. Then, this module can suggest data that the target model is likely to produce a wrong prediction. This method is task-agnostic as networks are learned from a single loss regardless of target tasks. We rigorously validate our method through image classification, object detection, and human pose estimation, with the recent network architectures. The results demonstrate that our method consistently outperforms the previous methods over the tasks.



### What Do Single-view 3D Reconstruction Networks Learn?
- **Arxiv ID**: http://arxiv.org/abs/1905.03678v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03678v1)
- **Published**: 2019-05-09 15:07:57+00:00
- **Updated**: 2019-05-09 15:07:57+00:00
- **Authors**: Maxim Tatarchenko, Stephan R. Richter, René Ranftl, Zhuwen Li, Vladlen Koltun, Thomas Brox
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional networks for single-view object reconstruction have shown impressive performance and have become a popular subject of research. All existing techniques are united by the idea of having an encoder-decoder network that performs non-trivial reasoning about the 3D structure of the output space. In this work, we set up two alternative approaches that perform image classification and retrieval respectively. These simple baselines yield better results than state-of-the-art methods, both qualitatively and quantitatively. We show that encoder-decoder methods are statistically indistinguishable from these baselines, thus indicating that the current state of the art in single-view object reconstruction does not actually perform reconstruction but image classification. We identify aspects of popular experimental procedures that elicit this behavior and discuss ways to improve the current state of research.



### Forecasting Pedestrian Trajectory with Machine-Annotated Training Data
- **Arxiv ID**: http://arxiv.org/abs/1905.03681v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1905.03681v1)
- **Published**: 2019-05-09 15:13:36+00:00
- **Updated**: 2019-05-09 15:13:36+00:00
- **Authors**: Olly Styles, Arun Ross, Victor Sanchez
- **Comment**: 6 pages, 5 figures. To appear in the proceedings of the 2019 IEEE
  Intelligent Vehicles Symposium (IV)
- **Journal**: None
- **Summary**: Reliable anticipation of pedestrian trajectory is imperative for the operation of autonomous vehicles and can significantly enhance the functionality of advanced driver assistance systems. While significant progress has been made in the field of pedestrian detection, forecasting pedestrian trajectories remains a challenging problem due to the unpredictable nature of pedestrians and the huge space of potentially useful features. In this work, we present a deep learning approach for pedestrian trajectory forecasting using a single vehicle-mounted camera. Deep learning models that have revolutionized other areas in computer vision have seen limited application to trajectory forecasting, in part due to the lack of richly annotated training data. We address the lack of training data by introducing a scalable machine annotation scheme that enables our model to be trained using a large dataset without human annotation. In addition, we propose Dynamic Trajectory Predictor (DTP), a model for forecasting pedestrian trajectory up to one second into the future. DTP is trained using both human and machine-annotated data, and anticipates dynamic motion that is not captured by linear models. Experimental evaluation confirms the benefits of the proposed model.



### Gaze-in-wild: A dataset for studying eye and head coordination in everyday activities
- **Arxiv ID**: http://arxiv.org/abs/1905.13146v1
- **DOI**: 10.1038/s41598-020-59251-5
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.13146v1)
- **Published**: 2019-05-09 16:33:03+00:00
- **Updated**: 2019-05-09 16:33:03+00:00
- **Authors**: Rakshit Kothari, Zhizhuo Yang, Christopher Kanan, Reynold Bailey, Jeff Pelz, Gabriel Diaz
- **Comment**: 23 pages, 11 figures, 10 tables, Dataset can be found at
  http://www.cis.rit.edu/~rsk3900/
- **Journal**: None
- **Summary**: The interaction between the vestibular and ocular system has primarily been studied in controlled environments. Consequently, off-the shelf tools for categorization of gaze events (e.g. fixations, pursuits, saccade) fail when head movements are allowed. Our approach was to collect a novel, naturalistic, and multimodal dataset of eye+head movements when subjects performed everyday tasks while wearing a mobile eye tracker equipped with an inertial measurement unit and a 3D stereo camera. This Gaze-in-the-Wild dataset (GW) includes eye+head rotational velocities (deg/s), infrared eye images and scene imagery (RGB+D). A portion was labelled by coders into gaze motion events with a mutual agreement of 0.72 sample based Cohen's $\kappa$. This labelled data was used to train and evaluate two machine learning algorithms, Random Forest and a Recurrent Neural Network model, for gaze event classification. Assessment involved the application of established and novel event based performance metrics. Classifiers achieve $\sim$90$\%$ human performance in detecting fixations and saccades but fall short (60$\%$) on detecting pursuit movements. Moreover, pursuit classification is far worse in the absence of head movement information. A subsequent analysis of feature significance in our best-performing model revealed a reliance upon absolute eye and head velocity, indicating that classification does not require spatial alignment of the head and eye tracking coordinate systems. The GW dataset, trained classifiers and evaluation metrics will be made publicly available with the intention of facilitating growth in the emerging area of head-free gaze event classification.



### Interactive Image Generation Using Scene Graphs
- **Arxiv ID**: http://arxiv.org/abs/1905.03743v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.03743v1)
- **Published**: 2019-05-09 16:39:31+00:00
- **Updated**: 2019-05-09 16:39:31+00:00
- **Authors**: Gaurav Mittal, Shubham Agrawal, Anuva Agarwal, Sushant Mehta, Tanya Marwah
- **Comment**: Published at ICLR 2019 Deep Generative Models for Highly Structured
  Data Workshop
- **Journal**: None
- **Summary**: Recent years have witnessed some exciting developments in the domain of generating images from scene-based text descriptions. These approaches have primarily focused on generating images from a static text description and are limited to generating images in a single pass. They are unable to generate an image interactively based on an incrementally additive text description (something that is more intuitive and similar to the way we describe an image). We propose a method to generate an image incrementally based on a sequence of graphs of scene descriptions (scene-graphs). We propose a recurrent network architecture that preserves the image content generated in previous steps and modifies the cumulative image as per the newly provided scene information. Our model utilizes Graph Convolutional Networks (GCN) to cater to variable-sized scene graphs along with Generative Adversarial image translation networks to generate realistic multi-object images without needing any intermediate supervision during training. We experiment with Coco-Stuff dataset which has multi-object images along with annotations describing the visual scene and show that our model significantly outperforms other approaches on the same dataset in generating visually consistent images for incrementally growing scene graphs.



### Learning Interpretable Features via Adversarially Robust Optimization
- **Arxiv ID**: http://arxiv.org/abs/1905.03767v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03767v2)
- **Published**: 2019-05-09 17:50:25+00:00
- **Updated**: 2019-08-19 09:18:32+00:00
- **Authors**: Ashkan Khakzar, Shadi Albarqouni, Nassir Navab
- **Comment**: MICCAI 2019 (Medical Image Computing and Computer Assisted
  Interventions)
- **Journal**: None
- **Summary**: Neural networks are proven to be remarkably successful for classification and diagnosis in medical applications. However, the ambiguity in the decision-making process and the interpretability of the learned features is a matter of concern. In this work, we propose a method for improving the feature interpretability of neural network classifiers. Initially, we propose a baseline convolutional neural network with state of the art performance in terms of accuracy and weakly supervised localization. Subsequently, the loss is modified to integrate robustness to adversarial examples into the training process. In this work, feature interpretability is quantified via evaluating the weakly supervised localization using the ground truth bounding boxes. Interpretability is also visually assessed using class activation maps and saliency maps. The method is applied to NIH ChestX-ray14, the largest publicly available chest x-rays dataset. We demonstrate that the adversarially robust optimization paradigm improves feature interpretability both quantitatively and visually.



### The Effect of Network Width on Stochastic Gradient Descent and Generalization: an Empirical Study
- **Arxiv ID**: http://arxiv.org/abs/1905.03776v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.03776v1)
- **Published**: 2019-05-09 17:58:13+00:00
- **Updated**: 2019-05-09 17:58:13+00:00
- **Authors**: Daniel S. Park, Jascha Sohl-Dickstein, Quoc V. Le, Samuel L. Smith
- **Comment**: 17 pages, 3 tables, 17 figures; accepted to ICML 2019
- **Journal**: None
- **Summary**: We investigate how the final parameters found by stochastic gradient descent are influenced by over-parameterization. We generate families of models by increasing the number of channels in a base network, and then perform a large hyper-parameter search to study how the test error depends on learning rate, batch size, and network width. We find that the optimal SGD hyper-parameters are determined by a "normalized noise scale," which is a function of the batch size, learning rate, and initialization conditions. In the absence of batch normalization, the optimal normalized noise scale is directly proportional to width. Wider networks, with their higher optimal noise scale, also achieve higher test accuracy. These observations hold for MLPs, ConvNets, and ResNets, and for two different parameterization schemes ("Standard" and "NTK"). We observe a similar trend with batch normalization for ResNets. Surprisingly, since the largest stable learning rate is bounded, the largest batch size consistent with the optimal normalized noise scale decreases as the width increases.



### Differential Recurrent Neural Network and its Application for Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1905.04293v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04293v1)
- **Published**: 2019-05-09 18:31:54+00:00
- **Updated**: 2019-05-09 18:31:54+00:00
- **Authors**: Naifan Zhuang, Guo-Jun Qi, The Duc Kieu, Kien A. Hua
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1504.06678,
  arXiv:1804.04192
- **Journal**: None
- **Summary**: The Long Short-Term Memory (LSTM) recurrent neural network is capable of processing complex sequential information since it utilizes special gating schemes for learning representations from long input sequences. It has the potential to model any sequential time-series data, where the current hidden state has to be considered in the context of the past hidden states. This property makes LSTM an ideal choice to learn the complex dynamics present in long sequences. Unfortunately, the conventional LSTMs do not consider the impact of spatio-temporal dynamics corresponding to the given salient motion patterns, when they gate the information that ought to be memorized through time. To address this problem, we propose a differential gating scheme for the LSTM neural network, which emphasizes on the change in information gain caused by the salient motions between the successive video frames. This change in information gain is quantified by Derivative of States (DoS), and thus the proposed LSTM model is termed as differential Recurrent Neural Network (dRNN). In addition, the original work used the hidden state at the last time-step to model the entire video sequence. Based on the energy profiling of DoS, we further propose to employ the State Energy Profile (SEP) to search for salient dRNN states and construct more informative representations. The effectiveness of the proposed model was demonstrated by automatically recognizing human actions from the real-world 2D and 3D single-person action datasets. We point out that LSTM is a special form of dRNN. As a result, we have introduced a new family of LSTMs. Our study is one of the first works towards demonstrating the potential of learning complex time-series representations via high-order derivatives of states.



### Hierarchical Cross-Modal Talking Face Generationwith Dynamic Pixel-Wise Loss
- **Arxiv ID**: http://arxiv.org/abs/1905.03820v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03820v1)
- **Published**: 2019-05-09 19:14:26+00:00
- **Updated**: 2019-05-09 19:14:26+00:00
- **Authors**: Lele Chen, Ross K. Maddox, Zhiyao Duan, Chenliang Xu
- **Comment**: None
- **Journal**: Published in CVPR 2019
- **Summary**: We devise a cascade GAN approach to generate talking face video, which is robust to different face shapes, view angles, facial characteristics, and noisy audio conditions. Instead of learning a direct mapping from audio to video frames, we propose first to transfer audio to high-level structure, i.e., the facial landmarks, and then to generate video frames conditioned on the landmarks. Compared to a direct audio-to-image approach, our cascade approach avoids fitting spurious correlations between audiovisual signals that are irrelevant to the speech content. We, humans, are sensitive to temporal discontinuities and subtle artifacts in video. To avoid those pixel jittering problems and to enforce the network to focus on audiovisual-correlated regions, we propose a novel dynamically adjustable pixel-wise loss with an attention mechanism. Furthermore, to generate a sharper image with well-synchronized facial movements, we propose a novel regression-based discriminator structure, which considers sequence-level information along with frame-level information. Thoughtful experiments on several datasets and real-world samples demonstrate significantly better results obtained by our method than the state-of-the-art methods in both quantitative and qualitative comparisons.



### The Art of Food: Meal Image Synthesis from Ingredients
- **Arxiv ID**: http://arxiv.org/abs/1905.13149v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.13149v1)
- **Published**: 2019-05-09 20:57:51+00:00
- **Updated**: 2019-05-09 20:57:51+00:00
- **Authors**: Fangda Han, Ricardo Guerrero, Vladimir Pavlovic
- **Comment**: 12 pages, 6 figures, 2 tables, under review as a conference paper at
  BMVC 2019
- **Journal**: None
- **Summary**: In this work we propose a new computational framework, based on generative deep models, for synthesis of photo-realistic food meal images from textual descriptions of its ingredients. Previous works on synthesis of images from text typically rely on pre-trained text models to extract text features, followed by a generative neural networks (GANs) aimed to generate realistic images conditioned on the text features. These works mainly focus on generating spatially compact and well-defined categories of objects, such as birds or flowers. In contrast, meal images are significantly more complex, consisting of multiple ingredients whose appearance and spatial qualities are further modified by cooking methods. We propose a method that first builds an attention-based ingredients-image association model, which is then used to condition a generative neural network tasked with synthesizing meal images. Furthermore, a cycle-consistent constraint is added to further improve image quality and control appearance. Extensive experiments show our model is able to generate meal image corresponding to the ingredients, which could be used to augment existing dataset for solving other computational food analysis problems.



### Exposure Interpolation by Combining Model-driven and Data-driven Methods
- **Arxiv ID**: http://arxiv.org/abs/1905.03890v6
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.03890v6)
- **Published**: 2019-05-09 23:36:55+00:00
- **Updated**: 2020-11-27 12:48:29+00:00
- **Authors**: Chaobing Zheng, Zhengguo Li, Shiqian Wu
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Deep learning based methods have penetrated many image processing problems and become dominant solutions to these problems. A natural question raised here is "Is there any space for conventional methods on these problems?" In this paper, exposure interpolation is taken as an example to answer this question and the answer is "Yes". A framework on fusing conventional and deep learning method is introduced to generate an medium exposure image for two large-exposureratio images. Experimental results indicate that the quality of the medium exposure image is increased significantly through using the deep learning method to refine the interpolated image via the conventional method. The conventional method can be adopted to improve the convergence speed of the deep learning method and to reduce the number of samples which is required by the deep learning method.



### Joint Segmentation and Path Classification of Curvilinear Structures
- **Arxiv ID**: http://arxiv.org/abs/1905.03892v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03892v1)
- **Published**: 2019-05-09 23:50:22+00:00
- **Updated**: 2019-05-09 23:50:22+00:00
- **Authors**: Agata Mosinska, Mateusz Kozinski, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: Detection of curvilinear structures in images has long been of interest. One of the most challenging aspects of this problem is inferring the graph representation of the curvilinear network. Most existing delineation approaches first perform binary segmentation of the image and then refine it using either a set of hand-designed heuristics or a separate classifier that assigns likelihood to paths extracted from the pixel-wise prediction. In our work, we bridge the gap between segmentation and path classification by training a deep network that performs those two tasks simultaneously. We show that this approach is beneficial because it enforces consistency across the whole processing pipeline. We apply our approach on roads and neurons datasets.



