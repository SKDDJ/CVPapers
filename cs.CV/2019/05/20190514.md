# Arxiv Papers in cs.CV on 2019-05-14
### Convolutional neural networks with fractional order gradient method
- **Arxiv ID**: http://arxiv.org/abs/1905.05336v2
- **DOI**: 10.1016/j.neucom.2019.10.017
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.05336v2)
- **Published**: 2019-05-14 01:20:33+00:00
- **Updated**: 2019-09-16 09:11:53+00:00
- **Authors**: Dian Sheng, Yiheng Wei, Yuquan Chen, Yong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a fractional order gradient method for the backward propagation of convolutional neural networks. To overcome the problem that fractional order gradient method cannot converge to real extreme point, a simplified fractional order gradient method is designed based on Caputo's definition. The parameters within layers are updated by the designed gradient method, but the propagations between layers still use integer order gradients, and thus the complicated derivatives of composite functions are avoided and the chain rule will be kept. By connecting every layers in series and adding loss functions, the proposed convolutional neural networks can be trained smoothly according to various tasks. Some practical experiments are carried out in order to demonstrate fast convergence, high accuracy and ability to escape local optimal point at last.



### Disparity-Augmented Trajectories for Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1905.05344v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05344v1)
- **Published**: 2019-05-14 02:06:40+00:00
- **Updated**: 2019-05-14 02:06:40+00:00
- **Authors**: Pejman Habashi, Boubakeur Boufama, Imran Shafiq Ahmad
- **Comment**: None
- **Journal**: None
- **Summary**: Numerous methods for human activity recognition have been proposed in the past two decades. Many of these methods are based on sparse representation, which describes the whole video content by a set of local features. Trajectories, being mid-level sparse features, are capable of describing the motion of an interest-point in 2D space. 2D trajectories might be affected by viewpoint changes, potentially decreasing their accuracy. In this paper, we initially propose and compare different 2D trajectory-based algorithms for human activity recognition. Moreover, we propose a new way of fusing disparity information with 2D trajectory information, without the calculation of 3D reconstruction. The obtained results show a 2.76\% improvement when using disparity-augmented trajectories, compared to using the classical 2D trajectory information only. Furthermore, we have also tested our method on the challenging Hollywood 3D dataset, and we have obtained competitive results, at a faster speed.



### Understanding Pedestrian-Vehicle Interactions with Vehicle Mounted Vision: An LSTM Model and Empirical Analysis
- **Arxiv ID**: http://arxiv.org/abs/1905.05350v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05350v1)
- **Published**: 2019-05-14 02:20:05+00:00
- **Updated**: 2019-05-14 02:20:05+00:00
- **Authors**: Daniela A. Ridel, Nachiket Deo, Denis Wolf, Mohan M. Trivedi
- **Comment**: IV 2019
- **Journal**: None
- **Summary**: Pedestrians and vehicles often share the road in complex inner city traffic. This leads to interactions between the vehicle and pedestrians, with each affecting the other's motion. In order to create robust methods to reason about pedestrian behavior and to design interfaces of communication between self-driving cars and pedestrians we need to better understand such interactions. In this paper, we present a data-driven approach to implicitly model pedestrians' interactions with vehicles, to better predict pedestrian behavior. We propose a LSTM model that takes as input the past trajectories of the pedestrian and ego-vehicle, and pedestrian head orientation, and predicts the future positions of the pedestrian. Our experiments based on a real-world, inner city dataset captured with vehicle mounted cameras, show that the usage of such cues improve pedestrian prediction when compared to a baseline that purely uses the past trajectory of the pedestrian.



### Listwise View Ranking for Image Cropping
- **Arxiv ID**: http://arxiv.org/abs/1905.05352v1
- **DOI**: 10.1109/ACCESS.2019.2925430
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05352v1)
- **Published**: 2019-05-14 02:21:59+00:00
- **Updated**: 2019-05-14 02:21:59+00:00
- **Authors**: Weirui Lu, Xiaofen Xing, Bolun Cai, Xiangmin Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Rank-based Learning with deep neural network has been widely used for image cropping. However, the performance of ranking-based methods is often poor and this is mainly due to two reasons: 1) image cropping is a listwise ranking task rather than pairwise comparison; 2) the rescaling caused by pooling layer and the deformation in view generation damage the performance of composition learning. In this paper, we develop a novel model to overcome these problems. To address the first problem, we formulate the image cropping as a listwise ranking problem to find the best view composition. For the second problem, a refined view sampling (called RoIRefine) is proposed to extract refined feature maps for candidate view generation. Given a series of candidate views, the proposed model learns the Top-1 probability distribution of views and picks up the best one. By integrating refined sampling and listwise ranking, the proposed network called LVRN achieves the state-of-the-art performance both in accuracy and speed.



### A Context-and-Spatial Aware Network for Multi-Person Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1905.05355v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05355v1)
- **Published**: 2019-05-14 02:25:40+00:00
- **Updated**: 2019-05-14 02:25:40+00:00
- **Authors**: Dongdong Yu, Kai Su, Xin Geng, Changhu Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-person pose estimation is a fundamental yet challenging task in computer vision. Both rich context information and spatial information are required to precisely locate the keypoints for all persons in an image. In this paper, a novel Context-and-Spatial Aware Network (CSANet), which integrates both a Context Aware Path and Spatial Aware Path, is proposed to obtain effective features involving both context information and spatial information. Specifically, we design a Context Aware Path with structure supervision strategy and spatial pyramid pooling strategy to enhance the context information. Meanwhile, a Spatial Aware Path is proposed to preserve the spatial information, which also shortens the information propagation path from low-level features to high-level features. On top of these two paths, we employ a Heavy Head Path to further combine and enhance the features effectively. Experimentally, our proposed network outperforms state-of-the-art methods on the COCO keypoint benchmark, which verifies the effectiveness of our method and further corroborates the above proposition.



### Streetscape augmentation using generative adversarial networks: insights related to health and wellbeing
- **Arxiv ID**: http://arxiv.org/abs/1905.06464v1
- **DOI**: 10.1016/j.scs.2019.101602
- **Categories**: **cs.CY**, cs.CV, cs.LG, stat.ML, I.2.10; J.5
- **Links**: [PDF](http://arxiv.org/pdf/1905.06464v1)
- **Published**: 2019-05-14 03:13:15+00:00
- **Updated**: 2019-05-14 03:13:15+00:00
- **Authors**: Jasper S. Wijnands, Kerry A. Nice, Jason Thompson, Haifeng Zhao, Mark Stevenson
- **Comment**: 20 pages, 8 figures. Preprint accepted for publication in Sustainable
  Cities and Society
- **Journal**: None
- **Summary**: Deep learning using neural networks has provided advances in image style transfer, merging the content of one image (e.g., a photo) with the style of another (e.g., a painting). Our research shows this concept can be extended to analyse the design of streetscapes in relation to health and wellbeing outcomes. An Australian population health survey (n=34,000) was used to identify the spatial distribution of health and wellbeing outcomes, including general health and social capital. For each outcome, the most and least desirable locations formed two domains. Streetscape design was sampled using around 80,000 Google Street View images per domain. Generative adversarial networks translated these images from one domain to the other, preserving the main structure of the input image, but transforming the `style' from locations where self-reported health was bad to locations where it was good. These translations indicate that areas in Melbourne with good general health are characterised by sufficient green space and compactness of the urban environment, whilst streetscape imagery related to high social capital contained more and wider footpaths, fewer fences and more grass. Beyond identifying relationships, the method is a first step towards computer-generated design interventions that have the potential to improve population health and wellbeing.



### Image quality assessment for determining efficacy and limitations of Super-Resolution Convolutional Neural Network (SRCNN)
- **Arxiv ID**: http://arxiv.org/abs/1905.05373v1
- **DOI**: 10.1117/12.2275157
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.05373v1)
- **Published**: 2019-05-14 03:19:40+00:00
- **Updated**: 2019-05-14 03:19:40+00:00
- **Authors**: Chris M. Ward, Josh Harguess, Brendan Crabb, Shibin Parameswaran
- **Comment**: None
- **Journal**: Proceedings Volume 10396, Applications of Digital Image Processing
  XL; 1039605 (2017)
- **Summary**: Traditional metrics for evaluating the efficacy of image processing techniques do not lend themselves to understanding the capabilities and limitations of modern image processing methods - particularly those enabled by deep learning. When applying image processing in engineering solutions, a scientist or engineer has a need to justify their design decisions with clear metrics. By applying blind/referenceless image spatial quality (BRISQUE), Structural SIMilarity (SSIM) index scores, and Peak signal-to-noise ratio (PSNR) to images before and after image processing, we can quantify quality improvements in a meaningful way and determine the lowest recoverable image quality for a given method.



### Self-supervised Audio Spatialization with Correspondence Classifier
- **Arxiv ID**: http://arxiv.org/abs/1905.05375v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.05375v1)
- **Published**: 2019-05-14 03:20:49+00:00
- **Updated**: 2019-05-14 03:20:49+00:00
- **Authors**: Yu-Ding Lu, Hsin-Ying Lee, Hung-Yu Tseng, Ming-Hsuan Yang
- **Comment**: ICIP 2019
- **Journal**: None
- **Summary**: Spatial audio is an essential medium to audiences for 3D visual and auditory experience. However, the recording devices and techniques are expensive or inaccessible to the general public. In this work, we propose a self-supervised audio spatialization network that can generate spatial audio given the corresponding video and monaural audio. To enhance spatialization performance, we use an auxiliary classifier to classify ground-truth videos and those with audio where the left and right channels are swapped. We collect a large-scale video dataset with spatial audio to validate the proposed method. Experimental results demonstrate the effectiveness of the proposed model on the audio spatialization task.



### A human-inspired recognition system for premodern Japanese historical documents
- **Arxiv ID**: http://arxiv.org/abs/1905.05377v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DL
- **Links**: [PDF](http://arxiv.org/pdf/1905.05377v1)
- **Published**: 2019-05-14 03:26:25+00:00
- **Updated**: 2019-05-14 03:26:25+00:00
- **Authors**: Anh Duc Le, Tarin Clanuwat, Asanobu Kitamoto
- **Comment**: None
- **Journal**: None
- **Summary**: Recognition of historical documents is a challenging problem due to the noised, damaged characters and background. However, in Japanese historical documents, not only contains the mentioned problems, pre-modern Japanese characters were written in cursive and are connected. Therefore, character segmentation based methods do not work well. This leads to the idea of creating a new recognition system. In this paper, we propose a human-inspired document reading system to recognize multiple lines of premodern Japanese historical documents. During the reading, people employ eyes movement to determine the start of a text line. Then, they move the eyes from the current character/word to the next character/word. They can also determine the end of a line or skip a figure to move to the next line. The eyes movement integrates with visual processing to operate the reading process in the brain. We employ attention-based encoder-decoder to implement this recognition system. First, the recognition system detects where to start a text line. Second, the system scans and recognize character by character until the text line is completed. Then, the system continues to detect the start of the next text line. This process is repeated until reading the whole document. We tested our human-inspired recognition system on the pre-modern Japanese historical document provide by the PRMU Kuzushiji competition. The results of the experiments demonstrate the superiority and effectiveness of our proposed system by achieving Sequence Error Rate of 9.87% and 53.81% on level 2 and level 3 of the dataset, respectively. These results outperform to any other systems participated in the PRMU Kuzushiji competition.



### 3D Dense Separated Convolution Module for Volumetric Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1905.08608v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08608v1)
- **Published**: 2019-05-14 03:26:27+00:00
- **Updated**: 2019-05-14 03:26:27+00:00
- **Authors**: Lei Qu, Changfeng Wu, Liang Zou
- **Comment**: 7 pages,5 figures
- **Journal**: None
- **Summary**: With the thriving of deep learning, 3D Convolutional Neural Networks have become a popular choice in volumetric image analysis due to their impressive 3D contexts mining ability. However, the 3D convolutional kernels will introduce a significant increase in the amount of trainable parameters. Considering the training data is often limited in biomedical tasks, a tradeoff has to be made between model size and its representational power. To address this concern, in this paper, we propose a novel 3D Dense Separated Convolution (3D-DSC) module to replace the original 3D convolutional kernels. The 3D-DSC module is constructed by a series of densely connected 1D filters. The decomposition of 3D kernel into 1D filters reduces the risk of over-fitting by removing the redundancy of 3D kernels in a topologically constrained manner, while providing the infrastructure for deepening the network. By further introducing nonlinear layers and dense connections between 1D filters, the network's representational power can be significantly improved while maintaining a compact architecture. We demonstrate the superiority of 3D-DSC on volumetric image classification and segmentation, which are two challenging tasks often encountered in biomedical image computing.



### End to End Recognition System for Recognizing Offline Unconstrained Vietnamese Handwriting
- **Arxiv ID**: http://arxiv.org/abs/1905.05381v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.05381v1)
- **Published**: 2019-05-14 03:59:46+00:00
- **Updated**: 2019-05-14 03:59:46+00:00
- **Authors**: Anh Duc Le, Hung Tuan Nguyen, Masaki Nakagawa
- **Comment**: None
- **Journal**: None
- **Summary**: Inspired by recent successes in neural machine translation and image caption generation, we present an attention based encoder decoder model (AED) to recognize Vietnamese Handwritten Text. The model composes of two parts: a DenseNet for extracting invariant features, and a Long Short-Term Memory network (LSTM) with an attention model incorporated for generating output text (LSTM decoder), which are connected from the CNN part to the attention model. The input of the CNN part is a handwritten text image and the target of the LSTM decoder is the corresponding text of the input image. Our model is trained end-to-end to predict the text from a given input image since all the parts are differential components. In the experiment section, we evaluate our proposed AED model on the VNOnDB-Word and VNOnDB-Line datasets to verify its efficiency. The experiential results show that our model achieves 12.30% of word error rate without using any language model. This result is competitive with the handwriting recognition system provided by Google in the Vietnamese Online Handwritten Text Recognition competition.



### Domain Adaptive Person Re-Identification via Camera Style Generation and Label Propagation
- **Arxiv ID**: http://arxiv.org/abs/1905.05382v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05382v1)
- **Published**: 2019-05-14 04:07:57+00:00
- **Updated**: 2019-05-14 04:07:57+00:00
- **Authors**: Chuan-Xian Ren, Bo-Hua Liang, Zhen Lei
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain adaptation in person re-identification resorts to labeled source data to promote the model training on target domain, facing the dilemmas caused by large domain shift and large camera variations. The non-overlapping labels challenge that source domain and target domain have entirely different persons further increases the re-identification difficulty. In this paper, we propose a novel algorithm to narrow such domain gaps. We derive a camera style adaptation framework to learn the style-based mappings between different camera views, from the target domain to the source domain, and then we can transfer the identity-based distribution from the source domain to the target domain on the camera level. To overcome the non-overlapping labels challenge and guide the person re-identification model to narrow the gap further, an efficient and effective soft-labeling method is proposed to mine the intrinsic local structure of the target domain through building the connection between GAN-translated source domain and the target domain. Experiment results conducted on real benchmark datasets indicate that our method gets state-of-the-art results.



### Population Based Augmentation: Efficient Learning of Augmentation Policy Schedules
- **Arxiv ID**: http://arxiv.org/abs/1905.05393v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.05393v1)
- **Published**: 2019-05-14 05:01:43+00:00
- **Updated**: 2019-05-14 05:01:43+00:00
- **Authors**: Daniel Ho, Eric Liang, Ion Stoica, Pieter Abbeel, Xi Chen
- **Comment**: ICML 2019
- **Journal**: None
- **Summary**: A key challenge in leveraging data augmentation for neural network training is choosing an effective augmentation policy from a large search space of candidate operations. Properly chosen augmentation policies can lead to significant generalization improvements; however, state-of-the-art approaches such as AutoAugment are computationally infeasible to run for the ordinary user. In this paper, we introduce a new data augmentation algorithm, Population Based Augmentation (PBA), which generates nonstationary augmentation policy schedules instead of a fixed augmentation policy. We show that PBA can match the performance of AutoAugment on CIFAR-10, CIFAR-100, and SVHN, with three orders of magnitude less overall compute. On CIFAR-10 we achieve a mean test error of 1.46%, which is a slight improvement upon the current state-of-the-art. The code for PBA is open source and is available at https://github.com/arcelien/pba.



### Diversify and Match: A Domain Adaptive Representation Learning Paradigm for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1905.05396v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05396v1)
- **Published**: 2019-05-14 05:23:23+00:00
- **Updated**: 2019-05-14 05:23:23+00:00
- **Authors**: Taekyung Kim, Minki Jeong, Seunghyeon Kim, Seokeon Choi, Changick Kim
- **Comment**: Accepted to CVPR 2019. Source code will be uploaded soon
- **Journal**: None
- **Summary**: We introduce a novel unsupervised domain adaptation approach for object detection. We aim to alleviate the imperfect translation problem of pixel-level adaptations, and the source-biased discriminativity problem of feature-level adaptations simultaneously. Our approach is composed of two stages, i.e., Domain Diversification (DD) and Multi-domain-invariant Representation Learning (MRL). At the DD stage, we diversify the distribution of the labeled data by generating various distinctive shifted domains from the source domain. At the MRL stage, we apply adversarial learning with a multi-domain discriminator to encourage feature to be indistinguishable among the domains. DD addresses the source-biased discriminativity, while MRL mitigates the imperfect image translation. We construct a structured domain adaptation framework for our learning paradigm and introduce a practical way of DD for implementation. Our method outperforms the state-of-the-art methods by a large margin of 3%~11% in terms of mean average precision (mAP) on various datasets.



### An Effective Two-Branch Model-Based Deep Network for Single Image Deraining
- **Arxiv ID**: http://arxiv.org/abs/1905.05404v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05404v2)
- **Published**: 2019-05-14 06:04:39+00:00
- **Updated**: 2019-09-17 12:27:06+00:00
- **Authors**: Yinglong Wang, Dong Gong, Jie Yang, Qinfeng Shi, Anton van den Hengel, Dehua Xie, Bing Zeng
- **Comment**: 10 pages, 9 figures, 3 tables
- **Journal**: None
- **Summary**: Removing rain effects from an image is of importance for various applications such as autonomous driving, drone piloting, and photo editing. Conventional methods rely on some heuristics to handcraft various priors to remove or separate the rain effects from an image. Recent deep learning models are proposed to learn end-to-end methods to complete this task. However, they often fail to obtain satisfactory results in many realistic scenarios, especially when the observed images suffer from heavy rain. Heavy rain brings not only rain streaks but also haze-like effect caused by the accumulation of tiny raindrops. Different from the existing deep learning deraining methods that mainly focus on handling the rain streaks, we design a deep neural network by incorporating a physical raining image model. Specifically, in the proposed model, two branches are designed to handle both the rain streaks and haze-like effects. An additional submodule is jointly trained to finally refine the results, which give the model flexibility to control the strength of removing the mist. Extensive experiments on several datasets show that our method outperforms the state-of-the-art in both objective assessments and visual quality.



### Plug-and-Play Methods Provably Converge with Properly Trained Denoisers
- **Arxiv ID**: http://arxiv.org/abs/1905.05406v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.05406v1)
- **Published**: 2019-05-14 06:12:42+00:00
- **Updated**: 2019-05-14 06:12:42+00:00
- **Authors**: Ernest K. Ryu, Jialin Liu, Sicheng Wang, Xiaohan Chen, Zhangyang Wang, Wotao Yin
- **Comment**: Published in the International Conference on Machine Learning, 2019
- **Journal**: None
- **Summary**: Plug-and-play (PnP) is a non-convex framework that integrates modern denoising priors, such as BM3D or deep learning-based denoisers, into ADMM or other proximal algorithms. An advantage of PnP is that one can use pre-trained denoisers when there is not sufficient data for end-to-end training. Although PnP has been recently studied extensively with great empirical success, theoretical analysis addressing even the most basic question of convergence has been insufficient. In this paper, we theoretically establish convergence of PnP-FBS and PnP-ADMM, without using diminishing stepsizes, under a certain Lipschitz condition on the denoisers. We then propose real spectral normalization, a technique for training deep learning-based denoisers to satisfy the proposed Lipschitz condition. Finally, we present experimental results validating the theory.



### Expression Conditional GAN for Facial Expression-to-Expression Translation
- **Arxiv ID**: http://arxiv.org/abs/1905.05416v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1905.05416v1)
- **Published**: 2019-05-14 06:52:03+00:00
- **Updated**: 2019-05-14 06:52:03+00:00
- **Authors**: Hao Tang, Wei Wang, Songsong Wu, Xinya Chen, Dan Xu, Nicu Sebe, Yan Yan
- **Comment**: 5 pages, 5 figures, accepted to ICIP 2019
- **Journal**: None
- **Summary**: In this paper, we focus on the facial expression translation task and propose a novel Expression Conditional GAN (ECGAN) which can learn the mapping from one image domain to another one based on an additional expression attribute. The proposed ECGAN is a generic framework and is applicable to different expression generation tasks where specific facial expression can be easily controlled by the conditional attribute label. Besides, we introduce a novel face mask loss to reduce the influence of background changing. Moreover, we propose an entire framework for facial expression generation and recognition in the wild, which consists of two modules, i.e., generation and recognition. Finally, we evaluate our framework on several public face datasets in which the subjects have different races, illumination, occlusion, pose, color, content and background conditions. Even though these datasets are very diverse, both the qualitative and quantitative results demonstrate that our approach is able to generate facial expressions accurately and robustly.



### Towards a Skeleton-Based Action Recognition For Realistic Scenarios
- **Arxiv ID**: http://arxiv.org/abs/1905.05420v1
- **DOI**: 10.13140/RG.2.2.23016.52485
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.05420v1)
- **Published**: 2019-05-14 07:05:18+00:00
- **Updated**: 2019-05-14 07:05:18+00:00
- **Authors**: Cagatay Odabasi, Jewel Jose
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding human actions is a crucial problem for service robots. However, the general trend in Action Recognition is developing and testing these systems on structured datasets. That's why this work presents a practical Skeleton-based Action Recognition framework which can be used in realistic scenarios. Our results show that although non-augmented and non-normalized data may yield comparable results on the test split of the dataset, it is far from being useful on another dataset which is a manually collected data.



### Panoramic Annular Localizer: Tackling the Variation Challenges of Outdoor Localization Using Panoramic Annular Images and Active Deep Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1905.05425v2
- **DOI**: 10.1109/ITSC.2019.8917508
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05425v2)
- **Published**: 2019-05-14 07:26:48+00:00
- **Updated**: 2019-07-15 16:24:41+00:00
- **Authors**: Ruiqi Cheng, Kaiwei Wang, Shufei Lin, Weijian Hu, Kailun Yang, Xiao Huang, Huabing Li, Dongming Sun, Jian Bai
- **Comment**: Accepted by ITSC 2019
- **Journal**: None
- **Summary**: Visual localization is an attractive problem that estimates the camera localization from database images based on the query image. It is a crucial task for various applications, such as autonomous vehicles, assistive navigation and augmented reality. The challenging issues of the task lie in various appearance variations between query and database images, including illumination variations, dynamic object variations and viewpoint variations. In order to tackle those challenges, Panoramic Annular Localizer into which panoramic annular lens and robust deep image descriptors are incorporated is proposed in this paper. The panoramic annular images captured by the single camera are processed and fed into the NetVLAD network to form the active deep descriptor, and sequential matching is utilized to generate the localization result. The experiments carried on the public datasets and in the field illustrate the validation of the proposed system.



### Revisiting Precision and Recall Definition for Generative Model Evaluation
- **Arxiv ID**: http://arxiv.org/abs/1905.05441v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.05441v1)
- **Published**: 2019-05-14 08:15:35+00:00
- **Updated**: 2019-05-14 08:15:35+00:00
- **Authors**: Loïc Simon, Ryan Webster, Julien Rabin
- **Comment**: ICML 2019
- **Journal**: PMLR 97:5799-5808, 2019
- **Summary**: In this article we revisit the definition of Precision-Recall (PR) curves for generative models proposed by Sajjadi et al. (arXiv:1806.00035). Rather than providing a scalar for generative quality, PR curves distinguish mode-collapse (poor recall) and bad quality (poor precision). We first generalize their formulation to arbitrary measures, hence removing any restriction to finite support. We also expose a bridge between PR curves and type I and type II error rates of likelihood ratio classifiers on the task of discriminating between samples of the two distributions. Building upon this new perspective, we propose a novel algorithm to approximate precision-recall curves, that shares some interesting methodological properties with the hypothesis testing technique from Lopez-Paz et al (arXiv:1610.06545). We demonstrate the interest of the proposed formulation over the original approach on controlled multi-modal datasets.



### LSANet: Feature Learning on Point Sets by Local Spatial Aware Layer
- **Arxiv ID**: http://arxiv.org/abs/1905.05442v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05442v3)
- **Published**: 2019-05-14 08:17:50+00:00
- **Updated**: 2019-06-20 12:07:08+00:00
- **Authors**: Lin-Zhuo Chen, Xuan-Yi Li, Deng-Ping Fan, Kai Wang, Shao-Ping Lu, Ming-Ming Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Directly learning features from the point cloud has become an active research direction in 3D understanding. Existing learning-based methods usually construct local regions from the point cloud and extract the corresponding features. However, most of these processes do not adequately take the spatial distribution of the point cloud into account, limiting the ability to perceive fine-grained patterns. We design a novel Local Spatial Aware (LSA) layer, which can learn to generate Spatial Distribution Weights (SDWs) hierarchically based on the spatial relationship in local region for spatial independent operations, to establish the relationship between these operations and spatial distribution, thus capturing the local geometric structure sensitively.We further propose the LSANet, which is based on LSA layer, aggregating the spatial information with associated features in each layer of the network better in network design.The experiments show that our LSANet can achieve on par or better performance than the state-of-the-art methods when evaluating on the challenging benchmark datasets. For example, our LSANet can achieve 93.2% accuracy on ModelNet40 dataset using only 1024 points, significantly higher than other methods under the same conditions. The source code is available at https://github.com/LinZhuoChen/LSANet.



### Transition Subspace Learning based Least Squares Regression for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1905.05445v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05445v2)
- **Published**: 2019-05-14 08:23:07+00:00
- **Updated**: 2019-06-14 07:10:54+00:00
- **Authors**: Zhe Chen, Xiao-Jun Wu, Josef Kittler
- **Comment**: None
- **Journal**: None
- **Summary**: Only learning one projection matrix from original samples to the corresponding binary labels is too strict and will consequentlly lose some intrinsic geometric structures of data. In this paper, we propose a novel transition subspace learning based least squares regression (TSL-LSR) model for multicategory image classification. The main idea of TSL-LSR is to learn a transition subspace between the original samples and binary labels to alleviate the problem of overfitting caused by strict projection learning. Moreover, in order to reflect the underlying low-rank structure of transition matrix and learn more discriminative projection matrix, a low-rank constraint is added to the transition subspace. Experimental results on several image datasets demonstrate the effectiveness of the proposed TSL-LSR model in comparison with state-of-the-art algorithms



### An Improved Self-supervised GAN via Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1905.05469v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.05469v1)
- **Published**: 2019-05-14 09:05:35+00:00
- **Updated**: 2019-05-14 09:05:35+00:00
- **Authors**: Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, Ngai-Man Cheung
- **Comment**: None
- **Journal**: None
- **Summary**: We propose to improve unconditional Generative Adversarial Networks (GAN) by training the self-supervised learning with the adversarial process. In particular, we apply self-supervised learning via the geometric transformation on input images and assign the pseudo-labels to these transformed images. (i) In addition to the GAN task, which distinguishes data (real) versus generated (fake) samples, we train the discriminator to predict the correct pseudo-labels of real transformed samples (classification task). Importantly, we find out that simultaneously training the discriminator to classify the fake class from the pseudo-classes of real samples for the classification task will improve the discriminator and subsequently lead better guides to train generator. (ii) The generator is trained by attempting to confuse the discriminator for not only the GAN task but also the classification task. For the classification task, the generator tries to confuse the discriminator recognizing the transformation of its output as one of the real transformed classes. Especially, we exploit that when the generator creates samples that result in a similar loss (via cross-entropy) as that of the real ones, the training is more stable and the generator distribution tends to match better the data distribution. When integrating our techniques into a state-of-the-art Auto-Encoder (AE) based-GAN model, they help to significantly boost the model's performance and also establish new state-of-the-art Fr\'echet Inception Distance (FID) scores in the literature of unconditional GAN for CIFAR-10 and STL-10 datasets.



### American Sign Language Alphabet Recognition using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.05487v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.05487v1)
- **Published**: 2019-05-14 09:51:58+00:00
- **Updated**: 2019-05-14 09:51:58+00:00
- **Authors**: Nikhil Kasukurthi, Brij Rokad, Shiv Bidani, Dr. Aju Dennisan
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: Tremendous headway has been made in the field of 3D hand pose estimation but the 3D depth cameras are usually inaccessible. We propose a model to recognize American Sign Language alphabet from RGB images. Images for the training were resized and pre-processed before training the Deep Neural Network. The model was trained on a squeezenet architecture to make it capable of running on mobile devices with an accuracy of 83.29%.



### Skin Cancer Recognition using Deep Residual Network
- **Arxiv ID**: http://arxiv.org/abs/1905.08610v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.08610v1)
- **Published**: 2019-05-14 10:04:38+00:00
- **Updated**: 2019-05-14 10:04:38+00:00
- **Authors**: Brij Rokad, Dr. Sureshkumar Nagarajan
- **Comment**: 6 pages, 7 figures
- **Journal**: None
- **Summary**: The advances in technology have enabled people to access internet from every part of the world. But to date, access to healthcare in remote areas is sparse. This proposed solution aims to bridge the gap between specialist doctors and patients. This prototype will be able to detect skin cancer from an image captured by the phone or any other camera. The network is deployed on cloud server-side processing for an even more accurate result. The Deep Residual learning model has been used for predicting the probability of cancer for server side The ResNet has three parametric layers. Each layer has Convolutional Neural Network, Batch Normalization, Maxpool and ReLU. Currently the model achieves an accuracy of 77% on the ISIC - 2017 challenge.



### Monocular 3D Object Detection via Geometric Reasoning on Keypoints
- **Arxiv ID**: http://arxiv.org/abs/1905.05618v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05618v1)
- **Published**: 2019-05-14 14:00:19+00:00
- **Updated**: 2019-05-14 14:00:19+00:00
- **Authors**: Ivan Barabanau, Alexey Artemov, Evgeny Burnaev, Vyacheslav Murashkin
- **Comment**: None
- **Journal**: None
- **Summary**: Monocular 3D object detection is well-known to be a challenging vision task due to the loss of depth information; attempts to recover depth using separate image-only approaches lead to unstable and noisy depth estimates, harming 3D detections. In this paper, we propose a novel keypoint-based approach for 3D object detection and localization from a single RGB image. We build our multi-branch model around 2D keypoint detection in images and complement it with a conceptually simple geometric reasoning method. Our network performs in an end-to-end manner, simultaneously and interdependently estimating 2D characteristics, such as 2D bounding boxes, keypoints, and orientation, along with full 3D pose in the scene. We fuse the outputs of distinct branches, applying a reprojection consistency loss during training. The experimental evaluation on the challenging KITTI dataset benchmark demonstrates that our network achieves state-of-the-art results among other monocular 3D detectors.



### Disentangled Human Body Embedding Based on Deep Hierarchical Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1905.05622v2
- **DOI**: 10.1109/TVCG.2020.2988476
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.05622v2)
- **Published**: 2019-05-14 14:06:54+00:00
- **Updated**: 2020-04-17 10:09:32+00:00
- **Authors**: Boyi Jiang, Juyong Zhang, Jianfei Cai, Jianmin Zheng
- **Comment**: This manuscript is accepted for publication in the IEEE Transactions
  on Visualization and Computer Graphics Journal (IEEE TVCG). The Code is
  available at https://github.com/Juyong/DHNN_BodyRepresentation
- **Journal**: None
- **Summary**: Human bodies exhibit various shapes for different identities or poses, but the body shape has certain similarities in structure and thus can be embedded in a low-dimensional space. This paper presents an autoencoder-like network architecture to learn disentangled shape and pose embedding specifically for the 3D human body. This is inspired by recent progress of deformation-based latent representation learning. To improve the reconstruction accuracy, we propose a hierarchical reconstruction pipeline for the disentangling process and construct a large dataset of human body models with consistent connectivity for the learning of the neural network. Our learned embedding can not only achieve superior reconstruction accuracy but also provide great flexibility in 3D human body generation via interpolation, bilinear interpolation, and latent space sampling. The results from extensive experiments demonstrate the powerfulness of our learned 3D human body embedding in various applications.



### "Tom" pet robot applied to urban autism
- **Arxiv ID**: http://arxiv.org/abs/1905.05652v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.05652v1)
- **Published**: 2019-05-14 14:53:38+00:00
- **Updated**: 2019-05-14 14:53:38+00:00
- **Authors**: Xingqian Li, Chenwei Lou, Jian Zhao, HuaPeng Wei, Hongwei Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: With the fast development of network information technology, more and more people are immersed in the virtual community environment brought by the network, ignoring the social interaction in real life. The consequent urban autism problem has become more and more serious. Promoting offline communication between people " and "eliminating loneliness through emotional communication between pet robots and breeders" to solve this problem, and has developed a design called "Tom". "Tom" is a smart pet robot with a pet robot-based social mechanism Called "Tom-Talker". The main contribution of this paper is to propose a social mechanism called "Tom-Talker" that encourages users to socialize offline. And "Tom-Talker" also has a corresponding reward mechanism and a friend recommendation algorithm. It also proposes a pet robot named "Tom" with an emotional interaction algorithm to recognize users' emotions, simulate animal emotions and communicate emotionally with use s. This paper designs experiments and analyzes the results. The results show that our pet robots have a good effect on solving urban autism problems.



### Efficient Ladder-style DenseNets for Semantic Segmentation of Large Images
- **Arxiv ID**: http://arxiv.org/abs/1905.05661v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05661v1)
- **Published**: 2019-05-14 15:14:51+00:00
- **Updated**: 2019-05-14 15:14:51+00:00
- **Authors**: Ivan Krešo, Josip Krapac, Siniša Šegvić
- **Comment**: 12 pages, 6 figures, under review
- **Journal**: None
- **Summary**: Recent progress of deep image classification models has provided great potential to improve state-of-the-art performance in related computer vision tasks. However, the transition to semantic segmentation is hampered by strict memory limitations of contemporary GPUs. The extent of feature map caching required by convolutional backprop poses significant challenges even for moderately sized Pascal images, while requiring careful architectural considerations when the source resolution is in the megapixel range. To address these concerns, we propose a novel DenseNet-based ladder-style architecture which features high modelling power and a very lean upsampling datapath. We also propose to substantially reduce the extent of feature map caching by exploiting inherent spatial efficiency of the DenseNet feature extractor. The resulting models deliver high performance with fewer parameters than competitive approaches, and allow training at megapixel resolution on commodity hardware. The presented experimental results outperform the state-of-the-art in terms of prediction accuracy and execution speed on Cityscapes, Pascal VOC 2012, CamVid and ROB 2018 datasets. Source code will be released upon publication.



### Neurons Activation Visualization and Information Theoretic Analysis
- **Arxiv ID**: http://arxiv.org/abs/1905.08618v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08618v3)
- **Published**: 2019-05-14 15:33:15+00:00
- **Updated**: 2019-11-05 16:01:46+00:00
- **Authors**: Longwei Wang, Peijie Chen
- **Comment**: the paper is not so well written and need to be revised
- **Journal**: None
- **Summary**: Understanding the inner working mechanism of deep neural networks (DNNs) is essential and important for researchers to design and improve the performance of DNNs. In this work, the entropy analysis is leveraged to study the neurons activation behavior of the fully connected layers of DNNs. The entropy of the activation patterns of each layer can provide a performance metric for the evaluation of the network model accuracy. The study is conducted based on a well trained network model. The activation patterns of shallow and deep layers of the fully connected layers are analyzed by inputting the images of a single class. It is found that for the well trained deep neural networks model, the entropy of the neuron activation pattern is monotonically reduced with the depth of the layers. That is, the neuron activation patterns become more and more stable with the depth of the fully connected layers. The entropy pattern of the fully connected layers can also provide guidelines as to how many fully connected layers are needed to guarantee the accuracy of the model. The study in this work provides a new perspective on the analysis of DNN, which shows some interesting results.



### The Algonauts Project: A Platform for Communication between the Sciences of Biological and Artificial Intelligence
- **Arxiv ID**: http://arxiv.org/abs/1905.05675v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1905.05675v1)
- **Published**: 2019-05-14 15:37:22+00:00
- **Updated**: 2019-05-14 15:37:22+00:00
- **Authors**: Radoslaw Martin Cichy, Gemma Roig, Alex Andonian, Kshitij Dwivedi, Benjamin Lahner, Alex Lascelles, Yalda Mohsenzadeh, Kandan Ramakrishnan, Aude Oliva
- **Comment**: 4 pages, 2 figures
- **Journal**: None
- **Summary**: In the last decade, artificial intelligence (AI) models inspired by the brain have made unprecedented progress in performing real-world perceptual tasks like object classification and speech recognition. Recently, researchers of natural intelligence have begun using those AI models to explore how the brain performs such tasks. These developments suggest that future progress will benefit from increased interaction between disciplines. Here we introduce the Algonauts Project as a structured and quantitative communication channel for interdisciplinary interaction between natural and artificial intelligence researchers. The project's core is an open challenge with a quantitative benchmark whose goal is to account for brain data through computational models. This project has the potential to provide better models of natural intelligence and to gather findings that advance AI. The 2019 Algonauts Project focuses on benchmarking computational models predicting human brain activity when people look at pictures of objects. The 2019 edition of the Algonauts Project is available online: http://algonauts.csail.mit.edu/.



### Graph Convolutional Gaussian Processes
- **Arxiv ID**: http://arxiv.org/abs/1905.05739v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.05739v1)
- **Published**: 2019-05-14 17:32:29+00:00
- **Updated**: 2019-05-14 17:32:29+00:00
- **Authors**: Ian Walker, Ben Glocker
- **Comment**: Accepted at ICML 2019
- **Journal**: None
- **Summary**: We propose a novel Bayesian nonparametric method to learn translation-invariant relationships on non-Euclidean domains. The resulting graph convolutional Gaussian processes can be applied to problems in machine learning for which the input observations are functions with domains on general graphs. The structure of these models allows for high dimensional inputs while retaining expressibility, as is the case with convolutional neural networks. We present applications of graph convolutional Gaussian processes to images and triangular meshes, demonstrating their versatility and effectiveness, comparing favorably to existing methods, despite being relatively simple models.



### DeepFlow: History Matching in the Space of Deep Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1905.05749v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, physics.comp-ph, physics.geo-ph, stat.ML, 68T10 (Primary) 68T20, 86-08, 86-04, 76T99 (Secondary), I.5.1; I.6.3; I.5.4; I.2.8
- **Links**: [PDF](http://arxiv.org/pdf/1905.05749v2)
- **Published**: 2019-05-14 17:52:52+00:00
- **Updated**: 2019-06-12 12:49:02+00:00
- **Authors**: Lukas Mosser, Olivier Dubrule, Martin J. Blunt
- **Comment**: 25 pages, 15 figures, fixed typos
- **Journal**: None
- **Summary**: The calibration of a reservoir model with observed transient data of fluid pressures and rates is a key task in obtaining a predictive model of the flow and transport behaviour of the earth's subsurface. The model calibration task, commonly referred to as "history matching", can be formalised as an ill-posed inverse problem where we aim to find the underlying spatial distribution of petrophysical properties that explain the observed dynamic data. We use a generative adversarial network pretrained on geostatistical object-based models to represent the distribution of rock properties for a synthetic model of a hydrocarbon reservoir. The dynamic behaviour of the reservoir fluids is modelled using a transient two-phase incompressible Darcy formulation. We invert for the underlying reservoir properties by first modeling property distributions using the pre-trained generative model then using the adjoint equations of the forward problem to perform gradient descent on the latent variables that control the output of the generative model. In addition to the dynamic observation data, we include well rock-type constraints by introducing an additional objective function. Our contribution shows that for a synthetic test case, we are able to obtain solutions to the inverse problem by optimising in the latent variable space of a deep generative model, given a set of transient observations of a non-linear forward problem.



### Learnable Triangulation of Human Pose
- **Arxiv ID**: http://arxiv.org/abs/1905.05754v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.05754v1)
- **Published**: 2019-05-14 17:59:20+00:00
- **Updated**: 2019-05-14 17:59:20+00:00
- **Authors**: Karim Iskakov, Egor Burkov, Victor Lempitsky, Yury Malkov
- **Comment**: Project page: https://saic-violet.github.io/learnable-triangulation
- **Journal**: None
- **Summary**: We present two novel solutions for multi-view 3D human pose estimation based on new learnable triangulation methods that combine 3D information from multiple 2D views. The first (baseline) solution is a basic differentiable algebraic triangulation with an addition of confidence weights estimated from the input images. The second solution is based on a novel method of volumetric aggregation from intermediate 2D backbone feature maps. The aggregated volume is then refined via 3D convolutions that produce final 3D joint heatmaps and allow modelling a human pose prior. Crucially, both approaches are end-to-end differentiable, which allows us to directly optimize the target metric. We demonstrate transferability of the solutions across datasets and considerably improve the multi-view state of the art on the Human3.6M dataset. Video demonstration, annotations and additional materials will be posted on our project page (https://saic-violet.github.io/learnable-triangulation).



### Improving Model Training by Periodic Sampling over Weight Distributions
- **Arxiv ID**: http://arxiv.org/abs/1905.05774v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.05774v2)
- **Published**: 2019-05-14 18:00:23+00:00
- **Updated**: 2020-03-19 21:37:12+00:00
- **Authors**: Samarth Tripathi, Jiayi Liu, Unmesh Kurup, Mohak Shah, Sauptik Dhar
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we explore techniques centered around periodic sampling of model weights that provide convergence improvements on gradient update methods (vanilla \acs{SGD}, Momentum, Adam) for a variety of vision problems (classification, detection, segmentation). Importantly, our algorithms provide better, faster and more robust convergence and training performance with only a slight increase in computation time. Our techniques are independent of the neural network model, gradient optimization methods or existing optimal training policies and converge in a less volatile fashion with performance improvements that are approximately monotonic. We conduct a variety of experiments to quantify these improvements and identify scenarios where these techniques could be more useful.



### Reconstruction-Aware Imaging System Ranking by use of a Sparsity-Driven Numerical Observer Enabled by Variational Bayesian Inference
- **Arxiv ID**: http://arxiv.org/abs/1905.05820v1
- **DOI**: 10.1109/TMI.2018.2880870
- **Categories**: **cs.CV**, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1905.05820v1)
- **Published**: 2019-05-14 20:12:49+00:00
- **Updated**: 2019-05-14 20:12:49+00:00
- **Authors**: Yujia Chen, Yang Lou, Kun Wang, Matthew A. Kupinski, Mark A. Anastasio
- **Comment**: IEEE transactions on medical imaging (2018)
- **Journal**: None
- **Summary**: It is widely accepted that optimization of imaging system performance should be guided by task-based measures of image quality (IQ). It has been advocated that imaging hardware or data-acquisition designs should be optimized by use of an ideal observer (IO) that exploits full statistical knowledge of the measurement noise and class of objects to be imaged, without consideration of the reconstruction method. In practice, accurate and tractable models of the complete object statistics are often difficult to determine. Moreover, in imaging systems that employ compressive sensing concepts, imaging hardware and sparse image reconstruction are innately coupled technologies. In this work, a sparsity-driven observer (SDO) that can be employed to optimize hardware by use of a stochastic object model describing object sparsity is described and investigated. The SDO and sparse reconstruction method can therefore be "matched" in the sense that they both utilize the same statistical information regarding the class of objects to be imaged. To efficiently compute the SDO test statistic, computational tools developed recently for variational Bayesian inference with sparse linear models are adopted. The use of the SDO to rank data-acquisition designs in a stylized example as motivated by magnetic resonance imaging (MRI) is demonstrated. This study reveals that the SDO can produce rankings that are consistent with visual assessments of the reconstructed images but different from those produced by use of the traditionally employed Hotelling observer (HO).



### Supervised Learning of the Next-Best-View for 3D Object Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1905.05833v1
- **DOI**: 10.1016/j.patrec.2020.02.024
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1905.05833v1)
- **Published**: 2019-05-14 20:39:38+00:00
- **Updated**: 2019-05-14 20:39:38+00:00
- **Authors**: Miguel Mendoza, J. Irving Vasquez-Gomez, Hind Taud, Luis Enrique Sucar, Carolina Reta
- **Comment**: Under review in Pattern Recognition Letters
- **Journal**: Pattern Recognition Letters, Volume 133, 2020, Pages 224-231, ISSN
  0167-8655
- **Summary**: Motivated by the advances in 3D sensing technology and the spreading of low-cost robotic platforms, 3D object reconstruction has become a common task in many areas. Nevertheless, the selection of the optimal sensor pose that maximizes the reconstructed surface is a problem that remains open. It is known in the literature as the next-best-view planning problem. In this paper, we propose a novel next-best-view planning scheme based on supervised deep learning. The scheme contains an algorithm for automatic generation of datasets and an original three-dimensional convolutional neural network (3D-CNN) used to learn the next-best-view. Unlike previous work where the problem is addressed as a search, the trained 3D-CNN directly predicts the sensor pose. We present a comparison of the proposed network against a similar net, and we present several experiments of the reconstruction of unknown objects validating the effectiveness of the proposed scheme.



### Consensus-based Interpretable Deep Neural Networks with Application to Mortality Prediction
- **Arxiv ID**: http://arxiv.org/abs/1905.05849v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.05849v2)
- **Published**: 2019-05-14 21:26:56+00:00
- **Updated**: 2019-09-11 05:32:07+00:00
- **Authors**: Shaeke Salman, Seyedeh Neelufar Payrovnaziri, Xiuwen Liu, Pablo Rengifo-Moreno, Zhe He
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Deep neural networks have achieved remarkable success in various challenging tasks. However, the black-box nature of such networks is not acceptable to critical applications, such as healthcare. In particular, the existence of adversarial examples and their overgeneralization to irrelevant, out-of-distribution inputs with high confidence makes it difficult, if not impossible, to explain decisions by such networks. In this paper, we analyze the underlying mechanism of generalization of deep neural networks and propose an ($n$, $k$) consensus algorithm which is insensitive to adversarial examples and can reliably reject out-of-distribution samples. Furthermore, the consensus algorithm is able to improve classification accuracy by using multiple trained deep neural networks. To handle the complexity of deep neural networks, we cluster linear approximations of individual models and identify highly correlated clusters among different models to capture feature importance robustly, resulting in improved interpretability. Motivated by the importance of building accurate and interpretable prediction models for healthcare, our experimental results on an ICU dataset show the effectiveness of our algorithm in enhancing both the prediction accuracy and the interpretability of deep neural network models on one-year patient mortality prediction. In particular, while the proposed method maintains similar interpretability as conventional shallow models such as logistic regression, it improves the prediction accuracy significantly.



### Improving Head Pose Estimation with a Combined Loss and Bounding Box Margin Adjustment
- **Arxiv ID**: http://arxiv.org/abs/1905.08609v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08609v1)
- **Published**: 2019-05-14 21:58:35+00:00
- **Updated**: 2019-05-14 21:58:35+00:00
- **Authors**: Mingzhen Shao, Zhun Sun, Mete Ozay, Takayuki Okatani
- **Comment**: IEEE International Conference on Automatic Face & Gesture Recognition
  (FG2019)
- **Journal**: None
- **Summary**: We address a problem of estimating pose of a person's head from its RGB image. The employment of CNNs for the problem has contributed to significant improvement in accuracy in recent works. However, we show that the following two methods, despite their simplicity, can attain further improvement: (i) proper adjustment of the margin of bounding box of a detected face, and (ii) choice of loss functions. We show that the integration of these two methods achieve the new state-of-the-art on standard benchmark datasets for in-the-wild head pose estimation.



### Budget-aware Semi-Supervised Semantic and Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.05880v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05880v2)
- **Published**: 2019-05-14 23:19:41+00:00
- **Updated**: 2019-05-23 21:24:46+00:00
- **Authors**: Miriam Bellver, Amaia Salvador, Jordi Torres, Xavier Giro-i-Nieto
- **Comment**: To appear in CVPR-W 2019 (DeepVision workshop)
- **Journal**: None
- **Summary**: Methods that move towards less supervised scenarios are key for image segmentation, as dense labels demand significant human intervention. Generally, the annotation burden is mitigated by labeling datasets with weaker forms of supervision, e.g. image-level labels or bounding boxes. Another option are semi-supervised settings, that commonly leverage a few strong annotations and a huge number of unlabeled/weakly-labeled data. In this paper, we revisit semi-supervised segmentation schemes and narrow down significantly the annotation budget (in terms of total labeling time of the training set) compared to previous approaches. With a very simple pipeline, we demonstrate that at low annotation budgets, semi-supervised methods outperform by a wide margin weakly-supervised ones for both semantic and instance segmentation. Our approach also outperforms previous semi-supervised works at a much reduced labeling cost. We present results for the Pascal VOC benchmark and unify weakly and semi-supervised approaches by considering the total annotation budget, thus allowing a fairer comparison between methods.



### Kernel Mean Matching for Content Addressability of GANs
- **Arxiv ID**: http://arxiv.org/abs/1905.05882v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.05882v1)
- **Published**: 2019-05-14 23:32:53+00:00
- **Updated**: 2019-05-14 23:32:53+00:00
- **Authors**: Wittawat Jitkrittum, Patsorn Sangkloy, Muhammad Waleed Gondal, Amit Raj, James Hays, Bernhard Schölkopf
- **Comment**: Wittawat Jitkrittum and Patsorn Sangkloy contributed equally to this
  work
- **Journal**: None
- **Summary**: We propose a novel procedure which adds "content-addressability" to any given unconditional implicit model e.g., a generative adversarial network (GAN). The procedure allows users to control the generative process by specifying a set (arbitrary size) of desired examples based on which similar samples are generated from the model. The proposed approach, based on kernel mean matching, is applicable to any generative models which transform latent vectors to samples, and does not require retraining of the model. Experiments on various high-dimensional image generation problems (CelebA-HQ, LSUN bedroom, bridge, tower) show that our approach is able to generate images which are consistent with the input set, while retaining the image quality of the original model. To our knowledge, this is the first work that attempts to construct, at test time, a content-addressable generative model from a trained marginal model.



