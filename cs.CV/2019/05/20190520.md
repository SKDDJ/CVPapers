# Arxiv Papers in cs.CV on 2019-05-20
### Testing DNN Image Classifiers for Confusion & Bias Errors
- **Arxiv ID**: http://arxiv.org/abs/1905.07831v3
- **DOI**: 10.1145/3377811.3380400
- **Categories**: **cs.SE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.07831v3)
- **Published**: 2019-05-20 00:00:24+00:00
- **Updated**: 2020-02-11 19:32:09+00:00
- **Authors**: Yuchi Tian, Ziyuan Zhong, Vicente Ordonez, Gail Kaiser, Baishakhi Ray
- **Comment**: None
- **Journal**: None
- **Summary**: Image classifiers are an important component of today's software, from consumer and business applications to safety-critical domains. The advent of Deep Neural Networks (DNNs) is the key catalyst behind such wide-spread success. However, wide adoption comes with serious concerns about the robustness of software systems dependent on DNNs for image classification, as several severe erroneous behaviors have been reported under sensitive and critical circumstances. We argue that developers need to rigorously test their software's image classifiers and delay deployment until acceptable. We present an approach to testing image classifier robustness based on class property violations.   We found that many of the reported erroneous cases in popular DNN image classifiers occur because the trained models confuse one class with another or show biases towards some classes over others. These bugs usually violate some class properties of one or more of those classes. Most DNN testing techniques focus on per-image violations, so fail to detect class-level confusions or biases.   We developed a testing technique to automatically detect class-based confusion and bias errors in DNN-driven image classification software. We evaluated our implementation, DeepInspect, on several popular image classifiers with precision up to 100% (avg.~72.6%) for confusion errors, and up to 84.3% (avg.~66.8%) for bias errors. DeepInspect found hundreds of classification mistakes in widely-used models, many exposing errors indicating confusion or bias.



### Enabling Computer Vision Driven Assistive Devices for the Visually Impaired via Micro-architecture Design Exploration
- **Arxiv ID**: http://arxiv.org/abs/1905.07836v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1905.07836v1)
- **Published**: 2019-05-20 01:30:15+00:00
- **Updated**: 2019-05-20 01:30:15+00:00
- **Authors**: Linda Wang, Alexander Wong
- **Comment**: None
- **Journal**: None
- **Summary**: Recent improvements in object detection have shown potential to aid in tasks where previous solutions were not able to achieve. A particular area is assistive devices for individuals with visual impairment. While state-of-the-art deep neural networks have been shown to achieve superior object detection performance, their high computational and memory requirements make them cost prohibitive for on-device operation. Alternatively, cloud-based operation leads to privacy concerns, both not attractive to potential users. To address these challenges, this study investigates creating an efficient object detection network specifically for OLIV, an AI-powered assistant for object localization for the visually impaired, via micro-architecture design exploration. In particular, we formulate the problem of finding an optimal network micro-architecture as an numerical optimization problem, where we find the set of hyperparameters controlling the MobileNetV2-SSD network micro-architecture that maximizes a modified NetScore objective function for the MSCOCO-OLIV dataset of indoor objects. Experimental results show that such a micro-architecture design exploration strategy leads to a compact deep neural network with a balanced trade-off between accuracy, size, and speed, making it well-suited for enabling on-device computer vision driven assistive devices for the visually impaired.



### Multimodal Transformer with Multi-View Visual Representation for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1905.07841v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.07841v1)
- **Published**: 2019-05-20 01:56:06+00:00
- **Updated**: 2019-05-20 01:56:06+00:00
- **Authors**: Jun Yu, Jing Li, Zhou Yu, Qingming Huang
- **Comment**: submitted to a journal
- **Journal**: None
- **Summary**: Image captioning aims to automatically generate a natural language description of a given image, and most state-of-the-art models have adopted an encoder-decoder framework. The framework consists of a convolution neural network (CNN)-based image encoder that extracts region-based visual features from the input image, and an recurrent neural network (RNN)-based caption decoder that generates the output caption words based on the visual features with the attention mechanism. Despite the success of existing studies, current methods only model the co-attention that characterizes the inter-modal interactions while neglecting the self-attention that characterizes the intra-modal interactions. Inspired by the success of the Transformer model in machine translation, here we extend it to a Multimodal Transformer (MT) model for image captioning. Compared to existing image captioning approaches, the MT model simultaneously captures intra- and inter-modal interactions in a unified attention block. Due to the in-depth modular composition of such attention blocks, the MT model can perform complex multimodal reasoning and output accurate captions. Moreover, to further improve the image captioning performance, multi-view visual features are seamlessly introduced into the MT model. We quantitatively and qualitatively evaluate our approach using the benchmark MSCOCO image captioning dataset and conduct extensive ablation studies to investigate the reasons behind its effectiveness. The experimental results show that our method significantly outperforms the previous state-of-the-art methods. With an ensemble of seven models, our solution ranks the 1st place on the real-time leaderboard of the MSCOCO image captioning challenge at the time of the writing of this paper.



### Implications of Computer Vision Driven Assistive Technologies Towards Individuals with Visual Impairment
- **Arxiv ID**: http://arxiv.org/abs/1905.07844v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1905.07844v1)
- **Published**: 2019-05-20 02:00:56+00:00
- **Updated**: 2019-05-20 02:00:56+00:00
- **Authors**: Linda Wang, Alexander Wong
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision based technology is becoming ubiquitous in society. One application area that has seen an increase in computer vision is assistive technologies, specifically for those with visual impairment. Research has shown the ability of computer vision models to achieve tasks such provide scene captions, detect objects and recognize faces. Although assisting individuals with visual impairment with these tasks increases their independence and autonomy, concerns over bias, privacy and potential usefulness arise. This paper addresses the positive and negative implications computer vision based assistive technologies have on individuals with visual impairment, as well as considerations for computer vision researchers and developers in order to mitigate the amount of negative implications.



### Boundary Loss for Remote Sensing Imagery Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.07852v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.07852v1)
- **Published**: 2019-05-20 03:02:44+00:00
- **Updated**: 2019-05-20 03:02:44+00:00
- **Authors**: Alexey Bokhovkin, Evgeny Burnaev
- **Comment**: 14 pages, 10 figures
- **Journal**: Proceedings of 16th International Symposium on Neural Networks,
  2019
- **Summary**: In response to the growing importance of geospatial data, its analysis including semantic segmentation becomes an increasingly popular task in computer vision today. Convolutional neural networks are powerful visual models that yield hierarchies of features and practitioners widely use them to process remote sensing data. When performing remote sensing image segmentation, multiple instances of one class with precisely defined boundaries are often the case, and it is crucial to extract those boundaries accurately. The accuracy of segments boundaries delineation influences the quality of the whole segmented areas explicitly. However, widely-used segmentation loss functions such as BCE, IoU loss or Dice loss do not penalize misalignment of boundaries sufficiently. In this paper, we propose a novel loss function, namely a differentiable surrogate of a metric accounting accuracy of boundary detection. We can use the loss function with any neural network for binary segmentation. We performed validation of our loss function with various modifications of UNet on a synthetic dataset, as well as using real-world data (ISPRS Potsdam, INRIA AIL). Trained with the proposed loss function, models outperform baseline methods in terms of IoU score.



### Learning Video Representations from Correspondence Proposals
- **Arxiv ID**: http://arxiv.org/abs/1905.07853v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.07853v1)
- **Published**: 2019-05-20 03:07:51+00:00
- **Updated**: 2019-05-20 03:07:51+00:00
- **Authors**: Xingyu Liu, Joon-Young Lee, Hailin Jin
- **Comment**: CVPR 2019 (Oral)
- **Journal**: None
- **Summary**: Correspondences between frames encode rich information about dynamic content in videos. However, it is challenging to effectively capture and learn those due to their irregular structure and complex dynamics. In this paper, we propose a novel neural network that learns video representations by aggregating information from potential correspondences. This network, named $CPNet$, can learn evolving 2D fields with temporal consistency. In particular, it can effectively learn representations for videos by mixing appearance and long-range motion with an RGB-only input. We provide extensive ablation experiments to validate our model. CPNet shows stronger performance than existing methods on Kinetics and achieves the state-of-the-art performance on Something-Something and Jester. We provide analysis towards the behavior of our model and show its robustness to errors in proposals.



### Not All Parts Are Created Equal: 3D Pose Estimation by Modelling Bi-directional Dependencies of Body Parts
- **Arxiv ID**: http://arxiv.org/abs/1905.07862v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.07862v1)
- **Published**: 2019-05-20 04:01:28+00:00
- **Updated**: 2019-05-20 04:01:28+00:00
- **Authors**: Jue Wang, Shaoli Huang, Xinchao Wang, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Not all the human body parts have the same~degree of freedom~(DOF) due to the physiological structure. For example, the limbs may move more flexibly and freely than the torso does. Most of the existing 3D pose estimation methods, despite the very promising results achieved, treat the body joints equally and consequently often lead to larger reconstruction errors on the limbs. In this paper, we propose a progressive approach that explicitly accounts for the distinct DOFs among the body parts. We model parts with higher DOFs like the elbows, as dependent components of the corresponding parts with lower DOFs like the torso, of which the 3D locations can be more reliably estimated. Meanwhile, the high-DOF parts may, in turn, impose a constraint on where the low-DOF ones lie. As a result, parts with different DOFs supervise one another, yielding physically constrained and plausible pose-estimation results. To further facilitate the prediction of the high-DOF parts, we introduce a pose-attribute estimation, where the relative location of a limb joint with respect to the torso, which has the least DOF of a human body, is explicitly estimated and further fed to the joint-estimation module. The proposed approach achieves very promising results, outperforming the state of the art on several benchmarks.



### Procedural Synthesis of Remote Sensing Images for Robust Change Detection with Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.07877v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.07877v1)
- **Published**: 2019-05-20 05:24:33+00:00
- **Updated**: 2019-05-20 05:24:33+00:00
- **Authors**: Maria Kolos, Anton Marin, Alexey Artemov, Evgeny Burnaev
- **Comment**: 17 pages, 11 figures
- **Journal**: 16th International Symposium on Neural Networks, ISNN 2019
- **Summary**: Data-driven methods such as convolutional neural networks (CNNs) are known to deliver state-of-the-art performance on image recognition tasks when the training data are abundant. However, in some instances, such as change detection in remote sensing images, annotated data cannot be obtained in sufficient quantities. In this work, we propose a simple and efficient method for creating realistic targeted synthetic datasets in the remote sensing domain, leveraging the opportunities offered by game development engines. We provide a description of the pipeline for procedural geometry generation and rendering as well as an evaluation of the efficiency of produced datasets in a change detection scenario. Our evaluations demonstrate that our pipeline helps to improve the performance and convergence of deep learning models when the amount of real-world data is severely limited.



### Learning to Count Objects with Few Exemplar Annotations
- **Arxiv ID**: http://arxiv.org/abs/1905.07898v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.07898v1)
- **Published**: 2019-05-20 06:28:44+00:00
- **Updated**: 2019-05-20 06:28:44+00:00
- **Authors**: Jianfeng Wang, Rong Xiao, Yandong Guo, Lei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the problem of object counting with incomplete annotations. Based on the observation that in many object counting problems the target objects are normally repeated and highly similar to each other, we are particularly interested in the setting when only a few exemplar annotations are provided. Directly applying object detection with incomplete annotations will result in severe accuracy degradation due to its improper handling of unlabeled object instances. To address the problem, we propose a positiveness-focused object detector (PFOD) to progressively propagate the incomplete labels before applying the general object detection algorithm. The PFOD focuses on the positive samples and ignore the negative instances at most of the learning time. This strategy, though simple, dramatically boosts the object counting accuracy. On the CARPK dataset for parking lot car counting, we improved mAP@0.5 from 4.58% to 72.44% using only 5 training images each with 5 bounding boxes. On the Drink35 dataset for shelf product counting, the mAP@0.5 is improved from 14.16% to 53.73% using 10 training images each with 5 bounding boxes.



### Skeleton-Based Hand Gesture Recognition by Learning SPD Matrices with Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.07917v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1905.07917v1)
- **Published**: 2019-05-20 07:10:49+00:00
- **Updated**: 2019-05-20 07:10:49+00:00
- **Authors**: Xuan Nguyen, Luc Brun, Olivier Lezoray, Sébastien Bougleux
- **Comment**: None
- **Journal**: 14th IEEE International Conference on Automatic Face and Gesture
  Recognition, May 2019, Lille, France
- **Summary**: In this paper, we propose a new hand gesture recognition method based on skeletal data by learning SPD matrices with neural networks. We model the hand skeleton as a graph and introduce a neural network for SPD matrix learning, taking as input the 3D coordinates of hand joints. The proposed network is based on two newly designed layers that transform a set of SPD matrices into a SPD matrix. For gesture recognition, we train a linear SVM classifier using features extracted from our network. Experimental results on a challenging dataset (Dynamic Hand Gesture dataset from the SHREC 2017 3D Shape Retrieval Contest) show that the proposed method outperforms state-of-the-art methods.



### Disparity-based HDR imaging
- **Arxiv ID**: http://arxiv.org/abs/1905.07918v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.07918v1)
- **Published**: 2019-05-20 07:13:08+00:00
- **Updated**: 2019-05-20 07:13:08+00:00
- **Authors**: Jennifer Bonnard, Gilles Valette, Céline Loscos
- **Comment**: Digital Image & Signal Processing, Apr 2019, St Hugh's College,
  Oxford University, United Kingdom, United Kingdom
- **Journal**: None
- **Summary**: High-dynamic range imaging permits to extend the dynamic range of intensity values to get close to what the human eye is able to perceive. Although there has been a huge progress in the digital camera sensor range capacity, the need of capturing several exposures in order to reconstruct high-dynamic range values persist. In this paper, we present a study on how to acquire high-dynamic range values for multi-stereo images. In many papers, disparity has been used to register pixels of different images and guide the reconstruction. In this paper, we show the limitations of such approaches and propose heuristics as solutions to identified problematic cases.



### Fast Regularity-Constrained Plane Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1905.07922v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.07922v1)
- **Published**: 2019-05-20 07:32:02+00:00
- **Updated**: 2019-05-20 07:32:02+00:00
- **Authors**: Yangbin Lin, Jialian Li, Cheng Wang, Zhonggui Chen, Zongyue Wang, Jonathan Li
- **Comment**: None
- **Journal**: None
- **Summary**: Man-made environments typically comprise planar structures that exhibit numerous geometric relationships, such as parallelism, coplanarity, and orthogonality. Making full use of these relationships can considerably improve the robustness of algorithmic plane reconstruction of complex scenes. This research leverages a constraint model requiring minimal prior knowledge to implicitly establish relationships among planes. We introduce a method based on energy minimization to reconstruct the planes consistent with our constraint model. The proposed algorithm is efficient, easily to understand, and simple to implement. The experimental results show that our algorithm successfully reconstructs planes under high percentages of noise and outliers. This is superior to other state-of-the-art regularity-constrained plane reconstruction methods in terms of speed and robustness.



### Learning Image-Specific Attributes by Hyperbolic Neighborhood Graph Propagation
- **Arxiv ID**: http://arxiv.org/abs/1905.07933v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.07933v2)
- **Published**: 2019-05-20 08:05:47+00:00
- **Updated**: 2019-05-25 02:19:27+00:00
- **Authors**: Xiaofeng Xu, Ivor W. Tsang, Xiaofeng Cao, Ruiheng Zhang, Chuancai Liu
- **Comment**: Accepted for IJCAI 2019
- **Journal**: None
- **Summary**: As a kind of semantic representation of visual object descriptions, attributes are widely used in various computer vision tasks. In most of existing attribute-based research, class-specific attributes (CSA), which are class-level annotations, are usually adopted due to its low annotation cost for each class instead of each individual image. However, class-specific attributes are usually noisy because of annotation errors and diversity of individual images. Therefore, it is desirable to obtain image-specific attributes (ISA), which are image-level annotations, from the original class-specific attributes. In this paper, we propose to learn image-specific attributes by graph-based attribute propagation. Considering the intrinsic property of hyperbolic geometry that its distance expands exponentially, hyperbolic neighborhood graph (HNG) is constructed to characterize the relationship between samples. Based on HNG, we define neighborhood consistency for each sample to identify inconsistent samples. Subsequently, inconsistent samples are refined based on their neighbors in HNG. Extensive experiments on five benchmark datasets demonstrate the significant superiority of the learned image-specific attributes over the original class-specific attributes in the zero-shot object classification task.



### Spin Detection in Robotic Table Tennis
- **Arxiv ID**: http://arxiv.org/abs/1905.07967v2
- **DOI**: 10.1109/ICRA40945.2020.9196536
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1905.07967v2)
- **Published**: 2019-05-20 10:01:32+00:00
- **Updated**: 2019-10-16 16:25:33+00:00
- **Authors**: Jonas Tebbe, Lukas Klamt, Yapeng Gao, Andreas Zell
- **Comment**: submitted to ICRA 2020
- **Journal**: 2020 IEEE International Conference on Robotics and Automation
  (ICRA), Paris, France, 2020, pp. 9694-9700
- **Summary**: In table tennis, the rotation (spin) of the ball plays a crucial role. A table tennis match will feature a variety of strokes. Each generates different amounts and types of spin. To develop a robot that can compete with a human player, the robot needs to detect spin, so it can plan an appropriate return stroke. In this paper we compare three methods to estimate spin. The first two approaches use a high-speed camera that captures the ball in flight at a frame rate of 380 Hz. This camera allows the movement of the circular brand logo printed on the ball to be seen. The first approach uses background difference to determine the position of the logo. In a second alternative, we train a CNN to predict the orientation of the logo. The third method evaluates the trajectory of the ball and derives the rotation from the effect of the Magnus force. This method gives the highest accuracy and is used for a demonstration. Our robot successfully copes with different spin types in a real table tennis rally against a human opponent.



### Deep Transfer Learning Methods for Colon Cancer Classification in Confocal Laser Microscopy Images
- **Arxiv ID**: http://arxiv.org/abs/1905.07991v1
- **DOI**: 10.1007/s11548-019-02004-1
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.07991v1)
- **Published**: 2019-05-20 11:02:21+00:00
- **Updated**: 2019-05-20 11:02:21+00:00
- **Authors**: Nils Gessert, Marcel Bengs, Lukas Wittig, Daniel Drömann, Tobias Keck, Alexander Schlaefer, David B. Ellebrecht
- **Comment**: Accepted for publication in the International Journal of Computer
  Assisted Radiology and Surgery (IJCARS)
- **Journal**: None
- **Summary**: Purpose: The gold standard for colorectal cancer metastases detection in the peritoneum is histological evaluation of a removed tissue sample. For feedback during interventions, real-time in-vivo imaging with confocal laser microscopy has been proposed for differentiation of benign and malignant tissue by manual expert evaluation. Automatic image classification could improve the surgical workflow further by providing immediate feedback.   Methods: We analyze the feasibility of classifying tissue from confocal laser microscopy in the colon and peritoneum. For this purpose, we adopt both classical and state-of-the-art convolutional neural networks to directly learn from the images. As the available dataset is small, we investigate several transfer learning strategies including partial freezing variants and full fine-tuning. We address the distinction of different tissue types, as well as benign and malignant tissue.   Results: We present a thorough analysis of transfer learning strategies for colorectal cancer with confocal laser microscopy. In the peritoneum, metastases are classified with an AUC of 97.1 and in the colon, the primarius is classified with an AUC of 73.1. In general, transfer learning substantially improves performance over training from scratch. We find that the optimal transfer learning strategy differs for models and classification tasks.   Conclusions: We demonstrate that convolutional neural networks and transfer learning can be used to identify cancer tissue with confocal laser microscopy. We show that there is no generally optimal transfer learning strategy and model as well as task-specific engineering is required. Given the high performance for the peritoneum, even with a small dataset, application for intraoperative decision support could be feasible.



### Less Memory, Faster Speed: Refining Self-Attention Module for Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1905.08008v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.08008v1)
- **Published**: 2019-05-20 11:43:37+00:00
- **Updated**: 2019-05-20 11:43:37+00:00
- **Authors**: Zheng Wang, Jianwu Li, Ge Song, Tieling Li
- **Comment**: None
- **Journal**: None
- **Summary**: Self-attention (SA) mechanisms can capture effectively global dependencies in deep neural networks, and have been applied to natural language processing and image processing successfully. However, SA modules for image reconstruction have high time and space complexity, which restrict their applications to higher-resolution images. In this paper, we refine the SA module in self-attention generative adversarial networks (SAGAN) via adapting a non-local operation, revising the connectivity among the units in SA module and re-implementing its computational pattern, such that its time and space complexity is reduced from $\text{O}(n^2)$ to $\text{O}(n)$, but it is still equivalent to the original SA module. Further, we explore the principles behind the module and discover that our module is a special kind of channel attention mechanisms. Experimental results based on two benchmark datasets of image reconstruction, verify that under the same computational environment, two models can achieve comparable effectiveness for image reconstruction, but the proposed one runs faster and takes up less memory space.



### Catastrophic forgetting: still a problem for DNNs
- **Arxiv ID**: http://arxiv.org/abs/1905.08077v1
- **DOI**: 10.1007/978-3-030-01418-6_48
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.08077v1)
- **Published**: 2019-05-20 13:06:30+00:00
- **Updated**: 2019-05-20 13:06:30+00:00
- **Authors**: B. Pfülb, A. Gepperth, S. Abdullah, A. Kilian
- **Comment**: 10 pages, 11 figures, Artificial Neural Networks and Machine Learning
  - ICANN 2018
- **Journal**: None
- **Summary**: We investigate the performance of DNNs when trained on class-incremental visual problems consisting of initial training, followed by retraining with added visual classes. Catastrophic forgetting (CF) behavior is measured using a new evaluation procedure that aims at an application-oriented view of incremental learning. In particular, it imposes that model selection must be performed on the initial dataset alone, as well as demanding that retraining control be performed only using the retraining dataset, as initial dataset is usually too large to be kept. Experiments are conducted on class-incremental problems derived from MNIST, using a variety of different DNN models, some of them recently proposed to avoid catastrophic forgetting. When comparing our new evaluation procedure to previous approaches for assessing CF, we find their findings are completely negated, and that none of the tested methods can avoid CF in all experiments. This stresses the importance of a realistic empirical measurement procedure for catastrophic forgetting, and the need for further research in incremental learning for DNNs.



### Activity Recognition and Prediction in Real Homes
- **Arxiv ID**: http://arxiv.org/abs/1905.08654v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.08654v1)
- **Published**: 2019-05-20 13:14:55+00:00
- **Updated**: 2019-05-20 13:14:55+00:00
- **Authors**: Flavia Dias Casagrande, Evi Zouganeli
- **Comment**: 12 pages, Symposium of the Norwegian AI Society NAIS 2019
- **Journal**: None
- **Summary**: In this paper, we present work in progress on activity recognition and prediction in real homes using either binary sensor data or depth video data. We present our field trial and set-up for collecting and storing the data, our methods, and our current results. We compare the accuracy of predicting the next binary sensor event using probabilistic methods and Long Short-Term Memory (LSTM) networks, include the time information to improve prediction accuracy, as well as predict both the next sensor event and its mean time of occurrence using one LSTM model. We investigate transfer learning between apartments and show that it is possible to pre-train the model with data from other apartments and achieve good accuracy in a new apartment straight away. In addition, we present preliminary results from activity recognition using low-resolution depth video data from seven apartments, and classify four activities - no movement, standing up, sitting down, and TV interaction - by using a relatively simple processing method where we apply an Infinite Impulse Response (IIR) filter to extract movements from the frames prior to feeding them to a convolutional LSTM network for the classification.



### Image Captioning based on Deep Learning Methods: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1905.08110v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.08110v1)
- **Published**: 2019-05-20 13:43:52+00:00
- **Updated**: 2019-05-20 13:43:52+00:00
- **Authors**: Yiyu Wang, Jungang Xu, Yingfei Sun, Ben He
- **Comment**: None
- **Journal**: None
- **Summary**: Image captioning is a challenging task and attracting more and more attention in the field of Artificial Intelligence, and which can be applied to efficient image retrieval, intelligent blind guidance and human-computer interaction, etc. In this paper, we present a survey on advances in image captioning based on Deep Learning methods, including Encoder-Decoder structure, improved methods in Encoder, improved methods in Decoder, and other improvements. Furthermore, we discussed future research directions.



### Zero-Shot Knowledge Distillation in Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.08114v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.08114v1)
- **Published**: 2019-05-20 13:49:28+00:00
- **Updated**: 2019-05-20 13:49:28+00:00
- **Authors**: Gaurav Kumar Nayak, Konda Reddy Mopuri, Vaisakh Shaj, R. Venkatesh Babu, Anirban Chakraborty
- **Comment**: Accepted in ICML 2019, codes will be available at
  https://github.com/vcl-iisc/ZSKD
- **Journal**: None
- **Summary**: Knowledge distillation deals with the problem of training a smaller model (Student) from a high capacity source model (Teacher) so as to retain most of its performance. Existing approaches use either the training data or meta-data extracted from it in order to train the Student. However, accessing the dataset on which the Teacher has been trained may not always be feasible if the dataset is very large or it poses privacy or safety concerns (e.g., bio-metric or medical data). Hence, in this paper, we propose a novel data-free method to train the Student from the Teacher. Without even using any meta-data, we synthesize the Data Impressions from the complex Teacher model and utilize these as surrogates for the original training data samples to transfer its learning to Student via knowledge distillation. We, therefore, dub our method "Zero-Shot Knowledge Distillation" and demonstrate that our framework results in competitive generalization performance as achieved by distillation using the actual training data samples on multiple benchmark datasets.



### Self-Supervised Similarity Learning for Digital Pathology
- **Arxiv ID**: http://arxiv.org/abs/1905.08139v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08139v3)
- **Published**: 2019-05-20 14:31:26+00:00
- **Updated**: 2020-01-13 11:20:38+00:00
- **Authors**: Jacob Gildenblat, Eldad Klaiman
- **Comment**: None
- **Journal**: None
- **Summary**: Using features extracted from networks pretrained on ImageNet is a common practice in applications of deep learning for digital pathology. However it presents the downside of missing domain specific image information. In digital pathology, supervised training data is expensive and difficult to collect. We propose a self-supervised method for feature extraction by similarity learning on whole slide images (WSI) that is simple to implement and allows creation of robust and compact image descriptors. We train a siamese network, exploiting image spatial continuity and assuming spatially adjacent tiles in the image are more similar to each other than distant tiles. Our network outputs feature vectors of length 128, which allows dramatically lower memory storage and faster processing than networks pretrained on ImageNet. We apply the method on digital pathology WSIs from the Camelyon16 train set and assess and compare our method by measuring image retrieval of tumor tiles and descriptor pair distance ratio for distant/near tiles in the Camelyon16 test set. We show that our method yields better retrieval task results than existing ImageNet based and generic self-supervised feature extraction methods. To the best of our knowledge, this is also the first published method for self-supervised learning tailored for digital pathology.



### DARC: Differentiable ARchitecture Compression
- **Arxiv ID**: http://arxiv.org/abs/1905.08170v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.08170v1)
- **Published**: 2019-05-20 15:30:06+00:00
- **Updated**: 2019-05-20 15:30:06+00:00
- **Authors**: Shashank Singh, Ashish Khetan, Zohar Karnin
- **Comment**: None
- **Journal**: None
- **Summary**: In many learning situations, resources at inference time are significantly more constrained than resources at training time. This paper studies a general paradigm, called Differentiable ARchitecture Compression (DARC), that combines model compression and architecture search to learn models that are resource-efficient at inference time. Given a resource-intensive base architecture, DARC utilizes the training data to learn which sub-components can be replaced by cheaper alternatives. The high-level technique can be applied to any neural architecture, and we report experiments on state-of-the-art convolutional neural networks for image classification. For a WideResNet with $97.2\%$ accuracy on CIFAR-10, we improve single-sample inference speed by $2.28\times$ and memory footprint by $5.64\times$, with no accuracy loss. For a ResNet with $79.15\%$ Top1 accuracy on ImageNet, we improve batch inference speed by $1.29\times$ and memory footprint by $3.57\times$ with $1\%$ accuracy loss. We also give theoretical Rademacher complexity bounds in simplified cases, showing how DARC avoids overfitting despite over-parameterization.



### Semi-Supervised Learning by Augmented Distribution Alignment
- **Arxiv ID**: http://arxiv.org/abs/1905.08171v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.08171v2)
- **Published**: 2019-05-20 15:30:10+00:00
- **Updated**: 2019-08-18 12:45:33+00:00
- **Authors**: Qin Wang, Wen Li, Luc Van Gool
- **Comment**: To appear in ICCV 2019
- **Journal**: None
- **Summary**: In this work, we propose a simple yet effective semi-supervised learning approach called Augmented Distribution Alignment. We reveal that an essential sampling bias exists in semi-supervised learning due to the limited number of labeled samples, which often leads to a considerable empirical distribution mismatch between labeled data and unlabeled data. To this end, we propose to align the empirical distributions of labeled and unlabeled data to alleviate the bias. On one hand, we adopt an adversarial training strategy to minimize the distribution distance between labeled and unlabeled data as inspired by domain adaptation works. On the other hand, to deal with the small sample size issue of labeled data, we also propose a simple interpolation strategy to generate pseudo training samples. Those two strategies can be easily implemented into existing deep neural networks. We demonstrate the effectiveness of our proposed approach on the benchmark SVHN and CIFAR10 datasets. Our code is available at \url{https://github.com/qinenergy/adanet}.



### Drone Shadow Tracking
- **Arxiv ID**: http://arxiv.org/abs/1905.08214v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08214v1)
- **Published**: 2019-05-20 16:57:21+00:00
- **Updated**: 2019-05-20 16:57:21+00:00
- **Authors**: Xiaoyan Zou, Ruofan Zhou, Majed El Helou, Sabine Süsstrunk
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: Aerial videos taken by a drone not too far above the surface may contain the drone's shadow projected on the scene. This deteriorates the aesthetic quality of videos. With the presence of other shadows, shadow removal cannot be directly applied, and the shadow of the drone must be tracked. Tracking a drone's shadow in a video is, however, challenging. The varying size, shape, change of orientation and drone altitude pose difficulties. The shadow can also easily disappear over dark areas. However, a shadow has specific properties that can be leveraged, besides its geometric shape. In this paper, we incorporate knowledge of the shadow's physical properties, in the form of shadow detection masks, into a correlation-based tracking algorithm. We capture a test set of aerial videos taken with different settings and compare our results to those of a state-of-the-art tracking algorithm.



### Patch-based 3D Human Pose Refinement
- **Arxiv ID**: http://arxiv.org/abs/1905.08231v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08231v1)
- **Published**: 2019-05-20 17:51:41+00:00
- **Updated**: 2019-05-20 17:51:41+00:00
- **Authors**: Qingfu Wan, Weichao Qiu, Alan L. Yuille
- **Comment**: Accepted by CVPR 2019 Augmented Human: Human-centric Understanding
  and 2D/3D Synthesis, and the third Look Into Person (LIP) Challenge Workshop
- **Journal**: None
- **Summary**: State-of-the-art 3D human pose estimation approaches typically estimate pose from the entire RGB image in a single forward run. In this paper, we develop a post-processing step to refine 3D human pose estimation from body part patches. Using local patches as input has two advantages. First, the fine details around body parts are zoomed in to high resolution for preciser 3D pose prediction. Second, it enables the part appearance to be shared between poses to benefit rare poses. In order to acquire informative representation of patches, we explore different input modalities and validate the superiority of fusing predicted segmentation with RGB. We show that our method consistently boosts the accuracy of state-of-the-art 3D human pose methods.



### Adversarially robust transfer learning
- **Arxiv ID**: http://arxiv.org/abs/1905.08232v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.08232v2)
- **Published**: 2019-05-20 17:57:57+00:00
- **Updated**: 2020-02-21 15:51:23+00:00
- **Authors**: Ali Shafahi, Parsa Saadatpanah, Chen Zhu, Amin Ghiasi, Christoph Studer, David Jacobs, Tom Goldstein
- **Comment**: None
- **Journal**: None
- **Summary**: Transfer learning, in which a network is trained on one task and re-purposed on another, is often used to produce neural network classifiers when data is scarce or full-scale training is too costly. When the goal is to produce a model that is not only accurate but also adversarially robust, data scarcity and computational limitations become even more cumbersome. We consider robust transfer learning, in which we transfer not only performance but also robustness from a source model to a target domain. We start by observing that robust networks contain robust feature extractors. By training classifiers on top of these feature extractors, we produce new models that inherit the robustness of their parent networks. We then consider the case of fine tuning a network by re-training end-to-end in the target domain. When using lifelong learning strategies, this process preserves the robustness of the source network while achieving high accuracy. By using such strategies, it is possible to produce accurate and robust models with little data, and without the cost of adversarial training. Additionally, we can improve the generalization of adversarially trained models, while maintaining their robustness.



### Few-Shot Adversarial Learning of Realistic Neural Talking Head Models
- **Arxiv ID**: http://arxiv.org/abs/1905.08233v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.08233v2)
- **Published**: 2019-05-20 17:58:04+00:00
- **Updated**: 2019-09-25 11:16:01+00:00
- **Authors**: Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, Victor Lempitsky
- **Comment**: UPDATE: the data we used for evaluation is available for download!
  See https://drive.google.com/open?id=1PeGG6zO3ZjrHk2GAXItB8khwMhPPyDHe and
  refer to the README for description
- **Journal**: None
- **Summary**: Several recent works have shown how highly realistic human head images can be obtained by training convolutional neural networks to generate them. In order to create a personalized talking head model, these works require training on a large dataset of images of a single person. However, in many practical scenarios, such personalized talking head models need to be learned from a few image views of a person, potentially even a single image. Here, we present a system with such few-shot capability. It performs lengthy meta-learning on a large dataset of videos, and after that is able to frame few- and one-shot learning of neural talking head models of previously unseen people as adversarial training problems with high capacity generators and discriminators. Crucially, the system is able to initialize the parameters of both the generator and the discriminator in a person-specific way, so that training can be based on just a few images and done quickly, despite the need to tune tens of millions of parameters. We show that such an approach is able to learn highly realistic and personalized talking head models of new people and even portrait paintings.



### Multitask Learning of Temporal Connectionism in Convolutional Networks using a Joint Distribution Loss Function to Simultaneously Identify Tools and Phase in Surgical Videos
- **Arxiv ID**: http://arxiv.org/abs/1905.08315v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.08315v2)
- **Published**: 2019-05-20 19:42:40+00:00
- **Updated**: 2019-05-25 16:38:08+00:00
- **Authors**: Shanka Subhra Mondal, Rachana Sathish, Debdoot Sheet
- **Comment**: 15 pages, 8 figures, 5th MedImage Workshop of 11th Indian Conference
  on Computer Vision, Graphics and Image Processing, Hyderabad, India, 2018
- **Journal**: None
- **Summary**: Surgical workflow analysis is of importance for understanding onset and persistence of surgical phases and individual tool usage across surgery and in each phase. It is beneficial for clinical quality control and to hospital administrators for understanding surgery planning. Video acquired during surgery typically can be leveraged for this task. Currently, a combination of convolutional neural network (CNN) and recurrent neural networks (RNN) are popularly used for video analysis in general, not only being restricted to surgical videos. In this paper, we propose a multi-task learning framework using CNN followed by a bi-directional long short term memory (Bi-LSTM) to learn to encapsulate both forward and backward temporal dependencies. Further, the joint distribution indicating set of tools associated with a phase is used as an additional loss during learning to correct for their co-occurrence in any predictions. Experimental evaluation is performed using the Cholec80 dataset. We report a mean average precision (mAP) score of 0.99 and 0.86 for tool and phase identification respectively which are higher compared to prior-art in the field.



### A Bi-Directional Co-Design Approach to Enable Deep Learning on IoT Devices
- **Arxiv ID**: http://arxiv.org/abs/1905.08369v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08369v1)
- **Published**: 2019-05-20 22:36:38+00:00
- **Updated**: 2019-05-20 22:36:38+00:00
- **Authors**: Xiaofan Zhang, Cong Hao, Yuhong Li, Yao Chen, Jinjun Xiong, Wen-mei Hwu, Deming Chen
- **Comment**: Accepted by the ICML 2019 Workshop on On-Device Machine Learning &
  Compact Deep Neural Network Representations (ODML-CDNNR)
- **Journal**: None
- **Summary**: Developing deep learning models for resource-constrained Internet-of-Things (IoT) devices is challenging, as it is difficult to achieve both good quality of results (QoR), such as DNN model inference accuracy, and quality of service (QoS), such as inference latency, throughput, and power consumption. Existing approaches typically separate the DNN model development step from its deployment on IoT devices, resulting in suboptimal solutions. In this paper, we first introduce a few interesting but counterintuitive observations about such a separate design approach, and empirically show why it may lead to suboptimal designs. Motivated by these observations, we then propose a novel and practical bi-directional co-design approach: a bottom-up DNN model design strategy together with a top-down flow for DNN accelerator design. It enables a joint optimization of both DNN models and their deployment configurations on IoT devices as represented as FPGAs. We demonstrate the effectiveness of the proposed co-design approach on a real-life object detection application using Pynq-Z1 embedded FPGA. Our method obtains the state-of-the-art results on both QoR with high accuracy (IoU) and QoS with high throughput (FPS) and high energy efficiency.



