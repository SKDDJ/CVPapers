# Arxiv Papers in cs.CV on 2019-05-21
### Convolutions on Spherical Images
- **Arxiv ID**: http://arxiv.org/abs/1905.08409v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08409v1)
- **Published**: 2019-05-21 02:38:36+00:00
- **Updated**: 2019-05-21 02:38:36+00:00
- **Authors**: Marc Eder, Jan-Michael Frahm
- **Comment**: Oral presentation at the 2019 SUMO Workshop on 360{\deg} Indoor Scene
  Understanding and Modeling at CVPR2019, in Proceedings of the IEEE Conference
  on Computer Vision and Pattern Recognition Workshops. 2019
- **Journal**: None
- **Summary**: Applying convolutional neural networks to spherical images requires particular considerations. We look to the millennia of work on cartographic map projections to provide the tools to define an optimal representation of spherical images for the convolution operation. We propose a representation for deep spherical image inference based on the icosahedral Snyder equal-area (ISEA) projection, a projection onto a geodesic grid, and show that it vastly exceeds the state-of-the-art for convolution on spherical images, improving semantic segmentation results by 12.6%.



### Dual-branch residual network for lung nodule segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.08413v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.08413v1)
- **Published**: 2019-05-21 02:57:56+00:00
- **Updated**: 2019-05-21 02:57:56+00:00
- **Authors**: Haichao Cao, Hong Liu, Enmin Song, Chih-Cheng Hung, Guangzhi Ma, Xiangyang Xu, Renchao Jin, Jianguo Lu
- **Comment**: 24 pages, 6 figures
- **Journal**: None
- **Summary**: An accurate segmentation of lung nodules in computed tomography (CT) images is critical to lung cancer analysis and diagnosis. However, due to the variety of lung nodules and the similarity of visual characteristics between nodules and their surroundings, a robust segmentation of nodules becomes a challenging problem. In this study, we propose the Dual-branch Residual Network (DB-ResNet) which is a data-driven model. Our approach integrates two new schemes to improve the generalization capability of the model: 1) the proposed model can simultaneously capture multi-view and multi-scale features of different nodules in CT images; 2) we combine the features of the intensity and the convolution neural networks (CNN). We propose a pooling method, called the central intensity-pooling layer (CIP), to extract the intensity features of the center voxel of the block, and then use the CNN to obtain the convolutional features of the center voxel of the block. In addition, we designed a weighted sampling strategy based on the boundary of nodules for the selection of those voxels using the weighting score, to increase the accuracy of the model. The proposed method has been extensively evaluated on the LIDC dataset containing 986 nodules. Experimental results show that the DB-ResNet achieves superior segmentation performance with an average dice score of 82.74% on the dataset. Moreover, we compared our results with those of four radiologists on the same dataset. The comparison showed that our average dice score was 0.49% higher than that of human experts. This proves that our proposed method is as good as the experienced radiologist.



### A novel algorithm for segmentation of leukocytes in peripheral blood
- **Arxiv ID**: http://arxiv.org/abs/1905.08416v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08416v1)
- **Published**: 2019-05-21 03:04:14+00:00
- **Updated**: 2019-05-21 03:04:14+00:00
- **Authors**: Haichao Cao, Hong Liu, Enmin Song
- **Comment**: 25 pages, 10 figures
- **Journal**: Biomedical Signal Processing and Control, 2018, 45:10-21
- **Summary**: In the detection of anemia, leukemia and other blood diseases, the number and type of leukocytes are essential evaluation parameters. However, the conventional leukocyte counting method is not only quite time-consuming but also error-prone. Consequently, many automation methods are introduced for the diagnosis of medical images. It remains difficult to accurately extract related features and count the number of cells under the variable conditions such as background, staining method, staining degree, light conditions and so on. Therefore, in order to adapt to various complex situations, we consider RGB color space, HSI color space, and the linear combination of G, H and S components, and propose a fast and accurate algorithm for the segmentation of peripheral blood leukocytes in this paper. First, the nucleus of leukocyte was separated by using the stepwise averaging method. Then based on the interval-valued fuzzy sets, the cytoplasm of leukocyte was segmented by minimizing the fuzzy divergence. Next, post-processing was carried out by using the concave-convex iterative repair algorithm and the decision mechanism of candidate mask sets. Experimental results show that the proposed method outperforms the existing non-fuzzy sets methods. Among the methods based on fuzzy sets, the interval-valued fuzzy sets perform slightly better than interval-valued intuitionistic fuzzy sets and intuitionistic fuzzy sets.



### Clustering with Similarity Preserving
- **Arxiv ID**: http://arxiv.org/abs/1905.08419v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.08419v1)
- **Published**: 2019-05-21 03:11:30+00:00
- **Updated**: 2019-05-21 03:11:30+00:00
- **Authors**: Zhao Kang, Honghui Xu, Boyu Wang, Hongyuan Zhu, Zenglin Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Graph-based clustering has shown promising performance in many tasks. A key step of graph-based approach is the similarity graph construction. In general, learning graph in kernel space can enhance clustering accuracy due to the incorporation of nonlinearity. However, most existing kernel-based graph learning mechanisms is not similarity-preserving, hence leads to sub-optimal performance. To overcome this drawback, we propose a more discriminative graph learning method which can preserve the pairwise similarities between samples in an adaptive manner for the first time. Specifically, we require the learned graph be close to a kernel matrix, which serves as a measure of similarity in raw data. Moreover, the structure is adaptively tuned so that the number of connected components of the graph is exactly equal to the number of clusters. Finally, our method unifies clustering and graph learning which can directly obtain cluster indicators from the graph itself without performing further clustering step. The effectiveness of this approach is examined on both single and multiple kernel learning scenarios in several datasets.



### S-Flow GAN
- **Arxiv ID**: http://arxiv.org/abs/1905.08474v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08474v2)
- **Published**: 2019-05-21 07:55:46+00:00
- **Updated**: 2019-09-25 08:15:13+00:00
- **Authors**: Yakov Miron, Yona Coscas
- **Comment**: None
- **Journal**: None
- **Summary**: Our work offers a new method for domain translation from semantic label maps and Computer Graphic (CG) simulation edge map images to photo-realistic images. We train a Generative Adversarial Network (GAN) in a conditional way to generate a photo-realistic version of a given CG scene. Existing architectures of GANs still lack the photo-realism capabilities needed to train DNNs for computer vision tasks, we address this issue by embedding edge maps, and training it in an adversarial mode. We also offer an extension to our model that uses our GAN architecture to create visually appealing and temporally coherent videos.



### PDH : Probabilistic deep hashing based on MAP estimation of Hamming distance
- **Arxiv ID**: http://arxiv.org/abs/1905.08501v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.08501v1)
- **Published**: 2019-05-21 08:51:02+00:00
- **Updated**: 2019-05-21 08:51:02+00:00
- **Authors**: Yosuke Kaga, Masakazu Fujio, Kenta Takahashi, Tetsushi Ohki, Masakatsu Nishigaki
- **Comment**: Accepted by the 26th IEEE International Conference on Image
  Processing(ICIP2019)
- **Journal**: 2019 26th IEEE International Conference on Image Processing (ICIP)
- **Summary**: With the growth of image on the web, research on hashing which enables high-speed image retrieval has been actively studied. In recent years, various hashing methods based on deep neural networks have been proposed and achieved higher precision than the other hashing methods. In these methods, multiple losses for hash codes and the parameters of neural networks are defined. They generate hash codes that minimize the weighted sum of the losses. Therefore, an expert has to tune the weights for the losses heuristically, and the probabilistic optimality of the loss function cannot be explained. In order to generate explainable hash codes without weight tuning, we theoretically derive a single loss function with no hyperparameters for the hash code from the probability distribution of the images. By generating hash codes that minimize this loss function, highly accurate image retrieval with probabilistic optimality is performed. We evaluate the performance of hashing using MNIST, CIFAR-10, SVHN and show that the proposed method outperforms the state-of-the-art hashing methods.



### Mesh-based Camera Pairs Selection and Occlusion-Aware Masking for Mesh Refinement
- **Arxiv ID**: http://arxiv.org/abs/1905.08502v1
- **DOI**: 10.1016/j.patrec.2019.05.006
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08502v1)
- **Published**: 2019-05-21 08:51:15+00:00
- **Updated**: 2019-05-21 08:51:15+00:00
- **Authors**: Andrea Romanoni, Matteo Matteucci
- **Comment**: Accepted for publication in Pattern Recognition Letters
- **Journal**: None
- **Summary**: Many Multi-View-Stereo algorithms extract a 3D mesh model of a scene, after fusing depth maps into a volumetric representation of the space. Due to the limited scalability of such representations, the estimated model does not capture fine details of the scene. Therefore a mesh refinement algorithm is usually applied; it improves the mesh resolution and accuracy by minimizing the photometric error induced by the 3D model into pairs of cameras. The choice of these pairs significantly affects the quality of the refinement and usually relies on sparse 3D points belonging to the surface. Instead, in this paper, to increase the quality of pairs selection, we exploit the 3D model (before the refinement) to compute five metrics: scene coverage, mutual image overlap, image resolution, camera parallax, and a new symmetry term. To improve the refinement robustness, we also propose an explicit method to manage occlusions, which may negatively affect the computation of the photometric error. The proposed method takes into account the depth of the model while computing the similarity measure and its gradient. We quantitatively and qualitatively validated our approach on publicly available datasets against state of the art reconstruction methods.



### Mutual Information Maximization in Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.08509v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.08509v4)
- **Published**: 2019-05-21 09:15:10+00:00
- **Updated**: 2020-03-24 02:54:08+00:00
- **Authors**: Xinhan Di, Pengqian Yu, Rui Bu, Mingchao Sun
- **Comment**: Accepted for presentation at IJCNN 2020
- **Journal**: None
- **Summary**: A variety of graph neural networks (GNNs) frameworks for representation learning on graphs have been recently developed. These frameworks rely on aggregation and iteration scheme to learn the representation of nodes. However, information between nodes is inevitably lost in the scheme during learning. In order to reduce the loss, we extend the GNNs frameworks by exploring the aggregation and iteration scheme in the methodology of mutual information. We propose a new approach of enlarging the normal neighborhood in the aggregation of GNNs, which aims at maximizing mutual information. Based on a series of experiments conducted on several benchmark datasets, we show that the proposed approach improves the state-of-the-art performance for four types of graph tasks, including supervised and semi-supervised graph classification, graph link prediction and graph edge generation and classification.



### Machine Learning Methods for Shark Detection
- **Arxiv ID**: http://arxiv.org/abs/1905.13309v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.13309v1)
- **Published**: 2019-05-21 09:34:58+00:00
- **Updated**: 2019-05-21 09:34:58+00:00
- **Authors**: Jordan F. Masakuna
- **Comment**: 19 figures, 31 pages. This is a project for masters degree
- **Journal**: None
- **Summary**: This essay reviews human observer-based methods employed in shark spotting in Muizenberg Beach. It investigates Machine Learning methods for automated shark detection with the aim of enhancing human observation. A questionnaire and interview were used to collect information about shark spotting, the motivation of the actual Shark Spotter program and its limitations. We have defined a list of desirable properties for our model and chosen the adequate mathematical techniques. The preliminary results of the research show that we can expect to extract useful information from shark images despite the geometric transformations that sharks perform, its features do not change. To conclude, we have partially implemented our model; the remaining implementation requires dataset.



### A Two-stage Classification Method for High-dimensional Data and Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1905.08538v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.08538v1)
- **Published**: 2019-05-21 10:45:59+00:00
- **Updated**: 2019-05-21 10:45:59+00:00
- **Authors**: Xiaohao Cai, Raymond Chan, Xiaoyu Xie, Tieyong Zeng
- **Comment**: 21 pages, 4 figures
- **Journal**: None
- **Summary**: High-dimensional data classification is a fundamental task in machine learning and imaging science. In this paper, we propose a two-stage multiphase semi-supervised classification method for classifying high-dimensional data and unstructured point clouds. To begin with, a fuzzy classification method such as the standard support vector machine is used to generate a warm initialization. We then apply a two-stage approach named SaT (smoothing and thresholding) to improve the classification. In the first stage, an unconstraint convex variational model is implemented to purify and smooth the initialization, followed by the second stage which is to project the smoothed partition obtained at stage one to a binary partition. These two stages can be repeated, with the latest result as a new initialization, to keep improving the classification quality. We show that the convex model of the smoothing stage has a unique solution and can be solved by a specifically designed primal-dual algorithm whose convergence is guaranteed. We test our method and compare it with the state-of-the-art methods on several benchmark data sets. The experimental results demonstrate clearly that our method is superior in both the classification accuracy and computation speed for high-dimensional data and point clouds.



### Contrast Enhancement of Medical X-Ray Image Using Morphological Operators with Optimal Structuring Element
- **Arxiv ID**: http://arxiv.org/abs/1905.08545v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.08545v1)
- **Published**: 2019-05-21 11:01:27+00:00
- **Updated**: 2019-05-21 11:01:27+00:00
- **Authors**: Rafsanjany Kushol, Md. Nishat Raihan, Md Sirajus Salekin, A. B. M. Ashikur Rahman
- **Comment**: 5 pages, 4 figures, conference paper
- **Journal**: None
- **Summary**: To guide surgical and medical treatment X-ray images have been used by physicians in every modern healthcare organization and hospitals. Doctor's evaluation process and disease identification in the area of skeletal system can be performed in a faster and efficient way with the help of X-ray imaging technique as they can depict bone structure painlessly. This paper presents an efficient contrast enhancement technique using morphological operators which will help to visualize important bone segments and soft tissues more clearly. Top-hat and Bottom-hat transform are utilized to enhance the image where gradient magnitude value is calculated for automatically selecting the structuring element (SE) size. Experimental evaluation on different x-ray imaging databases shows the effectiveness of our method which also produces comparatively better output against some existing image enhancement techniques.



### Online Signature Verification Based on Writer Specific Feature Selection and Fuzzy Similarity Measure
- **Arxiv ID**: http://arxiv.org/abs/1905.08574v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08574v1)
- **Published**: 2019-05-21 12:19:12+00:00
- **Updated**: 2019-05-21 12:19:12+00:00
- **Authors**: Chandra Sekhar V, Prerana Mukherjee, D. S. Guru, Viswanath Pulabaigari
- **Comment**: accepted in Applications of Computer Vision and Pattern Recognition
  to Media Forensics, CVPRW, 2019, Long Beach, California
- **Journal**: None
- **Summary**: Online Signature Verification (OSV) is a widely used biometric attribute for user behavioral characteristic verification in digital forensics. In this manuscript, owing to large intra-individual variability, a novel method for OSV based on an interval symbolic representation and a fuzzy similarity measure grounded on writer specific parameter selection is proposed. The two parameters, namely, writer specific acceptance threshold and optimal feature set to be used for authenticating the writer are selected based on minimum equal error rate (EER) attained during parameter fixation phase using the training signature samples. This is in variation to current techniques for OSV, which are primarily writer independent, in which a common set of features and acceptance threshold are chosen. To prove the robustness of our system, we have exhaustively assessed our system with four standard datasets i.e. MCYT-100 (DB1), MCYT-330 (DB2), SUSIG-Visual corpus and SVC-2004- Task2. Experimental outcome confirms the effectiveness of fuzzy similarity metric-based writer dependent parameter selection for OSV by achieving a lower error rate as compared to many recent and state-of-the art OSV models.



### Borrow from Anywhere: Pseudo Multi-modal Object Detection in Thermal Imagery
- **Arxiv ID**: http://arxiv.org/abs/1905.08789v2
- **DOI**: 10.1109/CVPRW.2019.00135
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08789v2)
- **Published**: 2019-05-21 12:24:40+00:00
- **Updated**: 2020-07-15 15:50:35+00:00
- **Authors**: Chaitanya Devaguptapu, Ninad Akolekar, Manuj M Sharma, Vineeth N Balasubramanian
- **Comment**: Accepted at Perception Beyond Visible Spectrum Workshop, CVPR 2019
- **Journal**: None
- **Summary**: Can we improve detection in the thermal domain by borrowing features from rich domains like visual RGB? In this paper, we propose a pseudo-multimodal object detector trained on natural image domain data to help improve the performance of object detection in thermal images. We assume access to a large-scale dataset in the visual RGB domain and relatively smaller dataset (in terms of instances) in the thermal domain, as is common today. We propose the use of well-known image-to-image translation frameworks to generate pseudo-RGB equivalents of a given thermal image and then use a multi-modal architecture for object detection in the thermal image. We show that our framework outperforms existing benchmarks without the explicit need for paired training examples from the two domains. We also show that our framework has the ability to learn with less data from thermal domain when using our approach. Our code and pre-trained models are made available at https://github.com/tdchaitanya/MMTOD



### Marginalized Average Attentional Network for Weakly-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.08586v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08586v1)
- **Published**: 2019-05-21 12:49:22+00:00
- **Updated**: 2019-05-21 12:49:22+00:00
- **Authors**: Yuan Yuan, Yueming Lyu, Xi Shen, Ivor W. Tsang, Dit-Yan Yeung
- **Comment**: None
- **Journal**: None
- **Summary**: In weakly-supervised temporal action localization, previous works have failed to locate dense and integral regions for each entire action due to the overestimation of the most salient regions. To alleviate this issue, we propose a marginalized average attentional network (MAAN) to suppress the dominant response of the most salient regions in a principled manner. The MAAN employs a novel marginalized average aggregation (MAA) module and learns a set of latent discriminative probabilities in an end-to-end fashion. MAA samples multiple subsets from the video snippet features according to a set of latent discriminative probabilities and takes the expectation over all the averaged subset features. Theoretically, we prove that the MAA module with learned latent discriminative probabilities successfully reduces the difference in responses between the most salient regions and the others. Therefore, MAAN is able to generate better class activation sequences and identify dense and integral action regions in the videos. Moreover, we propose a fast algorithm to reduce the complexity of constructing MAA from O($2^T$) to O($T^2$). Extensive experiments on two large-scale video datasets show that our MAAN achieves superior performance on weakly-supervised temporal action localization



### SharpNet: Fast and Accurate Recovery of Occluding Contours in Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1905.08598v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08598v3)
- **Published**: 2019-05-21 13:08:52+00:00
- **Updated**: 2019-11-11 23:32:10+00:00
- **Authors**: Michaël Ramamonjisoa, Vincent Lepetit
- **Comment**: Accepted at ICCV "3D Reconstruction in the Wild" workshop
- **Journal**: 2019 International Conference on Computer Vision (ICCV) Workshops
- **Summary**: We introduce SharpNet, a method that predicts an accurate depth map for an input color image, with a particular attention to the reconstruction of occluding contours: Occluding contours are an important cue for object recognition, and for realistic integration of virtual objects in Augmented Reality, but they are also notoriously difficult to reconstruct accurately. For example, they are a challenge for stereo-based reconstruction methods, as points around an occluding contour are visible in only one image. Inspired by recent methods that introduce normal estimation to improve depth prediction, we introduce a novel term that constrains depth and occluding contours predictions. Since ground truth depth is difficult to obtain with pixel-perfect accuracy along occluding contours, we use synthetic images for training, followed by fine-tuning on real data. We demonstrate our approach on the challenging NYUv2-Depth dataset, and show that our method outperforms the state-of-the-art along occluding contours, while performing on par with the best recent methods for the rest of the images. Its accuracy along the occluding contours is actually better than the `ground truth' acquired by a depth camera based on structured light. We show this by introducing a new benchmark based on NYUv2-Depth for evaluating occluding contours in monocular reconstruction, which is our second contribution.



### RASNet: Segmentation for Tracking Surgical Instruments in Surgical Videos Using Refined Attention Segmentation Network
- **Arxiv ID**: http://arxiv.org/abs/1905.08663v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08663v2)
- **Published**: 2019-05-21 14:25:36+00:00
- **Updated**: 2019-09-19 11:34:23+00:00
- **Authors**: Zhen-Liang Ni, Gui-Bin Bian, Xiao-Liang Xie, Zeng-Guang Hou, Xiao-Hu Zhou, Yan-Jie Zhou
- **Comment**: This paper has been accepted by 2019 41st Annual International
  Conference of the IEEE Engineering in Medicine &Biology Society (EMBC)
- **Journal**: None
- **Summary**: Segmentation for tracking surgical instruments plays an important role in robot-assisted surgery. Segmentation of surgical instruments contributes to capturing accurate spatial information for tracking. In this paper, a novel network, Refined Attention Segmentation Network, is proposed to simultaneously segment surgical instruments and identify their categories. The U-shape network which is popular in segmentation is used. Different from previous work, an attention module is adopted to help the network focus on key regions, which can improve the segmentation accuracy. To solve the class imbalance problem, the weighted sum of the cross entropy loss and the logarithm of the Jaccard index is used as loss function. Furthermore, transfer learning is adopted in our network. The encoder is pre-trained on ImageNet. The dataset from the MICCAI EndoVis Challenge 2017 is used to evaluate our network. Based on this dataset, our network achieves state-of-the-art performance 94.65% mean Dice and 90.33% mean IOU.



### Improved Optical Flow for Gesture-based Human-robot Interaction
- **Arxiv ID**: http://arxiv.org/abs/1905.08685v1
- **DOI**: 10.1109/ICRA.2019.8793825
- **Categories**: **cs.HC**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1905.08685v1)
- **Published**: 2019-05-21 15:03:31+00:00
- **Updated**: 2019-05-21 15:03:31+00:00
- **Authors**: Jen-Yen Chang, Antonio Tejero-de-Pablos, Tatsuya Harada
- **Comment**: Accepted by ICRA 2019 on Jan 31 2019
- **Journal**: None
- **Summary**: Gesture interaction is a natural way of communicating with a robot as an alternative to speech. Gesture recognition methods leverage optical flow in order to understand human motion. However, while accurate optical flow estimation (i.e., traditional) methods are costly in terms of runtime, fast estimation (i.e., deep learning) methods' accuracy can be improved. In this paper, we present a pipeline for gesture-based human-robot interaction that uses a novel optical flow estimation method in order to achieve an improved speed-accuracy trade-off. Our optical flow estimation method introduces four improvements to previous deep learning-based methods: strong feature extractors, attention to contours, midway features, and a combination of these three. This results in a better understanding of motion, and a finer representation of silhouettes. In order to evaluate our pipeline, we generated our own dataset, MIBURI, which contains gestures to command a house service robot. In our experiments, we show how our method improves not only optical flow estimation, but also gesture recognition, offering a speed-accuracy trade-off more realistic for practical robot applications.



### GAPNet: Graph Attention based Point Neural Network for Exploiting Local Feature of Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/1905.08705v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08705v1)
- **Published**: 2019-05-21 15:44:31+00:00
- **Updated**: 2019-05-21 15:44:31+00:00
- **Authors**: Can Chen, Luca Zanotti Fragonara, Antonios Tsourdos
- **Comment**: None
- **Journal**: None
- **Summary**: Exploiting fine-grained semantic features on point cloud is still challenging due to its irregular and sparse structure in a non-Euclidean space. Among existing studies, PointNet provides an efficient and promising approach to learn shape features directly on unordered 3D point cloud and has achieved competitive performance. However, local feature that is helpful towards better contextual learning is not considered. Meanwhile, attention mechanism shows efficiency in capturing node representation on graph-based data by attending over neighboring nodes. In this paper, we propose a novel neural network for point cloud, dubbed GAPNet, to learn local geometric representations by embedding graph attention mechanism within stacked Multi-Layer-Perceptron (MLP) layers. Firstly, we introduce a GAPLayer to learn attention features for each point by highlighting different attention weights on neighborhood. Secondly, in order to exploit sufficient features, a multi-head mechanism is employed to allow GAPLayer to aggregate different features from independent heads. Thirdly, we propose an attention pooling layer over neighbors to capture local signature aimed at enhancing network robustness. Finally, GAPNet applies stacked MLP layers to attention features and local signature to fully extract local geometric structures. The proposed GAPNet architecture is tested on the ModelNet40 and ShapeNet part datasets, and achieves state-of-the-art performance in both shape classification and part segmentation tasks.



### Lightweight Network Architecture for Real-Time Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1905.08711v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.08711v1)
- **Published**: 2019-05-21 15:50:24+00:00
- **Updated**: 2019-05-21 15:50:24+00:00
- **Authors**: Alexander Kozlov, Vadim Andronov, Yana Gritsenko
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: In this work we present a new efficient approach to Human Action Recognition called Video Transformer Network (VTN). It leverages the latest advances in Computer Vision and Natural Language Processing and applies them to video understanding. The proposed method allows us to create lightweight CNN models that achieve high accuracy and real-time speed using just an RGB mono camera and general purpose CPU. Furthermore, we explain how to improve accuracy by distilling from multiple models with different modalities into a single model. We conduct a comparison with state-of-the-art methods and show that our approach performs on par with most of them on famous Action Recognition datasets. We benchmark the inference time of the models using the modern inference framework and argue that our approach compares favorably with other methods in terms of speed/accuracy trade-off, running at 56 FPS on CPU. The models and the training code are available.



### Task Decomposition and Synchronization for Semantic Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.08720v2
- **DOI**: 10.1109/TIP.2020.3003735
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.08720v2)
- **Published**: 2019-05-21 15:59:04+00:00
- **Updated**: 2019-06-22 09:01:52+00:00
- **Authors**: Xuhua Ren, Lichi Zhang, Sahar Ahmad, Dong Nie, Fan Yang, Lei Xiang, Qian Wang, Dinggang Shen
- **Comment**: IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Semantic segmentation is essentially important to biomedical image analysis. Many recent works mainly focus on integrating the Fully Convolutional Network (FCN) architecture with sophisticated convolution implementation and deep supervision. In this paper, we propose to decompose the single segmentation task into three subsequent sub-tasks, including (1) pixel-wise image segmentation, (2) prediction of the class labels of the objects within the image, and (3) classification of the scene the image belonging to. While these three sub-tasks are trained to optimize their individual loss functions of different perceptual levels, we propose to let them interact by the task-task context ensemble. Moreover, we propose a novel sync-regularization to penalize the deviation between the outputs of the pixel-wise segmentation and the class prediction tasks. These effective regularizations help FCN utilize context information comprehensively and attain accurate semantic segmentation, even though the number of the images for training may be limited in many biomedical applications. We have successfully applied our framework to three diverse 2D/3D medical image datasets, including Robotic Scene Segmentation Challenge 18 (ROBOT18), Brain Tumor Segmentation Challenge 18 (BRATS18), and Retinal Fundus Glaucoma Challenge (REFUGE18). We have achieved top-tier performance in all three challenges.



### Medical Imaging with Deep Learning: MIDL 2019 -- Extended Abstract Track
- **Arxiv ID**: http://arxiv.org/abs/1907.08612v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.08612v2)
- **Published**: 2019-05-21 16:27:29+00:00
- **Updated**: 2019-07-22 13:14:56+00:00
- **Authors**: M. Jorge Cardoso, Aasa Feragen, Ben Glocker, Ender Konukoglu, Ipek Oguz, Gozde Unal, Tom Vercauteren
- **Comment**: Accepted extended abstracts can also be found at
  https://openreview.net/group?id=MIDL.io/2019/Conference#abstract-accept-papers
- **Journal**: None
- **Summary**: This compendium gathers all the accepted extended abstracts from the Second International Conference on Medical Imaging with Deep Learning (MIDL 2019), held in London, UK, 8-10 July 2019. Note that only accepted extended abstracts are listed here, the Proceedings of the MIDL 2019 Full Paper Track are published as Volume 102 of the Proceedings of Machine Learning Research (PMLR) http://proceedings.mlr.press/v102/.



### RIU-Net: Embarrassingly simple semantic segmentation of 3D LiDAR point cloud
- **Arxiv ID**: http://arxiv.org/abs/1905.08748v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08748v3)
- **Published**: 2019-05-21 16:51:35+00:00
- **Updated**: 2019-06-17 08:15:03+00:00
- **Authors**: Pierre Biasutti, Aurélie Bugeau, Jean-François Aujol, Mathieu Brédif
- **Comment**: This version of the article contains revised scores to match the
  evaluation process presented in SqueezeSegV2
- **Journal**: None
- **Summary**: This paper proposes RIU-Net (for Range-Image U-Net), the adaptation of a popular semantic segmentation network for the semantic segmentation of a 3D LiDAR point cloud. The point cloud is turned into a 2D range-image by exploiting the topology of the sensor. This image is then used as input to a U-net. This architecture has already proved its efficiency for the task of semantic segmentation of medical images. We demonstrate how it can also be used for the accurate semantic segmentation of a 3D LiDAR point cloud and how it represents a valid bridge between image processing and 3D point cloud processing. Our model is trained on range-images built from KITTI 3D object detection dataset. Experiments show that RIU-Net, despite being very simple, offers results that are comparable to the state-of-the-art of range-image based methods. Finally, we demonstrate that this architecture is able to operate at 90fps on a single GPU, which enables deployment for real-time segmentation.



### Toward Learning a Unified Many-to-Many Mapping for Diverse Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1905.08766v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08766v1)
- **Published**: 2019-05-21 17:35:57+00:00
- **Updated**: 2019-05-21 17:35:57+00:00
- **Authors**: Wenju Xu, Shawn Keshmiri, Guanghui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Image-to-image translation, which translates input images to a different domain with a learned one-to-one mapping, has achieved impressive success in recent years. The success of translation mainly relies on the network architecture to reserve the structural information while modify the appearance slightly at the pixel level through adversarial training. Although these networks are able to learn the mapping, the translated images are predictable without exclusion. It is more desirable to diversify them using image-to-image translation by introducing uncertainties, i.e., the generated images hold potential for variations in colors and textures in addition to the general similarity to the input images, and this happens in both the target and source domains. To this end, we propose a novel generative adversarial network (GAN) based model, InjectionGAN, to learn a many-to-many mapping. In this model, the input image is combined with latent variables, which comprise of domain-specific attribute and unspecific random variations. The domain-specific attribute indicates the target domain of the translation, while the unspecific random variations introduce uncertainty into the model. A unified framework is proposed to regroup these two parts and obtain diverse generations in each domain. Extensive experiments demonstrate that the diverse generations have high quality for the challenging image-to-image translation tasks where no pairing information of the training dataset exits. Both quantitative and qualitative results prove the superior performance of InjectionGAN over the state-of-the-art approaches.



### Textured Neural Avatars
- **Arxiv ID**: http://arxiv.org/abs/1905.08776v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.08776v1)
- **Published**: 2019-05-21 17:46:16+00:00
- **Updated**: 2019-05-21 17:46:16+00:00
- **Authors**: Aliaksandra Shysheya, Egor Zakharov, Kara-Ali Aliev, Renat Bashirov, Egor Burkov, Karim Iskakov, Aleksei Ivakhnenko, Yury Malkov, Igor Pasechnik, Dmitry Ulyanov, Alexander Vakhitov, Victor Lempitsky
- **Comment**: None
- **Journal**: None
- **Summary**: We present a system for learning full-body neural avatars, i.e. deep networks that produce full-body renderings of a person for varying body pose and camera position. Our system takes the middle path between the classical graphics pipeline and the recent deep learning approaches that generate images of humans using image-to-image translation. In particular, our system estimates an explicit two-dimensional texture map of the model surface. At the same time, it abstains from explicit shape modeling in 3D. Instead, at test time, the system uses a fully-convolutional network to directly map the configuration of body feature points w.r.t. the camera to the 2D texture coordinates of individual pixels in the image frame. We show that such a system is capable of learning to generate realistic renderings while being trained on videos annotated with 3D poses and foreground masks. We also demonstrate that maintaining an explicit texture representation helps our system to achieve better generalization compared to systems that use direct image-to-image translation.



### Semi-Supervised Learning with Scarce Annotations
- **Arxiv ID**: http://arxiv.org/abs/1905.08845v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.08845v2)
- **Published**: 2019-05-21 19:41:42+00:00
- **Updated**: 2020-04-21 21:20:30+00:00
- **Authors**: Sylvestre-Alvise Rebuffi, Sebastien Ehrhardt, Kai Han, Andrea Vedaldi, Andrew Zisserman
- **Comment**: Workshop on Deep Vision, CVPR 2020
- **Journal**: None
- **Summary**: While semi-supervised learning (SSL) algorithms provide an efficient way to make use of both labelled and unlabelled data, they generally struggle when the number of annotated samples is very small. In this work, we consider the problem of SSL multi-class classification with very few labelled instances. We introduce two key ideas. The first is a simple but effective one: we leverage the power of transfer learning among different tasks and self-supervision to initialize a good representation of the data without making use of any label. The second idea is a new algorithm for SSL that can exploit well such a pre-trained representation.   The algorithm works by alternating two phases, one fitting the labelled points and one fitting the unlabelled ones, with carefully-controlled information flow between them. The benefits are greatly reducing overfitting of the labelled data and avoiding issue with balancing labelled and unlabelled losses during training. We show empirically that this method can successfully train competitive models with as few as 10 labelled data points per class. More in general, we show that the idea of bootstrapping features using self-supervised learning always improves SSL on standard benchmarks. We show that our algorithm works increasingly well compared to other methods when refining from other tasks or datasets.



### DoPa: A Comprehensive CNN Detection Methodology against Physical Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1905.08790v4
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.08790v4)
- **Published**: 2019-05-21 19:53:38+00:00
- **Updated**: 2019-08-28 15:07:07+00:00
- **Authors**: Zirui Xu, Fuxun Yu, Xiang Chen
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: Recently, Convolutional Neural Networks (CNNs) demonstrate a considerable vulnerability to adversarial attacks, which can be easily misled by adversarial perturbations. With more aggressive methods proposed, adversarial attacks can be also applied to the physical world, causing practical issues to various CNN powered applications. To secure CNNs, adversarial attack detection is considered as the most critical approach. However, most existing works focus on superficial patterns and merely search a particular method to differentiate the adversarial inputs and natural inputs, ignoring the analysis of CNN inner vulnerability. Therefore, they can only target to specific physical adversarial attacks, lacking expected versatility to different attacks. To address this issue, we propose DoPa -- a comprehensive CNN detection methodology for various physical adversarial attacks. By interpreting the CNN's vulnerability, we find that non-semantic adversarial perturbations can activate CNN with significantly abnormal activations and even overwhelm other semantic input patterns' activations. Therefore, we add a self-verification stage to analyze the semantics of distinguished activation patterns, which improves the CNN recognition process. We apply such a detection methodology into both image and audio CNN recognition scenarios. Experiments show that DoPa can achieve an average rate of 90% success for image attack detection and 92% success for audio attack detection.   Announcement:[The original DoPa draft on arXiv was modified and submitted to a conference already, while this short abstract was submitted only for a presentation at the KDD 2019 AIoT Workshop.]



### Efficient Plane-Based Optimization of Geometry and Texture for Indoor RGB-D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1905.08853v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1905.08853v1)
- **Published**: 2019-05-21 20:11:03+00:00
- **Updated**: 2019-05-21 20:11:03+00:00
- **Authors**: Chao Wang, Xiaohu Guo
- **Comment**: In the SUMO Workshop of CVPR 2019
- **Journal**: None
- **Summary**: We propose a novel approach to reconstruct RGB-D indoor scene based on plane primitives. Our approach takes as input a RGB-D sequence and a dense coarse mesh reconstructed from it, and generates a lightweight, low-polygonal mesh with clear face textures and sharp features without losing geometry details from the original scene. Compared to existing methods which only cover large planar regions in the scene, our method builds the entire scene by adaptive planes without losing geometry details and also preserves sharp features in the mesh. Experiments show that our method is more efficient to generate textured mesh from RGB-D data than state-of-the-arts.



### Looking to Relations for Future Trajectory Forecast
- **Arxiv ID**: http://arxiv.org/abs/1905.08855v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08855v4)
- **Published**: 2019-05-21 20:12:42+00:00
- **Updated**: 2019-08-27 17:48:18+00:00
- **Authors**: Chiho Choi, Behzad Dariush
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Inferring relational behavior between road users as well as road users and their surrounding physical space is an important step toward effective modeling and prediction of navigation strategies adopted by participants in road scenes. To this end, we propose a relation-aware framework for future trajectory forecast. Our system aims to infer relational information from the interactions of road users with each other and with the environment. The first module involves visual encoding of spatio-temporal features, which captures human-human and human-space interactions over time. The following module explicitly constructs pair-wise relations from spatio-temporal interactions and identifies more descriptive relations that highly influence future motion of the target road user by considering its past trajectory. The resulting relational features are used to forecast future locations of the target, in the form of heatmaps with an additional guidance of spatial dependencies and consideration of the uncertainty. Extensive evaluations on the public benchmark datasets demonstrate the robustness and efficacy of the proposed framework as observed by performances higher than the state-of-the-art methods.



### Automated Pupillary Light Reflex Test on a Portable Platform
- **Arxiv ID**: http://arxiv.org/abs/1905.08886v1
- **DOI**: 10.1109/ISMR.2019.8710182
- **Categories**: **eess.IV**, cs.CV, eess.SP, I.4
- **Links**: [PDF](http://arxiv.org/pdf/1905.08886v1)
- **Published**: 2019-05-21 22:15:21+00:00
- **Updated**: 2019-05-21 22:15:21+00:00
- **Authors**: Dogancan Temel, Melvin J. Mathew, Ghassan AlRegib, Yousuf M. Khalifa
- **Comment**: 7 pages, 11 figures, 3 tables
- **Journal**: International Symposium on Medical Robotics (ISMR), Atlanta, GA,
  USA, 2019, pp. 1-7
- **Summary**: In this paper, we introduce a portable eye imaging device denoted as lab-on-a-headset, which can automatically perform a swinging flashlight test. We utilized this device in a clinical study to obtain high-resolution recordings of eyes while they are exposed to a varying light stimuli. Half of the participants had relative afferent pupillary defect (RAPD) while the other half was a control group. In case of positive RAPD, patients pupils constrict less or do not constrict when light stimuli swings from the unaffected eye to the affected eye. To automatically diagnose RAPD, we propose an algorithm based on pupil localization, pupil size measurement, and pupil size comparison of right and left eye during the light reflex test. We validate the algorithmic performance over a dataset obtained from 22 subjects and show that proposed algorithm can achieve a sensitivity of 93.8% and a specificity of 87.5%.



