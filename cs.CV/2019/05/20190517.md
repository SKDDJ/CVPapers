# Arxiv Papers in cs.CV on 2019-05-17
### Dream Distillation: A Data-Independent Model Compression Framework
- **Arxiv ID**: http://arxiv.org/abs/1905.07072v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.07072v1)
- **Published**: 2019-05-17 00:28:49+00:00
- **Updated**: 2019-05-17 00:28:49+00:00
- **Authors**: Kartikeya Bhardwaj, Naveen Suda, Radu Marculescu
- **Comment**: Presented at the ICML 2019 Joint Workshop on On-Device Machine
  Learning & Compact Deep Neural Network Representations (ODML-CDNNR)
- **Journal**: None
- **Summary**: Model compression is eminently suited for deploying deep learning on IoT-devices. However, existing model compression techniques rely on access to the original or some alternate dataset. In this paper, we address the model compression problem when no real data is available, e.g., when data is private. To this end, we propose Dream Distillation, a data-independent model compression framework. Our experiments show that Dream Distillation can achieve 88.5% accuracy on the CIFAR-10 test set without actually training on the original data!



### Deep Unified Multimodal Embeddings for Understanding both Content and Users in Social Media Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.07075v3
- **DOI**: None
- **Categories**: **cs.IR**, cs.CL, cs.CV, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1905.07075v3)
- **Published**: 2019-05-17 01:16:15+00:00
- **Updated**: 2019-06-11 03:17:53+00:00
- **Authors**: Karan Sikka, Lucas Van Bramer, Ajay Divakaran
- **Comment**: Preprint submitted to IJCV
- **Journal**: None
- **Summary**: There has been an explosion of multimodal content generated on social media networks in the last few years, which has necessitated a deeper understanding of social media content and user behavior. We present a novel content-independent content-user-reaction model for social multimedia content analysis. Compared to prior works that generally tackle semantic content understanding and user behavior modeling in isolation, we propose a generalized solution to these problems within a unified framework. We embed users, images and text drawn from open social media in a common multimodal geometric space, using a novel loss function designed to cope with distant and disparate modalities, and thereby enable seamless three-way retrieval. Our model not only outperforms unimodal embedding based methods on cross-modal retrieval tasks but also shows improvements stemming from jointly solving the two tasks on Twitter data. We also show that the user embeddings learned within our joint multimodal embedding model are better at predicting user interests compared to those learned with unimodal content on Instagram data. Our framework thus goes beyond the prior practice of using explicit leader-follower link information to establish affiliations by extracting implicit content-centric affiliations from isolated users. We provide qualitative results to show that the user clusters emerging from learned embeddings have consistent semantics and the ability of our model to discover fine-grained semantics from noisy and unstructured data. Our work reveals that social multimodal content is inherently multimodal and possesses a consistent structure because in social networks meaning is created through interactions between users and content.



### Group Re-Identification with Multi-grained Matching and Integration
- **Arxiv ID**: http://arxiv.org/abs/1905.07108v2
- **DOI**: 10.1109/TCYB.2019.2917713
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.07108v2)
- **Published**: 2019-05-17 04:04:47+00:00
- **Updated**: 2019-05-26 11:52:25+00:00
- **Authors**: Weiyao Lin, Yuxi Li, Hao Xiao, John See, Junni Zou, Hongkai Xiong, Jingdong Wang, Tao Mei
- **Comment**: 14 pages, 10 figures, to appear in IEEE transaction on Cybernetics
- **Journal**: None
- **Summary**: The task of re-identifying groups of people underdifferent camera views is an important yet less-studied problem.Group re-identification (Re-ID) is a very challenging task sinceit is not only adversely affected by common issues in traditionalsingle object Re-ID problems such as viewpoint and human posevariations, but it also suffers from changes in group layout andgroup membership. In this paper, we propose a novel conceptof group granularity by characterizing a group image by multi-grained objects: individual persons and sub-groups of two andthree people within a group. To achieve robust group Re-ID,we first introduce multi-grained representations which can beextracted via the development of two separate schemes, i.e. onewith hand-crafted descriptors and another with deep neuralnetworks. The proposed representation seeks to characterize bothappearance and spatial relations of multi-grained objects, and isfurther equipped with importance weights which capture varia-tions in intra-group dynamics. Optimal group-wise matching isfacilitated by a multi-order matching process which in turn,dynamically updates the importance weights in iterative fashion.We evaluated on three multi-camera group datasets containingcomplex scenarios and large dynamics, with experimental resultsdemonstrating the effectiveness of our approach. The published dataset can be found in \url{http://min.sjtu.edu.cn/lwydemo/GroupReID.html}



### Side Window Filtering
- **Arxiv ID**: http://arxiv.org/abs/1905.07177v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.07177v1)
- **Published**: 2019-05-17 09:39:53+00:00
- **Updated**: 2019-05-17 09:39:53+00:00
- **Authors**: Hui Yin, Yuanhao Gong, Guoping Qiu
- **Comment**: 2019 CVPR
- **Journal**: None
- **Summary**: Local windows are routinely used in computer vision and almost without exception the center of the window is aligned with the pixels being processed. We show that this conventional wisdom is not universally applicable. When a pixel is on an edge, placing the center of the window on the pixel is one of the fundamental reasons that cause many filtering algorithms to blur the edges. Based on this insight, we propose a new Side Window Filtering (SWF) technique which aligns the window's side or corner with the pixel being processed. The SWF technique is surprisingly simple yet theoretically rooted and very effective in practice. We show that many traditional linear and nonlinear filters can be easily implemented under the SWF framework. Extensive analysis and experiments show that implementing the SWF principle can significantly improve their edge preserving capabilities and achieve state of the art performances in applications such as image smoothing, denoising, enhancement, structure-preserving texture-removing, mutual-structure extraction, and HDR tone mapping. In addition to image filtering, we further show that the SWF principle can be extended to other applications involving the use of a local window. Using colorization by optimization as an example, we demonstrate that implementing the SWF principle can effectively prevent artifacts such as color leakage associated with the conventional implementation. Given the ubiquity of window based operations in computer vision, the new SWF technique is likely to benefit many more applications.



### Mechanically Powered Motion Imaging Phantoms: Proof of Concept
- **Arxiv ID**: http://arxiv.org/abs/1905.07198v1
- **DOI**: 10.1109/EMBC.2019.8856577
- **Categories**: **eess.IV**, cs.CV, physics.med-ph, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/1905.07198v1)
- **Published**: 2019-05-17 11:14:03+00:00
- **Updated**: 2019-05-17 11:14:03+00:00
- **Authors**: Alberto Gomez, Cornelia Schmitz, Markus Henningsson, James Housden, Yohan Noh, Veronika A. Zimmer, James R. Clough, Ilkay Oksuz, Nicolas Toussaint, Andrew P. King, Julia A. Schnabel
- **Comment**: Accepted for publication at IEEE EMBC (41st International Engineering
  in Medicine and Biology Conference) 2019
- **Journal**: None
- **Summary**: Motion imaging phantoms are expensive, bulky and difficult to transport and set-up. The purpose of this paper is to demonstrate a simple approach to the design of multi-modality motion imaging phantoms that use mechanically stored energy to produce motion. We propose two phantom designs that use mainsprings and elastic bands to store energy. A rectangular piece was attached to an axle at the end of the transmission chain of each phantom, and underwent a rotary motion upon release of the mechanical motor. The phantoms were imaged with MRI and US, and the image sequences were embedded in a 1D non linear manifold (Laplacian Eigenmap) and the spectrogram of the embedding was used to derive the angular velocity over time. The derived velocities were consistent and reproducible within a small error. The proposed motion phantom concept showed great potential for the construction of simple and affordable motion phantoms



### Training Object Detectors With Noisy Data
- **Arxiv ID**: http://arxiv.org/abs/1905.07202v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.07202v1)
- **Published**: 2019-05-17 11:20:33+00:00
- **Updated**: 2019-05-17 11:20:33+00:00
- **Authors**: Simon Chadwick, Paul Newman
- **Comment**: None
- **Journal**: None
- **Summary**: The availability of a large quantity of labelled training data is crucial for the training of modern object detectors. Hand labelling training data is time consuming and expensive while automatic labelling methods inevitably add unwanted noise to the labels. We examine the effect of different types of label noise on the performance of an object detector. We then show how co-teaching, a method developed for handling noisy labels and previously demonstrated on a classification problem, can be improved to mitigate the effects of label noise in an object detection setting. We illustrate our results using simulated noise on the KITTI dataset and on a vehicle detection task using automatically labelled data.



### Transfer Learning based Detection of Diabetic Retinopathy from Small Dataset
- **Arxiv ID**: http://arxiv.org/abs/1905.07203v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.07203v2)
- **Published**: 2019-05-17 11:21:36+00:00
- **Updated**: 2019-05-22 07:18:21+00:00
- **Authors**: Misgina Tsighe Hagos, Shri Kant
- **Comment**: None
- **Journal**: None
- **Summary**: Annotated training data insufficiency remains to be one of the challenges of applying deep learning in medical data classification problems. Transfer learning from an already trained deep convolutional network can be used to reduce the cost of training from scratch and to train with small training data for deep learning. This raises the question of whether we can use transfer learning to overcome the training data insufficiency problem in deep learning based medical data classifications. Deep convolutional networks have been achieving high performance results on the ImageNet Large Scale Visual Recognition Competition (ILSVRC) image classification challenge. One example is the Inception-V3 model that was the first runner up on the ILSVRC 2015 challenge. Inception modules that help to extract different sized features of input images in one level of convolution are the unique features of the Inception-V3. In this work, we have used a pretrained Inception-V3 model to take advantage of its Inception modules for Diabetic Retinopathy detection. In order to tackle the labelled data insufficiency problem, we sub-sampled a smaller version of the Kaggle Diabetic Retinopathy classification challenge dataset for model training, and tested the model's accuracy on a previously unseen data subset. Our technique could be used in other deep learning based medical image classification problems facing the challenge of labeled training data insufficiency.



### Neither Global Nor Local: A Hierarchical Robust Subspace Clustering For Image Data
- **Arxiv ID**: http://arxiv.org/abs/1905.07220v1
- **DOI**: 10.1016/j.ins.2019.11.031
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.07220v1)
- **Published**: 2019-05-17 12:07:01+00:00
- **Updated**: 2019-05-17 12:07:01+00:00
- **Authors**: Maryam Abdolali, Mohammad Rahmati
- **Comment**: None
- **Journal**: Information Sciences 514, pp. 333-353, 2020
- **Summary**: In this paper, we consider the problem of subspace clustering in presence of contiguous noise, occlusion and disguise. We argue that self-expressive representation of data in current state-of-the-art approaches is severely sensitive to occlusions and complex real-world noises. To alleviate this problem, we propose a hierarchical framework that brings robustness of local patches-based representations and discriminant property of global representations together. This approach consists of 1) a top-down stage, in which the input data is subject to repeated division to smaller patches and 2) a bottom-up stage, in which the low rank embedding of local patches in field of view of a corresponding patch in upper level are merged on a Grassmann manifold. This summarized information provides two key information for the corresponding patch on the upper level: cannot-links and recommended-links. This information is employed for computing a self-expressive representation of each patch at upper levels using a weighted sparse group lasso optimization problem. Numerical results on several real data sets confirm the efficiency of our approach.



### Using Photorealistic Face Synthesis and Domain Adaptation to Improve Facial Expression Analysis
- **Arxiv ID**: http://arxiv.org/abs/1905.08090v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08090v1)
- **Published**: 2019-05-17 12:36:52+00:00
- **Updated**: 2019-05-17 12:36:52+00:00
- **Authors**: Behzad Bozorgtabar, Mohammad Saeed Rad, Hazim Kemal Ekenel, Jean-Philippe Thiran
- **Comment**: 8 pages, 8 figures, 5 tables, accepted by FG 2019. arXiv admin note:
  substantial text overlap with arXiv:1905.00286
- **Journal**: None
- **Summary**: Cross-domain synthesizing realistic faces to learn deep models has attracted increasing attention for facial expression analysis as it helps to improve the performance of expression recognition accuracy despite having small number of real training images. However, learning from synthetic face images can be problematic due to the distribution discrepancy between low-quality synthetic images and real face images and may not achieve the desired performance when the learned model applies to real world scenarios. To this end, we propose a new attribute guided face image synthesis to perform a translation between multiple image domains using a single model. In addition, we adopt the proposed model to learn from synthetic faces by matching the feature distributions between different domains while preserving each domain's characteristics. We evaluate the effectiveness of the proposed approach on several face datasets on generating realistic face images. We demonstrate that the expression recognition performance can be enhanced by benefiting from our face synthesis model. Moreover, we also conduct experiments on a near-infrared dataset containing facial expression videos of drivers to assess the performance using in-the-wild data for driver emotion recognition.



### Texture Fields: Learning Texture Representations in Function Space
- **Arxiv ID**: http://arxiv.org/abs/1905.07259v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.07259v1)
- **Published**: 2019-05-17 13:39:18+00:00
- **Updated**: 2019-05-17 13:39:18+00:00
- **Authors**: Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo Strauss, Andreas Geiger
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, substantial progress has been achieved in learning-based reconstruction of 3D objects. At the same time, generative models were proposed that can generate highly realistic images. However, despite this success in these closely related tasks, texture reconstruction of 3D objects has received little attention from the research community and state-of-the-art methods are either limited to comparably low resolution or constrained experimental setups. A major reason for these limitations is that common representations of texture are inefficient or hard to interface for modern deep learning techniques. In this paper, we propose Texture Fields, a novel texture representation which is based on regressing a continuous 3D function parameterized with a neural network. Our approach circumvents limiting factors like shape discretization and parameterization, as the proposed texture representation is independent of the shape representation of the 3D object. We show that Texture Fields are able to represent high frequency texture and naturally blend with modern deep learning techniques. Experimentally, we find that Texture Fields compare favorably to state-of-the-art methods for conditional texture reconstruction of 3D objects and enable learning of probabilistic generative models for texturing unseen 3D models. We believe that Texture Fields will become an important building block for the next generation of generative 3D models.



### A deep learning approach to detecting volcano deformation from satellite imagery using synthetic datasets
- **Arxiv ID**: http://arxiv.org/abs/1905.07286v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.07286v1)
- **Published**: 2019-05-17 14:19:30+00:00
- **Updated**: 2019-05-17 14:19:30+00:00
- **Authors**: Nantheera Anantrasirichai, Juliet Biggs, Fabien Albino, David Bull
- **Comment**: None
- **Journal**: None
- **Summary**: Satellites enable widespread, regional or global surveillance of volcanoes and can provide the first indication of volcanic unrest or eruption. Here we consider Interferometric Synthetic Aperture Radar (InSAR), which can be employed to detect surface deformation with a strong statistical link to eruption. The ability of machine learning to automatically identify signals of interest in these large InSAR datasets has already been demonstrated, but data-driven techniques, such as convolutional neutral networks (CNN) require balanced training datasets of positive and negative signals to effectively differentiate between real deformation and noise. As only a small proportion of volcanoes are deforming and atmospheric noise is ubiquitous, the use of machine learning for detecting volcanic unrest is more challenging. In this paper, we address this problem using synthetic interferograms to train the AlexNet. The synthetic interferograms are composed of 3 parts: 1) deformation patterns based on a Monte Carlo selection of parameters for analytic forward models, 2) stratified atmospheric effects derived from weather models and 3) turbulent atmospheric effects based on statistical simulations of correlated noise. The AlexNet architecture trained with synthetic data outperforms that trained using real interferograms alone, based on classification accuracy and positive predictive value (PPV). However, the models used to generate the synthetic signals are a simplification of the natural processes, so we retrain the CNN with a combined dataset consisting of synthetic models and selected real examples, achieving a final PPV of 82%. Although applying atmospheric corrections to the entire dataset is computationally expensive, it is relatively simple to apply them to the small subset of positive results. This further improves the detection performance without a significant increase in computational burden.



### CNN-based Cost Volume Analysis as Confidence Measure for Dense Matching
- **Arxiv ID**: http://arxiv.org/abs/1905.07287v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.07287v2)
- **Published**: 2019-05-17 14:24:43+00:00
- **Updated**: 2019-10-22 12:04:48+00:00
- **Authors**: Max Mehltretter, Christian Heipke
- **Comment**: The IEEE International Conference on Computer Vision (ICCV) Workshops
  (2019)
- **Journal**: None
- **Summary**: Due to its capability to identify erroneous disparity assignments in dense stereo matching, confidence estimation is beneficial for a wide range of applications, e.g. autonomous driving, which needs a high degree of confidence as mandatory prerequisite. Especially, the introduction of deep learning based methods resulted in an increasing popularity of this field in recent years, caused by a significantly improved accuracy. Despite this remarkable development, most of these methods rely on features learned from disparity maps only, not taking into account the corresponding 3-dimensional cost volumes. However, it was already demonstrated that with conventional methods based on hand-crafted features this additional information can be used to further increase the accuracy. In order to combine the advantages of deep learning and cost volume based features, in this paper, we propose a novel Convolutional Neural Network (CNN) architecture to directly learn features for confidence estimation from volumetric 3D data. An extensive evaluation on three datasets using three common dense stereo matching techniques demonstrates the generality and state-of-the-art accuracy of the proposed method.



### LiDAR Sensor modeling and Data augmentation with GANs for Autonomous driving
- **Arxiv ID**: http://arxiv.org/abs/1905.07290v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.07290v1)
- **Published**: 2019-05-17 14:30:07+00:00
- **Updated**: 2019-05-17 14:30:07+00:00
- **Authors**: Ahmad El Sallab, Ibrahim Sobh, Mohamed Zahran, Nader Essam
- **Comment**: Accepted at ICML Workshop on AI for Autonomous Driving
- **Journal**: None
- **Summary**: In the autonomous driving domain, data collection and annotation from real vehicles are expensive and sometimes unsafe. Simulators are often used for data augmentation, which requires realistic sensor models that are hard to formulate and model in closed forms. Instead, sensors models can be learned from real data. The main challenge is the absence of paired data set, which makes traditional supervised learning techniques not suitable. In this work, we formulate the problem as image translation from unpaired data and employ CycleGANs to solve the sensor modeling problem for LiDAR, to produce realistic LiDAR from simulated LiDAR (sim2real). Further, we generate high-resolution, realistic LiDAR from lower resolution one (real2real). The LiDAR 3D point cloud is processed in Bird-eye View and Polar 2D representations. The experimental results show a high potential of the proposed approach.



### Semantic Analysis of Traffic Camera Data: Topic Signal Extraction and Anomalous Event Detection
- **Arxiv ID**: http://arxiv.org/abs/1905.07332v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.07332v1)
- **Published**: 2019-05-17 15:35:54+00:00
- **Updated**: 2019-05-17 15:35:54+00:00
- **Authors**: Jeffrey Liu, Andrew Weinert, Saurabh Amin
- **Comment**: None
- **Journal**: None
- **Summary**: Traffic Management Centers (TMCs) routinely use traffic cameras to provide situational awareness regarding traffic, road, and weather conditions. Camera footage is quite useful for a variety of diagnostic purposes; yet, most footage is kept for only a few days, if at all. This is largely due to the fact that currently, identification of notable footage is done via manual review by human operators---a laborious and inefficient process. In this article, we propose a semantics-oriented approach to analyzing sequential image data, and demonstrate its application for automatic detection of real-world, anomalous events in weather and traffic conditions. Our approach constructs semantic vector representations of image contents from textual labels which can be easily obtained from off-the-shelf, pretrained image labeling software. These semantic label vectors are used to construct semantic topic signals---time series representations of physical processes---using the Latent Dirichlet Allocation (LDA) topic model. By detecting anomalies in the topic signals, we identify notable footage corresponding to winter storms and anomalous traffic congestion. In validation against real-world events, anomaly detection using semantic topic signals significantly outperforms detection using any individual label signal.



### Online Hyper-parameter Learning for Auto-Augmentation Strategy
- **Arxiv ID**: http://arxiv.org/abs/1905.07373v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.07373v2)
- **Published**: 2019-05-17 16:59:31+00:00
- **Updated**: 2019-08-14 05:58:46+00:00
- **Authors**: Chen Lin, Minghao Guo, Chuming Li, Yuan Xin, Wei Wu, Dahua Lin, Wanli Ouyang, Junjie Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation is critical to the success of modern deep learning techniques. In this paper, we propose Online Hyper-parameter Learning for Auto-Augmentation (OHL-Auto-Aug), an economical solution that learns the augmentation policy distribution along with network training. Unlike previous methods on auto-augmentation that search augmentation strategies in an offline manner, our method formulates the augmentation policy as a parameterized probability distribution, thus allowing its parameters to be optimized jointly with network parameters. Our proposed OHL-Auto-Aug eliminates the need of re-training and dramatically reduces the cost of the overall search process, while establishes significantly accuracy improvements over baseline models. On both CIFAR-10 and ImageNet, our method achieves remarkable on search accuracy, 60x faster on CIFAR-10 and 24x faster on ImageNet, while maintaining competitive accuracies.



### AM-LFS: AutoML for Loss Function Search
- **Arxiv ID**: http://arxiv.org/abs/1905.07375v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.07375v2)
- **Published**: 2019-05-17 17:06:49+00:00
- **Updated**: 2019-08-14 05:58:25+00:00
- **Authors**: Chuming Li, Yuan Xin, Chen Lin, Minghao Guo, Wei Wu, Wanli Ouyang, Junjie Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Designing an effective loss function plays an important role in visual analysis. Most existing loss function designs rely on hand-crafted heuristics that require domain experts to explore the large design space, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Loss Function Search (AM-LFS) which leverages REINFORCE to search loss functions during the training process. The key contribution of this work is the design of search space which can guarantee the generalization and transferability on different vision tasks by including a bunch of existing prevailing loss functions in a unified formulation. We also propose an efficient optimization framework which can dynamically optimize the parameters of loss function's distribution during training. Extensive experimental results on four benchmark datasets show that, without any tricks, our method outperforms existing hand-crafted loss functions in various computer vision tasks.



### Integer Discrete Flows and Lossless Compression
- **Arxiv ID**: http://arxiv.org/abs/1905.07376v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.07376v4)
- **Published**: 2019-05-17 17:07:58+00:00
- **Updated**: 2019-12-06 10:15:54+00:00
- **Authors**: Emiel Hoogeboom, Jorn W. T. Peters, Rianne van den Berg, Max Welling
- **Comment**: Accepted as a conference paper at Neural Information Processing
  Systems (NeurIPS) 2019
- **Journal**: None
- **Summary**: Lossless compression methods shorten the expected representation size of data without loss of information, using a statistical model. Flow-based models are attractive in this setting because they admit exact likelihood optimization, which is equivalent to minimizing the expected number of bits per message. However, conventional flows assume continuous data, which may lead to reconstruction errors when quantized for compression. For that reason, we introduce a flow-based generative model for ordinal discrete data called Integer Discrete Flow (IDF): a bijective integer map that can learn rich transformations on high-dimensional data. As building blocks for IDFs, we introduce a flexible transformation layer called integer discrete coupling. Our experiments show that IDFs are competitive with other flow-based generative models. Furthermore, we demonstrate that IDF based compression achieves state-of-the-art lossless compression rates on CIFAR10, ImageNet32, and ImageNet64. To the best of our knowledge, this is the first lossless compression method that uses invertible neural networks.



### Representation Learning on Visual-Symbolic Graphs for Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/1905.07385v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.07385v2)
- **Published**: 2019-05-17 17:33:48+00:00
- **Updated**: 2020-09-30 15:55:51+00:00
- **Authors**: Effrosyni Mavroudi, Benjamín Béjar Haro, René Vidal
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Events in natural videos typically arise from spatio-temporal interactions between actors and objects and involve multiple co-occurring activities and object classes. To capture this rich visual and semantic context, we propose using two graphs: (1) an attributed spatio-temporal visual graph whose nodes correspond to actors and objects and whose edges encode different types of interactions, and (2) a symbolic graph that models semantic relationships. We further propose a graph neural network for refining the representations of actors, objects and their interactions on the resulting hybrid graph. Our model goes beyond current approaches that assume nodes and edges are of the same type, operate on graphs with fixed edge weights and do not use a symbolic graph. In particular, our framework: a) has specialized attention-based message functions for different node and edge types; b) uses visual edge features; c) integrates visual evidence with label relationships; and d) performs global reasoning in the semantic space. Experiments on challenging video understanding tasks, such as temporal action localization on the Charades dataset, show that the proposed method leads to state-of-the-art performance.



### POPQORN: Quantifying Robustness of Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.07387v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.07387v1)
- **Published**: 2019-05-17 17:35:04+00:00
- **Updated**: 2019-05-17 17:35:04+00:00
- **Authors**: Ching-Yun Ko, Zhaoyang Lyu, Tsui-Wei Weng, Luca Daniel, Ngai Wong, Dahua Lin
- **Comment**: 10 pages, Ching-Yun Ko and Zhaoyang Lyu contributed equally, accepted
  to ICML 2019. Please see arXiv source codes for the appendix by clicking
  [Other formats]
- **Journal**: None
- **Summary**: The vulnerability to adversarial attacks has been a critical issue for deep neural networks. Addressing this issue requires a reliable way to evaluate the robustness of a network. Recently, several methods have been developed to compute $\textit{robustness quantification}$ for neural networks, namely, certified lower bounds of the minimum adversarial perturbation. Such methods, however, were devised for feed-forward networks, e.g. multi-layer perceptron or convolutional networks. It remains an open problem to quantify robustness for recurrent networks, especially LSTM and GRU. For such networks, there exist additional challenges in computing the robustness quantification, such as handling the inputs at multiple steps and the interaction between gates and states. In this work, we propose $\textit{POPQORN}$ ($\textbf{P}$ropagated-$\textbf{o}$ut$\textbf{p}$ut $\textbf{Q}$uantified R$\textbf{o}$bustness for $\textbf{RN}$Ns), a general algorithm to quantify robustness of RNNs, including vanilla RNNs, LSTMs, and GRUs. We demonstrate its effectiveness on different network architectures and show that the robustness quantification on individual steps can lead to new insights.



### Dynamic Vision Sensor integration on FPGA-based CNN accelerators for high-speed visual classification
- **Arxiv ID**: http://arxiv.org/abs/1905.07419v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.07419v1)
- **Published**: 2019-05-17 18:06:12+00:00
- **Updated**: 2019-05-17 18:06:12+00:00
- **Authors**: Alejandro Linares-Barranco, Antonio Rios-Navarro, Ricardo Tapiador-Morales, Tobi Delbruck
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Deep-learning is a cutting edge theory that is being applied to many fields. For vision applications the Convolutional Neural Networks (CNN) are demanding significant accuracy for classification tasks. Numerous hardware accelerators have populated during the last years to improve CPU or GPU based solutions. This technology is commonly prototyped and tested over FPGAs before being considered for ASIC fabrication for mass production. The use of commercial typical cameras (30fps) limits the capabilities of these systems for high speed applications. The use of dynamic vision sensors (DVS) that emulate the behavior of a biological retina is taking an incremental importance to improve this applications due to its nature, where the information is represented by a continuous stream of spikes and the frames to be processed by the CNN are constructed collecting a fixed number of these spikes (called events). The faster an object is, the more events are produced by DVS, so the higher is the equivalent frame rate. Therefore, these DVS utilization allows to compute a frame at the maximum speed a CNN accelerator can offer. In this paper we present a VHDL/HLS description of a pipelined design for FPGA able to collect events from an Address-Event-Representation (AER) DVS retina to obtain a normalized histogram to be used by a particular CNN accelerator, called NullHop. VHDL is used to describe the circuit, and HLS for computation blocks, which are used to perform the normalization of a frame needed for the CNN. Results outperform previous implementations of frames collection and normalization using ARM processors running at 800MHz on a Zynq7100 in both latency and power consumption. A measured 67% speedup factor is presented for a Roshambo CNN real-time experiment running at 160fps peak rate.



### Galaxy Zoo: Probabilistic Morphology through Bayesian CNNs and Active Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.07424v2
- **DOI**: 10.1093/mnras/stz2816
- **Categories**: **astro-ph.GA**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.07424v2)
- **Published**: 2019-05-17 18:20:35+00:00
- **Updated**: 2019-10-04 16:38:44+00:00
- **Authors**: Mike Walmsley, Lewis Smith, Chris Lintott, Yarin Gal, Steven Bamford, Hugh Dickinson, Lucy Fortson, Sandor Kruk, Karen Masters, Claudia Scarlata, Brooke Simmons, Rebecca Smethurst, Darryl Wright
- **Comment**: Accepted by MNRAS. 21 pages, including appendices
- **Journal**: None
- **Summary**: We use Bayesian convolutional neural networks and a novel generative model of Galaxy Zoo volunteer responses to infer posteriors for the visual morphology of galaxies. Bayesian CNN can learn from galaxy images with uncertain labels and then, for previously unlabelled galaxies, predict the probability of each possible label. Our posteriors are well-calibrated (e.g. for predicting bars, we achieve coverage errors of 11.8% within a vote fraction deviation of 0.2) and hence are reliable for practical use. Further, using our posteriors, we apply the active learning strategy BALD to request volunteer responses for the subset of galaxies which, if labelled, would be most informative for training our network. We show that training our Bayesian CNNs using active learning requires up to 35-60% fewer labelled galaxies, depending on the morphological feature being classified. By combining human and machine intelligence, Galaxy Zoo will be able to classify surveys of any conceivable scale on a timescale of weeks, providing massive and detailed morphology catalogues to support research into galaxy evolution.



### AutoDispNet: Improving Disparity Estimation With AutoML
- **Arxiv ID**: http://arxiv.org/abs/1905.07443v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.07443v2)
- **Published**: 2019-05-17 19:05:25+00:00
- **Updated**: 2019-10-06 20:19:32+00:00
- **Authors**: Tonmoy Saikia, Yassine Marrakchi, Arber Zela, Frank Hutter, Thomas Brox
- **Comment**: In Proceedings of the 2019 IEEE International Conference on Computer
  Vision (ICCV)
- **Journal**: None
- **Summary**: Much research work in computer vision is being spent on optimizing existing network architectures to obtain a few more percentage points on benchmarks. Recent AutoML approaches promise to relieve us from this effort. However, they are mainly designed for comparatively small-scale classification tasks. In this work, we show how to use and extend existing AutoML techniques to efficiently optimize large-scale U-Net-like encoder-decoder architectures. In particular, we leverage gradient-based neural architecture search and Bayesian optimization for hyperparameter search. The resulting optimization does not require a large-scale compute cluster. We show results on disparity estimation that clearly outperform the manually optimized baseline and reach state-of-the-art performance.



### Limitations and Biases in Facial Landmark Detection -- An Empirical Study on Older Adults with Dementia
- **Arxiv ID**: http://arxiv.org/abs/1905.07446v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.07446v1)
- **Published**: 2019-05-17 19:15:15+00:00
- **Updated**: 2019-05-17 19:15:15+00:00
- **Authors**: Azin Asgarian, Shun Zhao, Ahmed B. Ashraf, M. Erin Browne, Kenneth M. Prkachin, Alex Mihailidis, Thomas Hadjistavropoulos, Babak Taati
- **Comment**: Face and Gesture Analysis for Health Informatics (FGAHI) Workshop at
  CVPR 2019
- **Journal**: None
- **Summary**: Accurate facial expression analysis is an essential step in various clinical applications that involve physical and mental health assessments of older adults (e.g. diagnosis of pain or depression). Although remarkable progress has been achieved toward developing robust facial landmark detection methods, state-of-the-art methods still face many challenges when encountering uncontrolled environments, different ranges of facial expressions, and different demographics of the population. A recent study has revealed that the health status of individuals can also affect the performance of facial landmark detection methods on front views of faces. In this work, we investigate this matter in a much greater context using seven facial landmark detection methods. We perform our evaluation not only on frontal faces but also on profile faces and in various regions of the face. Our results shed light on limitations of the existing methods and challenges of applying these methods in clinical settings by indicating: 1) a significant difference between the performance of state-of-the-art when tested on the profile or frontal faces of individuals with vs. without dementia; 2) insights on the existing bias for all regions of the face; and 3) the presence of this bias despite re-training/fine-tuning with various configurations of six datasets.



### REPLAB: A Reproducible Low-Cost Arm Benchmark Platform for Robotic Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.07447v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.07447v1)
- **Published**: 2019-05-17 19:16:54+00:00
- **Updated**: 2019-05-17 19:16:54+00:00
- **Authors**: Brian Yang, Jesse Zhang, Vitchyr Pong, Sergey Levine, Dinesh Jayaraman
- **Comment**: Extended version of paper accepted to ICRA 2019
- **Journal**: None
- **Summary**: Standardized evaluation measures have aided in the progress of machine learning approaches in disciplines such as computer vision and machine translation. In this paper, we make the case that robotic learning would also benefit from benchmarking, and present the "REPLAB" platform for benchmarking vision-based manipulation tasks. REPLAB is a reproducible and self-contained hardware stack (robot arm, camera, and workspace) that costs about 2000 USD, occupies a cuboid of size 70x40x60 cm, and permits full assembly within a few hours. Through this low-cost, compact design, REPLAB aims to drive wide participation by lowering the barrier to entry into robotics and to enable easy scaling to many robots. We envision REPLAB as a framework for reproducible research across manipulation tasks, and as a step in this direction, we define a template for a grasping benchmark consisting of a task definition, evaluation protocol, performance measures, and a dataset of 92k grasp attempts. We implement, evaluate, and analyze several previously proposed grasping approaches to establish baselines for this benchmark. Finally, we also implement and evaluate a deep reinforcement learning approach for 3D reaching tasks on our REPLAB platform. Project page with assembly instructions, code, and videos: https://goo.gl/5F9dP4.



### Automated 3D recovery from very high resolution multi-view satellite images
- **Arxiv ID**: http://arxiv.org/abs/1905.07475v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.07475v2)
- **Published**: 2019-05-17 20:56:52+00:00
- **Updated**: 2019-10-05 01:46:03+00:00
- **Authors**: Rongjun Qin
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents an automated pipeline for processing multi-view satellite images to 3D digital surface models (DSM). The proposed pipeline performs automated geo-referencing and generates high-quality densely matched point clouds. In particular, a novel approach is developed that fuses multiple depth maps derived by stereo matching to generate high-quality 3D maps. By learning critical configurations of stereo pairs from sample LiDAR data, we rank the image pairs based on the proximity of the results to the sample data. Multiple depth maps derived from individual image pairs are fused with an adaptive 3D median filter that considers the image spectral similarities. We demonstrate that the proposed adaptive median filter generally delivers better results in general as compared to normal median filter, and achieved an accuracy of improvement of 0.36 meters RMSE in the best case. Results and analysis are introduced in detail.



### Analysis of critical parameters of satellite stereo image for 3D reconstruction and mapping
- **Arxiv ID**: http://arxiv.org/abs/1905.07476v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.07476v1)
- **Published**: 2019-05-17 20:58:57+00:00
- **Updated**: 2019-05-17 20:58:57+00:00
- **Authors**: Rongjun Qin
- **Comment**: None
- **Journal**: None
- **Summary**: Although nowadays advanced dense image matching (DIM) algorithms are able to produce LiDAR (Light Detection And Ranging) comparable dense point clouds from satellite stereo images, the accuracy and completeness of such point clouds heavily depend on the geometric parameters of the satellite stereo images. The intersection angle between two images are normally seen as the most important one in stereo data acquisition, as the state-of-the-art DIM algorithms work best on narrow baseline (smaller intersection angle) stereos (E.g. Semi-Global Matching regards 15-25 degrees as good intersection angle). This factor is in line with the traditional aerial photogrammetry configuration, as the intersection angle directly relates to the base-high ratio and texture distortion in the parallax direction, thus both affecting the horizontal and vertical accuracy. However, our experiments found that even with very similar (and good) intersection angles, the same DIM algorithm applied on different stereo pairs (of the same area) produced point clouds with dramatically different accuracy as compared to the ground truth LiDAR data. This raises a very practical question that is often asked by practitioners: what factors constitute a good satellite stereo pair, such that it produces accurate and optimal results for mapping purpose? In this work, we provide a comprehensive analysis on this matter by performing stereo matching over 1,000 satellite stereo pairs with different acquisition parameters including their intersection angles, off-nadir angles, sun elevation & azimuth angles, as well as time differences, thus to offer a thorough answer to this question. This work will potentially provide a valuable reference to researchers working on multi-view satellite image reconstruction, as well as industrial practitioners minimizing costs for high-quality large-scale mapping.



### Multilinear Compressive Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.07481v3
- **DOI**: 10.1109/TNNLS.2020.2984831
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.07481v3)
- **Published**: 2019-05-17 21:04:44+00:00
- **Updated**: 2020-10-21 09:01:42+00:00
- **Authors**: Dat Thanh Tran, Mehmet Yamac, Aysen Degerli, Moncef Gabbouj, Alexandros Iosifidis
- **Comment**: accepted in IEEE Transactions on Neural Networks and Learning Systems
  2020
- **Journal**: None
- **Summary**: Compressive Learning is an emerging topic that combines signal acquisition via compressive sensing and machine learning to perform inference tasks directly on a small number of measurements. Many data modalities naturally have a multi-dimensional or tensorial format, with each dimension or tensor mode representing different features such as the spatial and temporal information in video sequences or the spatial and spectral information in hyperspectral images. However, in existing compressive learning frameworks, the compressive sensing component utilizes either random or learned linear projection on the vectorized signal to perform signal acquisition, thus discarding the multi-dimensional structure of the signals. In this paper, we propose Multilinear Compressive Learning, a framework that takes into account the tensorial nature of multi-dimensional signals in the acquisition step and builds the subsequent inference model on the structurally sensed measurements. Our theoretical complexity analysis shows that the proposed framework is more efficient compared to its vector-based counterpart in both memory and computation requirement. With extensive experiments, we also empirically show that our Multilinear Compressive Learning framework outperforms the vector-based framework in object classification and face recognition tasks, and scales favorably when the dimensionalities of the original signals increase, making it highly efficient for high-dimensional multi-dimensional signals.



### Learning to Reconstruct 3D Manhattan Wireframes from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1905.07482v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.07482v2)
- **Published**: 2019-05-17 21:09:27+00:00
- **Updated**: 2021-04-16 04:54:06+00:00
- **Authors**: Yichao Zhou, Haozhi Qi, Yuexiang Zhai, Qi Sun, Zhili Chen, Li-Yi Wei, Yi Ma
- **Comment**: ICCV 2019. A video demonstration can be found in
  https://youtu.be/l3sUtddPJPY
- **Journal**: None
- **Summary**: In this paper, we propose a method to obtain a compact and accurate 3D wireframe representation from a single image by effectively exploiting global structural regularities. Our method trains a convolutional neural network to simultaneously detect salient junctions and straight lines, as well as predict their 3D depth and vanishing points. Compared with the state-of-the-art learning-based wireframe detection methods, our network is simpler and more unified, leading to better 2D wireframe detection. With global structural priors from parallelism, our method further reconstructs a full 3D wireframe model, a compact vector representation suitable for a variety of high-level vision tasks such as AR and CAD. We conduct extensive evaluations on a large synthetic dataset of urban scenes as well as real images. Our code and datasets have been made public at https://github.com/zhou13/shapeunity.



### Rethinking Atmospheric Turbulence Mitigation
- **Arxiv ID**: http://arxiv.org/abs/1905.07498v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.07498v1)
- **Published**: 2019-05-17 22:59:11+00:00
- **Updated**: 2019-05-17 22:59:11+00:00
- **Authors**: Nicholas Chimitt, Zhiyuan Mao, Guanzhe Hong, Stanley H. Chan
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art atmospheric turbulence image restoration methods utilize standard image processing tools such as optical flow, lucky region and blind deconvolution to restore the images. While promising results have been reported over the past decade, many of the methods are agnostic to the physical model that generates the distortion. In this paper, we revisit the turbulence restoration problem by analyzing the reference frame generation and the blind deconvolution steps in a typical restoration pipeline. By leveraging tools in large deviation theory, we rigorously prove the minimum number of frames required to generate a reliable reference for both static and dynamic scenes. We discuss how a turbulence agnostic model can lead to potential flaws, and how to configure a simple spatial-temporal non-local weighted averaging method to generate references. For blind deconvolution, we present a new data-driven prior by analyzing the distributions of the point spread functions. We demonstrate how a simple prior can outperform state-of-the-art blind deconvolution methods.



### 3DViewGraph: Learning Global Features for 3D Shapes from A Graph of Unordered Views with Attention
- **Arxiv ID**: http://arxiv.org/abs/1905.07503v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.07503v1)
- **Published**: 2019-05-17 23:52:05+00:00
- **Updated**: 2019-05-17 23:52:05+00:00
- **Authors**: Zhizhong Han, Xiyang Wang, Chi-Man Vong, Yu-Shen Liu, Matthias Zwicker, C. L. Philip Chen
- **Comment**: To appear at IJCAI2019
- **Journal**: None
- **Summary**: Learning global features by aggregating information over multiple views has been shown to be effective for 3D shape analysis. For view aggregation in deep learning models, pooling has been applied extensively. However, pooling leads to a loss of the content within views, and the spatial relationship among views, which limits the discriminability of learned features. We propose 3DViewGraph to resolve this issue, which learns 3D global features by more effectively aggregating unordered views with attention. Specifically, unordered views taken around a shape are regarded as view nodes on a view graph. 3DViewGraph first learns a novel latent semantic mapping to project low-level view features into meaningful latent semantic embeddings in a lower dimensional space, which is spanned by latent semantic patterns. Then, the content and spatial information of each pair of view nodes are encoded by a novel spatial pattern correlation, where the correlation is computed among latent semantic patterns. Finally, all spatial pattern correlations are integrated with attention weights learned by a novel attention mechanism. This further increases the discriminability of learned features by highlighting the unordered view nodes with distinctive characteristics and depressing the ones with appearance ambiguity. We show that 3DViewGraph outperforms state-of-the-art methods under three large-scale benchmarks.



