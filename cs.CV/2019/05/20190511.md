# Arxiv Papers in cs.CV on 2019-05-11
### Learning Robotic Manipulation through Visual Planning and Acting
- **Arxiv ID**: http://arxiv.org/abs/1905.04411v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.04411v1)
- **Published**: 2019-05-11 00:30:21+00:00
- **Updated**: 2019-05-11 00:30:21+00:00
- **Authors**: Angelina Wang, Thanard Kurutach, Kara Liu, Pieter Abbeel, Aviv Tamar
- **Comment**: RSS 2019. Website at https://sites.google.com/berkeley.edu/vpa/home
- **Journal**: None
- **Summary**: Planning for robotic manipulation requires reasoning about the changes a robot can affect on objects. When such interactions can be modelled analytically, as in domains with rigid objects, efficient planning algorithms exist. However, in both domestic and industrial domains, the objects of interest can be soft, or deformable, and hard to model analytically. For such cases, we posit that a data-driven modelling approach is more suitable. In recent years, progress in deep generative models has produced methods that learn to `imagine' plausible images from data. Building on the recent Causal InfoGAN generative model, in this work we learn to imagine goal-directed object manipulation directly from raw image data of self-supervised interaction of the robot with the object. After learning, given a goal observation of the system, our model can generate an imagined plan -- a sequence of images that transition the object into the desired goal. To execute the plan, we use it as a reference trajectory to track with a visual servoing controller, which we also learn from the data as an inverse dynamics model. In a simulated manipulation task, we show that separating the problem into visual planning and visual tracking control is more sample efficient and more interpretable than alternative data-driven approaches. We further demonstrate our approach on learning to imagine and execute in 3 environments, the final of which is deformable rope manipulation on a PR2 robot.



### Long Short-Term Memory with Gate and State Level Fusion for Light Field-Based Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1905.04421v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04421v2)
- **Published**: 2019-05-11 01:38:50+00:00
- **Updated**: 2020-06-01 20:59:56+00:00
- **Authors**: Alireza Sepas-Moghaddam, Ali Etemad, Fernando Pereira, Paulo Lobato Correia
- **Comment**: Submitted to IEEE TIFS
- **Journal**: None
- **Summary**: Long Short-Term Memory (LSTM) is a prominent recurrent neural network for extracting dependencies from sequential data such as time-series and multi-view data, having achieved impressive results for different visual recognition tasks. A conventional LSTM network can learn a model to posteriorly extract information from one input sequence. However, if two or more dependent sequences of data are simultaneously acquired, the conventional LSTM networks may only process those sequences consecutively, not taking benefit of the information carried out by their mutual dependencies. In this context, this paper proposes two novel LSTM cell architectures that are able to jointly learn from multiple sequences simultaneously acquired, targeting to create richer and more effective models for recognition tasks. The efficacy of the novel LSTM cell architectures is assessed by integrating them into deep learning-based methods for face recognition with multi-view, light field images. The new cell architectures jointly learn the scene horizontal and vertical parallaxes available in a light field image, to capture richer spatio-angular information from both directions. A comprehensive evaluation, with the IST-EURECOM LFFD dataset using three challenging evaluation protocols, shows the advantage of using the novel LSTM cell architectures for face recognition over the state-of-the-art light field-based methods. These results highlight the added value of the novel cell architectures when learning from correlated input sequences.



### Structured Discriminative Tensor Dictionary Learning for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1905.04424v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.04424v1)
- **Published**: 2019-05-11 02:28:04+00:00
- **Updated**: 2019-05-11 02:28:04+00:00
- **Authors**: Songsong Wu, Yan Yan, Hao Tang, Jianjun Qian, Jian Zhang, Xiao-Yuan Jing
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised Domain Adaptation (UDA) addresses the problem of performance degradation due to domain shift between training and testing sets, which is common in computer vision applications. Most existing UDA approaches are based on vector-form data although the typical format of data or features in visual applications is multi-dimensional tensor. Besides, current methods, including the deep network approaches, assume that abundant labeled source samples are provided for training. However, the number of labeled source samples are always limited due to expensive annotation cost in practice, making sub-optimal performance been observed. In this paper, we propose to seek discriminative representation for multi-dimensional data by learning a structured dictionary in tensor space. The dictionary separates domain-specific information and class-specific information to guarantee the representation robust to domains. In addition, a pseudo-label estimation scheme is developed to combine with discriminant analysis in the algorithm iteration for avoiding the external classifier design. We perform extensive results on different datasets with limited source samples. Experimental results demonstrates that the proposed method outperforms the state-of-the-art approaches.



### Cyclone intensity estimate with context-aware cyclegan
- **Arxiv ID**: http://arxiv.org/abs/1905.04425v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.04425v1)
- **Published**: 2019-05-11 02:37:34+00:00
- **Updated**: 2019-05-11 02:37:34+00:00
- **Authors**: Yajing Xu, Haitao Yang, Mingfei Cheng, Si Li
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Deep learning approaches to cyclone intensity estimationhave recently shown promising results. However, sufferingfrom the extreme scarcity of cyclone data on specific in-tensity, most existing deep learning methods fail to achievesatisfactory performance on cyclone intensity estimation,especially on classes with few instances. To avoid the degra-dation of recognition performance caused by scarce samples,we propose a context-aware CycleGAN which learns the la-tent evolution features from adjacent cyclone intensity andsynthesizes CNN features of classes lacking samples fromunpaired source classes. Specifically, our approach synthe-sizes features conditioned on the learned evolution features,while the extra information is not required. Experimentalresults of several evaluation methods show the effectivenessof our approach, even can predicting unseen classes.



### Follow the Attention: Combining Partial Pose and Object Motion for Fine-Grained Action Detection
- **Arxiv ID**: http://arxiv.org/abs/1905.04430v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.04430v2)
- **Published**: 2019-05-11 02:54:01+00:00
- **Updated**: 2019-06-26 04:44:58+00:00
- **Authors**: Mohammad Mahdi Kazemi Moghaddam, Ehsan Abbasnejad, Javen Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Retailers have long been searching for ways to effectively understand their customers' behaviour in order to provide a smooth and pleasant shopping experience that attracts more customers everyday and maximises their revenue, consequently. Humans can flawlessly understand others' behaviour by combining different visual cues from activity to gestures and facial expressions. Empowering the computer vision systems to do so, however, is still an open problem due to its intrinsic challenges as well as extrinsic enforced difficulties like lack of publicly available data and unique environment conditions (wild). In this work, We emphasise on detecting the first and by far the most crucial cue in behaviour analysis; that is human activity detection in computer vision. To do so, we introduce a framework for integrating human pose and object motion to both temporally detect and classify the activities in a fine-grained manner (very short and similar activities). We incorporate partial human pose and interaction with the objects in a multi-stream neural network architecture to guide the spatiotemporal attention mechanism for more efficient activity recognition. To this end, in the absence of pose supervision, we propose to use the Generative Adversarial Network (GAN) to generate exact joint locations from noisy probability heat maps. Additionally, based on the intuition that complex actions demand more than one source of information to be identified even by humans, we integrate the second stream of object motion to our network as a prior knowledge that we quantitatively show improves the recognition results. We empirically show the capability of our approach by achieving state-of-the-art results on MERL shopping dataset. We further investigate the effectiveness of this approach on a new shopping dataset that we have collected to address existing shortcomings.



### Joint Learning of Self-Representation and Indicator for Multi-View Image Clustering
- **Arxiv ID**: http://arxiv.org/abs/1905.04432v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04432v1)
- **Published**: 2019-05-11 02:57:00+00:00
- **Updated**: 2019-05-11 02:57:00+00:00
- **Authors**: Songsong Wu, Zhiqiang Lu, Hao Tang, Yan Yan, Songhao Zhu, Xiao-Yuan Jing, Zuoyong Li
- **Comment**: Accepted by ICIP 2019
- **Journal**: None
- **Summary**: Multi-view subspace clustering aims to divide a set of multisource data into several groups according to their underlying subspace structure. Although the spectral clustering based methods achieve promotion in multi-view clustering, their utility is limited by the separate learning manner in which affinity matrix construction and cluster indicator estimation are isolated. In this paper, we propose to jointly learn the self-representation, continue and discrete cluster indicators in an unified model. Our model can explore the subspace structure of each view and fusion them to facilitate clustering simultaneously. Experimental results on two benchmark datasets demonstrate that our method outperforms other existing competitive multi-view clustering methods.



### Play and Prune: Adaptive Filter Pruning for Deep Model Compression
- **Arxiv ID**: http://arxiv.org/abs/1905.04446v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.04446v1)
- **Published**: 2019-05-11 04:37:10+00:00
- **Updated**: 2019-05-11 04:37:10+00:00
- **Authors**: Pravendra Singh, Vinay Kumar Verma, Piyush Rai, Vinay P. Namboodiri
- **Comment**: International Joint Conference on Artificial Intelligence
  (IJCAI-2019)
- **Journal**: None
- **Summary**: While convolutional neural networks (CNN) have achieved impressive performance on various classification/recognition tasks, they typically consist of a massive number of parameters. This results in significant memory requirement as well as computational overheads. Consequently, there is a growing need for filter-level pruning approaches for compressing CNN based models that not only reduce the total number of parameters but reduce the overall computation as well. We present a new min-max framework for filter-level pruning of CNNs. Our framework, called Play and Prune (PP), jointly prunes and fine-tunes CNN model parameters, with an adaptive pruning rate, while maintaining the model's predictive performance. Our framework consists of two modules: (1) An adaptive filter pruning (AFP) module, which minimizes the number of filters in the model; and (2) A pruning rate controller (PRC) module, which maximizes the accuracy during pruning. Moreover, unlike most previous approaches, our approach allows directly specifying the desired error tolerance instead of pruning level. Our compressed models can be deployed at run-time, without requiring any special libraries or hardware. Our approach reduces the number of parameters of VGG-16 by an impressive factor of 17.5X, and number of FLOPS by 6.43X, with no loss of accuracy, significantly outperforming other state-of-the-art filter pruning methods.



### Deep Plug-and-play Prior for Low-rank Tensor Completion
- **Arxiv ID**: http://arxiv.org/abs/1905.04449v3
- **DOI**: 10.1016/j.neucom.2020.03.018
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.04449v3)
- **Published**: 2019-05-11 05:10:45+00:00
- **Updated**: 2020-05-04 04:49:19+00:00
- **Authors**: Xi-Le Zhao, Wen-Hao Xu, Tai-Xiang Jiang, Yao Wang, Michael Ng
- **Comment**: Accepted by Neurocoputing. Code availiable at
  https://github.com/TaiXiangJiang/Deep-Plug-and-Play-Prior-for-Low-Rank-Tensor-Completion
- **Journal**: None
- **Summary**: Multi-dimensional images, such as color images and multi-spectral images, are highly correlated and contain abundant spatial and spectral information. However, real-world multi-dimensional images are usually corrupted by missing entries. By integrating deterministic low-rankness prior to the data-driven deep prior, we suggest a novel regularized tensor completion model for multi-dimensional image completion. In the objective function, we adopt the newly emerged tensor nuclear norm (TNN) to characterize the global low-rankness prior of the multi-dimensional images. We also formulate an implicit regularizer by plugging into a denoising neural network (termed as deep denoiser), which is convinced to express the deep image prior learned from a large number of natural images. The resulting model can be solved by the alternating directional method of multipliers algorithm under the plug-and-play (PnP) framework. Experimental results on color images, videos, and multi-spectral images demonstrate that the proposed method can recover both the global structure and fine details very well and achieve superior performance over competing methods in terms of quality metrics and visual effects.



### Offset Calibration for Appearance-Based Gaze Estimation via Gaze Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1905.04451v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04451v2)
- **Published**: 2019-05-11 05:24:48+00:00
- **Updated**: 2020-01-09 07:10:19+00:00
- **Authors**: Zhaokang Chen, Bertram E. Shi
- **Comment**: Accepted by WACV2020. This is not the camera-ready version
- **Journal**: None
- **Summary**: Appearance-based gaze estimation provides relatively unconstrained gaze tracking. However, subject-independent models achieve limited accuracy partly due to individual variations. To improve estimation, we propose a novel gaze decomposition method and a single gaze point calibration method, motivated by our finding that the inter-subject squared bias exceeds the intra-subject variance for a subject-independent estimator. We decompose the gaze angle into a subject-dependent bias term and a subject-independent term between the gaze angle and the bias. The subject-independent term is estimated by a deep convolutional network. For calibration-free tracking, we set the subject-dependent bias term to zero. For single gaze point calibration, we estimate the bias from a few images taken as the subject gazes at a point. Experiments on three datasets indicate that as a calibration-free estimator, the proposed method outperforms the state-of-the-art methods by up to $10.0\%$. The proposed calibration method is robust and reduces estimation error significantly (up to $35.6\%$), achieving state-of-the-art performance for appearance-based eye trackers with calibration.



### Self-Supervised Visual Place Recognition Learning in Mobile Robots
- **Arxiv ID**: http://arxiv.org/abs/1905.04453v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1905.04453v1)
- **Published**: 2019-05-11 05:49:36+00:00
- **Updated**: 2019-05-11 05:49:36+00:00
- **Authors**: Sudeep Pillai, John Leonard
- **Comment**: Presented at Learning for Localization and Mapping Workshop at IROS
  2017
- **Journal**: None
- **Summary**: Place recognition is a critical component in robot navigation that enables it to re-establish previously visited locations, and simultaneously use this information to correct the drift incurred in its dead-reckoned estimate. In this work, we develop a self-supervised approach to place recognition in robots. The task of visual loop-closure identification is cast as a metric learning problem, where the labels for positive and negative examples of loop-closures can be bootstrapped using a GPS-aided navigation solution that the robot already uses. By leveraging the synchronization between sensors, we show that we are able to learn an appropriate distance metric for arbitrary real-valued image descriptors (including state-of-the-art CNN models), that is specifically geared for visual place recognition in mobile robots. Furthermore, we show that the newly learned embedding can be particularly powerful in disambiguating visual scenes for the task of vision-based loop-closure identification in mobile robots.



### Triplet Distillation for Deep Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1905.04457v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.04457v2)
- **Published**: 2019-05-11 06:23:51+00:00
- **Updated**: 2019-05-19 14:13:37+00:00
- **Authors**: Yushu Feng, Huan Wang, Daniel T. Yi, Roland Hu
- **Comment**: 5 pages, 2 tables, accpeted by ICML 2019 ODML-CDNNR Workshop
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have achieved a great success in face recognition, which unfortunately comes at the cost of massive computation and storage consumption. Many compact face recognition networks are thus proposed to resolve this problem. Triplet loss is effective to further improve the performance of those compact models. However, it normally employs a fixed margin to all the samples, which neglects the informative similarity structures between different identities. In this paper, we propose an enhanced version of triplet loss, named triplet distillation, which exploits the capability of a teacher model to transfer the similarity information to a small model by adaptively varying the margin between positive and negative pairs. Experiments on LFW, AgeDB, and CPLFW datasets show the merits of our method compared to the original triplet loss.



### Monocular Depth Estimation with Directional Consistency by Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.04467v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.04467v1)
- **Published**: 2019-05-11 07:13:16+00:00
- **Updated**: 2019-05-11 07:13:16+00:00
- **Authors**: Fabian Truetsch, Alfred Schöttl
- **Comment**: None
- **Journal**: None
- **Summary**: As processing power has become more available, more human-like artificial intelligences are created to solve image processing tasks that we are inherently good at. As such we propose a model that estimates depth from a monocular image. Our approach utilizes a combination of structure from motion and stereo disparity. We estimate a pose between the source image and a different viewpoint and a dense depth map and use a simple transformation to reconstruct the image seen from said viewpoint. We can then use the real image at that viewpoint to act as supervision to train out model. The metric chosen for image comparison employs standard L1 and structural similarity and a consistency constraint between depth maps as well as smoothness constraint. We show that similar to human perception utilizing the correlation within the provided data by two different approaches increases the accuracy and outperforms the individual components.



### Graph Attention Memory for Visual Navigation
- **Arxiv ID**: http://arxiv.org/abs/1905.13315v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.13315v2)
- **Published**: 2019-05-11 08:00:54+00:00
- **Updated**: 2019-06-04 13:40:49+00:00
- **Authors**: Dong Li, Qichao Zhang, Dongbin Zhao, Yuzheng Zhuang, Bin Wang, Wulong Liu, Rasul Tutunov, Jun Wang
- **Comment**: 13 pages, 7 figures
- **Journal**: None
- **Summary**: Visual navigation in complex environments is inefficient with traditional reactive policy or general-purposed recurrent policy. To address the long-term memory issue, this paper proposes a graph attention memory (GAM) architecture consisting of memory construction module, graph attention module and control module. The memory construction module builds the topological graph based on supervised learning by taking the exploration prior. Then, guided attention features are extracted with the graph attention module. Finally, the deep reinforcement learning based control module makes decisions based on visual observations and guided attention features. Detailed convergence analysis of GAM is presented in this paper. We evaluate GAM-based navigation system in two complex 3D environments. Experimental results show that the GAM-based navigation system significantly improves learning efficiency and outperforms all baselines in average success rate.



### Moving Target Defense for Deep Visual Sensing against Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1905.13148v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.13148v1)
- **Published**: 2019-05-11 08:22:32+00:00
- **Updated**: 2019-05-11 08:22:32+00:00
- **Authors**: Qun Song, Zhenyu Yan, Rui Tan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning based visual sensing has achieved attractive accuracy but is shown vulnerable to adversarial example attacks. Specifically, once the attackers obtain the deep model, they can construct adversarial examples to mislead the model to yield wrong classification results. Deployable adversarial examples such as small stickers pasted on the road signs and lanes have been shown effective in misleading advanced driver-assistance systems. Many existing countermeasures against adversarial examples build their security on the attackers' ignorance of the defense mechanisms. Thus, they fall short of following Kerckhoffs's principle and can be subverted once the attackers know the details of the defense. This paper applies the strategy of moving target defense (MTD) to generate multiple new deep models after system deployment, that will collaboratively detect and thwart adversarial examples. Our MTD design is based on the adversarial examples' minor transferability to models differing from the one (e.g., the factory-designed model) used for attack construction. The post-deployment quasi-secret deep models significantly increase the bar for the attackers to construct effective adversarial examples. We also apply the technique of serial data fusion with early stopping to reduce the inference time by a factor of up to 5 while maintaining the sensing and defense performance. Extensive evaluation based on three datasets including a road sign image database and a GPU-equipped Jetson embedded computing board shows the effectiveness of our approach.



### Training CNNs with Selective Allocation of Channels
- **Arxiv ID**: http://arxiv.org/abs/1905.04509v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.04509v1)
- **Published**: 2019-05-11 12:00:55+00:00
- **Updated**: 2019-05-11 12:00:55+00:00
- **Authors**: Jongheon Jeong, Jinwoo Shin
- **Comment**: 15 pages; Accepted to ICML 2019
- **Journal**: None
- **Summary**: Recent progress in deep convolutional neural networks (CNNs) have enabled a simple paradigm of architecture design: larger models typically achieve better accuracy. Due to this, in modern CNN architectures, it becomes more important to design models that generalize well under certain resource constraints, e.g. the number of parameters. In this paper, we propose a simple way to improve the capacity of any CNN model having large-scale features, without adding more parameters. In particular, we modify a standard convolutional layer to have a new functionality of channel-selectivity, so that the layer is trained to select important channels to re-distribute their parameters. Our experimental results under various CNN architectures and datasets demonstrate that the proposed new convolutional layer allows new optima that generalize better via efficient resource utilization, compared to the baseline.



### Deep Zero-Shot Learning for Scene Sketch
- **Arxiv ID**: http://arxiv.org/abs/1905.04510v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04510v1)
- **Published**: 2019-05-11 12:07:16+00:00
- **Updated**: 2019-05-11 12:07:16+00:00
- **Authors**: Yao Xie, Peng Xu, Zhanyu Ma
- **Comment**: 5 pages, 3 figures, IEEE International Conference on Image Processing
  (ICIP)
- **Journal**: None
- **Summary**: We introduce a novel problem of scene sketch zero-shot learning (SSZSL), which is a challenging task, since (i) different from photo, the gap between common semantic domain (e.g., word vector) and sketch is too huge to exploit common semantic knowledge as the bridge for knowledge transfer, and (ii) compared with single-object sketch, more expressive feature representation for scene sketch is required to accommodate its high-level of abstraction and complexity. To overcome these challenges, we propose a deep embedding model for scene sketch zero-shot learning. In particular, we propose the augmented semantic vector to conduct domain alignment by fusing multi-modal semantic knowledge (e.g., cartoon image, natural image, text description), and adopt attention-based network for scene sketch feature learning. Moreover, we propose a novel distance metric to improve the similarity measure during testing. Extensive experiments and ablation studies demonstrate the benefit of our sketch-specific design.



### Unified Generator-Classifier for Efficient Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.04511v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04511v1)
- **Published**: 2019-05-11 12:11:42+00:00
- **Updated**: 2019-05-11 12:11:42+00:00
- **Authors**: Ayyappa Kumar Pambala, Titir Dutta, Soma Biswas
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: Generative models have achieved state-of-the-art performance for the zero-shot learning problem, but they require re-training the classifier every time a new object category is encountered. The traditional semantic embedding approaches, though very elegant, usually do not perform at par with their generative counterparts. In this work, we propose an unified framework termed GenClass, which integrates the generator with the classifier for efficient zero-shot learning, thus combining the representative power of the generative approaches and the elegance of the embedding approaches. End-to-end training of the unified framework not only eliminates the requirement of additional classifier for new object categories as in the generative approaches, but also facilitates the generation of more discriminative and useful features. Extensive evaluation on three standard zero-shot object classification datasets, namely AWA, CUB and SUN shows the effectiveness of the proposed approach. The approach without any modification, also gives state-of-the-art performance for zero-shot action classification, thus showing its generalizability to other domains.



### Multi-class Novelty Detection Using Mix-up Technique
- **Arxiv ID**: http://arxiv.org/abs/1905.04523v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04523v3)
- **Published**: 2019-05-11 13:10:39+00:00
- **Updated**: 2020-01-01 07:30:10+00:00
- **Authors**: Supritam Bhattacharjee, Devraj Mandal, Soma Biswas
- **Comment**: Updated version has been accepted in WACV 2020
- **Journal**: None
- **Summary**: Multi-class novelty detection is increasingly becoming an important area of research due to the continuous increase in the number of object categories. It tries to answer the pertinent question: given a test sample, should we even try to classify it? We propose a novel solution using the concept of mixup technique for novelty detection, termed as Segregation Network. During training, a pair of examples are selected from the training data and an interpolated data point using their convex combination is constructed. We develop a suitable loss function to train our model to predict its constituent classes. During testing, each input query is combined with the known class prototypes to generate mixed samples which are then passed through the trained network. Our model which is trained to reveal the constituent classes can then be used to determine whether the sample is novel or not. The intuition is that if a query comes from a known class and is mixed with the set of known class prototypes, then the prediction of the trained model for the correct class should be high. In contrast, for a query from a novel class, the predictions for all the known classes should be low. The proposed model is trained using only the available known class data and does not need access to any auxiliary dataset or attributes. Extensive experiments on two benchmark datasets, namely Caltech 256 and Stanford Dogs and comparisons with the state-of-the-art algorithms justifies the usefulness of our approach.



### Illumination-Adaptive Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1905.04525v2
- **DOI**: 10.1109/tmm.2020.2969782
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04525v2)
- **Published**: 2019-05-11 13:28:25+00:00
- **Updated**: 2020-04-23 13:31:19+00:00
- **Authors**: Zelong Zeng, Zhixiang Wang, Zheng Wang, Yinqiang Zheng, Yung-Yu Chuang, Shin'ichi Satoh
- **Comment**: Accepted by TMM
- **Journal**: None
- **Summary**: Most person re-identification (ReID) approaches assume that person images are captured under relatively similar illumination conditions. In reality, long-term person retrieval is common, and person images are often captured under different illumination conditions at different times across a day. In this situation, the performances of existing ReID models often degrade dramatically. This paper addresses the ReID problem with illumination variations and names it as {\em Illumination-Adaptive Person Re-identification (IA-ReID)}. We propose an Illumination-Identity Disentanglement (IID) network to dispel different scales of illuminations away while preserving individuals' identity information. To demonstrate the illumination issue and to evaluate our model, we construct two large-scale simulated datasets with a wide range of illumination variations. Experimental results on the simulated datasets and real-world images demonstrate the effectiveness of the proposed framework.



### Multitask Deep Learning with Spectral Knowledge for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1905.04535v4
- **DOI**: 10.1109/LGRS.2019.2962768
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04535v4)
- **Published**: 2019-05-11 14:50:34+00:00
- **Updated**: 2019-12-26 00:22:12+00:00
- **Authors**: Shengjie Liu, Qian Shi
- **Comment**: Accepted by IEEE GRSL
- **Journal**: None
- **Summary**: In this letter, we propose a multitask deep learning method for classification of multiple hyperspectral data in a single training. Deep learning models have achieved promising results on hyperspectral image classification, but their performance highly rely on sufficient labeled samples, which are scarce on hyperspectral images. However, samples from multiple data sets might be sufficient to train one deep learning model, thereby improving its performance. To do so, we trained an identical feature extractor for all data, and the extracted features were fed into corresponding Softmax classifiers. Spectral knowledge was introduced to ensure that the shared features were similar across domains. Four hyperspectral data sets were used in the experiments. We achieved higher classification accuracies on three data sets (Pavia University, Pavia Center, and Indian Pines) and competitive results on the Salinas Valley data compared with the baseline. Spectral knowledge was useful to prevent the deep network from overfitting when the data shared similar spectral response. The proposed method tested on two deep CNNs successfully shows its ability to utilize samples from multiple data sets and enhance networks' performance.



### Disentangling Content and Style via Unsupervised Geometry Distillation
- **Arxiv ID**: http://arxiv.org/abs/1905.04538v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.04538v1)
- **Published**: 2019-05-11 15:12:52+00:00
- **Updated**: 2019-05-11 15:12:52+00:00
- **Authors**: Wayne Wu, Kaidi Cao, Cheng Li, Chen Qian, Chen Change Loy
- **Comment**: Accepted to ICLR 2019 Workshop
- **Journal**: None
- **Summary**: It is challenging to disentangle an object into two orthogonal spaces of content and style since each can influence the visual observation differently and unpredictably. It is rare for one to have access to a large number of data to help separate the influences. In this paper, we present a novel framework to learn this disentangled representation in a completely unsupervised manner. We address this problem in a two-branch Autoencoder framework. For the structural content branch, we project the latent factor into a soft structured point tensor and constrain it with losses derived from prior knowledge. This constraint encourages the branch to distill geometry information. Another branch learns the complementary style information. The two branches form an effective framework that can disentangle object's content-style representation without any human annotation. We evaluate our approach on four image datasets, on which we demonstrate the superior disentanglement and visual analogy quality both in synthesized and real-world data. We are able to generate photo-realistic images with 256*256 resolution that are clearly disentangled in content and style.



### Deep Unsupervised Learning of 3D Point Clouds via Graph Topology Inference and Filtering
- **Arxiv ID**: http://arxiv.org/abs/1905.04571v2
- **DOI**: 10.1109/TIP.2019.2957935
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04571v2)
- **Published**: 2019-05-11 18:25:58+00:00
- **Updated**: 2019-11-25 05:20:46+00:00
- **Authors**: Siheng Chen, Chaojing Duan, Yaoqing Yang, Duanshun Li, Chen Feng, Dong Tian
- **Comment**: To appear in IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: We propose a deep autoencoder with graph topology inference and filtering to achieve compact representations of unorganized 3D point clouds in an unsupervised manner. Many previous works discretize 3D points to voxels and then use lattice-based methods to process and learn 3D spatial information; however, this leads to inevitable discretization errors. In this work, we handle raw 3D points without such compromise. The proposed networks follow the autoencoder framework with a focus on designing the decoder. The encoder adopts similar architectures as in PointNet. The decoder involves three novel modules. The folding module folds a canonical 2D lattice to the underlying surface of a 3D point cloud, achieving coarse reconstruction; the graph-topology-inference module learns a graph topology to represent pairwise relationships between 3D points, pushing the latent code to preserve both coordinates and pairwise relationships of points in 3D point clouds; and the graph-filtering module couples the above two modules, refining the coarse reconstruction through a learnt graph topology to obtain the final reconstruction. The proposed decoder leverages a learnable graph topology to push the codeword to preserve representative features and further improve the unsupervised-learning performance. We further provide theoretical analyses of the proposed architecture. In the experiments, we validate the proposed networks in three tasks, including 3D point cloud reconstruction, visualization, and transfer classification. The experimental results show that (1) the proposed networks outperform the state-of-the-art methods in various tasks; (2) a graph topology can be inferred as auxiliary information without specific supervision on graph topology inference; and (3) graph filtering refines the reconstruction, leading to better performances.



### Robustness of Object Recognition under Extreme Occlusion in Humans and Computational Models
- **Arxiv ID**: http://arxiv.org/abs/1905.04598v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.04598v2)
- **Published**: 2019-05-11 22:01:04+00:00
- **Updated**: 2019-06-04 06:56:11+00:00
- **Authors**: Hongru Zhu, Peng Tang, Jeongho Park, Soojin Park, Alan Yuille
- **Comment**: To be presented at the 41st Annual Meeting of the Cognitive Science
  Society
- **Journal**: None
- **Summary**: Most objects in the visual world are partially occluded, but humans can recognize them without difficulty. However, it remains unknown whether object recognition models like convolutional neural networks (CNNs) can handle real-world occlusion. It is also a question whether efforts to make these models robust to constant mask occlusion are effective for real-world occlusion. We test both humans and the above-mentioned computational models in a challenging task of object recognition under extreme occlusion, where target objects are heavily occluded by irrelevant real objects in real backgrounds. Our results show that human vision is very robust to extreme occlusion while CNNs are not, even with modifications to handle constant mask occlusion. This implies that the ability to handle constant mask occlusion does not entail robustness to real-world occlusion. As a comparison, we propose another computational model that utilizes object parts/subparts in a compositional manner to build robustness to occlusion. This performs significantly better than CNN-based models on our task with error patterns similar to humans. These findings suggest that testing under extreme occlusion can better reveal the robustness of visual recognition, and that the principle of composition can encourage such robustness.



