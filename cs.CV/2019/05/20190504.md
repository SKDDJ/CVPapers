# Arxiv Papers in cs.CV on 2019-05-04
### A Survey on Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1905.01392v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.01392v2)
- **Published**: 2019-05-04 00:08:49+00:00
- **Updated**: 2019-06-18 09:32:21+00:00
- **Authors**: Martin Wistuba, Ambrish Rawat, Tejaswini Pedapati
- **Comment**: None
- **Journal**: None
- **Summary**: The growing interest in both the automation of machine learning and deep learning has inevitably led to the development of a wide variety of automated methods for neural architecture search. The choice of the network architecture has proven to be critical, and many advances in deep learning spring from its immediate improvements. However, deep learning techniques are computationally intensive and their application requires a high level of domain knowledge. Therefore, even partial automation of this process helps to make deep learning more accessible to both researchers and practitioners. With this survey, we provide a formalism which unifies and categorizes the landscape of existing methods along with a detailed analysis that compares and contrasts the different approaches. We achieve this via a comprehensive discussion of the commonly adopted architecture search spaces and architecture optimization algorithms based on principles of reinforcement learning and evolutionary algorithms along with approaches that incorporate surrogate and one-shot models. Additionally, we address the new research directions which include constrained and multi-objective architecture search as well as automated data augmentation, optimizer and activation function search.



### Oriented Point Sampling for Plane Detection in Unorganized Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1905.02553v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1905.02553v1)
- **Published**: 2019-05-04 01:02:57+00:00
- **Updated**: 2019-05-04 01:02:57+00:00
- **Authors**: Bo Sun, Philippos Mordohai
- **Comment**: 7 pages, 3 figures, 2019 IEEE International Conference on Robotics
  and Automation (Accepted)
- **Journal**: None
- **Summary**: Plane detection in 3D point clouds is a crucial pre-processing step for applications such as point cloud segmentation, semantic mapping and SLAM. In contrast to many recent plane detection methods that are only applicable on organized point clouds, our work is targeted to unorganized point clouds that do not permit a 2D parametrization. We compare three methods for detecting planes in point clouds efficiently. One is a novel method proposed in this paper that generates plane hypotheses by sampling from a set of points with estimated normals. We named this method Oriented Point Sampling (OPS) to contrast with more conventional techniques that require the sampling of three unoriented points to generate plane hypotheses. We also implemented an efficient plane detection method based on local sampling of three unoriented points and compared it with OPS and the 3D-KHT algorithm, which is based on octrees, on the detection of planes on 10,000 point clouds from the SUN RGB-D dataset.



### Learning Spatio-Temporal Features with Two-Stream Deep 3D CNNs for Lipreading
- **Arxiv ID**: http://arxiv.org/abs/1905.02540v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1905.02540v2)
- **Published**: 2019-05-04 02:32:06+00:00
- **Updated**: 2019-07-19 03:19:21+00:00
- **Authors**: Xinshuo Weng, Kris Kitani
- **Comment**: camera ready version for BMVC 2019
- **Journal**: None
- **Summary**: We focus on the word-level visual lipreading, which requires recognizing the word being spoken, given only the video but not the audio. State-of-the-art methods explore the use of end-to-end neural networks, including a shallow (up to three layers) 3D convolutional neural network (CNN) + a deep 2D CNN (e.g., ResNet) as the front-end to extract visual features, and a recurrent neural network (e.g., bidirectional LSTM) as the back-end for classification. In this work, we propose to replace the shallow 3D CNNs + deep 2D CNNs front-end with recent successful deep 3D CNNs --- two-stream (i.e., grayscale video and optical flow streams) I3D. We evaluate different combinations of front-end and back-end modules with the grayscale video and optical flow inputs on the LRW dataset. The experiments show that, compared to the shallow 3D CNNs + deep 2D CNNs front-end, the deep 3D CNNs front-end with pre-training on the large-scale image and video datasets (e.g., ImageNet and Kinetics) can improve the classification accuracy. Also, we demonstrate that using the optical flow input alone can achieve comparable performance as using the grayscale video as input. Moreover, the two-stream network using both the grayscale video and optical flow inputs can further improve the performance. Overall, our two-stream I3D front-end with a Bi-LSTM back-end results in an absolute improvement of 5.3% over the previous art on the LRW dataset.



### Edge-labeling Graph Neural Network for Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.01436v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.01436v1)
- **Published**: 2019-05-04 05:58:17+00:00
- **Updated**: 2019-05-04 05:58:17+00:00
- **Authors**: Jongmin Kim, Taesup Kim, Sungwoong Kim, Chang D. Yoo
- **Comment**: accepted to CVPR 2019
- **Journal**: None
- **Summary**: In this paper, we propose a novel edge-labeling graph neural network (EGNN), which adapts a deep neural network on the edge-labeling graph, for few-shot learning. The previous graph neural network (GNN) approaches in few-shot learning have been based on the node-labeling framework, which implicitly models the intra-cluster similarity and the inter-cluster dissimilarity. In contrast, the proposed EGNN learns to predict the edge-labels rather than the node-labels on the graph that enables the evolution of an explicit clustering by iteratively updating the edge-labels with direct exploitation of both intra-cluster similarity and the inter-cluster dissimilarity. It is also well suited for performing on various numbers of classes without retraining, and can be easily extended to perform a transductive inference. The parameters of the EGNN are learned by episodic training with an edge-labeling loss to obtain a well-generalizable model for unseen low-data problem. On both of the supervised and semi-supervised few-shot image classification tasks with two benchmark datasets, the proposed EGNN significantly improves the performances over the existing GNNs.



### Leveraging Crowdsourced GPS Data for Road Extraction from Aerial Imagery
- **Arxiv ID**: http://arxiv.org/abs/1905.01447v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01447v1)
- **Published**: 2019-05-04 07:19:52+00:00
- **Updated**: 2019-05-04 07:19:52+00:00
- **Authors**: Tao Sun, Zonglin Di, Pengyu Che, Chun Liu, Yin Wang
- **Comment**: To be published in IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2019
- **Journal**: None
- **Summary**: Deep learning is revolutionizing the mapping industry. Under lightweight human curation, computer has generated almost half of the roads in Thailand on OpenStreetMap (OSM) using high-resolution aerial imagery. Bing maps are displaying 125 million computer-generated building polygons in the U.S. While tremendously more efficient than manual mapping, one cannot map out everything from the air. Especially for roads, a small prediction gap by image occlusion renders the entire road useless for routing. Misconnections can be more dangerous. Therefore computer-based mapping often requires local verifications, which is still labor intensive. In this paper, we propose to leverage crowdsourced GPS data to improve and support road extraction from aerial imagery. Through novel data augmentation, GPS rendering, and 1D transpose convolution techniques, we show almost 5% improvements over previous competition winning models, and much better robustness when predicting new areas without any new training data or domain adaptation.



### Kindling the Darkness: A Practical Low-light Image Enhancer
- **Arxiv ID**: http://arxiv.org/abs/1905.04161v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04161v1)
- **Published**: 2019-05-04 11:05:20+00:00
- **Updated**: 2019-05-04 11:05:20+00:00
- **Authors**: Yonghua Zhang, Jiawan Zhang, Xiaojie Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Images captured under low-light conditions often suffer from (partially) poor visibility. Besides unsatisfactory lightings, multiple types of degradations, such as noise and color distortion due to the limited quality of cameras, hide in the dark. In other words, solely turning up the brightness of dark regions will inevitably amplify hidden artifacts. This work builds a simple yet effective network for \textbf{Kin}dling the \textbf{D}arkness (denoted as KinD), which, inspired by Retinex theory, decomposes images into two components. One component (illumination) is responsible for light adjustment, while the other (reflectance) for degradation removal. In such a way, the original space is decoupled into two smaller subspaces, expecting to be better regularized/learned. It is worth to note that our network is trained with paired images shot under different exposure conditions, instead of using any ground-truth reflectance and illumination information. Extensive experiments are conducted to demonstrate the efficacy of our design and its superiority over state-of-the-art alternatives. Our KinD is robust against severe visual defects, and user-friendly to arbitrarily adjust light levels. In addition, our model spends less than 50ms to process an image in VGA resolution on a 2080Ti GPU. All the above merits make our KinD attractive for practical use.



### WoodScape: A multi-task, multi-camera fisheye dataset for autonomous driving
- **Arxiv ID**: http://arxiv.org/abs/1905.01489v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.01489v3)
- **Published**: 2019-05-04 13:14:12+00:00
- **Updated**: 2021-07-02 22:16:13+00:00
- **Authors**: Senthil Yogamani, Ciaran Hughes, Jonathan Horgan, Ganesh Sistu, Padraig Varley, Derek O'Dea, Michal Uricar, Stefan Milz, Martin Simon, Karl Amende, Christian Witt, Hazem Rashed, Sumanth Chennupati, Sanjaya Nayak, Saquib Mansoor, Xavier Perroton, Patrick Perez
- **Comment**: Accepted for Oral Presentation at IEEE International Conference on
  Computer Vision (ICCV) 2019. Please refer to our website
  https://woodscape.valeo.com and https://github.com/valeoai/woodscape for
  release status and updates
- **Journal**: None
- **Summary**: Fisheye cameras are commonly employed for obtaining a large field of view in surveillance, augmented reality and in particular automotive applications. In spite of their prevalence, there are few public datasets for detailed evaluation of computer vision algorithms on fisheye images. We release the first extensive fisheye automotive dataset, WoodScape, named after Robert Wood who invented the fisheye camera in 1906. WoodScape comprises of four surround view cameras and nine tasks including segmentation, depth estimation, 3D bounding box detection and soiling detection. Semantic annotation of 40 classes at the instance level is provided for over 10,000 images and annotation for other tasks are provided for over 100,000 images. With WoodScape, we would like to encourage the community to adapt computer vision models for fisheye camera instead of using naive rectification.



### SoilingNet: Soiling Detection on Automotive Surround-View Cameras
- **Arxiv ID**: http://arxiv.org/abs/1905.01492v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.01492v2)
- **Published**: 2019-05-04 13:39:48+00:00
- **Updated**: 2019-07-17 16:34:40+00:00
- **Authors**: Michal Uricar, Pavel Krizek, Ganesh Sistu, Senthil Yogamani
- **Comment**: Accepted for Oral Presentation at IEEE Intelligent Transportation
  Systems Conference (ITSC) 2019
- **Journal**: None
- **Summary**: Cameras are an essential part of sensor suite in autonomous driving. Surround-view cameras are directly exposed to external environment and are vulnerable to get soiled. Cameras have a much higher degradation in performance due to soiling compared to other sensors. Thus it is critical to accurately detect soiling on the cameras, particularly for higher levels of autonomous driving. We created a new dataset having multiple types of soiling namely opaque and transparent. It will be released publicly as part of our WoodScape dataset \cite{yogamani2019woodscape} to encourage further research. We demonstrate high accuracy using a Convolutional Neural Network (CNN) based architecture. We also show that it can be combined with the existing object detection task in a multi-task learning framework. Finally, we make use of Generative Adversarial Networks (GANs) to generate more images for data augmentation and show that it works successfully similar to the style transfer.



### Face Hallucination by Attentive Sequence Optimization with Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.01509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01509v1)
- **Published**: 2019-05-04 15:01:57+00:00
- **Updated**: 2019-05-04 15:01:57+00:00
- **Authors**: Yukai Shi, Guanbin Li, Qingxing Cao, Keze Wang, Liang Lin
- **Comment**: To be published in TPAMI
- **Journal**: None
- **Summary**: Face hallucination is a domain-specific super-resolution problem that aims to generate a high-resolution (HR) face image from a low-resolution~(LR) input. In contrast to the existing patch-wise super-resolution models that divide a face image into regular patches and independently apply LR to HR mapping to each patch, we implement deep reinforcement learning and develop a novel attention-aware face hallucination (Attention-FH) framework, which recurrently learns to attend a sequence of patches and performs facial part enhancement by fully exploiting the global interdependency of the image. Specifically, our proposed framework incorporates two components: a recurrent policy network for dynamically specifying a new attended region at each time step based on the status of the super-resolved image and the past attended region sequence, and a local enhancement network for selected patch hallucination and global state updating. The Attention-FH model jointly learns the recurrent policy network and local enhancement network through maximizing a long-term reward that reflects the hallucination result with respect to the whole HR image. Extensive experiments demonstrate that our Attention-FH significantly outperforms the state-of-the-art methods on in-the-wild face images with large pose and illumination variations.



### Deep 3D Convolutional Neural Network for Automated Lung Cancer Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/1906.01054v1
- **DOI**: 10.1007/978-981-13-7150-9_16
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.01054v1)
- **Published**: 2019-05-04 15:56:50+00:00
- **Updated**: 2019-05-04 15:56:50+00:00
- **Authors**: Sumita Mishra, Naresh Kumar Chaudhary, Pallavi Asthana, Anil Kumar
- **Comment**: Initial draft of PAPER Presented at IRSCNS 2018 , Goa , India final
  version available at Mishra S., Chaudhary N.K., Asthana P., Kumar A. (2019)
  Deep 3D Convolutional Neural Network for Automated Lung Cancer Diagnosis. In:
  Peng SL., Dey N., Bundele M. (eds) Computing and Network Sustainability.
  Lecture Notes in Networks and Systems, vol 75. Springer, Singapore
- **Journal**: None
- **Summary**: Computer Aided Diagnosis has emerged as an indispensible technique for validating the opinion of radiologists in CT interpretation. This paper presents a deep 3D Convolutional Neural Network (CNN) architecture for automated CT scan-based lung cancer detection system. It utilizes three dimensional spatial information to learn highly discriminative 3 dimensional features instead of 2D features like texture or geometric shape whick need to be generated manually. The proposed deep learning method automatically extracts the 3D features on the basis of spatio-temporal statistics.The developed model is end-to-end and is able to predict malignancy of each voxel for given input scan. Simulation results demonstrate the effectiveness of proposed 3D CNN network for classification of lung nodule in-spite of limited computational capabilities.



### Automated building image extraction from 360° panoramas for postdisaster evaluation
- **Arxiv ID**: http://arxiv.org/abs/1905.01524v2
- **DOI**: 10.1111/mice.12493
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01524v2)
- **Published**: 2019-05-04 16:58:37+00:00
- **Updated**: 2019-11-05 16:53:38+00:00
- **Authors**: Ali Lenjani, Chul Min Yeum, Shirley Dyke, Ilias Bilionis
- **Comment**: None
- **Journal**: Computer Aided Civil and Infrastructure Engineering (2019)
- **Summary**: After a disaster, teams of structural engineers collect vast amounts of images from damaged buildings to obtain new knowledge and extract lessons from the event. However, in many cases, the images collected are captured without sufficient spatial context. When damage is severe, it may be quite difficult to even recognize the building. Accessing images of the pre-disaster condition of those buildings is required to accurately identify the cause of the failure or the actual loss in the building. Here, to address this issue, we develop a method to automatically extract pre-event building images from 360o panorama images (panoramas). By providing a geotagged image collected near the target building as the input, panoramas close to the input image location are automatically downloaded through street view services (e.g., Google or Bing in the United States). By computing the geometric relationship between the panoramas and the target building, the most suitable projection direction for each panorama is identified to generate high-quality 2D images of the building. Region-based convolutional neural networks are exploited to recognize the building within those 2D images. Several panoramas are used so that the detected building images provide various viewpoints of the building. To demonstrate the capability of the technique, we consider residential buildings in Holiday Beach, Texas, the United States which experienced significant devastation in Hurricane Harvey in 2017. Using geotagged images gathered during actual post-disaster building reconnaissance missions, we verify the method by successfully extracting residential building images from Google Street View images, which were captured before the event.



### Mapping Missing Population in Rural India: A Deep Learning Approach with Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/1905.02196v1
- **DOI**: 10.1145/3306618.3314263
- **Categories**: **cs.CV**, eess.IV, I.2.10; I.2.6; J.2; J.4
- **Links**: [PDF](http://arxiv.org/pdf/1905.02196v1)
- **Published**: 2019-05-04 18:33:22+00:00
- **Updated**: 2019-05-04 18:33:22+00:00
- **Authors**: Wenjie Hu, Jay Harshadbhai Patel, Zoe-Alanah Robert, Paul Novosad, Samuel Asher, Zhongyi Tang, Marshall Burke, David Lobell, Stefano Ermon
- **Comment**: 7 pages
- **Journal**: AAAI/ACM Conference on AI, Ethics, and Society (AIES '19), January
  27-28, 2019, Honolulu, HI, USA
- **Summary**: Millions of people worldwide are absent from their country's census. Accurate, current, and granular population metrics are critical to improving government allocation of resources, to measuring disease control, to responding to natural disasters, and to studying any aspect of human life in these communities. Satellite imagery can provide sufficient information to build a population map without the cost and time of a government census. We present two Convolutional Neural Network (CNN) architectures which efficiently and effectively combine satellite imagery inputs from multiple sources to accurately predict the population density of a region. In this paper, we use satellite imagery from rural villages in India and population labels from the 2011 SECC census. Our best model achieves better performance than previous papers as well as LandScan, a community standard for global population distribution.



### Back to the Future: Predicting Traffic Shockwave Formation and Propagation Using a Convolutional Encoder-Decoder Network
- **Arxiv ID**: http://arxiv.org/abs/1905.02197v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1905.02197v1)
- **Published**: 2019-05-04 18:33:55+00:00
- **Updated**: 2019-05-04 18:33:55+00:00
- **Authors**: Mohammadreza Khajeh-Hosseini, Alireza Talebpour
- **Comment**: None
- **Journal**: None
- **Summary**: This study proposes a deep learning methodology to predict the propagation of traffic shockwaves. The input to the deep neural network is time-space diagram of the study segment, and the output of the network is the predicted (future) propagation of the shockwave on the study segment in the form of time-space diagram. The main feature of the proposed methodology is the ability to extract the features embedded in the time-space diagram to predict the propagation of traffic shockwaves.



### Single Image 3D Hand Reconstruction with Mesh Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1905.01326v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01326v3)
- **Published**: 2019-05-04 20:41:47+00:00
- **Updated**: 2019-08-05 12:07:34+00:00
- **Authors**: Dominik Kulon, Haoyang Wang, Riza Alp Güler, Michael Bronstein, Stefanos Zafeiriou
- **Comment**: Proceedings of the British Machine Vision Conference (BMVC 2019)
- **Journal**: None
- **Summary**: Monocular 3D reconstruction of deformable objects, such as human body parts, has been typically approached by predicting parameters of heavyweight linear models. In this paper, we demonstrate an alternative solution that is based on the idea of encoding images into a latent non-linear representation of meshes. The prior on 3D hand shapes is learned by training an autoencoder with intrinsic graph convolutions performed in the spectral domain. The pre-trained decoder acts as a non-linear statistical deformable model. The latent parameters that reconstruct the shape and articulated pose of hands in the image are predicted using an image encoder. We show that our system reconstructs plausible meshes and operates in real-time. We evaluate the quality of the mesh reconstructions produced by the decoder on a new dataset and show latent space interpolation results. Our code, data, and models will be made publicly available.



### Deep Multi-Sensor Lane Detection
- **Arxiv ID**: http://arxiv.org/abs/1905.01555v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01555v1)
- **Published**: 2019-05-04 20:43:42+00:00
- **Updated**: 2019-05-04 20:43:42+00:00
- **Authors**: Min Bai, Gellert Mattyus, Namdar Homayounfar, Shenlong Wang, Shrinidhi Kowshika Lakshmikanth, Raquel Urtasun
- **Comment**: IEEE International Conference on Intelligent Robots and Systems
  (IROS) 2018
- **Journal**: None
- **Summary**: Reliable and accurate lane detection has been a long-standing problem in the field of autonomous driving. In recent years, many approaches have been developed that use images (or videos) as input and reason in image space. In this paper we argue that accurate image estimates do not translate to precise 3D lane boundaries, which are the input required by modern motion planning algorithms. To address this issue, we propose a novel deep neural network that takes advantage of both LiDAR and camera sensors and produces very accurate estimates directly in 3D space. We demonstrate the performance of our approach on both highways and in cities, and show very accurate estimates in complex scenarios such as heavy traffic (which produces occlusion), fork, merges and intersections.



### Human Gait Database for Normal Walk Collected by Smartphone Accelerometer
- **Arxiv ID**: http://arxiv.org/abs/1905.03109v5
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.03109v5)
- **Published**: 2019-05-04 22:35:09+00:00
- **Updated**: 2023-05-16 13:43:08+00:00
- **Authors**: Amir Vajdi, Mohammad Reza Zaghian, Nazli Rafei Dehkordi, Elham Rastegari, Kian Maroofi, Saman Farahmand, Shaohua Jia, Marc Pomplun, Nurit Haspel, Akram Bayat
- **Comment**: There was a lack of method description and we suggest to use the
  previous version of the article where we provided a more extensive
  methodology
- **Journal**: None
- **Summary**: Gait recognition is the characterization of unique biometric patterns associated with each individual which can be utilized to identify a person without direct contact. A public gait database with a relatively large number of subjects can provide a great opportunity for future studies to build and validate gait authentication models. The goal of this study is to introduce a comprehensive gait database of 93 human subjects who walked between two endpoints (320 meters) during two different sessions and record their gait data using two smartphones, one attached to the right thigh and another one on the left side of the waist. This data is collected to be utilized by a deep learning-based method that requires enough time points. The metadata including age, gender, smoking, daily exercise time, height, and weight of an individual is recorded. this data set is publicly available.



### A Similarity Measure for Material Appearance
- **Arxiv ID**: http://arxiv.org/abs/1905.01562v1
- **DOI**: 10.1145/3306346.3323036
- **Categories**: **cs.GR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.01562v1)
- **Published**: 2019-05-04 22:48:27+00:00
- **Updated**: 2019-05-04 22:48:27+00:00
- **Authors**: Manuel Lagunas, Sandra Malpica, Ana Serrano, Elena Garces, Diego Gutierrez, Belen Masia
- **Comment**: 12 pages, 17 figures
- **Journal**: ACM Transactions on Graphics (SIGGRAPH 2019)
- **Summary**: We present a model to measure the similarity in appearance between different materials, which correlates with human similarity judgments. We first create a database of 9,000 rendered images depicting objects with varying materials, shape and illumination. We then gather data on perceived similarity from crowdsourced experiments; our analysis of over 114,840 answers suggests that indeed a shared perception of appearance similarity exists. We feed this data to a deep learning architecture with a novel loss function, which learns a feature space for materials that correlates with such perceived appearance similarity. Our evaluation shows that our model outperforms existing metrics. Last, we demonstrate several applications enabled by our metric, including appearance-based search for material suggestions, database visualization, clustering and summarization, and gamut mapping.



