# Arxiv Papers in cs.CV on 2019-05-05
### A Joint Convolutional Neural Networks and Context Transfer for Street Scenes Labeling
- **Arxiv ID**: http://arxiv.org/abs/1905.01574v1
- **DOI**: 10.1109/TITS.2017.2726546
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01574v1)
- **Published**: 2019-05-05 01:24:19+00:00
- **Updated**: 2019-05-05 01:24:19+00:00
- **Authors**: Qi Wang, Junyu Gao, Yuan Yuan
- **Comment**: IEEE T-ITS 2018
- **Journal**: None
- **Summary**: Street scene understanding is an essential task for autonomous driving. One important step towards this direction is scene labeling, which annotates each pixel in the images with a correct class label. Although many approaches have been developed, there are still some weak points. Firstly, many methods are based on the hand-crafted features whose image representation ability is limited. Secondly, they can not label foreground objects accurately due to the dataset bias. Thirdly, in the refinement stage, the traditional Markov Random Filed (MRF) inference is prone to over smoothness. For improving the above problems, this paper proposes a joint method of priori convolutional neural networks at superpixel level (called as ``priori s-CNNs'') and soft restricted context transfer. Our contributions are threefold: (1) A priori s-CNNs model that learns priori location information at superpixel level is proposed to describe various objects discriminatingly; (2) A hierarchical data augmentation method is presented to alleviate dataset bias in the priori s-CNNs training stage, which improves foreground objects labeling significantly; (3) A soft restricted MRF energy function is defined to improve the priori s-CNNs model's labeling performance and reduce the over smoothness at the same time. The proposed approach is verified on CamVid dataset (11 classes) and SIFT Flow Street dataset (16 classes) and achieves competitive performance.



### Embedding Structured Contour and Location Prior in Siamesed Fully Convolutional Networks for Road Detection
- **Arxiv ID**: http://arxiv.org/abs/1905.01575v1
- **DOI**: 10.1109/TITS.2017.2749964
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01575v1)
- **Published**: 2019-05-05 01:31:30+00:00
- **Updated**: 2019-05-05 01:31:30+00:00
- **Authors**: Qi Wang, Junyu Gao, Yuan Yuan
- **Comment**: IEEE T-ITS 2018
- **Journal**: None
- **Summary**: Road detection from the perspective of moving vehicles is a challenging issue in autonomous driving. Recently, many deep learning methods spring up for this task because they can extract high-level local features to find road regions from raw RGB data, such as Convolutional Neural Networks (CNN) and Fully Convolutional Networks (FCN). However, how to detect the boundary of road accurately is still an intractable problem. In this paper, we propose a siamesed fully convolutional networks (named as ``s-FCN-loc''), which is able to consider RGB-channel images, semantic contours and location priors simultaneously to segment road region elaborately. To be specific, the s-FCN-loc has two streams to process the original RGB images and contour maps respectively. At the same time, the location prior is directly appended to the siamesed FCN to promote the final detection performance. Our contributions are threefold: (1) An s-FCN-loc is proposed that learns more discriminative features of road boundaries than the original FCN to detect more accurate road regions; (2) Location prior is viewed as a type of feature map and directly appended to the final feature map in s-FCN-loc to promote the detection performance effectively, which is easier than other traditional methods, namely different priors for different inputs (image patches); (3) The convergent speed of training s-FCN-loc model is 30\% faster than the original FCN, because of the guidance of highly structured contours. The proposed approach is evaluated on KITTI Road Detection Benchmark and One-Class Road Detection Dataset, and achieves a competitive result with state of the arts.



### VSSA-NET: Vertical Spatial Sequence Attention Network for Traffic Sign Detection
- **Arxiv ID**: http://arxiv.org/abs/1905.01583v1
- **DOI**: 10.1109/TIP.2019.2896952
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01583v1)
- **Published**: 2019-05-05 02:16:43+00:00
- **Updated**: 2019-05-05 02:16:43+00:00
- **Authors**: Yuan Yuan, Zhitong Xiong, Qi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Although traffic sign detection has been studied for years and great progress has been made with the rise of deep learning technique, there are still many problems remaining to be addressed. For complicated real-world traffic scenes, there are two main challenges. Firstly, traffic signs are usually small size objects, which makes it more difficult to detect than large ones; Secondly, it is hard to distinguish false targets which resemble real traffic signs in complex street scenes without context information. To handle these problems, we propose a novel end-to-end deep learning method for traffic sign detection in complex environments. Our contributions are as follows: 1) We propose a multi-resolution feature fusion network architecture which exploits densely connected deconvolution layers with skip connections, and can learn more effective features for the small size object; 2) We frame the traffic sign detection as a spatial sequence classification and regression task, and propose a vertical spatial sequence attention (VSSA) module to gain more context information for better detection performance. To comprehensively evaluate the proposed method, we do experiments on several traffic sign datasets as well as the general object detection dataset and the results have shown the effectiveness of our proposed method.



### Accurate Face Detection for High Performance
- **Arxiv ID**: http://arxiv.org/abs/1905.01585v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01585v3)
- **Published**: 2019-05-05 02:34:57+00:00
- **Updated**: 2019-05-24 05:58:18+00:00
- **Authors**: Faen Zhang, Xinyu Fan, Guo Ai, Jianfei Song, Yongqiang Qin, Jiahong Wu
- **Comment**: 9 pages, 3 figures, technical report
- **Journal**: None
- **Summary**: Face detection has witnessed significant progress due to the advances of deep convolutional neural networks (CNNs). Its central issue in recent years is how to improve the detection performance of tiny faces. To this end, many recent works propose some specific strategies, redesign the architecture and introduce new loss functions for tiny object detection. In this report, we start from the popular one-stage RetinaNet approach and apply some recent tricks to obtain a high performance face detector. Specifically, we apply the Intersection over Union (IoU) loss function for regression, employ the two-step classification and regression for detection, revisit the data augmentation based on data-anchor-sampling for training, utilize the max-out operation for classification and use the multi-scale testing strategy for inference. As a consequence, the proposed face detection method achieves state-of-the-art performance on the most popular and challenging face detection benchmark WIDER FACE dataset.



### On Exploring Undetermined Relationships for Visual Relationship Detection
- **Arxiv ID**: http://arxiv.org/abs/1905.01595v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.01595v1)
- **Published**: 2019-05-05 03:57:12+00:00
- **Updated**: 2019-05-05 03:57:12+00:00
- **Authors**: Yibing Zhan, Jun Yu, Ting Yu, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: In visual relationship detection, human-notated relationships can be regarded as determinate relationships. However, there are still large amount of unlabeled data, such as object pairs with less significant relationships or even with no relationships. We refer to these unlabeled but potentially useful data as undetermined relationships. Although a vast body of literature exists, few methods exploit these undetermined relationships for visual relationship detection.   In this paper, we explore the beneficial effect of undetermined relationships on visual relationship detection. We propose a novel multi-modal feature based undetermined relationship learning network (MF-URLN) and achieve great improvements in relationship detection. In detail, our MF-URLN automatically generates undetermined relationships by comparing object pairs with human-notated data according to a designed criterion. Then, the MF-URLN extracts and fuses features of object pairs from three complementary modals: visual, spatial, and linguistic modals. Further, the MF-URLN proposes two correlated subnetworks: one subnetwork decides the determinate confidence, and the other predicts the relationships. We evaluate the MF-URLN on two datasets: the Visual Relationship Detection (VRD) and the Visual Genome (VG) datasets. The experimental results compared with state-of-the-art methods verify the significant improvements made by the undetermined relationships, e.g., the top-50 relation detection recall improves from 19.5% to 23.9% on the VRD dataset.



### PasteGAN: A Semi-Parametric Method to Generate Image from Scene Graph
- **Arxiv ID**: http://arxiv.org/abs/1905.01608v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01608v2)
- **Published**: 2019-05-05 05:16:09+00:00
- **Updated**: 2019-11-28 03:19:19+00:00
- **Authors**: Yikang Li, Tao Ma, Yeqi Bai, Nan Duan, Sining Wei, Xiaogang Wang
- **Comment**: 10 pages, 6 figures; Accepted by NeurIPS 2019
- **Journal**: None
- **Summary**: Despite some exciting progress on high-quality image generation from structured(scene graphs) or free-form(sentences) descriptions, most of them only guarantee the image-level semantical consistency, i.e. the generated image matching the semantic meaning of the description. They still lack the investigations on synthesizing the images in a more controllable way, like finely manipulating the visual appearance of every object. Therefore, to generate the images with preferred objects and rich interactions, we propose a semi-parametric method, PasteGAN, for generating the image from the scene graph and the image crops, where spatial arrangements of the objects and their pair-wise relationships are defined by the scene graph and the object appearances are determined by the given object crops. To enhance the interactions of the objects in the output, we design a Crop Refining Network and an Object-Image Fuser to embed the objects as well as their relationships into one map. Multiple losses work collaboratively to guarantee the generated images highly respecting the crops and complying with the scene graphs while maintaining excellent image quality. A crop selector is also proposed to pick the most-compatible crops from our external object tank by encoding the interactions around the objects in the scene graph if the crops are not provided. Evaluated on Visual Genome and COCO-Stuff dataset, our proposed method significantly outperforms the SOTA methods on Inception Score, Diversity Score and Fr\'echet Inception Distance. Extensive experiments also demonstrate our method's ability to generate complex and diverse images with given objects.



### A Review of Object Detection Models based on Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1905.01614v3
- **DOI**: 10.1007/978-981-15-4288-6_1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01614v3)
- **Published**: 2019-05-05 05:45:21+00:00
- **Updated**: 2019-10-01 10:27:07+00:00
- **Authors**: F. Sultana, A. Sufian, P. Dutta
- **Comment**: 17 pages, 11 figures, 1 table
- **Journal**: Intelligent Computing: Image Processing Based Applications.
  Advances in Intelligent Systems and Computing, vol 1157, pages 1-16, 2020
- **Summary**: Convolutional Neural Network (CNN) has become the state-of-the-art for object detection in image task. In this chapter, we have explained different state-of-the-art CNN based object detection models. We have made this review with categorization those detection models according to two different approaches: two-stage approach and one-stage approach. Through this chapter, it has shown advancements in object detection models from R-CNN to latest RefineDet. It has also discussed the model description and training details of each model. Here, we have also drawn a comparison among those models.



### Conditional Generative Neural System for Probabilistic Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/1905.01631v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.01631v2)
- **Published**: 2019-05-05 08:19:50+00:00
- **Updated**: 2019-07-28 08:26:20+00:00
- **Authors**: Jiachen Li, Hengbo Ma, Masayoshi Tomizuka
- **Comment**: Camera ready for IROS 2019
- **Journal**: None
- **Summary**: Effective understanding of the environment and accurate trajectory prediction of surrounding dynamic obstacles are critical for intelligent systems such as autonomous vehicles and wheeled mobile robotics navigating in complex scenarios to achieve safe and high-quality decision making, motion planning and control. Due to the uncertain nature of the future, it is desired to make inference from a probability perspective instead of deterministic prediction. In this paper, we propose a conditional generative neural system (CGNS) for probabilistic trajectory prediction to approximate the data distribution, with which realistic, feasible and diverse future trajectory hypotheses can be sampled. The system combines the strengths of conditional latent space learning and variational divergence minimization, and leverages both static context and interaction information with soft attention mechanisms. We also propose a regularization method for incorporating soft constraints into deep neural networks with differentiable barrier functions, which can regulate and push the generated samples into the feasible regions. The proposed system is evaluated on several public benchmark datasets for pedestrian trajectory prediction and a roundabout naturalistic driving dataset collected by ourselves. The experimental results demonstrate that our model achieves better performance than various baseline approaches in terms of prediction accuracy.



### Learning by Inertia: Self-supervised Monocular Visual Odometry for Road Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1905.01634v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01634v1)
- **Published**: 2019-05-05 08:58:35+00:00
- **Updated**: 2019-05-05 08:58:35+00:00
- **Authors**: Chengze Wang, Yuan Yuan, Qi Wang
- **Comment**: 2019 IEEE. Personal use of this material is permitted. Permission
  from IEEE must be obtained for all other uses, in any current or future
  media, including reprinting/republishing this material for advertising or
  promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: None
- **Summary**: In this paper, we present iDVO (inertia-embedded deep visual odometry), a self-supervised learning based monocular visual odometry (VO) for road vehicles. When modelling the geometric consistency within adjacent frames, most deep VO methods ignore the temporal continuity of the camera pose, which results in a very severe jagged fluctuation in the velocity curves. With the observation that road vehicles tend to perform smooth dynamic characteristics in most of the time, we design the inertia loss function to describe the abnormal motion variation, which assists the model to learn the consecutiveness from long-term camera ego-motion. Based on the recurrent convolutional neural network (RCNN) architecture, our method implicitly models the dynamics of road vehicles and the temporal consecutiveness by the extended Long Short-Term Memory (LSTM) block. Furthermore, we develop the dynamic hard-edge mask to handle the non-consistency in fast camera motion by blocking the boundary part and which generates more efficiency in the whole non-consistency mask. The proposed method is evaluated on the KITTI dataset, and the results demonstrate state-of-the-art performance with respect to other monocular deep VO and SLAM approaches.



### A Methodological Review of Visual Road Recognition Procedures for Autonomous Driving Applications
- **Arxiv ID**: http://arxiv.org/abs/1905.01635v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.01635v1)
- **Published**: 2019-05-05 09:03:06+00:00
- **Updated**: 2019-05-05 09:03:06+00:00
- **Authors**: Kai Li Lim, Thomas Bräunl
- **Comment**: 14 pages, 6 Figures, 2 Tables. Permission to reprint granted from
  original figure authors
- **Journal**: None
- **Summary**: The current research interest in autonomous driving is growing at a rapid pace, attracting great investments from both the academic and corporate sectors. In order for vehicles to be fully autonomous, it is imperative that the driver assistance system is adapt in road and lane keeping. In this paper, we present a methodological review of techniques with a focus on visual road detection and recognition. We adopt a pragmatic outlook in presenting this review, whereby the procedures of road recognition is emphasised with respect to its practical implementations. The contribution of this review hence covers the topic in two parts -- the first part describes the methodological approach to conventional road detection, which covers the algorithms and approaches involved to classify and segregate roads from non-road regions; and the other part focuses on recent state-of-the-art machine learning techniques that are applied to visual road recognition, with an emphasis on methods that incorporate convolutional neural networks and semantic segmentation. A subsequent overview of recent implementations in the commercial sector is also presented, along with some recent research works pertaining to road detections.



### Deep Video Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1905.01639v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01639v1)
- **Published**: 2019-05-05 09:23:35+00:00
- **Updated**: 2019-05-05 09:23:35+00:00
- **Authors**: Dahun Kim, Sanghyun Woo, Joon-Young Lee, In So Kweon
- **Comment**: Accepted at CVPR 2019
- **Journal**: None
- **Summary**: Video inpainting aims to fill spatio-temporal holes with plausible content in a video. Despite tremendous progress of deep neural networks for image inpainting, it is challenging to extend these methods to the video domain due to the additional time dimension. In this work, we propose a novel deep network architecture for fast video inpainting. Built upon an image-based encoder-decoder model, our framework is designed to collect and refine information from neighbor frames and synthesize still-unknown regions. At the same time, the output is enforced to be temporally consistent by a recurrent feedback and a temporal memory module. Compared with the state-of-the-art image inpainting algorithm, our method produces videos that are much more semantically correct and temporally smooth. In contrast to the prior video completion method which relies on time-consuming optimization, our method runs in near real-time while generating competitive video results. Finally, we applied our framework to video retargeting task, and obtain visually pleasing results.



### Towards More Realistic Human-Robot Conversation: A Seq2Seq-based Body Gesture Interaction System
- **Arxiv ID**: http://arxiv.org/abs/1905.01641v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01641v3)
- **Published**: 2019-05-05 09:53:29+00:00
- **Updated**: 2019-11-15 06:36:28+00:00
- **Authors**: Minjie Hua, Fuyuan Shi, Yibing Nan, Kai Wang, Hao Chen, Shiguo Lian
- **Comment**: Accepted by IROS 2019. Slides: https://youtu.be/zOp6tT_etQE
- **Journal**: None
- **Summary**: This paper presents a novel system that enables intelligent robots to exhibit realistic body gestures while communicating with humans. The proposed system consists of a listening model and a speaking model used in corresponding conversational phases. Both models are adapted from the sequence-to-sequence (seq2seq) architecture to synthesize body gestures represented by the movements of twelve upper-body keypoints. All the extracted 2D keypoints are firstly 3D-transformed, then rotated and normalized to discard irrelevant information. Substantial videos of human conversations from Youtube are collected and preprocessed to train the listening and speaking models separately, after which the two models are evaluated using metrics of mean squared error (MSE) and cosine similarity on the test dataset. The tuned system is implemented to drive a virtual avatar as well as Pepper, a physical humanoid robot, to demonstrate the improvement on conversational interaction abilities of our method in practice.



### Deep Convolutional Neural Network-Based Autonomous Drone Navigation
- **Arxiv ID**: http://arxiv.org/abs/1905.01657v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01657v1)
- **Published**: 2019-05-05 11:25:24+00:00
- **Updated**: 2019-05-05 11:25:24+00:00
- **Authors**: K. Amer, M. Samy, M. Shaker, M. ElHelw
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel approach for aerial drone autonomous navigation along predetermined paths using only visual input form an onboard camera and without reliance on a Global Positioning System (GPS). It is based on using a deep Convolutional Neural Network (CNN) combined with a regressor to output the drone steering commands. Furthermore, multiple auxiliary navigation paths that form a navigation envelope are used for data augmentation to make the system adaptable to real-life deployment scenarios. The approach is suitable for automating drone navigation in applications that exhibit regular trips or visits to same locations such as environmental and desertification monitoring, parcel/aid delivery and drone-based wireless internet delivery. In this case, the proposed algorithm replaces human operators, enhances accuracy of GPS-based map navigation, alleviates problems related to GPS-spoofing and enables navigation in GPS-denied environments. Our system is tested in two scenarios using the Unreal Engine-based AirSim plugin for drone simulation with promising results of average cross track distance less than 1.4 meters and mean waypoints minimum distance of less than 1 meter.



### Drone Path-Following in GPS-Denied Environments using Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.01658v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01658v1)
- **Published**: 2019-05-05 11:25:54+00:00
- **Updated**: 2019-05-05 11:25:54+00:00
- **Authors**: M. Samy, K. Amer, M. Shaker, M. ElHelw
- **Comment**: None
- **Journal**: None
- **Summary**: his paper presents a simple approach for drone navigation to follow a predetermined path using visual input only without reliance on a Global Positioning System (GPS). A Convolutional Neural Network (CNN) is used to output the steering command of the drone in an end-to-end approach. We tested our approach in two simulated environments in the Unreal Engine using the AirSim plugin for drone simulation. Results show that the proposed approach, despite its simplicity, has average cross track distance less than 2.9 meters in the simulated environment. We also investigate the significance of data augmentation in path following. Finally, we conclude by suggesting possible enhancements for extending our approach to more difficult paths in real life, in the hope that one day visual navigation will become the norm in GPS-denied zones.



### GETNET: A General End-to-end Two-dimensional CNN Framework for Hyperspectral Image Change Detection
- **Arxiv ID**: http://arxiv.org/abs/1905.01662v1
- **DOI**: 10.1109/TGRS.2018.2849692
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.01662v1)
- **Published**: 2019-05-05 11:36:53+00:00
- **Updated**: 2019-05-05 11:36:53+00:00
- **Authors**: Qi Wang, Zhenghang Yuan, Qian Du, Xuelong Li
- **Comment**: None
- **Journal**: None
- **Summary**: Change detection (CD) is an important application of remote sensing, which provides timely change information about large-scale Earth surface. With the emergence of hyperspectral imagery, CD technology has been greatly promoted, as hyperspectral data with the highspectral resolution are capable of detecting finer changes than using the traditional multispectral imagery. Nevertheless, the high dimension of hyperspectral data makes it difficult to implement traditional CD algorithms. Besides, endmember abundance information at subpixel level is often not fully utilized. In order to better handle high dimension problem and explore abundance information, this paper presents a General End-to-end Two-dimensional CNN (GETNET) framework for hyperspectral image change detection (HSI-CD). The main contributions of this work are threefold: 1) Mixed-affinity matrix that integrates subpixel representation is introduced to mine more cross-channel gradient features and fuse multi-source information; 2) 2-D CNN is designed to learn the discriminative features effectively from multi-source data at a higher level and enhance the generalization ability of the proposed CD algorithm; 3) A new HSI-CD data set is designed for the objective comparison of different methods. Experimental results on real hyperspectral data sets demonstrate the proposed method outperforms most of the state-of-the-arts.



### Learning Character-Agnostic Motion for Motion Retargeting in 2D
- **Arxiv ID**: http://arxiv.org/abs/1905.01680v1
- **DOI**: 10.1145/3306346.3322999
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01680v1)
- **Published**: 2019-05-05 13:16:53+00:00
- **Updated**: 2019-05-05 13:16:53+00:00
- **Authors**: Kfir Aberman, Rundi Wu, Dani Lischinski, Baoquan Chen, Daniel Cohen-Or
- **Comment**: SIGGRAPH 2019. arXiv admin note: text overlap with arXiv:1804.05653
  by other authors
- **Journal**: None
- **Summary**: Analyzing human motion is a challenging task with a wide variety of applications in computer vision and in graphics. One such application, of particular importance in computer animation, is the retargeting of motion from one performer to another. While humans move in three dimensions, the vast majority of human motions are captured using video, requiring 2D-to-3D pose and camera recovery, before existing retargeting approaches may be applied. In this paper, we present a new method for retargeting video-captured motion between different human performers, without the need to explicitly reconstruct 3D poses and/or camera parameters. In order to achieve our goal, we learn to extract, directly from a video, a high-level latent motion representation, which is invariant to the skeleton geometry and the camera view. Our key idea is to train a deep neural network to decompose temporal sequences of 2D poses into three components: motion, skeleton, and camera view-angle. Having extracted such a representation, we are able to re-combine motion with novel skeletons and camera views, and decode a retargeted temporal sequence, which we compare to a ground truth from a synthetic dataset. We demonstrate that our framework can be used to robustly extract human motion from videos, bypassing 3D reconstruction, and outperforming existing retargeting methods, when applied to videos in-the-wild. It also enables additional applications, such as performance cloning, video-driven cartoons, and motion retrieval.



### Deep Discriminative Clustering Analysis
- **Arxiv ID**: http://arxiv.org/abs/1905.01681v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.01681v1)
- **Published**: 2019-05-05 13:22:03+00:00
- **Updated**: 2019-05-05 13:22:03+00:00
- **Authors**: Jianlong Chang, Yiwen Guo, Lingfeng Wang, Gaofeng Meng, Shiming Xiang, Chunhong Pan
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional clustering methods often perform clustering with low-level indiscriminative representations and ignore relationships between patterns, resulting in slight achievements in the era of deep learning. To handle this problem, we develop Deep Discriminative Clustering (DDC) that models the clustering task by investigating relationships between patterns with a deep neural network. Technically, a global constraint is introduced to adaptively estimate the relationships, and a local constraint is developed to endow the network with the capability of learning high-level discriminative representations. By iteratively training the network and estimating the relationships in a mini-batch manner, DDC theoretically converges and the trained network enables to generate a group of discriminative representations that can be treated as clustering centers for straightway clustering. Extensive experiments strongly demonstrate that DDC outperforms current methods on eight image, text and audio datasets concurrently.



### Unsupervised Detection of Distinctive Regions on 3D Shapes
- **Arxiv ID**: http://arxiv.org/abs/1905.01684v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.01684v2)
- **Published**: 2019-05-05 13:41:05+00:00
- **Updated**: 2020-04-20 22:59:05+00:00
- **Authors**: Xianzhi Li, Lequan Yu, Chi-Wing Fu, Daniel Cohen-Or, Pheng-Ann Heng
- **Comment**: Accepted by ACM TOG
- **Journal**: None
- **Summary**: This paper presents a novel approach to learn and detect distinctive regions on 3D shapes. Unlike previous works, which require labeled data, our method is unsupervised. We conduct the analysis on point sets sampled from 3D shapes, then formulate and train a deep neural network for an unsupervised shape clustering task to learn local and global features for distinguishing shapes with respect to a given shape set. To drive the network to learn in an unsupervised manner, we design a clustering-based nonparametric softmax classifier with an iterative re-clustering of shapes, and an adapted contrastive loss for enhancing the feature embedding quality and stabilizing the learning process. By then, we encourage the network to learn the point distinctiveness on the input shapes. We extensively evaluate various aspects of our approach and present its applications for distinctiveness-guided shape retrieval, sampling, and view selection in 3D scenes.



### Tuned Inception V3 for Recognizing States of Cooking Ingredients
- **Arxiv ID**: http://arxiv.org/abs/1905.03715v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.03715v1)
- **Published**: 2019-05-05 16:38:52+00:00
- **Updated**: 2019-05-05 16:38:52+00:00
- **Authors**: Kin Ng
- **Comment**: None
- **Journal**: None
- **Summary**: Cooking is a task that must be performed in a daily basis, and thus it is an activity that many people take for granted. For humans preparing a meal comes naturally, but for robots even preparing a simple sandwich results in an extremely difficult task. In robotics, designing kitchen robots is complicated since cooking relies on a variety of physical interactions that are dependent on different conditions such as changes in the environment, proper execution of sequential instructions, along with motions, and detection of the different states in which cooking-ingredients can be in for their correct grasping and manipulation. In this paper, we focus on the challenge of state recognition and propose a fine tuned convolutional neural network that makes use of transfer learning by reusing the Inception V3 pre-trained model. The model is trained and validated on a cooking dataset consisting of eleven states (e.g. peeled, diced, whole, etc.). The work presented on this paper could provide insight into finding a potential solution to the problem.



### Intra-clip Aggregation for Video Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1905.01722v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.01722v4)
- **Published**: 2019-05-05 17:37:33+00:00
- **Updated**: 2021-08-16 17:39:50+00:00
- **Authors**: Takashi Isobe, Jian Han, Fang Zhu, Yali Li, Shengjin Wang
- **Comment**: ICIP 2020
- **Journal**: None
- **Summary**: Video-based person re-identification has drawn massive attention in recent years due to its extensive applications in video surveillance. While deep learning-based methods have led to significant progress, these methods are limited by ineffectively using complementary information, which is blamed on necessary data augmentation in the training process. Data augmentation has been widely used to mitigate the over-fitting trap and improve the ability of network representation. However, the previous methods adopt image-based data augmentation scheme to individually process the input frames, which corrupts the complementary information between consecutive frames and causes performance degradation. Extensive experiments on three benchmark datasets demonstrate that our framework outperforms the most recent state-of-the-art methods. We also perform cross-dataset validation to prove the generality of our method.



### Few-Shot Unsupervised Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1905.01723v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.MM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.01723v2)
- **Published**: 2019-05-05 17:41:31+00:00
- **Updated**: 2019-09-09 06:11:56+00:00
- **Authors**: Ming-Yu Liu, Xun Huang, Arun Mallya, Tero Karras, Timo Aila, Jaakko Lehtinen, Jan Kautz
- **Comment**: The paper will be presented at the International Conference on
  Computer Vision (ICCV) 2019
- **Journal**: ICCV 2019
- **Summary**: Unsupervised image-to-image translation methods learn to map images in a given class to an analogous image in a different class, drawing on unstructured (non-registered) datasets of images. While remarkably successful, current methods require access to many images in both source and destination classes at training time. We argue this greatly limits their use. Drawing inspiration from the human capability of picking up the essence of a novel object from a small number of examples and generalizing from there, we seek a few-shot, unsupervised image-to-image translation algorithm that works on previously unseen target classes that are specified, at test time, only by a few example images. Our model achieves this few-shot generation capability by coupling an adversarial training scheme with a novel network design. Through extensive experimental validation and comparisons to several baseline methods on benchmark datasets, we verify the effectiveness of the proposed framework. Our implementation and datasets are available at https://github.com/NVlabs/FUNIT .



### Better the Devil you Know: An Analysis of Evasion Attacks using Out-of-Distribution Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1905.01726v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.01726v1)
- **Published**: 2019-05-05 18:06:41+00:00
- **Updated**: 2019-05-05 18:06:41+00:00
- **Authors**: Vikash Sehwag, Arjun Nitin Bhagoji, Liwei Song, Chawin Sitawarin, Daniel Cullina, Mung Chiang, Prateek Mittal
- **Comment**: 18 pages, 5 figures, 9 tables
- **Journal**: None
- **Summary**: A large body of recent work has investigated the phenomenon of evasion attacks using adversarial examples for deep learning systems, where the addition of norm-bounded perturbations to the test inputs leads to incorrect output classification. Previous work has investigated this phenomenon in closed-world systems where training and test inputs follow a pre-specified distribution. However, real-world implementations of deep learning applications, such as autonomous driving and content classification are likely to operate in the open-world environment. In this paper, we demonstrate the success of open-world evasion attacks, where adversarial examples are generated from out-of-distribution inputs (OOD adversarial examples). In our study, we use 11 state-of-the-art neural network models trained on 3 image datasets of varying complexity. We first demonstrate that state-of-the-art detectors for out-of-distribution data are not robust against OOD adversarial examples. We then consider 5 known defenses for adversarial examples, including state-of-the-art robust training methods, and show that against these defenses, OOD adversarial examples can achieve up to 4$\times$ higher target success rates compared to adversarial examples generated from in-distribution data. We also take a quantitative look at how open-world evasion attacks may affect real-world systems. Finally, we present the first steps towards a robust open-world machine learning system.



### Breast Tumor Cellularity Assessment using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.01743v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.01743v3)
- **Published**: 2019-05-05 20:15:12+00:00
- **Updated**: 2019-09-03 08:31:24+00:00
- **Authors**: Alexander Rakhlin, Aleksei Tiulpin, Alexey A. Shvets, Alexandr A. Kalinin, Vladimir I. Iglovikov, Sergey Nikolenko
- **Comment**: None
- **Journal**: None
- **Summary**: Breast cancer is one of the main causes of death worldwide. Histopathological cellularity assessment of residual tumors in post-surgical tissues is used to analyze a tumor's response to a therapy. Correct cellularity assessment increases the chances of getting an appropriate treatment and facilitates the patient's survival. In current clinical practice, tumor cellularity is manually estimated by pathologists; this process is tedious and prone to errors or low agreement rates between assessors. In this work, we evaluated three strong novel Deep Learning-based approaches for automatic assessment of tumor cellularity from post-treated breast surgical specimens stained with hematoxylin and eosin. We validated the proposed methods on the BreastPathQ SPIE challenge dataset that consisted of 2395 image patches selected from whole slide images acquired from 64 patients. Compared to expert pathologist scoring, our best performing method yielded the Cohen's kappa coefficient of 0.70 (vs. 0.42 previously known in literature) and the intra-class correlation coefficient of 0.89 (vs. 0.83). Our results suggest that Deep Learning-based methods have a significant potential to alleviate the burden on pathologists, enhance the diagnostic workflow, and, thereby, facilitate better clinical outcomes in breast cancer treatment.



### Towards Instance-level Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1905.01744v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.01744v1)
- **Published**: 2019-05-05 20:16:41+00:00
- **Updated**: 2019-05-05 20:16:41+00:00
- **Authors**: Zhiqiang Shen, Mingyang Huang, Jianping Shi, Xiangyang Xue, Thomas Huang
- **Comment**: Accepted to CVPR 2019. Project page:
  http://zhiqiangshen.com/projects/INIT/index.html
- **Journal**: None
- **Summary**: Unpaired Image-to-image Translation is a new rising and challenging vision problem that aims to learn a mapping between unaligned image pairs in diverse domains. Recent advances in this field like MUNIT and DRIT mainly focus on disentangling content and style/attribute from a given image first, then directly adopting the global style to guide the model to synthesize new domain images. However, this kind of approaches severely incurs contradiction if the target domain images are content-rich with multiple discrepant objects. In this paper, we present a simple yet effective instance-aware image-to-image translation approach (INIT), which employs the fine-grained local (instance) and global styles to the target image spatially. The proposed INIT exhibits three import advantages: (1) the instance-level objective loss can help learn a more accurate reconstruction and incorporate diverse attributes of objects; (2) the styles used for target domain of local/global areas are from corresponding spatial regions in source domain, which intuitively is a more reasonable mapping; (3) the joint training process can benefit both fine and coarse granularity and incorporates instance information to improve the quality of global translation. We also collect a large-scale benchmark for the new instance-level translation task. We observe that our synthetic images can even benefit real-world vision tasks like generic object detection.



### Understanding urban landuse from the above and ground perspectives: a deep learning, multimodal solution
- **Arxiv ID**: http://arxiv.org/abs/1905.01752v1
- **DOI**: 10.1016/j.rse.2019.04.014
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01752v1)
- **Published**: 2019-05-05 21:36:59+00:00
- **Updated**: 2019-05-05 21:36:59+00:00
- **Authors**: Shivangi Srivastava, John E. Vargas-Muñoz, Devis Tuia
- **Comment**: None
- **Journal**: Remote Sensing of Environment, 228, pages 129 - 143, 2019
- **Summary**: Landuse characterization is important for urban planning. It is traditionally performed with field surveys or manual photo interpretation, two practices that are time-consuming and labor-intensive. Therefore, we aim to automate landuse mapping at the urban-object level with a deep learning approach based on data from multiple sources (or modalities). We consider two image modalities: overhead imagery from Google Maps and ensembles of ground-based pictures (side-views) per urban-object from Google Street View (GSV). These modalities bring complementary visual information pertaining to the urban-objects. We propose an end-to-end trainable model, which uses OpenStreetMap annotations as labels. The model can accommodate a variable number of GSV pictures for the ground-based branch and can also function in the absence of ground pictures at prediction time. We test the effectiveness of our model over the area of \^Ile-de-France, France, and test its generalization abilities on a set of urban-objects from the city of Nantes, France. Our proposed multimodal Convolutional Neural Network achieves considerably higher accuracies than methods that use a single image modality, making it suitable for automatic landuse map updates. Additionally, our approach could be easily scaled to multiple cities, because it is based on data sources available for many cities worldwide.



