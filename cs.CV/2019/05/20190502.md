# Arxiv Papers in cs.CV on 2019-05-02
### DPSNet: End-to-end Deep Plane Sweep Stereo
- **Arxiv ID**: http://arxiv.org/abs/1905.00538v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1905.00538v1)
- **Published**: 2019-05-02 00:59:31+00:00
- **Updated**: 2019-05-02 00:59:31+00:00
- **Authors**: Sunghoon Im, Hae-Gon Jeon, Stephen Lin, In So Kweon
- **Comment**: ICLR2019 accepted
- **Journal**: None
- **Summary**: Multiview stereo aims to reconstruct scene depth from images acquired by a camera under arbitrary motion. Recent methods address this problem through deep learning, which can utilize semantic cues to deal with challenges such as textureless and reflective regions. In this paper, we present a convolutional neural network called DPSNet (Deep Plane Sweep Network) whose design is inspired by best practices of traditional geometry-based approaches for dense depth reconstruction. Rather than directly estimating depth and/or optical flow correspondence from image pairs as done in many previous deep learning methods, DPSNet takes a plane sweep approach that involves building a cost volume from deep features using the plane sweep algorithm, regularizing the cost volume via a context-aware cost aggregation, and regressing the dense depth map from the cost volume. The cost volume is constructed using a differentiable warping process that allows for end-to-end training of the network. Through the effective incorporation of conventional multiview stereo concepts within a deep learning framework, DPSNet achieves state-of-the-art reconstruction results on a variety of challenging datasets.



### Billion-scale semi-supervised learning for image classification
- **Arxiv ID**: http://arxiv.org/abs/1905.00546v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00546v1)
- **Published**: 2019-05-02 02:08:18+00:00
- **Updated**: 2019-05-02 02:08:18+00:00
- **Authors**: I. Zeki Yalniz, Hervé Jégou, Kan Chen, Manohar Paluri, Dhruv Mahajan
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a study of semi-supervised learning with large convolutional networks. We propose a pipeline, based on a teacher/student paradigm, that leverages a large collection of unlabelled images (up to 1 billion). Our main goal is to improve the performance for a given target architecture, like ResNet-50 or ResNext. We provide an extensive analysis of the success factors of our approach, which leads us to formulate some recommendations to produce high-accuracy models for image classification with semi-supervised learning. As a result, our approach brings important gains to standard architectures for image, video and fine-grained classification. For instance, by leveraging one billion unlabelled images, our learned vanilla ResNet-50 achieves 81.2% top-1 accuracy on the ImageNet benchmark.



### Large-scale weakly-supervised pre-training for video action recognition
- **Arxiv ID**: http://arxiv.org/abs/1905.00561v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00561v1)
- **Published**: 2019-05-02 03:05:43+00:00
- **Updated**: 2019-05-02 03:05:43+00:00
- **Authors**: Deepti Ghadiyaram, Matt Feiszli, Du Tran, Xueting Yan, Heng Wang, Dhruv Mahajan
- **Comment**: None
- **Journal**: None
- **Summary**: Current fully-supervised video datasets consist of only a few hundred thousand videos and fewer than a thousand domain-specific labels. This hinders the progress towards advanced video architectures. This paper presents an in-depth study of using large volumes of web videos for pre-training video models for the task of action recognition. Our primary empirical finding is that pre-training at a very large scale (over 65 million videos), despite on noisy social-media videos and hashtags, substantially improves the state-of-the-art on three challenging public action recognition datasets. Further, we examine three questions in the construction of weakly-supervised video action datasets. First, given that actions involve interactions with objects, how should one construct a verb-object pre-training label space to benefit transfer learning the most? Second, frame-based models perform quite well on action recognition; is pre-training for good image features sufficient or is pre-training for spatio-temporal features valuable for optimal transfer learning? Finally, actions are generally less well-localized in long videos vs. short videos; since action labels are provided at a video level, how should one choose video clips for best performance, given some fixed budget of number or minutes of videos?



### 26ms Inference Time for ResNet-50: Towards Real-Time Execution of all DNNs on Smartphone
- **Arxiv ID**: http://arxiv.org/abs/1905.00571v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.00571v1)
- **Published**: 2019-05-02 04:37:27+00:00
- **Updated**: 2019-05-02 04:37:27+00:00
- **Authors**: Wei Niu, Xiaolong Ma, Yanzhi Wang, Bin Ren
- **Comment**: None
- **Journal**: None
- **Summary**: With the rapid emergence of a spectrum of high-end mobile devices, many applications that required desktop-level computation capability formerly can now run on these devices without any problem. However, without a careful optimization, executing Deep Neural Networks (a key building block of the real-time video stream processing that is the foundation of many popular applications) is still challenging, specifically, if an extremely low latency or high accuracy inference is needed. This work presents CADNN, a programming framework to efficiently execute DNN on mobile devices with the help of advanced model compression (sparsity) and a set of thorough architecture-aware optimization. The evaluation result demonstrates that CADNN outperforms all the state-of-the-art dense DNN execution frameworks like TensorFlow Lite and TVM.



### Agnostic Lane Detection
- **Arxiv ID**: http://arxiv.org/abs/1905.03704v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.03704v1)
- **Published**: 2019-05-02 05:58:17+00:00
- **Updated**: 2019-05-02 05:58:17+00:00
- **Authors**: Yuenan Hou
- **Comment**: 6 pages, 2 figures, our codes are available at
  https://github.com/cardwing/Codes-for-Lane-Detection
- **Journal**: None
- **Summary**: Lane detection is an important yet challenging task in autonomous driving, which is affected by many factors, e.g., light conditions, occlusions caused by other vehicles, irrelevant markings on the road and the inherent long and thin property of lanes. Conventional methods typically treat lane detection as a semantic segmentation task, which assigns a class label to each pixel of the image. This formulation heavily depends on the assumption that the number of lanes is pre-defined and fixed and no lane changing occurs, which does not always hold. To make the lane detection model applicable to an arbitrary number of lanes and lane changing scenarios, we adopt an instance segmentation approach, which first differentiates lanes and background and then classify each lane pixel into each lane instance. Besides, a multi-task learning paradigm is utilized to better exploit the structural information and the feature pyramid architecture is used to detect extremely thin lanes. Three popular lane detection benchmarks, i.e., TuSimple, CULane and BDD100K, are used to validate the effectiveness of our proposed algorithm.



### Recurrent Convolutional Strategies for Face Manipulation Detection in Videos
- **Arxiv ID**: http://arxiv.org/abs/1905.00582v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00582v3)
- **Published**: 2019-05-02 06:06:25+00:00
- **Updated**: 2019-05-16 06:56:55+00:00
- **Authors**: Ekraam Sabir, Jiaxin Cheng, Ayush Jaiswal, Wael AbdAlmageed, Iacopo Masi, Prem Natarajan
- **Comment**: To appear at Workshop on Applications of Computer Vision and Pattern
  Recognition to Media Forensics at CVPR 2019
- **Journal**: None
- **Summary**: The spread of misinformation through synthetically generated yet realistic images and videos has become a significant problem, calling for robust manipulation detection methods. Despite the predominant effort of detecting face manipulation in still images, less attention has been paid to the identification of tampered faces in videos by taking advantage of the temporal information present in the stream. Recurrent convolutional models are a class of deep learning models which have proven effective at exploiting the temporal information from image streams across domains. We thereby distill the best strategy for combining variations in these models along with domain specific face preprocessing techniques through extensive experimentation to obtain state-of-the-art performance on publicly available video-based facial manipulation benchmarks. Specifically, we attempt to detect Deepfake, Face2Face and FaceSwap tampered faces in video streams. Evaluation is performed on the recently introduced FaceForensics++ dataset, improving the previous state-of-the-art by up to 4.55% in accuracy.



### Directing DNNs Attention for Facial Attribution Classification using Gradient-weighted Class Activation Mapping
- **Arxiv ID**: http://arxiv.org/abs/1905.00593v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00593v1)
- **Published**: 2019-05-02 07:05:33+00:00
- **Updated**: 2019-05-02 07:05:33+00:00
- **Authors**: Xi Yang, Bojian Wu, Issei Sato, Takeo Igarashi
- **Comment**: CVPR-19 Workshop on Explainable AI
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have a high accuracy on image classification tasks. However, DNNs trained by such dataset with co-occurrence bias may rely on wrong features while making decisions for classification. It will greatly affect the transferability of pre-trained DNNs. In this paper, we propose an interactive method to direct classifiers paying attentions to the regions that are manually specified by the users, in order to mitigate the influence of co-occurrence bias. We test on CelebA dataset, the pre-trained AlexNet is fine-tuned to focus on the specific facial attributes based on the results of Grad-CAM.



### Human Activity Recognition Using Visual Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1905.03707v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.03707v1)
- **Published**: 2019-05-02 07:17:51+00:00
- **Updated**: 2019-05-02 07:17:51+00:00
- **Authors**: Schalk Wilhelm Pienaar, Reza Malekian
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Human Activity Recognition (HAR) and data fusion with other sensors can help us at tracking the behavior and activity of underground miners with little obstruction. Existing models, such as Single Shot Detector (SSD), trained on the Common Objects in Context (COCO) dataset is used in this paper to detect the current state of a miner, such as an injured miner vs a non-injured miner. Tensorflow is used for the abstraction layer of implementing machine learning algorithms, and although it uses Python to deal with nodes and tensors, the actual algorithms run on C++ libraries, providing a good balance between performance and speed of development. The paper further discusses evaluation methods for determining the accuracy of the machine-learning and an approach to increase the accuracy of the detected activity/state of people in a mining environment, by means of data fusion.



### Inverse Halftoning Through Structure-Aware Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.00637v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1905.00637v2)
- **Published**: 2019-05-02 09:39:54+00:00
- **Updated**: 2019-08-05 06:49:11+00:00
- **Authors**: Chang-Hwan Son
- **Comment**: None
- **Journal**: Signal Processing, vol. 173, Aug. 2020
- **Summary**: The primary issue in inverse halftoning is removing noisy dots on flat areas and restoring image structures (e.g., lines, patterns) on textured areas. Hence, a new structure-aware deep convolutional neural network that incorporates two subnetworks is proposed in this paper. One subnetwork is for image structure prediction while the other is for continuous-tone image reconstruction. First, to predict image structures, patch pairs comprising continuous-tone patches and the corresponding halftoned patches generated through digital halftoning are trained. Subsequently, gradient patches are generated by convolving gradient filters with the continuous-tone patches. The subnetwork for the image structure prediction is trained using the mini-batch gradient descent algorithm given the halftoned patches and gradient patches, which are fed into the input and loss layers of the subnetwork, respectively. Next, the predicted map including the image structures is stacked on the top of the input halftoned image through a fusion layer and fed into the image reconstruction subnetwork such that the entire network is trained adaptively to the image structures. The experimental results confirm that the proposed structure-aware network can remove noisy dot-patterns well on flat areas and restore details clearly on textured areas. Furthermore, it is demonstrated that the proposed method surpasses the conventional state-of-the-art methods based on deep convolutional neural networks and locally learned dictionaries.



### LivDet in Action - Fingerprint Liveness Detection Competition 2019
- **Arxiv ID**: http://arxiv.org/abs/1905.00639v1
- **DOI**: 10.1109/ICB45273.2019.8987281
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00639v1)
- **Published**: 2019-05-02 09:42:42+00:00
- **Updated**: 2019-05-02 09:42:42+00:00
- **Authors**: Giulia Orrù, Roberto Casula, Pierluigi Tuveri, Carlotta Bazzoni, Giovanna Dessalvi, Marco Micheletto, Luca Ghiani, Gian Luca Marcialis
- **Comment**: Preprint version of a paper accepted at ICB 2019
- **Journal**: None
- **Summary**: The International Fingerprint liveness Detection Competition (LivDet) is an open and well-acknowledged meeting point of academies and private companies that deal with the problem of distinguishing images coming from reproductions of fingerprints made of artificial materials and images relative to real fingerprints. In this edition of LivDet we invited the competitors to propose integrated algorithms with matching systems. The goal was to investigate at which extent this integration impact on the whole performance. Twelve algorithms were submitted to the competition, eight of which worked on integrated systems.



### RetinaFace: Single-stage Dense Face Localisation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1905.00641v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00641v2)
- **Published**: 2019-05-02 09:45:23+00:00
- **Updated**: 2019-05-04 22:03:26+00:00
- **Authors**: Jiankang Deng, Jia Guo, Yuxiang Zhou, Jinke Yu, Irene Kotsia, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: None
- **Summary**: Though tremendous strides have been made in uncontrolled face detection, accurate and efficient face localisation in the wild remains an open challenge. This paper presents a robust single-stage face detector, named RetinaFace, which performs pixel-wise face localisation on various scales of faces by taking advantages of joint extra-supervised and self-supervised multi-task learning. Specifically, We make contributions in the following five aspects: (1) We manually annotate five facial landmarks on the WIDER FACE dataset and observe significant improvement in hard face detection with the assistance of this extra supervision signal. (2) We further add a self-supervised mesh decoder branch for predicting a pixel-wise 3D shape face information in parallel with the existing supervised branches. (3) On the WIDER FACE hard test set, RetinaFace outperforms the state of the art average precision (AP) by 1.1% (achieving AP equal to 91.4%). (4) On the IJB-C test set, RetinaFace enables state of the art methods (ArcFace) to improve their results in face verification (TAR=89.59% for FAR=1e-6). (5) By employing light-weight backbone networks, RetinaFace can run real-time on a single CPU core for a VGA-resolution image. Extra annotations and code have been made available at: https://github.com/deepinsight/insightface/tree/master/RetinaFace.



### DS-VIO: Robust and Efficient Stereo Visual Inertial Odometry based on Dual Stage EKF
- **Arxiv ID**: http://arxiv.org/abs/1905.00684v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00684v1)
- **Published**: 2019-05-02 11:59:25+00:00
- **Updated**: 2019-05-02 11:59:25+00:00
- **Authors**: Xiaogang Xiong, Wenqing Chen, Zhichao Liu, Qiang Shen
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a dual stage EKF (Extended Kalman Filter)-based algorithm for the real-time and robust stereo VIO (visual inertial odometry). The first stage of this EKF-based algorithm performs the fusion of accelerometer and gyroscope while the second performs the fusion of stereo camera and IMU. Due to the sufficient complementary characteristics between accelerometer and gyroscope as well as stereo camera and IMU, the dual stage EKF-based algorithm can achieve a high precision of odometry estimations. At the same time, because of the low dimension of state vector in this algorithm, its computational efficiency is comparable to previous filter-based approaches. We call our approach DS-VIO (dual stage EKFbased stereo visual inertial odometry) and evaluate our DSVIO algorithm by comparing it with the state-of-art approaches including OKVIS, ROVIO, VINS-MONO and S-MSCKF on the EuRoC dataset. Results show that our algorithm can achieve comparable or even better performances in terms of the RMS error



### Face Identification using Local Ternary Tree Pattern based Spatial Structural Components
- **Arxiv ID**: http://arxiv.org/abs/1905.00693v2
- **DOI**: None
- **Categories**: **cs.CV**, Computer Science
- **Links**: [PDF](http://arxiv.org/pdf/1905.00693v2)
- **Published**: 2019-05-02 12:21:07+00:00
- **Updated**: 2020-07-16 05:24:57+00:00
- **Authors**: Rinku Datta Rakshit, Dakshina Ranjan Kisku, Massimo Tistarelli, Phalguni Gupta
- **Comment**: 13 pages, 5 figures, conference paper
- **Journal**: None
- **Summary**: This paper reports a face identification system which makes use of a novel local descriptor called Local Ternary Tree Pattern (LTTP). Exploiting and extracting distinctive local descriptor from a face image plays a crucial role in face identification task in the presence of a variety of face images including constrained, unconstrained and plastic surgery images. LTTP has been used to extract robust and useful spatial features which use to describe the various structural components on a face. To extract the features, a ternary tree is formed for each pixel with its eight neighbors in each block. LTTP pattern can be generated in four forms such as LTTP Left Depth (LTTP LD), LTTP Left Breadth (LTTP LB), LTTP Right Depth (LTTP RD) and LTTP Right Breadth (LTTP RB). The encoding schemes of these patterns are very simple and efficient in terms of computational as well as time complexity. The proposed face identification system is tested on six face databases, namely, the UMIST, the JAFFE, the extended Yale face B, the Plastic Surgery, the LFW and the UFI. The experimental evaluation demonstrates the most promising results considering a variety of faces captured under different environments. The proposed LTTP based system is also compared with some local descriptors under identical conditions.



### The 2019 DAVIS Challenge on VOS: Unsupervised Multi-Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.00737v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00737v1)
- **Published**: 2019-05-02 13:40:43+00:00
- **Updated**: 2019-05-02 13:40:43+00:00
- **Authors**: Sergi Caelles, Jordi Pont-Tuset, Federico Perazzi, Alberto Montes, Kevis-Kokitsi Maninis, Luc Van Gool
- **Comment**: CVPR 2019 Workshop/Challenge
- **Journal**: None
- **Summary**: We present the 2019 DAVIS Challenge on Video Object Segmentation, the third edition of the DAVIS Challenge series, a public competition designed for the task of Video Object Segmentation (VOS). In addition to the original semi-supervised track and the interactive track introduced in the previous edition, a new unsupervised multi-object track will be featured this year. In the newly introduced track, participants are asked to provide non-overlapping object proposals on each image, along with an identifier linking them between frames (i.e. video object proposals), without any test-time human supervision (no scribbles or masks provided on the test video). In order to do so, we have re-annotated the train and val sets of DAVIS 2017 in a concise way that facilitates the unsupervised track, and created new test-dev and test-challenge sets for the competition. Definitions, rules, and evaluation metrics for the unsupervised track are described in detail in this paper.



### Egocentric Hand Track and Object-based Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1905.00742v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00742v1)
- **Published**: 2019-05-02 13:43:28+00:00
- **Updated**: 2019-05-02 13:43:28+00:00
- **Authors**: Georgios Kapidis, Ronald Poppe, Elsbeth van Dam, Lucas P. J. J. Noldus, Remco C. Veltkamp
- **Comment**: Accepted for publication at UIC 2019:Track 3, 8 pages, 5 figures,
  Index terms: egocentric action recognition, hand detection, hand tracking,
  hand identification, sequence classification, code available at:
  https://github.com/georkap/hand_track_classification
- **Journal**: None
- **Summary**: Egocentric vision is an emerging field of computer vision that is characterized by the acquisition of images and video from the first person perspective. In this paper we address the challenge of egocentric human action recognition by utilizing the presence and position of detected regions of interest in the scene explicitly, without further use of visual features.   Initially, we recognize that human hands are essential in the execution of actions and focus on obtaining their movements as the principal cues that define actions. We employ object detection and region tracking techniques to locate hands and capture their movements. Prior knowledge about egocentric views facilitates hand identification between left and right. With regard to detection and tracking, we contribute a pipeline that successfully operates on unseen egocentric videos to find the camera wearer's hands and associate them through time. Moreover, we emphasize on the value of scene information for action recognition. We acknowledge that the presence of objects is significant for the execution of actions by humans and in general for the description of a scene. To acquire this information, we utilize object detection for specific classes that are relevant to the actions we want to recognize.   Our experiments are targeted on videos of kitchen activities from the Epic-Kitchens dataset. We model action recognition as a sequence learning problem of the detected spatial positions in the frames. Our results show that explicit hand and object detections with no other visual information can be relied upon to classify hand-related human actions. Testing against methods fully dependent on visual features, signals that for actions where hand motions are conceptually important, a region-of-interest-based description of a video contains equally expressive information with comparable classification performance.



### Human Action Recognition with Deep Temporal Pyramids
- **Arxiv ID**: http://arxiv.org/abs/1905.00745v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00745v1)
- **Published**: 2019-05-02 13:48:17+00:00
- **Updated**: 2019-05-02 13:48:17+00:00
- **Authors**: Ahmed Mazari, Hichem Sahbi
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) are nowadays achieving significant leaps in different pattern recognition tasks including action recognition. Current CNNs are increasingly deeper, data-hungrier and this makes their success tributary of the abundance of labeled training data. CNNs also rely on max/average pooling which reduces dimensionality of output layers and hence attenuates their sensitivity to the availability of labeled data. However, this process may dilute the information of upstream convolutional layers and thereby affect the discrimination power of the trained representations, especially when the learned categories are fine-grained.   In this paper, we introduce a novel hierarchical aggregation design, for final pooling, that controls granularity of the learned representations w.r.t the actual granularity of action categories. Our solution is based on a tree-structured temporal pyramid that aggregates outputs of CNNs at different levels. Top levels of this hierarchy are dedicated to coarse categories while deep levels are more suitable to fine-grained ones. The design of our temporal pyramid is based on solving a constrained minimization problem whose solution corresponds to the distribution of weights of different representations in the temporal pyramid. Experiments conducted using the challenging UCF101 database show the relevance of our hierarchical design w.r.t other related methods.



### Clustering Images by Unmasking - A New Baseline
- **Arxiv ID**: http://arxiv.org/abs/1905.00773v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.00773v1)
- **Published**: 2019-05-02 14:35:29+00:00
- **Updated**: 2019-05-02 14:35:29+00:00
- **Authors**: Mariana-Iuliana Georgescu, Radu Tudor Ionescu
- **Comment**: Accepted at ICIP 2019
- **Journal**: None
- **Summary**: We propose a novel agglomerative clustering method based on unmasking, a technique that was previously used for authorship verification of text documents and for abnormal event detection in videos. In order to join two clusters, we alternate between (i) training a binary classifier to distinguish between the samples from one cluster and the samples from the other cluster, and (ii) removing at each step the most discriminant features. The faster-decreasing accuracy rates of the intermediately-obtained classifiers indicate that the two clusters should be joined. To the best of our knowledge, this is the first work to apply unmasking in order to cluster images. We compare our method with k-means as well as a recent state-of-the-art clustering method. The empirical results indicate that our approach is able to improve performance for various (deep and shallow) feature representations and different tasks, such as handwritten digit recognition, texture classification and fine-grained object recognition.



### Full-Gradient Representation for Neural Network Visualization
- **Arxiv ID**: http://arxiv.org/abs/1905.00780v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.00780v4)
- **Published**: 2019-05-02 14:41:31+00:00
- **Updated**: 2019-12-03 13:59:05+00:00
- **Authors**: Suraj Srinivas, Francois Fleuret
- **Comment**: NeurIPS 2019
- **Journal**: None
- **Summary**: We introduce a new tool for interpreting neural net responses, namely full-gradients, which decomposes the neural net response into input sensitivity and per-neuron sensitivity components. This is the first proposed representation which satisfies two key properties: completeness and weak dependence, which provably cannot be satisfied by any saliency map-based interpretability method. For convolutional nets, we also propose an approximate saliency map representation, called FullGrad, obtained by aggregating the full-gradient components.   We experimentally evaluate the usefulness of FullGrad in explaining model behaviour with two quantitative tests: pixel perturbation and remove-and-retrain. Our experiments reveal that our method explains model behaviour correctly, and more comprehensively than other methods in the literature. Visual inspection also reveals that our saliency maps are sharper and more tightly confined to object regions than other methods.



### Toward Extremely Low Bit and Lossless Accuracy in DNNs with Progressive ADMM
- **Arxiv ID**: http://arxiv.org/abs/1905.00789v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.00789v1)
- **Published**: 2019-05-02 14:53:24+00:00
- **Updated**: 2019-05-02 14:53:24+00:00
- **Authors**: Sheng Lin, Xiaolong Ma, Shaokai Ye, Geng Yuan, Kaisheng Ma, Yanzhi Wang
- **Comment**: Accepted by ICML workshop (ODML-CDNNR2019). arXiv admin note:
  substantial text overlap with arXiv:1903.09769
- **Journal**: None
- **Summary**: Weight quantization is one of the most important techniques of Deep Neural Networks (DNNs) model compression method. A recent work using systematic framework of DNN weight quantization with the advanced optimization algorithm ADMM (Alternating Direction Methods of Multipliers) achieves one of state-of-art results in weight quantization. In this work, we first extend such ADMM-based framework to guarantee solution feasibility and we have further developed a multi-step, progressive DNN weight quantization framework, with dual benefits of (i) achieving further weight quantization thanks to the special property of ADMM regularization, and (ii) reducing the search space within each step. Extensive experimental results demonstrate the superior performance compared with prior work. Some highlights: we derive the first lossless and fully binarized (for all layers) LeNet-5 for MNIST; And we derive the first fully binarized (for all layers) VGG-16 for CIFAR-10 and ResNet for ImageNet with reasonable accuracy loss.



### Visualizing the Consequences of Climate Change Using Cycle-Consistent Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.03709v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.03709v1)
- **Published**: 2019-05-02 15:34:53+00:00
- **Updated**: 2019-05-02 15:34:53+00:00
- **Authors**: Victor Schmidt, Alexandra Luccioni, S. Karthik Mukkavilli, Narmada Balasooriya, Kris Sankaran, Jennifer Chayes, Yoshua Bengio
- **Comment**: None
- **Journal**: None
- **Summary**: We present a project that aims to generate images that depict accurate, vivid, and personalized outcomes of climate change using Cycle-Consistent Adversarial Networks (CycleGANs). By training our CycleGAN model on street-view images of houses before and after extreme weather events (e.g. floods, forest fires, etc.), we learn a mapping that can then be applied to images of locations that have not yet experienced these events. This visual transformation is paired with climate model predictions to assess likelihood and type of climate-related events in the long term (50 years) in order to bring the future closer in the viewers mind. The eventual goal of our project is to enable individuals to make more informed choices about their climate future by creating a more visceral understanding of the effects of climate change, while maintaining scientific credibility by drawing on climate model projections.



### Single Image Portrait Relighting
- **Arxiv ID**: http://arxiv.org/abs/1905.00824v1
- **DOI**: 10.1145/3306346.3323008
- **Categories**: **cs.GR**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.00824v1)
- **Published**: 2019-05-02 15:56:15+00:00
- **Updated**: 2019-05-02 15:56:15+00:00
- **Authors**: Tiancheng Sun, Jonathan T. Barron, Yun-Ta Tsai, Zexiang Xu, Xueming Yu, Graham Fyffe, Christoph Rhemann, Jay Busch, Paul Debevec, Ravi Ramamoorthi
- **Comment**: SIGGRAPH 2019 Technical Paper accepted
- **Journal**: ACM Transactions on Graphics (SIGGRAPH 2019) 38 (4)
- **Summary**: Lighting plays a central role in conveying the essence and depth of the subject in a portrait photograph. Professional photographers will carefully control the lighting in their studio to manipulate the appearance of their subject, while consumer photographers are usually constrained to the illumination of their environment. Though prior works have explored techniques for relighting an image, their utility is usually limited due to requirements of specialized hardware, multiple images of the subject under controlled or known illuminations, or accurate models of geometry and reflectance. To this end, we present a system for portrait relighting: a neural network that takes as input a single RGB image of a portrait taken with a standard cellphone camera in an unconstrained environment, and from that image produces a relit image of that subject as though it were illuminated according to any provided environment map. Our method is trained on a small database of 18 individuals captured under different directional light sources in a controlled light stage setup consisting of a densely sampled sphere of lights. Our proposed technique produces quantitatively superior results on our dataset's validation set compared to prior works, and produces convincing qualitative relighting results on a dataset of hundreds of real-world cellphone portraits. Because our technique can produce a 640 $\times$ 640 image in only 160 milliseconds, it may enable interactive user-facing photographic applications in the future.



### SinGAN: Learning a Generative Model from a Single Natural Image
- **Arxiv ID**: http://arxiv.org/abs/1905.01164v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01164v2)
- **Published**: 2019-05-02 16:15:38+00:00
- **Updated**: 2019-09-04 17:08:49+00:00
- **Authors**: Tamar Rott Shaham, Tali Dekel, Tomer Michaeli
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: We introduce SinGAN, an unconditional generative model that can be learned from a single natural image. Our model is trained to capture the internal distribution of patches within the image, and is then able to generate high quality, diverse samples that carry the same visual content as the image. SinGAN contains a pyramid of fully convolutional GANs, each responsible for learning the patch distribution at a different scale of the image. This allows generating new samples of arbitrary size and aspect ratio, that have significant variability, yet maintain both the global structure and the fine textures of the training image. In contrast to previous single image GAN schemes, our approach is not limited to texture images, and is not conditional (i.e. it generates samples from noise). User studies confirm that the generated samples are commonly confused to be real images. We illustrate the utility of SinGAN in a wide range of image manipulation tasks.



### Lifting Vectorial Variational Problems: A Natural Formulation based on Geometric Measure Theory and Discrete Exterior Calculus
- **Arxiv ID**: http://arxiv.org/abs/1905.00851v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.00851v1)
- **Published**: 2019-05-02 16:54:58+00:00
- **Updated**: 2019-05-02 16:54:58+00:00
- **Authors**: Thomas Möllenhoff, Daniel Cremers
- **Comment**: Oral presentation at CVPR 2019
- **Journal**: None
- **Summary**: Numerous tasks in imaging and vision can be formulated as variational problems over vector-valued maps. We approach the relaxation and convexification of such vectorial variational problems via a lifting to the space of currents. To that end, we recall that functionals with polyconvex Lagrangians can be reparametrized as convex one-homogeneous functionals on the graph of the function. This leads to an equivalent shape optimization problem over oriented surfaces in the product space of domain and codomain. A convex formulation is then obtained by relaxing the search space from oriented surfaces to more general currents. We propose a discretization of the resulting infinite-dimensional optimization problem using Whitney forms, which also generalizes recent "sublabel-accurate" multilabeling approaches.



### Self-supervised Learning for Video Correspondence Flow
- **Arxiv ID**: http://arxiv.org/abs/1905.00875v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.00875v5)
- **Published**: 2019-05-02 17:45:16+00:00
- **Updated**: 2019-07-27 22:59:37+00:00
- **Authors**: Zihang Lai, Weidi Xie
- **Comment**: BMVC2019 (Oral Presentation)
- **Journal**: None
- **Summary**: The objective of this paper is self-supervised learning of feature embeddings that are suitable for matching correspondences along the videos, which we term correspondence flow. By leveraging the natural spatial-temporal coherence in videos, we propose to train a ``pointer'' that reconstructs a target frame by copying pixels from a reference frame.   We make the following contributions: First, we introduce a simple information bottleneck that forces the model to learn robust features for correspondence matching, and prevent it from learning trivial solutions, \eg matching based on low-level colour information. Second, to tackle the challenges from tracker drifting, due to complex object deformations, illumination changes and occlusions, we propose to train a recursive model over long temporal windows with scheduled sampling and cycle consistency. Third, we achieve state-of-the-art performance on DAVIS 2017 video segmentation and JHMDB keypoint tracking tasks, outperforming all previous self-supervised learning approaches by a significant margin. Fourth, in order to shed light on the potential of self-supervised learning on the task of video correspondence flow, we probe the upper bound by training on additional data, \ie more diverse videos, further demonstrating significant improvements on video segmentation.



### DRIT++: Diverse Image-to-Image Translation via Disentangled Representations
- **Arxiv ID**: http://arxiv.org/abs/1905.01270v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01270v2)
- **Published**: 2019-05-02 17:49:30+00:00
- **Updated**: 2019-12-17 22:57:28+00:00
- **Authors**: Hsin-Ying Lee, Hung-Yu Tseng, Qi Mao, Jia-Bin Huang, Yu-Ding Lu, Maneesh Singh, Ming-Hsuan Yang
- **Comment**: IJCV Journal extension for ECCV 2018 "Diverse Image-to-Image
  Translation via Disentangled Representations" arXiv:1808.00948. Project Page:
  http://vllab.ucmerced.edu/hylee/DRIT_pp/ Code:
  https://github.com/HsinYingLee/DRIT
- **Journal**: None
- **Summary**: Image-to-image translation aims to learn the mapping between two visual domains. There are two main challenges for this task: 1) lack of aligned training pairs and 2) multiple possible outputs from a single input image. In this work, we present an approach based on disentangled representation for generating diverse outputs without paired training images. To synthesize diverse outputs, we propose to embed images onto two spaces: a domain-invariant content space capturing shared information across domains and a domain-specific attribute space. Our model takes the encoded content features extracted from a given input and attribute vectors sampled from the attribute space to synthesize diverse outputs at test time. To handle unpaired training data, we introduce a cross-cycle consistency loss based on disentangled representations. Qualitative results show that our model can generate diverse and realistic images on a wide range of tasks without paired training data. For quantitative evaluations, we measure realism with user study and Fr\'{e}chet inception distance, and measure diversity with the perceptual distance metric, Jensen-Shannon divergence, and number of statistically-different bins.



### Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines
- **Arxiv ID**: http://arxiv.org/abs/1905.00889v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1905.00889v1)
- **Published**: 2019-05-02 17:58:52+00:00
- **Updated**: 2019-05-02 17:58:52+00:00
- **Authors**: Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, Abhishek Kar
- **Comment**: SIGGRAPH 2019. Project page with video and code:
  http://people.eecs.berkeley.edu/~bmild/llff/
- **Journal**: None
- **Summary**: We present a practical and robust deep learning solution for capturing and rendering novel views of complex real world scenes for virtual exploration. Previous approaches either require intractably dense view sampling or provide little to no guidance for how users should sample views of a scene to reliably render high-quality novel views. Instead, we propose an algorithm for view synthesis from an irregular grid of sampled views that first expands each sampled view into a local light field via a multiplane image (MPI) scene representation, then renders novel views by blending adjacent local light fields. We extend traditional plenoptic sampling theory to derive a bound that specifies precisely how densely users should sample views of a given scene when using our algorithm. In practice, we apply this bound to capture and render views of real world scenes that achieve the perceptual quality of Nyquist rate view sampling while using up to 4000x fewer views. We demonstrate our approach's practicality with an augmented reality smartphone app that guides users to capture input images of a scene and viewers that enable realtime virtual exploration on desktop and mobile platforms.



### Joint High Dynamic Range Imaging and Super-Resolution from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1905.00933v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1905.00933v1)
- **Published**: 2019-05-02 18:57:18+00:00
- **Updated**: 2019-05-02 18:57:18+00:00
- **Authors**: Jae Woong Soh, Jae Sung Park, Nam Ik Cho
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: This paper presents a new framework for jointly enhancing the resolution and the dynamic range of an image, i.e., simultaneous super-resolution (SR) and high dynamic range imaging (HDRI), based on a convolutional neural network (CNN). From the common trends of both tasks, we train a CNN for the joint HDRI and SR by focusing on the reconstruction of high-frequency details. Specifically, the high-frequency component in our work is the reflectance component according to the Retinex-based image decomposition, and only the reflectance component is manipulated by the CNN while another component (illumination) is processed in a conventional way. In training the CNN, we devise an appropriate loss function that contributes to the naturalness quality of resulting images. Experiments show that our algorithm outperforms the cascade implementation of CNN-based SR and HDRI.



### A Splitting-Based Iterative Algorithm for GPU-Accelerated Statistical Dual-Energy X-Ray CT Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1905.00934v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.00934v1)
- **Published**: 2019-05-02 18:58:42+00:00
- **Updated**: 2019-05-02 18:58:42+00:00
- **Authors**: Fangda Li, Ankit Manerikar, Tanmay Prakash, Avinash Kak
- **Comment**: None
- **Journal**: None
- **Summary**: When dealing with material classification in baggage at airports, Dual-Energy Computed Tomography (DECT) allows characterization of any given material with coefficients based on two attenuative effects: Compton scattering and photoelectric absorption. However, straightforward projection-domain decomposition methods for this characterization often yield poor reconstructions due to the high dynamic range of material properties encountered in an actual luggage scan. Hence, for better reconstruction quality under a timing constraint, we propose a splitting-based, GPU-accelerated, statistical DECT reconstruction algorithm. Compared to prior art, our main contribution lies in the significant acceleration made possible by separating reconstruction and decomposition within an ADMM framework. Experimental results, on both synthetic and real-world baggage phantoms, demonstrate a significant reduction in time required for convergence.



### Enhanced free space detection in multiple lanes based on single CNN with scene identification
- **Arxiv ID**: http://arxiv.org/abs/1905.00941v2
- **DOI**: 10.1109/IVS.2019.8814181
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1905.00941v2)
- **Published**: 2019-05-02 19:26:07+00:00
- **Updated**: 2019-05-06 07:00:33+00:00
- **Authors**: Fabio Pizzati, Fernando García
- **Comment**: Will appear in the 2019 IEEE Intelligent Vehicles Symposium (IV 2019)
- **Journal**: 2019 IEEE Intelligent Vehicles Symposium (IV)
- **Summary**: Many systems for autonomous vehicles' navigation rely on lane detection. Traditional algorithms usually estimate only the position of the lanes on the road, but an autonomous control system may also need to know if a lane marking can be crossed or not, and what portion of space inside the lane is free from obstacles, to make safer control decisions. On the other hand, free space detection algorithms only detect navigable areas, without information about lanes. State-of-the-art algorithms use CNNs for both tasks, with significant consumption of computing resources. We propose a novel approach that estimates the free space inside each lane, with a single CNN. Additionally, adding only a small requirement concerning GPU RAM, we infer the road type, that will be useful for path planning. To achieve this result, we train a multi-task CNN. Then, we further elaborate the output of the network, to extract polygons that can be effectively used in navigation control. Finally, we provide a computationally efficient implementation, based on ROS, that can be executed in real time. Our code and trained models are available online.



### Omni-Scale Feature Learning for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1905.00953v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00953v6)
- **Published**: 2019-05-02 20:42:26+00:00
- **Updated**: 2019-12-18 09:29:53+00:00
- **Authors**: Kaiyang Zhou, Yongxin Yang, Andrea Cavallaro, Tao Xiang
- **Comment**: ICCV 2019; This version adds additional training recipes for
  practitioners
- **Journal**: None
- **Summary**: As an instance-level recognition problem, person re-identification (ReID) relies on discriminative features, which not only capture different spatial scales but also encapsulate an arbitrary combination of multiple scales. We call features of both homogeneous and heterogeneous scales omni-scale features. In this paper, a novel deep ReID CNN is designed, termed Omni-Scale Network (OSNet), for omni-scale feature learning. This is achieved by designing a residual block composed of multiple convolutional streams, each detecting features at a certain scale. Importantly, a novel unified aggregation gate is introduced to dynamically fuse multi-scale features with input-dependent channel-wise weights. To efficiently learn spatial-channel correlations and avoid overfitting, the building block uses pointwise and depthwise convolutions. By stacking such block layer-by-layer, our OSNet is extremely lightweight and can be trained from scratch on existing ReID benchmarks. Despite its small model size, OSNet achieves state-of-the-art performance on six person ReID datasets, outperforming most large-sized models, often by a clear margin. Code and models are available at: \url{https://github.com/KaiyangZhou/deep-person-reid}.



### Visualizing Deep Networks by Optimizing with Integrated Gradients
- **Arxiv ID**: http://arxiv.org/abs/1905.00954v2
- **DOI**: 10.1609/aaai.v34i07.6863
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00954v2)
- **Published**: 2019-05-02 20:45:37+00:00
- **Updated**: 2020-12-11 10:55:28+00:00
- **Authors**: Zhongang Qi, Saeed Khorram, Li Fuxin
- **Comment**: None
- **Journal**: AAAI 2020
- **Summary**: Understanding and interpreting the decisions made by deep learning models is valuable in many domains. In computer vision, computing heatmaps from a deep network is a popular approach for visualizing and understanding deep networks. However, heatmaps that do not correlate with the network may mislead human, hence the performance of heatmaps in providing a faithful explanation to the underlying deep network is crucial. In this paper, we propose I-GOS, which optimizes for a heatmap so that the classification scores on the masked image would maximally decrease. The main novelty of the approach is to compute descent directions based on the integrated gradients instead of the normal gradient, which avoids local optima and speeds up convergence. Compared with previous approaches, our method can flexibly compute heatmaps at any resolution for different user needs. Extensive experiments on several benchmark datasets show that the heatmaps produced by our approach are more correlated with the decision of the underlying deep network, in comparison with other state-of-the-art approaches.



### Improving Visual Relation Detection using Depth Maps
- **Arxiv ID**: http://arxiv.org/abs/1905.00966v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.00966v4)
- **Published**: 2019-05-02 21:14:35+00:00
- **Updated**: 2020-10-17 13:58:38+00:00
- **Authors**: Sahand Sharifzadeh, Sina Moayed Baharlou, Max Berrendorf, Rajat Koner, Volker Tresp
- **Comment**: International Conference on Pattern Recognition 2020
- **Journal**: None
- **Summary**: Visual relation detection methods rely on object information extracted from RGB images such as 2D bounding boxes, feature maps, and predicted class probabilities. We argue that depth maps can additionally provide valuable information on object relations, e.g. helping to detect not only spatial relations, such as standing behind, but also non-spatial relations, such as holding. In this work, we study the effect of using different object features with a focus on depth maps. To enable this study, we release a new synthetic dataset of depth maps, VG-Depth, as an extension to Visual Genome (VG). We also note that given the highly imbalanced distribution of relations in VG, typical evaluation metrics for visual relation detection cannot reveal improvements of under-represented relations. To address this problem, we propose using an additional metric, calling it Macro Recall@K, and demonstrate its remarkable performance on VG. Finally, our experiments confirm that by effective utilization of depth maps within a simple, yet competitive framework, the performance of visual relation detection can be improved by a margin of up to 8%.



### Remote measurement of sea ice dynamics with regularized optimal transport
- **Arxiv ID**: http://arxiv.org/abs/1905.00989v1
- **DOI**: 10.1029/2019GL083037
- **Categories**: **cs.CV**, stat.CO
- **Links**: [PDF](http://arxiv.org/pdf/1905.00989v1)
- **Published**: 2019-05-02 22:47:22+00:00
- **Updated**: 2019-05-02 22:47:22+00:00
- **Authors**: M. D. Parno, B. A. West, A. J. Song, T. S. Hodgdon, D. T. O'Connor
- **Comment**: None
- **Journal**: None
- **Summary**: As Arctic conditions rapidly change, human activity in the Arctic will continue to increase and so will the need for high-resolution observations of sea ice. While satellite imagery can provide high spatial resolution, it is temporally sparse and significant ice deformation can occur between observations. This makes it difficult to apply feature tracking or image correlation techniques that require persistent features to exist between images. With this in mind, we propose a technique based on optimal transport, which is commonly used to measure differences between probability distributions. When little ice enters or leaves the image scene, we show that regularized optimal transport can be used to quantitatively estimate ice deformation. We discuss the motivation for our approach and describe efficient computational implementations. Results are provided on a combination of synthetic and MODIS imagery to demonstrate the ability of our approach to estimate dynamics properties at the original image resolution.



