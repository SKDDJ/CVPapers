# Arxiv Papers in cs.CV on 2019-05-26
### Underwater Fish Detection with Weak Multi-Domain Supervision
- **Arxiv ID**: http://arxiv.org/abs/1905.10708v2
- **DOI**: 10.1109/IJCNN.2019.8851907
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.10708v2)
- **Published**: 2019-05-26 01:43:58+00:00
- **Updated**: 2019-11-02 02:29:12+00:00
- **Authors**: Dmitry A. Konovalov, Alzayat Saleh, Michael Bradley, Mangalam Sankupellay, Simone Marini, Marcus Sheaves
- **Comment**: Published in the 2019 International Joint Conference on Neural
  Networks (IJCNN-2019), Budapest, Hungary, July 14-19, 2019,
  https://www.ijcnn.org/ , https://ieeexplore.ieee.org/document/8851907
- **Journal**: 2019 International Joint Conference on Neural Networks (IJCNN),
  Budapest, Hungary, 2019, pp. 1-8
- **Summary**: Given a sufficiently large training dataset, it is relatively easy to train a modern convolution neural network (CNN) as a required image classifier. However, for the task of fish classification and/or fish detection, if a CNN was trained to detect or classify particular fish species in particular background habitats, the same CNN exhibits much lower accuracy when applied to new/unseen fish species and/or fish habitats. Therefore, in practice, the CNN needs to be continuously fine-tuned to improve its classification accuracy to handle new project-specific fish species or habitats. In this work we present a labelling-efficient method of training a CNN-based fish-detector (the Xception CNN was used as the base) on relatively small numbers (4,000) of project-domain underwater fish/no-fish images from 20 different habitats. Additionally, 17,000 of known negative (that is, missing fish) general-domain (VOC2012) above-water images were used. Two publicly available fish-domain datasets supplied additional 27,000 of above-water and underwater positive/fish images. By using this multi-domain collection of images, the trained Xception-based binary (fish/not-fish) classifier achieved 0.17% false-positives and 0.61% false-negatives on the project's 20,000 negative and 16,000 positive holdout test images, respectively. The area under the ROC curve (AUC) was 99.94%.



### Fixing Bias in Reconstruction-based Anomaly Detection with Lipschitz Discriminators
- **Arxiv ID**: http://arxiv.org/abs/1905.10710v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.10710v3)
- **Published**: 2019-05-26 01:57:42+00:00
- **Updated**: 2020-07-26 13:49:41+00:00
- **Authors**: Alexander Tong, Guy Wolf, Smita Krishnaswamy
- **Comment**: 6 pages, 4 figures, 2 tables, presented at IEEE MLSP
- **Journal**: None
- **Summary**: Anomaly detection is of great interest in fields where abnormalities need to be identified and corrected (e.g., medicine and finance). Deep learning methods for this task often rely on autoencoder reconstruction error, sometimes in conjunction with other errors. We show that this approach exhibits intrinsic biases that lead to undesirable results. Reconstruction-based methods are sensitive to training-data outliers and simple-to-reconstruct points. Instead, we introduce a new unsupervised Lipschitz anomaly discriminator that does not suffer from these biases. Our anomaly discriminator is trained, similar to the ones used in GANs, to detect the difference between the training data and corruptions of the training data. We show that this procedure successfully detects unseen anomalies with guarantees on those that have a certain Wasserstein distance from the data or corrupted training set. These additions allow us to show improved performance on MNIST, CIFAR10, and health record data.



### DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1905.10711v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10711v4)
- **Published**: 2019-05-26 01:58:28+00:00
- **Updated**: 2021-12-09 02:42:24+00:00
- **Authors**: Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir Mech, Ulrich Neumann
- **Comment**: None
- **Journal**: 33rd Annual Conference on Neural Information Processing Systems
  (NeurIPS 2019)
- **Summary**: Reconstructing 3D shapes from single-view images has been a long-standing research problem. In this paper, we present DISN, a Deep Implicit Surface Network which can generate a high-quality detail-rich 3D mesh from an 2D image by predicting the underlying signed distance fields. In addition to utilizing global image features, DISN predicts the projected location for each 3D point on the 2D image, and extracts local features from the image feature maps. Combining global and local features significantly improves the accuracy of the signed distance field prediction, especially for the detail-rich areas. To the best of our knowledge, DISN is the first method that constantly captures details such as holes and thin structures present in 3D shapes from single-view images. DISN achieves the state-of-the-art single-view reconstruction performance on a variety of shape categories reconstructed from both synthetic and real images. Code is available at https://github.com/xharlie/DISN The supplementary can be found at https://xharlie.github.io/images/neurips_2019_supp.pdf



### Efficient Weingarten Map and Curvature Estimation on Manifolds
- **Arxiv ID**: http://arxiv.org/abs/1905.10725v2
- **DOI**: 10.1007/s10994-021-05953-4
- **Categories**: **stat.ML**, cs.CV, cs.LG, math.DG
- **Links**: [PDF](http://arxiv.org/pdf/1905.10725v2)
- **Published**: 2019-05-26 04:48:32+00:00
- **Updated**: 2020-09-07 03:51:21+00:00
- **Authors**: Yueqi Cao, Didong Li, Huafei Sun, Amir H Assadi, Shiqiang Zhang
- **Comment**: 23 pages, 8 figures
- **Journal**: Machine Learning (2021)
- **Summary**: In this paper, we propose an efficient method to estimate the Weingarten map for point cloud data sampled from manifold embedded in Euclidean space. A statistical model is established to analyze the asymptotic property of the estimator. In particular, we show the convergence rate as the sample size tends to infinity. We verify the convergence rate through simulated data and apply the estimated Weingarten map to curvature estimation and point cloud simplification to multiple real data sets.



### Purifying Adversarial Perturbation with Adversarially Trained Auto-encoders
- **Arxiv ID**: http://arxiv.org/abs/1905.10729v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.10729v1)
- **Published**: 2019-05-26 04:57:55+00:00
- **Updated**: 2019-05-26 04:57:55+00:00
- **Authors**: Hebi Li, Qi Xiao, Shixin Tian, Jin Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning models are vulnerable to adversarial examples. Iterative adversarial training has shown promising results against strong white-box attacks. However, adversarial training is very expensive, and every time a model needs to be protected, such expensive training scheme needs to be performed. In this paper, we propose to apply iterative adversarial training scheme to an external auto-encoder, which once trained can be used to protect other models directly. We empirically show that our model outperforms other purifying-based methods against white-box attacks, and transfers well to directly protect other base models with different architectures.



### Disentangling Style and Content in Anime Illustrations
- **Arxiv ID**: http://arxiv.org/abs/1905.10742v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10742v3)
- **Published**: 2019-05-26 06:17:09+00:00
- **Updated**: 2019-12-20 14:52:26+00:00
- **Authors**: Sitao Xiang, Hao Li
- **Comment**: None
- **Journal**: None
- **Summary**: Existing methods for AI-generated artworks still struggle with generating high-quality stylized content, where high-level semantics are preserved, or separating fine-grained styles from various artists. We propose a novel Generative Adversarial Disentanglement Network which can disentangle two complementary factors of variations when only one of them is labelled in general, and fully decompose complex anime illustrations into style and content in particular. Training such model is challenging, since given a style, various content data may exist but not the other way round. Our approach is divided into two stages, one that encodes an input image into a style independent content, and one based on a dual-conditional generator. We demonstrate the ability to generate high-fidelity anime portraits with a fixed content and a large variety of styles from over a thousand artists, and vice versa, using a single end-to-end network and with applications in style transfer. We show this unique capability as well as superior output to the current state-of-the-art.



### Learning Smooth Representation for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1905.10748v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10748v4)
- **Published**: 2019-05-26 06:55:30+00:00
- **Updated**: 2021-08-16 12:28:11+00:00
- **Authors**: Guanyu Cai, Lianghua He, Mengchu Zhou, Hesham Alhumade, Die Hu
- **Comment**: Code is available at https://github.com/CuthbertCai/SRDA. Accepted by
  IEEE Transactions on Neural Networks and Learning Systems
- **Journal**: None
- **Summary**: Typical adversarial-training-based unsupervised domain adaptation methods are vulnerable when the source and target datasets are highly-complex or exhibit a large discrepancy between their data distributions. Recently, several Lipschitz-constraint-based methods have been explored. The satisfaction of Lipschitz continuity guarantees a remarkable performance on a target domain. However, they lack a mathematical analysis of why a Lipschitz constraint is beneficial to unsupervised domain adaptation and usually perform poorly on large-scale datasets. In this paper, we take the principle of utilizing a Lipschitz constraint further by discussing how it affects the error bound of unsupervised domain adaptation. A connection between them is built and an illustration of how Lipschitzness reduces the error bound is presented. A \textbf{local smooth discrepancy} is defined to measure Lipschitzness of a target distribution in a pointwise way. When constructing a deep end-to-end model, to ensure the effectiveness and stability of unsupervised domain adaptation, three critical factors are considered in our proposed optimization strategy, i.e., the sample amount of a target domain, dimension and batchsize of samples. Experimental results demonstrate that our model performs well on several standard benchmarks. Our ablation study shows that the sample amount of a target domain, the dimension and batchsize of samples indeed greatly impact Lipschitz-constraint-based methods' ability to handle large-scale datasets. Code is available at https://github.com/CuthbertCai/SRDA.



### Selective Transfer with Reinforced Transfer Network for Partial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1905.10756v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.10756v4)
- **Published**: 2019-05-26 07:59:36+00:00
- **Updated**: 2020-02-28 01:59:52+00:00
- **Authors**: Zhihong Chen, Chao Chen, Zhaowei Cheng, Boyuan Jiang, Ke Fang, Xinyu Jin
- **Comment**: None
- **Journal**: None
- **Summary**: One crucial aspect of partial domain adaptation (PDA) is how to select the relevant source samples in the shared classes for knowledge transfer. Previous PDA methods tackle this problem by re-weighting the source samples based on their high-level information (deep features). However, since the domain shift between source and target domains, only using the deep features for sample selection is defective. We argue that it is more reasonable to additionally exploit the pixel-level information for PDA problem, as the appearance difference between outlier source classes and target classes is significantly large. In this paper, we propose a reinforced transfer network (RTNet), which utilizes both high-level and pixel-level information for PDA problem. Our RTNet is composed of a reinforced data selector (RDS) based on reinforcement learning (RL), which filters out the outlier source samples, and a domain adaptation model which minimizes the domain discrepancy in the shared label space. Specifically, in the RDS, we design a novel reward based on the reconstruct errors of selected source samples on the target generator, which introduces the pixel-level information to guide the learning of RDS. Besides, we develope a state containing high-level information, which used by the RDS for sample selection. The proposed RDS is a general module, which can be easily integrated into existing DA models to make them fit the PDA situation. Extensive experiments indicate that RTNet can achieve state-of-the-art performance for PDA tasks on several benchmark datasets.



### HadaNets: Flexible Quantization Strategies for Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.10759v1
- **DOI**: 10.1109/CVPRW.2019.00078
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.10759v1)
- **Published**: 2019-05-26 08:17:54+00:00
- **Updated**: 2019-05-26 08:17:54+00:00
- **Authors**: Yash Akhauri
- **Comment**: Accepted in CVPR 2019, UAVision 2019
- **Journal**: None
- **Summary**: On-board processing elements on UAVs are currently inadequate for training and inference of Deep Neural Networks. This is largely due to the energy consumption of memory accesses in such a network. HadaNets introduce a flexible train-from-scratch tensor quantization scheme by pairing a full precision tensor to a binary tensor in the form of a Hadamard product. Unlike wider reduced precision neural network models, we preserve the train-time parameter count, thus out-performing XNOR-Nets without a train-time memory penalty. Such training routines could see great utility in semi-supervised online learning tasks. Our method also offers advantages in model compression, as we reduce the model size of ResNet-18 by 7.43 times with respect to a full precision model without utilizing any other compression techniques. We also demonstrate a 'Hadamard Binary Matrix Multiply' kernel, which delivers a 10-fold increase in performance over full precision matrix multiplication with a similarly optimized kernel.



### Cross-Resolution Face Recognition via Prior-Aided Face Hallucination and Residual Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/1905.10777v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10777v1)
- **Published**: 2019-05-26 10:08:17+00:00
- **Updated**: 2019-05-26 10:08:17+00:00
- **Authors**: Hanyang Kong, Jian Zhao, Xiaoguang Tu, Junliang Xing, Shengmei Shen, Jiashi Feng
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Recent deep learning based face recognition methods have achieved great performance, but it still remains challenging to recognize very low-resolution query face like 28x28 pixels when CCTV camera is far from the captured subject. Such face with very low-resolution is totally out of detail information of the face identity compared to normal resolution in a gallery and hard to find corresponding faces therein. To this end, we propose a Resolution Invariant Model (RIM) for addressing such cross-resolution face recognition problems, with three distinct novelties. First, RIM is a novel and unified deep architecture, containing a Face Hallucination sub-Net (FHN) and a Heterogeneous Recognition sub-Net (HRN), which are jointly learned end to end. Second, FHN is a well-designed tri-path Generative Adversarial Network (GAN) which simultaneously perceives facial structure and geometry prior information, i.e. landmark heatmaps and parsing maps, incorporated with an unsupervised cross-domain adversarial training strategy to super-resolve very low-resolution query image to its 8x larger ones without requiring them to be well aligned. Third, HRN is a generic Convolutional Neural Network (CNN) for heterogeneous face recognition with our proposed residual knowledge distillation strategy for learning discriminative yet generalized feature representation. Quantitative and qualitative experiments on several benchmarks demonstrate the superiority of the proposed model over the state-of-the-arts. Codes and models will be released upon acceptance.



### Impact of facial landmark localization on facial expression recognition
- **Arxiv ID**: http://arxiv.org/abs/1905.10784v3
- **DOI**: 10.1109/TAFFC.2021.3124142
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10784v3)
- **Published**: 2019-05-26 10:50:46+00:00
- **Updated**: 2021-07-19 07:11:14+00:00
- **Authors**: Romain Belmonte, Benjamin Allaert, Pierre Tirilly, Ioan Marius Bilasco, Chaabane Djeraba, Nicu Sebe
- **Comment**: None
- **Journal**: None
- **Summary**: Although facial landmark localization (FLL) approaches are becoming increasingly accurate for characterizing facial regions, one question remains unanswered: what is the impact of these approaches on subsequent related tasks? In this paper, the focus is put on facial expression recognition (FER), where facial landmarks are used for face registration, which is a common usage. Since the most used datasets for facial landmark localization do not allow for a proper measurement of performance according to the different difficulties (e.g., pose, expression, illumination, occlusion, motion blur), we also quantify the performance of recent approaches in the presence of head pose variations and facial expressions. Finally, a study of the impact of these approaches on FER is conducted. We show that the landmark accuracy achieved so far optimizing the conventional Euclidean distance does not necessarily guarantee a gain in performance for FER. To deal with this issue, we propose a new evaluation metric for FLL adapted to FER.



### Unsupervised Intuitive Physics from Past Experiences
- **Arxiv ID**: http://arxiv.org/abs/1905.10793v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1905.10793v1)
- **Published**: 2019-05-26 12:22:56+00:00
- **Updated**: 2019-05-26 12:22:56+00:00
- **Authors**: Sébastien Ehrhardt, Aron Monszpart, Niloy J. Mitra, Andrea Vedaldi
- **Comment**: Under review
- **Journal**: None
- **Summary**: We are interested in learning models of intuitive physics similar to the ones that animals use for navigation, manipulation and planning. In addition to learning general physical principles, however, we are also interested in learning ``on the fly'', from a few experiences, physical properties specific to new environments. We do all this in an unsupervised manner, using a meta-learning formulation where the goal is to predict videos containing demonstrations of physical phenomena, such as objects moving and colliding with a complex background. We introduce the idea of summarizing past experiences in a very compact manner, in our case using dynamic images, and show that this can be used to solve the problem well and efficiently. Empirically, we show via extensive experiments and ablation studies, that our model learns to perform physical predictions that generalize well in time and space, as well as to a variable number of interacting physical objects.



### Why do These Match? Explaining the Behavior of Image Similarity Models
- **Arxiv ID**: http://arxiv.org/abs/1905.10797v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10797v2)
- **Published**: 2019-05-26 12:48:23+00:00
- **Updated**: 2020-08-24 16:14:37+00:00
- **Authors**: Bryan A. Plummer, Mariya I. Vasileva, Vitali Petsiuk, Kate Saenko, David Forsyth
- **Comment**: Accepted at ECCV 2020
- **Journal**: None
- **Summary**: Explaining a deep learning model can help users understand its behavior and allow researchers to discern its shortcomings. Recent work has primarily focused on explaining models for tasks like image classification or visual question answering. In this paper, we introduce Salient Attributes for Network Explanation (SANE) to explain image similarity models, where a model's output is a score measuring the similarity of two inputs rather than a classification score. In this task, an explanation depends on both of the input images, so standard methods do not apply. Our SANE explanations pairs a saliency map identifying important image regions with an attribute that best explains the match. We find that our explanations provide additional information not typically captured by saliency maps alone, and can also improve performance on the classic task of attribute recognition. Our approach's ability to generalize is demonstrated on two datasets from diverse domains, Polyvore Outfits and Animals with Attributes 2. Code available at: https://github.com/VisionLearningGroup/SANE



### A Survey on Biomedical Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1905.13302v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.13302v1)
- **Published**: 2019-05-26 15:47:28+00:00
- **Updated**: 2019-05-26 15:47:28+00:00
- **Authors**: Vasiliki Kougia, John Pavlopoulos, Ion Androutsopoulos
- **Comment**: SiVL 2019
- **Journal**: None
- **Summary**: Image captioning applied to biomedical images can assist and accelerate the diagnosis process followed by clinicians. This article is the first survey of biomedical image captioning, discussing datasets, evaluation measures, and state of the art methods. Additionally, we suggest two baselines, a weak and a stronger one; the latter outperforms all current state of the art systems on one of the datasets.



### EgoFace: Egocentric Face Performance Capture and Videorealistic Reenactment
- **Arxiv ID**: http://arxiv.org/abs/1905.10822v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1905.10822v1)
- **Published**: 2019-05-26 15:57:47+00:00
- **Updated**: 2019-05-26 15:57:47+00:00
- **Authors**: Mohamed Elgharib, Mallikarjun BR, Ayush Tewari, Hyeongwoo Kim, Wentao Liu, Hans-Peter Seidel, Christian Theobalt
- **Comment**: Project Page: http://gvv.mpi-inf.mpg.de/projects/EgoFace/
- **Journal**: None
- **Summary**: Face performance capture and reenactment techniques use multiple cameras and sensors, positioned at a distance from the face or mounted on heavy wearable devices. This limits their applications in mobile and outdoor environments. We present EgoFace, a radically new lightweight setup for face performance capture and front-view videorealistic reenactment using a single egocentric RGB camera. Our lightweight setup allows operations in uncontrolled environments, and lends itself to telepresence applications such as video-conferencing from dynamic environments. The input image is projected into a low dimensional latent space of the facial expression parameters. Through careful adversarial training of the parameter-space synthetic rendering, a videorealistic animation is produced. Our problem is challenging as the human visual system is sensitive to the smallest face irregularities that could occur in the final results. This sensitivity is even stronger for video results. Our solution is trained in a pre-processing stage, through a supervised manner without manual annotations. EgoFace captures a wide variety of facial expressions, including mouth movements and asymmetrical expressions. It works under varying illuminations, background, movements, handles people from different ethnicities and can operate in real time.



### Feature Map Transform Coding for Energy-Efficient CNN Inference
- **Arxiv ID**: http://arxiv.org/abs/1905.10830v3
- **DOI**: 10.1109/IJCNN48605.2020.9206968
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.10830v3)
- **Published**: 2019-05-26 16:29:47+00:00
- **Updated**: 2019-09-26 14:43:47+00:00
- **Authors**: Brian Chmiel, Chaim Baskin, Ron Banner, Evgenii Zheltonozhskii, Yevgeny Yermolin, Alex Karbachevsky, Alex M. Bronstein, Avi Mendelson
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) achieve state-of-the-art accuracy in a variety of tasks in computer vision and beyond. One of the major obstacles hindering the ubiquitous use of CNNs for inference on low-power edge devices is their high computational complexity and memory bandwidth requirements. The latter often dominates the energy footprint on modern hardware. In this paper, we introduce a lossy transform coding approach, inspired by image and video compression, designed to reduce the memory bandwidth due to the storage of intermediate activation calculation results. Our method does not require fine-tuning the network weights and halves the data transfer volumes to the main memory by compressing feature maps, which are highly correlated, with variable length coding. Our method outperform previous approach in term of the number of bits per value with minor accuracy degradation on ResNet-34 and MobileNetV2. We analyze the performance of our approach on a variety of CNN architectures and demonstrate that FPGA implementation of ResNet-18 with our approach results in a reduction of around 40% in the memory energy footprint, compared to quantized network, with negligible impact on accuracy. When allowing accuracy degradation of up to 2%, the reduction of 60% is achieved. A reference implementation is available at https://github.com/CompressTeam/TransformCodingInference



### A multi-path 2.5 dimensional convolutional neural network system for segmenting stroke lesions in brain MRI images
- **Arxiv ID**: http://arxiv.org/abs/1905.10835v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10835v1)
- **Published**: 2019-05-26 16:39:04+00:00
- **Updated**: 2019-05-26 16:39:04+00:00
- **Authors**: Yunzhe Xue, Fadi G. Farhat, Olga Boukrina, A . M. Barrett, Jeffrey R. Binder, Usman W. Roshan, William W. Graves
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic identification of brain lesions from magnetic resonance imaging (MRI) scans of stroke survivors would be a useful aid in patient diagnosis and treatment planning. We propose a multi-modal multi-path convolutional neural network system for automating stroke lesion segmentation. Our system has nine end-to-end UNets that take as input 2-dimensional (2D) slices and examines all three planes with three different normalizations. Outputs from these nine total paths are concatenated into a 3D volume that is then passed to a 3D convolutional neural network to output a final lesion mask. We trained and tested our method on datasets from three sources: Medical College of Wisconsin (MCW), Kessler Foundation (KF), and the publicly available Anatomical Tracings of Lesions After Stroke (ATLAS) dataset. Cross-study validation results (with independent training and validation datasets) were obtained to compare with previous methods based on naive Bayes, random forests, and three recently published convolutional neural networks. Model performance was quantified in terms of the Dice coefficient. Training on the KF and MCW images and testing on the ATLAS images yielded a mean Dice coefficient of 0.54. This was reliably better than the next best previous model, UNet, at 0.47. Reversing the train and test datasets yields a mean Dice of 0.47 on KF and MCW images, whereas the next best UNet reaches 0.45. With all three datasets combined, the current system compared to previous methods also attained a reliably higher cross-validation accuracy. It also achieved high Dice values for many smaller lesions that existing methods have difficulty identifying. Overall, our system is a clear improvement over previous methods for automating stroke lesion segmentation, bringing us an important step closer to the inter-rater accuracy level of human experts.



### OOGAN: Disentangling GAN with One-Hot Sampling and Orthogonal Regularization
- **Arxiv ID**: http://arxiv.org/abs/1905.10836v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10836v5)
- **Published**: 2019-05-26 16:42:56+00:00
- **Updated**: 2020-03-10 21:14:52+00:00
- **Authors**: Bingchen Liu, Yizhe Zhu, Zuohui Fu, Gerard de Melo, Ahmed Elgammal
- **Comment**: AAAI 2020
- **Journal**: None
- **Summary**: Exploring the potential of GANs for unsupervised disentanglement learning, this paper proposes a novel GAN-based disentanglement framework with One-Hot Sampling and Orthogonal Regularization (OOGAN). While previous works mostly attempt to tackle disentanglement learning through VAE and seek to implicitly minimize the Total Correlation (TC) objective with various sorts of approximation methods, we show that GANs have a natural advantage in disentangling with an alternating latent variable (noise) sampling method that is straightforward and robust. Furthermore, we provide a brand-new perspective on designing the structure of the generator and discriminator, demonstrating that a minor structural change and an orthogonal regularization on model weights entails an improved disentanglement. Instead of experimenting on simple toy datasets, we conduct experiments on higher-resolution images and show that OOGAN greatly pushes the boundary of unsupervised disentanglement.



### Utilizing Automated Breast Cancer Detection to Identify Spatial Distributions of Tumor Infiltrating Lymphocytes in Invasive Breast Cancer
- **Arxiv ID**: http://arxiv.org/abs/1905.10841v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.10841v3)
- **Published**: 2019-05-26 17:25:18+00:00
- **Updated**: 2020-01-13 16:32:45+00:00
- **Authors**: Han Le, Rajarsi Gupta, Le Hou, Shahira Abousamra, Danielle Fassler, Tahsin Kurc, Dimitris Samaras, Rebecca Batiste, Tianhao Zhao, Arvind Rao, Alison L. Van Dyke, Ashish Sharma, Erich Bremer, Jonas S. Almeida, Joel Saltz
- **Comment**: The American Journal of Pathology
- **Journal**: None
- **Summary**: Quantitative assessment of Tumor-TIL spatial relationships is increasingly important in both basic science and clinical aspects of breast cancer research. We have developed and evaluated convolutional neural network (CNN) analysis pipelines to generate combined maps of cancer regions and tumor infiltrating lymphocytes (TILs) in routine diagnostic breast cancer whole slide tissue images (WSIs). We produce interactive whole slide maps that provide 1) insight about the structural patterns and spatial distribution of lymphocytic infiltrates and 2) facilitate improved quantification of TILs. We evaluated both tumor and TIL analyses using three CNN networks - Resnet-34, VGG16 and Inception v4, and demonstrated that the results compared favorably to those obtained by what believe are the best published methods. We have produced open-source tools and generated a public dataset consisting of tumor/TIL maps for 1,015 TCGA breast cancer images. We also present a customized web-based interface that enables easy visualization and interactive exploration of high-resolution combined Tumor-TIL maps for 1,015TCGA invasive breast cancer cases that can be downloaded for further downstream analyses.



### Technical Report of the Video Event Reconstruction and Analysis (VERA) System -- Shooter Localization, Models, Interface, and Beyond
- **Arxiv ID**: http://arxiv.org/abs/1905.13313v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1905.13313v5)
- **Published**: 2019-05-26 17:55:50+00:00
- **Updated**: 2019-07-05 05:23:13+00:00
- **Authors**: Junwei Liang, Jay D. Aronson, Alexander Hauptmann
- **Comment**: The code and models are available at
  https://github.com/JunweiLiang/VERA_Shooter_Localization . Our system is live
  at https://vera.cs.cmu.edu/
- **Journal**: None
- **Summary**: Every minute, hundreds of hours of video are uploaded to social media sites and the Internet from around the world. This material creates a visual record of the experiences of a significant percentage of humanity and can help illuminate how we live in the present moment. When properly analyzed, this video can also help analysts to reconstruct events of interest, including war crimes, human rights violations, and terrorist acts. Machine learning and computer vision can play a crucial role in this process. In this technical report, we describe the Video Event Reconstruction and Analysis (VERA) system. This new tool brings together a variety of capabilities we have developed over the past few years (including video synchronization and geolocation to order unstructured videos lacking metadata over time and space, and sound recognition algorithms) to enable the reconstruction and analysis of events captured on video. Among other uses, VERA enables the localization of a shooter from just a few videos that include the sound of gunshots. To demonstrate the efficacy of this suite of tools, we present the results of estimating the shooter's location of the Las Vegas Shooting in 2017 and show that VERA accurately predicts the shooter's location using only the first few gunshots. We then point out future directions that can help improve the system and further reduce unnecessary human labor in the process. All of the components of VERA run through a web interface that enables human-in-the-loop verification to ensure accurate estimations. All relevant source code, including the web interface and machine learning models, is freely available on Github. We hope that researchers and software developers will be inspired to improve and expand this system moving forward to better meet the needs of human rights and public safety.



### Integration of Text-maps in Convolutional Neural Networks for Region Detection among Different Textual Categories
- **Arxiv ID**: http://arxiv.org/abs/1905.10858v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10858v1)
- **Published**: 2019-05-26 18:59:32+00:00
- **Updated**: 2019-05-26 18:59:32+00:00
- **Authors**: Roberto Arroyo, Javier Tovar, Francisco J. Delgado, Emilio J. Almazán, Diego G. Serrador, Antonio Hurtado
- **Comment**: Conference on Computer Vision and Pattern Recognition (CVPR).
  Language and Vision Workshop 2019
- **Journal**: None
- **Summary**: In this work, we propose a new technique that combines appearance and text in a Convolutional Neural Network (CNN), with the aim of detecting regions of different textual categories. We define a novel visual representation of the semantic meaning of text that allows a seamless integration in a standard CNN architecture. This representation, referred to as text-map, is integrated with the actual image to provide a much richer input to the network. Text-maps are colored with different intensities depending on the relevance of the words recognized over the image. Concretely, these words are previously extracted using Optical Character Recognition (OCR) and they are colored according to the probability of belonging to a textual category of interest. In this sense, this solution is especially relevant in the context of item coding for supermarket products, where different types of textual categories must be identified, such as ingredients or nutritional facts. We evaluated our solution in the proprietary item coding dataset of Nielsen Brandbank, which contains more than 10,000 images for train and 2,000 images for test. The reported results demonstrate that our approach focused on visual and textual data outperforms state-of-the-art algorithms only based on appearance, such as standard Faster R-CNN. These enhancements are reflected in precision and recall, which are improved in 42 and 33 points respectively.



### Temporal Attentive Alignment for Video Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1905.10861v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1905.10861v5)
- **Published**: 2019-05-26 19:15:23+00:00
- **Updated**: 2019-06-07 03:45:29+00:00
- **Authors**: Min-Hung Chen, Zsolt Kira, Ghassan AlRegib
- **Comment**: CVPR2019 Workshop (Learning from Unlabeled Videos). Source code:
  http://github.com/cmhungsteve/TA3N
- **Journal**: None
- **Summary**: Although various image-based domain adaptation (DA) techniques have been proposed in recent years, domain shift in videos is still not well-explored. Most previous works only evaluate performance on small-scale datasets which are saturated. Therefore, we first propose a larger-scale dataset with larger domain discrepancy: UCF-HMDB_full. Second, we investigate different DA integration methods for videos, and show that simultaneously aligning and learning temporal dynamics achieves effective alignment even without sophisticated DA methods. Finally, we propose Temporal Attentive Adversarial Adaptation Network (TA3N), which explicitly attends to the temporal dynamics using domain discrepancy for more effective domain alignment, achieving state-of-the-art performance on three video DA datasets. The code and data are released at http://github.com/cmhungsteve/TA3N.



### Road Segmentation with Image-LiDAR Data Fusion
- **Arxiv ID**: http://arxiv.org/abs/1905.11559v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.11559v1)
- **Published**: 2019-05-26 19:18:12+00:00
- **Updated**: 2019-05-26 19:18:12+00:00
- **Authors**: Huafeng Liu, Yazhou Yao, Zeren Sun, Xiangrui Li, Ke Jia, Zhenmin Tang
- **Comment**: Accepted by Multimedia Tools and Applications
- **Journal**: None
- **Summary**: Robust road segmentation is a key challenge in self-driving research. Though many image-based methods have been studied and high performances in dataset evaluations have been reported, developing robust and reliable road segmentation is still a major challenge. Data fusion across different sensors to improve the performance of road segmentation is widely considered an important and irreplaceable solution. In this paper, we propose a novel structure to fuse image and LiDAR point cloud in an end-to-end semantic segmentation network, in which the fusion is performed at decoder stage instead of at, more commonly, encoder stage. During fusion, we improve the multi-scale LiDAR map generation to increase the precision of the multi-scale LiDAR map by introducing pyramid projection method. Additionally, we adapted the multi-path refinement network with our fusion strategy and improve the road prediction compared with transpose convolution with skip layers. Our approach has been tested on KITTI ROAD dataset and has competitive performance.



### Deep Representation Learning for Road Detection through Siamese Network
- **Arxiv ID**: http://arxiv.org/abs/1905.13394v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.13394v1)
- **Published**: 2019-05-26 19:44:10+00:00
- **Updated**: 2019-05-26 19:44:10+00:00
- **Authors**: Huafeng Liu, Xiaofeng Han, Xiangrui Li, Yazhou Yao, Pu Huang, Zhenming Tang
- **Comment**: Accepted by Multimedia Tools and Applications
- **Journal**: None
- **Summary**: Robust road detection is a key challenge in safe autonomous driving. Recently, with the rapid development of 3D sensors, more and more researchers are trying to fuse information across different sensors to improve the performance of road detection. Although many successful works have been achieved in this field, methods for data fusion under deep learning framework is still an open problem. In this paper, we propose a Siamese deep neural network based on FCN-8s to detect road region. Our method uses data collected from a monocular color camera and a Velodyne-64 LiDAR sensor. We project the LiDAR point clouds onto the image plane to generate LiDAR images and feed them into one of the branches of the network. The RGB images are fed into another branch of our proposed network. The feature maps that these two branches extract in multiple scales are fused before each pooling layer, via padding additional fusion layers. Extensive experimental results on public dataset KITTI ROAD demonstrate the effectiveness of our proposed approach.



### Unsupervised Domain Adaptation via Regularized Conditional Alignment
- **Arxiv ID**: http://arxiv.org/abs/1905.10885v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.10885v1)
- **Published**: 2019-05-26 21:25:45+00:00
- **Updated**: 2019-05-26 21:25:45+00:00
- **Authors**: Safa Cicek, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method for unsupervised domain adaptation that trains a shared embedding to align the joint distributions of inputs (domain) and outputs (classes), making any classifier agnostic to the domain. Joint alignment ensures that not only the marginal distributions of the domain are aligned, but the labels as well. We propose a novel objective function that encourages the class-conditional distributions to have disjoint support in feature space. We further exploit adversarial regularization to improve the performance of the classifier on the domain for which no annotated data is available.



### Seeing Convolution Through the Eyes of Finite Transformation Semigroup Theory: An Abstract Algebraic Interpretation of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.10901v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1905.10901v1)
- **Published**: 2019-05-26 23:11:18+00:00
- **Updated**: 2019-05-26 23:11:18+00:00
- **Authors**: Andrew Hryniowski, Alexander Wong
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Researchers are actively trying to gain better insights into the representational properties of convolutional neural networks for guiding better network designs and for interpreting a network's computational nature. Gaining such insights can be an arduous task due to the number of parameters in a network and the complexity of a network's architecture. Current approaches of neural network interpretation include Bayesian probabilistic interpretations and information theoretic interpretations. In this study, we take a different approach to studying convolutional neural networks by proposing an abstract algebraic interpretation using finite transformation semigroup theory. Specifically, convolutional layers are broken up and mapped to a finite space. The state space of the proposed finite transformation semigroup is then defined as a single element within the convolutional layer, with the acting elements defined by surrounding state elements combined with convolution kernel elements. Generators of the finite transformation semigroup are defined to complete the interpretation. We leverage this approach to analyze the basic properties of the resulting finite transformation semigroup to gain insights on the representational properties of convolutional neural networks, including insights into quantized network representation. Such a finite transformation semigroup interpretation can also enable better understanding outside of the confines of fixed lattice data structures, thus useful for handling data that lie on irregular lattices. Furthermore, the proposed abstract algebraic interpretation is shown to be viable for interpreting convolutional operations within a variety of convolutional neural network architectures.



### Automatic Delineation of Kidney Region in DCE-MRI
- **Arxiv ID**: http://arxiv.org/abs/1905.11387v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.11387v1)
- **Published**: 2019-05-26 23:25:59+00:00
- **Updated**: 2019-05-26 23:25:59+00:00
- **Authors**: Santosh Tirunagari, Norman Poh, Kevin Wells, Miroslaw Bober, Isky Gorden, David Windridge
- **Comment**: arXiv admin note: text overlap with arXiv:1905.10218
- **Journal**: None
- **Summary**: Delineation of the kidney region in dynamic contrast-enhanced magnetic resonance Imaging (DCE-MRI) is required during post-acquisition analysis in order to quantify various aspects of renal function, such as filtration and perfusion or blood flow. However, this can be obfuscated by the Partial Volume Effect (PVE), caused due to the mixing of any single voxel with two or more signal intensities from adjacent regions such as liver region and other tissues. To avoid this problem, firstly, a kidney region of interest (ROI) needs to be defined for the analysis. A clinician may choose to select a region avoiding edges where PV mixing is likely to be significant. However, this approach is time-consuming and labour intensive. To address this issue, we present Dynamic Mode Decomposition (DMD) coupled with thresholding and blob analysis as a framework for automatic delineation of the kidney region. This method is first validated on synthetically generated data with ground-truth available and then applied to ten healthy volunteers' kidney DCE-MRI datasets. We found that the result obtained from our proposed framework is comparable to that of a human expert. For example, while our result gives an average Root Mean Square Error (RMSE) of 0.0097, the baseline achieves an average RMSE of 0.1196 across the 10 datasets. As a result, we conclude automatic modelling via DMD framework is a promising approach.



