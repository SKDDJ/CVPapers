# Arxiv Papers in cs.CV on 2019-05-24
### Texture retrieval using periodically extended and adaptive curvelets
- **Arxiv ID**: http://arxiv.org/abs/1905.09976v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.09976v1)
- **Published**: 2019-05-24 00:15:19+00:00
- **Updated**: 2019-05-24 00:15:19+00:00
- **Authors**: Hasan Al-Marzouqi, Yuting Hu, Ghassan AlRegib
- **Comment**: None
- **Journal**: Signal Processing: Image Communication, Volume 76, 2019, Pages
  252-260, ISSN 0923-5965
- **Summary**: Image retrieval is an important problem in the area of multimedia processing. This paper presents two new curvelet-based algorithms for texture retrieval which are suitable for use in constrained-memory devices. The developed algorithms are tested on three publicly available texture datasets: CUReT, Mondial-Marmi, and STex-fabric. Our experiments confirm the effectiveness of the proposed system. Furthermore, a weighted version of the proposed retrieval algorithm is proposed, which is shown to achieve promising results in the classification of seismic activities.



### EnsembleNet: End-to-End Optimization of Multi-headed Models
- **Arxiv ID**: http://arxiv.org/abs/1905.09979v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.09979v2)
- **Published**: 2019-05-24 00:31:11+00:00
- **Updated**: 2019-09-26 04:00:03+00:00
- **Authors**: Hanhan Li, Joe Yue-Hei Ng, Paul Natsev
- **Comment**: None
- **Journal**: None
- **Summary**: Ensembling is a universally useful approach to boost the performance of machine learning models. However, individual models in an ensemble were traditionally trained independently in separate stages without information access about the overall ensemble. Many co-distillation approaches were proposed in order to treat model ensembling as first-class citizens. In this paper, we reveal a deeper connection between ensembling and distillation, and come up with a simpler yet more effective co-distillation architecture. On large-scale datasets including ImageNet, YouTube-8M, and Kinetics, we demonstrate a general procedure that can convert a single deep neural network to a multi-headed model that has not only a smaller size but also better performance. The model can be optimized end-to-end with our proposed co-distillation loss in a single stage without human intervention.



### Perception Evaluation -- A new solar image quality metric based on the multi-fractal property of texture features
- **Arxiv ID**: http://arxiv.org/abs/1905.09980v2
- **DOI**: 10.1007/s11207-019-1524-5
- **Categories**: **astro-ph.IM**, astro-ph.SR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.09980v2)
- **Published**: 2019-05-24 00:33:17+00:00
- **Updated**: 2019-06-26 08:31:02+00:00
- **Authors**: Yi Huang, Peng Jia, Dongmei Cai, Bojun Cai
- **Comment**: 15 pages, 13 figures, accepted by Solar Physics
- **Journal**: None
- **Summary**: Next-generation ground-based solar observations require good image quality metrics for post-facto processing techniques. Based on the assumption that texture features in solar images are multi-fractal which can be extracted by a trained deep neural network as feature maps, a new reduced-reference objective image quality metric, the perception evaluation is proposed. The perception evaluation is defined as cosine distance of Gram matrix between feature maps extracted from high resolution reference image and that from blurred images. We evaluate performance of the perception evaluation with simulated and real observation images. The results show that with a high resolution image as reference, the perception evaluation can give robust estimate of image quality for solar images of different scenes.



### Synthesizing Images from Spatio-Temporal Representations using Spike-based Backpropagation
- **Arxiv ID**: http://arxiv.org/abs/1906.08861v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.08861v1)
- **Published**: 2019-05-24 01:33:15+00:00
- **Updated**: 2019-05-24 01:33:15+00:00
- **Authors**: Deboleena Roy, Priyadarshini Panda, Kaushik Roy
- **Comment**: 17 pages, 10 Figures, 1 table
- **Journal**: None
- **Summary**: Spiking neural networks (SNNs) offer a promising alternative to current artificial neural networks to enable low-power event-driven neuromorphic hardware. Spike-based neuromorphic applications require processing and extracting meaningful information from spatio-temporal data, represented as series of spike trains over time. In this paper, we propose a method to synthesize images from multiple modalities in a spike-based environment. We use spiking auto-encoders to convert image and audio inputs into compact spatio-temporal representations that is then decoded for image synthesis. For this, we use a direct training algorithm that computes loss on the membrane potential of the output layer and back-propagates it by using a sigmoid approximation of the neuron's activation function to enable differentiability. The spiking autoencoders are benchmarked on MNIST and Fashion-MNIST and achieve very low reconstruction loss, comparable to ANNs. Then, spiking autoencoders are trained to learn meaningful spatio-temporal representations of the data, across the two modalities - audio and visual. We synthesize images from audio in a spike-based environment by first generating, and then utilizing such shared multi-modal spatio-temporal representations. Our audio to image synthesis model is tested on the task of converting TI-46 digits audio samples to MNIST images. We are able to synthesize images with high fidelity and the model achieves competitive performance against ANNs.



### FSD: Feature Skyscraper Detector for Stem End and Blossom End of Navel Orange
- **Arxiv ID**: http://arxiv.org/abs/1905.09994v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.09994v2)
- **Published**: 2019-05-24 01:44:57+00:00
- **Updated**: 2019-11-11 16:30:15+00:00
- **Authors**: Xiaoye Sun, Gongyan Li, Shaoyun Xu
- **Comment**: None
- **Journal**: None
- **Summary**: To accurately and efficiently distinguish the stem end and the blossom end of navel orange from its black spots, we propose a feature skyscraper detector (FSD) with low computational cost, compact architecture and high detection accuracy. The main part of the detector is inspired from small object that stem (blossom) end is complex and black spot is densely distributed, so we design the feature skyscraper networks (FSN) based on dense connectivity. In particular, FSN is distinguished from regular feature pyramids, and which provides more intensive detection of high-level features. Then we design the backbone of the FSD based on attention mechanism and dense block for better feature extraction to the FSN. In addition, the architecture of the detector is also added Swish to further improve the accuracy. And we create a dataset in Pascal VOC format annotated three types of detection targets the stem end, the blossom end and the black spot. Experimental results on our orange data set confirm that FSD has competitive results to the state-of-the-art one-stage detectors like SSD, DSOD, YOLOv2, YOLOv3, RFB and FSSD, and it achieves 87.479%mAP at 131 FPS with only 5.812M parameters.



### Self-Critical Reasoning for Robust Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1905.09998v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1905.09998v3)
- **Published**: 2019-05-24 01:52:31+00:00
- **Updated**: 2019-12-30 06:33:21+00:00
- **Authors**: Jialin Wu, Raymond J. Mooney
- **Comment**: In NeurIPS 2019
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) deep-learning systems tend to capture superficial statistical correlations in the training data because of strong language priors and fail to generalize to test data with a significantly different question-answer (QA) distribution. To address this issue, we introduce a self-critical training objective that ensures that visual explanations of correct answers match the most influential image regions more than other competitive answer candidates. The influential regions are either determined from human visual/textual explanations or automatically from just significant words in the question and answer. We evaluate our approach on the VQA generalization task using the VQA-CP dataset, achieving a new state-of-the-art i.e., 49.5% using textual explanations and 48.5% using automatically annotated regions.



### Implicit Label Augmentation on Partially Annotated Clips via Temporally-Adaptive Features Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.10000v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.10000v1)
- **Published**: 2019-05-24 02:02:35+00:00
- **Updated**: 2019-05-24 02:02:35+00:00
- **Authors**: Yongxi Lu, Ziyao Tang, Tara Javidi
- **Comment**: None
- **Journal**: None
- **Summary**: Partially annotated clips contain rich temporal contexts that can complement the sparse key frame annotations in providing supervision for model training. We present a novel paradigm called Temporally-Adaptive Features (TAF) learning that can utilize such data to learn better single frame models. By imposing distinct temporal change rate constraints on different factors in the model, TAF enables learning from unlabeled frames using context to enhance model accuracy. TAF generalizes "slow feature" learning and we present much stronger empirical evidence than prior works, showing convincing gains for the challenging semantic segmentation task over a variety of architecture designs and on two popular datasets. TAF can be interpreted as an implicit label augmentation method but is a more principled formulation compared to existing explicit augmentation techniques. Our work thus connects two promising methods that utilize partially annotated clips for single frame model training and can inspire future explorations in this direction.



### Light-Weight RetinaNet for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1905.10011v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10011v1)
- **Published**: 2019-05-24 02:56:28+00:00
- **Updated**: 2019-05-24 02:56:28+00:00
- **Authors**: Yixing Li, Fengbo Ren
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection has gained great progress driven by the development of deep learning. Compared with a widely studied task -- classification, generally speaking, object detection even need one or two orders of magnitude more FLOPs (floating point operations) in processing the inference task. To enable a practical application, it is essential to explore effective runtime and accuracy trade-off scheme. Recently, a growing number of studies are intended for object detection on resource constraint devices, such as YOLOv1, YOLOv2, SSD, MobileNetv2-SSDLite, whose accuracy on COCO test-dev detection results are yield to mAP around 22-25% (mAP-20-tier). On the contrary, very few studies discuss the computation and accuracy trade-off scheme for mAP-30-tier detection networks. In this paper, we illustrate the insights of why RetinaNet gives effective computation and accuracy trade-off for object detection and how to build a light-weight RetinaNet. We propose to only reduce FLOPs in computational intensive layers and keep other layer the same. Compared with most common way -- input image scaling for FLOPs-accuracy trade-off, the proposed solution shows a constantly better FLOPs-mAP trade-off line. Quantitatively, the proposed method result in 0.1% mAP improvement at 1.15x FLOPs reduction and 0.3% mAP improvement at 1.8x FLOPs reduction.



### Brain-mediated Transfer Learning of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.10037v3
- **DOI**: 10.1609/aaai.v34i04.5974
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1905.10037v3)
- **Published**: 2019-05-24 05:15:17+00:00
- **Updated**: 2019-10-09 02:20:53+00:00
- **Authors**: Satoshi Nishida, Yusuke Nakano, Antoine Blanc, Naoya Maeda, Masataka Kado, Shinji Nishimoto
- **Comment**: None
- **Journal**: Proc. Thirty-Fourth AAAI Conf. Artif. Intell. (2020) 5281-5288
- **Summary**: The human brain can effectively learn a new task from a small number of samples, which indicate that the brain can transfer its prior knowledge to solve tasks in different domains. This function is analogous to transfer learning (TL) in the field of machine learning. TL uses a well-trained feature space in a specific task domain to improve performance in new tasks with insufficient training data. TL with rich feature representations, such as features of convolutional neural networks (CNNs), shows high generalization ability across different task domains. However, such TL is still insufficient in making machine learning attain generalization ability comparable to that of the human brain. To examine if the internal representation of the brain could be used to achieve more efficient TL, we introduce a method for TL mediated by human brains. Our method transforms feature representations of audiovisual inputs in CNNs into those in activation patterns of individual brains via their association learned ahead using measured brain responses. Then, to estimate labels reflecting human cognition and behavior induced by the audiovisual inputs, the transformed representations are used for TL. We demonstrate that our brain-mediated TL (BTL) shows higher performance in the label estimation than the standard TL. In addition, we illustrate that the estimations mediated by different brains vary from brain to brain, and the variability reflects the individual variability in perception. Thus, our BTL provides a framework to improve the generalization ability of machine-learning feature representations and enable machine learning to estimate human-like cognition and behavior, including individual variability.



### Beyond Intra-modality: A Survey of Heterogeneous Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1905.10048v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10048v4)
- **Published**: 2019-05-24 06:27:32+00:00
- **Updated**: 2020-04-27 15:35:46+00:00
- **Authors**: Zheng Wang, Zhixiang Wang, Yinqiang Zheng, Yang Wu, Wenjun Zeng, Shin'ichi Satoh
- **Comment**: Accepted by IJCAI 2020. Project url:
  https://github.com/lightChaserX/Awesome-Hetero-reID
- **Journal**: IJCAI 2020
- **Summary**: An efficient and effective person re-identification (ReID) system relieves the users from painful and boring video watching and accelerates the process of video analysis. Recently, with the explosive demands of practical applications, a lot of research efforts have been dedicated to heterogeneous person re-identification (Hetero-ReID). In this paper, we provide a comprehensive review of state-of-the-art Hetero-ReID methods that address the challenge of inter-modality discrepancies. According to the application scenario, we classify the methods into four categories -- low-resolution, infrared, sketch, and text. We begin with an introduction of ReID, and make a comparison between Homogeneous ReID (Homo-ReID) and Hetero-ReID tasks. Then, we describe and compare existing datasets for performing evaluations, and survey the models that have been widely employed in Hetero-ReID. We also summarize and compare the representative approaches from two perspectives, i.e., the application scenario and the learning pipeline. We conclude by a discussion of some future research directions. Follow-up updates are avaible at: https://github.com/lightChaserX/Awesome-Hetero-reID



### A Compressive Sensing Video dataset using Pixel-wise coded exposure
- **Arxiv ID**: http://arxiv.org/abs/1905.10054v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10054v2)
- **Published**: 2019-05-24 06:39:12+00:00
- **Updated**: 2019-07-09 02:59:37+00:00
- **Authors**: Sathyaprakash Narayanan, Yeshwanth Bethi, Chetan Singh Thakur
- **Comment**: None
- **Journal**: None
- **Summary**: Manifold amount of video data gets generated every minute as we read this document, ranging from surveillance to broadcasting purposes. There are two roadblocks that restrain us from using this data as such, first being the storage which restricts us from only storing the information based on the hardware constraints. Secondly, the computation required to process this data is highly expensive which makes it infeasible to work on them. Compressive sensing(CS)[2] is a signal process technique[11], through optimization, the sparsity of a signal can be exploited to recover it from far fewer samples than required by the Shannon-Nyquist sampling theorem. There are two conditions under which recovery is possible. The first one is sparsity which requires the signal to be sparse in some domain. The second one is incoherence which is applied through the isometric property which is sufficient for sparse signals[9][10]. To sustain these characteristics, preserving all attributes in the uncompressed domain would help any kind of in this field. However, existing dataset fallback in terms of continuous tracking of all the object present in the scene, very few video datasets have comprehensive continuous tracking of objects. To address these problems collectively, in this work we propose a new comprehensive video dataset, where the data is compressed using pixel-wise coded exposure [3] that resolves various other impediments.



### Pose-adaptive Hierarchical Attention Network for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1905.10059v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10059v1)
- **Published**: 2019-05-24 06:54:14+00:00
- **Updated**: 2019-05-24 06:54:14+00:00
- **Authors**: Yuanyuan Liu, Jiyao Peng, Jiabei Zeng, Shiguang Shan
- **Comment**: 12 pages, 15 figures
- **Journal**: None
- **Summary**: Multi-view facial expression recognition (FER) is a challenging task because the appearance of an expression varies in poses. To alleviate the influences of poses, recent methods either perform pose normalization or learn separate FER classifiers for each pose. However, these methods usually have two stages and rely on good performance of pose estimators. Different from existing methods, we propose a pose-adaptive hierarchical attention network (PhaNet) that can jointly recognize the facial expressions and poses in unconstrained environment. Specifically, PhaNet discovers the most relevant regions to the facial expression by an attention mechanism in hierarchical scales, and the most informative scales are then selected to learn the pose-invariant and expression-discriminative representations. PhaNet is end-to-end trainable by minimizing the hierarchical attention losses, the FER loss and pose loss with dynamically learned loss weights. We validate the effectiveness of the proposed PhaNet on three multi-view datasets (BU-3DFE, Multi-pie, and KDEF) and two in-the-wild FER datasets (AffectNet and SFEW). Extensive experiments demonstrate that our framework outperforms the state-of-the-arts under both within-dataset and cross-dataset settings, achieving the average accuracies of 84.92\%, 93.53\%, 88.5\%, 54.82\% and 31.25\% respectively.



### OVSNet : Towards One-Pass Real-Time Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.10064v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10064v2)
- **Published**: 2019-05-24 07:12:48+00:00
- **Updated**: 2019-07-02 11:37:33+00:00
- **Authors**: Peng Sun, Peiwen Lin, Guangliang Cheng, Jianping Shi, Jiawan Zhang, Xi Li
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Video object segmentation aims at accurately segmenting the target object regions across consecutive frames. It is technically challenging for coping with complicated factors (e.g., shape deformations, occlusion and out of the lens). Recent approaches have largely solved them by using backforth re-identification and bi-directional mask propagation. However, their methods are extremely slow and only support offline inference, which in principle cannot be applied in real time. Motivated by this observation, we propose a efficient detection-based paradigm for video object segmentation. We propose an unified One-Pass Video Segmentation framework (OVS-Net) for modeling spatial-temporal representation in a unified pipeline, which seamlessly integrates object detection, object segmentation, and object re-identification. The proposed framework lends itself to one-pass inference that effectively and efficiently performs video object segmentation. Moreover, we propose a maskguided attention module for modeling the multi-scale object boundary and multi-level feature fusion. Experiments on the challenging DAVIS 2017 demonstrate the effectiveness of the proposed framework with comparable performance to the state-of-the-art, and the great efficiency about 11.5 FPS towards pioneering real-time work to our knowledge, more than 5 times faster than other state-of-the-art methods.



### Flow-based Intrinsic Curiosity Module
- **Arxiv ID**: http://arxiv.org/abs/1905.10071v3
- **DOI**: 10.24963/ijcai.2020/286
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.10071v3)
- **Published**: 2019-05-24 07:32:30+00:00
- **Updated**: 2020-07-15 04:39:13+00:00
- **Authors**: Hsuan-Kung Yang, Po-Han Chiang, Min-Fong Hong, Chun-Yi Lee
- **Comment**: The SOLE copyright holder is IJCAI (International Joint Conferences
  on Artificial Intelligence), all rights reserved. The link is provided as
  follows: https://www.ijcai.org/Proceedings/2020/286
- **Journal**: Proceedings of the Twenty-Ninth International Joint Conference on
  Artificial Intelligence Main track. Pages 2065-2072
- **Summary**: In this paper, we focus on a prediction-based novelty estimation strategy upon the deep reinforcement learning (DRL) framework, and present a flow-based intrinsic curiosity module (FICM) to exploit the prediction errors from optical flow estimation as exploration bonuses. We propose the concept of leveraging motion features captured between consecutive observations to evaluate the novelty of observations in an environment. FICM encourages a DRL agent to explore observations with unfamiliar motion features, and requires only two consecutive frames to obtain sufficient information when estimating the novelty. We evaluate our method and compare it with a number of existing methods on multiple benchmark environments, including Atari games, Super Mario Bros., and ViZDoom. We demonstrate that FICM is favorable to tasks or environments featuring moving objects, which allow FICM to utilize the motion features between consecutive observations. We further ablatively analyze the encoding efficiency of FICM, and discuss its applicable domains comprehensively.



### PCC Net: Perspective Crowd Counting via Spatial Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/1905.10085v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.10085v1)
- **Published**: 2019-05-24 08:23:13+00:00
- **Updated**: 2019-05-24 08:23:13+00:00
- **Authors**: Junyu Gao, Qi Wang, Xuelong Li
- **Comment**: accepted by IEEE T-CSVT
- **Journal**: None
- **Summary**: Crowd counting from a single image is a challenging task due to high appearance similarity, perspective changes and severe congestion. Many methods only focus on the local appearance features and they cannot handle the aforementioned challenges. In order to tackle them, we propose a Perspective Crowd Counting Network (PCC Net), which consists of three parts: 1) Density Map Estimation (DME) focuses on learning very local features for density map estimation; 2) Random High-level Density Classification (R-HDC) extracts global features to predict the coarse density labels of random patches in images; 3) Fore-/Background Segmentation (FBS) encodes mid-level features to segments the foreground and background. Besides, the DULR module is embedded in PCC Net to encode the perspective changes on four directions (Down, Up, Left and Right). The proposed PCC Net is verified on five mainstream datasets, which achieves the state-of-the-art performance on the one and attains the competitive results on the other four datasets. The source code is available at https://github.com/gjy3035/PCC-Net.



### ACNet: Attention Based Network to Exploit Complementary Features for RGBD Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.10089v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10089v1)
- **Published**: 2019-05-24 08:44:41+00:00
- **Updated**: 2019-05-24 08:44:41+00:00
- **Authors**: Xinxin Hu, Kailun Yang, Lei Fei, Kaiwei Wang
- **Comment**: Accepted to be published in 2019 IEEE International Conference on
  Image Processing (ICIP 2019), Sep 22-25, 2019, Taipei. IEEE Copyright notice
  added, 5 pages, 5 figures
- **Journal**: None
- **Summary**: Compared to RGB semantic segmentation, RGBD semantic segmentation can achieve better performance by taking depth information into consideration. However, it is still problematic for contemporary segmenters to effectively exploit RGBD information since the feature distributions of RGB and depth (D) images vary significantly in different scenes. In this paper, we propose an Attention Complementary Network (ACNet) that selectively gathers features from RGB and depth branches. The main contributions lie in the Attention Complementary Module (ACM) and the architecture with three parallel branches. More precisely, ACM is a channel attention-based module that extracts weighted features from RGB and depth branches. The architecture preserves the inference of the original RGB and depth branches, and enables the fusion branch at the same time. Based on the above structures, ACNet is capable of exploiting more high-quality features from different channels. We evaluate our model on SUN-RGBD and NYUDv2 datasets, and prove that our model outperforms state-of-the-art methods. In particular, a mIoU score of 48.3\% on NYUDv2 test set is achieved with ResNet50. We will release our source code based on PyTorch and the trained segmentation model at https://github.com/anheidelonghu/ACNet.



### Multi-Scale Dual-Branch Fully Convolutional Network for Hand Parsing
- **Arxiv ID**: http://arxiv.org/abs/1905.10100v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10100v1)
- **Published**: 2019-05-24 09:09:37+00:00
- **Updated**: 2019-05-24 09:09:37+00:00
- **Authors**: Yang Lu, Xiaohui Liang, Frederick W. B. Li
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, fully convolutional neural networks (FCNs) have shown significant performance in image parsing, including scene parsing and object parsing. Different from generic object parsing tasks, hand parsing is more challenging due to small size, complex structure, heavy self-occlusion and ambiguous texture problems. In this paper, we propose a novel parsing framework, Multi-Scale Dual-Branch Fully Convolutional Network (MSDB-FCN), for hand parsing tasks. Our network employs a Dual-Branch architecture to extract features of hand area, paying attention on the hand itself. These features are used to generate multi-scale features with pyramid pooling strategy. In order to better encode multi-scale features, we design a Deconvolution and Bilinear Interpolation Block (DB-Block) for upsampling and merging the features of different scales. To address data imbalance, which is a common problem in many computer vision tasks as well as hand parsing tasks, we propose a generalization of Focal Loss, namely Multi-Class Balanced Focal Loss, to tackle data imbalance in multi-class classification. Extensive experiments on RHD-PARSING dataset demonstrate that our MSDB-FCN has achieved the state-of-the-art performance for hand parsing.



### Guided Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/1905.10107v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.10107v1)
- **Published**: 2019-05-24 09:32:34+00:00
- **Updated**: 2019-05-24 09:32:34+00:00
- **Authors**: Matteo Poggi, Davide Pallotti, Fabio Tosi, Stefano Mattoccia
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Stereo is a prominent technique to infer dense depth maps from images, and deep learning further pushed forward the state-of-the-art, making end-to-end architectures unrivaled when enough data is available for training. However, deep networks suffer from significant drops in accuracy when dealing with new environments. Therefore, in this paper, we introduce Guided Stereo Matching, a novel paradigm leveraging a small amount of sparse, yet reliable depth measurements retrieved from an external source enabling to ameliorate this weakness. The additional sparse cues required by our method can be obtained with any strategy (e.g., a LiDAR) and used to enhance features linked to corresponding disparity hypotheses. Our formulation is general and fully differentiable, thus enabling to exploit the additional sparse inputs in pre-trained deep stereo networks as well as for training a new instance from scratch. Extensive experiments on three standard datasets and two state-of-the-art deep architectures show that even with a small set of sparse input cues, i) the proposed paradigm enables significant improvements to pre-trained networks. Moreover, ii) training from scratch notably increases accuracy and robustness to domain shifts. Finally, iii) it is suited and effective even with traditional stereo algorithms such as SGM.



### Continual Reinforcement Learning in 3D Non-stationary Environments
- **Arxiv ID**: http://arxiv.org/abs/1905.10112v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.10112v2)
- **Published**: 2019-05-24 09:38:42+00:00
- **Updated**: 2020-04-21 14:57:48+00:00
- **Authors**: Vincenzo Lomonaco, Karan Desai, Eugenio Culurciello, Davide Maltoni
- **Comment**: Accepted in the CLVision Workshop at CVPR2020: 13 pages, 4 figures, 5
  tables
- **Journal**: None
- **Summary**: High-dimensional always-changing environments constitute a hard challenge for current reinforcement learning techniques. Artificial agents, nowadays, are often trained off-line in very static and controlled conditions in simulation such that training observations can be thought as sampled i.i.d. from the entire observations space. However, in real world settings, the environment is often non-stationary and subject to unpredictable, frequent changes. In this paper we propose and openly release CRLMaze, a new benchmark for learning continually through reinforcement in a complex 3D non-stationary task based on ViZDoom and subject to several environmental changes. Then, we introduce an end-to-end model-free continual reinforcement learning strategy showing competitive results with respect to four different baselines and not requiring any access to additional supervised signals, previously encountered environmental conditions or observations.



### Robust Semantic Segmentation in Adverse Weather Conditions by means of Sensor Data Fusion
- **Arxiv ID**: http://arxiv.org/abs/1905.10117v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1905.10117v1)
- **Published**: 2019-05-24 09:55:57+00:00
- **Updated**: 2019-05-24 09:55:57+00:00
- **Authors**: Andreas Pfeuffer, Klaus Dietmayer
- **Comment**: None
- **Journal**: 22st International Conference on Information Fusion (FUSION)
  (2019)
- **Summary**: A robust and reliable semantic segmentation in adverse weather conditions is very important for autonomous cars, but most state-of-the-art approaches only achieve high accuracy rates in optimal weather conditions. The reason is that they are only optimized for good weather conditions and given noise models. However, most of them fail, if data with unknown disturbances occur, and their performance decrease enormously. One possibility to still obtain reliable results is to observe the environment with different sensor types, such as camera and lidar, and to fuse the sensor data by means of neural networks, since different sensors behave differently in diverse weather conditions. Hence, the sensors can complement each other by means of an appropriate sensor data fusion. Nevertheless, the fusion-based approaches are still susceptible to disturbances and fail to classify disturbed image areas correctly. This problem can be solved by means of a special training method, the so called Robust Learning Method (RLM), a method by which the neural network learns to handle unknown noise. In this work, two different sensor fusion architectures for semantic segmentation are compared and evaluated on several datasets. Furthermore, it is shown that the RLM increases the robustness in adverse weather conditions enormously, and achieve good results although no disturbance model has been learned by the neural network.



### HUMBO: Bridging Response Generation and Facial Expression Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1905.11240v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1905.11240v3)
- **Published**: 2019-05-24 10:22:16+00:00
- **Updated**: 2021-08-31 13:41:25+00:00
- **Authors**: Shang-Yu Su, Po-Wei Lin, Yun-Nung Chen
- **Comment**: The first two authors contributed to this work equally
- **Journal**: None
- **Summary**: Spoken dialogue systems that assist users to solve complex tasks such as movie ticket booking have become an emerging research topic in artificial intelligence and natural language processing areas. With a well-designed dialogue system as an intelligent personal assistant, people can accomplish certain tasks more easily via natural language interactions. Today there are several virtual intelligent assistants in the market; however, most systems only focus on textual or vocal interaction. In this paper, we present HUMBO, a system aiming at generating dialogue responses and simultaneously synthesize corresponding visual expressions on faces for better multimodal interaction. HUMBO can (1) let users determine the appearances of virtual assistants by a single image, and (2) generate coherent emotional utterances and facial expressions on the user-provided image. This is not only a brand new research direction but more importantly, an ultimate step toward more human-like virtual assistants.



### Saliency detection based on structural dissimilarity induced by image quality assessment model
- **Arxiv ID**: http://arxiv.org/abs/1905.10150v1
- **DOI**: 10.1117/1.JEI.28.2.023025
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1905.10150v1)
- **Published**: 2019-05-24 11:13:14+00:00
- **Updated**: 2019-05-24 11:13:14+00:00
- **Authors**: Yang Li, Xuanqin Mou
- **Comment**: For associated source code, see https://github.com/yangli-xjtu/SDS
- **Journal**: J. Electron. Imag. 28(2) 023025 (3 April 2019)
- **Summary**: The distinctiveness of image regions is widely used as the cue of saliency. Generally, the distinctiveness is computed according to the absolute difference of features. However, according to the image quality assessment (IQA) studies, the human visual system is highly sensitive to structural changes rather than absolute difference. Accordingly, we propose the computation of the structural dissimilarity between image patches as the distinctiveness measure for saliency detection. Similar to IQA models, the structural dissimilarity is computed based on the correlation of the structural features. The global structural dissimilarity of a patch to all the other patches represents saliency of the patch. We adopt two widely used structural features, namely the local contrast and gradient magnitude, into the structural dissimilarity computation in the proposed model. Without any postprocessing, the proposed model based on the correlation of either of the two structural features outperforms 11 state-of-the-art saliency models on three saliency databases.



### Optimizing Shallow Networks for Binary Classification
- **Arxiv ID**: http://arxiv.org/abs/1905.10161v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.10161v2)
- **Published**: 2019-05-24 11:40:24+00:00
- **Updated**: 2019-06-24 00:42:01+00:00
- **Authors**: Kalliopi Basioti, George V. Moustakides
- **Comment**: None
- **Journal**: None
- **Summary**: Data driven classification that relies on neural networks is based on optimization criteria that involve some form of distance between the output of the network and the desired label. Using the same mathematical analysis, for a multitude of such measures, we can show that their optimum solution matches the ideal likelihood ratio test classifier. In this work we introduce a different family of optimization problems which is not covered by the existing approaches and, therefore, opens possibilities for new training algorithms for neural network based classification. We give examples that lead to algorithms that are simple in implementation, exhibit stable convergence characteristics and are antagonistic to the most popular existing techniques.



### Fast Flow Reconstruction via Robust Invertible nxn Convolution
- **Arxiv ID**: http://arxiv.org/abs/1905.10170v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10170v3)
- **Published**: 2019-05-24 11:58:05+00:00
- **Updated**: 2022-08-07 03:32:35+00:00
- **Authors**: Thanh-Dat Truong, Khoa Luu, Chi Nhan Duong, Ngan Le, Minh-Triet Tran
- **Comment**: None
- **Journal**: None
- **Summary**: Flow-based generative models have recently become one of the most efficient approaches to model data generation. Indeed, they are constructed with a sequence of invertible and tractable transformations. Glow first introduced a simple type of generative flow using an invertible $1 \times 1$ convolution. However, the $1 \times 1$ convolution suffers from limited flexibility compared to the standard convolutions. In this paper, we propose a novel invertible $n \times n$ convolution approach that overcomes the limitations of the invertible $1 \times 1$ convolution. In addition, our proposed network is not only tractable and invertible but also uses fewer parameters than standard convolutions. The experiments on CIFAR-10, ImageNet and Celeb-HQ datasets, have shown that our invertible $n \times n$ convolution helps to improve the performance of generative models significantly.



### Functional Segmentation through Dynamic Mode Decomposition: Automatic Quantification of Kidney Function in DCE-MRI Images
- **Arxiv ID**: http://arxiv.org/abs/1905.10218v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.10218v1)
- **Published**: 2019-05-24 13:12:23+00:00
- **Updated**: 2019-05-24 13:12:23+00:00
- **Authors**: Santosh Tirunagari, Norman Poh, Kevin Wells, Miroslaw Bober, Isky Gorden, David Windridge
- **Comment**: None
- **Journal**: None
- **Summary**: Quantification of kidney function in Dynamic Contrast-Enhanced Magnetic Resonance Imaging (DCE-MRI) requires careful segmentation of the renal region of interest (ROI). Traditionally, human experts are required to manually delineate the kidney ROI across multiple images in the dynamic sequence. This approach is costly, time-consuming and labour intensive, and therefore acts to limit patient throughout and acts as one of the factors limiting the wider adoption of DCR-MRI in clinical practice. Therefore, to address this issue, we present the first use of Dynamic Mode Decomposition (DMD) as a basis for automatic segmentation of a dynamic sequence, in this case, kidney ROIs in DCE-MRI. Using DMD coupled combined with thresholding and connected component analysis is first validated on synthetically generated data with known ground-truth, and then applied to ten healthy volunteers' DCE-MRI datasets. We find that the segmentation result obtained from our proposed DMD framework is comparable to that of expert observers and very significantly better than that of an a-priori bounding box segmentation. Our result gives a mean Jaccard coefficient of 0.87, compared to mean scores of 0.85, 0.88 and 0.87 produced from three independent manual annotations. This represents the first use of DMD as a robust automatic data-driven segmentation approach without requiring any human intervention. This is a viable, efficient alternative approach to current manual methods of isolation of kidney function in DCE-MRI.



### Deep Reason: A Strong Baseline for Real-World Visual Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1905.10226v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10226v2)
- **Published**: 2019-05-24 13:34:21+00:00
- **Updated**: 2019-06-17 15:26:58+00:00
- **Authors**: Chenfei Wu, Yanzhao Zhou, Gen Li, Nan Duan, Duyu Tang, Xiaojie Wang
- **Comment**: CVPR 2019 Visual Question Answering and Dialog Workshop
- **Journal**: None
- **Summary**: This paper presents a strong baseline for real-world visual reasoning (GQA), which achieves 60.93% in GQA 2019 challenge and won the sixth place. GQA is a large dataset with 22M questions involving spatial understanding and multi-step inference. To help further research in this area, we identified three crucial parts that improve the performance, namely: multi-source features, fine-grained encoder, and score-weighted ensemble. We provide a series of analysis on their impact on performance.



### A Comparison and Strategy of Semantic Segmentation on Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/1905.10231v1
- **DOI**: 10.1007/978-3-030-32456-8_3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10231v1)
- **Published**: 2019-05-24 13:39:12+00:00
- **Updated**: 2019-05-24 13:39:12+00:00
- **Authors**: Junxing Hu, Ling Li, Yijun Lin, Fengge Wu, Junsuo Zhao
- **Comment**: 8 pages, 3 figures, ICNC-FSKD 2019
- **Journal**: None
- **Summary**: In recent years, with the development of aerospace technology, we use more and more images captured by satellites to obtain information. But a large number of useless raw images, limited data storage resource and poor transmission capability on satellites hinder our use of valuable images. Therefore, it is necessary to deploy an on-orbit semantic segmentation model to filter out useless images before data transmission. In this paper, we present a detailed comparison on the recent deep learning models. Considering the computing environment of satellites, we compare methods from accuracy, parameters and resource consumption on the same public dataset. And we also analyze the relation between them. Based on experimental results, we further propose a viable on-orbit semantic segmentation strategy. It will be deployed on the TianZhi-2 satellite which supports deep learning methods and will be lunched soon.



### A Research and Strategy of Remote Sensing Image Denoising Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1905.10236v1
- **DOI**: 10.1007/978-3-030-32591-6_75
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.10236v1)
- **Published**: 2019-05-24 13:47:19+00:00
- **Updated**: 2019-05-24 13:47:19+00:00
- **Authors**: Ling Li, Junxing Hu, Fengge Wu, Junsuo Zhao
- **Comment**: 9 pages, 4 figures, ICNC-FSKD 2019
- **Journal**: None
- **Summary**: Most raw data download from satellites are useless, resulting in transmission waste, one solution is to process data directly on satellites, then only transmit the processed results to the ground. Image processing is the main data processing on satellites, in this paper, we focus on image denoising which is the basic image processing. There are many high-performance denoising approaches at present, however, most of them rely on advanced computing resources or rich images on the ground. Considering the limited computing resources of satellites and the characteristics of remote sensing images, we do some research on these high-performance ground image denoising approaches and compare them in simulation experiments to analyze whether they are suitable for satellites. According to the analysis results, we propose two feasible image denoising strategies for satellites based on satellite TianZhi-1.



### From Here to There: Video Inbetweening Using Direct 3D Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1905.10240v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.10240v3)
- **Published**: 2019-05-24 14:01:08+00:00
- **Updated**: 2019-06-04 07:54:06+00:00
- **Authors**: Yunpeng Li, Dominik Roblek, Marco Tagliasacchi
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of generating plausible and diverse video sequences, when we are only given a start and an end frame. This task is also known as inbetweening, and it belongs to the broader area of stochastic video generation, which is generally approached by means of recurrent neural networks (RNN). In this paper, we propose instead a fully convolutional model to generate video sequences directly in the pixel domain. We first obtain a latent video representation using a stochastic fusion mechanism that learns how to incorporate information from the start and end frames. Our model learns to produce such latent representation by progressively increasing the temporal resolution, and then decode in the spatiotemporal domain using 3D convolutions. The model is trained end-to-end by minimizing an adversarial loss. Experiments on several widely-used benchmark datasets show that it is able to generate meaningful and diverse in-between video sequences, according to both quantitative and qualitative evaluations.



### Rank3DGAN: Semantic mesh generation using relative attributes
- **Arxiv ID**: http://arxiv.org/abs/1905.10257v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10257v2)
- **Published**: 2019-05-24 14:30:58+00:00
- **Updated**: 2019-05-28 14:06:24+00:00
- **Authors**: Yassir Saquil, Qun-Ce Xu, Yong-Liang Yang, Peter Hall
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we investigate a novel problem of using generative adversarial networks in the task of 3D shape generation according to semantic attributes. Recent works map 3D shapes into 2D parameter domain, which enables training Generative Adversarial Networks (GANs) for 3D shape generation task. We extend these architectures to the conditional setting, where we generate 3D shapes with respect to subjective attributes defined by the user. Given pairwise comparisons of 3D shapes, our model performs two tasks: it learns a generative model with a controlled latent space, and a ranking function for the 3D shapes based on their multi-chart representation in 2D. The capability of the model is demonstrated with experiments on HumanShape, Basel Face Model and reconstructed 3D CUB datasets. We also present various applications that benefit from our model, such as multi-attribute exploration, mesh editing, and mesh attribute transfer.



### Semi-supervised GAN for Classification of Multispectral Imagery Acquired by UAVs
- **Arxiv ID**: http://arxiv.org/abs/1905.10920v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.10920v1)
- **Published**: 2019-05-24 14:43:21+00:00
- **Updated**: 2019-05-24 14:43:21+00:00
- **Authors**: Hamideh Kerdegari, Manzoor Razaak, Vasileios Argyriou, Paolo Remagnino
- **Comment**: None
- **Journal**: None
- **Summary**: Unmanned aerial vehicles (UAV) are used in precision agriculture (PA) to enable aerial monitoring of farmlands. Intelligent methods are required to pinpoint weed infestations and make optimal choice of pesticide. UAV can fly a multispectral camera and collect data. However, the classification of multispectral images using supervised machine learning algorithms such as convolutional neural networks (CNN) requires large amount of training data. This is a common drawback in deep learning we try to circumvent making use of a semi-supervised generative adversarial networks (GAN), providing a pixel-wise classification for all the acquired multispectral images. Our algorithm consists of a generator network that provides photo-realistic images as extra training data to a multi-class classifier, acting as a discriminator and trained on small amounts of labeled data. The performance of the proposed method is evaluated on the weedNet dataset consisting of multispectral crop and weed images collected by a micro aerial vehicle (MAV). The results by the proposed semi-supervised GAN achieves high classification accuracy and demonstrates the potential of GAN-based methods for the challenging task of multispectral image classification.



### DEMEA: Deep Mesh Autoencoders for Non-Rigidly Deforming Objects
- **Arxiv ID**: http://arxiv.org/abs/1905.10290v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1905.10290v2)
- **Published**: 2019-05-24 15:35:37+00:00
- **Updated**: 2020-08-04 15:35:07+00:00
- **Authors**: Edgar Tretschk, Ayush Tewari, Michael Zollhöfer, Vladislav Golyanik, Christian Theobalt
- **Comment**: 27 pages, including supplementary material
- **Journal**: None
- **Summary**: Mesh autoencoders are commonly used for dimensionality reduction, sampling and mesh modeling. We propose a general-purpose DEep MEsh Autoencoder (DEMEA) which adds a novel embedded deformation layer to a graph-convolutional mesh autoencoder. The embedded deformation layer (EDL) is a differentiable deformable geometric proxy which explicitly models point displacements of non-rigid deformations in a lower dimensional space and serves as a local rigidity regularizer. DEMEA decouples the parameterization of the deformation from the final mesh resolution since the deformation is defined over a lower dimensional embedded deformation graph. We perform a large-scale study on four different datasets of deformable objects. Reasoning about the local rigidity of meshes using EDL allows us to achieve higher-quality results for highly deformable objects, compared to directly regressing vertex positions. We demonstrate multiple applications of DEMEA, including non-rigid 3D reconstruction from depth and shading cues, non-rigid surface tracking, as well as the transfer of deformations over different meshes.



### Uncertainty Estimation in One-Stage Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1905.10296v2
- **DOI**: 10.1109/ITSC.2019.8917494
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.10296v2)
- **Published**: 2019-05-24 15:51:59+00:00
- **Updated**: 2020-07-10 11:13:58+00:00
- **Authors**: Florian Kraus, Klaus Dietmayer
- **Comment**: None
- **Journal**: IEEE Intelligent Transportation Systems Conference (ITSC), 2019,
  pp. 53-60
- **Summary**: Environment perception is the task for intelligent vehicles on which all subsequent steps rely. A key part of perception is to safely detect other road users such as vehicles, pedestrians, and cyclists. With modern deep learning techniques huge progress was made over the last years in this field. However such deep learning based object detection models cannot predict how certain they are in their predictions, potentially hampering the performance of later steps such as tracking or sensor fusion. We present a viable approaches to estimate uncertainty in an one-stage object detector, while improving the detection performance of the baseline approach. The proposed model is evaluated on a large scale automotive pedestrian dataset. Experimental results show that the uncertainty outputted by our system is coupled with detection accuracy and the occlusion level of pedestrians.



### SCRAM: Spatially Coherent Randomized Attention Maps
- **Arxiv ID**: http://arxiv.org/abs/1905.10308v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.10308v1)
- **Published**: 2019-05-24 16:03:44+00:00
- **Updated**: 2019-05-24 16:03:44+00:00
- **Authors**: Dan A. Calian, Peter Roelants, Jacques Cali, Ben Carr, Krishna Dubba, John E. Reid, Dell Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Attention mechanisms and non-local mean operations in general are key ingredients in many state-of-the-art deep learning techniques. In particular, the Transformer model based on multi-head self-attention has recently achieved great success in natural language processing and computer vision. However, the vanilla algorithm computing the Transformer of an image with n pixels has O(n^2) complexity, which is often painfully slow and sometimes prohibitively expensive for large-scale image data. In this paper, we propose a fast randomized algorithm --- SCRAM --- that only requires O(n log(n)) time to produce an image attention map. Such a dramatic acceleration is attributed to our insight that attention maps on real-world images usually exhibit (1) spatial coherence and (2) sparse structure. The central idea of SCRAM is to employ PatchMatch, a randomized correspondence algorithm, to quickly pinpoint the most compatible key (argmax) for each query first, and then exploit that knowledge to design a sparse approximation to non-local mean operations. Using the argmax (mode) to dynamically construct the sparse approximation distinguishes our algorithm from all of the existing sparse approximate methods and makes it very efficient. Moreover, SCRAM is a broadly applicable approximation to any non-local mean layer in contrast to some other sparse approximations that can only approximate self-attention. Our preliminary experimental results suggest that SCRAM is indeed promising for speeding up or scaling up the computation of attention maps in the Transformer.



### Mask-Guided Portrait Editing with Conditional GANs
- **Arxiv ID**: http://arxiv.org/abs/1905.10346v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10346v1)
- **Published**: 2019-05-24 17:27:42+00:00
- **Updated**: 2019-05-24 17:27:42+00:00
- **Authors**: Shuyang Gu, Jianmin Bao, Hao Yang, Dong Chen, Fang Wen, Lu Yuan
- **Comment**: To appear in CVPR2019
- **Journal**: None
- **Summary**: Portrait editing is a popular subject in photo manipulation. The Generative Adversarial Network (GAN) advances the generating of realistic faces and allows more face editing. In this paper, we argue about three issues in existing techniques: diversity, quality, and controllability for portrait synthesis and editing. To address these issues, we propose a novel end-to-end learning framework that leverages conditional GANs guided by provided face masks for generating faces. The framework learns feature embeddings for every face component (e.g., mouth, hair, eye), separately, contributing to better correspondences for image translation, and local face editing. With the mask, our network is available to many applications, like face synthesis driven by mask, face Swap+ (including hair in swapping), and local manipulation. It can also boost the performance of face parsing a bit as an option of data augmentation.



### Deep Trajectory for Recognition of Human Behaviours
- **Arxiv ID**: http://arxiv.org/abs/1905.10357v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10357v1)
- **Published**: 2019-05-24 17:42:37+00:00
- **Updated**: 2019-05-24 17:42:37+00:00
- **Authors**: Tauseef Ali, Eissa Jaber Alreshidi
- **Comment**: None
- **Journal**: None
- **Summary**: Identifying human actions in complex scenes is widely considered as a challenging research problem due to the unpredictable behaviors and variation of appearances and postures. For extracting variations in motion and postures, trajectories provide meaningful way. However, simple trajectories are normally represented by vector of spatial coordinates. In order to identify human actions, we must exploit structural relationship between different trajectories. In this paper, we propose a method that divides the video into N number of segments and then for each segment we extract trajectories. We then compute trajectory descriptor for each segment which capture the structural relationship among different trajectories in the video segment. For trajectory descriptor, we project all extracted trajectories on the canvas. This will result in texture image which can store the relative motion and structural relationship among the trajectories. We then train Convolution Neural Network (CNN) to capture and learn the representation from dense trajectories. . Experimental results shows that our proposed method out performs state of the art methods by 90.01% on benchmark data set.



### Geometric Wavelet Scattering Networks on Compact Riemannian Manifolds
- **Arxiv ID**: http://arxiv.org/abs/1905.10448v4
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, math.FA
- **Links**: [PDF](http://arxiv.org/pdf/1905.10448v4)
- **Published**: 2019-05-24 21:19:04+00:00
- **Updated**: 2023-07-25 17:53:01+00:00
- **Authors**: Michael Perlmutter, Feng Gao, Guy Wolf, Matthew Hirn
- **Comment**: 35 pages; 3 figures; 2 tables; v4: Fixed a minor error. Convergence
  in Equation 13 is in L2 not p.w. modified proof of Theorem 3.3 accordingly
- **Journal**: Proceedings of The First Mathematical and Scientific Machine
  Learning Conference, PMLR 107:570-604, 2020
- **Summary**: The Euclidean scattering transform was introduced nearly a decade ago to improve the mathematical understanding of convolutional neural networks. Inspired by recent interest in geometric deep learning, which aims to generalize convolutional neural networks to manifold and graph-structured domains, we define a geometric scattering transform on manifolds. Similar to the Euclidean scattering transform, the geometric scattering transform is based on a cascade of wavelet filters and pointwise nonlinearities. It is invariant to local isometries and stable to certain types of diffeomorphisms. Empirical results demonstrate its utility on several geometric learning tasks. Our results generalize the deformation stability and local translation invariance of Euclidean scattering, and demonstrate the importance of linking the used filter structures to the underlying geometry of the data.



### Additive Noise Annealing and Approximation Properties of Quantized Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.10452v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.10452v1)
- **Published**: 2019-05-24 21:30:54+00:00
- **Updated**: 2019-05-24 21:30:54+00:00
- **Authors**: Matteo Spallanzani, Lukas Cavigelli, Gian Paolo Leonardi, Marko Bertogna, Luca Benini
- **Comment**: None
- **Journal**: None
- **Summary**: We present a theoretical and experimental investigation of the quantization problem for artificial neural networks. We provide a mathematical definition of quantized neural networks and analyze their approximation capabilities, showing in particular that any Lipschitz-continuous map defined on a hypercube can be uniformly approximated by a quantized neural network. We then focus on the regularization effect of additive noise on the arguments of multi-step functions inherent to the quantization of continuous variables. In particular, when the expectation operator is applied to a non-differentiable multi-step random function, and if the underlying probability density is differentiable (in either classical or weak sense), then a differentiable function is retrieved, with explicit bounds on its Lipschitz constant. Based on these results, we propose a novel gradient-based training algorithm for quantized neural networks that generalizes the straight-through estimator, acting on noise applied to the network's parameters. We evaluate our algorithm on the CIFAR-10 and ImageNet image classification benchmarks, showing state-of-the-art performance on AlexNet and MobileNetV2 for ternary networks.



### Scaling Video Analytics on Constrained Edge Nodes
- **Arxiv ID**: http://arxiv.org/abs/1905.13536v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.PF, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.13536v1)
- **Published**: 2019-05-24 23:11:07+00:00
- **Updated**: 2019-05-24 23:11:07+00:00
- **Authors**: Christopher Canel, Thomas Kim, Giulio Zhou, Conglong Li, Hyeontaek Lim, David G. Andersen, Michael Kaminsky, Subramanya R. Dulloor
- **Comment**: This paper is an extended version of a paper with the same title
  published in the 2nd SysML Conference, SysML '19 (Canel et. al., 2019)
- **Journal**: None
- **Summary**: As video camera deployments continue to grow, the need to process large volumes of real-time data strains wide area network infrastructure. When per-camera bandwidth is limited, it is infeasible for applications such as traffic monitoring and pedestrian tracking to offload high-quality video streams to a datacenter. This paper presents FilterForward, a new edge-to-cloud system that enables datacenter-based applications to process content from thousands of cameras by installing lightweight edge filters that backhaul only relevant video frames. FilterForward introduces fast and expressive per-application microclassifiers that share computation to simultaneously detect dozens of events on computationally constrained edge nodes. Only matching events are transmitted to the cloud. Evaluation on two real-world camera feed datasets shows that FilterForward reduces bandwidth use by an order of magnitude while improving computational efficiency and event detection accuracy for challenging video content.



### Fully Hyperbolic Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.10484v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.10484v3)
- **Published**: 2019-05-24 23:43:36+00:00
- **Updated**: 2020-07-07 18:02:05+00:00
- **Authors**: Keegan Lensink, Bas Peters, Eldad Haber
- **Comment**: 21 pages, 9 figures, Updated work to include additional numerical
  experiments, a section about VAEs and learnable wavelets
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) have recently seen tremendous success in various computer vision tasks. However, their application to problems with high dimensional input and output, such as high-resolution image and video segmentation or 3D medical imaging, has been limited by various factors. Primarily, in the training stage, it is necessary to store network activations for back propagation. In these settings, the memory requirements associated with storing activations can exceed what is feasible with current hardware, especially for problems in 3D. Motivated by the propagation of signals over physical networks, that are governed by the hyperbolic Telegraph equation, in this work we introduce a fully conservative hyperbolic network for problems with high dimensional input and output. We introduce a coarsening operation that allows completely reversible CNNs by using a learnable Discrete Wavelet Transform and its inverse to both coarsen and interpolate the network state and change the number of channels. We show that fully reversible networks are able to achieve results comparable to the state of the art in 4D time-lapse hyper spectral image segmentation and full 3D video segmentation, with a much lower memory footprint that is a constant independent of the network depth. We also extend the use of such networks to Variational Auto Encoders with high resolution input and output.



### Generative Latent Flow
- **Arxiv ID**: http://arxiv.org/abs/1905.10485v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.10485v2)
- **Published**: 2019-05-24 23:44:50+00:00
- **Updated**: 2019-09-22 22:57:15+00:00
- **Authors**: Zhisheng Xiao, Qing Yan, Yali Amit
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose the Generative Latent Flow (GLF), an algorithm for generative modeling of the data distribution. GLF uses an Auto-encoder (AE) to learn latent representations of the data, and a normalizing flow to map the distribution of the latent variables to that of simple i.i.d noise. In contrast to some other Auto-encoder based generative models, which use various regularizers that encourage the encoded latent distribution to match the prior distribution, our model explicitly constructs a mapping between these two distributions, leading to better density matching while avoiding over regularizing the latent variables. We compare our model with several related techniques, and show that it has many relative advantages including fast convergence, single stage training and minimal reconstruction trade-off. We also study the relationship between our model and its stochastic counterpart, and show that our model can be viewed as a vanishing noise limit of VAEs with flow prior. Quantitatively, under standardized evaluations, our method achieves state-of-the-art sample quality among AE based models on commonly used datasets, and is competitive with GANs' benchmarks.



