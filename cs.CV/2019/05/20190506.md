# Arxiv Papers in cs.CV on 2019-05-06
### Nostalgin: Extracting 3D City Models from Historical Image Data
- **Arxiv ID**: http://arxiv.org/abs/1905.01772v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.01772v1)
- **Published**: 2019-05-06 00:18:15+00:00
- **Updated**: 2019-05-06 00:18:15+00:00
- **Authors**: Amol Kapoor, Hunter Larco, Raimondas Kiveris
- **Comment**: None
- **Journal**: None
- **Summary**: What did it feel like to walk through a city from the past? In this work, we describe Nostalgin (Nostalgia Engine), a method that can faithfully reconstruct cities from historical images. Unlike existing work in city reconstruction, we focus on the task of reconstructing 3D cities from historical images. Working with historical image data is substantially more difficult, as there are significantly fewer buildings available and the details of the camera parameters which captured the images are unknown. Nostalgin can generate a city model even if there is only a single image per facade, regardless of viewpoint or occlusions. To achieve this, our novel architecture combines image segmentation, rectification, and inpainting. We motivate our design decisions with experimental analysis of individual components of our pipeline, and show that we can improve on baselines in both speed and visual realism. We demonstrate the efficacy of our pipeline by recreating two 1940s Manhattan city blocks. We aim to deploy Nostalgin as an open source platform where users can generate immersive historical experiences from their own photos.



### DLIMD: Dictionary Learning based Image-domain Material Decomposition for spectral CT
- **Arxiv ID**: http://arxiv.org/abs/1905.02567v2
- **DOI**: 10.1088/1361-6560/aba7ce
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.02567v2)
- **Published**: 2019-05-06 00:28:48+00:00
- **Updated**: 2019-05-24 09:15:41+00:00
- **Authors**: Weiwen Wu, Haijun Yu, Peijun Chen, Fulin Luo, Fenglin Liu, Qian Wang, Yining Zhu, Yanbo Zhang, Jian Feng, Hengyong Yu
- **Comment**: None
- **Journal**: Physics in Medicine & Biology, 2020
- **Summary**: The potential huge advantage of spectral computed tomography (CT) is its capability to provide accuracy material identification and quantitative tissue information. This can benefit clinical applications, such as brain angiography, early tumor recognition, etc. To achieve more accurate material components with higher material image quality, we develop a dictionary learning based image-domain material decomposition (DLIMD) for spectral CT in this paper. First, we reconstruct spectral CT image from projections and calculate material coefficients matrix by selecting uniform regions of basis materials from image reconstruction results. Second, we employ the direct inversion (DI) method to obtain initial material decomposition results, and a set of image patches are extracted from the mode-1 unfolding of normalized material image tensor to train a united dictionary by the K-SVD technique. Third, the trained dictionary is employed to explore the similarities from decomposed material images by constructing the DLIMD model. Fourth, more constraints (i.e., volume conservation and the bounds of each pixel within material maps) are further integrated into the model to improve the accuracy of material decomposition. Finally, both physical phantom and preclinical experiments are employed to evaluate the performance of the proposed DLIMD in material decomposition accuracy, material image edge preservation and feature recovery.



### Differentiable Architecture Search with Ensemble Gumbel-Softmax
- **Arxiv ID**: http://arxiv.org/abs/1905.01786v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.01786v1)
- **Published**: 2019-05-06 01:47:17+00:00
- **Updated**: 2019-05-06 01:47:17+00:00
- **Authors**: Jianlong Chang, Xinbang Zhang, Yiwen Guo, Gaofeng Meng, Shiming Xiang, Chunhong Pan
- **Comment**: None
- **Journal**: None
- **Summary**: For network architecture search (NAS), it is crucial but challenging to simultaneously guarantee both effectiveness and efficiency. Towards achieving this goal, we develop a differentiable NAS solution, where the search space includes arbitrary feed-forward network consisting of the predefined number of connections. Benefiting from a proposed ensemble Gumbel-Softmax estimator, our method optimizes both the architecture of a deep network and its parameters in the same round of backward propagation, yielding an end-to-end mechanism of searching network architectures. Extensive experiments on a variety of popular datasets strongly evidence that our method is capable of discovering high-performance architectures, while guaranteeing the requisite efficiency during searching.



### Creating Lightweight Object Detectors with Model Compression for Deployment on Edge Devices
- **Arxiv ID**: http://arxiv.org/abs/1905.01787v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.01787v1)
- **Published**: 2019-05-06 01:52:17+00:00
- **Updated**: 2019-05-06 01:52:17+00:00
- **Authors**: Yiwu Yao, Weiqiang Yang, Haoqi Zhu
- **Comment**: lightweight detector, automatic channel pruning, fixed channel
  deletion, knowledge distillation
- **Journal**: None
- **Summary**: To achieve lightweight object detectors for deployment on the edge devices, an effective model compression pipeline is proposed in this paper. The compression pipeline consists of automatic channel pruning for the backbone, fixed channel deletion for the branch layers and knowledge distillation for the guidance learning. As results, the Resnet50-v1d is auto-pruned and fine-tuned on ImageNet to attain a compact base model as the backbone of object detector. Then, lightweight object detectors are implemented with proposed compression pipeline. For instance, the SSD-300 with model size=16.3MB, FLOPS=2.31G, and mAP=71.2 is created, revealing a better result than SSD-300-MobileNet.



### Feature Aggregation Network for Video Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1905.01796v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.01796v2)
- **Published**: 2019-05-06 02:37:12+00:00
- **Updated**: 2019-09-12 09:03:09+00:00
- **Authors**: Zhaoxiang Liu, Huan Hu, Jinqiang Bai, Shaohua Li, Shiguo Lian
- **Comment**: 9 pages, 4 figures, Accepted by ICCV 2019 workshop
- **Journal**: None
- **Summary**: This paper aims to learn a compact representation of a video for video face recognition task. We make the following contributions: first, we propose a meta attention-based aggregation scheme which adaptively and fine-grained weighs the feature along each feature dimension among all frames to form a compact and discriminative representation. It makes the best to exploit the valuable or discriminative part of each frame to promote the performance of face recognition, without discarding or despising low quality frames as usual methods do. Second, we build a feature aggregation network comprised of a feature embedding module and a feature aggregation module. The embedding module is a convolutional neural network used to extract a feature vector from a face image, while the aggregation module consists of cascaded two meta attention blocks which adaptively aggregate the feature vectors into a single fixed-length representation. The network can deal with arbitrary number of frames, and is insensitive to frame order. Third, we validate the performance of proposed aggregation scheme. Experiments on publicly available datasets, such as YouTube face dataset and IJB-A dataset, show the effectiveness of our method, and it achieves competitive performances on both the verification and identification protocols.



### Extracting human emotions at different places based on facial expressions and spatial clustering analysis
- **Arxiv ID**: http://arxiv.org/abs/1905.01817v1
- **DOI**: 10.1111/tgis.12552
- **Categories**: **cs.CV**, cs.HC, I.2; I.4.9; I.5.3
- **Links**: [PDF](http://arxiv.org/pdf/1905.01817v1)
- **Published**: 2019-05-06 04:10:37+00:00
- **Updated**: 2019-05-06 04:10:37+00:00
- **Authors**: Yuhao Kang, Qingyuan Jia, Song Gao, Xiaohuan Zeng, Yueyao Wang, Stephan Angsuesser, Yu Liu, Xinyue Ye, Teng Fei
- **Comment**: 40 pages; 9 figures
- **Journal**: Transactions in GIS, Year 2019, Volume 23, Issue 3
- **Summary**: The emergence of big data enables us to evaluate the various human emotions at places from a statistic perspective by applying affective computing. In this study, a novel framework for extracting human emotions from large-scale georeferenced photos at different places is proposed. After the construction of places based on spatial clustering of user generated footprints collected in social media websites, online cognitive services are utilized to extract human emotions from facial expressions using the state-of-the-art computer vision techniques. And two happiness metrics are defined for measuring the human emotions at different places. To validate the feasibility of the framework, we take 80 tourist attractions around the world as an example and a happiness ranking list of places is generated based on human emotions calculated over 2 million faces detected out from over 6 million photos. Different kinds of geographical contexts are taken into consideration to find out the relationship between human emotions and environmental factors. Results show that much of the emotional variation at different places can be explained by a few factors such as openness. The research may offer insights on integrating human emotions to enrich the understanding of sense of place in geography and in place-based GIS.



### Transferring Multiscale Map Styles Using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.02200v2
- **DOI**: 10.1080/23729333.2019.1615729
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.2.1; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/1905.02200v2)
- **Published**: 2019-05-06 04:52:42+00:00
- **Updated**: 2019-05-18 05:47:15+00:00
- **Authors**: Yuhao Kang, Song Gao, Robert E. Roth
- **Comment**: 12 pages, 17 figure
- **Journal**: International Journal of Cartography, 2019
- **Summary**: The advancement of the Artificial Intelligence (AI) technologies makes it possible to learn stylistic design criteria from existing maps or other visual art and transfer these styles to make new digital maps. In this paper, we propose a novel framework using AI for map style transfer applicable across multiple map scales. Specifically, we identify and transfer the stylistic elements from a target group of visual examples, including Google Maps, OpenStreetMap, and artistic paintings, to unstylized GIS vector data through two generative adversarial network (GAN) models. We then train a binary classifier based on a deep convolutional neural network to evaluate whether the transfer styled map images preserve the original map design characteristics. Our experiment results show that GANs have a great potential for multiscale map style transferring, but many challenges remain requiring future research.



### P-ODN: Prototype based Open Deep Network for Open Set Recognition
- **Arxiv ID**: http://arxiv.org/abs/1905.01851v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01851v2)
- **Published**: 2019-05-06 07:30:59+00:00
- **Updated**: 2020-01-13 07:51:14+00:00
- **Authors**: Yu Shu, Yemin Shi, Yaowei Wang, Tiejun Huang, Yonghong Tian
- **Comment**: 15 pages, 10 figures
- **Journal**: None
- **Summary**: Most of the existing recognition algorithms are proposed for closed set scenarios, where all categories are known beforehand. However, in practice, recognition is essentially an open set problem. There are categories we know called "knowns", and there are more we do not know called "unknowns". Enumerating all categories beforehand is never possible, consequently it is infeasible to prepare sufficient training samples for those unknowns. Applying closed set recognition methods will naturally lead to unseen-category errors. To address this problem, we propose the prototype based Open Deep Network (P-ODN) for open set recognition tasks. Specifically, we introduce prototype learning into open set recognition. Prototypes and prototype radiuses are trained jointly to guide a CNN network to derive more discriminative features. Then P-ODN detects the unknowns by applying a multi-class triplet thresholding method based on the distance metric between features and prototypes. Manual labeling the unknowns which are detected in the previous process as new categories. Predictors for new categories are added to the classification layer to "open" the deep neural networks to incorporate new categories dynamically. The weights of new predictors are initialized exquisitely by applying a distances based algorithm to transfer the learned knowledge. Consequently, this initialization method speed up the fine-tuning process and reduce the samples needed to train new predictors. Extensive experiments show that P-ODN can effectively detect unknowns and needs only few samples with human intervention to recognize a new category. In the real world scenarios, our method achieves state-of-the-art performance on the UCF11, UCF50, UCF101 and HMDB51 datasets.



### The Missing Data Encoder: Cross-Channel Image Completion\\with Hide-And-Seek Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1905.01861v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01861v1)
- **Published**: 2019-05-06 07:51:04+00:00
- **Updated**: 2019-05-06 07:51:04+00:00
- **Authors**: Arnaud Dapogny, Matthieu Cord, Patrick Perez
- **Comment**: None
- **Journal**: None
- **Summary**: Image completion is the problem of generating whole images from fragments only. It encompasses inpainting (generating a patch given its surrounding), reverse inpainting/extrapolation (generating the periphery given the central patch) as well as colorization (generating one or several channels given other ones). In this paper, we employ a deep network to perform image completion, with adversarial training as well as perceptual and completion losses, and call it the ``missing data encoder'' (MDE). We consider several configurations based on how the seed fragments are chosen. We show that training MDE for ``random extrapolation and colorization'' (MDE-REC), i.e. using random channel-independent fragments, allows a better capture of the image semantics and geometry. MDE training makes use of a novel ``hide-and-seek'' adversarial loss, where the discriminator seeks the original non-masked regions, while the generator tries to hide them. We validate our models both qualitatively and quantitatively on several datasets, showing their interest for image completion, unsupervised representation learning as well as face occlusion handling.



### SEMEDA: Enhancing Segmentation Precision with Semantic Edge Aware Loss
- **Arxiv ID**: http://arxiv.org/abs/1905.01892v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01892v1)
- **Published**: 2019-05-06 09:14:01+00:00
- **Updated**: 2019-05-06 09:14:01+00:00
- **Authors**: Yifu Chen, Arnaud Dapogny, Matthieu Cord
- **Comment**: None
- **Journal**: None
- **Summary**: While nowadays deep neural networks achieve impressive performances on semantic segmentation tasks, they are usually trained by optimizing pixel-wise losses such as cross-entropy. As a result, the predictions outputted by such networks usually struggle to accurately capture the object boundaries and exhibit holes inside the objects. In this paper, we propose a novel approach to improve the structure of the predicted segmentation masks. We introduce a novel semantic edge detection network, which allows to match the predicted and ground truth segmentation masks. This Semantic Edge-Aware strategy (SEMEDA) can be combined with any backbone deep network in an end-to-end training framework. Through thorough experimental validation on Pascal VOC 2012 and Cityscapes datasets, we show that the proposed SEMEDA approach enhances the structure of the predicted segmentation masks by enforcing sharp boundaries and avoiding discontinuities inside objects, improving the segmentation performance. In addition, our semantic edge-aware loss can be integrated into any popular segmentation network without requiring any additional annotation and with negligible computational load, as compared to standard pixel-wise cross-entropy loss.



### Lesion Segmentation in Ultrasound Using Semi-pixel-wise Cycle Generative Adversarial Nets
- **Arxiv ID**: http://arxiv.org/abs/1905.01902v4
- **DOI**: 10.1109/TCBB.2020.2978470
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.01902v4)
- **Published**: 2019-05-06 09:49:57+00:00
- **Updated**: 2020-10-17 04:59:42+00:00
- **Authors**: Jie Xing, Zheren Li, Biyuan Wang, Yuji Qi, Bingbin Yu, Farhad G. Zanjani, Aiwen Zheng, Remco Duits, Tao Tan
- **Comment**: None
- **Journal**: IEEE/ACM Transactions on Computational Biology and Bioinformatics,
  04 March 2020, pp.1-1
- **Summary**: Breast cancer is the most common invasive cancer with the highest cancer occurrence in females. Handheld ultrasound is one of the most efficient ways to identify and diagnose the breast cancer. The area and the shape information of a lesion is very helpful for clinicians to make diagnostic decisions. In this study we propose a new deep-learning scheme, semi-pixel-wise cycle generative adversarial net (SPCGAN) for segmenting the lesion in 2D ultrasound. The method takes the advantage of a fully convolutional neural network (FCN) and a generative adversarial net to segment a lesion by using prior knowledge. We compared the proposed method to a fully connected neural network and the level set segmentation method on a test dataset consisting of 32 malignant lesions and 109 benign lesions. Our proposed method achieved a Dice similarity coefficient (DSC) of 0.92 while FCN and the level set achieved 0.90 and 0.79 respectively. Particularly, for malignant lesions, our method increases the DSC (0.90) of the fully connected neural network to 0.93 significantly (p$<$0.001). The results show that our SPCGAN can obtain robust segmentation results. The framework of SPCGAN is particularly effective when sufficient training samples are not available compared to FCN. Our proposed method may be used to relieve the radiologists' burden for annotation.



### Image Captioning with Clause-Focused Metrics in a Multi-Modal Setting for Marketing
- **Arxiv ID**: http://arxiv.org/abs/1905.01919v1
- **DOI**: 10.1109/MIPR.2019.00085
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01919v1)
- **Published**: 2019-05-06 10:42:10+00:00
- **Updated**: 2019-05-06 10:42:10+00:00
- **Authors**: Philipp Harzig, Dan Zecha, Rainer Lienhart, Carolin Kaiser, René Schallner
- **Comment**: 6 pages, accepted at MIPR 2019
- **Journal**: None
- **Summary**: Automatically generating descriptive captions for images is a well-researched area in computer vision. However, existing evaluation approaches focus on measuring the similarity between two sentences disregarding fine-grained semantics of the captions. In our setting of images depicting persons interacting with branded products, the subject, predicate, object and the name of the branded product are important evaluation criteria of the generated captions. Generating image captions with these constraints is a new challenge, which we tackle in this work. By simultaneously predicting integer-valued ratings that describe attributes of the human-product interaction, we optimize a deep neural network architecture in a multi-task learning setting, which considerably improves the caption quality. Furthermore, we introduce a novel metric that allows us to assess whether the generated captions meet our requirements (i.e., subject, predicate, object, and product name) and describe a series of experiments on caption quality and how to address annotator disagreements for the image ratings with an approach called soft targets. We also show that our novel clause-focused metrics are also applicable to other image captioning datasets, such as the popular MSCOCO dataset.



### FaceShapeGene: A Disentangled Shape Representation for Flexible Face Image Editing
- **Arxiv ID**: http://arxiv.org/abs/1905.01920v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01920v1)
- **Published**: 2019-05-06 10:48:44+00:00
- **Updated**: 2019-05-06 10:48:44+00:00
- **Authors**: Sen-Zhe Xu, Hao-Zhi Huang, Shi-Min Hu, Wei Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Existing methods for face image manipulation generally focus on editing the expression, changing some predefined attributes, or applying different filters. However, users lack the flexibility of controlling the shapes of different semantic facial parts in the generated face. In this paper, we propose an approach to compute a disentangled shape representation for a face image, namely the FaceShapeGene. The proposed FaceShapeGene encodes the shape information of each semantic facial part separately into a 1D latent vector. On the basis of the FaceShapeGene, a novel part-wise face image editing system is developed, which contains a shape-remix network and a conditional label-to-face transformer. The shape-remix network can freely recombine the part-wise latent vectors from different individuals, producing a remixed face shape in the form of a label map, which contains the facial characteristics of multiple subjects. The conditional label-to-face transformer, which is trained in an unsupervised cyclic manner, performs part-wise face editing while preserving the original identity of the subject. Experimental results on several tasks demonstrate that the proposed FaceShapeGene representation correctly disentangles the shape features of different semantic parts. %In addition, we test our system on several novel part-wise face editing tasks. Comparisons to existing methods demonstrate the superiority of the proposed method on accomplishing novel face editing tasks.



### Fast and Reliable Architecture Selection for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.01924v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.01924v1)
- **Published**: 2019-05-06 10:58:23+00:00
- **Updated**: 2019-05-06 10:58:23+00:00
- **Authors**: Lukas Hahn, Lutz Roese-Koerner, Klaus Friedrichs, Anton Kummert
- **Comment**: As published in the proceedings of the 27th European Symposium on
  Artificial Neural Networks, Computational Intelligence and Machine Learning
  (ESANN), pages 179-184, Bruges 2019. 6 pages, 2 figures, 1 table
- **Journal**: Proceedings of the 27th European Symposium on Artificial Neural
  Networks, Computational Intelligence and Machine Learning (ESANN), pages
  179-184, Bruges 2019
- **Summary**: The performance of a Convolutional Neural Network (CNN) depends on its hyperparameters, like the number of layers, kernel sizes, or the learning rate for example. Especially in smaller networks and applications with limited computational resources, optimisation is key. We present a fast and efficient approach for CNN architecture selection. Taking into account time consumption, precision and robustness, we develop a heuristic to quickly and reliably assess a network's performance. In combination with Bayesian optimisation (BO), to effectively cover the vast parameter space, our contribution offers a plain and powerful architecture search for this machine learning technique.



### Deep Visual City Recognition Visualization
- **Arxiv ID**: http://arxiv.org/abs/1905.01932v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01932v1)
- **Published**: 2019-05-06 11:24:33+00:00
- **Updated**: 2019-05-06 11:24:33+00:00
- **Authors**: Xiangwei Shi, Seyran Khademi, Jan van Gemert
- **Comment**: CVPR-19 workshop on Explainable AI
- **Journal**: None
- **Summary**: Understanding how cities visually differ from each others is interesting for planners, residents, and historians. We investigate the interpretation of deep features learned by convolutional neural networks (CNNs) for city recognition. Given a trained city recognition network, we first generate weighted masks using the known Grad-CAM technique and to select the most discriminate regions in the image. Since the image classification label is the city name, it contains no information of objects that are class-discriminate, we investigate the interpretability of deep representations with two methods. (i) Unsupervised method is used to cluster the objects appearing in the visual explanations. (ii) A pretrained semantic segmentation model is used to label objects in pixel level, and then we introduce statistical measures to quantitatively evaluate the interpretability of discriminate objects. The influence of network architectures and random initializations in training, is studied on the interpretability of CNN features for city recognition. The results suggest that network architectures would affect the interpretability of learned visual representations greater than different initializations.



### Few-Shot Adaptive Gaze Estimation
- **Arxiv ID**: http://arxiv.org/abs/1905.01941v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01941v2)
- **Published**: 2019-05-06 11:48:39+00:00
- **Updated**: 2019-10-14 14:55:59+00:00
- **Authors**: Seonwook Park, Shalini De Mello, Pavlo Molchanov, Umar Iqbal, Otmar Hilliges, Jan Kautz
- **Comment**: ICCV 2019 (Oral)
- **Journal**: None
- **Summary**: Inter-personal anatomical differences limit the accuracy of person-independent gaze estimation networks. Yet there is a need to lower gaze errors further to enable applications requiring higher quality. Further gains can be achieved by personalizing gaze networks, ideally with few calibration samples. However, over-parameterized neural networks are not amenable to learning from few examples as they can quickly over-fit. We embrace these challenges and propose a novel framework for Few-shot Adaptive GaZE Estimation (FAZE) for learning person-specific gaze networks with very few (less than or equal to 9) calibration samples. FAZE learns a rotation-aware latent representation of gaze via a disentangling encoder-decoder architecture along with a highly adaptable gaze estimator trained using meta-learning. It is capable of adapting to any new person to yield significant performance gains with as few as 3 samples, yielding state-of-the-art performance of 3.18 degrees on GazeCapture, a 19% improvement over prior art. We open-source our code at https://github.com/NVlabs/few_shot_gaze



### Closing the Accuracy Gap in an Event-Based Visual Recognition Task
- **Arxiv ID**: http://arxiv.org/abs/1906.08859v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.08859v1)
- **Published**: 2019-05-06 12:11:05+00:00
- **Updated**: 2019-05-06 12:11:05+00:00
- **Authors**: Bodo Rückauer, Nicolas Känzig, Shih-Chii Liu, Tobi Delbruck, Yulia Sandamirskaya
- **Comment**: None
- **Journal**: None
- **Summary**: Mobile and embedded applications require neural networks-based pattern recognition systems to perform well under a tight computational budget. In contrast to commonly used synchronous, frame-based vision systems and CNNs, asynchronous, spiking neural networks driven by event-based visual input respond with low latency to sparse, salient features in the input, leading to high efficiency at run-time. The discrete nature of the event-based data streams makes direct training of asynchronous neural networks challenging. This paper studies asynchronous spiking neural networks, obtained by conversion from a conventional CNN trained on frame-based data. As an example, we consider a CNN trained to steer a robot to follow a moving target. We identify possible pitfalls of the conversion and demonstrate how the proposed solutions bring the classification accuracy of the asynchronous network to only 3\% below the performance of the original synchronous CNN, while requiring 12x fewer computations. While being applied to a simple task, this work is an important step towards low-power, fast, and embedded neural networks-based vision solutions for robotic applications.



### Unsupervised Domain Adaptation using Graph Transduction Games
- **Arxiv ID**: http://arxiv.org/abs/1905.02036v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.GT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.02036v1)
- **Published**: 2019-05-06 13:34:04+00:00
- **Updated**: 2019-05-06 13:34:04+00:00
- **Authors**: Sebastiano Vascon, Sinem Aslan, Alessandro Torcinovich, Twan van Laarhoven, Elena Marchiori, Marcello Pelillo
- **Comment**: Oral IJCNN 2019
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) amounts to assigning class labels to the unlabeled instances of a dataset from a target domain, using labeled instances of a dataset from a related source domain. In this paper, we propose to cast this problem in a game-theoretic setting as a non-cooperative game and introduce a fully automatized iterative algorithm for UDA based on graph transduction games (GTG). The main advantages of this approach are its principled foundation, guaranteed termination of the iterative algorithms to a Nash equilibrium (which corresponds to a consistent labeling condition) and soft labels quantifying the uncertainty of the label assignment process. We also investigate the beneficial effect of using pseudo-labels from linear classifiers to initialize the iterative process. The performance of the resulting methods is assessed on publicly available object recognition benchmark datasets involving both shallow and deep features. Results of experiments demonstrate the suitability of the proposed game-theoretic approach for solving UDA tasks.



### Pixel-wise Regression: 3D Hand Pose Estimation via Spatial-form Representation and Differentiable Decoder
- **Arxiv ID**: http://arxiv.org/abs/1905.02085v2
- **DOI**: 10.1109/TMM.2020.3047552
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02085v2)
- **Published**: 2019-05-06 15:07:15+00:00
- **Updated**: 2019-05-27 02:14:02+00:00
- **Authors**: Xingyuan Zhang, Fuhai Zhang
- **Comment**: Update LaTeX version. Code coming soon
- **Journal**: None
- **Summary**: 3D Hand pose estimation from a single depth image is an essential topic in computer vision and human-computer interaction. Although the rising of deep learning method boosts the accuracy a lot, the problem is still hard to solve due to the complex structure of the human hand. Existing methods with deep learning either lose spatial information of hand structure or lack a direct supervision of joint coordinates. In this paper, we propose a novel Pixel-wise Regression method, which use spatial-form representation (SFR) and differentiable decoder (DD) to solve the two problems. To use our method, we build a model, in which we design a particular SFR and its correlative DD which divided the 3D joint coordinates into two parts, plane coordinates and depth coordinates and use two modules named Plane Regression (PR) and Depth Regression (DR) to deal with them respectively. We conduct an ablation experiment to show the method we proposed achieve better results than the former methods. We also make an exploration on how different training strategies influence the learned SFRs and results. The experiment on three public datasets demonstrates that our model is comparable with the existing state-of-the-art models and in one of them our model can reduce mean 3D joint error by 25%.



### Improved Hard Example Mining by Discovering Attribute-based Hard Person Identity
- **Arxiv ID**: http://arxiv.org/abs/1905.02102v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02102v3)
- **Published**: 2019-05-06 15:38:36+00:00
- **Updated**: 2019-08-05 23:23:10+00:00
- **Authors**: Xiao Wang, Ziliang Chen, Rui Yang, Bin Luo, Jin Tang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose Hard Person Identity Mining (HPIM) that attempts to refine the hard example mining to improve the exploration efficacy in person re-identification. It is motivated by following observation: the more attributes some people share, the more difficult to separate their identities. Based on this observation, we develop HPIM via a transferred attribute describer, a deep multi-attribute classifier trained from the source noisy person attribute datasets. We encode each image into the attribute probabilistic description in the target person re-ID dataset. Afterwards in the attribute code space, we consider each person as a distribution to generate his view-specific attribute codes in different practical scenarios. Hence we estimate the person-specific statistical moments from zeroth to higher order, which are further used to calculate the central moment discrepancies between persons. Such discrepancy is a ground to choose hard identity to organize proper mini-batches, without concerning the person representation changing in metric learning. It presents as a complementary tool of hard example mining, which helps to explore the global instead of the local hard example constraint in the mini-batch built by randomly sampled identities. Extensive experiments on two person re-identification benchmarks validated the effectiveness of our proposed algorithm.



### Localizing Adverts in Outdoor Scenes
- **Arxiv ID**: http://arxiv.org/abs/1905.02106v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02106v1)
- **Published**: 2019-05-06 15:44:31+00:00
- **Updated**: 2019-05-06 15:44:31+00:00
- **Authors**: Soumyabrata Dev, Murhaf Hossari, Matthew Nicholson, Killian McCabe, Atul Nautiyal, Clare Conran, Jian Tang, Wei Xu, François Pitié
- **Comment**: Published in 2019 IEEE International Conference on Multimedia & Expo
  Workshops (ICMEW)
- **Journal**: None
- **Summary**: Online videos have witnessed an unprecedented growth over the last decade, owing to wide range of content creation. This provides the advertisement and marketing agencies plethora of opportunities for targeted advertisements. Such techniques involve replacing an existing advertisement in a video frame, with a new advertisement. However, such post-processing of online videos is mostly done manually by video editors. This is cumbersome and time-consuming. In this paper, we propose DeepAds -- a deep neural network, based on the simple encoder-decoder architecture, that can accurately localize the position of an advert in a video frame. Our approach of localizing billboards in outdoor scenes using neural nets, is the first of its kind, and achieves the best performance. We benchmark our proposed method with other semantic segmentation algorithms, on a public dataset of outdoor scenes with manually annotated billboard binary maps.



### Learning Optimal Data Augmentation Policies via Bayesian Optimization for Image Classification Tasks
- **Arxiv ID**: http://arxiv.org/abs/1905.02610v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.02610v2)
- **Published**: 2019-05-06 15:49:02+00:00
- **Updated**: 2019-05-23 04:50:16+00:00
- **Authors**: Chunxu Zhang, Jiaxu Cui, Bo Yang
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, deep learning has achieved remarkable achievements in many fields, including computer vision, natural language processing, speech recognition and others. Adequate training data is the key to ensure the effectiveness of the deep models. However, obtaining valid data requires a lot of time and labor resources. Data augmentation (DA) is an effective alternative approach, which can generate new labeled data based on existing data using label-preserving transformations. Although we can benefit a lot from DA, designing appropriate DA policies requires a lot of expert experience and time consumption, and the evaluation of searching the optimal policies is costly. So we raise a new question in this paper: how to achieve automated data augmentation at as low cost as possible? We propose a method named BO-Aug for automating the process by finding the optimal DA policies using the Bayesian optimization approach. Our method can find the optimal policies at a relatively low search cost, and the searched policies based on a specific dataset are transferable across different neural network architectures or even different datasets. We validate the BO-Aug on three widely used image classification datasets, including CIFAR-10, CIFAR-100 and SVHN. Experimental results show that the proposed method can achieve state-of-the-art or near advanced classification accuracy. Code to reproduce our experiments is available at https://github.com/zhangxiaozao/BO-Aug.



### Visibility Constrained Generative Model for Depth-based 3D Facial Pose Tracking
- **Arxiv ID**: http://arxiv.org/abs/1905.02114v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02114v1)
- **Published**: 2019-05-06 15:58:41+00:00
- **Updated**: 2019-05-06 15:58:41+00:00
- **Authors**: Lu Sheng, Jianfei Cai, Tat-Jen Cham, Vladimir Pavlovic, King Ngi Ngan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a generative framework that unifies depth-based 3D facial pose tracking and face model adaptation on-the-fly, in the unconstrained scenarios with heavy occlusions and arbitrary facial expression variations. Specifically, we introduce a statistical 3D morphable model that flexibly describes the distribution of points on the surface of the face model, with an efficient switchable online adaptation that gradually captures the identity of the tracked subject and rapidly constructs a suitable face model when the subject changes. Moreover, unlike prior art that employed ICP-based facial pose estimation, to improve robustness to occlusions, we propose a ray visibility constraint that regularizes the pose based on the face model's visibility with respect to the input point cloud. Ablation studies and experimental results on Biwi and ICT-3DHP datasets demonstrate that the proposed framework is effective and outperforms completing state-of-the-art depth-based methods.



### 3D Packing for Self-Supervised Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1905.02693v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1905.02693v4)
- **Published**: 2019-05-06 17:09:52+00:00
- **Updated**: 2020-03-28 18:49:27+00:00
- **Authors**: Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raventos, Adrien Gaidon
- **Comment**: None
- **Journal**: None
- **Summary**: Although cameras are ubiquitous, robotic platforms typically rely on active sensors like LiDAR for direct 3D perception. In this work, we propose a novel self-supervised monocular depth estimation method combining geometry with a new deep network, PackNet, learned only from unlabeled monocular videos. Our architecture leverages novel symmetrical packing and unpacking blocks to jointly learn to compress and decompress detail-preserving representations using 3D convolutions. Although self-supervised, our method outperforms other self, semi, and fully supervised methods on the KITTI benchmark. The 3D inductive bias in PackNet enables it to scale with input resolution and number of parameters without overfitting, generalizing better on out-of-domain data such as the NuScenes dataset. Furthermore, it does not require large-scale supervised pretraining on ImageNet and can run in real-time. Finally, we release DDAD (Dense Depth for Automated Driving), a new urban driving dataset with more challenging and accurate depth evaluation, thanks to longer-range and denser ground-truth depth generated from high-density LiDARs mounted on a fleet of self-driving cars operating world-wide.



### Simulating CRF with CNN for CNN
- **Arxiv ID**: http://arxiv.org/abs/1905.02163v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02163v1)
- **Published**: 2019-05-06 17:27:32+00:00
- **Updated**: 2019-05-06 17:27:32+00:00
- **Authors**: Lena Gorelick, Olga Veksler
- **Comment**: None
- **Journal**: None
- **Summary**: Combining CNN with CRF for modeling dependencies between pixel labels is a popular research direction. This task is far from trivial, especially if end-to-end training is desired. In this paper, we propose a novel simple approach to CNN+CRF combination. In particular, we propose to simulate a CRF regularizer with a trainable module that has standard CNN architecture. We call this module a CRF Simulator. We can automatically generate an unlimited amount of ground truth for training such CRF Simulator without any user interaction, provided we have an efficient algorithm for optimization of the actual CRF regularizer. After our CRF Simulator is trained, it can be directly incorporated as part of any larger CNN architecture, enabling a seamless end-to-end training. In particular, the other modules can learn parameters that are more attuned to the performance of the CRF Simulator module. We demonstrate the effectiveness of our approach on the task of salient object segmentation regularized with the standard binary CRF energy. In contrast to previous work we do not need to develop and implement the complex mechanics of optimizing a specific CRF as part of CNN. In fact, our approach can be easily extended to other CRF energies, including multi-label. To the best of our knowledge we are the first to study the question of whether the output of CNNs can have regularization properties of CRFs.



### Spatio-Temporal Action Localization in a Weakly Supervised Setting
- **Arxiv ID**: http://arxiv.org/abs/1905.02171v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02171v1)
- **Published**: 2019-05-06 17:39:09+00:00
- **Updated**: 2019-05-06 17:39:09+00:00
- **Authors**: Kurt Degiorgio, Fabio Cuzzolin
- **Comment**: None
- **Journal**: None
- **Summary**: Enabling computational systems with the ability to localize actions in video-based content has manifold applications. Traditionally, such a problem is approached in a fully-supervised setting where video-clips with complete frame-by-frame annotations around the actions of interest are provided for training. However, the data requirements needed to achieve adequate generalization in this setting is prohibitive. In this work, we circumvent this issue by casting the problem in a weakly supervised setting, i.e., by considering videos as labelled `sets' of unlabelled video segments. Firstly, we apply unsupervised segmentation to take advantage of the elementary structure of each video. Subsequently, a convolutional neural network is used to extract RGB features from the resulting video segments. Finally, Multiple Instance Learning (MIL) is employed to predict labels at the video segment level, thus inherently performing spatio-temporal action detection. In contrast to previous work, we make use of a different MIL formulation in which the label of each video segment is continuous rather then discrete, making the resulting optimization function tractable. Additionally, we utilize a set splitting technique for regularization. Experimental results considering multiple performance indicators on the UCF-Sports data-set support the effectiveness of our approach.



### Adversarial Examples Are Not Bugs, They Are Features
- **Arxiv ID**: http://arxiv.org/abs/1905.02175v4
- **DOI**: None
- **Categories**: **stat.ML**, cs.CR, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.02175v4)
- **Published**: 2019-05-06 17:45:05+00:00
- **Updated**: 2019-08-12 14:36:10+00:00
- **Authors**: Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, Aleksander Madry
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.



### Computation of Circular Area and Spherical Volume Invariants via Boundary Integrals
- **Arxiv ID**: http://arxiv.org/abs/1905.02176v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CG, cs.CV, math.DG, 65D18, 65D30, 68U05, 65N38
- **Links**: [PDF](http://arxiv.org/pdf/1905.02176v1)
- **Published**: 2019-05-06 17:51:00+00:00
- **Updated**: 2019-05-06 17:51:00+00:00
- **Authors**: Riley O'Neill, Pedro Angulo-Umana, Jeff Calder, Bo Hessburg, Peter J. Olver, Chehrzad Shakiban, Katrina Yezzi-Woodley
- **Comment**: None
- **Journal**: None
- **Summary**: We show how to compute the circular area invariant of planar curves, and the spherical volume invariant of surfaces, in terms of line and surface integrals, respectively. We use the Divergence Theorem to express the area and volume integrals as line and surface integrals, respectively, against particular kernels; our results also extend to higher dimensional hypersurfaces. The resulting surface integrals are computable analytically on a triangulated mesh. This gives a simple computational algorithm for computing the spherical volume invariant for triangulated surfaces that does not involve discretizing the ambient space. We discuss potential applications to feature detection on broken bone fragments of interest in anthropology.



### Label-Noise Robust Multi-Domain Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1905.02185v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.02185v1)
- **Published**: 2019-05-06 17:57:43+00:00
- **Updated**: 2019-05-06 17:57:43+00:00
- **Authors**: Takuhiro Kaneko, Tatsuya Harada
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-domain image-to-image translation is a problem where the goal is to learn mappings among multiple domains. This problem is challenging in terms of scalability because it requires the learning of numerous mappings, the number of which increases proportional to the number of domains. However, generative adversarial networks (GANs) have emerged recently as a powerful framework for this problem. In particular, label-conditional extensions (e.g., StarGAN) have become a promising solution owing to their ability to address this problem using only a single unified model. Nonetheless, a limitation is that they rely on the availability of large-scale clean-labeled data, which are often laborious or impractical to collect in a real-world scenario. To overcome this limitation, we propose a novel model called the label-noise robust image-to-image translation model (RMIT) that can learn a clean label conditional generator even when noisy labeled data are only available. In particular, we propose a novel loss called the virtual cycle consistency loss that is able to regularize cyclic reconstruction independently of noisy labeled data, as well as we introduce advanced techniques to boost the performance in practice. Our experimental results demonstrate that RMIT is useful for obtaining label-noise robustness in various settings including synthetic and real-world noise.



### CARAFE: Content-Aware ReAssembly of FEatures
- **Arxiv ID**: http://arxiv.org/abs/1905.02188v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02188v3)
- **Published**: 2019-05-06 17:58:06+00:00
- **Updated**: 2019-10-29 12:41:51+00:00
- **Authors**: Jiaqi Wang, Kai Chen, Rui Xu, Ziwei Liu, Chen Change Loy, Dahua Lin
- **Comment**: ICCV 2019 Camera Ready (Oral)
- **Journal**: None
- **Summary**: Feature upsampling is a key operation in a number of modern convolutional network architectures, e.g. feature pyramids. Its design is critical for dense prediction tasks such as object detection and semantic/instance segmentation. In this work, we propose Content-Aware ReAssembly of FEatures (CARAFE), a universal, lightweight and highly effective operator to fulfill this goal. CARAFE has several appealing properties: (1) Large field of view. Unlike previous works (e.g. bilinear interpolation) that only exploit sub-pixel neighborhood, CARAFE can aggregate contextual information within a large receptive field. (2) Content-aware handling. Instead of using a fixed kernel for all samples (e.g. deconvolution), CARAFE enables instance-specific content-aware handling, which generates adaptive kernels on-the-fly. (3) Lightweight and fast to compute. CARAFE introduces little computational overhead and can be readily integrated into modern network architectures. We conduct comprehensive evaluations on standard benchmarks in object detection, instance/semantic segmentation and inpainting. CARAFE shows consistent and substantial gains across all the tasks (1.2%, 1.3%, 1.8%, 1.1db respectively) with negligible computational overhead. It has great potential to serve as a strong building block for future research. It has great potential to serve as a strong building block for future research. Code and models are available at https://github.com/open-mmlab/mmdetection.



### Sparse data interpolation using the geodesic distance affinity space
- **Arxiv ID**: http://arxiv.org/abs/1905.02229v1
- **DOI**: 10.1109/LSP.2019.2914004
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02229v1)
- **Published**: 2019-05-06 18:24:11+00:00
- **Updated**: 2019-05-06 18:24:11+00:00
- **Authors**: Mikhail G. Mozerov, Fei Yang, Joost van de Weijer
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we adapt the geodesic distance-based recursive filter to the sparse data interpolation problem. The proposed technique is general and can be easily applied to any kind of sparse data. We demonstrate the superiority over other interpolation techniques in three experiments for qualitative and quantitative evaluation.   In addition, we compare our method with the popular interpolation algorithm presented in the EpicFlow optical flow paper that is intuitively motivated by a similar geodesic distance principle. The comparison shows that our algorithm is more accurate and considerably faster than the EpicFlow interpolation technique.



### A Geometric Approach to Obtain a Bird's Eye View from an Image
- **Arxiv ID**: http://arxiv.org/abs/1905.02231v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02231v2)
- **Published**: 2019-05-06 18:26:49+00:00
- **Updated**: 2020-09-27 19:15:16+00:00
- **Authors**: Ammar Abbas, Andrew Zisserman
- **Comment**: ICCV Workshop "Geometry Meets Deep Learning" 2019
- **Journal**: None
- **Summary**: The objective of this paper is to rectify any monocular image by computing a homography matrix that transforms it to a bird's eye (overhead) view.   We make the following contributions: (i) we show that the homography matrix can be parameterised with only four parameters that specify the horizon line and the vertical vanishing point, or only two if the field of view or focal length is known; (ii) We introduce a novel representation for the geometry of a line or point (which can be at infinity) that is suitable for regression with a convolutional neural network (CNN); (iii) We introduce a large synthetic image dataset with ground truth for the orthogonal vanishing points, that can be used for training a CNN to predict these geometric entities; and finally (iv) We achieve state-of-the-art results on horizon detection, with 74.52% AUC on the Horizon Lines in the Wild dataset. Our method is fast and robust, and can be used to remove perspective distortion from videos in real time.



### Image Matters: Scalable Detection of Offensive and Non-Compliant Content / Logo in Product Images
- **Arxiv ID**: http://arxiv.org/abs/1905.02234v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.02234v2)
- **Published**: 2019-05-06 18:35:28+00:00
- **Updated**: 2019-08-02 07:38:26+00:00
- **Authors**: Shreyansh Gandhi, Samrat Kokkula, Abon Chaudhuri, Alessandro Magnani, Theban Stanley, Behzad Ahmadi, Venkatesh Kandaswamy, Omer Ovenc, Shie Mannor
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: In e-commerce, product content, especially product images have a significant influence on a customer's journey from product discovery to evaluation and finally, purchase decision. Since many e-commerce retailers sell items from other third-party marketplace sellers besides their own, the content published by both internal and external content creators needs to be monitored and enriched, wherever possible. Despite guidelines and warnings, product listings that contain offensive and non-compliant images continue to enter catalogs. Offensive and non-compliant content can include a wide range of objects, logos, and banners conveying violent, sexually explicit, racist, or promotional messages. Such images can severely damage the customer experience, lead to legal issues, and erode the company brand. In this paper, we present a computer vision driven offensive and non-compliant image detection system for extremely large image datasets. This paper delves into the unique challenges of applying deep learning to real-world product image data from retail world. We demonstrate how we resolve a number of technical challenges such as lack of training data, severe class imbalance, fine-grained class definitions etc. using a number of practical yet unique technical strategies. Our system combines state-of-the-art image classification and object detection techniques with budgeted crowdsourcing to develop a solution customized for a massive, diverse, and constantly evolving product catalog.



### Searching for MobileNetV3
- **Arxiv ID**: http://arxiv.org/abs/1905.02244v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02244v5)
- **Published**: 2019-05-06 19:38:31+00:00
- **Updated**: 2019-11-20 17:26:40+00:00
- **Authors**: Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, Hartwig Adam
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation.



### MixMatch: A Holistic Approach to Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.02249v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.02249v2)
- **Published**: 2019-05-06 19:56:03+00:00
- **Updated**: 2019-10-23 18:47:34+00:00
- **Authors**: David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, Colin Raffel
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-supervised learning has proven to be a powerful paradigm for leveraging unlabeled data to mitigate the reliance on large labeled datasets. In this work, we unify the current dominant approaches for semi-supervised learning to produce a new algorithm, MixMatch, that works by guessing low-entropy labels for data-augmented unlabeled examples and mixing labeled and unlabeled data using MixUp. We show that MixMatch obtains state-of-the-art results by a large margin across many datasets and labeled data amounts. For example, on CIFAR-10 with 250 labels, we reduce error rate by a factor of 4 (from 38% to 11%) and by a factor of 2 on STL-10. We also demonstrate how MixMatch can help achieve a dramatically better accuracy-privacy trade-off for differential privacy. Finally, we perform an ablation study to tease apart which components of MixMatch are most important for its success.



### Source Generator Attribution via Inversion
- **Arxiv ID**: http://arxiv.org/abs/1905.02259v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.02259v2)
- **Published**: 2019-05-06 20:47:28+00:00
- **Updated**: 2019-06-13 19:01:37+00:00
- **Authors**: Michael Albright, Scott McCloskey
- **Comment**: Updated with new experiments
- **Journal**: None
- **Summary**: With advances in Generative Adversarial Networks (GANs) leading to dramatically-improved synthetic images and video, there is an increased need for algorithms which extend traditional forensics to this new category of imagery. While GANs have been shown to be helpful in a number of computer vision applications, there are other problematic uses such as `deep fakes' which necessitate such forensics. Source camera attribution algorithms using various cues have addressed this need for imagery captured by a camera, but there are fewer options for synthetic imagery. We address the problem of attributing a synthetic image to a specific generator in a white box setting, by inverting the process of generation. This enables us to simultaneously determine whether the generator produced the image and recover an input which produces a close match to the synthetic image.



### Caveats in Generating Medical Imaging Labels from Radiology Reports
- **Arxiv ID**: http://arxiv.org/abs/1905.02283v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.02283v1)
- **Published**: 2019-05-06 22:38:18+00:00
- **Updated**: 2019-05-06 22:38:18+00:00
- **Authors**: Tobi Olatunji, Li Yao, Ben Covington, Alexander Rhodes, Anthony Upton
- **Comment**: Accepted workshop contribution for Medical Imaging with Deep Learning
  (MIDL), 2019
- **Journal**: None
- **Summary**: Acquiring high-quality annotations in medical imaging is usually a costly process. Automatic label extraction with natural language processing (NLP) has emerged as a promising workaround to bypass the need of expert annotation. Despite the convenience, the limitation of such an approximation has not been carefully examined and is not well understood. With a challenging set of 1,000 chest X-ray studies and their corresponding radiology reports, we show that there exists a surprisingly large discrepancy between what radiologists visually perceive and what they clinically report. Furthermore, with inherently flawed report as ground truth, the state-of-the-art medical NLP fails to produce high-fidelity labels.



### Simultaneous Object Detection and Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.02285v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.02285v2)
- **Published**: 2019-05-06 22:45:22+00:00
- **Updated**: 2020-02-13 13:47:18+00:00
- **Authors**: Niels Ole Salscheider
- **Comment**: None
- **Journal**: None
- **Summary**: Both object detection in and semantic segmentation of camera images are important tasks for automated vehicles. Object detection is necessary so that the planning and behavior modules can reason about other road users. Semantic segmentation provides for example free space information and information about static and dynamic parts of the environment. There has been a lot of research to solve both tasks using Convolutional Neural Networks. These approaches give good results but are computationally demanding. In practice, a compromise has to be found between detection performance, detection quality and the number of tasks. Otherwise it is not possible to meet the real-time requirements of automated vehicles. In this work, we propose a neural network architecture to solve both tasks simultaneously. This architecture was designed to run with around 10 Hz on 1 MP images on current hardware. Our approach achieves a mean IoU of 61.2% for the semantic segmentation task on the challenging Cityscapes benchmark. It also achieves an average precision of 69.3% for cars and 67.7% on the moderate difficulty level of the KITTI benchmark.



### Frame-wise Motion and Appearance for Real-time Multiple Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1905.02292v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02292v1)
- **Published**: 2019-05-06 23:37:05+00:00
- **Updated**: 2019-05-06 23:37:05+00:00
- **Authors**: Jimuyang Zhang, Sanping Zhou, Jinjun Wang, Dong Huang
- **Comment**: 13 pages, 4 figures, 4 tables
- **Journal**: None
- **Summary**: The main challenge of Multiple Object Tracking (MOT) is the efficiency in associating indefinite number of objects between video frames. Standard motion estimators used in tracking, e.g., Long Short Term Memory (LSTM), only deal with single object, while Re-IDentification (Re-ID) based approaches exhaustively compare object appearances. Both approaches are computationally costly when they are scaled to a large number of objects, making it very difficult for real-time MOT. To address these problems, we propose a highly efficient Deep Neural Network (DNN) that simultaneously models association among indefinite number of objects. The inference computation of the DNN does not increase with the number of objects. Our approach, Frame-wise Motion and Appearance (FMA), computes the Frame-wise Motion Fields (FMF) between two frames, which leads to very fast and reliable matching among a large number of object bounding boxes. As auxiliary information is used to fix uncertain matches, Frame-wise Appearance Features (FAF) are learned in parallel with FMFs. Extensive experiments on the MOT17 benchmark show that our method achieved real-time MOT with competitive results as the state-of-the-art approaches.



