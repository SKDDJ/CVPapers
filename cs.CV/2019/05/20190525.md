# Arxiv Papers in cs.CV on 2019-05-25
### GAN2GAN: Generative Noise Learning for Blind Denoising with Single Noisy Images
- **Arxiv ID**: http://arxiv.org/abs/1905.10488v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.10488v5)
- **Published**: 2019-05-25 00:16:09+00:00
- **Updated**: 2021-07-04 09:16:19+00:00
- **Authors**: Sungmin Cha, Taeeon Park, Byeongjoon Kim, Jongduk Baek, Taesup Moon
- **Comment**: ICLR 2021 camera ready version
- **Journal**: None
- **Summary**: We tackle a challenging blind image denoising problem, in which only single distinct noisy images are available for training a denoiser, and no information about noise is known, except for it being zero-mean, additive, and independent of the clean image. In such a setting, which often occurs in practice, it is not possible to train a denoiser with the standard discriminative training or with the recently developed Noise2Noise (N2N) training; the former requires the underlying clean image for the given noisy image, and the latter requires two independently realized noisy image pair for a clean image. To that end, we propose GAN2GAN (Generated-Artificial-Noise to Generated-Artificial-Noise) method that first learns a generative model that can 1) simulate the noise in the given noisy images and 2) generate a rough, noisy estimates of the clean images, then 3) iteratively trains a denoiser with subsequently synthesized noisy image pairs (as in N2N), obtained from the generative model. In results, we show the denoiser trained with our GAN2GAN achieves an impressive denoising performance on both synthetic and real-world datasets for the blind denoising setting; it almost approaches the performance of the standard discriminatively-trained or N2N-trained models that have more information than ours, and it significantly outperforms the recent baseline for the same setting, \textit{e.g.}, Noise2Void, and a more conventional yet strong one, BM3D. The official code of our method is available at https://github.com/csm9493/GAN2GAN.



### Cold Case: The Lost MNIST Digits
- **Arxiv ID**: http://arxiv.org/abs/1905.10498v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.10498v2)
- **Published**: 2019-05-25 01:50:51+00:00
- **Updated**: 2019-11-04 21:05:26+00:00
- **Authors**: Chhavi Yadav, LÃ©on Bottou
- **Comment**: Final NeurIPS version
- **Journal**: None
- **Summary**: Although the popular MNIST dataset [LeCun et al., 1994] is derived from the NIST database [Grother and Hanaoka, 1995], the precise processing steps for this derivation have been lost to time. We propose a reconstruction that is accurate enough to serve as a replacement for the MNIST dataset, with insignificant changes in accuracy. We trace each MNIST digit to its NIST source and its rich metadata such as writer identifier, partition identifier, etc. We also reconstruct the complete MNIST test set with 60,000 samples instead of the usual 10,000. Since the balance 50,000 were never distributed, they enable us to investigate the impact of twenty-five years of MNIST experiments on the reported testing performances. Our results unambiguously confirm the trends observed by Recht et al. [2018, 2019]: although the misclassification rates are slightly off, classifier ordering and model selection remain broadly reliable. We attribute this phenomenon to the pairing benefits of comparing classifiers on the same digits.



### 6-DOF GraspNet: Variational Grasp Generation for Object Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1905.10520v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1905.10520v2)
- **Published**: 2019-05-25 04:59:52+00:00
- **Updated**: 2019-08-17 00:23:16+00:00
- **Authors**: Arsalan Mousavian, Clemens Eppner, Dieter Fox
- **Comment**: Accepted to ICCV 2019. Extended camera ready version with additional
  experiments
- **Journal**: None
- **Summary**: Generating grasp poses is a crucial component for any robot object manipulation task. In this work, we formulate the problem of grasp generation as sampling a set of grasps using a variational autoencoder and assess and refine the sampled grasps using a grasp evaluator model. Both Grasp Sampler and Grasp Refinement networks take 3D point clouds observed by a depth camera as input. We evaluate our approach in simulation and real-world robot experiments. Our approach achieves 88\% success rate on various commonly used objects with diverse appearances, scales, and weights. Our model is trained purely in simulation and works in the real world without any extra steps. The video of our experiments can be found at: https://research.nvidia.com/publication/2019-10_6-DOF-GraspNet\%3A-Variational



### Domain Adaptive Attention Model for Unsupervised Cross-Domain Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1905.10529v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10529v1)
- **Published**: 2019-05-25 06:05:49+00:00
- **Updated**: 2019-05-25 06:05:49+00:00
- **Authors**: Yangru Huang, Peixi Peng, Yi Jin, Junliang Xing, Congyan Lang, Songhe Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification (Re-ID) across multiple datasets is a challenging yet important task due to the possibly large distinctions between different datasets and the lack of training samples in practical applications. This work proposes a novel unsupervised domain adaption framework which transfers discriminative representations from the labeled source domain (dataset) to the unlabeled target domain (dataset). We propose to formulate the domain adaption task as an one-class classification problem with a novel domain similarity loss. Given the feature map of any image from a backbone network, a novel domain adaptive attention model (DAAM) first automatically learns to separate the feature map of an image to a domain-shared feature (DSH) map and a domain-specific feature (DSP) map simultaneously. Specially, the residual attention mechanism is designed to model DSP feature map for avoiding negative transfer. Then, a DSH branch and a DSP branch are introduced to learn DSH and DSP feature maps respectively. To reduce domain divergence caused by that the source and target datasets are collected from different environments, we force to project the DSH feature maps from different domains to a new nominal domain, and a novel domain similarity loss is proposed based on one-class classification. In addition, a novel unsupervised person Re-ID loss is proposed to take full use of unlabeled target data. Extensive experiments on the Market-1501 and DukeMTMC-reID benchmarks demonstrate state-of-the-art performance of the proposed method. Code will be released to facilitate further studies on the cross-domain person re-identification task.



### Leveraging Domain Knowledge to Improve Microscopy Image Segmentation with Lifted Multicuts
- **Arxiv ID**: http://arxiv.org/abs/1905.10535v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10535v2)
- **Published**: 2019-05-25 06:54:18+00:00
- **Updated**: 2019-08-05 21:56:22+00:00
- **Authors**: Constantin Pape, Alex Matskevych, Adrian Wolny, Julian Hennies, Giula Mizzon, Marion Louveaux, Jacob Musser, Alexis Maizel, Detlev Arendt, Anna Kreshuk
- **Comment**: None
- **Journal**: None
- **Summary**: The throughput of electron microscopes has increased significantly in recent years, enabling detailed analysis of cell morphology and ultrastructure. Analysis of neural circuits at single-synapse resolution remains the flagship target of this technique, but applications to cell and developmental biology are also starting to emerge at scale. The amount of data acquired in such studies makes manual instance segmentation, a fundamental step in many analysis pipelines, impossible. While automatic segmentation approaches have improved significantly thanks to the adoption of convolutional neural networks, their accuracy still lags behind human annotations and requires additional manual proof-reading. A major hindrance to further improvements is the limited field of view of the segmentation networks preventing them from exploiting the expected cell morphology or other prior biological knowledge which humans use to inform their segmentation decisions. In this contribution, we show how such domain-specific information can be leveraged by expressing it as long-range interactions in a graph partitioning problem known as the lifted multicut problem. Using this formulation, we demonstrate significant improvement in segmentation accuracy for three challenging EM segmentation problems from neuroscience and cell biology.



### A New Clustering Method Based on Morphological Operations
- **Arxiv ID**: http://arxiv.org/abs/1905.10548v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1905.10548v1)
- **Published**: 2019-05-25 07:40:41+00:00
- **Updated**: 2019-05-25 07:40:41+00:00
- **Authors**: Zhenzhou Wang
- **Comment**: None
- **Journal**: None
- **Summary**: With the booming development of data science, many clustering methods have been proposed. All clustering methods have inherent merits and deficiencies. Therefore, they are only capable of clustering some specific types of data robustly. In addition, the accuracies of the clustering methods rely heavily on the characteristics of the data. In this paper, we propose a new clustering method based on the morphological operations. The morphological dilation is used to connect the data points based on their adjacency and form different connected domains. The iteration of the morphological dilation process stops when the number of connected domains equals the number of the clusters or when the maximum number of iteration is reached. The morphological dilation is then used to label the connected domains. The Euclidean distance between each data point and the points in each labeled connected domain is calculated. For each data point, there is a labeled connected domain that contains a point that yields the smallest Euclidean distance. The data point is assigned with the same labeling number as the labeled connected domain. We evaluate and compare the proposed method with state of the art clustering methods with different types of data. Experimental results show that the proposed method is more robust and generic for clustering two-dimensional or three-dimensional data.



### Ensemble of 3D CNN regressors with data fusion for fluid intelligence prediction
- **Arxiv ID**: http://arxiv.org/abs/1905.10550v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1905.10550v1)
- **Published**: 2019-05-25 07:54:56+00:00
- **Updated**: 2019-05-25 07:54:56+00:00
- **Authors**: Marina Pominova, Anna Kuzina, Ekaterina Kondrateva, Svetlana Sushchinskaya, Maxim Sharaev, Evgeny Burnaev, and Vyacheslav Yarkin
- **Comment**: 10 pages, 1 figure, 2 tables
- **Journal**: ABCD Neurocognitive Prediction Challenge, Springer LNCS, 2019
- **Summary**: In this work, we aim at predicting children's fluid intelligence scores based on structural T1-weighted MR images from the largest long-term study of brain development and child health. The target variable was regressed on a data collection site, socio-demographic variables and brain volume, thus being independent to the potentially informative factors, which are not directly related to the brain functioning. We investigate both feature extraction and deep learning approaches as well as different deep CNN architectures and their ensembles. We propose an advanced architecture of VoxCNNs ensemble, which yield MSE (92.838) on blind test.



### Robust Unsupervised Flexible Auto-weighted Local-Coordinate Concept Factorization for Image Clustering
- **Arxiv ID**: http://arxiv.org/abs/1905.10564v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.10564v1)
- **Published**: 2019-05-25 10:02:08+00:00
- **Updated**: 2019-05-25 10:02:08+00:00
- **Authors**: Zhao Zhang, Yan Zhang, Sheng Li, Guangcan Liu, Meng Wang, Shuicheng Yan
- **Comment**: Accepted at the 44th IEEE International Conference on Acoustics,
  Speech, and Signal Processing(ICASSP 2019)
- **Journal**: None
- **Summary**: We investigate the high-dimensional data clustering problem by proposing a novel and unsupervised representation learning model called Robust Flexible Auto-weighted Local-coordinate Concept Factorization (RFA-LCF). RFA-LCF integrates the robust flexible CF, robust sparse local-coordinate coding and the adaptive reconstruction weighting learning into a unified model. The adaptive weighting is driven by including the joint manifold preserving constraints on the recovered clean data, basis concepts and new representation. Specifically, our RFA-LCF uses a L2,1-norm based flexible residue to encode the mismatch between clean data and its reconstruction, and also applies the robust adaptive sparse local-coordinate coding to represent the data using a few nearby basis concepts, which can make the factorization more accurate and robust to noise. The robust flexible factorization is also performed in the recovered clean data space for enhancing representations. RFA-LCF also considers preserving the local manifold structures of clean data space, basis concept space and the new coordinate space jointly in an adaptive manner way. Extensive comparisons show that RFA-LCF can deliver enhanced clustering results.



### Scalable Block-Diagonal Locality-Constrained Projective Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.10568v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10568v1)
- **Published**: 2019-05-25 10:36:53+00:00
- **Updated**: 2019-05-25 10:36:53+00:00
- **Authors**: Zhao Zhang, Weiming Jiang, Zheng Zhang, Sheng Li, Guangcan Liu, Jie Qin
- **Comment**: Accepted at the 28th International Joint Conference on Artificial
  Intelligence(IJCAI 2019)
- **Journal**: None
- **Summary**: We propose a novel structured discriminative block-diagonal dictionary learning method, referred to as scalable Locality-Constrained Projective Dictionary Learning (LC-PDL), for efficient representation and classification. To improve the scalability by saving both training and testing time, our LC-PDL aims at learning a structured discriminative dictionary and a block-diagonal representation without using costly l0/l1-norm. Besides, it avoids extra time-consuming sparse reconstruction process with the well-trained dictionary for new sample as many existing models. More importantly, LC-PDL avoids using the complementary data matrix to learn the sub-dictionary over each class. To enhance the performance, we incorporate a locality constraint of atoms into the DL procedures to keep local information and obtain the codes of samples over each class separately. A block-diagonal discriminative approximation term is also derived to learn a discriminative projection to bridge data with their codes by extracting the special block-diagonal features from data, which can ensure the approximate coefficients to associate with its label information clearly. Then, a robust multiclass classifier is trained over extracted block-diagonal codes for accurate label predictions. Experimental results verify the effectiveness of our algorithm.



### Joint Label Prediction based Semi-Supervised Adaptive Concept Factorization for Robust Data Representation
- **Arxiv ID**: http://arxiv.org/abs/1905.10572v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10572v1)
- **Published**: 2019-05-25 11:18:45+00:00
- **Updated**: 2019-05-25 11:18:45+00:00
- **Authors**: Zhao Zhang, Yan Zhang, Guangcan Liu, Jinhui Tang, Shuicheng Yan, Meng Wang
- **Comment**: Accepted at IEEE TKDE
- **Journal**: DOI: 10.1109/TKDE.2019.2893956
- **Summary**: Constrained Concept Factorization (CCF) yields the enhanced representation ability over CF by incorporating label information as additional constraints, but it cannot classify and group unlabeled data appropriately. Minimizing the difference between the original data and its reconstruction directly can enable CCF to model a small noisy perturbation, but is not robust to gross sparse errors. Besides, CCF cannot preserve the manifold structures in new representation space explicitly, especially in an adaptive manner. In this paper, we propose a joint label prediction based Robust Semi-Supervised Adaptive Concept Factorization (RS2ACF) framework. To obtain robust representation, RS2ACF relaxes the factorization to make it simultaneously stable to small entrywise noise and robust to sparse errors. To enrich prior knowledge to enhance the discrimination, RS2ACF clearly uses class information of labeled data and more importantly propagates it to unlabeled data by jointly learning an explicit label indicator for unlabeled data. By the label indicator, RS2ACF can ensure the unlabeled data of the same predicted label to be mapped into the same class in feature space. Besides, RS2ACF incorporates the joint neighborhood reconstruction error over the new representations and predicted labels of both labeled and unlabeled data, so the manifold structures can be preserved explicitly and adaptively in the representation space and label space at the same time. Owing to the adaptive manner, the tricky process of determining the neighborhood size or kernel width can be avoided. Extensive results on public databases verify that our RS2ACF can deliver state-of-the-art data representation, compared with other related methods.



### Deep Image Feature Learning with Fuzzy Rules
- **Arxiv ID**: http://arxiv.org/abs/1905.10575v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.10575v3)
- **Published**: 2019-05-25 11:33:02+00:00
- **Updated**: 2023-03-17 06:10:49+00:00
- **Authors**: Xiang Ma, Liangzhe Chen, Zhaohong Deng, Peng Xu, Qisheng Yan, Kup-Sze Choi, Shitong Wang
- **Comment**: Accepted by IEEE Trans. Emerging Topics in Computational Intelligence
- **Journal**: None
- **Summary**: The methods of extracting image features are the key to many image processing tasks. At present, the most popular method is the deep neural network which can automatically extract robust features through end-to-end training instead of hand-crafted feature extraction. However, the deep neural network currently faces many challenges: 1) its effectiveness is heavily dependent on large datasets, so the computational complexity is very high; 2) it is usually regarded as a black box model with poor interpretability. To meet the above challenges, a more interpretable and scalable feature learning method, i.e., deep image feature learning with fuzzy rules (DIFL-FR), is proposed in the paper, which combines the rule-based fuzzy modeling technique and the deep stacked learning strategy. The method progressively learns image features through a layer-by-layer manner based on fuzzy rules, so the feature learning process can be better explained by the generated rules. More importantly, the learning process of the method is only based on forward propagation without back propagation and iterative learning, which results in the high learning efficiency. In addition, the method is under the settings of unsupervised learning and can be easily extended to scenes of supervised and semi-supervised learning. Extensive experiments are conducted on image datasets of different scales. The results obviously show the effectiveness of the proposed method.



### Efficient Object Annotation via Speaking and Pointing
- **Arxiv ID**: http://arxiv.org/abs/1905.10576v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1905.10576v4)
- **Published**: 2019-05-25 11:36:16+00:00
- **Updated**: 2019-12-19 12:57:30+00:00
- **Authors**: Michael Gygli, Vittorio Ferrari
- **Comment**: this article is an extension of arXiv:1811.09461, which was published
  at CVPR 2019
- **Journal**: None
- **Summary**: Deep neural networks deliver state-of-the-art visual recognition, but they rely on large datasets, which are time-consuming to annotate. These datasets are typically annotated in two stages: (1) determining the presence of object classes at the image level and (2) marking the spatial extent for all objects of these classes. In this work we use speech, together with mouse inputs, to speed up this process. We first improve stage one, by letting annotators indicate object class presence via speech. We then combine the two stages: annotators draw an object bounding box via the mouse and simultaneously provide its class label via speech. Using speech has distinct advantages over relying on mouse inputs alone. First, it is fast and allows for direct access to the class name, by simply saying it. Second, annotators can simultaneously speak and mark an object location. Finally, speech-based interfaces can be kept extremely simple, hence using them requires less mouse movement compared to existing approaches. Through extensive experiments on the COCO and ILSVRC datasets we show that our approach yields high-quality annotations at significant speed gains. Stage one takes 2.3x - 14.9x less annotation time than existing methods based on a hierarchical organization of the classes to be annotated. Moreover, when combining the two stages, we find that object class labels come for free: annotating them at the same time as bounding boxes has zero additional cost. On COCO, this makes the overall process 1.9x faster than the two-stage approach.



### Unsupervised Single Image Underwater Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1905.10595v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.10595v2)
- **Published**: 2019-05-25 13:26:44+00:00
- **Updated**: 2019-05-28 07:41:52+00:00
- **Authors**: Honey Gupta, Kaushik Mitra
- **Comment**: Accepted for publication at IEEE International Conference on Image
  Processing (ICIP), 2019
- **Journal**: None
- **Summary**: Depth estimation from a single underwater image is one of the most challenging problems and is highly ill-posed. Due to the absence of large generalized underwater depth datasets and the difficulty in obtaining ground truth depth-maps, supervised learning techniques such as direct depth regression cannot be used. In this paper, we propose an unsupervised method for depth estimation from a single underwater image taken `in the wild' by using haze as a cue for depth. Our approach is based on indirect depth-map estimation where we learn the mapping functions between unpaired RGB-D terrestrial images and arbitrary underwater images to estimate the required depth-map. We propose a method which is based on the principles of cycle-consistent learning and uses dense-block based auto-encoders as generator networks. We evaluate and compare our method both quantitatively and qualitatively on various underwater images with diverse attenuation and scattering conditions and show that our method produces state-of-the-art results for unsupervised depth estimation from a single underwater image.



### Best Pair Formulation & Accelerated Scheme for Non-convex Principal Component Pursuit
- **Arxiv ID**: http://arxiv.org/abs/1905.10598v2
- **DOI**: 10.1109/TSP.2020.3011024
- **Categories**: **math.OC**, cs.CV, cs.NA, math.NA, 65F30, 65K05, 49M15, 49M30, 65F50
- **Links**: [PDF](http://arxiv.org/pdf/1905.10598v2)
- **Published**: 2019-05-25 13:59:03+00:00
- **Updated**: 2019-05-28 09:24:16+00:00
- **Authors**: Aritra Dutta, Filip Hanzely, Jingwei Liang, Peter RichtÃ¡rik
- **Comment**: None
- **Journal**: None
- **Summary**: The best pair problem aims to find a pair of points that minimize the distance between two disjoint sets. In this paper, we formulate the classical robust principal component analysis (RPCA) as the best pair; which was not considered before. We design an accelerated proximal gradient scheme to solve it, for which we show global convergence, as well as the local linear rate. Our extensive numerical experiments on both real and synthetic data suggest that the algorithm outperforms relevant baseline algorithms in the literature.



### Reconstructing faces from voices
- **Arxiv ID**: http://arxiv.org/abs/1905.10604v2
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.10604v2)
- **Published**: 2019-05-25 14:33:59+00:00
- **Updated**: 2019-05-31 19:53:01+00:00
- **Authors**: Yandong Wen, Rita Singh, Bhiksha Raj
- **Comment**: None
- **Journal**: None
- **Summary**: Voice profiling aims at inferring various human parameters from their speech, e.g. gender, age, etc. In this paper, we address the challenge posed by a subtask of voice profiling - reconstructing someone's face from their voice. The task is designed to answer the question: given an audio clip spoken by an unseen person, can we picture a face that has as many common elements, or associations as possible with the speaker, in terms of identity? To address this problem, we propose a simple but effective computational framework based on generative adversarial networks (GANs). The network learns to generate faces from voices by matching the identities of generated faces to those of the speakers, on a training set. We evaluate the performance of the network by leveraging a closely related task - cross-modal matching. The results show that our model is able to generate faces that match several biometric characteristics of the speaker, and results in matching accuracies that are much better than chance.



### Exploring Feature Representation and Training strategies in Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/1905.10608v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10608v2)
- **Published**: 2019-05-25 14:55:48+00:00
- **Updated**: 2019-05-29 17:13:46+00:00
- **Authors**: Tingting Xie, Xiaoshan Yang, Tianzhu Zhang, Changsheng Xu, Ioannis Patras
- **Comment**: ICIP19 Camera Ready
- **Journal**: None
- **Summary**: Temporal action localization has recently attracted significant interest in the Computer Vision community. However, despite the great progress, it is hard to identify which aspects of the proposed methods contribute most to the increase in localization performance. To address this issue, we conduct ablative experiments on feature extraction methods, fixed-size feature representation methods and training strategies, and report how each influences the overall performance. Based on our findings, we propose a two-stage detector that outperforms the state of the art in THUMOS14, achieving a mAP@tIoU=0.5 equal to 44.2%.



### ShrinkTeaNet: Million-scale Lightweight Face Recognition via Shrinking Teacher-Student Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.10620v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10620v1)
- **Published**: 2019-05-25 15:44:40+00:00
- **Updated**: 2019-05-25 15:44:40+00:00
- **Authors**: Chi Nhan Duong, Khoa Luu, Kha Gia Quach, Ngan Le
- **Comment**: None
- **Journal**: None
- **Summary**: Large-scale face recognition in-the-wild has been recently achieved matured performance in many real work applications. However, such systems are built on GPU platforms and mostly deploy heavy deep network architectures. Given a high-performance heavy network as a teacher, this work presents a simple and elegant teacher-student learning paradigm, namely ShrinkTeaNet, to train a portable student network that has significantly fewer parameters and competitive accuracy against the teacher network. Far apart from prior teacher-student frameworks mainly focusing on accuracy and compression ratios in closed-set problems, our proposed teacher-student network is proved to be more robust against open-set problem, i.e. large-scale face recognition. In addition, this work introduces a novel Angular Distillation Loss for distilling the feature direction and the sample distributions of the teacher's hypersphere to its student. Then ShrinkTeaNet framework can efficiently guide the student's learning process with the teacher's knowledge presented in both intermediate and last stages of the feature embedding. Evaluations on LFW, CFP-FP, AgeDB, IJB-B and IJB-C Janus, and MegaFace with one million distractors have demonstrated the efficiency of the proposed approach to learn robust student networks which have satisfying accuracy and compact sizes. Our ShrinkTeaNet is able to support the light-weight architecture achieving high performance with 99.77% on LFW and 95.64% on large-scale Megaface protocols.



### Beyond Visual Semantics: Exploring the Role of Scene Text in Image Understanding
- **Arxiv ID**: http://arxiv.org/abs/1905.10622v3
- **DOI**: 10.1016/j.patrec.2021.06.011
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10622v3)
- **Published**: 2019-05-25 15:53:14+00:00
- **Updated**: 2019-12-04 11:17:25+00:00
- **Authors**: Arka Ujjal Dey, Suman Kumar Ghosh, Ernest Valveny, Gaurav Harit
- **Comment**: The paper is under consideration at Pattern Recognition Letters
- **Journal**: None
- **Summary**: Images with visual and scene text content are ubiquitous in everyday life. However, current image interpretation systems are mostly limited to using only the visual features, neglecting to leverage the scene text content. In this paper, we propose to jointly use scene text and visual channels for robust semantic interpretation of images. We do not only extract and encode visual and scene text cues, but also model their interplay to generate a contextual joint embedding with richer semantics. The contextual embedding thus generated is applied to retrieval and classification tasks on multimedia images, with scene text content, to demonstrate its effectiveness. In the retrieval framework, we augment our learned text-visual semantic representation with scene text cues, to mitigate vocabulary misses that may have occurred during the semantic embedding. To deal with irrelevant or erroneous recognition of scene text, we also apply query-based attention to our text channel. We show how the multi-channel approach, involving visual semantics and scene text, improves upon state of the art.



### Hyperparameter-Free Out-of-Distribution Detection Using Softmax of Scaled Cosine Similarity
- **Arxiv ID**: http://arxiv.org/abs/1905.10628v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10628v3)
- **Published**: 2019-05-25 16:33:38+00:00
- **Updated**: 2019-11-26 03:55:52+00:00
- **Authors**: Engkarat Techapanurak, Masanori Suganuma, Takayuki Okatani
- **Comment**: Extend the supplementary material
- **Journal**: None
- **Summary**: The ability to detect out-of-distribution (OOD) samples is vital to secure the reliability of deep neural networks in real-world applications. Considering the nature of OOD samples, detection methods should not have hyperparameters that need to be tuned depending on incoming OOD samples. However, most of the recently proposed methods do not meet this requirement, leading to compromised performance in real-world applications. In this paper, we propose a simple, hyperparameter-free method based on softmax of scaled cosine similarity. It resembles the approach employed by modern metric learning methods, but it differs in details; the differences are essential to achieve high detection performance. We show through experiments that our method outperforms the existing methods on the evaluation test recently proposed by Shafaei et al., which takes the above issue of hyperparameter dependency into account. We also show that it achieves at least comparable performance to other methods on the conventional test, where their hyperparameters are chosen using explicit OOD samples. Furthermore, it is computationally more efficient than most of the previous methods, since it needs only a single forward pass.



### Flexibly Regularized Mixture Models and Application to Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.10629v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1905.10629v3)
- **Published**: 2019-05-25 16:55:19+00:00
- **Updated**: 2022-02-08 16:45:18+00:00
- **Authors**: Jonathan Vacher, Claire Launay, Ruben Coen-Cagli
- **Comment**: 34 pages ( 31 + 3 for appendix). 12 figures + 1 in appendix, in press
  Neural Networks Journal
- **Journal**: None
- **Summary**: Probabilistic finite mixture models are widely used for unsupervised clustering. These models can often be improved by adapting them to the topology of the data. For instance, in order to classify spatially adjacent data points similarly, it is common to introduce a Laplacian constraint on the posterior probability that each data point belongs to a class. Alternatively, the mixing probabilities can be treated as free parameters, while assuming Gauss-Markov or more complex priors to regularize those mixing probabilities. However, these approaches are constrained by the shape of the prior and often lead to complicated or intractable inference. Here, we propose a new parametrization of the Dirichlet distribution to flexibly regularize the mixing probabilities of over-parametrized mixture distributions. Using the Expectation-Maximization algorithm, we show that our approach allows us to define any linear update rule for the mixing probabilities, including spatial smoothing regularization as a special case. We then show that this flexible design can be extended to share class information between multiple mixture models. We apply our algorithm to artificial and natural image segmentation tasks, and we provide quantitative and qualitative comparison of the performance of Gaussian and Student-t mixtures on the Berkeley Segmentation Dataset. We also demonstrate how to propagate class information across the layers of deep convolutional neural networks in a probabilistically optimal way, suggesting a new interpretation for feedback signals in biological visual systems. Our flexible approach can be easily generalized to adapt probabilistic mixture models to arbitrary data topologies.



### Exploring Temporal Information for Improved Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/1905.10654v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10654v1)
- **Published**: 2019-05-25 18:53:19+00:00
- **Updated**: 2019-05-25 18:53:19+00:00
- **Authors**: Yi Zhu
- **Comment**: PhD dissertation. Posting here for easier access and update. Chapter
  4, 6 and 7 are collaborative work
- **Journal**: None
- **Summary**: In this dissertation, I present my work towards exploring temporal information for better video understanding. Specifically, I have worked on two problems: action recognition and semantic segmentation. For action recognition, I have proposed a framework, termed hidden two-stream networks, to learn an optimal motion representation that does not require the computation of optical flow. My framework alleviates several challenges faced in video classification, such as learning motion representations, real-time inference, multi-framerate handling, generalizability to unseen actions, etc. For semantic segmentation, I have introduced a general framework that uses video prediction models to synthesize new training samples. By scaling up the training dataset, my trained models are more accurate and robust than previous models even without modifications to the network architectures or objective functions. I believe videos have much more potential to be mined, and temporal information is one of the most important cues for machines to perceive the visual world better.



### DIANet: Dense-and-Implicit Attention Network
- **Arxiv ID**: http://arxiv.org/abs/1905.10671v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.10671v2)
- **Published**: 2019-05-25 20:51:07+00:00
- **Updated**: 2019-09-23 08:23:50+00:00
- **Authors**: Zhongzhan Huang, Senwei Liang, Mingfu Liang, Haizhao Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Attention networks have successfully boosted the performance in various vision problems. Previous works lay emphasis on designing a new attention module and individually plug them into the networks. Our paper proposes a novel-and-simple framework that shares an attention module throughout different network layers to encourage the integration of layer-wise information and this parameter-sharing module is referred as Dense-and-Implicit-Attention (DIA) unit. Many choices of modules can be used in the DIA unit. Since Long Short Term Memory (LSTM) has a capacity of capturing long-distance dependency, we focus on the case when the DIA unit is the modified LSTM (refer as DIA-LSTM). Experiments on benchmark datasets show that the DIA-LSTM unit is capable of emphasizing layer-wise feature interrelation and leads to significant improvement of image classification accuracy. We further empirically show that the DIA-LSTM has a strong regularization ability on stabilizing the training of deep networks by the experiments with the removal of skip connections or Batch Normalization in the whole residual network. The code is released at https://github.com/gbup-group/DIANet.



### Constellation Loss: Improving the efficiency of deep metric learning loss functions for optimal embedding
- **Arxiv ID**: http://arxiv.org/abs/1905.10675v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.10675v1)
- **Published**: 2019-05-25 21:16:06+00:00
- **Updated**: 2019-05-25 21:16:06+00:00
- **Authors**: Alfonso Medela, Artzai Picon
- **Comment**: Submitted to NeurIPS 2019
- **Journal**: None
- **Summary**: Metric learning has become an attractive field for research on the latest years. Loss functions like contrastive loss, triplet loss or multi-class N-pair loss have made possible generating models capable of tackling complex scenarios with the presence of many classes and scarcity on the number of images per class not only work to build classifiers, but to many other applications where measuring similarity is the key. Deep Neural Networks trained via metric learning also offer the possibility to solve few-shot learning problems. Currently used state of the art loss functions such as triplet and contrastive loss functions, still suffer from slow convergence due to the selection of effective training samples that has been partially solved by the multi-class N-pair loss by simultaneously adding additional samples from the different classes. In this work, we extend triplet and multiclass-N-pair loss function by proposing the constellation loss metric where the distances among all class combinations are simultaneously learned. We have compared our constellation loss for visual class embedding showing that our loss function over-performs the other methods by obtaining more compact clusters while achieving better classification results.



### Improved object recognition using neural networks trained to mimic the brain's statistical properties
- **Arxiv ID**: http://arxiv.org/abs/1905.10679v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10679v4)
- **Published**: 2019-05-25 21:35:58+00:00
- **Updated**: 2020-07-15 18:25:16+00:00
- **Authors**: Callie Federer, Haoyan Xu, Alona Fyshe, Joel Zylberberg
- **Comment**: None
- **Journal**: None
- **Summary**: The current state-of-the-art object recognition algorithms, deep convolutional neural networks (DCNNs), are inspired by the architecture of the mammalian visual system, and are capable of human-level performance on many tasks. However, even these algorithms make errors. As they are trained for object recognition tasks, it has been shown that DCNNs develop hidden representations that resemble those observed in the mammalian visual system. Moreover, DCNNs trained on object recognition tasks are currently among the best models we have of the mammalian visual system. This led us to hypothesize that teaching DCNNs to achieve even more brain-like representations could improve their performance. To test this, we trained DCNNs on a composite task, wherein networks were trained to: a) classify images of objects; while b) having intermediate representations that resemble those observed in neural recordings from monkey visual cortex. Compared with DCNNs trained purely for object categorization, DCNNs trained on the composite task had better object recognition performance and are more robust to label corruption. Interestingly, we also found that neural data was not required, but randomized data with the same statistics as neural data also boosted performance. Our results outline a new way to train object recognition networks, using strategies in which the brain - or at least the statistical properties of its activation patterns - serves as a teacher signal for training DCNNs.



### Trust but Verify: An Information-Theoretic Explanation for the Adversarial Fragility of Machine Learning Systems, and a General Defense against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1905.11381v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.11381v1)
- **Published**: 2019-05-25 21:57:51+00:00
- **Updated**: 2019-05-25 21:57:51+00:00
- **Authors**: Jirong Yi, Hui Xie, Leixin Zhou, Xiaodong Wu, Weiyu Xu, Raghuraman Mudumbai
- **Comment**: 44 Pages, 2 Theorems, 35 Figures, 29 Tables. arXiv admin note:
  substantial text overlap with arXiv:1901.09413
- **Journal**: None
- **Summary**: Deep-learning based classification algorithms have been shown to be susceptible to adversarial attacks: minor changes to the input of classifiers can dramatically change their outputs, while being imperceptible to humans. In this paper, we present a simple hypothesis about a feature compression property of artificial intelligence (AI) classifiers and present theoretical arguments to show that this hypothesis successfully accounts for the observed fragility of AI classifiers to small adversarial perturbations. Drawing on ideas from information and coding theory, we propose a general class of defenses for detecting classifier errors caused by abnormally small input perturbations. We further show theoretical guarantees for the performance of this detection method. We present experimental results with (a) a voice recognition system, and (b) a digit recognition system using the MNIST database, to demonstrate the effectiveness of the proposed defense methods. The ideas in this paper are motivated by a simple analogy between AI classifiers and the standard Shannon model of a communication system.



### DAVE: A Deep Audio-Visual Embedding for Dynamic Saliency Prediction
- **Arxiv ID**: http://arxiv.org/abs/1905.10693v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.10693v2)
- **Published**: 2019-05-25 23:16:57+00:00
- **Updated**: 2020-01-07 21:52:07+00:00
- **Authors**: Hamed R. Tavakoli, Ali Borji, Esa Rahtu, Juho Kannala
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies audio-visual deep saliency prediction. It introduces a conceptually simple and effective Deep Audio-Visual Embedding for dynamic saliency prediction dubbed ``DAVE" in conjunction with our efforts towards building an Audio-Visual Eye-tracking corpus named ``AVE". Despite existing a strong relation between auditory and visual cues for guiding gaze during perception, video saliency models only consider visual cues and neglect the auditory information that is ubiquitous in dynamic scenes. Here, we investigate the applicability of audio cues in conjunction with visual ones in predicting saliency maps using deep neural networks. To this end, the proposed model is intentionally designed to be simple. Two baseline models are developed on the same architecture which consists of an encoder-decoder. The encoder projects the input into a feature space followed by a decoder that infers saliency. We conduct an extensive analysis on different modalities and various aspects of multi-model dynamic saliency prediction. Our results suggest that (1) audio is a strong contributing cue for saliency prediction, (2) salient visible sound-source is the natural cause of the superiority of our Audio-Visual model, (3) richer feature representations for the input space leads to more powerful predictions even in absence of more sophisticated saliency decoders, and (4) Audio-Visual model improves over 53.54\% of the frames predicted by the best Visual model (our baseline). Our endeavour demonstrates that audio is an important cue that boosts dynamic video saliency prediction and helps models to approach human performance. The code is available at https://github.com/hrtavakoli/DAVE



### Adversarial Distillation for Ordered Top-k Attacks
- **Arxiv ID**: http://arxiv.org/abs/1905.10695v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.10695v1)
- **Published**: 2019-05-25 23:24:15+00:00
- **Updated**: 2019-05-25 23:24:15+00:00
- **Authors**: Zekun Zhang, Tianfu Wu
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, especially white-box targeted attacks. One scheme of learning attacks is to design a proper adversarial objective function that leads to the imperceptible perturbation for any test image (e.g., the Carlini-Wagner (C&W) method). Most methods address targeted attacks in the Top-1 manner. In this paper, we propose to learn ordered Top-k attacks (k>= 1) for image classification tasks, that is to enforce the Top-k predicted labels of an adversarial example to be the k (randomly) selected and ordered labels (the ground-truth label is exclusive). To this end, we present an adversarial distillation framework: First, we compute an adversarial probability distribution for any given ordered Top-k targeted labels with respect to the ground-truth of a test image. Then, we learn adversarial examples by minimizing the Kullback-Leibler (KL) divergence together with the perturbation energy penalty, similar in spirit to the network distillation method. We explore how to leverage label semantic similarities in computing the targeted distributions, leading to knowledge-oriented attacks. In experiments, we thoroughly test Top-1 and Top-5 attacks in the ImageNet-1000 validation dataset using two popular DNNs trained with clean ImageNet-1000 train dataset, ResNet-50 and DenseNet-121. For both models, our proposed adversarial distillation approach outperforms the C&W method in the Top-1 setting, as well as other baseline methods. Our approach shows significant improvement in the Top-5 setting against a strong modified C&W method.



### Efficient Neural Task Adaptation by Maximum Entropy Initialization
- **Arxiv ID**: http://arxiv.org/abs/1905.10698v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1905.10698v2)
- **Published**: 2019-05-25 23:37:34+00:00
- **Updated**: 2019-07-12 02:32:53+00:00
- **Authors**: Farshid Varno, Behrouz Haji Soleimani, Marzie Saghayi, Lisa Di Jorio, Stan Matwin
- **Comment**: None
- **Journal**: None
- **Summary**: Transferring knowledge from one neural network to another has been shown to be helpful for learning tasks with few training examples. Prevailing fine-tuning methods could potentially contaminate pre-trained features by comparably high energy random noise. This noise is mainly delivered from a careless replacement of task-specific parameters. We analyze theoretically such knowledge contamination for classification tasks and propose a practical and easy to apply method to trap and minimize the contaminant. In our approach, the entropy of the output estimates gets maximized initially and the first back-propagated error is stalled at the output of the last layer. Our proposed method not only outperforms the traditional fine-tuning, but also significantly speeds up the convergence of the learner. It is robust to randomness and independent of the choice of architecture. Overall, our experiments show that the power of transfer learning has been substantially underestimated so far.



### Image Detection and Digit Recognition to solve Sudoku as a Constraint Satisfaction Problem
- **Arxiv ID**: http://arxiv.org/abs/1905.10701v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.10701v1)
- **Published**: 2019-05-25 23:47:08+00:00
- **Updated**: 2019-05-25 23:47:08+00:00
- **Authors**: Aditya Narayanaswamy, Yichuan Philip Ma, Piyush Shrivastava
- **Comment**: Pages: 9
- **Journal**: None
- **Summary**: Sudoku is a puzzle well-known to the scientific community with simple rules of completion, which may require a com-plex line of reasoning. This paper addresses the problem of partitioning the Sudoku image into a 1-D array, recognizing digits from the array and representing it as a Constraint Sat-isfaction Problem (CSP). In this paper, we introduce new fea-ture extraction techniques for recognizing digits, which are used with our benchmark classifiers in conjunction with the CSP algorithms to provide performance assessment. Experi-mental results show that application of CSP techniques can decrease the solution's search time by eliminating incon-sistent values from the search space.



