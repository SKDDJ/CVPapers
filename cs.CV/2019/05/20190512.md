# Arxiv Papers in cs.CV on 2019-05-12
### Generative Adversarial Networks and Conditional Random Fields for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1905.04621v1
- **DOI**: 10.1109/TCYB.2019.2915094
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.04621v1)
- **Published**: 2019-05-12 01:13:35+00:00
- **Updated**: 2019-05-12 01:13:35+00:00
- **Authors**: Zilong Zhong, Jonathan Li, David A. Clausi, Alexander Wong
- **Comment**: Accepted by IEEE T-CYB
- **Journal**: None
- **Summary**: In this paper, we address the hyperspectral image (HSI) classification task with a generative adversarial network and conditional random field (GAN-CRF) -based framework, which integrates a semi-supervised deep learning and a probabilistic graphical model, and make three contributions. First, we design four types of convolutional and transposed convolutional layers that consider the characteristics of HSIs to help with extracting discriminative features from limited numbers of labeled HSI samples. Second, we construct semi-supervised GANs to alleviate the shortage of training samples by adding labels to them and implicitly reconstructing real HSI data distribution through adversarial training. Third, we build dense conditional random fields (CRFs) on top of the random variables that are initialized to the softmax predictions of the trained GANs and are conditioned on HSIs to refine classification maps. This semi-supervised framework leverages the merits of discriminative and generative models through a game-theoretical approach. Moreover, even though we used very small numbers of labeled training HSI samples from the two most challenging and extensively studied datasets, the experimental results demonstrated that spectral-spatial GAN-CRF (SS-GAN-CRF) models achieved top-ranking accuracy for semi-supervised HSI classification.



### Predictive Ensemble Learning with Application to Scene Text Detection
- **Arxiv ID**: http://arxiv.org/abs/1905.04641v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04641v2)
- **Published**: 2019-05-12 03:49:29+00:00
- **Updated**: 2019-05-16 02:09:27+00:00
- **Authors**: Danlu Chen, Xu-Yao Zhang, Wei Zhang, Yao Lu, Xiuli Li, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning based approaches have achieved significant progresses in different tasks like classification, detection, segmentation, and so on. Ensemble learning is widely known to further improve performance by combining multiple complementary models. It is easy to apply ensemble learning for classification tasks, for example, based on averaging, voting, or other methods. However, for other tasks (like object detection) where the outputs are varying in quantity and unable to be simply compared, the ensemble of multiple models become difficult. In this paper, we propose a new method called Predictive Ensemble Learning (PEL), based on powerful predictive ability of deep neural networks, to directly predict the best performing model among a pool of base models for each test example, thus transforming ensemble learning to a traditional classification task. Taking scene text detection as the application, where no suitable ensemble learning strategy exists, PEL can significantly improve the performance, compared to either individual state-of-the-art models, or the fusion of multiple models by non-maximum suppression. Experimental results show the possibility and potential of PEL in predicting different models' performance based only on a query example, which can be extended for ensemble learning in many other complex tasks.



### Learning to Convolve: A Generalized Weight-Tying Approach
- **Arxiv ID**: http://arxiv.org/abs/1905.04663v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.04663v1)
- **Published**: 2019-05-12 07:55:59+00:00
- **Updated**: 2019-05-12 07:55:59+00:00
- **Authors**: Nichita Diaconu, Daniel E Worrall
- **Comment**: Accepted to ICML 2019
- **Journal**: None
- **Summary**: Recent work (Cohen & Welling, 2016) has shown that generalizations of convolutions, based on group theory, provide powerful inductive biases for learning. In these generalizations, filters are not only translated but can also be rotated, flipped, etc. However, coming up with exact models of how to rotate a 3 x 3 filter on a square pixel-grid is difficult. In this paper, we learn how to transform filters for use in the group convolution, focussing on roto-translation. For this, we learn a filter basis and all rotated versions of that filter basis. Filters are then encoded by a set of rotation invariant coefficients. To rotate a filter, we switch the basis. We demonstrate we can produce feature maps with low sensitivity to input rotations, while achieving high performance on MNIST and CIFAR-10.



### On Flow Profile Image for Video Representation
- **Arxiv ID**: http://arxiv.org/abs/1905.04668v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04668v1)
- **Published**: 2019-05-12 08:48:06+00:00
- **Updated**: 2019-05-12 08:48:06+00:00
- **Authors**: Mohammadreza Babaee, David Full, Gerhard Rigoll
- **Comment**: None
- **Journal**: None
- **Summary**: Video representation is a key challenge in many computer vision applications such as video classification, video captioning, and video surveillance. In this paper, we propose a novel approach for video representation that captures meaningful information including motion and appearance from a sequence of video frames and compacts it into a single image. To this end, we compute the optical flow and use it in a least squares optimization to find a new image, the so-called Flow Profile Image (FPI). This image encodes motions as well as foreground appearance information while background information is removed. The quality of this image is validated in activity recognition experiments and the results are compared with other video representation techniques such as dynamic images [1] and eigen images [2]. The experimental results as well as visual quality confirm that FPIs can be successfully used in video processing applications.



### Hierarchy Composition GAN for High-fidelity Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1905.04693v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04693v5)
- **Published**: 2019-05-12 11:11:24+00:00
- **Updated**: 2023-04-19 20:58:17+00:00
- **Authors**: Fangneng Zhan, Jiaxing Huang, Shijian Lu
- **Comment**: This work has been merged into another project
- **Journal**: None
- **Summary**: Despite the rapid progress of generative adversarial networks (GANs) in image synthesis in recent years, the existing image synthesis approaches work in either geometry domain or appearance domain alone which often introduces various synthesis artifacts. This paper presents an innovative Hierarchical Composition GAN (HIC-GAN) that incorporates image synthesis in geometry and appearance domains into an end-to-end trainable network and achieves superior synthesis realism in both domains simultaneously. We design an innovative hierarchical composition mechanism that is capable of learning realistic composition geometry and handling occlusions while multiple foreground objects are involved in image composition. In addition, we introduce a novel attention mask mechanism that guides to adapt the appearance of foreground objects which also helps to provide better training reference for learning in geometry domain. Extensive experiments on scene text image synthesis, portrait editing and indoor rendering tasks show that the proposed HIC-GAN achieves superior synthesis performance qualitatively and quantitatively.



### Ensemble Super-Resolution with A Reference Dataset
- **Arxiv ID**: http://arxiv.org/abs/1905.04696v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04696v1)
- **Published**: 2019-05-12 11:22:18+00:00
- **Updated**: 2019-05-12 11:22:18+00:00
- **Authors**: Junjun Jiang, Yi Yu, Zheng Wang, Suhua Tang, Ruimin Hu, Jiayi Ma
- **Comment**: 14 pages, 11 figures
- **Journal**: None
- **Summary**: By developing sophisticated image priors or designing deep(er) architectures, a variety of image Super-Resolution (SR) approaches have been proposed recently and achieved very promising performance. A natural question that arises is whether these methods can be reformulated into a unifying framework and whether this framework assists in SR reconstruction? In this paper, we present a simple but effective single image SR method based on ensemble learning, which can produce a better performance than that could be obtained from any of SR methods to be ensembled (or called component super-resolvers). Based on the assumption that better component super-resolver should have larger ensemble weight when performing SR reconstruction, we present a Maximum A Posteriori (MAP) estimation framework for the inference of optimal ensemble weights. Specially, we introduce a reference dataset, which is composed of High-Resolution (HR) and Low-Resolution (LR) image pairs, to measure the super-resolution abilities (prior knowledge) of different component super-resolvers. To obtain the optimal ensemble weights, we propose to incorporate the reconstruction constraint, which states that the degenerated HR image should be equal to the LR observation one, as well as the prior knowledge of ensemble weights into the MAP estimation framework. Moreover, the proposed optimization problem can be solved by an analytical solution. We study the performance of the proposed method by comparing with different competitive approaches, including four state-of-the-art non-deep learning based methods, four latest deep learning based methods and one ensemble learning based method, and prove its effectiveness and superiority on three public datasets.



### Data augmentation in microscopic images for material data mining
- **Arxiv ID**: http://arxiv.org/abs/1905.04711v3
- **DOI**: 10.1038/s41524-020-00392-6
- **Categories**: **cond-mat.mtrl-sci**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.04711v3)
- **Published**: 2019-05-12 12:37:57+00:00
- **Updated**: 2019-10-28 04:12:43+00:00
- **Authors**: Boyuan Ma, Xiaoyan Wei, Chuni Liu, Xiaojuan Ban, Haiyou Huang, Hao Wang, Weihua Xue, Stephen Wu, Mingfei Gao, Qing Shen, Adnan Omer Abuassba, Haokai Shen, Yanjing Su
- **Comment**: 17 pages, technical report
- **Journal**: npj computational materials 2020
- **Summary**: Recent progress in material data mining has been driven by high-capacity models trained on large datasets. However, collecting experimental data (real data) has been extremely costly since the amount of human effort and expertise required. Here, we develop a novel transfer learning strategy to address small or insufficient data problem. This strategy realizes the fusion of real and simulated data, and the augmentation of training data in data mining procedure. For a specific task of image segmentation, this strategy can generate synthetic images by fusing physical mechanism of simulated images and "image style" of real images. The result shows that the model trained with the acquired synthetic images and 35% of the real images outperforms the model trained on all real images. As the time required to generate synthetic data is almost negligible, this strategy is able to reduce the time cost of real data preparation by roughly 65%.



### Some Research Problems in Biometrics: The Future Beckons
- **Arxiv ID**: http://arxiv.org/abs/1905.04717v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1905.04717v1)
- **Published**: 2019-05-12 13:06:17+00:00
- **Updated**: 2019-05-12 13:06:17+00:00
- **Authors**: Arun Ross, Sudipta Banerjee, Cunjian Chen, Anurag Chowdhury, Vahid Mirjalili, Renu Sharma, Thomas Swearingen, Shivangi Yadav
- **Comment**: 8 pages, 12 figures, ICB-2019
- **Journal**: None
- **Summary**: The need for reliably determining the identity of a person is critical in a number of different domains ranging from personal smartphones to border security; from autonomous vehicles to e-voting; from tracking child vaccinations to preventing human trafficking; from crime scene investigation to personalization of customer service. Biometrics, which entails the use of biological attributes such as face, fingerprints and voice for recognizing a person, is being increasingly used in several such applications. While biometric technology has made rapid strides over the past decade, there are several fundamental issues that are yet to be satisfactorily resolved. In this article, we will discuss some of these issues and enumerate some of the exciting challenges in this field.



### One-Shot Image-to-Image Translation via Part-Global Learning with a Multi-adversarial Framework
- **Arxiv ID**: http://arxiv.org/abs/1905.04729v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.04729v1)
- **Published**: 2019-05-12 14:29:57+00:00
- **Updated**: 2019-05-12 14:29:57+00:00
- **Authors**: Ziqiang Zheng, Zhibin Yu, Haiyong Zheng, Yang Yang, Heng Tao Shen
- **Comment**: 9 pages, 13 figures
- **Journal**: None
- **Summary**: It is well known that humans can learn and recognize objects effectively from several limited image samples. However, learning from just a few images is still a tremendous challenge for existing main-stream deep neural networks. Inspired by analogical reasoning in the human mind, a feasible strategy is to translate the abundant images of a rich source domain to enrich the relevant yet different target domain with insufficient image data. To achieve this goal, we propose a novel, effective multi-adversarial framework (MA) based on part-global learning, which accomplishes one-shot cross-domain image-to-image translation. In specific, we first devise a part-global adversarial training scheme to provide an efficient way for feature extraction and prevent discriminators being over-fitted. Then, a multi-adversarial mechanism is employed to enhance the image-to-image translation ability to unearth the high-level semantic representation. Moreover, a balanced adversarial loss function is presented, which aims to balance the training data and stabilize the training process. Extensive experiments demonstrate that the proposed approach can obtain impressive results on various datasets between two extremely imbalanced image domains and outperform state-of-the-art methods on one-shot image-to-image translation.



### Flat Metric Minimization with Applications in Generative Modeling
- **Arxiv ID**: http://arxiv.org/abs/1905.04730v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.04730v1)
- **Published**: 2019-05-12 14:37:07+00:00
- **Updated**: 2019-05-12 14:37:07+00:00
- **Authors**: Thomas Möllenhoff, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: We take the novel perspective to view data not as a probability distribution but rather as a current. Primarily studied in the field of geometric measure theory, $k$-currents are continuous linear functionals acting on compactly supported smooth differential forms and can be understood as a generalized notion of oriented $k$-dimensional manifold. By moving from distributions (which are $0$-currents) to $k$-currents, we can explicitly orient the data by attaching a $k$-dimensional tangent plane to each sample point. Based on the flat metric which is a fundamental distance between currents, we derive FlatGAN, a formulation in the spirit of generative adversarial networks but generalized to $k$-currents. In our theoretical contribution we prove that the flat metric between a parametrized current and a reference current is Lipschitz continuous in the parameters. In experiments, we show that the proposed shift to $k>0$ leads to interpretable and disentangled latent representations which behave equivariantly to the specified oriented tangent planes.



### Social Relation Recognition in Egocentric Photostreams
- **Arxiv ID**: http://arxiv.org/abs/1905.04734v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04734v1)
- **Published**: 2019-05-12 15:20:26+00:00
- **Updated**: 2019-05-12 15:20:26+00:00
- **Authors**: Emanuel Sanchez Aimar, Petia Radeva, Mariella Dimiccoli
- **Comment**: Accepted at ICIP 2019
- **Journal**: None
- **Summary**: This paper proposes an approach to automatically categorize the social interactions of a user wearing a photo-camera 2fpm, by relying solely on what the camera is seeing. The problem is challenging due to the overwhelming complexity of social life and the extreme intra-class variability of social interactions captured under unconstrained conditions. We adopt the formalization proposed in Bugental's social theory, that groups human relations into five social domains with related categories. Our method is a new deep learning architecture that exploits the hierarchical structure of the label space and relies on a set of social attributes estimated at frame level to provide a semantic representation of social interactions. Experimental results on the new EgoSocialRelation dataset demonstrate the effectiveness of our proposal.



### Object Detection in Specific Traffic Scenes using YOLOv2
- **Arxiv ID**: http://arxiv.org/abs/1905.04740v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04740v1)
- **Published**: 2019-05-12 16:38:19+00:00
- **Updated**: 2019-05-12 16:38:19+00:00
- **Authors**: Shouyu Wang, Weitao Tang
- **Comment**: None
- **Journal**: None
- **Summary**: object detection framework plays crucial role in autonomous driving. In this paper, we introduce the real-time object detection framework called You Only Look Once (YOLOv1) and the related improvements of YOLOv2. We further explore the capability of YOLOv2 by implementing its pre-trained model to do the object detecting tasks in some specific traffic scenes. The four artificially designed traffic scenes include single-car, single-person, frontperson-rearcar and frontcar-rearperson.



### Approximated Oracle Filter Pruning for Destructive CNN Width Optimization
- **Arxiv ID**: http://arxiv.org/abs/1905.04748v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.04748v1)
- **Published**: 2019-05-12 17:14:19+00:00
- **Updated**: 2019-05-12 17:14:19+00:00
- **Authors**: Xiaohan Ding, Guiguang Ding, Yuchen Guo, Jungong Han, Chenggang Yan
- **Comment**: ICML 2019
- **Journal**: None
- **Summary**: It is not easy to design and run Convolutional Neural Networks (CNNs) due to: 1) finding the optimal number of filters (i.e., the width) at each layer is tricky, given an architecture; and 2) the computational intensity of CNNs impedes the deployment on computationally limited devices. Oracle Pruning is designed to remove the unimportant filters from a well-trained CNN, which estimates the filters' importance by ablating them in turn and evaluating the model, thus delivers high accuracy but suffers from intolerable time complexity, and requires a given resulting width but cannot automatically find it. To address these problems, we propose Approximated Oracle Filter Pruning (AOFP), which keeps searching for the least important filters in a binary search manner, makes pruning attempts by masking out filters randomly, accumulates the resulting errors, and finetunes the model via a multi-path framework. As AOFP enables simultaneous pruning on multiple layers, we can prune an existing very deep CNN with acceptable time cost, negligible accuracy drop, and no heuristic knowledge, or re-design a model which exerts higher accuracy and faster inference.



### Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints
- **Arxiv ID**: http://arxiv.org/abs/1905.04753v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.04753v4)
- **Published**: 2019-05-12 17:49:49+00:00
- **Updated**: 2020-06-30 00:45:59+00:00
- **Authors**: Mengtian Li, Ersin Yumer, Deva Ramanan
- **Comment**: ICLR 2020. Project page with code is at
  http://www.cs.cmu.edu/~mengtial/proj/budgetnn/
- **Journal**: None
- **Summary**: In most practical settings and theoretical analyses, one assumes that a model can be trained until convergence. However, the growing complexity of machine learning datasets and models may violate such assumptions. Indeed, current approaches for hyper-parameter tuning and neural architecture search tend to be limited by practical resource constraints. Therefore, we introduce a formal setting for studying training under the non-asymptotic, resource-constrained regime, i.e., budgeted training. We analyze the following problem: "given a dataset, algorithm, and fixed resource budget, what is the best achievable performance?" We focus on the number of optimization iterations as the representative resource. Under such a setting, we show that it is critical to adjust the learning rate schedule according to the given budget. Among budget-aware learning schedules, we find simple linear decay to be both robust and high-performing. We support our claim through extensive experiments with state-of-the-art models on ImageNet (image classification), Kinetics (video classification), MS COCO (object detection and instance segmentation), and Cityscapes (semantic segmentation). We also analyze our results and find that the key to a good schedule is budgeted convergence, a phenomenon whereby the gradient vanishes at the end of each allowed budget. We also revisit existing approaches for fast convergence and show that budget-aware learning schedules readily outperform such approaches under (the practical but under-explored) budgeted training setting.



### NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding
- **Arxiv ID**: http://arxiv.org/abs/1905.04757v2
- **DOI**: 10.1109/TPAMI.2019.2916873
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04757v2)
- **Published**: 2019-05-12 17:58:55+00:00
- **Updated**: 2019-06-10 07:04:29+00:00
- **Authors**: Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang, Ling-Yu Duan, Alex C. Kot
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI)
- **Journal**: None
- **Summary**: Research on depth-based human activity analysis achieved outstanding performance and demonstrated the effectiveness of 3D representation for action recognition. The existing depth-based and RGB+D-based action recognition benchmarks have a number of limitations, including the lack of large-scale training samples, realistic number of distinct class categories, diversity in camera views, varied environmental conditions, and variety of human subjects. In this work, we introduce a large-scale dataset for RGB+D human action recognition, which is collected from 106 distinct subjects and contains more than 114 thousand video samples and 8 million frames. This dataset contains 120 different action classes including daily, mutual, and health-related activities. We evaluate the performance of a series of existing 3D activity analysis methods on this dataset, and show the advantage of applying deep learning methods for 3D-based human action recognition. Furthermore, we investigate a novel one-shot 3D activity recognition problem on our dataset, and a simple yet effective Action-Part Semantic Relevance-aware (APSR) framework is proposed for this task, which yields promising results for recognition of the novel action classes. We believe the introduction of this large-scale dataset will enable the community to apply, adapt, and develop various data-hungry learning techniques for depth-based and RGB+D-based human activity understanding. [The dataset is available at: http://rose1.ntu.edu.sg/Datasets/actionRecognition.asp]



### Structure from Articulated Motion: Accurate and Stable Monocular 3D Reconstruction without Training Data
- **Arxiv ID**: http://arxiv.org/abs/1905.04789v2
- **DOI**: 10.3390/s19204603
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04789v2)
- **Published**: 2019-05-12 20:33:49+00:00
- **Updated**: 2019-11-12 13:08:08+00:00
- **Authors**: Onorina Kovalenko, Vladislav Golyanik, Jameel Malik, Ahmed Elhayek, Didier Stricker
- **Comment**: 21 pages, 8 figures, 2 tables
- **Journal**: Sensors 2019, 19(20), 4603
- **Summary**: Recovery of articulated 3D structure from 2D observations is a challenging computer vision problem with many applications. Current learning-based approaches achieve state-of-the-art accuracy on public benchmarks but are restricted to specific types of objects and motions covered by the training datasets. Model-based approaches do not rely on training data but show lower accuracy on these datasets. In this paper, we introduce a model-based method called Structure from Articulated Motion (SfAM), which can recover multiple object and motion types without training on extensive data collections. At the same time, it performs on par with learning-based state-of-the-art approaches on public benchmarks and outperforms previous non-rigid structure from motion (NRSfM) methods. SfAM is built upon a general-purpose NRSfM technique while integrating a soft spatio-temporal constraint on the bone lengths. We use alternating optimization strategy to recover optimal geometry (i.e., bone proportions) together with 3D joint positions by enforcing the bone lengths consistency over a series of frames. SfAM is highly robust to noisy 2D annotations, generalizes to arbitrary objects and does not rely on training data, which is shown in extensive experiments on public benchmarks and real video sequences. We believe that it brings a new perspective on the domain of monocular 3D recovery of articulated structures, including human motion capture.



### DeepIlluminance: Contextual Illuminance Estimation via Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.04791v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.04791v2)
- **Published**: 2019-05-12 20:43:59+00:00
- **Updated**: 2019-07-11 07:34:03+00:00
- **Authors**: Jun Zhang, Tong Zheng, Shengping Zhang, Meng Wang
- **Comment**: 12 pages, 7 figures
- **Journal**: None
- **Summary**: Computational color constancy refers to the estimation of the scene illumination and makes the perceived color relatively stable under varying illumination. In the past few years, deep Convolutional Neural Networks (CNNs) have delivered superior performance in illuminant estimation. Several representative methods formulate it as a multi-label prediction problem by learning the local appearance of image patches using CNNs. However, these approaches inevitably make incorrect estimations for the ambiguous patches affected by their neighborhood contexts. Inaccurate local estimates are likely to bring in degraded performance when combining into a global prediction. To address the above issues, we propose a contextual deep network for patch-based illuminant estimation equipped with refinement. First, the contextual net with a center-surround architecture extracts local contextual features from image patches, and generates initial illuminant estimates and the corresponding color corrected patches. The patches are sampled based on the observation that pixels with large color differences describe the illumination well. Then, the refinement net integrates the input patches with the corrected patches in conjunction with the use of intermediate features to improve the performance. To train such a network with numerous parameters, we propose a stage-wise training strategy, in which the features and the predicted illuminant from previous stages are provided to the next learning stage with more finer estimates recovered. Experiments show that our approach obtains competitive performance on two illuminant estimation benchmarks.



### Video Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.04804v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04804v4)
- **Published**: 2019-05-12 22:42:17+00:00
- **Updated**: 2019-08-16 17:49:13+00:00
- **Authors**: Linjie Yang, Yuchen Fan, Ning Xu
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: In this paper we present a new computer vision task, named video instance segmentation. The goal of this new task is simultaneous detection, segmentation and tracking of instances in videos. In words, it is the first time that the image instance segmentation problem is extended to the video domain. To facilitate research on this new task, we propose a large-scale benchmark called YouTube-VIS, which consists of 2883 high-resolution YouTube videos, a 40-category label set and 131k high-quality instance masks. In addition, we propose a novel algorithm called MaskTrack R-CNN for this task. Our new method introduces a new tracking branch to Mask R-CNN to jointly perform the detection, segmentation and tracking tasks simultaneously. Finally, we evaluate the proposed method and several strong baselines on our new dataset. Experimental results clearly demonstrate the advantages of the proposed algorithm and reveal insight for future improvement. We believe the video instance segmentation task will motivate the community along the line of research for video understanding.



