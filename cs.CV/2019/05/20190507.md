# Arxiv Papers in cs.CV on 2019-05-07
### Are Graph Neural Networks Miscalibrated?
- **Arxiv ID**: http://arxiv.org/abs/1905.02296v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.02296v2)
- **Published**: 2019-05-07 00:05:31+00:00
- **Updated**: 2019-06-03 14:10:50+00:00
- **Authors**: Leonardo Teixeira, Brian Jalaian, Bruno Ribeiro
- **Comment**: Presented at the ICML 2019 Workshop on Learning and Reasoning with
  Graph-Structured Data
- **Journal**: None
- **Summary**: Graph Neural Networks (GNNs) have proven to be successful in many classification tasks, outperforming previous state-of-the-art methods in terms of accuracy. However, accuracy alone is not enough for high-stakes decision making. Decision makers want to know the likelihood that a specific GNN prediction is correct. For this purpose, obtaining calibrated models is essential. In this work, we perform an empirical evaluation of the calibration of state-of-the-art GNNs on multiple datasets. Our experiments show that GNNs can be calibrated in some datasets but also badly miscalibrated in others, and that state-of-the-art calibration methods are helpful but do not fix the problem.



### Automatic 4D Facial Expression Recognition via Collaborative Cross-domain Dynamic Image Network
- **Arxiv ID**: http://arxiv.org/abs/1905.02319v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02319v2)
- **Published**: 2019-05-07 01:44:26+00:00
- **Updated**: 2020-02-07 09:32:31+00:00
- **Authors**: Muzammil Behzad, Nhat Vo, Xiaobai Li, Guoying Zhao
- **Comment**: Published in the 30th British Machine Vision Conference (BMVC) 2019
- **Journal**: None
- **Summary**: This paper proposes a novel 4D Facial Expression Recognition (FER) method using Collaborative Cross-domain Dynamic Image Network (CCDN). Given a 4D data of face scans, we first compute its geometrical images, and then combine their correlated information in the proposed cross-domain image representations. The acquired set is then used to generate cross-domain dynamic images (CDI) via rank pooling that encapsulates facial deformations over time in terms of a single image. For the training phase, these CDIs are fed into an end-to-end deep learning model, and the resultant predictions collaborate over multi-views for performance gain in expression classification. Furthermore, we propose a 4D augmentation scheme that not only expands the training data scale but also introduces significant facial muscle movement patterns to improve the FER performance. Results from extensive experiments on the commonly used BU-4DFE dataset under widely adopted settings show that our proposed method outperforms the state-of-the-art 4D FER methods by achieving an accuracy of 96.5% indicating its effectiveness.



### Spatially Constrained GAN for Face and Fashion Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1905.02320v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02320v2)
- **Published**: 2019-05-07 02:00:03+00:00
- **Updated**: 2021-12-06 08:02:15+00:00
- **Authors**: Songyao Jiang, Hongfu Liu, Yue Wu, Yun Fu
- **Comment**: Accepted to IEEE International Conference on Automatic Face and
  Gesture Recognition (FG), 2021
- **Journal**: None
- **Summary**: Image generation has raised tremendous attention in both academic and industrial areas, especially for the conditional and target-oriented image generation, such as criminal portrait and fashion design. Although the current studies have achieved preliminary results along this direction, they always focus on class labels as the condition where spatial contents are randomly generated from latent vectors. Edge details are usually blurred since spatial information is difficult to preserve. In light of this, we propose a novel Spatially Constrained Generative Adversarial Network (SCGAN), which decouples the spatial constraints from the latent vector and makes these constraints feasible as additional controllable signals. To enhance the spatial controllability, a generator network is specially designed to take a semantic segmentation, a latent vector and an attribute-level label as inputs step by step. Besides, a segmentor network is constructed to impose spatial constraints on the generator. Experimentally, we provide both visual and quantitative results on CelebA and DeepFashion datasets, and demonstrate that the proposed SCGAN is very effective in controlling the spatial contents as well as generating high-quality images.



### Semantic Adversarial Network for Zero-Shot Sketch-Based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1905.02327v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02327v2)
- **Published**: 2019-05-07 02:20:42+00:00
- **Updated**: 2019-10-18 02:49:59+00:00
- **Authors**: Xinxun Xu, Hao Wang, Leida Li, Cheng Deng
- **Comment**: There is a big problem with the paper and I hope it can be retracted
- **Journal**: None
- **Summary**: Zero-shot sketch-based image retrieval (ZS-SBIR) is a specific cross-modal retrieval task for retrieving natural images with free-hand sketches under zero-shot scenario. Previous works mostly focus on modeling the correspondence between images and sketches or synthesizing image features with sketch features. However, both of them ignore the large intra-class variance of sketches, thus resulting in unsatisfactory retrieval performance. In this paper, we propose a novel end-to-end semantic adversarial approach for ZS-SBIR. Specifically, we devise a semantic adversarial module to maximize the consistency between learned semantic features and category-level word vectors. Moreover, to preserve the discriminability of synthesized features within each training category, a triplet loss is employed for the generative module. Additionally, the proposed model is trained in an end-to-end strategy to exploit better semantic features suitable for ZS-SBIR. Extensive experiments conducted on two large-scale popular datasets demonstrate that our proposed approach remarkably outperforms state-of-the-art approaches by more than 12\% on Sketchy dataset and about 3\% on TU-Berlin dataset in the retrieval.



### Fully Parallel Architecture for Semi-global Stereo Matching with Refined Rank Method
- **Arxiv ID**: http://arxiv.org/abs/1905.03716v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03716v1)
- **Published**: 2019-05-07 02:33:37+00:00
- **Updated**: 2019-05-07 02:33:37+00:00
- **Authors**: Yiwu Yao, Yuhua Cheng
- **Comment**: stereo matching; SGM; Rank SAD; fully parallel architecture
- **Journal**: None
- **Summary**: Fully parallel architecture at disparity-level for efficient semi-global matching (SGM) with refined rank method is presented. The improved SGM algorithm is implemented with the non-parametric unified rank model which is the combination of Rank filter/AD and Rank SAD. Rank SAD is a novel definition by introducing the constraints of local image structure into the rank method. As a result, the unified rank model with Rank SAD can make up for the defects of Rank filter/AD. Experimental results show both excellent subjective quality and objective performance of the refined SGM algorithm. The fully parallel construction for hardware implementation of SGM is architected with reasonable strategies at disparity-level. The parallelism of the data-stream allows proper throughput for specific applications with acceptable maximum frequency. The results of RTL emulation and synthesis ensure that the proposed parallel architecture is suitable for VLSI implementation.



### Variational Representation Learning for Vehicle Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1905.02343v1
- **DOI**: 10.1109/ICIP.2019.8803366
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02343v1)
- **Published**: 2019-05-07 03:42:31+00:00
- **Updated**: 2019-05-07 03:42:31+00:00
- **Authors**: Saghir Ahmed Saghir Alfasly, Yongjian Hu, Tiancai Liang, Xiaofeng Jin, Qingli Zhao, Beibei Liu
- **Comment**: 5 pages, 1 figure, Accepted at ICIP 2019
- **Journal**: ICIP (2019), pp. 3118-3122
- **Summary**: Vehicle Re-identification is attracting more and more attention in recent years. One of the most challenging problems is to learn an efficient representation for a vehicle from its multi-viewpoint images. Existing methods tend to derive features of dimensions ranging from thousands to tens of thousands. In this work we proposed a deep learning based framework that can lead to an efficient representation of vehicles. While the dimension of the learned features can be as low as 256, experiments on different datasets show that the Top-1 and Top-5 retrieval accuracies exceed multiple state-of-the-art methods. The key to our framework is two-fold. Firstly, variational feature learning is employed to generate variational features which are more discriminating. Secondly, long short-term memory (LSTM) is used to learn the relationship among different viewpoints of a vehicle. The LSTM also plays as an encoder to downsize the features.



### Show, Price and Negotiate: A Negotiator with Online Value Look-Ahead
- **Arxiv ID**: http://arxiv.org/abs/1905.03721v2
- **DOI**: 10.1109/TMM.2021.3065169
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03721v2)
- **Published**: 2019-05-07 06:20:59+00:00
- **Updated**: 2021-03-12 07:10:54+00:00
- **Authors**: Amin Parvaneh, Ehsan Abbasnejad, Qi Wu, Javen Qinfeng Shi, Anton van den Hengel
- **Comment**: published in IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Negotiation, as an essential and complicated aspect of online shopping, is still challenging for an intelligent agent. To that end, we propose the Price Negotiator, a modular deep neural network that addresses the unsolved problems in recent studies by (1) considering images of the items as a crucial, though neglected, source of information in a negotiation, (2) heuristically finding the most similar items from an external online source to predict the potential value and an acceptable agreement price, (3) predicting a general price-based action at each turn which is fed into the language generator to output the supporting natural language, and (4) adjusting the prices based on the predicted actions. Empirically, we show that our model, that is trained in both supervised and reinforcement learning setting, significantly improves negotiation on the CraigslistBargain dataset, in terms of the agreement price, price consistency, and dialogue quality.



### Accurate Tissue Interface Segmentation via Adversarial Pre-Segmentation of Anterior Segment OCT Images
- **Arxiv ID**: http://arxiv.org/abs/1905.02378v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.02378v1)
- **Published**: 2019-05-07 06:44:56+00:00
- **Updated**: 2019-05-07 06:44:56+00:00
- **Authors**: Jiahong Ouyang, Tejas Sudharshan Mathai, Kira Lathrop, John Galeotti
- **Comment**: First two authors contributed equally. Biomedical Optics Express
  journal submission. 27 pages, 15 figures. Submitted to the journal on May 6th
  2019 at 11:38pm
- **Journal**: None
- **Summary**: Optical Coherence Tomography (OCT) is an imaging modality that has been widely adopted for visualizing corneal, retinal and limbal tissue structure with micron resolution. It can be used to diagnose pathological conditions of the eye, and for developing pre-operative surgical plans. In contrast to the posterior retina, imaging the anterior tissue structures, such as the limbus and cornea, results in B-scans that exhibit increased speckle noise patterns and imaging artifacts. These artifacts, such as shadowing and specularity, pose a challenge during the analysis of the acquired volumes as they substantially obfuscate the location of tissue interfaces. To deal with the artifacts and speckle noise patterns and accurately segment the shallowest tissue interface, we propose a cascaded neural network framework, which comprises of a conditional Generative Adversarial Network (cGAN) and a Tissue Interface Segmentation Network (TISN). The cGAN pre-segments OCT B-scans by removing undesired specular artifacts and speckle noise patterns just above the shallowest tissue interface, and the TISN combines the original OCT image with the pre-segmentation to segment the shallowest interface. We show the applicability of the cascaded framework to corneal datasets, demonstrate that it precisely segments the shallowest corneal interface, and also show its generalization capacity to limbal datasets. We also propose a hybrid framework, wherein the cGAN pre-segmentation is passed to a traditional image analysis-based segmentation algorithm, and describe the improved segmentation performance. To the best of our knowledge, this is the first approach to remove severe specular artifacts and speckle noise patterns (prior to the shallowest interface) that affects the interpretation of anterior segment OCT datasets, thereby resulting in the accurate segmentation of the shallowest tissue interface.



### On Applying Machine Learning/Object Detection Models for Analysing Digitally Captured Physical Prototypes from Engineering Design Projects
- **Arxiv ID**: http://arxiv.org/abs/1905.03697v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.03697v1)
- **Published**: 2019-05-07 07:33:53+00:00
- **Updated**: 2019-05-07 07:33:53+00:00
- **Authors**: Jorgen F. Erichsen, Sampsa Kohtala, Martin Steinert, Torgeir Welo
- **Comment**: 13 pages, 4 tables, 3 figures
- **Journal**: None
- **Summary**: While computer vision has received increasing attention in computer science over the last decade, there are few efforts in applying this to leverage engineering design research. Existing datasets and technologies allow researchers to capture and access more observations and video files, hence analysis is becoming a limiting factor. Therefore, this paper is investigating the application of machine learning, namely object detection methods to aid in the analysis of physical porotypes. With access to a large dataset of digitally captured physical prototypes from early-stage development projects (5950 images from 850 prototypes), the authors investigate applications that can be used for analysing this dataset. The authors retrained two pre-trained object detection models from two known frameworks, the TensorFlow Object Detection API and Darknet, using custom image sets of images of physical prototypes. As a result, a proof-of-concept of four trained models are presented; two models for detecting samples of wood-based sheet materials and two models for detecting samples containing microcontrollers. All models are evaluated using standard metrics for object detection model performance and the applicability of using object detection models in engineering design research is discussed. Results indicate that the models can successfully classify the type of material and type of pre-made component, respectively. However, more work is needed to fully integrate object detection models in the engineering design analysis workflow. The authors also extrapolate that the use of object detection for analysing images of physical prototypes will substantially reduce the effort required for analysing large datasets in engineering design research.



### Intentional Attention Mask Transformation for Robust CNN Classification
- **Arxiv ID**: http://arxiv.org/abs/1905.02719v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.02719v2)
- **Published**: 2019-05-07 08:16:46+00:00
- **Updated**: 2019-05-20 06:22:10+00:00
- **Authors**: Masanari Kimura, Masayuki Tanaka
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1904.13078
- **Journal**: None
- **Summary**: Convolutional Neural Networks have achieved impressive results in various tasks, but interpreting the internal mechanism is a challenging problem. To tackle this problem, we exploit a multi-channel attention mechanism in feature space. Our network architecture allows us to obtain an attention mask for each feature while existing CNN visualization methods provide only a common attention mask for all features. We apply the proposed multi-channel attention mechanism to multi-attribute recognition task. We can obtain different attention mask for each feature and for each attribute. Those analyses give us deeper insight into the feature space of CNNs. Furthermore, our proposed attention mechanism naturally derives a method for improving the robustness of CNNs. From the observation of feature space based on the proposed attention mask, we demonstrate that we can obtain robust CNNs by intentionally emphasizing features that are important for attributes. The experimental results for the benchmark dataset show that the proposed method gives high human interpretability while accurately grasping the attributes of the data, and improves network robustness.



### Convolutional Neural Networks Considering Local and Global features for Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1905.02899v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1905.02899v1)
- **Published**: 2019-05-07 08:20:30+00:00
- **Updated**: 2019-05-07 08:20:30+00:00
- **Authors**: Yuma Kinoshita, Hitoshi Kiya
- **Comment**: To appear in Proc. ICIP2019. arXiv admin note: text overlap with
  arXiv:1901.05686
- **Journal**: None
- **Summary**: In this paper, we propose a novel convolutional neural network (CNN) architecture considering both local and global features for image enhancement. Most conventional image enhancement methods, including Retinex-based methods, cannot restore lost pixel values caused by clipping and quantizing. CNN-based methods have recently been proposed to solve the problem, but they still have a limited performance due to network architectures not handling global features. To handle both local and global features, the proposed architecture consists of three networks: a local encoder, a global encoder, and a decoder. In addition, high dynamic range (HDR) images are used for generating training data for our networks. The use of HDR images makes it possible to train CNNs with better-quality images than images directly captured with cameras. Experimental results show that the proposed method can produce higher-quality images than conventional image enhancement methods including CNN-based methods, in terms of various objective quality metrics: TMQI, entropy, NIQE, and BRISQUE.



### A Deep Framework for Bone Age Assessment based on Finger Joint Localization
- **Arxiv ID**: http://arxiv.org/abs/1905.13124v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.13124v2)
- **Published**: 2019-05-07 08:38:28+00:00
- **Updated**: 2019-06-13 13:55:41+00:00
- **Authors**: Xiaoman Zhang, Ziyuan Zhao, Cen Chen, Songyou Peng, Min Wu, Zhongyao Cheng, Singee Teo, Le Zhang, Zeng Zeng
- **Comment**: Some changes will be made
- **Journal**: None
- **Summary**: Bone age assessment is an important clinical trial to measure skeletal child maturity and diagnose of growth disorders. Conventional approaches such as the Tanner-Whitehouse (TW) and Greulich and Pyle (GP) may not perform well due to their large inter-observer and intra-observer variations. In this paper, we propose a finger joint localization strategy to filter out most non-informative parts of images. When combining with the conventional full image-based deep network, we observe a much-improved performance. % Our approach utilizes full hand and specific joints images for skeletal maturity prediction. In this study, we applied powerful deep neural network and explored a process in the forecast of skeletal bone age with the specifically combine joints images to increase the performance accuracy compared with the whole hand images.



### Remote Photoplethysmograph Signal Measurement from Facial Videos Using Spatio-Temporal Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.02419v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02419v2)
- **Published**: 2019-05-07 09:02:37+00:00
- **Updated**: 2019-07-31 13:50:38+00:00
- **Authors**: Zitong Yu, Xiaobai Li, Guoying Zhao
- **Comment**: Accepted by BMVC2019
- **Journal**: None
- **Summary**: Recent studies demonstrated that the average heart rate (HR) can be measured from facial videos based on non-contact remote photoplethysmography (rPPG). However for many medical applications (e.g., atrial fibrillation (AF) detection) knowing only the average HR is not sufficient, and measuring precise rPPG signals from face for heart rate variability (HRV) analysis is needed. Here we propose an rPPG measurement method, which is the first work to use deep spatio-temporal networks for reconstructing precise rPPG signals from raw facial videos. With the constraint of trend-consistency with ground truth pulse curves, our method is able to recover rPPG signals with accurate pulse peaks. Comprehensive experiments are conducted on two benchmark datasets, and results demonstrate that our method can achieve superior performance on both HR and HRV levels comparing to the state-of-the-art methods. We also achieve promising results of using reconstructed rPPG signals for AF detection and emotion recognition.



### Representation of White- and Black-Box Adversarial Examples in Deep Neural Networks and Humans: A Functional Magnetic Resonance Imaging Study
- **Arxiv ID**: http://arxiv.org/abs/1905.02422v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.02422v1)
- **Published**: 2019-05-07 09:10:48+00:00
- **Updated**: 2019-05-07 09:10:48+00:00
- **Authors**: Chihye Han, Wonjun Yoon, Gihyun Kwon, Seungkyu Nam, Daeshik Kim
- **Comment**: Copyright 2019 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: None
- **Summary**: The recent success of brain-inspired deep neural networks (DNNs) in solving complex, high-level visual tasks has led to rising expectations for their potential to match the human visual system. However, DNNs exhibit idiosyncrasies that suggest their visual representation and processing might be substantially different from human vision. One limitation of DNNs is that they are vulnerable to adversarial examples, input images on which subtle, carefully designed noises are added to fool a machine classifier. The robustness of the human visual system against adversarial examples is potentially of great importance as it could uncover a key mechanistic feature that machine vision is yet to incorporate. In this study, we compare the visual representations of white- and black-box adversarial examples in DNNs and humans by leveraging functional magnetic resonance imaging (fMRI). We find a small but significant difference in representation patterns for different (i.e. white- versus black- box) types of adversarial examples for both humans and DNNs. However, human performance on categorical judgment is not degraded by noise regardless of the type unlike DNN. These results suggest that adversarial examples may be differentially represented in the human visual system, but unable to affect the perceptual experience.



### LEDNet: A Lightweight Encoder-Decoder Network for Real-Time Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.02423v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02423v3)
- **Published**: 2019-05-07 09:12:18+00:00
- **Updated**: 2019-05-13 07:57:09+00:00
- **Authors**: Yu Wang, Quan Zhou, Jia Liu, Jian Xiong, Guangwei Gao, Xiaofu Wu, Longin Jan Latecki
- **Comment**: 5 pages,3 figures,3 tables,accepted in IEEE ICIP 2019
- **Journal**: None
- **Summary**: The extensive computational burden limits the usage of CNNs in mobile devices for dense estimation tasks. In this paper, we present a lightweight network to address this problem,namely LEDNet, which employs an asymmetric encoder-decoder architecture for the task of real-time semantic segmentation.More specifically, the encoder adopts a ResNet as backbone network, where two new operations, channel split and shuffle, are utilized in each residual block to greatly reduce computation cost while maintaining higher segmentation accuracy. On the other hand, an attention pyramid network (APN) is employed in the decoder to further lighten the entire network complexity. Our model has less than 1M parameters,and is able to run at over 71 FPS in a single GTX 1080Ti GPU. The comprehensive experiments demonstrate that our approach achieves state-of-the-art results in terms of speed and accuracy trade-off on CityScapes dataset.



### Interactive Video Retrieval with Dialog
- **Arxiv ID**: http://arxiv.org/abs/1905.02442v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02442v1)
- **Published**: 2019-05-07 09:49:10+00:00
- **Updated**: 2019-05-07 09:49:10+00:00
- **Authors**: Sho Maeoki, Kohei Uehara, Tatsuya Harada
- **Comment**: None
- **Journal**: None
- **Summary**: Now that everyone can easily record videos, the quantity of which is continuously increasing, research on methods for improved video retrieval is important in the contemporary world. In cases where target videos are to be identified within a large collection gathered by individuals, the appropriate information must be obtained to retrieve the correct video within a large number of similar items in the target database. The purpose of this research is to retrieve target videos in such cases by introducing an interaction, or a dialog, between the system and the user. We propose a system to retrieve videos by asking questions about the content of the videos and leveraging the user's responses to the questions. Additionally, we confirmed the usefulness of the proposed system through experiments using the dataset called AVSD which includes videos and dialogs about the videos.



### Adapting Image Super-Resolution State-of-the-arts and Learning Multi-model Ensemble for Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1905.02462v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02462v1)
- **Published**: 2019-05-07 10:44:50+00:00
- **Updated**: 2019-05-07 10:44:50+00:00
- **Authors**: Chao Li, Dongliang He, Xiao Liu, Yukang Ding, Shilei Wen
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, image super-resolution has been widely studied and achieved significant progress by leveraging the power of deep convolutional neural networks. However, there has been limited advancement in video super-resolution (VSR) due to the complex temporal patterns in videos. In this paper, we investigate how to adapt state-of-the-art methods of image super-resolution for video super-resolution. The proposed adapting method is straightforward. The information among successive frames is well exploited, while the overhead on the original image super-resolution method is negligible. Furthermore, we propose a learning-based method to ensemble the outputs from multiple super-resolution models. Our methods show superior performance and rank second in the NTIRE2019 Video Super-Resolution Challenge Track 1.



### Adaptive Generation of Unrestricted Adversarial Inputs
- **Arxiv ID**: http://arxiv.org/abs/1905.02463v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.02463v2)
- **Published**: 2019-05-07 10:54:43+00:00
- **Updated**: 2019-10-01 12:43:55+00:00
- **Authors**: Isaac Dunn, Hadrien Pouget, Tom Melham, Daniel Kroening
- **Comment**: Updated to include new results
- **Journal**: None
- **Summary**: Neural networks are vulnerable to adversarially-constructed perturbations of their inputs. Most research so far has considered perturbations of a fixed magnitude under some $l_p$ norm. Although studying these attacks is valuable, there has been increasing interest in the construction of (and robustness to) unrestricted attacks, which are not constrained to a small and rather artificial subset of all possible adversarial inputs. We introduce a novel algorithm for generating such unrestricted adversarial inputs which, unlike prior work, is adaptive: it is able to tune its attacks to the classifier being targeted. It also offers a 400-2,000x speedup over the existing state of the art. We demonstrate our approach by generating unrestricted adversarial inputs that fool classifiers robust to perturbation-based attacks. We also show that, by virtue of being adaptive and unrestricted, our attack is able to defeat adversarial training against it.



### Ensemble of Convolutional Neural Networks Trained with Different Activation Functions
- **Arxiv ID**: http://arxiv.org/abs/1905.02473v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02473v5)
- **Published**: 2019-05-07 11:09:32+00:00
- **Updated**: 2020-09-21 23:28:27+00:00
- **Authors**: Gianluca Maguolo, Loris Nanni, Stefano Ghidoni
- **Comment**: None
- **Journal**: None
- **Summary**: Activation functions play a vital role in the training of Convolutional Neural Networks. For this reason, to develop efficient and performing functions is a crucial problem in the deep learning community. Key to these approaches is to permit a reliable parameter learning, avoiding vanishing gradient problems. The goal of this work is to propose an ensemble of Convolutional Neural Networks trained using several different activation functions. Moreover, a novel activation function is here proposed for the first time. Our aim is to improve the performance of Convolutional Neural Networks in small/medium size biomedical datasets. Our results clearly show that the proposed ensemble outperforms Convolutional Neural Networks trained with standard ReLU as activation function. The proposed ensemble outperforms with a p-value of 0.01 each tested stand-alone activation function; for reliable performance comparison we have tested our approach in more than 10 datasets, using two well-known Convolutional Neural Network: Vgg16 and ResNet50. MATLAB code used here will be available at https://github.com/LorisNanni.



### P2SGrad: Refined Gradients for Optimizing Deep Face Models
- **Arxiv ID**: http://arxiv.org/abs/1905.02479v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02479v1)
- **Published**: 2019-05-07 11:38:29+00:00
- **Updated**: 2019-05-07 11:38:29+00:00
- **Authors**: Xiao Zhang, Rui Zhao, Junjie Yan, Mengya Gao, Yu Qiao, Xiaogang Wang, Hongsheng Li
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: Cosine-based softmax losses significantly improve the performance of deep face recognition networks. However, these losses always include sensitive hyper-parameters which can make training process unstable, and it is very tricky to set suitable hyper parameters for a specific dataset. This paper addresses this challenge by directly designing the gradients for adaptively training deep neural networks. We first investigate and unify previous cosine softmax losses by analyzing their gradients. This unified view inspires us to propose a novel gradient called P2SGrad (Probability-to-Similarity Gradient), which leverages a cosine similarity instead of classification probability to directly update the testing metrics for updating neural network parameters. P2SGrad is adaptive and hyper-parameter free, which makes the training process more efficient and faster. We evaluate our P2SGrad on three face recognition benchmarks, LFW, MegaFace, and IJB-C. The results show that P2SGrad is stable in training, robust to noise, and achieves state-of-the-art performance on all the three benchmarks.



### Locality and Structure Regularized Low Rank Representation for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1905.02488v1
- **DOI**: 10.1109/TGRS.2018.2862899
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.02488v1)
- **Published**: 2019-05-07 12:05:52+00:00
- **Updated**: 2019-05-07 12:05:52+00:00
- **Authors**: Qi Wang, Xiange He, Xuelong Li
- **Comment**: 14 pages, 7 figures, TGRS2019
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing ( Volume: 57 ,
  Issue: 2 , Feb. 2019 )
- **Summary**: Hyperspectral image (HSI) classification, which aims to assign an accurate label for hyperspectral pixels, has drawn great interest in recent years. Although low rank representation (LRR) has been used to classify HSI, its ability to segment each class from the whole HSI data has not been exploited fully yet. LRR has a good capacity to capture the underlying lowdimensional subspaces embedded in original data. However, there are still two drawbacks for LRR. First, LRR does not consider the local geometric structure within data, which makes the local correlation among neighboring data easily ignored. Second, the representation obtained by solving LRR is not discriminative enough to separate different data. In this paper, a novel locality and structure regularized low rank representation (LSLRR) model is proposed for HSI classification. To overcome the above limitations, we present locality constraint criterion (LCC) and structure preserving strategy (SPS) to improve the classical LRR. Specifically, we introduce a new distance metric, which combines both spatial and spectral features, to explore the local similarity of pixels. Thus, the global and local structures of HSI data can be exploited sufficiently. Besides, we propose a structure constraint to make the representation have a near block-diagonal structure. This helps to determine the final classification labels directly. Extensive experiments have been conducted on three popular HSI datasets. And the experimental results demonstrate that the proposed LSLRR outperforms other state-of-the-art methods.



### Learning to Interpret Satellite Images in Global Scale Using Wikipedia
- **Arxiv ID**: http://arxiv.org/abs/1905.02506v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.02506v3)
- **Published**: 2019-05-07 12:47:39+00:00
- **Updated**: 2019-08-11 21:56:27+00:00
- **Authors**: Burak Uzkent, Evan Sheehan, Chenlin Meng, Zhongyi Tang, Marshall Burke, David Lobell, Stefano Ermon
- **Comment**: Accepted to IJCAI 2019
- **Journal**: None
- **Summary**: Despite recent progress in computer vision, finegrained interpretation of satellite images remains challenging because of a lack of labeled training data. To overcome this limitation, we construct a novel dataset called WikiSatNet by pairing georeferenced Wikipedia articles with satellite imagery of their corresponding locations. We then propose two strategies to learn representations of satellite images by predicting properties of the corresponding articles from the images. Leveraging this new multi-modal dataset, we can drastically reduce the quantity of human-annotated labels and time required for downstream tasks. On the recently released fMoW dataset, our pre-training strategies can boost the performance of a model pre-trained on ImageNet by up to 4:5% in F1 score.



### Contrastive Learning for Lifted Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.02507v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02507v2)
- **Published**: 2019-05-07 12:49:03+00:00
- **Updated**: 2019-07-26 15:40:16+00:00
- **Authors**: Christopher Zach, Virginia Estellers
- **Comment**: 9 pages, BMVC 2019
- **Journal**: None
- **Summary**: In this work we address supervised learning of neural networks via lifted network formulations. Lifted networks are interesting because they allow training on massively parallel hardware and assign energy models to discriminatively trained neural networks. We demonstrate that the training methods for lifted networks proposed in the literature have significant limitations and show how to use a contrastive loss to address those limitations. We demonstrate that this contrastive training approximates back-propagation in theory and in practice and that it is superior to the training objective regularly used for lifted networks.



### Rethinking Learning-based Demosaicing, Denoising, and Super-Resolution Pipeline
- **Arxiv ID**: http://arxiv.org/abs/1905.02538v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.02538v3)
- **Published**: 2019-05-07 13:19:05+00:00
- **Updated**: 2023-03-24 19:28:51+00:00
- **Authors**: Guocheng Qian, Yuanhao Wang, Jinjin Gu, Chao Dong, Wolfgang Heidrich, Bernard Ghanem, Jimmy S. Ren
- **Comment**: Accepted at ICCP'22. Code is available at:
  https://github.com/guochengqian/TENet
- **Journal**: None
- **Summary**: Imaging is usually a mixture problem of incomplete color sampling, noise degradation, and limited resolution. This mixture problem is typically solved by a sequential solution that applies demosaicing (DM), denoising (DN), and super-resolution (SR) sequentially in a fixed and predefined pipeline (execution order of tasks), DM$\to$DN$\to$SR. The most recent work on image processing focuses on developing more sophisticated architectures to achieve higher image quality. Little attention has been paid to the design of the pipeline, and it is still not clear how significant the pipeline is to image quality. In this work, we comprehensively study the effects of pipelines on the mixture problem of learning-based DN, DM, and SR, in both sequential and joint solutions. On the one hand, in sequential solutions, we find that the pipeline has a non-trivial effect on the resulted image quality. Our suggested pipeline DN$\to$SR$\to$DM yields consistently better performance than other sequential pipelines in various experimental settings and benchmarks. On the other hand, in joint solutions, we propose an end-to-end Trinity Pixel Enhancement NETwork (TENet) that achieves state-of-the-art performance for the mixture problem. We further present a novel and simple method that can integrate a certain pipeline into a given end-to-end network by providing intermediate supervision using a detachable head. Extensive experiments show that an end-to-end network with the proposed pipeline can attain only a consistent but insignificant improvement. Our work indicates that the investigation of pipelines is applicable in sequential solutions, but is not very necessary in end-to-end networks. \RR{Code, models, and our contributed PixelShift200 dataset are available at \url{https://github.com/guochengqian/TENet}



### Ocular Diseases Diagnosis in Fundus Images using a Deep Learning: Approaches, tools and Performance evaluation
- **Arxiv ID**: http://arxiv.org/abs/1905.02544v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1905.02544v1)
- **Published**: 2019-05-07 13:21:01+00:00
- **Updated**: 2019-05-07 13:21:01+00:00
- **Authors**: Yaroub Elloumi, Mohamed Akil, Henda Boudegga
- **Comment**: None
- **Journal**: SPIE Real-Time Image Processing and Deep Learning, Apr 2019,
  Baltimore, Maryland, United States
- **Summary**: Ocular pathology detection from fundus images presents an important challenge on health care. In fact, each pathology has different severity stages that may be deduced by verifying the existence of specific lesions. Each lesion is characterized by morphological features. Moreover, several lesions of different pathologies have similar features. We note that patient may be affected simultaneously by several pathologies. Consequently, the ocular pathology detection presents a multi-class classification with a complex resolution principle. Several detection methods of ocular pathologies from fundus images have been proposed. The methods based on deep learning are distinguished by higher performance detection, due to their capability to configure the network with respect to the detection objective. This work proposes a survey of ocular pathology detection methods based on deep learning. First, we study the existing methods either for lesion segmentation or pathology classification. Afterwards, we extract the principle steps of processing and we analyze the proposed neural network structures. Subsequently, we identify the hardware and software environment required to employ the deep learning architecture. Thereafter, we investigate about the experimentation principles involved to evaluate the methods and the databases used either for training and testing phases. The detection performance ratios and execution times are also reported and discussed.



### Efficient Neural Architecture Search on Low-Dimensional Data for OCT Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.02590v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.02590v1)
- **Published**: 2019-05-07 14:01:56+00:00
- **Updated**: 2019-05-07 14:01:56+00:00
- **Authors**: Nils Gessert, Alexander Schlaefer
- **Comment**: Accepted at MIDL 2019 [arXiv:1907.08612]
- **Journal**: None
- **Summary**: Typically, deep learning architectures are handcrafted for their respective learning problem. As an alternative, neural architecture search (NAS) has been proposed where the architecture's structure is learned in an additional optimization step. For the medical imaging domain, this approach is very promising as there are diverse problems and imaging modalities that require architecture design. However, NAS is very time-consuming and medical learning problems often involve high-dimensional data with high computational requirements. We propose an efficient approach for NAS in the context of medical, image-based deep learning problems by searching for architectures on low-dimensional data which are subsequently transferred to high-dimensional data. For OCT-based layer segmentation, we demonstrate that a search on 1D data reduces search time by 87.5% compared to a search on 2D data while the final 2D models achieve similar performance.



### High Frequency Residual Learning for Multi-Scale Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1905.02649v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.02649v1)
- **Published**: 2019-05-07 15:47:27+00:00
- **Updated**: 2019-05-07 15:47:27+00:00
- **Authors**: Bowen Cheng, Rong Xiao, Jianfeng Wang, Thomas Huang, Lei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel high frequency residual learning framework, which leads to a highly efficient multi-scale network (MSNet) architecture for mobile and embedded vision problems. The architecture utilizes two networks: a low resolution network to efficiently approximate low frequency components and a high resolution network to learn high frequency residuals by reusing the upsampled low resolution features. With a classifier calibration module, MSNet can dynamically allocate computation resources during inference to achieve a better speed and accuracy trade-off. We evaluate our methods on the challenging ImageNet-1k dataset and observe consistent improvements over different base networks. On ResNet-18 and MobileNet with alpha=1.0, MSNet gains 1.5% accuracy over both architectures without increasing computations. On the more efficient MobileNet with alpha=0.25, our method gains 3.8% accuracy with the same amount of computations.



### Attention-based Fusion for Multi-source Human Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1905.02655v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02655v1)
- **Published**: 2019-05-07 16:00:39+00:00
- **Updated**: 2019-05-07 16:00:39+00:00
- **Authors**: Stéphane Lathuilière, Enver Sangineto, Aliaksandr Siarohin, Nicu Sebe
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: We present a generalization of the person-image generation task, in which a human image is generated conditioned on a target pose and a set X of source appearance images. In this way, we can exploit multiple, possibly complementary images of the same person which are usually available at training and at testing time. The solution we propose is mainly based on a local attention mechanism which selects relevant information from different source image regions, avoiding the necessity to build specific generators for each specific cardinality of X. The empirical evaluation of our method shows the practical interest of addressing the person-image generation problem in a multi-source setting.



### An Empirical Evaluation of Adversarial Robustness under Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.02675v4
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.02675v4)
- **Published**: 2019-05-07 16:26:26+00:00
- **Updated**: 2019-06-08 22:25:52+00:00
- **Authors**: Todor Davchev, Timos Korres, Stathi Fotiadis, Nick Antonopoulos, Subramanian Ramamoorthy
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we evaluate adversarial robustness in the context of transfer learning from a source trained on CIFAR 100 to a target network trained on CIFAR 10. Specifically, we study the effects of using robust optimisation in the source and target networks. This allows us to identify transfer learning strategies under which adversarial defences are successfully retained, in addition to revealing potential vulnerabilities. We study the extent to which features learnt by a fast gradient sign method (FGSM) and its iterative alternative (PGD) can preserve their defence properties against black and white-box attacks under three different transfer learning strategies. We find that using PGD examples during training on the source task leads to more general robust features that are easier to transfer. Furthermore, under successful transfer, it achieves 5.2% more accuracy against white-box PGD attacks than suitable baselines. Overall, our empirical evaluations give insights on how well adversarial robustness under transfer learning can generalise.



### Feature-Fused Context-Encoding Network for Neuroanatomy Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.02686v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02686v1)
- **Published**: 2019-05-07 16:43:12+00:00
- **Updated**: 2019-05-07 16:43:12+00:00
- **Authors**: Yuemeng Li, Hangfan Liu, Hongming Li, Yong Fan
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic segmentation of fine-grained brain structures remains a challenging task. Current segmentation methods mainly utilize 2D and 3D deep neural networks. The 2D networks take image slices as input to produce coarse segmentation in less processing time, whereas the 3D networks take the whole image volumes to generated fine-detailed segmentation with more computational burden. In order to obtain accurate fine-grained segmentation efficiently, in this paper, we propose an end-to-end Feature-Fused Context-Encoding Network for brain structure segmentation from MR (magnetic resonance) images. Our model is implemented based on a 2D convolutional backbone, which integrates a 2D encoding module to acquire planar image features and a spatial encoding module to extract spatial context information. A global context encoding module is further introduced to capture global context semantics from the fused 2D encoding and spatial features. The proposed network aims to fully leverage the global anatomical prior knowledge learned from context semantics, which is represented by a structure-aware attention factor to recalibrate the outputs of the network. In this way, the network is guaranteed to be aware of the class-dependent feature maps to facilitate the segmentation. We evaluate our model on 2012 Brain Multi-Atlas Labelling Challenge dataset for 134 fine-grained structure segmentation. Besides, we validate our network on 27 coarse structure segmentation tasks. Experimental results have demonstrated that our model can achieve improved performance compared with the state-of-the-art approaches.



### Learning Unsupervised Multi-View Stereopsis via Robust Photometric Consistency
- **Arxiv ID**: http://arxiv.org/abs/1905.02706v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.02706v2)
- **Published**: 2019-05-07 17:45:22+00:00
- **Updated**: 2019-06-06 17:30:47+00:00
- **Authors**: Tejas Khot, Shubham Agrawal, Shubham Tulsiani, Christoph Mertz, Simon Lucey, Martial Hebert
- **Comment**: None
- **Journal**: None
- **Summary**: We present a learning based approach for multi-view stereopsis (MVS). While current deep MVS methods achieve impressive results, they crucially rely on ground-truth 3D training data, and acquisition of such precise 3D geometry for supervision is a major hurdle. Our framework instead leverages photometric consistency between multiple views as supervisory signal for learning depth prediction in a wide baseline MVS setup. However, naively applying photo consistency constraints is undesirable due to occlusion and lighting changes across views. To overcome this, we propose a robust loss formulation that: a) enforces first order consistency and b) for each point, selectively enforces consistency with some views, thus implicitly handling occlusions. We demonstrate our ability to learn MVS without 3D supervision using a real dataset, and show that each component of our proposed robust loss results in a significant improvement. We qualitatively observe that our reconstructions are often more complete than the acquired ground truth, further showing the merits of this approach. Lastly, our learned model generalizes to novel settings, and our approach allows adaptation of existing CNNs to datasets without ground-truth 3D by unsupervised finetuning. Project webpage: https://tejaskhot.github.io/unsup_mvs



### Inverse Rendering for Complex Indoor Scenes: Shape, Spatially-Varying Lighting and SVBRDF from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1905.02722v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02722v1)
- **Published**: 2019-05-07 17:47:40+00:00
- **Updated**: 2019-05-07 17:47:40+00:00
- **Authors**: Zhengqin Li, Mohammad Shafiei, Ravi Ramamoorthi, Kalyan Sunkavalli, Manmohan Chandraker
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a deep inverse rendering framework for indoor scenes. From a single RGB image of an arbitrary indoor scene, we create a complete scene reconstruction, estimating shape, spatially-varying lighting, and spatially-varying, non-Lambertian surface reflectance. To train this network, we augment the SUNCG indoor scene dataset with real-world materials and render them with a fast, high-quality, physically-based GPU renderer to create a large-scale, photorealistic indoor dataset. Our inverse rendering network incorporates physical insights -- including a spatially-varying spherical Gaussian lighting representation, a differentiable rendering layer to model scene appearance, a cascade structure to iteratively refine the predictions and a bilateral solver for refinement -- allowing us to jointly reason about shape, lighting, and reflectance. Experiments show that our framework outperforms previous methods for estimating individual scene components, which also enables various novel applications for augmented reality, such as photorealistic object insertion and material editing. Code and data will be made publicly available.



### Context-Aware Automatic Occlusion Removal
- **Arxiv ID**: http://arxiv.org/abs/1905.02710v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02710v1)
- **Published**: 2019-05-07 17:50:40+00:00
- **Updated**: 2019-05-07 17:50:40+00:00
- **Authors**: Kumara Kahatapitiya, Dumindu Tissera, Ranga Rodrigo
- **Comment**: Accepted to be published in Proceedings of IEEE International
  Conference on Image Processing (ICIP), Taipei, Taiwan, September 2019
- **Journal**: None
- **Summary**: Occlusion removal is an interesting application of image enhancement, for which, existing work suggests manually-annotated or domain-specific occlusion removal. No work tries to address automatic occlusion detection and removal as a context-aware generic problem. In this paper, we present a novel methodology to identify objects that do not relate to the image context as occlusions and remove them, reconstructing the space occupied coherently. The proposed system detects occlusions by considering the relation between foreground and background object classes represented as vector embeddings, and removes them through inpainting. We test our system on COCO-Stuff dataset and conduct a user study to establish a baseline in context-aware automatic occlusion removal.



### EDVR: Video Restoration with Enhanced Deformable Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.02716v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02716v1)
- **Published**: 2019-05-07 17:58:14+00:00
- **Updated**: 2019-05-07 17:58:14+00:00
- **Authors**: Xintao Wang, Kelvin C. K. Chan, Ke Yu, Chao Dong, Chen Change Loy
- **Comment**: To appear in CVPR 2019 Workshop. The winners in all four tracks in
  the NTIRE 2019 video restoration and enhancement challenges. Project page:
  https://xinntao.github.io/projects/EDVR , Code:
  https://github.com/xinntao/EDVR
- **Journal**: None
- **Summary**: Video restoration tasks, including super-resolution, deblurring, etc, are drawing increasing attention in the computer vision community. A challenging benchmark named REDS is released in the NTIRE19 Challenge. This new benchmark challenges existing methods from two aspects: (1) how to align multiple frames given large motions, and (2) how to effectively fuse different frames with diverse motion and blur. In this work, we propose a novel Video Restoration framework with Enhanced Deformable networks, termed EDVR, to address these challenges. First, to handle large motions, we devise a Pyramid, Cascading and Deformable (PCD) alignment module, in which frame alignment is done at the feature level using deformable convolutions in a coarse-to-fine manner. Second, we propose a Temporal and Spatial Attention (TSA) fusion module, in which attention is applied both temporally and spatially, so as to emphasize important features for subsequent restoration. Thanks to these modules, our EDVR wins the champions and outperforms the second place by a large margin in all four tracks in the NTIRE19 video restoration and enhancement challenges. EDVR also demonstrates superior performance to state-of-the-art published methods on video super-resolution and deblurring. The code is available at https://github.com/xinntao/EDVR.



### LiStereo: Generate Dense Depth Maps from LIDAR and Stereo Imagery
- **Arxiv ID**: http://arxiv.org/abs/1905.02744v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.02744v3)
- **Published**: 2019-05-07 18:09:22+00:00
- **Updated**: 2020-06-25 19:00:06+00:00
- **Authors**: Junming Zhang, Manikandasriram Srinivasan Ramanagopal, Ram Vasudevan, Matthew Johnson-Roberson
- **Comment**: 14 pages, 3 figures, 5 tables
- **Journal**: None
- **Summary**: An accurate depth map of the environment is critical to the safe operation of autonomous robots and vehicles. Currently, either light detection and ranging (LIDAR) or stereo matching algorithms are used to acquire such depth information. However, a high-resolution LIDAR is expensive and produces sparse depth map at large range; stereo matching algorithms are able to generate denser depth maps but are typically less accurate than LIDAR at long range. This paper combines these approaches together to generate high-quality dense depth maps. Unlike previous approaches that are trained using ground-truth labels, the proposed model adopts a self-supervised training process. Experiments show that the proposed method is able to generate high-quality dense depth maps and performs robustly even with low-resolution inputs. This shows the potential to reduce the cost by using LIDARs with lower resolution in concert with stereo systems while maintaining high resolution.



### DeepSWIR: A Deep Learning Based Approach for the Synthesis of Short-Wave InfraRed Band using Multi-Sensor Concurrent Datasets
- **Arxiv ID**: http://arxiv.org/abs/1905.02749v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.02749v1)
- **Published**: 2019-05-07 18:11:24+00:00
- **Updated**: 2019-05-07 18:11:24+00:00
- **Authors**: Litu Rout, Yatharath Bhateja, Ankur Garg, Indranil Mishra, S Manthira Moorthi, Debjyoti Dhar
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Network (CNN) is achieving remarkable progress in various computer vision tasks. In the past few years, the remote sensing community has observed Deep Neural Network (DNN) finally taking off in several challenging fields. In this study, we propose a DNN to generate a predefined High Resolution (HR) synthetic spectral band using an ensemble of concurrent Low Resolution (LR) bands and existing HR bands. Of particular interest, the proposed network, namely DeepSWIR, synthesizes Short-Wave InfraRed (SWIR) band at 5m Ground Sampling Distance (GSD) using Green (G), Red (R) and Near InfraRed (NIR) bands at both 24m and 5m GSD, and SWIR band at 24m GSD. To our knowledge, the highest spatial resolution of commercially deliverable SWIR band is at 7.5m GSD. Also, we propose a Gaussian feathering based image stitching approach in light of processing large satellite imagery. To experimentally validate the synthesized HR SWIR band, we critically analyse the qualitative and quantitative results produced by DeepSWIR using state-of-the-art evaluation metrics. Further, we convert the synthesized DN values to Top Of Atmosphere (TOA) reflectance and compare with the corresponding band of Sentinel-2B. Finally, we show one real world application of the synthesized band by using it to map wetland resources over our region of interest.



### Uncertainty Modeling of Contextual-Connections between Tracklets for Unconstrained Video-based Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1905.02756v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02756v2)
- **Published**: 2019-05-07 18:24:35+00:00
- **Updated**: 2019-08-21 18:44:18+00:00
- **Authors**: Jingxiao Zheng, Ruichi Yu, Jun-Cheng Chen, Boyu Lu, Carlos D. Castillo, Rama Chellappa
- **Comment**: To appear in ICCV 2019
- **Journal**: None
- **Summary**: Unconstrained video-based face recognition is a challenging problem due to significant within-video variations caused by pose, occlusion and blur. To tackle this problem, an effective idea is to propagate the identity from high-quality faces to low-quality ones through contextual connections, which are constructed based on context such as body appearance. However, previous methods have often propagated erroneous information due to lack of uncertainty modeling of the noisy contextual connections. In this paper, we propose the Uncertainty-Gated Graph (UGG), which conducts graph-based identity propagation between tracklets, which are represented by nodes in a graph. UGG explicitly models the uncertainty of the contextual connections by adaptively updating the weights of the edge gates according to the identity distributions of the nodes during inference. UGG is a generic graphical model that can be applied at only inference time or with end-to-end training. We demonstrate the effectiveness of UGG with state-of-the-art results in the recently released challenging Cast Search in Movies and IARPA Janus Surveillance Video Benchmark dataset.



### Generalization ability of region proposal networks for multispectral person detection
- **Arxiv ID**: http://arxiv.org/abs/1905.02758v1
- **DOI**: 10.1117/12.2520705
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02758v1)
- **Published**: 2019-05-07 18:29:00+00:00
- **Updated**: 2019-05-07 18:29:00+00:00
- **Authors**: Kevin Fritz, Daniel König, Ulrich Klauck, Michael Teutsch
- **Comment**: None
- **Journal**: SPIE Proceedings Volume 10988, Automatic Target Recognition XXIX;
  109880Y (2019)
- **Summary**: Multispectral person detection aims at automatically localizing humans in images that consist of multiple spectral bands. Usually, the visual-optical (VIS) and the thermal infrared (IR) spectra are combined to achieve higher robustness for person detection especially in insufficiently illuminated scenes. This paper focuses on analyzing existing detection approaches for their generalization ability. Generalization is a key feature for machine learning based detection algorithms that are supposed to perform well across different datasets. Inspired by recent literature regarding person detection in the VIS spectrum, we perform a cross-validation study to empirically determine the most promising dataset to train a well-generalizing detector. Therefore, we pick one reference Deep Convolutional Neural Network (DCNN) architecture and three different multispectral datasets. The Region Proposal Network (RPN) originally introduced for object detection within the popular Faster R-CNN is chosen as a reference DCNN. The reason is that a stand-alone RPN is able to serve as a competitive detector for two-class problems such as person detection. Furthermore, current state-of-the-art approaches initially apply an RPN followed by individual classifiers. The three considered datasets are the KAIST Multispectral Pedestrian Benchmark including recently published improved annotations for training and testing, the Tokyo Multi-spectral Semantic Segmentation dataset, and the OSU Color-Thermal dataset including recently released annotations. The experimental results show that the KAIST Multispectral Pedestrian Benchmark with its improved annotations provides the best basis to train a DCNN with good generalization ability compared to the other two multispectral datasets. On average, this detection model achieves a log-average Miss Rate (MR) of 29.74 % evaluated on the reasonable test subsets of the three datasets.



### Robust Dense Mapping for Large-Scale Dynamic Environments
- **Arxiv ID**: http://arxiv.org/abs/1905.02781v1
- **DOI**: 10.1109/ICRA.2018.8462974
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1905.02781v1)
- **Published**: 2019-05-07 19:38:27+00:00
- **Updated**: 2019-05-07 19:38:27+00:00
- **Authors**: Ioan Andrei Bârsan, Peidong Liu, Marc Pollefeys, Andreas Geiger
- **Comment**: Presented at IEEE International Conference on Robotics and Automation
  (ICRA), 2018
- **Journal**: Proceedings of the 2018 IEEE International Conference on Robotics
  and Automation (ICRA), pages 7510--7517, 21-25 May 2018, Brisbane, QLD,
  Australia
- **Summary**: We present a stereo-based dense mapping algorithm for large-scale dynamic urban environments. In contrast to other existing methods, we simultaneously reconstruct the static background, the moving objects, and the potentially moving but currently stationary objects separately, which is desirable for high-level mobile robotic tasks such as path planning in crowded environments. We use both instance-aware semantic segmentation and sparse scene flow to classify objects as either background, moving, or potentially moving, thereby ensuring that the system is able to model objects with the potential to transition from static to dynamic, such as parked cars. Given camera poses estimated from visual odometry, both the background and the (potentially) moving objects are reconstructed separately by fusing the depth maps computed from the stereo input. In addition to visual odometry, sparse scene flow is also used to estimate the 3D motions of the detected moving objects, in order to reconstruct them accurately. A map pruning technique is further developed to improve reconstruction accuracy and reduce memory consumption, leading to increased scalability. We evaluate our system thoroughly on the well-known KITTI dataset. Our system is capable of running on a PC at approximately 2.5Hz, with the primary bottleneck being the instance-aware semantic segmentation, which is a limitation we hope to address in future work. The source code is available from the project website (http://andreibarsan.github.io/dynslam).



### Skin Lesion Classification Using CNNs with Patch-Based Attention and Diagnosis-Guided Loss Weighting
- **Arxiv ID**: http://arxiv.org/abs/1905.02793v2
- **DOI**: 10.1109/TBME.2019.2915839
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02793v2)
- **Published**: 2019-05-07 20:04:20+00:00
- **Updated**: 2019-05-09 09:27:40+00:00
- **Authors**: Nils Gessert, Thilo Sentker, Frederic Madesta, Rüdiger Schmitz, Helge Kniep, Ivo Baltruschat, René Werner, Alexander Schlaefer
- **Comment**: Accepted for publication in IEEE Transactions on Biomedical
  Engineering
- **Journal**: None
- **Summary**: Objective: This work addresses two key problems of skin lesion classification. The first problem is the effective use of high-resolution images with pretrained standard architectures for image classification. The second problem is the high class imbalance encountered in real-world multi-class datasets. Methods: To use high-resolution images, we propose a novel patch-based attention architecture that provides global context between small, high-resolution patches. We modify three pretrained architectures and study the performance of patch-based attention. To counter class imbalance problems, we compare oversampling, balanced batch sampling, and class-specific loss weighting. Additionally, we propose a novel diagnosis-guided loss weighting method which takes the method used for ground-truth annotation into account. Results: Our patch-based attention mechanism outperforms previous methods and improves the mean sensitivity by 7%. Class balancing significantly improves the mean sensitivity and we show that our diagnosis-guided loss weighting method improves the mean sensitivity by 3% over normal loss balancing. Conclusion: The novel patch-based attention mechanism can be integrated into pretrained architectures and provides global context between local patches while outperforming other patch-based methods. Hence, pretrained architectures can be readily used with high-resolution images without downsampling. The new diagnosis-guided loss weighting method outperforms other methods and allows for effective training when facing class imbalance. Significance: The proposed methods improve automatic skin lesion classification. They can be extended to other clinical applications where high-resolution image data and class imbalance are relevant.



### LightTrack: A Generic Framework for Online Top-Down Human Pose Tracking
- **Arxiv ID**: http://arxiv.org/abs/1905.02822v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02822v1)
- **Published**: 2019-05-07 22:02:00+00:00
- **Updated**: 2019-05-07 22:02:00+00:00
- **Authors**: Guanghan Ning, Heng Huang
- **Comment**: 9 pages, 6 figures, 6 tables
- **Journal**: None
- **Summary**: In this paper, we propose a novel effective light-weight framework, called LightTrack, for online human pose tracking. The proposed framework is designed to be generic for top-down pose tracking and is faster than existing online and offline methods. Single-person Pose Tracking (SPT) and Visual Object Tracking (VOT) are incorporated into one unified functioning entity, easily implemented by a replaceable single-person pose estimation module. Our framework unifies single-person pose tracking with multi-person identity association and sheds first light upon bridging keypoint tracking with object tracking. We also propose a Siamese Graph Convolution Network (SGCN) for human pose matching as a Re-ID module in our pose tracking system. In contrary to other Re-ID modules, we use a graphical representation of human joints for matching. The skeleton-based representation effectively captures human pose similarity and is computationally inexpensive. It is robust to sudden camera shift that introduces human drifting. To the best of our knowledge, this is the first paper to propose an online human pose tracking framework in a top-down fashion. The proposed framework is general enough to fit other pose estimators and candidate matching mechanisms. Our method outperforms other online methods while maintaining a much higher frame rate, and is very competitive with our offline state-of-the-art. We make the code publicly available at: https://github.com/Guanghan/lighttrack.



### FANTrack: 3D Multi-Object Tracking with Feature Association Network
- **Arxiv ID**: http://arxiv.org/abs/1905.02843v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1905.02843v1)
- **Published**: 2019-05-07 23:26:03+00:00
- **Updated**: 2019-05-07 23:26:03+00:00
- **Authors**: Erkan Baser, Venkateshwaran Balasubramanian, Prarthana Bhattacharyya, Krzysztof Czarnecki
- **Comment**: 8 pages, 10 figures, IEEE Intelligent Vehicles Symposium (IV 19)
- **Journal**: None
- **Summary**: We propose a data-driven approach to online multi-object tracking (MOT) that uses a convolutional neural network (CNN) for data association in a tracking-by-detection framework. The problem of multi-target tracking aims to assign noisy detections to a-priori unknown and time-varying number of tracked objects across a sequence of frames. A majority of the existing solutions focus on either tediously designing cost functions or formulating the task of data association as a complex optimization problem that can be solved effectively. Instead, we exploit the power of deep learning to formulate the data association problem as inference in a CNN. To this end, we propose to learn a similarity function that combines cues from both image and spatial features of objects. Our solution learns to perform global assignments in 3D purely from data, handles noisy detections and a varying number of targets, and is easy to train. We evaluate our approach on the challenging KITTI dataset and show competitive results. Our code is available at https://git.uwaterloo.ca/wise-lab/fantrack.



### Feature Selection and Feature Extraction in Pattern Analysis: A Literature Review
- **Arxiv ID**: http://arxiv.org/abs/1905.02845v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.02845v1)
- **Published**: 2019-05-07 23:41:34+00:00
- **Updated**: 2019-05-07 23:41:34+00:00
- **Authors**: Benyamin Ghojogh, Maria N. Samad, Sayema Asif Mashhadi, Tania Kapoor, Wahab Ali, Fakhri Karray, Mark Crowley
- **Comment**: 14 pages, 1 figure, 2 tables, survey (literature review) paper
- **Journal**: None
- **Summary**: Pattern analysis often requires a pre-processing stage for extracting or selecting features in order to help the classification, prediction, or clustering stage discriminate or represent the data in a better way. The reason for this requirement is that the raw data are complex and difficult to process without extracting or selecting appropriate features beforehand. This paper reviews theory and motivation of different common methods of feature selection and extraction and introduces some of their applications. Some numerical implementations are also shown for these methods. Finally, the methods in feature selection and extraction are compared.



