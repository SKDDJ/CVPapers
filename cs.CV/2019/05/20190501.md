# Arxiv Papers in cs.CV on 2019-05-01
### Inferring the Importance of Product Appearance: A Step Towards the Screenless Revolution
- **Arxiv ID**: http://arxiv.org/abs/1905.03698v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.03698v2)
- **Published**: 2019-05-01 00:21:43+00:00
- **Updated**: 2019-05-18 09:43:24+00:00
- **Authors**: Yongshun Gong, Jinfeng Yi, Dongdong Chen, Jian Zhang, Jiayu Zhou, Zhihua Zhou
- **Comment**: We want to withdraw this paper. there are some insufficient
  explanations and bad figure presentation
- **Journal**: None
- **Summary**: Nowadays, almost all the online orders were placed through screened devices such as mobile phones, tablets, and computers. With the rapid development of the Internet of Things (IoT) and smart appliances, more and more screenless smart devices, e.g., smart speaker and smart refrigerator, appear in our daily lives. They open up new means of interaction and may provide an excellent opportunity to reach new customers and increase sales. However, not all the items are suitable for screenless shopping, since some items' appearance play an important role in consumer decision making. Typical examples include clothes, dolls, bags, and shoes. In this paper, we aim to infer the significance of every item's appearance in consumer decision making and identify the group of items that are suitable for screenless shopping. Specifically, we formulate the problem as a classification task that predicts if an item's appearance has a significant impact on people's purchase behavior. To solve this problem, we extract features from three different views, namely items' intrinsic properties, items' images, and users' comments, and collect a set of necessary labels via crowdsourcing. We then propose an iterative semi-supervised learning framework with three carefully designed loss functions. We conduct extensive experiments on a real-world transaction dataset collected from the online retail giant JD.com. Experimental results verify the effectiveness of the proposed method.



### Self-Supervised Convolutional Subspace Clustering Network
- **Arxiv ID**: http://arxiv.org/abs/1905.00149v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00149v1)
- **Published**: 2019-05-01 01:05:25+00:00
- **Updated**: 2019-05-01 01:05:25+00:00
- **Authors**: Junjian Zhang, Chun-Guang Li, Chong You, Xianbiao Qi, Honggang Zhang, Jun Guo, Zhouchen Lin
- **Comment**: 10 pages, 2 figures, and 5 tables. This paper has been accepted by
  CVPR2019
- **Journal**: None
- **Summary**: Subspace clustering methods based on data self-expression have become very popular for learning from data that lie in a union of low-dimensional linear subspaces. However, the applicability of subspace clustering has been limited because practical visual data in raw form do not necessarily lie in such linear subspaces. On the other hand, while Convolutional Neural Network (ConvNet) has been demonstrated to be a powerful tool for extracting discriminative features from visual data, training such a ConvNet usually requires a large amount of labeled data, which are unavailable in subspace clustering applications. To achieve simultaneous feature learning and subspace clustering, we propose an end-to-end trainable framework, called Self-Supervised Convolutional Subspace Clustering Network (S$^2$ConvSCN), that combines a ConvNet module (for feature learning), a self-expression module (for subspace clustering) and a spectral clustering module (for self-supervision) into a joint optimization framework. Particularly, we introduce a dual self-supervision that exploits the output of spectral clustering to supervise the training of the feature learning module (via a classification loss) and the self-expression module (via a spectral clustering loss). Our experiments on four benchmark datasets show the effectiveness of the dual self-supervision and demonstrate superior performance of our proposed approach.



### Precise Synthetic Image and LiDAR (PreSIL) Dataset for Autonomous Vehicle Perception
- **Arxiv ID**: http://arxiv.org/abs/1905.00160v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1905.00160v2)
- **Published**: 2019-05-01 02:14:22+00:00
- **Updated**: 2019-05-07 02:55:37+00:00
- **Authors**: Braden Hurl, Krzysztof Czarnecki, Steven Waslander
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce the Precise Synthetic Image and LiDAR (PreSIL) dataset for autonomous vehicle perception. Grand Theft Auto V (GTA V), a commercial video game, has a large detailed world with realistic graphics, which provides a diverse data collection environment. Existing works creating synthetic LiDAR data for autonomous driving with GTA V have not released their datasets, rely on an in-game raycasting function which represents people as cylinders, and can fail to capture vehicles past 30 metres. Our work creates a precise LiDAR simulator within GTA V which collides with detailed models for all entities no matter the type or position. The PreSIL dataset consists of over 50,000 frames and includes high-definition images with full resolution depth information, semantic segmentation (images), point-wise segmentation (point clouds), and detailed annotations for all vehicles and people. Collecting additional data with our framework is entirely automatic and requires no human annotation of any kind. We demonstrate the effectiveness of our dataset by showing an improvement of up to 5% average precision on the KITTI 3D Object Detection benchmark challenge when state-of-the-art 3D object detection networks are pre-trained with our data. The data and code are available at https://tinyurl.com/y3tb9sxy



### Unsupervised Temperature Scaling: An Unsupervised Post-Processing Calibration Method of Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.00174v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.00174v3)
- **Published**: 2019-05-01 03:43:29+00:00
- **Updated**: 2019-06-10 15:23:22+00:00
- **Authors**: Azadeh Sadat Mozafari, Hugo Siqueira Gomes, Wilson Leão, Christian Gagné
- **Comment**: arXiv admin note: text overlap with arXiv:1810.11586
- **Journal**: None
- **Summary**: The great performances of deep learning are undeniable, with impressive results over a wide range of tasks. However, the output confidence of these models is usually not well-calibrated, which can be an issue for applications where confidence on the decisions is central to providing trust and reliability (e.g., autonomous driving or medical diagnosis). For models using softmax at the last layer, Temperature Scaling (TS) is a state-of-the-art calibration method, with low time and memory complexity as well as demonstrated effectiveness. TS relies on a T parameter to rescale and calibrate values of the softmax layer, whose parameter value is computed from a labelled dataset. We are proposing an Unsupervised Temperature Scaling (UTS) approach, which does not depend on labelled samples to calibrate the model, which allows, for example, the use of a part of a test samples to calibrate the pre-trained model before going into inference mode. We provide theoretical justifications for UTS and assess its effectiveness on a wide range of deep models and datasets. We also demonstrate calibration results of UTS on skin lesion detection, a problem where a well-calibrated output can play an important role for accurate decision-making.



### Learning fashion compatibility across apparel categories for outfit recommendation
- **Arxiv ID**: http://arxiv.org/abs/1905.03703v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03703v1)
- **Published**: 2019-05-01 05:37:01+00:00
- **Updated**: 2019-05-01 05:37:01+00:00
- **Authors**: Luisa F. Polania, Satyajit Gupte
- **Comment**: Accepted for publication at ICIP 2019
- **Journal**: None
- **Summary**: This paper addresses the problem of generating recommendations for completing the outfit given that a user is interested in a particular apparel item. The proposed method is based on a siamese network used for feature extraction followed by a fully-connected network used for learning a fashion compatibility metric. The embeddings generated by the siamese network are augmented with color histogram features motivated by the important role that color plays in determining fashion compatibility. The training of the network is formulated as a maximum a posteriori (MAP) problem where Laplacian distributions are assumed for the filters of the siamese network to promote sparsity and matrix-variate normal distributions are assumed for the weights of the metric network to efficiently exploit correlations between the input units of each fully-connected layer.



### Automatic Dataset Augmentation Using Virtual Human Simulation
- **Arxiv ID**: http://arxiv.org/abs/1905.00261v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.00261v1)
- **Published**: 2019-05-01 11:01:39+00:00
- **Updated**: 2019-05-01 11:01:39+00:00
- **Authors**: Marcelo C. Ghilardi, Leandro Dihl, Estevão Testa, Pedro Braga, João P. Pianta, Isabel H. Manssour, Soraia R. Musse
- **Comment**: None
- **Journal**: None
- **Summary**: Virtual Human Simulation has been widely used for different purposes, such as comfort or accessibility analysis. In this paper, we investigate the possibility of using this type of technique to extend the training datasets of pedestrians to be used with machine learning techniques. Our main goal is to verify if Computer Graphics (CG) images of virtual humans with a simplistic rendering can be efficient in order to augment datasets used for training machine learning methods. In fact, from a machine learning point of view, there is a need to collect and label large datasets for ground truth, which sometimes demands manual annotation. In addition, find out images and videos with real people and also provide ground truth of people detection and counting is not trivial. If CG images, which can have a ground truth automatically generated, can also be used as training in machine learning techniques for pedestrian detection and counting, it can certainly facilitate and optimize the whole process of event detection. In particular, we propose to parametrize virtual humans using a data-driven approach. Results demonstrated that using the extended datasets with CG images outperforms the results when compared to only real images sequences.



### Learn to synthesize and synthesize to learn
- **Arxiv ID**: http://arxiv.org/abs/1905.00286v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00286v1)
- **Published**: 2019-05-01 12:45:31+00:00
- **Updated**: 2019-05-01 12:45:31+00:00
- **Authors**: Behzad Bozorgtabar, Mohammad Saeed Rad, Hazım Kemal Ekenel, Jean-Philippe Thiran
- **Comment**: Accepted to Computer Vision and Image Understanding (CVIU)
- **Journal**: None
- **Summary**: Attribute guided face image synthesis aims to manipulate attributes on a face image. Most existing methods for image-to-image translation can either perform a fixed translation between any two image domains using a single attribute or require training data with the attributes of interest for each subject. Therefore, these methods could only train one specific model for each pair of image domains, which limits their ability in dealing with more than two domains. Another disadvantage of these methods is that they often suffer from the common problem of mode collapse that degrades the quality of the generated images. To overcome these shortcomings, we propose attribute guided face image generation method using a single model, which is capable to synthesize multiple photo-realistic face images conditioned on the attributes of interest. In addition, we adopt the proposed model to increase the realism of the simulated face images while preserving the face characteristics. Compared to existing models, synthetic face images generated by our method present a good photorealistic quality on several face datasets. Finally, we demonstrate that generated facial images can be used for synthetic data augmentation, and improve the performance of the classifier used for facial expression recognition.



### AdaCos: Adaptively Scaling Cosine Logits for Effectively Learning Deep Face Representations
- **Arxiv ID**: http://arxiv.org/abs/1905.00292v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00292v2)
- **Published**: 2019-05-01 12:58:05+00:00
- **Updated**: 2019-05-07 10:46:56+00:00
- **Authors**: Xiao Zhang, Rui Zhao, Yu Qiao, Xiaogang Wang, Hongsheng Li
- **Comment**: CVPR 2019 Oral
- **Journal**: None
- **Summary**: The cosine-based softmax losses and their variants achieve great success in deep learning based face recognition. However, hyperparameter settings in these losses have significant influences on the optimization path as well as the final recognition performance. Manually tuning those hyperparameters heavily relies on user experience and requires many training tricks. In this paper, we investigate in depth the effects of two important hyperparameters of cosine-based softmax losses, the scale parameter and angular margin parameter, by analyzing how they modulate the predicted classification probability. Based on these analysis, we propose a novel cosine-based softmax loss, AdaCos, which is hyperparameter-free and leverages an adaptive scale parameter to automatically strengthen the training supervisions during the training process. We apply the proposed AdaCos loss to large-scale face verification and identification datasets, including LFW, MegaFace, and IJB-C 1:1 Verification. Our results show that training deep neural networks with the AdaCos loss is stable and able to achieve high face recognition accuracy. Our method outperforms state-of-the-art softmax losses on all the three datasets.



### 3DFaceGAN: Adversarial Nets for 3D Face Representation, Generation, and Translation
- **Arxiv ID**: http://arxiv.org/abs/1905.00307v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00307v2)
- **Published**: 2019-05-01 13:31:01+00:00
- **Updated**: 2019-05-09 22:24:51+00:00
- **Authors**: Stylianos Moschoglou, Stylianos Ploumpis, Mihalis Nicolaou, Athanasios Papaioannou, Stefanos Zafeiriou
- **Comment**: 15 pages, 12 figures. Submitted to International Journal of Computer
  Vision (IJCV), special issue: Generative Adversarial Networks for Computer
  Vision
- **Journal**: None
- **Summary**: Over the past few years, Generative Adversarial Networks (GANs) have garnered increased interest among researchers in Computer Vision, with applications including, but not limited to, image generation, translation, imputation, and super-resolution. Nevertheless, no GAN-based method has been proposed in the literature that can successfully represent, generate or translate 3D facial shapes (meshes). This can be primarily attributed to two facts, namely that (a) publicly available 3D face databases are scarce as well as limited in terms of sample size and variability (e.g., few subjects, little diversity in race and gender), and (b) mesh convolutions for deep networks present several challenges that are not entirely tackled in the literature, leading to operator approximations and model instability, often failing to preserve high-frequency components of the distribution. As a result, linear methods such as Principal Component Analysis (PCA) have been mainly utilized towards 3D shape analysis, despite being unable to capture non-linearities and high frequency details of the 3D face - such as eyelid and lip variations. In this work, we present 3DFaceGAN, the first GAN tailored towards modeling the distribution of 3D facial surfaces, while retaining the high frequency details of 3D face shapes. We conduct an extensive series of both qualitative and quantitative experiments, where the merits of 3DFaceGAN are clearly demonstrated against other, state-of-the-art methods in tasks such as 3D shape representation, generation, and translation.



### Towards computer vision powered color-nutrient assessment of pureed food
- **Arxiv ID**: http://arxiv.org/abs/1905.00310v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.00310v1)
- **Published**: 2019-05-01 13:42:19+00:00
- **Updated**: 2019-05-01 13:42:19+00:00
- **Authors**: Kaylen J. Pfisterer, Robert Amelard, Braeden Syrnyk, Alexander Wong
- **Comment**: 3 pages
- **Journal**: None
- **Summary**: With one in four individuals afflicted with malnutrition, computer vision may provide a way of introducing a new level of automation in the nutrition field to reliably monitor food and nutrient intake. In this study, we present a novel approach to modeling the link between color and vitamin A content using transmittance imaging of a pureed foods dilution series in a computer vision powered nutrient sensing system via a fine-tuned deep autoencoder network, which in this case was trained to predict the relative concentration of sweet potato purees. Experimental results show the deep autoencoder network can achieve an accuracy of 80% across beginner (6 month) and intermediate (8 month) commercially prepared pureed sweet potato samples. Prediction errors may be explained by fundamental differences in optical properties which are further discussed.



### Multi-level Encoder-Decoder Architectures for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1905.00322v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.00322v3)
- **Published**: 2019-05-01 14:13:51+00:00
- **Updated**: 2019-05-06 12:45:24+00:00
- **Authors**: Indra Deep Mastan, Shanmuganathan Raman
- **Comment**: Accepted in the IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR) Workshop: "New Trends in Image Restoration and Enhancement
  workshop (NTIRE) 2019"
- **Journal**: None
- **Summary**: Many real-world solutions for image restoration are learning-free and based on handcrafted image priors such as self-similarity. Recently, deep-learning methods that use training data have achieved state-of-the-art results in various image restoration tasks (e.g., super-resolution and inpainting). Ulyanov et al. bridge the gap between these two families of methods (CVPR 18). They have shown that learning-free methods perform close to the state-of-the-art learning-based methods (approximately 1 PSNR). Their approach benefits from the encoder-decoder network. In this paper, we propose a framework based on the multi-level extensions of the encoder-decoder network, to investigate interesting aspects of the relationship between image restoration and network construction independent of learning. Our framework allows various network structures by modifying the following network components: skip links, cascading of the network input into intermediate layers, a composition of the encoder-decoder subnetworks, and network depth. These handcrafted network structures illustrate how the construction of untrained networks influence the following image restoration tasks: denoising, super-resolution, and inpainting. We also demonstrate image reconstruction using flash and no-flash image pairs. We provide performance comparisons with the state-of-the-art methods for all the restoration tasks above.



### Bean Split Ratio for Dry Bean Canning Quality and Variety Analysis
- **Arxiv ID**: http://arxiv.org/abs/1905.00336v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00336v1)
- **Published**: 2019-05-01 14:52:01+00:00
- **Updated**: 2019-05-01 14:52:01+00:00
- **Authors**: Yunfei Long, Amber Bassett, Karen Cichy, Addie Thompson, Daniel Morris
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition Workshops,
  2019
- **Journal**: None
- **Summary**: Splits on canned beans appear in the process of preparation and canning. Researchers are studying how they are influenced by cooking environment and genotype. However, there is no existing method to automatically quantify or to characterize the severity of splits. To solve this, we propose two measures: the Bean Split Ratio (BSR) that quantifies the overall severity of splits, and the Bean Split Histogram (BSH) that characterizes the size distribution of splits. We create a pixel-wise segmentation method to automatically estimate these measures from images. We also present a bean dataset of recombinant inbred lines of two genotypes, use the BSR and BSH to assess canning quality, and explore heritability of these properties.



### Accurate Visual Localization for Automotive Applications
- **Arxiv ID**: http://arxiv.org/abs/1905.03706v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.03706v1)
- **Published**: 2019-05-01 15:48:10+00:00
- **Updated**: 2019-05-01 15:48:10+00:00
- **Authors**: Eli Brosh, Matan Friedmann, Ilan Kadar, Lev Yitzhak Lavy, Elad Levi, Shmuel Rippa, Yair Lempert, Bruno Fernandez-Ruiz, Roei Herzig, Trevor Darrell
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate vehicle localization is a crucial step towards building effective Vehicle-to-Vehicle networks and automotive applications. Yet standard grade GPS data, such as that provided by mobile phones, is often noisy and exhibits significant localization errors in many urban areas. Approaches for accurate localization from imagery often rely on structure-based techniques, and thus are limited in scale and are expensive to compute. In this paper, we present a scalable visual localization approach geared for real-time performance. We propose a hybrid coarse-to-fine approach that leverages visual and GPS location cues. Our solution uses a self-supervised approach to learn a compact road image representation. This representation enables efficient visual retrieval and provides coarse localization cues, which are fused with vehicle ego-motion to obtain high accuracy location estimates. As a benchmark to evaluate the performance of our visual localization approach, we introduce a new large-scale driving dataset based on video and GPS data obtained from a large-scale network of connected dash-cams. Our experiments confirm that our approach is highly effective in challenging urban environments, reducing localization error by an order of magnitude.



### Gender Classification from Iris Texture Images Using a New Set of Binary Statistical Image Features
- **Arxiv ID**: http://arxiv.org/abs/1905.00372v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00372v1)
- **Published**: 2019-05-01 16:46:49+00:00
- **Updated**: 2019-05-01 16:46:49+00:00
- **Authors**: Juan Tapia, Claudia Arellano
- **Comment**: A pre-print version of the paper accepted at 12th IAPR International
  Conference on Biometrics
- **Journal**: None
- **Summary**: Soft biometric information such as gender can contribute to many applications like as identification and security. This paper explores the use of a Binary Statistical Features (BSIF) algorithm for classifying gender from iris texture images captured with NIR sensors. It uses the same pipeline for iris recognition systems consisting of iris segmentation, normalisation and then classification. Experiments show that applying BSIF is not straightforward since it can create artificial textures causing misclassification. In order to overcome this limitation, a new set of filters was trained from eye images and different sized filters with padding bands were tested on a subject-disjoint database. A Modified-BSIF (MBSIF) method was implemented. The latter achieved better gender classification results (94.6\% and 91.33\% for the left and right eye respectively). These results are competitive with the state of the art in gender classification. In an additional contribution, a novel gender labelled database was created and it will be available upon request.



### Estimation of Tissue Oxygen Saturation from RGB images and Sparse Hyperspectral Signals based on Conditional Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1905.00391v2
- **DOI**: 10.1007/s11548-019-01940-2
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.00391v2)
- **Published**: 2019-05-01 17:25:01+00:00
- **Updated**: 2019-05-16 21:49:41+00:00
- **Authors**: Qingbiao Li, Jianyu Lin, Neil T. Clancy, Daniel S. Elson
- **Comment**: None
- **Journal**: International journal of computer assisted radiology and surgery
  (2019)
- **Summary**: Purpose: Intra-operative measurement of tissue oxygen saturation (StO2) is important in the detection of ischemia, monitoring perfusion and identifying disease. Hyperspectral imaging (HSI) measures the optical reflectance spectrum of the tissue and uses this information to quantify its composition, including StO2. However, real-time monitoring is difficult due to the capture rate and data processing time. Methods: An endoscopic system based on a multi-fiber probe was previously developed to sparsely capture HSI data (sHSI). These were combined with RGB images, via a deep neural network, to generate high-resolution hypercubes and calculate StO2. To improve accuracy and processing speed, we propose a dual-input conditional generative adversarial network (cGAN), Dual2StO2, to directly estimate StO2 by fusing features from both RGB and sHSI. Results: Validation experiments were carried out on in vivo porcine bowel data, where the ground truth StO2 was generated from the HSI camera. The performance was also compared to our previous super-spectral-resolution network, SSRNet in terms of mean StO2 prediction accuracy and structural similarity metrics. Dual2StO2 was also tested using simulated probe data with varying fiber number. Conclusions: StO2 estimation by Dual2StO2 is visually closer to ground truth in general structure, achieves higher prediction accuracy and faster processing speed than SSRNet. Simulations showed that results improved when a greater number of fibers are used in the probe. Future work will include refinement of the network architecture, hardware optimization based on simulation results, and evaluation of the technique in clinical applications beyond StO2 estimation.



### Sex-Prediction from Periocular Images across Multiple Sensors and Spectra
- **Arxiv ID**: http://arxiv.org/abs/1905.00396v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00396v1)
- **Published**: 2019-05-01 17:31:31+00:00
- **Updated**: 2019-05-01 17:31:31+00:00
- **Authors**: Juan Tapia, Christian Rathgeb, Christoph Busch
- **Comment**: Pre-print version of Paper presented at Proc. International Workshop
  on Ubiquitous implicit Biometrics and health signals monitoring for
  person-centric applications (UBIO 18), 2018
- **Journal**: None
- **Summary**: In this paper, we provide a comprehensive analysis of periocular-based sex-prediction (commonly referred to as gender classification) using state-of-the-art machine learning techniques. In order to reflect a more challenging scenario where periocular images are likely to be obtained from an unknown source, i.e. sensor, convolutional neural networks are trained on fused sets composed of several near-infrared (NIR) and visible wavelength (VW) image databases. In a cross-sensor scenario within each spectrum an average classification accuracy of approximately 85\% is achieved. When sex-prediction is performed across spectra an average classification accuracy of about 82\% is obtained. Finally, a multi-spectral sex-prediction yields a classification accuracy of 83\% on average. Compared to proposed works, obtained results provide a more realistic estimation of the feasibility to predict a subject's sex from the periocular region.



### Fast AutoAugment
- **Arxiv ID**: http://arxiv.org/abs/1905.00397v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.00397v2)
- **Published**: 2019-05-01 17:33:36+00:00
- **Updated**: 2019-05-25 16:44:21+00:00
- **Authors**: Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, Sungwoong Kim
- **Comment**: 8 pages, 2 figure
- **Journal**: None
- **Summary**: Data augmentation is an essential technique for improving generalization ability of deep learning models. Recently, AutoAugment has been proposed as an algorithm to automatically search for augmentation policies from a dataset and has significantly enhanced performances on many image recognition tasks. However, its search method requires thousands of GPU hours even for a relatively small dataset. In this paper, we propose an algorithm called Fast AutoAugment that finds effective augmentation policies via a more efficient search strategy based on density matching. In comparison to AutoAugment, the proposed algorithm speeds up the search time by orders of magnitude while achieves comparable performances on image recognition tasks with various models and datasets including CIFAR-10, CIFAR-100, SVHN, and ImageNet.



### Learn Stereo, Infer Mono: Siamese Networks for Self-Supervised, Monocular, Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1905.00401v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00401v1)
- **Published**: 2019-05-01 17:36:19+00:00
- **Updated**: 2019-05-01 17:36:19+00:00
- **Authors**: Matan Goldman, Tal Hassner, Shai Avidan
- **Comment**: None
- **Journal**: None
- **Summary**: The field of self-supervised monocular depth estimation has seen huge advancements in recent years. Most methods assume stereo data is available during training but usually under-utilize it and only treat it as a reference signal. We propose a novel self-supervised approach which uses both left and right images equally during training, but can still be used with a single input image at test time, for monocular depth estimation. Our Siamese network architecture consists of two, twin networks, each learns to predict a disparity map from a single image. At test time, however, only one of these networks is used in order to infer depth. We show state-of-the-art results on the standard KITTI Eigen split benchmark as well as being the highest scoring self-supervised method on the new KITTI single view benchmark. To demonstrate the ability of our method to generalize to new data sets, we further provide results on the Make3D benchmark, which was not used during training.



### Pushing the Boundaries of View Extrapolation with Multiplane Images
- **Arxiv ID**: http://arxiv.org/abs/1905.00413v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00413v1)
- **Published**: 2019-05-01 17:57:09+00:00
- **Updated**: 2019-05-01 17:57:09+00:00
- **Authors**: Pratul P. Srinivasan, Richard Tucker, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng, Noah Snavely
- **Comment**: Oral presentation at CVPR 2019
- **Journal**: None
- **Summary**: We explore the problem of view synthesis from a narrow baseline pair of images, and focus on generating high-quality view extrapolations with plausible disocclusions. Our method builds upon prior work in predicting a multiplane image (MPI), which represents scene content as a set of RGB$\alpha$ planes within a reference view frustum and renders novel views by projecting this content into the target viewpoints. We present a theoretical analysis showing how the range of views that can be rendered from an MPI increases linearly with the MPI disparity sampling frequency, as well as a novel MPI prediction procedure that theoretically enables view extrapolations of up to $4\times$ the lateral viewpoint movement allowed by prior work. Our method ameliorates two specific issues that limit the range of views renderable by prior methods: 1) We expand the range of novel views that can be rendered without depth discretization artifacts by using a 3D convolutional network architecture along with a randomized-resolution training procedure to allow our model to predict MPIs with increased disparity sampling frequency. 2) We reduce the repeated texture artifacts seen in disocclusions by enforcing a constraint that the appearance of hidden content at any depth must be drawn from visible content at or behind that depth. Please see our results video at: https://www.youtube.com/watch?v=aJqAaMNL2m4.



### NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.00441v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.00441v3)
- **Published**: 2019-05-01 18:20:09+00:00
- **Updated**: 2019-12-09 19:18:49+00:00
- **Authors**: Yandong Li, Lijun Li, Liqiang Wang, Tong Zhang, Boqing Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Powerful adversarial attack methods are vital for understanding how to construct robust deep neural networks (DNNs) and for thoroughly testing defense techniques. In this paper, we propose a black-box adversarial attack algorithm that can defeat both vanilla DNNs and those generated by various defense techniques developed recently. Instead of searching for an "optimal" adversarial example for a benign input to a targeted DNN, our algorithm finds a probability density distribution over a small region centered around the input, such that a sample drawn from this distribution is likely an adversarial example, without the need of accessing the DNN's internal layers or weights. Our approach is universal as it can successfully attack different neural networks by a single algorithm. It is also strong; according to the testing against 2 vanilla DNNs and 13 defended ones, it outperforms state-of-the-art black-box or white-box attack methods for most test cases. Additionally, our results reveal that adversarial training remains one of the best defense techniques, and the adversarial examples are not as transferable across defended DNNs as them across vanilla DNNs.



### Detection of Single Grapevine Berries in Images Using Fully Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.00458v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00458v1)
- **Published**: 2019-05-01 19:19:55+00:00
- **Updated**: 2019-05-01 19:19:55+00:00
- **Authors**: Laura Zabawa, Anna Kicherer, Lasse Klingbeil, Andres Milioto, Reinhard Töpfer, Heiner Kuhlmann, Ribana Roscher
- **Comment**: None
- **Journal**: CVPR Workshop on Computer Vision Problems in Plant Phenotyping,
  2019
- **Summary**: Yield estimation and forecasting are of special interest in the field of grapevine breeding and viticulture. The number of harvested berries per plant is strongly correlated with the resulting quality. Therefore, early yield forecasting can enable a focused thinning of berries to ensure a high quality end product. Traditionally yield estimation is done by extrapolating from a small sample size and by utilizing historic data. Moreover, it needs to be carried out by skilled experts with much experience in this field. Berry detection in images offers a cheap, fast and non-invasive alternative to the otherwise time-consuming and subjective on-site analysis by experts. We apply fully convolutional neural networks on images acquired with the Phenoliner, a field phenotyping platform. We count single berries in images to avoid the error-prone detection of grapevine clusters. Clusters are often overlapping and can vary a lot in the size which makes the reliable detection of them difficult. We address especially the detection of white grapes directly in the vineyard. The detection of single berries is formulated as a classification task with three classes, namely 'berry', 'edge' and 'background'. A connected component algorithm is applied to determine the number of berries in one image. We compare the automatically counted number of berries with the manually detected berries in 60 images showing Riesling plants in vertical shoot positioned trellis (VSP) and semi minimal pruned hedges (SMPH). We are able to detect berries correctly within the VSP system with an accuracy of 94.0 \% and for the SMPH system with 85.6 \%.



### Fully Automatic Brain Tumor Segmentation using a Normalized Gaussian Bayesian Classifier and 3D Fluid Vector Flow
- **Arxiv ID**: http://arxiv.org/abs/1905.00469v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.MM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.00469v1)
- **Published**: 2019-05-01 19:49:40+00:00
- **Updated**: 2019-05-01 19:49:40+00:00
- **Authors**: Tao Wang, Irene Cheng, Anup Basu
- **Comment**: ICIP 2010
- **Journal**: None
- **Summary**: Brain tumor segmentation from Magnetic Resonance Images (MRIs) is an important task to measure tumor responses to treatments. However, automatic segmentation is very challenging. This paper presents an automatic brain tumor segmentation method based on a Normalized Gaussian Bayesian classification and a new 3D Fluid Vector Flow (FVF) algorithm. In our method, a Normalized Gaussian Mixture Model (NGMM) is proposed and used to model the healthy brain tissues. Gaussian Bayesian Classifier is exploited to acquire a Gaussian Bayesian Brain Map (GBBM) from the test brain MR images. GBBM is further processed to initialize the 3D FVF algorithm, which segments the brain tumor. This algorithm has two major contributions. First, we present a NGMM to model healthy brains. Second, we extend our 2D FVF algorithm to 3D space and use it for brain tumor segmentation. The proposed method is validated on a publicly available dataset.



### A note on 'A fully parallel 3D thinning algorithm and its applications'
- **Arxiv ID**: http://arxiv.org/abs/1905.03705v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1905.03705v1)
- **Published**: 2019-05-01 19:57:49+00:00
- **Updated**: 2019-05-01 19:57:49+00:00
- **Authors**: Tao Wang, Anup Basu
- **Comment**: None
- **Journal**: Pattern Recognition Letters, 2007
- **Summary**: A 3D thinning algorithm erodes a 3D binary image layer by layer to extract the skeletons. This paper presents a correction to Ma and Sonka's thinning algorithm, A fully parallel 3D thinning algorithm and its applications, which fails to preserve connectivity of 3D objects. We start with Ma and Sonka's algorithm and examine its verification of connectivity preservation. Our analysis leads to a group of different deleting templates, which can preserve connectivity of 3D objects.



### Land Use and Land Cover Classification Using Deep Learning Techniques
- **Arxiv ID**: http://arxiv.org/abs/1905.00510v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.00510v1)
- **Published**: 2019-05-01 21:38:54+00:00
- **Updated**: 2019-05-01 21:38:54+00:00
- **Authors**: Nagesh Kumar Uba
- **Comment**: None
- **Journal**: None
- **Summary**: Large datasets of sub-meter aerial imagery represented as orthophoto mosaics are widely available today, and these data sets may hold a great deal of untapped information. This imagery has a potential to locate several types of features; for example, forests, parking lots, airports, residential areas, or freeways in the imagery. However, the appearances of these things vary based on many things including the time that the image is captured, the sensor settings, processing done to rectify the image, and the geographical and cultural context of the region captured by the image. This thesis explores the use of deep convolutional neural networks to classify land use from very high spatial resolution (VHR), orthorectified, visible band multispectral imagery. Recent technological and commercial applications have driven the collection a massive amount of VHR images in the visible red, green, blue (RGB) spectral bands, this work explores the potential for deep learning algorithms to exploit this imagery for automatic land use/ land cover (LULC) classification.



### Optimal Multi-view Correction of Local Affine Frames
- **Arxiv ID**: http://arxiv.org/abs/1905.00519v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00519v1)
- **Published**: 2019-05-01 22:17:59+00:00
- **Updated**: 2019-05-01 22:17:59+00:00
- **Authors**: Ivan Eichhardt, Daniel Barath
- **Comment**: None
- **Journal**: None
- **Summary**: The technique requires the epipolar geometry to be pre-estimated between each image pair. It exploits the constraints which the camera movement implies, in order to apply a closed-form correction to the parameters of the input affinities. Also, it is shown that the rotations and scales obtained by partially affine-covariant detectors, e.g., AKAZE or SIFT, can be completed to be full affine frames by the proposed algorithm. It is validated both in synthetic experiments and on publicly available real-world datasets that the method always improves the output of the evaluated affine-covariant feature detectors. As a by-product, these detectors are compared and the ones obtaining the most accurate affine frames are reported. For demonstrating the applicability, we show that the proposed technique as a pre-processing step improves the accuracy of pose estimation for a camera rig, surface normal and homography estimation.



### 3D BAT: A Semi-Automatic, Web-based 3D Annotation Toolbox for Full-Surround, Multi-Modal Data Streams
- **Arxiv ID**: http://arxiv.org/abs/1905.00525v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00525v1)
- **Published**: 2019-05-01 22:59:54+00:00
- **Updated**: 2019-05-01 22:59:54+00:00
- **Authors**: Walter Zimmer, Akshay Rangesh, Mohan Trivedi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we focus on obtaining 2D and 3D labels, as well as track IDs for objects on the road with the help of a novel 3D Bounding Box Annotation Toolbox (3D BAT). Our open source, web-based 3D BAT incorporates several smart features to improve usability and efficiency. For instance, this annotation toolbox supports semi-automatic labeling of tracks using interpolation, which is vital for downstream tasks like tracking, motion planning and motion prediction. Moreover, annotations for all camera images are automatically obtained by projecting annotations from 3D space into the image domain. In addition to the raw image and point cloud feeds, a Masterview consisting of the top view (bird's-eye-view), side view and front views is made available to observe objects of interest from different perspectives. Comparisons of our method with other publicly available annotation tools reveal that 3D annotations can be obtained faster and more efficiently by using our toolbox.



### RRPN: Radar Region Proposal Network for Object Detection in Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1905.00526v2
- **DOI**: 10.1109/ICIP.2019.8803392
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00526v2)
- **Published**: 2019-05-01 23:03:22+00:00
- **Updated**: 2019-05-15 14:45:23+00:00
- **Authors**: Ramin Nabati, Hairong Qi
- **Comment**: To appear in ICIP 2019
- **Journal**: None
- **Summary**: Region proposal algorithms play an important role in most state-of-the-art two-stage object detection networks by hypothesizing object locations in the image. Nonetheless, region proposal algorithms are known to be the bottleneck in most two-stage object detection networks, increasing the processing time for each image and resulting in slow networks not suitable for real-time applications such as autonomous driving vehicles. In this paper we introduce RRPN, a Radar-based real-time region proposal algorithm for object detection in autonomous driving vehicles. RRPN generates object proposals by mapping Radar detections to the image coordinate system and generating pre-defined anchor boxes for each mapped Radar detection point. These anchor boxes are then transformed and scaled based on the object's distance from the vehicle, to provide more accurate proposals for the detected objects. We evaluate our method on the newly released NuScenes dataset [1] using the Fast R-CNN object detection network [2]. Compared to the Selective Search object proposal algorithm [3], our model operates more than 100x faster while at the same time achieves higher detection precision and recall. Code has been made publicly available at https://github.com/mrnabati/RRPN .



