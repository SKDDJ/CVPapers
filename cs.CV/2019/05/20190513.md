# Arxiv Papers in cs.CV on 2019-05-13
### Programmable Spectrometry -- Per-pixel Classification of Materials using Learned Spectral Filters
- **Arxiv ID**: http://arxiv.org/abs/1905.04815v1
- **DOI**: 10.1109/ICCP48838.2020.9105281
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.04815v1)
- **Published**: 2019-05-13 00:20:31+00:00
- **Updated**: 2019-05-13 00:20:31+00:00
- **Authors**: Vishwanath Saragadam, Aswin C. Sankaranarayanan
- **Comment**: None
- **Journal**: None
- **Summary**: Many materials have distinct spectral profiles. This facilitates estimation of the material composition of a scene at each pixel by first acquiring its hyperspectral image, and subsequently filtering it using a bank of spectral profiles. This process is inherently wasteful since only a set of linear projections of the acquired measurements contribute to the classification task. We propose a novel programmable camera that is capable of producing images of a scene with an arbitrary spectral filter. We use this camera to optically implement the spectral filtering of the scene's hyperspectral image with the bank of spectral profiles needed to perform per-pixel material classification. This provides gains both in terms of acquisition speed --- since only the relevant measurements are acquired --- and in signal-to-noise ratio --- since we invariably avoid narrowband filters that are light inefficient. Given training data, we use a range of classical and modern techniques including SVMs and neural networks to identify the bank of spectral profiles that facilitate material classification. We verify the method in simulations on standard datasets as well as real data using a lab prototype of the camera.



### Leveraging synthetic imagery for collision-at-sea avoidance
- **Arxiv ID**: http://arxiv.org/abs/1905.04828v1
- **DOI**: 10.1117/12.2306113
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04828v1)
- **Published**: 2019-05-13 02:01:26+00:00
- **Updated**: 2019-05-13 02:01:26+00:00
- **Authors**: Chris M. Ward, Josh Harguess, Alexander G. Corelli
- **Comment**: None
- **Journal**: Proc. SPIE 10645, Geospatial Informatics, Motion Imagery, and
  Network Analytics VIII, 1064507 (4 May 2018)
- **Summary**: Maritime collisions involving multiple ships are considered rare, but in 2017 several United States Navy vessels were involved in fatal at-sea collisions that resulted in the death of seventeen American Servicemembers. The experimentation introduced in this paper is a direct response to these incidents. We propose a shipboard Collision-At-Sea avoidance system, based on video image processing, that will help ensure the safe stationing and navigation of maritime vessels. Our system leverages a convolutional neural network trained on synthetic maritime imagery in order to detect nearby vessels within a scene, perform heading analysis of detected vessels, and provide an alert in the presence of an inbound vessel. Additionally, we present the Navigational Hazards - Synthetic (NAVHAZ-Synthetic) dataset. This dataset, is comprised of one million annotated images of ten vessel classes observed from virtual vessel-mounted cameras, as well as a human "Topside Lookout" perspective. NAVHAZ-Synthetic includes imagery displaying varying sea-states, lighting conditions, and optical degradations such as fog, sea-spray, and salt-accumulation. We present our results on the use of synthetic imagery in a computer vision based collision-at-sea warning system with promising performance.



### A High-Efficiency Framework for Constructing Large-Scale Face Parsing Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1905.04830v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04830v1)
- **Published**: 2019-05-13 02:17:56+00:00
- **Updated**: 2019-05-13 02:17:56+00:00
- **Authors**: Yinglu Liu, Hailin Shi, Yue Si, Hao Shen, Xiaobo Wang, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: Face parsing, which is to assign a semantic label to each pixel in face images, has recently attracted increasing interest due to its huge application potentials. Although many face related fields (e.g., face recognition and face detection) have been well studied for many years, the existing datasets for face parsing are still severely limited in terms of the scale and quality, e.g., the widely used Helen dataset only contains 2,330 images. This is mainly because pixel-level annotation is a high cost and time-consuming work, especially for the facial parts without clear boundaries. The lack of accurate annotated datasets becomes a major obstacle in the progress of face parsing task. It is a feasible way to utilize dense facial landmarks to guide the parsing annotation. However, annotating dense landmarks on human face encounters the same issues as the parsing annotation. To overcome the above problems, in this paper, we develop a high-efficiency framework for face parsing annotation, which considerably simplifies and speeds up the parsing annotation by two consecutive modules. Benefit from the proposed framework, we construct a new Dense Landmark Guided Face Parsing (LaPa) benchmark. It consists of 22,000 face images with large variations in expression, pose, occlusion, etc. Each image is provided with accurate annotation of a 11-category pixel-level label map along with coordinates of 106-point landmarks. To the best of our knowledge, it is currently the largest public dataset for face parsing. To make full use of our LaPa dataset with abundant face shape and boundary priors, we propose a simple yet effective Boundary-Sensitive Parsing Network (BSPNet). Our network is taken as a baseline model on the proposed LaPa dataset, and meanwhile, it achieves the state-of-the-art performance on the Helen dataset without resorting to extra face alignment.



### Multi-Agent Image Classification via Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.04835v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.MA, cs.RO, cs.SY, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.04835v2)
- **Published**: 2019-05-13 02:24:19+00:00
- **Updated**: 2019-08-06 17:16:22+00:00
- **Authors**: Hossein K. Mousavi, Mohammadreza Nazari, Martin Takáč, Nader Motee
- **Comment**: Preprint of the paper to be published in IROS'19 proceedings
- **Journal**: None
- **Summary**: We investigate a classification problem using multiple mobile agents capable of collecting (partial) pose-dependent observations of an unknown environment. The objective is to classify an image over a finite time horizon. We propose a network architecture on how agents should form a local belief, take local actions, and extract relevant features from their raw partial observations. Agents are allowed to exchange information with their neighboring agents to update their own beliefs. It is shown how reinforcement learning techniques can be utilized to achieve decentralized implementation of the classification problem by running a decentralized consensus protocol. Our experimental results on the MNIST handwritten digit dataset demonstrates the effectiveness of our proposed framework.



### Dynamic Routing Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.04849v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.04849v5)
- **Published**: 2019-05-13 03:45:42+00:00
- **Updated**: 2020-11-08 13:11:45+00:00
- **Authors**: Shaofeng Cai, Yao Shu, Wei Wang, Beng Chin Ooi
- **Comment**: 10 pages, 3 figures, 3 tables
- **Journal**: None
- **Summary**: The deployment of deep neural networks in real-world applications is mostly restricted by their high inference costs. Extensive efforts have been made to improve the accuracy with expert-designed or algorithm-searched architectures. However, the incremental improvement is typically achieved with increasingly more expensive models that only a small portion of input instances really need. Inference with a static architecture that processes all input instances via the same transformation would thus incur unnecessary computational costs. Therefore, customizing the model capacity in an instance-aware manner is much needed for higher inference efficiency. In this paper, we propose Dynamic Routing Networks (DRNets), which support efficient instance-aware inference by routing the input instance to only necessary transformation branches selected from a candidate set of branches for each connection between transformation nodes. The branch selection is dynamically determined via the corresponding branch importance weights, which are first generated from lightweight hypernetworks (RouterNets) and then recalibrated with Gumbel-Softmax before the selection. Extensive experiments show that DRNets can reduce a substantial amount of parameter size and FLOPs during inference with prediction performance comparable to state-of-the-art architectures.



### DotSCN: Group Re-identification via Domain-Transferred Single and Couple Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.04854v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1905.04854v2)
- **Published**: 2019-05-13 04:19:10+00:00
- **Updated**: 2020-10-13 06:34:26+00:00
- **Authors**: Ziling Huang, Zheng Wang, Chung-Chi Tsai, Shin'ichi Satoh, Chia-Wen Lin
- **Comment**: accepted in IEEE Transctions on Circuits and Systems for Video
  Technology
- **Journal**: None
- **Summary**: Group re-identification (G-ReID) is an important yet less-studied task. Its challenges not only lie in appearance changes of individuals which have been well-investigated in general person re-identification (ReID), but also derive from group layout and membership changes. So the key task of G-ReID is to learn representations robust to such changes. To address this issue, we propose a Transferred Single and Couple Representation Learning Network (TSCN). Its merits are two aspects: 1) Due to the lack of labelled training samples, existing G-ReID methods mainly rely on unsatisfactory hand-crafted features. To gain the superiority of deep learning models, we treat a group as multiple persons and transfer the domain of a labeled ReID dataset to a G-ReID target dataset style to learn single representations. 2) Taking into account the neighborhood relationship in a group, we further propose learning a novel couple representation between two group members, that achieves more discriminative power in G-ReID tasks. In addition, an unsupervised weight learning method is exploited to adaptively fuse the results of different views together according to result patterns. Extensive experimental results demonstrate the effectiveness of our approach that significantly outperforms state-of-the-art methods by 11.7\% CMC-1 on the Road Group dataset and by 39.0\% CMC-1 on the DukeMCMT dataset.



### Quantifying and Alleviating the Language Prior Problem in Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1905.04877v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1905.04877v1)
- **Published**: 2019-05-13 06:31:33+00:00
- **Updated**: 2019-05-13 06:31:33+00:00
- **Authors**: Yangyang Guo, Zhiyong Cheng, Liqiang Nie, Yibing Liu, Yinglong Wang, Mohan Kankanhalli
- **Comment**: None
- **Journal**: None
- **Summary**: Benefiting from the advancement of computer vision, natural language processing and information retrieval techniques, visual question answering (VQA), which aims to answer questions about an image or a video, has received lots of attentions over the past few years. Although some progress has been achieved so far, several studies have pointed out that current VQA models are heavily affected by the \emph{language prior problem}, which means they tend to answer questions based on the co-occurrence patterns of question keywords (e.g., how many) and answers (e.g., 2) instead of understanding images and questions. Existing methods attempt to solve this problem by either balancing the biased datasets or forcing models to better understand images. However, only marginal effects and even performance deterioration are observed for the first and second solution, respectively. In addition, another important issue is the lack of measurement to quantitatively measure the extent of the language prior effect, which severely hinders the advancement of related techniques.   In this paper, we make contributions to solve the above problems from two perspectives. Firstly, we design a metric to quantitatively measure the language prior effect of VQA models. The proposed metric has been demonstrated to be effective in our empirical studies. Secondly, we propose a regularization method (i.e., score regularization module) to enhance current VQA models by alleviating the language prior problem as well as boosting the backbone model performance. The proposed score regularization module adopts a pair-wise learning strategy, which makes the VQA models answer the question based on the reasoning of the image (upon this question) instead of basing on question-answer patterns observed in the biased training set. The score regularization module is flexible to be integrated into various VQA models.



### FPGA-based Binocular Image Feature Extraction and Matching System
- **Arxiv ID**: http://arxiv.org/abs/1905.04890v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1905.04890v2)
- **Published**: 2019-05-13 07:31:26+00:00
- **Updated**: 2019-05-14 01:03:34+00:00
- **Authors**: Qi Ni, Fei Wang, Ziwei Zhao, Peng Gao
- **Comment**: Accepted for the 4th International Conference on Multimedia Systems
  and Signal Processing (ICMSSP 2019)
- **Journal**: None
- **Summary**: Image feature extraction and matching is a fundamental but computation intensive task in machine vision. This paper proposes a novel FPGA-based embedded system to accelerate feature extraction and matching. It implements SURF feature point detection and BRIEF feature descriptor construction and matching. For binocular stereo vision, feature matching includes both tracking matching and stereo matching, which simultaneously provide feature point correspondences and parallax information. Our system is evaluated on a ZYNQ XC7Z045 FPGA. The result demonstrates that it can process binocular video data at a high frame rate (640$\times$480 @ 162fps). Moreover, an extensive test proves our system has robustness for image compression, blurring and illumination.



### CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features
- **Arxiv ID**: http://arxiv.org/abs/1905.04899v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.04899v2)
- **Published**: 2019-05-13 08:10:22+00:00
- **Updated**: 2019-08-07 07:15:29+00:00
- **Authors**: Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, Youngjoon Yoo
- **Comment**: Accepted at ICCV 2019 (oral talk). 14 pages, 5 figures
- **Journal**: None
- **Summary**: Regional dropout strategies have been proposed to enhance the performance of convolutional neural network classifiers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout remove informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it leads to information loss and inefficiency during training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on the ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classifier, when used as a pretrained model, results in consistent performance gains in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances. Source code and pretrained models are available at https://github.com/clovaai/CutMix-PyTorch .



### Automatic Calibration of Multiple 3D LiDARs in Urban Environments
- **Arxiv ID**: http://arxiv.org/abs/1905.04912v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.04912v1)
- **Published**: 2019-05-13 08:51:30+00:00
- **Updated**: 2019-05-13 08:51:30+00:00
- **Authors**: Jianhao Jiao, Yang Yu, Qinghai Liao, Haoyang Ye, Ming Liu
- **Comment**: 7 pages, 10 figures, submitted to IROS 2019
- **Journal**: None
- **Summary**: Multiple LiDARs have progressively emerged on autonomous vehicles for rendering a wide field of view and dense measurements. However, the lack of precise calibration negatively affects their potential applications in localization and perception systems. In this paper, we propose a novel system that enables automatic multi-LiDAR calibration without any calibration target, prior environmental information, and initial values of the extrinsic parameters. Our approach starts with a hand-eye calibration for automatic initialization by aligning the estimated motions of each sensor. The resulting parameters are then refined with an appearance-based method by minimizing a cost function constructed from point-plane correspondences. Experimental results on simulated and real-world data sets demonstrate the reliability and accuracy of our calibration approach. The proposed approach can calibrate a multi-LiDAR system with the rotation and translation errors less than 0.04 [rad] and 0.1 [m] respectively for a mobile platform.



### Few-Shot Viewpoint Estimation
- **Arxiv ID**: http://arxiv.org/abs/1905.04957v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04957v2)
- **Published**: 2019-05-13 10:54:28+00:00
- **Updated**: 2019-07-31 22:10:01+00:00
- **Authors**: Hung-Yu Tseng, Shalini De Mello, Jonathan Tremblay, Sifei Liu, Stan Birchfield, Ming-Hsuan Yang, Jan Kautz
- **Comment**: BMVC 2019
- **Journal**: None
- **Summary**: Viewpoint estimation for known categories of objects has been improved significantly thanks to deep networks and large datasets, but generalization to unknown categories is still very challenging. With an aim towards improving performance on unknown categories, we introduce the problem of category-level few-shot viewpoint estimation. We design a novel framework to successfully train viewpoint networks for new categories with few examples (10 or less). We formulate the problem as one of learning to estimate category-specific 3D canonical shapes, their associated depth estimates, and semantic 2D keypoints. We apply meta-learning to learn weights for our network that are amenable to category-specific few-shot fine-tuning. Furthermore, we design a flexible meta-Siamese network that maximizes information sharing during meta-learning. Through extensive experimentation on the ObjectNet3D and Pascal3D+ benchmark datasets, we demonstrate that our framework, which we call MetaView, significantly outperforms fine-tuning the state-of-the-art models with few examples, and that the specific architectural innovations of our method are crucial to achieving good performance.



### Implicit Filter Sparsification In Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.04967v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.04967v1)
- **Published**: 2019-05-13 11:10:14+00:00
- **Updated**: 2019-05-13 11:10:14+00:00
- **Authors**: Dushyant Mehta, Kwang In Kim, Christian Theobalt
- **Comment**: ODML-CDNNR 2019 (ICML'19 workshop) extended abstract of the CVPR 2019
  paper "On Implicit Filter Level Sparsity in Convolutional Neural Networks,
  Mehta et al." (arXiv:1811.12495)
- **Journal**: None
- **Summary**: We show implicit filter level sparsity manifests in convolutional neural networks (CNNs) which employ Batch Normalization and ReLU activation, and are trained with adaptive gradient descent techniques and L2 regularization or weight decay. Through an extensive empirical study (Mehta et al., 2019) we hypothesize the mechanism behind the sparsification process, and find surprising links to certain filter sparsification heuristics proposed in literature. Emergence of, and the subsequent pruning of selective features is observed to be one of the contributing mechanisms, leading to feature sparsity at par or better than certain explicit sparsification / pruning approaches. In this workshop article we summarize our findings, and point out corollaries of selective-featurepenalization which could also be employed as heuristics for filter pruning



### Craquelure as a Graph: Application of Image Processing and Graph Neural Networks to the Description of Fracture Patterns
- **Arxiv ID**: http://arxiv.org/abs/1905.05010v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05010v2)
- **Published**: 2019-05-13 12:39:01+00:00
- **Updated**: 2019-12-04 06:44:09+00:00
- **Authors**: Oleksii Sidorov, Jon Yngve Hardeberg
- **Comment**: Published in ICCV 2019 Workshops
- **Journal**: None
- **Summary**: Cracks on a painting is not a defect but an inimitable signature of an artwork which can be used for origin examination, aging monitoring, damage identification, and even forgery detection. This work presents the development of a new methodology and corresponding toolbox for the extraction and characterization of information from an image of a craquelure pattern.   The proposed approach processes craquelure network as a graph. The graph representation captures the network structure via mutual organization of junctions and fractures. Furthermore, it is invariant to any geometrical distortions. At the same time, our tool extracts the properties of each node and edge individually, which allows to characterize the pattern statistically.   We illustrate benefits from the graph representation and statistical features individually using novel Graph Neural Network and hand-crafted descriptors correspondingly. However, we also show that the best performance is achieved when both techniques are merged into one framework. We perform experiments on the dataset for paintings' origin classification and demonstrate that our approach outperforms existing techniques by a large margin.



### Joint Object and State Recognition using Language Knowledge
- **Arxiv ID**: http://arxiv.org/abs/1905.08843v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.08843v1)
- **Published**: 2019-05-13 13:26:17+00:00
- **Updated**: 2019-05-13 13:26:17+00:00
- **Authors**: Ahmad Babaeian Jelodar, Yu Sun
- **Comment**: 5 pages, 4 figures, 1 table
- **Journal**: None
- **Summary**: The state of an object is an important piece of knowledge in robotics applications. States and objects are intertwined together, meaning that object information can help recognize the state of an image and vice versa. This paper addresses the state identification problem in cooking related images and uses state and object predictions together to improve the classification accuracy of objects and their states from a single image. The pipeline presented in this paper includes a CNN with a double classification layer and the Concept-Net language knowledge graph on top. The language knowledge creates a semantic likelihood between objects and states. The resulting object and state confidences from the deep architecture are used together with object and state relatedness estimates from a language knowledge graph to produce marginal probabilities for objects and states. The marginal probabilities and confidences of objects (or states) are fused together to improve the final object (or state) classification results. Experiments on a dataset of cooking objects show that using a language knowledge graph on top of a deep neural network effectively enhances object and state classification.



### Precipitation nowcasting using a stochastic variational frame predictor with learned prior distribution
- **Arxiv ID**: http://arxiv.org/abs/1905.05037v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, physics.ao-ph
- **Links**: [PDF](http://arxiv.org/pdf/1905.05037v1)
- **Published**: 2019-05-13 13:51:51+00:00
- **Updated**: 2019-05-13 13:51:51+00:00
- **Authors**: Alexander Bihlo
- **Comment**: 7 pages, 3 figures, release version
- **Journal**: None
- **Summary**: We propose the use of a stochastic variational frame prediction deep neural network with a learned prior distribution trained on two-dimensional rain radar reflectivity maps for precipitation nowcasting with lead times of up to 2 1/2 hours. We present a comparison to a standard convolutional LSTM network and assess the evolution of the structural similarity index for both methods. Case studies are presented that illustrate that the novel methodology can yield meaningful forecasts without excessive blur for the time horizons of interest.



### Object Detection in 20 Years: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1905.05055v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05055v3)
- **Published**: 2019-05-13 14:26:50+00:00
- **Updated**: 2023-01-18 14:23:50+00:00
- **Authors**: Zhengxia Zou, Keyan Chen, Zhenwei Shi, Yuhong Guo, Jieping Ye
- **Comment**: Accepted by Proceedings of the IEEE
- **Journal**: None
- **Summary**: Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Over the past two decades, we have seen a rapid technological evolution of object detection and its profound impact on the entire computer vision field. If we consider today's object detection technique as a revolution driven by deep learning, then back in the 1990s, we would see the ingenious thinking and long-term perspective design of early computer vision. This paper extensively reviews this fast-moving research field in the light of technical evolution, spanning over a quarter-century's time (from the 1990s to 2022). A number of topics have been covered in this paper, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed-up techniques, and the recent state-of-the-art detection methods.



### Medical image super-resolution method based on dense blended attention network
- **Arxiv ID**: http://arxiv.org/abs/1905.05084v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1905.05084v1)
- **Published**: 2019-05-13 15:25:37+00:00
- **Updated**: 2019-05-13 15:25:37+00:00
- **Authors**: Kewen Liu, Yuan Ma, Hongxia Xiong, Zejun Yan, Zhijun Zhou, Panpan Fang, Chaoyang Liu
- **Comment**: 12 pages, 4 figures, 32 references
- **Journal**: None
- **Summary**: In order to address the issue that medical image would suffer from severe blurring caused by the lack of high-frequency details in the process of image super-resolution reconstruction, a novel medical image super-resolution method based on dense neural network and blended attention mechanism is proposed. The proposed method adds blended attention blocks to dense neural network(DenseNet), so that the neural network can concentrate more attention to the regions and channels with sufficient high-frequency details. Batch normalization layers are removed to avoid loss of high-frequency texture details. Final obtained high resolution medical image are obtained using deconvolutional layers at the very end of the network as up-sampling operators. Experimental results show that the proposed method has an improvement of 0.05db to 11.25dB and 0.6% to 14.04% on the peak signal-to-noise ratio(PSNR) metric and structural similarity index(SSIM) metric, respectively, compared with the mainstream image super-resolution methods. This work provides a new idea for theoretical studies of medical image super-resolution reconstruction.



### A novel statistical metric learning for hyperspectral image classification
- **Arxiv ID**: http://arxiv.org/abs/1905.05087v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05087v1)
- **Published**: 2019-05-13 15:28:15+00:00
- **Updated**: 2019-05-13 15:28:15+00:00
- **Authors**: Zhiqiang Gong, Ping Zhong, Weidong Hu, Zixuan Xiao, Xuping Yin
- **Comment**: Submitted to Whispers2019
- **Journal**: None
- **Summary**: In this paper, a novel statistical metric learning is developed for spectral-spatial classification of the hyperspectral image. First, the standard variance of the samples of each class in each batch is used to decrease the intra-class variance within each class. Then, the distances between the means of different classes are used to penalize the inter-class variance of the training samples. Finally, the standard variance between the means of different classes is added as an additional diversity term to repulse different classes from each other. Experiments have conducted over two real-world hyperspectral image datasets and the experimental results have shown the effectiveness of the proposed statistical metric learning.



### Weakly-supervised Caricature Face Parsing through Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1905.05091v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05091v1)
- **Published**: 2019-05-13 15:36:41+00:00
- **Updated**: 2019-05-13 15:36:41+00:00
- **Authors**: Wenqing Chu, Wei-Chih Hung, Yi-Hsuan Tsai, Deng Cai, Ming-Hsuan Yang
- **Comment**: Accepted in ICIP 2019, code and model are available at
  https://github.com/ZJULearning/CariFaceParsing
- **Journal**: None
- **Summary**: A caricature is an artistic form of a person's picture in which certain striking characteristics are abstracted or exaggerated in order to create a humor or sarcasm effect. For numerous caricature related applications such as attribute recognition and caricature editing, face parsing is an essential pre-processing step that provides a complete facial structure understanding. However, current state-of-the-art face parsing methods require large amounts of labeled data on the pixel-level and such process for caricature is tedious and labor-intensive. For real photos, there are numerous labeled datasets for face parsing. Thus, we formulate caricature face parsing as a domain adaptation problem, where real photos play the role of the source domain, adapting to the target caricatures. Specifically, we first leverage a spatial transformer based network to enable shape domain shifts. A feed-forward style transfer network is then utilized to capture texture-level domain gaps. With these two steps, we synthesize face caricatures from real photos, and thus we can use parsing ground truths of the original photos to learn the parsing model. Experimental results on the synthetic and real caricatures demonstrate the effectiveness of the proposed domain adaptation algorithm. Code is available at: https://github.com/ZJULearning/CariFaceParsing .



### Joint Demosaicking and Denoising by Fine-Tuning of Bursts of Raw Images
- **Arxiv ID**: http://arxiv.org/abs/1905.05092v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.05092v2)
- **Published**: 2019-05-13 15:36:50+00:00
- **Updated**: 2019-09-10 09:21:54+00:00
- **Authors**: Thibaud Ehret, Axel Davy, Pablo Arias, Gabriele Facciolo
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Demosaicking and denoising are the first steps of any camera image processing pipeline and are key for obtaining high quality RGB images. A promising current research trend aims at solving these two problems jointly using convolutional neural networks. Due to the unavailability of ground truth data these networks cannot be currently trained using real RAW images. Instead, they resort to simulated data. In this paper we present a method to learn demosaicking directly from mosaicked images, without requiring ground truth RGB data. We apply this to learn joint demosaicking and denoising only from RAW images, thus enabling the use of real data. In addition we show that for this application fine-tuning a network to a specific burst improves the quality of restoration for both demosaicking and denoising.



### Block Coordinate Regularization by Denoising
- **Arxiv ID**: http://arxiv.org/abs/1905.05113v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.05113v2)
- **Published**: 2019-05-13 16:04:38+00:00
- **Updated**: 2019-09-19 23:47:05+00:00
- **Authors**: Yu Sun, Jiaming Liu, Ulugbek S. Kamilov
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of estimating a vector from its noisy measurements using a prior specified only through a denoising function. Recent work on plug-and-play priors (PnP) and regularization-by-denoising (RED) has shown the state-of-the-art performance of estimators under such priors in a range of imaging tasks. In this work, we develop a new block coordinate RED algorithm that decomposes a large-scale estimation problem into a sequence of updates over a small subset of the unknown variables. We theoretically analyze the convergence of the algorithm and discuss its relationship to the traditional proximal optimization. Our analysis complements and extends recent theoretical results for RED-based estimation methods. We numerically validate our method using several denoiser priors, including those based on convolutional neural network (CNN) denoisers.



### Harnessing the Vulnerability of Latent Layers in Adversarially Trained Models
- **Arxiv ID**: http://arxiv.org/abs/1905.05186v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.05186v2)
- **Published**: 2019-05-13 16:44:03+00:00
- **Updated**: 2019-06-25 19:38:57+00:00
- **Authors**: Mayank Singh, Abhishek Sinha, Nupur Kumari, Harshitha Machiraju, Balaji Krishnamurthy, Vineeth N Balasubramanian
- **Comment**: Accepted at IJCAI 2019
- **Journal**: None
- **Summary**: Neural networks are vulnerable to adversarial attacks -- small visually imperceptible crafted noise which when added to the input drastically changes the output. The most effective method of defending against these adversarial attacks is to use the methodology of adversarial training. We analyze the adversarially trained robust models to study their vulnerability against adversarial attacks at the level of the latent layers. Our analysis reveals that contrary to the input layer which is robust to adversarial attack, the latent layer of these robust models are highly susceptible to adversarial perturbations of small magnitude. Leveraging this information, we introduce a new technique Latent Adversarial Training (LAT) which comprises of fine-tuning the adversarially trained models to ensure the robustness at the feature layers. We also propose Latent Attack (LA), a novel algorithm for construction of adversarial examples. LAT results in minor improvement in test accuracy and leads to a state-of-the-art adversarial accuracy against the universal first-order adversarial PGD attack which is shown for the MNIST, CIFAR-10, CIFAR-100 datasets.



### VideoGraph: Recognizing Minutes-Long Human Activities in Videos
- **Arxiv ID**: http://arxiv.org/abs/1905.05143v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05143v2)
- **Published**: 2019-05-13 16:57:40+00:00
- **Updated**: 2019-10-13 09:44:11+00:00
- **Authors**: Noureldien Hussein, Efstratios Gavves, Arnold W. M. Smeulders
- **Comment**: None
- **Journal**: ICCV 2019, Workshop on Scene Graph Representation and Learning
- **Summary**: Many human activities take minutes to unfold. To represent them, related works opt for statistical pooling, which neglects the temporal structure. Others opt for convolutional methods, as CNN and Non-Local. While successful in learning temporal concepts, they are short of modeling minutes-long temporal dependencies. We propose VideoGraph, a method to achieve the best of two worlds: represent minutes-long human activities and learn their underlying temporal structure. VideoGraph learns a graph-based representation for human activities. The graph, its nodes and edges are learned entirely from video datasets, making VideoGraph applicable to problems without node-level annotation. The result is improvements over related works on benchmarks: Epic-Kitchen and Breakfast. Besides, we demonstrate that VideoGraph is able to learn the temporal structure of human activities in minutes-long videos.



### Zoom To Learn, Learn To Zoom
- **Arxiv ID**: http://arxiv.org/abs/1905.05169v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.05169v1)
- **Published**: 2019-05-13 17:56:15+00:00
- **Updated**: 2019-05-13 17:56:15+00:00
- **Authors**: Xuaner Cecilia Zhang, Qifeng Chen, Ren Ng, Vladlen Koltun
- **Comment**: CVPR 2019,
  https://ceciliavision.github.io/project-pages/project-zoom.html (paper,
  video, supp, code, dataset)
- **Journal**: None
- **Summary**: This paper shows that when applying machine learning to digital zoom for photography, it is beneficial to use real, RAW sensor data for training. Existing learning-based super-resolution methods do not use real sensor data, instead operating on RGB images. In practice, these approaches result in loss of detail and accuracy in their digitally zoomed output when zooming in on distant image regions. We also show that synthesizing sensor data by resampling high-resolution RGB images is an oversimplified approximation of real sensor data and noise, resulting in worse image quality. The key barrier to using real sensor data for training is that ground truth high-resolution imagery is missing. We show how to obtain the ground-truth data with optically zoomed images and contribute a dataset, SR-RAW, for real-world computational zoom. We use SR-RAW to train a deep network with a novel contextual bilateral loss (CoBi) that delivers critical robustness to mild misalignment in input-output image pairs. The trained network achieves state-of-the-art performance in 4X and 8X computational zoom.



### PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization
- **Arxiv ID**: http://arxiv.org/abs/1905.05172v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1905.05172v3)
- **Published**: 2019-05-13 17:59:56+00:00
- **Updated**: 2019-12-03 19:00:16+00:00
- **Authors**: Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, Hao Li
- **Comment**: project page: https://shunsukesaito.github.io/PIFu
- **Journal**: The IEEE International Conference on Computer Vision (ICCV), 2019,
  pp. 2304-2314
- **Summary**: We introduce Pixel-aligned Implicit Function (PIFu), a highly effective implicit representation that locally aligns pixels of 2D images with the global context of their corresponding 3D object. Using PIFu, we propose an end-to-end deep learning method for digitizing highly detailed clothed humans that can infer both 3D surface and texture from a single image, and optionally, multiple input images. Highly intricate shapes, such as hairstyles, clothing, as well as their variations and deformations can be digitized in a unified way. Compared to existing representations used for 3D deep learning, PIFu can produce high-resolution surfaces including largely unseen regions such as the back of a person. In particular, it is memory efficient unlike the voxel representation, can handle arbitrary topology, and the resulting surface is spatially aligned with the input image. Furthermore, while previous techniques are designed to process either a single image or multiple views, PIFu extends naturally to arbitrary number of views. We demonstrate high-resolution and robust reconstructions on real world images from the DeepFashion dataset, which contains a variety of challenging clothing types. Our method achieves state-of-the-art performance on a public benchmark and outperforms the prior work for clothed human digitization from a single image.



### Lightweight Monocular Depth Estimation Model by Joint End-to-End Filter pruning
- **Arxiv ID**: http://arxiv.org/abs/1905.05212v1
- **DOI**: 10.1109/ICIP.2019.8803544
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05212v1)
- **Published**: 2019-05-13 18:01:01+00:00
- **Updated**: 2019-05-13 18:01:01+00:00
- **Authors**: Sara Elkerdawy, Hong Zhang, Nilanjan Ray
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have emerged as the state-of-the-art in multiple vision tasks including depth estimation. However, memory and computing power requirements remain as challenges to be tackled in these models. Monocular depth estimation has significant use in robotics and virtual reality that requires deployment on low-end devices. Training a small model from scratch results in a significant drop in accuracy and it does not benefit from pre-trained large models. Motivated by the literature of model pruning, we propose a lightweight monocular depth model obtained from a large trained model. This is achieved by removing the least important features with a novel joint end-to-end filter pruning. We propose to learn a binary mask for each filter to decide whether to drop the filter or not. These masks are trained jointly to exploit relations between filters at different layers as well as redundancy within the same layer. We show that we can achieve around 5x compression rate with small drop in accuracy on the KITTI driving dataset. We also show that masking can improve accuracy over the baseline with fewer parameters, even without enforcing compression loss.



### Deep Neural Networks for Marine Debris Detection in Sonar Images
- **Arxiv ID**: http://arxiv.org/abs/1905.05241v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05241v1)
- **Published**: 2019-05-13 18:51:40+00:00
- **Updated**: 2019-05-13 18:51:40+00:00
- **Authors**: Matias Valdenegro-Toro
- **Comment**: PhD Thesis submitted to Heriot-Watt University
- **Journal**: None
- **Summary**: Garbage and waste disposal is one of the biggest challenges currently faced by mankind. Proper waste disposal and recycling is a must in any sustainable community, and in many coastal areas there is significant water pollution in the form of floating or submerged garbage. This is called marine debris. Submerged marine debris threatens marine life, and for shallow coastal areas, it can also threaten fishing vessels [I\~niguez et al. 2016, Renewable and Sustainable Energy Reviews]. Submerged marine debris typically stays in the environment for a long time (20+ years), and consists of materials that can be recycled, such as metals, plastics, glass, etc. Many of these items should not be disposed in water bodies as this has a negative effect in the environment and human health. This thesis performs a comprehensive evaluation on the use of DNNs for the problem of marine debris detection in FLS images, as well as related problems such as image classification, matching, and detection proposals. We do this in a dataset of 2069 FLS images that we captured with an ARIS Explorer 3000 sensor on marine debris objects lying in the floor of a small water tank. The objects we used to produce this dataset contain typical household marine debris and distractor marine objects (tires, hooks, valves, etc), divided in 10 classes plus a background class. Our results show that for the evaluated tasks, DNNs are a superior technique than the corresponding state of the art. There are large gains particularly for the matching and detection proposal tasks. We also study the effect of sample complexity and object size in many tasks, which is valuable information for practitioners. We expect that our results will advance the objective of using Autonomous Underwater Vehicles to automatically survey, detect and collect marine debris from underwater environments.



### Robustness Analysis of Face Obscuration
- **Arxiv ID**: http://arxiv.org/abs/1905.05243v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05243v2)
- **Published**: 2019-05-13 19:03:35+00:00
- **Updated**: 2019-10-15 20:30:48+00:00
- **Authors**: Hanxiang Hao, David Güera, János Horváth, Amy R. Reibman, Edward J. Delp
- **Comment**: None
- **Journal**: None
- **Summary**: Face obscuration is needed by law enforcement and mass media outlets to guarantee privacy. Sharing sensitive content where obscuration or redaction techniques have failed to completely remove all identifiable traces can lead to many legal and social issues. Hence, we need to be able to systematically measure the face obscuration performance of a given technique. In this paper we propose to measure the effectiveness of eight obscuration techniques. We do so by attacking the redacted faces in three scenarios: obscured face identification, verification, and reconstruction. Threat modeling is also considered to provide a vulnerability analysis for each studied obscuration technique. Based on our evaluation, we show that the k-same based methods are the most effective.



### Cooper: Cooperative Perception for Connected Autonomous Vehicles based on 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1905.05265v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.05265v1)
- **Published**: 2019-05-13 19:57:12+00:00
- **Updated**: 2019-05-13 19:57:12+00:00
- **Authors**: Qi Chen, Sihai Tang, Qing Yang, Song Fu
- **Comment**: Accepted by the 39th IEEE International Conference on Distributed
  Computing Systems (ICDCS 2019)
- **Journal**: None
- **Summary**: Autonomous vehicles may make wrong decisions due to inaccurate detection and recognition. Therefore, an intelligent vehicle can combine its own data with that of other vehicles to enhance perceptive ability, and thus improve detection accuracy and driving safety. However, multi-vehicle cooperative perception requires the integration of real world scenes and the traffic of raw sensor data exchange far exceeds the bandwidth of existing vehicular networks. To the best our knowledge, we are the first to conduct a study on raw-data level cooperative perception for enhancing the detection ability of self-driving systems. In this work, relying on LiDAR 3D point clouds, we fuse the sensor data collected from different positions and angles of connected vehicles. A point cloud based 3D object detection method is proposed to work on a diversity of aligned point clouds. Experimental results on KITTI and our collected dataset show that the proposed system outperforms perception by extending sensing area, improving detection accuracy and promoting augmented results. Most importantly, we demonstrate it is possible to transmit point clouds data for cooperative perception via existing vehicular network technologies.



### VGG Fine-tuning for Cooking State Recognition
- **Arxiv ID**: http://arxiv.org/abs/1905.08606v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08606v1)
- **Published**: 2019-05-13 20:49:14+00:00
- **Updated**: 2019-05-13 20:49:14+00:00
- **Authors**: Juan Wilches
- **Comment**: None
- **Journal**: None
- **Summary**: An important task that domestic robots need to achieve is the recognition of states of food ingredients so they can continue their cooking actions. This project focuses on a fine-tuning algorithm for the VGG (Visual Geometry Group) architecture of deep convolutional neural networks (CNN) for object recognition. The algorithm aims to identify eleven different ingredient cooking states for an image dataset. The original VGG model was adjusted and trained to properly classify the food states. The model was initialized with Imagenet weights. Different experiments were carried out in order to find the model parameters that provided the best performance. The accuracy achieved for the validation set was 76.7% and for the test set 76.6% after changing several parameters of the VGG model.



### TopoResNet: A hybrid deep learning architecture and its application to skin lesion classification
- **Arxiv ID**: http://arxiv.org/abs/1905.08607v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.08607v1)
- **Published**: 2019-05-13 21:16:39+00:00
- **Updated**: 2019-05-13 21:16:39+00:00
- **Authors**: Yu-Min Chung, Chuan-Shen Hu, Austin Lawson, Clifford Smyth
- **Comment**: None
- **Journal**: None
- **Summary**: Skin cancer is one of the most common cancers in the United States. As technological advancements are made, algorithmic diagnosis of skin lesions is becoming more important. In this paper, we develop algorithms for segmenting the actual diseased area of skin in a given image of a skin lesion, and for classifying different types of skin lesions pictured in a given image. The cores of the algorithms used were based in persistent homology, an algebraic topology technique that is part of the rising field of Topological Data Analysis (TDA). The segmentation algorithm utilizes a similar concept to persistent homology that captures the robustness of segmented regions. For classification, we design two families of topological features from persistence diagrams---which we refer to as {\em persistence statistics} (PS) and {\em persistence curves} (PC), and use linear support vector machine as classifiers. We also combined those topological features, PS and PC, into ResNet-101 model, which we call {\em TopoResNet-101}, the results show that PS and PC are effective in two folds---improving classification performances and stabilizing the training process. Although convolutional features are the most important learning targets in CNN models, global information of images may be lost in the training process. Because topological features were extracted globally, our results show that the global property of topological features provide additional information to machine learning models.



### Affine Variational Autoencoders: An Efficient Approach for Improving Generalization and Robustness to Distribution Shift
- **Arxiv ID**: http://arxiv.org/abs/1905.05300v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.05300v1)
- **Published**: 2019-05-13 21:56:27+00:00
- **Updated**: 2019-05-13 21:56:27+00:00
- **Authors**: Rene Bidart, Alexander Wong
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: In this study, we propose the Affine Variational Autoencoder (AVAE), a variant of Variational Autoencoder (VAE) designed to improve robustness by overcoming the inability of VAEs to generalize to distributional shifts in the form of affine perturbations. By optimizing an affine transform to maximize ELBO, the proposed AVAE transforms an input to the training distribution without the need to increase model complexity to model the full distribution of affine transforms. In addition, we introduce a training procedure to create an efficient model by learning a subset of the training distribution, and using the AVAE to improve generalization and robustness to distributional shift at test time. Experiments on affine perturbations demonstrate that the proposed AVAE significantly improves generalization and robustness to distributional shift in the form of affine perturbations without an increase in model complexity.



