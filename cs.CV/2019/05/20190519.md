# Arxiv Papers in cs.CV on 2019-05-19
### What Do Adversarially Robust Models Look At?
- **Arxiv ID**: http://arxiv.org/abs/1905.07666v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.07666v1)
- **Published**: 2019-05-19 00:06:08+00:00
- **Updated**: 2019-05-19 00:06:08+00:00
- **Authors**: Takahiro Itazuri, Yoshihiro Fukuhara, Hirokatsu Kataoka, Shigeo Morishima
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the open question: "What do adversarially robust models look at?" Recently, it has been reported in many works that there exists the trade-off between standard accuracy and adversarial robustness. According to prior works, this trade-off is rooted in the fact that adversarially robust and standard accurate models might depend on very different sets of features. However, it has not been well studied what kind of difference actually exists. In this paper, we analyze this difference through various experiments visually and quantitatively. Experimental results show that adversarially robust models look at things at a larger scale than standard models and pay less attention to fine textures. Furthermore, although it has been claimed that adversarially robust features are not compatible with standard accuracy, there is even a positive effect by using them as pre-trained models particularly in low resolution datasets.



### FORECAST-CLSTM: A New Convolutional LSTM Network for Cloudage Nowcasting
- **Arxiv ID**: http://arxiv.org/abs/1905.07700v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.07700v1)
- **Published**: 2019-05-19 07:34:57+00:00
- **Updated**: 2019-05-19 07:34:57+00:00
- **Authors**: Chao Tan, Xin Feng, Jianwu Long, Li Geng
- **Comment**: None
- **Journal**: IEEE Conference of Visual Communications and Image Processing 2018
- **Summary**: With the highly demand of large-scale and real-time weather service for public, a refinement of short-time cloudage prediction has become an essential part of the weather forecast productions. To provide a weather-service-compliant cloudage nowcasting, in this paper, we propose a novel hierarchical Convolutional Long-Short-Term Memory network based deep learning model, which we term as FORECAST-CLSTM, with a new Forecaster loss function to predict the future satellite cloud images. The model is designed to fuse multi-scale features in the hierarchical network structure to predict the pixel value and the morphological movement of the cloudage simultaneously. We also collect about 40K infrared satellite nephograms and create a large-scale Satellite Cloudage Map Dataset(SCMD). The proposed FORECAST-CLSTM model is shown to achieve better prediction performance compared with the state-of-the-art ConvLSTM model and the proposed Forecaster Loss Function is also demonstrated to retain the uncertainty of the real atmosphere condition better than conventional loss function.



### An Objective Evaluation Metric for image fusion based on Del Operator
- **Arxiv ID**: http://arxiv.org/abs/1905.07709v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.07709v2)
- **Published**: 2019-05-19 08:41:51+00:00
- **Updated**: 2019-05-25 14:36:12+00:00
- **Authors**: Ali A. Kiaei, Hassan Khotanlou, Mahdi Abbasi, Paniz Kiaei, Yasin Bhrouzi
- **Comment**: 22 pages, 14 Figures
- **Journal**: None
- **Summary**: In this paper, a novel objective evaluation metric for image fusion is presented. Remarkable and attractive points of the proposed metric are that it has no parameter, the result is probability in the range of [0, 1] and it is free from illumination dependence. This metric is easy to implement and the result is computed in four steps: (1) Smoothing the images using Gaussian filter. (2) Transforming images to a vector field using Del operator. (3) Computing the normal distribution function ({\mu},{\sigma}) for each corresponding pixel, and converting to the standard normal distribution function. (4) Computing the probability of being well-behaved fusion method as the result. To judge the quality of the proposed metric, it is compared to thirteen well-known non-reference objective evaluation metrics, where eight fusion methods are employed on seven experiments of multimodal medical images. The experimental results and statistical comparisons show that in contrast to the previously objective evaluation metrics the proposed one performs better in terms of both agreeing with human visual perception and evaluating fusion methods that are not performed at the same level.



### A 2D dilated residual U-Net for multi-organ segmentation in thoracic CT
- **Arxiv ID**: http://arxiv.org/abs/1905.07710v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.07710v1)
- **Published**: 2019-05-19 08:53:13+00:00
- **Updated**: 2019-05-19 08:53:13+00:00
- **Authors**: Sulaiman Vesal, Nishant Ravikumar, Andreas Maier
- **Comment**: ISBI-SegTHOR 2019 Challenge
- **Journal**: None
- **Summary**: Automatic segmentation of organs-at-risk (OAR) in computed tomography (CT) is an essential part of planning effective treatment strategies to combat lung and esophageal cancer. Accurate segmentation of organs surrounding tumours helps account for the variation in position and morphology inherent across patients, thereby facilitating adaptive and computer-assisted radiotherapy. Although manual delineation of OARs is still highly prevalent, it is prone to errors due to complex variations in the shape and position of organs across patients, and low soft tissue contrast between neighbouring organs in CT images. Recently, deep convolutional neural networks (CNNs) have gained tremendous traction and achieved state-of-the-art results in medical image segmentation. In this paper, we propose a deep learning framework to segment OARs in thoracic CT images, specifically for the: heart, esophagus, trachea and aorta. Our approach employs dilated convolutions and aggregated residual connections in the bottleneck of a U-Net styled network, which incorporates global context and dense information. Our method achieved an overall Dice score of 91.57% on 20 unseen test samples from the ISBI 2019 SegTHOR challenge.



### Geometric Pose Affordance: 3D Human Pose with Scene Constraints
- **Arxiv ID**: http://arxiv.org/abs/1905.07718v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.07718v2)
- **Published**: 2019-05-19 10:04:52+00:00
- **Updated**: 2021-12-09 01:20:45+00:00
- **Authors**: Zhe Wang, Liyan Chen, Shaurya Rathore, Daeyun Shin, Charless Fowlkes
- **Comment**: $\href{https://wangzheallen.github.io/GPA.html}{Project Page}$, in
  submission to CVIU
- **Journal**: None
- **Summary**: Full 3D estimation of human pose from a single image remains a challenging task despite many recent advances. In this paper, we explore the hypothesis that strong prior information about scene geometry can be used to improve pose estimation accuracy. To tackle this question empirically, we have assembled a novel $\textbf{Geometric Pose Affordance}$ dataset, consisting of multi-view imagery of people interacting with a variety of rich 3D environments. We utilized a commercial motion capture system to collect gold-standard estimates of pose and construct accurate geometric 3D CAD models of the scene itself.   To inject prior knowledge of scene constraints into existing frameworks for pose estimation from images, we introduce a novel, view-based representation of scene geometry, a $\textbf{multi-layer depth map}$, which employs multi-hit ray tracing to concisely encode multiple surface entry and exit points along each camera view ray direction. We propose two different mechanisms for integrating multi-layer depth information pose estimation: input as encoded ray features used in lifting 2D pose to full 3D, and secondly as a differentiable loss that encourages learned models to favor geometrically consistent pose estimates. We show experimentally that these techniques can improve the accuracy of 3D pose estimates, particularly in the presence of occlusion and complex scene geometry.



### Leveraging Semantic Embeddings for Safety-Critical Applications
- **Arxiv ID**: http://arxiv.org/abs/1905.07733v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.07733v1)
- **Published**: 2019-05-19 12:42:41+00:00
- **Updated**: 2019-05-19 12:42:41+00:00
- **Authors**: Thomas Brunner, Frederik Diehl, Michael Truong Le, Alois Knoll
- **Comment**: Accepted at CVPR 2019 Workshop: Safe Artificial Intelligence for
  Automated Driving
- **Journal**: None
- **Summary**: Semantic Embeddings are a popular way to represent knowledge in the field of zero-shot learning. We observe their interpretability and discuss their potential utility in a safety-critical context. Concretely, we propose to use them to add introspection and error detection capabilities to neural network classifiers. First, we show how to create embeddings from symbolic domain knowledge. We discuss how to use them for interpreting mispredictions and propose a simple error detection scheme. We then introduce the concept of semantic distance: a real-valued score that measures confidence in the semantic space. We evaluate this score on a traffic sign classifier and find that it achieves near state-of-the-art performance, while being significantly faster to compute than other confidence scores. Our approach requires no changes to the original network and is thus applicable to any task for which domain knowledge is available.



### Multimodal 3D Object Detection from Simulated Pretraining
- **Arxiv ID**: http://arxiv.org/abs/1905.07754v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.07754v1)
- **Published**: 2019-05-19 15:13:41+00:00
- **Updated**: 2019-05-19 15:13:41+00:00
- **Authors**: Åsmund Brekke, Fredrik Vatsendvik, Frank Lindseth
- **Comment**: 12 pages, part of proceedings for the NAIS 2019 symposium
- **Journal**: None
- **Summary**: The need for simulated data in autonomous driving applications has become increasingly important, both for validation of pretrained models and for training new models. In order for these models to generalize to real-world applications, it is critical that the underlying dataset contains a variety of driving scenarios and that simulated sensor readings closely mimics real-world sensors. We present the Carla Automated Dataset Extraction Tool (CADET), a novel tool for generating training data from the CARLA simulator to be used in autonomous driving research. The tool is able to export high-quality, synchronized LIDAR and camera data with object annotations, and offers configuration to accurately reflect a real-life sensor array. Furthermore, we use this tool to generate a dataset consisting of 10 000 samples and use this dataset in order to train the 3D object detection network AVOD-FPN, with finetuning on the KITTI dataset in order to evaluate the potential for effective pretraining. We also present two novel LIDAR feature map configurations in Bird's Eye View for use with AVOD-FPN that can be easily modified. These configurations are tested on the KITTI and CADET datasets in order to evaluate their performance as well as the usability of the simulated dataset for pretraining. Although insufficient to fully replace the use of real world data, and generally not able to exceed the performance of systems fully trained on real data, our results indicate that simulated data can considerably reduce the amount of training on real data required to achieve satisfactory levels of accuracy.



### Low-latency Visual SLAM with Appearance-Enhanced Local Map Building
- **Arxiv ID**: http://arxiv.org/abs/1905.07797v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.07797v1)
- **Published**: 2019-05-19 19:34:03+00:00
- **Updated**: 2019-05-19 19:34:03+00:00
- **Authors**: Yipu Zhao, Wenkai Ye, Patricio A. Vela
- **Comment**: 7 pages, 6 figures, accepted at ICRA 2019
- **Journal**: None
- **Summary**: A local map module is often implemented in modern VO/VSLAM systems to improve data association and pose estimation. Conventionally, the local map contents are determined by co-visibility. While co-visibility is cheap to establish, it utilizes the relatively-weak temporal prior (i.e. seen before, likely to be seen now), therefore admitting more features into the local map than necessary. This paper describes an enhancement to co-visibility local map building by incorporating a strong appearance prior, which leads to a more compact local map and latency reduction in downstream data association. The appearance prior collected from the current image influences the local map contents: only the map features visually similar to the current measurements are potentially useful for data association. To that end, mapped features are indexed and queried with Multi-index Hashing (MIH). An online hash table selection algorithm is developed to further reduce the query overhead of MIH and the local map size. The proposed appearance-based local map building method is integrated into a state-of-the-art VO/VSLAM system. When evaluated on two public benchmarks, the size of the local map, as well as the latency of real-time pose tracking in VO/VSLAM are significantly reduced. Meanwhile, the VO/VSLAM mean performance is preserved or improves.



### Characterizing SLAM Benchmarks and Methods for the Robust Perception Age
- **Arxiv ID**: http://arxiv.org/abs/1905.07808v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.07808v1)
- **Published**: 2019-05-19 20:51:18+00:00
- **Updated**: 2019-05-19 20:51:18+00:00
- **Authors**: Wenkai Ye, Yipu Zhao, Patricio A. Vela
- **Comment**: 7 pages, 5 figures, accepted at ICRA 2019 Workshop on Dataset
  Generation and Benchmarking of SLAM Algorithms for Robotics and VR/AR
- **Journal**: None
- **Summary**: The diversity of SLAM benchmarks affords extensive testing of SLAM algorithms to understand their performance, individually or in relative terms. The ad-hoc creation of these benchmarks does not necessarily illuminate the particular weak points of a SLAM algorithm when performance is evaluated. In this paper, we propose to use a decision tree to identify challenging benchmark properties for state-of-the-art SLAM algorithms and important components within the SLAM pipeline regarding their ability to handle these challenges. Establishing what factors of a particular sequence lead to track failure or degradation relative to these characteristics is important if we are to arrive at a strong understanding for the core computational needs of a robust SLAM algorithm. Likewise, we argue that it is important to profile the computational performance of the individual SLAM components for use when benchmarking. In particular, we advocate the use of time-dilation during ROS bag playback, or what we refer to as slo-mo playback. Using slo-mo to benchmark SLAM instantiations can provide clues to how SLAM implementations should be improved at the computational component level. Three prevalent VO/SLAM algorithms and two low-latency algorithms of our own are tested on selected typical sequences, which are generated from benchmark characterization, to further demonstrate the benefits achieved from computationally efficient components.



### Spatio-Temporal Adversarial Learning for Detecting Unseen Falls
- **Arxiv ID**: http://arxiv.org/abs/1905.07817v2
- **DOI**: 10.1007/s10044-020-00901-9
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.07817v2)
- **Published**: 2019-05-19 22:19:17+00:00
- **Updated**: 2020-03-02 17:57:23+00:00
- **Authors**: Shehroz S. Khan, Jacob Nogas, Alex Mihailidis
- **Comment**: 17 pages, 10 figures, 4 tables, 39 references
- **Journal**: Pattern Analysis and Applications, 2020
- **Summary**: Fall detection is an important problem from both the health and machine learning perspective. A fall can lead to severe injuries, long term impairments or even death in some cases. In terms of machine learning, it presents a severely class imbalance problem with very few or no training data for falls owing to the fact that falls occur rarely. In this paper, we take an alternate philosophy to detect falls in the absence of their training data, by training the classifier on only the normal activities (that are available in abundance) and identifying a fall as an anomaly. To realize such a classifier, we use an adversarial learning framework, which comprises of a spatio-temporal autoencoder for reconstructing input video frames and a spatio-temporal convolution network to discriminate them against original video frames. 3D convolutions are used to learn spatial and temporal features from the input video frames. The adversarial learning of the spatio-temporal autoencoder will enable reconstructing the normal activities of daily living efficiently; thus, rendering detecting unseen falls plausible within this framework. We tested the performance of the proposed framework on camera sensing modalities that may preserve an individual's privacy (fully or partially), such as thermal and depth camera. Our results on three publicly available datasets show that the proposed spatio-temporal adversarial framework performed better than other baseline frame based (or spatial) adversarial learning methods.



### U-Net Based Multi-instance Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.07826v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.07826v1)
- **Published**: 2019-05-19 23:22:49+00:00
- **Updated**: 2019-05-19 23:22:49+00:00
- **Authors**: Heguang Liu, Jingle Jiang
- **Comment**: Stanford cs231n class project
- **Journal**: None
- **Summary**: Multi-instance video object segmentation is to segment specific instances throughout a video sequence in pixel level, given only an annotated first frame. In this paper, we implement an effective fully convolutional networks with U-Net similar structure built on top of OSVOS fine-tuned layer. We use instance isolation to transform this multi-instance segmentation problem into binary labeling problem, and use weighted cross entropy loss and dice coefficient loss as our loss function. Our best model achieves F mean of 0.467 and J mean of 0.424 on DAVIS dataset, which is a comparable performance with the State-of-the-Art approach. But case analysis shows this model can achieve a smoother contour and better instance coverage, meaning it better for recall focused segmentation scenario. We also did experiments on other convolutional neural networks, including Seg-Net, Mask R-CNN, and provide insightful comparison and discussion.



