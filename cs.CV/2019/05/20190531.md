# Arxiv Papers in cs.CV on 2019-05-31
### Multi-modal Discriminative Model for Vision-and-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/1905.13358v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.13358v1)
- **Published**: 2019-05-31 00:07:24+00:00
- **Updated**: 2019-05-31 00:07:24+00:00
- **Authors**: Haoshuo Huang, Vihan Jain, Harsh Mehta, Jason Baldridge, Eugene Ie
- **Comment**: Accepted at SpLU-RoboNLP 2019 (workshop at NAACL)
- **Journal**: None
- **Summary**: Vision-and-Language Navigation (VLN) is a natural language grounding task where agents have to interpret natural language instructions in the context of visual scenes in a dynamic environment to achieve prescribed navigation goals. Successful agents must have the ability to parse natural language of varying linguistic styles, ground them in potentially unfamiliar scenes, plan and react with ambiguous environmental feedback. Generalization ability is limited by the amount of human annotated data. In particular, \emph{paired} vision-language sequence data is expensive to collect. We develop a discriminator that evaluates how well an instruction explains a given path in VLN task using multi-modal alignment. Our study reveals that only a small fraction of the high-quality augmented data from \citet{Fried:2018:Speaker}, as scored by our discriminator, is useful for training VLN agents with similar performance on previously unseen environments. We also show that a VLN agent warm-started with pre-trained components from the discriminator outperforms the benchmark success rates of 35.5 by 10\% relative measure on previously unseen environments.



### Supervised Online Hashing via Similarity Distribution Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.13382v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1905.13382v1)
- **Published**: 2019-05-31 02:12:41+00:00
- **Updated**: 2019-05-31 02:12:41+00:00
- **Authors**: Mingbao Lin, Rongrong Ji, Shen Chen, Feng Zheng, Xiaoshuai Sun, Baochang Zhang, Liujuan Cao, Guodong Guo, Feiyue Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Online hashing has attracted extensive research attention when facing streaming data. Most online hashing methods, learning binary codes based on pairwise similarities of training instances, fail to capture the semantic relationship, and suffer from a poor generalization in large-scale applications due to large variations. In this paper, we propose to model the similarity distributions between the input data and the hashing codes, upon which a novel supervised online hashing method, dubbed as Similarity Distribution based Online Hashing (SDOH), is proposed, to keep the intrinsic semantic relationship in the produced Hamming space. Specifically, we first transform the discrete similarity matrix into a probability matrix via a Gaussian-based normalization to address the extremely imbalanced distribution issue. And then, we introduce a scaling Student t-distribution to solve the challenging initialization problem, and efficiently bridge the gap between the known and unknown distributions. Lastly, we align the two distributions via minimizing the Kullback-Leibler divergence (KL-diverence) with stochastic gradient descent (SGD), by which an intuitive similarity constraint is imposed to update hashing model on the new streaming data with a powerful generalizing ability to the past data. Extensive experiments on three widely-used benchmarks validate the superiority of the proposed SDOH over the state-of-the-art methods in the online retrieval task.



### Residual Networks as Nonlinear Systems: Stability Analysis using Linearization
- **Arxiv ID**: http://arxiv.org/abs/1905.13386v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.13386v1)
- **Published**: 2019-05-31 02:44:28+00:00
- **Updated**: 2019-05-31 02:44:28+00:00
- **Authors**: Kai Rothauge, Zhewei Yao, Zixi Hu, Michael W. Mahoney
- **Comment**: None
- **Journal**: None
- **Summary**: We regard pre-trained residual networks (ResNets) as nonlinear systems and use linearization, a common method used in the qualitative analysis of nonlinear systems, to understand the behavior of the networks under small perturbations of the input images. We work with ResNet-56 and ResNet-110 trained on the CIFAR-10 data set. We linearize these networks at the level of residual units and network stages, and the singular value decomposition is used in the stability analysis of these components. It is found that most of the singular values of the linearizations of residual units are 1 and, in spite of the fact that the linearizations depend directly on the activation maps, the singular values differ only slightly for different input images. However, adjusting the scaling of the skip connection or the values of the weights in a residual unit has a significant impact on the singular value distributions. Inspection of how random and adversarial perturbations of input images propagate through the network reveals that there is a dramatic jump in the magnitude of adversarial perturbations towards the end of the final stage of the network that is not present in the case of random perturbations. We attempt to gain a better understanding of this phenomenon by projecting the perturbations onto singular vectors of the linearizations of the residual units.



### Design Light-weight 3D Convolutional Networks for Video Recognition Temporal Residual, Fully Separable Block, and Fast Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1905.13388v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.13388v1)
- **Published**: 2019-05-31 02:48:21+00:00
- **Updated**: 2019-05-31 02:48:21+00:00
- **Authors**: Haonan Wang, Jun Lin, Zhongfeng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep 3-dimensional (3D) Convolutional Network (ConvNet) has shown promising performance on video recognition tasks because of its powerful spatio-temporal information fusion ability. However, the extremely intensive requirements on memory access and computing power prohibit it from being used in resource-constrained scenarios, such as portable and edge devices. So in this paper, we first propose a two-stage Fully Separable Block (FSB) to significantly compress the model sizes of 3D ConvNets. Then a feature enhancement approach named Temporal Residual Gradient (TRG) is developed to improve the performance of compressed model on video tasks, which provides higher accuracy, faster convergency and better robustness. Moreover, in order to further decrease the computing workload, we propose a hybrid Fast Algorithm (hFA) to drastically reduce the computation complexity of convolutions. These methods are effectively combined to design a light-weight and efficient ConvNet for video recognition tasks. Experiments on the popular dataset report 2.3x compression rate, 3.6x workload reduction, and 6.3% top-1 accuracy gain, over the state-of-the-art SlowFast model, which is already a highly compact model. The proposed methods also show good adaptability on traditional 3D ConvNet, demonstrating 7.4x more compact model, 11.0x less workload, and 3.0% higher accuracy



### Multi-Precision Quantized Neural Networks via Encoding Decomposition of -1 and +1
- **Arxiv ID**: http://arxiv.org/abs/1905.13389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.13389v1)
- **Published**: 2019-05-31 02:49:08+00:00
- **Updated**: 2019-05-31 02:49:08+00:00
- **Authors**: Qigong Sun, Fanhua Shang, Kang Yang, Xiufang Li, Yan Ren, Licheng Jiao
- **Comment**: 9 pages, 2 figures, Proc. 33rd AAAI Conf. Artif. Intell., 2019
- **Journal**: None
- **Summary**: The training of deep neural networks (DNNs) requires intensive resources both for computation and for storage performance. Thus, DNNs cannot be efficiently applied to mobile phones and embedded devices, which seriously limits their applicability in industry applications. To address this issue, we propose a novel encoding scheme of using {-1,+1} to decompose quantized neural networks (QNNs) into multi-branch binary networks, which can be efficiently implemented by bitwise operations (xnor and bitcount) to achieve model compression, computational acceleration and resource saving. Based on our method, users can easily achieve different encoding precisions arbitrarily according to their requirements and hardware resources. The proposed mechanism is very suitable for the use of FPGA and ASIC in terms of data storage and computation, which provides a feasible idea for smart chips. We validate the effectiveness of our method on both large-scale image classification tasks (e.g., ImageNet) and object detection tasks. In particular, our method with low-bit encoding can still achieve almost the same performance as its full-precision counterparts.



### Rethinking Table Recognition using Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.13391v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.13391v2)
- **Published**: 2019-05-31 02:58:04+00:00
- **Updated**: 2019-07-03 17:59:36+00:00
- **Authors**: Shah Rukh Qasim, Hassan Mahmood, Faisal Shafait
- **Comment**: Accepted to ICDAR 2019
- **Journal**: None
- **Summary**: Document structure analysis, such as zone segmentation and table recognition, is a complex problem in document processing and is an active area of research. The recent success of deep learning in solving various computer vision and machine learning problems has not been reflected in document structure analysis since conventional neural networks are not well suited to the input structure of the problem. In this paper, we propose an architecture based on graph networks as a better alternative to standard neural networks for table recognition. We argue that graph networks are a more natural choice for these problems, and explore two gradient-based graph neural networks. Our proposed architecture combines the benefits of convolutional neural networks for visual feature extraction and graph networks for dealing with the problem structure. We empirically demonstrate that our method outperforms the baseline by a significant margin. In addition, we identify the lack of large scale datasets as a major hindrance for deep learning research for structure analysis and present a new large scale synthetic dataset for the problem of table recognition. Finally, we open-source our implementation of dataset generation and the training framework of our graph networks to promote reproducible research in this direction.



### TACNet: Transition-Aware Context Network for Spatio-Temporal Action Detection
- **Arxiv ID**: http://arxiv.org/abs/1905.13417v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68T06
- **Links**: [PDF](http://arxiv.org/pdf/1905.13417v1)
- **Published**: 2019-05-31 05:14:39+00:00
- **Updated**: 2019-05-31 05:14:39+00:00
- **Authors**: Lin Song, Shiwei Zhang, Gang Yu, Hongbin Sun
- **Comment**: CVPR-2019
- **Journal**: None
- **Summary**: Current state-of-the-art approaches for spatio-temporal action detection have achieved impressive results but remain unsatisfactory for temporal extent detection. The main reason comes from that, there are some ambiguous states similar to the real actions which may be treated as target actions even by a well-trained network. In this paper, we define these ambiguous samples as "transitional states", and propose a Transition-Aware Context Network (TACNet) to distinguish transitional states. The proposed TACNet includes two main components, i.e., temporal context detector and transition-aware classifier. The temporal context detector can extract long-term context information with constant time complexity by constructing a recurrent network. The transition-aware classifier can further distinguish transitional states by classifying action and transitional states simultaneously. Therefore, the proposed TACNet can substantially improve the performance of spatio-temporal action detection. We extensively evaluate the proposed TACNet on UCF101-24 and J-HMDB datasets. The experimental results demonstrate that TACNet obtains competitive performance on JHMDB and significantly outperforms the state-of-the-art methods on the untrimmed UCF101-24 in terms of both frame-mAP and video-mAP.



### Point Clouds Learning with Attention-based Graph Convolution Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.13445v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.13445v1)
- **Published**: 2019-05-31 07:10:12+00:00
- **Updated**: 2019-05-31 07:10:12+00:00
- **Authors**: Zhuyang Xie, Junzhou Chen, Bo Peng
- **Comment**: 11 pages, 9 figures
- **Journal**: None
- **Summary**: Point clouds data, as one kind of representation of 3D objects, are the most primitive output obtained by 3D sensors. Unlike 2D images, point clouds are disordered and unstructured. Hence it is not straightforward to apply classification techniques such as the convolution neural network to point clouds analysis directly. To solve this problem, we propose a novel network structure, named Attention-based Graph Convolution Networks (AGCN), to extract point clouds features. Taking the learning process as a message propagation between adjacent points, we introduce an attention mechanism to AGCN for analyzing the relationships between local features of the points. In addition, we introduce an additional global graph structure network to compensate for the relative information of the individual points in the graph structure network. The proposed network is also extended to an encoder-decoder structure for segmentation tasks. Experimental results show that the proposed network can achieve state-of-the-art performance in both classification and segmentation tasks.



### Using AI for Economic Upliftment of Handicraft Industry
- **Arxiv ID**: http://arxiv.org/abs/1907.02014v1
- **DOI**: None
- **Categories**: **cs.OH**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1907.02014v1)
- **Published**: 2019-05-31 07:33:42+00:00
- **Updated**: 2019-05-31 07:33:42+00:00
- **Authors**: Nitya Raviprakash, Sonam Damani, Ankush Chatterjee, Meghana Joshi, Puneet Agrawal
- **Comment**: None
- **Journal**: None
- **Summary**: The handicraft industry is a strong pillar of Indian economy which provides large-scale employment opportunities to artisans in rural and underprivileged communities. However, in this era of globalization, diverse modern designs have rendered traditional designs old and monotonous, causing an alarming decline of handicraft sales. For this age-old industry to survive the global competition, it is imperative to integrate contemporary designs with Indian handicrafts. In this paper, we use novel AI techniques to generate contemporary designs for two popular Indian handicrafts - Ikat and Block Print. These techniques were successfully employed by communities across India to manufacture and sell products with greater appeal and revenue. The designs are evaluated to be significantly more likeable and marketable than the current designs used by artisans.



### Combining Noise-to-Image and Image-to-Image GANs: Brain MR Image Augmentation for Tumor Detection
- **Arxiv ID**: http://arxiv.org/abs/1905.13456v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.13456v3)
- **Published**: 2019-05-31 08:14:19+00:00
- **Updated**: 2019-10-09 12:20:15+00:00
- **Authors**: Changhee Han, Leonardo Rundo, Ryosuke Araki, Yudai Nagano, Yujiro Furukawa, Giancarlo Mauri, Hideki Nakayama, Hideaki Hayashi
- **Comment**: 12 pages, 7 figures, accepted to IEEE ACCESS
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) achieve excellent computer-assisted diagnosis with sufficient annotated training data. However, most medical imaging datasets are small and fragmented. In this context, Generative Adversarial Networks (GANs) can synthesize realistic/diverse additional training images to fill the data lack in the real image distribution; researchers have improved classification by augmenting data with noise-to-image (e.g., random noise samples to diverse pathological images) or image-to-image GANs (e.g., a benign image to a malignant one). Yet, no research has reported results combining noise-to-image and image-to-image GANs for further performance boost. Therefore, to maximize the DA effect with the GAN combinations, we propose a two-step GAN-based DA that generates and refines brain Magnetic Resonance (MR) images with/without tumors separately: (i) Progressive Growing of GANs (PGGANs), multi-stage noise-to-image GAN for high-resolution MR image generation, first generates realistic/diverse 256 X 256 images; (ii) Multimodal UNsupervised Image-to-image Translation (MUNIT) that combines GANs/Variational AutoEncoders or SimGAN that uses a DA-focused GAN loss, further refines the texture/shape of the PGGAN-generated images similarly to the real ones. We thoroughly investigate CNN-based tumor classification results, also considering the influence of pre-training on ImageNet and discarding weird-looking GAN-generated images. The results show that, when combined with classic DA, our two-step GAN-based DA can significantly outperform the classic DA alone, in tumor detection (i.e., boosting sensitivity 93.67% to 97.48%) and also in other medical imaging tasks.



### Joint Representation of Multiple Geometric Priors via a Shape Decomposition Model for Single Monocular 3D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1905.13466v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.13466v1)
- **Published**: 2019-05-31 08:47:05+00:00
- **Updated**: 2019-05-31 08:47:05+00:00
- **Authors**: Mengxi Jiang, Zhuliang Yu, Cuihua Li, Yunqi Lei
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we aim to recover the 3D human pose from 2D body joints of a single image. The major challenge in this task is the depth ambiguity since different 3D poses may produce similar 2D poses. Although many recent advances in this problem are found in both unsupervised and supervised learning approaches, the performances of most of these approaches are greatly affected by insufficient diversities and richness of training data. To alleviate this issue, we propose an unsupervised learning approach, which is capable of estimating various complex poses well under limited available training data. Specifically, we propose a Shape Decomposition Model (SDM) in which a 3D pose is considered as the superposition of two parts which are global structure together with some deformations. Based on SDM, we estimate these two parts explicitly by solving two sets of different distributed combination coefficients of geometric priors. In addition, to obtain geometric priors, a joint dictionary learning algorithm is proposed to extract both coarse and fine pose clues simultaneously from limited training data. Quantitative evaluations on several widely used datasets demonstrate that our approach yields better performances over other competitive approaches. Especially, on some categories with more complex deformations, significant improvements are achieved by our approach. Furthermore, qualitative experiments conducted on in-the-wild images also show the effectiveness of the proposed approach.



### Deep interpretable architecture for plant diseases classification
- **Arxiv ID**: http://arxiv.org/abs/1905.13523v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.13523v2)
- **Published**: 2019-05-31 11:41:16+00:00
- **Updated**: 2019-06-13 15:32:53+00:00
- **Authors**: Mohammed Brahimi, Said Mahmoudi, Kamel Boukhalfa, Abdelouhab Moussaoui
- **Comment**: 10 pages, 8 figures, Submitted to Signal Processing Algorithms,
  Architectures, Arrangements and Applications (SPA2019),
  https://github.com/Tahedi1/Teacher_Student_Architecture
- **Journal**: None
- **Summary**: Recently, many works have been inspired by the success of deep learning in computer vision for plant diseases classification. Unfortunately, these end-to-end deep classifiers lack transparency which can limit their adoption in practice. In this paper, we propose a new trainable visualization method for plant diseases classification based on a Convolutional Neural Network (CNN) architecture composed of two deep classifiers. The first one is named Teacher and the second one Student. This architecture leverages the multitask learning to train the Teacher and the Student jointly. Then, the communicated representation between the Teacher and the Student is used as a proxy to visualize the most important image regions for classification. This new architecture produces sharper visualization than the existing methods in plant diseases context. All experiments are achieved on PlantVillage dataset that contains 54306 plant images.



### Fast Solar Image Classification Using Deep Learning and its Importance for Automation in Solar Physics
- **Arxiv ID**: http://arxiv.org/abs/1905.13575v1
- **DOI**: 10.1007/s11207-019-1473-z
- **Categories**: **astro-ph.SR**, astro-ph.IM, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.13575v1)
- **Published**: 2019-05-31 12:27:55+00:00
- **Updated**: 2019-05-31 12:27:55+00:00
- **Authors**: John A. Armstrong, Lyndsay Fletcher
- **Comment**: 19 pages, 9 figures, accepted for publication in Solar Physics
- **Journal**: None
- **Summary**: The volume of data being collected in solar physics has exponentially increased over the past decade and with the introduction of the $\textit{Daniel K. Inouye Solar Telescope}$ (DKIST) we will be entering the age of petabyte solar data. Automated feature detection will be an invaluable tool for post-processing of solar images to create catalogues of data ready for researchers to use. We propose a deep learning model to accomplish this; a deep convolutional neural network is adept at feature extraction and processing images quickly. We train our network using data from $\textit{Hinode/Solar Optical Telescope}$ (SOT) H$\alpha$ images of a small subset of solar features with different geometries: filaments, prominences, flare ribbons, sunspots and the quiet Sun ($\textit{i.e.}$ the absence of any of the other four features). We achieve near perfect performance on classifying unseen images from SOT ($\approx$99.9\%) in 4.66 seconds. We also for the first time explore transfer learning in a solar context. Transfer learning uses pre-trained deep neural networks to help train new deep learning models $\textit{i.e.}$ it teaches a new model. We show that our network is robust to changes in resolution by degrading images from SOT resolution ($\approx$0.33$^{\prime \prime}$ at $\lambda$=6563\AA{}) to $\textit{Solar Dynamics Observatory/Atmospheric Imaging Assembly}$ (SDO/AIA) resolution ($\approx$1.2$^{\prime \prime}$) without a change in performance of our network. However, we also observe where the network fails to generalise to sunspots from SDO/AIA bands 1600/1700\AA{} due to small-scale brightenings around the sunspots and prominences in SDO/AIA 304\AA{} due to coronal emission.



### Deep Dual Relation Modeling for Egocentric Interaction Recognition
- **Arxiv ID**: http://arxiv.org/abs/1905.13586v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.13586v1)
- **Published**: 2019-05-31 12:49:25+00:00
- **Updated**: 2019-05-31 12:49:25+00:00
- **Authors**: Haoxin Li, Yijun Cai, Wei-Shi Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Egocentric interaction recognition aims to recognize the camera wearer's interactions with the interactor who faces the camera wearer in egocentric videos. In such a human-human interaction analysis problem, it is crucial to explore the relations between the camera wearer and the interactor. However, most existing works directly model the interactions as a whole and lack modeling the relations between the two interacting persons. To exploit the strong relations for egocentric interaction recognition, we introduce a dual relation modeling framework which learns to model the relations between the camera wearer and the interactor based on the individual action representations of the two persons. Specifically, we develop a novel interactive LSTM module, the key component of our framework, to explicitly model the relations between the two interacting persons based on their individual action representations, which are collaboratively learned with an interactor attention module and a global-local motion module. Experimental results on three egocentric interaction datasets show the effectiveness of our method and advantage over state-of-the-arts.



### Known-plaintext attack and ciphertext-only attack for encrypted single-pixel imaging
- **Arxiv ID**: http://arxiv.org/abs/1905.13594v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.13594v1)
- **Published**: 2019-05-31 13:01:22+00:00
- **Updated**: 2019-05-31 13:01:22+00:00
- **Authors**: Shuming Jiao, Yang Gao, Ting Lei, Zhenwei Xie, Xiaocong Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: In many previous works, a single-pixel imaging (SPI) system is constructed as an optical image encryption system. Unauthorized users are not able to reconstruct the plaintext image from the ciphertext intensity sequence without knowing the illumination pattern key. However, little cryptanalysis about encrypted SPI has been investigated in the past. In this work, we propose a known-plaintext attack scheme and a ciphertext-only attack scheme to an encrypted SPI system for the first time. The known-plaintext attack is implemented by interchanging the roles of illumination patterns and object images in the SPI model. The ciphertext-only attack is implemented based on the statistical features of single-pixel intensity values. The two schemes can crack encrypted SPI systems and successfully recover the key containing correct illumination patterns.



### 3DPalsyNet: A Facial Palsy Grading and Motion Recognition Framework using Fully 3D Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.13607v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1905.13607v1)
- **Published**: 2019-05-31 13:24:30+00:00
- **Updated**: 2019-05-31 13:24:30+00:00
- **Authors**: Gary Storey, Richard Jiang, Shelagh Keogh, Ahmed Bouridane, Chang-Tsun Li
- **Comment**: None
- **Journal**: IEEE Access 2019
- **Summary**: The capability to perform facial analysis from video sequences has significant potential to positively impact in many areas of life. One such area relates to the medical domain to specifically aid in the diagnosis and rehabilitation of patients with facial palsy. With this application in mind, this paper presents an end-to-end framework, named 3DPalsyNet, for the tasks of mouth motion recognition and facial palsy grading. 3DPalsyNet utilizes a 3D CNN architecture with a ResNet backbone for the prediction of these dynamic tasks. Leveraging transfer learning from a 3D CNNs pre-trained on the Kinetics data set for general action recognition, the model is modified to apply joint supervised learning using center and softmax loss concepts. 3DPalsyNet is evaluated on a test set consisting of individuals with varying ranges of facial palsy and mouth motions and the results have shown an attractive level of classification accuracy in these task of 82% and 86% respectively. The frame duration and the loss function affect was studied in terms of the predictive qualities of the proposed 3DPalsyNet, where it was found shorter frame duration's of 8 performed best for this specific task. Centre loss and softmax have shown improvements in spatio-temporal feature learning than softmax loss alone, this is in agreement with earlier work involving the spatial domain.



### Regression Networks for Meta-Learning Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/1905.13613v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.13613v2)
- **Published**: 2019-05-31 13:35:41+00:00
- **Updated**: 2020-06-18 20:09:01+00:00
- **Authors**: Arnout Devos, Matthias Grossglauser
- **Comment**: 7th ICML Workshop on Automated Machine Learning (2020)
- **Journal**: ICML Workshop on Automated Machine Learning (2020)
- **Summary**: We propose regression networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each class. In high dimensional embedding spaces the direction of data generally contains richer information than magnitude. Next to this, state-of-the-art few-shot metric methods that compare distances with aggregated class representations, have shown superior performance. Combining these two insights, we propose to meta-learn classification of embedded points by regressing the closest approximation in every class subspace while using the regression error as a distance metric. Similarly to recent approaches for few-shot learning, regression networks reflect a simple inductive bias that is beneficial in this limited-data regime and they achieve excellent results, especially when more aggregate class representations can be formed with multiple shots.



### Time Series Anomaly Detection Using Convolutional Neural Networks and Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.13628v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.13628v1)
- **Published**: 2019-05-31 14:12:13+00:00
- **Updated**: 2019-05-31 14:12:13+00:00
- **Authors**: Tailai Wen, Roy Keyes
- **Comment**: 8 pages, 8 figures, AI for Internet of Things Workshop in IJCAI 2019
- **Journal**: None
- **Summary**: Time series anomaly detection plays a critical role in automated monitoring systems. Most previous deep learning efforts related to time series anomaly detection were based on recurrent neural networks (RNN). In this paper, we propose a time series segmentation approach based on convolutional neural networks (CNN) for anomaly detection. Moreover, we propose a transfer learning framework that pretrains a model on a large-scale synthetic univariate time series data set and then fine-tunes its weights on small-scale, univariate or multivariate data sets with previously unseen classes of anomalies. For the multivariate case, we introduce a novel network architecture. The approach was tested on multiple synthetic and real data sets successfully.



### Scene Text Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1905.13648v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.13648v2)
- **Published**: 2019-05-31 14:47:55+00:00
- **Updated**: 2019-10-16 13:54:22+00:00
- **Authors**: Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marçal Rusiñol, Ernest Valveny, C. V. Jawahar, Dimosthenis Karatzas
- **Comment**: International Conference on Computer Vision (ICCV 2019)
- **Journal**: None
- **Summary**: Current visual question answering datasets do not consider the rich semantic information conveyed by text within an image. In this work, we present a new dataset, ST-VQA, that aims to highlight the importance of exploiting high-level semantic information present in images as textual cues in the VQA process. We use this dataset to define a series of tasks of increasing difficulty for which reading the scene text in the context provided by the visual information is necessary to reason and generate an appropriate answer. We propose a new evaluation metric for these tasks to account both for reasoning errors as well as shortcomings of the text recognition module. In addition we put forward a series of baseline methods, which provide further insight to the newly released dataset, and set the scene for further research.



### A Riemanian Approach to Blob Detection in Manifold-Valued Images
- **Arxiv ID**: http://arxiv.org/abs/1905.13653v1
- **DOI**: 10.1007/978-3-319-68445-1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.13653v1)
- **Published**: 2019-05-31 14:53:44+00:00
- **Updated**: 2019-05-31 14:53:44+00:00
- **Authors**: Aleksei Shestov, Mikhail Kumskov
- **Comment**: Published in GSI 2017 proceedings
- **Journal**: Third International Conference, GSI 2017, Paris, France, November
  7-9, 2017, Proceedings, pp. 727-736
- **Summary**: This paper is devoted to the problem of blob detection in manifold-valued images. Our solution is based on new definitions of blob response functions. We define the blob response functions by means of curvatures of an image graph, considered as a submanifold. We call the proposed framework Riemannian blob detection. We prove that our approach can be viewed as a generalization of the grayscale blob detection technique. An expression of the Riemannian blob response functions through the image Hessian is derived. We provide experiments for the case of vector-valued images on 2D surfaces: the proposed framework is tested on the task of chemical compounds classification.



### Partial Scanning Transmission Electron Microscopy with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.13667v2
- **DOI**: 10.1038/s41598-020-65261-0
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.13667v2)
- **Published**: 2019-05-31 15:13:32+00:00
- **Updated**: 2020-02-14 11:50:23+00:00
- **Authors**: Jeffrey M. Ede, Richard Beanland
- **Comment**: 20 pages, 11 figures
- **Journal**: Sci Rep 10, 8332 (2020)
- **Summary**: Compressed sensing algorithms are used to decrease electron microscope scan time and electron beam exposure with minimal information loss. Following successful applications of deep learning to compressed sensing, we have developed a two-stage multiscale generative adversarial neural network to complete realistic 512$\times$512 scanning transmission electron micrographs from spiral, jittered gridlike, and other partial scans. For spiral scans and mean squared error based pre-training, this enables electron beam coverage to be decreased by 17.9$\times$ with a 3.8\% test set root mean squared intensity error, and by 87.0$\times$ with a 6.2\% error. Our generator networks are trained on partial scans created from a new dataset of 16227 scanning transmission electron micrographs. High performance is achieved with adaptive learning rate clipping of loss spikes and an auxiliary trainer network. Our source code, new dataset, and pre-trained models have been made publicly available at https://github.com/Jeffrey-Ede/partial-STEM



### Multimodal Joint Emotion and Game Context Recognition in League of Legends Livestreams
- **Arxiv ID**: http://arxiv.org/abs/1905.13694v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.13694v1)
- **Published**: 2019-05-31 16:12:24+00:00
- **Updated**: 2019-05-31 16:12:24+00:00
- **Authors**: Charles Ringer, James Alfred Walker, Mihalis A. Nicolaou
- **Comment**: 8 Pages, IEEE Conference on Games 2019
- **Journal**: None
- **Summary**: Video game streaming provides the viewer with a rich set of audio-visual data, conveying information both with regards to the game itself, through game footage and audio, as well as the streamer's emotional state and behaviour via webcam footage and audio. Analysing player behaviour and discovering correlations with game context is crucial for modelling and understanding important aspects of livestreams, but comes with a significant set of challenges - such as fusing multimodal data captured by different sensors in uncontrolled ('in-the-wild') conditions. Firstly, we present, to our knowledge, the first data set of League of Legends livestreams, annotated for both streamer affect and game context. Secondly, we propose a method that exploits tensor decompositions for high-order fusion of multimodal representations. The proposed method is evaluated on the problem of jointly predicting game context and player affect, compared with a set of baseline fusion approaches such as late and early fusion.



### Representation Theoretic Patterns in Multi-Frequency Class Averaging for Three-Dimensional Cryo-Electron Microscopy
- **Arxiv ID**: http://arxiv.org/abs/1906.01082v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, math.FA, 20G05, 33C45, 33C55, 55R25, I.4.5; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/1906.01082v4)
- **Published**: 2019-05-31 16:21:55+00:00
- **Updated**: 2021-07-05 20:25:16+00:00
- **Authors**: Yifeng Fan, Tingran Gao, Zhizhen Zhao
- **Comment**: 38 pages, 17 figures
- **Journal**: None
- **Summary**: We develop in this paper a novel intrinsic classification algorithm -- multi-frequency class averaging (MFCA) -- for classifying noisy projection images obtained from three-dimensional cryo-electron microscopy (cryo-EM) by the similarity among their viewing directions. This new algorithm leverages multiple irreducible representations of the unitary group to introduce additional redundancy into the representation of the optimal in-plane rotational alignment, extending and outperforming the existing class averaging algorithm that uses only a single representation. The formal algebraic model and representation theoretic patterns of the proposed MFCA algorithm extend the framework of Hadani and Singer to arbitrary irreducible representations of the unitary group. We conceptually establish the consistency and stability of MFCA by inspecting the spectral properties of a generalized local parallel transport operator through the lens of Wigner $D$-matrices. We demonstrate the efficacy of the proposed algorithm with numerical experiments.



### Are Labels Required for Improving Adversarial Robustness?
- **Arxiv ID**: http://arxiv.org/abs/1905.13725v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.13725v4)
- **Published**: 2019-05-31 17:19:13+00:00
- **Updated**: 2019-12-05 16:57:27+00:00
- **Authors**: Jonathan Uesato, Jean-Baptiste Alayrac, Po-Sen Huang, Robert Stanforth, Alhussein Fawzi, Pushmeet Kohli
- **Comment**: Appears in the Thirty-Third Annual Conference on Neural Information
  Processing Systems (NeurIPS 2019)
- **Journal**: None
- **Summary**: Recent work has uncovered the interesting (and somewhat surprising) finding that training models to be invariant to adversarial perturbations requires substantially larger datasets than those required for standard classification. This result is a key hurdle in the deployment of robust machine learning models in many real world applications where labeled data is expensive. Our main insight is that unlabeled data can be a competitive alternative to labeled data for training adversarially robust models. Theoretically, we show that in a simple statistical setting, the sample complexity for learning an adversarially robust model from unlabeled data matches the fully supervised case up to constant factors. On standard datasets like CIFAR-10, a simple Unsupervised Adversarial Training (UAT) approach using unlabeled data improves robust accuracy by 21.7% over using 4K supervised examples alone, and captures over 95% of the improvement from the same number of labeled examples. Finally, we report an improvement of 4% over the previous state-of-the-art on CIFAR-10 against the strongest known attack by using additional unlabeled data from the uncurated 80 Million Tiny Images dataset. This demonstrates that our finding extends as well to the more realistic case where unlabeled data is also uncurated, therefore opening a new avenue for improving adversarial training.



### Unlabeled Data Improves Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/1905.13736v4
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.13736v4)
- **Published**: 2019-05-31 17:41:33+00:00
- **Updated**: 2022-01-13 17:20:07+00:00
- **Authors**: Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang, John C. Duchi
- **Comment**: Corrected some math typos in the proof of Lemma 1
- **Journal**: None
- **Summary**: We demonstrate, theoretically and empirically, that adversarial robustness can significantly benefit from semisupervised learning. Theoretically, we revisit the simple Gaussian model of Schmidt et al. that shows a sample complexity gap between standard and robust classification. We prove that unlabeled data bridges this gap: a simple semisupervised learning procedure (self-training) achieves high robust accuracy using the same number of labels required for achieving high standard accuracy. Empirically, we augment CIFAR-10 with 500K unlabeled images sourced from 80 Million Tiny Images and use robust self-training to outperform state-of-the-art robust accuracies by over 5 points in (i) $\ell_\infty$ robustness against several strong attacks via adversarial training and (ii) certified $\ell_2$ and $\ell_\infty$ robustness via randomized smoothing. On SVHN, adding the dataset's own extra training set with the labels removed provides gains of 4 to 10 points, within 1 point of the gain from using the extra labels.



### Visual Understanding and Narration: A Deeper Understanding and Explanation of Visual Scenes
- **Arxiv ID**: http://arxiv.org/abs/1906.00038v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.00038v2)
- **Published**: 2019-05-31 19:12:55+00:00
- **Updated**: 2019-09-23 20:27:47+00:00
- **Authors**: Stephanie M. Lukin, Claire Bonial, Clare R. Voss
- **Comment**: 2-page extended abstract, presented at the Workshop on Shortcomings
  in Vision and Language (SiVL), 2019, at the North American Association for
  Computational Linguistics (NAACL)
- **Journal**: None
- **Summary**: We describe the task of Visual Understanding and Narration, in which a robot (or agent) generates text for the images that it collects when navigating its environment, by answering open-ended questions, such as 'what happens, or might have happened, here?'



### DISCO: Depth Inference from Stereo using Context
- **Arxiv ID**: http://arxiv.org/abs/1906.00050v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.00050v1)
- **Published**: 2019-05-31 20:01:43+00:00
- **Updated**: 2019-05-31 20:01:43+00:00
- **Authors**: Kunal Swami, Kaushik Raghavan, Nikhilanj Pelluri, Rituparna Sarkar, Pankaj Bajpai
- **Comment**: This work was completed in October 2018 and is accepted in IEEE
  International Conference on Multimedia & Expo (ICME) 2019
- **Journal**: None
- **Summary**: Recent deep learning based approaches have outperformed classical stereo matching methods. However, current deep learning based end-to-end stereo matching methods adopt a generic encoder-decoder style network with skip connections. To limit computational requirement, many networks perform excessive down sampling, which results in significant loss of useful low-level information. Additionally, many network designs do not exploit the rich multi-scale contextual information. In this work, we address these aforementioned problems by carefully designing the network architecture to preserve required spatial information throughout the network, while at the same time achieve large effective receptive field to extract multiscale contextual information. For the first time, we create a synthetic disparity dataset reflecting real life images captured using a smartphone; this enables us to obtain state-of-the-art results on common real life images. The proposed model DISCO is pre-trained on the synthetic Scene Flow dataset and evaluated on popular benchmarks and our in-house dataset of challenging real life images. The proposed model outperforms existing state-of-the-art methods in terms of quality as well as quantitative metrics.



### OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge
- **Arxiv ID**: http://arxiv.org/abs/1906.00067v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1906.00067v2)
- **Published**: 2019-05-31 20:29:01+00:00
- **Updated**: 2019-09-04 10:43:20+00:00
- **Authors**: Kenneth Marino, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) in its ideal form lets us study reasoning in the joint space of vision and language and serves as a proxy for the AI task of scene understanding. However, most VQA benchmarks to date are focused on questions such as simple counting, visual attributes, and object detection that do not require reasoning or knowledge beyond what is in the image. In this paper, we address the task of knowledge-based visual question answering and provide a benchmark, called OK-VQA, where the image content is not sufficient to answer the questions, encouraging methods that rely on external knowledge resources. Our new dataset includes more than 14,000 questions that require external knowledge to answer. We show that the performance of the state-of-the-art VQA models degrades drastically in this new setting. Our analysis shows that our knowledge-based VQA task is diverse, difficult, and large compared to previous knowledge-based VQA datasets. We hope that this dataset enables researchers to open up new avenues for research in this domain. See http://okvqa.allenai.org to download and browse the dataset.



### Comparing Energy Efficiency of CPU, GPU and FPGA Implementations for Vision Kernels
- **Arxiv ID**: http://arxiv.org/abs/1906.11879v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.11879v1)
- **Published**: 2019-05-31 21:25:42+00:00
- **Updated**: 2019-05-31 21:25:42+00:00
- **Authors**: Murad Qasaimeh, Kristof Denolf, Jack Lo, Kees Vissers, Joseph Zambreno, Phillip H. Jones
- **Comment**: 8 pages, Design Automation Conference (DAC), The 15th IEEE
  International Conference on Embedded Software and Systems, 2019
- **Journal**: None
- **Summary**: Developing high performance embedded vision applications requires balancing run-time performance with energy constraints. Given the mix of hardware accelerators that exist for embedded computer vision (e.g. multi-core CPUs, GPUs, and FPGAs), and their associated vendor optimized vision libraries, it becomes a challenge for developers to navigate this fragmented solution space. To aid with determining which embedded platform is most suitable for their application, we conduct a comprehensive benchmark of the run-time performance and energy efficiency of a wide range of vision kernels. We discuss rationales for why a given underlying hardware architecture innately performs well or poorly based on the characteristics of a range of vision kernel categories. Specifically, our study is performed for three commonly used HW accelerators for embedded vision applications: ARM57 CPU, Jetson TX2 GPU and ZCU102 FPGA, using their vendor optimized vision libraries: OpenCV, VisionWorks and xfOpenCV. Our results show that the GPU achieves an energy/frame reduction ratio of 1.1-3.2x compared to the others for simple kernels. While for more complicated kernels and complete vision pipelines, the FPGA outperforms the others with energy/frame reduction ratios of 1.2-22.3x. It is also observed that the FPGA performs increasingly better as a vision application's pipeline complexity grows.



### Driver Behavior Analysis Using Lane Departure Detection Under Challenging Conditions
- **Arxiv ID**: http://arxiv.org/abs/1906.00093v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.00093v1)
- **Published**: 2019-05-31 21:55:41+00:00
- **Updated**: 2019-05-31 21:55:41+00:00
- **Authors**: Luis Riera, Koray Ozcan, Jennifer Merickel, Mathew Rizzo, Soumik Sarkar, Anuj Sharma
- **Comment**: 6 pages, 4 figures, 2 algorithms
- **Journal**: None
- **Summary**: In this paper, we present a novel model to detect lane regions and extract lane departure events (changes and incursions) from challenging, lower-resolution videos recorded with mobile cameras. Our algorithm used a Mask-RCNN based lane detection model as pre-processor. Recently, deep learning-based models provide state-of-the-art technology for object detection combined with segmentation. Among the several deep learning architectures, convolutional neural networks (CNNs) outperformed other machine learning models, especially for region proposal and object detection tasks. Recent development in object detection has been driven by the success of region proposal methods and region-based CNNs (R-CNNs). Our algorithm utilizes lane segmentation mask for detection and Fix-lag Kalman filter for tracking, rather than the usual approach of detecting lane lines from single video frames. The algorithm permits detection of driver lane departures into left or right lanes from continuous lane detections. Preliminary results show promise for robust detection of lane departure events. The overall sensitivity for lane departure events on our custom test dataset is 81.81%.



### Out of Sight But Not Out of Mind: An Answer Set Programming Based Online Abduction Framework for Visual Sensemaking in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1906.00107v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LO
- **Links**: [PDF](http://arxiv.org/pdf/1906.00107v1)
- **Published**: 2019-05-31 22:23:15+00:00
- **Updated**: 2019-05-31 22:23:15+00:00
- **Authors**: Jakob Suchan, Mehul Bhatt, Srikrishna Varadarajan
- **Comment**: IJCAI 2019: the 28th International Joint Conference on Artificial
  Intelligence (IJCAI) 2019, August 10 - 16, Macao. (Preprint / to appear)
- **Journal**: None
- **Summary**: We demonstrate the need and potential of systematically integrated vision and semantics} solutions for visual sensemaking (in the backdrop of autonomous driving). A general method for online visual sensemaking using answer set programming is systematically formalised and fully implemented. The method integrates state of the art in (deep learning based) visual computing, and is developed as a modular framework usable within hybrid architectures for perception & control. We evaluate and demo with community established benchmarks KITTIMOD and MOT. As use-case, we focus on the significance of human-centred visual sensemaking ---e.g., semantic representation and explainability, question-answering, commonsense interpolation--- in safety-critical autonomous driving situations.



### CLAREL: Classification via retrieval loss for zero-shot learning
- **Arxiv ID**: http://arxiv.org/abs/1906.11892v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.11892v3)
- **Published**: 2019-05-31 22:24:53+00:00
- **Updated**: 2020-04-05 14:59:22+00:00
- **Authors**: Boris N. Oreshkin, Negar Rostamzadeh, Pedro O. Pinheiro, Christopher Pal
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of learning fine-grained cross-modal representations. We propose an instance-based deep metric learning approach in joint visual and textual space. The key novelty of this paper is that it shows that using per-image semantic supervision leads to substantial improvement in zero-shot performance over using class-only supervision. On top of that, we provide a probabilistic justification for a metric rescaling approach that solves a very common problem in the generalized zero-shot learning setting, i.e., classifying test images from unseen classes as one of the classes seen during training. We evaluate our approach on two fine-grained zero-shot learning datasets: CUB and FLOWERS. We find that on the generalized zero-shot classification task CLAREL consistently outperforms the existing approaches on both datasets.



