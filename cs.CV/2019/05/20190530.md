# Arxiv Papers in cs.CV on 2019-05-30
### Evaluating Artificial Systems for Pairwise Ranking Tasks Sensitive to Individual Differences
- **Arxiv ID**: http://arxiv.org/abs/1905.13560v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.13560v1)
- **Published**: 2019-05-30 00:13:16+00:00
- **Updated**: 2019-05-30 00:13:16+00:00
- **Authors**: Xing Liu, Takayuki Okatani
- **Comment**: None
- **Journal**: None
- **Summary**: Owing to the advancement of deep learning, artificial systems are now rival to humans in several pattern recognition tasks, such as visual recognition of object categories. However, this is only the case with the tasks for which correct answers exist independent of human perception. There is another type of tasks for which what to predict is human perception itself, in which there are often individual differences. Then, there are no longer single "correct" answers to predict, which makes evaluation of artificial systems difficult. In this paper, focusing on pairwise ranking tasks sensitive to individual differences, we propose an evaluation method. Given a ranking result for multiple item pairs that is generated by an artificial system, our method quantifies the probability that the same ranking result will be generated by humans, and judges if it is distinguishable from human-generated results. We introduce a probabilistic model of human ranking behavior, and present an efficient computation method for the judgment. To estimate model parameters accurately from small-size samples, we present a method that uses confidence scores given by annotators for ranking each item pair. Taking as an example a task of ranking image pairs according to material attributes of objects, we demonstrate how the proposed method works.



### Fashion IQ: A New Dataset Towards Retrieving Images by Natural Language Feedback
- **Arxiv ID**: http://arxiv.org/abs/1905.12794v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.12794v3)
- **Published**: 2019-05-30 00:15:12+00:00
- **Updated**: 2020-11-25 22:10:37+00:00
- **Authors**: Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven Rennie, Kristen Grauman, Rogerio Feris
- **Comment**: None
- **Journal**: None
- **Summary**: Conversational interfaces for the detail-oriented retail fashion domain are more natural, expressive, and user friendly than classical keyword-based search interfaces. In this paper, we introduce the Fashion IQ dataset to support and advance research on interactive fashion image retrieval. Fashion IQ is the first fashion dataset to provide human-generated captions that distinguish similar pairs of garment images together with side-information consisting of real-world product descriptions and derived visual attribute labels for these images. We provide a detailed analysis of the characteristics of the Fashion IQ data, and present a transformer-based user simulator and interactive image retriever that can seamlessly integrate visual attributes with image features, user feedback, and dialog history, leading to improved performance over the state of the art in dialog-based image retrieval. We believe that our dataset will encourage further work on developing more natural and real-world applicable conversational shopping assistants.



### Bandlimiting Neural Networks Against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1905.12797v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, cs.NE, stat.ML, I.1.5
- **Links**: [PDF](http://arxiv.org/pdf/1905.12797v1)
- **Published**: 2019-05-30 00:34:50+00:00
- **Updated**: 2019-05-30 00:34:50+00:00
- **Authors**: Yuping Lin, Kasra Ahmadi K. A., Hui Jiang
- **Comment**: Summitted to NeurIPS 2019
- **Journal**: None
- **Summary**: In this paper, we study the adversarial attack and defence problem in deep learning from the perspective of Fourier analysis. We first explicitly compute the Fourier transform of deep ReLU neural networks and show that there exist decaying but non-zero high frequency components in the Fourier spectrum of neural networks. We demonstrate that the vulnerability of neural networks towards adversarial samples can be attributed to these insignificant but non-zero high frequency components. Based on this analysis, we propose to use a simple post-averaging technique to smooth out these high frequency components to improve the robustness of neural networks against adversarial attacks. Experimental results on the ImageNet dataset have shown that our proposed method is universally effective to defend many existing adversarial attacking methods proposed in the literature, including FGSM, PGD, DeepFool and C&W attacks. Our post-averaging method is simple since it does not require any re-training, and meanwhile it can successfully defend over 95% of the adversarial samples generated by these methods without introducing any significant performance degradation (less than 1%) on the original clean images.



### InsectUp: Crowdsourcing Insect Observations to Assess Demographic Shifts and Improve Classification
- **Arxiv ID**: http://arxiv.org/abs/1906.11898v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.11898v2)
- **Published**: 2019-05-30 00:57:15+00:00
- **Updated**: 2020-01-29 18:39:03+00:00
- **Authors**: Léonard Boussioux, Tomás Giro-Larraz, Charles Guille-Escuret, Mehdi Cherti, Balázs Kégl
- **Comment**: Appearing at the International Conference on Machine Learning, AI for
  Social Good Workshop, Long Beach, United States, 2019 Appearing at the
  International Conference on Computer Vision, AI for Wildlife Conservation
  Workshop, Seoul, South Korea, 2019 5 pages, 6 figures
- **Journal**: None
- **Summary**: Insects play such a crucial role in ecosystems that a shift in demography of just a few species can have devastating consequences at environmental, social and economic levels. Despite this, evaluation of insect demography is strongly limited by the difficulty of collecting census data at sufficient scale. We propose a method to gather and leverage observations from bystanders, hikers, and entomology enthusiasts in order to provide researchers with data that could significantly help anticipate and identify environmental threats. Finally, we show that there is indeed interest on both sides for such collaboration.



### Deep Learning Approach for Receipt Recognition
- **Arxiv ID**: http://arxiv.org/abs/1905.12817v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.12817v1)
- **Published**: 2019-05-30 01:33:13+00:00
- **Updated**: 2019-05-30 01:33:13+00:00
- **Authors**: Anh Duc Le, Dung Van Pham, Tuan Anh Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: Inspired by the recent successes of deep learning on Computer Vision and Natural Language Processing, we present a deep learning approach for recognizing scanned receipts. The recognition system has two main modules: text detection based on Connectionist Text Proposal Network and text recognition based on Attention-based Encoder-Decoder. We also proposed pre-processing to extract receipt area and OCR verification to ignore handwriting. The experiments on the dataset of the Robust Reading Challenge on Scanned Receipts OCR and Information Extraction 2019 demonstrate that the accuracies were improved by integrating the pre-processing and the OCR verification. Our recognition system achieved 71.9% of the F1 score for detection and recognition task.



### Wasserstein Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1905.12828v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.12828v1)
- **Published**: 2019-05-30 02:09:28+00:00
- **Updated**: 2019-05-30 02:09:28+00:00
- **Authors**: Youssef Mroueh
- **Comment**: None
- **Journal**: None
- **Summary**: We propose Gaussian optimal transport for Image style transfer in an Encoder/Decoder framework. Optimal transport for Gaussian measures has closed forms Monge mappings from source to target distributions. Moreover interpolates between a content and a style image can be seen as geodesics in the Wasserstein Geometry. Using this insight, we show how to mix different target styles , using Wasserstein barycenter of Gaussian measures. Since Gaussians are closed under Wasserstein barycenter, this allows us a simple style transfer and style mixing and interpolation. Moreover we show how mixing different styles can be achieved using other geodesic metrics between gaussians such as the Fisher Rao metric, while the transport of the content to the new interpolate style is still performed with Gaussian OT maps. Our simple methodology allows to generate new stylized content interpolating between many artistic styles. The metric used in the interpolation results in different stylizations.



### Attention: A Big Surprise for Cross-Domain Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1905.12830v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.12830v1)
- **Published**: 2019-05-30 02:17:07+00:00
- **Updated**: 2019-05-30 02:17:07+00:00
- **Authors**: Haijun Liu, Jian Cheng, Shiguang Wang, Wen Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we focus on model generalization and adaptation for cross-domain person re-identification (Re-ID). Unlike existing cross-domain Re-ID methods, leveraging the auxiliary information of those unlabeled target-domain data, we aim at enhancing the model generalization and adaptation by discriminative feature learning, and directly exploiting a pre-trained model to new domains (datasets) without any utilization of the information from target domains. To address the discriminative feature learning problem, we surprisingly find that simply introducing the attention mechanism to adaptively extract the person features for every domain is of great effectiveness. We adopt two popular type of attention mechanisms, long-range dependency based attention and direct generation based attention. Both of them can perform the attention via spatial or channel dimensions alone, even the combination of spatial and channel dimensions. The outline of different attentions are well illustrated. Moreover, we also incorporate the attention results into the final output of model through skip-connection to improve the features with both high and middle level semantic visual information. In the manner of directly exploiting a pre-trained model to new domains, the attention incorporation method truly could enhance the model generalization and adaptation to perform the cross-domain person Re-ID. We conduct extensive experiments between three large datasets, Market-1501, DukeMTMC-reID and MSMT17. Surprisingly, introducing only attention can achieve state-of-the-art performance, even much better than those cross-domain Re-ID methods utilizing auxiliary information from the target domain.



### The General Pair-based Weighting Loss for Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.12837v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.12837v1)
- **Published**: 2019-05-30 02:59:26+00:00
- **Updated**: 2019-05-30 02:59:26+00:00
- **Authors**: Haijun Liu, Jian Cheng, Wen Wang, Yanzhou Su
- **Comment**: None
- **Journal**: None
- **Summary**: Deep metric learning aims at learning the distance metric between pair of samples, through the deep neural networks to extract the semantic feature embeddings where similar samples are close to each other while dissimilar samples are farther apart. A large amount of loss functions based on pair distances have been presented in the literature for guiding the training of deep metric learning. In this paper, we unify them in a general pair-based weighting loss function, where the minimizing objective loss is just the distances weighting of informative pairs. The general pair-based weighting loss includes two main aspects, (1) samples mining and (2) pairs weighting. Samples mining aims at selecting the informative positive and negative pair sets to exploit the structured relationship of samples in a mini-batch and also reduce the number of non-trivial pairs. Pair weighting aims at assigning different weights for different pairs according to the pair distances for discriminatively training the network. We detailedly review those existing pair-based losses inline with our general loss function, and explore some possible methods from the perspective of samples mining and pairs weighting. Finally, extensive experiments on three image retrieval datasets show that our general pair-based weighting loss obtains new state-of-the-art performance, demonstrating the effectiveness of the pair-based samples mining and pairs weighting for deep metric learning.



### Unsupervised Classification of Street Architectures Based on InfoGAN
- **Arxiv ID**: http://arxiv.org/abs/1905.12844v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.12844v1)
- **Published**: 2019-05-30 03:42:19+00:00
- **Updated**: 2019-05-30 03:42:19+00:00
- **Authors**: Ning Wang, Xianhan Zeng, Renjie Xie, Zefei Gao, Yi Zheng, Ziran Liao, Junyan Yang, Qiao Wang
- **Comment**: arXiv admin note: text overlap with arXiv:1804.08286,
  arXiv:1606.03657 by other authors
- **Journal**: None
- **Summary**: Street architectures play an essential role in city image and streetscape analysing. However, existing approaches are all supervised which require costly labeled data. To solve this, we propose a street architectural unsupervised classification framework based on Information maximizing Generative Adversarial Nets (InfoGAN), in which we utilize the auxiliary distribution $Q$ of InfoGAN as an unsupervised classifier. Experiments on database of true street view images in Nanjing, China validate the practicality and accuracy of our framework. Furthermore, we draw a series of heuristic conclusions from the intrinsic information hidden in true images. These conclusions will assist planners to know the architectural categories better.



### Towards Photo-Realistic Visible Watermark Removal with Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.12845v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.12845v3)
- **Published**: 2019-05-30 04:08:49+00:00
- **Updated**: 2019-08-19 05:40:10+00:00
- **Authors**: Xiang Li, Chan Lu, Danni Cheng, Wei-Hong Li, Mei Cao, Bo Liu, Jiechao Ma, Wei-Shi Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Visible watermark plays an important role in image copyright protection and the robustness of a visible watermark to an attack is shown to be essential. To evaluate and improve the effectiveness of watermark, watermark removal attracts increasing attention and becomes a hot research top. Current methods cast the watermark removal as an image-to-image translation problem where the encode-decode architectures with pixel-wise loss are adopted to transfer the transparent watermarked pixels into unmarked pixels. However, when a number of realistic images are presented, the watermarks are more likely to be unknown and diverse (i.e., the watermarks might be opaque or semi-transparent; the category and pattern of watermarks are unknown). When applying existing methods to the real-world scenarios, they mostly can not satisfactorily reconstruct the hidden information obscured under the complex and various watermarks (i.e., the residual watermark traces remain and the reconstructed images lack reality). To address this difficulty, in this paper, we present a new watermark processing framework using the conditional generative adversarial networks (cGANs) for visible watermark removal in the real-world application. The proposed method enables the watermark removal solution to be more closed to the photo-realistic reconstruction using a patch-based discriminator conditioned on the watermarked images, which is adversarially trained to differentiate the difference between the recovered images and original watermark-free images. Extensive experimental results on a large-scale visible watermark dataset demonstrate the effectiveness of the proposed method and clearly indicate that our proposed approach can produce more photo-realistic and convincing results compared with the state-of-the-art methods.



### RoNIN: Robust Neural Inertial Navigation in the Wild: Benchmark, Evaluations, and New Methods
- **Arxiv ID**: http://arxiv.org/abs/1905.12853v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1905.12853v1)
- **Published**: 2019-05-30 05:06:57+00:00
- **Updated**: 2019-05-30 05:06:57+00:00
- **Authors**: Hang Yan, Sachini Herath, Yasutaka Furukawa
- **Comment**: None
- **Journal**: None
- **Summary**: This paper sets a new foundation for data-driven inertial navigation research, where the task is the estimation of positions and orientations of a moving subject from a sequence of IMU sensor measurements. More concretely, the paper presents 1) a new benchmark containing more than 40 hours of IMU sensor data from 100 human subjects with ground-truth 3D trajectories under natural human motions; 2) novel neural inertial navigation architectures, making significant improvements for challenging motion cases; and 3) qualitative and quantitative evaluations of the competing methods over three inertial navigation benchmarks. We will share the code and data to promote further research.



### Characterizing Bias in Classifiers using Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1906.11891v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.11891v1)
- **Published**: 2019-05-30 05:48:40+00:00
- **Updated**: 2019-05-30 05:48:40+00:00
- **Authors**: Daniel McDuff, Shuang Ma, Yale Song, Ashish Kapoor
- **Comment**: None
- **Journal**: None
- **Summary**: Models that are learned from real-world data are often biased because the data used to train them is biased. This can propagate systemic human biases that exist and ultimately lead to inequitable treatment of people, especially minorities. To characterize bias in learned classifiers, existing approaches rely on human oracles labeling real-world examples to identify the "blind spots" of the classifiers; these are ultimately limited due to the human labor required and the finite nature of existing image examples. We propose a simulation-based approach for interrogating classifiers using generative adversarial models in a systematic manner. We incorporate a progressive conditional generative model for synthesizing photo-realistic facial images and Bayesian Optimization for an efficient interrogation of independent facial image classification systems. We show how this approach can be used to efficiently characterize racial and gender biases in commercial systems.



### CS-R-FCN: Cross-supervised Learning for Large-Scale Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1905.12863v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.12863v2)
- **Published**: 2019-05-30 05:54:48+00:00
- **Updated**: 2020-01-15 05:10:05+00:00
- **Authors**: Ye Guo, Yali Li, Shengjin Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Generic object detection is one of the most fundamental problems in computer vision, yet it is difficult to provide all the bounding-box-level annotations aiming at large-scale object detection for thousands of categories. In this paper, we present a novel cross-supervised learning pipeline for large-scale object detection, denoted as CS-R-FCN. First, we propose to utilize the data flow of image-level annotated images in the fully-supervised two-stage object detection framework, leading to cross-supervised learning combining bounding-box-level annotated data and image-level annotated data. Second, we introduce a semantic aggregation strategy utilizing the relationships among the cross-supervised categories to reduce the unreasonable mutual inhibition effects during the feature learning. Experimental results show that the proposed CS-R-FCN improves the mAP by a large margin compared to previous related works.



### A Trainable Multiplication Layer for Auto-correlation and Co-occurrence Extraction
- **Arxiv ID**: http://arxiv.org/abs/1905.12871v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.12871v1)
- **Published**: 2019-05-30 06:21:54+00:00
- **Updated**: 2019-05-30 06:21:54+00:00
- **Authors**: Hideaki Hayashi, Seiichi Uchida
- **Comment**: None
- **Journal**: In Proceedings of the 14th Asian Conference on Computer Vision
  (ACCV 2018)
- **Summary**: In this paper, we propose a trainable multiplication layer (TML) for a neural network that can be used to calculate the multiplication between the input features. Taking an image as an input, the TML raises each pixel value to the power of a weight and then multiplies them, thereby extracting the higher-order local auto-correlation from the input image. The TML can also be used to extract co-occurrence from the feature map of a convolutional network. The training of the TML is formulated based on backpropagation with constraints to the weights, enabling us to learn discriminative multiplication patterns in an end-to-end manner. In the experiments, the characteristics of the TML are investigated by visualizing learned kernels and the corresponding output features. The applicability of the TML for classification and neural network interpretation is also evaluated using public datasets.



### P3SGD: Patient Privacy Preserving SGD for Regularizing Deep CNNs in Pathological Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1905.12883v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.12883v1)
- **Published**: 2019-05-30 07:07:29+00:00
- **Updated**: 2019-05-30 07:07:29+00:00
- **Authors**: Bingzhe Wu, Shiwan Zhao, Guangyu Sun, Xiaolu Zhang, Zhong Su, Caihong Zeng, Zhihong Liu
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: Recently, deep convolutional neural networks (CNNs) have achieved great success in pathological image classification. However, due to the limited number of labeled pathological images, there are still two challenges to be addressed: (1) overfitting: the performance of a CNN model is undermined by the overfitting due to its huge amounts of parameters and the insufficiency of labeled training data. (2) privacy leakage: the model trained using a conventional method may involuntarily reveal the private information of the patients in the training dataset. The smaller the dataset, the worse the privacy leakage. To tackle the above two challenges, we introduce a novel stochastic gradient descent (SGD) scheme, named patient privacy preserving SGD (P3SGD), which performs the model update of the SGD in the patient level via a large-step update built upon each patient's data. Specifically, to protect privacy and regularize the CNN model, we propose to inject the well-designed noise into the updates. Moreover, we equip our P3SGD with an elaborated strategy to adaptively control the scale of the injected noise. To validate the effectiveness of P3SGD, we perform extensive experiments on a real-world clinical dataset and quantitatively demonstrate the superior ability of P3SGD in reducing the risk of overfitting. We also provide a rigorous analysis of the privacy cost under differential privacy. Additionally, we find that the models trained with P3SGD are resistant to the model-inversion attack compared with those trained using non-private SGD.



### iSAID: A Large-scale Dataset for Instance Segmentation in Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/1905.12886v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.12886v2)
- **Published**: 2019-05-30 07:18:28+00:00
- **Updated**: 2019-08-28 05:57:00+00:00
- **Authors**: Syed Waqas Zamir, Aditya Arora, Akshita Gupta, Salman Khan, Guolei Sun, Fahad Shahbaz Khan, Fan Zhu, Ling Shao, Gui-Song Xia, Xiang Bai
- **Comment**: CVPR'19 Workshops (Detecting Objects in Aerial Images). The dataset
  is publicly available at: https://captain-whu.github.io/iSAID/index.html
- **Journal**: None
- **Summary**: Existing Earth Vision datasets are either suitable for semantic segmentation or object detection. In this work, we introduce the first benchmark dataset for instance segmentation in aerial imagery that combines instance-level object detection and pixel-level segmentation tasks. In comparison to instance segmentation in natural scenes, aerial images present unique challenges e.g., a huge number of instances per image, large object-scale variations and abundant tiny objects. Our large-scale and densely annotated Instance Segmentation in Aerial Images Dataset (iSAID) comes with 655,451 object instances for 15 categories across 2,806 high-resolution images. Such precise per-pixel annotations for each instance ensure accurate localization that is essential for detailed scene analysis. Compared to existing small-scale aerial image based instance segmentation datasets, iSAID contains 15$\times$ the number of object categories and 5$\times$ the number of instances. We benchmark our dataset using two popular instance segmentation approaches for natural images, namely Mask R-CNN and PANet. In our experiments we show that direct application of off-the-shelf Mask R-CNN and PANet on aerial images provide suboptimal instance segmentation results, thus requiring specialized solutions from the research community. The dataset is publicly available at: https://captain-whu.github.io/iSAID/index.html



### Does computer vision matter for action?
- **Arxiv ID**: http://arxiv.org/abs/1905.12887v2
- **DOI**: 10.1126/scirobotics.aaw6661
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1905.12887v2)
- **Published**: 2019-05-30 07:18:33+00:00
- **Updated**: 2019-10-22 06:33:45+00:00
- **Authors**: Brady Zhou, Philipp Krähenbühl, Vladlen Koltun
- **Comment**: Published in Science Robotics, 4(30), May 2019
- **Journal**: Science Robotics 22 May 2019: Vol. 4, Issue 30, eaaw6661
- **Summary**: Computer vision produces representations of scene content. Much computer vision research is predicated on the assumption that these intermediate representations are useful for action. Recent work at the intersection of machine learning and robotics calls this assumption into question by training sensorimotor systems directly for the task at hand, from pixels to actions, with no explicit intermediate representations. Thus the central question of our work: Does computer vision matter for action? We probe this question and its offshoots via immersive simulation, which allows us to conduct controlled reproducible experiments at scale. We instrument immersive three-dimensional environments to simulate challenges such as urban driving, off-road trail traversal, and battle. Our main finding is that computer vision does matter. Models equipped with intermediate representations train faster, achieve higher task performance, and generalize better to previously unseen environments. A video that summarizes the work and illustrates the results can be found at https://youtu.be/4MfWa2yZ0Jc



### Learning Semantics-aware Distance Map with Semantics Layering Network for Amodal Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.12898v2
- **DOI**: 10.1145/3343031.3350911
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.12898v2)
- **Published**: 2019-05-30 07:46:54+00:00
- **Updated**: 2019-08-22 17:34:05+00:00
- **Authors**: Ziheng Zhang, Anpei Chen, Ling Xie, Jingyi Yu, Shenghua Gao
- **Comment**: This paper is submitted to ACMMM19
- **Journal**: None
- **Summary**: In this work, we demonstrate yet another approach to tackle the amodal segmentation problem. Specifically, we first introduce a new representation, namely a semantics-aware distance map (sem-dist map), to serve as our target for amodal segmentation instead of the commonly used masks and heatmaps. The sem-dist map is a kind of level-set representation, of which the different regions of an object are placed into different levels on the map according to their visibility. It is a natural extension of masks and heatmaps, where modal, amodal segmentation, as well as depth order information, are all well-described. Then we also introduce a novel convolutional neural network (CNN) architecture, which we refer to as semantic layering network, to estimate sem-dist maps layer by layer, from the global-level to the instance-level, for all objects in an image. Extensive experiments on the COCOA and D2SA datasets have demonstrated that our framework can predict amodal segmentation, occlusion and depth order with state-of-the-art performance.



### Interactive-predictive neural multimodal systems
- **Arxiv ID**: http://arxiv.org/abs/1905.12980v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1905.12980v1)
- **Published**: 2019-05-30 11:47:04+00:00
- **Updated**: 2019-05-30 11:47:04+00:00
- **Authors**: Álvaro Peris, Francisco Casacuberta
- **Comment**: To appear at IbPRIA 2019
- **Journal**: None
- **Summary**: Despite the advances achieved by neural models in sequence to sequence learning, exploited in a variety of tasks, they still make errors. In many use cases, these are corrected by a human expert in a posterior revision process. The interactive-predictive framework aims to minimize the human effort spent on this process by considering partial corrections for iteratively refining the hypothesis. In this work, we generalize the interactive-predictive approach, typically applied in to machine translation field, to tackle other multimodal problems namely, image and video captioning. We study the application of this framework to multimodal neural sequence to sequence models. We show that, following this framework, we approximately halve the effort spent for correcting the outputs generated by the automatic systems. Moreover, we deploy our systems in a publicly accessible demonstration, that allows to better understand the behavior of the interactive-predictive framework.



### 3D Reconstruction of Whole Stomach from Endoscope Video Using Structure-from-Motion
- **Arxiv ID**: http://arxiv.org/abs/1905.12988v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.12988v1)
- **Published**: 2019-05-30 12:05:20+00:00
- **Updated**: 2019-05-30 12:05:20+00:00
- **Authors**: Aji Resindra Widya, Yusuke Monno, Kosuke Imahori, Masatoshi Okutomi, Sho Suzuki, Takuji Gotoda, Kenji Miki
- **Comment**: 5 pages, 4 figures, accepted in EMBC 2019
- **Journal**: None
- **Summary**: Gastric endoscopy is a common clinical practice that enables medical doctors to diagnose the stomach inside a body. In order to identify a gastric lesion's location such as early gastric cancer within the stomach, this work addressed to reconstruct the 3D shape of a whole stomach with color texture information generated from a standard monocular endoscope video. Previous works have tried to reconstruct the 3D structures of various organs from endoscope images. However, they are mainly focused on a partial surface. In this work, we investigated how to enable structure-from-motion (SfM) to reconstruct the whole shape of a stomach from a standard endoscope video. We specifically investigated the combined effect of chromo-endoscopy and color channel selection on SfM. Our study found that 3D reconstruction of the whole stomach can be achieved by using red channel images captured under chromo-endoscopy by spreading indigo carmine (IC) dye on the stomach surface.



### Generalized Separable Nonnegative Matrix Factorization
- **Arxiv ID**: http://arxiv.org/abs/1905.12995v2
- **DOI**: 10.1109/TPAMI.2019.2956046
- **Categories**: **cs.LG**, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.12995v2)
- **Published**: 2019-05-30 12:18:25+00:00
- **Updated**: 2019-10-15 11:22:33+00:00
- **Authors**: Junjun Pan, Nicolas Gillis
- **Comment**: 31 pages, 12 figures, 4 tables. We have added discussions about the
  identifiability of the model, we have modified the first synthetic
  experiment, we have clarified some aspects of the contribution
- **Journal**: IEEE Trans. on Pattern Analysis and Machine Intelligence 43 (5),
  pp. 1546-1561, 2021
- **Summary**: Nonnegative matrix factorization (NMF) is a linear dimensionality technique for nonnegative data with applications such as image analysis, text mining, audio source separation and hyperspectral unmixing. Given a data matrix $M$ and a factorization rank $r$, NMF looks for a nonnegative matrix $W$ with $r$ columns and a nonnegative matrix $H$ with $r$ rows such that $M \approx WH$. NMF is NP-hard to solve in general. However, it can be computed efficiently under the separability assumption which requires that the basis vectors appear as data points, that is, that there exists an index set $\mathcal{K}$ such that $W = M(:,\mathcal{K})$. In this paper, we generalize the separability assumption: We only require that for each rank-one factor $W(:,k)H(k,:)$ for $k=1,2,\dots,r$, either $W(:,k) = M(:,j)$ for some $j$ or $H(k,:) = M(i,:)$ for some $i$. We refer to the corresponding problem as generalized separable NMF (GS-NMF). We discuss some properties of GS-NMF and propose a convex optimization model which we solve using a fast gradient method. We also propose a heuristic algorithm inspired by the successive projection algorithm. To verify the effectiveness of our methods, we compare them with several state-of-the-art separable NMF algorithms on synthetic, document and image data sets.



### Memory-efficient and fast implementation of local adaptive binarization methods
- **Arxiv ID**: http://arxiv.org/abs/1905.13038v3
- **DOI**: None
- **Categories**: **cs.CV**, 68U10 (Primary) 68T45, 68W40 (Secondary), I.4.6; I.7.5
- **Links**: [PDF](http://arxiv.org/pdf/1905.13038v3)
- **Published**: 2019-05-30 13:17:18+00:00
- **Updated**: 2019-07-31 14:01:21+00:00
- **Authors**: Chungkwong Chan
- **Comment**: 8 pages, 4 figures, corrected typos and added reference to source
  code
- **Journal**: None
- **Summary**: Binarization is widely used as an image preprocessing step to separate object especially text from background before recognition. For noisy images with uneven illumination such as degraded documents, threshold values need to be computed pixel by pixel to obtain a good segmentation. Since local threshold values typically depend on moment-based statistics such as mean and variance of gray levels inside rectangular windows, integral images which are memory consuming are commonly used to accelerate the calculation. Observed that moment-based statistics as well as quantiles in a sliding window can be computed recursively, integral images can be avoided without neglecting speed, more binarization methods can be accelerated too. In particular, given a $H\times W$ input image, Sauvola's method and alike can run in $\Theta (HW)$ time independent of window size, while only around $6\min\{H,W\}$ bytes of auxiliary space is needed, which is significantly lower than the $16HW$ bytes occupied by the two integral images. Since the proposed technique enable various well-known local adaptive binarization methods to be applied in real-time use cases on devices with limited resources, it has the potential of wide application.



### Align-and-Attend Network for Globally and Locally Coherent Video Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1905.13066v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.13066v1)
- **Published**: 2019-05-30 14:14:08+00:00
- **Updated**: 2019-05-30 14:14:08+00:00
- **Authors**: Sanghyun Woo, Dahun Kim, KwanYong Park, Joon-Young Lee, In So Kweon
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel feed-forward network for video inpainting. We use a set of sampled video frames as the reference to take visible contents to fill the hole of a target frame. Our video inpainting network consists of two stages. The first stage is an alignment module that uses computed homographies between the reference frames and the target frame. The visible patches are then aggregated based on the frame similarity to fill in the target holes roughly. The second stage is a non-local attention module that matches the generated patches with known reference patches (in space and time) to refine the previous global alignment stage. Both stages consist of large spatial-temporal window size for the reference and thus enable modeling long-range correlations between distant information and the hole regions. Therefore, even challenging scenes with large or slowly moving holes can be handled, which have been hardly modeled by existing flow-based approach. Our network is also designed with a recurrent propagation stream to encourage temporal consistency in video results. Experiments on video object removal demonstrate that our method inpaints the holes with globally and locally coherent contents.



### Robust Sparse Regularization: Simultaneously Optimizing Neural Network Robustness and Compactness
- **Arxiv ID**: http://arxiv.org/abs/1905.13074v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.13074v1)
- **Published**: 2019-05-30 14:32:21+00:00
- **Updated**: 2019-05-30 14:32:21+00:00
- **Authors**: Adnan Siraj Rakin, Zhezhi He, Li Yang, Yanzhi Wang, Liqiang Wang, Deliang Fan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Network (DNN) trained by the gradient descent method is known to be vulnerable to maliciously perturbed adversarial input, aka. adversarial attack. As one of the countermeasures against adversarial attack, increasing the model capacity for DNN robustness enhancement was discussed and reported as an effective approach by many recent works. In this work, we show that shrinking the model size through proper weight pruning can even be helpful to improve the DNN robustness under adversarial attack. For obtaining a simultaneously robust and compact DNN model, we propose a multi-objective training method called Robust Sparse Regularization (RSR), through the fusion of various regularization techniques, including channel-wise noise injection, lasso weight penalty, and adversarial training. We conduct extensive experiments across popular ResNet-20, ResNet-18 and VGG-16 DNN architectures to demonstrate the effectiveness of RSR against popular white-box (i.e., PGD and FGSM) and black-box attacks. Thanks to RSR, 85% weight connections of ResNet-18 can be pruned while still achieving 0.68% and 8.72% improvement in clean- and perturbed-data accuracy respectively on CIFAR-10 dataset, in comparison to its PGD adversarial training baseline.



### A Hierarchical Probabilistic U-Net for Modeling Multi-Scale Ambiguities
- **Arxiv ID**: http://arxiv.org/abs/1905.13077v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.13077v1)
- **Published**: 2019-05-30 14:49:08+00:00
- **Updated**: 2019-05-30 14:49:08+00:00
- **Authors**: Simon A. A. Kohl, Bernardino Romera-Paredes, Klaus H. Maier-Hein, Danilo Jimenez Rezende, S. M. Ali Eslami, Pushmeet Kohli, Andrew Zisserman, Olaf Ronneberger
- **Comment**: 25 pages, 15 figures
- **Journal**: None
- **Summary**: Medical imaging only indirectly measures the molecular identity of the tissue within each voxel, which often produces only ambiguous image evidence for target measures of interest, like semantic segmentation. This diversity and the variations of plausible interpretations are often specific to given image regions and may thus manifest on various scales, spanning all the way from the pixel to the image level. In order to learn a flexible distribution that can account for multiple scales of variations, we propose the Hierarchical Probabilistic U-Net, a segmentation network with a conditional variational auto-encoder (cVAE) that uses a hierarchical latent space decomposition. We show that this model formulation enables sampling and reconstruction of segmenations with high fidelity, i.e. with finely resolved detail, while providing the flexibility to learn complex structured distributions across scales. We demonstrate these abilities on the task of segmenting ambiguous medical scans as well as on instance segmentation of neurobiological and natural images. Our model automatically separates independent factors across scales, an inductive bias that we deem beneficial in structured output prediction tasks beyond segmentation.



### Semantics-Aligned Representation Learning for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1905.13143v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.13143v3)
- **Published**: 2019-05-30 16:09:28+00:00
- **Updated**: 2020-03-18 13:02:27+00:00
- **Authors**: Xin Jin, Cuiling Lan, Wenjun Zeng, Guoqiang Wei, Zhibo Chen
- **Comment**: Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20),
  code has been released
- **Journal**: None
- **Summary**: Person re-identification (reID) aims to match person images to retrieve the ones with the same identity. This is a challenging task, as the images to be matched are generally semantically misaligned due to the diversity of human poses and capture viewpoints, incompleteness of the visible bodies (due to occlusion), etc. In this paper, we propose a framework that drives the reID network to learn semantics-aligned feature representation through delicate supervision designs. Specifically, we build a Semantics Aligning Network (SAN) which consists of a base network as encoder (SA-Enc) for re-ID, and a decoder (SA-Dec) for reconstructing/regressing the densely semantics aligned full texture image. We jointly train the SAN under the supervisions of person re-identification and aligned texture generation. Moreover, at the decoder, besides the reconstruction loss, we add Triplet ReID constraints over the feature maps as the perceptual losses. The decoder is discarded in the inference and thus our scheme is computationally efficient. Ablation studies demonstrate the effectiveness of our design. We achieve the state-of-the-art performances on the benchmark datasets CUHK03, Market1501, MSMT17, and the partial person reID dataset Partial REID. Code for our proposed method is available at: https://github.com/microsoft/Semantics-Aligned-Representation-Learning-for-Person-Re-identification.



### Prostate Cancer Detection using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.13145v1
- **DOI**: 10.1038/s41598-019-55972-4
- **Categories**: **cs.CV**, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1905.13145v1)
- **Published**: 2019-05-30 16:10:39+00:00
- **Updated**: 2019-05-30 16:10:39+00:00
- **Authors**: Sunghwan Yoo, Isha Gujrathi, Masoom A. Haider, Farzad Khalvati
- **Comment**: None
- **Journal**: Scientific Reports 9, 19518 (2019)
- **Summary**: Prostate cancer is one of the most common forms of cancer and the third leading cause of cancer death in North America. As an integrated part of computer-aided detection (CAD) tools, diffusion-weighted magnetic resonance imaging (DWI) has been intensively studied for accurate detection of prostate cancer. With deep convolutional neural networks (CNNs) significant success in computer vision tasks such as object detection and segmentation, different CNNs architectures are increasingly investigated in medical imaging research community as promising solutions for designing more accurate CAD tools for cancer detection. In this work, we developed and implemented an automated CNNs-based pipeline for detection of clinically significant prostate cancer (PCa) for a given axial DWI image and for each patient. DWI images of 427 patients were used as the dataset, which contained 175 patients with PCa and 252 healthy patients. To measure the performance of the proposed pipeline, a test set of 108 (out of 427) patients were set aside and not used in the training phase. The proposed pipeline achieved area under the receiver operating characteristic curve (AUC) of 0.87 (95% Confidence Interval (CI): 0.84-0.90) and 0.84 (95% CI: 0.76-0.91) at slice level and patient level, respectively.



### Grounding Language Attributes to Objects using Bayesian Eigenobjects
- **Arxiv ID**: http://arxiv.org/abs/1905.13153v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.13153v2)
- **Published**: 2019-05-30 16:15:36+00:00
- **Updated**: 2019-08-01 19:07:02+00:00
- **Authors**: Vanya Cohen, Benjamin Burchfiel, Thao Nguyen, Nakul Gopalan, Stefanie Tellex, George Konidaris
- **Comment**: None
- **Journal**: None
- **Summary**: We develop a system to disambiguate object instances within the same class based on simple physical descriptions. The system takes as input a natural language phrase and a depth image containing a segmented object and predicts how similar the observed object is to the object described by the phrase. Our system is designed to learn from only a small amount of human-labeled language data and generalize to viewpoints not represented in the language-annotated depth image training set. By decoupling 3D shape representation from language representation, this method is able to ground language to novel objects using a small amount of language-annotated depth-data and a larger corpus of unlabeled 3D object meshes, even when these objects are partially observed from unusual viewpoints. Our system is able to disambiguate between novel objects, observed via depth images, based on natural language descriptions. Our method also enables view-point transfer; trained on human-annotated data on a small set of depth images captured from frontal viewpoints, our system successfully predicted object attributes from rear views despite having no such depth images in its training set. Finally, we demonstrate our approach on a Baxter robot, enabling it to pick specific objects based on human-provided natural language descriptions.



### Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels
- **Arxiv ID**: http://arxiv.org/abs/1905.13192v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.13192v2)
- **Published**: 2019-05-30 17:23:23+00:00
- **Updated**: 2019-11-04 15:30:12+00:00
- **Authors**: Simon S. Du, Kangcheng Hou, Barnabás Póczos, Ruslan Salakhutdinov, Ruosong Wang, Keyulu Xu
- **Comment**: In NeurIPS 2019. Code available: https://github.com/KangchengHou/gntk
- **Journal**: None
- **Summary**: While graph kernels (GKs) are easy to train and enjoy provable theoretical guarantees, their practical performances are limited by their expressive power, as the kernel function often depends on hand-crafted combinatorial features of graphs. Compared to graph kernels, graph neural networks (GNNs) usually achieve better practical performance, as GNNs use multi-layer architectures and non-linear activation functions to extract high-order information of graphs as features. However, due to the large number of hyper-parameters and the non-convex nature of the training procedure, GNNs are harder to train. Theoretical guarantees of GNNs are also not well-understood. Furthermore, the expressive power of GNNs scales with the number of parameters, and thus it is hard to exploit the full power of GNNs when computing resources are limited. The current paper presents a new class of graph kernels, Graph Neural Tangent Kernels (GNTKs), which correspond to infinitely wide multi-layer GNNs trained by gradient descent. GNTKs enjoy the full expressive power of GNNs and inherit advantages of GKs. Theoretically, we show GNTKs provably learn a class of smooth functions on graphs. Empirically, we test GNTKs on graph classification datasets and show they achieve strong performance.



### An attention-based multi-resolution model for prostate whole slide imageclassification and localization
- **Arxiv ID**: http://arxiv.org/abs/1905.13208v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.13208v1)
- **Published**: 2019-05-30 17:50:56+00:00
- **Updated**: 2019-05-30 17:50:56+00:00
- **Authors**: Jiayun Li, Wenyuan Li, Arkadiusz Gertych, Beatrice S. Knudsen, William Speier, Corey W. Arnold
- **Comment**: 8 pages, 4 figures, CVPR 2019 Towards Causal, Explainable and
  Universal Medical Visual Diagnosis (MVD) Workshop
- **Journal**: None
- **Summary**: Histology review is often used as the `gold standard' for disease diagnosis. Computer aided diagnosis tools can potentially help improve current pathology workflows by reducing examination time and interobserver variability. Previous work in cancer grading has focused mainly on classifying pre-defined regions of interest (ROIs), or relied on large amounts of fine-grained labels. In this paper, we propose a two-stage attention-based multiple instance learning model for slide-level cancer grading and weakly-supervised ROI detection and demonstrate its use in prostate cancer. Compared with existing Gleason classification models, our model goes a step further by utilizing visualized saliency maps to select informative tiles for fine-grained grade classification. The model was primarily developed on a large-scale whole slide dataset consisting of 3,521 prostate biopsy slides with only slide-level labels from 718 patients. The model achieved state-of-the-art performance for prostate cancer grading with an accuracy of 85.11\% for classifying benign, low-grade (Gleason grade 3+3 or 3+4), and high-grade (Gleason grade 4+3 or higher) slides on an independent test set.



### AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures
- **Arxiv ID**: http://arxiv.org/abs/1905.13209v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1905.13209v4)
- **Published**: 2019-05-30 17:51:03+00:00
- **Updated**: 2020-05-27 15:56:37+00:00
- **Authors**: Michael S. Ryoo, AJ Piergiovanni, Mingxing Tan, Anelia Angelova
- **Comment**: None
- **Journal**: ICLR 2020
- **Summary**: Learning to represent videos is a very challenging task both algorithmically and computationally. Standard video CNN architectures have been designed by directly extending architectures devised for image understanding to include the time dimension, using modules such as 3D convolutions, or by using two-stream design to capture both appearance and motion in videos. We interpret a video CNN as a collection of multi-stream convolutional blocks connected to each other, and propose the approach of automatically finding neural architectures with better connectivity and spatio-temporal interactions for video understanding. This is done by evolving a population of overly-connected architectures guided by connection weight learning. Architectures combining representations that abstract different input types (i.e., RGB and optical flow) at multiple temporal resolutions are searched for, allowing different types or sources of information to interact with each other. Our method, referred to as AssembleNet, outperforms prior approaches on public video datasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and 34.27% accuracy on Moments-in-Time.



### What Can Neural Networks Reason About?
- **Arxiv ID**: http://arxiv.org/abs/1905.13211v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.13211v4)
- **Published**: 2019-05-30 17:53:30+00:00
- **Updated**: 2020-02-15 06:56:25+00:00
- **Authors**: Keyulu Xu, Jingling Li, Mozhi Zhang, Simon S. Du, Ken-ichi Kawarabayashi, Stefanie Jegelka
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks have succeeded in many reasoning tasks. Empirically, these tasks require specialized network structures, e.g., Graph Neural Networks (GNNs) perform well on many such tasks, but less structured networks fail. Theoretically, there is limited understanding of why and when a network structure generalizes better than others, although they have equal expressive power. In this paper, we develop a framework to characterize which reasoning tasks a network can learn well, by studying how well its computation structure aligns with the algorithmic structure of the relevant reasoning process. We formally define this algorithmic alignment and derive a sample complexity bound that decreases with better alignment. This framework offers an explanation for the empirical success of popular reasoning models, and suggests their limitations. As an example, we unify seemingly different reasoning tasks, such as intuitive physics, visual question answering, and shortest paths, via the lens of a powerful algorithmic paradigm, dynamic programming (DP). We show that GNNs align with DP and thus are expected to solve these tasks. On several reasoning tasks, our theory is supported by empirical results.



### On Network Design Spaces for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1905.13214v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.13214v1)
- **Published**: 2019-05-30 17:56:17+00:00
- **Updated**: 2019-05-30 17:56:17+00:00
- **Authors**: Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, Piotr Dollár
- **Comment**: tech report
- **Journal**: None
- **Summary**: Over the past several years progress in designing better neural network architectures for visual recognition has been substantial. To help sustain this rate of progress, in this work we propose to reexamine the methodology for comparing network architectures. In particular, we introduce a new comparison paradigm of distribution estimates, in which network design spaces are compared by applying statistical techniques to populations of sampled models, while controlling for confounding factors like network complexity. Compared to current methodologies of comparing point and curve estimates of model families, distribution estimates paint a more complete picture of the entire design landscape. As a case study, we examine design spaces used in neural architecture search (NAS). We find significant statistical differences between recent NAS design space variants that have been largely overlooked. Furthermore, our analysis reveals that the design spaces for standard model families like ResNeXt can be comparable to the more complex ones used in recent NAS work. We hope these insights into distribution analysis will enable more robust progress toward discovering better networks for visual recognition.



### Video from Stills: Lensless Imaging with Rolling Shutter
- **Arxiv ID**: http://arxiv.org/abs/1905.13221v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1905.13221v1)
- **Published**: 2019-05-30 17:59:01+00:00
- **Updated**: 2019-05-30 17:59:01+00:00
- **Authors**: Nick Antipa, Patrick Oare, Emrah Bostan, Ren Ng, Laura Waller
- **Comment**: 8 pages, 7 figures, IEEE International Conference on Computational
  Photography 2019, Tokyo
- **Journal**: None
- **Summary**: Because image sensor chips have a finite bandwidth with which to read out pixels, recording video typically requires a trade-off between frame rate and pixel count. Compressed sensing techniques can circumvent this trade-off by assuming that the image is compressible. Here, we propose using multiplexing optics to spatially compress the scene, enabling information about the whole scene to be sampled from a row of sensor pixels, which can be read off quickly via a rolling shutter CMOS sensor. Conveniently, such multiplexing can be achieved with a simple lensless, diffuser-based imaging system. Using sparse recovery methods, we are able to recover 140 video frames at over 4,500 frames per second, all from a single captured image with a rolling shutter sensor. Our proof-of-concept system uses easily-fabricated diffusers paired with an off-the-shelf sensor. The resulting prototype enables compressive encoding of high frame rate video into a single rolling shutter exposure, and exceeds the sampling-limited performance of an equivalent global shutter system for sufficiently sparse objects.



### A survey of advances in vision-based vehicle re-identification
- **Arxiv ID**: http://arxiv.org/abs/1905.13258v1
- **DOI**: 10.1016/j.cviu.2019.03.001
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.13258v1)
- **Published**: 2019-05-30 18:45:40+00:00
- **Updated**: 2019-05-30 18:45:40+00:00
- **Authors**: Sultan Daud Khan, Habib Ullah
- **Comment**: 17 pages; 21 figures; journal paper
- **Journal**: Computer Vision and Image Understanding 2019
- **Summary**: Vehicle re-identification (V-reID) has become significantly popular in the community due to its applications and research significance. In particular, the V-reID is an important problem that still faces numerous open challenges. This paper reviews different V-reID methods including sensor based methods, hybrid methods, and vision based methods which are further categorized into hand-crafted feature based methods and deep feature based methods. The vision based methods make the V-reID problem particularly interesting, and our review systematically addresses and evaluates these methods for the first time. We conduct experiments on four comprehensive benchmark datasets and compare the performances of recent hand-crafted feature based methods and deep feature based methods. We present the detail analysis of these methods in terms of mean average precision (mAP) and cumulative matching curve (CMC). These analyses provide objective insight into the strengths and weaknesses of these methods. We also provide the details of different V-reID datasets and critically discuss the challenges and future trends of V-reID methods.



### Large Scale Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.13260v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.13260v1)
- **Published**: 2019-05-30 18:51:39+00:00
- **Updated**: 2019-05-30 18:51:39+00:00
- **Authors**: Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, Yun Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Modern machine learning suffers from catastrophic forgetting when learning new classes incrementally. The performance dramatically degrades due to the missing data of old classes. Incremental learning methods have been proposed to retain the knowledge acquired from the old classes, by using knowledge distilling and keeping a few exemplars from the old classes. However, these methods struggle to scale up to a large number of classes. We believe this is because of the combination of two factors: (a) the data imbalance between the old and new classes, and (b) the increasing number of visually similar classes. Distinguishing between an increasing number of visually similar classes is particularly challenging, when the training data is unbalanced. We propose a simple and effective method to address this data imbalance issue. We found that the last fully connected layer has a strong bias towards the new classes, and this bias can be corrected by a linear model. With two bias parameters, our method performs remarkably well on two large datasets: ImageNet (1000 classes) and MS-Celeb-1M (10000 classes), outperforming the state-of-the-art algorithms by 11.1% and 13.2% respectively.



### Efficient Object Detection Model for Real-Time UAV Applications
- **Arxiv ID**: http://arxiv.org/abs/1906.00786v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.00786v1)
- **Published**: 2019-05-30 20:24:13+00:00
- **Updated**: 2019-05-30 20:24:13+00:00
- **Authors**: Subrahmanyam Vaddi, Chandan Kumar, Ali Jannesari
- **Comment**: 10 pages, 4 figures, Under Review. arXiv admin note: substantial text
  overlap with arXiv:1808.07256 by other authors without attribution;
  substantial text overlap with arXiv:1807.06789, arXiv:1612.03144,
  arXiv:1809.03193 by other authors
- **Journal**: None
- **Summary**: Unmanned Aerial Vehicles (UAVs) especially drones, equipped with vision techniques have become very popular in recent years, with their extensive use in wide range of applications. Many of these applications require use of computer vision techniques, particularly object detection from the information captured by on-board camera. In this paper, we propose an end to end object detection model running on a UAV platform which is suitable for real-time applications. We propose a deep feature pyramid architecture which makes use of inherent properties of features extracted from Convolutional Networks by capturing more generic features in the images (such as edge, color etc.) along with the minute detailed features specific to the classes contained in our problem. We use VisDrone-18 dataset for our studies which contain different objects such as pedestrians, vehicles, bicycles etc. We provide software and hardware architecture of our platform used in this study. We implemented our model with both ResNet and MobileNet as convolutional bases. Our model combined with modified focal loss function, produced a desirable performance of 30.6 mAP for object detection with an inference time of 14 fps. We compared our results with RetinaNet-ResNet-50 and HAL-RetinaNet and shown that our model combined with MobileNet as backend feature extractor gave the best results in terms of accuracy, speed and memory efficiency and is best suitable for real time object detection with drones.



### Seeing the Wind: Visual Wind Speed Prediction with a Coupled Convolutional and Recurrent Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1905.13290v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, physics.flu-dyn
- **Links**: [PDF](http://arxiv.org/pdf/1905.13290v3)
- **Published**: 2019-05-30 20:24:25+00:00
- **Updated**: 2019-10-28 20:55:33+00:00
- **Authors**: Jennifer L Cardona, Michael F Howland, John O Dabiri
- **Comment**: NeurIPS 2019 (to appear). The dataset has been expanded to include
  videos of a tree canopy in addition to flags. The models were retrained, and
  results were updated accordingly. The introduction and related work sections
  were also expand upon. Clarifying details were added to explain author
  choices such as time averaging windows and to further discuss test set
  results
- **Journal**: Advances in Neural Information Processing Systems 32 (2019)
- **Summary**: Wind energy resource quantification, air pollution monitoring, and weather forecasting all rely on rapid, accurate measurement of local wind conditions. Visual observations of the effects of wind---the swaying of trees and flapping of flags, for example---encode information regarding local wind conditions that can potentially be leveraged for visual anemometry that is inexpensive and ubiquitous. Here, we demonstrate a coupled convolutional neural network and recurrent neural network architecture that extracts the wind speed encoded in visually recorded flow-structure interactions of a flag and tree in naturally occurring wind. Predictions for wind speeds ranging from 0.75-11 m/s showed agreement with measurements from a cup anemometer on site, with a root-mean-squared error approaching the natural wind speed variability due to atmospheric turbulence. Generalizability of the network was demonstrated by successful prediction of wind speed based on recordings of other flags in the field and in a controlled wind tunnel test. Furthermore, physics-based scaling of the flapping dynamics accurately predicts the dependence of the network performance on the video frame rate and duration.



### Counting and Segmenting Sorghum Heads
- **Arxiv ID**: http://arxiv.org/abs/1905.13291v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.13291v1)
- **Published**: 2019-05-30 20:27:26+00:00
- **Updated**: 2019-05-30 20:27:26+00:00
- **Authors**: Min-hwan Oh, Peder Olsen, Karthikeyan Natesan Ramamurthy
- **Comment**: 23 pages, 23 figures, 5 tables
- **Journal**: None
- **Summary**: Phenotyping is the process of measuring an organism's observable traits. Manual phenotyping of crops is a labor-intensive, time-consuming, costly, and error prone process. Accurate, automated, high-throughput phenotyping can relieve a huge burden in the crop breeding pipeline. In this paper, we propose a scalable, high-throughput approach to automatically count and segment panicles (heads), a key phenotype, from aerial sorghum crop imagery. Our counting approach uses the image density map obtained from dot or region annotation as the target with a novel deep convolutional neural network architecture. We also propose a novel instance segmentation algorithm using the estimated density map, to identify the individual panicles in the presence of occlusion. With real Sorghum aerial images, we obtain a mean absolute error (MAE) of 1.06 for counting which is better than using well-known crowd counting approaches such as CCNN, MCNN and CSRNet models. The instance segmentation model also produces respectable results which will be ultimately useful in reducing the manual annotation workload for future data.



### Multitask Text-to-Visual Embedding with Titles and Clickthrough Data
- **Arxiv ID**: http://arxiv.org/abs/1905.13339v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1905.13339v1)
- **Published**: 2019-05-30 22:33:15+00:00
- **Updated**: 2019-05-30 22:33:15+00:00
- **Authors**: Pranav Aggarwal, Zhe Lin, Baldo Faieta, Saeid Motiian
- **Comment**: 4 pages. Language and Vision Workshop, in conjunction with CVPR 2019
- **Journal**: None
- **Summary**: Text-visual (or called semantic-visual) embedding is a central problem in vision-language research. It typically involves mapping of an image and a text description to a common feature space through a CNN image encoder and a RNN language encoder. In this paper, we propose a new method for learning text-visual embedding using both image titles and click-through data from an image search engine. We also propose a new triplet loss function by modeling positive awareness of the embedding, and introduce a novel mini-batch-based hard negative sampling approach for better data efficiency in the learning process. Experimental results show that our proposed method outperforms existing methods, and is also effective for real-world text-to-visual retrieval.



### All-In-One Underwater Image Enhancement using Domain-Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.13342v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.13342v1)
- **Published**: 2019-05-30 22:42:05+00:00
- **Updated**: 2019-05-30 22:42:05+00:00
- **Authors**: Pritish Uplavikar, Zhenyu Wu, Zhangyang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Raw underwater images are degraded due to wavelength dependent light attenuation and scattering, limiting their applicability in vision systems. Another factor that makes enhancing underwater images particularly challenging is the diversity of the water types in which they are captured. For example, images captured in deep oceanic waters have a different distribution from those captured in shallow coastal waters. Such diversity makes it hard to train a single model to enhance underwater images. In this work, we propose a novel model which nicely handles the diversity of water during the enhancement, by adversarially learning the content features of the images by disentangling the unwanted nuisances corresponding to water types (viewed as different domains). We use the learned domain agnostic features to generate enhanced underwater images. We train our model on a dataset consisting images of 10 Jerlov water types. Experimental results show that the proposed model not only outperforms the previous methods in SSIM and PSNR scores for almost all Jerlov water types but also generalizes well on real-world datasets. The performance of a high-level vision task (object detection) also shows improvement using enhanced images with our model.



