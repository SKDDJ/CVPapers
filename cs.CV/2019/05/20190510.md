# Arxiv Papers in cs.CV on 2019-05-10
### Ship classification from overhead imagery using synthetic data and domain adaptation
- **Arxiv ID**: http://arxiv.org/abs/1905.03894v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03894v1)
- **Published**: 2019-05-10 00:01:52+00:00
- **Updated**: 2019-05-10 00:01:52+00:00
- **Authors**: Chris M. Ward, Josh Harguess, Cameron Hilton
- **Comment**: OCEANS 2018 MTS/IEEE Charleston
- **Journal**: None
- **Summary**: In this paper, we revisit the problem of classifying ships (maritime vessels) detected from overhead imagery. Despite the last decade of research on this very important and pertinent problem, it remains largely unsolved. One of the major issues with the detection and classification of ships and other objects in the maritime domain is the lack of substantial ground truth data needed to train state-of-the-art machine learning algorithms. We address this issue by building a large (200k) synthetic image dataset using the Unity gaming engine and 3D ship models. We demonstrate that with the use of synthetic data, classification performance increases dramatically, particularly when there are very few annotated images used in training.



### Deep Sky Modeling for Single Image Outdoor Lighting Estimation
- **Arxiv ID**: http://arxiv.org/abs/1905.03897v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03897v1)
- **Published**: 2019-05-10 00:12:20+00:00
- **Updated**: 2019-05-10 00:12:20+00:00
- **Authors**: Yannick Hold-Geoffroy, Akshaya Athawale, Jean-Fran√ßois Lalonde
- **Comment**: CVPR'19 paper
- **Journal**: None
- **Summary**: We propose a data-driven learned sky model, which we use for outdoor lighting estimation from a single image. As no large-scale dataset of images and their corresponding ground truth illumination is readily available, we use complementary datasets to train our approach, combining the vast diversity of illumination conditions of SUN360 with the radiometrically calibrated and physically accurate Laval HDR sky database. Our key contribution is to provide a holistic view of both lighting modeling and estimation, solving both problems end-to-end. From a test image, our method can directly estimate an HDR environment map of the lighting without relying on analytical lighting models. We demonstrate the versatility and expressivity of our learned sky model and show that it can be used to recover plausible illumination, leading to visually pleasant virtual object insertions. To further evaluate our method, we capture a dataset of HDR 360{\deg} panoramas and show through extensive validation that we significantly outperform previous state-of-the-art.



### Illumination Normalization via Merging Locally Enhanced Textures for Robust Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1905.03904v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03904v1)
- **Published**: 2019-05-10 01:07:53+00:00
- **Updated**: 2019-05-10 01:07:53+00:00
- **Authors**: Chaobing Zheng, Shiqian Wu, Wangming Xu, Shoulie Xie
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: In order to improve the accuracy of face recognition under varying illumination conditions, a local texture enhanced illumination normalization method based on fusion of differential filtering images (FDFI-LTEIN) is proposed to weaken the influence caused by illumination changes. Firstly, the dynamic range of the face image in dark or shadowed regions is expanded by logarithmic transformation. Then, the global contrast enhanced face image is convolved with difference of Gaussian filters and difference of bilateral filters, and the filtered images are weighted and merged using a coefficient selection rule based on the standard deviation (SD) of image, which can enhance image texture information while filtering out most noise. Finally, the local contrast equalization (LCE) is performed on the fused face image to reduce the influence caused by over or under saturated pixel values in highlight or dark regions. Experimental results on the Extended Yale B face database and CMU PIE face database demonstrate that the proposed method is more robust to illumination changes and achieve higher recognition accuracy when compared with other illumination normalization methods and a deep CNNs based illumination invariant face recognition method



### Building 3D Object Models during Manipulation by Reconstruction-Aware Trajectory Optimization
- **Arxiv ID**: http://arxiv.org/abs/1905.03907v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.03907v1)
- **Published**: 2019-05-10 01:33:29+00:00
- **Updated**: 2019-05-10 01:33:29+00:00
- **Authors**: Kanrun Huang, Tucker Hermans
- **Comment**: None
- **Journal**: None
- **Summary**: Object shape provides important information for robotic manipulation; for instance, selecting an effective grasp depends on both the global and local shape of the object of interest, while reaching into clutter requires accurate surface geometry to avoid unintended contact with the environment. Model-based 3D object manipulation is a widely studied problem; however, obtaining the accurate 3D object models for multiple objects often requires tedious work. In this letter, we exploit Gaussian process implicit surfaces (GPIS) extracted from RGB-D sensor data to grasp an unknown object. We propose a reconstruction-aware trajectory optimization that makes use of the extracted GPIS model plan a motion to improve the ability to estimate the object's 3D geometry, while performing a pick-and-place action. We present a probabilistic approach for a robot to autonomously learn and track the object, while achieve the manipulation task.   We use a sampling-based trajectory generation method to explore the unseen parts of the object using the estimated conditional entropy of the GPIS model. We validate our method with physical robot experiments across eleven different objects of varying shape from the YCB object dataset. Our experiments show that our reconstruction-aware trajectory optimization provides higher-quality 3D object reconstruction when compared with directly solving the manipulation task or using a heuristic to view unseen portions of the object.



### Multi-scale Aggregation R-CNN for 2D Multi-person Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1905.03912v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03912v1)
- **Published**: 2019-05-10 02:17:34+00:00
- **Updated**: 2019-05-10 02:17:34+00:00
- **Authors**: Gyeongsik Moon, Ju Yong Chang, Kyoung Mu Lee
- **Comment**: Published at CVPRW 2019
- **Journal**: None
- **Summary**: Multi-person pose estimation from a 2D image is challenging because it requires not only keypoint localization but also human detection. In state-of-the-art top-down methods, multi-scale information is a crucial factor for the accurate pose estimation because it contains both of local information around the keypoints and global information of the entire person. Although multi-scale information allows these methods to achieve the state-of-the-art performance, the top-down methods still require a huge amount of computation because they need to use an additional human detector to feed the cropped human image to their pose estimation model. To effectively utilize multi-scale information with the smaller computation, we propose a multi-scale aggregation R-CNN (MSA R-CNN). It consists of multi-scale RoIAlign block (MS-RoIAlign) and multi-scale keypoint head network (MS-KpsNet) which are designed to effectively utilize multi-scale information. Also, in contrast to previous top-down methods, the MSA R-CNN performs human detection and keypoint localization in a single model, which results in reduced computation. The proposed model achieved the best performance among single model-based methods and its results are comparable to those of separated model-based methods with a smaller amount of computation on the publicly available 2D multi-person keypoint localization dataset.



### EENA: Efficient Evolution of Neural Architecture
- **Arxiv ID**: http://arxiv.org/abs/1905.07320v3
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.07320v3)
- **Published**: 2019-05-10 02:34:23+00:00
- **Updated**: 2019-08-27 03:32:59+00:00
- **Authors**: Hui Zhu, Zhulin An, Chuanguang Yang, Kaiqiang Xu, Erhu Zhao, Yongjun Xu
- **Comment**: Accepted by ICCV2019 Neural Architects Workshop (ICCVW)
- **Journal**: None
- **Summary**: Latest algorithms for automatic neural architecture search perform remarkable but are basically directionless in search space and computational expensive in training of every intermediate architecture. In this paper, we propose a method for efficient architecture search called EENA (Efficient Evolution of Neural Architecture). Due to the elaborately designed mutation and crossover operations, the evolution process can be guided by the information have already been learned. Therefore, less computational effort will be required while the searching and training time can be reduced significantly. On CIFAR-10 classification, EENA using minimal computational resources (0.65 GPU-days) can design highly effective neural architecture which achieves 2.56% test error with 8.47M parameters. Furthermore, the best architecture discovered is also transferable for CIFAR-100.



### Spatio-temporal Video Re-localization by Warp LSTM
- **Arxiv ID**: http://arxiv.org/abs/1905.03922v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03922v1)
- **Published**: 2019-05-10 03:27:26+00:00
- **Updated**: 2019-05-10 03:27:26+00:00
- **Authors**: Yang Feng, Lin Ma, Wei Liu, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: The need for efficiently finding the video content a user wants is increasing because of the erupting of user-generated videos on the Web. Existing keyword-based or content-based video retrieval methods usually determine what occurs in a video but not when and where. In this paper, we make an answer to the question of when and where by formulating a new task, namely spatio-temporal video re-localization. Specifically, given a query video and a reference video, spatio-temporal video re-localization aims to localize tubelets in the reference video such that the tubelets semantically correspond to the query. To accurately localize the desired tubelets in the reference video, we propose a novel warp LSTM network, which propagates the spatio-temporal information for a long period and thereby captures the corresponding long-term dependencies. Another issue for spatio-temporal video re-localization is the lack of properly labeled video datasets. Therefore, we reorganize the videos in the AVA dataset to form a new dataset for spatio-temporal video re-localization research. Extensive experimental results show that the proposed model achieves superior performances over the designed baselines on the spatio-temporal video re-localization task.



### Memory-Attended Recurrent Network for Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/1905.03966v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03966v1)
- **Published**: 2019-05-10 06:47:57+00:00
- **Updated**: 2019-05-10 06:47:57+00:00
- **Authors**: Wenjie Pei, Jiyuan Zhang, Xiangrong Wang, Lei Ke, Xiaoyong Shen, Yu-Wing Tai
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: Typical techniques for video captioning follow the encoder-decoder framework, which can only focus on one source video being processed. A potential disadvantage of such design is that it cannot capture the multiple visual context information of a word appearing in more than one relevant videos in training data. To tackle this limitation, we propose the Memory-Attended Recurrent Network (MARN) for video captioning, in which a memory structure is designed to explore the full-spectrum correspondence between a word and its various similar visual contexts across videos in training data. Thus, our model is able to achieve a more comprehensive understanding for each word and yield higher captioning quality. Furthermore, the built memory structure enables our method to model the compatibility between adjacent words explicitly instead of asking the model to learn implicitly, as most existing models do. Extensive validation on two real-word datasets demonstrates that our MARN consistently outperforms state-of-the-art methods.



### MobiVSR: A Visual Speech Recognition Solution for Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/1905.03968v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.03968v3)
- **Published**: 2019-05-10 06:58:35+00:00
- **Updated**: 2019-06-05 03:49:26+00:00
- **Authors**: Nilay Shrivastava, Astitwa Saxena, Yaman Kumar, Rajiv Ratn Shah, Debanjan Mahata, Amanda Stent
- **Comment**: None
- **Journal**: None
- **Summary**: Visual speech recognition (VSR) is the task of recognizing spoken language from video input only, without any audio. VSR has many applications as an assistive technology, especially if it could be deployed in mobile devices and embedded systems. The need of intensive computational resources and large memory footprint are two of the major obstacles in developing neural network models for VSR in a resource constrained environment. We propose a novel end-to-end deep neural network architecture for word level VSR called MobiVSR with a design parameter that aids in balancing the model's accuracy and parameter count. We use depthwise-separable 3D convolution for the first time in the domain of VSR and show how it makes our model efficient. MobiVSR achieves an accuracy of 73\% on a challenging Lip Reading in the Wild dataset with 6 times fewer parameters and 20 times lesser memory footprint than the current state of the art. MobiVSR can also be compressed to 6 MB by applying post training quantization.



### A fast online cascaded regression algorithm for face alignment
- **Arxiv ID**: http://arxiv.org/abs/1905.04010v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04010v1)
- **Published**: 2019-05-10 08:37:33+00:00
- **Updated**: 2019-05-10 08:37:33+00:00
- **Authors**: Lin Feng, Caifeng Liu, Shenglan Liu, Huibing Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional face alignment based on machine learning usually tracks the localizations of facial landmarks employing a static model trained offline where all of the training data is available in advance. When new training samples arrive, the static model must be retrained from scratch, which is excessively time-consuming and memory-consuming. In many real-time applications, the training data is obtained one by one or batch by batch. It results in that the static model limits its performance on sequential images with extensive variations. Therefore, the most critical and challenging aspect in this field is dynamically updating the tracker's models to enhance predictive and generalization capabilities continuously. In order to address this question, we develop a fast and accurate online learning algorithm for face alignment. Particularly, we incorporate on-line sequential extreme learning machine into a parallel cascaded regression framework, coined incremental cascade regression(ICR). To the best of our knowledge, this is the first incremental cascaded framework with the non-linear regressor. One main advantage of ICR is that the tracker model can be fast updated in an incremental way without the entire retraining process when a new input is incoming. Experimental results demonstrate that the proposed ICR is more accurate and efficient on still or sequential images compared with the recent state-of-the-art cascade approaches. Furthermore, the incremental learning proposed in this paper can update the trained model in real time.



### Supervized Segmentation with Graph-Structured Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.04014v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.04014v2)
- **Published**: 2019-05-10 08:55:49+00:00
- **Updated**: 2019-06-24 15:03:33+00:00
- **Authors**: Loic Landrieu, Mohamed Boussaha
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1904.02113
- **Journal**: None
- **Summary**: We present a fully-supervized method for learning to segment data structured by an adjacency graph. We introduce the graph-structured contrastive loss, a loss function structured by a ground truth segmentation. It promotes learning vertex embeddings which are homogeneous within desired segments, and have high contrast at their interface. Thus, computing a piecewise-constant approximation of such embeddings produces a graph-partition close to the objective segmentation. This loss is fully backpropagable, which allows us to learn vertex embeddings with deep learning algorithms. We evaluate our methods on a 3D point cloud oversegmentation task, defining a new state-of-the-art by a large margin. These results are based on the published work of Landrieu and Boussaha 2019.



### Exact Adversarial Attack to Image Captioning via Structured Output Learning with Latent Variables
- **Arxiv ID**: http://arxiv.org/abs/1905.04016v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.04016v1)
- **Published**: 2019-05-10 09:00:53+00:00
- **Updated**: 2019-05-10 09:00:53+00:00
- **Authors**: Yan Xu, Baoyuan Wu, Fumin Shen, Yanbo Fan, Yong Zhang, Heng Tao Shen, Wei Liu
- **Comment**: Accepted to CVPR 2019. Yan Xu and Baoyuan Wu are co-first authors
- **Journal**: None
- **Summary**: In this work, we study the robustness of a CNN+RNN based image captioning system being subjected to adversarial noises. We propose to fool an image captioning system to generate some targeted partial captions for an image polluted by adversarial noises, even the targeted captions are totally irrelevant to the image content. A partial caption indicates that the words at some locations in this caption are observed, while words at other locations are not restricted.It is the first work to study exact adversarial attacks of targeted partial captions. Due to the sequential dependencies among words in a caption, we formulate the generation of adversarial noises for targeted partial captions as a structured output learning problem with latent variables. Both the generalized expectation maximization algorithm and structural SVMs with latent variables are then adopted to optimize the problem. The proposed methods generate very successful at-tacks to three popular CNN+RNN based image captioning models. Furthermore, the proposed attack methods are used to understand the inner mechanism of image captioning systems, providing the guidance to further improve automatic image captioning systems towards human captioning.



### Prototype Propagation Networks (PPN) for Weakly-supervised Few-shot Learning on Category Graph
- **Arxiv ID**: http://arxiv.org/abs/1905.04042v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.04042v2)
- **Published**: 2019-05-10 09:57:23+00:00
- **Updated**: 2019-06-02 12:40:13+00:00
- **Authors**: Lu Liu, Tianyi Zhou, Guodong Long, Jing Jiang, Lina Yao, Chengqi Zhang
- **Comment**: Accepted to IJCAI 2019, Code is publicly available at:
  https://github.com/liulu112601/PPN
- **Journal**: None
- **Summary**: A variety of machine learning applications expect to achieve rapid learning from a limited number of labeled data. However, the success of most current models is the result of heavy training on big data. Meta-learning addresses this problem by extracting common knowledge across different tasks that can be quickly adapted to new tasks. However, they do not fully explore weakly-supervised information, which is usually free or cheap to collect. In this paper, we show that weakly-labeled data can significantly improve the performance of meta-learning on few-shot classification. We propose prototype propagation network (PPN) trained on few-shot tasks together with data annotated by coarse-label. Given a category graph of the targeted fine-classes and some weakly-labeled coarse-classes, PPN learns an attention mechanism which propagates the prototype of one class to another on the graph, so that the K-nearest neighbor (KNN) classifier defined on the propagated prototypes results in high accuracy across different few-shot tasks. The training tasks are generated by subgraph sampling, and the training objective is obtained by accumulating the level-wise classification loss on the subgraph. The resulting graph of prototypes can be continually re-used and updated for new tasks and classes. We also introduce two practical test/inference settings which differ according to whether the test task can leverage any weakly-supervised information as in training. On two benchmarks, PPN significantly outperforms most recent few-shot learning methods in different settings, even when they are also allowed to train on weakly-labeled data.



### Analysis of Probabilistic multi-scale fractional order fusion-based de-hazing algorithm
- **Arxiv ID**: http://arxiv.org/abs/1905.04302v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.04302v1)
- **Published**: 2019-05-10 10:33:54+00:00
- **Updated**: 2019-05-10 10:33:54+00:00
- **Authors**: U. A. Nnolim
- **Comment**: 22 pages, 8 figures, journal preprint
- **Journal**: None
- **Summary**: In this report, a de-hazing algorithm based on probability and multi-scale fractional order-based fusion is proposed. The proposed scheme improves on a previously implemented multiscale fraction order-based fusion by augmenting its local contrast and edge sharpening features. It also brightens de-hazed images, while avoiding sky region over-enhancement. The results of the proposed algorithm are analyzed and compared with existing methods from the literature and indicate better performance in most cases.



### Towards Egocentric Person Re-identification and Social Pattern Analysis
- **Arxiv ID**: http://arxiv.org/abs/1905.04073v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04073v1)
- **Published**: 2019-05-10 11:17:47+00:00
- **Updated**: 2019-05-10 11:17:47+00:00
- **Authors**: Estefania Talavera, Alexandre Cola, Nicolai Petkov, Petia Radeva
- **Comment**: None
- **Journal**: None
- **Summary**: Wearable cameras capture a first-person view of the daily activities of the camera wearer, offering a visual diary of the user behaviour. Detection of the appearance of people the camera user interacts with for social interactions analysis is of high interest. Generally speaking, social events, lifestyle and health are highly correlated, but there is a lack of tools to monitor and analyse them. We consider that egocentric vision provides a tool to obtain information and understand users social interactions. We propose a model that enables us to evaluate and visualize social traits obtained by analysing social interactions appearance within egocentric photostreams. Given sets of egocentric images, we detect the appearance of faces within the days of the camera wearer, and rely on clustering algorithms to group their feature descriptors in order to re-identify persons. Recurrence of detected faces within photostreams allows us to shape an idea of the social pattern of behaviour of the user. We validated our model over several weeks recorded by different camera wearers. Our findings indicate that social profiles are potentially useful for social behaviour interpretation.



### Region Attention Networks for Pose and Occlusion Robust Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1905.04075v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04075v2)
- **Published**: 2019-05-10 11:18:13+00:00
- **Updated**: 2019-09-05 02:06:24+00:00
- **Authors**: Kai Wang, Xiaojiang Peng, Jianfei Yang, Debin Meng, Yu Qiao
- **Comment**: The test set and the code of this paper will be available at
  https://github.com/kaiwang960112/Challenge-condition-FER-dataset
- **Journal**: None
- **Summary**: Occlusion and pose variations, which can change facial appearance significantly, are two major obstacles for automatic Facial Expression Recognition (FER). Though automatic FER has made substantial progresses in the past few decades, occlusion-robust and pose-invariant issues of FER have received relatively less attention, especially in real-world scenarios. This paper addresses the real-world pose and occlusion robust FER problem with three-fold contributions. First, to stimulate the research of FER under real-world occlusions and variant poses, we build several in-the-wild facial expression datasets with manual annotations for the community. Second, we propose a novel Region Attention Network (RAN), to adaptively capture the importance of facial regions for occlusion and pose variant FER. The RAN aggregates and embeds varied number of region features produced by a backbone convolutional neural network into a compact fixed-length representation. Last, inspired by the fact that facial expressions are mainly defined by facial action units, we propose a region biased loss to encourage high attention weights for the most important regions. We validate our RAN and region biased loss on both our built test datasets and four popular datasets: FERPlus, AffectNet, RAF-DB, and SFEW. Extensive experiments show that our RAN and region biased loss largely improve the performance of FER with occlusion and variant pose. Our method also achieves state-of-the-art results on FERPlus, AffectNet, RAF-DB, and SFEW. Code and the collected test data will be publicly available.



### Unsupervised routine discovery in egocentric photo-streams
- **Arxiv ID**: http://arxiv.org/abs/1905.04076v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04076v1)
- **Published**: 2019-05-10 11:27:50+00:00
- **Updated**: 2019-05-10 11:27:50+00:00
- **Authors**: Estefania Talavera, Nicolai Petkov, Petia Radeva
- **Comment**: None
- **Journal**: None
- **Summary**: The routine of a person is defined by the occurrence of activities throughout different days, and can directly affect the person's health. In this work, we address the recognition of routine related days. To do so, we rely on egocentric images, which are recorded by a wearable camera and allow to monitor the life of the user from a first-person view perspective. We propose an unsupervised model that identifies routine related days, following an outlier detection approach. We test the proposed framework over a total of 72 days in the form of photo-streams covering around 2 weeks of the life of 5 different camera wearers. Our model achieves an average of 76% Accuracy and 68% Weighted F-Score for all the users. Thus, we show that our framework is able to recognise routine related days and opens the door to the understanding of the behaviour of people.



### Support Relation Analysis for Objects in Multiple View RGB-D Images
- **Arxiv ID**: http://arxiv.org/abs/1905.04084v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04084v1)
- **Published**: 2019-05-10 11:49:49+00:00
- **Updated**: 2019-05-10 11:49:49+00:00
- **Authors**: Peng Zhang, Xiaoyu Ge, Jochen Renz
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding physical relations between objects, especially their support relations, is crucial for robotic manipulation. There has been work on reasoning about support relations and structural stability of simple configurations in RGB-D images. In this paper, we propose a method for extracting more detailed physical knowledge from a set of RGB-D images taken from the same scene but from different views using qualitative reasoning and intuitive physical models. Rather than providing a simple contact relation graph and approximating stability over convex shapes, our method is able to provide a detailed supporting relation analysis based on a volumetric representation. Specifically, true supporting relations between objects (e.g., if an object supports another object by touching it on the side or if the object above contributes to the stability of the object below) are identified. We apply our method to real-world structures captured in warehouse scenarios and show our method works as desired.



### SPLINE-Net: Sparse Photometric Stereo through Lighting Interpolation and Normal Estimation Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.04088v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04088v2)
- **Published**: 2019-05-10 11:51:49+00:00
- **Updated**: 2019-10-09 05:05:33+00:00
- **Authors**: Qian Zheng, Yiming Jia, Boxin Shi, Xudong Jiang, Ling-Yu Duan, Alex C. Kot
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: This paper solves the Sparse Photometric stereo through Lighting Interpolation and Normal Estimation using a generative Network (SPLINE-Net). SPLINE-Net contains a lighting interpolation network to generate dense lighting observations given a sparse set of lights as inputs followed by a normal estimation network to estimate surface normals. Both networks are jointly constrained by the proposed symmetric and asymmetric loss functions to enforce isotropic constrain and perform outlier rejection of global illumination effects. SPLINE-Net is verified to outperform existing methods for photometric stereo of general BRDFs by using only ten images of different lights instead of using nearly one hundred images.



### Towards Unsupervised Familiar Scene Recognition in Egocentric Videos
- **Arxiv ID**: http://arxiv.org/abs/1905.04093v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04093v1)
- **Published**: 2019-05-10 11:58:10+00:00
- **Updated**: 2019-05-10 11:58:10+00:00
- **Authors**: Estefania Talavera, Nicolai Petkov, Petia Radeva
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, there is an upsurge of interest in using lifelogging devices. Such devices generate huge amounts of image data; consequently, the need for automatic methods for analyzing and summarizing these data is drastically increasing. We present a new method for familiar scene recognition in egocentric videos, based on background pattern detection through automatically configurable COSFIRE filters. We present some experiments over egocentric data acquired with the Narrative Clip.



### Domain Adversarial Reinforcement Learning for Partial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1905.04094v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.04094v1)
- **Published**: 2019-05-10 12:02:32+00:00
- **Updated**: 2019-05-10 12:02:32+00:00
- **Authors**: Jin Chen, Xinxiao Wu, Lixin Duan, Shenghua Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Partial domain adaptation aims to transfer knowledge from a label-rich source domain to a label-scarce target domain which relaxes the fully shared label space assumption across different domains. In this more general and practical scenario, a major challenge is how to select source instances in the shared classes across different domains for positive transfer. To address this issue, we propose a Domain Adversarial Reinforcement Learning (DARL) framework to automatically select source instances in the shared classes for circumventing negative transfer as well as to simultaneously learn transferable features between domains by reducing the domain shift. Specifically, in this framework, we employ deep Q-learning to learn policies for an agent to make selection decisions by approximating the action-value function. Moreover, domain adversarial learning is introduced to learn domain-invariant features for the selected source instances by the agent and the target instances, and also to determine rewards for the agent based on how relevant the selected source instances are to the target domain. Experiments on several benchmark datasets demonstrate that the superior performance of our DARL method over existing state of the arts for partial domain adaptation.



### Hierarchical approach to classify food scenes in egocentric photo-streams
- **Arxiv ID**: http://arxiv.org/abs/1905.04097v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04097v1)
- **Published**: 2019-05-10 12:07:28+00:00
- **Updated**: 2019-05-10 12:07:28+00:00
- **Authors**: Estefania Talavera, Maria Leyva-Vallina, Md. Mostafa Kamal Sarker, Domenec Puig, Nicolai Petkov, Petia Radeva
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies have shown that the environment where people eat can affect their nutritional behaviour. In this work, we provide automatic tools for a personalised analysis of a person's health habits by the examination of daily recorded egocentric photo-streams. Specifically, we propose a new automatic approach for the classification of food-related environments, that is able to classify up to 15 such scenes. In this way, people can monitor the context around their food intake in order to get an objective insight into their daily eating routine. We propose a model that classifies food-related scenes organized in a semantic hierarchy. Additionally, we present and make available a new egocentric dataset composed of more than 33000 images recorded by a wearable camera, over which our proposed model has been tested. Our approach obtains an accuracy and F-score of 56\% and 65\%, respectively, clearly outperforming the baseline methods.



### Which Contrast Does Matter? Towards a Deep Understanding of MR Contrast using Collaborative GAN
- **Arxiv ID**: http://arxiv.org/abs/1905.04105v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.04105v1)
- **Published**: 2019-05-10 12:18:19+00:00
- **Updated**: 2019-05-10 12:18:19+00:00
- **Authors**: Dongwook Lee, Won-Jin Moon, Jong Chul Ye
- **Comment**: 32 pages, 6 figures
- **Journal**: None
- **Summary**: Thanks to the recent success of generative adversarial network (GAN) for image synthesis, there are many exciting GAN approaches that successfully synthesize MR image contrast from other images with different contrasts. These approaches are potentially important for image imputation problems, where complete set of data is often difficult to obtain and image synthesis is one of the key solutions for handling the missing data problem. Unfortunately, the lack of the scalability of the existing GAN-based image translation approaches poses a fundamental challenge to understand the nature of the MR contrast imputation problem: which contrast does matter? Here, we present a systematic approach using Collaborative Generative Adversarial Networks (CollaGAN), which enable the learning of the joint image manifold of multiple MR contrasts to investigate which contrasts are essential. Our experimental results showed that the exogenous contrast from contrast agents is not replaceable, but other endogenous contrast such as T1, T2, etc can be synthesized from other contrast. These findings may give important guidance to the acquisition protocol design for MR in real clinical environment.



### Towards Emotion Retrieval in Egocentric PhotoStream
- **Arxiv ID**: http://arxiv.org/abs/1905.04107v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04107v1)
- **Published**: 2019-05-10 12:24:00+00:00
- **Updated**: 2019-05-10 12:24:00+00:00
- **Authors**: Estefania Talavera, Petia Radeva, Nicolai Petkov
- **Comment**: None
- **Journal**: None
- **Summary**: The availability and use of egocentric data are rapidly increasing due to the growing use of wearable cameras. Our aim is to study the effect (positive, neutral or negative) of egocentric images or events on an observer. Given egocentric photostreams capturing the wearer's days, we propose a method that aims to assign sentiment to events extracted from egocentric photostreams. Such moments can be candidates to retrieve according to their possibility of representing a positive experience for the camera's wearer. The proposed approach obtained a classification accuracy of 75% on the test set, with a deviation of 8%. Our model makes a step forward opening the door to sentiment recognition in egocentric photostreams.



### Neural-Guided RANSAC: Learning Where to Sample Model Hypotheses
- **Arxiv ID**: http://arxiv.org/abs/1905.04132v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04132v2)
- **Published**: 2019-05-10 12:50:27+00:00
- **Updated**: 2019-07-31 08:47:41+00:00
- **Authors**: Eric Brachmann, Carsten Rother
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: We present Neural-Guided RANSAC (NG-RANSAC), an extension to the classic RANSAC algorithm from robust optimization. NG-RANSAC uses prior information to improve model hypothesis search, increasing the chance of finding outlier-free minimal sets. Previous works use heuristic side-information like hand-crafted descriptor distance to guide hypothesis search. In contrast, we learn hypothesis search in a principled fashion that lets us optimize an arbitrary task loss during training, leading to large improvements on classic computer vision tasks. We present two further extensions to NG-RANSAC. Firstly, using the inlier count itself as training signal allows us to train neural guidance in a self-supervised fashion. Secondly, we combine neural guidance with differentiable RANSAC to build neural networks which focus on certain parts of the input data and make the output predictions as good as possible. We evaluate NG-RANSAC on a wide array of computer vision tasks, namely estimation of epipolar geometry, horizon line estimation and camera re-localization. We achieve superior or competitive results compared to state-of-the-art robust estimators, including very recent, learned ones.



### DeepICP: An End-to-End Deep Neural Network for 3D Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/1905.04153v2
- **DOI**: 10.1109/ICCV.2019.00010
- **Categories**: **cs.CV**, cs.CG, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1905.04153v2)
- **Published**: 2019-05-10 13:08:28+00:00
- **Updated**: 2019-09-16 08:49:52+00:00
- **Authors**: Weixin Lu, Guowei Wan, Yao Zhou, Xiangyu Fu, Pengfei Yuan, Shiyu Song
- **Comment**: 10 pages, 6 figures, 3 tables, typos corrected, experimental results
  updated, accepted by ICCV 2019
- **Journal**: The IEEE International Conference on Computer Vision (ICCV), 2019,
  pp. 12-21
- **Summary**: We present DeepICP - a novel end-to-end learning-based 3D point cloud registration framework that achieves comparable registration accuracy to prior state-of-the-art geometric methods. Different from other keypoint based methods where a RANSAC procedure is usually needed, we implement the use of various deep neural network structures to establish an end-to-end trainable network. Our keypoint detector is trained through this end-to-end structure and enables the system to avoid the inference of dynamic objects, leverages the help of sufficiently salient features on stationary objects, and as a result, achieves high robustness. Rather than searching the corresponding points among existing points, the key contribution is that we innovatively generate them based on learned matching probabilities among a group of candidates, which can boost the registration accuracy. Our loss function incorporates both the local similarity and the global geometric constraints to ensure all above network designs can converge towards the right direction. We comprehensively validate the effectiveness of our approach using both the KITTI dataset and the Apollo-SouthBay dataset. Results demonstrate that our method achieves comparable or better performance than the state-of-the-art geometry-based methods. Detailed ablation and visualization analysis are included to further illustrate the behavior and insights of our network. The low registration error and high robustness of our method makes it attractive for substantial applications relying on the point cloud registration task.



### Single-Path NAS: Device-Aware Efficient ConvNet Design
- **Arxiv ID**: http://arxiv.org/abs/1905.04159v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.04159v1)
- **Published**: 2019-05-10 13:23:48+00:00
- **Updated**: 2019-05-10 13:23:48+00:00
- **Authors**: Dimitrios Stamoulis, Ruizhou Ding, Di Wang, Dimitrios Lymberopoulos, Bodhi Priyantha, Jie Liu, Diana Marculescu
- **Comment**: ODML-CDNNR 2019 (ICML'19 workshop) oral presentation (extended
  abstract, required non-archival version). Full paper: arXiv:1904.02877
- **Journal**: None
- **Summary**: Can we automatically design a Convolutional Network (ConvNet) with the highest image classification accuracy under the latency constraint of a mobile device? Neural Architecture Search (NAS) for ConvNet design is a challenging problem due to the combinatorially large design space and search time (at least 200 GPU-hours). To alleviate this complexity, we propose Single-Path NAS, a novel differentiable NAS method for designing device-efficient ConvNets in less than 4 hours. 1. Novel NAS formulation: our method introduces a single-path, over-parameterized ConvNet to encode all architectural decisions with shared convolutional kernel parameters. 2. NAS efficiency: Our method decreases the NAS search cost down to 8 epochs (30 TPU-hours), i.e., up to 5,000x faster compared to prior work. 3. On-device image classification: Single-Path NAS achieves 74.96% top-1 accuracy on ImageNet with 79ms inference latency on a Pixel 1 phone, which is state-of-the-art accuracy compared to NAS methods with similar latency (<80ms).



### On the Connection Between Adversarial Robustness and Saliency Map Interpretability
- **Arxiv ID**: http://arxiv.org/abs/1905.04172v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.04172v1)
- **Published**: 2019-05-10 13:45:21+00:00
- **Updated**: 2019-05-10 13:45:21+00:00
- **Authors**: Christian Etmann, Sebastian Lunz, Peter Maass, Carola-Bibiane Sch√∂nlieb
- **Comment**: 12 pages, accepted for publication at the 36th International
  Conference on Machine Learning 2019
- **Journal**: None
- **Summary**: Recent studies on the adversarial vulnerability of neural networks have shown that models trained to be more robust to adversarial attacks exhibit more interpretable saliency maps than their non-robust counterparts. We aim to quantify this behavior by considering the alignment between input image and saliency map. We hypothesize that as the distance to the decision boundary grows,so does the alignment. This connection is strictly true in the case of linear models. We confirm these theoretical findings with experiments based on models trained with a local Lipschitz regularization and identify where the non-linear nature of neural networks weakens the relation.



### AI in the media and creative industries
- **Arxiv ID**: http://arxiv.org/abs/1905.04175v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1905.04175v1)
- **Published**: 2019-05-10 13:56:52+00:00
- **Updated**: 2019-05-10 13:56:52+00:00
- **Authors**: Giuseppe Amato, Malte Behrmann, Fr√©d√©ric Bimbot, Baptiste Caramiaux, Fabrizio Falchi, Ander Garcia, Joost Geurts, Jaume Gibert, Guillaume Gravier, Hadmut Holken, Hartmut Koenitz, Sylvain Lefebvre, Antoine Liutkus, Fabien Lotte, Andrew Perkis, Rafael Redondo, Enrico Turrin, Thierry Vieville, Emmanuel Vincent
- **Comment**: None
- **Journal**: None
- **Summary**: Thanks to the Big Data revolution and increasing computing capacities, Artificial Intelligence (AI) has made an impressive revival over the past few years and is now omnipresent in both research and industry. The creative sectors have always been early adopters of AI technologies and this continues to be the case. As a matter of fact, recent technological developments keep pushing the boundaries of intelligent systems in creative applications: the critically acclaimed movie "Sunspring", released in 2016, was entirely written by AI technology, and the first-ever Music Album, called "Hello World", produced using AI has been released this year. Simultaneously, the exploratory nature of the creative process is raising important technical challenges for AI such as the ability for AI-powered techniques to be accurate under limited data resources, as opposed to the conventional "Big Data" approach, or the ability to process, analyse and match data from multiple modalities (text, sound, images, etc.) at the same time. The purpose of this white paper is to understand future technological advances in AI and their growing impact on creative industries. This paper addresses the following questions: Where does AI operate in creative Industries? What is its operative role? How will AI transform creative industries in the next ten years? This white paper aims to provide a realistic perspective of the scope of AI actions in creative industries, proposes a vision of how this technology could contribute to research and development works in such context, and identifies research and development challenges.



### T-Net: Nested encoder-decoder architecture for the main vessel segmentation in coronary angiography
- **Arxiv ID**: http://arxiv.org/abs/1905.04197v2
- **DOI**: 10.1016/j.neunet.2020.05.002
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.04197v2)
- **Published**: 2019-05-10 14:38:24+00:00
- **Updated**: 2020-05-20 04:47:32+00:00
- **Authors**: Tae Joon Jun, Jihoon Kweon, Young-Hak Kim, Daeyoung Kim
- **Comment**: Neural Networks, Accepted
- **Journal**: None
- **Summary**: In this paper, we proposed T-Net containing a small encoder-decoder inside the encoder-decoder structure (EDiED). T-Net overcomes the limitation that U-Net can only have a single set of the concatenate layer between encoder and decoder block. To be more precise, the U-Net symmetrically forms the concatenate layers, so the low-level feature of the encoder is connected to the latter part of the decoder, and the high-level feature is connected to the beginning of the decoder. T-Net arranges the pooling and up-sampling appropriately during the encoder process, and likewise during the decoding process so that feature-maps of various sizes are obtained in a single block. As a result, all features from the low-level to the high-level extracted from the encoder are delivered from the beginning of the decoder to predict a more accurate mask. We evaluated T-Net for the problem of segmenting three main vessels in coronary angiography images. The experiment consisted of a comparison of U-Net and T-Nets under the same conditions, and an optimized T-Net for the main vessel segmentation. As a result, T-Net recorded a Dice Similarity Coefficient score (DSC) of 0.815, 0.095 higher than that of U-Net, and the optimized T-Net recorded a DSC of 0.890 which was 0.170 higher than that of U-Net. In addition, we visualized the weight activation of the convolutional layer of T-Net and U-Net to show that T-Net actually predicts the mask from earlier decoders. Therefore, we expect that T-Net can be effectively applied to other similar medical image segmentation problems.



### Virtual Mixup Training for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1905.04215v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04215v4)
- **Published**: 2019-05-10 15:24:17+00:00
- **Updated**: 2019-09-06 17:32:55+00:00
- **Authors**: Xudong Mao, Yun Ma, Zhenguo Yang, Yangbin Chen, Qing Li
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of unsupervised domain adaptation which aims to adapt models trained on a labeled source domain to a completely unlabeled target domain. Recently, the cluster assumption has been applied to unsupervised domain adaptation and achieved strong performance. One critical factor in successful training of the cluster assumption is to impose the locally-Lipschitz constraint to the model. Existing methods only impose the locally-Lipschitz constraint around the training points while miss the other areas, such as the points in-between training data. In this paper, we address this issue by encouraging the model to behave linearly in-between training points. We propose a new regularization method called Virtual Mixup Training (VMT), which is able to incorporate the locally-Lipschitz constraint to the areas in-between training data. Unlike the traditional mixup model, our method constructs the combination samples without using the label information, allowing it to apply to unsupervised domain adaptation. The proposed method is generic and can be combined with most existing models such as the recent state-of-the-art model called VADA. Extensive experiments demonstrate that VMT significantly improves the performance of VADA on six domain adaptation benchmark datasets. For the challenging task of adapting MNIST to SVHN, VMT can improve the accuracy of VADA by over 30\%. Code is available at \url{https://github.com/xudonmao/VMT}.



### EdgeSegNet: A Compact Network for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.04222v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1905.04222v1)
- **Published**: 2019-05-10 15:43:23+00:00
- **Updated**: 2019-05-10 15:43:23+00:00
- **Authors**: Zhong Qiu Lin, Brendan Chwyl, Alexander Wong
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: In this study, we introduce EdgeSegNet, a compact deep convolutional neural network for the task of semantic segmentation. A human-machine collaborative design strategy is leveraged to create EdgeSegNet, where principled network design prototyping is coupled with machine-driven design exploration to create networks with customized module-level macroarchitecture and microarchitecture designs tailored for the task. Experimental results showed that EdgeSegNet can achieve semantic segmentation accuracy comparable with much larger and computationally complex networks (>20x} smaller model size than RefineNet) as well as achieving an inference speed of ~38.5 FPS on an NVidia Jetson AGX Xavier. As such, the proposed EdgeSegNet is well-suited for low-power edge scenarios.



### Talking With Your Hands: Scaling Hand Gestures and Recognition With CNNs
- **Arxiv ID**: http://arxiv.org/abs/1905.04225v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.04225v2)
- **Published**: 2019-05-10 15:49:16+00:00
- **Updated**: 2019-08-30 08:28:07+00:00
- **Authors**: Okan K√∂p√ºkl√º, Yao Rong, Gerhard Rigoll
- **Comment**: Accepted to ICCV 2019 workshop - Observing and Understanding Hands in
  Action (HANDS 2019)
- **Journal**: None
- **Summary**: The use of hand gestures provides a natural alternative to cumbersome interface devices for Human-Computer Interaction (HCI) systems. As the technology advances and communication between humans and machines becomes more complex, HCI systems should also be scaled accordingly in order to accommodate the introduced complexities. In this paper, we propose a methodology to scale hand gestures by forming them with predefined gesture-phonemes, and a convolutional neural network (CNN) based framework to recognize hand gestures by learning only their constituents of gesture-phonemes. The total number of possible hand gestures can be increased exponentially by increasing the number of used gesture-phonemes. For this objective, we introduce a new benchmark dataset named Scaled Hand Gestures Dataset (SHGD) with only gesture-phonemes in its training set and 3-tuples gestures in the test set. In our experimental analysis, we achieve to recognize hand gestures containing one and three gesture-phonemes with an accuracy of 98.47% (in 15 classes) and 94.69% (in 810 classes), respectively. Our dataset, code and pretrained models are publicly available.



### Synthetic-Neuroscore: Using A Neuro-AI Interface for Evaluating Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.04243v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1905.04243v2)
- **Published**: 2019-05-10 16:25:07+00:00
- **Updated**: 2020-02-03 00:55:46+00:00
- **Authors**: Zhengwei Wang, Qi She, Alan F. Smeaton, Tomas E. Ward, Graham Healy
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) are increasingly attracting attention in the computer vision, natural language processing, speech synthesis and similar domains. Arguably the most striking results have been in the area of image synthesis. However, evaluating the performance of GANs is still an open and challenging problem. Existing evaluation metrics primarily measure the dissimilarity between real and generated images using automated statistical methods. They often require large sample sizes for evaluation and do not directly reflect human perception of image quality. In this work, we describe an evaluation metric we call Neuroscore, for evaluating the performance of GANs, that more directly reflects psychoperceptual image quality through the utilization of brain signals. Our results show that Neuroscore has superior performance to the current evaluation metrics in that: (1) It is more consistent with human judgment; (2) The evaluation process needs much smaller numbers of samples; and (3) It is able to rank the quality of images on a per GAN basis. A convolutional neural network (CNN) based neuro-AI interface is proposed to predict Neuroscore from GAN-generated images directly without the need for neural responses. Importantly, we show that including neural responses during the training phase of the network can significantly improve the prediction capability of the proposed model. Materials related to this work are provided at https://github.com/villawang/Neuro-AI-Interface.



### Breast Tumor Classification and Segmentation using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.04247v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.04247v1)
- **Published**: 2019-05-10 16:33:23+00:00
- **Updated**: 2019-05-10 16:33:23+00:00
- **Authors**: Parvin Yousefikamal
- **Comment**: 12 Pages
- **Journal**: None
- **Summary**: Breast cancer is considered as the most fatal type of cancer among women worldwide and it is crucially important to be diagnosed at its early stages. In the current study, we aim to represent a fast and efficient framework which consists of two main parts:1- image classification, and 2- tumor region segmentation. At the initial stage, the images are classified into the two categories of normal and abnormal. Since the Deep Neural Networks have performed successfully in machine vision task, we would employ the convolutional neural networks for the classification of images. In the second stage, the suggested framework is to diagnose and segment the tumor in the mammography images. First, the mammography images are pre-processed by removing noise and artifacts, and then, segment the image using the level-set algorithm based on the spatial fuzzy c-means clustering. The proper initialization and optimal configuration have strong effects on the performance of the level-set segmentation. Thus, in our suggested framework, we have improved the level-set algorithm by utilizing the spatial fuzzy c-means clustering which ultimately results in a more precise segmentation. In order to evaluate the proposed approach, we conducted experiments using the Mammographic Image Analysis (MIAS) dataset. The tests have shown that the convolutional neural networks could achieve high accuracy in classification of images. Moreover, the improved level-set segmentation method, along with the fuzzy c-means clustering, could perfectly do the segmentation on the tumor area. The suggested method has classified the images with the accuracy of 78% and the AUC of 69%, which, as compared to the previous methods, is 2% more accurate and 6% better AUC; and has been able to extract the tumor area in a more precise way.



### Exploiting temporal context for 3D human pose estimation in the wild
- **Arxiv ID**: http://arxiv.org/abs/1905.04266v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04266v1)
- **Published**: 2019-05-10 17:12:27+00:00
- **Updated**: 2019-05-10 17:12:27+00:00
- **Authors**: Anurag Arnab, Carl Doersch, Andrew Zisserman
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: We present a bundle-adjustment-based algorithm for recovering accurate 3D human pose and meshes from monocular videos. Unlike previous algorithms which operate on single frames, we show that reconstructing a person over an entire sequence gives extra constraints that can resolve ambiguities. This is because videos often give multiple views of a person, yet the overall body shape does not change and 3D positions vary slowly. Our method improves not only on standard mocap-based datasets like Human 3.6M -- where we show quantitative improvements -- but also on challenging in-the-wild datasets such as Kinetics. Building upon our algorithm, we present a new dataset of more than 3 million frames of YouTube videos from Kinetics with automatically generated 3D poses and meshes. We show that retraining a single-frame 3D pose estimator on this data improves accuracy on both real-world and mocap data by evaluating on the 3DPW and HumanEVA datasets.



### Interpreting and Evaluating Neural Network Robustness
- **Arxiv ID**: http://arxiv.org/abs/1905.04270v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.04270v1)
- **Published**: 2019-05-10 17:21:15+00:00
- **Updated**: 2019-05-10 17:21:15+00:00
- **Authors**: Fuxun Yu, Zhuwei Qin, Chenchen Liu, Liang Zhao, Yanzhi Wang, Xiang Chen
- **Comment**: Accepted in IJCAI'19
- **Journal**: None
- **Summary**: Recently, adversarial deception becomes one of the most considerable threats to deep neural networks. However, compared to extensive research in new designs of various adversarial attacks and defenses, the neural networks' intrinsic robustness property is still lack of thorough investigation. This work aims to qualitatively interpret the adversarial attack and defense mechanism through loss visualization, and establish a quantitative metric to evaluate the neural network model's intrinsic robustness. The proposed robustness metric identifies the upper bound of a model's prediction divergence in the given domain and thus indicates whether the model can maintain a stable prediction. With extensive experiments, our metric demonstrates several advantages over conventional adversarial testing accuracy based robustness estimation: (1) it provides a uniformed evaluation to models with different structures and parameter scales; (2) it over-performs conventional accuracy based robustness estimation and provides a more reliable evaluation that is invariant to different test settings; (3) it can be fast generated without considerable testing cost.



### FastDraw: Addressing the Long Tail of Lane Detection by Adapting a Sequential Prediction Network
- **Arxiv ID**: http://arxiv.org/abs/1905.04354v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04354v2)
- **Published**: 2019-05-10 19:39:32+00:00
- **Updated**: 2019-05-14 12:59:26+00:00
- **Authors**: Jonah Philion
- **Comment**: CVPR 2019 poster
- **Journal**: None
- **Summary**: The search for predictive models that generalize to the long tail of sensor inputs is the central difficulty when developing data-driven models for autonomous vehicles. In this paper, we use lane detection to study modeling and training techniques that yield better performance on real world test drives. On the modeling side, we introduce a novel fully convolutional model of lane detection that learns to decode lane structures instead of delegating structure inference to post-processing. In contrast to previous works, our convolutional decoder is able to represent an arbitrary number of lanes per image, preserves the polyline representation of lanes without reducing lanes to polynomials, and draws lanes iteratively without requiring the computational and temporal complexity of recurrent neural networks. Because our model includes an estimate of the joint distribution of neighboring pixels belonging to the same lane, our formulation includes a natural and computationally cheap definition of uncertainty. On the training side, we demonstrate a simple yet effective approach to adapt the model to new environments using unsupervised style transfer. By training FastDraw to make predictions of lane structure that are invariant to low-level stylistic differences between images, we achieve strong performance at test time in weather and lighting conditions that deviate substantially from those of the annotated datasets that are publicly available. We quantitatively evaluate our approach on the CVPR 2017 Tusimple lane marking challenge, difficult CULane datasets, and a small labeled dataset of our own and achieve competitive accuracy while running at 90 FPS.



### Digital Passport: A Novel Technological Strategy for Intellectual Property Protection of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.04368v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.04368v1)
- **Published**: 2019-05-10 20:13:38+00:00
- **Updated**: 2019-05-10 20:13:38+00:00
- **Authors**: Lixin Fan, KamWoh Ng, Chee Seng Chan
- **Comment**: This paper proposes a new timely IPR solution that embed digital
  passports into CNN models to prevent the unauthorized network usage (i.e.
  infringement) by paralyzing the networks while maintaining its functionality
  for verified users
- **Journal**: None
- **Summary**: In order to prevent deep neural networks from being infringed by unauthorized parties, we propose a generic solution which embeds a designated digital passport into a network, and subsequently, either paralyzes the network functionalities for unauthorized usages or maintain its functionalities in the presence of a verified passport. Such a desired network behavior is successfully demonstrated in a number of implementation schemes, which provide reliable, preventive and timely protections against tens of thousands of fake-passport deceptions. Extensive experiments also show that the deep neural network performance under unauthorized usages deteriorate significantly (e.g. with 33% to 82% reductions of CIFAR10 classification accuracies), while networks endorsed with valid passports remain intact.



### Efficient video indexing for monitoring disease activity and progression in the upper gastrointestinal tract
- **Arxiv ID**: http://arxiv.org/abs/1905.04384v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.04384v1)
- **Published**: 2019-05-10 21:31:11+00:00
- **Updated**: 2019-05-10 21:31:11+00:00
- **Authors**: Sharib Ali, Jens Rittscher
- **Comment**: Accepted at IEEE International Symposium on Biomedical Imaging
  (ISBI), 2019
- **Journal**: None
- **Summary**: Endoscopy is a routine imaging technique used for both diagnosis and minimally invasive surgical treatment. While the endoscopy video contains a wealth of information, tools to capture this information for the purpose of clinical reporting are rather poor. In date, endoscopists do not have any access to tools that enable them to browse the video data in an efficient and user friendly manner. Fast and reliable video retrieval methods could for example, allow them to review data from previous exams and therefore improve their ability to monitor disease progression. Deep learning provides new avenues of compressing and indexing video in an extremely efficient manner. In this study, we propose to use an autoencoder for efficient video compression and fast retrieval of video images. To boost the accuracy of video image retrieval and to address data variability like multi-modality and view-point changes, we propose the integration of a Siamese network. We demonstrate that our approach is competitive in retrieving images from 3 large scale videos of 3 different patients obtained against the query samples of their previous diagnosis. Quantitative validation shows that the combined approach yield an overall improvement of 5% and 8% over classical and variational autoencoders, respectively.



### Ink removal from histopathology whole slide images by combining classification, detection and image generation models
- **Arxiv ID**: http://arxiv.org/abs/1905.04385v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.04385v1)
- **Published**: 2019-05-10 21:33:12+00:00
- **Updated**: 2019-05-10 21:33:12+00:00
- **Authors**: Sharib Ali, Nasullah Khalid Alham, Clare Verrill, Jens Rittscher
- **Comment**: Accepted paper at IEEE International Symposium on Biomedical Imaging
  (ISBI) 2019, Venice, Italy
- **Journal**: None
- **Summary**: Histopathology slides are routinely marked by pathologists using permanent ink markers that should not be removed as they form part of the medical record. Often tumour regions are marked up for the purpose of highlighting features or other downstream processing such an gene sequencing. Once digitised there is no established method for removing this information from the whole slide images limiting its usability in research and study. Removal of marker ink from these high-resolution whole slide images is non-trivial and complex problem as they contaminate different regions and in an inconsistent manner. We propose an efficient pipeline using convolution neural networks that results in ink-free images without compromising information and image resolution. Our pipeline includes a sequential classical convolution neural network for accurate classification of contaminated image tiles, a fast region detector and a domain adaptive cycle consistent adversarial generative model for restoration of foreground pixels. Both quantitative and qualitative results on four different whole slide images show that our approach yields visually coherent ink-free whole slide images.



### Large-Scale Spectrum Occupancy Learning via Tensor Decomposition and LSTM Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.04392v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1905.04392v1)
- **Published**: 2019-05-10 22:22:04+00:00
- **Updated**: 2019-05-10 22:22:04+00:00
- **Authors**: Mohsen Joneidi, Ismail Alkhouri, Nazanin Rahnavard
- **Comment**: Submitted to the 2019 IEEE Global Communications Conference
  (GLOBECOM)
- **Journal**: None
- **Summary**: A new paradigm for large-scale spectrum occupancy learning based on long short-term memory (LSTM) recurrent neural networks is proposed. Studies have shown that spectrum usage is a highly correlated time series. Moreover, there is a correlation for occupancy of spectrum between different frequency channels. Therefore, revealing all these correlations using learning and prediction of one-dimensional time series is not a trivial task. In this paper, we introduce a new framework for representing the spectrum measurements in a tensor format. Next, a time-series prediction method based on CANDECOMP/PARFAC (CP) tensor decomposition and LSTM recurrent neural networks is proposed. The proposed method is computationally efficient and is able to capture different types of correlation within the measured spectrum. Moreover, it is robust against noise and missing entries of sensed spectrum. The superiority of the proposed method is evaluated over a large-scale synthetic dataset in terms of prediction accuracy and computational efficiency.



### Few-Shot Learning with Embedded Class Models and Shot-Free Meta Training
- **Arxiv ID**: http://arxiv.org/abs/1905.04398v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.04398v2)
- **Published**: 2019-05-10 23:21:31+00:00
- **Updated**: 2020-04-21 18:15:58+00:00
- **Authors**: Avinash Ravichandran, Rahul Bhotika, Stefano Soatto
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: We propose a method for learning embeddings for few-shot learning that is suitable for use with any number of ways and any number of shots (shot-free). Rather than fixing the class prototypes to be the Euclidean average of sample embeddings, we allow them to live in a higher-dimensional space (embedded class models) and learn the prototypes along with the model parameters. The class representation function is defined implicitly, which allows us to deal with a variable number of shots per each class with a simple constant-size architecture. The class embedding encompasses metric learning, that facilitates adding new classes without crowding the class representation space. Despite being general and not tuned to the benchmark, our approach achieves state-of-the-art performance on the standard few-shot benchmark datasets.



### Language-Conditioned Graph Networks for Relational Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1905.04405v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.04405v2)
- **Published**: 2019-05-10 23:47:05+00:00
- **Updated**: 2019-08-16 18:55:14+00:00
- **Authors**: Ronghang Hu, Anna Rohrbach, Trevor Darrell, Kate Saenko
- **Comment**: None
- **Journal**: None
- **Summary**: Solving grounded language tasks often requires reasoning about relationships between objects in the context of a given task. For example, to answer the question "What color is the mug on the plate?" we must check the color of the specific mug that satisfies the "on" relationship with respect to the plate. Recent work has proposed various methods capable of complex relational reasoning. However, most of their power is in the inference structure, while the scene is represented with simple local appearance features. In this paper, we take an alternate approach and build contextualized representations for objects in a visual scene to support relational reasoning. We propose a general framework of Language-Conditioned Graph Networks (LCGN), where each node represents an object, and is described by a context-aware representation from related objects through iterative message passing conditioned on the textual input. E.g., conditioning on the "on" relationship to the plate, the object "mug" gathers messages from the object "plate" to update its representation to "mug on the plate", which can be easily consumed by a simple classifier for answer prediction. We experimentally show that our LCGN approach effectively supports relational reasoning and improves performance across several tasks and datasets. Our code is available at http://ronghanghu.com/lcgn.



