# Arxiv Papers in cs.CV on 2019-05-03
### Anti-Confusing: Region-Aware Network for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1905.00996v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00996v2)
- **Published**: 2019-05-03 00:53:03+00:00
- **Updated**: 2019-05-27 03:52:51+00:00
- **Authors**: Xuan Cao, Yanhao Ge, Ying Tai, Wei Zhang, Jian Li, Chengjie Wang, Jilin Li, Feiyue Huang
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a novel framework named Region-Aware Network (RANet), which learns the ability of anti-confusing in case of heavy occlusion, nearby person and symmetric appearance, for human pose estimation. Specifically, the proposed method addresses three key aspects, i.e., data augmentation, feature learning and prediction fusion, respectively. First, we propose Parsing-based Data Augmentation (PDA) to generate abundant data that synthesizes confusing textures. Second, we not only propose a Feature Pyramid Stem (FPS) to learn stronger low-level features in lower stage; but also incorporate an Effective Region Extraction (ERE) module to excavate better target-specific features. Third, we introduce Cascade Voting Fusion (CVF) to explicitly exclude the inferior predictions and fuse the rest effective predictions for the final pose estimation. Extensive experimental results on two popular benchmarks, i.e. MPII and LSP, demonstrate the effectiveness of our method against the state-of-the-art competitors. Especially on easily-confusable joints, our method makes significant improvement.



### Blind Deconvolution Method using Omnidirectional Gabor Filter-based Edge Information
- **Arxiv ID**: http://arxiv.org/abs/1905.01003v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01003v1)
- **Published**: 2019-05-03 01:51:47+00:00
- **Updated**: 2019-05-03 01:51:47+00:00
- **Authors**: Trung Dung Do, Xuenan Cui, Thi Hai Binh Nguyen, Hakil Kim, Van Huan Nguyen
- **Comment**: 6 pages, 7 figures, 3 tables, conference paper
- **Journal**: None
- **Summary**: In the previous blind deconvolution methods, de-blurred images can be obtained by using the edge or pixel information. However, the existing edge-based methods did not take advantage of edge information in ommi-directions, but only used horizontal and vertical edges when recovering the de-blurred images. This limitation lowers the quality of the recovered images. This paper proposes a method which utilizes edges in different directions to recover the true sharp image. We also provide a statistical table score to show how many directions are enough to recover a high quality true sharp image. In order to grade the quality of the deblurring image, we introduce a measurement, namely Haar defocus score that takes advantage of the Haar-Wavelet transform. The experimental results prove that the proposed method obtains a high quality deblurred image with respect to both the Haar defocus score and the Peak Signal to Noise Ratio.



### Real-time and robust multiple-view gender classification using gait features in video surveillance
- **Arxiv ID**: http://arxiv.org/abs/1905.01013v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01013v1)
- **Published**: 2019-05-03 02:50:41+00:00
- **Updated**: 2019-05-03 02:50:41+00:00
- **Authors**: Trung Dung Do, Hakil Kim, Van Huan Nguyen
- **Comment**: 14 pages, 8 figures, 8 tables, journal paper
- **Journal**: None
- **Summary**: It is common to view people in real applications walking in arbitrary directions, holding items, or wearing heavy coats. These factors are challenges in gait-based application methods because they significantly change a person's appearance. This paper proposes a novel method for classifying human gender in real time using gait information. The use of an average gait image (AGI), rather than a gait energy image (GEI), allows this method to be computationally efficient and robust against view changes. A viewpoint (VP) model is created for automatically determining the viewing angle during the testing phase. A distance signal (DS) model is constructed to remove any areas with an attachment (carried items, worn coats) from a silhouette to reduce the interference in the resulting classification. Finally, the human gender is classified using multiple view-dependent classifiers trained using a support vector machine. Experiment results confirm that the proposed method achieves a high accuracy of 98.8% on the CASIA Dataset B and outperforms the recent state-of-the-art methods.



### Learned Quality Enhancement via Multi-Frame Priors for HEVC Compliant Low-Delay Applications
- **Arxiv ID**: http://arxiv.org/abs/1905.01025v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.01025v1)
- **Published**: 2019-05-03 03:05:09+00:00
- **Updated**: 2019-05-03 03:05:09+00:00
- **Authors**: Ming Lu, Ming Cheng, Yiling Xu, Shiliang Pu, Qiu Shen, Zhan Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Networked video applications, e.g., video conferencing, often suffer from poor visual quality due to unexpected network fluctuation and limited bandwidth. In this paper, we have developed a Quality Enhancement Network (QENet) to reduce the video compression artifacts, leveraging the spatial and temporal priors generated by respective multi-scale convolutions spatially and warped temporal predictions in a recurrent fashion temporally. We have integrated this QENet as a standard-alone post-processing subsystem to the High Efficiency Video Coding (HEVC) compliant decoder. Experimental results show that our QENet demonstrates the state-of-the-art performance against default in-loop filters in HEVC and other deep learning based methods with noticeable objective gains in Peak-Signal-to-Noise Ratio (PSNR) and subjective gains visually.



### Bilinear discriminant feature line analysis for image feature extraction
- **Arxiv ID**: http://arxiv.org/abs/1905.03710v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.03710v1)
- **Published**: 2019-05-03 04:54:43+00:00
- **Updated**: 2019-05-03 04:54:43+00:00
- **Authors**: Lijun Yan, Jun-Bao Li, Xiaorui Zhu, Jeng-Shyang Pan, Linlin Tang
- **Comment**: None
- **Journal**: None
- **Summary**: A novel bilinear discriminant feature line analysis (BDFLA) is proposed for image feature extraction. The nearest feature line (NFL) is a powerful classifier. Some NFL-based subspace algorithms were introduced recently. In most of the classical NFL-based subspace learning approaches, the input samples are vectors. For image classification tasks, the image samples should be transformed to vectors first. This process induces a high computational complexity and may also lead to loss of the geometric feature of samples. The proposed BDFLA is a matrix-based algorithm. It aims to minimise the within-class scatter and maximise the between-class scatter based on a two-dimensional (2D) NFL. Experimental results on two-image databases confirm the effectiveness.



### PFA-ScanNet: Pyramidal Feature Aggregation with Synergistic Learning for Breast Cancer Metastasis Analysis
- **Arxiv ID**: http://arxiv.org/abs/1905.01040v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01040v2)
- **Published**: 2019-05-03 06:31:38+00:00
- **Updated**: 2019-07-13 02:45:07+00:00
- **Authors**: Zixu Zhao, Huangjing Lin, Hao Chen, Pheng-Ann Heng
- **Comment**: Accepted by MICCAI 2019
- **Journal**: None
- **Summary**: Automatic detection of cancer metastasis from whole slide images (WSIs) is a crucial step for following patient staging and prognosis. Recent convolutional neural network based approaches are struggling with the trade-off between accuracy and computational efficiency due to the difficulty in processing large-scale gigapixel WSIs. To meet this challenge, we propose a novel Pyramidal Feature Aggregation ScanNet (PFA-ScanNet) for robust and fast analysis of breast cancer metastasis. Our method mainly benefits from the aggregation of extracted local-to-global features with diverse receptive fields, as well as the proposed synergistic learning for training the main detector and extra decoder with semantic guidance. Furthermore, a high-efficiency inference mechanism is designed with dense pooling layers, which allows dense and fast scanning for gigapixel WSI analysis. As a result, the proposed PFA-ScanNet achieved the state-of-the-art FROC of 90.2% on the Camelyon16 dataset, as well as competitive kappa score of 0.905 on the Camelyon17 leaderboard. In addition, our method shows leading speed advantage over other methods, about 7.2 min per WSI with a single GPU, making automatic analysis of breast cancer metastasis more applicable in the clinical usage.



### Lifting 2d Human Pose to 3d : A Weakly Supervised Approach
- **Arxiv ID**: http://arxiv.org/abs/1905.01047v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.01047v1)
- **Published**: 2019-05-03 06:51:11+00:00
- **Updated**: 2019-05-03 06:51:11+00:00
- **Authors**: Sandika Biswas, Sanjana Sinha, Kavya Gupta, Brojeshwar Bhowmick
- **Comment**: Accepted in IJCNN 2019
- **Journal**: None
- **Summary**: Estimating 3d human pose from monocular images is a challenging problem due to the variety and complexity of human poses and the inherent ambiguity in recovering depth from the single view. Recent deep learning based methods show promising results by using supervised learning on 3d pose annotated datasets. However, the lack of large-scale 3d annotated training data captured under in-the-wild settings makes the 3d pose estimation difficult for in-the-wild poses. Few approaches have utilized training images from both 3d and 2d pose datasets in a weakly-supervised manner for learning 3d poses in unconstrained settings. In this paper, we propose a method which can effectively predict 3d human pose from 2d pose using a deep neural network trained in a weakly-supervised manner on a combination of ground-truth 3d pose and ground-truth 2d pose. Our method uses re-projection error minimization as a constraint to predict the 3d locations of body joints, and this is crucial for training on data where the 3d ground-truth is not present. Since minimizing re-projection error alone may not guarantee an accurate 3d pose, we also use additional geometric constraints on skeleton pose to regularize the pose in 3d. We demonstrate the superior generalization ability of our method by cross-dataset validation on a challenging 3d benchmark dataset MPI-INF-3DHP containing in the wild 3d poses.



### Semantic Segmentation of Video Sequences with Convolutional LSTMs
- **Arxiv ID**: http://arxiv.org/abs/1905.01058v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.01058v1)
- **Published**: 2019-05-03 07:52:32+00:00
- **Updated**: 2019-05-03 07:52:32+00:00
- **Authors**: Andreas Pfeuffer, Karina Schulz, Klaus Dietmayer
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: IEEE Intelligent Vehicles Symposium 2019 (IV'19)
- **Summary**: Most of the semantic segmentation approaches have been developed for single image segmentation, and hence, video sequences are currently segmented by processing each frame of the video sequence separately. The disadvantage of this is that temporal image information is not considered, which improves the performance of the segmentation approach. One possibility to include temporal information is to use recurrent neural networks. However, there are only a few approaches using recurrent networks for video segmentation so far. These approaches extend the encoder-decoder network architecture of well-known segmentation approaches and place convolutional LSTM layers between encoder and decoder. However, in this paper it is shown that this position is not optimal, and that other positions in the network exhibit better performance. Nowadays, state-of-the-art segmentation approaches rarely use the classical encoder-decoder structure, but use multi-branch architectures. These architectures are more complex, and hence, it is more difficult to place the recurrent units at a proper position. In this work, the multi-branch architectures are extended by convolutional LSTM layers at different positions and evaluated on two different datasets in order to find the best one. It turned out that the proposed approach outperforms the pure CNN-based approach for up to 1.6 percent.



### Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask
- **Arxiv ID**: http://arxiv.org/abs/1905.01067v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.01067v4)
- **Published**: 2019-05-03 08:21:07+00:00
- **Updated**: 2020-03-03 05:40:51+00:00
- **Authors**: Hattie Zhou, Janice Lan, Rosanne Liu, Jason Yosinski
- **Comment**: NeurIPS 2019 camera ready version
- **Journal**: None
- **Summary**: The recent "Lottery Ticket Hypothesis" paper by Frankle & Carbin showed that a simple approach to creating sparse networks (keeping the large weights) results in models that are trainable from scratch, but only when starting from the same initial weights. The performance of these networks often exceeds the performance of the non-sparse base model, but for reasons that were not well understood. In this paper we study the three critical components of the Lottery Ticket (LT) algorithm, showing that each may be varied significantly without impacting the overall results. Ablating these factors leads to new insights for why LT networks perform as well as they do. We show why setting weights to zero is important, how signs are all you need to make the reinitialized network train, and why masking behaves like training. Finally, we discover the existence of Supermasks, masks that can be applied to an untrained, randomly initialized network to produce a model with performance far better than chance (86% on MNIST, 41% on CIFAR-10).



### Known-class Aware Self-ensemble for Open Set Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1905.01068v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01068v1)
- **Published**: 2019-05-03 08:25:03+00:00
- **Updated**: 2019-05-03 08:25:03+00:00
- **Authors**: Qing Lian, Wen Li, Lin Chen, Lixin Duan
- **Comment**: None
- **Journal**: None
- **Summary**: Existing domain adaptation methods generally assume different domains have the identical label space, which is quite restrict for real-world applications. In this paper, we focus on a more realistic and challenging case of open set domain adaptation. Particularly, in open set domain adaptation, we allow the classes from the source and target domains to be partially overlapped. In this case, the assumption of conventional distribution alignment does not hold anymore, due to the different label spaces in two domains. To tackle this challenge, we propose a new approach coined as Known-class Aware Self-Ensemble (KASE), which is built upon the recently developed self-ensemble model. In KASE, we first introduce a Known-class Aware Recognition (KAR) module to identify the known and unknown classes from the target domain, which is achieved by encouraging a low cross-entropy for known classes and a high entropy based on the source data from the unknown class. Then, we develop a Known-class Aware Adaptation (KAA) module to better adapt from the source domain to the target by reweighing the adaptation loss based on the likeliness to belong to known classes of unlabeled target samples as predicted by KAR. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our approach.



### Temporal Deformable Convolutional Encoder-Decoder Networks for Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/1905.01077v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01077v1)
- **Published**: 2019-05-03 08:59:10+00:00
- **Updated**: 2019-05-03 08:59:10+00:00
- **Authors**: Jingwen Chen, Yingwei Pan, Yehao Li, Ting Yao, Hongyang Chao, Tao Mei
- **Comment**: AAAI 2019
- **Journal**: None
- **Summary**: It is well believed that video captioning is a fundamental but challenging task in both computer vision and artificial intelligence fields. The prevalent approach is to map an input video to a variable-length output sentence in a sequence to sequence manner via Recurrent Neural Network (RNN). Nevertheless, the training of RNN still suffers to some degree from vanishing/exploding gradient problem, making the optimization difficult. Moreover, the inherently recurrent dependency in RNN prevents parallelization within a sequence during training and therefore limits the computations. In this paper, we present a novel design --- Temporal Deformable Convolutional Encoder-Decoder Networks (dubbed as TDConvED) that fully employ convolutions in both encoder and decoder networks for video captioning. Technically, we exploit convolutional block structures that compute intermediate states of a fixed number of inputs and stack several blocks to capture long-term relationships. The structure in encoder is further equipped with temporal deformable convolution to enable free-form deformation of temporal sampling. Our model also capitalizes on temporal attention mechanism for sentence generation. Extensive experiments are conducted on both MSVD and MSR-VTT video captioning datasets, and superior results are reported when comparing to conventional RNN-based encoder-decoder techniques. More remarkably, TDConvED increases CIDEr-D performance from 58.8% to 67.2% on MSVD.



### Machine Vision in the Context of Robotics: A Systematic Literature Review
- **Arxiv ID**: http://arxiv.org/abs/1905.03708v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.03708v1)
- **Published**: 2019-05-03 09:00:07+00:00
- **Updated**: 2019-05-03 09:00:07+00:00
- **Authors**: Javad Ghofrani, Robert Kirschne, Daniel Rossburg, Dirk Reichelt, Tom Dimter
- **Comment**: 10 pages 5 figures, systematic literature study
- **Journal**: None
- **Summary**: Machine vision is critical to robotics due to a wide range of applications which rely on input from visual sensors such as autonomous mobile robots and smart production systems. To create the smart homes and systems of tomorrow, an overview about current challenges in the research field would be of use to identify further possible directions, created in a systematic and reproducible manner. In this work a systematic literature review was conducted covering research from the last 10 years. We screened 172 papers from four databases and selected 52 relevant papers. While robustness and computation time were improved greatly, occlusion and lighting variance are still the biggest problems faced. From the number of recent publications, we conclude that the observed field is of relevance and interest to the research community. Further challenges arise in many areas of the field.



### Generating Classification Weights with GNN Denoising Autoencoders for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.01102v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.01102v1)
- **Published**: 2019-05-03 10:11:54+00:00
- **Updated**: 2019-05-03 10:11:54+00:00
- **Authors**: Spyros Gidaris, Nikos Komodakis
- **Comment**: Oral presentation at CVPR 2019. The code and models of our paper will
  be published on: https://github.com/gidariss/wDAE_GNN_FewShot
- **Journal**: None
- **Summary**: Given an initial recognition model already trained on a set of base classes, the goal of this work is to develop a meta-model for few-shot learning. The meta-model, given as input some novel classes with few training examples per class, must properly adapt the existing recognition model into a new model that can correctly classify in a unified way both the novel and the base classes. To accomplish this goal it must learn to output the appropriate classification weight vectors for those two types of classes. To build our meta-model we make use of two main innovations: we propose the use of a Denoising Autoencoder network (DAE) that (during training) takes as input a set of classification weights corrupted with Gaussian noise and learns to reconstruct the target-discriminative classification weights. In this case, the injected noise on the classification weights serves the role of regularizing the weight generating meta-model. Furthermore, in order to capture the co-dependencies between different classes in a given task instance of our meta-model, we propose to implement the DAE model as a Graph Neural Network (GNN). In order to verify the efficacy of our approach, we extensively evaluate it on ImageNet based few-shot benchmarks and we report strong results that surpass prior approaches. The code and models of our paper will be published on: https://github.com/gidariss/wDAE_GNN_FewShot



### DisplaceNet: Recognising Displaced People from Images by Exploiting Dominance Level
- **Arxiv ID**: http://arxiv.org/abs/1905.02025v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02025v1)
- **Published**: 2019-05-03 11:07:27+00:00
- **Updated**: 2019-05-03 11:07:27+00:00
- **Authors**: Grigorios Kalliatakis, Shoaib Ehsan, Maria Fasli, Klaus McDonald-Maier
- **Comment**: To be published in CVPR Workshop on Computer Vision for Global
  Challenges (CV4GC). arXiv admin note: substantial text overlap with
  arXiv:1902.03817
- **Journal**: None
- **Summary**: Every year millions of men, women and children are forced to leave their homes and seek refuge from wars, human rights violations, persecution, and natural disasters. The number of forcibly displaced people came at a record rate of 44,400 every day throughout 2017, raising the cumulative total to 68.5 million at the years end, overtaken the total population of the United Kingdom. Up to 85% of the forcibly displaced find refuge in low- and middle-income countries, calling for increased humanitarian assistance worldwide. To reduce the amount of manual labour required for human-rights-related image analysis, we introduce DisplaceNet, a novel model which infers potential displaced people from images by integrating the control level of the situation and conventional convolutional neural network (CNN) classifier into one framework for image classification. Experimental results show that DisplaceNet achieves up to 4% coverage-the proportion of a data set for which a classifier is able to produce a prediction-gain over the sole use of a CNN classifier. Our dataset, codes and trained models will be available online at https://github.com/GKalliatakis/DisplaceNet.



### Group Emotion Recognition Using Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.01118v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.01118v1)
- **Published**: 2019-05-03 11:21:25+00:00
- **Updated**: 2019-05-03 11:21:25+00:00
- **Authors**: Samanyou Garg
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic facial emotion recognition is a challenging task that has gained significant scientific interest over the past few years, but the problem of emotion recognition for a group of people has been less extensively studied. However, it is slowly gaining popularity due to the massive amount of data available on social networking sites containing images of groups of people participating in various social events. Group emotion recognition is a challenging problem due to obstructions like head and body pose variations, occlusions, variable lighting conditions, variance of actors, varied indoor and outdoor settings and image quality. The objective of this task is to classify a group's perceived emotion as Positive, Neutral or Negative. In this report, we describe our solution which is a hybrid machine learning system that incorporates deep neural networks and Bayesian classifiers. Deep Convolutional Neural Networks (CNNs) work from bottom to top, analysing facial expressions expressed by individual faces extracted from the image. The Bayesian network works from top to bottom, inferring the global emotion for the image, by integrating the visual features of the contents of the image obtained through a scene descriptor. In the final pipeline, the group emotion category predicted by an ensemble of CNNs in the bottom-up module is passed as input to the Bayesian Network in the top-down module and an overall prediction for the image is obtained. Experimental results show that the stated system achieves 65.27% accuracy on the validation set which is in line with state-of-the-art results. As an outcome of this project, a Progressive Web Application and an accompanying Android app with a simple and intuitive user interface are presented, allowing users to test out the system with their own pictures.



### Distance Metric Learned Collaborative Representation Classifier
- **Arxiv ID**: http://arxiv.org/abs/1905.01168v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01168v3)
- **Published**: 2019-05-03 13:02:09+00:00
- **Updated**: 2021-10-01 00:02:31+00:00
- **Authors**: Tapabrata Chakraborti, Brendan McCane, Steven Mills, Umapada Pal
- **Comment**: arXiv admin note: text overlap with arXiv:1903.09123
- **Journal**: None
- **Summary**: Any generic deep machine learning algorithm is essentially a function fitting exercise, where the network tunes its weights and parameters to learn discriminatory features by minimizing some cost function. Though the network tries to learn the optimal feature space, it seldom tries to learn an optimal distance metric in the cost function, and hence misses out on an additional layer of abstraction. We present a simple effective way of achieving this by learning a generic Mahalanabis distance in a collaborative loss function in an end-to-end fashion with any standard convolutional network as the feature learner. The proposed method DML-CRC gives state-of-the-art performance on benchmark fine-grained classification datasets CUB Birds, Oxford Flowers and Oxford-IIIT Pets using the VGG-19 deep network. The method is network agnostic and can be used for any similar classification tasks.



### Computational analysis of laminar structure of the human cortex based on local neuron features
- **Arxiv ID**: http://arxiv.org/abs/1905.01173v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.NC, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1905.01173v2)
- **Published**: 2019-05-03 13:15:54+00:00
- **Updated**: 2019-12-13 15:19:04+00:00
- **Authors**: Andrija Štajduhar, Tomislav Lipić, Goran Sedmak, Sven Lončarić, Miloš Judaš
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel method for analysis and segmentation of laminar structure of the cortex based on tissue characteristics whose change across the gray matter underlies distinctive between cortical layers. We develop and analyze features of individual neurons to investigate changes in cytoarchitectonic differentiation and present a novel high-performance, automated framework for neuron-level histological image analysis. Local tissue and cell descriptors such as density, neuron size and other measures are used for development of more complex neuron features used in machine learning model trained on data manually labeled by three human experts. Final neuron layer classifications were obtained by training a separate model for each expert and combining their probability outputs. Importances of developed neuron features on both global model level and individual prediction level are presented and discussed.



### Query-guided End-to-End Person Search
- **Arxiv ID**: http://arxiv.org/abs/1905.01203v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01203v1)
- **Published**: 2019-05-03 14:31:23+00:00
- **Updated**: 2019-05-03 14:31:23+00:00
- **Authors**: Bharti Munjal, Sikandar Amin, Federico Tombari, Fabio Galasso
- **Comment**: Accepted as poster in CVPR 2019
- **Journal**: None
- **Summary**: Person search has recently gained attention as the novel task of finding a person, provided as a cropped sample, from a gallery of non-cropped images, whereby several other people are also visible. We believe that i. person detection and re-identification should be pursued in a joint optimization framework and that ii. the person search should leverage the query image extensively (e.g. emphasizing unique query patterns). However, so far, no prior art realizes this. We introduce a novel query-guided end-to-end person search network (QEEPS) to address both aspects. We leverage a most recent joint detector and re-identification work, OIM [37]. We extend this with i. a query-guided Siamese squeeze-and-excitation network (QSSE-Net) that uses global context from both the query and gallery images, ii. a query-guided region proposal network (QRPN) to produce query-relevant proposals, and iii. a query-guided similarity subnetwork (QSimNet), to learn a query-guided reidentification score. QEEPS is the first end-to-end query-guided detection and re-id network. On both the most recent CUHK-SYSU [37] and PRW [46] datasets, we outperform the previous state-of-the-art by a large margin.



### Offline Writer Identification based on the Path Signature Feature
- **Arxiv ID**: http://arxiv.org/abs/1905.01207v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01207v1)
- **Published**: 2019-05-03 14:41:55+00:00
- **Updated**: 2019-05-03 14:41:55+00:00
- **Authors**: Songxuan Lai, Lianwen Jin
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel set of features for offline writer identification based on the path signature approach, which provides a principled way to express information contained in a path. By extracting local pathlets from handwriting contours, the path signature can also characterize the offline handwriting style. A codebook method based on the log path signature---a more compact way to express the path signature---is used in this work and shows competitive results on several benchmark offline writer identification datasets, namely the IAM, Firemaker, CVL and ICDAR2013 writer identification contest dataset.



### Seamless Scene Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.01220v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01220v1)
- **Published**: 2019-05-03 15:21:25+00:00
- **Updated**: 2019-05-03 15:21:25+00:00
- **Authors**: Lorenzo Porzi, Samuel Rota Bulò, Aleksander Colovic, Peter Kontschieder
- **Comment**: extended version of the accepted CVPR 2019 paper
- **Journal**: None
- **Summary**: In this work we introduce a novel, CNN-based architecture that can be trained end-to-end to deliver seamless scene segmentation results. Our goal is to predict consistent semantic segmentation and detection results by means of a panoptic output format, going beyond the simple combination of independently trained segmentation and detection models. The proposed architecture takes advantage of a novel segmentation head that seamlessly integrates multi-scale features generated by a Feature Pyramid Network with contextual information conveyed by a light-weight DeepLab-like module. As additional contribution we review the panoptic metric and propose an alternative that overcomes its limitations when evaluating non-instance categories. Our proposed network architecture yields state-of-the-art results on three challenging street-level datasets, i.e. Cityscapes, Indian Driving Dataset and Mapillary Vistas.



### Scaling and Benchmarking Self-Supervised Visual Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.01235v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.01235v2)
- **Published**: 2019-05-03 15:50:51+00:00
- **Updated**: 2019-06-06 13:08:29+00:00
- **Authors**: Priya Goyal, Dhruv Mahajan, Abhinav Gupta, Ishan Misra
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised learning aims to learn representations from the data itself without explicit manual supervision. Existing efforts ignore a crucial aspect of self-supervised learning - the ability to scale to large amount of data because self-supervision requires no manual labels. In this work, we revisit this principle and scale two popular self-supervised approaches to 100 million images. We show that by scaling on various axes (including data size and problem 'hardness'), one can largely match or even exceed the performance of supervised pre-training on a variety of tasks such as object detection, surface normal estimation (3D) and visual navigation using reinforcement learning. Scaling these methods also provides many interesting insights into the limitations of current self-supervised techniques and evaluations. We conclude that current self-supervised methods are not 'hard' enough to take full advantage of large scale data and do not seem to learn effective high level semantic representations. We also introduce an extensive benchmark across 9 different datasets and tasks. We believe that such a benchmark along with comparable evaluation settings is necessary to make meaningful progress. Code is at: https://github.com/facebookresearch/fair_self_supervision_benchmark.



### Processing Megapixel Images with Deep Attention-Sampling Models
- **Arxiv ID**: http://arxiv.org/abs/1905.03711v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.03711v2)
- **Published**: 2019-05-03 16:27:46+00:00
- **Updated**: 2019-07-17 15:20:50+00:00
- **Authors**: Angelos Katharopoulos, François Fleuret
- **Comment**: Presented in ICML 2019. Code is available at
  https://github.com/idiap/attention-sampling
- **Journal**: Proceedings of the 36th International Conference on Machine
  Learning, PMLR 97:3282-3291, 2019
- **Summary**: Existing deep architectures cannot operate on very large signals such as megapixel images due to computational and memory constraints. To tackle this limitation, we propose a fully differentiable end-to-end trainable model that samples and processes only a fraction of the full resolution input image. The locations to process are sampled from an attention distribution computed from a low resolution view of the input. We refer to our method as attention sampling and it can process images of several megapixels with a standard single GPU setup. We show that sampling from the attention distribution results in an unbiased estimator of the full model with minimal variance, and we derive an unbiased estimator of the gradient that we use to train our model end-to-end with a normal SGD procedure. This new method is evaluated on three classification tasks, where we show that it allows to reduce computation and memory footprint by an order of magnitude for the same accuracy as classical architectures. We also show the consistency of the sampling that indeed focuses on informative parts of the input images.



### Learning Cross-Modal Embeddings with Adversarial Networks for Cooking Recipes and Food Images
- **Arxiv ID**: http://arxiv.org/abs/1905.01273v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1905.01273v1)
- **Published**: 2019-05-03 17:08:01+00:00
- **Updated**: 2019-05-03 17:08:01+00:00
- **Authors**: Hao Wang, Doyen Sahoo, Chenghao Liu, Ee-peng Lim, Steven C. H. Hoi
- **Comment**: Accepted at CVPR 2019
- **Journal**: None
- **Summary**: Food computing is playing an increasingly important role in human daily life, and has found tremendous applications in guiding human behavior towards smart food consumption and healthy lifestyle. An important task under the food-computing umbrella is retrieval, which is particularly helpful for health related applications, where we are interested in retrieving important information about food (e.g., ingredients, nutrition, etc.). In this paper, we investigate an open research task of cross-modal retrieval between cooking recipes and food images, and propose a novel framework Adversarial Cross-Modal Embedding (ACME) to resolve the cross-modal retrieval task in food domains. Specifically, the goal is to learn a common embedding feature space between the two modalities, in which our approach consists of several novel ideas: (i) learning by using a new triplet loss scheme together with an effective sampling strategy, (ii) imposing modality alignment using an adversarial learning strategy, and (iii) imposing cross-modal translation consistency such that the embedding of one modality is able to recover some important information of corresponding instances in the other modality. ACME achieves the state-of-the-art performance on the benchmark Recipe1M dataset, validating the efficacy of the proposed technique.



### Unsupervised Pre-Training of Image Features on Non-Curated Data
- **Arxiv ID**: http://arxiv.org/abs/1905.01278v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01278v3)
- **Published**: 2019-05-03 17:20:55+00:00
- **Updated**: 2019-08-13 08:31:13+00:00
- **Authors**: Mathilde Caron, Piotr Bojanowski, Julien Mairal, Armand Joulin
- **Comment**: Accepted at ICCV 2019 (Oral)
- **Journal**: None
- **Summary**: Pre-training general-purpose visual features with convolutional neural networks without relying on annotations is a challenging and important task. Most recent efforts in unsupervised feature learning have focused on either small or highly curated datasets like ImageNet, whereas using uncurated raw datasets was found to decrease the feature quality when evaluated on a transfer task. Our goal is to bridge the performance gap between unsupervised methods trained on curated data, which are costly to obtain, and massive raw datasets that are easily available. To that effect, we propose a new unsupervised approach which leverages self-supervision and clustering to capture complementary statistics from large-scale data. We validate our approach on 96 million images from YFCC100M, achieving state-of-the-art results among unsupervised methods on standard benchmarks, which confirms the potential of unsupervised learning when only uncurated data are available. We also show that pre-training a supervised VGG-16 with our method achieves 74.9% top-1 classification accuracy on the validation set of ImageNet, which is an improvement of +0.8% over the same network trained from scratch. Our code is available at https://github.com/facebookresearch/DeeperCluster.



### PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings
- **Arxiv ID**: http://arxiv.org/abs/1905.01296v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.01296v3)
- **Published**: 2019-05-03 17:54:09+00:00
- **Updated**: 2019-09-30 16:45:21+00:00
- **Authors**: Nicholas Rhinehart, Rowan McAllister, Kris Kitani, Sergey Levine
- **Comment**: To appear at the IEEE International Conference on Computer Vision
  (ICCV 2019). Website: https://sites.google.com/view/precog
- **Journal**: None
- **Summary**: For autonomous vehicles (AVs) to behave appropriately on roads populated by human-driven vehicles, they must be able to reason about the uncertain intentions and decisions of other drivers from rich perceptual information. Towards these capabilities, we present a probabilistic forecasting model of future interactions between a variable number of agents. We perform both standard forecasting and the novel task of conditional forecasting, which reasons about how all agents will likely respond to the goal of a controlled agent (here, the AV). We train models on real and simulated data to forecast vehicle trajectories given past positions and LIDAR. Our evaluation shows that our model is substantially more accurate in multi-agent driving scenarios compared to existing state-of-the-art. Beyond its general ability to perform conditional forecasting queries, we show that our model's predictions of all agents improve when conditioned on knowledge of the AV's goal, further illustrating its capability to model agent interactions.



### SCOPS: Self-Supervised Co-Part Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.01298v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01298v1)
- **Published**: 2019-05-03 17:55:23+00:00
- **Updated**: 2019-05-03 17:55:23+00:00
- **Authors**: Wei-Chih Hung, Varun Jampani, Sifei Liu, Pavlo Molchanov, Ming-Hsuan Yang, Jan Kautz
- **Comment**: Accepted in CVPR 2019. Project page:
  http://varunjampani.github.io/scops
- **Journal**: None
- **Summary**: Parts provide a good intermediate representation of objects that is robust with respect to the camera, pose and appearance variations. Existing works on part segmentation is dominated by supervised approaches that rely on large amounts of manual annotations and can not generalize to unseen object categories. We propose a self-supervised deep learning approach for part segmentation, where we devise several loss functions that aids in predicting part segments that are geometrically concentrated, robust to object variations and are also semantically consistent across different object instances. Extensive experiments on different types of image collections demonstrate that our approach can produce part segments that adhere to object boundaries and also more semantically consistent across object instances compared to existing self-supervised techniques.



### TriDepth: Triangular Patch-based Deep Depth Prediction
- **Arxiv ID**: http://arxiv.org/abs/1905.01312v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01312v2)
- **Published**: 2019-05-03 18:00:01+00:00
- **Updated**: 2020-03-11 07:49:05+00:00
- **Authors**: Masaya Kaneko, Ken Sakurada, Kiyoharu Aizawa
- **Comment**: Project webpage: https://meshdepth.github.io/
- **Journal**: None
- **Summary**: We propose a novel and efficient representation for single-view depth estimation using Convolutional Neural Networks (CNNs). Point-cloud is generally used for CNN-based 3D scene reconstruction; however it has some drawbacks: (1) it is redundant as a representation for planar surfaces, and (2) no spatial relationships between points are available (e.g, texture and surface). As a more efficient representation, we introduce a triangular-patch-cloud, which represents the surface of the 3D structure using a set of triangular patches, and propose a CNN framework for its 3D structure estimation. In our framework, we create it by separating all the faces in a 2D mesh, which are determined adaptively from the input image, and estimate depths and normals of all the faces. Using a common RGBD-dataset, we show that our representation has a better or comparable performance than the existing point-cloud-based methods, although it has much less parameters.



### DeepSignals: Predicting Intent of Drivers Through Visual Signals
- **Arxiv ID**: http://arxiv.org/abs/1905.01333v1
- **DOI**: 10.1109/ICRA.2019.8794214
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.01333v1)
- **Published**: 2019-05-03 18:21:46+00:00
- **Updated**: 2019-05-03 18:21:46+00:00
- **Authors**: Davi Frossard, Eric Kee, Raquel Urtasun
- **Comment**: To be presented at the IEEE International Conference on Robotics and
  Automation (ICRA), 2019
- **Journal**: In IEEE 2019 International Conference on Robotics and Automation
  (ICRA) (pp. 9697-9703)
- **Summary**: Detecting the intention of drivers is an essential task in self-driving, necessary to anticipate sudden events like lane changes and stops. Turn signals and emergency flashers communicate such intentions, providing seconds of potentially critical reaction time. In this paper, we propose to detect these signals in video sequences by using a deep neural network that reasons about both spatial and temporal information. Our experiments on more than a million frames show high per-frame accuracy in very challenging scenarios.



### Controllable Artistic Text Style Transfer via Shape-Matching GAN
- **Arxiv ID**: http://arxiv.org/abs/1905.01354v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01354v2)
- **Published**: 2019-05-03 20:05:18+00:00
- **Updated**: 2019-08-10 16:10:08+00:00
- **Authors**: Shuai Yang, Zhangyang Wang, Zhaowen Wang, Ning Xu, Jiaying Liu, Zongming Guo
- **Comment**: Accepted by ICCV 2019. Code is available at
  https://williamyang1991.github.io/projects/ICCV2019/SMGAN.html
- **Journal**: None
- **Summary**: Artistic text style transfer is the task of migrating the style from a source image to the target text to create artistic typography. Recent style transfer methods have considered texture control to enhance usability. However, controlling the stylistic degree in terms of shape deformation remains an important open challenge. In this paper, we present the first text style transfer network that allows for real-time control of the crucial stylistic degree of the glyph through an adjustable parameter. Our key contribution is a novel bidirectional shape matching framework to establish an effective glyph-style mapping at various deformation levels without paired ground truth. Based on this idea, we propose a scale-controllable module to empower a single network to continuously characterize the multi-scale shape features of the style image and transfer these features to the target text. The proposed method demonstrates its superiority over previous state-of-the-arts in generating diverse, controllable and high-quality stylized text.



### Toward Standardized Classification of Foveated Displays
- **Arxiv ID**: http://arxiv.org/abs/1905.06229v2
- **DOI**: 10.1109/TVCG.2020.2973053
- **Categories**: **cs.GR**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1905.06229v2)
- **Published**: 2019-05-03 20:45:05+00:00
- **Updated**: 2020-07-02 16:14:09+00:00
- **Authors**: Josef Spjut, Ben Boudaoud, Jonghyun Kim, Trey Greer, Rachel Albert, Michael Stengel, Kaan Aksit, David Luebke
- **Comment**: 9 pages, 8 figures, presented at IEEE VR 2020
- **Journal**: in IEEE Transactions on Visualization and Computer Graphics, vol.
  26, no. 5, pp. 2126-2134, May 2020
- **Summary**: Emergent in the field of head mounted display design is a desire to leverage the limitations of the human visual system to reduce the computation, communication, and display workload in power and form-factor constrained systems. Fundamental to this reduced workload is the ability to match display resolution to the acuity of the human visual system, along with a resulting need to follow the gaze of the eye as it moves, a process referred to as foveation. A display that moves its content along with the eye may be called a Foveated Display, though this term is also commonly used to describe displays with non-uniform resolution that attempt to mimic human visual acuity. We therefore recommend a definition for the term Foveated Display that accepts both of these interpretations. Furthermore, we include a simplified model for human visual Acuity Distribution Functions (ADFs) at various levels of visual acuity, across wide fields of view and propose comparison of this ADF with the Resolution Distribution Function of a foveated display for evaluation of its resolution at a particular gaze direction. We also provide a taxonomy to allow the field to meaningfully compare and contrast various aspects of foveated displays in a display and optical technology-agnostic manner.



### Steadiface: Real-Time Face-Centric Stabilization on Mobile Phones
- **Arxiv ID**: http://arxiv.org/abs/1905.01382v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01382v1)
- **Published**: 2019-05-03 22:57:37+00:00
- **Updated**: 2019-05-03 22:57:37+00:00
- **Authors**: Fuhao Shi, Sung-Fang Tsai, Youyou Wang, Chia-Kai Liang
- **Comment**: Accepted to be presented at ICIP 2019
- **Journal**: None
- **Summary**: We present Steadiface, a new real-time face-centric video stabilization method that simultaneously removes hand shake and keeps subject's head stable. We use a CNN to estimate the face landmarks and use them to optimize a stabilized head center. We then formulate an optimization problem to find a virtual camera pose that locates the face to the stabilized head center while retains smooth rotation and translation transitions across frames. We test the proposed method on fieldtest videos and show it stabilizes both the head motion and background. It is robust to large head pose, occlusion, facial appearance variations, and different kinds of camera motions. We show our method advances the state of art in selfie video stabilization by comparing against alternative methods. The whole process runs very efficiently on a modern mobile phone (8.1 ms/frame).



### FlowSAN: Privacy-enhancing Semi-Adversarial Networks to Confound Arbitrary Face-based Gender Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1905.01388v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.01388v1)
- **Published**: 2019-05-03 23:37:28+00:00
- **Updated**: 2019-05-03 23:37:28+00:00
- **Authors**: Vahid Mirjalili, Sebastian Raschka, Arun Ross
- **Comment**: 11 pages, 8 figures, 3 and 2 supplementary pages
- **Journal**: None
- **Summary**: Privacy concerns in the modern digital age have prompted researchers to develop techniques that allow users to selectively suppress certain information in collected data while allowing for other information to be extracted. In this regard, Semi-Adversarial Networks (SAN) have recently emerged as a method for imparting soft-biometric privacy to face images. SAN enables modifications of input face images so that the resulting face images can still be reliably used by arbitrary conventional face matchers for recognition purposes, while attribute classifiers, such as gender classifiers, are confounded. However, the generalizability of SANs across arbitrary gender classifiers has remained an open concern. In this work, we propose a new method, FlowSAN, for allowing SANs to generalize to multiple unseen gender classifiers. We propose combining a diverse set of SAN models to compensate each other's weaknesses, thereby, forming a robust model with improved generalization capability. Extensive experiments using different unseen gender classifiers and face matchers demonstrate the efficacy of the proposed paradigm in imparting gender privacy to face images.



