# Arxiv Papers in cs.CV on 2019-05-08
### Goal-oriented Object Importance Estimation in On-road Driving Videos
- **Arxiv ID**: http://arxiv.org/abs/1905.02848v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02848v1)
- **Published**: 2019-05-08 00:07:26+00:00
- **Updated**: 2019-05-08 00:07:26+00:00
- **Authors**: Mingfei Gao, Ashish Tawari, Sujitha Martin
- **Comment**: None
- **Journal**: None
- **Summary**: We formulate a new problem as Object Importance Estimation (OIE) in on-road driving videos, where the road users are considered as important objects if they have influence on the control decision of the ego-vehicle's driver. The importance of a road user depends on both its visual dynamics, e.g., appearance, motion and location, in the driving scene and the driving goal, \emph{e.g}., the planned path, of the ego vehicle. We propose a novel framework that incorporates both visual model and goal representation to conduct OIE. To evaluate our framework, we collect an on-road driving dataset at traffic intersections in the real world and conduct human-labeled annotation of the important objects. Experimental results show that our goal-oriented method outperforms baselines and has much more improvement on the left-turn and right-turn scenarios. Furthermore, we explore the possibility of using object importance for driving control prediction and demonstrate that binary brake prediction can be improved with the information of object importance.



### Learning Cascaded Siamese Networks for High Performance Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1905.02857v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1905.02857v1)
- **Published**: 2019-05-08 01:06:23+00:00
- **Updated**: 2019-05-08 01:06:23+00:00
- **Authors**: Peng Gao, Yipeng Ma, Ruyue Yuan, Liyi Xiao, Fei Wang
- **Comment**: Accepted for IEEE 26th International Conference on Image Processing
  (ICIP 2019)
- **Journal**: None
- **Summary**: Visual tracking is one of the most challenging computer vision problems. In order to achieve high performance visual tracking in various negative scenarios, a novel cascaded Siamese network is proposed and developed based on two different deep learning networks: a matching subnetwork and a classification subnetwork. The matching subnetwork is a fully convolutional Siamese network. According to the similarity score between the exemplar image and the candidate image, it aims to search possible object positions and crop scaled candidate patches. The classification subnetwork is designed to further evaluate the cropped candidate patches and determine the optimal tracking results based on the classification score. The matching subnetwork is trained offline and fixed online, while the classification subnetwork performs stochastic gradient descent online to learn more target-specific information. To improve the tracking performance further, an effective classification subnetwork update method based on both similarity and classification scores is utilized for updating the classification subnetwork. Extensive experimental results demonstrate that our proposed approach achieves state-of-the-art performance in recent benchmarks.



### Neural 3D Morphable Models: Spiral Convolutional Networks for 3D Shape Representation Learning and Generation
- **Arxiv ID**: http://arxiv.org/abs/1905.02876v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.02876v3)
- **Published**: 2019-05-08 02:37:27+00:00
- **Updated**: 2019-08-03 00:14:45+00:00
- **Authors**: Giorgos Bouritsas, Sergiy Bokhnyak, Stylianos Ploumpis, Michael Bronstein, Stefanos Zafeiriou
- **Comment**: to appear at ICCV 2019
- **Journal**: None
- **Summary**: Generative models for 3D geometric data arise in many important applications in 3D computer vision and graphics. In this paper, we focus on 3D deformable shapes that share a common topological structure, such as human faces and bodies. Morphable Models and their variants, despite their linear formulation, have been widely used for shape representation, while most of the recently proposed nonlinear approaches resort to intermediate representations, such as 3D voxel grids or 2D views. In this work, we introduce a novel graph convolutional operator, acting directly on the 3D mesh, that explicitly models the inductive bias of the fixed underlying graph. This is achieved by enforcing consistent local orderings of the vertices of the graph, through the spiral operator, thus breaking the permutation invariance property that is adopted by all the prior work on Graph Neural Networks. Our operator comes by construction with desirable properties (anisotropic, topology-aware, lightweight, easy-to-optimise), and by using it as a building block for traditional deep generative architectures, we demonstrate state-of-the-art results on a variety of 3D shape datasets compared to the linear Morphable Model and other graph convolutional operators.



### Frame-Recurrent Video Inpainting by Robust Optical Flow Inference
- **Arxiv ID**: http://arxiv.org/abs/1905.02882v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02882v1)
- **Published**: 2019-05-08 03:13:54+00:00
- **Updated**: 2019-05-08 03:13:54+00:00
- **Authors**: Yifan Ding, Chuan Wang, Haibin Huang, Jiaming Liu, Jue Wang, Liqiang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a new inpainting framework for recovering missing regions of video frames. Compared with image inpainting, performing this task on video presents new challenges such as how to preserving temporal consistency and spatial details, as well as how to handle arbitrary input video size and length fast and efficiently. Towards this end, we propose a novel deep learning architecture which incorporates ConvLSTM and optical flow for modeling the spatial-temporal consistency in videos. It also saves much computational resource such that our method can handle videos with larger frame size and arbitrary length streamingly in real-time. Furthermore, to generate an accurate optical flow from corrupted frames, we propose a robust flow generation module, where two sources of flows are fed and a flow blending network is trained to fuse them. We conduct extensive experiments to evaluate our method in various scenarios and different datasets, both qualitatively and quantitatively. The experimental results demonstrate the superior of our method compared with the state-of-the-art inpainting approaches.



### Deep Flow-Guided Video Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1905.02884v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02884v1)
- **Published**: 2019-05-08 03:27:15+00:00
- **Updated**: 2019-05-08 03:27:15+00:00
- **Authors**: Rui Xu, Xiaoxiao Li, Bolei Zhou, Chen Change Loy
- **Comment**: cvpr'19
- **Journal**: None
- **Summary**: Video inpainting, which aims at filling in missing regions of a video, remains challenging due to the difficulty of preserving the precise spatial and temporal coherence of video contents. In this work we propose a novel flow-guided video inpainting approach. Rather than filling in the RGB pixels of each frame directly, we consider video inpainting as a pixel propagation problem. We first synthesize a spatially and temporally coherent optical flow field across video frames using a newly designed Deep Flow Completion network. Then the synthesized flow field is used to guide the propagation of pixels to fill up the missing regions in the video. Specifically, the Deep Flow Completion network follows a coarse-to-fine refinement to complete the flow fields, while their quality is further improved by hard flow example mining. Following the guide of the completed flow, the missing video regions can be filled up precisely. Our method is evaluated on DAVIS and YouTube-VOS datasets qualitatively and quantitatively, achieving the state-of-the-art performance in terms of inpainting quality and speed.



### Photometric Transformer Networks and Label Adjustment for Breast Density Prediction
- **Arxiv ID**: http://arxiv.org/abs/1905.02906v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02906v1)
- **Published**: 2019-05-08 04:32:34+00:00
- **Updated**: 2019-05-08 04:32:34+00:00
- **Authors**: Jaehwan Lee, Donggeon Yoo, Jung Yin Huh, Hyo-Eun Kim
- **Comment**: miccai 2019 submission
- **Journal**: None
- **Summary**: Grading breast density is highly sensitive to normalization settings of digital mammogram as the density is tightly correlated with the distribution of pixel intensity. Also, the grade varies with readers due to uncertain grading criteria. These issues are inherent in the density assessment of digital mammography. They are problematic when designing a computer-aided prediction model for breast density and become worse if the data comes from multiple sites. In this paper, we proposed two novel deep learning techniques for breast density prediction: 1) photometric transformation which adaptively normalizes the input mammograms, and 2) label distillation which adjusts the label by using its output prediction. The photometric transformer network predicts optimal parameters for photometric transformation on the fly, learned jointly with the main prediction network. The label distillation, a type of pseudo-label techniques, is intended to mitigate the grading variation. We experimentally showed that the proposed methods are beneficial in terms of breast density prediction, resulting in significant performance improvement compared to various previous approaches.



### ShapeGlot: Learning Language for Shape Differentiation
- **Arxiv ID**: http://arxiv.org/abs/1905.02925v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.02925v1)
- **Published**: 2019-05-08 06:01:33+00:00
- **Updated**: 2019-05-08 06:01:33+00:00
- **Authors**: Panos Achlioptas, Judy Fan, Robert X. D. Hawkins, Noah D. Goodman, Leonidas J. Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we explore how fine-grained differences between the shapes of common objects are expressed in language, grounded on images and 3D models of the objects. We first build a large scale, carefully controlled dataset of human utterances that each refers to a 2D rendering of a 3D CAD model so as to distinguish it from a set of shape-wise similar alternatives. Using this dataset, we develop neural language understanding (listening) and production (speaking) models that vary in their grounding (pure 3D forms via point-clouds vs. rendered 2D images), the degree of pragmatic reasoning captured (e.g. speakers that reason about a listener or not), and the neural architecture (e.g. with or without attention). We find models that perform well with both synthetic and human partners, and with held out utterances and objects. We also find that these models are amenable to zero-shot transfer learning to novel object classes (e.g. transfer from training on chairs to testing on lamps), as well as to real-world images drawn from furniture catalogs. Lesion studies indicate that the neural listeners depend heavily on part-related words and associate these words correctly with visual parts of objects (without any explicit network training on object parts), and that transfer to novel classes is most successful when known part-words are available. This work illustrates a practical approach to language grounding, and provides a case study in the relationship between object shape and linguistic structure when it comes to object differentiation.



### Deep Blind Video Decaptioning by Temporal Aggregation and Recurrence
- **Arxiv ID**: http://arxiv.org/abs/1905.02949v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02949v1)
- **Published**: 2019-05-08 08:04:35+00:00
- **Updated**: 2019-05-08 08:04:35+00:00
- **Authors**: Dahun Kim, Sanghyun Woo, Joon-Young Lee, In So Kweon
- **Comment**: Accepted at CVPR 2019
- **Journal**: None
- **Summary**: Blind video decaptioning is a problem of automatically removing text overlays and inpainting the occluded parts in videos without any input masks. While recent deep learning based inpainting methods deal with a single image and mostly assume that the positions of the corrupted pixels are known, we aim at automatic text removal in video sequences without mask information. In this paper, we propose a simple yet effective framework for fast blind video decaptioning. We construct an encoder-decoder model, where the encoder takes multiple source frames that can provide visible pixels revealed from the scene dynamics. These hints are aggregated and fed into the decoder. We apply a residual connection from the input frame to the decoder output to enforce our network to focus on the corrupted regions only. Our proposed model was ranked in the first place in the ECCV Chalearn 2018 LAP Inpainting Competition Track2: Video decaptioning. In addition, we further improve this strong model by applying a recurrent feedback. The recurrent feedback not only enforces temporal coherence but also provides strong clues on where the corrupted pixels are. Both qualitative and quantitative experiments demonstrate that our full model produces accurate and temporally consistent video results in real time (50+ fps).



### Multimodal Semantic Attention Network for Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/1905.02963v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.02963v1)
- **Published**: 2019-05-08 08:51:12+00:00
- **Updated**: 2019-05-08 08:51:12+00:00
- **Authors**: Liang Sun, Bing Li, Chunfeng Yuan, Zhengjun Zha, Weiming Hu
- **Comment**: 6 pages, 4 figures, accepted by IEEE International Conference on
  Multimedia and Expo (ICME) 2019
- **Journal**: None
- **Summary**: Inspired by the fact that different modalities in videos carry complementary information, we propose a Multimodal Semantic Attention Network(MSAN), which is a new encoder-decoder framework incorporating multimodal semantic attributes for video captioning. In the encoding phase, we detect and generate multimodal semantic attributes by formulating it as a multi-label classification problem. Moreover, we add auxiliary classification loss to our model that can obtain more effective visual features and high-level multimodal semantic attribute distributions for sufficient video encoding. In the decoding phase, we extend each weight matrix of the conventional LSTM to an ensemble of attribute-dependent weight matrices, and employ attention mechanism to pay attention to different attributes at each time of the captioning process. We evaluate algorithm on two popular public benchmarks: MSVD and MSR-VTT, achieving competitive results with current state-of-the-art across six evaluation metrics.



### Multi-task human analysis in still images: 2D/3D pose, depth map, and multi-part segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.03003v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03003v1)
- **Published**: 2019-05-08 10:55:02+00:00
- **Updated**: 2019-05-08 10:55:02+00:00
- **Authors**: Daniel Sánchez, Marc Oliu, Meysam Madadi, Xavier Baró, Sergio Escalera
- **Comment**: 8 pages, 4 Figures, 5 Tables, Conference Faces and Gestures 2019
- **Journal**: None
- **Summary**: While many individual tasks in the domain of human analysis have recently received an accuracy boost from deep learning approaches, multi-task learning has mostly been ignored due to a lack of data. New synthetic datasets are being released, filling this gap with synthetic generated data. In this work, we analyze four related human analysis tasks in still images in a multi-task scenario by leveraging such datasets. Specifically, we study the correlation of 2D/3D pose estimation, body part segmentation and full-body depth estimation. These tasks are learned via the well-known Stacked Hourglass module such that each of the task-specific streams shares information with the others. The main goal is to analyze how training together these four related tasks can benefit each individual task for a better generalization. Results on the newly released SURREAL dataset show that all four tasks benefit from the multi-task approach, but with different combinations of tasks: while combining all four tasks improves 2D pose estimation the most, 2D pose improves neither 3D pose nor full-body depth estimation. On the other hand 2D parts segmentation can benefit from 2D pose but not from 3D pose. In all cases, as expected, the maximum improvement is achieved on those human body parts that show more variability in terms of spatial distribution, appearance and shape, e.g. wrists and ankles.



### Algorithms for Grey-Weighted Distance Computations
- **Arxiv ID**: http://arxiv.org/abs/1905.03017v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03017v1)
- **Published**: 2019-05-08 11:53:45+00:00
- **Updated**: 2019-05-08 11:53:45+00:00
- **Authors**: Magnus Gedda
- **Comment**: 16 pages, preprint submitted for journal publication, published in
  printed phd thesis
- **Journal**: None
- **Summary**: With the increasing size of datasets and demand for real time response for interactive applications, improving runtime for algorithms with excessive computational requirements has become increasingly important. Many different algorithms combining efficient priority queues with various helper structures have been proposed for computing grey-weighted distance transforms. Here we compare the performance of popular competitive algorithms in different scenarios to form practical guidelines easy to adopt. The label-setting category of algorithms is shown to be the best choice for all scenarios. The hierarchical heap with a pointer array to keep track of nodes on the heap is shown to be the best choice as priority queue. However, if memory is a critical issue, then the best choice is the Dial priority queue for integer valued costs and the Untidy priority queue for real valued costs.



### A Genetic Algorithm Enabled Similarity-Based Attack on Cancellable Biometrics
- **Arxiv ID**: http://arxiv.org/abs/1905.03021v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03021v2)
- **Published**: 2019-05-08 12:03:39+00:00
- **Updated**: 2019-09-15 16:09:26+00:00
- **Authors**: Xingbo Dong, Zhe Jin, Andrew Teoh Beng Jin
- **Comment**: 7 pages, 4 figures, for BTAS 2019
- **Journal**: 10th IEEE International Conference on Biometrics: Theory,
  Applications and Systems (BTAS),23-26 September 2019, Tampa, Florida
- **Summary**: Cancellable biometrics (CB) as a means for biometric template protection approach refers to an irreversible yet similarity preserving transformation on the original template. With similarity preserving property, the matching between template and query instance can be performed in the transform domain without jeopardizing accuracy performance. Unfortunately, this trait invites a class of attack, namely similarity-based attack (SA). SA produces a preimage, an inverse of transformed template, which can be exploited for impersonation and cross-matching. In this paper, we propose a Genetic Algorithm enabled similarity-based attack framework (GASAF) to demonstrate that CB schemes whose possess similarity preserving property are highly vulnerable to similarity-based attack. Besides that, a set of new metrics is designed to measure the effectiveness of the similarity-based attack. We conduct the experiment on two representative CB schemes, i.e. BioHashing and Bloom-filter. The experimental results attest the vulnerability under this type of attack.



### Automatic Video Colorization using 3D Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.03023v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03023v1)
- **Published**: 2019-05-08 12:06:29+00:00
- **Updated**: 2019-05-08 12:06:29+00:00
- **Authors**: Panagiotis Kouzouglidis, Giorgos Sfikas, Christophoros Nikou
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: In this work, we present a method for automatic colorization of grayscale videos. The core of the method is a Generative Adversarial Network that is trained and tested on sequences of frames in a sliding window manner. Network convolutional and deconvolutional layers are three-dimensional, with frame height, width and time as the dimensions taken into account. Multiple chrominance estimates per frame are aggregated and combined with available luminance information to recreate a colored sequence. Colorization trials are run succesfully on a dataset of old black-and-white films. The usefulness of our method is also validated with numerical results, computed with a newly proposed metric that measures colorization consistency over a frame sequence.



### 3d-SMRnet: Achieving a new quality of MPI system matrix recovery by deep learning
- **Arxiv ID**: http://arxiv.org/abs/1905.03026v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.03026v1)
- **Published**: 2019-05-08 12:21:39+00:00
- **Updated**: 2019-05-08 12:21:39+00:00
- **Authors**: Ivo Matteo Baltruschat, Patryk Szwargulski, Florian Griese, Mirco Grosser, René Werner, Tobias Knopp
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic particle imaging (MPI) data is commonly reconstructed using a system matrix acquired in a time-consuming calibration measurement. The calibration approach has the important advantage over model-based reconstruction that it takes the complex particle physics as well as system imperfections into account. This benefit comes for the cost that the system matrix needs to be re-calibrated whenever the scan parameters, particle types or even the particle environment (e.g. viscosity or temperature) changes. One route for reducing the calibration time is the sampling of the system matrix at a subset of the spatial positions of the intended field-of-view and employing system matrix recovery. Recent approaches used compressed sensing (CS) and achieved subsampling factors up to 28 that still allowed reconstructing MPI images of sufficient quality. In this work, we propose a novel framework with a 3d-System Matrix Recovery Network and demonstrate it to recover a 3d system matrix with a subsampling factor of 64 in less than one minute and to outperform CS in terms of system matrix quality, reconstructed image quality, and processing time. The advantage of our method is demonstrated by reconstructing open access MPI datasets. The model is further shown to be capable of inferring system matrices for different particle types.



### PiNet: A Permutation Invariant Graph Neural Network for Graph Classification
- **Arxiv ID**: http://arxiv.org/abs/1905.03046v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.03046v1)
- **Published**: 2019-05-08 12:51:52+00:00
- **Updated**: 2019-05-08 12:51:52+00:00
- **Authors**: Peter Meltzer, Marcelo Daniel Gutierrez Mallea, Peter J. Bentley
- **Comment**: 7 pages, 4 figures
- **Journal**: None
- **Summary**: We propose an end-to-end deep learning learning model for graph classification and representation learning that is invariant to permutation of the nodes of the input graphs. We address the challenge of learning a fixed size graph representation for graphs of varying dimensions through a differentiable node attention pooling mechanism. In addition to a theoretical proof of its invariance to permutation, we provide empirical evidence demonstrating the statistically significant gain in accuracy when faced with an isomorphic graph classification task given only a small number of training examples. We analyse the effect of four different matrices to facilitate the local message passing mechanism by which graph convolutions are performed vs. a matrix parametrised by a learned parameter pair able to transition smoothly between the former. Finally, we show that our model achieves competitive classification performance with existing techniques on a set of molecule datasets.



### Training a Fast Object Detector for LiDAR Range Images Using Labeled Data from Sensors with Higher Resolution
- **Arxiv ID**: http://arxiv.org/abs/1905.03066v3
- **DOI**: 10.1109/ITSC.2019.8917011
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1905.03066v3)
- **Published**: 2019-05-08 13:43:03+00:00
- **Updated**: 2019-12-05 14:03:40+00:00
- **Authors**: Manuel Herzog, Klaus Dietmayer
- **Comment**: None
- **Journal**: 2019 IEEE Intelligent Transportation Systems Conference (ITSC),
  Auckland, New Zealand, 2019, pp. 2707-2713
- **Summary**: In this paper, we describe a strategy for training neural networks for object detection in range images obtained from one type of LiDAR sensor using labeled data from a different type of LiDAR sensor. Additionally, an efficient model for object detection in range images for use in self-driving cars is presented. Currently, the highest performing algorithms for object detection from LiDAR measurements are based on neural networks. Training these networks using supervised learning requires large annotated datasets. Therefore, most research using neural networks for object detection from LiDAR point clouds is conducted on a very small number of publicly available datasets. Consequently, only a small number of sensor types are used. We use an existing annotated dataset to train a neural network that can be used with a LiDAR sensor that has a lower resolution than the one used for recording the annotated dataset. This is done by simulating data from the lower resolution LiDAR sensor based on the higher resolution dataset. Furthermore, improvements to models that use LiDAR range images for object detection are presented. The results are validated using both simulated sensor data and data from an actual lower resolution sensor mounted to a research vehicle. It is shown that the model can detect objects from 360{\deg} range images in real time.



### Capture, Learning, and Synthesis of 3D Speaking Styles
- **Arxiv ID**: http://arxiv.org/abs/1905.03079v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03079v1)
- **Published**: 2019-05-08 14:16:37+00:00
- **Updated**: 2019-05-08 14:16:37+00:00
- **Authors**: Daniel Cudeiro, Timo Bolkart, Cassidy Laidlaw, Anurag Ranjan, Michael J. Black
- **Comment**: To appear in CVPR 2019
- **Journal**: None
- **Summary**: Audio-driven 3D facial animation has been widely explored, but achieving realistic, human-like performance is still unsolved. This is due to the lack of available 3D datasets, models, and standard evaluation metrics. To address this, we introduce a unique 4D face dataset with about 29 minutes of 4D scans captured at 60 fps and synchronized audio from 12 speakers. We then train a neural network on our dataset that factors identity from facial motion. The learned model, VOCA (Voice Operated Character Animation) takes any speech signal as input - even speech in languages other than English - and realistically animates a wide range of adult faces. Conditioning on subject labels during training allows the model to learn a variety of realistic speaking styles. VOCA also provides animator controls to alter speaking style, identity-dependent facial shape, and pose (i.e. head, jaw, and eyeball rotations) during animation. To our knowledge, VOCA is the only realistic 3D facial animation model that is readily applicable to unseen subjects without retargeting. This makes VOCA suitable for tasks like in-game video, virtual reality avatars, or any scenario in which the speaker, speech, or language is not known in advance. We make the dataset and model available for research purposes at http://voca.is.tue.mpg.de.



### Unsupervised Learning through Temporal Smoothing and Entropy Maximization
- **Arxiv ID**: http://arxiv.org/abs/1905.03100v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.03100v1)
- **Published**: 2019-05-08 14:37:38+00:00
- **Updated**: 2019-05-08 14:37:38+00:00
- **Authors**: Per Rutquist
- **Comment**: This paper has been submitted to the 58th IEEE Conference on Decision
  and Control (CDC2019)
- **Journal**: None
- **Summary**: This paper proposes a method for machine learning from unlabeled data in the form of a time-series. The mapping that is learned is shown to extract slowly evolving information that would be useful for control applications, while efficiently filtering out unwanted, higher-frequency noise.   The method consists of training a feedforward artificial neural network with backpropagation using two opposing objectives.   The first of these is to minimize the squared changes in activations between time steps of each unit in the network. This "temporal smoothing" has the effect of correlating inputs that occur close in time with outputs that are close in the L2-norm.   The second objective is to maximize the log determinant of the covariance matrix of activations in each layer of the network. This objective ensures that information from each layer is passed through to the next. This second objective acts as a balance to the first, which on its own would result in a network with all input weights equal to zero.



### Thinking Outside the Box: Generation of Unconstrained 3D Room Layouts
- **Arxiv ID**: http://arxiv.org/abs/1905.03105v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03105v1)
- **Published**: 2019-05-08 14:43:26+00:00
- **Updated**: 2019-05-08 14:43:26+00:00
- **Authors**: Henry Howard-Jenkins, Shuda Li, Victor Prisacariu
- **Comment**: Asian Conference on Computer Vision (ACCV), 2018
- **Journal**: None
- **Summary**: We propose a method for room layout estimation that does not rely on the typical box approximation or Manhattan world assumption. Instead, we reformulate the geometry inference problem as an instance detection task, which we solve by directly regressing 3D planes using an R-CNN. We then use a variant of probabilistic clustering to combine the 3D planes regressed at each frame in a video sequence, with their respective camera poses, into a single global 3D room layout estimate. Finally, we showcase results which make no assumptions about perpendicular alignment, so can deal effectively with walls in any alignment.



### TE141K: Artistic Text Benchmark for Text Effect Transfer
- **Arxiv ID**: http://arxiv.org/abs/1905.03646v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03646v3)
- **Published**: 2019-05-08 15:57:39+00:00
- **Updated**: 2020-03-24 13:22:55+00:00
- **Authors**: Shuai Yang, Wenjing Wang, Jiaying Liu
- **Comment**: Accepted by TPAMI 2020. Project page:
  https://daooshee.github.io/TE141K/
- **Journal**: None
- **Summary**: Text effects are combinations of visual elements such as outlines, colors and textures of text, which can dramatically improve its artistry. Although text effects are extensively utilized in the design industry, they are usually created by human experts due to their extreme complexity; this is laborious and not practical for normal users. In recent years, some efforts have been made toward automatic text effect transfer; however, the lack of data limits the capabilities of transfer models. To address this problem, we introduce a new text effects dataset, TE141K, with 141,081 text effect/glyph pairs in total. Our dataset consists of 152 professionally designed text effects rendered on glyphs, including English letters, Chinese characters, and Arabic numerals. To the best of our knowledge, this is the largest dataset for text effect transfer to date. Based on this dataset, we propose a baseline approach called text effect transfer GAN (TET-GAN), which supports the transfer of all 152 styles in one model and can efficiently extend to new styles. Finally, we conduct a comprehensive comparison in which 14 style transfer models are benchmarked. Experimental results demonstrate the superiority of TET-GAN both qualitatively and quantitatively and indicate that our dataset is effective and challenging.



### Unsupervised Domain Adaptation using Generative Adversarial Networks for Semantic Segmentation of Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/1905.03198v1
- **DOI**: 10.3390/rs11111369
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03198v1)
- **Published**: 2019-05-08 16:35:02+00:00
- **Updated**: 2019-05-08 16:35:02+00:00
- **Authors**: Bilel Benjdira, Yakoub Bazi, Anis Koubaa, Kais Ouni
- **Comment**: submitted to a journal
- **Journal**: MDPI Remote Sensing, Volume 11, Issue 11, 2019
- **Summary**: Segmenting aerial images is being of great potential in surveillance and scene understanding of urban areas. It provides a mean for automatic reporting of the different events that happen in inhabited areas. This remarkably promotes public safety and traffic management applications. After the wide adoption of convolutional neural networks methods, the accuracy of semantic segmentation algorithms could easily surpass 80% if a robust dataset is provided. Despite this success, the deployment of a pre-trained segmentation model to survey a new city that is not included in the training set significantly decreases the accuracy. This is due to the domain shift between the source dataset on which the model is trained and the new target domain of the new city images. In this paper, we address this issue and consider the challenge of domain adaptation in semantic segmentation of aerial images. We design an algorithm that reduces the domain shift impact using Generative Adversarial Networks (GANs). In the experiments, we test the proposed methodology on the International Society for Photogrammetry and Remote Sensing (ISPRS) semantic segmentation dataset and found that our method improves the overall accuracy from 35% to 52% when passing from Potsdam domain (considered as source domain) to Vaihingen domain (considered as target domain). In addition, the method allows recovering efficiently the inverted classes due to sensor variation. In particular, it improves the average segmentation accuracy of the inverted classes due to sensor variation from 14% to 61%.



### Endoscopy artifact detection (EAD 2019) challenge dataset
- **Arxiv ID**: http://arxiv.org/abs/1905.03209v1
- **DOI**: 10.17632/C7FJBXCGJ9.1
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.03209v1)
- **Published**: 2019-05-08 16:53:14+00:00
- **Updated**: 2019-05-08 16:53:14+00:00
- **Authors**: Sharib Ali, Felix Zhou, Christian Daul, Barbara Braden, Adam Bailey, Stefano Realdon, James East, Georges Wagnières, Victor Loschenov, Enrico Grisan, Walter Blondel, Jens Rittscher
- **Comment**: 12 pages, EAD2019 dataset description
- **Journal**: None
- **Summary**: Endoscopic artifacts are a core challenge in facilitating the diagnosis and treatment of diseases in hollow organs. Precise detection of specific artifacts like pixel saturations, motion blur, specular reflections, bubbles and debris is essential for high-quality frame restoration and is crucial for realizing reliable computer-assisted tools for improved patient care. At present most videos in endoscopy are currently not analyzed due to the abundant presence of multi-class artifacts in video frames. Through the endoscopic artifact detection (EAD 2019) challenge, we address this key bottleneck problem by solving the accurate identification and localization of endoscopic frame artifacts to enable further key quantitative analysis of unusable video frames such as mosaicking and 3D reconstruction which is crucial for delivering improved patient care. This paper summarizes the challenge tasks and describes the dataset and evaluation criteria established in the EAD 2019 challenge.



### Evaluating the Stability of Recurrent Neural Models during Training with Eigenvalue Spectra Analysis
- **Arxiv ID**: http://arxiv.org/abs/1905.03219v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.03219v1)
- **Published**: 2019-05-08 17:12:51+00:00
- **Updated**: 2019-05-08 17:12:51+00:00
- **Authors**: Priyadarshini Panda, Efstathia Soufleri, Kaushik Roy
- **Comment**: Accepted in IJCNN 2019
- **Journal**: None
- **Summary**: We analyze the stability of recurrent networks, specifically, reservoir computing models during training by evaluating the eigenvalue spectra of the reservoir dynamics. To circumvent the instability arising in examining a closed loop reservoir system with feedback, we propose to break the closed loop system. Essentially, we unroll the reservoir dynamics over time while incorporating the feedback effects that preserve the overall temporal integrity of the system. We evaluate our methodology for fixed point and time varying targets with least squares regression and FORCE training, respectively. Our analysis establishes eigenvalue spectra (which is, shrinking of spectral circle as training progresses) as a valid and effective metric to gauge the convergence of training as well as the convergence of the chaotic activity of the reservoir toward stable states.



### Convolutional Mesh Regression for Single-Image Human Shape Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1905.03244v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03244v1)
- **Published**: 2019-05-08 17:59:06+00:00
- **Updated**: 2019-05-08 17:59:06+00:00
- **Authors**: Nikos Kolotouros, Georgios Pavlakos, Kostas Daniilidis
- **Comment**: To appear at CVPR 2019 (Oral Presentation). Project page:
  https://www.seas.upenn.edu/~nkolot/projects/cmr/
- **Journal**: None
- **Summary**: This paper addresses the problem of 3D human pose and shape estimation from a single image. Previous approaches consider a parametric model of the human body, SMPL, and attempt to regress the model parameters that give rise to a mesh consistent with image evidence. This parameter regression has been a very challenging task, with model-based approaches underperforming compared to nonparametric solutions in terms of pose estimation. In our work, we propose to relax this heavy reliance on the model's parameter space. We still retain the topology of the SMPL template mesh, but instead of predicting model parameters, we directly regress the 3D location of the mesh vertices. This is a heavy task for a typical network, but our key insight is that the regression becomes significantly easier using a Graph-CNN. This architecture allows us to explicitly encode the template mesh structure within the network and leverage the spatial locality the mesh has to offer. Image-based features are attached to the mesh vertices and the Graph-CNN is responsible to process them on the mesh structure, while the regression target for each vertex is its 3D location. Having recovered the complete 3D geometry of the mesh, if we still require a specific model parametrization, this can be reliably regressed from the vertices locations. We demonstrate the flexibility and the effectiveness of our proposed graph-based mesh regression by attaching different types of features on the mesh vertices. In all cases, we outperform the comparable baselines relying on model parameter regression, while we also achieve state-of-the-art results among model-based pose estimation approaches.



### End-to-End Wireframe Parsing
- **Arxiv ID**: http://arxiv.org/abs/1905.03246v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03246v3)
- **Published**: 2019-05-08 17:59:41+00:00
- **Updated**: 2021-05-04 21:36:28+00:00
- **Authors**: Yichao Zhou, Haozhi Qi, Yi Ma
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: We present a conceptually simple yet effective algorithm to detect wireframes in a given image. Compared to the previous methods which first predict an intermediate heat map and then extract straight lines with heuristic algorithms, our method is end-to-end trainable and can directly output a vectorized wireframe that contains semantically meaningful and geometrically salient junctions and lines. To better understand the quality of the outputs, we propose a new metric for wireframe evaluation that penalizes overlapped line segments and incorrect line connectivities. We conduct extensive experiments and show that our method significantly outperforms the previous state-of-the-art wireframe and line extraction algorithms. We hope our simple approach can be served as a baseline for future wireframe parsing studies. Code has been made publicly available at https://github.com/zhou13/lcnn.



### Handheld Multi-Frame Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1905.03277v2
- **DOI**: 10.1145/3306346.3323024
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.03277v2)
- **Published**: 2019-05-08 18:09:15+00:00
- **Updated**: 2021-02-16 23:06:58+00:00
- **Authors**: Bartlomiej Wronski, Ignacio Garcia-Dorado, Manfred Ernst, Damien Kelly, Michael Krainin, Chia-Kai Liang, Marc Levoy, Peyman Milanfar
- **Comment**: 24 pages, accepted to Siggraph 2019 Technical Papers program
- **Journal**: None
- **Summary**: Compared to DSLR cameras, smartphone cameras have smaller sensors, which limits their spatial resolution; smaller apertures, which limits their light gathering ability; and smaller pixels, which reduces their signal-to noise ratio. The use of color filter arrays (CFAs) requires demosaicing, which further degrades resolution. In this paper, we supplant the use of traditional demosaicing in single-frame and burst photography pipelines with a multiframe super-resolution algorithm that creates a complete RGB image directly from a burst of CFA raw images. We harness natural hand tremor, typical in handheld photography, to acquire a burst of raw frames with small offsets. These frames are then aligned and merged to form a single image with red, green, and blue values at every pixel site. This approach, which includes no explicit demosaicing step, serves to both increase image resolution and boost signal to noise ratio. Our algorithm is robust to challenging scene conditions: local motion, occlusion, or scene changes. It runs at 100 milliseconds per 12-megapixel RAW input burst frame on mass-produced mobile phones. Specifically, the algorithm is the basis of the Super-Res Zoom feature, as well as the default merge method in Night Sight mode (whether zooming or not) on Google's flagship phone.



### Advancements in Image Classification using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1905.03288v1
- **DOI**: 10.1109/ICRCICN.2018.8718718
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.03288v1)
- **Published**: 2019-05-08 18:34:19+00:00
- **Updated**: 2019-05-08 18:34:19+00:00
- **Authors**: Farhana Sultana, A. Sufian, Paramartha Dutta
- **Comment**: 9 pages, 15 figures, 3 Tables. Submitted to 2018 Fourth International
  Conference on Research in Computational Intelligence and Communication
  Networks(ICRCICN 2018)
- **Journal**: 2018 Fourth International Conference on Research in Computational
  Intelligence and Communication Networks (ICRCICN)
- **Summary**: Convolutional Neural Network (CNN) is the state-of-the-art for image classification task. Here we have briefly discussed different components of CNN. In this paper, We have explained different CNN architectures for image classification. Through this paper, we have shown advancements in CNN from LeNet-5 to latest SENet model. We have discussed the model description and training details of each model. We have also drawn a comparison among those models.



### Deep Closest Point: Learning Representations for Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/1905.03304v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03304v1)
- **Published**: 2019-05-08 19:10:01+00:00
- **Updated**: 2019-05-08 19:10:01+00:00
- **Authors**: Yue Wang, Justin M. Solomon
- **Comment**: None
- **Journal**: None
- **Summary**: Point cloud registration is a key problem for computer vision applied to robotics, medical imaging, and other applications. This problem involves finding a rigid transformation from one point cloud into another so that they align. Iterative Closest Point (ICP) and its variants provide simple and easily-implemented iterative methods for this task, but these algorithms can converge to spurious local optima. To address local optima and other difficulties in the ICP pipeline, we propose a learning-based method, titled Deep Closest Point (DCP), inspired by recent techniques in computer vision and natural language processing. Our model consists of three parts: a point cloud embedding network, an attention-based module combined with a pointer generation layer, to approximate combinatorial matching, and a differentiable singular value decomposition (SVD) layer to extract the final rigid transformation. We train our model end-to-end on the ModelNet40 dataset and show in several settings that it performs better than ICP, its variants (e.g., Go-ICP, FGR), and the recently-proposed learning-based method PointNetLK. Beyond providing a state-of-the-art registration technique, we evaluate the suitability of our learned features transferred to unseen objects. We also provide preliminary analysis of our learned model to help understand whether domain-specific and/or global features facilitate rigid registration.



### Weakly Labeling the Antarctic: The Penguin Colony Case
- **Arxiv ID**: http://arxiv.org/abs/1905.03313v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03313v2)
- **Published**: 2019-05-08 20:03:18+00:00
- **Updated**: 2019-05-19 08:17:19+00:00
- **Authors**: Hieu Le, Bento Gonçalves, Dimitris Samaras, Heather Lynch
- **Comment**: CVPR19 - CV4GC workshop
- **Journal**: None
- **Summary**: Antarctic penguins are important ecological indicators -- especially in the face of climate change. In this work, we present a deep learning based model for semantic segmentation of Ad\'elie penguin colonies in high-resolution satellite imagery. To train our segmentation models, we take advantage of the Penguin Colony Dataset: a unique dataset with 2044 georeferenced cropped images from 193 Ad\'elie penguin colonies in Antarctica. In the face of a scarcity of pixel-level annotation masks, we propose a weakly-supervised framework to effectively learn a segmentation model from weak labels. We use a classification network to filter out data unsuitable for the segmentation network. This segmentation network is trained with a specific loss function, based on the average activation, to effectively learn from the data with the weakly-annotated labels. Our experiments show that adding weakly-annotated training examples significantly improves segmentation performance, increasing the mean Intersection-over-Union from 42.3 to 60.0% on the Penguin Colony Dataset.



### Learning to Evolve
- **Arxiv ID**: http://arxiv.org/abs/1905.03389v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV, cs.LG, stat.ML, 62M45, 68T05, 68W25, 68T20, 90C40, 91A22, 92D15, 92D25, G.1.6; I.2.6; I.2.8; G.3; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/1905.03389v1)
- **Published**: 2019-05-08 23:35:02+00:00
- **Updated**: 2019-05-08 23:35:02+00:00
- **Authors**: Jan Schuchardt, Vladimir Golkov, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: Evolution and learning are two of the fundamental mechanisms by which life adapts in order to survive and to transcend limitations. These biological phenomena inspired successful computational methods such as evolutionary algorithms and deep learning. Evolution relies on random mutations and on random genetic recombination. Here we show that learning to evolve, i.e. learning to mutate and recombine better than at random, improves the result of evolution in terms of fitness increase per generation and even in terms of attainable fitness. We use deep reinforcement learning to learn to dynamically adjust the strategy of evolutionary algorithms to varying circumstances. Our methods outperform classical evolutionary algorithms on combinatorial and continuous optimization problems.



