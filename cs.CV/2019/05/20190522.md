# Arxiv Papers in cs.CV on 2019-05-22
### A Neural-Symbolic Architecture for Inverse Graphics Improved by Lifelong Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.08910v2
- **DOI**: 10.1007/978-3-030-33676-9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08910v2)
- **Published**: 2019-05-22 00:48:12+00:00
- **Updated**: 2019-09-05 00:55:13+00:00
- **Authors**: Michael Kissner, Helmut Mayer
- **Comment**: German Conference on Pattern Recognition (GCPR) 2019
- **Journal**: 41st German Conference, GCPR 2019, Proceedings
- **Summary**: We follow the idea of formulating vision as inverse graphics and propose a new type of element for this task, a neural-symbolic capsule. It is capable of de-rendering a scene into semantic information feed-forward, as well as rendering it feed-backward. An initial set of capsules for graphical primitives is obtained from a generative grammar and connected into a full capsule network. Lifelong meta-learning continuously improves this network's detection capabilities by adding capsules for new and more complex objects it detects in a scene using few-shot learning. Preliminary results demonstrate the potential of our novel approach.



### Joint Information Preservation for Heterogeneous Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1905.08924v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.08924v1)
- **Published**: 2019-05-22 02:15:18+00:00
- **Updated**: 2019-05-22 02:15:18+00:00
- **Authors**: Peng Xu, Zhaohong Deng, Kup-Sze Choi, Jun Wang, Shitong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Domain adaptation aims to assist the modeling tasks of the target domain with knowledge of the source domain. The two domains often lie in different feature spaces due to diverse data collection methods, which leads to the more challenging task of heterogeneous domain adaptation (HDA). A core issue of HDA is how to preserve the information of the original data during adaptation. In this paper, we propose a joint information preservation method to deal with the problem. The method preserves the information of the original data from two aspects. On the one hand, although paired samples often exist between the two domains of the HDA, current algorithms do not utilize such information sufficiently. The proposed method preserves the paired information by maximizing the correlation of the paired samples in the shared subspace. On the other hand, the proposed method improves the strategy of preserving the structural information of the original data, where the local and global structural information are preserved simultaneously. Finally, the joint information preservation is integrated by distribution matching. Experimental results show the superiority of the proposed method over the state-of-the-art HDA algorithms.



### Learning Fully Dense Neural Networks for Image Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.08929v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08929v1)
- **Published**: 2019-05-22 02:41:53+00:00
- **Updated**: 2019-05-22 02:41:53+00:00
- **Authors**: Mingmin Zhen, Jinglu Wang, Lei Zhou, Tian Fang, Long Quan
- **Comment**: None
- **Journal**: AAAI 2019
- **Summary**: Semantic segmentation is pixel-wise classification which retains critical spatial information. The "feature map reuse" has been commonly adopted in CNN based approaches to take advantage of feature maps in the early layers for the later spatial reconstruction. Along this direction, we go a step further by proposing a fully dense neural network with an encoder-decoder structure that we abbreviate as FDNet. For each stage in the decoder module, feature maps of all the previous blocks are adaptively aggregated to feed-forward as input. On the one hand, it reconstructs the spatial boundaries accurately. On the other hand, it learns more efficiently with the more efficient gradient backpropagation. In addition, we propose the boundary-aware loss function to focus more attention on the pixels near the boundary, which boosts the "hard examples" labeling. We have demonstrated the best performance of the FDNet on the two benchmark datasets: PASCAL VOC 2012, NYUDv2 over previous works when not considering training on other datasets.



### Domain Adaptation for Vehicle Detection from Bird's Eye View LiDAR Point Cloud Data
- **Arxiv ID**: http://arxiv.org/abs/1905.08955v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.08955v1)
- **Published**: 2019-05-22 05:24:26+00:00
- **Updated**: 2019-05-22 05:24:26+00:00
- **Authors**: Khaled Saleh, Ahmed Abobakr, Mohammed Attia, Julie Iskander, Darius Nahavandi, Mohammed Hossny
- **Comment**: Under review for IEEE SMC 2019
- **Journal**: None
- **Summary**: Point cloud data from 3D LiDAR sensors are one of the most crucial sensor modalities for versatile safety-critical applications such as self-driving vehicles. Since the annotations of point cloud data is an expensive and time-consuming process, therefore recently the utilisation of simulated environments and 3D LiDAR sensors for this task started to get some popularity. With simulated sensors and environments, the process for obtaining an annotated synthetic point cloud data became much easier. However, the generated synthetic point cloud data are still missing the artefacts usually exist in point cloud data from real 3D LiDAR sensors. As a result, the performance of the trained models on this data for perception tasks when tested on real point cloud data is degraded due to the domain shift between simulated and real environments. Thus, in this work, we are proposing a domain adaptation framework for bridging this gap between synthetic and real point cloud data. Our proposed framework is based on the deep cycle-consistent generative adversarial networks (CycleGAN) architecture. We have evaluated the performance of our proposed framework on the task of vehicle detection from a bird's eye view (BEV) point cloud images coming from real 3D LiDAR sensors. The framework has shown competitive results with an improvement of more than 7% in average precision score over other baseline approaches when tested on real BEV point cloud images.



### Segmentation-Aware Image Denoising without Knowing True Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.08965v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08965v1)
- **Published**: 2019-05-22 06:05:36+00:00
- **Updated**: 2019-05-22 06:05:36+00:00
- **Authors**: Sicheng Wang, Bihan Wen, Junru Wu, Dacheng Tao, Zhangyang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Several recent works discussed application-driven image restoration neural networks, which are capable of not only removing noise in images but also preserving their semantic-aware details, making them suitable for various high-level computer vision tasks as the pre-processing step. However, such approaches require extra annotations for their high-level vision tasks, in order to train the joint pipeline using hybrid losses. The availability of those annotations is yet often limited to a few image sets, potentially restricting the general applicability of these methods to denoising more unseen and unannotated images. Motivated by that, we propose a segmentation-aware image denoising model dubbed U-SAID, based on a novel unsupervised approach with a pixel-wise uncertainty loss. U-SAID does not need any ground-truth segmentation map, and thus can be applied to any image dataset. It generates denoised images with comparable or even better quality, and the denoised results show stronger robustness for subsequent semantic segmentation tasks, when compared to either its supervised counterpart or classical "application-agnostic" denoisers. Moreover, we demonstrate the superior generalizability of U-SAID in three-folds, by plugging its "universal" denoiser without fine-tuning: (1) denoising unseen types of images; (2) denoising as pre-processing for segmenting unseen noisy images; and (3) denoising for unseen high-level tasks. Extensive experiments demonstrate the effectiveness, robustness and generalizability of the proposed U-SAID over various popular image sets.



### LapTool-Net: A Contextual Detector of Surgical Tools in Laparoscopic Videos Based on Recurrent Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.08983v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.08983v1)
- **Published**: 2019-05-22 06:55:41+00:00
- **Updated**: 2019-05-22 06:55:41+00:00
- **Authors**: Babak Namazi, Ganesh Sankaranarayanan, Venkat Devarajan
- **Comment**: 18 pages, 4 figures, Submitted to Medical Image Analysis
- **Journal**: None
- **Summary**: We propose a new multilabel classifier, called LapTool-Net to detect the presence of surgical tools in each frame of a laparoscopic video. The novelty of LapTool-Net is the exploitation of the correlation among the usage of different tools and, the tools and tasks - namely, the context of the tools' usage. Towards this goal, the pattern in the co-occurrence of the tools is utilized for designing a decision policy for a multilabel classifier based on a Recurrent Convolutional Neural Network (RCNN) architecture to simultaneously extract the spatio-temporal features. In contrast to the previous multilabel classification methods, the RCNN and the decision model are trained in an end-to-end manner using a multitask learning scheme. To overcome the high imbalance and avoid overfitting caused by the lack of variety in the training data, a high down-sampling rate is chosen based on the more frequent combinations. Furthermore, at the post-processing step, the prediction for all the frames of a video are corrected by designing a bi-directional RNN to model the long-term task's order. LapTool-net was trained using a publicly available dataset of laparoscopic cholecystectomy. The results show LapTool-Net outperforms existing methods significantly, even while using fewer training samples and a shallower architecture.



### Attributes Guided Feature Learning for Vehicle Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1905.08997v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08997v2)
- **Published**: 2019-05-22 07:42:02+00:00
- **Updated**: 2021-11-10 12:01:58+00:00
- **Authors**: Hongchao Li, Xianmin Lin, Aihua Zheng, Chenglong Li, Bin Luo, Ran He, Amir Hussain
- **Comment**: None
- **Journal**: None
- **Summary**: Vehicle Re-ID has recently attracted enthusiastic attention due to its potential applications in smart city and urban surveillance. However, it suffers from large intra-class variation caused by view variations and illumination changes, and inter-class similarity especially for different identities with the similar appearance. To handle these issues, in this paper, we propose a novel deep network architecture, which guided by meaningful attributes including camera views, vehicle types and colors for vehicle Re-ID. In particular, our network is end-to-end trained and contains three subnetworks of deep features embedded by the corresponding attributes (i.e., camera view, vehicle type and vehicle color). Moreover, to overcome the shortcomings of limited vehicle images of different views, we design a view-specified generative adversarial network to generate the multi-view vehicle images. For network training, we annotate the view labels on the VeRi-776 dataset. Note that one can directly adopt the pre-trained view (as well as type and color) subnetwork on the other datasets with only ID information, which demonstrates the generalization of our model. Extensive experiments on the benchmark datasets VeRi-776 and VehicleID suggest that the proposed approach achieves the promising performance and yields to a new state-of-the-art for vehicle Re-ID.



### Underwater Color Restoration Using U-Net Denoising Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1905.09000v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.09000v1)
- **Published**: 2019-05-22 07:49:45+00:00
- **Updated**: 2019-05-22 07:49:45+00:00
- **Authors**: Yousif Hashisho, Mohamad Albadawi, Tom Krause, Uwe Freiherr von Lukas
- **Comment**: 6 pages, 8 figures
- **Journal**: None
- **Summary**: Visual inspection of underwater structures by vehicles, e.g. remotely operated vehicles (ROVs), plays an important role in scientific, military, and commercial sectors. However, the automatic extraction of information using software tools is hindered by the characteristics of water which degrade the quality of captured videos. As a contribution for restoring the color of underwater images, Underwater Denoising Autoencoder (UDAE) model is developed using a denoising autoencoder with U-Net architecture. The proposed network takes into consideration the accuracy and the computation cost to enable real-time implementation on underwater visual tasks using end-to-end autoencoder network. Underwater vehicles perception is improved by reconstructing captured frames; hence obtaining better performance in underwater tasks. Related learning methods use generative adversarial networks (GANs) to generate color corrected underwater images, and to our knowledge this paper is the first to deal with a single autoencoder capable of producing same or better results. Moreover, image pairs are constructed for training the proposed network, where it is hard to obtain such dataset from underwater scenery. At the end, the proposed model is compared to a state-of-the-art method.



### PEPSI++: Fast and Lightweight Network for Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1905.09010v5
- **DOI**: 10.1109/TNNLS.2020.2978501
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.09010v5)
- **Published**: 2019-05-22 08:18:48+00:00
- **Updated**: 2020-03-06 23:33:43+00:00
- **Authors**: Yong-Goo Shin, Min-Cheol Sagong, Yoon-Jae Yeo, Seung-Wook Kim, Sung-Jea Ko
- **Comment**: Accepted to IEEE transactions on Neural Networks and Learning
  Systems. To be published
- **Journal**: None
- **Summary**: Among the various generative adversarial network (GAN)-based image inpainting methods, a coarse-to-fine network with a contextual attention module (CAM) has shown remarkable performance. However, owing to two stacked generative networks, the coarse-to-fine network needs numerous computational resources such as convolution operations and network parameters, which result in low speed. To address this problem, we propose a novel network architecture called PEPSI: parallel extended-decoder path for semantic inpainting network, which aims at reducing the hardware costs and improving the inpainting performance. PEPSI consists of a single shared encoding network and parallel decoding networks called coarse and inpainting paths. The coarse path produces a preliminary inpainting result to train the encoding network for the prediction of features for the CAM. Simultaneously, the inpainting path generates higher inpainting quality using the refined features reconstructed via the CAM. In addition, we propose Diet-PEPSI that significantly reduces the network parameters while maintaining the performance. In Diet-PEPSI, to capture the global contextual information with low hardware costs, we propose novel rate-adaptive dilated convolutional layers, which employ the common weights but produce dynamic features depending on the given dilation rates. Extensive experiments comparing the performance with state-of-the-art image inpainting methods demonstrate that both PEPSI and Diet-PEPSI improve the qualitative scores, i.e. the peak signal-to-noise ratio (PSNR) and structural similarity (SSIM), as well as significantly reduce hardware costs such as computational time and the number of network parameters.



### Spatial Sampling Network for Fast Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/1905.09033v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.09033v1)
- **Published**: 2019-05-22 09:24:17+00:00
- **Updated**: 2019-05-22 09:24:17+00:00
- **Authors**: Davide Mazzini, Raimondo Schettini
- **Comment**: Accepted at CVPR2019 Workshop on Autonomous Driving
- **Journal**: None
- **Summary**: We propose a network architecture to perform efficient scene understanding. This work presents three main novelties: the first is an Improved Guided Upsampling Module that can replace in toto the decoder part in common semantic segmentation networks. Our second contribution is the introduction of a new module based on spatial sampling to perform Instance Segmentation. It provides a very fast instance segmentation, needing only thresholding as post-processing step at inference time. Finally, we propose a novel efficient network design that includes the new modules and test it against different datasets for outdoor scene understanding. To our knowledge, our network is one of the themost efficient architectures for scene understanding published to date, furthermore being 8.6% more accurate than the fastest competitor on semantic segmentation and almost five times faster than the most efficient network for instance segmentation.



### What Would You Expect? Anticipating Egocentric Actions with Rolling-Unrolling LSTMs and Modality Attention
- **Arxiv ID**: http://arxiv.org/abs/1905.09035v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.09035v2)
- **Published**: 2019-05-22 09:25:25+00:00
- **Updated**: 2019-08-05 21:09:08+00:00
- **Authors**: Antonino Furnari, Giovanni Maria Farinella
- **Comment**: Accepted as oral to ICCV [International Conference on Computer
  Vision] 2019
- **Journal**: None
- **Summary**: Egocentric action anticipation consists in understanding which objects the camera wearer will interact with in the near future and which actions they will perform. We tackle the problem proposing an architecture able to anticipate actions at multiple temporal scales using two LSTMs to 1) summarize the past, and 2) formulate predictions about the future. The input video is processed considering three complimentary modalities: appearance (RGB), motion (optical flow) and objects (object-based features). Modality-specific predictions are fused using a novel Modality ATTention (MATT) mechanism which learns to weigh modalities in an adaptive fashion. Extensive evaluations on two large-scale benchmark datasets show that our method outperforms prior art by up to +7% on the challenging EPIC-Kitchens dataset including more than 2500 actions, and generalizes to EGTEA Gaze+. Our approach is also shown to generalize to the tasks of early action recognition and action recognition. Our method is ranked first in the public leaderboard of the EPIC-Kitchens egocentric action anticipation challenge 2019. Please see our web pages for code and examples: http://iplab.dmi.unict.it/rulstm - https://github.com/fpv-iplab/rulstm.



### Robust Motion Segmentation from Pairwise Matches
- **Arxiv ID**: http://arxiv.org/abs/1905.09043v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.09043v1)
- **Published**: 2019-05-22 09:52:22+00:00
- **Updated**: 2019-05-22 09:52:22+00:00
- **Authors**: Federica Arrigoni, Tomas Pajdla
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we address a classification problem that has not been considered before, namely motion segmentation given pairwise matches only. Our contribution to this unexplored task is a novel formulation of motion segmentation as a two-step process. First, motion segmentation is performed on image pairs independently. Secondly, we combine independent pairwise segmentation results in a robust way into the final globally consistent segmentation. Our approach is inspired by the success of averaging methods. We demonstrate in simulated as well as in real experiments that our method is very effective in reducing the errors in the pairwise motion segmentation and can cope with large number of mismatches.



### End-to-End Learned Random Walker for Seeded Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.09045v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.09045v1)
- **Published**: 2019-05-22 09:56:04+00:00
- **Updated**: 2019-05-22 09:56:04+00:00
- **Authors**: Lorenzo Cerrone, Alexander Zeilmann, Fred A. Hamprecht
- **Comment**: None
- **Journal**: None
- **Summary**: We present an end-to-end learned algorithm for seeded segmentation. Our method is based on the Random Walker algorithm, where we predict the edge weights of the underlying graph using a convolutional neural network. This can be interpreted as learning context-dependent diffusivities for a linear diffusion process. Besides calculating the exact gradient for optimizing these diffusivities, we also propose simplifications that sparsely sample the gradient and still yield competitive results. The proposed method achieves the currently best results on a seeded version of the CREMI neuron segmentation challenge.



### Automated Segmentation for Hyperdense Middle Cerebral Artery Sign of Acute Ischemic Stroke on Non-Contrast CT Images
- **Arxiv ID**: http://arxiv.org/abs/1905.09049v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.09049v1)
- **Published**: 2019-05-22 10:07:26+00:00
- **Updated**: 2019-05-22 10:07:26+00:00
- **Authors**: Jia You, Philip L. H. Yu, Anderson C. O. Tsang, Eva L. H. Tsui, Pauline P. S. Woo, Gilberto K. K. Leung
- **Comment**: None
- **Journal**: None
- **Summary**: The hyperdense middle cerebral artery (MCA) dot sign has been reported as an important factor in the diagnosis of acute ischemic stroke due to large vessel occlusion. Interpreting the initial CT brain scan in these patients requires high level of expertise, and has high inter-observer variability. An automated computerized interpretation of the urgent CT brain image, with an emphasis to pick up early signs of ischemic stroke will facilitate early patient diagnosis, triage, and shorten the door-to-revascularization time for these group of patients. In this paper, we present an automated detection method of segmenting the MCA dot sign on non-contrast CT brain image scans based on powerful deep learning technique.



### Beyond Alternating Updates for Matrix Factorization with Inertial Bregman Proximal Gradient Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1905.09050v2
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.IR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.09050v2)
- **Published**: 2019-05-22 10:09:54+00:00
- **Updated**: 2019-12-06 12:33:13+00:00
- **Authors**: Mahesh Chandra Mukkamala, Peter Ochs
- **Comment**: Accepted at NeuRIPS 2019. Paper url:
  http://papers.nips.cc/paper/8679-beyond-alternating-updates-for-matrix-factorization-with-inertial-bregman-proximal-gradient-algorithms
- **Journal**: None
- **Summary**: Matrix Factorization is a popular non-convex optimization problem, for which alternating minimization schemes are mostly used. They usually suffer from the major drawback that the solution is biased towards one of the optimization variables. A remedy is non-alternating schemes. However, due to a lack of Lipschitz continuity of the gradient in matrix factorization problems, convergence cannot be guaranteed. A recently developed approach relies on the concept of Bregman distances, which generalizes the standard Euclidean distance. We exploit this theory by proposing a novel Bregman distance for matrix factorization problems, which, at the same time, allows for simple/closed form update steps. Therefore, for non-alternating schemes, such as the recently introduced Bregman Proximal Gradient (BPG) method and an inertial variant Convex--Concave Inertial BPG (CoCaIn BPG), convergence of the whole sequence to a stationary point is proved for Matrix Factorization. In several experiments, we observe a superior performance of our non-alternating schemes in terms of speed and objective value at the limit point.



### Fine-grained Optimization of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.09054v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.09054v1)
- **Published**: 2019-05-22 10:18:58+00:00
- **Updated**: 2019-05-22 10:18:58+00:00
- **Authors**: Mete Ozay
- **Comment**: None
- **Journal**: None
- **Summary**: In recent studies, several asymptotic upper bounds on generalization errors on deep neural networks (DNNs) are theoretically derived. These bounds are functions of several norms of weights of the DNNs, such as the Frobenius and spectral norms, and they are computed for weights grouped according to either input and output channels of the DNNs. In this work, we conjecture that if we can impose multiple constraints on weights of DNNs to upper bound the norms of the weights, and train the DNNs with these weights, then we can attain empirical generalization errors closer to the derived theoretical bounds, and improve accuracy of the DNNs.   To this end, we pose two problems. First, we aim to obtain weights whose different norms are all upper bounded by a constant number, e.g. 1.0. To achieve these bounds, we propose a two-stage renormalization procedure; (i) normalization of weights according to different norms used in the bounds, and (ii) reparameterization of the normalized weights to set a constant and finite upper bound of their norms. In the second problem, we consider training DNNs with these renormalized weights. To this end, we first propose a strategy to construct joint spaces (manifolds) of weights according to different constraints in DNNs. Next, we propose a fine-grained SGD algorithm (FG-SGD) for optimization on the weight manifolds to train DNNs with assurance of convergence to minima. Experimental results show that image classification accuracy of baseline DNNs can be boosted using FG-SGD on collections of manifolds identified by multiple constraints.



### A Comprehensive Study of ImageNet Pre-Training for Historical Document Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1905.09113v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.09113v1)
- **Published**: 2019-05-22 13:07:00+00:00
- **Updated**: 2019-05-22 13:07:00+00:00
- **Authors**: Linda Studer, Michele Alberti, Vinaychandran Pondenkandath, Pinar Goktepe, Thomas Kolonko, Andreas Fischer, Marcus Liwicki, Rolf Ingold
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic analysis of scanned historical documents comprises a wide range of image analysis tasks, which are often challenging for machine learning due to a lack of human-annotated learning samples. With the advent of deep neural networks, a promising way to cope with the lack of training data is to pre-train models on images from a different domain and then fine-tune them on historical documents. In the current research, a typical example of such cross-domain transfer learning is the use of neural networks that have been pre-trained on the ImageNet database for object recognition. It remains a mostly open question whether or not this pre-training helps to analyse historical documents, which have fundamentally different image properties when compared with ImageNet. In this paper, we present a comprehensive empirical survey on the effect of ImageNet pre-training for diverse historical document analysis tasks, including character recognition, style classification, manuscript dating, semantic segmentation, and content-based retrieval. While we obtain mixed results for semantic segmentation at pixel-level, we observe a clear trend across different network architectures that ImageNet pre-training has a positive effect on classification as well as content-based retrieval.



### A Comparison of Stereo-Matching Cost between Convolutional Neural Network and Census for Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/1905.09147v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.09147v1)
- **Published**: 2019-05-22 14:00:11+00:00
- **Updated**: 2019-05-22 14:00:11+00:00
- **Authors**: Bihe Chen, Rongjun Qin, Xu Huang, Shuang Song, Xiaohu Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Stereo dense image matching can be categorized to low-level feature based matching and deep feature based matching according to their matching cost metrics. Census has been proofed to be one of the most efficient low-level feature based matching methods, while fast Convolutional Neural Network (fst-CNN), as a deep feature based method, has small computing time and is robust for satellite images. Thus, a comparison between fst-CNN and census is critical for further studies in stereo dense image matching. This paper used cost function of fst-CNN and census to do stereo matching, then utilized semi-global matching method to obtain optimized disparity images. Those images are used to produce digital surface model to compare with ground truth points. It addresses that fstCNN performs better than census in the aspect of absolute matching accuracy, histogram of error distribution and matching completeness, but these two algorithms still performs in the same order of magnitude.



### Using Orthophoto for Building Boundary Sharpening in the Digital Surface Model
- **Arxiv ID**: http://arxiv.org/abs/1905.09150v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.09150v1)
- **Published**: 2019-05-22 14:02:58+00:00
- **Updated**: 2019-05-22 14:02:58+00:00
- **Authors**: Xiaohu Lu, Rongjun Qin, Xu Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays dense stereo matching has become one of the dominant tools in 3D reconstruction of urban regions for its low cost and high flexibility in generating dense 3D points. However, state-of-the-art stereo matching algorithms usually apply a semi-global matching (SGM) strategy. This strategy normally assumes the surface geometry pieceswise planar, where a smooth penalty is imposed to deal with non-texture or repeating-texture areas. This on one hand, generates much smooth surface models, while on the other hand, may partially leads to smoothing on depth discontinuities, particularly for fence-shaped regions or densely built areas with narrow streets. To solve this problem, in this work, we propose to use the line segment information extracted from the corresponding orthophoto as a pose-processing tool to sharpen the building boundary of the Digital Surface Model (DSM) generated by SGM. Two methods which are based on graph-cut and plane fitting are proposed and compared. Experimental results on several satellite datasets with ground truth show the robustness and effectiveness of the proposed DSM sharpening method.



### Multi-View Large-Scale Bundle Adjustment Method for High-Resolution Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/1905.09152v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.09152v1)
- **Published**: 2019-05-22 14:06:09+00:00
- **Updated**: 2019-05-22 14:06:09+00:00
- **Authors**: Xu Huang, Rongjun Qin
- **Comment**: None
- **Journal**: None
- **Summary**: Given enough multi-view image corresponding points (also called tie points) and ground control points (GCP), bundle adjustment for high-resolution satellite images is used to refine the orientations or most often used geometric parameters Rational Polynomial Coefficients (RPC) of each satellite image in a unified geodetic framework, which is very critical in many photogrammetry and computer vision applications. However, the growing number of high resolution spaceborne optical sensors has brought two challenges to the bundle adjustment: 1) images come from different satellite cameras may have different imaging dates, viewing angles, resolutions, etc., thus resulting in geometric and radiometric distortions in the bundle adjustment; 2) The large-scale mapping area always corresponds to vast number of bundle adjustment corrections (including RPC bias and object space point coordinates). Due to the limitation of computer memory, it is hard to refine all corrections at the same time. Hence, how to efficiently realize the bundle adjustment in large-scale regions is very important. This paper particularly addresses the multi-view large-scale bundle adjustment problem by two steps: 1) to get robust tie points among different satellite images, we design a multi-view, multi-source tie point matching algorithm based on plane rectification and epipolar constraints, which is able to compensate geometric and local nonlinear radiometric distortions among satellite datasets, and 2) to solve dozens of thousands or even millions of variables bundle adjustment corrections in the large scale bundle adjustment, we use an efficient solution with only a little computer memory. Experiments on in-track and off-track satellite datasets show that the proposed method is capable of computing sub-pixel accuracy bundle adjustment results.



### Spatio-Temporal Deep Learning Models for Tip Force Estimation During Needle Insertion
- **Arxiv ID**: http://arxiv.org/abs/1905.09282v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.09282v1)
- **Published**: 2019-05-22 15:45:50+00:00
- **Updated**: 2019-05-22 15:45:50+00:00
- **Authors**: Nils Gessert, Torben Priegnitz, Thore Saathoff, Sven-Thomas Antoni, David Meyer, Moritz Franz Hamann, Klaus-Peter Jünemann, Christoph Otte, Alexander Schlaefer
- **Comment**: Accepted for publication in the International Journal of Computer
  Assisted Radiology and Surgery
- **Journal**: None
- **Summary**: Purpose. Precise placement of needles is a challenge in a number of clinical applications such as brachytherapy or biopsy. Forces acting at the needle cause tissue deformation and needle deflection which in turn may lead to misplacement or injury. Hence, a number of approaches to estimate the forces at the needle have been proposed. Yet, integrating sensors into the needle tip is challenging and a careful calibration is required to obtain good force estimates.   Methods. We describe a fiber-optical needle tip force sensor design using a single OCT fiber for measurement. The fiber images the deformation of an epoxy layer placed below the needle tip which results in a stream of 1D depth profiles. We study different deep learning approaches to facilitate calibration between this spatio-temporal image data and the related forces. In particular, we propose a novel convGRU-CNN architecture for simultaneous spatial and temporal data processing.   Results. The needle can be adapted to different operating ranges by changing the stiffness of the epoxy layer. Likewise, calibration can be adapted by training the deep learning models. Our novel convGRU-CNN architecture results in the lowest mean absolute error of 1.59 +- 1.3 mN and a cross-correlation coefficient of 0.9997, and clearly outperforms the other methods. Ex vivo experiments in human prostate tissue demonstrate the needle's application.   Conclusions. Our OCT-based fiber-optical sensor presents a viable alternative for needle tip force estimation. The results indicate that the rich spatio-temporal information included in the stream of images showing the deformation throughout the epoxy layer can be effectively used by deep learning models. Particularly, we demonstrate that the convGRU-CNN architecture performs favorably, making it a promising approach for other spatio-temporal learning problems.



### Segmentation-Aware Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1905.09211v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.09211v1)
- **Published**: 2019-05-22 16:03:01+00:00
- **Updated**: 2019-05-22 16:03:01+00:00
- **Authors**: Berkan Demirel, Omer Ozdil, Yunus Emre Esin, Safak Ozturk
- **Comment**: To appear at International Geoscience and Remote Sensing Symposium
  (IGARSS) 2019
- **Journal**: None
- **Summary**: In this paper, we propose an unified hyperspectral image classification method which takes three-dimensional hyperspectral data cube as an input and produces a classification map. In the proposed method, a deep neural network which uses spectral and spatial information together with residual connections, and pixel affinity network based segmentation-aware superpixels are used together. In the architecture, segmentation-aware superpixels run on the initial classification map of deep residual network, and apply majority voting on obtained results. Experimental results show that our propoped method yields state-of-the-art results in two benchmark datasets. Moreover, we also show that the segmentation-aware superpixels have great contribution to the success of hyperspectral image classification methods in cases where training data is insufficient.



### WPU-Net: Boundary Learning by Using Weighted Propagation in Convolution Network
- **Arxiv ID**: http://arxiv.org/abs/1905.09226v2
- **DOI**: 10.1016/j.jocs.2022.101709
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.09226v2)
- **Published**: 2019-05-22 16:23:23+00:00
- **Updated**: 2019-08-30 15:52:09+00:00
- **Authors**: Boyuan Ma, Chuni Liu, Xiaojuan Ban, Hao Wang, Weihua Xue, Haiyou Huang
- **Comment**: technical report
- **Journal**: Journal of Computational Science, 2022
- **Summary**: Deep learning has driven a great progress in natural and biological image processing. However, in material science and engineering, there are often some flaws and indistinctions in material microscopic images induced from complex sample preparation, even due to the material itself, hindering the detection of target objects. In this work, we propose WPU-net that redesigns the architecture and weighted loss of U-Net, which forces the network to integrate information from adjacent slices and pays more attention to the topology in boundary detection task. Then, the WPU-net is applied into a typical material example, i.e., the grain boundary detection of polycrystalline material. Experiments demonstrate that the proposed method achieves promising performance and outperforms state-of-the-art methods. Besides, we propose a new method for object tracking between adjacent slices, which can effectively reconstruct 3D structure of the whole material. Finally, we present a material microscopic image dataset with the goal of advancing the state-of-the-art in image processing for material science.



### Separating Overlapping Tissue Layers from Microscopy Images
- **Arxiv ID**: http://arxiv.org/abs/1905.09231v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.09231v1)
- **Published**: 2019-05-22 16:31:17+00:00
- **Updated**: 2019-05-22 16:31:17+00:00
- **Authors**: Zahra Montazeri, Gopi M
- **Comment**: None
- **Journal**: None
- **Summary**: Manual preparation of tissue slices for microscopy imaging can introduce tissue tears and overlaps. Typically, further digital processing algorithms such as registration and 3D reconstruction from tissue image stacks cannot handle images with tissue tear/overlap artifacts, and so such images are usually discarded. In this paper, we propose an imaging model and an algorithm to digitally separate overlapping tissue data of mouse brain images into two layers. We show the correctness of our model and the algorithm by comparing our results with the ground truth.



### Oculum afficit: Ocular Affect Recognition
- **Arxiv ID**: http://arxiv.org/abs/1905.09240v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.09240v1)
- **Published**: 2019-05-22 16:58:12+00:00
- **Updated**: 2019-05-22 16:58:12+00:00
- **Authors**: Elmar Langholz
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing human affect and emotions is a problem that has a wide range of applications within both academia and industry. Affect and emotion recognition within computer vision primarily relies on images of faces. With the prevalence of portable devices (e.g. smart phones and/or smart glasses),acquiring user facial images requires focus, time, and precision. While existing systems work great for full frontal faces, they tend to not work so well with partial faces like those of the operator of the device when under use. Due to this, we propose a methodology in which we can accurately infer the overall affect of a person by looking at the ocular region of an individual.



### Dual Active Sampling on Batch-Incremental Active Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.09247v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.09247v1)
- **Published**: 2019-05-22 17:11:57+00:00
- **Updated**: 2019-05-22 17:11:57+00:00
- **Authors**: Johan Phan, Massimiliano Ruocco, Francesco Scibilia
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Recently, Convolutional Neural Networks (CNNs) have shown unprecedented success in the field of computer vision, especially on challenging image classification tasks by relying on a universal approach, i.e., training a deep model on a massive dataset of supervised examples. While unlabeled data are often an abundant resource, collecting a large set of labeled data, on the other hand, are very expensive, which often require considerable human efforts. One way to ease out this is to effectively select and label highly informative instances from a pool of unlabeled data (i.e., active learning). This paper proposed a new method of batch-mode active learning, Dual Active Sampling(DAS), which is based on a simple assumption, if two deep neural networks (DNNs) of the same structure and trained on the same dataset give significantly different output for a given sample, then that particular sample should be picked for additional training. While other state of the art methods in this field usually require intensive computational power or relying on a complicated structure, DAS is simpler to implement and, managed to get improved results on Cifar-10 with preferable computational time compared to the core-set method.



### Bridging Stereo Matching and Optical Flow via Spatiotemporal Correspondence
- **Arxiv ID**: http://arxiv.org/abs/1905.09265v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.09265v1)
- **Published**: 2019-05-22 17:51:00+00:00
- **Updated**: 2019-05-22 17:51:00+00:00
- **Authors**: Hsueh-Ying Lai, Yi-Hsuan Tsai, Wei-Chen Chiu
- **Comment**: Accepted in CVPR'19. Code and model are available at:
  https://github.com/lelimite4444/BridgeDepthFlow
- **Journal**: None
- **Summary**: Stereo matching and flow estimation are two essential tasks for scene understanding, spatially in 3D and temporally in motion. Existing approaches have been focused on the unsupervised setting due to the limited resource to obtain the large-scale ground truth data. To construct a self-learnable objective, co-related tasks are often linked together to form a joint framework. However, the prior work usually utilizes independent networks for each task, thus not allowing to learn shared feature representations across models. In this paper, we propose a single and principled network to jointly learn spatiotemporal correspondence for stereo matching and flow estimation, with a newly designed geometric connection as the unsupervised signal for temporally adjacent stereo pairs. We show that our method performs favorably against several state-of-the-art baselines for both unsupervised depth and flow estimation on the KITTI benchmark dataset.



### Data-Efficient Image Recognition with Contrastive Predictive Coding
- **Arxiv ID**: http://arxiv.org/abs/1905.09272v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.09272v3)
- **Published**: 2019-05-22 17:57:49+00:00
- **Updated**: 2020-07-01 11:22:05+00:00
- **Authors**: Olivier J. Hénaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami, Aaron van den Oord
- **Comment**: None
- **Journal**: None
- **Summary**: Human observers can learn to recognize new categories of images from a handful of examples, yet doing so with artificial ones remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable. We therefore revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations. This new implementation produces features which support state-of-the-art linear classification accuracy on the ImageNet dataset. When used as input for non-linear classification with deep neural networks, this representation allows us to use 2-5x less labels than classifiers trained directly on image pixels. Finally, this unsupervised representation substantially improves transfer learning to object detection on the PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet classifiers.



### PoseRBPF: A Rao-Blackwellized Particle Filter for 6D Object Pose Tracking
- **Arxiv ID**: http://arxiv.org/abs/1905.09304v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1905.09304v1)
- **Published**: 2019-05-22 18:01:55+00:00
- **Updated**: 2019-05-22 18:01:55+00:00
- **Authors**: Xinke Deng, Arsalan Mousavian, Yu Xiang, Fei Xia, Timothy Bretl, Dieter Fox
- **Comment**: Accepted to RSS 2019
- **Journal**: None
- **Summary**: Tracking 6D poses of objects from videos provides rich information to a robot in performing different tasks such as manipulation and navigation. In this work, we formulate the 6D object pose tracking problem in the Rao-Blackwellized particle filtering framework, where the 3D rotation and the 3D translation of an object are decoupled. This factorization allows our approach, called PoseRBPF, to efficiently estimate the 3D translation of an object along with the full distribution over the 3D rotation. This is achieved by discretizing the rotation space in a fine-grained manner, and training an auto-encoder network to construct a codebook of feature embeddings for the discretized rotations. As a result, PoseRBPF can track objects with arbitrary symmetries while still maintaining adequate posterior distributions. Our approach achieves state-of-the-art results on two 6D pose estimation benchmarks. A video showing the experiments can be found at https://youtu.be/lE5gjzRKWuA



### Joint learning of cartesian undersampling and reconstruction for accelerated MRI
- **Arxiv ID**: http://arxiv.org/abs/1905.09324v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, eess.SP, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1905.09324v2)
- **Published**: 2019-05-22 18:55:18+00:00
- **Updated**: 2020-04-05 06:56:04+00:00
- **Authors**: Tomer Weiss, Sanketh Vedula, Ortal Senouf, Oleg Michailovich, Michael Zibulevsky, Alex Bronstein
- **Comment**: ICASSP 2020
- **Journal**: None
- **Summary**: Magnetic Resonance Imaging (MRI) is considered today the golden-standard modality for soft tissues. The long acquisition times, however, make it more prone to motion artifacts as well as contribute to the relatively high costs of this examination. Over the years, multiple studies concentrated on designing reduced measurement schemes and image reconstruction schemes for MRI, however, these problems have been so far addressed separately. On the other hand, recent works in optical computational imaging have demonstrated growing success of the simultaneous learning-based design of the acquisition and reconstruction schemes manifesting significant improvement in the reconstruction quality with a constrained time budget. Inspired by these successes, in this work, we propose to learn accelerated MR acquisition schemes (in the form of Cartesian trajectories) jointly with the image reconstruction operator. To this end, we propose an algorithm for training the combined acquisition-reconstruction pipeline end-to-end in a differentiable way. We demonstrate the significance of using the learned Cartesian trajectories at different speed up rates.



### Self-supervised learning of inverse problem solvers in medical imaging
- **Arxiv ID**: http://arxiv.org/abs/1905.09325v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1905.09325v1)
- **Published**: 2019-05-22 18:56:26+00:00
- **Updated**: 2019-05-22 18:56:26+00:00
- **Authors**: Ortal Senouf, Sanketh Vedula, Tomer Weiss, Alex Bronstein, Oleg Michailovich, Michael Zibulevsky
- **Comment**: preprint
- **Journal**: None
- **Summary**: In the past few years, deep learning-based methods have demonstrated enormous success for solving inverse problems in medical imaging. In this work, we address the following question:\textit{Given a set of measurements obtained from real imaging experiments, what is the best way to use a learnable model and the physics of the modality to solve the inverse problem and reconstruct the latent image?} Standard supervised learning based methods approach this problem by collecting data sets of known latent images and their corresponding measurements. However, these methods are often impractical due to the lack of availability of appropriately sized training sets, and, more generally, due to the inherent difficulty in measuring the "groundtruth" latent image. In light of this, we propose a self-supervised approach to training inverse models in medical imaging in the absence of aligned data. Our method only requiring access to the measurements and the forward model at training. We showcase its effectiveness on inverse problems arising in accelerated magnetic resonance imaging (MRI).



### Automating Whole Brain Histology to MRI Registration: Implementation of a Computational Pipeline
- **Arxiv ID**: http://arxiv.org/abs/1905.09339v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/1905.09339v1)
- **Published**: 2019-05-22 19:27:49+00:00
- **Updated**: 2019-05-22 19:27:49+00:00
- **Authors**: Maryana Alegro, Eduardo J. L. Alho, Maria da Graca Morais Martin, Lea Teneholz Grinberg, Helmut Heinsen, Roseli de Deus Lopes, Edson Amaro-Jr, Lilla Zöllei
- **Comment**: None
- **Journal**: None
- **Summary**: Although the latest advances in MRI technology have allowed the acquisition of higher resolution images, reliable delineation of cytoarchitectural or subcortical nuclei boundaries is not possible. As a result, histological images are still required to identify the exact limits of neuroanatomical structures. However, histological processing is associated with tissue distortion and fixation artifacts, which prevent a direct comparison between the two modalities. Our group has previously proposed a histological procedure based on celloidin embedding that reduces the amount of artifacts and yields high quality whole brain histological slices. Celloidin embedded tissue, nevertheless, still bears distortions that must be corrected. We propose a computational pipeline designed to semi-automatically process the celloidin embedded histology and register them to their MRI counterparts. In this paper we report the accuracy of our pipeline in two whole brain volumes from the Brain Bank of the Brazilian Aging Brain Study Group (BBBABSG). Results were assessed by comparison of manual segmentations from two experts in both MRIs and the registered histological volumes. The two whole brain histology/MRI datasets were successfully registered using minimal user interaction. We also point to possible improvements based on recent implementations that could be added to this pipeline, potentially allowing for higher precision and further performance gains.



### Real-time Approximate Bayesian Computation for Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/1905.13307v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.13307v1)
- **Published**: 2019-05-22 20:03:13+00:00
- **Updated**: 2019-05-22 20:03:13+00:00
- **Authors**: Javier Felip, Nilesh Ahuja, David Gómez-Gutiérrez, Omesh Tickoo, Vikash Mansinghka
- **Comment**: None
- **Journal**: None
- **Summary**: Consider scene understanding problems such as predicting where a person is probably reaching, or inferring the pose of 3D objects from depth images, or inferring the probable street crossings of pedestrians at a busy intersection. This paper shows how to solve these problems using Approximate Bayesian Computation. The underlying generative models are built from realistic simulation software, wrapped in a Bayesian error model for the gap between simulation outputs and real data. The simulators are drawn from off-the-shelf computer graphics, video game, and traffic simulation code. The paper introduces two techniques for speeding up inference that can be used separately or in combination. The first is to train neural surrogates of the simulators, using a simple form of domain randomization to make the surrogates more robust to the gap between the simulation and reality. The second is to adaptively discretize the latent variables using a Tree-pyramid approach adapted from computer graphics. This paper also shows performance and accuracy measurements on real-world problems, establishing that it is feasible to solve these problems in real-time.



### AttentionRNN: A Structured Spatial Attention Mechanism
- **Arxiv ID**: http://arxiv.org/abs/1905.09400v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.09400v1)
- **Published**: 2019-05-22 23:13:05+00:00
- **Updated**: 2019-05-22 23:13:05+00:00
- **Authors**: Siddhesh Khandelwal, Leonid Sigal
- **Comment**: None
- **Journal**: None
- **Summary**: Visual attention mechanisms have proven to be integrally important constituent components of many modern deep neural architectures. They provide an efficient and effective way to utilize visual information selectively, which has shown to be especially valuable in multi-modal learning tasks. However, all prior attention frameworks lack the ability to explicitly model structural dependencies among attention variables, making it difficult to predict consistent attention masks. In this paper we develop a novel structured spatial attention mechanism which is end-to-end trainable and can be integrated with any feed-forward convolutional neural network. This proposed AttentionRNN layer explicitly enforces structure over the spatial attention variables by sequentially predicting attention values in the spatial mask in a bi-directional raster-scan and inverse raster-scan order. As a result, each attention value depends not only on local image or contextual information, but also on the previously predicted attention values. Our experiments show consistent quantitative and qualitative improvements on a variety of recognition tasks and datasets; including image categorization, question answering and image generation.



