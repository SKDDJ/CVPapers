# Arxiv Papers in cs.CV on 2016-11-02
### Natural-Parameter Networks: A Class of Probabilistic Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.00448v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1611.00448v1)
- **Published**: 2016-11-02 02:32:05+00:00
- **Updated**: 2016-11-02 02:32:05+00:00
- **Authors**: Hao Wang, Xingjian Shi, Dit-Yan Yeung
- **Comment**: To appear at NIPS 2016
- **Journal**: None
- **Summary**: Neural networks (NN) have achieved state-of-the-art performance in various applications. Unfortunately in applications where training data is insufficient, they are often prone to overfitting. One effective way to alleviate this problem is to exploit the Bayesian approach by using Bayesian neural networks (BNN). Another shortcoming of NN is the lack of flexibility to customize different distributions for the weights and neurons according to the data, as is often done in probabilistic graphical models. To address these problems, we propose a class of probabilistic neural networks, dubbed natural-parameter networks (NPN), as a novel and lightweight Bayesian treatment of NN. NPN allows the usage of arbitrary exponential-family distributions to model the weights and neurons. Different from traditional NN and BNN, NPN takes distributions as input and goes through layers of transformation before producing distributions to match the target output distributions. As a Bayesian treatment, efficient backpropagation (BP) is performed to learn the natural parameters for the distributions over both the weights and neurons. The output distributions of each layer, as byproducts, may be used as second-order representations for the associated tasks such as link prediction. Experiments on real-world datasets show that NPN can achieve state-of-the-art performance.



### Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in the Blanks
- **Arxiv ID**: http://arxiv.org/abs/1611.00454v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1611.00454v1)
- **Published**: 2016-11-02 02:49:44+00:00
- **Updated**: 2016-11-02 02:49:44+00:00
- **Authors**: Hao Wang, Xingjian Shi, Dit-Yan Yeung
- **Comment**: To appear at NIPS 2016
- **Journal**: None
- **Summary**: Hybrid methods that utilize both content and rating information are commonly used in many recommender systems. However, most of them use either handcrafted features or the bag-of-words representation as a surrogate for the content information but they are neither effective nor natural enough. To address this problem, we develop a collaborative recurrent autoencoder (CRAE) which is a denoising recurrent autoencoder (DRAE) that models the generation of content sequences in the collaborative filtering (CF) setting. The model generalizes recent advances in recurrent deep learning from i.i.d. input to non-i.i.d. (CF-based) input and provides a new denoising scheme along with a novel learnable pooling scheme for the recurrent autoencoder. To do this, we first develop a hierarchical Bayesian model for the DRAE and then generalize it to the CF setting. The synergy between denoising and CF enables CRAE to make accurate recommendations while learning to fill in the blanks in sequences. Experiments on real-world datasets from different domains (CiteULike and Netflix) show that, by jointly modeling the order-aware generation of sequences for the content information and performing CF for the ratings, CRAE is able to significantly outperform the state of the art on both the recommendation task based on ratings and the sequence generation task based on content information.



### CRF-CNN: Modeling Structured Information in Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1611.00468v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.00468v1)
- **Published**: 2016-11-02 04:42:40+00:00
- **Updated**: 2016-11-02 04:42:40+00:00
- **Authors**: Xiao Chu, Wanli Ouyang, Hongsheng Li, Xiaogang Wang
- **Comment**: NIPS
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNN) have achieved great success. On the other hand, modeling structural information has been proved critical in many vision problems. It is of great interest to integrate them effectively. In a classical neural network, there is no message passing between neurons in the same layer. In this paper, we propose a CRF-CNN framework which can simultaneously model structural information in both output and hidden feature layers in a probabilistic way, and it is applied to human pose estimation. A message passing scheme is proposed, so that in various layers each body joint receives messages from all the others in an efficient way. Such message passing can be implemented with convolution between features maps in the same layer, and it is also integrated with feedforward propagation in neural networks. Finally, a neural network implementation of end-to-end learning CRF-CNN is provided. Its effectiveness is demonstrated through experiments on two benchmark datasets.



### Dual Attention Networks for Multimodal Reasoning and Matching
- **Arxiv ID**: http://arxiv.org/abs/1611.00471v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.00471v2)
- **Published**: 2016-11-02 05:17:03+00:00
- **Updated**: 2017-03-21 05:12:02+00:00
- **Authors**: Hyeonseob Nam, Jung-Woo Ha, Jeonghee Kim
- **Comment**: None
- **Journal**: None
- **Summary**: We propose Dual Attention Networks (DANs) which jointly leverage visual and textual attention mechanisms to capture fine-grained interplay between vision and language. DANs attend to specific regions in images and words in text through multiple steps and gather essential information from both modalities. Based on this framework, we introduce two types of DANs for multimodal reasoning and matching, respectively. The reasoning model allows visual and textual attentions to steer each other during collaborative inference, which is useful for tasks such as Visual Question Answering (VQA). In addition, the matching model exploits the two attention mechanisms to estimate the similarity between images and sentences by focusing on their shared semantics. Our extensive experiments validate the effectiveness of DANs in combining vision and language, achieving the state-of-the-art performance on public benchmarks for VQA and image-text matching.



### Wearable Vision Detection of Environmental Fall Risks using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.00684v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.00684v1)
- **Published**: 2016-11-02 16:54:39+00:00
- **Updated**: 2016-11-02 16:54:39+00:00
- **Authors**: Mina Nouredanesh, Andrew McCormick, Sunil L. Kukreja, James Tung
- **Comment**: Accepted paper-The 38th Annual International Conference of the IEEE
  Engineering in Medicine and Biology Society (EMBC 2016)
- **Journal**: None
- **Summary**: In this paper, a method to detect environmental hazards related to a fall risk using a mobile vision system is proposed. First-person perspective videos are proposed to provide objective evidence on cause and circumstances of perturbed balance during activities of daily living, targeted to seniors. A classification problem was defined with 12 total classes of potential fall risks, including slope changes (e.g., stairs, curbs, ramps) and surfaces (e.g., gravel, grass, concrete). Data was collected using a chest-mounted GoPro camera. We developed a convolutional neural network for automatic feature extraction, reduction, and classification of frames. Initial results, with a mean square error of 8%, are promising.



### Bayesian Modeling of Motion Perception using Dynamical Stochastic Textures
- **Arxiv ID**: http://arxiv.org/abs/1611.01390v2
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1611.01390v2)
- **Published**: 2016-11-02 21:20:03+00:00
- **Updated**: 2018-08-21 21:02:49+00:00
- **Authors**: Jonathan Vacher, Andrew Isaac Meso, Laurent U. Perrinet, Gabriel Peyr√©
- **Comment**: article+supplementary, 34+5 pages, 10+1 figures, accepted to Neural
  Computation. arXiv admin note: text overlap with arXiv:1511.02705
- **Journal**: None
- **Summary**: A common practice to account for psychophysical biases in vision is to frame them as consequences of a dynamic process relying on optimal inference with respect to a generative model. The present study details the complete formulation of such a generative model intended to probe visual motion perception with a dynamic texture model. It is first derived in a set of axiomatic steps constrained by biological plausibility. We extend previous contributions by detailing three equivalent formulations of this texture model. First, the composite dynamic textures are constructed by the random aggregation of warped patterns, which can be viewed as 3D Gaussian fields. Secondly, these textures are cast as solutions to a stochastic partial differential equation (sPDE). This essential step enables real time, on-the-fly texture synthesis using time-discretized auto-regressive processes. It also allows for the derivation of a local motion-energy model, which corresponds to the log-likelihood of the probability density. The log-likelihoods are essential for the construction of a Bayesian inference framework. We use the dynamic texture model to psychophysically probe speed perception in humans using zoom-like changes in the spatial frequency content of the stimulus. The human data replicates previous findings showing perceived speed to be positively biased by spatial frequency increments. A Bayesian observer who combines a Gaussian likelihood centered at the true speed and a spatial frequency dependent width with a "slow speed prior" successfully accounts for the perceptual bias. More precisely, the bias arises from a decrease in the observer's likelihood width estimated from the experiments as the spatial frequency increases. Such a trend is compatible with the trend of the dynamic texture likelihood width.



### Learning Deep Embeddings with Histogram Loss
- **Arxiv ID**: http://arxiv.org/abs/1611.00822v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.00822v1)
- **Published**: 2016-11-02 21:48:32+00:00
- **Updated**: 2016-11-02 21:48:32+00:00
- **Authors**: Evgeniya Ustinova, Victor Lempitsky
- **Comment**: NIPS 2016
- **Journal**: None
- **Summary**: We suggest a loss for learning deep embeddings. The new loss does not introduce parameters that need to be tuned and results in very good embeddings across a range of datasets and problems. The loss is computed by estimating two distribution of similarities for positive (matching) and negative (non-matching) sample pairs, and then computing the probability of a positive pair to have a lower similarity score than a negative pair based on the estimated similarity distributions. We show that such operations can be performed in a simple and piecewise-differentiable manner using 1D histograms with soft assignment operations. This makes the proposed loss suitable for learning deep embeddings using stochastic optimization. In the experiments, the new loss performs favourably compared to recently proposed alternatives.



### Initialization and Coordinate Optimization for Multi-way Matching
- **Arxiv ID**: http://arxiv.org/abs/1611.00838v5
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1611.00838v5)
- **Published**: 2016-11-02 23:12:05+00:00
- **Updated**: 2019-07-18 05:34:20+00:00
- **Authors**: Da Tang, Tony Jebara
- **Comment**: Artificial Intelligence and Statistics (AISTATS), 2017
- **Journal**: None
- **Summary**: We consider the problem of consistently matching multiple sets of elements to each other, which is a common task in fields such as computer vision. To solve the underlying NP-hard objective, existing methods often relax or approximate it, but end up with unsatisfying empirical performance due to a misaligned objective. We propose a coordinate update algorithm that directly optimizes the target objective. By using pairwise alignment information to build an undirected graph and initializing the permutation matrices along the edges of its Maximum Spanning Tree, our algorithm successfully avoids bad local optima. Theoretically, with high probability our algorithm guarantees an optimal solution under reasonable noise assumptions. Empirically, our algorithm consistently and significantly outperforms existing methods on several benchmark tasks on real datasets.



### Deep Convolutional Neural Network Design Patterns
- **Arxiv ID**: http://arxiv.org/abs/1611.00847v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1611.00847v3)
- **Published**: 2016-11-02 23:48:04+00:00
- **Updated**: 2016-11-14 14:10:41+00:00
- **Authors**: Leslie N. Smith, Nicholay Topin
- **Comment**: Submitted as a conference paper at ICLR 2017
- **Journal**: None
- **Summary**: Recent research in the deep learning field has produced a plethora of new architectures. At the same time, a growing number of groups are applying deep learning to new applications. Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet). Here we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures. In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files is available at https://github.com/iPhysicist/CNNDesignPatterns). We hope others are inspired to build on our preliminary work.



