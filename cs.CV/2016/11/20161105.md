# Arxiv Papers in cs.CV on 2016-11-05
### Efficient Branching Cascaded Regression for Face Alignment under Significant Head Rotation
- **Arxiv ID**: http://arxiv.org/abs/1611.01584v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.01584v2)
- **Published**: 2016-11-05 01:42:39+00:00
- **Updated**: 2016-11-10 04:53:39+00:00
- **Authors**: Brandon M. Smith, Charles R. Dyer
- **Comment**: None
- **Journal**: None
- **Summary**: Despite much interest in face alignment in recent years, the large majority of work has focused on near-frontal faces. Algorithms typically break down on profile faces, or are too slow for real-time applications. In this work we propose an efficient approach to face alignment that can handle 180 degrees of head rotation in a unified way (e.g., without resorting to view-based models) using 2D training data. The foundation of our approach is cascaded shape regression (CSR), which has emerged recently as the leading strategy. We propose a generalization of conventional CSRs that we call branching cascaded regression (BCR). Conventional CSRs are single-track; that is, they progress from one cascade level to the next in a straight line, with each regressor attempting to fit the entire dataset. We instead split the regression problem into two or more simpler ones after each cascade level. Intuitively, each regressor can then operate on a simpler objective function (i.e., with fewer conflicting gradient directions). Within the BCR framework, we model and infer pose-related landmark visibility and face shape simultaneously using Structured Point Distribution Models (SPDMs). We propose to learn task-specific feature mapping functions that are adaptive to landmark visibility, and that use SPDM parameters as regression targets instead of 2D landmark coordinates. Additionally, we introduce a new in-the-wild dataset of profile faces to validate our approach.



### LipNet: End-to-End Sentence-level Lipreading
- **Arxiv ID**: http://arxiv.org/abs/1611.01599v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1611.01599v2)
- **Published**: 2016-11-05 04:05:18+00:00
- **Updated**: 2016-12-16 16:09:34+00:00
- **Authors**: Yannis M. Assael, Brendan Shillingford, Shimon Whiteson, Nando de Freitas
- **Comment**: None
- **Journal**: None
- **Summary**: Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).



### Robustly representing uncertainty in deep neural networks through sampling
- **Arxiv ID**: http://arxiv.org/abs/1611.01639v7
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1611.01639v7)
- **Published**: 2016-11-05 12:32:16+00:00
- **Updated**: 2018-01-20 13:44:32+00:00
- **Authors**: Patrick McClure, Nikolaus Kriegeskorte
- **Comment**: Bayesian Deep Learning Workshop (NIPS 2017)
- **Journal**: None
- **Summary**: As deep neural networks (DNNs) are applied to increasingly challenging problems, they will need to be able to represent their own uncertainty. Modeling uncertainty is one of the key features of Bayesian methods. Using Bernoulli dropout with sampling at prediction time has recently been proposed as an efficient and well performing variational inference method for DNNs. However, sampling from other multiplicative noise based variational distributions has not been investigated in depth. We evaluated Bayesian DNNs trained with Bernoulli or Gaussian multiplicative masking of either the units (dropout) or the weights (dropconnect). We tested the calibration of the probabilistic predictions of Bayesian convolutional neural networks (CNNs) on MNIST and CIFAR-10. Sampling at prediction time increased the calibration of the DNNs' probabalistic predictions. Sampling weights, whether Gaussian or Bernoulli, led to more robust representation of uncertainty compared to sampling of units. However, using either Gaussian or Bernoulli dropout led to increased test set classification accuracy. Based on these findings we used both Bernoulli dropout and Gaussian dropconnect concurrently, which we show approximates the use of a spike-and-slab variational distribution without increasing the number of learned parameters. We found that spike-and-slab sampling had higher test set performance than Gaussian dropconnect and more robustly represented its uncertainty compared to Bernoulli dropout.



### What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?
- **Arxiv ID**: http://arxiv.org/abs/1611.01640v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.01640v1)
- **Published**: 2016-11-05 12:44:40+00:00
- **Updated**: 2016-11-05 12:44:40+00:00
- **Authors**: Jiedong Hao, Jing Dong, Wei Wang, Tieniu Tan
- **Comment**: The verison submitted to ICLR
- **Journal**: None
- **Summary**: Previous work has shown that feature maps of deep convolutional neural networks (CNNs) can be interpreted as feature representation of a particular image region. Features aggregated from these feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in recent years. The key to the success of such methods is the feature representation. However, the different factors that impact the effectiveness of features are still not explored thoroughly. There are much less discussion about the best combination of them.   The main contribution of our paper is the thorough evaluations of the various factors that affect the discriminative ability of the features extracted from CNNs. Based on the evaluation results, we also identify the best choices for different factors and propose a new multi-scale image feature representation method to encode the image effectively. Finally, we show that the proposed method generalises well and outperforms the state-of-the-art methods on four typical datasets used for visual instance retrieval.



### GPU-based Pedestrian Detection for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1611.01642v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.01642v1)
- **Published**: 2016-11-05 12:47:32+00:00
- **Updated**: 2016-11-05 12:47:32+00:00
- **Authors**: Victor Campmany, Sergio Silva, Antonio Espinosa, Juan Carlos Moure, David Vázquez, Antonio M. López
- **Comment**: 10 pages
- **Journal**: International Conference on Computational Science 2016 Volume 80
  Pages 2377 to 2381
- **Summary**: We propose a real-time pedestrian detection system for the embedded Nvidia Tegra X1 GPU-CPU hybrid platform. The pipeline is composed by the following state-of-the-art algorithms: Histogram of Local Binary Patterns (LBP) and Histograms of Oriented Gradients (HOG) features extracted from the input image; Pyramidal Sliding Window technique for candidate generation; and Support Vector Machine (SVM) for classification. Results show a 8x speedup in the target Tegra X1 platform and a better performance/watt ratio than desktop CUDA platforms in study.



### Boosting Image Captioning with Attributes
- **Arxiv ID**: http://arxiv.org/abs/1611.01646v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.01646v1)
- **Published**: 2016-11-05 13:12:29+00:00
- **Updated**: 2016-11-05 13:12:29+00:00
- **Authors**: Ting Yao, Yingwei Pan, Yehao Li, Zhaofan Qiu, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. To incorporate attributes, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework achieves superior results when compared to state-of-the-art deep models. Most remarkably, we obtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and publicly available splits in (Karpathy & Fei-Fei, 2015) when extracting image representations by GoogleNet and achieve to date top-1 performance on COCO captioning Leaderboard.



### End-to-end Optimized Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1611.01704v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1611.01704v3)
- **Published**: 2016-11-05 21:39:53+00:00
- **Updated**: 2017-03-03 14:53:13+00:00
- **Authors**: Johannes Ballé, Valero Laparra, Eero P. Simoncelli
- **Comment**: Published as a conference paper at ICLR 2017
- **Journal**: Presented at: Int'l Conf on Learning Representations, Toulon,
  France, April 2017
- **Summary**: We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.



