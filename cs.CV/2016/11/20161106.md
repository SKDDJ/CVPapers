# Arxiv Papers in cs.CV on 2016-11-06
### Validation of Tsallis Entropy In Inter-Modality Neuroimage Registration
- **Arxiv ID**: http://arxiv.org/abs/1611.01730v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.01730v1)
- **Published**: 2016-11-06 05:56:12+00:00
- **Updated**: 2016-11-06 05:56:12+00:00
- **Authors**: Henrique Tomaz Amaral-Silva, Luiz Otavio Murta-Jr, Paulo Mazzoncini de Azevedo-Marques, Lauro Wichert-Ana, V. B. Surya Prasath, Colin Studholme
- **Comment**: 15 pages, 11 figures, 2 tables
- **Journal**: None
- **Summary**: Medical image registration plays an important role in determining topographic and morphological changes for functional diagnostic and therapeutic purposes. Manual alignment and semi-automated software still have been used; however they are subjective and make specialists spend precious time. Fully automated methods are faster and user-independent, but the critical point is registration reliability. Similarity measurement using Mutual Information (MI) with Shannon entropy (MIS) is the most common automated method that is being currently applied in medical images, although more reliable algorithms have been proposed over the last decade, suggesting improvements and different entropies; such as Studholme et al, (1999), who demonstrated that the normalization of Mutual Information (NMI) provides an invariant entropy measure for 3D medical image registration. In this paper, we described a set of experiments to evaluate the applicability of Tsallis entropy in the Mutual Information (MIT) and in the Normalized Mutual Information (NMIT) as cost functions for Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET) and Computed Tomography (CT) exams registration. The effect of changing overlap in a simple image model and clinical experiments on current entropies (Entropy Correlation Coefficient - ECC, MIS and NMI) and the proposed ones (MIT and NMT) showed NMI and NMIT with Tsallis parameter close to 1 as the best options (confidence and accuracy) for CT to MRI and PET to MRI automatic neuroimaging registration.



### Deep Label Distribution Learning with Label Ambiguity
- **Arxiv ID**: http://arxiv.org/abs/1611.01731v2
- **DOI**: 10.1109/TIP.2017.2689998
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.01731v2)
- **Published**: 2016-11-06 06:46:50+00:00
- **Updated**: 2017-05-10 13:47:26+00:00
- **Authors**: Bin-Bin Gao, Chao Xing, Chen-Wei Xie, Jianxin Wu, Xin Geng
- **Comment**: Accepted by IEEE TIP 2017. Projects page, see
  http://lamda.nju.edu.cn/gaobb/projects/DLDL.html
- **Journal**: IEEE Transactions on Image Processing 26(6), 2017:2825-2838
- **Summary**: Convolutional Neural Networks (ConvNets) have achieved excellent recognition performance in various visual recognition tasks. A large labeled training set is one of the most important factors for its success. However, it is difficult to collect sufficient training images with precise labels in some domains such as apparent age estimation, head pose estimation, multi-label classification and semantic segmentation. Fortunately, there is ambiguous information among labels, which makes these tasks different from traditional classification. Based on this observation, we convert the label of each image into a discrete label distribution, and learn the label distribution by minimizing a Kullback-Leibler divergence between the predicted and ground-truth label distributions using deep ConvNets. The proposed DLDL (Deep Label Distribution Learning) method effectively utilizes the label ambiguity in both feature learning and classifier learning, which help prevent the network from over-fitting even when the training set is small. Experimental results show that the proposed approach produces significantly better results than state-of-the-art methods for age estimation and head pose estimation. At the same time, it also improves recognition performance for multi-label classification and semantic segmentation tasks.



### Deep Convolutional Neural Network Features and the Original Image
- **Arxiv ID**: http://arxiv.org/abs/1611.01751v1
- **DOI**: 10.1109/FG.2017.85
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.01751v1)
- **Published**: 2016-11-06 10:32:37+00:00
- **Updated**: 2016-11-06 10:32:37+00:00
- **Authors**: Connor J. Parde, Carlos Castillo, Matthew Q. Hill, Y. Ivette Colon, Swami Sankaranarayanan, Jun-Cheng Chen, Alice J. O'Toole
- **Comment**: Submitted to Face and Gesture Conference, 2017
- **Journal**: None
- **Summary**: Face recognition algorithms based on deep convolutional neural networks (DCNNs) have made progress on the task of recognizing faces in unconstrained viewing conditions. These networks operate with compact feature-based face representations derived from learning a very large number of face images. While the learned features produced by DCNNs can be highly robust to changes in viewpoint, illumination, and appearance, little is known about the nature of the face code that emerges at the top level of such networks. We analyzed the DCNN features produced by two face recognition algorithms. In the first set of experiments we used the top-level features from the DCNNs as input into linear classifiers aimed at predicting metadata about the images. The results show that the DCNN features contain surprisingly accurate information about the yaw and pitch of a face, and about whether the face came from a still image or a video frame. In the second set of experiments, we measured the extent to which individual DCNN features operated in a view-dependent or view-invariant manner. We found that view-dependent coding was a characteristic of the identities rather than the DCNN features - with some identities coded consistently in a view-dependent way and others in a view-independent way. In our third analysis, we visualized the DCNN feature space for over 24,000 images of 500 identities. Images in the center of the space were uniformly of low quality (e.g., extreme views, face occlusion, low resolution). Image quality increased monotonically as a function of distance from the origin. This result suggests that image quality information is available in the DCNN features, such that consistently average feature values reflect coding failures that reliably indicate poor or unusable images. Combined, the results offer insight into the coding mechanisms that support robust representation of faces in DCNNs.



### The Shallow End: Empowering Shallower Deep-Convolutional Networks through Auxiliary Outputs
- **Arxiv ID**: http://arxiv.org/abs/1611.01773v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.01773v6)
- **Published**: 2016-11-06 13:20:06+00:00
- **Updated**: 2020-02-16 04:18:25+00:00
- **Authors**: Yong Guo, Jian Chen, Qing Du, Anton Van Den Hengel, Qinfeng Shi, Mingkui Tan
- **Comment**: None
- **Journal**: None
- **Summary**: Depth is one of the key factors behind the success of convolutional neural networks (CNNs). Since ResNet, we are able to train very deep CNNs as the gradient vanishing issue has been largely addressed by the introduction of skip connections. However, we observe that, when the depth is very large, the intermediate layers (especially shallow layers) may fail to receive sufficient supervision from the loss due to the severe transformation through a long backpropagation path. As a result, the representation power of intermediate layers can be very weak and the model becomes very redundant with limited performance. In this paper, we first investigate the supervision vanishing issue in existing backpropagation (BP) methods. And then, we propose to address it via an effective method, called Multi-way BP (MW-BP), which relies on multiple auxiliary losses added to the intermediate layers of the network. The proposed MW-BP method can be applied to most deep architectures with slight modifications, such as ResNet and MobileNet. Our method often gives rise to much more compact models (denoted by "Mw+Architecture") than existing methods. For example, MwResNet-44 with 44 layers performs better than ResNet-110 with 110 layers on CIFAR-10 and CIFAR-100. More critically, the resultant models even outperform the light models obtained by state-of-the-art model compression methods. Last, our method inherently produces multiple compact models with different depths at the same time, which is helpful for model selection.



### Learning to Act by Predicting the Future
- **Arxiv ID**: http://arxiv.org/abs/1611.01779v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1611.01779v2)
- **Published**: 2016-11-06 13:45:00+00:00
- **Updated**: 2017-02-14 19:47:46+00:00
- **Authors**: Alexey Dosovitskiy, Vladlen Koltun
- **Comment**: Published as a conference paper at ICLR 2017
- **Journal**: None
- **Summary**: We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The presented formulation enables learning without a fixed goal at training time, and pursuing dynamically changing goals at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments.



### Learning to Perform Physics Experiments via Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1611.01843v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG, cs.NE, physics.soc-ph
- **Links**: [PDF](http://arxiv.org/pdf/1611.01843v3)
- **Published**: 2016-11-06 20:55:19+00:00
- **Updated**: 2017-08-17 19:51:29+00:00
- **Authors**: Misha Denil, Pulkit Agrawal, Tejas D Kulkarni, Tom Erez, Peter Battaglia, Nando de Freitas
- **Comment**: None
- **Journal**: None
- **Summary**: When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that state of art deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.



