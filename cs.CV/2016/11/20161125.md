# Arxiv Papers in cs.CV on 2016-11-25
### A Distance Function for Comparing Straight-Edge Geometric Figures
- **Arxiv ID**: http://arxiv.org/abs/1612.01400v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, 65D10 (primary), 51K05 (secondary)
- **Links**: [PDF](http://arxiv.org/pdf/1612.01400v1)
- **Published**: 2016-11-25 01:19:55+00:00
- **Updated**: 2016-11-25 01:19:55+00:00
- **Authors**: Apoorva Honnegowda Roopa, Shrisha Rao
- **Comment**: 29 pages, 12 figures including appendices
- **Journal**: None
- **Summary**: This paper defines a distance function that measures the dissimilarity between planar geometric figures formed with straight lines. This function can in turn be used in partial matching of different geometric figures. For a given pair of geometric figures that are graphically isomorphic, one function measures the angular dissimilarity and another function measures the edge length disproportionality. The distance function is then defined as the convex sum of these two functions. The novelty of the presented function is that it satisfies all properties of a distance function and the computation of the same is done by projecting appropriate features to a cartesian plane. To compute the deviation from the angular similarity property, the Euclidean distance between the given angular pairs and the corresponding points on the $y=x$ line is measured. Further while computing the deviation from the edge length proportionality property, the best fit line, for the set of edge lengths, which passes through the origin is found, and the Euclidean distance between the given edge length pairs and the corresponding point on a $y=mx$ line is calculated. Iterative Proportional Fitting Procedure (IPFP) is used to find this best fit line. We demonstrate the behavior of the defined function for some sample pairs of figures.



### Learning an Invariant Hilbert Space for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1611.08350v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08350v2)
- **Published**: 2016-11-25 04:26:35+00:00
- **Updated**: 2017-04-17 09:12:00+00:00
- **Authors**: Samitha Herath, Mehrtash Harandi, Fatih Porikli
- **Comment**: 24 pages, 7 figures
- **Journal**: None
- **Summary**: This paper introduces a learning scheme to construct a Hilbert space (i.e., a vector space along its inner product) to address both unsupervised and semi-supervised domain adaptation problems. This is achieved by learning projections from each domain to a latent space along the Mahalanobis metric of the latent space to simultaneously minimizing a notion of domain variance while maximizing a measure of discriminatory power. In particular, we make use of the Riemannian optimization techniques to match statistical properties (e.g., first and second order statistics) between samples projected into the latent space from different domains. Upon availability of class labels, we further deem samples sharing the same label to form more compact clusters while pulling away samples coming from different classes.We extensively evaluate and contrast our proposal against state-of-the-art methods for the task of visual domain adaptation using both handcrafted and deep-net features. Our experiments show that even with a simple nearest neighbor classifier, the proposed method can outperform several state-of-the-art methods benefitting from more involved classification schemes.



### Deep Video Deblurring
- **Arxiv ID**: http://arxiv.org/abs/1611.08387v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08387v1)
- **Published**: 2016-11-25 08:51:51+00:00
- **Updated**: 2016-11-25 08:51:51+00:00
- **Authors**: Shuochen Su, Mauricio Delbracio, Jue Wang, Guillermo Sapiro, Wolfgang Heidrich, Oliver Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Motion blur from camera shake is a major problem in videos captured by hand-held devices. Unlike single-image deblurring, video-based approaches can take advantage of the abundant information that exists across neighboring frames. As a result the best performing methods rely on aligning nearby frames. However, aligning images is a computationally expensive and fragile procedure, and methods that aggregate information must therefore be able to identify which regions have been accurately aligned and which have not, a task which requires high level scene understanding. In this work, we introduce a deep learning solution to video deblurring, where a CNN is trained end-to-end to learn how to accumulate information across frames. To train this network, we collected a dataset of real videos recorded with a high framerate camera, which we use to generate synthetic motion blur for supervision. We show that the features learned from this dataset extend to deblurring motion blur that arises due to camera shake in a wide range of videos, and compare the quality of results to a number of other baselines.



### Color Constancy with Derivative Colors
- **Arxiv ID**: http://arxiv.org/abs/1611.08389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08389v1)
- **Published**: 2016-11-25 09:06:18+00:00
- **Updated**: 2016-11-25 09:06:18+00:00
- **Authors**: Huan Lei, Guang Jiang, Long Quan
- **Comment**: None
- **Journal**: None
- **Summary**: Information about the illuminant color is well contained in both achromatic regions and the specular components of highlight regions. In this paper, we propose a novel way to achieve color constancy by exploiting such clues. The key to our approach lies in the use of suitably extracted derivative colors, which are able to compute the illuminant color robustly with kernel density estimation. While extracting derivative colors from achromatic regions to approximate the illuminant color well is basically straightforward, the success of our extraction in highlight regions is attributed to the different rates of variation of the diffuse and specular magnitudes in the dichromatic reflection model. The proposed approach requires no training phase and is simple to implement. More significantly, it performs quite satisfactorily under inter-database parameter settings. Our experiments on three standard databases demonstrate its effectiveness and fine performance in comparison to state-of-the-art methods.



### Geometric deep learning on graphs and manifolds using mixture model CNNs
- **Arxiv ID**: http://arxiv.org/abs/1611.08402v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08402v3)
- **Published**: 2016-11-25 10:05:03+00:00
- **Updated**: 2016-12-06 21:38:12+00:00
- **Authors**: Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodolà, Jan Svoboda, Michael M. Bronstein
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has achieved a remarkable performance breakthrough in several fields, most notably in speech recognition, natural language processing, and computer vision. In particular, convolutional neural network (CNN) architectures currently produce state-of-the-art performance on a variety of image analysis tasks such as object detection and recognition. Most of deep learning research has so far focused on dealing with 1D, 2D, or 3D Euclidean-structured data such as acoustic signals, images, or videos. Recently, there has been an increasing interest in geometric deep learning, attempting to generalize deep learning methods to non-Euclidean structured data such as graphs and manifolds, with a variety of applications from the domains of network analysis, computational social science, or computer graphics. In this paper, we propose a unified framework allowing to generalize CNN architectures to non-Euclidean domains (graphs and manifolds) and learn local, stationary, and compositional task-specific features. We show that various non-Euclidean CNN methods previously proposed in the literature can be considered as particular instances of our framework. We test the proposed method on standard tasks from the realms of image-, graph- and 3D shape analysis and show that it consistently outperforms previous approaches.



### Semantic Segmentation using Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.08408v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08408v1)
- **Published**: 2016-11-25 10:36:30+00:00
- **Updated**: 2016-11-25 10:36:30+00:00
- **Authors**: Pauline Luc, Camille Couprie, Soumith Chintala, Jakob Verbeek
- **Comment**: None
- **Journal**: NIPS Workshop on Adversarial Training, Dec 2016, Barcelona, Spain
- **Summary**: Adversarial training has been shown to produce state of the art results for generative image modeling. In this paper we propose an adversarial training approach to train semantic segmentation models. We train a convolutional semantic segmentation network along with an adversarial network that discriminates segmentation maps coming either from the ground truth or from the segmentation network. The motivation for our approach is that it can detect and correct higher-order inconsistencies between ground truth segmentation maps and the ones produced by the segmentation net. Our experiments show that our adversarial training approach leads to improved accuracy on the Stanford Background and PASCAL VOC 2012 datasets.



### Discriminative Correlation Filter with Channel and Spatial Reliability
- **Arxiv ID**: http://arxiv.org/abs/1611.08461v3
- **DOI**: 10.1007/s11263-017-1061-3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08461v3)
- **Published**: 2016-11-25 14:18:52+00:00
- **Updated**: 2019-01-14 08:05:50+00:00
- **Authors**: Alan Lukežič, Tomáš Vojíř, Luka Čehovin, Jiří Matas, Matej Kristan
- **Comment**: Accepted to: International Journal of Computer Vision:
  https://link.springer.com/article/10.1007/s11263-017-1061-3
- **Journal**: None
- **Summary**: Short-term tracking is an open and challenging problem for which discriminative correlation filters (DCF) have shown excellent performance. We introduce the channel and spatial reliability concepts to DCF tracking and provide a novel learning algorithm for its efficient and seamless integration in the filter update and the tracking process. The spatial reliability map adjusts the filter support to the part of the object suitable for tracking. This both allows to enlarge the search region and improves tracking of non-rectangular objects. Reliability scores reflect channel-wise quality of the learned filters and are used as feature weighting coefficients in localization. Experimentally, with only two simple standard features, HoGs and Colornames, the novel CSR-DCF method -- DCF with Channel and Spatial Reliability -- achieves state-of-the-art results on VOT 2016, VOT 2015 and OTB100. The CSR-DCF runs in real-time on a CPU.



### Multimodal Latent Variable Analysis
- **Arxiv ID**: http://arxiv.org/abs/1611.08472v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08472v1)
- **Published**: 2016-11-25 14:38:50+00:00
- **Updated**: 2016-11-25 14:38:50+00:00
- **Authors**: Vardan Papyan, Ronen Talmon
- **Comment**: None
- **Journal**: None
- **Summary**: Consider a set of multiple, multimodal sensors capturing a complex system or a physical phenomenon of interest. Our primary goal is to distinguish the underlying sources of variability manifested in the measured data. The first step in our analysis is to find the common source of variability present in all sensor measurements. We base our work on a recent paper, which tackles this problem with alternating diffusion (AD). In this work, we suggest to further the analysis by extracting the sensor-specific variables in addition to the common source. We propose an algorithm, which we analyze theoretically, and then demonstrate on three different applications: a synthetic example, a toy problem, and the task of fetal ECG extraction.



### Person Re-Identification by Unsupervised Video Matching
- **Arxiv ID**: http://arxiv.org/abs/1611.08512v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08512v2)
- **Published**: 2016-11-25 16:47:39+00:00
- **Updated**: 2016-11-28 09:20:29+00:00
- **Authors**: Xiaolong Ma, Xiatian Zhu, Shaogang Gong, Xudong Xie, Jianming Hu, Kin-Man Lam, Yisheng Zhong
- **Comment**: None
- **Journal**: None
- **Summary**: Most existing person re-identification (ReID) methods rely only on the spatial appearance information from either one or multiple person images, whilst ignore the space-time cues readily available in video or image-sequence data. Moreover, they often assume the availability of exhaustively labelled cross-view pairwise data for every camera pair, making them non-scalable to ReID applications in real-world large scale camera networks. In this work, we introduce a novel video based person ReID method capable of accurately matching people across views from arbitrary unaligned image-sequences without any labelled pairwise data. Specifically, we introduce a new space-time person representation by encoding multiple granularities of spatio-temporal dynamics in form of time series. Moreover, a Time Shift Dynamic Time Warping (TS-DTW) model is derived for performing automatically alignment whilst achieving data selection and matching between inherently inaccurate and incomplete sequences in a unified way. We further extend the TS-DTW model for accommodating multiple feature-sequences of an image-sequence in order to fuse information from different descriptions. Crucially, this model does not require pairwise labelled training data (i.e. unsupervised) therefore readily scalable to large scale camera networks of arbitrary camera pairs without the need for exhaustive data annotation for every camera pair. We show the effectiveness and advantages of the proposed method by extensive comparisons with related state-of-the-art approaches using two benchmarking ReID datasets, PRID2011 and iLIDS-VID.



### Clickstream analysis for crowd-based object segmentation with confidence
- **Arxiv ID**: http://arxiv.org/abs/1611.08527v4
- **DOI**: 10.1109/TPAMI.2017.2777967
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08527v4)
- **Published**: 2016-11-25 17:29:58+00:00
- **Updated**: 2017-11-29 13:15:26+00:00
- **Authors**: Eric Heim, Alexander Seitel, Jonas Andrulis, Fabian Isensee, Christian Stock, Tobias Ross, Lena Maier-Hein
- **Comment**: to appear in IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)
- **Journal**: None
- **Summary**: With the rapidly increasing interest in machine learning based solutions for automatic image annotation, the availability of reference annotations for algorithm training is one of the major bottlenecks in the field. Crowdsourcing has evolved as a valuable option for low-cost and large-scale data annotation; however, quality control remains a major issue which needs to be addressed. To our knowledge, we are the first to analyze the annotation process to improve crowd-sourced image segmentation. Our method involves training a regressor to estimate the quality of a segmentation from the annotator's clickstream data. The quality estimation can be used to identify spam and weight individual annotations by their (estimated) quality when merging multiple segmentations of one image. Using a total of 29,000 crowd annotations performed on publicly available data of different object classes, we show that (1) our method is highly accurate in estimating the segmentation quality based on clickstream data, (2) outperforms state-of-the-art methods for merging multiple annotations. As the regressor does not need to be trained on the object class that it is applied to it can be regarded as a low-cost option for quality control and confidence analysis in the context of crowd-based image annotation.



### Online Real-time Multiple Spatiotemporal Action Localisation and Prediction
- **Arxiv ID**: http://arxiv.org/abs/1611.08563v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08563v6)
- **Published**: 2016-11-25 19:21:36+00:00
- **Updated**: 2017-08-24 09:15:13+00:00
- **Authors**: Gurkirt Singh, Suman Saha, Michael Sapienza, Philip Torr, Fabio Cuzzolin
- **Comment**: 10 pages 3 figures, ICCV 2017, Added link to new annotations of
  ucf101-24
- **Journal**: None
- **Summary**: We present a deep-learning framework for real-time multiple spatio-temporal (S/T) action localisation, classification and early prediction. Current state-of-the-art approaches work offline and are too slow to be useful in real- world settings. To overcome their limitations we introduce two major developments. Firstly, we adopt real-time SSD (Single Shot MultiBox Detector) convolutional neural networks to regress and classify detection boxes in each video frame potentially containing an action of interest. Secondly, we design an original and efficient online algorithm to incrementally construct and label `action tubes' from the SSD frame level detections. As a result, our system is not only capable of performing S/T detection in real time, but can also perform early action prediction in an online fashion. We achieve new state-of-the-art results in both S/T action localisation and early action prediction on the challenging UCF101-24 and J-HMDB-21 benchmarks, even when compared to the top offline competitors. To the best of our knowledge, ours is the first real-time (up to 40fps) system able to perform online S/T action localisation and early action prediction on the untrimmed videos of UCF101-24.



### Learning from Maps: Visual Common Sense for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1611.08583v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08583v2)
- **Published**: 2016-11-25 20:56:55+00:00
- **Updated**: 2016-12-07 22:24:52+00:00
- **Authors**: Ari Seff, Jianxiong Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: Today's autonomous vehicles rely extensively on high-definition 3D maps to navigate the environment. While this approach works well when these maps are completely up-to-date, safe autonomous vehicles must be able to corroborate the map's information via a real time sensor-based system. Our goal in this work is to develop a model for road layout inference given imagery from on-board cameras, without any reliance on high-definition maps. However, no sufficient dataset for training such a model exists. Here, we leverage the availability of standard navigation maps and corresponding street view images to construct an automatically labeled, large-scale dataset for this complex scene understanding problem. By matching road vectors and metadata from navigation maps with Google Street View images, we can assign ground truth road layout attributes (e.g., distance to an intersection, one-way vs. two-way street) to the images. We then train deep convolutional networks to predict these road layout attributes given a single monocular RGB image. Experimental evaluation demonstrates that our model learns to correctly infer the road attributes using only panoramas captured by car-mounted cameras as input. Additionally, our results indicate that this method may be suitable to the novel application of recommending safety improvements to infrastructure (e.g., suggesting an alternative speed limit for a street).



### Fast deterministic tourist walk for texture analysis
- **Arxiv ID**: http://arxiv.org/abs/1611.08624v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08624v1)
- **Published**: 2016-11-25 22:21:05+00:00
- **Updated**: 2016-11-25 22:21:05+00:00
- **Authors**: Lucas Correia Ribas, Odemir Martinez Bruno
- **Comment**: 7 page, 7 figure
- **Journal**: WVC 2016 proceedings p45-50
- **Summary**: Deterministic tourist walk (DTW) has attracted increasing interest in computer vision. In the last years, different methods for analysis of dynamic and static textures were proposed. So far, all works based on the DTW for texture analysis use all image pixels as initial point of a walk. However, this requires much runtime. In this paper, we conducted a study to verify the performance of the DTW method according to the number of initial points to start a walk. The proposed method assigns a unique code to each image pixel, then, the pixels whose code is not divisible by a given $k$ value are ignored as initial points of walks. Feature vectors were extracted and a classification process was performed for different percentages of initial points. Experimental results on the Brodatz and Vistex datasets indicate that to use fewer pixels as initial points significantly improves the runtime compared to use all image pixels. In addition, the correct classification rate decreases very little.



### Directional Mean Curvature for Textured Image Demixing
- **Arxiv ID**: http://arxiv.org/abs/1611.08625v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08625v1)
- **Published**: 2016-11-25 22:21:06+00:00
- **Updated**: 2016-11-25 22:21:06+00:00
- **Authors**: Duy Hoang Thai, David Banks
- **Comment**: 72 pages, including main manuscript (36 pages) and Appendix (36
  pages); 14 figures; journal
- **Journal**: None
- **Summary**: Approximation theory plays an important role in image processing, especially image deconvolution and decomposition. For piecewise smooth images, there are many methods that have been developed over the past thirty years. The goal of this study is to devise similar and practical methodology for handling textured images. This problem is motivated by forensic imaging, since fingerprints, shoeprints and bullet ballistic evidence are textured images. In particular, it is known that texture information is almost destroyed by a blur operator, such as a blurred ballistic image captured from a low-cost microscope. The contribution of this work is twofold: first, we propose a mathematical model for textured image deconvolution and decomposition into four meaningful components, using a high-order partial differential equation approach based on the directional mean curvature. Second, we uncover a link between functional analysis and multiscale sampling theory, e.g., harmonic analysis and filter banks. Both theoretical results and examples with natural images are provided to illustrate the performance of the proposed model.



### Texture analysis using deterministic partially self-avoiding walk with thresholds
- **Arxiv ID**: http://arxiv.org/abs/1611.08629v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08629v1)
- **Published**: 2016-11-25 22:51:07+00:00
- **Updated**: 2016-11-25 22:51:07+00:00
- **Authors**: Lucas Correia Ribas, Wesley Nunes Gonçalves, Odemir Martinez Bruno
- **Comment**: 8 pages, 3 figures
- **Journal**: WVC proceedings 2016, pages 39-44
- **Summary**: In this paper, we propose a new texture analysis method using the deterministic partially self-avoiding walk performed on maps modified with thresholds. In this method, two pixels of the map are neighbors if the Euclidean distance between them is less than $\sqrt{2}$ and the weight (difference between its intensities) is less than a given threshold. The maps obtained by using different thresholds highlight several properties of the image that are extracted by the deterministic walk. To compose the feature vector, deterministic walks are performed with different thresholds and its statistics are concatenated. Thus, this approach can be considered as a multi-scale analysis. We validate our method on the Brodatz database, which is very well known public image database and widely used by texture analysis methods. Experimental results indicate that the proposed method presents a good texture discrimination, overcoming traditional texture methods.



