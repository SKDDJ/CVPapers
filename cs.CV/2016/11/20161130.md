# Arxiv Papers in cs.CV on 2016-11-30
### Efficient Likelihood Bayesian Constrained Local Model
- **Arxiv ID**: http://arxiv.org/abs/1611.09956v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09956v1)
- **Published**: 2016-11-30 00:41:32+00:00
- **Updated**: 2016-11-30 00:41:32+00:00
- **Authors**: Hailiang Li, Kin-Man Lam, Man-Yau Chiu, Kangheng Wu, Zhibin Lei
- **Comment**: 6 pages, for submitting to ICME-2017
- **Journal**: None
- **Summary**: The constrained local model (CLM) proposes a paradigm that the locations of a set of local landmark detectors are constrained to lie in a subspace, spanned by a shape point distribution model (PDM). Fitting the model to an object involves two steps. A response map, which represents the likelihood of the location of a landmark, is first computed for each landmark using local-texture detectors. Then, an optimal PDM is determined by jointly maximizing all the response maps simultaneously, with a global shape constraint. This global optimization can be considered as a Bayesian inference problem, where the posterior distribution of the shape parameters, as well as the pose parameters, can be inferred using maximum a posteriori (MAP). In this paper, we present a cascaded face-alignment approach, which employs random-forest regressors to estimate the positions of each landmark, as a likelihood term, efficiently in the CLM model. Interpretation from CLM framework, this algorithm is named as an efficient likelihood Bayesian constrained local model (elBCLM). Furthermore, in each stage of the regressors, the PDM non-rigid parameters of previous stage can work as shape clues for training each stage regressors. Experimental results on benchmarks show our approach achieve about 3 to 5 times speed-up compared with CLM models and improve around 10% on fitting quality compare with the same setting regression models.



### Machine Learning for Dental Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1611.09958v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1611.09958v2)
- **Published**: 2016-11-30 01:17:34+00:00
- **Updated**: 2016-12-02 11:27:37+00:00
- **Authors**: Young-jun Yu
- **Comment**: This study was reviewed and approved by the institutional review
  board of the Pusan National University Dental Hospital (PNUPH-2015-034)
- **Journal**: None
- **Summary**: In order to study the application of artificial intelligence (AI) to dental imaging, we applied AI technology to classify a set of panoramic radiographs using (a) a convolutional neural network (CNN) which is a form of an artificial neural network (ANN), (b) representative image cognition algorithms that implement scale-invariant feature transform (SIFT), and (c) histogram of oriented gradients (HOG).



### Attend in groups: a weakly-supervised deep learning framework for learning from web data
- **Arxiv ID**: http://arxiv.org/abs/1611.09960v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09960v1)
- **Published**: 2016-11-30 01:23:43+00:00
- **Updated**: 2016-11-30 01:23:43+00:00
- **Authors**: Bohan Zhuang, Lingqiao Liu, Yao Li, Chunhua Shen, Ian Reid
- **Comment**: None
- **Journal**: None
- **Summary**: Large-scale datasets have driven the rapid development of deep neural networks for visual recognition. However, annotating a massive dataset is expensive and time-consuming. Web images and their labels are, in comparison, much easier to obtain, but direct training on such automatically harvested images can lead to unsatisfactory performance, because the noisy labels of Web images adversely affect the learned recognition models. To address this drawback we propose an end-to-end weakly-supervised deep learning framework which is robust to the label noise in Web images. The proposed framework relies on two unified strategies -- random grouping and attention -- to effectively reduce the negative impact of noisy web image annotations. Specifically, random grouping stacks multiple images into a single training instance and thus increases the labeling accuracy at the instance level. Attention, on the other hand, suppresses the noisy signals from both incorrectly labeled images and less discriminative image regions. By conducting intensive experiments on two challenging datasets, including a newly collected fine-grained dataset with Web images of different car models, the superior performance of the proposed methods over competitive baselines is clearly demonstrated.



### Semantic Facial Expression Editing using Autoencoded Flow
- **Arxiv ID**: http://arxiv.org/abs/1611.09961v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09961v1)
- **Published**: 2016-11-30 01:24:22+00:00
- **Updated**: 2016-11-30 01:24:22+00:00
- **Authors**: Raymond Yeh, Ziwei Liu, Dan B Goldman, Aseem Agarwala
- **Comment**: None
- **Journal**: None
- **Summary**: High-level manipulation of facial expressions in images --- such as changing a smile to a neutral expression --- is challenging because facial expression changes are highly non-linear, and vary depending on the appearance of the face. We present a fully automatic approach to editing faces that combines the advantages of flow-based face manipulation with the more recent generative capabilities of Variational Autoencoders (VAEs). During training, our model learns to encode the flow from one expression to another over a low-dimensional latent space. At test time, expression editing can be done simply using latent vector arithmetic. We evaluate our methods on two applications: 1) single-image facial expression editing, and 2) facial expression interpolation between two images. We demonstrate that our method generates images of higher perceptual quality than previous VAE and flow-based methods.



### Sequential Person Recognition in Photo Albums with a Recurrent Network
- **Arxiv ID**: http://arxiv.org/abs/1611.09967v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09967v1)
- **Published**: 2016-11-30 01:45:23+00:00
- **Updated**: 2016-11-30 01:45:23+00:00
- **Authors**: Yao Li, Guosheng Lin, Bohan Zhuang, Lingqiao Liu, Chunhua Shen, Anton van den Hengel
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing the identities of people in everyday photos is still a very challenging problem for machine vision, due to non-frontal faces, changes in clothing, location, lighting and similar. Recent studies have shown that rich relational information between people in the same photo can help in recognizing their identities. In this work, we propose to model the relational information between people as a sequence prediction task. At the core of our work is a novel recurrent network architecture, in which relational information between instances' labels and appearance are modeled jointly. In addition to relational cues, scene context is incorporated in our sequence prediction model with no additional cost. In this sense, our approach is a unified framework for modeling both contextual cues and visual appearance of person instances. Our model is trained end-to-end with a sequence of annotated instances in a photo as inputs, and a sequence of corresponding labels as targets. We demonstrate that this simple but elegant formulation achieves state-of-the-art performance on the newly released People In Photo Albums (PIPA) dataset.



### High-Resolution Image Inpainting using Multi-Scale Neural Patch Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1611.09969v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09969v2)
- **Published**: 2016-11-30 01:58:54+00:00
- **Updated**: 2017-04-13 06:56:06+00:00
- **Authors**: Chao Yang, Xin Lu, Zhe Lin, Eli Shechtman, Oliver Wang, Hao Li
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in deep learning have shown exciting promise in filling large holes in natural images with semantically plausible and context aware details, impacting fundamental image manipulation tasks such as object removal. While these learning-based methods are significantly more effective in capturing high-level features than prior techniques, they can only handle very low-resolution inputs due to memory limitations and difficulty in training. Even for slightly larger images, the inpainted regions would appear blurry and unpleasant boundaries become visible. We propose a multi-scale neural patch synthesis approach based on joint optimization of image content and texture constraints, which not only preserves contextual structures but also produces high-frequency details by matching and adapting patches with the most similar mid-layer feature correlations of a deep classification network. We evaluate our method on the ImageNet and Paris Streetview datasets and achieved state-of-the-art inpainting accuracy. We show our approach produces sharper and more coherent results than prior methods, especially for high-resolution images.



### Modeling Relationships in Referential Expressions with Compositional Modular Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.09978v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09978v1)
- **Published**: 2016-11-30 02:52:09+00:00
- **Updated**: 2016-11-30 02:52:09+00:00
- **Authors**: Ronghang Hu, Marcus Rohrbach, Jacob Andreas, Trevor Darrell, Kate Saenko
- **Comment**: None
- **Journal**: None
- **Summary**: People often refer to entities in an image in terms of their relationships with other entities. For example, "the black cat sitting under the table" refers to both a "black cat" entity and its relationship with another "table" entity. Understanding these relationships is essential for interpreting and grounding such natural language expressions. Most prior work focuses on either grounding entire referential expressions holistically to one region, or localizing relationships based on a fixed set of categories. In this paper we instead present a modular deep architecture capable of analyzing referential expressions into their component parts, identifying entities and relationships mentioned in the input expression and grounding them all in the scene. We call this approach Compositional Modular Networks (CMNs): a novel architecture that learns linguistic analysis and visual inference end-to-end. Our approach is built around two types of neural modules that inspect local regions and pairwise interactions between regions. We evaluate CMNs on multiple referential expression datasets, outperforming state-of-the-art approaches on all tasks.



### Deep Cuboid Detection: Beyond 2D Bounding Boxes
- **Arxiv ID**: http://arxiv.org/abs/1611.10010v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.10010v1)
- **Published**: 2016-11-30 06:00:47+00:00
- **Updated**: 2016-11-30 06:00:47+00:00
- **Authors**: Debidatta Dwibedi, Tomasz Malisiewicz, Vijay Badrinarayanan, Andrew Rabinovich
- **Comment**: None
- **Journal**: None
- **Summary**: We present a Deep Cuboid Detector which takes a consumer-quality RGB image of a cluttered scene and localizes all 3D cuboids (box-like objects). Contrary to classical approaches which fit a 3D model from low-level cues like corners, edges, and vanishing points, we propose an end-to-end deep learning system to detect cuboids across many semantic categories (e.g., ovens, shipping boxes, and furniture). We localize cuboids with a 2D bounding box, and simultaneously localize the cuboid's corners, effectively producing a 3D interpretation of box-like objects. We refine keypoints by pooling convolutional features iteratively, improving the baseline method significantly. Our deep learning cuboid detector is trained in an end-to-end fashion and is suitable for real-time applications in augmented reality (AR) and robotics.



### Speed/accuracy trade-offs for modern convolutional object detectors
- **Arxiv ID**: http://arxiv.org/abs/1611.10012v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.10012v3)
- **Published**: 2016-11-30 06:06:15+00:00
- **Updated**: 2017-04-25 03:42:55+00:00
- **Authors**: Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, Kevin Murphy
- **Comment**: Accepted to CVPR 2017
- **Journal**: None
- **Summary**: The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-to-apples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [Ren et al., 2015], R-FCN [Dai et al., 2016] and SSD [Liu et al., 2015] systems, which we view as "meta-architectures" and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task.



### Fast Supervised Discrete Hashing and its Analysis
- **Arxiv ID**: http://arxiv.org/abs/1611.10017v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1611.10017v1)
- **Published**: 2016-11-30 06:35:39+00:00
- **Updated**: 2016-11-30 06:35:39+00:00
- **Authors**: Gou Koutaki, Keiichiro Shirai, Mitsuru Ambai
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: In this paper, we propose a learning-based supervised discrete hashing method. Binary hashing is widely used for large-scale image retrieval as well as video and document searches because the compact representation of binary code is essential for data storage and reasonable for query searches using bit-operations. The recently proposed Supervised Discrete Hashing (SDH) efficiently solves mixed-integer programming problems by alternating optimization and the Discrete Cyclic Coordinate descent (DCC) method. We show that the SDH model can be simplified without performance degradation based on some preliminary experiments; we call the approximate model for this the "Fast SDH" (FSDH) model. We analyze the FSDH model and provide a mathematically exact solution for it. In contrast to SDH, our model does not require an alternating optimization algorithm and does not depend on initial values. FSDH is also easier to implement than Iterative Quantization (ITQ). Experimental results involving a large-scale database showed that FSDH outperforms conventional SDH in terms of precision, recall, and computation time.



### Active Deep Learning for Classification of Hyperspectral Images
- **Arxiv ID**: http://arxiv.org/abs/1611.10031v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1611.10031v1)
- **Published**: 2016-11-30 07:34:46+00:00
- **Updated**: 2016-11-30 07:34:46+00:00
- **Authors**: Peng Liu, Hui Zhang, Kie B. Eom
- **Comment**: None
- **Journal**: None
- **Summary**: Active deep learning classification of hyperspectral images is considered in this paper. Deep learning has achieved success in many applications, but good-quality labeled samples are needed to construct a deep learning network. It is expensive getting good labeled samples in hyperspectral images for remote sensing applications. An active learning algorithm based on a weighted incremental dictionary learning is proposed for such applications. The proposed algorithm selects training samples that maximize two selection criteria, namely representative and uncertainty. This algorithm trains a deep network efficiently by actively selecting training samples at each iteration. The proposed algorithm is applied for the classification of hyperspectral images, and compared with other classification algorithms employing active learning. It is shown that the proposed algorithm is efficient and effective in classifying hyperspectral images.



### Wider or Deeper: Revisiting the ResNet Model for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1611.10080v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.10080v1)
- **Published**: 2016-11-30 10:24:32+00:00
- **Updated**: 2016-11-30 10:24:32+00:00
- **Authors**: Zifeng Wu, Chunhua Shen, Anton van den Hengel
- **Comment**: Code available at: https://github.com/itijyou/ademxapp
- **Journal**: None
- **Summary**: The trend towards increasingly deep neural networks has been driven by a general observation that increasing depth increases the performance of a network. Recently, however, evidence has been amassing that simply increasing depth may not be the best way to increase performance, particularly given other limitations. Investigations into deep residual networks have also suggested that they may not in fact be operating as a single deep network, but rather as an ensemble of many relatively shallow networks. We examine these issues, and in doing so arrive at a new interpretation of the unravelled view of deep residual networks which explains some of the behaviours that have been observed experimentally. As a result, we are able to derive a new, shallower, architecture of residual networks which significantly outperforms much deeper models such as ResNet-200 on the ImageNet classification dataset. We also show that this performance is transferable to other problem domains by developing a semantic segmentation approach which outperforms the state-of-the-art by a remarkable margin on datasets including PASCAL VOC, PASCAL Context, and Cityscapes. The architecture that we propose thus outperforms its comparators, including very deep ResNets, and yet is more efficient in memory use and sometimes also in training time. The code and models are available at https://github.com/itijyou/ademxapp



### User Dependent Features in Online Signature Verification
- **Arxiv ID**: http://arxiv.org/abs/1611.10104v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.10104v1)
- **Published**: 2016-11-30 11:51:50+00:00
- **Updated**: 2016-11-30 11:51:50+00:00
- **Authors**: D. S. Guru, K. S. Manjunatha, S. Manjunath
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: In this paper, we propose a novel approach for verification of on-line signatures based on user dependent feature selection and symbolic representation. Unlike other signature verification methods, which work with same features for all users, the proposed approach introduces the concept of user dependent features. It exploits the typicality of each and every user to select different features for different users. Initially all possible features are extracted for all users and a method of feature selection is employed for selecting user dependent features. The selected features are clustered using Fuzzy C means algorithm. In order to preserve the intra-class variation within each user, we recommend to represent each cluster in the form of an interval valued symbolic feature vector. A method of signature verification based on the proposed cluster based symbolic representation is also presented. Extensive experimentations are conducted on MCYT-100 User (DB1) and MCYT-330 User (DB2) online signature data sets to demonstrate the effectiveness of the proposed novel approach.



### Combining Data-driven and Model-driven Methods for Robust Facial Landmark Detection
- **Arxiv ID**: http://arxiv.org/abs/1611.10152v2
- **DOI**: 10.1109/TIFS.2018.2800901
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.10152v2)
- **Published**: 2016-11-30 14:01:45+00:00
- **Updated**: 2018-02-12 03:03:59+00:00
- **Authors**: Hongwen Zhang, Qi Li, Zhenan Sun, Yunfan Liu
- **Comment**: Journal article accepted for publication in IEEE TIFS
- **Journal**: None
- **Summary**: Facial landmark detection is an important yet challenging task for real-world computer vision applications. This paper proposes an effective and robust approach for facial landmark detection by combining data- and model-driven methods. Firstly, a Fully Convolutional Network (FCN) is trained to compute response maps of all facial landmark points. Such a data-driven method could make full use of holistic information in a facial image for global estimation of facial landmarks. After that, the maximum points in the response maps are fitted with a pre-trained Point Distribution Model (PDM) to generate the initial facial shape. This model-driven method is able to correct the inaccurate locations of outliers by considering the shape prior information. Finally, a weighted version of Regularized Landmark Mean-Shift (RLMS) is employed to fine-tune the facial shape iteratively. This Estimation-Correction-Tuning process perfectly combines the advantages of the global robustness of data-driven method (FCN), outlier correction capability of model-driven method (PDM) and non-parametric optimization of RLMS. Results of extensive experiments demonstrate that our approach achieves state-of-the-art performances on challenging datasets including 300W, AFLW, AFW and COFW. The proposed method is able to produce satisfying detection results on face images with exaggerated expressions, large head poses, and partial occlusions.



### Effective Quantization Methods for Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.10176v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1611.10176v1)
- **Published**: 2016-11-30 14:33:08+00:00
- **Updated**: 2016-11-30 14:33:08+00:00
- **Authors**: Qinyao He, He Wen, Shuchang Zhou, Yuxin Wu, Cong Yao, Xinyu Zhou, Yuheng Zou
- **Comment**: None
- **Journal**: None
- **Summary**: Reducing bit-widths of weights, activations, and gradients of a Neural Network can shrink its storage size and memory usage, and also allow for faster training and inference by exploiting bitwise operations. However, previous attempts for quantization of RNNs show considerable performance degradation when using low bit-width weights and activations. In this paper, we propose methods to quantize the structure of gates and interlinks in LSTM and GRU cells. In addition, we propose balanced quantization methods for weights to further reduce performance degradation. Experiments on PTB and IMDB datasets confirm effectiveness of our methods as performances of our models match or surpass the previous state-of-the-art of quantized RNN.



### POSEidon: Face-from-Depth for Driver Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1611.10195v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.10195v3)
- **Published**: 2016-11-30 14:57:06+00:00
- **Updated**: 2017-12-12 22:08:30+00:00
- **Authors**: Guido Borghi, Marco Venturelli, Roberto Vezzani, Rita Cucchiara
- **Comment**: Accepted in Computer Vision and Pattern Recognition (CVPR 2017)
- **Journal**: None
- **Summary**: Fast and accurate upper-body and head pose estimation is a key task for automatic monitoring of driver attention, a challenging context characterized by severe illumination changes, occlusions and extreme poses. In this work, we present a new deep learning framework for head localization and pose estimation on depth images. The core of the proposal is a regression neural network, called POSEidon, which is composed of three independent convolutional nets followed by a fusion layer, specially conceived for understanding the pose by depth. In addition, to recover the intrinsic value of face appearance for understanding head position and orientation, we propose a new Face-from-Depth approach for learning image faces from depth. Results in face reconstruction are qualitatively impressive. We test the proposed framework on two public datasets, namely Biwi Kinect Head Pose and ICT-3DHP, and on Pandora, a new challenging dataset mainly inspired by the automotive setup. Results show that our method overcomes all recent state-of-art works, running in real time at more than 30 frames per second.



### End-to-End Training of Hybrid CNN-CRF Models for Stereo
- **Arxiv ID**: http://arxiv.org/abs/1611.10229v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.10229v2)
- **Published**: 2016-11-30 15:45:02+00:00
- **Updated**: 2017-05-03 09:33:20+00:00
- **Authors**: Patrick Knöbelreiter, Christian Reinbacher, Alexander Shekhovtsov, Thomas Pock
- **Comment**: To appear at CVPR 2017
- **Journal**: None
- **Summary**: We propose a novel and principled hybrid CNN+CRF model for stereo estimation. Our model allows to exploit the advantages of both, convolutional neural networks (CNNs) and conditional random fields (CRFs) in an unified approach. The CNNs compute expressive features for matching and distinctive color edges, which in turn are used to compute the unary and binary costs of the CRF. For inference, we apply a recently proposed highly parallel dual block descent algorithm which only needs a small fixed number of iterations to compute a high-quality approximate minimizer. As the main contribution of the paper, we propose a theoretically sound method based on the structured output support vector machine (SSVM) to train the hybrid CNN+CRF model on large-scale data end-to-end. Our trained models perform very well despite the fact that we are using shallow CNNs and do not apply any kind of post-processing to the final output of the CRF. We evaluate our combined models on challenging stereo benchmarks such as Middlebury 2014 and Kitti 2015 and also investigate the performance of each individual component.



### Sync-DRAW: Automatic Video Generation using Deep Recurrent Attentive Architectures
- **Arxiv ID**: http://arxiv.org/abs/1611.10314v4
- **DOI**: 10.1145/3123266.3123309
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.10314v4)
- **Published**: 2016-11-30 19:07:07+00:00
- **Updated**: 2017-10-21 21:02:46+00:00
- **Authors**: Gaurav Mittal, Tanya Marwah, Vineeth N. Balasubramanian
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a novel approach for generating videos called Synchronized Deep Recurrent Attentive Writer (Sync-DRAW). Sync-DRAW can also perform text-to-video generation which, to the best of our knowledge, makes it the first approach of its kind. It combines a Variational Autoencoder~(VAE) with a Recurrent Attention Mechanism in a novel manner to create a temporally dependent sequence of frames that are gradually formed over time. The recurrent attention mechanism in Sync-DRAW attends to each individual frame of the video in sychronization, while the VAE learns a latent distribution for the entire video at the global level. Our experiments with Bouncing MNIST, KTH and UCF-101 suggest that Sync-DRAW is efficient in learning the spatial and temporal information of the videos and generates frames with high structural integrity, and can generate videos from simple captions on these datasets. (Accepted as oral paper in ACM-Multimedia 2017)



### An Artificial Agent for Robust Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1611.10336v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.10336v1)
- **Published**: 2016-11-30 20:05:55+00:00
- **Updated**: 2016-11-30 20:05:55+00:00
- **Authors**: Rui Liao, Shun Miao, Pierre de Tournemire, Sasa Grbic, Ali Kamen, Tommaso Mansi, Dorin Comaniciu
- **Comment**: To appear in AAAI Conference 2017
- **Journal**: None
- **Summary**: 3-D image registration, which involves aligning two or more images, is a critical step in a variety of medical applications from diagnosis to therapy. Image registration is commonly performed by optimizing an image matching metric as a cost function. However, this task is challenging due to the non-convex nature of the matching metric over the plausible registration parameter space and insufficient approaches for a robust optimization. As a result, current approaches are often customized to a specific problem and sensitive to image quality and artifacts. In this paper, we propose a completely different approach to image registration, inspired by how experts perform the task. We first cast the image registration problem as a "strategy learning" process, where the goal is to find the best sequence of motion actions (e.g. up, down, etc.) that yields image alignment. Within this approach, an artificial agent is learned, modeled using deep convolutional neural networks, with 3D raw image data as the input, and the next optimal action as the output. To cope with the dimensionality of the problem, we propose a greedy supervised approach for an end-to-end training, coupled with attention-driven hierarchical strategy. The resulting registration approach inherently encodes both a data-driven matching metric and an optimal registration strategy (policy). We demonstrate, on two 3-D/3-D medical image registration examples with drastically different nature of challenges, that the artificial agent outperforms several state-of-art registration methods by a large margin in terms of both accuracy and robustness.



### Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space
- **Arxiv ID**: http://arxiv.org/abs/1612.00005v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00005v2)
- **Published**: 2016-11-30 20:56:36+00:00
- **Updated**: 2017-04-12 06:39:52+00:00
- **Authors**: Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, Jason Yosinski
- **Comment**: CVPR camera-ready
- **Journal**: None
- **Summary**: Generating high-resolution, photo-realistic images has been a long-standing goal in machine learning. Recently, Nguyen et al. (2016) showed one interesting way to synthesize novel images by performing gradient ascent in the latent space of a generator network to maximize the activations of one or multiple neurons in a separate classifier network. In this paper we extend this method by introducing an additional prior on the latent code, improving both sample quality and sample diversity, leading to a state-of-the-art generative model that produces high quality images at higher resolutions (227x227) than previous generative models, and does so for all 1000 ImageNet categories. In addition, we provide a unified probabilistic interpretation of related activation maximization methods and call the general class of models "Plug and Play Generative Networks". PPGNs are composed of 1) a generator network G that is capable of drawing a wide range of image types and 2) a replaceable "condition" network C that tells the generator what to draw. We demonstrate the generation of images conditioned on a class (when C is an ImageNet or MIT Places classification network) and also conditioned on a caption (when C is an image captioning network). Our method also improves the state of the art of Multifaceted Feature Visualization, which generates the set of synthetic inputs that activate a neuron in order to better understand how deep neural networks operate. Finally, we show that our model performs reasonably well at the task of image inpainting. While image models are used in this paper, the approach is modality-agnostic and can be applied to many types of data.



