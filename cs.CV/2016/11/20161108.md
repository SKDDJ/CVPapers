# Arxiv Papers in cs.CV on 2016-11-08
### Multiple Object Tracking with Kernelized Correlation Filters in Urban Mixed Traffic
- **Arxiv ID**: http://arxiv.org/abs/1611.02364v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.02364v2)
- **Published**: 2016-11-08 02:20:09+00:00
- **Updated**: 2017-03-27 14:36:59+00:00
- **Authors**: Yuebin Yang, Guillaume-Alexandre Bilodeau
- **Comment**: Accepted for CRV 2017
- **Journal**: None
- **Summary**: Recently, the Kernelized Correlation Filters tracker (KCF) achieved competitive performance and robustness in visual object tracking. On the other hand, visual trackers are not typically used in multiple object tracking. In this paper, we investigate how a robust visual tracker like KCF can improve multiple object tracking. Since KCF is a fast tracker, many can be used in parallel and still result in fast tracking. We build a multiple object tracking system based on KCF and background subtraction. Background subtraction is applied to extract moving objects and get their scale and size in combination with KCF outputs, while KCF is used for data association and to handle fragmentation and occlusion problems. As a result, KCF and background subtraction help each other to take tracking decision at every frame. Sometimes KCF outputs are the most trustworthy (e.g. during occlusion), while in some other case, it is the background subtraction outputs. To validate the effectiveness of our system, the algorithm is demonstrated on four urban video recordings from a standard dataset. Results show that our method is competitive with state-of-the-art trackers even if we use a much simpler data association step.



### Domain Adaptation with L2 constraints for classifying images from different endoscope systems
- **Arxiv ID**: http://arxiv.org/abs/1611.02443v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1611.02443v2)
- **Published**: 2016-11-08 09:29:17+00:00
- **Updated**: 2018-02-02 09:01:20+00:00
- **Authors**: Toru Tamaki, Shoji Sonoyama, Takio Kurita, Tsubasa Hirakawa, Bisser Raytchev, Kazufumi Kaneda, Tetsushi Koide, Shigeto Yoshida, Hiroshi Mieno, Shinji Tanaka, Kazuaki Chayama
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: This paper proposes a method for domain adaptation that extends the maximum margin domain transfer (MMDT) proposed by Hoffman et al., by introducing L2 distance constraints between samples of different domains; thus, our method is denoted as MMDTL2. Motivated by the differences between the images taken by narrow band imaging (NBI) endoscopic devices, we utilize different NBI devices as different domains and estimate the transformations between samples of different domains, i.e., image samples taken by different NBI endoscope systems. We first formulate the problem in the primal form, and then derive the dual form with much lesser computational costs as compared to the naive approach. From our experimental results using NBI image datasets from two different NBI endoscopic devices, we find that MMDTL2 is better than MMDT and also support vector machines without adaptation, especially when NBI image features are high-dimensional and the per-class training samples are greater than 20.



### Action Recognition Based on Joint Trajectory Maps Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.02447v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.02447v2)
- **Published**: 2016-11-08 09:35:17+00:00
- **Updated**: 2016-11-13 23:24:58+00:00
- **Authors**: Pichao Wang, Zhaoyang Li, Yonghong Hou, Wanqing Li
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Convolutional Neural Networks (ConvNets) have shown promising performances in many computer vision tasks, especially image-based recognition. How to effectively use ConvNets for video-based recognition is still an open problem. In this paper, we propose a compact, effective yet simple method to encode spatio-temporal information carried in $3D$ skeleton sequences into multiple $2D$ images, referred to as Joint Trajectory Maps (JTM), and ConvNets are adopted to exploit the discriminative features for real-time human action recognition. The proposed method has been evaluated on three public benchmarks, i.e., MSRC-12 Kinect gesture dataset (MSRC-12), G3D dataset and UTD multimodal human action dataset (UTD-MHAD) and achieved the state-of-the-art results.



### The Loss Surface of Residual Networks: Ensembles and the Role of Batch Normalization
- **Arxiv ID**: http://arxiv.org/abs/1611.02525v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.02525v1)
- **Published**: 2016-11-08 14:17:13+00:00
- **Updated**: 2016-11-08 14:17:13+00:00
- **Authors**: Etai Littwin, Lior Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Residual Networks present a premium in performance in comparison to conventional networks of the same depth and are trainable at extreme depths. It has recently been shown that Residual Networks behave like ensembles of relatively shallow networks. We show that these ensembles are dynamic: while initially the virtual ensemble is mostly at depths lower than half the network's depth, as training progresses, it becomes deeper and deeper. The main mechanism that controls the dynamic ensemble behavior is the scaling introduced, e.g., by the Batch Normalization technique. We explain this behavior and demonstrate the driving force behind it. As a main tool in our analysis, we employ generalized spin glass models, which we also use in order to study the number of critical points in the optimization of Residual Networks.



### Estimating motion with principal component regression strategies
- **Arxiv ID**: http://arxiv.org/abs/1611.02637v1
- **DOI**: 10.1109/mmsp.2009.5293264
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.02637v1)
- **Published**: 2016-11-08 18:01:54+00:00
- **Updated**: 2016-11-08 18:01:54+00:00
- **Authors**: Felipe P. do Carmo, Vania Vieira Estrela, Joaquim Teixeira de Assis
- **Comment**: 6 pages, 3 figures. arXiv admin note: substantial text overlap with
  arXiv:1610.02923
- **Journal**: Proceedings of the IEEE International Workshop on Multimedia
  Signal Processing, 2009, MMSP '09, 2009
- **Summary**: In this paper, two simple principal component regression methods for estimating the optical flow between frames of video sequences according to a pel-recursive manner are introduced. These are easy alternatives to dealing with mixtures of motion vectors in addition to the lack of prior information on spatial-temporal statistics (although they are supposed to be normal in a local sense). The 2D motion vector estimation approaches take into consideration simple image properties and are used to harmonize regularized least square estimates. Their main advantage is that no knowledge of the noise distribution is necessary, although there is an underlying assumption of localized smoothness. Preliminary experiments indicate that this approach provides robust estimates of the optical flow.



### Gradients of Counterfactuals
- **Arxiv ID**: http://arxiv.org/abs/1611.02639v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1611.02639v2)
- **Published**: 2016-11-08 18:10:44+00:00
- **Updated**: 2016-11-15 19:55:26+00:00
- **Authors**: Mukund Sundararajan, Ankur Taly, Qiqi Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Gradients have been used to quantify feature importance in machine learning models. Unfortunately, in nonlinear deep networks, not only individual neurons but also the whole network can saturate, and as a result an important input feature can have a tiny gradient. We study various networks, and observe that this phenomena is indeed widespread, across many inputs.   We propose to examine interior gradients, which are gradients of counterfactual inputs constructed by scaling down the original input. We apply our method to the GoogleNet architecture for object recognition in images, as well as a ligand-based virtual screening network with categorical features and an LSTM based language model for the Penn Treebank dataset. We visualize how interior gradients better capture feature importance. Furthermore, interior gradients are applicable to a wide variety of deep networks, and have the attribution property that the feature importance scores sum to the the prediction score.   Best of all, interior gradients can be computed just as easily as gradients. In contrast, previous methods are complex to implement, which hinders practical adoption.



### Multispectral Deep Neural Networks for Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/1611.02644v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.02644v1)
- **Published**: 2016-11-08 18:22:31+00:00
- **Updated**: 2016-11-08 18:22:31+00:00
- **Authors**: Jingjing Liu, Shaoting Zhang, Shu Wang, Dimitris N. Metaxas
- **Comment**: 13 pages, 8 figures, BMVC 2016 oral
- **Journal**: None
- **Summary**: Multispectral pedestrian detection is essential for around-the-clock applications, e.g., surveillance and autonomous driving. We deeply analyze Faster R-CNN for multispectral pedestrian detection task and then model it into a convolutional network (ConvNet) fusion problem. Further, we discover that ConvNet-based pedestrian detectors trained by color or thermal images separately provide complementary information in discriminating human instances. Thus there is a large potential to improve pedestrian detection by using color and thermal images in DNNs simultaneously. We carefully design four ConvNet fusion architectures that integrate two-branch ConvNets on different DNNs stages, all of which yield better performance compared with the baseline detector. Our experimental results on KAIST pedestrian benchmark show that the Halfway Fusion model that performs fusion on the middle-level convolutional features outperforms the baseline method by 11% and yields a missing rate 3.5% lower than the other proposed architectures.



### Inferring low-dimensional microstructure representations using convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1611.02764v2
- **DOI**: 10.1103/PhysRevE.96.052111
- **Categories**: **physics.comp-ph**, cond-mat.mtrl-sci, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1611.02764v2)
- **Published**: 2016-11-08 23:10:24+00:00
- **Updated**: 2018-11-30 21:40:46+00:00
- **Authors**: Nicholas Lubbers, Turab Lookman, Kipton Barros
- **Comment**: 14 Pages, 12 Figures
- **Journal**: Phys. Rev. E 96, 052111 (2017)
- **Summary**: We apply recent advances in machine learning and computer vision to a central problem in materials informatics: The statistical representation of microstructural images. We use activations in a pre-trained convolutional neural network to provide a high-dimensional characterization of a set of synthetic microstructural images. Next, we use manifold learning to obtain a low-dimensional embedding of this statistical characterization. We show that the low-dimensional embedding extracts the parameters used to generate the images. According to a variety of metrics, the convolutional neural network method yields dramatically better embeddings than the analogous method derived from two-point correlations alone.



### A backward pass through a CNN using a generative model of its activations
- **Arxiv ID**: http://arxiv.org/abs/1611.02767v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.02767v1)
- **Published**: 2016-11-08 23:18:50+00:00
- **Updated**: 2016-11-08 23:18:50+00:00
- **Authors**: Huayan Wang, Anna Chen, Yi Liu, Dileep George, D. Scott Phoenix
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks have shown to be a practical way of building a very complex mapping between a pre-specified input space and output space. For example, a convolutional neural network (CNN) mapping an image into one of a thousand object labels is approaching human performance in this particular task. However the mapping (neural network) does not automatically lend itself to other forms of queries, for example, to detect/reconstruct object instances, to enforce top-down signal on ambiguous inputs, or to recover object instances from occlusion. One way to address these queries is a backward pass through the network that fuses top-down and bottom-up information. In this paper, we show a way of building such a backward pass by defining a generative model of the neural network's activations. Approximate inference of the model would naturally take the form of a backward pass through the CNN layers, and it addresses the aforementioned queries in a unified framework.



### Deep Convolutional Neural Network for 6-DOF Image Localization
- **Arxiv ID**: http://arxiv.org/abs/1611.02776v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.02776v1)
- **Published**: 2016-11-08 23:59:16+00:00
- **Updated**: 2016-11-08 23:59:16+00:00
- **Authors**: Daoyuan Jia, Yongchi Su, Chunping Li
- **Comment**: will update soon
- **Journal**: None
- **Summary**: We present an accurate and robust method for six degree of freedom image localization. There are two key-points of our method, 1. automatic immense photo synthesis and labeling from point cloud model and, 2. pose estimation with deep convolutional neural networks regression. Our model can directly regresses 6-DOF camera poses from images, accurately describing where and how it was captured. We achieved an accuracy within 1 meters and 1 degree on our out-door dataset, which covers about 2 acres on our school campus.



