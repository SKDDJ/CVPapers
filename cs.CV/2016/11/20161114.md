# Arxiv Papers in cs.CV on 2016-11-14
### Convolutional Regression for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1611.04215v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.04215v2)
- **Published**: 2016-11-14 01:10:21+00:00
- **Updated**: 2016-11-15 07:53:11+00:00
- **Authors**: Kai Chen, Wenbing Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, discriminatively learned correlation filters (DCF) has drawn much attention in visual object tracking community. The success of DCF is potentially attributed to the fact that a large amount of samples are utilized to train the ridge regression model and predict the location of object. To solve the regression problem in an efficient way, these samples are all generated by circularly shifting from a search patch. However, these synthetic samples also induce some negative effects which weaken the robustness of DCF based trackers.   In this paper, we propose a Convolutional Regression framework for visual tracking (CRT). Instead of learning the linear regression model in a closed form, we try to solve the regression problem by optimizing a one-channel-output convolution layer with Gradient Descent (GD). In particular, the receptive field size of the convolution layer is set to the size of object. Contrary to DCF, it is possible to incorporate all "real" samples clipped from the whole image. A critical issue of the GD approach is that most of the convolutional samples are negative and the contribution of positive samples will be suppressed. To address this problem, we propose a novel Automatic Hard Negative Mining method to eliminate easy negatives and enhance positives. Extensive experiments are conducted on a widely-used benchmark with 100 sequences. The results show that the proposed algorithm achieves outstanding performance and outperforms almost all the existing DCF based algorithms.



### Growing Interpretable Part Graphs on ConvNets via Multi-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1611.04246v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.04246v2)
- **Published**: 2016-11-14 04:13:37+00:00
- **Updated**: 2017-03-13 07:23:20+00:00
- **Authors**: Quanshi Zhang, Ruiming Cao, Ying Nian Wu, Song-Chun Zhu
- **Comment**: in the Thirty-First AAAI Conference on Artificial Intelligence
  (AAAI-17)
- **Journal**: None
- **Summary**: This paper proposes a learning strategy that extracts object-part concepts from a pre-trained convolutional neural network (CNN), in an attempt to 1) explore explicit semantics hidden in CNN units and 2) gradually grow a semantically interpretable graphical model on the pre-trained CNN for hierarchical object understanding. Given part annotations on very few (e.g., 3-12) objects, our method mines certain latent patterns from the pre-trained CNN and associates them with different semantic parts. We use a four-layer And-Or graph to organize the mined latent patterns, so as to clarify their internal semantic hierarchy. Our method is guided by a small number of part annotations, and it achieves superior performance (about 13%-107% improvement) in part center prediction on the PASCAL VOC and ImageNet datasets.



### Baseline CNN structure analysis for facial expression recognition
- **Arxiv ID**: http://arxiv.org/abs/1611.04251v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.04251v1)
- **Published**: 2016-11-14 04:57:18+00:00
- **Updated**: 2016-11-14 04:57:18+00:00
- **Authors**: Minchul Shin, Munsang Kim, Dong-Soo Kwon
- **Comment**: 6 pages, RO-MAN2016 Conference
- **Journal**: None
- **Summary**: We present a baseline convolutional neural network (CNN) structure and image preprocessing methodology to improve facial expression recognition algorithm using CNN. To analyze the most efficient network structure, we investigated four network structures that are known to show good performance in facial expression recognition. Moreover, we also investigated the effect of input image preprocessing methods. Five types of data input (raw, histogram equalization, isotropic smoothing, diffusion-based normalization, difference of Gaussian) were tested, and the accuracy was compared. We trained 20 different CNN models (4 networks x 5 data input types) and verified the performance of each network with test images from five different databases. The experiment result showed that a three-layer structure consisting of a simple convolutional and a max pooling layer with histogram equalization image input was the most efficient. We describe the detailed training procedure and analyze the result of the test accuracy based on considerable observation.



### A DNN Framework For Text Image Rectification From Planar Transformations
- **Arxiv ID**: http://arxiv.org/abs/1611.04298v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.04298v1)
- **Published**: 2016-11-14 09:40:38+00:00
- **Updated**: 2016-11-14 09:40:38+00:00
- **Authors**: Chengzhe Yan, Jie Hu, Changshui Zhang
- **Comment**: 9 pages, 10 figures
- **Journal**: None
- **Summary**: In this paper, a novel neural network architecture is proposed attempting to rectify text images with mild assumptions. A new dataset of text images is collected to verify our model and open to public. We explored the capability of deep neural network in learning geometric transformation and found the model could segment the text image without explicit supervised segmentation information. Experiments show the architecture proposed can restore planar transformations with wonderful robustness and effectiveness.



### Herding Generalizes Diverse M -Best Solutions
- **Arxiv ID**: http://arxiv.org/abs/1611.04353v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.04353v2)
- **Published**: 2016-11-14 12:13:58+00:00
- **Updated**: 2017-01-30 13:40:12+00:00
- **Authors**: Ece Ozkan, Gemma Roig, Orcun Goksel, Xavier Boix
- **Comment**: 8 pages, 2 algorithms, 3 figures
- **Journal**: None
- **Summary**: We show that the algorithm to extract diverse M -solutions from a Conditional Random Field (called divMbest [1]) takes exactly the form of a Herding procedure [2], i.e. a deterministic dynamical system that produces a sequence of hypotheses that respect a set of observed moment constraints. This generalization enables us to invoke properties of Herding that show that divMbest enforces implausible constraints which may yield wrong assumptions for some problem settings. Our experiments in semantic segmentation demonstrate that seeing divMbest as an instance of Herding leads to better alternatives for the implausible constraints of divMbest.



### Selfie Detection by Synergy-Constraint Based Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1611.04357v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.04357v1)
- **Published**: 2016-11-14 12:22:34+00:00
- **Updated**: 2016-11-14 12:22:34+00:00
- **Authors**: Yashas Annadani, Vijayakrishna Naganoor, Akshay Kumar Jagadish, Krishnan Chemmangat
- **Comment**: 8 Pages, Accepted for Publication at IEEE SITIS 2016
- **Journal**: None
- **Summary**: Categorisation of huge amount of data on the multimedia platform is a crucial task. In this work, we propose a novel approach to address the subtle problem of selfie detection for image database segregation on the web, given rapid rise in number of selfies clicked. A Convolutional Neural Network (CNN) is modeled to learn a synergy feature in the common subspace of head and shoulder orientation, derived from Local Binary Pattern (LBP) and Histogram of Oriented Gradients (HOG) features respectively. This synergy was captured by projecting the aforementioned features using Canonical Correlation Analysis (CCA). We show that the resulting network's convolutional activations in the neighbourhood of spatial keypoints captured by SIFT are discriminative for selfie-detection. In general, proposed approach aids in capturing intricacies present in the image data and has the potential for usage in other subtle image analysis scenarios apart from just selfie detection. We investigate and analyse the performance of popular CNN architectures (GoogleNet, AlexNet), used for other image classification tasks, when subjected to the task of detecting the selfies on the multimedia platform. The results of the proposed approach are compared with these popular architectures on a dataset of ninety thousand images comprising of roughly equal number of selfies and non-selfies. Experimental results on this dataset shows the effectiveness of the proposed approach.



### Joint Graph Decomposition and Node Labeling: Problem, Algorithms, Applications
- **Arxiv ID**: http://arxiv.org/abs/1611.04399v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.DM
- **Links**: [PDF](http://arxiv.org/pdf/1611.04399v2)
- **Published**: 2016-11-14 14:48:38+00:00
- **Updated**: 2017-02-21 09:50:46+00:00
- **Authors**: Evgeny Levinkov, Jonas Uhrig, Siyu Tang, Mohamed Omran, Eldar Insafutdinov, Alexander Kirillov, Carsten Rother, Thomas Brox, Bernt Schiele, Bjoern Andres
- **Comment**: None
- **Journal**: None
- **Summary**: We state a combinatorial optimization problem whose feasible solutions define both a decomposition and a node labeling of a given graph. This problem offers a common mathematical abstraction of seemingly unrelated computer vision tasks, including instance-separating semantic segmentation, articulated human body pose estimation and multiple object tracking. Conceptually, the problem we state generalizes the unconstrained integer quadratic program and the minimum cost lifted multicut problem, both of which are NP-hard. In order to find feasible solutions efficiently, we define two local search algorithms that converge monotonously to a local optimum, offering a feasible solution at any time. To demonstrate their effectiveness in tackling computer vision tasks, we apply these algorithms to instances of the problem that we construct from published data, using published algorithms. We report state-of-the-art application-specific accuracy for the three above-mentioned applications.



### Automatic discovery of discriminative parts as a quadratic assignment problem
- **Arxiv ID**: http://arxiv.org/abs/1611.04413v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.04413v1)
- **Published**: 2016-11-14 15:17:48+00:00
- **Updated**: 2016-11-14 15:17:48+00:00
- **Authors**: Ronan Sicre, Julien Rabin, Yannis Avrithis, Teddy Furon, Frederic Jurie
- **Comment**: None
- **Journal**: None
- **Summary**: Part-based image classification consists in representing categories by small sets of discriminative parts upon which a representation of the images is built. This paper addresses the question of how to automatically learn such parts from a set of labeled training images. The training of parts is cast as a quadratic assignment problem in which optimal correspondences between image regions and parts are automatically learned. The paper analyses different assignment strategies and thoroughly evaluates them on two public datasets: Willow actions and MIT 67 scenes. State-of-the art results are obtained on these datasets.



### Can fully convolutional networks perform well for general image restoration problems?
- **Arxiv ID**: http://arxiv.org/abs/1611.04481v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.04481v2)
- **Published**: 2016-11-14 17:13:29+00:00
- **Updated**: 2017-04-13 15:04:50+00:00
- **Authors**: Subhajit Chaudhury, Hiya Roy
- **Comment**: Accepted at IAPR MVA 2017
- **Journal**: None
- **Summary**: We present a fully convolutional network(FCN) based approach for color image restoration. FCNs have recently shown remarkable performance for high-level vision problem like semantic segmentation. In this paper, we investigate if FCN models can show promising performance for low-level problems like image restoration as well. We propose a fully convolutional model, that learns a direct end-to-end mapping between the corrupted images as input and the desired clean images as output. Our proposed method takes inspiration from domain transformation techniques but presents a data-driven task specific approach where filters for novel basis projection, task dependent coefficient alterations, and image reconstruction are represented as convolutional networks. Experimental results show that our FCN model outperforms traditional sparse coding based methods and demonstrates competitive performance compared to the state-of-the-art methods for image denoising. We further show that our proposed model can solve the difficult problem of blind image inpainting and can produce reconstructed images of impressive visual quality.



### Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot
- **Arxiv ID**: http://arxiv.org/abs/1611.04503v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1611.04503v3)
- **Published**: 2016-11-14 18:07:54+00:00
- **Updated**: 2017-07-23 15:52:08+00:00
- **Authors**: Hideki Nakayama, Noriki Nishida
- **Comment**: Some error corrections in Sect.2.2 and Table 5, Machine Translation,
  2017
- **Journal**: None
- **Summary**: We propose an approach to build a neural machine translation system with no supervised resources (i.e., no parallel corpora) using multimodal embedded representation over texts and images. Based on the assumption that text documents are often likely to be described with other multimedia information (e.g., images) somewhat related to the content, we try to indirectly estimate the relevance between two languages. Using multimedia as the "pivot", we project all modalities into one common hidden space where samples belonging to similar semantic concepts should come close to each other, whatever the observed space of each sample is. This modality-agnostic representation is the key to bridging the gap between different modalities. Putting a decoder on top of it, our network can flexibly draw the outputs from any input modality. Notably, in the testing phase, we need only source language texts as the input for translation. In experiments, we tested our method on two benchmarks to show that it can achieve reasonable translation performance. We compared and investigated several possible implementations and found that an end-to-end model that simultaneously optimized both rank loss in multimodal encoders and cross-entropy loss in decoders performed the best.



### Fast Task-Specific Target Detection via Graph Based Constraints Representation and Checking
- **Arxiv ID**: http://arxiv.org/abs/1611.04519v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.04519v2)
- **Published**: 2016-11-14 19:01:06+00:00
- **Updated**: 2016-11-23 04:57:36+00:00
- **Authors**: Went Luan, Yezhou Yang, Cornelia Fermuller, John S. Baras
- **Comment**: The paper is withdrawn for another work's convenience. We will upload
  it later
- **Journal**: None
- **Summary**: In this work, we present a fast target detection framework for real-world robotics applications. Considering that an intelligent agent attends to a task-specific object target during execution, our goal is to detect the object efficiently. We propose the concept of early recognition, which influences the candidate proposal process to achieve fast and reliable detection performance. To check the target constraints efficiently, we put forward a novel policy to generate a sub-optimal checking order, and prove that it has bounded time cost compared to the optimal checking sequence, which is not achievable in polynomial time. Experiments on two different scenarios: 1) rigid object and 2) non-rigid body part detection validate our pipeline. To show that our method is widely applicable, we further present a human-robot interaction system based on our non-rigid body part detection.



### 3-D Convolutional Neural Networks for Glioblastoma Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1611.04534v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.04534v1)
- **Published**: 2016-11-14 19:21:33+00:00
- **Updated**: 2016-11-14 19:21:33+00:00
- **Authors**: Darvin Yi, Mu Zhou, Zhao Chen, Olivier Gevaert
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) have emerged as powerful tools for learning discriminative image features. In this paper, we propose a framework of 3-D fully CNN models for Glioblastoma segmentation from multi-modality MRI data. By generalizing CNN models to true 3-D convolutions in learning 3-D tumor MRI data, the proposed approach utilizes a unique network architecture to decouple image pixels. Specifically, we design a convolutional layer with pre-defined Difference- of-Gaussian (DoG) filters to perform true 3-D convolution incorporating local neighborhood information at each pixel. We then use three trained convolutional layers that act to decouple voxels from the initial 3-D convolution. The proposed framework allows identification of high-level tumor structures on MRI. We evaluate segmentation performance on the BRATS segmentation dataset with 274 tumor samples. Extensive experimental results demonstrate encouraging performance of the proposed approach comparing to the state-of-the-art methods. Our data-driven approach achieves a median Dice score accuracy of 89% in whole tumor glioblastoma segmentation, revealing a generalized low-bias possibility to learn from medium-size MRI datasets.



### When Saliency Meets Sentiment: Understanding How Image Content Invokes Emotion and Sentiment
- **Arxiv ID**: http://arxiv.org/abs/1611.04636v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1611.04636v1)
- **Published**: 2016-11-14 22:02:09+00:00
- **Updated**: 2016-11-14 22:02:09+00:00
- **Authors**: Honglin Zheng, Tianlang Chen, Jiebo Luo
- **Comment**: 7 pages, 5 figures, submitted to AAAI-17
- **Journal**: None
- **Summary**: Sentiment analysis is crucial for extracting social signals from social media content. Due to the prevalence of images in social media, image sentiment analysis is receiving increasing attention in recent years. However, most existing systems are black-boxes that do not provide insight on how image content invokes sentiment and emotion in the viewers. Psychological studies have confirmed that salient objects in an image often invoke emotions. In this work, we investigate more fine-grained and more comprehensive interaction between visual saliency and visual sentiment. In particular, we partition images in several primary scene-type dimensions, including: open-closed, natural-manmade, indoor-outdoor, and face-noface. Using state of the art saliency detection algorithm and sentiment classification algorithm, we examine how the sentiment of the salient region(s) in an image relates to the overall sentiment of the image. The experiments on a representative image emotion dataset have shown interesting correlation between saliency and sentiment in different scene types and in turn shed light on the mechanism of visual sentiment evocation.



