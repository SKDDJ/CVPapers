# Arxiv Papers in cs.CV on 2016-11-23
### Learning Joint Feature Adaptation for Zero-Shot Recognition
- **Arxiv ID**: http://arxiv.org/abs/1611.07593v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07593v2)
- **Published**: 2016-11-23 01:13:37+00:00
- **Updated**: 2016-12-03 03:17:02+00:00
- **Authors**: Ziming Zhang, Venkatesh Saligrama
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-shot recognition (ZSR) aims to recognize target-domain data instances of unseen classes based on the models learned from associated pairs of seen-class source and target domain data. One of the key challenges in ZSR is the relative scarcity of source-domain features (e.g. one feature vector per class), which do not fully account for wide variability in target-domain instances. In this paper we propose a novel framework of learning data-dependent feature transforms for scoring similarity between an arbitrary pair of source and target data instances to account for the wide variability in target domain. Our proposed approach is based on optimizing over a parameterized family of local feature displacements that maximize the source-target adaptive similarity functions. Accordingly we propose formulating zero-shot learning (ZSL) using latent structural SVMs to learn our similarity functions from training data. As demonstration we design a specific algorithm under the proposed framework involving bilinear similarity functions and regularized least squares as penalties for feature displacement. We test our approach on several benchmark datasets for ZSR and show significant improvement over the state-of-the-art. For instance, on aP&Y dataset we can achieve 80.89% in terms of recognition accuracy, outperforming the state-of-the-art by 11.15%.



### Fast Fourier Color Constancy
- **Arxiv ID**: http://arxiv.org/abs/1611.07596v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07596v3)
- **Published**: 2016-11-23 01:28:43+00:00
- **Updated**: 2020-08-13 17:14:28+00:00
- **Authors**: Jonathan T. Barron, Yun-Ta Tsai
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: We present Fast Fourier Color Constancy (FFCC), a color constancy algorithm which solves illuminant estimation by reducing it to a spatial localization task on a torus. By operating in the frequency domain, FFCC produces lower error rates than the previous state-of-the-art by 13-20% while being 250-3000 times faster. This unconventional approach introduces challenges regarding aliasing, directional statistics, and preconditioning, which we address. By producing a complete posterior distribution over illuminants instead of a single illuminant estimate, FFCC enables better training techniques, an effective temporal smoothing technique, and richer methods for error analysis. Our implementation of FFCC runs at ~700 frames per second on a mobile device, allowing it to be used as an accurate, real-time, temporally-coherent automatic white balance algorithm.



### T-CONV: A Convolutional Neural Network For Multi-scale Taxi Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/1611.07635v3
- **DOI**: None
- **Categories**: **cs.CV**, I.2.6; H.2.8
- **Links**: [PDF](http://arxiv.org/pdf/1611.07635v3)
- **Published**: 2016-11-23 04:23:49+00:00
- **Updated**: 2017-08-11 10:26:29+00:00
- **Authors**: Jianming Lv, Qing Li, Xintong Wang
- **Comment**: None
- **Journal**: BigComp 2018
- **Summary**: Precise destination prediction of taxi trajectories can benefit many intelligent location based services such as accurate ad for passengers. Traditional prediction approaches, which treat trajectories as one-dimensional sequences and process them in single scale, fail to capture the diverse two-dimensional patterns of trajectories in different spatial scales. In this paper, we propose T-CONV which models trajectories as two-dimensional images, and adopts multi-layer convolutional neural networks to combine multi-scale trajectory patterns to achieve precise prediction. Furthermore, we conduct gradient analysis to visualize the multi-scale spatial patterns captured by T-CONV and extract the areas with distinct influence on the ultimate prediction. Finally, we integrate multiple local enhancement convolutional fields to explore these important areas deeply for better prediction. Comprehensive experiments based on real trajectory data show that T-CONV can achieve higher accuracy than the state-of-the-art methods.



### Multigrid Neural Architectures
- **Arxiv ID**: http://arxiv.org/abs/1611.07661v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1611.07661v2)
- **Published**: 2016-11-23 06:55:53+00:00
- **Updated**: 2017-05-11 19:24:33+00:00
- **Authors**: Tsung-Wei Ke, Michael Maire, Stella X. Yu
- **Comment**: updated with ImageNet results; to appear at CVPR 2017
- **Journal**: None
- **Summary**: We propose a multigrid extension of convolutional neural networks (CNNs). Rather than manipulating representations living on a single spatial grid, our network layers operate across scale space, on a pyramid of grids. They consume multigrid inputs and produce multigrid outputs; convolutional filters themselves have both within-scale and cross-scale extent. This aspect is distinct from simple multiscale designs, which only process the input at different scales. Viewed in terms of information flow, a multigrid network passes messages across a spatial pyramid. As a consequence, receptive field size grows exponentially with depth, facilitating rapid integration of context. Most critically, multigrid structure enables networks to learn internal attention and dynamic routing mechanisms, and use them to accomplish tasks on which modern CNNs fail.   Experiments demonstrate wide-ranging performance advantages of multigrid. On CIFAR and ImageNet classification tasks, flipping from a single grid to multigrid within the standard CNN paradigm improves accuracy, while being compute and parameter efficient. Multigrid is independent of other architectural choices; we show synergy in combination with residual connections. Multigrid yields dramatic improvement on a synthetic semantic segmentation dataset. Most strikingly, relatively shallow multigrid networks can learn to directly perform spatial transformation tasks, where, in contrast, current CNNs fail. Together, our results suggest that continuous evolution of features on a multigrid pyramid is a more powerful alternative to existing CNN designs on a flat grid.



### Video Captioning with Transferred Semantic Attributes
- **Arxiv ID**: http://arxiv.org/abs/1611.07675v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07675v1)
- **Published**: 2016-11-23 07:59:59+00:00
- **Updated**: 2016-11-23 07:59:59+00:00
- **Authors**: Yingwei Pan, Ting Yao, Houqiang Li, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: Automatically generating natural language descriptions of videos plays a fundamental challenge for computer vision community. Most recent progress in this problem has been achieved through employing 2-D and/or 3-D Convolutional Neural Networks (CNN) to encode video content and Recurrent Neural Networks (RNN) to decode a sentence. In this paper, we present Long Short-Term Memory with Transferred Semantic Attributes (LSTM-TSA)---a novel deep architecture that incorporates the transferred semantic attributes learnt from images and videos into the CNN plus RNN framework, by training them in an end-to-end manner. The design of LSTM-TSA is highly inspired by the facts that 1) semantic attributes play a significant contribution to captioning, and 2) images and videos carry complementary semantics and thus can reinforce each other for captioning. To boost video captioning, we propose a novel transfer unit to model the mutually correlated attributes learnt from images and videos. Extensive experiments are conducted on three public datasets, i.e., MSVD, M-VAD and MPII-MD. Our proposed LSTM-TSA achieves to-date the best published performance in sentence generation on MSVD: 52.8% and 74.0% in terms of BLEU@4 and CIDEr-D. Superior results when compared to state-of-the-art methods are also reported on M-VAD and MPII-MD.



### Generalized Fourier-Bessel operator and almost-periodic interpolation and approximation
- **Arxiv ID**: http://arxiv.org/abs/1612.00056v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, math.GR
- **Links**: [PDF](http://arxiv.org/pdf/1612.00056v1)
- **Published**: 2016-11-23 08:12:34+00:00
- **Updated**: 2016-11-23 08:12:34+00:00
- **Authors**: Jean-Paul Gauthier, Dario Prandi
- **Comment**: 15 pages, 2 figures
- **Journal**: None
- **Summary**: We consider functions $f$ of two real variables, given as trigonometric functions over a finite set $F$ of frequencies. This set is assumed to be closed under rotations in the frequency plane of angle $\frac{2k\pi}{M}$ for some integer $M$. Firstly, we address the problem of evaluating these functions over a similar finite set $E$ in the space plane and, secondly, we address the problems of interpolating or approximating a function $g$ of two variables by such an $f$ over the grid $E.$ In particular, for this aim, we establish an abstract factorization theorem for the evaluation function, which is a key point for an efficient numerical solution to these problems. This result is based on the very special structure of the group $SE(2,N)$, subgroup of the group $SE(2)$ of motions of the plane corresponding to discrete rotations, which is a maximally almost periodic group.   Although the motivation of this paper comes from our previous works on biomimetic image reconstruction and pattern recognition, where these questions appear naturally, this topic is related with several classical problems: the FFT in polar coordinates, the Non Uniform FFT, the evaluation of general trigonometric polynomials, and so on.



### UniMiB SHAR: a new dataset for human activity recognition using acceleration data from smartphones
- **Arxiv ID**: http://arxiv.org/abs/1611.07688v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07688v5)
- **Published**: 2016-11-23 08:45:49+00:00
- **Updated**: 2017-08-08 09:55:41+00:00
- **Authors**: Daniela Micucci, Marco Mobilio, Paolo Napoletano
- **Comment**: submitted to MDPI Sensors
- **Journal**: None
- **Summary**: Smartphones, smartwatches, fitness trackers, and ad-hoc wearable devices are being increasingly used to monitor human activities. Data acquired by the hosted sensors are usually processed by machine-learning-based algorithms to classify human activities. The success of those algorithms mostly depends on the availability of training (labeled) data that, if made publicly available, would allow researchers to make objective comparisons between techniques. Nowadays, publicly available data sets are few, often contain samples from subjects with too similar characteristics, and very often lack of specific information so that is not possible to select subsets of samples according to specific criteria. In this article, we present a new dataset of acceleration samples acquired with an Android smartphone designed for human activity recognition and fall detection. The dataset includes 11,771 samples of both human activities and falls performed by 30 subjects of ages ranging from 18 to 60 years. Samples are divided in 17 fine grained classes grouped in two coarse grained classes: one containing samples of 9 types of activities of daily living (ADL) and the other containing samples of 8 types of falls. The dataset has been stored to include all the information useful to select samples according to different criteria, such as the type of ADL, the age, the gender, and so on. Finally, the dataset has been benchmarked with four different classifiers and with two different feature vectors. We evaluated four different classification tasks: fall vs no fall, 9 activities, 8 falls, 17 activities and falls. For each classification task we performed a subject-dependent and independent evaluation. The major findings of the evaluation are the following: i) it is more difficult to distinguish between types of falls than types of activities; ii) subject-dependent evaluation outperforms the subject-independent one



### 3D Menagerie: Modeling the 3D shape and pose of animals
- **Arxiv ID**: http://arxiv.org/abs/1611.07700v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07700v2)
- **Published**: 2016-11-23 09:30:50+00:00
- **Updated**: 2017-04-12 10:39:46+00:00
- **Authors**: Silvia Zuffi, Angjoo Kanazawa, David Jacobs, Michael J. Black
- **Comment**: Accepted at CVPR 2017 (camera ready version)
- **Journal**: None
- **Summary**: There has been significant work on learning realistic, articulated, 3D models of the human body. In contrast, there are few such models of animals, despite many applications. The main challenge is that animals are much less cooperative than humans. The best human body models are learned from thousands of 3D scans of people in specific poses, which is infeasible with live animals. Consequently, we learn our model from a small set of 3D scans of toy figurines in arbitrary poses. We employ a novel part-based shape model to compute an initial registration to the scans. We then normalize their pose, learn a statistical shape model, and refine the registrations and the model together. In this way, we accurately align animal scans from different quadruped families with very different shapes and poses. With the registration to a common template we learn a shape space representing animals including lions, cats, dogs, horses, cows and hippos. Animal shapes can be sampled from the model, posed, animated, and fit to data. We demonstrate generalization by fitting it to images of real animals including species not seen in training.



### 'Part'ly first among equals: Semantic part-based benchmarking for state-of-the-art object recognition systems
- **Arxiv ID**: http://arxiv.org/abs/1611.07703v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07703v2)
- **Published**: 2016-11-23 09:38:09+00:00
- **Updated**: 2016-11-24 14:06:06+00:00
- **Authors**: Ravi Kiran Sarvadevabhatla, Shanthakumar Venkatraman, R. Venkatesh Babu
- **Comment**: Extended version of our ACCV-2016 paper. Author formatting modified
- **Journal**: None
- **Summary**: An examination of object recognition challenge leaderboards (ILSVRC, PASCAL-VOC) reveals that the top-performing classifiers typically exhibit small differences amongst themselves in terms of error rate/mAP. To better differentiate the top performers, additional criteria are required. Moreover, the (test) images, on which the performance scores are based, predominantly contain fully visible objects. Therefore, `harder' test images, mimicking the challenging conditions (e.g. occlusion) in which humans routinely recognize objects, need to be utilized for benchmarking. To address the concerns mentioned above, we make two contributions. First, we systematically vary the level of local object-part content, global detail and spatial context in images from PASCAL VOC 2010 to create a new benchmarking dataset dubbed PPSS-12. Second, we propose an object-part based benchmarking procedure which quantifies classifiers' robustness to a range of visibility and contextual settings. The benchmarking procedure relies on a semantic similarity measure that naturally addresses potential semantic granularity differences between the category labels in training and test datasets, thus eliminating manual mapping. We use our procedure on the PPSS-12 dataset to benchmark top-performing classifiers trained on the ILSVRC-2012 dataset. Our results show that the proposed benchmarking procedure enables additional differentiation among state-of-the-art object classifiers in terms of their ability to handle missing content and insufficient object detail. Given this capability for additional differentiation, our approach can potentially supplement existing benchmarking procedures used in object recognition challenge leaderboards.



### Fully Convolutional Instance-aware Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1611.07709v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07709v2)
- **Published**: 2016-11-23 09:53:57+00:00
- **Updated**: 2017-04-10 09:00:54+00:00
- **Authors**: Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, Yichen Wei
- **Comment**: None
- **Journal**: None
- **Summary**: We present the first fully convolutional end-to-end solution for instance-aware semantic segmentation task. It inherits all the merits of FCNs for semantic segmentation and instance mask proposal. It performs instance mask prediction and classification jointly. The underlying convolutional representation is fully shared between the two sub-tasks, as well as between all regions of interest. The proposed network is highly integrated and achieves state-of-the-art performance in both accuracy and efficiency. It wins the COCO 2016 segmentation competition by a large margin. Code would be released at \url{https://github.com/daijifeng001/TA-FCN}.



### Deep Feature Flow for Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/1611.07715v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07715v2)
- **Published**: 2016-11-23 10:06:30+00:00
- **Updated**: 2017-06-05 11:41:51+00:00
- **Authors**: Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, Yichen Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neutral networks have achieved great success on image recognition tasks. Yet, it is non-trivial to transfer the state-of-the-art image recognition networks to videos as per-frame evaluation is too slow and unaffordable. We present deep feature flow, a fast and accurate framework for video recognition. It runs the expensive convolutional sub-network only on sparse key frames and propagates their deep feature maps to other frames via a flow field. It achieves significant speedup as flow computation is relatively fast. The end-to-end training of the whole architecture significantly boosts the recognition accuracy. Deep feature flow is flexible and general. It is validated on two recent large scale video datasets. It makes a large step towards practical video recognition.



### Deep Convolutional Neural Networks with Merge-and-Run Mappings
- **Arxiv ID**: http://arxiv.org/abs/1611.07718v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07718v2)
- **Published**: 2016-11-23 10:08:40+00:00
- **Updated**: 2017-07-19 07:27:21+00:00
- **Authors**: Liming Zhao, Jingdong Wang, Xi Li, Zhuowen Tu, Wenjun Zeng
- **Comment**: None
- **Journal**: None
- **Summary**: A deep residual network, built by stacking a sequence of residual blocks, is easy to train, because identity mappings skip residual branches and thus improve information flow. To further reduce the training difficulty, we present a simple network architecture, deep merge-and-run neural networks. The novelty lies in a modularized building block, merge-and-run block, which assembles residual branches in parallel through a merge-and-run mapping: Average the inputs of these residual branches (Merge), and add the average to the output of each residual branch as the input of the subsequent residual branch (Run), respectively. We show that the merge-and-run mapping is a linear idempotent function in which the transformation matrix is idempotent, and thus improves information flow, making training easy. In comparison to residual networks, our networks enjoy compelling advantages: they contain much shorter paths, and the width, i.e., the number of channels, is increased. We evaluate the performance on the standard recognition tasks. Our approach demonstrates consistent improvements over ResNets with the comparable setup, and achieves competitive results (e.g., $3.57\%$ testing error on CIFAR-$10$, $19.00\%$ on CIFAR-$100$, $1.51\%$ on SVHN).



### iCaRL: Incremental Classifier and Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1611.07725v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1611.07725v2)
- **Published**: 2016-11-23 10:24:11+00:00
- **Updated**: 2017-04-14 16:41:02+00:00
- **Authors**: Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, Christoph H. Lampert
- **Comment**: Accepted paper at CVPR 2017
- **Journal**: None
- **Summary**: A major open problem on the road to artificial intelligence is the development of incrementally learning systems that learn about more and more concepts over time from a stream of data. In this work, we introduce a new training strategy, iCaRL, that allows learning in such a class-incremental way: only the training data for a small number of classes has to be present at the same time and new classes can be added progressively. iCaRL learns strong classifiers and a data representation simultaneously. This distinguishes it from earlier works that were fundamentally limited to fixed data representations and therefore incompatible with deep learning architectures. We show by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can learn many classes incrementally over a long period of time where other strategies quickly fail.



### PoseTrack: Joint Multi-Person Pose Estimation and Tracking
- **Arxiv ID**: http://arxiv.org/abs/1611.07727v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07727v3)
- **Published**: 2016-11-23 10:30:06+00:00
- **Updated**: 2017-04-07 14:16:38+00:00
- **Authors**: Umar Iqbal, Anton Milan, Juergen Gall
- **Comment**: Accepted to CVPR 2017
- **Journal**: None
- **Summary**: In this work, we introduce the challenging problem of joint multi-person pose estimation and tracking of an unknown number of persons in unconstrained videos. Existing methods for multi-person pose estimation in images cannot be applied directly to this problem, since it also requires to solve the problem of person association over time in addition to the pose estimation for each person. We therefore propose a novel method that jointly models multi-person pose estimation and tracking in a single formulation. To this end, we represent body joint detections in a video by a spatio-temporal graph and solve an integer linear program to partition the graph into sub-graphs that correspond to plausible body pose trajectories for each person. The proposed approach implicitly handles occlusion and truncation of persons. Since the problem has not been addressed quantitatively in the literature, we introduce a challenging "Multi-Person PoseTrack" dataset, and also propose a completely unconstrained evaluation protocol that does not make any assumptions about the scale, size, location or the number of persons. Finally, we evaluate the proposed approach and several baseline methods on our new dataset.



### Convergence Analysis of MAP based Blur Kernel Estimation
- **Arxiv ID**: http://arxiv.org/abs/1611.07752v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07752v2)
- **Published**: 2016-11-23 11:53:15+00:00
- **Updated**: 2017-08-28 14:05:20+00:00
- **Authors**: Sunghyun Cho, Seungyong Lee
- **Comment**: None
- **Journal**: None
- **Summary**: One popular approach for blind deconvolution is to formulate a maximum a posteriori (MAP) problem with sparsity priors on the gradients of the latent image, and then alternatingly estimate the blur kernel and the latent image. While several successful MAP based methods have been proposed, there has been much controversy and confusion about their convergence, because sparsity priors have been shown to prefer blurry images to sharp natural images. In this paper, we revisit this problem and provide an analysis on the convergence of MAP based approaches. We first introduce a slight modification to a conventional joint energy function for blind deconvolution. The reformulated energy function yields the same alternating estimation process, but more clearly reveals how blind deconvolution works. We then show the energy function can actually favor the right solution instead of the no-blur solution under certain conditions, which explains the success of previous MAP based approaches. The reformulated energy function and our conditions for the convergence also provide a way to compare the qualities of different blur kernels, and we demonstrate its applicability to automatic blur kernel size selection, blur kernel estimation using light streaks, and defocus estimation.



### Multi-View 3D Object Detection Network for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1611.07759v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07759v3)
- **Published**: 2016-11-23 12:08:38+00:00
- **Updated**: 2017-06-22 03:23:51+00:00
- **Authors**: Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, Tian Xia
- **Comment**: To appear in IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR) 2017
- **Journal**: None
- **Summary**: This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efficiently from the bird's eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25% and 30% AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 10.3% higher AP than the state-of-the-art on the hard data among the LIDAR-based methods.



### Multiframe Motion Coupling for Video Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/1611.07767v2
- **DOI**: None
- **Categories**: **cs.CV**, math.OC, I.4; G.1.6; G.4
- **Links**: [PDF](http://arxiv.org/pdf/1611.07767v2)
- **Published**: 2016-11-23 12:36:45+00:00
- **Updated**: 2017-12-04 10:06:43+00:00
- **Authors**: Jonas Geiping, Hendrik Dirks, Daniel Cremers, Michael Moeller
- **Comment**: None
- **Journal**: None
- **Summary**: The idea of video super resolution is to use different view points of a single scene to enhance the overall resolution and quality. Classical energy minimization approaches first establish a correspondence of the current frame to all its neighbors in some radius and then use this temporal information for enhancement. In this paper, we propose the first variational super resolution approach that computes several super resolved frames in one batch optimization procedure by incorporating motion information between the high-resolution image frames themselves. As a consequence, the number of motion estimation problems grows linearly in the number of frames, opposed to a quadratic growth of classical methods and temporal consistency is enforced naturally. We use infimal convolution regularization as well as an automatic parameter balancing scheme to automatically determine the reliability of the motion information and reweight the regularization locally. We demonstrate that our approach yields state-of-the-art results and even is competitive with machine learning approaches.



### Adaptive Down-Sampling and Dimension Reduction in Time Elastic Kernel Machines for Efficient Recognition of Isolated Gestures
- **Arxiv ID**: http://arxiv.org/abs/1611.07781v1
- **DOI**: 10.1007/978-3-319-45763-5_3
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1611.07781v1)
- **Published**: 2016-11-23 13:18:17+00:00
- **Updated**: 2016-11-23 13:18:17+00:00
- **Authors**: Pierre-François Marteau, Sylvie Gibet, Clément Reverdy
- **Comment**: None
- **Journal**: Guillet, Fabrice and Pinaud, Bruno and Venturini, Gilles. Advances
  in Knowledge Discovery and Management: volume 6, Volume (665), Springer
  International Publishing, pp.39 - 59, 2016, Studies in Computational
  Intelligence, 978-3-319-45763-5
- **Summary**: In the scope of gestural action recognition, the size of the feature vector representing movements is in general quite large especially when full body movements are considered. Furthermore, this feature vector evolves during the movement performance so that a complete movement is fully represented by a matrix M of size DxT , whose element M i, j represents the value of feature i at timestamps j. Many studies have addressed dimensionality reduction considering only the size of the feature vector lying in R D to reduce both the variability of gestural sequences expressed in the reduced space, and the computational complexity of their processing. In return, very few of these methods have explicitly addressed the dimensionality reduction along the time axis. Yet this is a major issue when considering the use of elastic distances which are characterized by a quadratic complexity along the time axis. We present in this paper an evaluation of straightforward approaches aiming at reducing the dimensionality of the matrix M for each movement, leading to consider both the dimensionality reduction of the feature vector as well as its reduction along the time axis. The dimensionality reduction of the feature vector is achieved by selecting remarkable joints in the skeleton performing the movement, basically the extremities of the articulatory chains composing the skeleton. The temporal dimen-sionality reduction is achieved using either a regular or adaptive down-sampling that seeks to minimize the reconstruction error of the movements. Elastic and Euclidean kernels are then compared through support vector machine learning. Two data sets 1 that are widely referenced in the domain of human gesture recognition, and quite distinctive in terms of quality of motion capture, are used for the experimental assessment of the proposed approaches. On these data sets we experimentally show that it is feasible, and possibly desirable, to significantly reduce simultaneously the size of the feature vector and the number of skeleton frames to represent body movements while maintaining a very good recognition rate. The method proves to give satisfactory results at a level currently reached by state-of-the-art methods on these data sets. We experimentally show that the computational complexity reduction that is obtained makes this approach eligible for real-time applications.



### Object Detection using Image Processing
- **Arxiv ID**: http://arxiv.org/abs/1611.07791v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07791v1)
- **Published**: 2016-11-23 13:48:07+00:00
- **Updated**: 2016-11-23 13:48:07+00:00
- **Authors**: Fares Jalled, Ilia Voronkov
- **Comment**: None
- **Journal**: None
- **Summary**: An Unmanned Ariel vehicle (UAV) has greater importance in the army for border security. The main objective of this article is to develop an OpenCV-Python code using Haar Cascade algorithm for object and face detection. Currently, UAVs are used for detecting and attacking the infiltrated ground targets. The main drawback for this type of UAVs is that sometimes the object are not properly detected, which thereby causes the object to hit the UAV. This project aims to avoid such unwanted collisions and damages of UAV. UAV is also used for surveillance that uses Voila-jones algorithm to detect and track humans. This algorithm uses cascade object detector function and vision. train function to train the algorithm. The main advantage of this code is the reduced processing time. The Python code was tested with the help of available database of video and image, the output was verified.



### Learning Invariant Representations Of Planar Curves
- **Arxiv ID**: http://arxiv.org/abs/1611.07807v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07807v2)
- **Published**: 2016-11-23 14:20:17+00:00
- **Updated**: 2017-02-16 21:44:00+00:00
- **Authors**: Gautam Pai, Aaron Wetzler, Ron Kimmel
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a metric learning framework for the construction of invariant geometric functions of planar curves for the Eucledian and Similarity group of transformations. We leverage on the representational power of convolutional neural networks to compute these geometric quantities. In comparison with axiomatic constructions, we show that the invariants approximated by the learning architectures have better numerical qualities such as robustness to noise, resiliency to sampling, as well as the ability to adapt to occlusion and partiality. Finally, we develop a novel multi-scale representation in a similarity metric learning paradigm.



### A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering
- **Arxiv ID**: http://arxiv.org/abs/1611.07810v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07810v2)
- **Published**: 2016-11-23 14:22:51+00:00
- **Updated**: 2017-02-05 17:51:19+00:00
- **Authors**: Tegan Maharaj, Nicolas Ballas, Anna Rohrbach, Aaron Courville, Christopher Pal
- **Comment**: None
- **Journal**: None
- **Summary**: While deep convolutional neural networks frequently approach or exceed human-level performance at benchmark tasks involving static images, extending this success to moving images is not straightforward. Having models which can learn to understand video is of interest for many applications, including content recommendation, prediction, summarization, event/object detection and understanding human visual perception, but many domains lack sufficient data to explore and perfect video models. In order to address the need for a simple, quantitative benchmark for developing and understanding video, we present MovieFIB, a fill-in-the-blank question-answering dataset with over 300,000 examples, based on descriptive video annotations for the visually impaired. In addition to presenting statistics and a description of the dataset, we perform a detailed analysis of 5 different models' predictions, and compare these with human performance. We investigate the relative importance of language, static (2D) visual features, and moving (3D) visual features; the effects of increasing dataset size, the number of frames sampled; and of vocabulary size. We illustrate that: this task is not solvable by a language model alone; our model combining 2D and 3D visual information indeed provides the best result; all models perform significantly worse than human-level. We provide human evaluations for responses given by different models and find that accuracy on the MovieFIB evaluation corresponds well with human judgement. We suggest avenues for improving video models, and hope that the proposed dataset can be useful for measuring and encouraging progress in this very interesting field.



### Coarse-to-Fine Volumetric Prediction for Single-Image 3D Human Pose
- **Arxiv ID**: http://arxiv.org/abs/1611.07828v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07828v2)
- **Published**: 2016-11-23 15:06:18+00:00
- **Updated**: 2017-07-26 12:10:16+00:00
- **Authors**: Georgios Pavlakos, Xiaowei Zhou, Konstantinos G. Derpanis, Kostas Daniilidis
- **Comment**: CVPR 2017 Camera Ready. Project Page:
  https://www.seas.upenn.edu/~pavlakos/projects/volumetric/
- **Journal**: None
- **Summary**: This paper addresses the challenge of 3D human pose estimation from a single color image. Despite the general success of the end-to-end learning paradigm, top performing approaches employ a two-step solution consisting of a Convolutional Network (ConvNet) for 2D joint localization and a subsequent optimization step to recover 3D pose. In this paper, we identify the representation of 3D pose as a critical issue with current ConvNet approaches and make two important contributions towards validating the value of end-to-end learning for this task. First, we propose a fine discretization of the 3D space around the subject and train a ConvNet to predict per voxel likelihoods for each joint. This creates a natural representation for 3D pose and greatly improves performance over the direct regression of joint coordinates. Second, to further improve upon initial estimates, we employ a coarse-to-fine prediction scheme. This step addresses the large dimensionality increase and enables iterative refinement and repeated processing of the image features. The proposed approach outperforms all state-of-the-art methods on standard benchmarks achieving a relative error reduction greater than 30% on average. Additionally, we investigate using our volumetric representation in a related architecture which is suboptimal compared to our end-to-end approach, but is of practical interest, since it enables training when no image with corresponding 3D groundtruth is available, and allows us to present compelling results for in-the-wild images.



### Adaptive Feature Abstraction for Translating Video to Text
- **Arxiv ID**: http://arxiv.org/abs/1611.07837v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1611.07837v3)
- **Published**: 2016-11-23 15:21:48+00:00
- **Updated**: 2017-11-17 05:13:16+00:00
- **Authors**: Yunchen Pu, Martin Renqiang Min, Zhe Gan, Lawrence Carin
- **Comment**: Accepted to AAAI 2018
- **Journal**: None
- **Summary**: Previous models for video captioning often use the output from a specific layer of a Convolutional Neural Network (CNN) as video features. However, the variable context-dependent semantics in the video may make it more appropriate to adaptively select features from the multiple CNN layers. We propose a new approach for generating adaptive spatiotemporal representations of videos for the captioning task. A novel attention mechanism is developed, that adaptively and sequentially focuses on different layers of CNN features (levels of feature "abstraction"), as well as local spatiotemporal regions of the feature maps at each layer. The proposed approach is evaluated on three benchmark datasets: YouTube2Text, M-VAD and MSR-VTT. Along with visualizing the results and how the model works, these experiments quantitatively demonstrate the effectiveness of the proposed adaptive spatiotemporal feature abstraction for translating videos to sentences with rich semantics.



### Controlling Perceptual Factors in Neural Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1611.07865v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07865v2)
- **Published**: 2016-11-23 16:24:08+00:00
- **Updated**: 2017-05-11 16:55:01+00:00
- **Authors**: Leon A. Gatys, Alexander S. Ecker, Matthias Bethge, Aaron Hertzmann, Eli Shechtman
- **Comment**: Accepted at CVPR2017
- **Journal**: None
- **Summary**: Neural Style Transfer has shown very exciting results enabling new forms of image manipulation. Here we extend the existing method to introduce control over spatial location, colour information and across spatial scale. We demonstrate how this enhances the method by allowing high-resolution controlled stylisation and helps to alleviate common failure cases such as applying ground textures to sky regions. Furthermore, by decomposing style into these perceptual factors we enable the combination of style information from multiple sources to generate new, perceptually appealing styles from existing ones. We also describe how these methods can be used to more efficiently produce large size, high-quality stylisation. Finally we show how the introduced control measures can be applied in recent methods for Fast Neural Style Transfer.



### The World of Fast Moving Objects
- **Arxiv ID**: http://arxiv.org/abs/1611.07889v1
- **DOI**: 10.1109/CVPR.2017.514
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07889v1)
- **Published**: 2016-11-23 17:20:04+00:00
- **Updated**: 2016-11-23 17:20:04+00:00
- **Authors**: Denys Rozumnyi, Jan Kotera, Filip Sroubek, Lukas Novotny, Jiri Matas
- **Comment**: None
- **Journal**: 2017 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)
- **Summary**: The notion of a Fast Moving Object (FMO), i.e. an object that moves over a distance exceeding its size within the exposure time, is introduced. FMOs may, and typically do, rotate with high angular speed. FMOs are very common in sports videos, but are not rare elsewhere. In a single frame, such objects are often barely visible and appear as semi-transparent streaks.   A method for the detection and tracking of FMOs is proposed. The method consists of three distinct algorithms, which form an efficient localization pipeline that operates successfully in a broad range of conditions. We show that it is possible to recover the appearance of the object and its axis of rotation, despite its blurred appearance. The proposed method is evaluated on a new annotated dataset. The results show that existing trackers are inadequate for the problem of FMO localization and a new approach is required. Two applications of localization, temporal super-resolution and highlighting, are presented.



### Image-based localization using LSTMs for structured feature correlation
- **Arxiv ID**: http://arxiv.org/abs/1611.07890v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07890v4)
- **Published**: 2016-11-23 17:22:27+00:00
- **Updated**: 2017-08-20 22:07:43+00:00
- **Authors**: Florian Walch, Caner Hazirbas, Laura Leal-Taixé, Torsten Sattler, Sebastian Hilsenbeck, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we propose a new CNN+LSTM architecture for camera pose regression for indoor and outdoor scenes. CNNs allow us to learn suitable feature representations for localization that are robust against motion blur and illumination changes. We make use of LSTM units on the CNN output, which play the role of a structured dimensionality reduction on the feature vector, leading to drastic improvements in localization performance. We provide extensive quantitative comparison of CNN-based and SIFT-based localization methods, showing the weaknesses and strengths of each. Furthermore, we present a new large-scale indoor dataset with accurate ground truth from a laser scanner. Experimental results on both indoor and outdoor public datasets show our method outperforms existing deep architectures, and can localize images in hard conditions, e.g., in the presence of mostly textureless surfaces, where classic SIFT-based methods fail.



### PVANet: Lightweight Deep Neural Networks for Real-time Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1611.08588v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08588v2)
- **Published**: 2016-11-23 17:43:28+00:00
- **Updated**: 2016-12-09 22:30:17+00:00
- **Authors**: Sanghoon Hong, Byungseok Roh, Kye-Hyeon Kim, Yeongjae Cheon, Minje Park
- **Comment**: Presented at NIPS 2016 Workshop on Efficient Methods for Deep Neural
  Networks (EMDNN). Continuation of arXiv:1608.08021. The affiliation has been
  corrected
- **Journal**: None
- **Summary**: In object detection, reducing computational cost is as important as improving accuracy for most practical usages. This paper proposes a novel network structure, which is an order of magnitude lighter than other state-of-the-art networks while maintaining the accuracy. Based on the basic principle of more layers with less channels, this new deep neural network minimizes its redundancy by adopting recent innovations including C.ReLU and Inception structure. We also show that this network can be trained efficiently to achieve solid results on well-known object detection benchmarks: 84.9% and 84.2% mAP on VOC2007 and VOC2012 while the required compute is less than 10% of the recent ResNet-101.



### Image Segmentation Using Overlapping Group Sparsity
- **Arxiv ID**: http://arxiv.org/abs/1611.07909v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07909v4)
- **Published**: 2016-11-23 18:08:33+00:00
- **Updated**: 2016-12-21 15:36:41+00:00
- **Authors**: Shervin Minaee, Yao Wang
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1602.02434.
  appears in IEEE Signal Processing in Medicine and Biology Symposium, 2016
- **Journal**: None
- **Summary**: Sparse decomposition has been widely used for different applications, such as source separation, image classification and image denoising. This paper presents a new algorithm for segmentation of an image into background and foreground text and graphics using sparse decomposition. First, the background is represented using a suitable smooth model, which is a linear combination of a few smoothly varying basis functions, and the foreground text and graphics are modeled as a sparse component overlaid on the smooth background. Then the background and foreground are separated using a sparse decomposition framework and imposing some prior information, which promote the smoothness of background, and the sparsity and connectivity of foreground pixels. This algorithm has been tested on a dataset of images extracted from HEVC standard test sequences for screen content coding, and is shown to outperform prior methods, including least absolute deviation fitting, k-means clustering based segmentation in DjVu, and shape primitive extraction and coding algorithm.



### Straight to Shapes: Real-time Detection of Encoded Shapes
- **Arxiv ID**: http://arxiv.org/abs/1611.07932v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07932v2)
- **Published**: 2016-11-23 19:04:43+00:00
- **Updated**: 2017-07-05 17:25:25+00:00
- **Authors**: Saumya Jetley, Michael Sapienza, Stuart Golodetz, Philip H. S. Torr
- **Comment**: 16 pages including appendix; Published at CVPR 2017
- **Journal**: None
- **Summary**: Current object detection approaches predict bounding boxes, but these provide little instance-specific information beyond location, scale and aspect ratio. In this work, we propose to directly regress to objects' shapes in addition to their bounding boxes and categories. It is crucial to find an appropriate shape representation that is compact and decodable, and in which objects can be compared for higher-order concepts such as view similarity, pose variation and occlusion. To achieve this, we use a denoising convolutional auto-encoder to establish an embedding space, and place the decoder after a fast end-to-end network trained to regress directly to the encoded shape vectors. This yields what to the best of our knowledge is the first real-time shape prediction network, running at ~35 FPS on a high-end desktop. With higher-order shape reasoning well-integrated into the network pipeline, the network shows the useful practical quality of generalising to unseen categories similar to the ones in the training set, something that most existing approaches fail to handle.



### Multi-Modal Mean-Fields via Cardinality-Based Clamping
- **Arxiv ID**: http://arxiv.org/abs/1611.07941v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1611.07941v1)
- **Published**: 2016-11-23 19:14:25+00:00
- **Updated**: 2016-11-23 19:14:25+00:00
- **Authors**: Pierre Baqué, François Fleuret, Pascal Fua
- **Comment**: Submitted for review to CVPR 2017
- **Journal**: None
- **Summary**: Mean Field inference is central to statistical physics. It has attracted much interest in the Computer Vision community to efficiently solve problems expressible in terms of large Conditional Random Fields. However, since it models the posterior probability distribution as a product of marginal probabilities, it may fail to properly account for important dependencies between variables. We therefore replace the fully factorized distribution of Mean Field by a weighted mixture of such distributions, that similarly minimizes the KL-Divergence to the true posterior. By introducing two new ideas, namely, conditioning on groups of variables instead of single ones and using a parameter of the conditional random field potentials, that we identify to the temperature in the sense of statistical physics to select such groups, we can perform this minimization efficiently. Our extension of the clamping method proposed in previous works allows us to both produce a more descriptive approximation of the true posterior and, inspired by the diverse MAP paradigms, fit a mixture of Mean Field approximations. We demonstrate that this positively impacts real-world algorithms that initially relied on mean fields.



### GuessWhat?! Visual object discovery through multi-modal dialogue
- **Arxiv ID**: http://arxiv.org/abs/1611.08481v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1611.08481v2)
- **Published**: 2016-11-23 20:56:13+00:00
- **Updated**: 2017-02-06 12:52:53+00:00
- **Authors**: Harm de Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo Larochelle, Aaron Courville
- **Comment**: 23 pages; CVPR 2017 submission; see https://guesswhat.ai
- **Journal**: None
- **Summary**: We introduce GuessWhat?!, a two-player guessing game as a testbed for research on the interplay of computer vision and dialogue systems. The goal of the game is to locate an unknown object in a rich image scene by asking a sequence of questions. Higher-level image understanding, like spatial reasoning and language grounding, is required to solve the proposed task. Our key contribution is the collection of a large-scale dataset consisting of 150K human-played games with a total of 800K visual question-answer pairs on 66K images. We explain our design decisions in collecting the dataset and introduce the oracle and questioner tasks that are associated with the two players of the game. We prototyped deep learning models to establish initial baselines of the introduced tasks.



### Semantic Compositional Networks for Visual Captioning
- **Arxiv ID**: http://arxiv.org/abs/1611.08002v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1611.08002v2)
- **Published**: 2016-11-23 21:22:22+00:00
- **Updated**: 2017-03-28 18:33:51+00:00
- **Authors**: Zhe Gan, Chuang Gan, Xiaodong He, Yunchen Pu, Kenneth Tran, Jianfeng Gao, Lawrence Carin, Li Deng
- **Comment**: Accepted in CVPR 2017
- **Journal**: None
- **Summary**: A Semantic Compositional Network (SCN) is developed for image captioning, in which semantic concepts (i.e., tags) are detected from the image, and the probability of each tag is used to compose the parameters in a long short-term memory (LSTM) network. The SCN extends each weight matrix of the LSTM to an ensemble of tag-dependent weight matrices. The degree to which each member of the ensemble is used to generate an image caption is tied to the image-dependent probability of the corresponding tag. In addition to captioning images, we also extend the SCN to generate captions for video clips. We qualitatively analyze semantic composition in SCNs, and quantitatively evaluate the algorithm on three benchmark datasets: COCO, Flickr30k, and Youtube2Text. Experimental results show that the proposed method significantly outperforms prior state-of-the-art approaches, across multiple evaluation metrics.



