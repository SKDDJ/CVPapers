# Arxiv Papers in cs.CV on 2016-11-17
### Deep Action- and Context-Aware Sequence Learning for Activity Recognition and Anticipation
- **Arxiv ID**: http://arxiv.org/abs/1611.05520v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05520v2)
- **Published**: 2016-11-17 01:08:56+00:00
- **Updated**: 2016-11-18 01:41:40+00:00
- **Authors**: Mohammad Sadegh Aliakbarian, Fatemehsadat Saleh, Basura Fernando, Mathieu Salzmann, Lars Petersson, Lars Andersson
- **Comment**: 10 pages, 4 figures, 7 tables
- **Journal**: None
- **Summary**: Action recognition and anticipation are key to the success of many computer vision applications. Existing methods can roughly be grouped into those that extract global, context-aware representations of the entire image or sequence, and those that aim at focusing on the regions where the action occurs. While the former may suffer from the fact that context is not always reliable, the latter completely ignore this source of information, which can nonetheless be helpful in many situations. In this paper, we aim at making the best of both worlds by developing an approach that leverages both context-aware and action-aware features. At the core of our method lies a novel multi-stage recurrent architecture that allows us to effectively combine these two sources of information throughout a video. This architecture first exploits the global, context-aware features, and merges the resulting representation with the localized, action-aware ones. Our experiments on standard datasets evidence the benefits of our approach over methods that use each information type separately. We outperform the state-of-the-art methods that, as us, rely only on RGB frames as input for both action recognition and anticipation.



### Zero-Shot Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1611.05546v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1611.05546v2)
- **Published**: 2016-11-17 03:21:00+00:00
- **Updated**: 2016-11-20 21:51:24+00:00
- **Authors**: Damien Teney, Anton van den Hengel
- **Comment**: None
- **Journal**: None
- **Summary**: Part of the appeal of Visual Question Answering (VQA) is its promise to answer new questions about previously unseen images. Most current methods demand training questions that illustrate every possible concept, and will therefore never achieve this capability, since the volume of required training data would be prohibitive. Answering general questions about images requires methods capable of Zero-Shot VQA, that is, methods able to answer questions beyond the scope of the training questions. We propose a new evaluation protocol for VQA methods which measures their ability to perform Zero-Shot VQA, and in doing so highlights significant practical deficiencies of current approaches, some of which are masked by the biases in current datasets. We propose and evaluate several strategies for achieving Zero-Shot VQA, including methods based on pretrained word embeddings, object classifiers with semantic embeddings, and test-time retrieval of example images. Our extensive experiments are intended to serve as baselines for Zero-Shot VQA, and they also achieve state-of-the-art performance in the standard VQA evaluation setting.



### DelugeNets: Deep Networks with Efficient and Flexible Cross-layer Information Inflows
- **Arxiv ID**: http://arxiv.org/abs/1611.05552v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1611.05552v5)
- **Published**: 2016-11-17 03:45:48+00:00
- **Updated**: 2017-08-23 14:09:55+00:00
- **Authors**: Jason Kuen, Xiangfei Kong, Gang Wang, Yap-Peng Tan
- **Comment**: Code: https://github.com/xternalz/DelugeNets
- **Journal**: None
- **Summary**: Deluge Networks (DelugeNets) are deep neural networks which efficiently facilitate massive cross-layer information inflows from preceding layers to succeeding layers. The connections between layers in DelugeNets are established through cross-layer depthwise convolutional layers with learnable filters, acting as a flexible yet efficient selection mechanism. DelugeNets can propagate information across many layers with greater flexibility and utilize network parameters more effectively compared to ResNets, whilst being more efficient than DenseNets. Remarkably, a DelugeNet model with just model complexity of 4.31 GigaFLOPs and 20.2M network parameters, achieve classification errors of 3.76% and 19.02% on CIFAR-10 and CIFAR-100 dataset respectively. Moreover, DelugeNet-122 performs competitively to ResNet-200 on ImageNet dataset, despite costing merely half of the computations needed by the latter.



### Instance-aware Image and Sentence Matching with Selective Multimodal LSTM
- **Arxiv ID**: http://arxiv.org/abs/1611.05588v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05588v1)
- **Published**: 2016-11-17 06:57:01+00:00
- **Updated**: 2016-11-17 06:57:01+00:00
- **Authors**: Yan Huang, Wei Wang, Liang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Effective image and sentence matching depends on how to well measure their global visual-semantic similarity. Based on the observation that such a global similarity arises from a complex aggregation of multiple local similarities between pairwise instances of image (objects) and sentence (words), we propose a selective multimodal Long Short-Term Memory network (sm-LSTM) for instance-aware image and sentence matching. The sm-LSTM includes a multimodal context-modulated attention scheme at each timestep that can selectively attend to a pair of instances of image and sentence, by predicting pairwise instance-aware saliency maps for image and sentence. For selected pairwise instances, their representations are obtained based on the predicted saliency maps, and then compared to measure their local similarity. By similarly measuring multiple local similarities within a few timesteps, the sm-LSTM sequentially aggregates them with hidden states to obtain a final matching score as the desired global similarity. Extensive experiments show that our model can well match image and sentence with complex content, and achieve the state-of-the-art results on two public benchmark datasets.



### Multimodal Memory Modelling for Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/1611.05592v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05592v1)
- **Published**: 2016-11-17 07:24:03+00:00
- **Updated**: 2016-11-17 07:24:03+00:00
- **Authors**: Junbo Wang, Wei Wang, Yan Huang, Liang Wang, Tieniu Tan
- **Comment**: None
- **Journal**: None
- **Summary**: Video captioning which automatically translates video clips into natural language sentences is a very important task in computer vision. By virtue of recent deep learning technologies, e.g., convolutional neural networks (CNNs) and recurrent neural networks (RNNs), video captioning has made great progress. However, learning an effective mapping from visual sequence space to language space is still a challenging problem. In this paper, we propose a Multimodal Memory Model (M3) to describe videos, which builds a visual and textual shared memory to model the long-term visual-textual dependency and further guide global visual attention on described targets. Specifically, the proposed M3 attaches an external memory to store and retrieve both visual and textual contents by interacting with video and sentence with multiple read and write operations. First, text representation in the Long Short-Term Memory (LSTM) based text decoder is written into the memory, and the memory contents will be read out to guide an attention to select related visual targets. Then, the selected visual information is written into the memory, which will be further read out to the text decoder. To evaluate the proposed model, we perform experiments on two publicly benchmark datasets: MSVD and MSR-VTT. The experimental results demonstrate that our method outperforms the state-of-theart methods in terms of BLEU and METEOR.



### SCA-CNN: Spatial and Channel-wise Attention in Convolutional Networks for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1611.05594v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05594v2)
- **Published**: 2016-11-17 07:39:46+00:00
- **Updated**: 2017-04-12 05:48:44+00:00
- **Authors**: Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian Shao, Wei Liu, Tat-Seng Chua
- **Comment**: None
- **Journal**: None
- **Summary**: Visual attention has been successfully applied in structural prediction tasks such as visual captioning and question answering. Existing visual attention models are generally spatial, i.e., the attention is modeled as spatial probabilities that re-weight the last conv-layer feature map of a CNN encoding an input image. However, we argue that such spatial attention does not necessarily conform to the attention mechanism --- a dynamic feature extractor that combines contextual fixations over time, as CNN features are naturally spatial, channel-wise and multi-layer. In this paper, we introduce a novel convolutional neural network dubbed SCA-CNN that incorporates Spatial and Channel-wise Attentions in a CNN. In the task of image captioning, SCA-CNN dynamically modulates the sentence generation context in multi-layer feature maps, encoding where (i.e., attentive spatial locations at multiple layers) and what (i.e., attentive channels) the visual attention is. We evaluate the proposed SCA-CNN architecture on three benchmark image captioning datasets: Flickr8K, Flickr30K, and MSCOCO. It is consistently observed that SCA-CNN significantly outperforms state-of-the-art visual attention-based image captioning methods.



### Multimodal Transfer: A Hierarchical Deep Convolutional Neural Network for Fast Artistic Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1612.01895v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1612.01895v2)
- **Published**: 2016-11-17 07:48:25+00:00
- **Updated**: 2017-04-11 05:27:13+00:00
- **Authors**: Xin Wang, Geoffrey Oxholm, Da Zhang, Yuan-Fang Wang
- **Comment**: Accepted by CVPR 2017
- **Journal**: None
- **Summary**: Transferring artistic styles onto everyday photographs has become an extremely popular task in both academia and industry. Recently, offline training has replaced on-line iterative optimization, enabling nearly real-time stylization. When those stylization networks are applied directly to high-resolution images, however, the style of localized regions often appears less similar to the desired artistic style. This is because the transfer process fails to capture small, intricate textures and maintain correct texture scales of the artworks. Here we propose a multimodal convolutional neural network that takes into consideration faithful representations of both color and luminance channels, and performs stylization hierarchically with multiple losses of increasing scales. Compared to state-of-the-art networks, our network can also perform style transfer in nearly real-time by conducting much more sophisticated training offline. By properly handling style and texture cues at multiple scales using several modalities, we can transfer not just large-scale, obvious style cues but also subtle, exquisite ones. That is, our scheme can generate results that are visually pleasing and more similar to multiple desired artistic styles with color and texture cues at multiple scales.



### Weakly-supervised Learning of Mid-level Features for Pedestrian Attribute Recognition and Localization
- **Arxiv ID**: http://arxiv.org/abs/1611.05603v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05603v1)
- **Published**: 2016-11-17 08:20:23+00:00
- **Updated**: 2016-11-17 08:20:23+00:00
- **Authors**: Kai Yu, Biao Leng, Zhang Zhang, Dangwei Li, Kaiqi Huang
- **Comment**: Containing 9 pages and 5 figures. Codes open-sourced on
  https://github.com/kyu-sz/WPAL-network
- **Journal**: None
- **Summary**: State-of-the-art methods treat pedestrian attribute recognition as a multi-label image classification problem. The location information of person attributes is usually eliminated or simply encoded in the rigid splitting of whole body in previous work. In this paper, we formulate the task in a weakly-supervised attribute localization framework. Based on GoogLeNet, firstly, a set of mid-level attribute features are discovered by novelly designed detection layers, where a max-pooling based weakly-supervised object detection technique is used to train these layers with only image-level labels without the need of bounding box annotations of pedestrian attributes. Secondly, attribute labels are predicted by regression of the detection response magnitudes. Finally, the locations and rough shapes of pedestrian attributes can be inferred by performing clustering on a fusion of activation maps of the detection layers, where the fusion weights are estimated as the correlation strengths between each attribute and its relevant mid-level features. Extensive experiments are performed on the two currently largest pedestrian attribute datasets, i.e. the PETA dataset and the RAP dataset. Results show that the proposed method has achieved competitive performance on attribute recognition, compared to other state-of-the-art methods. Moreover, the results of attribute localization are visualized to understand the characteristics of the proposed method.



### Optical Flow Requires Multiple Strategies (but only one network)
- **Arxiv ID**: http://arxiv.org/abs/1611.05607v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1611.05607v3)
- **Published**: 2016-11-17 08:31:56+00:00
- **Updated**: 2017-02-02 10:52:03+00:00
- **Authors**: Tal Schuster, Lior Wolf, David Gadot
- **Comment**: None
- **Journal**: None
- **Summary**: We show that the matching problem that underlies optical flow requires multiple strategies, depending on the amount of image motion and other factors. We then study the implications of this observation on training a deep neural network for representing image patches in the context of descriptor based optical flow. We propose a metric learning method, which selects suitable negative samples based on the nature of the true match. This type of training produces a network that displays multiple strategies depending on the input and leads to state of the art results on the KITTI 2012 and KITTI 2015 optical flow benchmarks.



### Inverting The Generator Of A Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1611.05644v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1611.05644v1)
- **Published**: 2016-11-17 11:55:16+00:00
- **Updated**: 2016-11-17 11:55:16+00:00
- **Authors**: Antonia Creswell, Anil Anthony Bharath
- **Comment**: Accepted at NIPS 2016 Workshop on Adversarial Training
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) learn to synthesise new samples from a high-dimensional distribution by passing samples drawn from a latent space through a generative network. When the high-dimensional distribution describes images of a particular data set, the network should learn to generate visually similar image samples for latent variables that are close to each other in the latent space. For tasks such as image retrieval and image classification, it may be useful to exploit the arrangement of the latent space by projecting images into it, and using this as a representation for discriminative tasks. GANs often consist of multiple layers of non-linear computations, making them very difficult to invert. This paper introduces techniques for projecting image samples into the latent space using any pre-trained GAN, provided that the computational graph is available. We evaluate these techniques on both MNIST digits and Omniglot handwritten characters. In the case of MNIST digits, we show that projections into the latent space maintain information about the style and the identity of the digit. In the case of Omniglot characters, we show that even characters from alphabets that have not been seen during training may be projected well into the latent space; this suggests that this approach may have applications in one-shot learning.



### Learning to detect and localize many objects from few examples
- **Arxiv ID**: http://arxiv.org/abs/1611.05664v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1611.05664v1)
- **Published**: 2016-11-17 12:51:18+00:00
- **Updated**: 2016-11-17 12:51:18+00:00
- **Authors**: Bastien Moysset, Christoper Kermorvant, Christian Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: The current trend in object detection and localization is to learn predictions with high capacity deep neural networks trained on a very large amount of annotated data and using a high amount of processing power. In this work, we propose a new neural model which directly predicts bounding box coordinates. The particularity of our contribution lies in the local computations of predictions with a new form of local parameter sharing which keeps the overall amount of trainable parameters low. Key components of the model are spatial 2D-LSTM recurrent layers which convey contextual information between the regions of the image. We show that this model is more powerful than the state of the art in applications where training data is not as abundant as in the classical configuration of natural images and Imagenet/Pascal VOC tasks. We particularly target the detection of text in document images, but our method is not limited to this setting. The proposed model also facilitates the detection of many objects in a single image and can deal with inputs of variable sizes without resizing.



### A Discriminatively Learned CNN Embedding for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1611.05666v2
- **DOI**: 10.1145/3159171
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05666v2)
- **Published**: 2016-11-17 12:59:22+00:00
- **Updated**: 2017-02-03 05:53:00+00:00
- **Authors**: Zhedong Zheng, Liang Zheng, Yi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: We revisit two popular convolutional neural networks (CNN) in person re-identification (re-ID), i.e, verification and classification models. The two models have their respective advantages and limitations due to different loss functions. In this paper, we shed light on how to combine the two models to learn more discriminative pedestrian descriptors. Specifically, we propose a new siamese network that simultaneously computes identification loss and verification loss. Given a pair of training images, the network predicts the identities of the two images and whether they belong to the same identity. Our network learns a discriminative embedding and a similarity measurement at the same time, thus making full usage of the annotations. Albeit simple, the learned embedding improves the state-of-the-art performance on two public person re-ID benchmarks. Further, we show our architecture can also be applied in image retrieval.



### End-to-end Learning of Cost-Volume Aggregation for Real-time Dense Stereo
- **Arxiv ID**: http://arxiv.org/abs/1611.05689v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05689v1)
- **Published**: 2016-11-17 14:14:02+00:00
- **Updated**: 2016-11-17 14:14:02+00:00
- **Authors**: Andrey Kuzmin, Dmitry Mikushin, Victor Lempitsky
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new deep learning-based approach for dense stereo matching. Compared to previous works, our approach does not use deep learning of pixel appearance descriptors, employing very fast classical matching scores instead. At the same time, our approach uses a deep convolutional network to predict the local parameters of cost volume aggregation process, which in this paper we implement using differentiable domain transform. By treating such transform as a recurrent neural network, we are able to train our whole system that includes cost volume computation, cost-volume aggregation (smoothing), and winner-takes-all disparity selection end-to-end. The resulting method is highly efficient at test time, while achieving good matching accuracy. On the KITTI 2015 benchmark, it achieves a result of 6.34\% error rate while running at 29 frames per second rate on a modern GPU.



### DSAC - Differentiable RANSAC for Camera Localization
- **Arxiv ID**: http://arxiv.org/abs/1611.05705v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05705v4)
- **Published**: 2016-11-17 14:39:53+00:00
- **Updated**: 2018-03-21 13:16:09+00:00
- **Authors**: Eric Brachmann, Alexander Krull, Sebastian Nowozin, Jamie Shotton, Frank Michel, Stefan Gumhold, Carsten Rother
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: RANSAC is an important algorithm in robust optimization and a central building block for many computer vision applications. In recent years, traditionally hand-crafted pipelines have been replaced by deep learning pipelines, which can be trained in an end-to-end fashion. However, RANSAC has so far not been used as part of such deep learning pipelines, because its hypothesis selection procedure is non-differentiable. In this work, we present two different ways to overcome this limitation. The most promising approach is inspired by reinforcement learning, namely to replace the deterministic hypothesis selection by a probabilistic selection for which we can derive the expected loss w.r.t. to all learnable parameters. We call this approach DSAC, the differentiable counterpart of RANSAC. We apply DSAC to the problem of camera localization, where deep learning has so far failed to improve on traditional approaches. We demonstrate that by directly minimizing the expected loss of the output camera poses, robustly estimated by RANSAC, we achieve an increase in accuracy. In the future, any deep learning pipeline can use DSAC as a robust optimization component.



### Learning to Fuse 2D and 3D Image Cues for Monocular Body Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1611.05708v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05708v3)
- **Published**: 2016-11-17 14:40:53+00:00
- **Updated**: 2017-04-10 10:49:00+00:00
- **Authors**: Bugra Tekin, Pablo Márquez-Neila, Mathieu Salzmann, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: Most recent approaches to monocular 3D human pose estimation rely on Deep Learning. They typically involve regressing from an image to either 3D joint coordinates directly or 2D joint locations from which 3D coordinates are inferred. Both approaches have their strengths and weaknesses and we therefore propose a novel architecture designed to deliver the best of both worlds by performing both simultaneously and fusing the information along the way. At the heart of our framework is a trainable fusion scheme that learns how to fuse the information optimally instead of being hand-designed. This yields significant improvements upon the state-of-the-art on standard 3D human pose estimation benchmarks.



### Factorized Bilinear Models for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1611.05709v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05709v2)
- **Published**: 2016-11-17 14:40:57+00:00
- **Updated**: 2017-09-04 08:14:01+00:00
- **Authors**: Yanghao Li, Naiyan Wang, Jiaying Liu, Xiaodi Hou
- **Comment**: Accepted by ICCV 2017
- **Journal**: None
- **Summary**: Although Deep Convolutional Neural Networks (CNNs) have liberated their power in various computer vision tasks, the most important components of CNN, convolutional layers and fully connected layers, are still limited to linear transformations. In this paper, we propose a novel Factorized Bilinear (FB) layer to model the pairwise feature interactions by considering the quadratic terms in the transformations. Compared with existing methods that tried to incorporate complex non-linearity structures into CNNs, the factorized parameterization makes our FB layer only require a linear increase of parameters and affordable computational cost. To further reduce the risk of overfitting of the FB layer, a specific remedy called DropFactor is devised during the training process. We also analyze the connection between FB layer and some existing models, and show FB layer is a generalization to them. Finally, we validate the effectiveness of FB layer on several widely adopted datasets including CIFAR-10, CIFAR-100 and ImageNet, and demonstrate superior results compared with various state-of-the-art deep models.



### Hard-Aware Deeply Cascaded Embedding
- **Arxiv ID**: http://arxiv.org/abs/1611.05720v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05720v2)
- **Published**: 2016-11-17 14:54:33+00:00
- **Updated**: 2017-08-01 07:22:25+00:00
- **Authors**: Yuhui Yuan, Kuiyuan Yang, Chao Zhang
- **Comment**: accepted by ICCV 2017
- **Journal**: None
- **Summary**: Riding on the waves of deep neural networks, deep metric learning has also achieved promising results in various tasks using triplet network or Siamese network. Though the basic goal of making images from the same category closer than the ones from different categories is intuitive, it is hard to directly optimize due to the quadratic or cubic sample size. To solve the problem, hard example mining which only focuses on a subset of samples that are considered hard is widely used. However, hard is defined relative to a model, where complex models treat most samples as easy ones and vice versa for simple models, and both are not good for training. Samples are also with different hard levels, it is hard to define a model with the just right complexity and choose hard examples adequately. This motivates us to ensemble a set of models with different complexities in cascaded manner and mine hard examples adaptively, a sample is judged by a series of models with increasing complexities and only updates models that consider the sample as a hard case. We evaluate our method on CARS196, CUB-200-2011, Stanford Online Products, VehicleID and DeepFashion datasets. Our method outperforms state-of-the-art methods by a large margin.



### PolyNet: A Pursuit of Structural Diversity in Very Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.05725v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05725v2)
- **Published**: 2016-11-17 15:00:42+00:00
- **Updated**: 2017-07-17 05:18:01+00:00
- **Authors**: Xingcheng Zhang, Zhizhong Li, Chen Change Loy, Dahua Lin
- **Comment**: Tech report
- **Journal**: None
- **Summary**: A number of studies have shown that increasing the depth or width of convolutional networks is a rewarding approach to improve the performance of image recognition. In our study, however, we observed difficulties along both directions. On one hand, the pursuit for very deep networks is met with a diminishing return and increased training difficulty; on the other hand, widening a network would result in a quadratic growth in both computational cost and memory demand. These difficulties motivate us to explore structural diversity in designing deep networks, a new dimension beyond just depth and width. Specifically, we present a new family of modules, namely the PolyInception, which can be flexibly inserted in isolation or in a composition as replacements of different parts of a network. Choosing PolyInception modules with the guidance of architectural efficiency can improve the expressive power while preserving comparable computational cost. The Very Deep PolyNet, designed following this direction, demonstrates substantial improvements over the state-of-the-art on the ILSVRC 2012 benchmark. Compared to Inception-ResNet-v2, it reduces the top-5 validation error on single crops from 4.9% to 4.25%, and that on multi-crops from 3.7% to 3.45%.



### Building Deep Networks on Grassmann Manifolds
- **Arxiv ID**: http://arxiv.org/abs/1611.05742v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05742v3)
- **Published**: 2016-11-17 15:37:23+00:00
- **Updated**: 2018-01-29 14:18:04+00:00
- **Authors**: Zhiwu Huang, Jiqing Wu, Luc Van Gool
- **Comment**: AAAI'18 paper
- **Journal**: None
- **Summary**: Learning representations on Grassmann manifolds is popular in quite a few visual recognition tasks. In order to enable deep learning on Grassmann manifolds, this paper proposes a deep network architecture by generalizing the Euclidean network paradigm to Grassmann manifolds. In particular, we design full rank mapping layers to transform input Grassmannian data to more desirable ones, exploit re-orthonormalization layers to normalize the resulting matrices, study projection pooling layers to reduce the model complexity in the Grassmannian context, and devise projection mapping layers to respect Grassmannian geometry and meanwhile achieve Euclidean forms for regular output layers. To train the Grassmann networks, we exploit a stochastic gradient descent setting on manifolds of the connection weights, and study a matrix generalization of backpropagation to update the structured data. The evaluations on three visual recognition tasks show that our Grassmann networks have clear advantages over existing Grassmann learning methods, and achieve results comparable with state-of-the-art approaches.



### Compensating for Large In-Plane Rotations in Natural Images
- **Arxiv ID**: http://arxiv.org/abs/1611.05744v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05744v1)
- **Published**: 2016-11-17 15:50:36+00:00
- **Updated**: 2016-11-17 15:50:36+00:00
- **Authors**: Lokesh Boominathan, Suraj Srinivas, R. Venkatesh Babu
- **Comment**: Accepted at Indian Conference on Computer Vision, Graphics and Image
  Processing (ICVGIP) 2016
- **Journal**: None
- **Summary**: Rotation invariance has been studied in the computer vision community primarily in the context of small in-plane rotations. This is usually achieved by building invariant image features. However, the problem of achieving invariance for large rotation angles remains largely unexplored. In this work, we tackle this problem by directly compensating for large rotations, as opposed to building invariant features. This is inspired by the neuro-scientific concept of mental rotation, which humans use to compare pairs of rotated objects. Our contributions here are three-fold. First, we train a Convolutional Neural Network (CNN) to detect image rotations. We find that generic CNN architectures are not suitable for this purpose. To this end, we introduce a convolutional template layer, which learns representations for canonical 'unrotated' images. Second, we use Bayesian Optimization to quickly sift through a large number of candidate images to find the canonical 'unrotated' image. Third, we use this method to achieve robustness to large angles in an image retrieval scenario. Our method is task-agnostic, and can be used as a pre-processing step in any computer vision system.



### Cross-Domain Face Verification: Matching ID Document and Self-Portrait Photographs
- **Arxiv ID**: http://arxiv.org/abs/1611.05755v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05755v1)
- **Published**: 2016-11-17 16:05:11+00:00
- **Updated**: 2016-11-17 16:05:11+00:00
- **Authors**: Guilherme Folego, Marcus A. Angeloni, José Augusto Stuchi, Alan Godoy, Anderson Rocha
- **Comment**: XII WORKSHOP DE VIS\~AO COMPUTACIONAL (Campo Grande, Brazil). In XII
  Workshop de Vis\~ao Computacional (pp. 311-316) (2016)
- **Journal**: None
- **Summary**: Cross-domain biometrics has been emerging as a new necessity, which poses several additional challenges, including harsh illumination changes, noise, pose variation, among others. In this paper, we explore approaches to cross-domain face verification, comparing self-portrait photographs ("selfies") to ID documents. We approach the problem with proper image photometric adjustment and data standardization techniques, along with deep learning methods to extract the most prominent features from the data, reducing the effects of domain shift in this problem. We validate the methods using a novel dataset comprising 50 individuals. The obtained results are promising and indicate that the adopted path is worth further investigation.



### Examining the Impact of Blur on Recognition by Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.05760v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05760v2)
- **Published**: 2016-11-17 16:17:10+00:00
- **Updated**: 2017-05-30 13:56:22+00:00
- **Authors**: Igor Vasiljevic, Ayan Chakrabarti, Gregory Shakhnarovich
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art algorithms for many semantic visual tasks are based on the use of convolutional neural networks. These networks are commonly trained, and evaluated, on large annotated datasets of artifact-free high-quality images. In this paper, we investigate the effect of one such artifact that is quite common in natural capture settings: optical blur. We show that standard network models, trained only on high-quality images, suffer a significant degradation in performance when applied to those degraded by blur due to defocus, or subject or camera motion. We investigate the extent to which this degradation is due to the mismatch between training and input image statistics. Specifically, we find that fine-tuning a pre-trained model with blurred images added to the training set allows it to regain much of the lost accuracy. We also show that there is a fair amount of generalization between different degrees and types of blur, which implies that a single network model can be used robustly for recognition when the nature of the blur in the input is unknown. We find that this robustness arises as a result of these models learning to generate blur invariant representations in their hidden layers. Our findings provide useful insights towards developing vision systems that can perform reliably on real world images affected by blur.



### DeeperBind: Enhancing Prediction of Sequence Specificities of DNA Binding Proteins
- **Arxiv ID**: http://arxiv.org/abs/1611.05777v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05777v1)
- **Published**: 2016-11-17 16:52:41+00:00
- **Updated**: 2016-11-17 16:52:41+00:00
- **Authors**: Hamid Reza Hassanzadeh, May D. Wang
- **Comment**: in 2016 IEEE International Conference on Bioinformatics and
  Biomedicine (BIBM)
- **Journal**: None
- **Summary**: Transcription factors (TFs) are macromolecules that bind to \textit{cis}-regulatory specific sub-regions of DNA promoters and initiate transcription. Finding the exact location of these binding sites (aka motifs) is important in a variety of domains such as drug design and development. To address this need, several \textit{in vivo} and \textit{in vitro} techniques have been developed so far that try to characterize and predict the binding specificity of a protein to different DNA loci. The major problem with these techniques is that they are not accurate enough in prediction of the binding affinity and characterization of the corresponding motifs. As a result, downstream analysis is required to uncover the locations where proteins of interest bind. Here, we propose DeeperBind, a long short term recurrent convolutional network for prediction of protein binding specificities with respect to DNA probes. DeeperBind can model the positional dynamics of probe sequences and hence reckons with the contributions made by individual sub-regions in DNA sequences, in an effective way. Moreover, it can be trained and tested on datasets containing varying-length sequences. We apply our pipeline to the datasets derived from protein binding microarrays (PBMs), an in-vitro high-throughput technology for quantification of protein-DNA binding preferences, and present promising results. To the best of our knowledge, this is the most accurate pipeline that can predict binding specificities of DNA sequences from the data produced by high-throughput technologies through utilization of the power of deep learning for feature generation and positional dynamics modeling.



### The Freiburg Groceries Dataset
- **Arxiv ID**: http://arxiv.org/abs/1611.05799v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05799v1)
- **Published**: 2016-11-17 17:35:21+00:00
- **Updated**: 2016-11-17 17:35:21+00:00
- **Authors**: Philipp Jund, Nichola Abdo, Andreas Eitel, Wolfram Burgard
- **Comment**: Link to dataset:
  http://www2.informatik.uni-freiburg.de/~eitel/freiburg_groceries_dataset.html
  Link to code: https://github.com/PhilJd/freiburg_groceries_dataset
- **Journal**: None
- **Summary**: With the increasing performance of machine learning techniques in the last few years, the computer vision and robotics communities have created a large number of datasets for benchmarking object recognition tasks. These datasets cover a large spectrum of natural images and object categories, making them not only useful as a testbed for comparing machine learning approaches, but also a great resource for bootstrapping different domain-specific perception and robotic systems. One such domain is domestic environments, where an autonomous robot has to recognize a large variety of everyday objects such as groceries. This is a challenging task due to the large variety of objects and products, and where there is great need for real-world training data that goes beyond product images available online. In this paper, we address this issue and present a dataset consisting of 5,000 images covering 25 different classes of groceries, with at least 97 images per class. We collected all images from real-world settings at different stores and apartments. In contrast to existing groceries datasets, our dataset includes a large variety of perspectives, lighting conditions, and degrees of clutter. Overall, our images contain thousands of different object instances. It is our hope that machine learning and robotics researchers find this dataset of use for training, testing, and bootstrapping their approaches. As a baseline classifier to facilitate comparison, we re-trained the CaffeNet architecture (an adaptation of the well-known AlexNet) on our dataset and achieved a mean accuracy of 78.9%. We release this trained model along with the code and data splits we used in our experiments.



### AutoScaler: Scale-Attention Networks for Visual Correspondence
- **Arxiv ID**: http://arxiv.org/abs/1611.05837v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05837v1)
- **Published**: 2016-11-17 20:01:05+00:00
- **Updated**: 2016-11-17 20:01:05+00:00
- **Authors**: Shenlong Wang, Linjie Luo, Ning Zhang, Jia Li
- **Comment**: None
- **Journal**: None
- **Summary**: Finding visual correspondence between local features is key to many computer vision problems. While defining features with larger contextual scales usually implies greater discriminativeness, it could also lead to less spatial accuracy of the features. We propose AutoScaler, a scale-attention network to explicitly optimize this trade-off in visual correspondence tasks. Our network consists of a weight-sharing feature network to compute multi-scale feature maps and an attention network to combine them optimally in the scale space. This allows our network to have adaptive receptive field sizes over different scales of the input. The entire network is trained end-to-end in a siamese framework for visual correspondence tasks. Our method achieves favorable results compared to state-of-the-art methods on challenging optical flow and semantic matching benchmarks, including Sintel, KITTI and CUB-2011. We also show that our method can generalize to improve hand-crafted descriptors (e.g Daisy) on general visual correspondence tasks. Finally, our attention network can generate visually interpretable scale attention maps.



### Video Processing from Electro-optical Sensors for Object Detection and Tracking in Maritime Environment: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1611.05842v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05842v1)
- **Published**: 2016-11-17 20:11:51+00:00
- **Updated**: 2016-11-17 20:11:51+00:00
- **Authors**: D. K. Prasad, D. Rajan, L. Rachmawati, E. Rajabaly, C. Quek
- **Comment**: 23 pages
- **Journal**: None
- **Summary**: We present a survey on maritime object detection and tracking approaches, which are essential for the development of a navigational system for autonomous ships. The electro-optical (EO) sensor considered here is a video camera that operates in the visible or the infrared spectra, which conventionally complement radar and sonar and have demonstrated effectiveness for situational awareness at sea has demonstrated its effectiveness over the last few years. This paper provides a comprehensive overview of various approaches of video processing for object detection and tracking in the maritime environment. We follow an approach-based taxonomy wherein the advantages and limitations of each approach are compared. The object detection system consists of the following modules: horizon detection, static background subtraction and foreground segmentation. Each of these has been studied extensively in maritime situations and has been shown to be challenging due to the presence of background motion especially due to waves and wakes. The main processes involved in object tracking include video frame registration, dynamic background subtraction, and the object tracking algorithm itself. The challenges for robust tracking arise due to camera motion, dynamic background and low contrast of tracked object, possibly due to environmental degradation. The survey also discusses multisensor approaches and commercial maritime systems that use EO sensors. The survey also highlights methods from computer vision research which hold promise to perform well in maritime EO data processing. Performance of several maritime and computer vision techniques is evaluated on newly proposed Singapore Maritime Dataset.



### Answering Image Riddles using Vision and Reasoning through Probabilistic Soft Logic
- **Arxiv ID**: http://arxiv.org/abs/1611.05896v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1611.05896v1)
- **Published**: 2016-11-17 21:10:33+00:00
- **Updated**: 2016-11-17 21:10:33+00:00
- **Authors**: Somak Aditya, Yezhou Yang, Chitta Baral, Yiannis Aloimonos
- **Comment**: 14 pages, 10 figures
- **Journal**: None
- **Summary**: In this work, we explore a genre of puzzles ("image riddles") which involves a set of images and a question. Answering these puzzles require both capabilities involving visual detection (including object, activity recognition) and, knowledge-based or commonsense reasoning. We compile a dataset of over 3k riddles where each riddle consists of 4 images and a groundtruth answer. The annotations are validated using crowd-sourced evaluation. We also define an automatic evaluation metric to track future progress. Our task bears similarity with the commonly known IQ tasks such as analogy solving, sequence filling that are often used to test intelligence.   We develop a Probabilistic Reasoning-based approach that utilizes probabilistic commonsense knowledge to answer these riddles with a reasonable accuracy. We demonstrate the results of our approach using both automatic and human evaluations. Our approach achieves some promising results for these riddles and provides a strong baseline for future attempts. We make the entire dataset and related materials publicly available to the community in ImageRiddle Website (http://bit.ly/22f9Ala).



### Generative One-Class Models for Text-based Person Retrieval in Forensic Applications
- **Arxiv ID**: http://arxiv.org/abs/1611.05915v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05915v1)
- **Published**: 2016-11-17 22:02:06+00:00
- **Updated**: 2016-11-17 22:02:06+00:00
- **Authors**: David Gerónimo, Hedvig Kjellström
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic forensic image analysis assists criminal investigation experts in the search for suspicious persons, abnormal behaviors detection and identity matching in images. In this paper we propose a person retrieval system that uses textual queries (e.g., "black trousers and green shirt") as descriptions and a one-class generative color model with outlier filtering to represent the images both to train the models and to perform the search. The method is evaluated in terms of its efficiency in fulfilling the needs of a forensic retrieval system: limited annotation, robustness, extensibility, adaptability and computational cost. The proposed generative method is compared to a corresponding discriminative approach. Experiments are carried out using a range of queries in three different databases. The experiments show that the two evaluated algorithms provide average retrieval performance and adaptable to new datasets. The proposed generative algorithm has some advantages over the discriminative one, specifically its capability to work with very few training samples and its much lower computational requirements when the number of training examples increases.



### Squared Earth Mover's Distance-based Loss for Training Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.05916v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05916v4)
- **Published**: 2016-11-17 22:03:35+00:00
- **Updated**: 2017-04-03 02:30:57+00:00
- **Authors**: Le Hou, Chen-Ping Yu, Dimitris Samaras
- **Comment**: None
- **Journal**: None
- **Summary**: In the context of single-label classification, despite the huge success of deep learning, the commonly used cross-entropy loss function ignores the intricate inter-class relationships that often exist in real-life tasks such as age classification. In this work, we propose to leverage these relationships between classes by training deep nets with the exact squared Earth Mover's Distance (also known as Wasserstein distance) for single-label classification. The squared EMD loss uses the predicted probabilities of all classes and penalizes the miss-predictions according to a ground distance matrix that quantifies the dissimilarities between classes. We demonstrate that on datasets with strong inter-class relationships such as an ordering between classes, our exact squared EMD losses yield new state-of-the-art results. Furthermore, we propose a method to automatically learn this matrix using the CNN's own features during training. We show that our method can learn a ground distance matrix efficiently with no inter-class relationship priors and yield the same performance gain. Finally, we show that our method can be generalized to applications that lack strong inter-class relationships and still maintain state-of-the-art performance. Therefore, with limited computational overhead, one can always deploy the proposed loss function on any dataset over the conventional cross-entropy.



### Generalized BackPropagation, Étude De Cas: Orthogonality
- **Arxiv ID**: http://arxiv.org/abs/1611.05927v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05927v1)
- **Published**: 2016-11-17 22:55:09+00:00
- **Updated**: 2016-11-17 22:55:09+00:00
- **Authors**: Mehrtash Harandi, Basura Fernando
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces an extension of the backpropagation algorithm that enables us to have layers with constrained weights in a deep network. In particular, we make use of the Riemannian geometry and optimization techniques on matrix manifolds to step outside of normal practice in training deep networks, equipping the network with structures such as orthogonality or positive definiteness. Based on our development, we make another contribution by introducing the Stiefel layer, a layer with orthogonal weights. Among various applications, Stiefel layers can be used to design orthogonal filter banks, perform dimensionality reduction and feature extraction. We demonstrate the benefits of having orthogonality in deep networks through a broad set of experiments, ranging from unsupervised feature learning to fine-grained image classification.



