# Arxiv Papers in cs.CV on 2016-11-04
### Bayesian Optical Flow with Uncertainty Quantification
- **Arxiv ID**: http://arxiv.org/abs/1611.01230v2
- **DOI**: 10.1088/1361-6420/aad7cc
- **Categories**: **stat.ML**, cs.CV, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/1611.01230v2)
- **Published**: 2016-11-04 00:29:02+00:00
- **Updated**: 2018-08-20 11:41:49+00:00
- **Authors**: Jie Sun, Fernando J. Quevedo, Erik Bollt
- **Comment**: Published 20 August 2018
- **Journal**: Inverse Problems 34, 105008 (2018)
- **Summary**: Optical flow refers to the visual motion observed between two consecutive images. Since the degree of freedom is typically much larger than the constraints imposed by the image observations, the straightforward formulation of optical flow as an inverse problem is ill-posed. Standard approaches to determine optical flow rely on formulating and solving an optimization problem that contains both a data fidelity term and a regularization term, the latter effectively resolves the otherwise ill-posedness of the inverse problem. In this work, we depart from the deterministic formalism, and instead treat optical flow as a statistical inverse problem. We discuss how a classical optical flow solution can be interpreted as a point estimate in this more general framework. The statistical approach, whose "solution" is a distribution of flow fields, which we refer to as Bayesian optical flow, allows not only "point" estimates (e.g., the computation of average flow field), but also statistical estimates (e.g., quantification of uncertainty) that are beyond any standard method for optical flow. As application, we benchmark Bayesian optical flow together with uncertainty quantification using several types of prescribed ground-truth flow fields and images.



### Adversarial Machine Learning at Scale
- **Arxiv ID**: http://arxiv.org/abs/1611.01236v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1611.01236v2)
- **Published**: 2016-11-04 01:11:02+00:00
- **Updated**: 2017-02-11 00:15:46+00:00
- **Authors**: Alexey Kurakin, Ian Goodfellow, Samy Bengio
- **Comment**: 17 pages, 5 figures
- **Journal**: None
- **Summary**: Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a "label leaking" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.



### Learning Identity Mappings with Residual Gates
- **Arxiv ID**: http://arxiv.org/abs/1611.01260v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1611.01260v2)
- **Published**: 2016-11-04 04:34:38+00:00
- **Updated**: 2016-12-29 01:36:47+00:00
- **Authors**: Pedro H. P. Savarese, Leonardo O. Mazza, Daniel R. Figueiredo
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new layer design by adding a linear gating mechanism to shortcut connections. By using a scalar parameter to control each gate, we provide a way to learn identity mappings by optimizing only one parameter. We build upon the motivation behind Residual Networks, where a layer is reformulated in order to make learning identity mappings less problematic to the optimizer. The augmentation introduces only one extra parameter per layer, and provides easier optimization by making degeneration into identity mappings simpler. We propose a new model, the Gated Residual Network, which is the result when augmenting Residual Networks. Experimental results show that augmenting layers provides better optimization, increased performance, and more layer independence. We evaluate our method on MNIST using fully-connected networks, showing empirical indications that our augmentation facilitates the optimization of deep models, and that it provides high tolerance to full layer removal: the model retains over 90% of its performance even after half of its layers have been randomly removed. We also evaluate our model on CIFAR-10 and CIFAR-100 using Wide Gated ResNets, achieving 3.65% and 18.27% error, respectively.



### Regularized Pel-Recursive Motion Estimation Using Generalized Cross-Validation and Spatial Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1611.01298v1
- **DOI**: 10.1109/SIBGRA.2003.1241027
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.01298v1)
- **Published**: 2016-11-04 09:51:21+00:00
- **Updated**: 2016-11-04 09:51:21+00:00
- **Authors**: Vania V. Estrela, Luis A. Rivera, Paulo C. Beggio, Ricardo T. Lopes
- **Comment**: 8 pages, 6 figures in Proceedings of the XVI Brazilian Symposium on
  Computer Graphics and Image Processing, 2003. SIBGRAPI 2003. IEEE. arXiv
  admin note: text overlap with arXiv:1403.7365, arXiv:1611.00960
- **Journal**: None
- **Summary**: The computation of 2-D optical flow by means of regularized pel-recursive algorithms raises a host of issues, which include the treatment of outliers, motion discontinuities and occlusion among other problems. We propose a new approach which allows us to deal with these issues within a common framework. Our approach is based on the use of a technique called Generalized Cross-Validation to estimate the best regularization scheme for a given pixel. In our model, the regularization parameter is a matrix whose entries can account for diverse sources of error. The estimation of the motion vectors takes into consideration local properties of the image following a spatially adaptive approach where each moving pixel is supposed to have its own regularization matrix. Preliminary experiments indicate that this approach provides robust estimates of the optical flow.



### RenderGAN: Generating Realistic Labeled Data
- **Arxiv ID**: http://arxiv.org/abs/1611.01331v5
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1611.01331v5)
- **Published**: 2016-11-04 11:20:38+00:00
- **Updated**: 2017-01-12 13:37:26+00:00
- **Authors**: Leon Sixt, Benjamin Wild, Tim Landgraf
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks. Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible. We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model. We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines.



### Nonnegative Matrix Underapproximation for Robust Multiple Model Fitting
- **Arxiv ID**: http://arxiv.org/abs/1611.01408v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.01408v5)
- **Published**: 2016-11-04 15:09:24+00:00
- **Updated**: 2017-04-10 12:15:40+00:00
- **Authors**: Mariano Tepper, Guillermo Sapiro
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we introduce a highly efficient algorithm to address the nonnegative matrix underapproximation (NMU) problem, i.e., nonnegative matrix factorization (NMF) with an additional underapproximation constraint. NMU results are interesting as, compared to traditional NMF, they present additional sparsity and part-based behavior, explaining unique data features. To show these features in practice, we first present an application to the analysis of climate data. We then present an NMU-based algorithm to robustly fit multiple parametric models to a dataset. The proposed approach delivers state-of-the-art results for the estimation of multiple fundamental matrices and homographies, outperforming other alternatives in the literature and exemplifying the use of efficient NMU computations.



### STDP-based spiking deep convolutional neural networks for object recognition
- **Arxiv ID**: http://arxiv.org/abs/1611.01421v3
- **DOI**: 10.1016/j.neunet.2017.12.005
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.01421v3)
- **Published**: 2016-11-04 15:25:13+00:00
- **Updated**: 2017-12-25 12:57:57+00:00
- **Authors**: Saeed Reza Kheradpisheh, Mohammad Ganjtabesh, Simon J Thorpe, Timoth√©e Masquelier
- **Comment**: None
- **Journal**: Neural Networks 2018
- **Summary**: Previous studies have shown that spike-timing-dependent plasticity (STDP) can be used in spiking neural networks (SNN) to extract visual features of low or intermediate complexity in an unsupervised manner. These studies, however, used relatively shallow architectures, and only one layer was trainable. Another line of research has demonstrated - using rate-based neural networks trained with back-propagation - that having many layers increases the recognition robustness, an approach known as deep learning. We thus designed a deep SNN, comprising several convolutional (trainable with STDP) and pooling layers. We used a temporal coding scheme where the most strongly activated neurons fire first, and less activated neurons fire later or not at all. The network was exposed to natural images. Thanks to STDP, neurons progressively learned features corresponding to prototypical patterns that were both salient and frequent. Only a few tens of examples per category were required and no label was needed. After learning, the complexity of the extracted features increased along the hierarchy, from edge detectors in the first layer to object prototypes in the last layer. Coding was very sparse, with only a few thousands spikes per image, and in some cases the object category could be reasonably well inferred from the activity of a single higher-order neuron. More generally, the activity of a few hundreds of such neurons contained robust category information, as demonstrated using a classifier on Caltech 101, ETH-80, and MNIST databases. We also demonstrate the superiority of STDP over other unsupervised techniques such as random crops (HMAX) or auto-encoders. Taken together, our results suggest that the combination of STDP with latency coding may be a key to understanding the way that the primate visual system learns, its remarkable processing speed and its low energy consumption.



### UMDFaces: An Annotated Face Dataset for Training Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.01484v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.01484v2)
- **Published**: 2016-11-04 18:37:41+00:00
- **Updated**: 2017-05-21 08:00:42+00:00
- **Authors**: Ankan Bansal, Anirudh Nanduri, Carlos Castillo, Rajeev Ranjan, Rama Chellappa
- **Comment**: Updates: Verified keypoints, removed duplicate subjects, released
  test protocol
- **Journal**: None
- **Summary**: Recent progress in face detection (including keypoint detection), and recognition is mainly being driven by (i) deeper convolutional neural network architectures, and (ii) larger datasets. However, most of the large datasets are maintained by private companies and are not publicly available. The academic computer vision community needs larger and more varied datasets to make further progress.   In this paper we introduce a new face dataset, called UMDFaces, which has 367,888 annotated faces of 8,277 subjects. We also introduce a new face recognition evaluation protocol which will help advance the state-of-the-art in this area. We discuss how a large dataset can be collected and annotated using human annotators and deep networks. We provide human curated bounding boxes for faces. We also provide estimated pose (roll, pitch and yaw), locations of twenty-one key-points and gender information generated by a pre-trained neural network. In addition, the quality of keypoint annotations has been verified by humans for about 115,000 images. Finally, we compare the quality of the dataset with other publicly available face datasets at similar scales.



