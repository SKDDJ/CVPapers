# Arxiv Papers in cs.CV on 2016-11-12
### Audio Event and Scene Recognition: A Unified Approach using Strongly and Weakly Labeled Data
- **Arxiv ID**: http://arxiv.org/abs/1611.04871v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1611.04871v3)
- **Published**: 2016-11-12 07:39:50+00:00
- **Updated**: 2017-02-18 07:18:32+00:00
- **Authors**: Anurag Kumar, Bhiksha Raj
- **Comment**: IJCNN 2017, 8 pages
- **Journal**: None
- **Summary**: In this paper we propose a novel learning framework called Supervised and Weakly Supervised Learning where the goal is to learn simultaneously from weakly and strongly labeled data. Strongly labeled data can be simply understood as fully supervised data where all labeled instances are available. In weakly supervised learning only data is weakly labeled which prevents one from directly applying supervised learning methods. Our proposed framework is motivated by the fact that a small amount of strongly labeled data can give considerable improvement over only weakly supervised learning. The primary problem domain focus of this paper is acoustic event and scene detection in audio recordings. We first propose a naive formulation for leveraging labeled data in both forms. We then propose a more general framework for Supervised and Weakly Supervised Learning (SWSL). Based on this general framework, we propose a graph based approach for SWSL. Our main method is based on manifold regularization on graphs in which we show that the unified learning can be formulated as a constraint optimization problem which can be solved by iterative concave-convex procedure (CCCP). Our experiments show that our proposed framework can address several concerns of audio content analysis using weakly labeled data.



### Learning Scene-specific Object Detectors Based on a Generative-Discriminative Model with Minimal Supervision
- **Arxiv ID**: http://arxiv.org/abs/1611.03968v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.03968v4)
- **Published**: 2016-11-12 08:15:26+00:00
- **Updated**: 2018-03-13 01:58:08+00:00
- **Authors**: Dapeng Luo, Zhipeng Zeng, Nong Sang, Xiang Wu, Longsheng Wei, Quanzheng Mou, Jun Cheng, Chen Luo
- **Comment**: None
- **Journal**: None
- **Summary**: One object class may show large variations due to diverse illuminations, backgrounds and camera viewpoints. Traditional object detection methods often perform worse under unconstrained video environments. To address this problem, many modern approaches model deep hierarchical appearance representations for object detection. Most of these methods require a timeconsuming training process on large manual labelling sample set. In this paper, the proposed framework takes a remarkably different direction to resolve the multi-scene detection problem in a bottom-up fashion. First, a scene-specific objector is obtained from a fully autonomous learning process triggered by marking several bounding boxes around the object in the first video frame via a mouse. Here the human labeled training data or a generic detector are not needed. Second, this learning process is conveniently replicated many times in different surveillance scenes and results in particular detectors under various camera viewpoints. Thus, the proposed framework can be employed in multi-scene object detection applications with minimal supervision. Obviously, the initial scene-specific detector, initialized by several bounding boxes, exhibits poor detection performance and is difficult to improve with traditional online learning algorithm. Consequently, we propose Generative-Discriminative model to partition detection response space and assign each partition an individual descriptor that progressively achieves high classification accuracy. A novel online gradual optimized process is proposed to optimize the Generative-Discriminative model and focus on the hard samples.Experimental results on six video datasets show our approach achieves comparable performance to robust supervised methods, and outperforms the state of the art self-learning methods under varying imaging conditions.



### Optimized clothes segmentation to boost gender classification in unconstrained scenarios
- **Arxiv ID**: http://arxiv.org/abs/1611.03999v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.03999v1)
- **Published**: 2016-11-12 13:39:55+00:00
- **Updated**: 2016-11-12 13:39:55+00:00
- **Authors**: D. Freire-Obregón, M. Castrillón-Santana, J. Lorenzo-Navarro
- **Comment**: None
- **Journal**: None
- **Summary**: Several applications require demographic information of ordinary people in unconstrained scenarios. This is not a trivial task due to significant human appearance variations. In this work, we introduce trixels for clustering image regions, enumerating their advantages compared to superpixels. The classical GrabCut algorithm is later modified to segment trixels instead of pixels in an unsupervised context. Combining with face detection lead us to a clothes segmentation approach close to real time. The study uses the challenging Pascal VOC dataset for segmentation evaluation experiments. A final experiment analyzes the fusion of clothes features with state-of-the-art gender classifiers in ClothesDB, revealing a significant performance improvement in gender classification.



### Leveraging Video Descriptions to Learn Video Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1611.04021v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1611.04021v2)
- **Published**: 2016-11-12 17:15:57+00:00
- **Updated**: 2016-12-19 16:07:33+00:00
- **Authors**: Kuo-Hao Zeng, Tseng-Hung Chen, Ching-Yao Chuang, Yuan-Hong Liao, Juan Carlos Niebles, Min Sun
- **Comment**: 7 pages, 5 figures. Accepted to AAAI 2017. Camera-ready version
- **Journal**: None
- **Summary**: We propose a scalable approach to learn video-based question answering (QA): answer a "free-form natural language question" about a video content. Our approach automatically harvests a large number of videos and descriptions freely available online. Then, a large number of candidate QA pairs are automatically generated from descriptions rather than manually annotated. Next, we use these candidate QA pairs to train a number of video-based QA methods extended fromMN (Sukhbaatar et al. 2015), VQA (Antol et al. 2015), SA (Yao et al. 2015), SS (Venugopalan et al. 2015). In order to handle non-perfect candidate QA pairs, we propose a self-paced learning procedure to iteratively identify them and mitigate their effects in training. Finally, we evaluate performance on manually generated video-based QA pairs. The results show that our self-paced learning procedure is effective, and the extended SS model outperforms various baselines.



### Sparsey: Event Recognition via Deep Hierarchical Spare Distributed Codes
- **Arxiv ID**: http://arxiv.org/abs/1611.04023v1
- **DOI**: 10.3389/fncom.2014.00160
- **Categories**: **q-bio.NC**, cs.CV, cs.NE, 68
- **Links**: [PDF](http://arxiv.org/pdf/1611.04023v1)
- **Published**: 2016-11-12 17:35:23+00:00
- **Updated**: 2016-11-12 17:35:23+00:00
- **Authors**: Gerard J. Rinkus
- **Comment**: This is a manuscript form of a paper published in Frontiers in
  Computational Neuroscience in 2014
  (http://dx.doi.org/10.3389/fncom.2014.00160). 65 pages, 28 figures, 8 tables
- **Journal**: Frontiers in Computational Neuroscience, Vol. 8, Article 160
  (2014)
- **Summary**: Visual cortex's hierarchical, multi-level organization is captured in many biologically inspired computational vision models, the general idea being that progressively larger scale, more complex spatiotemporal features are represented in progressively higher areas. However, most earlier models use localist representations (codes) in each representational field, which we equate with the cortical macrocolumn (mac), at each level. In localism, each represented feature/event (item) is coded by a single unit. Our model, Sparsey, is also hierarchical but crucially, uses sparse distributed coding (SDC) in every mac in all levels. In SDC, each represented item is coded by a small subset of the mac's units. SDCs of different items can overlap and the size of overlap between items can represent their similarity. The difference between localism and SDC is crucial because SDC allows the two essential operations of associative memory, storing a new item and retrieving the best-matching stored item, to be done in fixed time for the life of the model. Since the model's core algorithm, which does both storage and retrieval (inference), makes a single pass over all macs on each time step, the overall model's storage/retrieval operation is also fixed-time, a criterion we consider essential for scalability to huge datasets. A 2010 paper described a nonhierarchical version of this model in the context of purely spatial pattern processing. Here, we elaborate a fully hierarchical model (arbitrary numbers of levels and macs per level), describing novel model principles like progressive critical periods, dynamic modulation of principal cells' activation functions based on a mac-level familiarity measure, representation of multiple simultaneously active hypotheses, a novel method of time warp invariant recognition, and we report results showing learning/recognition of spatiotemporal patterns.



