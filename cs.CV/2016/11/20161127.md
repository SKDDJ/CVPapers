# Arxiv Papers in cs.CV on 2016-11-27
### Real-Time Video Highlights for Yahoo Esports
- **Arxiv ID**: http://arxiv.org/abs/1611.08780v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08780v1)
- **Published**: 2016-11-27 03:58:41+00:00
- **Updated**: 2016-11-27 03:58:41+00:00
- **Authors**: Yale Song
- **Comment**: None
- **Journal**: None
- **Summary**: Esports has gained global popularity in recent years and several companies have started offering live streaming videos of esports games and events. This creates opportunities to develop large scale video understanding systems for new product features and services. We present a technique for detecting highlights from live streaming videos of esports game matches. Most video games use pronounced visual effects to emphasize highlight moments; we use CNNs to learn convolution filters of those visual effects for detecting highlights. We propose a cascaded prediction approach that allows us to deal with several challenges arise in a production environment. We demonstrate our technique on our new dataset of three popular game titles, Heroes of the Storm, League of Legends, and Dota 2. Our technique achieves 18 FPS on a single CPU with an average precision of up to 83.18%. Part of our technique is currently deployed in production on Yahoo Esports.



### SAD-GAN: Synthetic Autonomous Driving using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.08788v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1611.08788v1)
- **Published**: 2016-11-27 05:01:39+00:00
- **Updated**: 2016-11-27 05:01:39+00:00
- **Authors**: Arna Ghosh, Biswarup Bhattacharya, Somnath Basu Roy Chowdhury
- **Comment**: 5 pages; 4 figures; Accepted at the Deep Learning for Action and
  Interaction Workshop, 30th Conference on Neural Information Processing
  Systems (NIPS 2016), Barcelona, Spain; All authors have equal contribution
- **Journal**: None
- **Summary**: Autonomous driving is one of the most recent topics of interest which is aimed at replicating human driving behavior keeping in mind the safety issues. We approach the problem of learning synthetic driving using generative neural networks. The main idea is to make a controller trainer network using images plus key press data to mimic human learning. We used the architecture of a stable GAN to make predictions between driving scenes using key presses. We train our model on one video game (Road Rash) and tested the accuracy and compared it by running the model on other maps in Road Rash to determine the extent of learning.



### Handwriting Profiling using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.08789v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1611.08789v1)
- **Published**: 2016-11-27 05:02:47+00:00
- **Updated**: 2016-11-27 05:02:47+00:00
- **Authors**: Arna Ghosh, Biswarup Bhattacharya, Somnath Basu Roy Chowdhury
- **Comment**: 2 pages; 2 figures; Accepted at The Thirty-First AAAI Conference on
  Artificial Intelligence (AAAI-17 Student Abstract and Poster Program), San
  Francisco, USA; All authors have equal contribution
- **Journal**: None
- **Summary**: Handwriting is a skill learned by humans from a very early age. The ability to develop one's own unique handwriting as well as mimic another person's handwriting is a task learned by the brain with practice. This paper deals with this very problem where an intelligent system tries to learn the handwriting of an entity using Generative Adversarial Networks (GANs). We propose a modified architecture of DCGAN (Radford, Metz, and Chintala 2015) to achieve this. We also discuss about applying reinforcement learning techniques to achieve faster learning. Our algorithm hopes to give new insights in this area and its uses include identification of forged documents, signature verification, computer generated art, digitization of documents among others. Our early implementation of the algorithm illustrates a good performance with MNIST datasets.



### Deep Deformable Registration: Enhancing Accuracy by Fully Convolutional Neural Net
- **Arxiv ID**: http://arxiv.org/abs/1611.08796v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08796v1)
- **Published**: 2016-11-27 06:41:49+00:00
- **Updated**: 2016-11-27 06:41:49+00:00
- **Authors**: Sayan Ghosal, Nilanjan Ray
- **Comment**: None
- **Journal**: None
- **Summary**: Deformable registration is ubiquitous in medical image analysis. Many deformable registration methods minimize sum of squared difference (SSD) as the registration cost with respect to deformable model parameters. In this work, we construct a tight upper bound of the SSD registration cost by using a fully convolutional neural network (FCNN) in the registration pipeline. The upper bound SSD (UB-SSD) enhances the original deformable model parameter space by adding a heatmap output from FCNN. Next, we minimize this UB-SSD by adjusting both the parameters of the FCNN and the parameters of the deformable model in coordinate descent. Our coordinate descent framework is end-to-end and can work with any deformable registration method that uses SSD. We demonstrate experimentally that our method enhances the accuracy of deformable registration algorithms significantly on two publicly available 3D brain MRI data sets.



### Predicting the Category and Attributes of Visual Search Targets Using Deep Gaze Pooling
- **Arxiv ID**: http://arxiv.org/abs/1611.10162v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1611.10162v3)
- **Published**: 2016-11-27 07:44:49+00:00
- **Updated**: 2017-04-03 11:05:07+00:00
- **Authors**: Hosnieh Sattar, Andreas Bulling, Mario Fritz
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting the target of visual search from eye fixation (gaze) data is a challenging problem with many applications in human-computer interaction. In contrast to previous work that has focused on individual instances as a search target, we propose the first approach to predict categories and attributes of search targets based on gaze data. However, state of the art models for categorical recognition, in general, require large amounts of training data, which is prohibitive for gaze data. To address this challenge, we propose a novel Gaze Pooling Layer that integrates gaze information into CNN-based architectures as an attention mechanism - incorporating both spatial and temporal aspects of human gaze behavior. We show that our approach is effective even when the gaze pooling layer is added to an already trained CNN, thus eliminating the need for expensive joint data collection of visual and gaze data. We propose an experimental setup and data set and demonstrate the effectiveness of our method for search target prediction based on gaze behavior. We further study how to integrate temporal and spatial gaze information most effectively, and indicate directions for future research in the gaze-based prediction of mental states.



### Kernel classification of connectomes based on earth mover's distance between graph spectra
- **Arxiv ID**: http://arxiv.org/abs/1611.08812v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1611.08812v1)
- **Published**: 2016-11-27 09:35:04+00:00
- **Updated**: 2016-11-27 09:35:04+00:00
- **Authors**: Yulia Dodonova, Mikhail Belyaev, Anna Tkachev, Dmitry Petrov, Leonid Zhukov
- **Comment**: Presented at The MICCAI-BACON 16 Workshop (arXiv:1611.03363)
- **Journal**: None
- **Summary**: In this paper, we tackle a problem of predicting phenotypes from structural connectomes. We propose that normalized Laplacian spectra can capture structural properties of brain networks, and hence graph spectral distributions are useful for a task of connectome-based classification. We introduce a kernel that is based on earth mover's distance (EMD) between spectral distributions of brain networks. We access performance of an SVM classifier with the proposed kernel for a task of classification of autism spectrum disorder versus typical development based on a publicly available dataset. Classification quality (area under the ROC-curve) obtained with the EMD-based kernel on spectral distributions is 0.71, which is higher than that based on simpler graph embedding methods.



### Did Evolution get it right? An evaluation of Near-Infrared imaging in semantic scene segmentation using deep learning
- **Arxiv ID**: http://arxiv.org/abs/1611.08815v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08815v1)
- **Published**: 2016-11-27 09:59:10+00:00
- **Updated**: 2016-11-27 09:59:10+00:00
- **Authors**: J. Rafid Siddiqui
- **Comment**: None
- **Journal**: None
- **Summary**: Animals have evolved to restrict their sensing capabilities to certain region of electromagnetic spectrum. This is surprisingly a very narrow band on a vast scale which makes one think if there is a systematic bias underlying such selective filtration. The situation becomes even more intriguing when we find a sharp cutoff point at Near-infrared point whereby almost all animal vision systems seem to have a lower bound. This brings us to an interesting question: did evolution "intentionally" performed such a restriction in order to evolve higher visual cognition? In this work this question is addressed by experimenting with Near-infrared images for their potential applicability in higher visual processing such as semantic segmentation. A modified version of Fully Convolutional Networks are trained on NIR images and RGB images respectively and compared for their respective effectiveness in the wake of semantic segmentation. The results from the experiments show that visible part of the spectrum alone is sufficient for the robust semantic segmentation of the indoor as well as outdoor scenes.



### Long-Term Image Boundary Prediction
- **Arxiv ID**: http://arxiv.org/abs/1611.08841v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1611.08841v2)
- **Published**: 2016-11-27 13:45:14+00:00
- **Updated**: 2017-11-23 15:23:40+00:00
- **Authors**: Apratim Bhattacharyya, Mateusz Malinowski, Bernt Schiele, Mario Fritz
- **Comment**: Accepted in the AAAI Conference for Artificial Intelligence, 2018
- **Journal**: None
- **Summary**: Boundary estimation in images and videos has been a very active topic of research, and organizing visual information into boundaries and segments is believed to be a corner stone of visual perception. While prior work has focused on estimating boundaries for observed frames, our work aims at predicting boundaries of future unobserved frames. This requires our model to learn about the fate of boundaries and corresponding motion patterns -- including a notion of "intuitive physics". We experiment on natural video sequences along with synthetic sequences with deterministic physics-based and agent-based motions. While not being our primary goal, we also show that fusion of RGB and boundary prediction leads to improved RGB predictions.



### A neuro-mathematical model for geometrical optical illusions
- **Arxiv ID**: http://arxiv.org/abs/1611.08844v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1611.08844v1)
- **Published**: 2016-11-27 13:52:24+00:00
- **Updated**: 2016-11-27 13:52:24+00:00
- **Authors**: B. Franceschiello, A. Sarti, G. Citti
- **Comment**: 13 pages, 38 figures divided in 15 groups
- **Journal**: None
- **Summary**: Geometrical optical illusions have been object of many studies due to the possibility they offer to understand the behaviour of low-level visual processing. They consist in situations in which the perceived geometrical properties of an object differ from those of the object in the visual stimulus. Starting from the geometrical model introduced by Citti and Sarti in [3], we provide a mathematical model and a computational algorithm which allows to interpret these phenomena and to qualitatively reproduce the perceived misperception.



### It's Written All Over Your Face: Full-Face Appearance-Based Gaze Estimation
- **Arxiv ID**: http://arxiv.org/abs/1611.08860v4
- **DOI**: 10.1109/CVPRW.2017.284
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1611.08860v4)
- **Published**: 2016-11-27 15:00:10+00:00
- **Updated**: 2023-05-16 10:00:49+00:00
- **Authors**: Xucong Zhang, Yusuke Sugano, Mario Fritz, Andreas Bulling
- **Comment**: None
- **Journal**: None
- **Summary**: Eye gaze is an important non-verbal cue for human affect analysis. Recent gaze estimation work indicated that information from the full face region can benefit performance. Pushing this idea further, we propose an appearance-based method that, in contrast to a long-standing line of work in computer vision, only takes the full face image as input. Our method encodes the face image using a convolutional neural network with spatial weights applied on the feature maps to flexibly suppress or enhance information in different facial regions. Through extensive evaluation, we show that our full-face method significantly outperforms the state of the art for both 2D and 3D gaze estimation, achieving improvements of up to 14.3% on MPIIGaze and 27.7% on EYEDIAP for person-independent 3D gaze estimation. We further show that this improvement is consistent across different illumination conditions and gaze directions and particularly pronounced for the most challenging extreme head poses.



### Uniform Information Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1611.08896v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08896v1)
- **Published**: 2016-11-27 19:31:03+00:00
- **Updated**: 2016-11-27 19:31:03+00:00
- **Authors**: Radhakrishna Achanta, Pablo Márquez-Neila, Pascal Fua, Sabine Süsstrunk
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Size uniformity is one of the main criteria of superpixel methods. But size uniformity rarely conforms to the varying content of an image. The chosen size of the superpixels therefore represents a compromise - how to obtain the fewest superpixels without losing too much important detail. We propose that a more appropriate criterion for creating image segments is information uniformity. We introduce a novel method for segmenting an image based on this criterion. Since information is a natural way of measuring image complexity, our proposed algorithm leads to image segments that are smaller and denser in areas of high complexity and larger in homogeneous regions, thus simplifying the image while preserving its details. Our algorithm is simple and requires just one input parameter - a threshold on the information content. On segmentation comparison benchmarks it proves to be superior to the state-of-the-art. In addition, our method is computationally very efficient, approaching real-time performance, and is easily extensible to three-dimensional image stacks and video volumes.



### Voronoi-based compact image descriptors: Efficient Region-of-Interest retrieval with VLAD and deep-learning-based descriptors
- **Arxiv ID**: http://arxiv.org/abs/1611.08906v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08906v2)
- **Published**: 2016-11-27 20:35:48+00:00
- **Updated**: 2017-03-20 18:37:56+00:00
- **Authors**: Aaron Chadha, Yiannis Andreopoulos
- **Comment**: IEEE Transaction on Multimedia, to appear
- **Journal**: None
- **Summary**: We investigate the problem of image retrieval based on visual queries when the latter comprise arbitrary regions-of-interest (ROI) rather than entire images. Our proposal is a compact image descriptor that combines the state-of-the-art in content-based descriptor extraction with a multi-level, Voronoi-based spatial partitioning of each dataset image. The proposed multi-level Voronoi-based encoding uses a spatial hierarchical K-means over interest-point locations, and computes a content-based descriptor over each cell. In order to reduce the matching complexity with minimal or no sacrifice in retrieval performance: (i) we utilize the tree structure of the spatial hierarchical K-means to perform a top-to-bottom pruning for local similarity maxima; (ii) we propose a new image similarity score that combines relevant information from all partition levels into a single measure for similarity; (iii) we combine our proposal with a novel and efficient approach for optimal bit allocation within quantized descriptor representations. By deriving both a Voronoi-based VLAD descriptor (termed as Fast-VVLAD) and a Voronoi-based deep convolutional neural network (CNN) descriptor (termed as Fast-VDCNN), we demonstrate that our Voronoi-based framework is agnostic to the descriptor basis, and can easily be slotted into existing frameworks. Via a range of ROI queries in two standard datasets, it is shown that the Voronoi-based descriptors achieve comparable or higher mean Average Precision against conventional grid-based spatial search, while offering more than two-fold reduction in complexity. Finally, beyond ROI queries, we show that Voronoi partitioning improves the geometric invariance of compact CNN descriptors, thereby resulting in competitive performance to the current state-of-the-art on whole image retrieval.



### Invariant Representations for Noisy Speech Recognition
- **Arxiv ID**: http://arxiv.org/abs/1612.01928v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG, cs.SD, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1612.01928v1)
- **Published**: 2016-11-27 22:20:51+00:00
- **Updated**: 2016-11-27 22:20:51+00:00
- **Authors**: Dmitriy Serdyuk, Kartik Audhkhasi, Philémon Brakel, Bhuvana Ramabhadran, Samuel Thomas, Yoshua Bengio
- **Comment**: 5 pages, 1 figure, 1 table, NIPS workshop on end-to-end speech
  recognition
- **Journal**: None
- **Summary**: Modern automatic speech recognition (ASR) systems need to be robust under acoustic variability arising from environmental, speaker, channel, and recording conditions. Ensuring such robustness to variability is a challenge in modern day neural network-based ASR systems, especially when all types of variability are not seen during training. We attempt to address this problem by encouraging the neural network acoustic model to learn invariant feature representations. We use ideas from recent research on image generation using Generative Adversarial Networks and domain adaptation ideas extending adversarial gradient-based training. A recent work from Ganin et al. proposes to use adversarial training for image domain adaptation by using an intermediate representation from the main target classification network to deteriorate the domain classifier performance through a separate neural network. Our work focuses on investigating neural architectures which produce representations invariant to noise conditions for ASR. We evaluate the proposed architecture on the Aurora-4 task, a popular benchmark for noise robust ASR. We show that our method generalizes better than the standard multi-condition training especially when only a few noise categories are seen during training.



