# Arxiv Papers in cs.CV on 2016-11-15
### Motion Estimated-Compensated Reconstruction with Preserved-Features in Free-Breathing Cardiac MRI
- **Arxiv ID**: http://arxiv.org/abs/1611.04655v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1611.04655v1)
- **Published**: 2016-11-15 00:33:47+00:00
- **Updated**: 2016-11-15 00:33:47+00:00
- **Authors**: Aurelien Bustin, Anne Menini, Martin A. Janich, Darius Burschka, Jacques Felblinger, Anja C. S. Brau, Freddy Odille
- **Comment**: 12 pages, 6 figures, accepted at MICCAI 2016
- **Journal**: None
- **Summary**: To develop an efficient motion-compensated reconstruction technique for free-breathing cardiac magnetic resonance imaging (MRI) that allows high-quality images to be reconstructed from multiple undersampled single-shot acquisitions. The proposed method is a joint image reconstruction and motion correction method consisting of several steps, including a non-rigid motion extraction and a motion-compensated reconstruction. The reconstruction includes a denoising with the Beltrami regularization, which offers an ideal compromise between feature preservation and staircasing reduction. Results were assessed in simulation, phantom and volunteer experiments. The proposed joint image reconstruction and motion correction method exhibits visible quality improvement over previous methods while reconstructing sharper edges. Moreover, when the acceleration factor increases, standard methods show blurry results while the proposed method preserves image quality. The method was applied to free-breathing single-shot cardiac MRI, successfully achieving high image quality and higher spatial resolution than conventional segmented methods, with the potential to offer high-quality delayed enhancement scans in challenging patients.



### Feature Extraction and Soft Computing Methods for Aerospace Structure Defect Classification
- **Arxiv ID**: http://arxiv.org/abs/1611.04782v1
- **DOI**: 10.1016/j.measurement.2016.02.027
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.04782v1)
- **Published**: 2016-11-15 10:47:12+00:00
- **Updated**: 2016-11-15 10:47:12+00:00
- **Authors**: Gianni D'Angelo, Salvatore Rampone
- **Comment**: None
- **Journal**: Measurement Volume 85, May 2016, Pages 192-209
- **Summary**: This study concerns the effectiveness of several techniques and methods of signals processing and data interpretation for the diagnosis of aerospace structure defects. This is done by applying different known feature extraction methods, in addition to a new CBIR-based one; and some soft computing techniques including a recent HPC parallel implementation of the U-BRAIN learning algorithm on Non Destructive Testing data. The performance of the resulting detection systems are measured in terms of Accuracy, Sensitivity, Specificity, and Precision. Their effectiveness is evaluated by the Matthews correlation, the Area Under Curve (AUC), and the F-Measure. Several experiments are performed on a standard dataset of eddy current signal samples for aircraft structures. Our experimental results evidence that the key to a successful defect classifier is the feature extraction method - namely the novel CBIR-based one outperforms all the competitors - and they illustrate the greater effectiveness of the U-BRAIN algorithm and the MLP neural network among the soft computing methods in this kind of application.   Keywords- Non-destructive testing (NDT); Soft Computing; Feature Extraction; Classification Algorithms; Content-Based Image Retrieval (CBIR); Eddy Currents (EC).



### Multilinear Low-Rank Tensors on Graphs & Applications
- **Arxiv ID**: http://arxiv.org/abs/1611.04835v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1611.04835v1)
- **Published**: 2016-11-15 14:05:43+00:00
- **Updated**: 2016-11-15 14:05:43+00:00
- **Authors**: Nauman Shahid, Francesco Grassi, Pierre Vandergheynst
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new framework for the analysis of low-rank tensors which lies at the intersection of spectral graph theory and signal processing. As a first step, we present a new graph based low-rank decomposition which approximates the classical low-rank SVD for matrices and multi-linear SVD for tensors. Then, building on this novel decomposition we construct a general class of convex optimization problems for approximately solving low-rank tensor inverse problems, such as tensor Robust PCA. The whole framework is named as 'Multilinear Low-rank tensors on Graphs (MLRTG)'. Our theoretical analysis shows: 1) MLRTG stands on the notion of approximate stationarity of multi-dimensional signals on graphs and 2) the approximation error depends on the eigen gaps of the graphs. We demonstrate applications for a wide variety of 4 artificial and 12 real tensor datasets, such as EEG, FMRI, BCI, surveillance videos and hyperspectral images. Generalization of the tensor concepts to non-euclidean domain, orders of magnitude speed-up, low-memory requirement and significantly enhanced performance at low SNR are the key aspects of our framework.



### Deeply supervised salient object detection with short connections
- **Arxiv ID**: http://arxiv.org/abs/1611.04849v4
- **DOI**: 10.1109/TPAMI.2018.2815688
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.04849v4)
- **Published**: 2016-11-15 14:19:06+00:00
- **Updated**: 2018-03-16 01:46:40+00:00
- **Authors**: Qibin Hou, Ming-Ming Cheng, Xiao-Wei Hu, Ali Borji, Zhuowen Tu, Philip Torr
- **Comment**: IEEE TPAMI 2018 (IEEE CVPR 2017)
- **Journal**: Q. Hou, M. M. Cheng, X. Hu, A. Borji, Z. Tu and P. H. S. Torr,
  "Deeply Supervised Salient Object Detection with Short Connections," IEEE
  Transactions on Pattern Analysis and Machine Intelligence. 2018
- **Summary**: Recent progress on saliency detection is substantial, benefiting mostly from the explosive development of Convolutional Neural Networks (CNNs). Semantic segmentation and saliency detection algorithms developed lately have been mostly based on Fully Convolutional Neural Networks (FCNs). There is still a large room for improvement over the generic FCN models that do not explicitly deal with the scale-space problem. Holistically-Nested Edge Detector (HED) provides a skip-layer structure with deep supervision for edge and boundary detection, but the performance gain of HED on salience detection is not obvious. In this paper, we propose a new method for saliency detection by introducing short connections to the skip-layer structures within the HED architecture. Our framework provides rich multi-scale feature maps at each layer, a property that is critically needed to perform segment detection. Our method produces state-of-the-art results on 5 widely tested salient object detection benchmarks, with advantages in terms of efficiency (0.15 seconds per image), effectiveness, and simplicity over the existing algorithms.



### Scale-constrained Unsupervised Evaluation Method for Multi-scale Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1611.04850v1
- **DOI**: 10.1109/ICIP.2016.7532821
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.04850v1)
- **Published**: 2016-11-15 14:19:42+00:00
- **Updated**: 2016-11-15 14:19:42+00:00
- **Authors**: Yuhang Lu, Youchuan Wan, Gang Li
- **Comment**: 5 pages, 2016 IEEE International Conference on Image Processing
- **Journal**: None
- **Summary**: Unsupervised evaluation of segmentation quality is a crucial step in image segmentation applications. Previous unsupervised evaluation methods usually lacked the adaptability to multi-scale segmentation. A scale-constrained evaluation method that evaluates segmentation quality according to the specified target scale is proposed in this paper. First, regional saliency and merging cost are employed to describe intra-region homogeneity and inter-region heterogeneity, respectively. Subsequently, both of them are standardized into equivalent spectral distances of a predefined region. Finally, by analyzing the relationship between image characteristics and segmentation quality, we establish the evaluation model. Experimental results show that the proposed method outperforms four commonly used unsupervised methods in multi-scale evaluation tasks.



### Constrained Low-Rank Learning Using Least Squares-Based Regularization
- **Arxiv ID**: http://arxiv.org/abs/1611.04870v1
- **DOI**: 10.1109/TCYB.2016.2623638
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1611.04870v1)
- **Published**: 2016-11-15 14:50:31+00:00
- **Updated**: 2016-11-15 14:50:31+00:00
- **Authors**: Ping Li, Jun Yu, Meng Wang, Luming Zhang, Deng Cai, Xuelong Li
- **Comment**: 14 pages, 7 figures, accepted to appear in IEEE Transactions on
  Cybernetics
- **Journal**: IEEE Transactions on Cybernetics, 2016
- **Summary**: Low-rank learning has attracted much attention recently due to its efficacy in a rich variety of real-world tasks, e.g., subspace segmentation and image categorization. Most low-rank methods are incapable of capturing low-dimensional subspace for supervised learning tasks, e.g., classification and regression. This paper aims to learn both the discriminant low-rank representation (LRR) and the robust projecting subspace in a supervised manner. To achieve this goal, we cast the problem into a constrained rank minimization framework by adopting the least squares regularization. Naturally, the data label structure tends to resemble that of the corresponding low-dimensional representation, which is derived from the robust subspace projection of clean data by low-rank learning. Moreover, the low-dimensional representation of original data can be paired with some informative structure by imposing an appropriate constraint, e.g., Laplacian regularizer. Therefore, we propose a novel constrained LRR method. The objective function is formulated as a constrained nuclear norm minimization problem, which can be solved by the inexact augmented Lagrange multiplier algorithm. Extensive experiments on image classification, human pose estimation, and robust face recovery have confirmed the superiority of our method.



### Diversity encouraged learning of unsupervised LSTM ensemble for neural activity video prediction
- **Arxiv ID**: http://arxiv.org/abs/1611.04899v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.04899v2)
- **Published**: 2016-11-15 15:56:08+00:00
- **Updated**: 2018-07-03 02:39:38+00:00
- **Authors**: Yilin Song, Jonathan Viventi, Yao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Being able to predict the neural signal in the near future from the current and previous observations has the potential to enable real-time responsive brain stimulation to suppress seizures. We have investigated how to use an auto-encoder model consisting of LSTM cells for such prediction. Recog- nizing that there exist multiple activity pattern clusters, we have further explored to train an ensemble of LSTM mod- els so that each model can specialize in modeling certain neural activities, without explicitly clustering the training data. We train the ensemble using an ensemble-awareness loss, which jointly solves the model assignment problem and the error minimization problem. During training, for each training sequence, only the model that has the lowest recon- struction and prediction error is updated. Intrinsically such a loss function enables each LTSM model to be adapted to a subset of the training sequences that share similar dynamic behavior. We demonstrate this can be trained in an end- to-end manner and achieve significant accuracy in neural activity prediction.



### CIFAR-10: KNN-based Ensemble of Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1611.04905v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.04905v1)
- **Published**: 2016-11-15 16:02:58+00:00
- **Updated**: 2016-11-15 16:02:58+00:00
- **Authors**: Yehya Abouelnaga, Ola S. Ali, Hager Rady, Mohamed Moustafa
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the performance of different classifiers on the CIFAR-10 dataset, and build an ensemble of classifiers to reach a better performance. We show that, on CIFAR-10, K-Nearest Neighbors (KNN) and Convolutional Neural Network (CNN), on some classes, are mutually exclusive, thus yield in higher accuracy when combined. We reduce KNN overfitting using Principal Component Analysis (PCA), and ensemble it with a CNN to increase its accuracy. Our approach improves our best CNN model from 93.33% to 94.03%.



### One-to-Many Network for Visually Pleasing Compression Artifacts Reduction
- **Arxiv ID**: http://arxiv.org/abs/1611.04994v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.04994v2)
- **Published**: 2016-11-15 19:22:58+00:00
- **Updated**: 2017-04-11 09:18:59+00:00
- **Authors**: Jun Guo, Hongyang Chao
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the compression artifacts reduction problem, where a compressed image is transformed into an artifact-free image. Recent approaches for this problem typically train a one-to-one mapping using a per-pixel $L_2$ loss between the outputs and the ground-truths. We point out that these approaches used to produce overly smooth results, and PSNR doesn't reflect their real performance. In this paper, we propose a one-to-many network, which measures output quality using a perceptual loss, a naturalness loss, and a JPEG loss. We also avoid grid-like artifacts during deconvolution using a "shift-and-average" strategy. Extensive experimental results demonstrate the dramatic visual improvement of our approach over the state of the arts.



### Light Field Stitching for Extended Synthetic Aperture
- **Arxiv ID**: http://arxiv.org/abs/1611.05003v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05003v1)
- **Published**: 2016-11-15 19:49:21+00:00
- **Updated**: 2016-11-15 19:49:21+00:00
- **Authors**: M. Umair Mukati, Bahadir K. Gunturk
- **Comment**: None
- **Journal**: None
- **Summary**: Through capturing spatial and angular radiance distribution, light field cameras introduce new capabilities that are not possible with conventional cameras. So far in the light field imaging literature, the focus has been on the theory and applications of single light field capture. By combining multiple light fields, it is possible to obtain new capabilities and enhancements, and even exceed physical limitations, such as spatial resolution and aperture size of the imaging device. In this paper, we present an algorithm to register and stitch multiple light fields. We utilize the regularity of the spatial and angular sampling in light field data, and extend some techniques developed for stereo vision systems to light field data. Such an extension is not straightforward for a micro-lens array (MLA) based light field camera due to extremely small baseline and low spatial resolution. By merging multiple light fields captured by an MLA based camera, we obtain larger synthetic aperture, which results in improvements in light field capabilities, such as increased depth estimation range/accuracy and wider perspective shift range.



### Hybrid Light Field Imaging for Improved Spatial Resolution and Depth Range
- **Arxiv ID**: http://arxiv.org/abs/1611.05008v2
- **DOI**: 10.1007/s00138-017-0862-2
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05008v2)
- **Published**: 2016-11-15 20:04:40+00:00
- **Updated**: 2017-07-31 08:26:27+00:00
- **Authors**: M. Zeshan Alam, Bahadir K. Gunturk
- **Comment**: Machine Vision and Applications
- **Journal**: None
- **Summary**: Light field imaging involves capturing both angular and spatial distribution of light; it enables new capabilities, such as post-capture digital refocusing, camera aperture adjustment, perspective shift, and depth estimation. Micro-lens array (MLA) based light field cameras provide a cost-effective approach to light field imaging. There are two main limitations of MLA-based light field cameras: low spatial resolution and narrow baseline. While low spatial resolution limits the general purpose use and applicability of light field cameras, narrow baseline limits the depth estimation range and accuracy. In this paper, we present a hybrid stereo imaging system that includes a light field camera and a regular camera. The hybrid system addresses both spatial resolution and narrow baseline issues of the MLA-based light field cameras while preserving light field imaging capabilities.



### OctNet: Learning Deep 3D Representations at High Resolutions
- **Arxiv ID**: http://arxiv.org/abs/1611.05009v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05009v4)
- **Published**: 2016-11-15 20:05:45+00:00
- **Updated**: 2017-04-10 08:46:56+00:00
- **Authors**: Gernot Riegler, Ali Osman Ulusoy, Andreas Geiger
- **Comment**: CVPR 2017 camera ready
- **Journal**: None
- **Summary**: We present OctNet, a representation for deep learning with sparse 3D data. In contrast to existing models, our representation enables 3D convolutional networks which are both deep and high resolution. Towards this goal, we exploit the sparsity in the input data to hierarchically partition the space using a set of unbalanced octrees where each leaf node stores a pooled feature representation. This allows to focus memory allocation and computation to the relevant dense regions and enables deeper networks without compromising resolution. We demonstrate the utility of our OctNet representation by analyzing the impact of resolution on several 3D tasks including 3D object classification, orientation estimation and point cloud labeling.



### Learning Detailed Face Reconstruction from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1611.05053v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05053v2)
- **Published**: 2016-11-15 21:08:15+00:00
- **Updated**: 2017-04-06 15:05:16+00:00
- **Authors**: Elad Richardson, Matan Sela, Roy Or-El, Ron Kimmel
- **Comment**: 15 pages, supplementary material included
- **Journal**: None
- **Summary**: Reconstructing the detailed geometric structure of a face from a given image is a key to many computer vision and graphics applications, such as motion capture and reenactment. The reconstruction task is challenging as human faces vary extensively when considering expressions, poses, textures, and intrinsic geometries. While many approaches tackle this complexity by using additional data to reconstruct the face of a single subject, extracting facial surface from a single image remains a difficult problem. As a result, single-image based methods can usually provide only a rough estimate of the facial geometry. In contrast, we propose to leverage the power of convolutional neural networks to produce a highly detailed face reconstruction from a single image. For this purpose, we introduce an end-to-end CNN framework which derives the shape in a coarse-to-fine fashion. The proposed architecture is composed of two main blocks, a network that recovers the coarse facial geometry (CoarseNet), followed by a CNN that refines the facial features of that geometry (FineNet). The proposed networks are connected by a novel layer which renders a depth image given a mesh in 3D. Unlike object recognition and detection problems, there are no suitable datasets for training CNNs to perform face geometry reconstruction. Therefore, our training regime begins with a supervised phase, based on synthetic images, followed by an unsupervised phase that uses only unconstrained facial images. The accuracy and robustness of the proposed model is demonstrated by both qualitative and quantitative evaluation tests.



### Learning a Deep Embedding Model for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1611.05088v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05088v4)
- **Published**: 2016-11-15 22:51:43+00:00
- **Updated**: 2019-07-19 12:05:03+00:00
- **Authors**: Li Zhang, Tao Xiang, Shaogang Gong
- **Comment**: CVPR2017. Adding GZSL results. Code is available at
  https://github.com/lzrobots/DeepEmbeddingModel_ZSL
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) models rely on learning a joint embedding space where both textual/semantic description of object classes and visual representation of object images can be projected to for nearest neighbour search. Despite the success of deep neural networks that learn an end-to-end model between text and images in other vision problems such as image captioning, very few deep ZSL model exists and they show little advantage over ZSL models that utilise deep feature representations but do not learn an end-to-end embedding. In this paper we argue that the key to make deep ZSL models succeed is to choose the right embedding space. Instead of embedding into a semantic space or an intermediate space, we propose to use the visual space as the embedding space. This is because that in this space, the subsequent nearest neighbour search would suffer much less from the hubness problem and thus become more effective. This model design also provides a natural mechanism for multiple semantic modalities (e.g., attributes and sentence descriptions) to be fused and optimised jointly in an end-to-end manner. Extensive experiments on four benchmarks show that our model significantly outperforms the existing models. Code is available at https://github.com/lzrobots/DeepEmbeddingModel_ZSL



