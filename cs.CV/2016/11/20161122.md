# Arxiv Papers in cs.CV on 2016-11-22
### Max-Margin Deep Generative Models for (Semi-)Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1611.07119v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1611.07119v1)
- **Published**: 2016-11-22 01:36:29+00:00
- **Updated**: 2016-11-22 01:36:29+00:00
- **Authors**: Chongxuan Li, Jun Zhu, Bo Zhang
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1504.06787
- **Journal**: None
- **Summary**: Deep generative models (DGMs) are effective on learning multilayered representations of complex data and performing inference of input data by exploring the generative ability. However, it is relatively insufficient to empower the discriminative ability of DGMs on making accurate predictions. This paper presents max-margin deep generative models (mmDGMs) and a class-conditional variant (mmDCGMs), which explore the strongly discriminative principle of max-margin learning to improve the predictive performance of DGMs in both supervised and semi-supervised learning, while retaining the generative capability. In semi-supervised learning, we use the predictions of a max-margin classifier as the missing labels instead of performing full posterior inference for efficiency; we also introduce additional max-margin and label-balance regularization terms of unlabeled data for effectiveness. We develop an efficient doubly stochastic subgradient algorithm for the piecewise linear objectives in different settings. Empirical results on various datasets demonstrate that: (1) max-margin learning can significantly improve the prediction performance of DGMs and meanwhile retain the generative ability; (2) in supervised learning, mmDGMs are competitive to the best fully discriminative networks when employing convolutional neural networks as the generative and recognition models; and (3) in semi-supervised learning, mmDCGMs can perform efficient inference and achieve state-of-the-art classification results on several benchmarks.



### Cascaded Neural Networks with Selective Classifiers and its evaluation using Lung X-ray CT Images
- **Arxiv ID**: http://arxiv.org/abs/1611.07136v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07136v1)
- **Published**: 2016-11-22 03:21:05+00:00
- **Updated**: 2016-11-22 03:21:05+00:00
- **Authors**: Masaharu Sakamoto, Hiroki Nakano
- **Comment**: None
- **Journal**: None
- **Summary**: Lung nodule detection is a class imbalanced problem because nodules are found with much lower frequency than non-nodules. In the class imbalanced problem, conventional classifiers tend to be overwhelmed by the majority class and ignore the minority class. We therefore propose cascaded convolutional neural networks to cope with the class imbalanced problem. In the proposed approach, cascaded convolutional neural networks that perform as selective classifiers filter out obvious non-nodules. Successively, a convolutional neural network trained with a balanced data set calculates nodule probabilities. The proposed method achieved the detection sensitivity of 85.3% and 90.7% at 1 and 4 false positives per scan in FROC curve, respectively.



### Learning Multi-level Features For Sensor-based Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1611.07143v2
- **DOI**: None
- **Categories**: **cs.CV**, I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/1611.07143v2)
- **Published**: 2016-11-22 04:24:00+00:00
- **Updated**: 2017-09-02 17:57:21+00:00
- **Authors**: Yan Xu, Zhengyang Shen, Xin Zhang, Yifan Gao, Shujian Deng, Yipei Wang, Yubo Fan, Eric I-Chao Chang
- **Comment**: 26 pages, 23 figures
- **Journal**: Pervasive and Mobile Computing, Volume 40, September 2017, Pages
  324-338
- **Summary**: This paper proposes a multi-level feature learning framework for human action recognition using a single body-worn inertial sensor. The framework consists of three phases, respectively designed to analyze signal-based (low-level), components (mid-level) and semantic (high-level) information. Low-level features capture the time and frequency domain property while mid-level representations learn the composition of the action. The Max-margin Latent Pattern Learning (MLPL) method is proposed to learn high-level semantic descriptions of latent action patterns as the output of our framework. The proposed method achieves the state-of-the-art performances, 88.7%, 98.8% and 72.6% (weighted F1 score) respectively, on Skoda, WISDM and OPP datasets.



### Learning Multi-level Deep Representations for Image Emotion Classification
- **Arxiv ID**: http://arxiv.org/abs/1611.07145v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07145v2)
- **Published**: 2016-11-22 05:12:19+00:00
- **Updated**: 2018-09-25 10:18:23+00:00
- **Authors**: Tianrong Rao, Min Xu, Dong Xu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a new deep network that learns multi-level deep representations for image emotion classification (MldrNet). Image emotion can be recognized through image semantics, image aesthetics and low-level visual features from both global and local views. Existing image emotion classification works using hand-crafted features or deep features mainly focus on either low-level visual features or semantic-level image representations without taking all factors into consideration. The proposed MldrNet combines deep representations of different levels, i.e. image semantics, image aesthetics, and low-level visual features to effectively classify the emotion types of different kinds of images, such as abstract paintings and web images. Extensive experiments on both Internet images and abstract paintings demonstrate the proposed method outperforms the state-of-the-art methods using deep features or hand-crafted features. The proposed approach also outperforms the state-of-the-art methods with at least 6% performance improvement in terms of overall classification accuracy.



### Exploiting Web Images for Dataset Construction: A Domain Robust Approach
- **Arxiv ID**: http://arxiv.org/abs/1611.07156v4
- **DOI**: 10.1109/TMM.2017.2684626
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1611.07156v4)
- **Published**: 2016-11-22 06:22:19+00:00
- **Updated**: 2017-03-28 06:30:41+00:00
- **Authors**: Yazhou Yao, Jian Zhang, Fumin Shen, Xiansheng Hua, Jingsong Xu, Zhenmin Tang
- **Comment**: Journal
- **Journal**: None
- **Summary**: Labelled image datasets have played a critical role in high-level image understanding. However, the process of manual labelling is both time-consuming and labor intensive. To reduce the cost of manual labelling, there has been increased research interest in automatically constructing image datasets by exploiting web images. Datasets constructed by existing methods tend to have a weak domain adaptation ability, which is known as the "dataset bias problem". To address this issue, we present a novel image dataset construction framework that can be generalized well to unseen target domains. Specifically, the given queries are first expanded by searching the Google Books Ngrams Corpus to obtain a rich semantic description, from which the visually non-salient and less relevant expansions are filtered out. By treating each selected expansion as a "bag" and the retrieved images as "instances", image selection can be formulated as a multi-instance learning problem with constrained positive bags. We propose to solve the employed problems by the cutting-plane and concave-convex procedure (CCCP) algorithm. By using this approach, images from different distributions can be kept while noisy images are filtered out. To verify the effectiveness of our proposed approach, we build an image dataset with 20 categories. Extensive experiments on image classification, cross-dataset generalization, diversity comparison and object detection demonstrate the domain robustness of our dataset.



### Distributable Consistent Multi-Object Matching
- **Arxiv ID**: http://arxiv.org/abs/1611.07191v3
- **DOI**: None
- **Categories**: **cs.DS**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1611.07191v3)
- **Published**: 2016-11-22 08:21:38+00:00
- **Updated**: 2018-04-10 18:57:46+00:00
- **Authors**: Nan Hu, Qixing Huang, Boris Thibert, Leonidas Guibas
- **Comment**: Final version for CVPR2018
- **Journal**: None
- **Summary**: In this paper we propose an optimization-based framework to multiple object matching. The framework takes maps computed between pairs of objects as input, and outputs maps that are consistent among all pairs of objects. The central idea of our approach is to divide the input object collection into overlapping sub-collections and enforce map consistency among each sub-collection. This leads to a distributed formulation, which is scalable to large-scale datasets. We also present an equivalence condition between this decoupled scheme and the original scheme. Experiments on both synthetic and real-world datasets show that our framework is competitive against state-of-the-art multi-object matching techniques.



### Recurrent Attention Models for Depth-Based Person Identification
- **Arxiv ID**: http://arxiv.org/abs/1611.07212v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07212v1)
- **Published**: 2016-11-22 09:27:30+00:00
- **Updated**: 2016-11-22 09:27:30+00:00
- **Authors**: Albert Haque, Alexandre Alahi, Li Fei-Fei
- **Comment**: Computer Vision and Pattern Recognition (CVPR) 2016
- **Journal**: None
- **Summary**: We present an attention-based model that reasons on human body shape and motion dynamics to identify individuals in the absence of RGB information, hence in the dark. Our approach leverages unique 4D spatio-temporal signatures to address the identification problem across days. Formulated as a reinforcement learning task, our model is based on a combination of convolutional and recurrent neural networks with the goal of identifying small, discriminative regions indicative of human identity. We demonstrate that our model produces state-of-the-art results on several published datasets given only depth images. We further study the robustness of our model towards viewpoint, appearance, and volumetric changes. Finally, we share insights gleaned from interpretable 2D, 3D, and 4D visualizations of our model's spatio-temporal attention.



### Deep neural networks can be improved using human-derived contextual expectations
- **Arxiv ID**: http://arxiv.org/abs/1611.07218v4
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1611.07218v4)
- **Published**: 2016-11-22 09:45:01+00:00
- **Updated**: 2018-03-29 00:59:25+00:00
- **Authors**: Harish Katti, Marius V. Peelen, S. P. Arun
- **Comment**: 30 pages, 5 figures, 3 tables, 2 supplementary tables
- **Journal**: None
- **Summary**: Real-world objects occur in specific contexts. Such context has been shown to facilitate detection by constraining the locations to search. But can context directly benefit object detection? To do so, context needs to be learned independently from target features. This is impossible in traditional object detection where classifiers are trained on images containing both target features and surrounding context. In contrast, humans can learn context and target features separately, such as when we see highways without cars. Here we show for the first time that human-derived scene expectations can be used to improve object detection performance in machines. To measure contextual expectations, we asked human subjects to indicate the scale, location and likelihood at which cars or people might occur in scenes without these objects. Humans showed highly systematic expectations that we could accurately predict using scene features. This allowed us to predict human expectations on novel scenes without requiring manual annotation. On augmenting deep neural networks with predicted human expectations, we obtained substantial gains in accuracy for detecting cars and people (1-3%) as well as on detecting associated objects (3-20%). In contrast, augmenting deep networks with other conventional features yielded far smaller gains. This improvement was due to relatively poor matches at highly likely locations being correctly labelled as target and conversely strong matches at unlikely locations being correctly rejected as false alarms. Taken together, our results show that augmenting deep neural networks with human-derived context features improves their performance, suggesting that humans learn scene context separately unlike deep networks.



### A Spatial and Temporal Non-Local Filter Based Data Fusion
- **Arxiv ID**: http://arxiv.org/abs/1611.07231v1
- **DOI**: 10.1109/TGRS.2017.2692802
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07231v1)
- **Published**: 2016-11-22 10:08:54+00:00
- **Updated**: 2016-11-22 10:08:54+00:00
- **Authors**: Qing Cheng, Huiqing Liu, Huanfeng Shen, Penghai Wu, Liangpei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The trade-off in remote sensing instruments that balances the spatial resolution and temporal frequency limits our capacity to monitor spatial and temporal dynamics effectively. The spatiotemporal data fusion technique is considered as a cost-effective way to obtain remote sensing data with both high spatial resolution and high temporal frequency, by blending observations from multiple sensors with different advantages or characteristics. In this paper, we develop the spatial and temporal non-local filter based fusion model (STNLFFM) to enhance the prediction capacity and accuracy, especially for complex changed landscapes. The STNLFFM method provides a new transformation relationship between the fine-resolution reflectance images acquired from the same sensor at different dates with the help of coarse-resolution reflectance data, and makes full use of the high degree of spatiotemporal redundancy in the remote sensing image sequence to produce the final prediction. The proposed method was tested over both the Coleambally Irrigation Area study site and the Lower Gwydir Catchment study site. The results show that the proposed method can provide a more accurate and robust prediction, especially for heterogeneous landscapes and temporally dynamic areas.



### CAS-CNN: A Deep Convolutional Neural Network for Image Compression Artifact Suppression
- **Arxiv ID**: http://arxiv.org/abs/1611.07233v1
- **DOI**: 10.1109/IJCNN.2017.7965927
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1611.07233v1)
- **Published**: 2016-11-22 10:11:58+00:00
- **Updated**: 2016-11-22 10:11:58+00:00
- **Authors**: Lukas Cavigelli, Pascal Hager, Luca Benini
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Lossy image compression algorithms are pervasively used to reduce the size of images transmitted over the web and recorded on data storage media. However, we pay for their high compression rate with visual artifacts degrading the user experience. Deep convolutional neural networks have become a widespread tool to address high-level computer vision tasks very successfully. Recently, they have found their way into the areas of low-level computer vision and image processing to solve regression problems mostly with relatively shallow networks.   We present a novel 12-layer deep convolutional network for image compression artifact suppression with hierarchical skip connections and a multi-scale loss function. We achieve a boost of up to 1.79 dB in PSNR over ordinary JPEG and an improvement of up to 0.36 dB over the best previous ConvNet result. We show that a network trained for a specific quality factor (QF) is resilient to the QF used to compress the input image - a single network trained for QF 60 provides a PSNR gain of more than 1.5 dB over the wide QF range from 40 to 76.



### Single-View and Multi-View Depth Fusion
- **Arxiv ID**: http://arxiv.org/abs/1611.07245v2
- **DOI**: 10.1109/LRA.2017.2715400
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1611.07245v2)
- **Published**: 2016-11-22 10:51:43+00:00
- **Updated**: 2017-06-27 09:37:04+00:00
- **Authors**: José M. Fácil, Alejo Concha, Luis Montesano, Javier Civera
- **Comment**: Accepted for publication in IEEE Robotics and Automation Letters
- **Journal**: None
- **Summary**: Dense and accurate 3D mapping from a monocular sequence is a key technology for several applications and still an open research area. This paper leverages recent results on single-view CNN-based depth estimation and fuses them with multi-view depth estimation. Both approaches present complementary strengths. Multi-view depth is highly accurate but only in high-texture areas and high-parallax cases. Single-view depth captures the local structure of mid-level regions, including texture-less areas, but the estimated depth lacks global coherence. The single and multi-view fusion we propose is challenging in several aspects. First, both depths are related by a deformation that depends on the image content. Second, the selection of multi-view points of high accuracy might be difficult for low-parallax configurations. We present contributions for both problems. Our results in the public datasets of NYUv2 and TUM shows that our algorithm outperforms the individual single and multi-view approaches. A video showing the key aspects of mapping in our Single and Multi-view depth proposal is available at https://youtu.be/ipc5HukTb4k



### Active learning with version spaces for object detection
- **Arxiv ID**: http://arxiv.org/abs/1611.07285v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07285v2)
- **Published**: 2016-11-22 12:58:24+00:00
- **Updated**: 2016-11-29 06:47:29+00:00
- **Authors**: Soumya Roy, Vinay P. Namboodiri, Arijit Biswas
- **Comment**: None
- **Journal**: None
- **Summary**: Given an image, we would like to learn to detect objects belonging to particular object categories. Common object detection methods train on large annotated datasets which are annotated in terms of bounding boxes that contain the object of interest. Previous works on object detection model the problem as a structured regression problem which ranks the correct bounding boxes more than the background ones. In this paper we develop algorithms which actively obtain annotations from human annotators for a small set of images, instead of all images, thereby reducing the annotation effort. Towards this goal, we make the following contributions: 1. We develop a principled version space based active learning method that solves for object detection as a structured prediction problem in a weakly supervised setting 2. We also propose two variants of the margin sampling strategy 3. We analyse the results on standard object detection benchmarks that show that with only 20% of the data we can obtain more than 95% of the localization accuracy of full supervision. Our methods outperform random sampling and the classical uncertainty-based active learning algorithms like entropy



### PVR: Patch-to-Volume Reconstruction for Large Area Motion Correction of Fetal MRI
- **Arxiv ID**: http://arxiv.org/abs/1611.07289v2
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.4.3; D.1.3
- **Links**: [PDF](http://arxiv.org/pdf/1611.07289v2)
- **Published**: 2016-11-22 13:21:14+00:00
- **Updated**: 2016-11-25 17:54:39+00:00
- **Authors**: Amir Alansary, Bernhard Kainz, Martin Rajchl, Maria Murgasova, Mellisa Damodaram, David F. A. Lloyd, Alice Davidson, Steven G. McDonagh, Mary Rutherford, Joseph V. Hajnal, Daniel Rueckert
- **Comment**: 10 pages, 13 figures, submitted to IEEE Transactions on Medical
  Imaging. v2: wadded funders acknowledgements to preprint
- **Journal**: None
- **Summary**: In this paper we present a novel method for the correction of motion artifacts that are present in fetal Magnetic Resonance Imaging (MRI) scans of the whole uterus. Contrary to current slice-to-volume registration (SVR) methods, requiring an inflexible anatomical enclosure of a single investigated organ, the proposed patch-to-volume reconstruction (PVR) approach is able to reconstruct a large field of view of non-rigidly deforming structures. It relaxes rigid motion assumptions by introducing a specific amount of redundant information that is exploited with parallelized patch-wise optimization, super-resolution, and automatic outlier rejection. We further describe and provide an efficient parallel implementation of PVR allowing its execution within reasonable time on commercially available graphics processing units (GPU), enabling its use in the clinical practice. We evaluate PVR's computational overhead compared to standard methods and observe improved reconstruction accuracy in presence of affine motion artifacts of approximately 30% compared to conventional SVR in synthetic experiments. Furthermore, we have evaluated our method qualitatively and quantitatively on real fetal MRI data subject to maternal breathing and sudden fetal movements. We evaluate peak-signal-to-noise ratio (PSNR), structural similarity index (SSIM), and cross correlation (CC) with respect to the originally acquired data and provide a method for visual inspection of reconstruction uncertainty. With these experiments we demonstrate successful application of PVR motion compensation to the whole uterus, the human fetus, and the human placenta.



### Geometry of 3D Environments and Sum of Squares Polynomials
- **Arxiv ID**: http://arxiv.org/abs/1611.07369v3
- **DOI**: None
- **Categories**: **math.OC**, cs.CG, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1611.07369v3)
- **Published**: 2016-11-22 15:40:14+00:00
- **Updated**: 2017-03-07 22:46:15+00:00
- **Authors**: Amir Ali Ahmadi, Georgina Hall, Ameesh Makadia, Vikas Sindhwani
- **Comment**: None
- **Journal**: None
- **Summary**: Motivated by applications in robotics and computer vision, we study problems related to spatial reasoning of a 3D environment using sublevel sets of polynomials. These include: tightly containing a cloud of points (e.g., representing an obstacle) with convex or nearly-convex basic semialgebraic sets, computation of Euclidean distances between two such sets, separation of two convex basic semalgebraic sets that overlap, and tight containment of the union of several basic semialgebraic sets with a single convex one. We use algebraic techniques from sum of squares optimization that reduce all these tasks to semidefinite programs of small size and present numerical experiments in realistic scenarios.



### Smart Library: Identifying Books in a Library using Richly Supervised Deep Scene Text Reading
- **Arxiv ID**: http://arxiv.org/abs/1611.07385v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07385v1)
- **Published**: 2016-11-22 16:12:03+00:00
- **Updated**: 2016-11-22 16:12:03+00:00
- **Authors**: Xiao Yang, Dafang He, Wenyi Huang, Zihan Zhou, Alex Ororbia, Dan Kifer, C. Lee Giles
- **Comment**: None
- **Journal**: None
- **Summary**: Physical library collections are valuable and long standing resources for knowledge and learning. However, managing books in a large bookshelf and finding books on it often leads to tedious manual work, especially for large book collections where books might be missing or misplaced. Recently, deep neural models, such as Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) have achieved great success for scene text detection and recognition. Motivated by these recent successes, we aim to investigate their viability in facilitating book management, a task that introduces further challenges including large amounts of cluttered scene text, distortion, and varied lighting conditions. In this paper, we present a library inventory building and retrieval system based on scene text reading methods. We specifically design our scene text recognition model using rich supervision to accelerate training and achieve state-of-the-art performance on several benchmark datasets. Our proposed system has the potential to greatly reduce the amount of human labor required in managing book inventories as well as the space needed to store book information.



### 3D Image Reconstruction from X-Ray Measurements with Overlap
- **Arxiv ID**: http://arxiv.org/abs/1611.07390v1
- **DOI**: 10.1007/978-3-319-46466-4_2
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1611.07390v1)
- **Published**: 2016-11-22 16:22:20+00:00
- **Updated**: 2016-11-22 16:22:20+00:00
- **Authors**: Maria Klodt, Raphael Hauser
- **Comment**: Published in Computer Vision - ECCV 2016. The final publication is
  available at link.springer.com/chapter/10.1007/978-3-319-46466-4_2
- **Journal**: None
- **Summary**: 3D image reconstruction from a set of X-ray projections is an important image reconstruction problem, with applications in medical imaging, industrial inspection and airport security. The innovation of X-ray emitter arrays allows for a novel type of X-ray scanners with multiple simultaneously emitting sources. However, two or more sources emitting at the same time can yield measurements from overlapping rays, imposing a new type of image reconstruction problem based on nonlinear constraints. Using traditional linear reconstruction methods, respective scanner geometries have to be implemented such that no rays overlap, which severely restricts the scanner design. We derive a new type of 3D image reconstruction model with nonlinear constraints, based on measurements with overlapping X-rays. Further, we show that the arising optimization problem is partially convex, and present an algorithm to solve it. Experiments show highly improved image reconstruction results from both simulated and real-world measurements.



### Grad-CAM: Why did you say that?
- **Arxiv ID**: http://arxiv.org/abs/1611.07450v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1611.07450v2)
- **Published**: 2016-11-22 18:34:36+00:00
- **Updated**: 2017-01-25 16:33:29+00:00
- **Authors**: Ramprasaath R Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, Dhruv Batra
- **Comment**: Presented at NIPS 2016 Workshop on Interpretable Machine Learning in
  Complex Systems. This is an extended abstract version of arXiv:1610.02391
  (CVPR format)
- **Journal**: None
- **Summary**: We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models.   We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.



### Scene Labeling using Gated Recurrent Units with Explicit Long Range Conditioning
- **Arxiv ID**: http://arxiv.org/abs/1611.07485v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07485v2)
- **Published**: 2016-11-22 19:43:24+00:00
- **Updated**: 2017-03-28 05:12:44+00:00
- **Authors**: Qiangui Huang, Weiyue Wang, Kevin Zhou, Suya You, Ulrich Neumann
- **Comment**: updated version 2
- **Journal**: None
- **Summary**: Recurrent neural network (RNN), as a powerful contextual dependency modeling framework, has been widely applied to scene labeling problems. However, this work shows that directly applying traditional RNN architectures, which unfolds a 2D lattice grid into a sequence, is not sufficient to model structure dependencies in images due to the "impact vanishing" problem. First, we give an empirical analysis about the "impact vanishing" problem. Then, a new RNN unit named Recurrent Neural Network with explicit long range conditioning (RNN-ELC) is designed to alleviate this problem. A novel neural network architecture is built for scene labeling tasks where one of the variants of the new RNN unit, Gated Recurrent Unit with Explicit Long-range Conditioning (GRU-ELC), is used to model multi scale contextual dependencies in images. We validate the use of GRU-ELC units with state-of-the-art performance on three standard scene labeling datasets. Comprehensive experiments demonstrate that the new GRU-ELC unit benefits scene labeling problem a lot as it can encode longer contextual dependencies in images more effectively than traditional RNN units.



### Inducing Interpretable Representations with Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1611.07492v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1611.07492v1)
- **Published**: 2016-11-22 20:04:59+00:00
- **Updated**: 2016-11-22 20:04:59+00:00
- **Authors**: N. Siddharth, Brooks Paige, Alban Desmaison, Jan-Willem Van de Meent, Frank Wood, Noah D. Goodman, Pushmeet Kohli, Philip H. S. Torr
- **Comment**: Presented at NIPS 2016 Workshop on Interpretable Machine Learning in
  Complex Systems
- **Journal**: None
- **Summary**: We develop a framework for incorporating structured graphical models in the \emph{encoders} of variational autoencoders (VAEs) that allows us to induce interpretable representations through approximate variational inference. This allows us to both perform reasoning (e.g. classification) under the structural constraints of a given graphical model, and use deep generative models to deal with messy, high-dimensional domains where it is often difficult to model all the variation. Learning in this framework is carried out end-to-end with a variational objective, applying to both unsupervised and semi-supervised schemes.



### Self-learning Scene-specific Pedestrian Detectors using a Progressive Latent Model
- **Arxiv ID**: http://arxiv.org/abs/1611.07544v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07544v1)
- **Published**: 2016-11-22 21:33:07+00:00
- **Updated**: 2016-11-22 21:33:07+00:00
- **Authors**: Qixiang Ye, Tianliang Zhang, Qiang Qiu, Baochang Zhang, Jie Chen, Guillermo Sapiro
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a self-learning approach is proposed towards solving scene-specific pedestrian detection problem without any human' annotation involved. The self-learning approach is deployed as progressive steps of object discovery, object enforcement, and label propagation. In the learning procedure, object locations in each frame are treated as latent variables that are solved with a progressive latent model (PLM). Compared with conventional latent models, the proposed PLM incorporates a spatial regularization term to reduce ambiguities in object proposals and to enforce object localization, and also a graph-based label propagation to discover harder instances in adjacent frames. With the difference of convex (DC) objective functions, PLM can be efficiently optimized with a concave-convex programming and thus guaranteeing the stability of self-learning. Extensive experiments demonstrate that even without annotation the proposed self-learning approach outperforms weakly supervised learning approaches, while achieving comparable performance with transfer learning and fully supervised approaches.



### Sar image despeckling based on nonlocal similarity sparse decomposition
- **Arxiv ID**: http://arxiv.org/abs/1611.07559v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07559v1)
- **Published**: 2016-11-22 22:28:37+00:00
- **Updated**: 2016-11-22 22:28:37+00:00
- **Authors**: Chengwei Sang, Hong Sun, Quisong Xia
- **Comment**: 5pages,5 figures,20 conference
- **Journal**: None
- **Summary**: This letter presents a method of synthetic aperture radar (SAR) image despeckling aimed to preserve the detail information while suppressing speckle noise. This method combines the nonlocal self-similarity partition and a proposed modified sparse decomposition. The nonlocal partition method groups a series of structure-similarity data sets. Each data set has a good sparsity for learning an over-complete dictionary in sparse representation. In the sparse decomposition, we propose a novel method to identify principal atoms from over-complete dictionary to form a principal dictionary. Despeckling is performed on each data set over the principal dictionary with principal atoms. Experimental results demonstrate that the proposed method can achieve high performances in terms of both speckle noise reduction and structure details preservation.



### Quad-networks: unsupervised learning to rank for interest point detection
- **Arxiv ID**: http://arxiv.org/abs/1611.07571v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1611.07571v2)
- **Published**: 2016-11-22 22:46:17+00:00
- **Updated**: 2017-04-10 21:15:18+00:00
- **Authors**: Nikolay Savinov, Akihito Seki, Lubor Ladicky, Torsten Sattler, Marc Pollefeys
- **Comment**: Accepted at CVPR 2017
- **Journal**: None
- **Summary**: Several machine learning tasks require to represent the data using only a sparse set of interest points. An ideal detector is able to find the corresponding interest points even if the data undergo a transformation typical for a given domain. Since the task is of high practical interest in computer vision, many hand-crafted solutions were proposed. In this paper, we ask a fundamental question: can we learn such detectors from scratch? Since it is often unclear what points are "interesting", human labelling cannot be used to find a truly unbiased solution. Therefore, the task requires an unsupervised formulation. We are the first to propose such a formulation: training a neural network to rank points in a transformation-invariant manner. Interest points are then extracted from the top/bottom quantiles of this ranking. We validate our approach on two tasks: standard RGB image interest point detection and challenging cross-modal interest point detection between RGB and depth images. We quantitatively show that our unsupervised method performs better or on-par with baselines.



### Relaxed Earth Mover's Distances for Chain- and Tree-connected Spaces and their use as a Loss Function in Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1611.07573v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07573v1)
- **Published**: 2016-11-22 22:54:05+00:00
- **Updated**: 2016-11-22 22:54:05+00:00
- **Authors**: Manuel Martinez, Monica Haurilet, Ziad Al-Halah, Makarand Tapaswi, Rainer Stiefelhagen
- **Comment**: None
- **Journal**: None
- **Summary**: The Earth Mover's Distance (EMD) computes the optimal cost of transforming one distribution into another, given a known transport metric between them. In deep learning, the EMD loss allows us to embed information during training about the output space structure like hierarchical or semantic relations. This helps in achieving better output smoothness and generalization. However EMD is computationally expensive.Moreover, solving EMD optimization problems usually require complex techniques like lasso. These properties limit the applicability of EMD-based approaches in large scale machine learning.   We address in this work the difficulties facing incorporation of EMD-based loss in deep learning frameworks. Additionally, we provide insight and novel solutions on how to integrate such loss function in training deep neural networks. Specifically, we make three main contributions: (i) we provide an in-depth analysis of the fastest state-of-the-art EMD algorithm (Sinkhorn Distance) and discuss its limitations in deep learning scenarios. (ii) we derive fast and numerically stable closed-form solutions for the EMD gradient in output spaces with chain- and tree- connectivity; and (iii) we propose a relaxed form of the EMD gradient with equivalent computational complexity but faster convergence rate. We support our claims with experiments on real datasets. In a restricted data setting on the ImageNet dataset, we train a model to classify 1000 categories using 50K images, and demonstrate that our relaxed EMD loss achieves better Top-1 accuracy than the cross entropy loss. Overall, we show that our relaxed EMD loss criterion is a powerful asset for deep learning in the small data regime.



### Alternating Direction Graph Matching
- **Arxiv ID**: http://arxiv.org/abs/1611.07583v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07583v4)
- **Published**: 2016-11-22 23:51:03+00:00
- **Updated**: 2018-02-23 12:10:18+00:00
- **Authors**: D. Khuê Lê-Huu, Nikos Paragios
- **Comment**: Accepted for publication at the 2017 IEEE Conference on Computer
  Vision and Pattern Recognition (CVPR)
- **Journal**: None
- **Summary**: In this paper, we introduce a graph matching method that can account for constraints of arbitrary order, with arbitrary potential functions. Unlike previous decomposition approaches that rely on the graph structures, we introduce a decomposition of the matching constraints. Graph matching is then reformulated as a non-convex non-separable optimization problem that can be split into smaller and much-easier-to-solve subproblems, by means of the alternating direction method of multipliers. The proposed framework is modular, scalable, and can be instantiated into different variants. Two instantiations are studied exploring pairwise and higher-order constraints. Experimental results on widely adopted benchmarks involving synthetic and real examples demonstrate that the proposed solutions outperform existing pairwise graph matching methods, and competitive with the state of the art in higher-order settings.



