# Arxiv Papers in cs.CV on 2016-11-29
### Social Behavior Prediction from First Person Videos
- **Arxiv ID**: http://arxiv.org/abs/1611.09464v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09464v1)
- **Published**: 2016-11-29 03:04:41+00:00
- **Updated**: 2016-11-29 03:04:41+00:00
- **Authors**: Shan Su, Jung Pyo Hong, Jianbo Shi, Hyun Soo Park
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a method to predict the future movements (location and gaze direction) of basketball players as a whole from their first person videos. The predicted behaviors reflect an individual physical space that affords to take the next actions while conforming to social behaviors by engaging to joint attention. Our key innovation is to use the 3D reconstruction of multiple first person cameras to automatically annotate each other's the visual semantics of social configurations.   We leverage two learning signals uniquely embedded in first person videos. Individually, a first person video records the visual semantics of a spatial and social layout around a person that allows associating with past similar situations. Collectively, first person videos follow joint attention that can link the individuals to a group. We learn the egocentric visual semantics of group movements using a Siamese neural network to retrieve future trajectories. We consolidate the retrieved trajectories from all players by maximizing a measure of social compatibility---the gaze alignment towards joint attention predicted by their social formation, where the dynamics of joint attention is learned by a long-term recurrent convolutional network. This allows us to characterize which social configuration is more plausible and predict future group trajectories.



### Inertial-Based Scale Estimation for Structure from Motion on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/1611.09498v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09498v2)
- **Published**: 2016-11-29 05:56:25+00:00
- **Updated**: 2017-08-11 09:44:42+00:00
- **Authors**: Janne Mustaniemi, Juho Kannala, Simo Särkkä, Jiri Matas, Janne Heikkilä
- **Comment**: None
- **Journal**: None
- **Summary**: Structure from motion algorithms have an inherent limitation that the reconstruction can only be determined up to the unknown scale factor. Modern mobile devices are equipped with an inertial measurement unit (IMU), which can be used for estimating the scale of the reconstruction. We propose a method that recovers the metric scale given inertial measurements and camera poses. In the process, we also perform a temporal and spatial alignment of the camera and the IMU. Therefore, our solution can be easily combined with any existing visual reconstruction software. The method can cope with noisy camera pose estimates, typically caused by motion blur or rolling shutter artifacts, via utilizing a Rauch-Tung-Striebel (RTS) smoother. Furthermore, the scale estimation is performed in the frequency domain, which provides more robustness to inaccurate sensor time stamps and noisy IMU samples than the previously used time domain representation. In contrast to previous methods, our approach has no parameters that need to be tuned for achieving a good performance. In the experiments, we show that the algorithm outperforms the state-of-the-art in both accuracy and convergence speed of the scale estimate. The accuracy of the scale is around $1\%$ from the ground truth depending on the recording. We also demonstrate that our method can improve the scale accuracy of the Project Tango's build-in motion tracking.



### Deep Quantization: Encoding Convolutional Activations with Deep Generative Model
- **Arxiv ID**: http://arxiv.org/abs/1611.09502v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09502v1)
- **Published**: 2016-11-29 06:07:28+00:00
- **Updated**: 2016-11-29 06:07:28+00:00
- **Authors**: Zhaofan Qiu, Ting Yao, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) have proven highly effective for visual recognition, where learning a universal representation from activations of convolutional layer plays a fundamental problem. In this paper, we present Fisher Vector encoding with Variational Auto-Encoder (FV-VAE), a novel deep architecture that quantizes the local activations of convolutional layer in a deep generative model, by training them in an end-to-end manner. To incorporate FV encoding strategy into deep generative models, we introduce Variational Auto-Encoder model, which steers a variational inference and learning in a neural network which can be straightforwardly optimized using standard stochastic gradient method. Different from the FV characterized by conventional generative models (e.g., Gaussian Mixture Model) which parsimoniously fit a discrete mixture model to data distribution, the proposed FV-VAE is more flexible to represent the natural property of data for better generalization. Extensive experiments are conducted on three public datasets, i.e., UCF101, ActivityNet, and CUB-200-2011 in the context of video action recognition and fine-grained image classification, respectively. Superior results are reported when compared to state-of-the-art representations. Most remarkably, our proposed FV-VAE achieves to-date the best published accuracy of 94.2% on UCF101.



### Is a picture worth a thousand words? A Deep Multi-Modal Fusion Architecture for Product Classification in e-commerce
- **Arxiv ID**: http://arxiv.org/abs/1611.09534v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1611.09534v1)
- **Published**: 2016-11-29 09:05:11+00:00
- **Updated**: 2016-11-29 09:05:11+00:00
- **Authors**: Tom Zahavy, Alessandro Magnani, Abhinandan Krishnan, Shie Mannor
- **Comment**: None
- **Journal**: None
- **Summary**: Classifying products into categories precisely and efficiently is a major challenge in modern e-commerce. The high traffic of new products uploaded daily and the dynamic nature of the categories raise the need for machine learning models that can reduce the cost and time of human editors. In this paper, we propose a decision level fusion approach for multi-modal product classification using text and image inputs. We train input specific state-of-the-art deep neural networks for each input source, show the potential of forging them together into a multi-modal architecture and train a novel policy network that learns to choose between them. Finally, we demonstrate that our multi-modal network improves the top-1 accuracy % over both networks on a real-world large-scale product classification dataset that we collected fromWalmart.com. While we focus on image-text fusion that characterizes e-commerce domains, our algorithms can be easily applied to other modalities such as audio, video, physical sensors, etc.



### Lens Distortion Rectification using Triangulation based Interpolation
- **Arxiv ID**: http://arxiv.org/abs/1611.09559v2
- **DOI**: 10.1007/978-3-319-27863-6_4
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09559v2)
- **Published**: 2016-11-29 10:39:12+00:00
- **Updated**: 2017-07-03 20:06:04+00:00
- **Authors**: Burak Benligiray, Cihan Topal
- **Comment**: International Symposium on Visual Computing, 2015
- **Journal**: None
- **Summary**: Nonlinear lens distortion rectification is a common first step in image processing applications where the assumption of a linear camera model is essential. For rectifying the lens distortion, forward distortion model needs to be known. However, many self-calibration methods estimate the inverse distortion model. In the literature, the inverse of the estimated model is approximated for image rectification, which introduces additional error to the system. We propose a novel distortion rectification method that uses the inverse distortion model directly. The method starts by mapping the distorted pixels to the rectified image using the inverse distortion model. The resulting set of points with subpixel locations are triangulated. The pixel values of the rectified image are linearly interpolated based on this triangulation. The method is applicable to all camera calibration methods that estimate the inverse distortion model and performs well across a large range of parameters.



### Predicting Human Eye Fixations via an LSTM-based Saliency Attentive Model
- **Arxiv ID**: http://arxiv.org/abs/1611.09571v4
- **DOI**: 10.1109/TIP.2018.2851672
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09571v4)
- **Published**: 2016-11-29 11:27:19+00:00
- **Updated**: 2018-07-09 10:18:43+00:00
- **Authors**: Marcella Cornia, Lorenzo Baraldi, Giuseppe Serra, Rita Cucchiara
- **Comment**: IEEE Transactions on Image Processing 2018
- **Journal**: None
- **Summary**: Data-driven saliency has recently gained a lot of attention thanks to the use of Convolutional Neural Networks for predicting gaze fixations. In this paper we go beyond standard approaches to saliency prediction, in which gaze maps are computed with a feed-forward network, and present a novel model which can predict accurate saliency maps by incorporating neural attentive mechanisms. The core of our solution is a Convolutional LSTM that focuses on the most salient regions of the input image to iteratively refine the predicted saliency map. Additionally, to tackle the center bias typical of human eye fixations, our model can learn a set of prior maps generated with Gaussian functions. We show, through an extensive evaluation, that the proposed architecture outperforms the current state of the art on public saliency prediction datasets. We further study the contribution of each key component to demonstrate their robustness on different scenarios.



### Occlusion-Aware Video Deblurring with a New Layered Blur Model
- **Arxiv ID**: http://arxiv.org/abs/1611.09572v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09572v1)
- **Published**: 2016-11-29 11:27:38+00:00
- **Updated**: 2016-11-29 11:27:38+00:00
- **Authors**: Byeongjoo Ahn, Tae Hyun Kim, Wonsik Kim, Kyoung Mu Lee
- **Comment**: None
- **Journal**: None
- **Summary**: We present a deblurring method for scenes with occluding objects using a carefully designed layered blur model. Layered blur model is frequently used in the motion deblurring problem to handle locally varying blurs, which is caused by object motions or depth variations in a scene. However, conventional models have a limitation in representing the layer interactions occurring at occlusion boundaries. In this paper, we address this limitation in both theoretical and experimental ways, and propose a new layered blur model reflecting actual blur generation process. Based on this model, we develop an occlusion-aware deblurring method that can estimate not only the clear foreground and background, but also the object motion more accurately. We also provide a novel analysis on the blur kernel at object boundaries, which shows the distinctive characteristics of the blur kernel that cannot be captured by conventional blur models. Experimental results on synthetic and real blurred videos demonstrate that the proposed method yields superior results, especially at object boundaries.



### Fast Face-swap Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.09577v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09577v2)
- **Published**: 2016-11-29 11:44:20+00:00
- **Updated**: 2017-07-27 13:31:39+00:00
- **Authors**: Iryna Korshunova, Wenzhe Shi, Joni Dambre, Lucas Theis
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of face swapping in images, where an input identity is transformed into a target identity while preserving pose, facial expression, and lighting. To perform this mapping, we use convolutional neural networks trained to capture the appearance of the target identity from an unstructured collection of his/her photographs.This approach is enabled by framing the face swapping problem in terms of style transfer, where the goal is to render an image in the style of another one. Building on recent advances in this area, we devise a new loss function that enables the network to produce highly photorealistic results. By combining neural networks with simple pre- and post-processing steps, we aim at making face swap work in real-time with no input from the user.



### A Large-scale Distributed Video Parsing and Evaluation Platform
- **Arxiv ID**: http://arxiv.org/abs/1611.09580v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09580v1)
- **Published**: 2016-11-29 12:07:37+00:00
- **Updated**: 2016-11-29 12:07:37+00:00
- **Authors**: Kai Yu, Yang Zhou, Da Li, Zhang Zhang, Kaiqi Huang
- **Comment**: Accepted by Chinese Conference on Intelligent Visual Surveillance
  2016
- **Journal**: None
- **Summary**: Visual surveillance systems have become one of the largest data sources of Big Visual Data in real world. However, existing systems for video analysis still lack the ability to handle the problems of scalability, expansibility and error-prone, though great advances have been achieved in a number of visual recognition tasks and surveillance applications, e.g., pedestrian/vehicle detection, people/vehicle counting. Moreover, few algorithms explore the specific values/characteristics in large-scale surveillance videos. To address these problems in large-scale video analysis, we develop a scalable video parsing and evaluation platform through combining some advanced techniques for Big Data processing, including Spark Streaming, Kafka and Hadoop Distributed Filesystem (HDFS). Also, a Web User Interface is designed in the system, to collect users' degrees of satisfaction on the recognition tasks so as to evaluate the performance of the whole system. Furthermore, the highly extensible platform running on the long-term surveillance videos makes it possible to develop more intelligent incremental algorithms to enhance the performance of various visual recognition tasks.



### Surveillance Video Parsing with Single Frame Supervision
- **Arxiv ID**: http://arxiv.org/abs/1611.09587v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09587v1)
- **Published**: 2016-11-29 12:22:46+00:00
- **Updated**: 2016-11-29 12:22:46+00:00
- **Authors**: Si Liu, Changhu Wang, Ruihe Qian, Han Yu, Renda Bao
- **Comment**: None
- **Journal**: None
- **Summary**: Surveillance video parsing, which segments the video frames into several labels, e.g., face, pants, left-leg, has wide applications. However,pixel-wisely annotating all frames is tedious and inefficient. In this paper, we develop a Single frame Video Parsing (SVP) method which requires only one labeled frame per video in training stage. To parse one particular frame, the video segment preceding the frame is jointly considered. SVP (1) roughly parses the frames within the video segment, (2) estimates the optical flow between frames and (3) fuses the rough parsing results warped by optical flow to produce the refined parsing result. The three components of SVP, namely frame parsing, optical flow estimation and temporal fusion are integrated in an end-to-end manner. Experimental results on two surveillance video datasets show the superiority of SVP over state-of-the-arts.



### Efficient Linear Programming for Dense CRFs
- **Arxiv ID**: http://arxiv.org/abs/1611.09718v2
- **DOI**: None
- **Categories**: **cs.CV**, G.1.6; I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/1611.09718v2)
- **Published**: 2016-11-29 16:46:54+00:00
- **Updated**: 2017-02-14 07:34:13+00:00
- **Authors**: Thalaiyasingam Ajanthan, Alban Desmaison, Rudy Bunel, Mathieu Salzmann, Philip H. S. Torr, M. Pawan Kumar
- **Comment**: 24 pages, 10 figures and 4 tables
- **Journal**: None
- **Summary**: The fully connected conditional random field (CRF) with Gaussian pairwise potentials has proven popular and effective for multi-class semantic segmentation. While the energy of a dense CRF can be minimized accurately using a linear programming (LP) relaxation, the state-of-the-art algorithm is too slow to be useful in practice. To alleviate this deficiency, we introduce an efficient LP minimization algorithm for dense CRFs. To this end, we develop a proximal minimization framework, where the dual of each proximal problem is optimized via block coordinate descent. We show that each block of variables can be efficiently optimized. Specifically, for one block, the problem decomposes into significantly smaller subproblems, each of which is defined over a single pixel. For the other block, the problem is optimized via conditional gradient descent. This has two advantages: 1) the conditional gradient can be computed in a time linear in the number of pixels and labels; and 2) the optimal step size can be computed analytically. Our experiments on standard datasets provide compelling evidence that our approach outperforms all existing baselines including the previous LP based approach for dense CRFs.



### Gossip training for deep learning
- **Arxiv ID**: http://arxiv.org/abs/1611.09726v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1611.09726v1)
- **Published**: 2016-11-29 17:01:31+00:00
- **Updated**: 2016-11-29 17:01:31+00:00
- **Authors**: Michael Blot, David Picard, Matthieu Cord, Nicolas Thome
- **Comment**: None
- **Journal**: None
- **Summary**: We address the issue of speeding up the training of convolutional networks. Here we study a distributed method adapted to stochastic gradient descent (SGD). The parallel optimization setup uses several threads, each applying individual gradient descents on a local variable. We propose a new way to share information between different threads inspired by gossip algorithms and showing good consensus convergence properties. Our method called GoSGD has the advantage to be fully asynchronous and decentralized. We compared our method to the recent EASGD in \cite{elastic} on CIFAR-10 show encouraging results.



### Computer Aided Detection of Oral Lesions on CT Images
- **Arxiv ID**: http://arxiv.org/abs/1611.09769v1
- **DOI**: 10.1088/1748-0221-10-12-C12030
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09769v1)
- **Published**: 2016-11-29 18:24:23+00:00
- **Updated**: 2016-11-29 18:24:23+00:00
- **Authors**: Shaikat Galib, Fahima Islam, Muhammad Abir, Hyoung-Koo Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Oral lesions are important findings on computed tomography (CT) images. In this study, a fully automatic method to detect oral lesions in mandibular region from dental CT images is proposed. Two methods were developed to recognize two types of lesions namely (1) Close border (CB) lesions and (2) Open border (OB) lesions, which cover most of the lesion types that can be found on CT images. For the detection of CB lesions, fifteen features were extracted from each initial lesion candidates and multi layer perceptron (MLP) neural network was used to classify suspicious regions. Moreover, OB lesions were detected using a rule based image processing method, where no feature extraction or classification algorithm were used. The results were validated using a CT dataset of 52 patients, where 22 patients had abnormalities and 30 patients were normal. Using non-training dataset, CB detection algorithm yielded 71% sensitivity with 0.31 false positives per patient. Furthermore, OB detection algorithm achieved 100% sensitivity with 0.13 false positives per patient. Results suggest that, the proposed framework, which consists of two methods, has the potential to be used in clinical context, and assist radiologists for better diagnosis.



### On the Existence of Synchrostates in Multichannel EEG Signals during Face-perception Tasks
- **Arxiv ID**: http://arxiv.org/abs/1611.09791v1
- **DOI**: 10.1088/2057-1976/1/1/015002
- **Categories**: **physics.med-ph**, cs.CV, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1611.09791v1)
- **Published**: 2016-11-29 19:03:29+00:00
- **Updated**: 2016-11-29 19:03:29+00:00
- **Authors**: Wasifa Jamal, Saptarshi Das, Koushik Maharatna, Fabio Apicella, Georgia Chronaki, Federico Sicca, David Cohen, Filippo Muratori
- **Comment**: 30 pages, 22 figures, 2 tables
- **Journal**: Biomedical Physics & Engineering Express, vol. 1, no. 1, pp.
  015002, 2015
- **Summary**: Phase synchronisation in multichannel EEG is known as the manifestation of functional brain connectivity. Traditional phase synchronisation studies are mostly based on time average synchrony measures hence do not preserve the temporal evolution of the phase difference. Here we propose a new method to show the existence of a small set of unique phase synchronised patterns or "states" in multi-channel EEG recordings, each "state" being stable of the order of ms, from typical and pathological subjects during face perception tasks. The proposed methodology bridges the concepts of EEG microstates and phase synchronisation in time and frequency domain respectively. The analysis is reported for four groups of children including typical, Autism Spectrum Disorder (ASD), low and high anxiety subjects - a total of 44 subjects. In all cases, we observe consistent existence of these states - termed as synchrostates - within specific cognition related frequency bands (beta and gamma bands), though the topographies of these synchrostates differ for different subject groups with different pathological conditions. The inter-synchrostate switching follows a well-defined sequence capturing the underlying inter-electrode phase relation dynamics in stimulus- and person-centric manner. Our study is motivated from the well-known EEG microstate exhibiting stable potential maps over the scalp. However, here we report a similar observation of quasi-stable phase synchronised states in multichannel EEG. The existence of the synchrostates coupled with their unique switching sequence characteristics could be considered as a potentially new field over contemporary EEG phase synchronisation studies.



### InterpoNet, A brain inspired neural network for optical flow dense interpolation
- **Arxiv ID**: http://arxiv.org/abs/1611.09803v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09803v3)
- **Published**: 2016-11-29 19:37:57+00:00
- **Updated**: 2017-02-24 13:48:29+00:00
- **Authors**: Shay Zweig, Lior Wolf
- **Comment**: 16 pages, 11 figures, 8 tables
- **Journal**: None
- **Summary**: Sparse-to-dense interpolation for optical flow is a fundamental phase in the pipeline of most of the leading optical flow estimation algorithms. The current state-of-the-art method for interpolation, EpicFlow, is a local average method based on an edge aware geodesic distance. We propose a new data-driven sparse-to-dense interpolation algorithm based on a fully convolutional network. We draw inspiration from the filling-in process in the visual cortex and introduce lateral dependencies between neurons and multi-layer supervision into our learning process. We also show the importance of the image contour to the learning process. Our method is robust and outperforms EpicFlow on competitive optical flow benchmarks with several underlying matching algorithms. This leads to state-of-the-art performance on the Sintel and KITTI 2012 benchmarks.



### 3D Ultrasound image segmentation: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1611.09811v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09811v1)
- **Published**: 2016-11-29 19:57:28+00:00
- **Updated**: 2016-11-29 19:57:28+00:00
- **Authors**: Mohammad Hamed Mozaffari, WonSook Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Three-dimensional Ultrasound image segmentation methods are surveyed in this paper. The focus of this report is to investigate applications of these techniques and a review of the original ideas and concepts. Although many two-dimensional image segmentation in the literature have been considered as a three-dimensional approach by mistake but we review them as a three-dimensional technique. We select the studies that have addressed the problem of medical three-dimensional Ultrasound image segmentation utilizing their proposed techniques. The evaluation methods and comparison between them are presented and tabulated in terms of evaluation techniques, interactivity, and robustness.



### Monocular 3D Human Pose Estimation In The Wild Using Improved CNN Supervision
- **Arxiv ID**: http://arxiv.org/abs/1611.09813v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09813v5)
- **Published**: 2016-11-29 20:03:19+00:00
- **Updated**: 2017-10-04 15:21:46+00:00
- **Authors**: Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal Fua, Oleksandr Sotnychenko, Weipeng Xu, Christian Theobalt
- **Comment**: Accepted at the International Conference on 3D Vision (3DV) 2017
- **Journal**: None
- **Summary**: We propose a CNN-based approach for 3D human body pose estimation from single RGB images that addresses the issue of limited generalizability of models trained solely on the starkly limited publicly available 3D pose data. Using only the existing 3D pose data and 2D pose data, we show state-of-the-art performance on established benchmarks through transfer of learned features, while also generalizing to in-the-wild scenes. We further introduce a new training set for human body pose estimation from monocular images of real humans that has the ground truth captured with a multi-camera marker-less motion capture system. It complements existing corpora with greater diversity in pose, human appearance, clothing, occlusion, and viewpoints, and enables an increased scope of augmentation. We also contribute a new benchmark that covers outdoor and indoor scenes, and demonstrate that our 3D pose dataset shows better in-the-wild performance than existing annotated data, which is further improved in conjunction with transfer learning from 2D pose data. All in all, we argue that the use of transfer learning of representations in tandem with algorithmic and data contributions is crucial for general 3D body pose estimation.



### Measuring and modeling the perception of natural and unconstrained gaze in humans and machines
- **Arxiv ID**: http://arxiv.org/abs/1611.09819v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1611.09819v1)
- **Published**: 2016-11-29 20:11:09+00:00
- **Updated**: 2016-11-29 20:11:09+00:00
- **Authors**: Daniel Harari, Tao Gao, Nancy Kanwisher, Joshua Tenenbaum, Shimon Ullman
- **Comment**: Daniel Harari and Tao Gao contributed equally to this work
- **Journal**: None
- **Summary**: Humans are remarkably adept at interpreting the gaze direction of other individuals in their surroundings. This skill is at the core of the ability to engage in joint visual attention, which is essential for establishing social interactions. How accurate are humans in determining the gaze direction of others in lifelike scenes, when they can move their heads and eyes freely, and what are the sources of information for the underlying perceptual processes? These questions pose a challenge from both empirical and computational perspectives, due to the complexity of the visual input in real-life situations. Here we measure empirically human accuracy in perceiving the gaze direction of others in lifelike scenes, and study computationally the sources of information and representations underlying this cognitive capacity. We show that humans perform better in face-to-face conditions compared with recorded conditions, and that this advantage is not due to the availability of input dynamics. We further show that humans are still performing well when only the eyes-region is visible, rather than the whole face. We develop a computational model, which replicates the pattern of human performance, including the finding that the eyes-region contains on its own, the required information for estimating both head orientation and direction of gaze. Consistent with neurophysiological findings on task-specific face regions in the brain, the learned computational representations reproduce perceptual effects such as the Wollaston illusion, when trained to estimate direction of gaze, but not when trained to recognize objects or faces.



### Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction
- **Arxiv ID**: http://arxiv.org/abs/1611.09842v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09842v3)
- **Published**: 2016-11-29 20:55:42+00:00
- **Updated**: 2017-04-20 08:12:21+00:00
- **Authors**: Richard Zhang, Phillip Isola, Alexei A. Efros
- **Comment**: Accepted to CVPR 2017
- **Journal**: None
- **Summary**: We propose split-brain autoencoders, a straightforward modification of the traditional autoencoder architecture, for unsupervised representation learning. The method adds a split to the network, resulting in two disjoint sub-networks. Each sub-network is trained to perform a difficult task -- predicting one subset of the data channels from another. Together, the sub-networks extract features from the entire input signal. By forcing the network to solve cross-channel prediction tasks, we induce a representation within the network which transfers well to other, unseen tasks. This method achieves state-of-the-art performance on several large-scale transfer learning benchmarks.



### Learning a Discriminative Filter Bank within a CNN for Fine-grained Recognition
- **Arxiv ID**: http://arxiv.org/abs/1611.09932v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09932v3)
- **Published**: 2016-11-29 23:01:59+00:00
- **Updated**: 2018-06-12 01:48:17+00:00
- **Authors**: Yaming Wang, Vlad I. Morariu, Larry S. Davis
- **Comment**: None
- **Journal**: None
- **Summary**: Compared to earlier multistage frameworks using CNN features, recent end-to-end deep approaches for fine-grained recognition essentially enhance the mid-level learning capability of CNNs. Previous approaches achieve this by introducing an auxiliary network to infuse localization information into the main classification network, or a sophisticated feature encoding method to capture higher order feature statistics. We show that mid-level representation learning can be enhanced within the CNN framework, by learning a bank of convolutional filters that capture class-specific discriminative patches without extra part or bounding box annotations. Such a filter bank is well structured, properly initialized and discriminatively learned through a novel asymmetric multi-stream architecture with convolutional filter supervision and a non-random layer initialization. Experimental results show that our approach achieves state-of-the-art on three publicly available fine-grained recognition datasets (CUB-200-2011, Stanford Cars and FGVC-Aircraft). Ablation studies and visualizations are provided to understand our approach.



### Photographic home styles in Congress: a computer vision approach
- **Arxiv ID**: http://arxiv.org/abs/1611.09942v2
- **DOI**: None
- **Categories**: **cs.SI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1611.09942v2)
- **Published**: 2016-11-29 23:41:00+00:00
- **Updated**: 2016-12-05 23:34:49+00:00
- **Authors**: L. Jason Anastasopoulos, Dhruvil Badani, Crystal Lee, Shiry Ginosar, Jake Williams
- **Comment**: None
- **Journal**: None
- **Summary**: While members of Congress now routinely communicate with constituents using images on a variety of internet platforms, little is known about how images are used as a means of strategic political communication. This is due primarily to computational limitations which have prevented large-scale, systematic analyses of image features. New developments in computer vision, however, are bringing the systematic study of images within reach. Here, we develop a framework for understanding visual political communication by extending Fenno's analysis of home style (Fenno 1978) to images and introduce "photographic" home styles. Using approximately 192,000 photographs collected from MCs Facebook profiles, we build machine learning software with convolutional neural networks and conduct an image manipulation experiment to explore how the race of people that MCs pose with shape photographic home styles. We find evidence that electoral pressures shape photographic home styles and demonstrate that Democratic and Republican members of Congress use images in very different ways.



