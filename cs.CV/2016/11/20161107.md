# Arxiv Papers in cs.CV on 2016-11-07
### Action2Activity: Recognizing Complex Activities from Sensor Data
- **Arxiv ID**: http://arxiv.org/abs/1611.01872v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.01872v1)
- **Published**: 2016-11-07 02:01:29+00:00
- **Updated**: 2016-11-07 02:01:29+00:00
- **Authors**: Ye Liu, Liqiang Nie, Lei Han, Luming Zhang, David S Rosenblum
- **Comment**: IJCAI 2015
- **Journal**: None
- **Summary**: As compared to simple actions, activities are much more complex, but semantically consistent with a human's real life. Techniques for action recognition from sensor generated data are mature. However, there has been relatively little work on bridging the gap between actions and activities. To this end, this paper presents a novel approach for complex activity recognition comprising of two components. The first component is temporal pattern mining, which provides a mid-level feature representation for activities, encodes temporal relatedness among actions, and captures the intrinsic properties of activities. The second component is adaptive Multi-Task Learning, which captures relatedness among activities and selects discriminant features. Extensive experiments on a real-world dataset demonstrate the effectiveness of our work.



### High-Resolution Semantic Labeling with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.01962v1
- **DOI**: 10.1109/TGRS.2017.2740362
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.01962v1)
- **Published**: 2016-11-07 10:02:49+00:00
- **Updated**: 2016-11-07 10:02:49+00:00
- **Authors**: Emmanuel Maggiori, Yuliya Tarabalka, Guillaume Charpiat, Pierre Alliez
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have received increasing attention over the last few years. They were initially conceived for image categorization, i.e., the problem of assigning a semantic label to an entire input image.   In this paper we address the problem of dense semantic labeling, which consists in assigning a semantic label to every pixel in an image. Since this requires a high spatial accuracy to determine where labels are assigned, categorization CNNs, intended to be highly robust to local deformations, are not directly applicable.   By adapting categorization networks, many semantic labeling CNNs have been recently proposed. Our first contribution is an in-depth analysis of these architectures. We establish the desired properties of an ideal semantic labeling CNN, and assess how those methods stand with regard to these properties. We observe that even though they provide competitive results, these CNNs often underexploit properties of semantic labeling that could lead to more effective and efficient architectures.   Out of these observations, we then derive a CNN framework specifically adapted to the semantic labeling problem. In addition to learning features at different resolutions, it learns how to combine these features. By integrating local and global information in an efficient and flexible manner, it outperforms previous techniques. We evaluate the proposed framework and compare it with state-of-the-art architectures on public benchmarks of high-resolution aerial image labeling.



### Fixed-point Factorized Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.01972v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1611.01972v2)
- **Published**: 2016-11-07 10:26:41+00:00
- **Updated**: 2017-08-29 09:46:41+00:00
- **Authors**: Peisong Wang, Jian Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, Deep Neural Networks (DNN) based methods have achieved remarkable performance in a wide range of tasks and have been among the most powerful and widely used techniques in computer vision. However, DNN-based methods are both computational-intensive and resource-consuming, which hinders the application of these methods on embedded systems like smart phones. To alleviate this problem, we introduce a novel Fixed-point Factorized Networks (FFN) for pretrained models to reduce the computational complexity as well as the storage requirement of networks. The resulting networks have only weights of -1, 0 and 1, which significantly eliminates the most resource-consuming multiply-accumulate operations (MACs). Extensive experiments on large-scale ImageNet classification task show the proposed FFN only requires one-thousandth of multiply operations with comparable accuracy.



### Chinese/English mixed Character Segmentation as Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1611.01982v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.01982v2)
- **Published**: 2016-11-07 10:53:29+00:00
- **Updated**: 2016-11-16 01:46:11+00:00
- **Authors**: Huabin Zheng, Jingyu Wang, Zhengjie Huang, Yang Yang, Rong Pan
- **Comment**: Submitted to CVPR 2017
- **Journal**: None
- **Summary**: OCR character segmentation for multilingual printed documents is difficult due to the diversity of different linguistic characters. Previous approaches mainly focus on monolingual texts and are not suitable for multilingual-lingual cases. In this work, we particularly tackle the Chinese/English mixed case by reframing it as a semantic segmentation problem. We take advantage of the successful architecture called fully convolutional networks (FCN) in the field of semantic segmentation. Given a wide enough receptive field, FCN can utilize the necessary context around a horizontal position to determinate whether this is a splitting point or not. As a deep neural architecture, FCN can automatically learn useful features from raw text line images. Although trained on synthesized samples with simulated random disturbance, our FCN model generalizes well to real-world samples. The experimental results show that our model significantly outperforms the previous methods.



### Hamiltonian operator for spectral shape analysis
- **Arxiv ID**: http://arxiv.org/abs/1611.01990v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1611.01990v2)
- **Published**: 2016-11-07 11:11:10+00:00
- **Updated**: 2017-06-25 17:45:00+00:00
- **Authors**: Yoni Choukroun, Alon Shtern, Alex Bronstein, Ron Kimmel
- **Comment**: None
- **Journal**: None
- **Summary**: Many shape analysis methods treat the geometry of an object as a metric space that can be captured by the Laplace-Beltrami operator. In this paper, we propose to adapt the classical Hamiltonian operator from quantum mechanics to the field of shape analysis. To this end we study the addition of a potential function to the Laplacian as a generator for dual spaces in which shape processing is performed. We present a general optimization approach for solving variational problems involving the basis defined by the Hamiltonian using perturbation theory for its eigenvectors. The suggested operator is shown to produce better functional spaces to operate with, as demonstrated on different shape analysis tasks.



### Real-Time Visual Place Recognition for Personal Localization on a Mobile Device
- **Arxiv ID**: http://arxiv.org/abs/1611.02061v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1611.02061v2)
- **Published**: 2016-11-07 14:11:12+00:00
- **Updated**: 2017-04-27 09:50:30+00:00
- **Authors**: Michał Nowicki, Jan Wietrzykowski, Piotr Skrzypczyński
- **Comment**: None
- **Journal**: None
- **Summary**: The paper presents an approach to indoor personal localization on a mobile device based on visual place recognition. We implemented on a smartphone two state-of-the-art algorithms that are representative to two different approaches to visual place recognition: FAB-MAP that recognizes places using individual images, and ABLE-M that utilizes sequences of images. These algorithms are evaluated in environments of different structure, focusing on problems commonly encountered when a mobile device camera is used. The conclusions drawn from this evaluation are guidelines to design the FastABLE system, which is based on the ABLE-M algorithm, but introduces major modifications to the concept of image matching. The improvements radically cut down the processing time and improve scalability, making it possible to localize the user in long image sequences with the limited computing power of a mobile device. The resulting place recognition system compares favorably to both the ABLE-M and the FAB-MAP solutions in the context of real-time personal localization.



### A Fully Convolutional Neural Network based Structured Prediction Approach Towards the Retinal Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1611.02064v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.02064v2)
- **Published**: 2016-11-07 14:16:18+00:00
- **Updated**: 2016-11-16 09:21:40+00:00
- **Authors**: Avijit Dasgupta, Sonam Singh
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic segmentation of retinal blood vessels from fundus images plays an important role in the computer aided diagnosis of retinal diseases. The task of blood vessel segmentation is challenging due to the extreme variations in morphology of the vessels against noisy background. In this paper, we formulate the segmentation task as a multi-label inference task and utilize the implicit advantages of the combination of convolutional neural networks and structured prediction. Our proposed convolutional neural network based model achieves strong performance and significantly outperforms the state-of-the-art for automatic retinal blood vessel segmentation on DRIVE dataset with 95.33% accuracy and 0.974 AUC score.



### Texture and Color-based Image Retrieval Using the Local Extrema Features and Riemannian Distance
- **Arxiv ID**: http://arxiv.org/abs/1611.02102v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.02102v2)
- **Published**: 2016-11-07 15:20:59+00:00
- **Updated**: 2017-03-03 09:25:17+00:00
- **Authors**: Minh-Tan Pham, Grégoire Mercier, Lionel Bombrun, Julien Michel
- **Comment**: This paper has been withdrawn by the author due to a crucial equation
  modification in part II.B
- **Journal**: None
- **Summary**: A novel efficient method for content-based image retrieval (CBIR) is developed in this paper using both texture and color features. Our motivation is to represent and characterize an input image by a set of local descriptors extracted at characteristic points (i.e. keypoints) within the image. Then, dissimilarity measure between images is calculated based on the geometric distance between the topological feature spaces (i.e. manifolds) formed by the sets of local descriptors generated from these images. In this work, we propose to extract and use the local extrema pixels as our feature points. Then, the so-called local extrema-based descriptor (LED) is generated for each keypoint by integrating all color, spatial as well as gradient information captured by a set of its nearest local extrema. Hence, each image is encoded by a LED feature point cloud and riemannian distances between these point clouds enable us to tackle CBIR. Experiments performed on Vistex, Stex and colored Brodatz texture databases using the proposed approach provide very efficient and competitive results compared to the state-of-the-art methods.



### Crowdsourcing in Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1611.02145v1
- **DOI**: 10.1561/0600000073
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1611.02145v1)
- **Published**: 2016-11-07 16:11:19+00:00
- **Updated**: 2016-11-07 16:11:19+00:00
- **Authors**: Adriana Kovashka, Olga Russakovsky, Li Fei-Fei, Kristen Grauman
- **Comment**: A 69-page meta review of the field, Foundations and Trends in
  Computer Graphics and Vision, 2016
- **Journal**: None
- **Summary**: Computer vision systems require large amounts of manually annotated data to properly learn challenging visual concepts. Crowdsourcing platforms offer an inexpensive method to capture human knowledge and understanding, for a vast number of visual perception tasks. In this survey, we describe the types of annotations computer vision researchers have collected using crowdsourcing, and how they have ensured that this data is of high quality while annotation effort is minimized. We begin by discussing data collection on both classic (e.g., object recognition) and recent (e.g., visual story-telling) vision tasks. We then summarize key design decisions for creating effective data collection interfaces and workflows, and present strategies for intelligently selecting the most important data instances to annotate. Finally, we conclude with some thoughts on the future of crowdsourcing in computer vision.



### Spatiotemporal Residual Networks for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1611.02155v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.02155v1)
- **Published**: 2016-11-07 16:17:16+00:00
- **Updated**: 2016-11-07 16:17:16+00:00
- **Authors**: Christoph Feichtenhofer, Axel Pinz, Richard P. Wildes
- **Comment**: NIPS 2016
- **Journal**: None
- **Summary**: Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping these with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art.



### Unsupervised Cross-Domain Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1611.02200v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.02200v1)
- **Published**: 2016-11-07 18:14:57+00:00
- **Updated**: 2016-11-07 18:14:57+00:00
- **Authors**: Yaniv Taigman, Adam Polyak, Lior Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given function f, which accepts inputs in either domains, would remain unchanged. Other than the function f, the training data is unsupervised and consist of a set of samples from each domain. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f-constancy component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.



### Meat adulteration detection through digital image analysis of histological cuts using LBP
- **Arxiv ID**: http://arxiv.org/abs/1611.02260v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.02260v1)
- **Published**: 2016-11-07 20:40:57+00:00
- **Updated**: 2016-11-07 20:40:57+00:00
- **Authors**: João J. de Macedo Neto, Jefersson A. dos Santos, William Robson Schwartz
- **Comment**: None
- **Journal**: None
- **Summary**: Food fraud has been an area of great concern due to its risk to public health, reduction of food quality or nutritional value and for its economic consequences. For this reason, it's been object of regulation in many countries (e.g. [1], [2]). One type of food that has been frequently object of fraud through the addition of water or an aqueous solution is bovine meat. The traditional methods used to detect this kind of fraud are expensive, time-consuming and depend on physicochemical analysis that require complex laboratory techniques, specific for each added substance. In this paper, based on digital images of histological cuts of adulterated and not-adulterated (normal) bovine meat, we evaluate the of digital image analysis methods to identify the aforementioned kind of fraud, with focus on the Local Binary Pattern (LBP) algorithm.



### Memory-augmented Attention Modelling for Videos
- **Arxiv ID**: http://arxiv.org/abs/1611.02261v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1611.02261v4)
- **Published**: 2016-11-07 20:50:08+00:00
- **Updated**: 2017-04-24 07:26:01+00:00
- **Authors**: Rasool Fakoor, Abdel-rahman Mohamed, Margaret Mitchell, Sing Bing Kang, Pushmeet Kohli
- **Comment**: Revised version, minor changes, add the link for the source codes
- **Journal**: None
- **Summary**: We present a method to improve video description generation by modeling higher-order interactions between video frames and described concepts. By storing past visual attention in the video associated to previously generated words, the system is able to decide what to look at and describe in light of what it has already looked at and described. This enables not only more effective local attention, but tractable consideration of the video sequence while generating each word. Evaluation on the challenging and popular MSVD and Charades datasets demonstrates that the proposed architecture outperforms previous video description approaches without requiring external temporal video features.



