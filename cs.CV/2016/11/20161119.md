# Arxiv Papers in cs.CV on 2016-11-19
### Understanding Anatomy Classification Through Attentive Response Maps
- **Arxiv ID**: http://arxiv.org/abs/1611.06284v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06284v3)
- **Published**: 2016-11-19 00:20:38+00:00
- **Updated**: 2018-02-07 15:58:59+00:00
- **Authors**: Devinder Kumar, Vlado Menkovski, Graham W. Taylor, Alexander Wong
- **Comment**: Accepted at ISBI, 2018
- **Journal**: None
- **Summary**: One of the main challenges for broad adoption of deep learning based models such as convolutional neural networks (CNN), is the lack of understanding of their decisions. In many applications, a simpler, less capable model that can be easily understood is favorable to a black-box model that has superior performance. In this paper, we present an approach for designing CNNs based on visualization of the internal activations of the model. We visualize the model's response through attentive response maps obtained using a fractional stride convolution technique and compare the results with known imaging landmarks from the medical literature. We show that sufficiently deep and capable models can be successfully trained to use the same medical landmarks a human expert would use. Our approach allows for communicating the model decision process well, but also offers insight towards detecting biases.



### A Bayesian approach to type-specific conic fitting
- **Arxiv ID**: http://arxiv.org/abs/1611.06296v1
- **DOI**: None
- **Categories**: **cs.CV**, G.1.6; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1611.06296v1)
- **Published**: 2016-11-19 02:51:48+00:00
- **Updated**: 2016-11-19 02:51:48+00:00
- **Authors**: Matthew Collett
- **Comment**: 27 pages, 9 figures
- **Journal**: None
- **Summary**: A perturbative approach is used to quantify the effect of noise in data points on fitted parameters in a general homogeneous linear model, and the results applied to the case of conic sections. There is an optimal choice of normalisation that minimises bias, and iteration with the correct reweighting significantly improves statistical reliability. By conditioning on an appropriate prior, an unbiased type-specific fit can be obtained. Error estimates for the conic coefficients may also be used to obtain both bias corrections and confidence intervals for other curve parameters.



### Inferring Restaurant Styles by Mining Crowd Sourced Photos from User-Review Websites
- **Arxiv ID**: http://arxiv.org/abs/1611.06301v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06301v3)
- **Published**: 2016-11-19 04:27:28+00:00
- **Updated**: 2022-03-23 16:27:31+00:00
- **Authors**: Haofu Liao, Yuncheng Li, Tianran Hu, Jiebo Luo
- **Comment**: 10 pages, Accepted by IEEE BigData 2016
- **Journal**: None
- **Summary**: When looking for a restaurant online, user uploaded photos often give people an immediate and tangible impression about a restaurant. Due to their informativeness, such user contributed photos are leveraged by restaurant review websites to provide their users an intuitive and effective search experience. In this paper, we present a novel approach to inferring restaurant types or styles (ambiance, dish styles, suitability for different occasions) from user uploaded photos on user-review websites. To that end, we first collect a novel restaurant photo dataset associating the user contributed photos with the restaurant styles from TripAdvior. We then propose a deep multi-instance multi-label learning (MIML) framework to deal with the unique problem setting of the restaurant style classification task. We employ a two-step bootstrap strategy to train a multi-label convolutional neural network (CNN). The multi-label CNN is then used to compute the confidence scores of restaurant styles for all the images associated with a restaurant. The computed confidence scores are further used to train a final binary classifier for each restaurant style tag. Upon training, the styles of a restaurant can be profiled by analyzing restaurant photos with the trained multi-label CNN and SVM models. Experimental evaluation has demonstrated that our crowd sourcing-based approach can effectively infer the restaurant style when there are a sufficient number of user uploaded photos for a given restaurant.



### Multi-Scale Saliency Detection using Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/1611.06307v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06307v3)
- **Published**: 2016-11-19 05:27:12+00:00
- **Updated**: 2017-07-05 03:46:15+00:00
- **Authors**: Shubham Pachori
- **Comment**: None
- **Journal**: None
- **Summary**: Saliency detection has drawn a lot of attention of researchers in various fields over the past several years. Saliency is the perceptual quality that makes an object, person to draw the attention of humans at the very sight. Salient object detection in an image has been used centrally in many computational photography and computer vision applications like video compression, object recognition and classification, object segmentation, adaptive content delivery, motion detection, content aware resizing, camouflage images and change blindness images to name a few. We propose a method to detect saliency in the objects using multimodal dictionary learning which has been recently used in classification and image fusion. The multimodal dictionary that we are learning is task driven which gives improved performance over its counterpart (the one which is not task specific).



### Learning the Number of Neurons in Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.06321v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1611.06321v3)
- **Published**: 2016-11-19 07:18:17+00:00
- **Updated**: 2018-10-11 07:18:09+00:00
- **Authors**: Jose M Alvarez, Mathieu Salzmann
- **Comment**: NIPS 2016
- **Journal**: None
- **Summary**: Nowadays, the number of layers and of neurons in each layer of a deep network are typically set manually. While very deep and wide networks have proven effective in general, they come at a high memory and computation cost, thus making them impractical for constrained platforms. These networks, however, are known to have many redundant parameters, and could thus, in principle, be replaced by more compact architectures. In this paper, we introduce an approach to automatically determining the number of neurons in each layer of a deep network during learning. To this end, we propose to make use of structured sparsity during learning. More precisely, we use a group sparsity regularizer on the parameters of the network, where each group is defined to act on a single neuron. Starting from an overcomplete network, we show that our approach can reduce the number of parameters by up to 80\% while retaining or even improving the network accuracy.



### Beyond Deep Residual Learning for Image Restoration: Persistent Homology-Guided Manifold Simplification
- **Arxiv ID**: http://arxiv.org/abs/1611.06345v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06345v4)
- **Published**: 2016-11-19 11:43:43+00:00
- **Updated**: 2017-06-08 16:52:33+00:00
- **Authors**: Woong Bae, Jaejun Yoo, Jong Chul Ye
- **Comment**: Accepted at CVPRW 2017 source code :
  https://github.com/iorism/CNN.git
- **Journal**: None
- **Summary**: The latest deep learning approaches perform better than the state-of-the-art signal processing approaches in various image restoration tasks. However, if an image contains many patterns and structures, the performance of these CNNs is still inferior. To address this issue, here we propose a novel feature space deep residual learning algorithm that outperforms the existing residual learning. The main idea is originated from the observation that the performance of a learning algorithm can be improved if the input and/or label manifolds can be made topologically simpler by an analytic mapping to a feature space. Our extensive numerical studies using denoising experiments and NTIRE single-image super-resolution (SISR) competition demonstrate that the proposed feature space residual learning outperforms the existing state-of-the-art approaches. Moreover, our algorithm was ranked third in NTIRE competition with 5-10 times faster computational time compared to the top ranked teams. The source code is available on page : https://github.com/iorism/CNN.git



### Invertible Conditional GANs for image editing
- **Arxiv ID**: http://arxiv.org/abs/1611.06355v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1611.06355v1)
- **Published**: 2016-11-19 12:35:01+00:00
- **Updated**: 2016-11-19 12:35:01+00:00
- **Authors**: Guim Perarnau, Joost van de Weijer, Bogdan Raducanu, Jose M. √Ålvarez
- **Comment**: Accepted paper at NIPS 2016 Workshop on Adversarial Training
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have recently demonstrated to successfully approximate complex data distributions. A relevant extension of this model is conditional GANs (cGANs), where the introduction of external information allows to determine specific representations of the generated images. In this work, we evaluate encoders to inverse the mapping of a cGAN, i.e., mapping a real image into a latent space and a conditional representation. This allows, for example, to reconstruct and modify real images of faces conditioning on arbitrary attributes. Additionally, we evaluate the design of cGANs. The combination of an encoder with a cGAN, which we call Invertible cGAN (IcGAN), enables to re-generate real images with deterministic complex modifications.



### Ordinal Constrained Binary Code Learning for Nearest Neighbor Search
- **Arxiv ID**: http://arxiv.org/abs/1611.06362v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.7, H.3.3
- **Links**: [PDF](http://arxiv.org/pdf/1611.06362v1)
- **Published**: 2016-11-19 13:24:10+00:00
- **Updated**: 2016-11-19 13:24:10+00:00
- **Authors**: Hong Liu, Rongrong Ji, Yongjian Wu, Feiyue Huang
- **Comment**: Accepted to AAAI 2017
- **Journal**: None
- **Summary**: Recent years have witnessed extensive attention in binary code learning, a.k.a. hashing, for nearest neighbor search problems. It has been seen that high-dimensional data points can be quantized into binary codes to give an efficient similarity approximation via Hamming distance. Among existing schemes, ranking-based hashing is recent promising that targets at preserving ordinal relations of ranking in the Hamming space to minimize retrieval loss. However, the size of the ranking tuples, which shows the ordinal relations, is quadratic or cubic to the size of training samples. By given a large-scale training data set, it is very expensive to embed such ranking tuples in binary code learning. Besides, it remains a dificulty to build ranking tuples efficiently for most ranking-preserving hashing, which are deployed over an ordinal graph-based setting. To handle these problems, we propose a novel ranking-preserving hashing method, dubbed Ordinal Constraint Hashing (OCH), which efficiently learns the optimal hashing functions with a graph-based approximation to embed the ordinal relations. The core idea is to reduce the size of ordinal graph with ordinal constraint projection, which preserves the ordinal relations through a small data set (such as clusters or random samples). In particular, to learn such hash functions effectively, we further relax the discrete constraints and design a specific stochastic gradient decent algorithm for optimization. Experimental results on three large-scale visual search benchmark datasets, i.e. LabelMe, Tiny100K and GIST1M, show that the proposed OCH method can achieve superior performance over the state-of-the-arts approaches.



### Deep Residual Learning for Compressed Sensing CT Reconstruction via Persistent Homology Analysis
- **Arxiv ID**: http://arxiv.org/abs/1611.06391v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06391v2)
- **Published**: 2016-11-19 16:05:43+00:00
- **Updated**: 2016-11-25 06:54:06+00:00
- **Authors**: Yo Seob Han, Jaejun Yoo, Jong Chul Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, compressed sensing (CS) computed tomography (CT) using sparse projection views has been extensively investigated to reduce the potential risk of radiation to patient. However, due to the insufficient number of projection views, an analytic reconstruction approach results in severe streaking artifacts and CS-based iterative approach is computationally very expensive. To address this issue, here we propose a novel deep residual learning approach for sparse view CT reconstruction. Specifically, based on a novel persistent homology analysis showing that the manifold of streaking artifacts is topologically simpler than original ones, a deep residual learning architecture that estimates the streaking artifacts is developed. Once a streaking artifact image is estimated, an artifact-free image can be obtained by subtracting the streaking artifacts from the input image. Using extensive experiments with real patient data set, we confirm that the proposed residual learning provides significantly better image reconstruction performance with several orders of magnitude faster computational speed.



### Semantic tracking: Single-target tracking with inter-supervised convolutional networks
- **Arxiv ID**: http://arxiv.org/abs/1611.06395v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06395v1)
- **Published**: 2016-11-19 16:10:03+00:00
- **Updated**: 2016-11-19 16:10:03+00:00
- **Authors**: Jingjing Xiao, Qiang Lan, Linbo Qiao, Ales Leonardis
- **Comment**: None
- **Journal**: None
- **Summary**: This article presents a semantic tracker which simultaneously tracks a single target and recognises its category. In general, it is hard to design a tracking model suitable for all object categories, e.g., a rigid tracker for a car is not suitable for a deformable gymnast. Category-based trackers usually achieve superior tracking performance for the objects of that specific category, but have difficulties being generalised. Therefore, we propose a novel unified robust tracking framework which explicitly encodes both generic features and category-based features. The tracker consists of a shared convolutional network (NetS), which feeds into two parallel networks, NetC for classification and NetT for tracking. NetS is pre-trained on ImageNet to serve as a generic feature extractor across the different object categories for NetC and NetT. NetC utilises those features within fully connected layers to classify the object category. NetT has multiple branches, corresponding to multiple categories, to distinguish the tracked object from the background. Since each branch in NetT is trained by the videos of a specific category or groups of similar categories, NetT encodes category-based features for tracking. During online tracking, NetC and NetT jointly determine the target regions with the right category and foreground labels for target estimation. To improve the robustness and precision, NetC and NetT inter-supervise each other and trigger network adaptation when their outputs are ambiguous for the same image regions (i.e., when the category label contradicts the foreground/background classification). We have compared the performance of our tracker to other state-of-the-art trackers on a large-scale tracking benchmark (100 sequences)---the obtained results demonstrate the effectiveness of our proposed tracker as it outperformed other 38 state-of-the-art tracking algorithms.



### Deep Outdoor Illumination Estimation
- **Arxiv ID**: http://arxiv.org/abs/1611.06403v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06403v3)
- **Published**: 2016-11-19 17:23:15+00:00
- **Updated**: 2018-04-11 15:47:14+00:00
- **Authors**: Yannick Hold-Geoffroy, Kalyan Sunkavalli, Sunil Hadap, Emiliano Gambaretto, Jean-Fran√ßois Lalonde
- **Comment**: CVPR'17 preprint, 8 pages + 2 pages of citations, 12 figures
- **Journal**: None
- **Summary**: We present a CNN-based technique to estimate high-dynamic range outdoor illumination from a single low dynamic range image. To train the CNN, we leverage a large dataset of outdoor panoramas. We fit a low-dimensional physically-based outdoor illumination model to the skies in these panoramas giving us a compact set of parameters (including sun position, atmospheric conditions, and camera parameters). We extract limited field-of-view images from the panoramas, and train a CNN with this large set of input image--output lighting parameter pairs. Given a test image, this network can be used to infer illumination parameters that can, in turn, be used to reconstruct an outdoor illumination environment map. We demonstrate that our approach allows the recovery of plausible illumination conditions and enables photorealistic virtual object insertion from a single image. An extensive evaluation on both the panorama dataset and captured HDR environment maps shows that our technique significantly outperforms previous solutions to this problem.



### Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.06430v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06430v1)
- **Published**: 2016-11-19 21:02:14+00:00
- **Updated**: 2016-11-19 21:02:14+00:00
- **Authors**: Emily Denton, Sam Gross, Rob Fergus
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a simple semi-supervised learning approach for images based on in-painting using an adversarial loss. Images with random patches removed are presented to a generator whose task is to fill in the hole, based on the surrounding pixels. The in-painted images are then presented to a discriminator network that judges if they are real (unaltered training images) or not. This task acts as a regularizer for standard supervised training of the discriminator. Using our approach we are able to directly train large VGG-style networks in a semi-supervised fashion. We evaluate on STL-10 and PASCAL datasets, where our approach obtains performance comparable or superior to existing methods.



### PsyPhy: A Psychophysics Driven Evaluation Framework for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1611.06448v6
- **DOI**: 10.1109/TPAMI.2018.2849989
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06448v6)
- **Published**: 2016-11-19 23:23:32+00:00
- **Updated**: 2018-07-04 18:39:18+00:00
- **Authors**: Brandon RichardWebster, Samuel E. Anthony, Walter J. Scheirer
- **Comment**: 9 pages, 4 figures. Published at IEEE Transactions on Pattern
  Analysis and Machine Intelligence. For supplemental material see
  http://bjrichardwebster.com/papers/psyphy/supp
- **Journal**: None
- **Summary**: By providing substantial amounts of data and standardized evaluation protocols, datasets in computer vision have helped fuel advances across all areas of visual recognition. But even in light of breakthrough results on recent benchmarks, it is still fair to ask if our recognition algorithms are doing as well as we think they are. The vision sciences at large make use of a very different evaluation regime known as Visual Psychophysics to study visual perception. Psychophysics is the quantitative examination of the relationships between controlled stimuli and the behavioral responses they elicit in experimental test subjects. Instead of using summary statistics to gauge performance, psychophysics directs us to construct item-response curves made up of individual stimulus responses to find perceptual thresholds, thus allowing one to identify the exact point at which a subject can no longer reliably recognize the stimulus class. In this article, we introduce a comprehensive evaluation framework for visual recognition models that is underpinned by this methodology. Over millions of procedurally rendered 3D scenes and 2D images, we compare the performance of well-known convolutional neural networks. Our results bring into question recent claims of human-like performance, and provide a path forward for correcting newly surfaced algorithmic deficiencies.



