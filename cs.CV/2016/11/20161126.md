# Arxiv Papers in cs.CV on 2016-11-26
### Convolutional Experts Constrained Local Model for Facial Landmark Detection
- **Arxiv ID**: http://arxiv.org/abs/1611.08657v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1611.08657v5)
- **Published**: 2016-11-26 04:47:34+00:00
- **Updated**: 2017-07-26 19:46:15+00:00
- **Authors**: Amir Zadeh, Tadas Baltrušaitis, Louis-Philippe Morency
- **Comment**: Accepted at CVPR-W 2017
- **Journal**: None
- **Summary**: Constrained Local Models (CLMs) are a well-established family of methods for facial landmark detection. However, they have recently fallen out of favor to cascaded regression-based approaches. This is in part due to the inability of existing CLM local detectors to model the very complex individual landmark appearance that is affected by expression, illumination, facial hair, makeup, and accessories. In our work, we present a novel local detector -- Convolutional Experts Network (CEN) -- that brings together the advantages of neural architectures and mixtures of experts in an end-to-end framework. We further propose a Convolutional Experts Constrained Local Model (CE-CLM) algorithm that uses CEN as local detectors. We demonstrate that our proposed CE-CLM algorithm outperforms competitive state-of-the-art baselines for facial landmark detection by a large margin on four publicly-available datasets. Our approach is especially accurate and robust on challenging profile images.



### Multi-Task Zero-Shot Action Recognition with Prioritised Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1611.08663v1
- **DOI**: 10.1007/978-3-319-46475-6_22
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08663v1)
- **Published**: 2016-11-26 05:51:11+00:00
- **Updated**: 2016-11-26 05:51:11+00:00
- **Authors**: Xun Xu, Timothy M. Hospedales, Shaogang Gong
- **Comment**: Published in ECCV 2016
- **Journal**: None
- **Summary**: Zero-Shot Learning (ZSL) promises to scale visual recognition by bypassing the conventional model training requirement of annotated examples for every category. This is achieved by establishing a mapping connecting low-level features and a semantic description of the label space, referred as visual-semantic mapping, on auxiliary data. Reusing the learned mapping to project target videos into an embedding space thus allows novel-classes to be recognised by nearest neighbour inference. However, existing ZSL methods suffer from auxiliary-target domain shift intrinsically induced by assuming the same mapping for the disjoint auxiliary and target classes. This compromises the generalisation accuracy of ZSL recognition on the target data. In this work, we improve the ability of ZSL to generalise across this domain shift in both model- and data-centric ways by formulating a visual-semantic mapping with better generalisation properties and a dynamic data re-weighting method to prioritise auxiliary data that are relevant to the target classes. Specifically: (1) We introduce a multi-task visual-semantic mapping to improve generalisation by constraining the semantic mapping parameters to lie on a low-dimensional manifold, (2) We explore prioritised data augmentation by expanding the pool of auxiliary data with additional instances weighted by relevance to the target domain. The proposed new model is applied to the challenging zero-shot action recognition problem to demonstrate its advantages over existing ZSL models.



### Semi-supervised Learning using Denoising Autoencoders for Brain Lesion Detection and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1611.08664v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08664v4)
- **Published**: 2016-11-26 06:19:09+00:00
- **Updated**: 2017-01-10 04:59:00+00:00
- **Authors**: Varghese Alex, Kiran Vaidhya, Subramaniam Thirunavukkarasu, Chandrasekharan Kesavdas, Ganapathy Krishnamurthi
- **Comment**: 11 Pages, 42 Images
- **Journal**: None
- **Summary**: The work presented explores the use of denoising autoencoders (DAE) for brain lesion detection, segmentation and false positive reduction. Stacked denoising autoencoders (SDAE) were pre-trained using a large number of unlabeled patient volumes and fine tuned with patches drawn from a limited number of patients (n=20, 40, 65). The results show negligible loss in performance even when SDAE was fine tuned using 20 patients. Low grade glioma (LGG) segmentation was achieved using a transfer learning approach wherein a network pre-trained with High Grade Glioma (HGG) data was fine tuned using LGG image patches. The weakly supervised SDAE (for HGG) and transfer learning based LGG network were also shown to generalize well and provide good segmentation on unseen BraTS 2013 & BraTS 2015 test data. An unique contribution includes a single layer DAE, referred to as novelty detector(ND). ND was trained to accurately reconstruct non-lesion patches using a mean squared error loss function. The reconstruction error maps of test data were used to identify regions containing lesions. The error maps were shown to assign unique error distributions to various constituents of the glioma, enabling localization. The ND learns the non-lesion brain accurately as it was also shown to provide good segmentation performance on ischemic brain lesions in images from a different database.



### Visual Dialog
- **Arxiv ID**: http://arxiv.org/abs/1611.08669v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1611.08669v5)
- **Published**: 2016-11-26 06:39:28+00:00
- **Updated**: 2017-08-01 22:04:37+00:00
- **Authors**: Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José M. F. Moura, Devi Parikh, Dhruv Batra
- **Comment**: 23 pages, 18 figures, CVPR 2017 camera-ready, results on VisDial v0.9
  dataset, Webpage: http://visualdialog.org
- **Journal**: None
- **Summary**: We introduce the task of Visual Dialog, which requires an AI agent to hold a meaningful dialog with humans in natural, conversational language about visual content. Specifically, given an image, a dialog history, and a question about the image, the agent has to ground the question in image, infer context from history, and answer the question accurately. Visual Dialog is disentangled enough from a specific downstream task so as to serve as a general test of machine intelligence, while being grounded in vision enough to allow objective evaluation of individual responses and benchmark progress. We develop a novel two-person chat data-collection protocol to curate a large-scale Visual Dialog dataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10 question-answer pairs on ~120k images from COCO, with a total of ~1.2M dialog question-answer pairs.   We introduce a family of neural encoder-decoder models for Visual Dialog with 3 encoders -- Late Fusion, Hierarchical Recurrent Encoder and Memory Network -- and 2 decoders (generative and discriminative), which outperform a number of sophisticated baselines. We propose a retrieval-based evaluation protocol for Visual Dialog where the AI agent is asked to sort a set of candidate answers and evaluated on metrics such as mean-reciprocal-rank of human response. We quantify gap between machine and human performance on the Visual Dialog task via human studies. Putting it all together, we demonstrate the first 'visual chatbot'! Our dataset, code, trained models and visual chatbot are available on https://visualdialog.org



### What Can Be Predicted from Six Seconds of Driver Glances?
- **Arxiv ID**: http://arxiv.org/abs/1611.08754v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1611.08754v1)
- **Published**: 2016-11-26 22:41:51+00:00
- **Updated**: 2016-11-26 22:41:51+00:00
- **Authors**: Lex Fridman, Heishiro Toyoda, Sean Seaman, Bobbie Seppelt, Linda Angell, Joonbum Lee, Bruce Mehler, Bryan Reimer
- **Comment**: None
- **Journal**: None
- **Summary**: We consider a large dataset of real-world, on-road driving from a 100-car naturalistic study to explore the predictive power of driver glances and, specifically, to answer the following question: what can be predicted about the state of the driver and the state of the driving environment from a 6-second sequence of macro-glances? The context-based nature of such glances allows for application of supervised learning to the problem of vision-based gaze estimation, making it robust, accurate, and reliable in messy, real-world conditions. So, it's valuable to ask whether such macro-glances can be used to infer behavioral, environmental, and demographic variables? We analyze 27 binary classification problems based on these variables. The takeaway is that glance can be used as part of a multi-sensor real-time system to predict radio-tuning, fatigue state, failure to signal, talking, and several environment variables.



