# Arxiv Papers in cs.CV on 2016-11-28
### Semantic Scene Completion from a Single Depth Image
- **Arxiv ID**: http://arxiv.org/abs/1611.08974v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08974v1)
- **Published**: 2016-11-28 03:38:42+00:00
- **Updated**: 2016-11-28 03:38:42+00:00
- **Authors**: Shuran Song, Fisher Yu, Andy Zeng, Angel X. Chang, Manolis Savva, Thomas Funkhouser
- **Comment**: None
- **Journal**: None
- **Summary**: This paper focuses on semantic scene completion, a task for producing a complete 3D voxel representation of volumetric occupancy and semantic labels for a scene from a single-view depth map observation. Previous work has considered scene completion and semantic labeling of depth maps separately. However, we observe that these two problems are tightly intertwined. To leverage the coupled nature of these two tasks, we introduce the semantic scene completion network (SSCNet), an end-to-end 3D convolutional network that takes a single depth image as input and simultaneously outputs occupancy and semantic labels for all voxels in the camera view frustum. Our network uses a dilation-based 3D context module to efficiently expand the receptive field and enable 3D context learning. To train our network, we construct SUNCG - a manually created large-scale dataset of synthetic 3D scenes with dense volumetric annotations. Our experiments demonstrate that the joint model outperforms methods addressing each task in isolation and outperforms alternative approaches on the semantic scene completion task.



### Range Loss for Deep Face Recognition with Long-tail
- **Arxiv ID**: http://arxiv.org/abs/1611.08976v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08976v1)
- **Published**: 2016-11-28 03:41:46+00:00
- **Updated**: 2016-11-28 03:41:46+00:00
- **Authors**: Xiao Zhang, Zhiyuan Fang, Yandong Wen, Zhifeng Li, Yu Qiao
- **Comment**: 9 pages, 5 figures, Submitted to CVPR, 2017
- **Journal**: None
- **Summary**: Convolutional neural networks have achieved great improvement on face recognition in recent years because of its extraordinary ability in learning discriminative features of people with different identities. To train such a well-designed deep network, tremendous amounts of data is indispensable. Long tail distribution specifically refers to the fact that a small number of generic entities appear frequently while other objects far less existing. Considering the existence of long tail distribution of the real world data, large but uniform distributed data are usually hard to retrieve. Empirical experiences and analysis show that classes with more samples will pose greater impact on the feature learning process and inversely cripple the whole models feature extracting ability on tail part data. Contrary to most of the existing works that alleviate this problem by simply cutting the tailed data for uniform distributions across the classes, this paper proposes a new loss function called range loss to effectively utilize the whole long tailed data in training process. More specifically, range loss is designed to reduce overall intra-personal variations while enlarging inter-personal differences within one mini-batch simultaneously when facing even extremely unbalanced data. The optimization objective of range loss is the $k$ greatest range's harmonic mean values in one class and the shortest inter-class distance within one batch. Extensive experiments on two famous and challenging face recognition benchmarks (Labeled Faces in the Wild (LFW) and YouTube Faces (YTF) not only demonstrate the effectiveness of the proposed approach in overcoming the long tail effect but also show the good generalization ability of the proposed approach.



### Analyzing the group sparsity based on the rank minimization methods
- **Arxiv ID**: http://arxiv.org/abs/1611.08983v12
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08983v12)
- **Published**: 2016-11-28 05:11:52+00:00
- **Updated**: 2017-06-12 12:18:36+00:00
- **Authors**: Zhiyuan Zha, Xin Liu, Xiaohua Huang, Henglin Shi, Yingyue Xu, Qiong Wang, Lan Tang, Xinggan Zhang
- **Comment**: arXiv admin note: text overlap with arXiv:1702.04463
- **Journal**: None
- **Summary**: Sparse coding has achieved a great success in various image processing studies. However, there is not any benchmark to measure the sparsity of image patch/group because sparse discriminant conditions cannot keep unchanged. This paper analyzes the sparsity of group based on the strategy of the rank minimization. Firstly, an adaptive dictionary for each group is designed. Then, we prove that group-based sparse coding is equivalent to the rank minimization problem, and thus the sparse coefficient of each group is measured by estimating the singular values of each group. Based on that measurement, the weighted Schatten $p$-norm minimization (WSNM) has been found to be the closest solution to the real singular values of each group. Thus, WSNM can be equivalently transformed into a non-convex $\ell_p$-norm minimization problem in group-based sparse coding. To make the proposed scheme tractable and robust, the alternating direction method of multipliers (ADMM) is used to solve the $\ell_p$-norm minimization problem. Experimental results on two applications: image inpainting and image compressive sensing (CS) recovery have shown that the proposed scheme outperforms many state-of-the-art methods.



### Improving Fully Convolution Network for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1611.08986v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08986v1)
- **Published**: 2016-11-28 05:31:10+00:00
- **Updated**: 2016-11-28 05:31:10+00:00
- **Authors**: Bing Shuai, Ting Liu, Gang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Fully Convolution Networks (FCN) have achieved great success in dense prediction tasks including semantic segmentation. In this paper, we start from discussing FCN by understanding its architecture limitations in building a strong segmentation network. Next, we present our Improved Fully Convolution Network (IFCN). In contrast to FCN, IFCN introduces a context network that progressively expands the receptive fields of feature maps. In addition, dense skip connections are added so that the context network can be effectively optimized. More importantly, these dense skip connections enable IFCN to fuse rich-scale context to make reliable predictions. Empirically, those architecture modifications are proven to be significant to enhance the segmentation performance. Without engaging any contextual post-processing, IFCN significantly advances the state-of-the-arts on ADE20K (ImageNet scene parsing), Pascal Context, Pascal VOC 2012 and SUN-RGBD segmentation datasets.



### Object Detection Free Instance Segmentation With Labeling Transformations
- **Arxiv ID**: http://arxiv.org/abs/1611.08991v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08991v2)
- **Published**: 2016-11-28 05:52:37+00:00
- **Updated**: 2016-11-29 05:42:11+00:00
- **Authors**: Long Jin, Zeyu Chen, Zhuowen Tu
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Instance segmentation has attracted recent attention in computer vision and existing methods in this domain mostly have an object detection stage. In this paper, we study the intrinsic challenge of the instance segmentation problem, the presence of a quotient space (swapping the labels of different instances leads to the same result), and propose new methods that are object proposal- and object detection- free. We propose three alternative methods, namely pixel-based affinity mapping, superpixel-based affinity learning, and boundary-based component segmentation, all focusing on performing labeling transformations to cope with the quotient space problem. By adopting fully convolutional neural networks (FCN) like models, our framework attains competitive results on both the PASCAL dataset (object-centric) and the Gland dataset (texture-centric), which the existing methods are not able to do. Our work also has the advantages in its transparency, simplicity, and being all segmentation based.



### DeepSetNet: Predicting Sets with Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.08998v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1611.08998v5)
- **Published**: 2016-11-28 06:42:56+00:00
- **Updated**: 2017-08-11 02:52:36+00:00
- **Authors**: S. Hamid Rezatofighi, Vijay Kumar B G, Anton Milan, Ehsan Abbasnejad, Anthony Dick, Ian Reid
- **Comment**: Accepted in IEEE International Conference on Computer Vision (ICCV),
  Venice, 2017, (Spotlight)
- **Journal**: None
- **Summary**: This paper addresses the task of set prediction using deep learning. This is important because the output of many computer vision tasks, including image tagging and object detection, are naturally expressed as sets of entities rather than vectors. As opposed to a vector, the size of a set is not fixed in advance, and it is invariant to the ordering of entities within it. We define a likelihood for a set distribution and learn its parameters using a deep neural network. We also derive a loss for predicting a discrete distribution corresponding to set cardinality. Set prediction is demonstrated on the problem of multi-class image classification. Moreover, we show that the proposed cardinality loss can also trivially be applied to the tasks of object counting and pedestrian detection. Our approach outperforms existing methods in all three cases on standard datasets.



### Hyperspectral CNN Classification with Limited Training Samples
- **Arxiv ID**: http://arxiv.org/abs/1611.09007v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09007v1)
- **Published**: 2016-11-28 07:29:29+00:00
- **Updated**: 2016-11-28 07:29:29+00:00
- **Authors**: Lloyd Windrim, Rishi Ramakrishnan, Arman Melkumyan, Richard Murphy
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Hyperspectral imaging sensors are becoming increasingly popular in robotics applications such as agriculture and mining, and allow per-pixel thematic classification of materials in a scene based on their unique spectral signatures. Recently, convolutional neural networks have shown remarkable performance for classification tasks, but require substantial amounts of labelled training data. This data must sufficiently cover the variability expected to be encountered in the environment. For hyperspectral data, one of the main variations encountered outdoors is due to incident illumination, which can change in spectral shape and intensity depending on the scene geometry. For example, regions occluded from the sun have a lower intensity and their incident irradiance skewed towards shorter wavelengths.   In this work, a data augmentation strategy based on relighting is used during training of a hyperspectral convolutional neural network. It allows training to occur in the outdoor environment given only a small labelled region, which does not need to sufficiently represent the geometric variability of the entire scene. This is important for applications where obtaining large amounts of training data is labourious, hazardous or difficult, such as labelling pixels within shadows. Radiometric normalisation approaches for pre-processing the hyperspectral data are analysed and it is shown that methods based on the raw pixel data are sufficient to be used as input for the classifier. This removes the need for external hardware such as calibration boards, which can restrict the application of hyperspectral sensors in robotics applications. Experiments to evaluate the classification system are carried out on two datasets captured from a field-based platform.



### 3D Human Pose Estimation from a Single Image via Distance Matrix Regression
- **Arxiv ID**: http://arxiv.org/abs/1611.09010v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09010v1)
- **Published**: 2016-11-28 07:36:31+00:00
- **Updated**: 2016-11-28 07:36:31+00:00
- **Authors**: Francesc Moreno-Noguer
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of 3D human pose estimation from a single image. We follow a standard two-step pipeline by first detecting the 2D position of the $N$ body joints, and then using these observations to infer 3D pose. For the first step, we use a recent CNN-based detector. For the second step, most existing approaches perform 2$N$-to-3$N$ regression of the Cartesian joint coordinates. We show that more precise pose estimates can be obtained by representing both the 2D and 3D human poses using $N\times N$ distance matrices, and formulating the problem as a 2D-to-3D distance matrix regression. For learning such a regressor we leverage on simple Neural Network architectures, which by construction, enforce positivity and symmetry of the predicted matrices. The approach has also the advantage to naturally handle missing observations and allowing to hypothesize the position of non-observed joints. Quantitative results on Humaneva and Human3.6M datasets demonstrate consistent performance gains over state-of-the-art. Qualitative evaluation on the images in-the-wild of the LSP dataset, using the regressor learned on Human3.6M, reveals very promising generalization results.



### Awesome Typography: Statistics-Based Text Effects Transfer
- **Arxiv ID**: http://arxiv.org/abs/1611.09026v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09026v2)
- **Published**: 2016-11-28 08:48:28+00:00
- **Updated**: 2016-12-06 04:49:14+00:00
- **Authors**: Shuai Yang, Jiaying Liu, Zhouhui Lian, Zongming Guo
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we explore the problem of generating fantastic special-effects for the typography. It is quite challenging due to the model diversities to illustrate varied text effects for different characters. To address this issue, our key idea is to exploit the analytics on the high regularity of the spatial distribution for text effects to guide the synthesis process. Specifically, we characterize the stylized patches by their normalized positions and the optimal scales to depict their style elements. Our method first estimates these two features and derives their correlation statistically. They are then converted into soft constraints for texture transfer to accomplish adaptive multi-scale texture synthesis and to make style element distribution uniform. It allows our algorithm to produce artistic typography that fits for both local texture patterns and the global spatial distribution in the example. Experimental results demonstrate the superiority of our method for various text effects over conventional style transfer methods. In addition, we validate the effectiveness of our algorithm with extensive artistic typography library generation.



### Deep, Dense, and Low-Rank Gaussian Conditional Random Fields
- **Arxiv ID**: http://arxiv.org/abs/1611.09051v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09051v1)
- **Published**: 2016-11-28 10:29:53+00:00
- **Updated**: 2016-11-28 10:29:53+00:00
- **Authors**: Siddhartha Chandra, Iasonas Kokkinos
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we introduce a fully-connected graph structure in the Deep Gaussian Conditional Random Field (G-CRF) model. For this we express the pairwise interactions between pixels as the inner-products of low-dimensional embeddings, delivered by a new subnetwork of a deep architecture. We efficiently minimize the resulting energy by solving the resulting low-rank linear system with conjugate gradients, and derive an analytic expression for the gradient of our embeddings which allows us to train them end-to-end with backpropagation.   We demonstrate the merit of our approach by achieving state of the art results on three challenging Computer Vision benchmarks, namely semantic segmentation, human parts segmentation, and saliency estimation. Our implementation is fully GPU based, built on top of the Caffe library, and will be made publicly available.



### Bidirectional Multirate Reconstruction for Temporal Modeling in Videos
- **Arxiv ID**: http://arxiv.org/abs/1611.09053v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09053v1)
- **Published**: 2016-11-28 10:32:03+00:00
- **Updated**: 2016-11-28 10:32:03+00:00
- **Authors**: Linchao Zhu, Zhongwen Xu, Yi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the recent success of neural networks in image feature learning, a major problem in the video domain is the lack of sufficient labeled data for learning to model temporal information. In this paper, we propose an unsupervised temporal modeling method that learns from untrimmed videos. The speed of motion varies constantly, e.g., a man may run quickly or slowly. We therefore train a Multirate Visual Recurrent Model (MVRM) by encoding frames of a clip with different intervals. This learning process makes the learned model more capable of dealing with motion speed variance. Given a clip sampled from a video, we use its past and future neighboring clips as the temporal context, and reconstruct the two temporal transitions, i.e., present$\rightarrow$past transition and present$\rightarrow$future transition, reflecting the temporal information in different views. The proposed method exploits the two transitions simultaneously by incorporating a bidirectional reconstruction which consists of a backward reconstruction and a forward reconstruction. We apply the proposed method to two challenging video tasks, i.e., complex event detection and video captioning, in which it achieves state-of-the-art performance. Notably, our method generates the best single feature for event detection with a relative improvement of 10.4% on the MEDTest-13 dataset and achieves the best performance in video captioning across all evaluation metrics on the YouTube2Text dataset.



### Social Scene Understanding: End-to-End Multi-Person Action Localization and Collective Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1611.09078v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09078v1)
- **Published**: 2016-11-28 11:34:20+00:00
- **Updated**: 2016-11-28 11:34:20+00:00
- **Authors**: Timur Bagautdinov, Alexandre Alahi, François Fleuret, Pascal Fua, Silvio Savarese
- **Comment**: None
- **Journal**: None
- **Summary**: We present a unified framework for understanding human social behaviors in raw image sequences. Our model jointly detects multiple individuals, infers their social actions, and estimates the collective actions with a single feed-forward pass through a neural network. We propose a single architecture that does not rely on external detection algorithms but rather is trained end-to-end to generate dense proposal maps that are refined via a novel inference scheme. The temporal consistency is handled via a person-level matching Recurrent Neural Network. The complete model takes as input a sequence of frames and outputs detections along with the estimates of individual actions and collective activities. We demonstrate state-of-the-art performance of our algorithm on multiple publicly available benchmarks.



### On the Role and the Importance of Features for Background Modeling and Foreground Detection
- **Arxiv ID**: http://arxiv.org/abs/1611.09099v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09099v1)
- **Published**: 2016-11-28 12:55:16+00:00
- **Updated**: 2016-11-28 12:55:16+00:00
- **Authors**: Thierry Bouwmans, Caroline Silva, Cristina Marghes, Mohammed Sami Zitouni, Harish Bhaskar, Carl Frelicot
- **Comment**: To be submitted to Computer Science Review
- **Journal**: None
- **Summary**: Background modeling has emerged as a popular foreground detection technique for various applications in video surveillance. Background modeling methods have become increasing efficient in robustly modeling the background and hence detecting moving objects in any visual scene. Although several background subtraction and foreground detection have been proposed recently, no traditional algorithm today still seem to be able to simultaneously address all the key challenges of illumination variation, dynamic camera motion, cluttered background and occlusion. This limitation can be attributed to the lack of systematic investigation concerning the role and importance of features within background modeling and foreground detection. With the availability of a rather large set of invariant features, the challenge is in determining the best combination of features that would improve accuracy and robustness in detection. The purpose of this study is to initiate a rigorous and comprehensive survey of features used within background modeling and foreground detection. Further, this paper presents a systematic experimental and statistical analysis of techniques that provide valuable insight on the trends in background modeling and use it to draw meaningful recommendations for practitioners. In this paper, a preliminary review of the key characteristics of features based on the types and sizes is provided in addition to investigating their intrinsic spectral, spatial and temporal properties. Furthermore, improvements using statistical and fuzzy tools are examined and techniques based on multiple features are benchmarked against reliability and selection criterion. Finally, a description of the different resources available such as datasets and codes is provided.



### Learning Deep Representations Using Convolutional Auto-encoders with Symmetric Skip Connections
- **Arxiv ID**: http://arxiv.org/abs/1611.09119v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09119v2)
- **Published**: 2016-11-28 13:56:20+00:00
- **Updated**: 2017-03-28 14:49:42+00:00
- **Authors**: Jianfeng Dong, Xiao-Jiao Mao, Chunhua Shen, Yu-Bin Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised pre-training was a critical technique for training deep neural networks years ago. With sufficient labeled data and modern training techniques, it is possible to train very deep neural networks from scratch in a purely supervised manner nowadays. However, unlabeled data is easier to obtain and usually of very large scale. How to make use of them better to help supervised learning is still a well-valued topic. In this paper, we investigate convolutional denoising auto-encoders to show that unsupervised pre-training can still improve the performance of high-level image related tasks such as image classification and semantic segmentation. The architecture we use is a convolutional auto-encoder network with symmetric shortcut connections. We empirically show that symmetric shortcut connections are very important for learning abstract representations via image reconstruction. When no extra unlabeled data are available, unsupervised pre-training with our network can regularize the supervised training and therefore lead to better generalization performance. With the help of unsupervised pre-training, our method achieves very competitive results in image classification using very simple all-convolution networks. When labeled data are limited but extra unlabeled data are available, our method achieves good results in several semi-supervised learning tasks.



### Large-Scale Shape Retrieval with Sparse 3D Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.09159v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09159v2)
- **Published**: 2016-11-28 15:09:10+00:00
- **Updated**: 2017-07-14 15:50:22+00:00
- **Authors**: Alexandr Notchenko, Ermek Kapushev, Evgeny Burnaev
- **Comment**: 8 pages, 3 figures, 2 tables, accepted to 3D Deep Learning Workshop
  at NIPS 2016
- **Journal**: None
- **Summary**: In this paper we present results of performance evaluation of S3DCNN - a Sparse 3D Convolutional Neural Network - on a large-scale 3D Shape benchmark ModelNet40, and measure how it is impacted by voxel resolution of input shape. We demonstrate comparable classification and retrieval performance to state-of-the-art models, but with much less computational costs in training and inference phases. We also notice that benefits of higher input resolution can be limited by an ability of a neural network to generalize high level features.



### Who's that Actor? Automatic Labelling of Actors in TV series starting from IMDB Images
- **Arxiv ID**: http://arxiv.org/abs/1611.09162v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09162v1)
- **Published**: 2016-11-28 15:09:26+00:00
- **Updated**: 2016-11-28 15:09:26+00:00
- **Authors**: Rahaf Aljundi, Punarjay Chakravarty, Tinne Tuytelaars
- **Comment**: ACCV 2016 accepted paper
- **Journal**: None
- **Summary**: In this work, we aim at automatically labeling actors in a TV series. Rather than relying on transcripts and subtitles, as has been demonstrated in the past, we show how to achieve this goal starting from a set of example images of each of the main actors involved, collected from the Internet Movie Database (IMDB). The problem then becomes one of domain adaptation: actors' IMDB photos are typically taken at awards ceremonies and are quite different from their appearances in TV series. In each series as well, there is considerable change in actor appearance due to makeup, lighting, ageing, etc. To bridge this gap, we propose a graph-matching based self-labelling algorithm, which we coin HSL (Hungarian Self Labeling). Further, we propose a new edge cost to be used in this context, as well as an extension that is more robust to outliers, where prototypical faces for each of the actors are selected based on a hierarchical clustering procedure. We conduct experiments with 15 episodes from 3 different TV series and demonstrate automatic annotation with an accuracy of 90% and up.



### Image Based Appraisal of Real Estate Properties
- **Arxiv ID**: http://arxiv.org/abs/1611.09180v2
- **DOI**: 10.1109/TMM.2017.2710804
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1611.09180v2)
- **Published**: 2016-11-28 15:23:14+00:00
- **Updated**: 2017-07-27 19:18:27+00:00
- **Authors**: Quanzeng You, Ran Pang, Liangliang Cao, Jiebo Luo
- **Comment**: 8 pages, 8 figures
- **Journal**: None
- **Summary**: Real estate appraisal, which is the process of estimating the price for real estate properties, is crucial for both buys and sellers as the basis for negotiation and transaction. Traditionally, the repeat sales model has been widely adopted to estimate real estate price. However, it depends the design and calculation of a complex economic related index, which is challenging to estimate accurately. Today, real estate brokers provide easy access to detailed online information on real estate properties to their clients. We are interested in estimating the real estate price from these large amounts of easily accessed data. In particular, we analyze the prediction power of online house pictures, which is one of the key factors for online users to make a potential visiting decision. The development of robust computer vision algorithms makes the analysis of visual content possible. In this work, we employ a Recurrent Neural Network (RNN) to predict real estate price using the state-of-the-art visual features. The experimental results indicate that our model outperforms several of other state-of-the-art baseline algorithms in terms of both mean absolute error (MAE) and mean absolute percentage error (MAPE).



### Computational Mapping of the Ground Reflectivity with Laser Scanners
- **Arxiv ID**: http://arxiv.org/abs/1611.09203v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09203v2)
- **Published**: 2016-11-28 15:45:57+00:00
- **Updated**: 2017-03-09 17:34:26+00:00
- **Authors**: Juan Castorena
- **Comment**: Submitted to TIP
- **Journal**: None
- **Summary**: In this investigation we focus on the problem of mapping the ground reflectivity with multiple laser scanners mounted on mobile robots/vehicles. The problem originates because regions of the ground become populated with a varying number of reflectivity measurements whose value depends on the observer and its corresponding perspective. Here, we propose a novel automatic, data-driven computational mapping framework specifically aimed at preserving edge sharpness in the map reconstruction process and that considers the sources of measurement variation. Our new formulation generates map-perspective gradients and applies sub-set selection fusion and de-noising operators to these through iterative algorithms that minimize an $\ell_1$ sparse regularized least squares formulation. Reconstruction of the ground reflectivity is then carried out based on Poisson's formulation posed as an $\ell_2$ term promoting consistency with the fused gradient of map-perspectives and a term that ensures equality constraints with reference measurement data. We demonstrate our new framework outperforms the capabilities of existing ones with experiments realized on Ford's fleet of autonomous vehicles. For example, we show we can achieve map enhancement (i.e., contrast enhancement), artifact removal, de-noising and map-stitching without requiring an additional reflectivity adjustment to calibrate sensors to the specific mounting and robot/vehicle motion.



### ECO: Efficient Convolution Operators for Tracking
- **Arxiv ID**: http://arxiv.org/abs/1611.09224v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09224v2)
- **Published**: 2016-11-28 16:26:27+00:00
- **Updated**: 2017-04-10 18:13:05+00:00
- **Authors**: Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, Michael Felsberg
- **Comment**: Accepted at CVPR 2017. Includes supplementary material
- **Journal**: None
- **Summary**: In recent years, Discriminative Correlation Filter (DCF) based methods have significantly advanced the state-of-the-art in tracking. However, in the pursuit of ever increasing tracking performance, their characteristic speed and real-time capability have gradually faded. Further, the increasingly complex models, with massive number of trainable parameters, have introduced the risk of severe over-fitting. In this work, we tackle the key causes behind the problems of computational complexity and over-fitting, with the aim of simultaneously improving both speed and performance.   We revisit the core DCF formulation and introduce: (i) a factorized convolution operator, which drastically reduces the number of parameters in the model; (ii) a compact generative model of the training sample distribution, that significantly reduces memory and time complexity, while providing better diversity of samples; (iii) a conservative model update strategy with improved robustness and reduced complexity. We perform comprehensive experiments on four benchmarks: VOT2016, UAV123, OTB-2015, and TempleColor. When using expensive deep features, our tracker provides a 20-fold speedup and achieves a 13.0% relative gain in Expected Average Overlap compared to the top ranked method in the VOT2016 challenge. Moreover, our fast variant, using hand-crafted features, operates at 60 Hz on a single CPU, while obtaining 65.0% AUC on OTB-2015.



### Multi-resolution Data Fusion for Super-Resolution Electron Microscopy
- **Arxiv ID**: http://arxiv.org/abs/1612.00874v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00874v1)
- **Published**: 2016-11-28 17:35:10+00:00
- **Updated**: 2016-11-28 17:35:10+00:00
- **Authors**: Suhas Sreehari, S. V. Venkatakrishnan, Katherine L. Bouman, Jeffrey P. Simmons, Lawrence F. Drummy, Charles A. Bouman
- **Comment**: None
- **Journal**: None
- **Summary**: Perhaps surprisingly, the total electron microscopy (EM) data collected to date is less than a cubic millimeter. Consequently, there is an enormous demand in the materials and biological sciences to image at greater speed and lower dosage, while maintaining resolution. Traditional EM imaging based on homogeneous raster-order scanning severely limits the volume of high-resolution data that can be collected, and presents a fundamental limitation to understanding physical processes such as material deformation, crack propagation, and pyrolysis.   We introduce a novel multi-resolution data fusion (MDF) method for super-resolution computational EM. Our method combines innovative data acquisition with novel algorithmic techniques to dramatically improve the resolution/volume/speed trade-off. The key to our approach is to collect the entire sample at low resolution, while simultaneously collecting a small fraction of data at high resolution. The high-resolution measurements are then used to create a material-specific patch-library that is used within the "plug-and-play" framework to dramatically improve super-resolution of the low-resolution data. We present results using FEI electron microscope data that demonstrate super-resolution factors of 4x, 8x, and 16x, while substantially maintaining high image quality and reducing dosage.



### Gaze Embeddings for Zero-Shot Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1611.09309v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09309v2)
- **Published**: 2016-11-28 20:00:16+00:00
- **Updated**: 2017-04-12 18:19:41+00:00
- **Authors**: Nour Karessli, Zeynep Akata, Bernt Schiele, Andreas Bulling
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-shot image classification using auxiliary information, such as attributes describing discriminative object properties, requires time-consuming annotation by domain experts. We instead propose a method that relies on human gaze as auxiliary information, exploiting that even non-expert users have a natural ability to judge class membership. We present a data collection paradigm that involves a discrimination task to increase the information content obtained from gaze data. Our method extracts discriminative descriptors from the data and learns a compatibility function between image and gaze using three novel gaze embeddings: Gaze Histograms (GH), Gaze Features with Grid (GFG) and Gaze Features with Sequence (GFS). We introduce two new gaze-annotated datasets for fine-grained image classification and show that human gaze data is indeed class discriminative, provides a competitive alternative to expert-annotated attributes, and outperforms other baselines for zero-shot image classification.



### Hierarchical Boundary-Aware Neural Encoder for Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/1611.09312v3
- **DOI**: 10.1109/CVPR.2017.339
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09312v3)
- **Published**: 2016-11-28 20:02:28+00:00
- **Updated**: 2017-04-10 13:36:08+00:00
- **Authors**: Lorenzo Baraldi, Costantino Grana, Rita Cucchiara
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: The use of Recurrent Neural Networks for video captioning has recently gained a lot of attention, since they can be used both to encode the input video and to generate the corresponding description. In this paper, we present a recurrent video encoding scheme which can discover and leverage the hierarchical structure of the video. Unlike the classical encoder-decoder approach, in which a video is encoded continuously by a recurrent layer, we propose a novel LSTM cell, which can identify discontinuity points between frames or segments and modify the temporal connections of the encoding layer accordingly. We evaluate our approach on three large-scale datasets: the Montreal Video Annotation dataset, the MPII Movie Description dataset and the Microsoft Video Description Corpus. Experiments show that our approach can discover appropriate hierarchical representations of input videos and improve the state of the art results on movie description datasets.



### What Is Around The Camera?
- **Arxiv ID**: http://arxiv.org/abs/1611.09325v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09325v2)
- **Published**: 2016-11-28 20:27:53+00:00
- **Updated**: 2017-08-01 00:55:25+00:00
- **Authors**: Stamatios Georgoulis, Konstantinos Rematas, Tobias Ritschel, Mario Fritz, Tinne Tuytelaars, Luc Van Gool
- **Comment**: Accepted to ICCV. Project:
  http://homes.esat.kuleuven.be/~sgeorgou/multinatillum/
- **Journal**: None
- **Summary**: How much does a single image reveal about the environment it was taken in? In this paper, we investigate how much of that information can be retrieved from a foreground object, combined with the background (i.e. the visible part of the environment). Assuming it is not perfectly diffuse, the foreground object acts as a complexly shaped and far-from-perfect mirror. An additional challenge is that its appearance confounds the light coming from the environment with the unknown materials it is made of. We propose a learning-based approach to predict the environment from multiple reflectance maps that are computed from approximate surface normals. The proposed method allows us to jointly model the statistics of environments and material properties. We train our system from synthesized training data, but demonstrate its applicability to real-world data. Interestingly, our analysis shows that the information obtained from objects made out of multiple materials often is complementary and leads to better performance.



### The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1611.09326v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09326v3)
- **Published**: 2016-11-28 20:27:54+00:00
- **Updated**: 2017-10-31 13:10:48+00:00
- **Authors**: Simon Jégou, Michal Drozdzal, David Vazquez, Adriana Romero, Yoshua Bengio
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art approaches for semantic image segmentation are built on Convolutional Neural Networks (CNNs). The typical segmentation architecture is composed of (a) a downsampling path responsible for extracting coarse semantic features, followed by (b) an upsampling path trained to recover the input image resolution at the output of the model and, optionally, (c) a post-processing module (e.g. Conditional Random Fields) to refine the model predictions.   Recently, a new CNN architecture, Densely Connected Convolutional Networks (DenseNets), has shown excellent results on image classification tasks. The idea of DenseNets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train.   In this paper, we extend DenseNets to deal with the problem of semantic segmentation. We achieve state-of-the-art results on urban scene benchmark datasets such as CamVid and Gatech, without any further post-processing module nor pretraining. Moreover, due to smart construction of the model, our approach has much less parameters than currently published best entries for these datasets.   Code to reproduce the experiments is available here : https://github.com/SimJeg/FC-DenseNet/blob/master/train.py



### Generating Holistic 3D Scene Abstractions for Text-based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1611.09392v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1611.09392v2)
- **Published**: 2016-11-28 21:29:07+00:00
- **Updated**: 2017-04-11 20:37:18+00:00
- **Authors**: Ang Li, Jin Sun, Joe Yue-Hei Ng, Ruichi Yu, Vlad I. Morariu, Larry S. Davis
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: Spatial relationships between objects provide important information for text-based image retrieval. As users are more likely to describe a scene from a real world perspective, using 3D spatial relationships rather than 2D relationships that assume a particular viewing direction, one of the main challenges is to infer the 3D structure that bridges images with users' text descriptions. However, direct inference of 3D structure from images requires learning from large scale annotated data. Since interactions between objects can be reduced to a limited set of atomic spatial relations in 3D, we study the possibility of inferring 3D structure from a text description rather than an image, applying physical relation models to synthesize holistic 3D abstract object layouts satisfying the spatial constraints present in a textual description. We present a generic framework for retrieving images from a textual description of a scene by matching images with these generated abstract object layouts. Images are ranked by matching object detection outputs (bounding boxes) to 2D layout candidates (also represented by bounding boxes) which are obtained by projecting the 3D scenes with sampled camera directions. We validate our approach using public indoor scene datasets and show that our method outperforms baselines built upon object occurrence histograms and learned 2D pairwise relations.



### Material Recognition from Local Appearance in Global Context
- **Arxiv ID**: http://arxiv.org/abs/1611.09394v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.09394v3)
- **Published**: 2016-11-28 21:36:31+00:00
- **Updated**: 2017-04-12 04:05:10+00:00
- **Authors**: Gabriel Schwartz, Ko Nishino
- **Comment**: None
- **Journal**: None
- **Summary**: Recognition of materials has proven to be a challenging problem due to the wide variation in appearance within and between categories. Global image context, such as where the material is or what object it makes up, can be crucial to recognizing the material. Existing methods, however, operate on an implicit fusion of materials and context by using large receptive fields as input (i.e., large image patches). Many recent material recognition methods treat materials as yet another set of labels like objects. Materials are, however, fundamentally different from objects as they have no inherent shape or defined spatial extent. Approaches that ignore this can only take advantage of limited implicit context as it appears during training. We instead show that recognizing materials purely from their local appearance and integrating separately recognized global contextual cues including objects and places leads to superior dense, per-pixel, material recognition. We achieve this by training a fully-convolutional material recognition network end-to-end with only material category supervision. We integrate object and place estimates to this network from independent CNNs. This approach avoids the necessity of preparing an impractically-large amount of training data to cover the product space of materials, objects, and scenes, while fully leveraging contextual cues for dense material recognition. Furthermore, we perform a detailed analysis of the effects of context granularity, spatial resolution, and the network level at which we introduce context. On a recently introduced comprehensive and diverse material database \cite{Schwartz2016}, we confirm that our method achieves state-of-the-art accuracy with significantly less training data compared to past methods.



### Easy-setup eye movement recording system for human-computer interaction
- **Arxiv ID**: http://arxiv.org/abs/1611.09427v1
- **DOI**: 10.1109/RIVF.2008.4586369
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1611.09427v1)
- **Published**: 2016-11-28 23:35:01+00:00
- **Updated**: 2016-11-28 23:35:01+00:00
- **Authors**: Manh Duong Phung, Quang Vinh Tran, Kenji Hara, Hirohito Inagaki, Masanobu Abe
- **Comment**: In IEEE International Conference on Research, Innovation and Vision
  for the Future (RIVF), 2008
- **Journal**: None
- **Summary**: Tracking the movement of human eyes is expected to yield natural and convenient applications based on human-computer interaction (HCI). To implement an effective eye-tracking system, eye movements must be recorded without placing any restriction on the user's behavior or user discomfort. This paper describes an eye movement recording system that offers free-head, simple configuration. It does not require the user to wear anything on her head, and she can move her head freely. Instead of using a computer, the system uses a visual digital signal processor (DSP) camera to detect the position of eye corner, the center of pupil and then calculate the eye movement. Evaluation tests show that the sampling rate of the system can be 300 Hz and the accuracy is about 1.8 degree/s.



