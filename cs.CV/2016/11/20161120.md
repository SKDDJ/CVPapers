# Arxiv Papers in cs.CV on 2016-11-20
### Fast Video Classification via Adaptive Cascading of Deep Models
- **Arxiv ID**: http://arxiv.org/abs/1611.06453v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1611.06453v2)
- **Published**: 2016-11-20 00:21:32+00:00
- **Updated**: 2017-07-02 02:17:00+00:00
- **Authors**: Haichen Shen, Seungyeop Han, Matthai Philipose, Arvind Krishnamurthy
- **Comment**: Accepted at IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR), 2017
- **Journal**: None
- **Summary**: Recent advances have enabled "oracle" classifiers that can classify across many classes and input distributions with high accuracy without retraining. However, these classifiers are relatively heavyweight, so that applying them to classify video is costly. We show that day-to-day video exhibits highly skewed class distributions over the short term, and that these distributions can be classified by much simpler models. We formulate the problem of detecting the short-term skews online and exploiting models based on it as a new sequential decision making problem dubbed the Online Bandit Problem, and present a new algorithm to solve it. When applied to recognizing faces in TV shows and movies, we realize end-to-end classification speedups of 2.4-7.8x/2.6-11.2x (on GPU/CPU) relative to a state-of-the-art convolutional neural network, at competitive accuracy.



### On The Stability of Video Detection and Tracking
- **Arxiv ID**: http://arxiv.org/abs/1611.06467v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06467v2)
- **Published**: 2016-11-20 03:45:34+00:00
- **Updated**: 2017-04-04 15:00:14+00:00
- **Authors**: Hong Zhang, Naiyan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study an important yet less explored aspect in video detection and tracking -- stability. Surprisingly, there is no prior work that tried to study it. As a result, we start our work by proposing a novel evaluation metric for video detection which considers both stability and accuracy. For accuracy, we extend the existing accuracy metric mean Average Precision (mAP). For stability, we decompose it into three terms: fragment error, center position error, scale and ratio error. Each error represents one aspect of stability. Furthermore, we demonstrate that the stability metric has low correlation with accuracy metric. Thus, it indeed captures a different perspective of quality. Lastly, based on this metric, we evaluate several existing methods for video detection and show how they affect accuracy and stability. We believe our work can provide guidance and solid baselines for future researches in the related areas.



### LCNN: Lookup-based Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1611.06473v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06473v2)
- **Published**: 2016-11-20 05:50:57+00:00
- **Updated**: 2017-06-13 02:43:30+00:00
- **Authors**: Hessam Bagherinezhad, Mohammad Rastegari, Ali Farhadi
- **Comment**: CVPR 17
- **Journal**: None
- **Summary**: Porting state of the art deep learning algorithms to resource constrained compute platforms (e.g. VR, AR, wearables) is extremely challenging. We propose a fast, compact, and accurate model for convolutional neural networks that enables efficient learning and inference. We introduce LCNN, a lookup-based convolutional neural network that encodes convolutions by few lookups to a dictionary that is trained to cover the space of weights in CNNs. Training LCNN involves jointly learning a dictionary and a small set of linear combinations. The size of the dictionary naturally traces a spectrum of trade-offs between efficiency and accuracy. Our experimental results on ImageNet challenge show that LCNN can offer 3.2x speedup while achieving 55.1% top-1 accuracy using AlexNet architecture. Our fastest LCNN offers 37.6x speed up over AlexNet while maintaining 44.3% top-1 accuracy. LCNN not only offers dramatic speed ups at inference, but it also enables efficient training. In this paper, we show the benefits of LCNN in few-shot learning and few-iteration learning, two crucial aspects of on-device training of deep learning models.



### Nazr-CNN: Fine-Grained Classification of UAV Imagery for Damage Assessment
- **Arxiv ID**: http://arxiv.org/abs/1611.06474v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06474v2)
- **Published**: 2016-11-20 05:54:06+00:00
- **Updated**: 2017-08-22 08:27:52+00:00
- **Authors**: N. Attari, F. Ofli, M. Awad, J. Lucas, S. Chawla
- **Comment**: Accepted for publication in the 4th IEEE International Conference on
  Data Science and Advanced Analytics (DSAA) 2017
- **Journal**: None
- **Summary**: We propose Nazr-CNN1, a deep learning pipeline for object detection and fine-grained classification in images acquired from Unmanned Aerial Vehicles (UAVs) for damage assessment and monitoring. Nazr-CNN consists of two components. The function of the first component is to localize objects (e.g. houses or infrastructure) in an image by carrying out a pixel-level classification. In the second component, a hidden layer of a Convolutional Neural Network (CNN) is used to encode Fisher Vectors (FV) of the segments generated from the first component in order to help discriminate between different levels of damage. To showcase our approach we use data from UAVs that were deployed to assess the level of damage in the aftermath of a devastating cyclone that hit the island of Vanuatu in 2015. The collected images were labeled by a crowdsourcing effort and the labeling categories consisted of fine-grained levels of damage to built structures. Since our data set is relatively small, a pre- trained network for pixel-level classification and FV encoding was used. Nazr-CNN attains promising results both for object detection and damage assessment suggesting that the integrated pipeline is robust in the face of small data sets and labeling errors by annotators. While the focus of Nazr-CNN is on assessment of UAV images in a post-disaster scenario, our solution is general and can be applied in many diverse settings. We show one such case of transfer learning to assess the level of damage in aerial images collected after a typhoon in Philippines.



### Recurrent Memory Addressing for describing videos
- **Arxiv ID**: http://arxiv.org/abs/1611.06492v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1611.06492v2)
- **Published**: 2016-11-20 10:07:54+00:00
- **Updated**: 2017-03-23 14:01:20+00:00
- **Authors**: Arnav Kumar Jain, Abhinav Agarwalla, Kumar Krishna Agrawal, Pabitra Mitra
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce Key-Value Memory Networks to a multimodal setting and a novel key-addressing mechanism to deal with sequence-to-sequence models. The proposed model naturally decomposes the problem of video captioning into vision and language segments, dealing with them as key-value pairs. More specifically, we learn a semantic embedding (v) corresponding to each frame (k) in the video, thereby creating (k, v) memory slots. We propose to find the next step attention weights conditioned on the previous attention distributions for the key-value memory slots in the memory addressing schema. Exploiting this flexibility of the framework, we additionally capture spatial dependencies while mapping from the visual to semantic embedding. Experiments done on the Youtube2Text dataset demonstrate usefulness of recurrent key-addressing, while achieving competitive scores on BLEU@4, METEOR metrics against state-of-the-art models.



### Learning Fully Convolutional Networks for Iterative Non-blind Deconvolution
- **Arxiv ID**: http://arxiv.org/abs/1611.06495v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06495v1)
- **Published**: 2016-11-20 10:25:06+00:00
- **Updated**: 2016-11-20 10:25:06+00:00
- **Authors**: Jiawei Zhang, Jinshan Pan, Wei-Sheng Lai, Rynson Lau, Ming-Hsuan Yang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a fully convolutional networks for iterative non-blind deconvolution We decompose the non-blind deconvolution problem into image denoising and image deconvolution. We train a FCNN to remove noises in the gradient domain and use the learned gradients to guide the image deconvolution step. In contrast to the existing deep neural network based methods, we iteratively deconvolve the blurred images in a multi-stage framework. The proposed method is able to learn an adaptive image prior, which keeps both local (details) and global (structures) information. Both quantitative and qualitative evaluations on benchmark datasets demonstrate that the proposed method performs favorably against state-of-the-art algorithms in terms of quality and speed.



### Deep Tensor Convolution on Multicores
- **Arxiv ID**: http://arxiv.org/abs/1611.06565v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1611.06565v3)
- **Published**: 2016-11-20 18:41:48+00:00
- **Updated**: 2017-06-11 15:29:16+00:00
- **Authors**: David Budden, Alexander Matveev, Shibani Santurkar, Shraman Ray Chaudhuri, Nir Shavit
- **Comment**: 11 pages, 4 figures, 1 supplementary doc
- **Journal**: None
- **Summary**: Deep convolutional neural networks (ConvNets) of 3-dimensional kernels allow joint modeling of spatiotemporal features. These networks have improved performance of video and volumetric image analysis, but have been limited in size due to the low memory ceiling of GPU hardware. Existing CPU implementations overcome this constraint but are impractically slow. Here we extend and optimize the faster Winograd-class of convolutional algorithms to the $N$-dimensional case and specifically for CPU hardware. First, we remove the need to manually hand-craft algorithms by exploiting the relaxed constraints and cheap sparse access of CPU memory. Second, we maximize CPU utilization and multicore scalability by transforming data matrices to be cache-aware, integer multiples of AVX vector widths. Treating 2-dimensional ConvNets as a special (and the least beneficial) case of our approach, we demonstrate a 5 to 25-fold improvement in throughput compared to previous state-of-the-art.



### Object Recognition with and without Objects
- **Arxiv ID**: http://arxiv.org/abs/1611.06596v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06596v3)
- **Published**: 2016-11-20 21:20:32+00:00
- **Updated**: 2017-05-25 18:15:06+00:00
- **Authors**: Zhuotun Zhu, Lingxi Xie, Alan L. Yuille
- **Comment**: To Appear in IJCAI 2017
- **Journal**: None
- **Summary**: While recent deep neural networks have achieved a promising performance on object recognition, they rely implicitly on the visual contents of the whole image. In this paper, we train deep neural net- works on the foreground (object) and background (context) regions of images respectively. Consider- ing human recognition in the same situations, net- works trained on the pure background without ob- jects achieves highly reasonable recognition performance that beats humans by a large margin if only given context. However, humans still outperform networks with pure object available, which indicates networks and human beings have different mechanisms in understanding an image. Furthermore, we straightforwardly combine multiple trained networks to explore different visual cues learned by different networks. Experiments show that useful visual hints can be explicitly learned separately and then combined to achieve higher performance, which verifies the advantages of the proposed framework.



### A Hierarchical Approach for Generating Descriptive Image Paragraphs
- **Arxiv ID**: http://arxiv.org/abs/1611.06607v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1611.06607v2)
- **Published**: 2016-11-20 23:10:51+00:00
- **Updated**: 2017-04-10 17:59:15+00:00
- **Authors**: Jonathan Krause, Justin Johnson, Ranjay Krishna, Li Fei-Fei
- **Comment**: CVPR 2017 spotlight
- **Journal**: None
- **Summary**: Recent progress on image captioning has made it possible to generate novel sentences describing images in natural language, but compressing an image into a single sentence can describe visual content in only coarse detail. While one new captioning approach, dense captioning, can potentially describe images in finer levels of detail by captioning many regions within an image, it in turn is unable to produce a coherent story for an image. In this paper we overcome these limitations by generating entire paragraphs for describing images, which can tell detailed, unified stories. We develop a model that decomposes both images and paragraphs into their constituent parts, detecting semantic regions in images and using a hierarchical recurrent neural network to reason about language. Linguistic analysis confirms the complexity of the paragraph generation task, and thorough experiments on a new dataset of image and paragraph pairs demonstrate the effectiveness of our approach.



### RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1611.06612v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06612v3)
- **Published**: 2016-11-20 23:39:52+00:00
- **Updated**: 2016-11-25 02:01:05+00:00
- **Authors**: Guosheng Lin, Anton Milan, Chunhua Shen, Ian Reid
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense classification problems such as semantic segmentation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments and set new state-of-the-art results on seven public datasets. In particular, we achieve an intersection-over-union score of 83.4 on the challenging PASCAL VOC 2012 dataset, which is the best reported result to date.



