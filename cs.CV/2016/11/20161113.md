# Arxiv Papers in cs.CV on 2016-11-13
### Least Squares Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.04076v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.04076v3)
- **Published**: 2016-11-13 03:38:28+00:00
- **Updated**: 2017-04-05 05:44:47+00:00
- **Authors**: Xudong Mao, Qing Li, Haoran Xie, Raymond Y. K. Lau, Zhen Wang, Stephen Paul Smolley
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson $\chi^2$ divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.



### Responses to Critiques on Machine Learning of Criminality Perceptions (Addendum of arXiv:1611.04135)
- **Arxiv ID**: http://arxiv.org/abs/1611.04135v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.04135v3)
- **Published**: 2016-11-13 13:32:11+00:00
- **Updated**: 2017-05-26 07:48:10+00:00
- **Authors**: Xiaolin Wu, Xi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In November 2016 we submitted to arXiv our paper "Automated Inference on Criminality Using Face Images". It generated a great deal of discussions in the Internet and some media outlets. Our work is only intended for pure academic discussions; how it has become a media consumption is a total surprise to us. Although in agreement with our critics on the need and importance of policing AI research for the general good of the society, we are deeply baffled by the ways some of them mispresented our work, in particular the motive and objective of our research.



### Hand Gesture Recognition for Contactless Device Control in Operating Rooms
- **Arxiv ID**: http://arxiv.org/abs/1611.04138v1
- **DOI**: 10.1007/s11548-017-1588-3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.04138v1)
- **Published**: 2016-11-13 14:02:54+00:00
- **Updated**: 2016-11-13 14:02:54+00:00
- **Authors**: Ebrahim Nasr-Esfahani, Nader Karimi, S. M. Reza Soroushmehr, M. Hossein Jafari, M. Amin Khorsandi, Shadrokh Samavi, Kayvan Najarian
- **Comment**: None
- **Journal**: None
- **Summary**: Hand gesture is one of the most important means of touchless communication between human and machines. There is a great interest for commanding electronic equipment in surgery rooms by hand gesture for reducing the time of surgery and the potential for infection. There are challenges in implementation of a hand gesture recognition system. It has to fulfill requirements such as high accuracy and fast response. In this paper we introduce a system of hand gesture recognition based on a deep learning approach. Deep learning is known as an accurate detection model, but its high complexity prevents it from being fabricated as an embedded system. To cope with this problem, we applied some changes in the structure of our work to achieve low complexity. As a result, the proposed method could be implemented on a naive embedded system. Our experiments show that the proposed system results in higher accuracy while having less complexity in comparison with the existing comparable methods.



### Semi-Dense 3D Semantic Mapping from Monocular SLAM
- **Arxiv ID**: http://arxiv.org/abs/1611.04144v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.04144v1)
- **Published**: 2016-11-13 15:31:31+00:00
- **Updated**: 2016-11-13 15:31:31+00:00
- **Authors**: Xuanpeng Li, Rachid Belaroussi
- **Comment**: None
- **Journal**: None
- **Summary**: The bundle of geometry and appearance in computer vision has proven to be a promising solution for robots across a wide variety of applications. Stereo cameras and RGB-D sensors are widely used to realise fast 3D reconstruction and trajectory tracking in a dense way. However, they lack flexibility of seamless switch between different scaled environments, i.e., indoor and outdoor scenes. In addition, semantic information are still hard to acquire in a 3D mapping. We address this challenge by combining the state-of-art deep learning method and semi-dense Simultaneous Localisation and Mapping (SLAM) based on video stream from a monocular camera. In our approach, 2D semantic information are transferred to 3D mapping via correspondence between connective Keyframes with spatial consistency. There is no need to obtain a semantic segmentation for each frame in a sequence, so that it could achieve a reasonable computation time. We evaluate our method on indoor/outdoor datasets and lead to an improvement in the 2D semantic labelling over baseline single frame predictions.



### CAD2RL: Real Single-Image Flight without a Single Real Image
- **Arxiv ID**: http://arxiv.org/abs/1611.04201v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1611.04201v4)
- **Published**: 2016-11-13 23:08:42+00:00
- **Updated**: 2017-06-08 07:21:39+00:00
- **Authors**: Fereshteh Sadeghi, Sergey Levine
- **Comment**: To appear at Robotics: Science and Systems Conference (R:SS), 2017.
  Supplementary video: https://www.youtube.com/watch?v=nXBWmzFrj5s
- **Journal**: None
- **Summary**: Deep reinforcement learning has emerged as a promising and powerful technique for automatically acquiring control policies that can process raw sensory inputs, such as images, and perform complex behaviors. However, extending deep RL to real-world robotic tasks has proven challenging, particularly in safety-critical domains such as autonomous flight, where a trial-and-error learning process is often impractical. In this paper, we explore the following question: can we train vision-based navigation policies entirely in simulation, and then transfer them into the real world to achieve real-world flight without a single real training image? We propose a learning method that we call CAD$^2$RL, which can be used to perform collision-free indoor flight in the real world while being trained entirely on 3D CAD models. Our method uses single RGB images from a monocular camera, without needing to explicitly reconstruct the 3D geometry of the environment or perform explicit motion planning. Our learned collision avoidance policy is represented by a deep convolutional neural network that directly processes raw monocular images and outputs velocity commands. This policy is trained entirely on simulated images, with a Monte Carlo policy evaluation algorithm that directly optimizes the network's ability to produce collision-free flight. By highly randomizing the rendering settings for our simulated training set, we show that we can train a policy that generalizes to the real world, without requiring the simulator to be particularly realistic or high-fidelity. We evaluate our method by flying a real quadrotor through indoor environments, and further evaluate the design choices in our simulator through a series of ablation studies on depth prediction. For supplementary video see: https://youtu.be/nXBWmzFrj5s



