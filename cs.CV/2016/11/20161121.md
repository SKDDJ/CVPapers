# Arxiv Papers in cs.CV on 2016-11-21
### Temporal Generative Adversarial Nets with Singular Value Clipping
- **Arxiv ID**: http://arxiv.org/abs/1611.06624v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1611.06624v3)
- **Published**: 2016-11-21 01:10:50+00:00
- **Updated**: 2017-08-18 02:32:16+00:00
- **Authors**: Masaki Saito, Eiichi Matsumoto, Shunta Saito
- **Comment**: to appear in ICCV 2017
- **Journal**: None
- **Summary**: In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods.



### Not Afraid of the Dark: NIR-VIS Face Recognition via Cross-spectral Hallucination and Low-rank Embedding
- **Arxiv ID**: http://arxiv.org/abs/1611.06638v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06638v1)
- **Published**: 2016-11-21 03:22:23+00:00
- **Updated**: 2016-11-21 03:22:23+00:00
- **Authors**: Jose Lezama, Qiang Qiu, Guillermo Sapiro
- **Comment**: None
- **Journal**: None
- **Summary**: Surveillance cameras today often capture NIR (near infrared) images in low-light environments. However, most face datasets accessible for training and verification are only collected in the VIS (visible light) spectrum. It remains a challenging problem to match NIR to VIS face images due to the different light spectrum. Recently, breakthroughs have been made for VIS face recognition by applying deep learning on a huge amount of labeled VIS face samples. The same deep learning approach cannot be simply applied to NIR face recognition for two main reasons: First, much limited NIR face images are available for training compared to the VIS spectrum. Second, face galleries to be matched are mostly available only in the VIS spectrum. In this paper, we propose an approach to extend the deep learning breakthrough for VIS face recognition to the NIR spectrum, without retraining the underlying deep models that see only VIS faces. Our approach consists of two core components, cross-spectral hallucination and low-rank embedding, to optimize respectively input and output of a VIS deep model for cross-spectral face recognition. Cross-spectral hallucination produces VIS faces from NIR images through a deep learning approach. Low-rank embedding restores a low-rank structure for faces deep features across both NIR and VIS spectrum. We observe that it is often equally effective to perform hallucination to input NIR images or low-rank embedding to output deep features for a VIS deep model for cross-spectral recognition. When hallucination and low-rank embedding are deployed together, we observe significant further improvement; we obtain state-of-the-art accuracy on the CASIA NIR-VIS v2.0 benchmark, without the need at all to re-train the recognition system.



### Phrase Localization and Visual Relationship Detection with Comprehensive Image-Language Cues
- **Arxiv ID**: http://arxiv.org/abs/1611.06641v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06641v4)
- **Published**: 2016-11-21 03:43:22+00:00
- **Updated**: 2017-08-09 00:25:47+00:00
- **Authors**: Bryan A. Plummer, Arun Mallya, Christopher M. Cervantes, Julia Hockenmaier, Svetlana Lazebnik
- **Comment**: IEEE ICCV 2017 accepted paper
- **Journal**: None
- **Summary**: This paper presents a framework for localization or grounding of phrases in images using a large collection of linguistic and visual cues. We model the appearance, size, and position of entity bounding boxes, adjectives that contain attribute information, and spatial relationships between pairs of entities connected by verbs or prepositions. Special attention is given to relationships between people and clothing or body part mentions, as they are useful for distinguishing individuals. We automatically learn weights for combining these cues and at test time, perform joint inference over all phrases in a caption. The resulting system produces state of the art performance on phrase localization on the Flickr30k Entities dataset and visual relationship detection on the Stanford VRD dataset.



### Cascaded Face Alignment via Intimacy Definition Feature
- **Arxiv ID**: http://arxiv.org/abs/1611.06642v2
- **DOI**: 10.1117/1.JEI.26.5.053024
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06642v2)
- **Published**: 2016-11-21 03:53:30+00:00
- **Updated**: 2017-04-12 04:24:41+00:00
- **Authors**: Hailiang Li, Kin-Man Lam, Edmond M. Y. Chiu, Kangheng Wu, Zhibin Lei
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a random-forest based fast cascaded regression model for face alignment, via a novel local feature. Our proposed local lightweight feature, namely intimacy definition feature (IDF), is more discriminative than landmark pose-indexed feature, more efficient than histogram of oriented gradients (HOG) feature and scale-invariant feature transform (SIFT) feature, and more compact than the local binary feature (LBF). Experimental results show that our approach achieves state-of-the-art performance when tested on the most challenging datasets. Compared with an LBF-based algorithm, our method can achieve about two times the speed-up and more than 20% improvement, in terms of alignment accuracy measurement, and save an order of magnitude of memory requirement.



### Self-Supervised Video Representation Learning With Odd-One-Out Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.06646v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06646v4)
- **Published**: 2016-11-21 04:35:45+00:00
- **Updated**: 2017-04-05 05:52:00+00:00
- **Authors**: Basura Fernando, Hakan Bilen, Efstratios Gavves, Stephen Gould
- **Comment**: Accepted in In IEEE International Conference on Computer Vision and
  Pattern Recognition CVPR 2017
- **Journal**: None
- **Summary**: We propose a new self-supervised CNN pre-training technique based on a novel auxiliary task called "odd-one-out learning". In this task, the machine is asked to identify the unrelated or odd element from a set of otherwise related elements. We apply this technique to self-supervised video representation learning where we sample subsequences from videos and ask the network to learn to predict the odd video subsequence. The odd video subsequence is sampled such that it has wrong temporal order of frames while the even ones have the correct temporal order. Therefore, to generate a odd-one-out question no manual annotation is required. Our learning machine is implemented as multi-stream convolutional neural network, which is learned end-to-end. Using odd-one-out networks, we learn temporal representations for videos that generalizes to other related tasks such as action recognition.   On action classification, our method obtains 60.3\% on the UCF101 dataset using only UCF101 data for training which is approximately 10% better than current state-of-the-art self-supervised learning methods. Similarly, on HMDB51 dataset we outperform self-supervised state-of-the art methods by 12.7% on action classification task.



### Deep Learning for the Classification of Lung Nodules
- **Arxiv ID**: http://arxiv.org/abs/1611.06651v2
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1611.06651v2)
- **Published**: 2016-11-21 05:12:44+00:00
- **Updated**: 2016-11-26 21:43:48+00:00
- **Authors**: He Yang, Hengyong Yu, Ge Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning, as a promising new area of machine learning, has attracted a rapidly increasing attention in the field of medical imaging. Compared to the conventional machine learning methods, deep learning requires no hand-tuned feature extractor, and has shown a superior performance in many visual object recognition applications. In this study, we develop a deep convolutional neural network (CNN) and apply it to thoracic CT images for the classification of lung nodules. We present the CNN architecture and classification accuracy for the original images of lung nodules. In order to understand the features of lung nodules, we further construct new datasets, based on the combination of artificial geometric nodules and some transformations of the original images, as well as a stochastic nodule shape model. It is found that simplistic geometric nodules cannot capture the important features of lung nodules.



### ResFeats: Residual Network Based Features for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1611.06656v1
- **DOI**: 10.1109/ICIP.2017.8296551
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06656v1)
- **Published**: 2016-11-21 05:42:13+00:00
- **Updated**: 2016-11-21 05:42:13+00:00
- **Authors**: Ammar Mahmood, Mohammed Bennamoun, Senjian An, Ferdous Sohel
- **Comment**: None
- **Journal**: A. Mahmood, M. Bennamoun, S. An and F. Sohel, "Resfeats: Residual
  network based features for image classification," 2017 IEEE International
  Conference on Image Processing (ICIP), Beijing, 2017, pp. 1597-1601
- **Summary**: Deep residual networks have recently emerged as the state-of-the-art architecture in image segmentation and object detection. In this paper, we propose new image features (called ResFeats) extracted from the last convolutional layer of deep residual networks pre-trained on ImageNet. We propose to use ResFeats for diverse image classification tasks namely, object classification, scene classification and coral classification and show that ResFeats consistently perform better than their CNN counterparts on these classification tasks. Since the ResFeats are large feature vectors, we propose to use PCA for dimensionality reduction. Experimental results are provided to show the effectiveness of ResFeats with state-of-the-art classification accuracies on Caltech-101, Caltech-256 and MLC datasets and a significant performance improvement on MIT-67 dataset compared to the widely used CNN features.



### Gland Instance Segmentation Using Deep Multichannel Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.06661v3
- **DOI**: 10.1109/TBME.2017.2686418
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06661v3)
- **Published**: 2016-11-21 06:13:20+00:00
- **Updated**: 2017-11-23 10:57:50+00:00
- **Authors**: Yan Xu, Yang Li, Yipei Wang, Mingyuan Liu, Yubo Fan, Maode Lai, Eric I-Chao Chang
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1607.04889
- **Journal**: IEEE Transactions on Biomedical Engineering, Volume: 64, Issue:
  12, Dec. 2017, Pages: 2901 - 2912
- **Summary**: Objective: A new image instance segmentation method is proposed to segment individual glands (instances) in colon histology images. This process is challenging since the glands not only need to be segmented from a complex background, they must also be individually identified. Methods: We leverage the idea of image-to-image prediction in recent deep learning by designing an algorithm that automatically exploits and fuses complex multichannel information - regional, location, and boundary cues - in gland histology images. Our proposed algorithm, a deep multichannel framework, alleviates heavy feature design due to the use of convolutional neural networks and is able to meet multifarious requirements by altering channels. Results: Compared with methods reported in the 2015 MICCAI Gland Segmentation Challenge and other currently prevalent instance segmentation methods, we observe state-of-the-art results based on the evaluation metrics. Conclusion: The proposed deep multichannel algorithm is an effective method for gland instance segmentation. Significance: The generalization ability of our model not only enable the algorithm to solve gland instance segmentation problems, but the channel is also alternative that can be replaced for a specific task.



### Estimation of respiratory pattern from video using selective ensemble aggregation
- **Arxiv ID**: http://arxiv.org/abs/1611.06674v1
- **DOI**: 10.1109/TSP.2017.2664048
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06674v1)
- **Published**: 2016-11-21 08:02:50+00:00
- **Updated**: 2016-11-21 08:02:50+00:00
- **Authors**: A. P. Prathosh, Pragathi Praveena, Lalit K. Mestha, Sanjay Bharadwaj
- **Comment**: None
- **Journal**: None
- **Summary**: Non-contact estimation of respiratory pattern (RP) and respiration rate (RR) has multiple applications. Existing methods for RP and RR measurement fall into one of the three categories - (i) estimation through nasal air flow measurement, (ii) estimation from video-based remote photoplethysmography, and (iii) estimation by measurement of motion induced by respiration using motion detectors. These methods, however, require specialized sensors, are computationally expensive and/or critically depend on selection of a region of interest (ROI) for processing. In this paper a general framework is described for estimating a periodic signal driving noisy LTI channels connected in parallel with unknown dynamics. The method is then applied to derive a computationally inexpensive method for estimating RP using 2D cameras that does not critically depend on ROI. Specifically, RP is estimated by imaging the changes in the reflected light caused by respiration-induced motion. Each spatial location in the field of view of the camera is modeled as a noise-corrupted linear time-invariant (LTI) measurement channel with unknown system dynamics, driven by a single generating respiratory signal. Estimation of RP is cast as a blind deconvolution problem and is solved through a method comprising subspace projection and statistical aggregation. Experiments are carried out on 31 healthy human subjects by generating multiple RPs and comparing the proposed estimates with simultaneously acquired ground truth from an impedance pneumograph device. The proposed estimator agrees well with the ground truth device in terms of correlation measures, despite variability in clothing pattern, angle of view and ROI.



### Deep Temporal Linear Encoding Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.06678v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06678v1)
- **Published**: 2016-11-21 08:27:31+00:00
- **Updated**: 2016-11-21 08:27:31+00:00
- **Authors**: Ali Diba, Vivek Sharma, Luc Van Gool
- **Comment**: Ali Diba and Vivek Sharma contributed equally to this work and listed
  in alphabetical order
- **Journal**: None
- **Summary**: The CNN-encoding of features from entire videos for the representation of human actions has rarely been addressed. Instead, CNN work has focused on approaches to fuse spatial and temporal networks, but these were typically limited to processing shorter sequences. We present a new video representation, called temporal linear encoding (TLE) and embedded inside of CNNs as a new layer, which captures the appearance and motion throughout entire videos. It encodes this aggregated information into a robust video feature representation, via end-to-end learning. Advantages of TLEs are: (a) they encode the entire video into a compact feature representation, learning the semantics and a discriminative feature space; (b) they are applicable to all kinds of networks like 2D and 3D CNNs for video classification; and (c) they model feature interactions in a more expressive way and without loss of information. We conduct experiments on two challenging human action datasets: HMDB51 and UCF101. The experiments show that TLE outperforms current state-of-the-art methods on both datasets.



### Covariate conscious approach for Gait recognition based upon Zernike moment invariants
- **Arxiv ID**: http://arxiv.org/abs/1611.06683v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06683v1)
- **Published**: 2016-11-21 08:48:47+00:00
- **Updated**: 2016-11-21 08:48:47+00:00
- **Authors**: Himanshu Aggarwal, Dinesh K. Vishwakarma
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Gait recognition i.e. identification of an individual from his/her walking pattern is an emerging field. While existing gait recognition techniques perform satisfactorily in normal walking conditions, there performance tend to suffer drastically with variations in clothing and carrying conditions. In this work, we propose a novel covariate cognizant framework to deal with the presence of such covariates. We describe gait motion by forming a single 2D spatio-temporal template from video sequence, called Average Energy Silhouette image (AESI). Zernike moment invariants (ZMIs) are then computed to screen the parts of AESI infected with covariates. Following this, features are extracted from Spatial Distribution of Oriented Gradients (SDOGs) and novel Mean of Directional Pixels (MDPs) methods. The obtained features are fused together to form the final well-endowed feature set. Experimental evaluation of the proposed framework on three publicly available datasets i.e. CASIA dataset B, OU-ISIR Treadmill dataset B and USF Human-ID challenge dataset with recently published gait recognition approaches, prove its superior performance.



### Multi-Modality Fusion based on Consensus-Voting and 3D Convolution for Isolated Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/1611.06689v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06689v2)
- **Published**: 2016-11-21 09:16:21+00:00
- **Updated**: 2016-11-28 08:16:27+00:00
- **Authors**: Jiali Duan, Shuai Zhou, Jun Wan, Xiaoyuan Guo, Stan Z. Li
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, the popularity of depth-sensors such as Kinect has made depth videos easily available while its advantages have not been fully exploited. This paper investigates, for gesture recognition, to explore the spatial and temporal information complementarily embedded in RGB and depth sequences. We propose a convolutional twostream consensus voting network (2SCVN) which explicitly models both the short-term and long-term structure of the RGB sequences. To alleviate distractions from background, a 3d depth-saliency ConvNet stream (3DDSN) is aggregated in parallel to identify subtle motion characteristics. These two components in an unified framework significantly improve the recognition accuracy. On the challenging Chalearn IsoGD benchmark, our proposed method outperforms the first place on the leader-board by a large margin (10.29%) while also achieving the best result on RGBD-HuDaAct dataset (96.74%). Both quantitative experiments and qualitative analysis shows the effectiveness of our proposed framework and codes will be released to facilitate future research.



### Training Sparse Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.06694v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1611.06694v1)
- **Published**: 2016-11-21 09:24:24+00:00
- **Updated**: 2016-11-21 09:24:24+00:00
- **Authors**: Suraj Srinivas, Akshayvarun Subramanya, R. Venkatesh Babu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks with lots of parameters are typically used for large-scale computer vision tasks such as image classification. This is a result of using dense matrix multiplications and convolutions. However, sparse computations are known to be much more efficient. In this work, we train and build neural networks which implicitly use sparse computations. We introduce additional gate variables to perform parameter selection and show that this is equivalent to using a spike-and-slab prior. We experimentally validate our method on both small and large networks and achieve state-of-the-art compression results for sparse neural network models.



### Crowd Counting by Adapting Convolutional Neural Networks with Side Information
- **Arxiv ID**: http://arxiv.org/abs/1611.06748v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06748v1)
- **Published**: 2016-11-21 12:09:06+00:00
- **Updated**: 2016-11-21 12:09:06+00:00
- **Authors**: Di Kang, Debarun Dhar, Antoni B. Chan
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Computer vision tasks often have side information available that is helpful to solve the task. For example, for crowd counting, the camera perspective (e.g., camera angle and height) gives a clue about the appearance and scale of people in the scene. While side information has been shown to be useful for counting systems using traditional hand-crafted features, it has not been fully utilized in counting systems based on deep learning. In order to incorporate the available side information, we propose an adaptive convolutional neural network (ACNN), where the convolutional filter weights adapt to the current scene context via the side information. In particular, we model the filter weights as a low-dimensional manifold, parametrized by the side information, within the high-dimensional space of filter weights. With the help of side information and adaptive weights, the ACNN can disentangle the variations related to the side information, and extract discriminative features related to the current context. Since existing crowd counting datasets do not contain ground-truth side information, we collect a new dataset with the ground-truth camera angle and height as the side information. On experiments in crowd counting, the ACNN improves counting accuracy compared to a plain CNN with a similar number of parameters. We also apply ACNN to image deconvolution to show its potential effectiveness on other computer vision applications.



### Non-Local Color Image Denoising with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.06757v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1611.06757v2)
- **Published**: 2016-11-21 12:36:10+00:00
- **Updated**: 2017-07-10 20:06:26+00:00
- **Authors**: Stamatios Lefkimmiatis
- **Comment**: 15 pages, accepted to CVPR 2017
- **Journal**: None
- **Summary**: We propose a novel deep network architecture for grayscale and color image denoising that is based on a non-local image model. Our motivation for the overall design of the proposed network stems from variational methods that exploit the inherent non-local self-similarity property of natural images. We build on this concept and introduce deep networks that perform non-local processing and at the same time they significantly benefit from discriminative learning. Experiments on the Berkeley segmentation dataset, comparing several state-of-the-art methods, show that the proposed non-local models achieve the best reported denoising performance both for grayscale and color images for all the tested noise levels. It is also worth noting that this increase in performance comes at no extra cost on the capacity of the network compared to existing alternative deep network architectures. In addition, we highlight a direct link of the proposed non-local models to convolutional neural networks. This connection is of significant importance since it allows our models to take full advantage of the latest advances on GPU computing in deep learning and makes them amenable to efficient implementations through their inherent parallelism.



### Efficient Convolutional Neural Network with Binary Quantization Layer
- **Arxiv ID**: http://arxiv.org/abs/1611.06764v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06764v1)
- **Published**: 2016-11-21 12:55:30+00:00
- **Updated**: 2016-11-21 12:55:30+00:00
- **Authors**: Mahdyar Ravanbakhsh, Hossein Mousavi, Moin Nabi, Lucio Marcenaro, Carlo Regazzoni
- **Comment**: Workshop on Efficient Methods for Deep Neural Networks (EMDNN), NIPS
  2016, Barcelona, Spain. arXiv admin note: substantial text overlap with
  arXiv:1609.09220
- **Journal**: None
- **Summary**: In this paper we introduce a novel method for segmentation that can benefit from general semantics of Convolutional Neural Network (CNN). Our segmentation proposes visually and semantically coherent image segments. We use binary encoding of CNN features to overcome the difficulty of the clustering on the high-dimensional CNN feature space. These binary encoding can be embedded into the CNN as an extra layer at the end of the network. This results in real-time segmentation. To the best of our knowledge our method is the first attempt on general semantic image segmentation using CNN. All the previous papers were limited to few number of category of the images (e.g. PASCAL VOC). Experiments show that our segmentation algorithm outperform the state-of-the-art non-semantic segmentation methods by a large margin.



### Effective Deterministic Initialization for $k$-Means-Like Methods via Local Density Peaks Searching
- **Arxiv ID**: http://arxiv.org/abs/1611.06777v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1611.06777v1)
- **Published**: 2016-11-21 13:26:37+00:00
- **Updated**: 2016-11-21 13:26:37+00:00
- **Authors**: Fengfu Li, Hong Qiao, Bo Zhang
- **Comment**: 16 pages, 9 figures, journal paper
- **Journal**: None
- **Summary**: The $k$-means clustering algorithm is popular but has the following main drawbacks: 1) the number of clusters, $k$, needs to be provided by the user in advance, 2) it can easily reach local minima with randomly selected initial centers, 3) it is sensitive to outliers, and 4) it can only deal with well separated hyperspherical clusters. In this paper, we propose a Local Density Peaks Searching (LDPS) initialization framework to address these issues. The LDPS framework includes two basic components: one of them is the local density that characterizes the density distribution of a data set, and the other is the local distinctiveness index (LDI) which we introduce to characterize how distinctive a data point is compared with its neighbors. Based on these two components, we search for the local density peaks which are characterized with high local densities and high LDIs to deal with 1) and 2). Moreover, we detect outliers characterized with low local densities but high LDIs, and exclude them out before clustering begins. Finally, we apply the LDPS initialization framework to $k$-medoids, which is a variant of $k$-means and chooses data samples as centers, with diverse similarity measures other than the Euclidean distance to fix the last drawback of $k$-means. Combining the LDPS initialization framework with $k$-means and $k$-medoids, we obtain two novel clustering methods called LDPS-means and LDPS-medoids, respectively. Experiments on synthetic data sets verify the effectiveness of the proposed methods, especially when the ground truth of the cluster number $k$ is large. Further, experiments on several real world data sets, Handwritten Pendigits, Coil-20, Coil-100 and Olivetti Face Database, illustrate that our methods give a superior performance than the analogous approaches on both estimating $k$ and unsupervised object categorization.



### TextBoxes: A Fast Text Detector with a Single Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1611.06779v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06779v1)
- **Published**: 2016-11-21 13:35:15+00:00
- **Updated**: 2016-11-21 13:35:15+00:00
- **Authors**: Minghui Liao, Baoguang Shi, Xiang Bai, Xinggang Wang, Wenyu Liu
- **Comment**: Accepted by AAAI2017
- **Journal**: None
- **Summary**: This paper presents an end-to-end trainable fast scene text detector, named TextBoxes, which detects scene text with both high accuracy and efficiency in a single network forward pass, involving no post-process except for a standard non-maximum suppression. TextBoxes outperforms competing methods in terms of text localization accuracy and is much faster, taking only 0.09s per image in a fast implementation. Furthermore, combined with a text recognizer, TextBoxes significantly outperforms state-of-the-art approaches on word spotting and end-to-end text recognition tasks.



### Generalized Dropout
- **Arxiv ID**: http://arxiv.org/abs/1611.06791v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1611.06791v1)
- **Published**: 2016-11-21 14:06:48+00:00
- **Updated**: 2016-11-21 14:06:48+00:00
- **Authors**: Suraj Srinivas, R. Venkatesh Babu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks often require good regularizers to generalize well. Dropout is one such regularizer that is widely used among Deep Learning practitioners. Recent work has shown that Dropout can also be viewed as performing Approximate Bayesian Inference over the network parameters. In this work, we generalize this notion and introduce a rich family of regularizers which we call Generalized Dropout. One set of methods in this family, called Dropout++, is a version of Dropout with trainable parameters. Classical Dropout emerges as a special case of this method. Another member of this family selects the width of neural network layers. Experiments show that these methods help in improving generalization performance over Dropout.



### SANet: Structure-Aware Network for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1611.06878v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06878v3)
- **Published**: 2016-11-21 16:19:30+00:00
- **Updated**: 2017-05-01 17:43:07+00:00
- **Authors**: Heng Fan, Haibin Ling
- **Comment**: In CVPR Deep Vision Workshop, 2017
- **Journal**: None
- **Summary**: Convolutional neural network (CNN) has drawn increasing interest in visual tracking owing to its powerfulness in feature extraction. Most existing CNN-based trackers treat tracking as a classification problem. However, these trackers are sensitive to similar distractors because their CNN models mainly focus on inter-class classification. To address this problem, we use self-structure information of object to distinguish it from distractors. Specifically, we utilize recurrent neural network (RNN) to model object structure, and incorporate it into CNN to improve its robustness to similar distractors. Considering that convolutional layers in different levels characterize the object from different perspectives, we use multiple RNNs to model object structure in different levels respectively. Extensive experiments on three benchmarks, OTB100, TC-128 and VOT2015, show that the proposed algorithm outperforms other methods. Code is released at http://www.dabi.temple.edu/~hbling/code/SANet/SANet.html.



### The subset-matched Jaccard index for evaluation of Segmentation for Plant Images
- **Arxiv ID**: http://arxiv.org/abs/1611.06880v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06880v1)
- **Published**: 2016-11-21 16:21:20+00:00
- **Updated**: 2016-11-21 16:21:20+00:00
- **Authors**: Jonathan Bell, Hannah M. Dee
- **Comment**: None
- **Journal**: None
- **Summary**: We describe a new measure for the evaluation of region level segmentation of objects, as applied to evaluating the accuracy of leaf-level segmentation of plant images. The proposed approach enforces the rule that a region (e.g. a leaf) in either the image being evaluated or the ground truth image evaluated against can be mapped to no more than one region in the other image. We call this measure the subset-matched Jaccard index.



### Multi-Scale Anisotropic Fourth-Order Diffusion Improves Ridge and Valley Localization
- **Arxiv ID**: http://arxiv.org/abs/1611.06906v2
- **DOI**: 10.1007/s10851-017-0729-1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06906v2)
- **Published**: 2016-11-21 17:08:44+00:00
- **Updated**: 2017-04-18 10:31:11+00:00
- **Authors**: Shekoufeh Gorgi Zadeh, Stephan Didas, Maximilian W. M. Wintergerst, Thomas Schultz
- **Comment**: 12 pages, 8 figures, 1 table
- **Journal**: Journal of Mathematical Imaging and Vision, 1-13, 2017
- **Summary**: Ridge and valley enhancing filters are widely used in applications such as vessel detection in medical image computing. When images are degraded by noise or include vessels at different scales, such filters are an essential step for meaningful and stable vessel localization. In this work, we propose a novel multi-scale anisotropic fourth-order diffusion equation that allows us to smooth along vessels, while sharpening them in the orthogonal direction. The proposed filter uses a fourth order diffusion tensor whose eigentensors and eigenvalues are determined from the local Hessian matrix, at a scale that is automatically selected for each pixel. We discuss efficient implementation using a Fast Explicit Diffusion scheme and demonstrate results on synthetic images and vessels in fundus images. Compared to previous isotropic and anisotropic fourth-order filters, as well as established second-order vessel enhancing filters, our newly proposed one better restores the centerlines in all cases.



### Predicting 1p19q Chromosomal Deletion of Low-Grade Gliomas from MR Images using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1611.06939v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06939v1)
- **Published**: 2016-11-21 18:43:20+00:00
- **Updated**: 2016-11-21 18:43:20+00:00
- **Authors**: Zeynettin Akkus, Issa Ali, Jiri Sedlar, Timothy L. Kline, Jay P. Agrawal, Ian F. Parney, Caterina Giannini, Bradley J. Erickson
- **Comment**: This work has been presented in Conference on Machine Intelligence in
  Medical Imaging 2016 and RSNA 2016
- **Journal**: None
- **Summary**: Objective: Several studies have associated codeletion of chromosome arms 1p/19q in low-grade gliomas (LGG) with positive response to treatment and longer progression free survival. Therefore, predicting 1p/19q status is crucial for effective treatment planning of LGG. In this study, we predict the 1p/19q status from MR images using convolutional neural networks (CNN), which could be a noninvasive alternative to surgical biopsy and histopathological analysis. Method: Our method consists of three main steps: image registration, tumor segmentation, and classification of 1p/19q status using CNN. We included a total of 159 LGG with 3 image slices each who had biopsy-proven 1p/19q status (57 nondeleted and 102 codeleted) and preoperative postcontrast-T1 (T1C) and T2 images. We divided our data into training, validation, and test sets. The training data was balanced for equal class probability and then augmented with iterations of random translational shift, rotation, and horizontal and vertical flips to increase the size of the training set. We shuffled and augmented the training data to counter overfitting in each epoch. Finally, we evaluated several configurations of a multi-scale CNN architecture until training and validation accuracies became consistent. Results: The results of the best performing configuration on the unseen test set were 93.3% (sensitivity), 82.22% (specificity), and 87.7% (accuracy). Conclusion: Multi-scale CNN with their self-learning capability provides promising results for predicting 1p/19q status noninvasively based on T1C and T2 images. Significance: Predicting 1p/19q status noninvasively from MR images would allow selecting effective treatment strategies for LGG patients without the need for surgical biopsy.



### Dense Captioning with Joint Inference and Visual Context
- **Arxiv ID**: http://arxiv.org/abs/1611.06949v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06949v2)
- **Published**: 2016-11-21 18:58:58+00:00
- **Updated**: 2017-08-07 23:17:34+00:00
- **Authors**: Linjie Yang, Kevin Tang, Jianchao Yang, Li-Jia Li
- **Comment**: None
- **Journal**: None
- **Summary**: Dense captioning is a newly emerging computer vision topic for understanding images with dense language descriptions. The goal is to densely detect visual concepts (e.g., objects, object parts, and interactions between them) from images, labeling each with a short descriptive phrase. We identify two key challenges of dense captioning that need to be properly addressed when tackling the problem. First, dense visual concept annotations in each image are associated with highly overlapping target regions, making accurate localization of each visual concept challenging. Second, the large amount of visual concepts makes it hard to recognize each of them by appearance alone. We propose a new model pipeline based on two novel ideas, joint inference and context fusion, to alleviate these two challenges. We design our model architecture in a methodical manner and thoroughly evaluate the variations in architecture. Our final model, compact and efficient, achieves state-of-the-art accuracy on Visual Genome for dense captioning with a relative gain of 73\% compared to the previous best algorithm. Qualitative experiments also reveal the semantic capabilities of our model in dense captioning.



### Statistical Learning for OCR Text Correction
- **Arxiv ID**: http://arxiv.org/abs/1611.06950v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1611.06950v1)
- **Published**: 2016-11-21 19:00:32+00:00
- **Updated**: 2016-11-21 19:00:32+00:00
- **Authors**: Jie Mei, Aminul Islam, Yajing Wu, Abidalrahman Moh'd, Evangelos E. Milios
- **Comment**: None
- **Journal**: None
- **Summary**: The accuracy of Optical Character Recognition (OCR) is crucial to the success of subsequent applications used in text analyzing pipeline. Recent models of OCR post-processing significantly improve the quality of OCR-generated text, but are still prone to suggest correction candidates from limited observations while insufficiently accounting for the characteristics of OCR errors. In this paper, we show how to enlarge candidate suggestion space by using external corpus and integrating OCR-specific features in a regression approach to correct OCR-generated errors. The evaluation results show that our model can correct 61.5% of the OCR-errors (considering the top 1 suggestion) and 71.5% of the OCR-errors (considering the top 3 suggestions), for cases where the theoretical correction upper-bound is 78%.



### Sampled Image Tagging and Retrieval Methods on User Generated Content
- **Arxiv ID**: http://arxiv.org/abs/1611.06962v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06962v3)
- **Published**: 2016-11-21 19:24:58+00:00
- **Updated**: 2016-12-02 20:52:40+00:00
- **Authors**: Karl Ni, Kyle Zaragoza, Charles Foster, Carmen Carrano, Barry Chen, Yonas Tesfaye, Alex Gude
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional image tagging and retrieval algorithms have limited value as a result of being trained with heavily curated datasets. These limitations are most evident when arbitrary search words are used that do not intersect with training set labels. Weak labels from user generated content (UGC) found in the wild (e.g., Google Photos, FlickR, etc.) have an almost unlimited number of unique words in the metadata tags. Prior work on word embeddings successfully leveraged unstructured text with large vocabularies, and our proposed method seeks to apply similar cost functions to open source imagery. Specifically, we train a deep learning image tagging and retrieval system on large scale, user generated content (UGC) using sampling methods and joint optimization of word embeddings. By using the Yahoo! FlickR Creative Commons (YFCC100M) dataset, such an approach builds robustness to common unstructured data issues that include but are not limited to irrelevant tags, misspellings, multiple languages, polysemy, and tag imbalance. As a result, the final proposed algorithm will not only yield comparable results to state of the art in conventional image tagging, but will enable new capability to train algorithms on large, scale unstructured text in the YFCC100M dataset and outperform cited work in zero-shot capability.



### Kernel Cross-View Collaborative Representation based Classification for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1611.06969v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06969v1)
- **Published**: 2016-11-21 19:44:50+00:00
- **Updated**: 2016-11-21 19:44:50+00:00
- **Authors**: Raphael Prates, William Robson Schwartz
- **Comment**: Paper submitted to CVPR 2017 conference
- **Journal**: None
- **Summary**: Person re-identification aims at the maintenance of a global identity as a person moves among non-overlapping surveillance cameras. It is a hard task due to different illumination conditions, viewpoints and the small number of annotated individuals from each pair of cameras (small-sample-size problem). Collaborative Representation based Classification (CRC) has been employed successfully to address the small-sample-size problem in computer vision. However, the original CRC formulation is not well-suited for person re-identification since it does not consider that probe and gallery samples are from different cameras. Furthermore, it is a linear model, while appearance changes caused by different camera conditions indicate a strong nonlinear transition between cameras. To overcome such limitations, we propose the Kernel Cross-View Collaborative Representation based Classification (Kernel X-CRC) that represents probe and gallery images by balancing representativeness and similarity nonlinearly. It assumes that a probe and its corresponding gallery image are represented with similar coding vectors using individuals from the training set. Experimental results demonstrate that our assumption is true when using a high-dimensional feature vector and becomes more compelling when dealing with a low-dimensional and discriminative representation computed using a common subspace learning method. We achieve state-of-the-art for rank-1 matching rates in two person re-identification datasets (PRID450S and GRID) and the second best results on VIPeR and CUHK01 datasets.



### RhoanaNet Pipeline: Dense Automatic Neural Annotation
- **Arxiv ID**: http://arxiv.org/abs/1611.06973v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1611.06973v1)
- **Published**: 2016-11-21 19:48:29+00:00
- **Updated**: 2016-11-21 19:48:29+00:00
- **Authors**: Seymour Knowles-Barley, Verena Kaynig, Thouis Ray Jones, Alyssa Wilson, Joshua Morgan, Dongil Lee, Daniel Berger, Narayanan Kasthuri, Jeff W. Lichtman, Hanspeter Pfister
- **Comment**: 13 pages, 4 figures
- **Journal**: None
- **Summary**: Reconstructing a synaptic wiring diagram, or connectome, from electron microscopy (EM) images of brain tissue currently requires many hours of manual annotation or proofreading (Kasthuri and Lichtman, 2010; Lichtman and Sanes, 2008; Seung, 2009). The desire to reconstruct ever larger and more complex networks has pushed the collection of ever larger EM datasets. A cubic millimeter of raw imaging data would take up 1 PB of storage and present an annotation project that would be impractical without relying heavily on automatic segmentation methods. The RhoanaNet image processing pipeline was developed to automatically segment large volumes of EM data and ease the burden of manual proofreading and annotation. Based on (Kaynig et al., 2015), we updated every stage of the software pipeline to provide better throughput performance and higher quality segmentation results. We used state of the art deep learning techniques to generate improved membrane probability maps, and Gala (Nunez-Iglesias et al., 2014) was used to agglomerate 2D segments into 3D objects.   We applied the RhoanaNet pipeline to four densely annotated EM datasets, two from mouse cortex, one from cerebellum and one from mouse lateral geniculate nucleus (LGN). All training and test data is made available for benchmark comparisons. The best segmentation results obtained gave $V^\text{Info}_\text{F-score}$ scores of 0.9054 and 09182 for the cortex datasets, 0.9438 for LGN, and 0.9150 for Cerebellum.   The RhoanaNet pipeline is open source software. All source code, training data, test data, and annotations for all four benchmark datasets are available at www.rhoana.org.



### Multiple-View Spectral Clustering for Group-wise Functional Community Detection
- **Arxiv ID**: http://arxiv.org/abs/1611.06981v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1611.06981v1)
- **Published**: 2016-11-21 19:57:48+00:00
- **Updated**: 2016-11-21 19:57:48+00:00
- **Authors**: Nathan D. Cahill, Harmeet Singh, Chao Zhang, Daryl A. Corcoran, Alison M. Prengaman, Paul S. Wenger, John F. Hamilton, Peter Bajorski, Andrew M. Michael
- **Comment**: Presented at The MICCAI-BACON 16 Workshop
  (https://arxiv.org/abs/1611.03363)
- **Journal**: None
- **Summary**: Functional connectivity analysis yields powerful insights into our understanding of the human brain. Group-wise functional community detection aims to partition the brain into clusters, or communities, in which functional activity is inter-regionally correlated in a common manner across a group of subjects. In this article, we show how to use multiple-view spectral clustering to perform group-wise functional community detection. In a series of experiments on 291 subjects from the Human Connectome Project, we compare three versions of multiple-view spectral clustering: MVSC (uniform weights), MVSCW (weights based on subject-specific embedding quality), and AASC (weights optimized along with the embedding) with the competing technique of Joint Diagonalization of Laplacians (JDL). Results show that multiple-view spectral clustering not only yields group-wise functional communities that are more consistent than JDL when using randomly selected subsets of individual brains, but it is several orders of magnitude faster than JDL.



### Sublabel-Accurate Discretization of Nonconvex Free-Discontinuity Problems
- **Arxiv ID**: http://arxiv.org/abs/1611.06987v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.06987v2)
- **Published**: 2016-11-21 20:09:18+00:00
- **Updated**: 2017-08-05 07:46:22+00:00
- **Authors**: Thomas Möllenhoff, Daniel Cremers
- **Comment**: ICCV 2017 version
- **Journal**: None
- **Summary**: In this work we show how sublabel-accurate multilabeling approaches can be derived by approximating a classical label-continuous convex relaxation of nonconvex free-discontinuity problems. This insight allows to extend these sublabel-accurate approaches from total variation to general convex and nonconvex regularizations. Furthermore, it leads to a systematic approach to the discretization of continuous convex relaxations. We study the relationship to existing discretizations and to discrete-continuous MRFs. Finally, we apply the proposed approach to obtain a sublabel-accurate and convex solution to the vectorial Mumford-Shah functional and show in several experiments that it leads to more precise solutions using fewer labels.



### Image-to-Image Translation with Conditional Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.07004v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.07004v3)
- **Published**: 2016-11-21 20:48:16+00:00
- **Updated**: 2018-11-26 13:54:12+00:00
- **Authors**: Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros
- **Comment**: Website: https://phillipi.github.io/pix2pix/, CVPR 2017
- **Journal**: None
- **Summary**: We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.



