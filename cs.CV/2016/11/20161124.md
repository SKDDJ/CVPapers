# Arxiv Papers in cs.CV on 2016-11-24
### Robotic Grasp Detection using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.08036v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1611.08036v4)
- **Published**: 2016-11-24 00:07:39+00:00
- **Updated**: 2017-07-21 22:09:02+00:00
- **Authors**: Sulabh Kumra, Christopher Kanan
- **Comment**: 8 pages, 9 figures, 2017 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS 2017)
- **Journal**: None
- **Summary**: Deep learning has significantly advanced computer vision and natural language processing. While there have been some successes in robotics using deep learning, it has not been widely adopted. In this paper, we present a novel robotic grasp detection system that predicts the best grasping pose of a parallel-plate robotic gripper for novel objects using the RGB-D image of the scene. The proposed model uses a deep convolutional neural network to extract features from the scene and then uses a shallow convolutional neural network to predict the grasp configuration for the object of interest. Our multi-modal model achieved an accuracy of 89.21% on the standard Cornell Grasp Dataset and runs at real-time speeds. This redefines the state-of-the-art for robotic grasp detection.



### Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields
- **Arxiv ID**: http://arxiv.org/abs/1611.08050v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08050v2)
- **Published**: 2016-11-24 01:58:16+00:00
- **Updated**: 2017-04-14 00:19:18+00:00
- **Authors**: Zhe Cao, Tomas Simon, Shih-En Wei, Yaser Sheikh
- **Comment**: Accepted as CVPR 2017 Oral. Video result:
  https://youtu.be/pW6nZXeWlGM
- **Journal**: None
- **Summary**: We present an approach to efficiently detect the 2D pose of multiple people in an image. The approach uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed first in the inaugural COCO 2016 keypoints challenge, and significantly exceeds the previous state-of-the-art result on the MPII Multi-Person benchmark, both in performance and efficiency.



### Recalling Holistic Information for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1611.08061v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08061v1)
- **Published**: 2016-11-24 03:46:37+00:00
- **Updated**: 2016-11-24 03:46:37+00:00
- **Authors**: Hexiang Hu, Zhiwei Deng, Guang-tong Zhou, Fei Sha, Greg Mori
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation requires a detailed labeling of image pixels by object category. Information derived from local image patches is necessary to describe the detailed shape of individual objects. However, this information is ambiguous and can result in noisy labels. Global inference of image content can instead capture the general semantic concepts present. We advocate that high-recall holistic inference of image concepts provides valuable information for detailed pixel labeling. We build a two-stream neural network architecture that facilitates information flow from holistic information to local pixels, while keeping common image features shared among the low-level layers of both the holistic analysis and segmentation branches. We empirically evaluate our network on four standard semantic segmentation datasets. Our network obtains state-of-the-art performance on PASCAL-Context and NYUDv2, and ablation studies verify its effectiveness on ADE20K and SIFT-Flow.



### 3D Fully Convolutional Network for Vehicle Detection in Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/1611.08069v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1611.08069v2)
- **Published**: 2016-11-24 05:06:05+00:00
- **Updated**: 2017-01-16 05:56:01+00:00
- **Authors**: Bo Li
- **Comment**: None
- **Journal**: None
- **Summary**: 2D fully convolutional network has been recently successfully applied to object detection from images. In this paper, we extend the fully convolutional network based detection techniques to 3D and apply it to point cloud data. The proposed approach is verified on the task of vehicle detection from lidar point cloud for autonomous driving. Experiments on the KITTI dataset shows a significant performance improvement over the previous point cloud based detection approaches.



### Deep Joint Face Hallucination and Recognition
- **Arxiv ID**: http://arxiv.org/abs/1611.08091v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08091v1)
- **Published**: 2016-11-24 08:19:49+00:00
- **Updated**: 2016-11-24 08:19:49+00:00
- **Authors**: Junyu Wu, Shengyong Ding, Wei Xu, Hongyang Chao
- **Comment**: 10 pages, 2 figures
- **Journal**: None
- **Summary**: Deep models have achieved impressive performance for face hallucination tasks. However, we observe that directly feeding the hallucinated facial images into recog- nition models can even degrade the recognition performance despite the much better visualization quality. In this paper, we address this problem by jointly learning a deep model for two tasks, i.e. face hallucination and recognition. In particular, we design an end-to-end deep convolution network with hallucination sub-network cascaded by recognition sub-network. The recognition sub- network are responsible for producing discriminative feature representations using the hallucinated images as inputs generated by hallucination sub-network. During training, we feed LR facial images into the network and optimize the parameters by minimizing two loss items, i.e. 1) face hallucination loss measured by the pixel wise difference between the ground truth HR images and network-generated images; and 2) verification loss which is measured by the classification error and intra-class distance. We extensively evaluate our method on LFW and YTF datasets. The experimental results show that our method can achieve recognition accuracy 97.95% on 4x down-sampled LFW testing set, outperforming the accuracy 96.35% of conventional face recognition model. And on the more challenging YTF dataset, we achieve recognition accuracy 90.65%, a margin over the recognition accuracy 89.45% obtained by conventional face recognition model on the 4x down-sampled version.



### Geometric deep learning: going beyond Euclidean data
- **Arxiv ID**: http://arxiv.org/abs/1611.08097v2
- **DOI**: 10.1109/MSP.2017.2693418
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08097v2)
- **Published**: 2016-11-24 08:45:01+00:00
- **Updated**: 2017-05-03 12:37:19+00:00
- **Authors**: Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, Pierre Vandergheynst
- **Comment**: None
- **Journal**: None
- **Summary**: Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.



### Automatically Building Face Datasets of New Domains from Weakly Labeled Data with Pretrained Models
- **Arxiv ID**: http://arxiv.org/abs/1611.08107v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08107v1)
- **Published**: 2016-11-24 09:11:21+00:00
- **Updated**: 2016-11-24 09:11:21+00:00
- **Authors**: Shengyong Ding, Junyu Wu, Wei Xu, Hongyang Chao
- **Comment**: None
- **Journal**: None
- **Summary**: Training data are critical in face recognition systems. However, labeling a large scale face data for a particular domain is very tedious. In this paper, we propose a method to automatically and incrementally construct datasets from massive weakly labeled data of the target domain which are readily available on the Internet under the help of a pretrained face model. More specifically, given a large scale weakly labeled dataset in which each face image is associated with a label, i.e. the name of an identity, we create a graph for each identity with edges linking matched faces verified by the existing model under a tight threshold. Then we use the maximal subgraph as the cleaned data for that identity. With the cleaned dataset, we update the existing face model and use the new model to filter the original dataset to get a larger cleaned dataset. We collect a large weakly labeled dataset containing 530,560 Asian face images of 7,962 identities from the Internet, which will be published for the study of face recognition. By running the filtering process, we obtain a cleaned datasets (99.7+% purity) of size 223,767 (recall 70.9%). On our testing dataset of Asian faces, the model trained by the cleaned dataset achieves recognition rate 93.1%, which obviously outperforms the model trained by the public dataset CASIA whose recognition rate is 85.9%.



### Extraction of airway trees using multiple hypothesis tracking and template matching
- **Arxiv ID**: http://arxiv.org/abs/1611.08131v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08131v1)
- **Published**: 2016-11-24 10:42:07+00:00
- **Updated**: 2016-11-24 10:42:07+00:00
- **Authors**: Raghavendra Selvan, Jens Petersen, Jesper H. Pedersen, Marleen de Bruijne
- **Comment**: 12 pages. Presented at the MICCAI Pulmonary Image Analysis Workshop,
  Athens, Greece, 2016
- **Journal**: None
- **Summary**: Knowledge of airway tree morphology has important clinical applications in diagnosis of chronic obstructive pulmonary disease. We present an automatic tree extraction method based on multiple hypothesis tracking and template matching for this purpose and evaluate its performance on chest CT images. The method is adapted from a semi-automatic method devised for vessel segmentation. Idealized tubular templates are constructed that match airway probability obtained from a trained classifier and ranked based on their relative significance. Several such regularly spaced templates form the local hypotheses used in constructing a multiple hypothesis tree, which is then traversed to reach decisions. The proposed modifications remove the need for local thresholding of hypotheses as decisions are made entirely based on statistical comparisons involving the hypothesis tree. The results show improvements in performance when compared to the original method and region growing on intensity images. We also compare the method with region growing on the probability images, where the presented method does not show substantial improvement, but we expect it to be less sensitive to local anomalies in the data.



### Comparative study of histogram distance measures for re-identification
- **Arxiv ID**: http://arxiv.org/abs/1611.08134v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08134v1)
- **Published**: 2016-11-24 10:59:33+00:00
- **Updated**: 2016-11-24 10:59:33+00:00
- **Authors**: Pedro A. Marín-Reyes, Javier Lorenzo-Navarro, Modesto Castrillón-Santana
- **Comment**: None
- **Journal**: None
- **Summary**: Color based re-identification methods usually rely on a distance function to measure the similarity between individuals. In this paper we study the behavior of several histogram distance measures in different color spaces. We wonder whether there is a particular histogram distance measure better than others, likewise also, if there is a color space that present better discrimination features. Several experiments are designed and evaluated in several images to obtain measures against various color spaces. We test in several image databases. A measure ranking is generated to calculate the area under the CMC, this area is the indicator used to evaluate which distance measure and color space present the best performance for the considered databases. Also, other parameters such as the image division in horizontal stripes and number of histogram bins, have been studied.



### Interferences in match kernels
- **Arxiv ID**: http://arxiv.org/abs/1611.08194v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08194v1)
- **Published**: 2016-11-24 14:25:43+00:00
- **Updated**: 2016-11-24 14:25:43+00:00
- **Authors**: Naila Murray, Hervé Jégou, Florent Perronnin, Andrew Zisserman
- **Comment**: Accepted as regular paper in IEEE Transactions on Pattern Analysis
  and Machine Intelligence (TPAMI)
- **Journal**: None
- **Summary**: We consider the design of an image representation that embeds and aggregates a set of local descriptors into a single vector. Popular representations of this kind include the bag-of-visual-words, the Fisher vector and the VLAD. When two such image representations are compared with the dot-product, the image-to-image similarity can be interpreted as a match kernel. In match kernels, one has to deal with interference, i.e. with the fact that even if two descriptors are unrelated, their matching score may contribute to the overall similarity.   We formalise this problem and propose two related solutions, both aimed at equalising the individual contributions of the local descriptors in the final representation. These methods modify the aggregation stage by including a set of per-descriptor weights. They differ by the objective function that is optimised to compute those weights. The first is a "democratisation" strategy that aims at equalising the relative importance of each descriptor in the set comparison metric. The second one involves equalising the match of a single descriptor to the aggregated vector.   These concurrent methods give a substantial performance boost over the state of the art in image search with short or mid-size vectors, as demonstrated by our experiments on standard public image retrieval benchmarks.



### Domain Adaptation by Mixture of Alignments of Second- or Higher-Order Scatter Tensors
- **Arxiv ID**: http://arxiv.org/abs/1611.08195v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08195v2)
- **Published**: 2016-11-24 14:27:08+00:00
- **Updated**: 2017-04-11 13:43:24+00:00
- **Authors**: Piotr Koniusz, Yusuf Tas, Fatih Porikli
- **Comment**: CVPR'17
- **Journal**: None
- **Summary**: In this paper, we propose an approach to the domain adaptation, dubbed Second- or Higher-order Transfer of Knowledge (So-HoT), based on the mixture of alignments of second- or higher-order scatter statistics between the source and target domains. The human ability to learn from few labeled samples is a recurring motivation in the literature for domain adaptation. Towards this end, we investigate the supervised target scenario for which few labeled target training samples per category exist. Specifically, we utilize two CNN streams: the source and target networks fused at the classifier level. Features from the fully connected layers fc7 of each network are used to compute second- or even higher-order scatter tensors; one per network stream per class. As the source and target distributions are somewhat different despite being related, we align the scatters of the two network streams of the same class (within-class scatters) to a desired degree with our bespoke loss while maintaining good separation of the between-class scatters. We train the entire network in end-to-end fashion. We provide evaluations on the standard Office benchmark (visual domains), RGB-D combined with Caltech256 (depth-to-rgb transfer) and Pascal VOC2007 combined with the TU Berlin dataset (image-to-sketch transfer). We attain state-of-the-art results.



### Texture Synthesis with Spatial Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.08207v4
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1611.08207v4)
- **Published**: 2016-11-24 15:01:42+00:00
- **Updated**: 2017-09-08 15:09:52+00:00
- **Authors**: Nikolay Jetchev, Urs Bergmann, Roland Vollgraf
- **Comment**: presented at the NIPS 2016 adversarial learning workshop, Barcelona,
  Spain
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) are a recent approach to train generative models of data, which have been shown to work particularly well on image data. In the current paper we introduce a new model for texture synthesis based on GAN learning. By extending the input noise distribution space from a single vector to a whole spatial tensor, we create an architecture with properties well suited to the task of texture synthesis, which we call spatial GAN (SGAN). To our knowledge, this is the first successful completely data-driven texture synthesis method based on GANs.   Our method has the following features which make it a state of the art algorithm for texture synthesis: high image quality of the generated textures, very high scalability w.r.t. the output texture size, fast real-time forward generation, the ability to fuse multiple diverse source images in complex textures. To illustrate these capabilities we present multiple experiments with different classes of texture images and use cases. We also discuss some limitations of our method with respect to the types of texture images it can synthesize, and compare it to other neural techniques for texture generation.



### Learning Where to Attend Like a Human Driver
- **Arxiv ID**: http://arxiv.org/abs/1611.08215v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1611.08215v2)
- **Published**: 2016-11-24 15:14:23+00:00
- **Updated**: 2017-05-09 16:24:16+00:00
- **Authors**: Andrea Palazzi, Francesco Solera, Simone Calderara, Stefano Alletto, Rita Cucchiara
- **Comment**: To appear in IEEE Intelligent Vehicles Symposium 2017
- **Journal**: None
- **Summary**: Despite the advent of autonomous cars, it's likely - at least in the near future - that human attention will still maintain a central role as a guarantee in terms of legal responsibility during the driving task. In this paper we study the dynamics of the driver's gaze and use it as a proxy to understand related attentional mechanisms. First, we build our analysis upon two questions: where and what the driver is looking at? Second, we model the driver's gaze by training a coarse-to-fine convolutional network on short sequences extracted from the DR(eye)VE dataset. Experimental comparison against different baselines reveal that the driver's gaze can indeed be learnt to some extent, despite i) being highly subjective and ii) having only one driver's gaze available for each sequence due to the irreproducibility of the scene. Eventually, we advocate for a new assisted driving paradigm which suggests to the driver, with no intervention, where she should focus her attention.



### AdaScan: Adaptive Scan Pooling in Deep Convolutional Neural Networks for Human Action Recognition in Videos
- **Arxiv ID**: http://arxiv.org/abs/1611.08240v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08240v4)
- **Published**: 2016-11-24 16:26:11+00:00
- **Updated**: 2017-06-25 08:55:48+00:00
- **Authors**: Amlan Kar, Nishant Rai, Karan Sikka, Gaurav Sharma
- **Comment**: CVPR 2017 Camera Ready Version
- **Journal**: None
- **Summary**: We propose a novel method for temporally pooling frames in a video for the task of human action recognition. The method is motivated by the observation that there are only a small number of frames which, together, contain sufficient information to discriminate an action class present in a video, from the rest. The proposed method learns to pool such discriminative and informative frames, while discarding a majority of the non-informative frames in a single temporal scan of the video. Our algorithm does so by continuously predicting the discriminative importance of each video frame and subsequently pooling them in a deep learning framework. We show the effectiveness of our proposed pooling method on standard benchmarks where it consistently improves on baseline pooling methods, with both RGB and optical flow based Convolutional networks. Further, in combination with complementary video representations, we show results that are competitive with respect to the state-of-the-art results on two challenging and publicly available benchmark datasets.



### Weakly Supervised Cascaded Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.08258v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08258v1)
- **Published**: 2016-11-24 17:07:48+00:00
- **Updated**: 2016-11-24 17:07:48+00:00
- **Authors**: Ali Diba, Vivek Sharma, Ali Pazandeh, Hamed Pirsiavash, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection is a challenging task in visual understanding domain, and even more so if the supervision is to be weak. Recently, few efforts to handle the task without expensive human annotations is established by promising deep neural network. A new architecture of cascaded networks is proposed to learn a convolutional neural network (CNN) under such conditions. We introduce two such architectures, with either two cascade stages or three which are trained in an end-to-end pipeline. The first stage of both architectures extracts best candidate of class specific region proposals by training a fully convolutional network. In the case of the three stage architecture, the middle stage provides object segmentation, using the output of the activation maps of first stage. The final stage of both architectures is a part of a convolutional neural network that performs multiple instance learning on proposals extracted in the previous stage(s). Our experiments on the PASCAL VOC 2007, 2010, 2012 and large scale object datasets, ILSVRC 2013, 2014 datasets show improvements in the areas of weakly-supervised object detection, classification and localization.



### InstanceCut: from Edges to Instances with MultiCut
- **Arxiv ID**: http://arxiv.org/abs/1611.08272v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08272v1)
- **Published**: 2016-11-24 17:54:32+00:00
- **Updated**: 2016-11-24 17:54:32+00:00
- **Authors**: Alexander Kirillov, Evgeny Levinkov, Bjoern Andres, Bogdan Savchynskyy, Carsten Rother
- **Comment**: The code would be released at
  https://github.com/alexander-kirillov/InstanceCut
- **Journal**: None
- **Summary**: This work addresses the task of instance-aware semantic segmentation. Our key motivation is to design a simple method with a new modelling-paradigm, which therefore has a different trade-off between advantages and disadvantages compared to known approaches. Our approach, we term InstanceCut, represents the problem by two output modalities: (i) an instance-agnostic semantic segmentation and (ii) all instance-boundaries. The former is computed from a standard convolutional neural network for semantic segmentation, and the latter is derived from a new instance-aware edge detection model. To reason globally about the optimal partitioning of an image into instances, we combine these two modalities into a novel MultiCut formulation. We evaluate our approach on the challenging CityScapes dataset. Despite the conceptual simplicity of our approach, we achieve the best result among all published methods, and perform particularly well for rare object classes.



### Two-Level Structural Sparsity Regularization for Identifying Lattices and Defects in Noisy Images
- **Arxiv ID**: http://arxiv.org/abs/1611.08280v4
- **DOI**: 10.1214/17-AOAS1096
- **Categories**: **stat.AP**, cs.CV, 62P35, G.3; I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/1611.08280v4)
- **Published**: 2016-11-24 18:24:58+00:00
- **Updated**: 2017-09-01 19:22:15+00:00
- **Authors**: Xin Li, Alex Belianinov, Ondrej Dyck, Stephen Jesse, Chiwoo Park
- **Comment**: Accepted to Annals of Applied Statistics
- **Journal**: Annals of Applied Statistics 2018, Vol. 12, No. 1, 348-377
- **Summary**: This paper presents a regularized regression model with a two-level structural sparsity penalty applied to locate individual atoms in a noisy scanning transmission electron microscopy image (STEM). In crystals, the locations of atoms is symmetric, condensed into a few lattice groups. Therefore, by identifying the underlying lattice in a given image, individual atoms can be accurately located. We propose to formulate the identification of the lattice groups as a sparse group selection problem. Furthermore, real atomic scale images contain defects and vacancies, so atomic identification based solely on a lattice group may result in false positives and false negatives. To minimize error, model includes an individual sparsity regularization in addition to the group sparsity for a within-group selection, which results in a regression model with a two-level sparsity regularization. We propose a modification of the group orthogonal matching pursuit (gOMP) algorithm with a thresholding step to solve the atom finding problem. The convergence and statistical analyses of the proposed algorithm are presented. The proposed algorithm is also evaluated through numerical experiments with simulated images. The applicability of the algorithm on determination of atom structures and identification of imaging distortions and atomic defects was demonstrated using three real STEM images. We believe this is an important step toward automatic phase identification and assignment with the advent of genomic databases for materials.



### Deep Watershed Transform for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1611.08303v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08303v2)
- **Published**: 2016-11-24 20:46:33+00:00
- **Updated**: 2017-05-04 21:13:12+00:00
- **Authors**: Min Bai, Raquel Urtasun
- **Comment**: None
- **Journal**: None
- **Summary**: Most contemporary approaches to instance segmentation use complex pipelines involving conditional random fields, recurrent neural networks, object proposals, or template matching schemes. In our paper, we present a simple yet powerful end-to-end convolutional neural network to tackle this task. Our approach combines intuitions from the classical watershed transform and modern deep learning to produce an energy map of the image where object instances are unambiguously represented as basins in the energy map. We then perform a cut at a single energy level to directly yield connected components corresponding to object instances. Our model more than doubles the performance of the state-of-the-art on the challenging Cityscapes Instance Level Segmentation task.



### Training and Evaluating Multimodal Word Embeddings with Large-scale Web Annotated Images
- **Arxiv ID**: http://arxiv.org/abs/1611.08321v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, I.2.6; I.2.7; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1611.08321v1)
- **Published**: 2016-11-24 23:15:56+00:00
- **Updated**: 2016-11-24 23:15:56+00:00
- **Authors**: Junhua Mao, Jiajing Xu, Yushi Jing, Alan Yuille
- **Comment**: Appears in NIPS 2016. The datasets introduced in this work will be
  gradually released on the project page
- **Journal**: None
- **Summary**: In this paper, we focus on training and evaluating effective word embeddings with both text and visual information. More specifically, we introduce a large-scale dataset with 300 million sentences describing over 40 million images crawled and downloaded from publicly available Pins (i.e. an image with sentence descriptions uploaded by users) on Pinterest. This dataset is more than 200 times larger than MS COCO, the standard large-scale image dataset with sentence descriptions. In addition, we construct an evaluation dataset to directly assess the effectiveness of word embeddings in terms of finding semantically similar or related words and phrases. The word/phrase pairs in this evaluation dataset are collected from the click data with millions of users in an image search system, thus contain rich semantic relationships. Based on these datasets, we propose and compare several Recurrent Neural Networks (RNNs) based multimodal (text and image) models. Experiments show that our model benefits from incorporating the visual information into the word embeddings, and a weight sharing strategy is crucial for learning such multimodal embeddings. The project page is: http://www.stat.ucla.edu/~junhua.mao/multimodal_embedding.html



### Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes
- **Arxiv ID**: http://arxiv.org/abs/1611.08323v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.08323v2)
- **Published**: 2016-11-24 23:55:28+00:00
- **Updated**: 2016-12-06 19:36:19+00:00
- **Authors**: Tobias Pohlen, Alexander Hermans, Markus Mathias, Bastian Leibe
- **Comment**: Changes in v2: Fixed equation (10), fixed legend of Figure 6, fixed
  legend of Figure 9, added page numbers, fixed minor spelling mistakes
- **Journal**: None
- **Summary**: Semantic image segmentation is an essential component of modern autonomous driving systems, as an accurate understanding of the surrounding scene is crucial to navigation and action planning. Current state-of-the-art approaches in semantic image segmentation rely on pre-trained networks that were initially developed for classifying images as a whole. While these networks exhibit outstanding recognition performance (i.e., what is visible?), they lack localization accuracy (i.e., where precisely is something located?). Therefore, additional processing steps have to be performed in order to obtain pixel-accurate segmentation masks at the full image resolution. To alleviate this problem we propose a novel ResNet-like architecture that exhibits strong localization and recognition performance. We combine multi-scale context with pixel-level accuracy by using two processing streams within our network: One stream carries information at the full image resolution, enabling precise adherence to segment boundaries. The other stream undergoes a sequence of pooling operations to obtain robust features for recognition. The two streams are coupled at the full image resolution using residuals. Without additional processing steps and without pre-training, our approach achieves an intersection-over-union score of 71.8% on the Cityscapes dataset.



