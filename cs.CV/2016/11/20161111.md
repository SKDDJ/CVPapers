# Arxiv Papers in cs.CV on 2016-11-11
### Construction Inspection through Spatial Database
- **Arxiv ID**: http://arxiv.org/abs/1611.03566v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.03566v3)
- **Published**: 2016-11-11 02:23:32+00:00
- **Updated**: 2017-04-21 21:29:24+00:00
- **Authors**: Ahmad Hasan, Ashraf Qadir, Ian Nordeng, Jeremiah Neubert
- **Comment**: 8 pages, 8 figues, 3 tables, 1 graph
- **Journal**: None
- **Summary**: This paper presents a novel pipeline for development of an efficient set of tools for extracting information from the video of a structure, captured by an Unmanned Aircraft System (UAS) to produce as-built documentation to aid inspection of large multi-storied building during construction. Our system uses the output from a Simultaneous Localization and Mapping system and a 3D CAD model of the structure in order to construct a spatial database to store images into the 3D CAD model space. This allows the user to perform a spatial query for images through spatial indexing into the 3D CAD model space. The image returned by the spatial query is used to extract metric information. The spatial database is also used to generate a 3D textured model which provides a visual as-built documentation.



### Adaptive Deep Pyramid Matching for Remote Sensing Scene Classification
- **Arxiv ID**: http://arxiv.org/abs/1611.03589v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.03589v1)
- **Published**: 2016-11-11 05:17:56+00:00
- **Updated**: 2016-11-11 05:17:56+00:00
- **Authors**: Qingshan Liu, Renlong Hang, Huihui Song, Fuping Zhu, Javier Plaza, Antonio Plaza
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have attracted increasing attention in the remote sensing community. Most CNNs only take the last fully-connected layers as features for the classification of remotely sensed images, discarding the other convolutional layer features which may also be helpful for classification purposes. In this paper, we propose a new adaptive deep pyramid matching (ADPM) model that takes advantage of the features from all of the convolutional layers for remote sensing image classification. To this end, the optimal fusing weights for different convolutional layers are learned from the data itself. In remotely sensed scenes, the objects of interest exhibit different scales in distinct scenes, and even a single scene may contain objects with different sizes. To address this issue, we select the CNN with spatial pyramid pooling (SPP-net) as the basic deep network, and further construct a multi-scale ADPM model to learn complementary information from multi-scale images. Our experiments have been conducted using two widely used remote sensing image databases, and the results show that the proposed method significantly improves the performance when compared to other state-of-the-art methods.



### Learning Multi-Scale Deep Features for High-Resolution Satellite Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1611.03591v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.03591v1)
- **Published**: 2016-11-11 05:31:42+00:00
- **Updated**: 2016-11-11 05:31:42+00:00
- **Authors**: Qingshan Liu, Renlong Hang, Huihui Song, Zhi Li
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a multi-scale deep feature learning method for high-resolution satellite image classification. Specifically, we firstly warp the original satellite image into multiple different scales. The images in each scale are employed to train a deep convolutional neural network (DCNN). However, simultaneously training multiple DCNNs is time-consuming. To address this issue, we explore DCNN with spatial pyramid pooling (SPP-net). Since different SPP-nets have the same number of parameters, which share the identical initial values, and only fine-tuning the parameters in fully-connected layers ensures the effectiveness of each network, thereby greatly accelerating the training process. Then, the multi-scale satellite images are fed into their corresponding SPP-nets respectively to extract multi-scale deep features. Finally, a multiple kernel learning method is developed to automatically learn the optimal combination of such features. Experiments on two difficult datasets show that the proposed method achieves favorable performance compared to other state-of-the-art methods.



### Deep Recurrent Neural Network for Mobile Human Activity Recognition with High Throughput
- **Arxiv ID**: http://arxiv.org/abs/1611.03607v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1611.03607v1)
- **Published**: 2016-11-11 08:21:09+00:00
- **Updated**: 2016-11-11 08:21:09+00:00
- **Authors**: Masaya Inoue, Sozo Inoue, Takeshi Nishida
- **Comment**: 10 pages, 13 figures
- **Journal**: None
- **Summary**: In this paper, we propose a method of human activity recognition with high throughput from raw accelerometer data applying a deep recurrent neural network (DRNN), and investigate various architectures and its combination to find the best parameter values. The "high throughput" refers to short time at a time of recognition. We investigated various parameters and architectures of the DRNN by using the training dataset of 432 trials with 6 activity classes from 7 people. The maximum recognition rate was 95.42% and 83.43% against the test data of 108 segmented trials each of which has single activity class and 18 multiple sequential trials, respectively. Here, the maximum recognition rates by traditional methods were 71.65% and 54.97% for each. In addition, the efficiency of the found parameters was evaluated by using additional dataset. Further, as for throughput of the recognition per unit time, the constructed DRNN was requiring only 1.347 [ms], while the best traditional method required 11.031 [ms] which includes 11.027 [ms] for feature calculation. These advantages are caused by the compact and small architecture of the constructed real time oriented DRNN.



### Oriented bounding boxes using multiresolution contours for fast interference detection of arbitrary geometry objects
- **Arxiv ID**: http://arxiv.org/abs/1611.03666v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1611.03666v1)
- **Published**: 2016-11-11 11:50:59+00:00
- **Updated**: 2016-11-11 11:50:59+00:00
- **Authors**: L. A. Rivera, Vania V. Estrela, P. C. P. Carvalho
- **Comment**: 8 pages, 10 figures
- **Journal**: The 12-th International Conference in Central Europe on Computer
  Graphics, Visualization and Computer Vision'2004, WSCG 2004, University of
  West Bohemia, Campus Bory, Plzen-Bory, Czech Republic, February 2-6, 2004
- **Summary**: Interference detection of arbitrary geometric objects is not a trivial task due to the heavy computational load imposed by implementation issues. The hierarchically structured bounding boxes help us to quickly isolate the contour of segments in interference. In this paper, a new approach is introduced to treat the interference detection problem involving the representation of arbitrary shaped objects. Our proposed method relies upon searching for the best possible way to represent contours by means of hierarchically structured rectangular oriented bounding boxes. This technique handles 2D objects boundaries defined by closed B-spline curves with roughness details. Each oriented box is adapted and fitted to the segments of the contour using second order statistical indicators from some elements of the segments of the object contour in a multiresolution framework. Our method is efficient and robust when it comes to 2D animations in real time. It can deal with smooth curves and polygonal approximations as well results are present to illustrate the performance of the new method.



### Learning to Navigate in Complex Environments
- **Arxiv ID**: http://arxiv.org/abs/1611.03673v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1611.03673v3)
- **Published**: 2016-11-11 12:14:45+00:00
- **Updated**: 2017-01-13 11:15:22+00:00
- **Authors**: Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J. Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, Dharshan Kumaran, Raia Hadsell
- **Comment**: 11 pages, 5 appendix pages, 11 figures, 3 tables, under review as a
  conference paper at ICLR 2017
- **Journal**: None
- **Summary**: Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks leveraging multimodal sensory inputs. In particular we consider jointly learning the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classification tasks. This approach can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour, its ability to localise, and its network activity dynamics, showing that the agent implicitly learns key navigation abilities.



### Deep Convolutional Neural Network for Inverse Problems in Imaging
- **Arxiv ID**: http://arxiv.org/abs/1611.03679v1
- **DOI**: 10.1109/TIP.2017.2713099
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.03679v1)
- **Published**: 2016-11-11 12:35:08+00:00
- **Updated**: 2016-11-11 12:35:08+00:00
- **Authors**: Kyong Hwan Jin, Michael T. McCann, Emmanuel Froustey, Michael Unser
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing, vol. 26, no. 9, pp.
  4509-4522, Sept. 2017
- **Summary**: In this paper, we propose a novel deep convolutional neural network (CNN)-based algorithm for solving ill-posed inverse problems. Regularized iterative algorithms have emerged as the standard approach to ill-posed inverse problems in the past few decades. These methods produce excellent results, but can be challenging to deploy in practice due to factors including the high computational cost of the forward and adjoint operators and the difficulty of hyper parameter selection. The starting point of our work is the observation that unrolled iterative methods have the form of a CNN (filtering followed by point-wise non-linearity) when the normal operator (H*H, the adjoint of H times H) of the forward model is a convolution. Based on this observation, we propose using direct inversion followed by a CNN to solve normal-convolutional inverse problems. The direct inversion encapsulates the physical model of the system, but leads to artifacts when the problem is ill-posed; the CNN combines multiresolution decomposition and residual learning in order to learn to remove these artifacts while preserving image structure. We demonstrate the performance of the proposed network in sparse-view reconstruction (down to 50 views) on parallel beam X-ray computed tomography in synthetic phantoms as well as in real experimental sinograms. The proposed network outperforms total variation-regularized iterative reconstruction for the more realistic phantoms and requires less than a second to reconstruct a 512 x 512 image on GPU.



### Hierarchical Object Detection with Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1611.03718v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1611.03718v2)
- **Published**: 2016-11-11 14:25:54+00:00
- **Updated**: 2016-11-25 14:31:07+00:00
- **Authors**: Miriam Bellver, Xavier Giro-i-Nieto, Ferran Marques, Jordi Torres
- **Comment**: Deep Reinforcement Learning Workshop (NIPS 2016). Project page at
  https://imatge-upc.github.io/detection-2016-nipsws/
- **Journal**: None
- **Summary**: We present a method for performing hierarchical object detection in images guided by a deep reinforcement learning agent. The key idea is to focus on those parts of the image that contain richer information and zoom on them. We train an intelligent agent that, given an image window, is capable of deciding where to focus the attention among five different predefined region candidates (smaller windows). This procedure is iterated providing a hierarchical image analysis.We compare two different candidate proposal strategies to guide the object search: with and without overlap. Moreover, our work compares two different strategies to extract features from a convolutional neural network for each region proposal: a first one that computes new feature maps for each region proposal, and a second one that computes the feature maps for the whole image to later generate crops for each region proposal. Experiments indicate better results for the overlapping candidate proposal strategy and a loss of performance for the cropped image features due to the loss of spatial resolution. We argue that, while this loss seems unavoidable when working with large amounts of object candidates, the much more reduced amount of region proposals generated by our reinforcement learning agent allows considering to extract features for each location without sharing convolutional computation among regions.



### MCMC Shape Sampling for Image Segmentation with Nonparametric Shape Priors
- **Arxiv ID**: http://arxiv.org/abs/1611.03749v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.03749v1)
- **Published**: 2016-11-11 15:37:06+00:00
- **Updated**: 2016-11-11 15:37:06+00:00
- **Authors**: Ertunc Erdil, Sinan Yıldırım, Müjdat Çetin, Tolga Taşdizen
- **Comment**: Computer Vision and Pattern Recognition conference, 2016
- **Journal**: None
- **Summary**: Segmenting images of low quality or with missing data is a challenging problem. Integrating statistical prior information about the shapes to be segmented can improve the segmentation results significantly. Most shape-based segmentation algorithms optimize an energy functional and find a point estimate for the object to be segmented. This does not provide a measure of the degree of confidence in that result, neither does it provide a picture of other probable solutions based on the data and the priors. With a statistical view, addressing these issues would involve the problem of characterizing the posterior densities of the shapes of the objects to be segmented. For such characterization, we propose a Markov chain Monte Carlo (MCMC) sampling-based image segmentation algorithm that uses statistical shape priors. In addition to better characterization of the statistical structure of the problem, such an approach would also have the potential to address issues with getting stuck at local optima, suffered by existing shape-based segmentation methods. Our approach is able to characterize the posterior probability density in the space of shapes through its samples, and to return multiple solutions, potentially from different modes of a multimodal probability density, which would be encountered, e.g., in segmenting objects from multiple shape classes. We present promising results on a variety of data sets. We also provide an extension for segmenting shapes of objects with parts that can go through independent shape variations. This extension involves the use of local shape priors on object parts and provides robustness to limitations in shape training data size.



### HoneyFaces: Increasing the Security and Privacy of Authentication Using Synthetic Facial Images
- **Arxiv ID**: http://arxiv.org/abs/1611.03811v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1611.03811v1)
- **Published**: 2016-11-11 18:40:32+00:00
- **Updated**: 2016-11-11 18:40:32+00:00
- **Authors**: Mor Ohana, Orr Dunkelman, Stuart Gibson, Margarita Osadchy
- **Comment**: None
- **Journal**: None
- **Summary**: One of the main challenges faced by Biometric-based authentication systems is the need to offer secure authentication while maintaining the privacy of the biometric data. Previous solutions, such as Secure Sketch and Fuzzy Extractors, rely on assumptions that cannot be guaranteed in practice, and often affect the authentication accuracy.   In this paper, we introduce HoneyFaces: the concept of adding a large set of synthetic faces (indistinguishable from real) into the biometric "password file". This password inflation protects the privacy of users and increases the security of the system without affecting the accuracy of the authentication. In particular, privacy for the real users is provided by "hiding" them among a large number of fake users (as the distributions of synthetic and real faces are equal). In addition to maintaining the authentication accuracy, and thus not affecting the security of the authentication process, HoneyFaces offer several security improvements: increased exfiltration hardness, improved leakage detection, and the ability to use a Two-server setting like in HoneyWords. Finally, HoneyFaces can be combined with other security and privacy mechanisms for biometric data.   We implemented the HoneyFaces system and tested it with a password file composed of 270 real users. The "password file" was then inflated to accommodate up to $2^{36.5}$ users (resulting in a 56.6 TB "password file"). At the same time, the inclusion of additional faces does not affect the true acceptance rate or false acceptance rate which were 93.33\% and 0.01\%, respectively.



### Effective sparse representation of X-Ray medical images
- **Arxiv ID**: http://arxiv.org/abs/1611.03873v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.03873v1)
- **Published**: 2016-11-11 21:00:58+00:00
- **Updated**: 2016-11-11 21:00:58+00:00
- **Authors**: Laura Rebollo-Neira
- **Comment**: Routines for implementing the approach are available on
  http://www.nonlinear-approx.info/examples/node06.html
- **Journal**: None
- **Summary**: Effective sparse representation of X-Ray medical images within the context of data reduction is considered. The proposed framework is shown to render an enormous reduction in the cardinality of the data set required to represent this class of images at very good quality. The particularity of the approach is that it can be implemented at very competitive processing time and low memory requirements



### When Fashion Meets Big Data: Discriminative Mining of Best Selling Clothing Features
- **Arxiv ID**: http://arxiv.org/abs/1611.03915v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.03915v2)
- **Published**: 2016-11-11 23:58:06+00:00
- **Updated**: 2017-02-22 05:34:33+00:00
- **Authors**: Kuan-Ting Chen, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: With the prevalence of e-commence websites and the ease of online shopping, consumers are embracing huge amounts of various options in products. Undeniably, shopping is one of the most essential activities in our society and studying consumer's shopping behavior is important for the industry as well as sociology and psychology. Indisputable, one of the most popular e-commerce categories is clothing business. There arises the needs for analysis of popular and attractive clothing features which could further boost many emerging applications, such as clothing recommendation and advertising. In this work, we design a novel system that consists of three major components: 1) exploring and organizing a large-scale clothing dataset from a online shopping website, 2) pruning and extracting images of best-selling products in clothing item data and user transaction history, and 3) utilizing a machine learning based approach to discovering fine-grained clothing attributes as the representative and discriminative characteristics of popular clothing style elements. Through the experiments over a large-scale online clothing shopping dataset, we demonstrate the effectiveness of our proposed system, and obtain useful insights on clothing consumption trends and profitable clothing features.



