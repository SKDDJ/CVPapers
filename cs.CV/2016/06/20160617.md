# Arxiv Papers in cs.CV on 2016-06-17
### CMS-RCNN: Contextual Multi-Scale Region-based CNN for Unconstrained Face Detection
- **Arxiv ID**: http://arxiv.org/abs/1606.05413v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.05413v1)
- **Published**: 2016-06-17 03:19:09+00:00
- **Updated**: 2016-06-17 03:19:09+00:00
- **Authors**: Chenchen Zhu, Yutong Zheng, Khoa Luu, Marios Savvides
- **Comment**: None
- **Journal**: None
- **Summary**: Robust face detection in the wild is one of the ultimate components to support various facial related problems, i.e. unconstrained face recognition, facial periocular recognition, facial landmarking and pose estimation, facial expression recognition, 3D facial model construction, etc. Although the face detection problem has been intensely studied for decades with various commercial applications, it still meets problems in some real-world scenarios due to numerous challenges, e.g. heavy facial occlusions, extremely low resolutions, strong illumination, exceptionally pose variations, image or video compression artifacts, etc. In this paper, we present a face detection approach named Contextual Multi-Scale Region-based Convolution Neural Network (CMS-RCNN) to robustly solve the problems mentioned above. Similar to the region-based CNNs, our proposed network consists of the region proposal component and the region-of-interest (RoI) detection component. However, far apart of that network, there are two main contributions in our proposed network that play a significant role to achieve the state-of-the-art performance in face detection. Firstly, the multi-scale information is grouped both in region proposal and RoI detection to deal with tiny face regions. Secondly, our proposed network allows explicit body contextual reasoning in the network inspired from the intuition of human vision system. The proposed approach is benchmarked on two recent challenging face detection databases, i.e. the WIDER FACE Dataset which contains high degree of variability, as well as the Face Detection Dataset and Benchmark (FDDB). The experimental results show that our proposed approach trained on WIDER FACE Dataset outperforms strong baselines on WIDER FACE Dataset by a large margin, and consistently achieves competitive results on FDDB against the recent state-of-the-art face detection methods.



### Multi-feature combined cloud and cloud shadow detection in GaoFen-1 wide field of view imagery
- **Arxiv ID**: http://arxiv.org/abs/1606.05415v4
- **DOI**: 10.1016/j.rse.2017.01.026
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.05415v4)
- **Published**: 2016-06-17 03:26:54+00:00
- **Updated**: 2017-02-05 04:59:29+00:00
- **Authors**: Zhiwei Li, Huanfeng Shen, Huifang Li, Guisong Xia, Paolo Gamba, Liangpei Zhang
- **Comment**: This manuscript has been accepted for publication in Remote Sensing
  of Environment, vol. 191, pp.342-358, 2017.
  (http://www.sciencedirect.com/science/article/pii/S003442571730038X)
- **Journal**: Remote Sensing of Environment, vol. 191, pp.342-358, 2017
- **Summary**: The wide field of view (WFV) imaging system onboard the Chinese GaoFen-1 (GF-1) optical satellite has a 16-m resolution and four-day revisit cycle for large-scale Earth observation. The advantages of the high temporal-spatial resolution and the wide field of view make the GF-1 WFV imagery very popular. However, cloud cover is an inevitable problem in GF-1 WFV imagery, which influences its precise application. Accurate cloud and cloud shadow detection in GF-1 WFV imagery is quite difficult due to the fact that there are only three visible bands and one near-infrared band. In this paper, an automatic multi-feature combined (MFC) method is proposed for cloud and cloud shadow detection in GF-1 WFV imagery. The MFC algorithm first implements threshold segmentation based on the spectral features and mask refinement based on guided filtering to generate a preliminary cloud mask. The geometric features are then used in combination with the texture features to improve the cloud detection results and produce the final cloud mask. Finally, the cloud shadow mask can be acquired by means of the cloud and shadow matching and follow-up correction process. The method was validated using 108 globally distributed scenes. The results indicate that MFC performs well under most conditions, and the average overall accuracy of MFC cloud detection is as high as 96.8%. In the contrastive analysis with the official provided cloud fractions, MFC shows a significant improvement in cloud fraction estimation, and achieves a high accuracy for the cloud and cloud shadow detection in the GF-1 WFV imagery with fewer spectral bands. The proposed method could be used as a preprocessing step in the future to monitor land-cover change, and it could also be easily extended to other optical satellite imagery which has a similar spectral setting.



### DecomposeMe: Simplifying ConvNets for End-to-End Learning
- **Arxiv ID**: http://arxiv.org/abs/1606.05426v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.05426v1)
- **Published**: 2016-06-17 06:48:12+00:00
- **Updated**: 2016-06-17 06:48:12+00:00
- **Authors**: Jose Alvarez, Lars Petersson
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning and convolutional neural networks (ConvNets) have been successfully applied to most relevant tasks in the computer vision community. However, these networks are computationally demanding and not suitable for embedded devices where memory and time consumption are relevant.   In this paper, we propose DecomposeMe, a simple but effective technique to learn features using 1D convolutions. The proposed architecture enables both simplicity and filter sharing leading to increased learning capacity. A comprehensive set of large-scale experiments on ImageNet and Places2 demonstrates the ability of our method to improve performance while significantly reducing the number of parameters required. Notably, on Places2, we obtain an improvement in relative top-1 classification accuracy of 7.7\% with an architecture that requires 92% fewer parameters compared to VGG-B. The proposed network is also demonstrated to generalize to other tasks by converting existing networks.



### FVQA: Fact-based Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1606.05433v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.05433v4)
- **Published**: 2016-06-17 07:37:32+00:00
- **Updated**: 2017-08-08 05:10:20+00:00
- **Authors**: Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, Anthony Dick
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) has attracted a lot of attention in both Computer Vision and Natural Language Processing communities, not least because it offers insight into the relationships between two important sources of information. Current datasets, and the models built upon them, have focused on questions which are answerable by direct analysis of the question and image alone. The set of such questions that require no external information to answer is interesting, but very limited. It excludes questions which require common sense, or basic factual knowledge to answer, for example. Here we introduce FVQA, a VQA dataset which requires, and supports, much deeper reasoning. FVQA only contains questions which require external information to answer.   We thus extend a conventional visual question answering dataset, which contains image-question-answerg triplets, through additional image-question-answer-supporting fact tuples. The supporting fact is represented as a structural triplet, such as <Cat,CapableOf,ClimbingTrees>.   We evaluate several baseline models on the FVQA dataset, and describe a novel model which is capable of reasoning about an image on the basis of supporting facts.



### YodaNN: An Architecture for Ultra-Low Power Binary-Weight CNN Acceleration
- **Arxiv ID**: http://arxiv.org/abs/1606.05487v4
- **DOI**: None
- **Categories**: **cs.AR**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1606.05487v4)
- **Published**: 2016-06-17 11:48:29+00:00
- **Updated**: 2017-02-24 08:46:12+00:00
- **Authors**: Renzo Andri, Lukas Cavigelli, Davide Rossi, Luca Benini
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have revolutionized the world of computer vision over the last few years, pushing image classification beyond human accuracy. The computational effort of today's CNNs requires power-hungry parallel processors or GP-GPUs. Recent developments in CNN accelerators for system-on-chip integration have reduced energy consumption significantly. Unfortunately, even these highly optimized devices are above the power envelope imposed by mobile and deeply embedded applications and face hard limitations caused by CNN weight I/O and storage. This prevents the adoption of CNNs in future ultra-low power Internet of Things end-nodes for near-sensor analytics. Recent algorithmic and theoretical advancements enable competitive classification accuracy even when limiting CNNs to binary (+1/-1) weights during training. These new findings bring major optimization opportunities in the arithmetic core by removing the need for expensive multiplications, as well as reducing I/O bandwidth and storage. In this work, we present an accelerator optimized for binary-weight CNNs that achieves 1510 GOp/s at 1.2 V on a core area of only 1.33 MGE (Million Gate Equivalent) or 0.19 mm$^2$ and with a power dissipation of 895 {\mu}W in UMC 65 nm technology at 0.6 V. Our accelerator significantly outperforms the state-of-the-art in terms of energy and area efficiency achieving 61.2 TOp/s/W@0.6 V and 1135 GOp/s/MGE@1.2 V, respectively.



### Learning Abstract Classes using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1606.05506v1
- **DOI**: 10.4108/eai.3-12-2015.2262468
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1606.05506v1)
- **Published**: 2016-06-17 12:51:23+00:00
- **Updated**: 2016-06-17 12:51:23+00:00
- **Authors**: Sebastian Stabinger, Antonio Rodriguez-Sanchez, Justus Piater
- **Comment**: To be published in the proceedings of the International Conference on
  Bio-inspired Information and Communications Technologies 2015
- **Journal**: None
- **Summary**: Humans are generally good at learning abstract concepts about objects and scenes (e.g.\ spatial orientation, relative sizes, etc.). Over the last years convolutional neural networks have achieved almost human performance in recognizing concrete classes (i.e.\ specific object categories). This paper tests the performance of a current CNN (GoogLeNet) on the task of differentiating between abstract classes which are trivially differentiable for humans. We trained and tested the CNN on the two abstract classes of horizontal and vertical orientation and determined how well the network is able to transfer the learned classes to other, previously unseen objects.



### Tensor Ring Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1606.05535v1
- **DOI**: None
- **Categories**: **cs.NA**, cs.CV, cs.DS
- **Links**: [PDF](http://arxiv.org/pdf/1606.05535v1)
- **Published**: 2016-06-17 14:40:18+00:00
- **Updated**: 2016-06-17 14:40:18+00:00
- **Authors**: Qibin Zhao, Guoxu Zhou, Shengli Xie, Liqing Zhang, Andrzej Cichocki
- **Comment**: None
- **Journal**: None
- **Summary**: Tensor networks have in recent years emerged as the powerful tools for solving the large-scale optimization problems. One of the most popular tensor network is tensor train (TT) decomposition that acts as the building blocks for the complicated tensor networks. However, the TT decomposition highly depends on permutations of tensor dimensions, due to its strictly sequential multilinear products over latent cores, which leads to difficulties in finding the optimal TT representation. In this paper, we introduce a fundamental tensor decomposition model to represent a large dimensional tensor by a circular multilinear products over a sequence of low dimensional cores, which can be graphically interpreted as a cyclic interconnection of 3rd-order tensors, and thus termed as tensor ring (TR) decomposition. The key advantage of TR model is the circular dimensional permutation invariance which is gained by employing the trace operation and treating the latent cores equivalently. TR model can be viewed as a linear combination of TT decompositions, thus obtaining the powerful and generalized representation abilities. For optimization of latent cores, we present four different algorithms based on the sequential SVDs, ALS scheme, and block-wise ALS techniques. Furthermore, the mathematical properties of TR model are investigated, which shows that the basic multilinear algebra can be performed efficiently by using TR representaions and the classical tensor decompositions can be conveniently transformed into the TR representation. Finally, the experiments on both synthetic signals and real-world datasets were conducted to evaluate the performance of different algorithms.



### Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?
- **Arxiv ID**: http://arxiv.org/abs/1606.05589v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1606.05589v1)
- **Published**: 2016-06-17 17:00:02+00:00
- **Updated**: 2016-06-17 17:00:02+00:00
- **Authors**: Abhishek Das, Harsh Agrawal, C. Lawrence Zitnick, Devi Parikh, Dhruv Batra
- **Comment**: 5 pages, 4 figures, 3 tables, presented at 2016 ICML Workshop on
  Human Interpretability in Machine Learning (WHI 2016), New York, NY. arXiv
  admin note: substantial text overlap with arXiv:1606.03556
- **Journal**: None
- **Summary**: We conduct large-scale studies on `human attention' in Visual Question Answering (VQA) to understand where humans choose to look to answer questions about images. We design and test multiple game-inspired novel attention-annotation interfaces that require the subject to sharpen regions of a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human ATtention) dataset. We evaluate attention maps generated by state-of-the-art VQA models against human attention both qualitatively (via visualizations) and quantitatively (via rank-order correlation). Overall, our experiments show that current attention models in VQA do not seem to be looking at the same regions as humans.



### High-resolution LIDAR-based Depth Mapping using Bilateral Filter
- **Arxiv ID**: http://arxiv.org/abs/1606.05614v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.05614v1)
- **Published**: 2016-06-17 18:14:59+00:00
- **Updated**: 2016-06-17 18:14:59+00:00
- **Authors**: C. Premebida, L. Garrote, A. Asvadi, A. Pedro Ribeiro, U. Nunes
- **Comment**: 8 pages, 6 figures, submitted to IEEE-ITSC'16
- **Journal**: None
- **Summary**: High resolution depth-maps, obtained by upsampling sparse range data from a 3D-LIDAR, find applications in many fields ranging from sensory perception to semantic segmentation and object detection. Upsampling is often based on combining data from a monocular camera to compensate the low-resolution of a LIDAR. This paper, on the other hand, introduces a novel framework to obtain dense depth-map solely from a single LIDAR point cloud; which is a research direction that has been barely explored. The formulation behind the proposed depth-mapping process relies on local spatial interpolation, using sliding-window (mask) technique, and on the Bilateral Filter (BF) where the variable of interest, the distance from the sensor, is considered in the interpolation problem. In particular, the BF is conveniently modified to perform depth-map upsampling such that the edges (foreground-background discontinuities) are better preserved by means of a proposed method which influences the range-based weighting term. Other methods for spatial upsampling are discussed, evaluated and compared in terms of different error measures. This paper also researches the role of the mask's size in the performance of the implemented methods. Quantitative and qualitative results from experiments on the KITTI Database, using LIDAR point clouds only, show very satisfactory performance of the approach introduced in this work.



### DeepFood: Deep Learning-Based Food Image Recognition for Computer-Aided Dietary Assessment
- **Arxiv ID**: http://arxiv.org/abs/1606.05675v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.05675v1)
- **Published**: 2016-06-17 21:03:19+00:00
- **Updated**: 2016-06-17 21:03:19+00:00
- **Authors**: Chang Liu, Yu Cao, Yan Luo, Guanling Chen, Vinod Vokkarane, Yunsheng Ma
- **Comment**: 12 pages, 2 figures, 6 tables, ICOST 2016
- **Journal**: None
- **Summary**: Worldwide, in 2014, more than 1.9 billion adults, 18 years and older, were overweight. Of these, over 600 million were obese. Accurately documenting dietary caloric intake is crucial to manage weight loss, but also presents challenges because most of the current methods for dietary assessment must rely on memory to recall foods eaten. The ultimate goal of our research is to develop computer-aided technical solutions to enhance and improve the accuracy of current measurements of dietary intake. Our proposed system in this paper aims to improve the accuracy of dietary assessment by analyzing the food images captured by mobile devices (e.g., smartphone). The key technique innovation in this paper is the deep learning-based food image recognition algorithms. Substantial research has demonstrated that digital imaging accurately estimates dietary intake in many environments and it has many advantages over other methods. However, how to derive the food information (e.g., food type and portion size) from food image effectively and efficiently remains a challenging and open research problem. We propose a new Convolutional Neural Network (CNN)-based food image recognition algorithm to address this problem. We applied our proposed approach to two real-world food image data sets (UEC-256 and Food-101) and achieved impressive results. To the best of our knowledge, these results outperformed all other reported work using these two data sets. Our experiments have demonstrated that the proposed approach is a promising solution for addressing the food image recognition problem. Our future work includes further improving the performance of the algorithms and integrating our system into a real-world mobile and cloud computing-based system to enhance the accuracy of current measurements of dietary intake.



### Hierarchical Data Generator based on Tree-Structured Stick Breaking Process for Benchmarking Clustering Methods
- **Arxiv ID**: http://arxiv.org/abs/1606.05681v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.05681v3)
- **Published**: 2016-06-17 21:21:15+00:00
- **Updated**: 2020-04-04 23:45:18+00:00
- **Authors**: Łukasz P. Olech, Michał Spytkowski, Halina Kwaśnicka, Zbigniew Michalewicz
- **Comment**: This article was submitted to Elsevier Journal
- **Journal**: None
- **Summary**: Object Cluster Hierarchies is a new variant of Hierarchical Cluster Analysis that gains interest in the field of Machine Learning. Being still at an early stage of development, the lack of tools for systematic analysis of Object Cluster Hierarchies inhibits its further improvement. In this paper we address this issue by proposing a generator of synthetic hierarchical data that can be used for benchmarking Object Cluster Hierarchy methods. The article presents a thorough empirical and theoretical analysis of the generator and provides guidance on how to control its parameters. Conducted experiments show the usefulness of the data generator that is capable of producing a wide range of differently structured data. Further, benchmarking datasets that mirror the most common types of hierarchies are generated and made available to the public, together with the developed generator (http://kio.pwr.edu.pl/?page\_id=396).



### A Survey of Pansharpening Methods with A New Band-Decoupled Variational Model
- **Arxiv ID**: http://arxiv.org/abs/1606.05703v1
- **DOI**: 10.1016/j.isprsjprs.2016.12.013
- **Categories**: **cs.CV**, math.OC, 15A29, 65F22, 65K10, 68U10, 90C25, 90C46
- **Links**: [PDF](http://arxiv.org/pdf/1606.05703v1)
- **Published**: 2016-06-17 23:12:32+00:00
- **Updated**: 2016-06-17 23:12:32+00:00
- **Authors**: Joan Duran, Antoni Buades, Bartomeu Coll, Catalina Sbert, Gwendoline Blanchet
- **Comment**: None
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing, vol. 125, pp.
  78-105, 2017
- **Summary**: Most satellites decouple the acquisition of a panchromatic image at high spatial resolution from the acquisition of a multispectral image at lower spatial resolution. Pansharpening is a fusion technique used to increase the spatial resolution of the multispectral data while simultaneously preserving its spectral information. In this paper, we consider pansharpening as an optimization problem minimizing a cost function with a nonlocal regularization term. The energy functional which is to be minimized decouples for each band, thus permitting the application to misregistered spectral components. This requirement is achieved by dropping the, commonly used, assumption that relates the spectral and panchromatic modalities by a linear transformation. Instead, a new constraint that preserves the radiometric ratio between the panchromatic and each spectral component is introduced. An exhaustive performance comparison of the proposed fusion method with several classical and state-of-the-art pansharpening techniques illustrates its superiority in preserving spatial details, reducing color distortions, and avoiding the creation of aliasing artifacts.



### Strategies for Searching Video Content with Text Queries or Video Examples
- **Arxiv ID**: http://arxiv.org/abs/1606.05705v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1606.05705v1)
- **Published**: 2016-06-17 23:27:06+00:00
- **Updated**: 2016-06-17 23:27:06+00:00
- **Authors**: Shoou-I Yu, Yi Yang, Zhongwen Xu, Shicheng Xu, Deyu Meng, Zexi Mao, Zhigang Ma, Ming Lin, Xuanchong Li, Huan Li, Zhenzhong Lan, Lu Jiang, Alexander G. Hauptmann, Chuang Gan, Xingzhong Du, Xiaojun Chang
- **Comment**: None
- **Journal**: None
- **Summary**: The large number of user-generated videos uploaded on to the Internet everyday has led to many commercial video search engines, which mainly rely on text metadata for search. However, metadata is often lacking for user-generated videos, thus these videos are unsearchable by current search engines. Therefore, content-based video retrieval (CBVR) tackles this metadata-scarcity problem by directly analyzing the visual and audio streams of each video. CBVR encompasses multiple research topics, including low-level feature design, feature fusion, semantic detector training and video search/reranking. We present novel strategies in these topics to enhance CBVR in both accuracy and speed under different query inputs, including pure textual queries and query by video examples. Our proposed strategies have been incorporated into our submission for the TRECVID 2014 Multimedia Event Detection evaluation, where our system outperformed other submissions in both text queries and video example queries, thus demonstrating the effectiveness of our proposed approaches.



