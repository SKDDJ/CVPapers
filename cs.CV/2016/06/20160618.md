# Arxiv Papers in cs.CV on 2016-06-18
### Deep Learning for Identifying Metastatic Breast Cancer
- **Arxiv ID**: http://arxiv.org/abs/1606.05718v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1606.05718v1)
- **Published**: 2016-06-18 04:00:31+00:00
- **Updated**: 2016-06-18 04:00:31+00:00
- **Authors**: Dayong Wang, Aditya Khosla, Rishab Gargeya, Humayun Irshad, Andrew H. Beck
- **Comment**: None
- **Journal**: None
- **Summary**: The International Symposium on Biomedical Imaging (ISBI) held a grand challenge to evaluate computational systems for the automated detection of metastatic breast cancer in whole slide images of sentinel lymph node biopsies. Our team won both competitions in the grand challenge, obtaining an area under the receiver operating curve (AUC) of 0.925 for the task of whole slide image classification and a score of 0.7051 for the tumor localization task. A pathologist independently reviewed the same images, obtaining a whole slide image classification AUC of 0.966 and a tumor localization score of 0.733. Combining our deep learning system's predictions with the human pathologist's diagnoses increased the pathologist's AUC to 0.995, representing an approximately 85 percent reduction in human error rate. These results demonstrate the power of using deep learning to produce significant improvements in the accuracy of pathological diagnoses.



### RRV: A Spatiotemporal Descriptor for Rigid Body Motion Recognition
- **Arxiv ID**: http://arxiv.org/abs/1606.05729v2
- **DOI**: 10.1109/TCYB.2017.2705227
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1606.05729v2)
- **Published**: 2016-06-18 08:10:59+00:00
- **Updated**: 2017-06-04 14:53:15+00:00
- **Authors**: Yao Guo, Youfu Li, Zhanpeng Shao
- **Comment**: To be published in the future issue of IEEE Trans. on Cybernetics
- **Journal**: None
- **Summary**: Motion behaviors of a rigid body can be characterized by a 6-dimensional motion trajectory, which contains position vectors of a reference point on the rigid body and rotations of this rigid body over time. This paper devises a Rotation and Relative Velocity (RRV) descriptor by exploring the local translational and rotational invariants of motion trajectories of rigid bodies, which is insensitive to noise, invariant to rigid transformation and scaling. A flexible metric is also introduced to measure the distance between two RRV descriptors. The RRV descriptor is then applied to characterize motions of a human body skeleton modeled as articulated interconnections of multiple rigid bodies. To illustrate the descriptive ability of the RRV descriptor, we explore it for different rigid body motion recognition tasks. The experimental results on benchmark datasets demonstrate that this simple RRV descriptor outperforms the previous ones regarding recognition accuracy without increasing computational cost.



### Online and Offline Handwritten Chinese Character Recognition: A Comprehensive Study and New Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1606.05763v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.05763v1)
- **Published**: 2016-06-18 14:49:32+00:00
- **Updated**: 2016-06-18 14:49:32+00:00
- **Authors**: Xu-Yao Zhang, Yoshua Bengio, Cheng-Lin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent deep learning based methods have achieved the state-of-the-art performance for handwritten Chinese character recognition (HCCR) by learning discriminative representations directly from raw data. Nevertheless, we believe that the long-and-well investigated domain-specific knowledge should still help to boost the performance of HCCR. By integrating the traditional normalization-cooperated direction-decomposed feature map (directMap) with the deep convolutional neural network (convNet), we are able to obtain new highest accuracies for both online and offline HCCR on the ICDAR-2013 competition database. With this new framework, we can eliminate the needs for data augmentation and model ensemble, which are widely used in other systems to achieve their best results. This makes our framework to be efficient and effective for both training and testing. Furthermore, although directMap+convNet can achieve the best results and surpass human-level performance, we show that writer adaptation in this case is still effective. A new adaptation layer is proposed to reduce the mismatch between training and test data on a particular source layer. The adaptation process can be efficiently and effectively implemented in an unsupervised manner. By adding the adaptation layer into the pre-trained convNet, it can adapt to the new handwriting styles of particular writers, and the recognition accuracy can be further improved consistently and significantly. This paper gives an overview and comparison of recent deep learning based approaches for HCCR, and also sets new benchmarks for both online and offline HCCR.



### Automatic 3D Reconstruction for Symmetric Shapes
- **Arxiv ID**: http://arxiv.org/abs/1606.05785v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1606.05785v1)
- **Published**: 2016-06-18 17:42:27+00:00
- **Updated**: 2016-06-18 17:42:27+00:00
- **Authors**: Atishay Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Generic 3D reconstruction from a single image is a difficult problem. A lot of data loss occurs in the projection. A domain based approach to reconstruction where we solve a smaller set of problems for a particular use case lead to greater returns. The project provides a way to automatically generate full 3-D renditions of actual symmetric images that have some prior information provided in the pipeline by a recognition algorithm. We provide a critical analysis on how this can be enhanced and improved to provide a general reconstruction framework for automatic reconstruction for any symmetric shape.



### Eye Tracking for Everyone
- **Arxiv ID**: http://arxiv.org/abs/1606.05814v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.05814v1)
- **Published**: 2016-06-18 23:53:54+00:00
- **Updated**: 2016-06-18 23:53:54+00:00
- **Authors**: Kyle Krafka, Aditya Khosla, Petr Kellnhofer, Harini Kannan, Suchendra Bhandarkar, Wojciech Matusik, Antonio Torralba
- **Comment**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2016
- **Journal**: None
- **Summary**: From scientific research to commercial applications, eye tracking is an important tool across many domains. Despite its range of applications, eye tracking has yet to become a pervasive technology. We believe that we can put the power of eye tracking in everyone's palm by building eye tracking software that works on commodity hardware such as mobile phones and tablets, without the need for additional sensors or devices. We tackle this problem by introducing GazeCapture, the first large-scale dataset for eye tracking, containing data from over 1450 people consisting of almost 2.5M frames. Using GazeCapture, we train iTracker, a convolutional neural network for eye tracking, which achieves a significant reduction in error over previous approaches while running in real time (10-15fps) on a modern mobile device. Our model achieves a prediction error of 1.71cm and 2.53cm without calibration on mobile phones and tablets respectively. With calibration, this is reduced to 1.34cm and 2.12cm. Further, we demonstrate that the features learned by iTracker generalize well to other datasets, achieving state-of-the-art results. The code, data, and models are available at http://gazecapture.csail.mit.edu.



