# Arxiv Papers in cs.CV on 2016-06-03
### Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet
- **Arxiv ID**: http://arxiv.org/abs/1606.00972v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1606.00972v2)
- **Published**: 2016-06-03 05:36:06+00:00
- **Updated**: 2017-05-29 23:26:38+00:00
- **Authors**: Jianwen Xie, Song-Chun Zhu, Ying Nian Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Video sequences contain rich dynamic patterns, such as dynamic texture patterns that exhibit stationarity in the temporal domain, and action patterns that are non-stationary in either spatial or temporal domain. We show that a spatial-temporal generative ConvNet can be used to model and synthesize dynamic patterns. The model defines a probability distribution on the video sequence, and the log probability is defined by a spatial-temporal ConvNet that consists of multiple layers of spatial-temporal filters to capture spatial-temporal patterns of different scales. The model can be learned from the training video sequences by an "analysis by synthesis" learning algorithm that iterates the following two steps. Step 1 synthesizes video sequences from the currently learned model. Step 2 then updates the model parameters based on the difference between the synthesized video sequences and the observed training sequences. We show that the learning algorithm can synthesize realistic dynamic patterns.



### On Recognizing Transparent Objects in Domestic Environments Using Fusion of Multiple Sensor Modalities
- **Arxiv ID**: http://arxiv.org/abs/1606.01001v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.01001v1)
- **Published**: 2016-06-03 08:08:30+00:00
- **Updated**: 2016-06-03 08:08:30+00:00
- **Authors**: Alexander Hagg, Frederik Hegger, Paul Plöger
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Current object recognition methods fail on object sets that include both diffuse, reflective and transparent materials, although they are very common in domestic scenarios. We show that a combination of cues from multiple sensor modalities, including specular reflectance and unavailable depth information, allows us to capture a larger subset of household objects by extending a state of the art object recognition method. This leads to a significant increase in robustness of recognition over a larger set of commonly used objects.



### Optically lightweight tracking of objects around a corner
- **Arxiv ID**: http://arxiv.org/abs/1606.01873v1
- **DOI**: 10.1038/srep32491
- **Categories**: **cs.CV**, cs.GR, physics.optics, I.3.7; I.3.8; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1606.01873v1)
- **Published**: 2016-06-03 09:17:02+00:00
- **Updated**: 2016-06-03 09:17:02+00:00
- **Authors**: Jonathan Klein, Christoph Peters, Jaime Martín, Martin Laurenzis, Matthias B. Hullin
- **Comment**: 15 pages, 6 figures
- **Journal**: None
- **Summary**: The observation of objects located in inaccessible regions is a recurring challenge in a wide variety of important applications. Recent work has shown that indirect diffuse light reflections can be used to reconstruct objects and two-dimensional (2D) patterns around a corner. However, these prior methods always require some specialized setup involving either ultrafast detectors or narrowband light sources. Here we show that occluded objects can be tracked in real time using a standard 2D camera and a laser pointer. Unlike previous methods based on the backprojection approach, we formulate the problem in an analysis-by-synthesis sense. By repeatedly simulating light transport through the scene, we determine the set of object parameters that most closely fits the measured intensity distribution. We experimentally demonstrate that this approach is capable of following the translation of unknown objects, and translation and orientation of a known object, in real time.



### Automatic Separation of Compound Figures in Scientific Articles
- **Arxiv ID**: http://arxiv.org/abs/1606.01021v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1606.01021v2)
- **Published**: 2016-06-03 09:53:01+00:00
- **Updated**: 2016-10-18 06:26:58+00:00
- **Authors**: Mario Taschwer, Oge Marques
- **Comment**: accepted for Multimedia Tools and Applications with minor revisions
- **Journal**: None
- **Summary**: Content-based analysis and retrieval of digital images found in scientific articles is often hindered by images consisting of multiple subfigures (compound figures). We address this problem by proposing a method to automatically classify and separate compound figures, which consists of two main steps: (i) a supervised compound figure classifier (CFC) discriminates between compound and non-compound figures using task-specific image features; and (ii) an image processing algorithm is applied to predicted compound images to perform compound figure separation (CFS). Our CFC approach is shown to achieve state-of-the-art classification performance on a published dataset. Our CFS algorithm shows superior separation accuracy on two different datasets compared to other known automatic approaches. Finally, we propose a method to evaluate the effectiveness of the CFC-CFS process chain and use it to optimize the misclassification loss of CFC for maximal effectiveness in the process chain.



### Learning under Distributed Weak Supervision
- **Arxiv ID**: http://arxiv.org/abs/1606.01100v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.01100v1)
- **Published**: 2016-06-03 14:28:28+00:00
- **Updated**: 2016-06-03 14:28:28+00:00
- **Authors**: Martin Rajchl, Matthew C. H. Lee, Franklin Schrans, Alice Davidson, Jonathan Passerat-Palmbach, Giacomo Tarroni, Amir Alansary, Ozan Oktay, Bernhard Kainz, Daniel Rueckert
- **Comment**: None
- **Journal**: None
- **Summary**: The availability of training data for supervision is a frequently encountered bottleneck of medical image analysis methods. While typically established by a clinical expert rater, the increase in acquired imaging data renders traditional pixel-wise segmentations less feasible. In this paper, we examine the use of a crowdsourcing platform for the distribution of super-pixel weak annotation tasks and collect such annotations from a crowd of non-expert raters. The crowd annotations are subsequently used for training a fully convolutional neural network to address the problem of fetal brain segmentation in T2-weighted MR images. Using this approach we report encouraging results compared to highly targeted, fully supervised methods and potentially address a frequent problem impeding image analysis research.



### Generalizing the Convolution Operator to extend CNNs to Irregular Domains
- **Arxiv ID**: http://arxiv.org/abs/1606.01166v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1606.01166v4)
- **Published**: 2016-06-03 16:18:22+00:00
- **Updated**: 2017-10-25 12:28:26+00:00
- **Authors**: Jean-Charles Vialatte, Vincent Gripon, Grégoire Mercier
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have become the state-of-the-art in supervised learning vision tasks. Their convolutional filters are of paramount importance for they allow to learn patterns while disregarding their locations in input images. When facing highly irregular domains, generalized convolutional operators based on an underlying graph structure have been proposed. However, these operators do not exactly match standard ones on grid graphs, and introduce unwanted additional invariance (e.g. with regards to rotations). We propose a novel approach to generalize CNNs to irregular domains using weight sharing and graph-based operators. Using experiments, we show that these models resemble CNNs on regular domains and offer better performance than multilayer perceptrons on distorded ones.



### How Deep is the Feature Analysis underlying Rapid Visual Categorization?
- **Arxiv ID**: http://arxiv.org/abs/1606.01167v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.01167v1)
- **Published**: 2016-06-03 16:21:07+00:00
- **Updated**: 2016-06-03 16:21:07+00:00
- **Authors**: Sven Eberhardt, Jonah Cader, Thomas Serre
- **Comment**: Sven Eberhardt and Jonah Cader contributed equally
- **Journal**: None
- **Summary**: Rapid categorization paradigms have a long history in experimental psychology: Characterized by short presentation times and speedy behavioral responses, these tasks highlight the efficiency with which our visual system processes natural object categories. Previous studies have shown that feed-forward hierarchical models of the visual cortex provide a good fit to human visual decisions. At the same time, recent work in computer vision has demonstrated significant gains in object recognition accuracy with increasingly deep hierarchical architectures. But it is unclear how well these models account for human visual decisions and what they may reveal about the underlying brain processes.   We have conducted a large-scale psychophysics study to assess the correlation between computational models and human participants on a rapid animal vs. non-animal categorization task. We considered visual representations of varying complexity by analyzing the output of different stages of processing in three state-of-the-art deep networks. We found that recognition accuracy increases with higher stages of visual processing (higher level stages indeed outperforming human participants on the same task) but that human decisions agree best with predictions from intermediate stages.   Overall, these results suggest that human participants may rely on visual features of intermediate complexity and that the complexity of visual representations afforded by modern deep network models may exceed those used by human participants during rapid categorization.



### Reinforcement Learning for Semantic Segmentation in Indoor Scenes
- **Arxiv ID**: http://arxiv.org/abs/1606.01178v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1606.01178v1)
- **Published**: 2016-06-03 16:35:58+00:00
- **Updated**: 2016-06-03 16:35:58+00:00
- **Authors**: Md. Alimoor Reza, Jana Kosecka
- **Comment**: None
- **Journal**: None
- **Summary**: Future advancements in robot autonomy and sophistication of robotics tasks rest on robust, efficient, and task-dependent semantic understanding of the environment. Semantic segmentation is the problem of simultaneous segmentation and categorization of a partition of sensory data. The majority of current approaches tackle this using multi-class segmentation and labeling in a Conditional Random Field (CRF) framework or by generating multiple object hypotheses and combining them sequentially. In practical settings, the subset of semantic labels that are needed depend on the task and particular scene and labelling every single pixel is not always necessary. We pursue these observations in developing a more modular and flexible approach to multi-class parsing of RGBD data based on learning strategies for combining independent binary object-vs-background segmentations in place of the usual monolithic multi-label CRF approach. Parameters for the independent binary segmentation models can be learned very efficiently, and the combination strategy---learned using reinforcement learning---can be set independently and can vary over different tasks and environments. Accuracy is comparable to state-of-art methods on a subset of the NYU-V2 dataset of indoor scenes, while providing additional flexibility and modularity.



### Statistical Pattern Recognition for Driving Styles Based on Bayesian Probability and Kernel Density Estimation
- **Arxiv ID**: http://arxiv.org/abs/1606.01284v1
- **DOI**: 10.1049/iet-its.2017.0379
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1606.01284v1)
- **Published**: 2016-06-03 21:48:53+00:00
- **Updated**: 2016-06-03 21:48:53+00:00
- **Authors**: Wenshuo Wang, Junqiang Xi, Xiaohan Li
- **Comment**: 10 pages, 9 figures. Submitted to International Journal of Automotive
  Technology
- **Journal**: IET Intelligent Transportation Systems, 2018
- **Summary**: Driving styles have a great influence on vehicle fuel economy, active safety, and drivability. To recognize driving styles of path-tracking behaviors for different divers, a statistical pattern-recognition method is developed to deal with the uncertainty of driving styles or characteristics based on probability density estimation. First, to describe driver path-tracking styles, vehicle speed and throttle opening are selected as the discriminative parameters, and a conditional kernel density function of vehicle speed and throttle opening is built, respectively, to describe the uncertainty and probability of two representative driving styles, e.g., aggressive and normal. Meanwhile, a posterior probability of each element in feature vector is obtained using full Bayesian theory. Second, a Euclidean distance method is involved to decide to which class the driver should be subject instead of calculating the complex covariance between every two elements of feature vectors. By comparing the Euclidean distance between every elements in feature vector, driving styles are classified into seven levels ranging from low normal to high aggressive. Subsequently, to show benefits of the proposed pattern-recognition method, a cross-validated method is used, compared with a fuzzy logic-based pattern-recognition method. The experiment results show that the proposed statistical pattern-recognition method for driving styles based on kernel density estimation is more efficient and stable than the fuzzy logic-based method.



### Incorporating long-range consistency in CNN-based texture generation
- **Arxiv ID**: http://arxiv.org/abs/1606.01286v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.01286v2)
- **Published**: 2016-06-03 21:57:48+00:00
- **Updated**: 2016-11-05 01:11:15+00:00
- **Authors**: G. Berger, R. Memisevic
- **Comment**: None
- **Journal**: None
- **Summary**: Gatys et al. (2015) showed that pair-wise products of features in a convolutional network are a very effective representation of image textures. We propose a simple modification to that representation which makes it possible to incorporate long-range structure into image generation, and to render images that satisfy various symmetry constraints. We show how this can greatly improve rendering of regular textures and of images that contain other kinds of symmetric structure. We also present applications to inpainting and season transfer.



### RAISR: Rapid and Accurate Image Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/1606.01299v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.01299v3)
- **Published**: 2016-06-03 22:56:49+00:00
- **Updated**: 2016-10-04 21:22:51+00:00
- **Authors**: Yaniv Romano, John Isidoro, Peyman Milanfar
- **Comment**: Supplementary material can be found at https://goo.gl/D0ETxG
- **Journal**: None
- **Summary**: Given an image, we wish to produce an image of larger size with significantly more pixels and higher image quality. This is generally known as the Single Image Super-Resolution (SISR) problem. The idea is that with sufficient training data (corresponding pairs of low and high resolution images) we can learn set of filters (i.e. a mapping) that when applied to given image that is not in the training set, will produce a higher resolution version of it, where the learning is preferably low complexity. In our proposed approach, the run-time is more than one to two orders of magnitude faster than the best competing methods currently available, while producing results comparable or better than state-of-the-art.   A closely related topic is image sharpening and contrast enhancement, i.e., improving the visual quality of a blurry image by amplifying the underlying details (a wide range of frequencies). Our approach additionally includes an extremely efficient way to produce an image that is significantly sharper than the input blurry one, without introducing artifacts such as halos and noise amplification. We illustrate how this effective sharpening algorithm, in addition to being of independent interest, can be used as a pre-processing step to induce the learning of more effective upscaling filters with built-in sharpening and contrast enhancement effect.



### Scene Grammars, Factor Graphs, and Belief Propagation
- **Arxiv ID**: http://arxiv.org/abs/1606.01307v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1606.01307v3)
- **Published**: 2016-06-03 23:49:02+00:00
- **Updated**: 2019-08-13 16:06:15+00:00
- **Authors**: Jeroen Chua, Pedro F. Felzenszwalb
- **Comment**: None
- **Journal**: None
- **Summary**: We describe a general framework for probabilistic modeling of complex scenes and inference from ambiguous observations. The approach is motivated by applications in image analysis and is based on the use of priors defined by stochastic grammars. We define a class of grammars that capture relationships between the objects in a scene and provide important contextual cues for statistical inference. The distribution over scenes defined by a probabilistic scene grammar can be represented by a graphical model and this construction can be used for efficient inference with loopy belief propagation.   We show experimental results with two different applications. One application involves the reconstruction of binary contour maps. Another application involves detecting and localizing faces in images. In both applications the same framework leads to robust inference algorithms that can effectively combine local information to reason about a scene.



