# Arxiv Papers in cs.CV on 2016-06-15
### Natural Scene Character Recognition Using Robust PCA and Sparse Representation
- **Arxiv ID**: http://arxiv.org/abs/1606.04616v1
- **DOI**: 10.1109/DAS.2016.32
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.04616v1)
- **Published**: 2016-06-15 01:58:06+00:00
- **Updated**: 2016-06-15 01:58:06+00:00
- **Authors**: Zheng Zhang, Yong Xu, Cheng-Lin Liu
- **Comment**: The 12th IAPR International Workshop on Document Analysis Systems
  (DAS); The natural scene character image features used in this paper have
  been released at
  http://www.yongxu.org/Natural%20Scene%20Character%20Recognition%20Datasets.html
- **Journal**: None
- **Summary**: Natural scene character recognition is challenging due to the cluttered background, which is hard to separate from text. In this paper, we propose a novel method for robust scene character recognition. Specifically, we first use robust principal component analysis (PCA) to denoise character image by recovering the missing low-rank component and filtering out the sparse noise term, and then use a simple Histogram of oriented Gradient (HOG) to perform image feature extraction, and finally, use a sparse representation based classifier for recognition. In experiments on four public datasets, namely the Char74K dataset, ICADAR 2003 robust reading dataset, Street View Text (SVT) dataset and IIIT5K-word dataset, our method was demonstrated to be competitive with the state-of-the-art methods.



### High-speed real-time single-pixel microscopy based on Fourier sampling
- **Arxiv ID**: http://arxiv.org/abs/1606.05200v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1606.05200v1)
- **Published**: 2016-06-15 02:03:54+00:00
- **Updated**: 2016-06-15 02:03:54+00:00
- **Authors**: Qiang Guo, Hongwei Chen, Yuxi Wang, Yong Guo, Peng Liu, Xiurui Zhu, Zheng Cheng, Zhenming Yu, Minghua Chen, Sigang Yang, Shizhong Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Single-pixel cameras based on the concepts of compressed sensing (CS) leverage the inherent structure of images to retrieve them with far fewer measurements and operate efficiently over a significantly broader spectral range than conventional silicon-based cameras. Recently, photonic time-stretch (PTS) technique facilitates the emergence of high-speed single-pixel cameras. A significant breakthrough in imaging speed of single-pixel cameras enables observation of fast dynamic phenomena. However, according to CS theory, image reconstruction is an iterative process that consumes enormous amounts of computational time and cannot be performed in real time. To address this challenge, we propose a novel single-pixel imaging technique that can produce high-quality images through rapid acquisition of their effective spatial Fourier spectrum. We employ phase-shifting sinusoidal structured illumination instead of random illumination for spectrum acquisition and apply inverse Fourier transform to the obtained spectrum for image restoration. We evaluate the performance of our prototype system by recognizing quick response (QR) codes and flow cytometric screening of cells. A frame rate of 625 kHz and a compression ratio of 10% are experimentally demonstrated in accordance with the recognition rate of the QR code. An imaging flow cytometer enabling high-content screening with an unprecedented throughput of 100,000 cells/s is also demonstrated. For real-time imaging applications, the proposed single-pixel microscope can significantly reduce the time required for image reconstruction by two orders of magnitude, which can be widely applied in industrial quality control and label-free biomedical imaging.



### Watch What You Just Said: Image Captioning with Text-Conditional Attention
- **Arxiv ID**: http://arxiv.org/abs/1606.04621v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.04621v3)
- **Published**: 2016-06-15 02:26:22+00:00
- **Updated**: 2016-11-24 04:36:42+00:00
- **Authors**: Luowei Zhou, Chenliang Xu, Parker Koch, Jason J. Corso
- **Comment**: source code is available online
- **Journal**: None
- **Summary**: Attention mechanisms have attracted considerable interest in image captioning due to its powerful performance. However, existing methods use only visual content as attention and whether textual context can improve attention in image captioning remains unsolved. To explore this problem, we propose a novel attention mechanism, called \textit{text-conditional attention}, which allows the caption generator to focus on certain image features given previously generated text. To obtain text-related image features for our attention model, we adopt the guiding Long Short-Term Memory (gLSTM) captioning architecture with CNN fine-tuning. Our proposed method allows joint learning of the image embedding, text embedding, text-conditional attention and language model with one network architecture in an end-to-end manner. We perform extensive experiments on the MS-COCO dataset. The experimental results show that our method outperforms state-of-the-art captioning methods on various quantitative metrics as well as in human evaluation, which supports the use of our text-conditional attention in image captioning.



### Ego-Surfing: Person Localization in First-Person Videos Using Ego-Motion Signatures
- **Arxiv ID**: http://arxiv.org/abs/1606.04637v2
- **DOI**: 10.1109/TPAMI.2017.2771767
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.04637v2)
- **Published**: 2016-06-15 04:31:49+00:00
- **Updated**: 2017-11-29 01:16:46+00:00
- **Authors**: Ryo Yonetani, Kris M. Kitani, Yoichi Sato
- **Comment**: To appear in IEEE TPAMI
- **Journal**: None
- **Summary**: We envision a future time when wearable cameras are worn by the masses and recording first-person point-of-view videos of everyday life. While these cameras can enable new assistive technologies and novel research challenges, they also raise serious privacy concerns. For example, first-person videos passively recorded by wearable cameras will necessarily include anyone who comes into the view of a camera -- with or without consent. Motivated by these benefits and risks, we developed a self-search technique tailored to first-person videos. The key observation of our work is that the egocentric head motion of a target person (ie, the self) is observed both in the point-of-view video of the target and observer. The motion correlation between the target person's video and the observer's video can then be used to identify instances of the self uniquely. We incorporate this feature into the proposed approach that computes the motion correlation over densely-sampled trajectories to search for a target individual in observer videos. Our approach significantly improves self-search performance over several well-known face detectors and recognizers. Furthermore, we show how our approach can enable several practical applications such as privacy filtering, target video retrieval, and social group clustering.



### A practical local tomography reconstruction algorithm based on known subregion
- **Arxiv ID**: http://arxiv.org/abs/1606.04940v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1606.04940v1)
- **Published**: 2016-06-15 09:41:11+00:00
- **Updated**: 2016-06-15 09:41:11+00:00
- **Authors**: Pierre Paleo, Michel Desvignes, Alessandro Mirone
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new method to reconstruct data acquired in a local tomography setup. This method uses an initial reconstruction and refines it by correcting the low frequency artifacts known as the cupping effect. A basis of Gaussian functions is used to correct the initial reconstruction. The coefficients of this basis are iteratively optimized under the constraint of a known subregion. Using a coarse basis reduces the degrees of freedom of the problem while actually correcting the cupping effect. Simulations show that the known region constraint yields an unbiased reconstruction, in accordance to uniqueness theorems stated in local tomography.



### DeepProposals: Hunting Objects and Actions by Cascading Deep Convolutional Layers
- **Arxiv ID**: http://arxiv.org/abs/1606.04702v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.04702v1)
- **Published**: 2016-06-15 09:57:27+00:00
- **Updated**: 2016-06-15 09:57:27+00:00
- **Authors**: Amir Ghodrati, Ali Diba, Marco Pedersoli, Tinne Tuytelaars, Luc Van Gool
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: In this paper, a new method for generating object and action proposals in images and videos is proposed. It builds on activations of different convolutional layers of a pretrained CNN, combining the localization accuracy of the early layers with the high informative-ness (and hence recall) of the later layers. To this end, we build an inverse cascade that, going backward from the later to the earlier convolutional layers of the CNN, selects the most promising locations and refines them in a coarse-to-fine manner. The method is efficient, because i) it re-uses the same features extracted for detection, ii) it aggregates features using integral images, and iii) it avoids a dense evaluation of the proposals thanks to the use of the inverse coarse-to-fine cascade. The method is also accurate. We show that our DeepProposals outperform most of the previously proposed object proposal and action proposal approaches and, when plugged into a CNN-based object detector, produce state-of-the-art detection performance.



### Probe-based Rapid Hybrid Hyperspectral and Tissue Surface Imaging Aided by Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1606.04766v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.04766v1)
- **Published**: 2016-06-15 14:00:07+00:00
- **Updated**: 2016-06-15 14:00:07+00:00
- **Authors**: Jianyu Lin, Neil T. Clancy, Xueqing Sun, Ji Qi, Mirek Janatka, Danail Stoyanov, Daniel S. Elson
- **Comment**: This paper has been submitted to MICCAI2016 on 17 March, 2016, and
  conditionally accepted on 2 June, 2016
- **Journal**: None
- **Summary**: Tissue surface shape and reflectance spectra provide rich intra-operative information useful in surgical guidance. We propose a hybrid system which displays an endoscopic image with a fast joint inspection of tissue surface shape using structured light (SL) and hyperspectral imaging (HSI). For SL a miniature fibre probe is used to project a coloured spot pattern onto the tissue surface. In HSI mode standard endoscopic illumination is used, with the fibre probe collecting reflected light and encoding the spatial information into a linear format that can be imaged onto the slit of a spectrograph. Correspondence between the arrangement of fibres at the distal and proximal ends of the bundle was found using spectral encoding. Then during pattern decoding, a fully convolutional network (FCN) was used for spot detection, followed by a matching propagation algorithm for spot identification. This method enabled fast reconstruction (12 frames per second) using a GPU. The hyperspectral image was combined with the white light image and the reconstructed surface, showing the spectral information of different areas. Validation of this system using phantom and ex vivo experiments has been demonstrated.



### Free Form based active contours for image segmentation and free space perception
- **Arxiv ID**: http://arxiv.org/abs/1606.04774v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1606.04774v1)
- **Published**: 2016-06-15 14:11:21+00:00
- **Updated**: 2016-06-15 14:11:21+00:00
- **Authors**: Ouiddad Labbani I., Pauline Merveilleux O, Olivier Ruatta
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present a novel approach for representing and evolving deformable active contours. The method combines piecewise regular B{\'e}zier models and curve evolution defined by local Free Form Deformation. The contour deformation is locally constrained which allows contour convergence with almost linear complexity while adapting to various shape settings and handling topology changes of the active contour.   We demonstrate the effectiveness of the new active contour scheme for visual free space perception and segmentation using omnidirectional images acquired by a robot exploring unknown indoor and outdoor environments. Several experiments validate the approach with comparison to state-of-the art parametric and geometric active contours and provide fast and real-time robot free space segmentation and navigation.



### V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1606.04797v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.04797v1)
- **Published**: 2016-06-15 14:55:09+00:00
- **Updated**: 2016-06-15 14:55:09+00:00
- **Authors**: Fausto Milletari, Nassir Navab, Seyed-Ahmad Ahmadi
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.



### A Powerful Generative Model Using Random Weights for the Deep Image Representation
- **Arxiv ID**: http://arxiv.org/abs/1606.04801v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1606.04801v2)
- **Published**: 2016-06-15 14:56:42+00:00
- **Updated**: 2016-06-16 06:53:55+00:00
- **Authors**: Kun He, Yan Wang, John Hopcroft
- **Comment**: 10 pages, 10 figures, submited to NIPS 2016 conference. Computer
  Vision and Pattern Recognition, Neurons and Cognition, Neural and
  Evolutionary Computing
- **Journal**: None
- **Summary**: To what extent is the success of deep visualization due to the training? Could we do deep visualization using untrained, random weight networks? To address this issue, we explore new and powerful generative models for three popular deep visualization tasks using untrained, random weight convolutional neural networks. First we invert representations in feature spaces and reconstruct images from white noise inputs. The reconstruction quality is statistically higher than that of the same method applied on well trained networks with the same architecture. Next we synthesize textures using scaled correlations of representations in multiple layers and our results are almost indistinguishable with the original natural texture and the synthesized textures based on the trained network. Third, by recasting the content of an image in the style of various artworks, we create artistic images with high perceptual quality, highly competitive to the prior work of Gatys et al. on pretrained networks. To our knowledge this is the first demonstration of image representations using untrained deep neural networks. Our work provides a new and fascinating tool to study the representation of deep network architecture and sheds light on new understandings on deep visualization.



### The ND-IRIS-0405 Iris Image Dataset
- **Arxiv ID**: http://arxiv.org/abs/1606.04853v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.04853v1)
- **Published**: 2016-06-15 16:40:51+00:00
- **Updated**: 2016-06-15 16:40:51+00:00
- **Authors**: Kevin W. Bowyer, Patrick J. Flynn
- **Comment**: 13 pages, 8 figures
- **Journal**: None
- **Summary**: The Computer Vision Research Lab at the University of Notre Dame began collecting iris images in the spring semester of 2004. The initial data collections used an LG 2200 iris imaging system for image acquisition. Image datasets acquired in 2004-2005 at Notre Dame with this LG 2200 have been used in the ICE 2005 and ICE 2006 iris biometric evaluations. The ICE 2005 iris image dataset has been distributed to over 100 research groups around the world. The purpose of this document is to describe the content of the ND-IRIS-0405 iris image dataset. This dataset is a superset of the iris image datasets used in ICE 2005 and ICE 2006. The ND 2004-2005 iris image dataset contains 64,980 images corresponding to 356 unique subjects, and 712 unique irises. The age range of the subjects is 18 to 75 years old. 158 of the subjects are female, and 198 are male. 250 of the subjects are Caucasian, 82 are Asian, and 24 are other ethnicities.



### cltorch: a Hardware-Agnostic Backend for the Torch Deep Neural Network Library, Based on OpenCL
- **Arxiv ID**: http://arxiv.org/abs/1606.04884v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, I.2.6; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/1606.04884v1)
- **Published**: 2016-06-15 17:59:31+00:00
- **Updated**: 2016-06-15 17:59:31+00:00
- **Authors**: Hugh Perkins
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: This paper presents cltorch, a hardware-agnostic backend for the Torch neural network framework. cltorch enables training of deep neural networks on GPUs from diverse hardware vendors, including AMD, NVIDIA, and Intel. cltorch contains sufficient implementation to run models such as AlexNet, VGG, Overfeat, and GoogleNet. It is written using the OpenCL language, a portable compute language, governed by the Khronos Group. cltorch is the top-ranked hardware-agnostic machine learning framework on Chintala's convnet-benchmarks page.   This paper presents the technical challenges encountered whilst creating the cltorch backend for Torch, and looks in detail at the challenges related to obtaining a fast hardware-agnostic implementation.   The convolutional layers are identified as the key area of focus for accelerating hardware-agnostic frameworks. Possible approaches to accelerating the convolutional implementation are identified including: implementation of the convolutions using the implicitgemm or winograd algorithm, using a GEMM implementation adapted to the geometries associated with the convolutional algorithm, or using a pluggable hardware-specific convolutional implementation.



### Combining multiscale features for classification of hyperspectral images: a sequence based kernel approach
- **Arxiv ID**: http://arxiv.org/abs/1606.04985v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1606.04985v1)
- **Published**: 2016-06-15 21:19:54+00:00
- **Updated**: 2016-06-15 21:19:54+00:00
- **Authors**: Yanwei Cui, Laetitia Chapel, Sébastien Lefèvre
- **Comment**: 8th IEEE GRSS Workshop on Hyperspectral Image and Signal Processing:
  Evolution in Remote Sensing (WHISPERS 2016), UCLA in Los Angeles, California,
  U.S
- **Journal**: None
- **Summary**: Nowadays, hyperspectral image classification widely copes with spatial information to improve accuracy. One of the most popular way to integrate such information is to extract hierarchical features from a multiscale segmentation. In the classification context, the extracted features are commonly concatenated into a long vector (also called stacked vector), on which is applied a conventional vector-based machine learning technique (e.g. SVM with Gaussian kernel). In this paper, we rather propose to use a sequence structured kernel: the spectrum kernel. We show that the conventional stacked vector-based kernel is actually a special case of this kernel. Experiments conducted on various publicly available hyperspectral datasets illustrate the improvement of the proposed kernel w.r.t. conventional ones using the same hierarchical spatial features.



### A Hierarchical Pose-Based Approach to Complex Action Understanding Using Dictionaries of Actionlets and Motion Poselets
- **Arxiv ID**: http://arxiv.org/abs/1606.04992v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.04992v1)
- **Published**: 2016-06-15 21:51:25+00:00
- **Updated**: 2016-06-15 21:51:25+00:00
- **Authors**: Ivan Lillo, Juan Carlos Niebles, Alvaro Soto
- **Comment**: 10 pages, IEEE International Conference on Computer Vision and
  Pattern Recognition (CVPR) 2016
- **Journal**: None
- **Summary**: In this paper, we introduce a new hierarchical model for human action recognition using body joint locations. Our model can categorize complex actions in videos, and perform spatio-temporal annotations of the atomic actions that compose the complex action being performed.That is, for each atomic action, the model generates temporal action annotations by estimating its starting and ending times, as well as, spatial annotations by inferring the human body parts that are involved in executing the action. our model includes three key novel properties: (i) it can be trained with no spatial supervision, as it can automatically discover active body parts from temporal action annotations only; (ii) it jointly learns flexible representations for motion poselets and actionlets that encode the visual variability of body parts and atomic actions; (iii) a mechanism to discard idle or non-informative body parts which increases its robustness to common pose estimation errors. We evaluate the performance of our method using multiple action recognition benchmarks. Our model consistently outperforms baselines and state-of-the-art action recognition methods.



### 3DFS: Deformable Dense Depth Fusion and Segmentation for Object Reconstruction from a Handheld Camera
- **Arxiv ID**: http://arxiv.org/abs/1606.05002v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.05002v2)
- **Published**: 2016-06-15 23:23:08+00:00
- **Updated**: 2016-07-27 20:38:19+00:00
- **Authors**: Tanmay Gupta, Daeyun Shin, Naren Sivagnanadasan, Derek Hoiem
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an approach for 3D reconstruction and segmentation of a single object placed on a flat surface from an input video. Our approach is to perform dense depth map estimation for multiple views using a proposed objective function that preserves detail. The resulting depth maps are then fused using a proposed implicit surface function that is robust to estimation error, producing a smooth surface reconstruction of the entire scene. Finally, the object is segmented from the remaining scene using a proposed 2D-3D segmentation that incorporates image and depth cues with priors and regularization over the 3D volume and 2D segmentations. We evaluate 3D reconstructions qualitatively on our Object-Videos dataset, comparing to fusion, multiview stereo, and segmentation baselines. We also quantitatively evaluate the dense depth estimation using the RGBD Scenes V2 dataset [Henry et al. 2013] and the segmentation using keyframe annotations of the Object-Videos dataset.



