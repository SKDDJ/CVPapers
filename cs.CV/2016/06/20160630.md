# Arxiv Papers in cs.CV on 2016-06-30
### Zero-Shot Learning with Multi-Battery Factor Analysis
- **Arxiv ID**: http://arxiv.org/abs/1606.09349v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.09349v1)
- **Published**: 2016-06-30 05:32:37+00:00
- **Updated**: 2016-06-30 05:32:37+00:00
- **Authors**: Zhong Ji, Yuzhong Xie, Yanwei Pang, Lei Chen, Zhongfei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) extends the conventional image classification technique to a more challenging situation where the test image categories are not seen in the training samples. Most studies on ZSL utilize side information such as attributes or word vectors to bridge the relations between the seen classes and the unseen classes. However, existing approaches on ZSL typically exploit a shared space for each type of side information independently, which cannot make full use of the complementary knowledge of different types of side information. To this end, this paper presents an MBFA-ZSL approach to embed different types of side information as well as the visual feature into one shared space. Specifically, we first develop an algorithm named Multi-Battery Factor Analysis (MBFA) to build a unified semantic space, and then employ multiple types of side information in it to achieve the ZSL. The close-form solution makes MBFA-ZSL simple to implement and efficient to run on large datasets. Extensive experiments on the popular AwA, CUB, and SUN datasets show its significant superiority over the state-of-the-art approaches.



### Parking Stall Vacancy Indicator System Based on Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1606.09367v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.09367v1)
- **Published**: 2016-06-30 06:57:11+00:00
- **Updated**: 2016-06-30 06:57:11+00:00
- **Authors**: Sepehr Valipour, Mennatullah Siam, Eleni Stroulia, Martin Jagersand
- **Comment**: None
- **Journal**: None
- **Summary**: Parking management systems, and vacancy-indication services in particular, can play a valuable role in reducing traffic and energy waste in large cities. Visual detection methods represent a cost-effective option, since they can take advantage of hardware usually already available in many parking lots, namely cameras. However, visual detection methods can be fragile and not easily generalizable. In this paper, we present a robust detection algorithm based on deep convolutional neural networks. We implemented and tested our algorithm on a large baseline dataset, and also on a set of image feeds from actual cameras already installed in parking lots. We have developed a fully functional system, from server-side image analysis to front-end user interface, to demonstrate the practicality of our method.



### Steering a Predator Robot using a Mixed Frame/Event-Driven Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1606.09433v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1606.09433v1)
- **Published**: 2016-06-30 11:17:00+00:00
- **Updated**: 2016-06-30 11:17:00+00:00
- **Authors**: Diederik Paul Moeys, Federico Corradi, Emmett Kerr, Philip Vance, Gautham Das, Daniel Neil, Dermot Kerr, Tobi Delbruck
- **Comment**: Paper presented at the conference: Second International Conference on
  Event-Based Control, Communication and Signal Processing (EBCCSP) 2016, At
  Krakow, Poland
- **Journal**: None
- **Summary**: This paper describes the application of a Convolutional Neural Network (CNN) in the context of a predator/prey scenario. The CNN is trained and run on data from a Dynamic and Active Pixel Sensor (DAVIS) mounted on a Summit XL robot (the predator), which follows another one (the prey). The CNN is driven by both conventional image frames and dynamic vision sensor "frames" that consist of a constant number of DAVIS ON and OFF events. The network is thus "data driven" at a sample rate proportional to the scene activity, so the effective sample rate varies from 15 Hz to 240 Hz depending on the robot speeds. The network generates four outputs: steer right, left, center and non-visible. After off-line training on labeled data, the network is imported on the on-board Summit XL robot which runs jAER and receives steering directions in real time. Successful results on closed-loop trials, with accuracies up to 87% or 92% (depending on evaluation criteria) are reported. Although the proposed approach discards the precise DAVIS event timing, it offers the significant advantage of compatibility with conventional deep learning technology without giving up the advantage of data-driven computing.



### maskSLIC: Regional Superpixel Generation with Application to Local Pathology Characterisation in Medical Images
- **Arxiv ID**: http://arxiv.org/abs/1606.09518v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.09518v2)
- **Published**: 2016-06-30 14:46:41+00:00
- **Updated**: 2017-02-09 16:32:07+00:00
- **Authors**: Benjamin Irving
- **Comment**: The article has been submitted to IEEE TPAMI
- **Journal**: None
- **Summary**: Supervoxel methods such as Simple Linear Iterative Clustering (SLIC) are an effective technique for partitioning an image or volume into locally similar regions, and are a common building block for the development of detection, segmentation and analysis methods. We introduce maskSLIC an extension of SLIC to create supervoxels within regions-of-interest, and demonstrate,on examples from 2-dimensions to 4-dimensions, that maskSLIC overcomes issues that affect SLIC within an irregular mask. We highlight the benefits of this method through examples, and show that it is able to better represent underlying tumour subregions and achieves significantly better results than SLIC on the BRATS 2013 brain tumour challenge data (p=0.001) - outperforming SLIC on 18/20 scans. Finally, we show an application of this method for the analysis of functional tumour subregions and demonstrate that it is more effective than voxel clustering.



### Fully-Convolutional Siamese Networks for Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1606.09549v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.09549v3)
- **Published**: 2016-06-30 16:00:43+00:00
- **Updated**: 2021-12-01 19:21:43+00:00
- **Authors**: Luca Bertinetto, Jack Valmadre, Jo√£o F. Henriques, Andrea Vedaldi, Philip H. S. Torr
- **Comment**: The first two authors contributed equally, and are listed in
  alphabetical order. Code available at
  http://www.robots.ox.ac.uk/~luca/siamese-fc.html
- **Journal**: None
- **Summary**: The problem of arbitrary object tracking has traditionally been tackled by learning a model of the object's appearance exclusively online, using as sole training data the video itself. Despite the success of these methods, their online-only approach inherently limits the richness of the model they can learn. Recently, several attempts have been made to exploit the expressive power of deep convolutional networks. However, when the object to track is not known beforehand, it is necessary to perform Stochastic Gradient Descent online to adapt the weights of the network, severely compromising the speed of the system. In this paper we equip a basic tracking algorithm with a novel fully-convolutional Siamese network trained end-to-end on the ILSVRC15 dataset for object detection in video. Our tracker operates at frame-rates beyond real-time and, despite its extreme simplicity, achieves state-of-the-art performance in multiple benchmarks.



