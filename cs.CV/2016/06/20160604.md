# Arxiv Papers in cs.CV on 2016-06-04
### A Tale of Two Bases: Local-Nonlocal Regularization on Image Patches with Convolution Framelets
- **Arxiv ID**: http://arxiv.org/abs/1606.01377v3
- **DOI**: None
- **Categories**: **cs.CV**, 68U10, 68Q25, I.4.4; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/1606.01377v3)
- **Published**: 2016-06-04 13:39:18+00:00
- **Updated**: 2016-09-12 23:07:29+00:00
- **Authors**: Rujie Yin, Tingran Gao, Yue M. Lu, Ingrid Daubechies
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an image representation scheme combining the local and nonlocal characterization of patches in an image. Our representation scheme can be shown to be equivalent to a tight frame constructed from convolving local bases (e.g. wavelet frames, discrete cosine transforms, etc.) with nonlocal bases (e.g. spectral basis induced by nonlinear dimension reduction on patches), and we call the resulting frame elements {\it convolution framelets}. Insight gained from analyzing the proposed representation leads to a novel interpretation of a recent high-performance patch-based image inpainting algorithm using Point Integral Method (PIM) and Low Dimension Manifold Model (LDMM) [Osher, Shi and Zhu, 2016]. In particular, we show that LDMM is a weighted $\ell_2$-regularization on the coefficients obtained by decomposing images into linear combinations of convolution framelets; based on this understanding, we extend the original LDMM to a reweighted version that yields further improved inpainting results. In addition, we establish the energy concentration property of convolution framelet coefficients for the setting where the local basis is constructed from a given nonlocal basis via a linear reconstruction framework; a generalization of this framework to unions of local embeddings can provide a natural setting for interpreting BM3D, one of the state-of-the-art image denoising algorithms.



### Automated Image Captioning for Rapid Prototyping and Resource Constrained Environments
- **Arxiv ID**: http://arxiv.org/abs/1606.01393v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.01393v1)
- **Published**: 2016-06-04 16:51:37+00:00
- **Updated**: 2016-06-04 16:51:37+00:00
- **Authors**: Karan Sharma, Arun CS Kumar, Suchendra Bhandarkar
- **Comment**: None
- **Journal**: None
- **Summary**: Significant performance gains in deep learning coupled with the exponential growth of image and video data on the Internet have resulted in the recent emergence of automated image captioning systems. Ensuring scalability of automated image captioning systems with respect to the ever increasing volume of image and video data is a significant challenge. This paper provides a valuable insight in that the detection of a few significant (top) objects in an image allows one to extract other relevant information such as actions (verbs) in the image. We expect this insight to be useful in the design of scalable image captioning systems. We address two parameters by which the scalability of image captioning systems could be quantified, i.e., the traditional algorithmic time complexity which is important given the resource limitations of the user device and the system development time since the programmers' time is a critical resource constraint in many real-world scenarios. Additionally, we address the issue of how word embeddings could be used to infer the verb (action) from the nouns (objects) in a given image in a zero-shot manner. Our results show that it is possible to attain reasonably good performance on predicting actions and captioning images using our approaches with the added advantage of simplicity of implementation.



