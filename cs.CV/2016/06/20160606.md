# Arxiv Papers in cs.CV on 2016-06-06
### Deep Linear Discriminant Analysis on Fisher Networks: A Hybrid Architecture for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1606.01595v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.01595v1)
- **Published**: 2016-06-06 02:11:15+00:00
- **Updated**: 2016-06-06 02:11:15+00:00
- **Authors**: Lin Wu, Chunhua Shen, Anton van den Hengel
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Person re-identification is to seek a correct match for a person of interest across views among a large number of imposters. It typically involves two procedures of non-linear feature extractions against dramatic appearance changes, and subsequent discriminative analysis in order to reduce intra- personal variations while enlarging inter-personal differences. In this paper, we introduce a hybrid architecture which combines Fisher vectors and deep neural networks to learn non-linear representations of person images to a space where data can be linearly separable. We reinforce a Linear Discriminant Analysis (LDA) on top of the deep neural network such that linearly separable latent representations can be learnt in an end-to-end fashion. By optimizing an objective function modified from LDA, the network is enforced to produce feature distributions which have a low variance within the same class and high variance between classes. The objective is essentially derived from the general LDA eigenvalue problem and allows to train the network with stochastic gradient descent and back-propagate LDA gradients to compute the gradients involved in Fisher vector encoding. For evaluation we test our approach on four benchmark data sets in person re-identification (VIPeR [1], CUHK03 [2], CUHK01 [3], and Market1501 [4]). Extensive experiments on these benchmarks show that our model can achieve state-of-the-art results.



### shapeDTW: shape Dynamic Time Warping
- **Arxiv ID**: http://arxiv.org/abs/1606.01601v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.01601v1)
- **Published**: 2016-06-06 02:38:01+00:00
- **Updated**: 2016-06-06 02:38:01+00:00
- **Authors**: Jiaping Zhao, Laurent Itti
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Dynamic Time Warping (DTW) is an algorithm to align temporal sequences with possible local non-linear distortions, and has been widely applied to audio, video and graphics data alignments. DTW is essentially a point-to-point matching method under some boundary and temporal consistency constraints. Although DTW obtains a global optimal solution, it does not necessarily achieve locally sensible matchings. Concretely, two temporal points with entirely dissimilar local structures may be matched by DTW. To address this problem, we propose an improved alignment algorithm, named shape Dynamic Time Warping (shapeDTW), which enhances DTW by taking point-wise local structural information into consideration. shapeDTW is inherently a DTW algorithm, but additionally attempts to pair locally similar structures and to avoid matching points with distinct neighborhood structures. We apply shapeDTW to align audio signal pairs having ground-truth alignments, as well as artificially simulated pairs of aligned sequences, and obtain quantitatively much lower alignment errors than DTW and its two variants. When shapeDTW is used as a distance measure in a nearest neighbor classifier (NN-shapeDTW) to classify time series, it beats DTW on 64 out of 84 UCR time series datasets, with significantly improved classification accuracies. By using a properly designed local structure descriptor, shapeDTW improves accuracies by more than 10% on 18 datasets. To the best of our knowledge, shapeDTW is the first distance measure under the nearest neighbor classifier scheme to significantly outperform DTW, which had been widely recognized as the best distance measure to date. Our code is publicly accessible at: https://github.com/jiapingz/shapeDTW.



### Deep Recurrent Convolutional Networks for Video-based Person Re-identification: An End-to-End Approach
- **Arxiv ID**: http://arxiv.org/abs/1606.01609v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.01609v2)
- **Published**: 2016-06-06 04:29:16+00:00
- **Updated**: 2016-06-12 10:52:09+00:00
- **Authors**: Lin Wu, Chunhua Shen, Anton van den Hengel
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: In this paper, we present an end-to-end approach to simultaneously learn spatio-temporal features and corresponding similarity metric for video-based person re-identification. Given the video sequence of a person, features from each frame that are extracted from all levels of a deep convolutional network can preserve a higher spatial resolution from which we can model finer motion patterns. These low-level visual percepts are leveraged into a variant of recurrent model to characterize the temporal variation between time-steps. Features from all time-steps are then summarized using temporal pooling to produce an overall feature representation for the complete sequence. The deep convolutional network, recurrent layer, and the temporal pooling are jointly trained to extract comparable hidden-unit representations from input pair of time series to compute their corresponding similarity value. The proposed framework combines time series modeling and metric learning to jointly learn relevant features and a good similarity measure between time sequences of person.   Experiments demonstrate that our approach achieves the state-of-the-art performance for video-based person re-identification on iLIDS-VID and PRID 2011, the two primary public datasets for this purpose.



### Photo Aesthetics Ranking Network with Attributes and Content Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1606.01621v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1606.01621v2)
- **Published**: 2016-06-06 06:14:00+00:00
- **Updated**: 2016-07-27 00:20:07+00:00
- **Authors**: Shu Kong, Xiaohui Shen, Zhe Lin, Radomir Mech, Charless Fowlkes
- **Comment**: None
- **Journal**: None
- **Summary**: Real-world applications could benefit from the ability to automatically generate a fine-grained ranking of photo aesthetics. However, previous methods for image aesthetics analysis have primarily focused on the coarse, binary categorization of images into high- or low-aesthetic categories. In this work, we propose to learn a deep convolutional neural network to rank photo aesthetics in which the relative ranking of photo aesthetics are directly modeled in the loss function. Our model incorporates joint learning of meaningful photographic attributes and image content information which can help regularize the complicated photo aesthetics rating problem.   To train and analyze this model, we have assembled a new aesthetics and attributes database (AADB) which contains aesthetic scores and meaningful attributes assigned to each image by multiple human raters. Anonymized rater identities are recorded across images allowing us to exploit intra-rater consistency using a novel sampling strategy when computing the ranking loss of training image pairs. We show the proposed sampling strategy is very effective and robust in face of subjective judgement of image aesthetics by individuals with different aesthetic tastes. Experiments demonstrate that our unified model can generate aesthetic rankings that are more consistent with human ratings. To further validate our model, we show that by simply thresholding the estimated aesthetic scores, we are able to achieve state-or-the-art classification performance on the existing AVA dataset benchmark.



### Predictive Coding for Dynamic Vision : Development of Functional Hierarchy in a Multiple Spatio-Temporal Scales RNN Model
- **Arxiv ID**: http://arxiv.org/abs/1606.01672v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.01672v3)
- **Published**: 2016-06-06 09:34:19+00:00
- **Updated**: 2017-03-17 05:20:07+00:00
- **Authors**: Minkyu Choi, Jun Tani
- **Comment**: None
- **Journal**: None
- **Summary**: The current paper presents a novel recurrent neural network model, the predictive multiple spatio-temporal scales RNN (P-MSTRNN), which can generate as well as recognize dynamic visual patterns in the predictive coding framework. The model is characterized by multiple spatio-temporal scales imposed on neural unit dynamics through which an adequate spatio-temporal hierarchy develops via learning from exemplars. The model was evaluated by conducting an experiment of learning a set of whole body human movement patterns which was generated by following a hierarchically defined movement syntax. The analysis of the trained model clarifies what types of spatio-temporal hierarchy develop in dynamic neural activity as well as how robust generation and recognition of movement patterns can be achieved by using the error minimization principle.



### Less is More: Micro-expression Recognition from Video using Apex Frame
- **Arxiv ID**: http://arxiv.org/abs/1606.01721v3
- **DOI**: 10.1016/j.image.2017.11.006
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.01721v3)
- **Published**: 2016-06-06 12:59:14+00:00
- **Updated**: 2018-02-15 06:28:15+00:00
- **Authors**: Sze-Teng Liong, John See, KokSheik Wong, Raphael C. -W. Phan
- **Comment**: 14 pages double-column, author affiliations updated, acknowledgment
  of grant support added
- **Journal**: Signal Processing: Image Communication, Vol. 62, March 2018, pages
  82-92
- **Summary**: Despite recent interest and advances in facial micro-expression research, there is still plenty room for improvement in terms of micro-expression recognition. Conventional feature extraction approaches for micro-expression video consider either the whole video sequence or a part of it, for representation. However, with the high-speed video capture of micro-expressions (100-200 fps), are all frames necessary to provide a sufficiently meaningful representation? Is the luxury of data a bane to accurate recognition? A novel proposition is presented in this paper, whereby we utilize only two images per video: the apex frame and the onset frame. The apex frame of a video contains the highest intensity of expression changes among all frames, while the onset is the perfect choice of a reference frame with neutral expression. A new feature extractor, Bi-Weighted Oriented Optical Flow (Bi-WOOF) is proposed to encode essential expressiveness of the apex frame. We evaluated the proposed method on five micro-expression databases: CAS(ME)$^2$, CASME II, SMIC-HS, SMIC-NIR and SMIC-VIS. Our experiments lend credence to our hypothesis, with our proposed technique achieving a state-of-the-art F1-score recognition performance of 61% and 62% in the high frame rate CASME II and SMIC-HS databases respectively.



### Integrated perception with recurrent multi-task neural networks
- **Arxiv ID**: http://arxiv.org/abs/1606.01735v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1606.01735v2)
- **Published**: 2016-06-06 13:27:25+00:00
- **Updated**: 2016-11-29 14:38:00+00:00
- **Authors**: Hakan Bilen, Andrea Vedaldi
- **Comment**: 9 pages, 3 figures, 2 tables
- **Journal**: Advances in Neural Information Processing (NIPS) 2016
- **Summary**: Modern discriminative predictors have been shown to match natural intelligences in specific perceptual tasks in image classification, object and part detection, boundary extraction, etc. However, a major advantage that natural intelligences still have is that they work well for "all" perceptual problems together, solving them efficiently and coherently in an "integrated manner". In order to capture some of these advantages in machine perception, we ask two questions: whether deep neural networks can learn universal image representations, useful not only for a single task but for all of them, and how the solutions to the different tasks can be integrated in this framework. We answer by proposing a new architecture, which we call "MultiNet", in which not only deep image features are shared between tasks, but where tasks can interact in a recurrent manner by encoding the results of their analysis in a common shared representation of the data. In this manner, we show that the performance of individual tasks in standard benchmarks can be improved first by sharing features between them and then, more significantly, by integrating their solutions in the common representation.



### Unsupervised classification of children's bodies using currents
- **Arxiv ID**: http://arxiv.org/abs/1606.01746v1
- **DOI**: None
- **Categories**: **stat.ME**, cs.CV, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1606.01746v1)
- **Published**: 2016-06-06 13:52:24+00:00
- **Updated**: 2016-06-06 13:52:24+00:00
- **Authors**: Sonia Barahona, Ximo Gual-Arnau, Maria Victoria Ibáñez, Amelia Simó
- **Comment**: None
- **Journal**: None
- **Summary**: Object classification according to their shape and size is of key importance in many scientific fields. This work focuses on the case where the size and shape of an object is characterized by a current}. A current is a mathematical object which has been proved relevant to the modeling of geometrical data, like submanifolds, through integration of vector fields along them. As a consequence of the choice of a vector-valued Reproducing Kernel Hilbert Space (RKHS) as a test space for integrating manifolds, it is possible to consider that shapes are embedded in this Hilbert Space. A vector-valued RKHS is a Hilbert space of vector fields; therefore, it is possible to compute a mean of shapes, or to calculate a distance between two manifolds. This embedding enables us to consider size-and-shape classification algorithms.   These algorithms are applied to a 3D database obtained from an anthropometric survey of the Spanish child population with a potential application to online sales of children's wear.



### Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding
- **Arxiv ID**: http://arxiv.org/abs/1606.01847v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1606.01847v3)
- **Published**: 2016-06-06 17:59:56+00:00
- **Updated**: 2016-09-24 01:58:59+00:00
- **Authors**: Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, Marcus Rohrbach
- **Comment**: Accepted to EMNLP 2016
- **Journal**: None
- **Summary**: Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge.



