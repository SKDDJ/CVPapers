# Arxiv Papers in cs.CV on 2016-06-08
### SE3-Nets: Learning Rigid Body Motion using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1606.02378v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1606.02378v3)
- **Published**: 2016-06-08 02:36:11+00:00
- **Updated**: 2017-03-30 22:41:40+00:00
- **Authors**: Arunkumar Byravan, Dieter Fox
- **Comment**: 8 pages. To appear at the IEEE International Conference on Robotics
  and Automation (ICRA), 2017. V2 Update: Final version submitted to ICRA with
  experiments testing the robustness of the system to noise and preliminary
  results on real world data
- **Journal**: None
- **Summary**: We introduce SE3-Nets, which are deep neural networks designed to model and learn rigid body motion from raw point cloud data. Based only on sequences of depth images along with action vectors and point wise data associations, SE3-Nets learn to segment effected object parts and predict their motion resulting from the applied force. Rather than learning point wise flow vectors, SE3-Nets predict SE3 transformations for different parts of the scene. Using simulated depth data of a table top scene and a robot manipulator, we show that the structure underlying SE3-Nets enables them to generate a far more consistent prediction of object motion than traditional flow based networks. Additional experiments with a depth camera observing a Baxter robot pushing objects on a table show that SE3-Nets also work well on real data.



### Deep Learning Convolutional Networks for Multiphoton Microscopy Vasculature Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1606.02382v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.2.6; I.5.1; I.5.4; I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/1606.02382v1)
- **Published**: 2016-06-08 02:57:00+00:00
- **Updated**: 2016-06-08 02:57:00+00:00
- **Authors**: Petteri Teikari, Marc Santos, Charissa Poon, Kullervo Hynynen
- **Comment**: 23 pages, 10 figures
- **Journal**: None
- **Summary**: Recently there has been an increasing trend to use deep learning frameworks for both 2D consumer images and for 3D medical images. However, there has been little effort to use deep frameworks for volumetric vascular segmentation. We wanted to address this by providing a freely available dataset of 12 annotated two-photon vasculature microscopy stacks. We demonstrated the use of deep learning framework consisting both 2D and 3D convolutional filters (ConvNet). Our hybrid 2D-3D architecture produced promising segmentation result. We derived the architectures from Lee et al. who used the ZNN framework initially designed for electron microscope image segmentation. We hope that by sharing our volumetric vasculature datasets, we will inspire other researchers to experiment with vasculature dataset and improve the used network architectures.



### Progressive Attention Networks for Visual Attribute Prediction
- **Arxiv ID**: http://arxiv.org/abs/1606.02393v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.02393v5)
- **Published**: 2016-06-08 04:27:52+00:00
- **Updated**: 2018-08-06 22:03:19+00:00
- **Authors**: Paul Hongsuck Seo, Zhe Lin, Scott Cohen, Xiaohui Shen, Bohyung Han
- **Comment**: BMVC 2018 accepted paper
- **Journal**: None
- **Summary**: We propose a novel attention model that can accurately attends to target objects of various scales and shapes in images. The model is trained to gradually suppress irrelevant regions in an input image via a progressive attentive process over multiple layers of a convolutional neural network. The attentive process in each layer determines whether to pass or block features at certain spatial locations for use in the subsequent layers. The proposed progressive attention mechanism works well especially when combined with hard attention. We further employ local contexts to incorporate neighborhood features of each location and estimate a better attention probability map. The experiments on synthetic and real datasets show that the proposed attention networks outperform traditional attention methods in visual attribute prediction tasks.



### Structured Convolution Matrices for Energy-efficient Deep learning
- **Arxiv ID**: http://arxiv.org/abs/1606.02407v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1606.02407v1)
- **Published**: 2016-06-08 05:31:43+00:00
- **Updated**: 2016-06-08 05:31:43+00:00
- **Authors**: Rathinakumar Appuswamy, Tapan Nayak, John Arthur, Steven Esser, Paul Merolla, Jeffrey Mckinstry, Timothy Melano, Myron Flickner, Dharmendra Modha
- **Comment**: None
- **Journal**: None
- **Summary**: We derive a relationship between network representation in energy-efficient neuromorphic architectures and block Toplitz convolutional matrices. Inspired by this connection, we develop deep convolutional networks using a family of structured convolutional matrices and achieve state-of-the-art trade-off between energy efficiency and classification accuracy for well-known image recognition tasks. We also put forward a novel method to train binary convolutional networks by utilising an existing connection between noisy-rectified linear units and binary activations.



### Point-wise mutual information-based video segmentation with high temporal consistency
- **Arxiv ID**: http://arxiv.org/abs/1606.02467v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.02467v1)
- **Published**: 2016-06-08 09:33:17+00:00
- **Updated**: 2016-06-08 09:33:17+00:00
- **Authors**: Margret Keuper, Thomas Brox
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we tackle the problem of temporally consistent boundary detection and hierarchical segmentation in videos. While finding the best high-level reasoning of region assignments in videos is the focus of much recent research, temporal consistency in boundary detection has so far only rarely been tackled. We argue that temporally consistent boundaries are a key component to temporally consistent region assignment. The proposed method is based on the point-wise mutual information (PMI) of spatio-temporal voxels. Temporal consistency is established by an evaluation of PMI-based point affinities in the spectral domain over space and time. Thus, the proposed method is independent of any optical flow computation or previously learned motion models. The proposed low-level video segmentation method outperforms the learning-based state of the art in terms of standard region metrics.



### Convolutional Neural Fabrics
- **Arxiv ID**: http://arxiv.org/abs/1606.02492v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1606.02492v4)
- **Published**: 2016-06-08 10:17:51+00:00
- **Updated**: 2017-01-30 12:28:29+00:00
- **Authors**: Shreyas Saxena, Jakob Verbeek
- **Comment**: Corrected typos (In proceedings of NIPS16 )
- **Journal**: None
- **Summary**: Despite the success of CNNs, selecting the optimal architecture for a given task remains an open problem. Instead of aiming to select a single optimal architecture, we propose a "fabric" that embeds an exponentially large number of architectures. The fabric consists of a 3D trellis that connects response maps at different layers, scales, and channels with a sparse homogeneous local connectivity pattern. The only hyper-parameters of a fabric are the number of channels and layers. While individual architectures can be recovered as paths, the fabric can in addition ensemble all embedded architectures together, sharing their weights where their paths overlap. Parameters can be learned using standard methods based on back-propagation, at a cost that scales linearly in the fabric size. We present benchmark results competitive with the state of the art for image classification on MNIST and CIFAR10, and for semantic segmentation on the Part Labels dataset.



### Estimation of solar irradiance using ground-based whole sky imagers
- **Arxiv ID**: http://arxiv.org/abs/1606.02546v2
- **DOI**: None
- **Categories**: **astro-ph.IM**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1606.02546v2)
- **Published**: 2016-06-08 13:21:30+00:00
- **Updated**: 2017-04-06 09:38:37+00:00
- **Authors**: Soumyabrata Dev, Florian M. Savoy, Yee Hui Lee, Stefan Winkler
- **Comment**: Accepted in Proc. IEEE International Geoscience and Remote Sensing
  Symposium (IGARSS), July 2016
- **Journal**: None
- **Summary**: Ground-based whole sky imagers (WSIs) can provide localized images of the sky of high temporal and spatial resolution, which permits fine-grained cloud observation. In this paper, we show how images taken by WSIs can be used to estimate solar radiation. Sky cameras are useful here because they provide additional information about cloud movement and coverage, which are otherwise not available from weather station data. Our setup includes ground-based weather stations at the same location as the imagers. We use their measurements to validate our methods.



### DISCO Nets: DISsimilarity COefficient Networks
- **Arxiv ID**: http://arxiv.org/abs/1606.02556v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1606.02556v5)
- **Published**: 2016-06-08 13:57:44+00:00
- **Updated**: 2016-10-28 11:27:45+00:00
- **Authors**: Diane Bouchacourt, M. Pawan Kumar, Sebastian Nowozin
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new type of probabilistic model which we call DISsimilarity COefficient Networks (DISCO Nets). DISCO Nets allow us to efficiently sample from a posterior distribution parametrised by a neural network. During training, DISCO Nets are learned by minimising the dissimilarity coefficient between the true distribution and the estimated distribution. This allows us to tailor the training to the loss related to the task at hand. We empirically show that (i) by modeling uncertainty on the output value, DISCO Nets outperform equivalent non-probabilistic predictive networks and (ii) DISCO Nets accurately model the uncertainty of the output, outperforming existing probabilistic models based on deep neural networks.



### Convolution by Evolution: Differentiable Pattern Producing Networks
- **Arxiv ID**: http://arxiv.org/abs/1606.02580v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1606.02580v1)
- **Published**: 2016-06-08 14:37:39+00:00
- **Updated**: 2016-06-08 14:37:39+00:00
- **Authors**: Chrisantha Fernando, Dylan Banarse, Malcolm Reynolds, Frederic Besse, David Pfau, Max Jaderberg, Marc Lanctot, Daan Wierstra
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we introduce a differentiable version of the Compositional Pattern Producing Network, called the DPPN. Unlike a standard CPPN, the topology of a DPPN is evolved but the weights are learned. A Lamarckian algorithm, that combines evolution and learning, produces DPPNs to reconstruct an image. Our main result is that DPPNs can be evolved/trained to compress the weights of a denoising autoencoder from 157684 to roughly 200 parameters, while achieving a reconstruction accuracy comparable to a fully connected network with more than two orders of magnitude more parameters. The regularization ability of the DPPN allows it to rediscover (approximate) convolutional network architectures embedded within a fully connected architecture. Such convolutional architectures are the current state of the art for many computer vision applications, so it is satisfying that DPPNs are capable of discovering this structure rather than having to build it in by design. DPPNs exhibit better generalization when tested on the Omniglot dataset after being trained on MNIST, than directly encoded fully connected autoencoders. DPPNs are therefore a new framework for integrating learning and evolution.



### Fully Convolutional Networks for Dense Semantic Labelling of High-Resolution Aerial Imagery
- **Arxiv ID**: http://arxiv.org/abs/1606.02585v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.02585v1)
- **Published**: 2016-06-08 14:52:04+00:00
- **Updated**: 2016-06-08 14:52:04+00:00
- **Authors**: Jamie Sherrah
- **Comment**: None
- **Journal**: None
- **Summary**: The trend towards higher resolution remote sensing imagery facilitates a transition from land-use classification to object-level scene understanding. Rather than relying purely on spectral content, appearance-based image features come into play. In this work, deep convolutional neural networks (CNNs) are applied to semantic labelling of high-resolution remote sensing data. Recent advances in fully convolutional networks (FCNs) are adapted to overhead data and shown to be as effective as in other domains. A full-resolution labelling is inferred using a deep FCN with no downsampling, obviating the need for deconvolution or interpolation. To make better use of image features, a pre-trained CNN is fine-tuned on remote sensing data in a hybrid network context, resulting in superior results compared to a network trained from scratch. The proposed approach is applied to the problem of labelling high-resolution aerial imagery, where fine boundary detail is important. The dense labelling yields state-of-the-art accuracy for the ISPRS Vaihingen and Potsdam benchmark data sets.



### Fast and Extensible Online Multivariate Kernel Density Estimation
- **Arxiv ID**: http://arxiv.org/abs/1606.02608v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, I.5.1; I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/1606.02608v1)
- **Published**: 2016-06-08 15:39:17+00:00
- **Updated**: 2016-06-08 15:39:17+00:00
- **Authors**: Jaime Ferreira, David Martins de Matos, Ricardo Ribeiro
- **Comment**: 17 pages, 1 figure, 7 tables, submission to Pattern Recognition
  Letters, review version
- **Journal**: None
- **Summary**: We present xokde++, a state-of-the-art online kernel density estimation approach that maintains Gaussian mixture models input data streams. The approach follows state-of-the-art work on online density estimation, but was redesigned with computational efficiency, numerical robustness, and extensibility in mind. Our approach produces comparable or better results than the current state-of-the-art, while achieving significant computational performance gains and improved numerical stability. The use of diagonal covariance Gaussian kernels, which further improve performance and stability, at a small loss of modelling quality, is also explored. Our approach is up to 40 times faster, while requiring 90\% less memory than the closest state-of-the-art counterpart.



### Rotation Invariant Angular Descriptor Via A Bandlimited Gaussian-like Kernel
- **Arxiv ID**: http://arxiv.org/abs/1606.02753v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.02753v1)
- **Published**: 2016-06-08 20:51:23+00:00
- **Updated**: 2016-06-08 20:51:23+00:00
- **Authors**: Michael T. McCann, Matthew Fickus, Jelena Kovacevic
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new smooth, Gaussian-like kernel that allows the kernel density estimate for an angular distribution to be exactly represented by a finite number of its Fourier series coefficients. Distributions of angular quantities, such as gradients, are a central part of several state-of-the-art image processing algorithms, but these distributions are usually described via histograms and therefore lack rotation invariance due to binning artifacts. Replacing histograming with kernel density estimation removes these binning artifacts and can provide a finite-dimensional descriptor of the distribution, provided that the kernel is selected to be bandlimited. In this paper, we present a new band-limited kernel that has the added advantage of being Gaussian-like in the angular domain. We then show that it compares favorably to gradient histograms for patch matching, person detection, and texture segmentation.



