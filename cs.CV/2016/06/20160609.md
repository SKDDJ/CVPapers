# Arxiv Papers in cs.CV on 2016-06-09
### Spontaneous Subtle Expression Detection and Recognition based on Facial Strain
- **Arxiv ID**: http://arxiv.org/abs/1606.02792v1
- **DOI**: 10.1016/j.image.2016.06.004
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.02792v1)
- **Published**: 2016-06-09 00:47:30+00:00
- **Updated**: 2016-06-09 00:47:30+00:00
- **Authors**: Sze-Teng Liong, John See, Raphael Chung-Wei Phan, Yee-Hui Oh, Anh Cat Le Ngo, KokSheik Wong, Su-Wei Tan
- **Comment**: 21 pages (including references), single column format, accepted to
  Signal Processing: Image Communication journal
- **Journal**: Signal Proc. Image Comm. 47 (2016) 170-182
- **Summary**: Optical strain is an extension of optical flow that is capable of quantifying subtle changes on faces and representing the minute facial motion intensities at the pixel level. This is computationally essential for the relatively new field of spontaneous micro-expression, where subtle expressions can be technically challenging to pinpoint. In this paper, we present a novel method for detecting and recognizing micro-expressions by utilizing facial optical strain magnitudes to construct optical strain features and optical strain weighted features. The two sets of features are then concatenated to form the resultant feature histogram. Experiments were performed on the CASME II and SMIC databases. We demonstrate on both databases, the usefulness of optical strain information and more importantly, that our best approaches are able to outperform the original baseline results for both detection and recognition tasks. A comparison of the proposed method with other existing spatio-temporal feature extraction approaches is also presented.



### Machine Learning Techniques and Applications For Ground-based Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1606.02811v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.02811v1)
- **Published**: 2016-06-09 03:33:11+00:00
- **Updated**: 2016-06-09 03:33:11+00:00
- **Authors**: Soumyabrata Dev, Bihan Wen, Yee Hui Lee, Stefan Winkler
- **Comment**: None
- **Journal**: None
- **Summary**: Ground-based whole sky cameras have opened up new opportunities for monitoring the earth's atmosphere. These cameras are an important complement to satellite images by providing geoscientists with cheaper, faster, and more localized data. The images captured by whole sky imagers can have high spatial and temporal resolution, which is an important pre-requisite for applications such as solar energy modeling, cloud attenuation analysis, local weather prediction, etc.   Extracting valuable information from the huge amount of image data by detecting and analyzing the various entities in these images is challenging. However, powerful machine learning techniques have become available to aid with the image analysis. This article provides a detailed walk-through of recent developments in these techniques and their applications in ground-based imaging. We aim to bridge the gap between computer vision and remote sensing with the help of illustrative examples. We demonstrate the advantages of using machine learning techniques in ground-based image analysis via three primary applications -- segmentation, classification, and denoising.



### Low-shot Visual Recognition by Shrinking and Hallucinating Features
- **Arxiv ID**: http://arxiv.org/abs/1606.02819v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.02819v4)
- **Published**: 2016-06-09 04:28:07+00:00
- **Updated**: 2017-11-04 15:52:48+00:00
- **Authors**: Bharath Hariharan, Ross Girshick
- **Comment**: ICCV 2017 spotlight
- **Journal**: None
- **Summary**: Low-shot visual learning---the ability to recognize novel object categories from very few examples---is a hallmark of human visual intelligence. Existing machine learning approaches fail to generalize in the same way. To make progress on this foundational problem, we present a low-shot learning benchmark on complex images that mimics challenges faced by recognition systems in the wild. We then propose a) representation regularization techniques, and b) techniques to hallucinate additional training examples for data-starved classes. Together, our methods improve the effectiveness of convolutional networks in low-shot learning, improving the one-shot accuracy on novel classes by 2.3x on the challenging ImageNet dataset.



### Simultaneous Inpainting and Denoising by Directional Global Three-part Decomposition: Connecting Variational and Fourier Domain Based Image Processing
- **Arxiv ID**: http://arxiv.org/abs/1606.02861v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.02861v1)
- **Published**: 2016-06-09 08:21:38+00:00
- **Updated**: 2016-06-09 08:21:38+00:00
- **Authors**: Duy Hoang Thai, Carsten Gottschlich
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the very challenging task of restoring images (i) which have a large number of missing pixels, (ii) whose existing pixels are corrupted by noise and (iii) the ideal image to be restored contains both cartoon and texture elements. The combination of these three properties makes this inverse problem a very difficult one. The solution proposed in this manuscript is based on directional global three-part decomposition (DG3PD) [ThaiGottschlich2016] with directional total variation norm, directional G-norm and $\ell_\infty$-norm in curvelet domain as key ingredients of the model. Image decomposition by DG3PD enables a decoupled inpainting and denoising of the cartoon and texture components. A comparison to existing approaches for inpainting and denoising shows the advantages of the proposed method. Moreover, we regard the image restoration problem from the viewpoint of a Bayesian framework and we discuss the connections between the proposed solution by function space and related image representation by harmonic analysis and pyramid decomposition.



### A Comprehensive Analysis of Deep Learning Based Representation for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1606.02894v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.02894v1)
- **Published**: 2016-06-09 10:25:24+00:00
- **Updated**: 2016-06-09 10:25:24+00:00
- **Authors**: Mostafa Mehdipour Ghazi, Hazim Kemal Ekenel
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning based approaches have been dominating the face recognition field due to the significant performance improvement they have provided on the challenging wild datasets. These approaches have been extensively tested on such unconstrained datasets, on the Labeled Faces in the Wild and YouTube Faces, to name a few. However, their capability to handle individual appearance variations caused by factors such as head pose, illumination, occlusion, and misalignment has not been thoroughly assessed till now. In this paper, we present a comprehensive study to evaluate the performance of deep learning based face representation under several conditions including the varying head pose angles, upper and lower face occlusion, changing illumination of different strengths, and misalignment due to erroneous facial feature localization. Two successful and publicly available deep learning models, namely VGG-Face and Lightened CNN have been utilized to extract face representations. The obtained results show that although deep learning provides a powerful representation for face recognition, it can still benefit from preprocessing, for example, for pose and illumination normalization in order to achieve better performance under various conditions. Particularly, if these variations are not included in the dataset used to train the deep learning model, the role of preprocessing becomes more crucial. Experimental results also show that deep learning based representation is robust to misalignment and can tolerate facial feature localization errors up to 10% of the interocular distance.



### Apparent Age Estimation Using Ensemble of Deep Learning Models
- **Arxiv ID**: http://arxiv.org/abs/1606.02909v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.02909v1)
- **Published**: 2016-06-09 11:00:21+00:00
- **Updated**: 2016-06-09 11:00:21+00:00
- **Authors**: Refik Can Malli, Mehmet Aygun, Hazim Kemal Ekenel
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the problem of apparent age estimation. Different from estimating the real age of individuals, in which each face image has a single age label, in this problem, face images have multiple age labels, corresponding to the ages perceived by the annotators, when they look at these images. This provides an intriguing computer vision problem, since in generic image or object classification tasks, it is typical to have a single ground truth label per class. To account for multiple labels per image, instead of using average age of the annotated face image as the class label, we have grouped the face images that are within a specified age range. Using these age groups and their age-shifted groupings, we have trained an ensemble of deep learning models. Before feeding an input face image to a deep learning model, five facial landmark points are detected and used for 2-D alignment. We have employed and fine tuned convolutional neural networks (CNNs) that are based on VGG-16 [24] architecture and pretrained on the IMDB-WIKI dataset [22]. The outputs of these deep learning models are then combined to produce the final estimation. Proposed method achieves 0.3668 error in the final ChaLearn LAP 2016 challenge test set [5].



### Implicit Tubular Surface Generation Guided by Centerline
- **Arxiv ID**: http://arxiv.org/abs/1606.03014v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.03014v1)
- **Published**: 2016-06-09 16:14:22+00:00
- **Updated**: 2016-06-09 16:14:22+00:00
- **Authors**: Haoyin Zhou, James K. Min, Guanglei Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: Most machine learning-based coronary artery segmentation methods represent the vascular lumen surface in an implicit way by the centerline and the associated lumen radii, which makes the subsequent modeling process to generate a whole piece of watertight coronary artery tree model difficult. To solve this problem, in this paper, we propose a modeling method with the learning-based segmentation results by (1) considering mesh vertices as physical particles and using interaction force model and particle expansion model to generate uniformly distributed point cloud on the implicit lumen surface and; (2) doing incremental Delaunay-based triangulation. Our method has the advantage of being able to consider the complex shape of the coronary artery tree as a whole piece; hence no extra stitching or intersection removal algorithm is needed to generate a watertight model. Experiment results demonstrate that our method is capable of generating high quality mesh model which is highly consistent with the given implicit vascular lumen surface, with an average error of 0.08 mm.



### Feature-based Recursive Observer Design for Homography Estimation
- **Arxiv ID**: http://arxiv.org/abs/1606.03021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.03021v1)
- **Published**: 2016-06-09 16:35:46+00:00
- **Updated**: 2016-06-09 16:35:46+00:00
- **Authors**: Minh-Duc Hua, Jochen Trumpf, Tarek Hamel, Robert Mahony, Pascal Morin
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new algorithm for online estimation of a sequence of homographies applicable to image sequences obtained from robotic vehicles equipped with vision sensors. The approach taken exploits the underlying Special Linear group structure of the set of homographies along with gyroscope measurements and direct point-feature correspondences between images to develop temporal filter for the homography estimate. Theoretical analysis and experimental results are provided to demonstrate the robustness of the proposed algorithm. The experimental results show excellent performance even in the case of very fast camera motion (relative to frame rate), severe occlusion, and in the presence of specular reflections.



### Convolutional Sketch Inversion
- **Arxiv ID**: http://arxiv.org/abs/1606.03073v1
- **DOI**: 10.1007/978-3-319-46604-0_56
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.03073v1)
- **Published**: 2016-06-09 19:27:41+00:00
- **Updated**: 2016-06-09 19:27:41+00:00
- **Authors**: Yağmur Güçlütürk, Umut Güçlü, Rob van Lier, Marcel A. J. van Gerven
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we use deep neural networks for inverting face sketches to synthesize photorealistic face images. We first construct a semi-simulated dataset containing a very large number of computer-generated face sketches with different styles and corresponding face images by expanding existing unconstrained face data sets. We then train models achieving state-of-the-art results on both computer-generated sketches and hand-drawn sketches by leveraging recent advances in deep learning such as batch normalization, deep residual learning, perceptual losses and stochastic optimization in combination with our new dataset. We finally demonstrate potential applications of our models in fine arts and forensic arts. In contrast to existing patch-based approaches, our deep-neural-network-based approach can be used for synthesizing photorealistic face images by inverting face sketches in the wild.



### Mutual Exclusivity Loss for Semi-Supervised Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1606.03141v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1606.03141v1)
- **Published**: 2016-06-09 23:15:16+00:00
- **Updated**: 2016-06-09 23:15:16+00:00
- **Authors**: Mehdi Sajjadi, Mehran Javanmardi, Tolga Tasdizen
- **Comment**: 5 pages, 1 figures, ICIP 2016
- **Journal**: None
- **Summary**: In this paper we consider the problem of semi-supervised learning with deep Convolutional Neural Networks (ConvNets). Semi-supervised learning is motivated on the observation that unlabeled data is cheap and can be used to improve the accuracy of classifiers. In this paper we propose an unsupervised regularization term that explicitly forces the classifier's prediction for multiple classes to be mutually-exclusive and effectively guides the decision boundary to lie on the low density space between the manifolds corresponding to different classes of data. Our proposed approach is general and can be used with any backpropagation-based learning method. We show through different experiments that our method can improve the object recognition performance of ConvNets using unlabeled data.



