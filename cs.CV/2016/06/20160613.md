# Arxiv Papers in cs.CV on 2016-06-13
### Unsupervised Non Linear Dimensionality Reduction Machine Learning methods applied to Multiparametric MRI in cerebral ischemia: Preliminary Results
- **Arxiv ID**: http://arxiv.org/abs/1606.03788v1
- **DOI**: 10.1117/12.2044001
- **Categories**: **cs.CV**, H.3.3, I.5.3, I.5.4, J.6
- **Links**: [PDF](http://arxiv.org/pdf/1606.03788v1)
- **Published**: 2016-06-13 01:29:04+00:00
- **Updated**: 2016-06-13 01:29:04+00:00
- **Authors**: Vishwa S. Parekh, Jeremy R. Jacobs, Michael A. Jacobs
- **Comment**: 9 pages
- **Journal**: Proceedings of the SPIE, Volume 9034, id. 90342O 9 pp. (2014)
- **Summary**: The evaluation and treatment of acute cerebral ischemia requires a technique that can determine the total area of tissue at risk for infarction using diagnostic magnetic resonance imaging (MRI) sequences. Typical MRI data sets consist of T1- and T2-weighted imaging (T1WI, T2WI) along with advanced MRI parameters of diffusion-weighted imaging (DWI) and perfusion weighted imaging (PWI) methods. Each of these parameters has distinct radiological-pathological meaning. For example, DWI interrogates the movement of water in the tissue and PWI gives an estimate of the blood flow, both are critical measures during the evolution of stroke. In order to integrate these data and give an estimate of the tissue at risk or damaged, we have developed advanced machine learning methods based on unsupervised non-linear dimensionality reduction (NLDR) techniques. NLDR methods are a class of algorithms that uses mathematically defined manifolds for statistical sampling of multidimensional classes to generate a discrimination rule of guaranteed statistical accuracy and they can generate a two- or three-dimensional map, which represents the prominent structures of the data and provides an embedded image of meaningful low-dimensional structures hidden in their high-dimensional observations. In this manuscript, we develop NLDR methods on high dimensional MRI data sets of preclinical animals and clinical patients with stroke. On analyzing the performance of these methods, we observed that there was a high of similarity between multiparametric embedded images from NLDR methods and the ADC map and perfusion map. It was also observed that embedded scattergram of abnormal (infarcted or at risk) tissue can be visualized and provides a mechanism for automatic methods to delineate potential stroke volumes and early tissue at risk.



### Deep Image Homography Estimation
- **Arxiv ID**: http://arxiv.org/abs/1606.03798v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.03798v1)
- **Published**: 2016-06-13 02:46:38+00:00
- **Updated**: 2016-06-13 02:46:38+00:00
- **Authors**: Daniel DeTone, Tomasz Malisiewicz, Andrew Rabinovich
- **Comment**: RSS Workshop on Limits and Potentials of Deep Learning in Robotics
- **Journal**: None
- **Summary**: We present a deep convolutional neural network for estimating the relative homography between a pair of images. Our feed-forward network has 10 layers, takes two stacked grayscale images as input, and produces an 8 degree of freedom homography which can be used to map the pixels from the first image to the second. We present two convolutional neural network architectures for HomographyNet: a regression network which directly estimates the real-valued homography parameters, and a classification network which produces a distribution over quantized homographies. We use a 4-point homography parameterization which maps the four corners from one image into the second image. Our networks are trained in an end-to-end fashion using warped MS-COCO images. Our approach works without the need for separate local feature detection and transformation estimation stages. Our deep models are compared to a traditional homography estimator based on ORB features and we highlight the scenarios where HomographyNet outperforms the traditional technique. We also describe a variety of applications powered by deep homography estimation, thus showcasing the flexibility of a deep learning approach.



### Laplacian LRR on Product Grassmann Manifolds for Human Activity Clustering in Multi-Camera Video Surveillance
- **Arxiv ID**: http://arxiv.org/abs/1606.03838v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.03838v1)
- **Published**: 2016-06-13 07:09:39+00:00
- **Updated**: 2016-06-13 07:09:39+00:00
- **Authors**: Boyue Wang, Yongli Hu, Junbin Gao, Yanfeng Sun, Baocai Yin
- **Comment**: 14pages,submitting to IEEE TCSVT with minor revision
- **Journal**: None
- **Summary**: In multi-camera video surveillance, it is challenging to represent videos from different cameras properly and fuse them efficiently for specific applications such as human activity recognition and clustering. In this paper, a novel representation for multi-camera video data, namely the Product Grassmann Manifold (PGM), is proposed to model video sequences as points on the Grassmann manifold and integrate them as a whole in the product manifold form. Additionally, with a new geometry metric on the product manifold, the conventional Low Rank Representation (LRR) model is extended onto PGM and the new LRR model can be used for clustering non-linear data, such as multi-camera video data. To evaluate the proposed method, a number of clustering experiments are conducted on several multi-camera video datasets of human activity, including Dongzhimen Transport Hub Crowd action dataset, ACT 42 Human action dataset and SKIG action dataset. The experiment results show that the proposed method outperforms many state-of-the-art clustering methods.



### Photo Stylistic Brush: Robust Style Transfer via Superpixel-Based Bipartite Graph
- **Arxiv ID**: http://arxiv.org/abs/1606.03871v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.03871v2)
- **Published**: 2016-06-13 09:28:19+00:00
- **Updated**: 2016-07-15 09:05:02+00:00
- **Authors**: Jiaying Liu, Wenhan Yang, Xiaoyan Sun, Wenjun Zeng
- **Comment**: None
- **Journal**: None
- **Summary**: With the rapid development of social network and multimedia technology, customized image and video stylization has been widely used for various social-media applications. In this paper, we explore the problem of exemplar-based photo style transfer, which provides a flexible and convenient way to invoke fantastic visual impression. Rather than investigating some fixed artistic patterns to represent certain styles as was done in some previous works, our work emphasizes styles related to a series of visual effects in the photograph, e.g. color, tone, and contrast. We propose a photo stylistic brush, an automatic robust style transfer approach based on Superpixel-based BIpartite Graph (SuperBIG). A two-step bipartite graph algorithm with different granularity levels is employed to aggregate pixels into superpixels and find their correspondences. In the first step, with the extracted hierarchical features, a bipartite graph is constructed to describe the content similarity for pixel partition to produce superpixels. In the second step, superpixels in the input/reference image are rematched to form a new superpixel-based bipartite graph, and superpixel-level correspondences are generated by a bipartite matching. Finally, the refined correspondence guides SuperBIG to perform the transformation in a decorrelated color space. Extensive experimental results demonstrate the effectiveness and robustness of the proposed method for transferring various styles of exemplar images, even for some challenging cases, such as night images.



### Visual-Inertial-Semantic Scene Representation for 3-D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1606.03968v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1606.03968v2)
- **Published**: 2016-06-13 14:22:10+00:00
- **Updated**: 2017-04-17 20:25:26+00:00
- **Authors**: Jingming Dong, Xiaohan Fei, Stefano Soatto
- **Comment**: To appear in CVPR 2017
- **Journal**: None
- **Summary**: We describe a system to detect objects in three-dimensional space using video and inertial sensors (accelerometer and gyrometer), ubiquitous in modern mobile platforms from phones to drones. Inertials afford the ability to impose class-specific scale priors for objects, and provide a global orientation reference. A minimal sufficient representation, the posterior of semantic (identity) and syntactic (pose) attributes of objects in space, can be decomposed into a geometric term, which can be maintained by a localization-and-mapping filter, and a likelihood function, which can be approximated by a discriminatively-trained convolutional neural network. The resulting system can process the video stream causally in real time, and provides a representation of objects in the scene that is persistent: Confidence in the presence of objects grows with evidence, and objects previously seen are kept in memory even when temporarily occluded, with their return into view automatically predicted to prime re-detection.



