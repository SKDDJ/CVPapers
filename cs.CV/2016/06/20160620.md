# Arxiv Papers in cs.CV on 2016-06-20
### Learning Convolutional Neural Networks using Hybrid Orthogonal Projection and Estimation
- **Arxiv ID**: http://arxiv.org/abs/1606.05929v4
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1606.05929v4)
- **Published**: 2016-06-20 00:19:43+00:00
- **Updated**: 2016-09-11 03:25:25+00:00
- **Authors**: Hengyue Pan, Hui Jiang
- **Comment**: 7 Pages, 5 figures, submitted to AAAI 2017
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have yielded the excellent performance in a variety of computer vision tasks, where CNNs typically adopt a similar structure consisting of convolution layers, pooling layers and fully connected layers. In this paper, we propose to apply a novel method, namely Hybrid Orthogonal Projection and Estimation (HOPE), to CNNs in order to introduce orthogonality into the CNN structure. The HOPE model can be viewed as a hybrid model to combine feature extraction using orthogonal linear projection with mixture models. It is an effective model to extract useful information from the original high-dimension feature vectors and meanwhile filter out irrelevant noises. In this work, we present three different ways to apply the HOPE models to CNNs, i.e., {\em HOPE-Input}, {\em single-HOPE-Block} and {\em multi-HOPE-Blocks}. For {\em HOPE-Input} CNNs, a HOPE layer is directly used right after the input to de-correlate high-dimension input feature vectors. Alternatively, in {\em single-HOPE-Block} and {\em multi-HOPE-Blocks} CNNs, we consider to use HOPE layers to replace one or more blocks in the CNNs, where one block may include several convolutional layers and one pooling layer. The experimental results on both Cifar-10 and Cifar-100 data sets have shown that the orthogonal constraints imposed by the HOPE layers can significantly improve the performance of CNNs in these image classification tasks (we have achieved one of the best performance when image augmentation has not been applied, and top 5 performance with image augmentation).



### A New Parallel Algorithm for Two-Pass Connected Component Labeling
- **Arxiv ID**: http://arxiv.org/abs/1606.05973v1
- **DOI**: 10.1109/IPDPSW.2014.152
- **Categories**: **cs.DS**, cs.CV, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/1606.05973v1)
- **Published**: 2016-06-20 05:13:28+00:00
- **Updated**: 2016-06-20 05:13:28+00:00
- **Authors**: Siddharth Gupta, Diana Palsetia, Md. Mostofa Ali Patwary, Ankit Agrawal, Alok Choudhary
- **Comment**: Parallel & Distributed Processing Symposium Workshops (IPDPSW), 2014
- **Journal**: None
- **Summary**: Connected Component Labeling (CCL) is an important step in pattern recognition and image processing. It assigns labels to the pixels such that adjacent pixels sharing the same features are assigned the same label. Typically, CCL requires several passes over the data. We focus on two-pass technique where each pixel is given a provisional label in the first pass whereas an actual label is assigned in the second pass.   We present a scalable parallel two-pass CCL algorithm, called PAREMSP, which employs a scan strategy and the best union-find technique called REMSP, which uses REM's algorithm for storing label equivalence information of pixels in a 2-D image. In the first pass, we divide the image among threads and each thread runs the scan phase along with REMSP simultaneously. In the second phase, we assign the final labels to the pixels. As REMSP is easily parallelizable, we use the parallel version of REMSP for merging the pixels on the boundary. Our experiments show the scalability of PAREMSP achieving speedups up to $20.1$ using $24$ cores on shared memory architecture using OpenMP for an image of size $465.20$ MB. We find that our proposed parallel algorithm achieves linear scaling for a large resolution fixed problem size as the number of processing elements are increased. Additionally, the parallel algorithm does not make use of any hardware specific routines, and thus is highly portable.



### Perfect Fingerprint Orientation Fields by Locally Adaptive Global Models
- **Arxiv ID**: http://arxiv.org/abs/1606.06007v1
- **DOI**: 10.1049/iet-bmt.2016.0087
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.06007v1)
- **Published**: 2016-06-20 08:23:36+00:00
- **Updated**: 2016-06-20 08:23:36+00:00
- **Authors**: Carsten Gottschlich, Benjamin Tams, Stephan Huckemann
- **Comment**: None
- **Journal**: IET Biometrics, vol. 6, no.3, pp. 183-190, May 2017
- **Summary**: Fingerprint recognition is widely used for verification and identification in many commercial, governmental and forensic applications. The orientation field (OF) plays an important role at various processing stages in fingerprint recognition systems. OFs are used for image enhancement, fingerprint alignment, for fingerprint liveness detection, fingerprint alteration detection and fingerprint matching. In this paper, a novel approach is presented to globally model an OF combined with locally adaptive methods. We show that this model adapts perfectly to the 'true OF' in the limit. This perfect OF is described by a small number of parameters with straightforward geometric interpretation. Applications are manifold: Quick expert marking of very poor quality (for instance latent) OFs, high fidelity low parameter OF compression and a direct road to ground truth OFs markings for large databases, say. In this contribution we describe an algorithm to perfectly estimate OF parameters automatically or semi-automatically, depending on image quality, and we establish the main underlying claim of high fidelity low parameter OF compression.



### DualNet: Domain-Invariant Network for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1606.06108v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.06108v2)
- **Published**: 2016-06-20 13:28:35+00:00
- **Updated**: 2017-05-04 07:54:06+00:00
- **Authors**: Kuniaki Saito, Andrew Shin, Yoshitaka Ushiku, Tatsuya Harada
- **Comment**: Accepted as an oral paper by ICME 2017
- **Journal**: None
- **Summary**: Visual question answering (VQA) task not only bridges the gap between images and language, but also requires that specific contents within the image are understood as indicated by linguistic context of the question, in order to generate the accurate answers. Thus, it is critical to build an efficient embedding of images and texts. We implement DualNet, which fully takes advantage of discriminative power of both image and textual features by separately performing two operations. Building an ensemble of DualNet further boosts the performance. Contrary to common belief, our method proved effective in both real images and abstract scenes, in spite of significantly different properties of respective domain. Our method was able to outperform previous state-of-the-art methods in real images category even without explicitly employing attention mechanism, and also outperformed our own state-of-the-art method in abstract scenes category, which recently won the first place in VQA Challenge 2016.



### Cutting out the middleman: measuring nuclear area in histopathology slides without segmentation
- **Arxiv ID**: http://arxiv.org/abs/1606.06127v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.06127v1)
- **Published**: 2016-06-20 14:10:32+00:00
- **Updated**: 2016-06-20 14:10:32+00:00
- **Authors**: Mitko Veta, Paul J. van Diest, Josien P. W. Pluim
- **Comment**: Conditionally accepted for MICCAI 2016
- **Journal**: None
- **Summary**: The size of nuclei in histological preparations from excised breast tumors is predictive of patient outcome (large nuclei indicate poor outcome). Pathologists take into account nuclear size when performing breast cancer grading. In addition, the mean nuclear area (MNA) has been shown to have independent prognostic value. The straightforward approach to measuring nuclear size is by performing nuclei segmentation. We hypothesize that given an image of a tumor region with known nuclei locations, the area of the individual nuclei and region statistics such as the MNA can be reliably computed directly from the image data by employing a machine learning model, without the intermediate step of nuclei segmentation. Towards this goal, we train a deep convolutional neural network model that is applied locally at each nucleus location, and can reliably measure the area of the individual nuclei and the MNA. Furthermore, we show how such an approach can be extended to perform combined nuclei detection and measurement, which is reminiscent of granulometry.



### The Minimum Cost Connected Subgraph Problem in Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1606.06135v1
- **DOI**: 10.1007/978-3-319-46726-9_46
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.06135v1)
- **Published**: 2016-06-20 14:22:31+00:00
- **Updated**: 2016-06-20 14:22:31+00:00
- **Authors**: Markus Rempfler, Bjoern Andres, Bjoern H. Menze
- **Comment**: accepted at MICCAI 2016
- **Journal**: None
- **Summary**: Several important tasks in medical image analysis can be stated in the form of an optimization problem whose feasible solutions are connected subgraphs. Examples include the reconstruction of neural or vascular structures under connectedness constraints. We discuss the minimum cost connected subgraph (MCCS) problem and its approximations from the perspective of medical applications. We propose a) objective-dependent constraints and b) novel constraint generation schemes to solve this optimization problem exactly by means of a branch-and-cut algorithm. These are shown to improve scalability and allow us to solve instances of two medical benchmark datasets to optimality for the first time. This enables us to perform a quantitative comparison between exact and approximative algorithms, where we identify the geodesic tree algorithm as an excellent alternative to exact inference on the examined datasets.



### Pragmatic factors in image description: the case of negations
- **Arxiv ID**: http://arxiv.org/abs/1606.06164v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1606.06164v2)
- **Published**: 2016-06-20 15:08:22+00:00
- **Updated**: 2016-06-27 09:06:28+00:00
- **Authors**: Emiel van Miltenburg, Roser Morante, Desmond Elliott
- **Comment**: Accepted as a short paper for the 5th Workshop on Vision and
  Language, collocated with ACL 2016, Berlin
- **Journal**: None
- **Summary**: We provide a qualitative analysis of the descriptions containing negations (no, not, n't, nobody, etc) in the Flickr30K corpus, and a categorization of negation uses. Based on this analysis, we provide a set of requirements that an image description system should have in order to generate negation sentences. As a pilot experiment, we used our categorization to manually annotate sentences containing negations in the Flickr30K corpus, with an agreement score of K=0.67. With this paper, we hope to open up a broader discussion of subjective language in image descriptions.



### Detection and Tracking of Liquids with Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1606.06266v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1606.06266v1)
- **Published**: 2016-06-20 19:40:29+00:00
- **Updated**: 2016-06-20 19:40:29+00:00
- **Authors**: Connor Schenck, Dieter Fox
- **Comment**: Published in the Proceedings of Robotics Science & Systems (RSS) 2016
  Workshop Are the Skeptics Right? Limits and Potentials of Deep Learning in
  Robotics
- **Journal**: None
- **Summary**: Recent advances in AI and robotics have claimed many incredible results with deep learning, yet no work to date has applied deep learning to the problem of liquid perception and reasoning. In this paper, we apply fully-convolutional deep neural networks to the tasks of detecting and tracking liquids. We evaluate three models: a single-frame network, multi-frame network, and a LSTM recurrent network. Our results show that the best liquid detection results are achieved when aggregating data over multiple frames, in contrast to standard image segmentation. They also show that the LSTM network outperforms the other two in both tasks. This suggests that LSTM-based neural networks have the potential to be a key component for enabling robots to handle liquids using robust, closed-loop controllers.



### Multiple Hypothesis Colorization
- **Arxiv ID**: http://arxiv.org/abs/1606.06314v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.06314v3)
- **Published**: 2016-06-20 20:15:34+00:00
- **Updated**: 2017-03-01 17:02:29+00:00
- **Authors**: Mohammad Haris Baig, Lorenzo Torresani
- **Comment**: 16 Pages (including Appendices)
- **Journal**: None
- **Summary**: In this work we focus on the problem of colorization for image compression. Since color information occupies a large proportion of the total storage size of an image, a method that can predict accurate color from its grayscale version can produce dramatic reduction in image file size. But colorization for compression poses several challenges. First, while colorization for artistic purposes simply involves predicting plausible chroma, colorization for compression requires generating output colors that are as close as possible to the ground truth. Second, many objects in the real world exhibit multiple possible colors. Thus, to disambiguate the colorization problem some additional information must be stored to reproduce the true colors with good accuracy. To account for the multimodal color distribution of objects we propose a deep tree-structured network that generates multiple color hypotheses for every pixel from a grayscale picture (as opposed to a single color produced by most prior colorization approaches). We show how to leverage the multimodal output of our model to reproduce with high fidelity the true colors of an image by storing very little additional information. In the experiments we show that our proposed method outperforms traditional JPEG color coding by a large margin, producing colors that are nearly indistinguishable from the ground truth at the storage cost of just a few hundred bytes for high-resolution pictures!



### Recognizing Surgical Activities with Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1606.06329v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.06329v2)
- **Published**: 2016-06-20 20:56:47+00:00
- **Updated**: 2016-06-22 14:07:56+00:00
- **Authors**: Robert DiPietro, Colin Lea, Anand Malpani, Narges Ahmidi, S. Swaroop Vedula, Gyusung I. Lee, Mija R. Lee, Gregory D. Hager
- **Comment**: Conditionally accepted at MICCAI 2016
- **Journal**: None
- **Summary**: We apply recurrent neural networks to the task of recognizing surgical activities from robot kinematics. Prior work in this area focuses on recognizing short, low-level activities, or gestures, and has been based on variants of hidden Markov models and conditional random fields. In contrast, we work on recognizing both gestures and longer, higher-level activites, or maneuvers, and we model the mapping from kinematics to gestures/maneuvers with recurrent neural networks. To our knowledge, we are the first to apply recurrent neural networks to this task. Using a single model and a single set of hyperparameters, we match state-of-the-art performance for gesture recognition and advance state-of-the-art performance for maneuver recognition, in terms of both accuracy and edit distance. Code is available at https://github.com/rdipietro/miccai-2016-surgical-activity-rec .



### Multiple Instance Hyperspectral Target Characterization
- **Arxiv ID**: http://arxiv.org/abs/1606.06354v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.06354v3)
- **Published**: 2016-06-20 22:35:12+00:00
- **Updated**: 2017-09-13 16:25:03+00:00
- **Authors**: Alina Zare, Changzhe Jiao, Taylor Glenn
- **Comment**: accepted version after revisions based on reviewer comments
- **Journal**: None
- **Summary**: In this paper, two methods for multiple instance target characterization, MI-SMF and MI-ACE, are presented. MI-SMF and MI-ACE estimate a discriminative target signature from imprecisely-labeled and mixed training data. In many applications, such as sub-pixel target detection in remotely-sensed hyperspectral imagery, accurate pixel-level labels on training data is often unavailable and infeasible to obtain. Furthermore, since sub-pixel targets are smaller in size than the resolution of a single pixel, training data is comprised only of mixed data points (in which target training points are mixtures of responses from both target and non-target classes). Results show improved, consistent performance over existing multiple instance concept learning methods on several hyperspectral sub-pixel target detection problems.



