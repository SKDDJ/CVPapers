# Arxiv Papers in cs.CV on 2016-06-14
### Inverting face embeddings with convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1606.04189v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1606.04189v2)
- **Published**: 2016-06-14 01:35:12+00:00
- **Updated**: 2016-07-07 18:52:57+00:00
- **Authors**: Andrey Zhmoginov, Mark Sandler
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have dramatically advanced the state of the art for many areas of machine learning. Recently they have been shown to have a remarkable ability to generate highly complex visual artifacts such as images and text rather than simply recognize them.   In this work we use neural networks to effectively invert low-dimensional face embeddings while producing realistically looking consistent images. Our contribution is twofold, first we show that a gradient ascent style approaches can be used to reproduce consistent images, with a help of a guiding image. Second, we demonstrate that we can train a separate neural network to effectively solve the minimization problem in one pass, and generate images in real-time. We then evaluate the loss imposed by using a neural network instead of the gradient descent by comparing the final values of the minimized loss function.



### DCNNs on a Diet: Sampling Strategies for Reducing the Training Set Size
- **Arxiv ID**: http://arxiv.org/abs/1606.04232v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1606.04232v1)
- **Published**: 2016-06-14 07:38:13+00:00
- **Updated**: 2016-06-14 07:38:13+00:00
- **Authors**: Maya Kabkab, Azadeh Alavi, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: Large-scale supervised classification algorithms, especially those based on deep convolutional neural networks (DCNNs), require vast amounts of training data to achieve state-of-the-art performance. Decreasing this data requirement would significantly speed up the training process and possibly improve generalization. Motivated by this objective, we consider the task of adaptively finding concise training subsets which will be iteratively presented to the learner. We use convex optimization methods, based on an objective criterion and feedback from the current performance of the classifier, to efficiently identify informative samples to train on. We propose an algorithm to decompose the optimization problem into smaller per-class problems, which can be solved in parallel. We test our approach on standard classification tasks and demonstrate its effectiveness in decreasing the training set size without compromising performance. We also show that our approach can make the classifier more robust in the presence of label noise and class imbalance.



### Richardson-Lucy Deblurring for Moving Light Field Cameras
- **Arxiv ID**: http://arxiv.org/abs/1606.04308v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.04308v2)
- **Published**: 2016-06-14 10:57:34+00:00
- **Updated**: 2017-05-19 08:20:54+00:00
- **Authors**: Donald G. Dansereau, Anders Eriksson, JÃ¼rgen Leitner
- **Comment**: Paper accepted for oral presentation at LF4CV workshop at CVPR
- **Journal**: None
- **Summary**: We generalize Richardson-Lucy (RL) deblurring to 4-D light fields by replacing the convolution steps with light field rendering of motion blur. The method deals correctly with blur caused by 6-degree-of-freedom camera motion in complex 3-D scenes, without performing depth estimation. We introduce a novel regularization term that maintains parallax information in the light field while reducing noise and ringing. We demonstrate the method operating effectively on rendered scenes and scenes captured using an off-the-shelf light field camera. An industrial robot arm provides repeatable and known trajectories, allowing us to establish quantitative performance in complex 3-D scenes. Qualitative and quantitative results confirm the effectiveness of the method, including commonly occurring cases for which previously published methods fail. We include mathematical proof that the algorithm converges to the maximum-likelihood estimate of the unblurred scene under Poisson noise. We expect extension to blind methods to be possible following the generalization of 2-D Richardson-Lucy to blind deconvolution.



### Neither Quick Nor Proper -- Evaluation of QuickProp for Learning Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1606.04333v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.04333v2)
- **Published**: 2016-06-14 12:57:56+00:00
- **Updated**: 2016-06-15 11:00:47+00:00
- **Authors**: Clemens-Alexander Brust, Sven Sickert, Marcel Simon, Erik Rodner, Joachim Denzler
- **Comment**: Technical Report, 11 pages, 6 figures
- **Journal**: None
- **Summary**: Neural networks and especially convolutional neural networks are of great interest in current computer vision research. However, many techniques, extensions, and modifications have been published in the past, which are not yet used by current approaches. In this paper, we study the application of a method called QuickProp for training of deep neural networks. In particular, we apply QuickProp during learning and testing of fully convolutional networks for the task of semantic segmentation. We compare QuickProp empirically with gradient descent, which is the current standard method. Experiments suggest that QuickProp can not compete with standard gradient descent techniques for complex computer vision tasks like semantic segmentation.



### Deep Learning with Darwin: Evolutionary Synthesis of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1606.04393v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1606.04393v3)
- **Published**: 2016-06-14 14:36:55+00:00
- **Updated**: 2017-02-06 22:51:33+00:00
- **Authors**: Mohammad Javad Shafiee, Akshaya Mishra, Alexander Wong
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Taking inspiration from biological evolution, we explore the idea of "Can deep neural networks evolve naturally over successive generations into highly efficient deep neural networks?" by introducing the notion of synthesizing new highly efficient, yet powerful deep neural networks over successive generations via an evolutionary process from ancestor deep neural networks. The architectural traits of ancestor deep neural networks are encoded using synaptic probability models, which can be viewed as the `DNA' of these networks. New descendant networks with differing network architectures are synthesized based on these synaptic probability models from the ancestor networks and computational environmental factor models, in a random manner to mimic heredity, natural selection, and random mutation. These offspring networks are then trained into fully functional networks, like one would train a newborn, and have more efficient, more diverse network architectures than their ancestor networks, while achieving powerful modeling capabilities. Experimental results for the task of visual saliency demonstrated that the synthesized `evolved' offspring networks can achieve state-of-the-art performance while having network architectures that are significantly more efficient (with a staggering $\sim$48-fold decrease in synapses by the fourth generation) compared to the original ancestor network.



### End-to-End Comparative Attention Networks for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1606.04404v2
- **DOI**: 10.1109/TIP.2017.2700762
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.04404v2)
- **Published**: 2016-06-14 14:51:59+00:00
- **Updated**: 2017-04-28 16:02:15+00:00
- **Authors**: Hao Liu, Jiashi Feng, Meibin Qi, Jianguo Jiang, Shuicheng Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification across disjoint camera views has been widely applied in video surveillance yet it is still a challenging problem. One of the major challenges lies in the lack of spatial and temporal cues, which makes it difficult to deal with large variations of lighting conditions, viewing angles, body poses and occlusions. Recently, several deep learning based person re-identification approaches have been proposed and achieved remarkable performance. However, most of those approaches extract discriminative features from the whole frame at one glimpse without differentiating various parts of the persons to identify. It is essentially important to examine multiple highly discriminative local regions of the person images in details through multiple glimpses for dealing with the large appearance variance. In this paper, we propose a new soft attention based model, i.e., the end to-end Comparative Attention Network (CAN), specifically tailored for the task of person re-identification. The end-to-end CAN learns to selectively focus on parts of pairs of person images after taking a few glimpses of them and adaptively comparing their appearance. The CAN model is able to learn which parts of images are relevant for discerning persons and automatically integrates information from different parts to determine whether a pair of images belongs to the same person. In other words, our proposed CAN model simulates the human perception process to verify whether two images are from the same person. Extensive experiments on three benchmark person re-identification datasets, including CUHK01, CHUHK03 and Market-1501, clearly demonstrate that our proposed end-to-end CAN for person re-identification outperforms well established baselines significantly and offer new state-of-the-art performance.



### Attend Refine Repeat: Active Box Proposal Generation via In-Out Localization
- **Arxiv ID**: http://arxiv.org/abs/1606.04446v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.04446v1)
- **Published**: 2016-06-14 16:35:08+00:00
- **Updated**: 2016-06-14 16:35:08+00:00
- **Authors**: Spyros Gidaris, Nikos Komodakis
- **Comment**: Technical report. Code as well as box proposals computed for several
  datasets are available at:: https://github.com/gidariss/AttractioNet
- **Journal**: None
- **Summary**: The problem of computing category agnostic bounding box proposals is utilized as a core component in many computer vision tasks and thus has lately attracted a lot of attention. In this work we propose a new approach to tackle this problem that is based on an active strategy for generating box proposals that starts from a set of seed boxes, which are uniformly distributed on the image, and then progressively moves its attention on the promising image areas where it is more likely to discover well localized bounding box proposals. We call our approach AttractioNet and a core component of it is a CNN-based category agnostic object location refinement module that is capable of yielding accurate and robust bounding box predictions regardless of the object category.   We extensively evaluate our AttractioNet approach on several image datasets (i.e. COCO, PASCAL, ImageNet detection and NYU-Depth V2 datasets) reporting on all of them state-of-the-art results that surpass the previous work in the field by a significant margin and also providing strong empirical evidence that our approach is capable to generalize to unseen categories. Furthermore, we evaluate our AttractioNet proposals in the context of the object detection task using a VGG16-Net based detector and the achieved detection performance on COCO manages to significantly surpass all other VGG16-Net based detectors while even being competitive with a heavily tuned ResNet-101 based detector. Code as well as box proposals computed for several datasets are available at:: https://github.com/gidariss/AttractioNet.



### Multiple Human Tracking in RGB-D Data: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1606.04450v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.04450v1)
- **Published**: 2016-06-14 16:41:55+00:00
- **Updated**: 2016-06-14 16:41:55+00:00
- **Authors**: Massimo Camplani, Adeline Paiement, Majid Mirmehdi, Dima Damen, Sion Hannuna, Tilo Burghardt, Lili Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Multiple human tracking (MHT) is a fundamental task in many computer vision applications. Appearance-based approaches, primarily formulated on RGB data, are constrained and affected by problems arising from occlusions and/or illumination variations. In recent years, the arrival of cheap RGB-Depth (RGB-D) devices has {led} to many new approaches to MHT, and many of these integrate color and depth cues to improve each and every stage of the process. In this survey, we present the common processing pipeline of these methods and review their methodology based (a) on how they implement this pipeline and (b) on what role depth plays within each stage of it. We identify and introduce existing, publicly available, benchmark datasets and software resources that fuse color and depth data for MHT. Finally, we present a brief comparative evaluation of the performance of those works that have applied their methods to these datasets.



### Efficient adaptation of complex-valued noiselet sensing matrices for compressed single-pixel imaging
- **Arxiv ID**: http://arxiv.org/abs/1606.04535v1
- **DOI**: 10.1364/AO.55.005141
- **Categories**: **cs.CV**, cs.IT, math.IT, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1606.04535v1)
- **Published**: 2016-06-14 17:34:37+00:00
- **Updated**: 2016-06-14 17:34:37+00:00
- **Authors**: Anna Pastuszczak, BartÅomiej SzczygieÅ, MichaÅ MikoÅajczyk, RafaÅ KotyÅski
- **Comment**: 9 pages, 3 figures, 1 table; accepted for publication in Applied
  Optics (2016)
- **Journal**: Appl. Opt. 55(19), 5141-5148 (2016)
- **Summary**: Minimal mutual coherence of discrete noiselets and Haar wavelets makes this pair of bases an essential choice for the measurement and compression matrices in compressed-sensing-based single-pixel detectors. In this paper we propose an efficient way of using complex-valued and non-binary noiselet functions for object sampling in single-pixel cameras with binary spatial light modulators and incoherent illumination. The proposed method allows to determine m complex noiselet coefficients from m+1 binary sampling measurements. Further, we introduce a modification to the complex fast noiselet transform, which enables computationally-efficient real-time generation of the binary noiselet-based patterns using efficient integer calculations on bundled patterns. The proposed method is verified experimentally with a single-pixel camera system using a binary spatial light modulator.



### Max-Margin Feature Selection
- **Arxiv ID**: http://arxiv.org/abs/1606.04506v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1606.04506v1)
- **Published**: 2016-06-14 19:05:01+00:00
- **Updated**: 2016-06-14 19:05:01+00:00
- **Authors**: Yamuna Prasad, Dinesh Khandelwal, K. K. Biswas
- **Comment**: submitted to PR Letters
- **Journal**: None
- **Summary**: Many machine learning applications such as in vision, biology and social networking deal with data in high dimensions. Feature selection is typically employed to select a subset of features which im- proves generalization accuracy as well as reduces the computational cost of learning the model. One of the criteria used for feature selection is to jointly minimize the redundancy and maximize the rele- vance of the selected features. In this paper, we formulate the task of feature selection as a one class SVM problem in a space where features correspond to the data points and instances correspond to the dimensions. The goal is to look for a representative subset of the features (support vectors) which describes the boundary for the region where the set of the features (data points) exists. This leads to a joint optimization of relevance and redundancy in a principled max-margin framework. Additionally, our formulation enables us to leverage existing techniques for optimizing the SVM objective resulting in highly computationally efficient solutions for the task of feature selection. Specifically, we employ the dual coordinate descent algorithm (Hsieh et al., 2008), originally proposed for SVMs, for our formulation. We use a sparse representation to deal with data in very high dimensions. Experiments on seven publicly available benchmark datasets from a variety of domains show that our approach results in orders of magnitude faster solutions even while retaining the same level of accuracy compared to the state of the art feature selection techniques.



### Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1606.04586v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.04586v1)
- **Published**: 2016-06-14 22:30:08+00:00
- **Updated**: 2016-06-14 22:30:08+00:00
- **Authors**: Mehdi Sajjadi, Mehran Javanmardi, Tolga Tasdizen
- **Comment**: 9 pages, 2 figures, 5 tables
- **Journal**: None
- **Summary**: Effective convolutional neural networks are trained on large sets of labeled data. However, creating large labeled datasets is a very costly and time-consuming task. Semi-supervised learning uses unlabeled data to train a model with higher accuracy when there is a limited set of labeled data available. In this paper, we consider the problem of semi-supervised learning with convolutional neural networks. Techniques such as randomized data augmentation, dropout and random max-pooling provide better generalization and stability for classifiers that are trained using gradient descent. Multiple passes of an individual sample through the network might lead to different predictions due to the non-deterministic behavior of these techniques. We propose an unsupervised loss function that takes advantage of the stochastic nature of these methods and minimizes the difference between the predictions of multiple passes of a training sample through the network. We evaluate the proposed method on several benchmark datasets.



### In the Shadows, Shape Priors Shine: Using Occlusion to Improve Multi-Region Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1606.04590v1
- **DOI**: None
- **Categories**: **cs.CV**, 65D19, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/1606.04590v1)
- **Published**: 2016-06-14 23:12:40+00:00
- **Updated**: 2016-06-14 23:12:40+00:00
- **Authors**: Yuka Kihara, Matvey Soloviev, Tsuhan Chen
- **Comment**: Camera ready version accepted at CVPR 2016
- **Journal**: None
- **Summary**: We present a new algorithm for multi-region segmentation of 2D images with objects that may partially occlude each other. Our algorithm is based on the observation hat human performance on this task is based both on prior knowledge about plausible shapes and taking into account the presence of occluding objects whose shape is already known - once an occluded region is identified, the shape prior can be used to guess the shape of the missing part. We capture the former aspect using a deep learning model of shape; for the latter, we simultaneously minimize the energy of all regions and consider only unoccluded pixels for data agreement.   Existing algorithms incorporating object shape priors consider every object separately in turn and can't distinguish genuine deviation from the expected shape from parts missing due to occlusion. We show that our method significantly improves on the performance of a representative algorithm, as evaluated on both preprocessed natural and synthetic images. Furthermore, on the synthetic images, we recover the ground truth segmentation with good accuracy.



