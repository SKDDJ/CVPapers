# Arxiv Papers in cs.CV on 2016-06-02
### Dictionary Learning for Robotic Grasp Recognition and Detection
- **Arxiv ID**: http://arxiv.org/abs/1606.00538v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1606.00538v1)
- **Published**: 2016-06-02 05:20:14+00:00
- **Updated**: 2016-06-02 05:20:14+00:00
- **Authors**: Ludovic Trottier, Philippe Gigu√®re, Brahim Chaib-draa
- **Comment**: Submitted at the 2016 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS 2016)
- **Journal**: None
- **Summary**: The ability to grasp ordinary and potentially never-seen objects is an important feature in both domestic and industrial robotics. For a system to accomplish this, it must autonomously identify grasping locations by using information from various sensors, such as Microsoft Kinect 3D camera. Despite numerous progress, significant work still remains to be done in this field. To this effect, we propose a dictionary learning and sparse representation (DLSR) framework for representing RGBD images from 3D sensors in the context of determining such good grasping locations. In contrast to previously proposed approaches that relied on sophisticated regularization or very large datasets, the derived perception system has a fast training phase and can work with small datasets. It is also theoretically founded for dealing with masked-out entries, which are common with 3D sensors. We contribute by presenting a comparative study of several DLSR approach combinations for recognizing and detecting grasp candidates on the standard Cornell dataset. Importantly, experimental results show a performance improvement of 1.69% in detection and 3.16% in recognition over current state-of-the-art convolutional neural network (CNN). Even though nowadays most popular vision-based approach is CNN, this suggests that DLSR is also a viable alternative with interesting advantages that CNN has not.



### Recursive Autoconvolution for Unsupervised Learning of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1606.00611v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1606.00611v2)
- **Published**: 2016-06-02 10:37:46+00:00
- **Updated**: 2017-03-26 18:31:05+00:00
- **Authors**: Boris Knyazev, Erhardt Barth, Thomas Martinetz
- **Comment**: 8 pages, accepted to International Joint Conference on Neural
  Networks (IJCNN 2017)
- **Journal**: None
- **Summary**: In visual recognition tasks, such as image classification, unsupervised learning exploits cheap unlabeled data and can help to solve these tasks more efficiently. We show that the recursive autoconvolution operator, adopted from physics, boosts existing unsupervised methods by learning more discriminative filters. We take well established convolutional neural networks and train their filters layer-wise. In addition, based on previous works we design a network which extracts more than 600k features per sample, but with the total number of trainable parameters greatly reduced by introducing shared filters in higher layers. We evaluate our networks on the MNIST, CIFAR-10, CIFAR-100 and STL-10 image classification benchmarks and report several state of the art results among other unsupervised methods.



### Storytelling of Photo Stream with Bidirectional Multi-thread Recurrent Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1606.00625v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.00625v1)
- **Published**: 2016-06-02 11:13:04+00:00
- **Updated**: 2016-06-02 11:13:04+00:00
- **Authors**: Yu Liu, Jianlong Fu, Tao Mei, Chang Wen Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Visual storytelling aims to generate human-level narrative language (i.e., a natural paragraph with multiple sentences) from a photo streams. A typical photo story consists of a global timeline with multi-thread local storylines, where each storyline occurs in one different scene. Such complex structure leads to large content gaps at scene transitions between consecutive photos. Most existing image/video captioning methods can only achieve limited performance, because the units in traditional recurrent neural networks (RNN) tend to "forget" the previous state when the visual sequence is inconsistent. In this paper, we propose a novel visual storytelling approach with Bidirectional Multi-thread Recurrent Neural Network (BMRNN). First, based on the mined local storylines, a skip gated recurrent unit (sGRU) with delay control is proposed to maintain longer range visual information. Second, by using sGRU as basic units, the BMRNN is trained to align the local storylines into the global sequential timeline. Third, a new training scheme with a storyline-constrained objective function is proposed by jointly considering both global and local matches. Experiments on three standard storytelling datasets show that the BMRNN model outperforms the state-of-the-art methods.



### Multi-View Treelet Transform
- **Arxiv ID**: http://arxiv.org/abs/1606.00800v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.SI, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1606.00800v2)
- **Published**: 2016-06-02 18:51:09+00:00
- **Updated**: 2016-06-17 19:07:46+00:00
- **Authors**: Brian A. Mitchell, Linda R. Petzold
- **Comment**: None
- **Journal**: None
- **Summary**: Current multi-view factorization methods make assumptions that are not acceptable for many kinds of data, and in particular, for graphical data with hierarchical structure. At the same time, current hierarchical methods work only in the single-view setting. We generalize the Treelet Transform to the Multi-View Treelet Transform (MVTT) to allow for the capture of hierarchical structure when multiple views are available. Further, we show how this generalization is consistent with the existing theory and how it might be used in denoising empirical networks and in computing the shared response of functional brain data.



### Unifying Geometric Features and Facial Action Units for Improved Performance of Facial Expression Analysis
- **Arxiv ID**: http://arxiv.org/abs/1606.00822v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1606.00822v1)
- **Published**: 2016-06-02 19:43:29+00:00
- **Updated**: 2016-06-02 19:43:29+00:00
- **Authors**: Mehdi Ghayoumi, Arvind K Bansal
- **Comment**: 8 pages, ISBN: 978-1-61804-285-9
- **Journal**: None
- **Summary**: Previous approaches to model and analyze facial expression analysis use three different techniques: facial action units, geometric features and graph based modelling. However, previous approaches have treated these technique separately. There is an interrelationship between these techniques. The facial expression analysis is significantly improved by utilizing these mappings between major geometric features involved in facial expressions and the subset of facial action units whose presence or absence are unique to a facial expression. This paper combines dimension reduction techniques and image classification with search space pruning achieved by this unique subset of facial action units to significantly prune the search space. The performance results on the publicly facial expression database shows an improvement in performance by 70% over time while maintaining the emotion recognition correctness.



### Face Detection with End-to-End Integration of a ConvNet and a 3D Model
- **Arxiv ID**: http://arxiv.org/abs/1606.00850v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.00850v3)
- **Published**: 2016-06-02 20:08:28+00:00
- **Updated**: 2016-08-29 14:57:17+00:00
- **Authors**: Yunzhu Li, Benyuan Sun, Tianfu Wu, Yizhou Wang
- **Comment**: 16 pages, Y. Li and B. Sun contributed equally to this work
- **Journal**: None
- **Summary**: This paper presents a method for face detection in the wild, which integrates a ConvNet and a 3D mean face model in an end-to-end multi-task discriminative learning framework. The 3D mean face model is predefined and fixed (e.g., we used the one provided in the AFLW dataset). The ConvNet consists of two components: (i) The face pro- posal component computes face bounding box proposals via estimating facial key-points and the 3D transformation (rotation and translation) parameters for each predicted key-point w.r.t. the 3D mean face model. (ii) The face verification component computes detection results by prun- ing and refining proposals based on facial key-points based configuration pooling. The proposed method addresses two issues in adapting state- of-the-art generic object detection ConvNets (e.g., faster R-CNN) for face detection: (i) One is to eliminate the heuristic design of prede- fined anchor boxes in the region proposals network (RPN) by exploit- ing a 3D mean face model. (ii) The other is to replace the generic RoI (Region-of-Interest) pooling layer with a configuration pooling layer to respect underlying object structures. The multi-task loss consists of three terms: the classification Softmax loss and the location smooth l1 -losses [14] of both the facial key-points and the face bounding boxes. In ex- periments, our ConvNet is trained on the AFLW dataset only and tested on the FDDB benchmark with fine-tuning and on the AFW benchmark without fine-tuning. The proposed method obtains very competitive state-of-the-art performance in the two benchmarks.



### DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs
- **Arxiv ID**: http://arxiv.org/abs/1606.00915v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.00915v2)
- **Published**: 2016-06-02 21:52:21+00:00
- **Updated**: 2017-05-12 03:25:47+00:00
- **Authors**: Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille
- **Comment**: Accepted by TPAMI
- **Journal**: None
- **Summary**: In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "DeepLab" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7% mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.



### Comparison of 14 different families of classification algorithms on 115 binary datasets
- **Arxiv ID**: http://arxiv.org/abs/1606.00930v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1606.00930v1)
- **Published**: 2016-06-02 23:01:25+00:00
- **Updated**: 2016-06-02 23:01:25+00:00
- **Authors**: Jacques Wainer
- **Comment**: None
- **Journal**: None
- **Summary**: We tested 14 very different classification algorithms (random forest, gradient boosting machines, SVM - linear, polynomial, and RBF - 1-hidden-layer neural nets, extreme learning machines, k-nearest neighbors and a bagging of knn, naive Bayes, learning vector quantization, elastic net logistic regression, sparse linear discriminant analysis, and a boosting of linear classifiers) on 115 real life binary datasets. We followed the Demsar analysis and found that the three best classifiers (random forest, gbm and RBF SVM) are not significantly different from each other. We also discuss that a change of less then 0.0112 in the error rate should be considered as an irrelevant change, and used a Bayesian ANOVA analysis to conclude that with high probability the differences between these three classifiers is not of practical consequence. We also verified the execution time of "standard implementations" of these algorithms and concluded that RBF SVM is the fastest (significantly so) both in training time and in training plus testing time.



