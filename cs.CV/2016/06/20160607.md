# Arxiv Papers in cs.CV on 2016-06-07
### Deep neural networks are robust to weight binarization and other non-linear distortions
- **Arxiv ID**: http://arxiv.org/abs/1606.01981v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1606.01981v1)
- **Published**: 2016-06-07 00:28:42+00:00
- **Updated**: 2016-06-07 00:28:42+00:00
- **Authors**: Paul Merolla, Rathinakumar Appuswamy, John Arthur, Steve K. Esser, Dharmendra Modha
- **Comment**: None
- **Journal**: None
- **Summary**: Recent results show that deep neural networks achieve excellent performance even when, during training, weights are quantized and projected to a binary representation. Here, we show that this is just the tip of the iceberg: these same networks, during testing, also exhibit a remarkable robustness to distortions beyond quantization, including additive and multiplicative noise, and a class of non-linear projections where binarization is just a special case. To quantify this robustness, we show that one such network achieves 11% test error on CIFAR-10 even with 0.68 effective bits per weight. Furthermore, we find that a common training heuristic--namely, projecting quantized weights during backpropagation--can be altered (or even removed) and networks still achieve a base level of robustness during testing. Specifically, training with weight projections other than quantization also works, as does simply clipping the weights, both of which have never been reported before. We confirm our results for CIFAR-10 and ImageNet datasets. Finally, drawing from these ideas, we propose a stochastic projection rule that leads to a new state of the art network with 7.64% test error on CIFAR-10 using no data augmentation.



### Learning deep structured network for weakly supervised change detection
- **Arxiv ID**: http://arxiv.org/abs/1606.02009v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.02009v2)
- **Published**: 2016-06-07 03:20:37+00:00
- **Updated**: 2017-05-23 01:22:06+00:00
- **Authors**: Salman H Khan, Xuming He, Fatih Porikli, Mohammed Bennamoun, Ferdous Sohel, Roberto Togneri
- **Comment**: None
- **Journal**: None
- **Summary**: Conventional change detection methods require a large number of images to learn background models or depend on tedious pixel-level labeling by humans. In this paper, we present a weakly supervised approach that needs only image-level labels to simultaneously detect and localize changes in a pair of images. To this end, we employ a deep neural network with DAG topology to learn patterns of change from image-level labeled training data. On top of the initial CNN activations, we define a CRF model to incorporate the local differences and context with the dense connections between individual pixels. We apply a constrained mean-field algorithm to estimate the pixel-level labels, and use the estimated labels to update the parameters of the CNN in an iterative EM framework. This enables imposing global constraints on the observed foreground probability mass function. Our evaluations on four benchmark datasets demonstrate superior detection and localization performance.



### Hand Action Detection from Ego-centric Depth Sequences with Error-correcting Hough Transform
- **Arxiv ID**: http://arxiv.org/abs/1606.02031v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.02031v1)
- **Published**: 2016-06-07 05:02:14+00:00
- **Updated**: 2016-06-07 05:02:14+00:00
- **Authors**: Chi Xu, Lakshmi Narasimhan Govindarajan, Li Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting hand actions from ego-centric depth sequences is a practically challenging problem, owing mostly to the complex and dexterous nature of hand articulations as well as non-stationary camera motion. We address this problem via a Hough transform based approach coupled with a discriminatively learned error-correcting component to tackle the well known issue of incorrect votes from the Hough transform. In this framework, local parts vote collectively for the start $\&$ end positions of each action over time. We also construct an in-house annotated dataset of 300 long videos, containing 3,177 single-action subsequences over 16 action classes collected from 26 individuals. Our system is empirically evaluated on this real-life dataset for both the action recognition and detection tasks, and is shown to produce satisfactory results. To facilitate reproduction, the new dataset and our implementation are also provided online.



### Enhanced high dynamic range 3D shape measurement based on generalized phase-shifting algorithm
- **Arxiv ID**: http://arxiv.org/abs/1606.02288v1
- **DOI**: 10.1016/j.optcom.2016.10.023
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1606.02288v1)
- **Published**: 2016-06-07 07:25:56+00:00
- **Updated**: 2016-06-07 07:25:56+00:00
- **Authors**: Minmin Wang, Guangliang Du, Canlin Zhou, Chaorui Zhang, Shuchun Si, Hui Li, Zhenkun Lei, YanJie Li
- **Comment**: 18 pages, 22 figures
- **Journal**: None
- **Summary**: It is a challenge for Phase Measurement Profilometry (PMP) to measure objects with a large range of reflectivity variation across the surface. Saturated or dark pixels in the deformed fringe patterns captured by the camera will lead to phase fluctuations and errors. Jiang et al. proposed a high dynamic range real-time 3D shape measurement method without changing camera exposures. Three inverted phase-shifted fringe patterns are used to complement three regular phase-shifted fringe patterns for phase retrieval when any of the regular fringe patterns are saturated. But Jiang's method still has some drawbacks: (1) The phases in saturated pixels are respectively estimated by different formulas for different cases. It is shortage of an universal formula; (2) it cannot be extended to four-step phase-shifting algorithm because inverted fringe patterns are the repetition of regular fringe patterns; (3) only three unsaturated intensity values at every pixel of fringe patterns are chosen for phase demodulation, lying idle the other unsaturated ones. We proposed a method for enhanced high dynamic range 3D shape measurement based on generalized phase-shifting algorithm, which combines the complementary technique of inverted and regular fringe patterns with generalized phase-shifting algorithm. Firstly, two sets of complementary phase-shifted fringe patterns, namely regular and inverted fringe patterns are projected and collected. Then all unsaturated intensity values at the same camera pixel from two sets of fringe patterns are selected, and employed to retrieve the phase by generalized phase-shifting algorithm. Finally, simulations and experiments are conducted to prove the validity of the proposed method. The results are analyzed and compared with Jiang's method, which demonstrate that the proposed method not only expands the scope of Jiang's method, but also improves the measurement accuracy.



### Joint Recursive Monocular Filtering of Camera Motion and Disparity Map
- **Arxiv ID**: http://arxiv.org/abs/1606.02092v1
- **DOI**: None
- **Categories**: **cs.CV**, math.OC, 49J15
- **Links**: [PDF](http://arxiv.org/pdf/1606.02092v1)
- **Published**: 2016-06-07 10:53:19+00:00
- **Updated**: 2016-06-07 10:53:19+00:00
- **Authors**: Johannes Berger, Christoph Schn√∂rr
- **Comment**: Preprint. The final publication will be available at Springer
- **Journal**: None
- **Summary**: Monocular scene reconstruction is essential for modern applications such as robotics or autonomous driving. Although stereo methods usually result in better accuracy than monocular methods, they are more expensive and more difficult to calibrate. In this work, we present a novel second order optimal minimum energy filter that jointly estimates the camera motion, the disparity map and also higher order kinematics recursively on a product Lie group containing a novel disparity group. This mathematical framework enables to cope with non-Euclidean state spaces, non-linear observations and high dimensions which is infeasible for most classical filters. To be robust against outliers, we use a generalized Charbonnier energy function in this framework rather than a quadratic energy function as proposed in related work. Experiments confirm that our method enables accurate reconstructions on-par with state-of-the-art.



### ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1606.02147v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.02147v1)
- **Published**: 2016-06-07 14:09:27+00:00
- **Updated**: 2016-06-07 14:09:27+00:00
- **Authors**: Adam Paszke, Abhishek Chaurasia, Sangpil Kim, Eugenio Culurciello
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to perform pixel-wise semantic segmentation in real-time is of paramount importance in mobile applications. Recent deep neural networks aimed at this task have the disadvantage of requiring a large number of floating point operations and have long run-times that hinder their usability. In this paper, we propose a novel deep neural network architecture named ENet (efficient neural network), created specifically for tasks requiring low latency operation. ENet is up to 18$\times$ faster, requires 75$\times$ less FLOPs, has 79$\times$ less parameters, and provides similar or better accuracy to existing models. We have tested it on CamVid, Cityscapes and SUN datasets and report on comparisons with existing state-of-the-art methods, and the trade-offs between accuracy and processing time of a network. We present performance measurements of the proposed architecture on embedded systems and suggest possible software improvements that could make ENet even faster.



### Latent Constrained Correlation Filters for Object Localization
- **Arxiv ID**: http://arxiv.org/abs/1606.02170v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.02170v2)
- **Published**: 2016-06-07 15:13:00+00:00
- **Updated**: 2017-06-03 09:05:04+00:00
- **Authors**: Shangzhen Luan, Baochang Zhang, Jungong Han, Chen Chen, Ling Shao, Alessandro Perina, Linlin Shen
- **Comment**: There are small problems and theories need to be perfected
- **Journal**: None
- **Summary**: There is a neglected fact in the traditional machine learning methods that the data sampling can actually lead to the solution sampling. We consider this observation to be important because having the solution sampling available makes the variable distribution estimation, which is a problem in many learning-related applications, more tractable. In this paper, we implement this idea on correlation filter, which has attracted much attention in the past few years due to its high performance with a low computational cost. More specifically, we propose a new method, named latent constrained correlation filters (LCCF) by mapping the correlation filters to a given latent subspace, in which we establish a new learning framework that embeds distribution-related constraints into the original problem. We further introduce a subspace based alternating direction method of multipliers (SADMM) to efficiently solve the optimization problem, which is proved to converge at the saddle point. Our approach is successfully applied to two different tasks inclduing eye localization and car detection. Extensive experiments demonstrate that LCCF outperforms the state-of-the-art methods when samples are suffered from noise and occlusion.



### Selective Unsupervised Feature Learning with Convolutional Neural Network (S-CNN)
- **Arxiv ID**: http://arxiv.org/abs/1606.02210v1
- **DOI**: 10.1109/ICPR.2016.7900009
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.02210v1)
- **Published**: 2016-06-07 16:52:47+00:00
- **Updated**: 2016-06-07 16:52:47+00:00
- **Authors**: Amir Ghaderi, Vassilis Athitsos
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised learning of convolutional neural networks (CNNs) can require very large amounts of labeled data. Labeling thousands or millions of training examples can be extremely time consuming and costly. One direction towards addressing this problem is to create features from unlabeled data. In this paper we propose a new method for training a CNN, with no need for labeled instances. This method for unsupervised feature learning is then successfully applied to a challenging object recognition task. The proposed algorithm is relatively simple, but attains accuracy comparable to that of more sophisticated methods. The proposed method is significantly easier to train, compared to existing CNN methods, making fewer requirements on manually labeled training data. It is also shown to be resistant to overfitting. We provide results on some well-known datasets, namely STL-10, CIFAR-10, and CIFAR-100. The results show that our method provides competitive performance compared with existing alternative methods. Selective Convolutional Neural Network (S-CNN) is a simple and fast algorithm, it introduces a new way to do unsupervised feature learning, and it provides discriminative features which generalize well.



### Systematic evaluation of CNN advances on the ImageNet
- **Arxiv ID**: http://arxiv.org/abs/1606.02228v2
- **DOI**: 10.1016/j.cviu.2017.05.007
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1606.02228v2)
- **Published**: 2016-06-07 17:38:06+00:00
- **Updated**: 2016-06-13 13:48:39+00:00
- **Authors**: Dmytro Mishkin, Nikolay Sergievskiy, Jiri Matas
- **Comment**: Submitted to CVIU Special Issue on Deep Learning. Updated dataset
  quality experiment
- **Journal**: None
- **Summary**: The paper systematically studies the impact of a range of recent advances in CNN architectures and learning methods on the object categorization (ILSVRC) problem. The evalution tests the influence of the following choices of the architecture: non-linearity (ReLU, ELU, maxout, compatibility with batch normalization), pooling variants (stochastic, max, average, mixed), network width, classifier design (convolutional, fully-connected, SPP), image pre-processing, and of learning parameters: learning rate, batch size, cleanliness of the data, etc.   The performance gains of the proposed modifications are first tested individually and then in combination. The sum of individual gains is bigger than the observed improvement when all modifications are introduced, but the "deficit" is small suggesting independence of their benefits. We show that the use of 128x128 pixel images is sufficient to make qualitative conclusions about optimal network structure that hold for the full size Caffe and VGG nets. The results are obtained an order of magnitude faster than with the standard 224 pixel images.



### Longitudinal Face Modeling via Temporal Deep Restricted Boltzmann Machines
- **Arxiv ID**: http://arxiv.org/abs/1606.02254v1
- **DOI**: 10.1007/s11263-019-01165-5
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.02254v1)
- **Published**: 2016-06-07 18:37:32+00:00
- **Updated**: 2016-06-07 18:37:32+00:00
- **Authors**: Chi Nhan Duong, Khoa Luu, Kha Gia Quach, Tien D. Bui
- **Comment**: in The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR) 2016
- **Journal**: None
- **Summary**: Modeling the face aging process is a challenging task due to large and non-linear variations present in different stages of face development. This paper presents a deep model approach for face age progression that can efficiently capture the non-linear aging process and automatically synthesize a series of age-progressed faces in various age ranges. In this approach, we first decompose the long-term age progress into a sequence of short-term changes and model it as a face sequence. The Temporal Deep Restricted Boltzmann Machines based age progression model together with the prototype faces are then constructed to learn the aging transformation between faces in the sequence. In addition, to enhance the wrinkles of faces in the later age ranges, the wrinkle models are further constructed using Restricted Boltzmann Machines to capture their variations in different facial regions. The geometry constraints are also taken into account in the last step for more consistent age-progressed results. The proposed approach is evaluated using various face aging databases, i.e. FG-NET, Cross-Age Celebrity Dataset (CACD) and MORPH, and our collected large-scale aging database named AginG Faces in the Wild (AGFW). In addition, when ground-truth age is not available for input image, our proposed system is able to automatically estimate the age of the input face before aging process is employed.



### Multilingual Visual Sentiment Concept Matching
- **Arxiv ID**: http://arxiv.org/abs/1606.02276v1
- **DOI**: 10.1145/2911996.2912016
- **Categories**: **cs.CL**, cs.CV, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1606.02276v1)
- **Published**: 2016-06-07 19:40:00+00:00
- **Updated**: 2016-06-07 19:40:00+00:00
- **Authors**: Nikolaos Pappas, Miriam Redi, Mercan Topkara, Brendan Jou, Hongyi Liu, Tao Chen, Shih-Fu Chang
- **Comment**: None
- **Journal**: Proceedings ICMR '16 Proceedings of the 2016 ACM on International
  Conference on Multimedia Retrieval Pages 151-158
- **Summary**: The impact of culture in visual emotion perception has recently captured the attention of multimedia research. In this study, we pro- vide powerful computational linguistics tools to explore, retrieve and browse a dataset of 16K multilingual affective visual concepts and 7.3M Flickr images. First, we design an effective crowdsourc- ing experiment to collect human judgements of sentiment connected to the visual concepts. We then use word embeddings to repre- sent these concepts in a low dimensional vector space, allowing us to expand the meaning around concepts, and thus enabling insight about commonalities and differences among different languages. We compare a variety of concept representations through a novel evaluation task based on the notion of visual semantic relatedness. Based on these representations, we design clustering schemes to group multilingual visual concepts, and evaluate them with novel metrics based on the crowdsourced sentiment annotations as well as visual semantic relatedness. The proposed clustering framework enables us to analyze the full multilingual dataset in-depth and also show an application on a facial data subset, exploring cultural in- sights of portrait-related affective visual concepts.



### Semi-Supervised Domain Adaptation for Weakly Labeled Semantic Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1606.02280v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.02280v1)
- **Published**: 2016-06-07 19:54:53+00:00
- **Updated**: 2016-06-07 19:54:53+00:00
- **Authors**: Huiling Wang, Tapani Raiko, Lasse Lensu, Tinghuai Wang, Juha Karhunen
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) have been immensely successful in many high-level computer vision tasks given large labeled datasets. However, for video semantic object segmentation, a domain where labels are scarce, effectively exploiting the representation power of CNN with limited training data remains a challenge. Simply borrowing the existing pretrained CNN image recognition model for video segmentation task can severely hurt performance. We propose a semi-supervised approach to adapting CNN image recognition model trained from labeled image data to the target domain exploiting both semantic evidence learned from CNN, and the intrinsic structures of video data. By explicitly modeling and compensating for the domain shift from the source domain to the target domain, this proposed approach underpins a robust semantic object segmentation method against the changes in appearance, shape and occlusion in natural videos. We present extensive experiments on challenging datasets that demonstrate the superior performance of our approach compared with the state-of-the-art methods.



