# Arxiv Papers in cs.CV on 2016-06-28
### Alternating Back-Propagation for Generator Network
- **Arxiv ID**: http://arxiv.org/abs/1606.08571v4
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1606.08571v4)
- **Published**: 2016-06-28 06:46:05+00:00
- **Updated**: 2016-12-06 04:04:19+00:00
- **Authors**: Tian Han, Yang Lu, Song-Chun Zhu, Ying Nian Wu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes an alternating back-propagation algorithm for learning the generator network model. The model is a non-linear generalization of factor analysis. In this model, the mapping from the continuous latent factors to the observed signal is parametrized by a convolutional neural network. The alternating back-propagation algorithm iterates the following two steps: (1) Inferential back-propagation, which infers the latent factors by Langevin dynamics or gradient descent. (2) Learning back-propagation, which updates the parameters given the inferred latent factors by gradient descent. The gradient computations in both steps are powered by back-propagation, and they share most of their code in common. We show that the alternating back-propagation algorithm can learn realistic generator models of natural images, video sequences, and sounds. Moreover, it can also be used to learn from incomplete or indirect training data.



### Diversified Visual Attention Networks for Fine-Grained Object Classification
- **Arxiv ID**: http://arxiv.org/abs/1606.08572v2
- **DOI**: 10.1109/TMM.2017.2648498
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.08572v2)
- **Published**: 2016-06-28 06:47:41+00:00
- **Updated**: 2017-05-18 07:36:27+00:00
- **Authors**: Bo Zhao, Xiao Wu, Jiashi Feng, Qiang Peng, Shuicheng Yan
- **Comment**: None
- **Journal**: Published in: IEEE Transactions on Multimedia ( Volume: 19, Issue:
  6, June 2017 )
- **Summary**: Fine-grained object classification is a challenging task due to the subtle inter-class difference and large intra-class variation. Recently, visual attention models have been applied to automatically localize the discriminative regions of an image for better capturing critical difference and demonstrated promising performance. However, without consideration of the diversity in attention process, most of existing attention models perform poorly in classifying fine-grained objects. In this paper, we propose a diversified visual attention network (DVAN) to address the problems of fine-grained object classification, which substan- tially relieves the dependency on strongly-supervised information for learning to localize discriminative regions compared with attentionless models. More importantly, DVAN explicitly pursues the diversity of attention and is able to gather discriminative information to the maximal extent. Multiple attention canvases are generated to extract convolutional features for attention. An LSTM recurrent unit is employed to learn the attentiveness and discrimination of attention canvases. The proposed DVAN has the ability to attend the object from coarse to fine granularity, and a dynamic internal representation for classification is built up by incrementally combining the information from different locations and scales of the image. Extensive experiments con- ducted on CUB-2011, Stanford Dogs and Stanford Cars datasets have demonstrated that the proposed diversified visual attention networks achieve competitive performance compared to the state- of-the-art approaches, without using any prior knowledge, user interaction or external resource in training or testing.



### Facial Expression Classification Using Rotation Slepian-based Moment Invariants
- **Arxiv ID**: http://arxiv.org/abs/1607.01040v1
- **DOI**: None
- **Categories**: **cs.CV**, 30E05, 33E10, 14L24
- **Links**: [PDF](http://arxiv.org/pdf/1607.01040v1)
- **Published**: 2016-06-28 07:54:03+00:00
- **Updated**: 2016-06-28 07:54:03+00:00
- **Authors**: Cuiming Zou, Kit Ian Kou
- **Comment**: 13 pages, 4 figures
- **Journal**: None
- **Summary**: Rotation moment invariants have been of great interest in image processing and pattern recognition. This paper presents a novel kind of rotation moment invariants based on the Slepian functions, which were originally introduced in the method of separation of variables for Helmholtz equations. They were first proposed for time series by Slepian and his coworkers in the 1960s. Recent studies have shown that these functions have an good performance in local approximation compared to other approximation basis. Motivated by the good approximation performance, we construct the Slepian-based moments and derive the rotation invariant. We not only theoretically prove the invariance, but also discuss the experiments on real data. The proposed rotation invariants are robust to noise and yield decent performance in facial expression classification.



### Scalable image coding based on epitomes
- **Arxiv ID**: http://arxiv.org/abs/1606.08694v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.08694v1)
- **Published**: 2016-06-28 13:37:41+00:00
- **Updated**: 2016-06-28 13:37:41+00:00
- **Authors**: Martin Alain, Christine Guillemot, Dominique Thoreau, Philippe Guillotel
- **Comment**: Preprint submitted to IEEE Trans. on Image Processing
- **Journal**: None
- **Summary**: In this paper, we propose a novel scheme for scalable image coding based on the concept of epitome. An epitome can be seen as a factorized representation of an image. Focusing on spatial scalability, the enhancement layer of the proposed scheme contains only the epitome of the input image. The pixels of the enhancement layer not contained in the epitome are then restored using two approaches inspired from local learning-based super-resolution methods. In the first method, a locally linear embedding model is learned on base layer patches and then applied to the corresponding epitome patches to reconstruct the enhancement layer. The second approach learns linear mappings between pairs of co-located base layer and epitome patches. Experiments have shown that significant improvement of the rate-distortion performances can be achieved compared to an SHVC reference.



### Theta-RBM: Unfactored Gated Restricted Boltzmann Machine for Rotation-Invariant Representations
- **Arxiv ID**: http://arxiv.org/abs/1606.08805v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.08805v2)
- **Published**: 2016-06-28 18:02:32+00:00
- **Updated**: 2016-06-29 09:57:08+00:00
- **Authors**: Mario Valerio Giuffrida, Sotirios A. Tsaftaris
- **Comment**: 9 pages, 2 figures, 3 tables
- **Journal**: None
- **Summary**: Learning invariant representations is a critical task in computer vision. In this paper, we propose the Theta-Restricted Boltzmann Machine ({\theta}-RBM in short), which builds upon the original RBM formulation and injects the notion of rotation-invariance during the learning procedure. In contrast to previous approaches, we do not transform the training set with all possible rotations. Instead, we rotate the gradient filters when they are computed during the Contrastive Divergence algorithm. We formulate our model as an unfactored gated Boltzmann machine, where another input layer is used to modulate the input visible layer to drive the optimisation procedure. Among our contributions is a mathematical proof that demonstrates that {\theta}-RBM is able to learn rotation-invariant features according to a recently proposed invariance measure. Our method reaches an invariance score of ~90% on mnist-rot dataset, which is the highest result compared with the baseline methods and the current state of the art in transformation-invariant feature learning in RBM. Using an SVM classifier, we also showed that our network learns discriminative features as well, obtaining ~10% of testing error.



### A Topological Lowpass Filter for Quasiperiodic Signals
- **Arxiv ID**: http://arxiv.org/abs/1607.06032v1
- **DOI**: 10.1109/LSP.2016.2619678
- **Categories**: **cs.CV**, math.DS, math.NA, math.OC, 37C05, 94A12, 60G35
- **Links**: [PDF](http://arxiv.org/pdf/1607.06032v1)
- **Published**: 2016-06-28 18:10:03+00:00
- **Updated**: 2016-06-28 18:10:03+00:00
- **Authors**: Michael Robinson
- **Comment**: None
- **Journal**: None
- **Summary**: This article presents a two-stage topological algorithm for recovering an estimate of a quasiperiodic function from a set of noisy measurements. The first stage of the algorithm is a topological phase estimator, which detects the quasiperiodic structure of the function without placing additional restrictions on the function. By respecting this phase estimate, the algorithm avoids creating distortion even when it uses a large number of samples for the estimate of the function.



