# Arxiv Papers in cs.CV on 2016-06-27
### Out-of-Sample Extension for Dimensionality Reduction of Noisy Time Series
- **Arxiv ID**: http://arxiv.org/abs/1606.08282v3
- **DOI**: 10.1109/TIP.2017.2735189
- **Categories**: **stat.ML**, cs.CG, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1606.08282v3)
- **Published**: 2016-06-27 14:03:40+00:00
- **Updated**: 2017-07-29 01:37:40+00:00
- **Authors**: Hamid Dadkhahi, Marco F. Duarte, Benjamin Marlin
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes an out-of-sample extension framework for a global manifold learning algorithm (Isomap) that uses temporal information in out-of-sample points in order to make the embedding more robust to noise and artifacts. Given a set of noise-free training data and its embedding, the proposed framework extends the embedding for a noisy time series. This is achieved by adding a spatio-temporal compactness term to the optimization objective of the embedding. To the best of our knowledge, this is the first method for out-of-sample extension of manifold embeddings that leverages timing information available for the extension set. Experimental results demonstrate that our out-of-sample extension algorithm renders a more robust and accurate embedding of sequentially ordered image data in the presence of various noise and artifacts when compared to other timing-aware embeddings. Additionally, we show that an out-of-sample extension framework based on the proposed algorithm outperforms the state of the art in eye-gaze estimation.



### Interpreting extracted rules from ensemble of trees: Application to computer-aided diagnosis of breast MRI
- **Arxiv ID**: http://arxiv.org/abs/1606.08288v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1606.08288v1)
- **Published**: 2016-06-27 14:35:09+00:00
- **Updated**: 2016-06-27 14:35:09+00:00
- **Authors**: Cristina Gallego-Ortiz, Anne L. Martel
- **Comment**: presented at 2016 ICML Workshop on Human Interpretability in Machine
  Learning (WHI 2016), New York, NY
- **Journal**: None
- **Summary**: High predictive performance and ease of use and interpretability are important requirements for the applicability of a computer-aided diagnosis (CAD) to human reading studies. We propose a CAD system specifically designed to be more comprehensible to the radiologist reviewing screening breast MRI studies. Multiparametric imaging features are combined to produce a CAD system for differentiating cancerous and non-cancerous lesions. The complete system uses a rule-extraction algorithm to present lesion classification results in an easy to understand graph visualization.



### Depth Estimation from Single Image using Sparse Representations
- **Arxiv ID**: http://arxiv.org/abs/1606.08315v1
- **DOI**: 10.13140/RG.2.1.5059.0323
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.08315v1)
- **Published**: 2016-06-27 15:23:04+00:00
- **Updated**: 2016-06-27 15:23:04+00:00
- **Authors**: Yigit Oktar
- **Comment**: None
- **Journal**: None
- **Summary**: Monocular depth estimation is an interesting and challenging problem as there is no analytic mapping known between an intensity image and its depth map. Recently there has been a lot of data accumulated through depth-sensing cameras, in parallel to that researchers started to tackle this task using various learning algorithms. In this paper, a deep sparse coding method is proposed for monocular depth estimation along with an approach for deterministic dictionary initialization.



### Revisiting Visual Question Answering Baselines
- **Arxiv ID**: http://arxiv.org/abs/1606.08390v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.08390v2)
- **Published**: 2016-06-27 18:07:58+00:00
- **Updated**: 2016-11-22 21:26:06+00:00
- **Authors**: Allan Jabri, Armand Joulin, Laurens van der Maaten
- **Comment**: European Conference on Computer Vision
- **Journal**: None
- **Summary**: Visual question answering (VQA) is an interesting learning setting for evaluating the abilities and shortcomings of current systems for image understanding. Many of the recently proposed VQA systems include attention or memory mechanisms designed to support "reasoning". For multiple-choice VQA, nearly all of these systems train a multi-class classifier on image and question features to predict an answer. This paper questions the value of these common practices and develops a simple alternative model based on binary classification. Instead of treating answers as competing choices, our model receives the answer as input and predicts whether or not an image-question-answer triplet is correct. We evaluate our model on the Visual7W Telling and the VQA Real Multiple Choice tasks, and find that even simple versions of our model perform competitively. Our best model achieves state-of-the-art performance on the Visual7W Telling task and compares surprisingly well with the most complex systems proposed for the VQA Real Multiple Choice task. We explore variants of the model and study its transferability between both datasets. We also present an error analysis of our model that suggests a key problem of current VQA systems lies in the lack of visual grounding of concepts that occur in the questions and answers. Overall, our results suggest that the performance of current VQA systems is not significantly better than that of systems designed to exploit dataset biases.



