# Arxiv Papers in cs.CV on 2016-06-05
### An Interactive Medical Image Segmentation Framework Using Iterative Refinement
- **Arxiv ID**: http://arxiv.org/abs/1606.01453v1
- **DOI**: 10.1016/j.compbiomed.2017.02.002
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1606.01453v1)
- **Published**: 2016-06-05 02:35:53+00:00
- **Updated**: 2016-06-05 02:35:53+00:00
- **Authors**: Pratik Kalshetti, Manas Bundele, Parag Rahangdale, Dinesh Jangra, Chiranjoy Chattopadhyay, Gaurav Harit, Abhay Elhence
- **Comment**: 19 pages, 19 figures, Submitted for review in Computers in Biology
  and Medicine
- **Journal**: None
- **Summary**: Image segmentation is often performed on medical images for identifying diseases in clinical evaluation. Hence it has become one of the major research areas. Conventional image segmentation techniques are unable to provide satisfactory segmentation results for medical images as they contain irregularities. They need to be pre-processed before segmentation. In order to obtain the most suitable method for medical image segmentation, we propose a two stage algorithm. The first stage automatically generates a binary marker image of the region of interest using mathematical morphology. This marker serves as the mask image for the second stage which uses GrabCut on the input image thus resulting in an efficient segmented result. The obtained result can be further refined by user interaction which can be done using the Graphical User Interface (GUI). Experimental results show that the proposed method is accurate and provides satisfactory segmentation results with minimum user interaction on medical as well as natural images.



### Multimodal Residual Learning for Visual QA
- **Arxiv ID**: http://arxiv.org/abs/1606.01455v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.01455v2)
- **Published**: 2016-06-05 02:38:20+00:00
- **Updated**: 2016-08-31 08:28:38+00:00
- **Authors**: Jin-Hwa Kim, Sang-Woo Lee, Dong-Hyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang
- **Comment**: 13 pages, 7 figures, accepted for NIPS 2016
- **Journal**: None
- **Summary**: Deep neural networks continue to advance the state-of-the-art of image recognition tasks with various methods. However, applications of these methods to multimodality remain limited. We present Multimodal Residual Networks (MRN) for the multimodal residual learning of visual question-answering, which extends the idea of the deep residual learning. Unlike the deep residual learning, MRN effectively learns the joint representation from vision and language information. The main idea is to use element-wise multiplication for the joint residual mappings exploiting the residual learning of the attentional models in recent studies. Various alternative models introduced by multimodality are explored based on our study. We achieve the state-of-the-art results on the Visual QA dataset for both Open-Ended and Multiple-Choice tasks. Moreover, we introduce a novel method to visualize the attention effect of the joint representations for each learning block using back-propagation algorithm, even though the visual features are collapsed without spatial information.



### Nighttime Haze Removal with Illumination Correction
- **Arxiv ID**: http://arxiv.org/abs/1606.01460v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.01460v1)
- **Published**: 2016-06-05 04:15:53+00:00
- **Updated**: 2016-06-05 04:15:53+00:00
- **Authors**: Jing Zhang, Yang Cao, Zengfu Wang
- **Comment**: 14 pages, 18 figures
- **Journal**: None
- **Summary**: Haze removal is important for computational photography and computer vision applications. However, most of the existing methods for dehazing are designed for daytime images, and cannot always work well in the nighttime. Different from the imaging conditions in the daytime, images captured in nighttime haze condition may suffer from non-uniform illumination due to artificial light sources, which exhibit low brightness/contrast and color distortion. In this paper, we present a new nighttime hazy imaging model that takes into account both the non-uniform illumination from artificial light sources and the scattering and attenuation effects of haze. Accordingly, we propose an efficient dehazing algorithm for nighttime hazy images. The proposed algorithm includes three sequential steps. i) It enhances the overall brightness by performing a gamma correction step after estimating the illumination from the original image. ii) Then it achieves a color-balance result by performing a color correction step after estimating the color characteristics of the incident light. iii) Finally, it remove the haze effect by applying the dark channel prior and estimating the point-wise environmental light based on the previous illumination-balance result. Experimental results show that the proposed algorithm can achieve illumination-balance and haze-free results with good color rendition ability.



### Better Image Segmentation by Exploiting Dense Semantic Predictions
- **Arxiv ID**: http://arxiv.org/abs/1606.01481v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.01481v1)
- **Published**: 2016-06-05 08:52:06+00:00
- **Updated**: 2016-06-05 08:52:06+00:00
- **Authors**: Qiyang Zhao, Lewis D Griffin
- **Comment**: None
- **Journal**: None
- **Summary**: It is well accepted that image segmentation can benefit from utilizing multilevel cues. The paper focuses on utilizing the FCNN-based dense semantic predictions in the bottom-up image segmentation, arguing to take semantic cues into account from the very beginning. By this we can avoid merging regions of similar appearance but distinct semantic categories as possible. The semantic inefficiency problem is handled. We also propose a straightforward way to use the contour cues to suppress the noise in multilevel cues, thus to improve the segmentation robustness. The evaluation on the BSDS500 shows that we obtain the competitive region and boundary performance. Furthermore, since all individual regions can be assigned with appropriate semantic labels during the computation, we are capable of extracting the adjusted semantic segmentations. The experiment on Pascal VOC 2012 shows our improvement to the original semantic segmentations which derives directly from the dense predictions.



### A Deep Learning Approach to Block-based Compressed Sensing of Images
- **Arxiv ID**: http://arxiv.org/abs/1606.01519v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.01519v1)
- **Published**: 2016-06-05 14:40:54+00:00
- **Updated**: 2016-06-05 14:40:54+00:00
- **Authors**: Amir Adler, David Boublil, Michael Elad, Michael Zibulevsky
- **Comment**: None
- **Journal**: None
- **Summary**: Compressed sensing (CS) is a signal processing framework for efficiently reconstructing a signal from a small number of measurements, obtained by linear projections of the signal. Block-based CS is a lightweight CS approach that is mostly suitable for processing very high-dimensional images and videos: it operates on local patches, employs a low-complexity reconstruction operator and requires significantly less memory to store the sensing matrix. In this paper we present a deep learning approach for block-based CS, in which a fully-connected network performs both the block-based linear sensing and non-linear reconstruction stages. During the training phase, the sensing matrix and the non-linear reconstruction operator are \emph{jointly} optimized, and the proposed approach outperforms state-of-the-art both in terms of reconstruction quality and computation time. For example, at a 25% sensing rate the average PSNR advantage is 0.77dB and computation time is over 200-times faster.



### What is the Best Feature Learning Procedure in Hierarchical Recognition Architectures?
- **Arxiv ID**: http://arxiv.org/abs/1606.01535v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.01535v1)
- **Published**: 2016-06-05 17:31:39+00:00
- **Updated**: 2016-06-05 17:31:39+00:00
- **Authors**: Kevin Jarrett, Koray Kvukcuoglu, Karol Gregor, Yann LeCun
- **Comment**: 17 pages, 3 figures
- **Journal**: None
- **Summary**: (This paper was written in November 2011 and never published. It is posted on arXiv.org in its original form in June 2016). Many recent object recognition systems have proposed using a two phase training procedure to learn sparse convolutional feature hierarchies: unsupervised pre-training followed by supervised fine-tuning. Recent results suggest that these methods provide little improvement over purely supervised systems when the appropriate nonlinearities are included. This paper presents an empirical exploration of the space of learning procedures for sparse convolutional networks to assess which method produces the best performance. In our study, we introduce an augmentation of the Predictive Sparse Decomposition method that includes a discriminative term (DPSD). We also introduce a new single phase supervised learning procedure that places an L1 penalty on the output state of each layer of the network. This forces the network to produce sparse codes without the expensive pre-training phase. Using DPSD with a new, complex predictor that incorporates lateral inhibition, combined with multi-scale feature pooling, and supervised refinement, the system achieves a 70.6\% recognition rate on Caltech-101. With the addition of convolutional training, a 77\% recognition was obtained on the CIfAR-10 dataset.



### Pairwise Quantization
- **Arxiv ID**: http://arxiv.org/abs/1606.01550v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1606.01550v1)
- **Published**: 2016-06-05 19:57:06+00:00
- **Updated**: 2016-06-05 19:57:06+00:00
- **Authors**: Artem Babenko, Relja ArandjeloviÄ‡, Victor Lempitsky
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the task of lossy compression of high-dimensional vectors through quantization. We propose the approach that learns quantization parameters by minimizing the distortion of scalar products and squared distances between pairs of points. This is in contrast to previous works that obtain these parameters through the minimization of the reconstruction error of individual points. The proposed approach proceeds by finding a linear transformation of the data that effectively reduces the minimization of the pairwise distortions to the minimization of individual reconstruction errors. After such transformation, any of the previously-proposed quantization approaches can be used. Despite the simplicity of this transformation, the experiments demonstrate that it achieves considerable reduction of the pairwise distortions compared to applying quantization directly to the untransformed data.



### Shallow Networks for High-Accuracy Road Object-Detection
- **Arxiv ID**: http://arxiv.org/abs/1606.01561v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.01561v1)
- **Published**: 2016-06-05 21:04:46+00:00
- **Updated**: 2016-06-05 21:04:46+00:00
- **Authors**: Khalid Ashraf, Bichen Wu, Forrest N. Iandola, Mattthew W. Moskewicz, Kurt Keutzer
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: The ability to automatically detect other vehicles on the road is vital to the safety of partially-autonomous and fully-autonomous vehicles. Most of the high-accuracy techniques for this task are based on R-CNN or one of its faster variants. In the research community, much emphasis has been applied to using 3D vision or complex R-CNN variants to achieve higher accuracy. However, are there more straightforward modifications that could deliver higher accuracy? Yes. We show that increasing input image resolution (i.e. upsampling) offers up to 12 percentage-points higher accuracy compared to an off-the-shelf baseline. We also find situations where earlier/shallower layers of CNN provide higher accuracy than later/deeper layers. We further show that shallow models and upsampled images yield competitive accuracy. Our findings contrast with the current trend towards deeper and larger models to achieve high accuracy in domain specific detection tasks.



### Active Regression with Adaptive Huber Loss
- **Arxiv ID**: http://arxiv.org/abs/1606.01568v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1606.01568v2)
- **Published**: 2016-06-05 21:59:34+00:00
- **Updated**: 2016-06-26 07:24:43+00:00
- **Authors**: Jacopo Cavazza, Vittorio Murino
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the scalar regression problem through a novel solution to exactly optimize the Huber loss in a general semi-supervised setting, which combines multi-view learning and manifold regularization. We propose a principled algorithm to 1) avoid computationally expensive iterative schemes while 2) adapting the Huber loss threshold in a data-driven fashion and 3) actively balancing the use of labelled data to remove noisy or inconsistent annotations at the training stage. In a wide experimental evaluation, dealing with diverse applications, we assess the superiority of our paradigm which is able to combine robustness towards noise with both strong performance and low computational cost.



