# Arxiv Papers in cs.CV on 2016-06-21
### Efficient 2D and 3D Facade Segmentation using Auto-Context
- **Arxiv ID**: http://arxiv.org/abs/1606.06437v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.06437v1)
- **Published**: 2016-06-21 06:50:35+00:00
- **Updated**: 2016-06-21 06:50:35+00:00
- **Authors**: Raghudeep Gadde, Varun Jampani, Renaud Marlet, Peter V. Gehler
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: This paper introduces a fast and efficient segmentation technique for 2D images and 3D point clouds of building facades. Facades of buildings are highly structured and consequently most methods that have been proposed for this problem aim to make use of this strong prior information. Contrary to most prior work, we are describing a system that is almost domain independent and consists of standard segmentation methods. We train a sequence of boosted decision trees using auto-context features. This is learned using stacked generalization. We find that this technique performs better, or comparable with all previous published methods and present empirical results on all available 2D and 3D facade benchmark datasets. The proposed method is simple to implement, easy to extend, and very efficient at test-time inference.



### Social-sparsity brain decoders: faster spatial sparsity
- **Arxiv ID**: http://arxiv.org/abs/1606.06439v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1606.06439v1)
- **Published**: 2016-06-21 06:51:57+00:00
- **Updated**: 2016-06-21 06:51:57+00:00
- **Authors**: GaÃ«l Varoquaux, Matthieu Kowalski, Bertrand Thirion
- **Comment**: in Pattern Recognition in NeuroImaging, Jun 2016, Trento, Italy. 2016
- **Journal**: None
- **Summary**: Spatially-sparse predictors are good models for brain decoding: they give accurate predictions and their weight maps are interpretable as they focus on a small number of regions. However, the state of the art, based on total variation or graph-net, is computationally costly. Here we introduce sparsity in the local neighborhood of each voxel with social-sparsity, a structured shrinkage operator. We find that, on brain imaging classification problems, social-sparsity performs almost as well as total-variation models and better than graph-net, for a fraction of the computational cost. It also very clearly outlines predictive regions. We give details of the model and the algorithm.



### DeepWriter: A Multi-Stream Deep CNN for Text-independent Writer Identification
- **Arxiv ID**: http://arxiv.org/abs/1606.06472v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.06472v2)
- **Published**: 2016-06-21 08:25:25+00:00
- **Updated**: 2016-08-03 03:26:58+00:00
- **Authors**: Linjie Xing, Yu Qiao
- **Comment**: This article will be presented at ICFHR 2016
- **Journal**: None
- **Summary**: Text-independent writer identification is challenging due to the huge variation of written contents and the ambiguous written styles of different writers. This paper proposes DeepWriter, a deep multi-stream CNN to learn deep powerful representation for recognizing writers. DeepWriter takes local handwritten patches as input and is trained with softmax classification loss. The main contributions are: 1) we design and optimize multi-stream structure for writer identification task; 2) we introduce data augmentation learning to enhance the performance of DeepWriter; 3) we introduce a patch scanning strategy to handle text image with different lengths. In addition, we find that different languages such as English and Chinese may share common features for writer identification, and joint training can yield better performance. Experimental results on IAM and HWDB datasets show that our models achieve high identification accuracy: 99.01% on 301 writers and 97.03% on 657 writers with one English sentence input, 93.85% on 300 writers with one Chinese character input, which outperform previous methods with a large margin. Moreover, our models obtain accuracy of 98.01% on 301 writers with only 4 English alphabets as input.



### Drawing and Recognizing Chinese Characters with Recurrent Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1606.06539v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.06539v1)
- **Published**: 2016-06-21 12:35:31+00:00
- **Updated**: 2016-06-21 12:35:31+00:00
- **Authors**: Xu-Yao Zhang, Fei Yin, Yan-Ming Zhang, Cheng-Lin Liu, Yoshua Bengio
- **Comment**: None
- **Journal**: None
- **Summary**: Recent deep learning based approaches have achieved great success on handwriting recognition. Chinese characters are among the most widely adopted writing systems in the world. Previous research has mainly focused on recognizing handwritten Chinese characters. However, recognition is only one aspect for understanding a language, another challenging and interesting task is to teach a machine to automatically write (pictographic) Chinese characters. In this paper, we propose a framework by using the recurrent neural network (RNN) as both a discriminative model for recognizing Chinese characters and a generative model for drawing (generating) Chinese characters. To recognize Chinese characters, previous methods usually adopt the convolutional neural network (CNN) models which require transforming the online handwriting trajectory into image-like representations. Instead, our RNN based approach is an end-to-end system which directly deals with the sequential structure and does not require any domain-specific knowledge. With the RNN system (combining an LSTM and GRU), state-of-the-art performance can be achieved on the ICDAR-2013 competition database. Furthermore, under the RNN framework, a conditional generative model with character embedding is proposed for automatically drawing recognizable Chinese characters. The generated characters (in vector format) are human-readable and also can be recognized by the discriminative RNN model with high accuracy. Experimental results verify the effectiveness of using RNNs as both generative and discriminative models for the tasks of drawing and recognizing Chinese characters.



### Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1606.06582v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1606.06582v1)
- **Published**: 2016-06-21 14:12:52+00:00
- **Updated**: 2016-06-21 14:12:52+00:00
- **Authors**: Yuting Zhang, Kibok Lee, Honglak Lee
- **Comment**: International Conference on Machine Learning (ICML), 2016
- **Journal**: PMLR 48:612-621, 2016
- **Summary**: Unsupervised learning and supervised learning are key research topics in deep learning. However, as high-capacity supervised neural networks trained with a large amount of labels have achieved remarkable success in many computer vision tasks, the availability of large-scale labeled images reduced the significance of unsupervised learning. Inspired by the recent trend toward revisiting the importance of unsupervised learning, we investigate joint supervised and unsupervised learning in a large-scale setting by augmenting existing neural networks with decoding pathways for reconstruction. First, we demonstrate that the intermediate activations of pretrained large-scale classification networks preserve almost all the information of input images except a portion of local spatial details. Then, by end-to-end training of the entire augmented architecture with the reconstructive objective, we show improvement of the network performance for supervised tasks. We evaluate several variants of autoencoders, including the recently proposed "what-where" autoencoder that uses the encoder pooling switches, to study the importance of the architecture design. Taking the 16-layer VGGNet trained under the ImageNet ILSVRC 2012 protocol as a strong baseline for image classification, our methods improve the validation-set accuracy by a noticeable margin.



### Question Relevance in VQA: Identifying Non-Visual And False-Premise Questions
- **Arxiv ID**: http://arxiv.org/abs/1606.06622v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1606.06622v3)
- **Published**: 2016-06-21 15:38:27+00:00
- **Updated**: 2016-09-26 15:24:28+00:00
- **Authors**: Arijit Ray, Gordon Christie, Mohit Bansal, Dhruv Batra, Devi Parikh
- **Comment**: Conference on Empirical Methods in Natural Language Processing
  (EMNLP) 2016
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) is the task of answering natural-language questions about images. We introduce the novel problem of determining the relevance of questions to images in VQA. Current VQA models do not reason about whether a question is even related to the given image (e.g. What is the capital of Argentina?) or if it requires information from external resources to answer correctly. This can break the continuity of a dialogue in human-machine interaction. Our approaches for determining relevance are composed of two stages. Given an image and a question, (1) we first determine whether the question is visual or not, (2) if visual, we determine whether the question is relevant to the given image or not. Our approaches, based on LSTM-RNNs, VQA model uncertainty, and caption-question similarity, are able to outperform strong baselines on both relevance tasks. We also present human studies showing that VQA models augmented with such question relevance reasoning are perceived as more intelligent, reasonable, and human-like.



### 3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation
- **Arxiv ID**: http://arxiv.org/abs/1606.06650v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.06650v1)
- **Published**: 2016-06-21 16:42:20+00:00
- **Updated**: 2016-06-21 16:42:20+00:00
- **Authors**: ÃzgÃ¼n ÃiÃ§ek, Ahmed Abdulkadir, Soeren S. Lienkamp, Thomas Brox, Olaf Ronneberger
- **Comment**: Conditionally accepted for MICCAI 2016
- **Journal**: None
- **Summary**: This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup, the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup, we assume that a representative, sparsely annotated training set exists. Trained on this data set, the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch, i.e., no pre-trained network is required. We test the performance of the proposed method on a complex, highly variable 3D structure, the Xenopus kidney, and achieve good results for both use cases.



### Crowdsourcing scoring of immunohistochemistry images: Evaluating Performance of the Crowd and an Automated Computational Method
- **Arxiv ID**: http://arxiv.org/abs/1606.06681v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.06681v2)
- **Published**: 2016-06-21 17:50:38+00:00
- **Updated**: 2016-06-23 20:15:30+00:00
- **Authors**: Humayun Irshad, Eun-Yeong Oh, Daniel Schmolze, Liza M Quintana, Laura Collins, Rulla M. Tamimi, Andrew H. Beck
- **Comment**: None
- **Journal**: None
- **Summary**: The assessment of protein expression in immunohistochemistry (IHC) images provides important diagnostic, prognostic and predictive information for guiding cancer diagnosis and therapy. Manual scoring of IHC images represents a logistical challenge, as the process is labor intensive and time consuming. Since the last decade, computational methods have been developed to enable the application of quantitative methods for the analysis and interpretation of protein expression in IHC images. These methods have not yet replaced manual scoring for the assessment of IHC in the majority of diagnostic laboratories and in many large-scale research studies. An alternative approach is crowdsourcing the quantification of IHC images to an undefined crowd. The aim of this study is to quantify IHC images for labeling of ER status with two different crowdsourcing approaches, image labeling and nuclei labeling, and compare their performance with automated methods. Crowdsourcing-derived scores obtained greater concordance with the pathologist interpretations for both image labeling and nuclei labeling tasks (83% and 87%), as compared to the pathologist concordance achieved by the automated method (81%) on 5,483 TMA images from 1,909 breast cancer patients. This analysis shows that crowdsourcing the scoring of protein expression in IHC images is a promising new approach for large scale cancer molecular pathology studies.



### Tagger: Deep Unsupervised Perceptual Grouping
- **Arxiv ID**: http://arxiv.org/abs/1606.06724v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, 97R40
- **Links**: [PDF](http://arxiv.org/pdf/1606.06724v2)
- **Published**: 2016-06-21 19:55:32+00:00
- **Updated**: 2016-11-28 18:59:28+00:00
- **Authors**: Klaus Greff, Antti Rasmus, Mathias Berglund, Tele Hotloo Hao, JÃ¼rgen Schmidhuber, Harri Valpola
- **Comment**: 14 pages + 5 pages supplementary, accepted at NIPS 2016
- **Journal**: None
- **Summary**: We present a framework for efficient perceptual inference that explicitly reasons about the segmentation of its inputs and features. Rather than being trained for any specific segmentation, our framework learns the grouping process in an unsupervised manner or alongside any supervised task. By enriching the representations of a neural network, we enable it to group the representations of different objects in an iterative manner. By allowing the system to amortize the iterative inference of the groupings, we achieve very fast convergence. In contrast to many other recently proposed methods for addressing multi-object scenes, our system does not assume the inputs to be images and can therefore directly handle other modalities. For multi-digit classification of very cluttered images that require texture segmentation, our method offers improved classification performance over convolutional networks despite being fully connected. Furthermore, we observe that our system greatly improves on the semi-supervised result of a baseline Ladder network on our dataset, indicating that segmentation can also improve sample efficiency.



