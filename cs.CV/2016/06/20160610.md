# Arxiv Papers in cs.CV on 2016-06-10
### Survey on RGB, 3D, Thermal, and Multimodal Approaches for Facial Expression Recognition: History, Trends, and Affect-related Applications
- **Arxiv ID**: http://arxiv.org/abs/1606.03237v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.03237v1)
- **Published**: 2016-06-10 09:12:05+00:00
- **Updated**: 2016-06-10 09:12:05+00:00
- **Authors**: Ciprian Corneanu, Marc Oliu, Jeffrey F. Cohn, Sergio Escalera
- **Comment**: None
- **Journal**: None
- **Summary**: Facial expressions are an important way through which humans interact socially. Building a system capable of automatically recognizing facial expressions from images and video has been an intense field of study in recent years. Interpreting such expressions remains challenging and much research is needed about the way they relate to human affect. This paper presents a general overview of automatic RGB, 3D, thermal and multimodal facial expression analysis. We define a new taxonomy for the field, encompassing all steps from face detection to facial expression recognition, and describe and classify the state of the art methods accordingly. We also present the important datasets and the bench-marking of most influential methods. We conclude with a general discussion about trends, important questions and future lines of research.



### IDNet: Smartphone-based Gait Recognition with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1606.03238v3
- **DOI**: 10.1016/j.patcog.2017.09.005
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1606.03238v3)
- **Published**: 2016-06-10 09:14:28+00:00
- **Updated**: 2016-10-19 12:11:57+00:00
- **Authors**: Matteo Gadaleta, Michele Rossi
- **Comment**: None
- **Journal**: None
- **Summary**: Here, we present IDNet, a user authentication framework from smartphone-acquired motion signals. Its goal is to recognize a target user from their way of walking, using the accelerometer and gyroscope (inertial) signals provided by a commercial smartphone worn in the front pocket of the user's trousers. IDNet features several innovations including: i) a robust and smartphone-orientation-independent walking cycle extraction block, ii) a novel feature extractor based on convolutional neural networks, iii) a one-class support vector machine to classify walking cycles, and the coherent integration of these into iv) a multi-stage authentication technique. IDNet is the first system that exploits a deep learning approach as universal feature extractors for gait recognition, and that combines classification results from subsequent walking cycles into a multi-stage decision making framework. Experimental results show the superiority of our approach against state-of-the-art techniques, leading to misclassification rates (either false negatives or positives) smaller than 0.15% with fewer than five walking cycles. Design choices are discussed and motivated throughout, assessing their impact on the user authentication performance.



### FOMTrace: Interactive Video Segmentation By Image Graphs and Fuzzy Object Models
- **Arxiv ID**: http://arxiv.org/abs/1606.03369v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.03369v1)
- **Published**: 2016-06-10 15:30:30+00:00
- **Updated**: 2016-06-10 15:30:30+00:00
- **Authors**: Thiago Vallin Spina, Alexandre Xavier Falc√£o
- **Comment**: None
- **Journal**: None
- **Summary**: Common users have changed from mere consumers to active producers of multimedia data content. Video editing plays an important role in this scenario, calling for simple segmentation tools that can handle fast-moving and deformable video objects with possible occlusions, color similarities with the background, among other challenges. We present an interactive video segmentation method, named FOMTrace, which addresses the problem in an effective and efficient way. From a user-provided object mask in a first frame, the method performs semi-automatic video segmentation on a spatiotemporal superpixel-graph, and then estimates a Fuzzy Object Model (FOM), which refines segmentation of the second frame by constraining delineation on a pixel-graph within a region where the object's boundary is expected to be. The user can correct/accept the refined object mask in the second frame, which is then similarly used to improve the spatiotemporal video segmentation of the remaining frames. Both steps are repeated alternately, within interactive response times, until the segmentation refinement of the final frame is accepted by the user. Extensive experiments demonstrate FOMTrace's ability for tracing objects in comparison with state-of-the-art approaches for interactive video segmentation, supervised, and unsupervised object tracking.



### Face Detection with the Faster R-CNN
- **Arxiv ID**: http://arxiv.org/abs/1606.03473v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.03473v1)
- **Published**: 2016-06-10 20:34:39+00:00
- **Updated**: 2016-06-10 20:34:39+00:00
- **Authors**: Huaizu Jiang, Erik Learned-Miller
- **Comment**: technical report
- **Journal**: None
- **Summary**: The Faster R-CNN has recently demonstrated impressive results on various object detection benchmarks. By training a Faster R-CNN model on the large scale WIDER face dataset, we report state-of-the-art results on two widely used face detection benchmarks, FDDB and the recently released IJB-A.



### The Mythos of Model Interpretability
- **Arxiv ID**: http://arxiv.org/abs/1606.03490v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1606.03490v3)
- **Published**: 2016-06-10 21:28:47+00:00
- **Updated**: 2017-03-06 08:51:10+00:00
- **Authors**: Zachary C. Lipton
- **Comment**: presented at 2016 ICML Workshop on Human Interpretability in Machine
  Learning (WHI 2016), New York, NY
- **Journal**: None
- **Summary**: Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.



### Improved Techniques for Training GANs
- **Arxiv ID**: http://arxiv.org/abs/1606.03498v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1606.03498v1)
- **Published**: 2016-06-10 22:53:35+00:00
- **Updated**: 2016-06-10 22:53:35+00:00
- **Authors**: Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen
- **Comment**: None
- **Journal**: None
- **Summary**: We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.



