# Arxiv Papers in cs.CV on 2016-06-22
### Where to Focus: Query Adaptive Matching for Instance Retrieval Using Convolutional Feature Maps
- **Arxiv ID**: http://arxiv.org/abs/1606.06811v1
- **DOI**: None
- **Categories**: **cs.CV**, H.3.3; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1606.06811v1)
- **Published**: 2016-06-22 03:33:25+00:00
- **Updated**: 2016-06-22 03:33:25+00:00
- **Authors**: Jiewei Cao, Lingqiao Liu, Peng Wang, Zi Huang, Chunhua Shen, Heng Tao Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Instance retrieval requires one to search for images that contain a particular object within a large corpus. Recent studies show that using image features generated by pooling convolutional layer feature maps (CFMs) of a pretrained convolutional neural network (CNN) leads to promising performance for this task. However, due to the global pooling strategy adopted in those works, the generated image feature is less robust to image clutter and tends to be contaminated by the irrelevant image patterns. In this article, we alleviate this drawback by proposing a novel reranking algorithm using CFMs to refine the retrieval result obtained by existing methods. Our key idea, called query adaptive matching (QAM), is to first represent the CFMs of each image by a set of base regions which can be freely combined into larger regions-of-interest. Then the similarity between the query and a candidate image is measured by the best similarity score that can be attained by comparing the query feature and the feature pooled from a combined region. We show that the above procedure can be cast as an optimization problem and it can be solved efficiently with an off-the-shelf solver. Besides this general framework, we also propose two practical ways to create the base regions. One is based on the property of the CFM and the other one is based on a multi-scale spatial pyramid scheme. Through extensive experiments, we show that our reranking approaches bring substantial performance improvement and by applying them we can outperform the state of the art on several instance retrieval benchmarks.



### Model-based Deep Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1606.06854v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.06854v1)
- **Published**: 2016-06-22 08:47:06+00:00
- **Updated**: 2016-06-22 08:47:06+00:00
- **Authors**: Xingyi Zhou, Qingfu Wan, Wei Zhang, Xiangyang Xue, Yichen Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Previous learning based hand pose estimation methods does not fully exploit the prior information in hand model geometry. Instead, they usually rely a separate model fitting step to generate valid hand poses. Such a post processing is inconvenient and sub-optimal. In this work, we propose a model based deep learning approach that adopts a forward kinematics based layer to ensure the geometric validity of estimated poses. For the first time, we show that embedding such a non-linear generative process in deep learning is feasible for hand pose estimation. Our approach is verified on challenging public datasets and achieves state-of-the-art performance.



### Joint M-Best-Diverse Labelings as a Parametric Submodular Minimization
- **Arxiv ID**: http://arxiv.org/abs/1606.07015v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.07015v2)
- **Published**: 2016-06-22 17:26:17+00:00
- **Updated**: 2016-06-23 08:10:46+00:00
- **Authors**: Alexander Kirillov, Alexander Shekhovtsov, Carsten Rother, Bogdan Savchynskyy
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of jointly inferring the M-best diverse labelings for a binary (high-order) submodular energy of a graphical model. Recently, it was shown that this problem can be solved to a global optimum, for many practically interesting diversity measures. It was noted that the labelings are, so-called, nested. This nestedness property also holds for labelings of a class of parametric submodular minimization problems, where different values of the global parameter $\gamma$ give rise to different solutions. The popular example of the parametric submodular minimization is the monotonic parametric max-flow problem, which is also widely used for computing multiple labelings. As the main contribution of this work we establish a close relationship between diversity with submodular energies and the parametric submodular minimization. In particular, the joint M-best diverse labelings can be obtained by running a non-parametric submodular minimization (in the special case - max-flow) solver for M different values of $\gamma$ in parallel, for certain diversity measures. Importantly, the values for $\gamma$ can be computed in a closed form in advance, prior to any optimization. These theoretical results suggest two simple yet efficient algorithms for the joint M-best diverse problem, which outperform competitors in terms of runtime and quality of results. In particular, as we show in the paper, the new methods compute the exact M-best diverse labelings faster than a popular method of Batra et al., which in some sense only obtains approximate solutions.



