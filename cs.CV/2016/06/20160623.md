# Arxiv Papers in cs.CV on 2016-06-23
### 3D Display Calibration by Visual Pattern Analysis
- **Arxiv ID**: http://arxiv.org/abs/1606.07166v1
- **DOI**: 10.1109/TIP.2017.2665043
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.07166v1)
- **Published**: 2016-06-23 02:20:20+00:00
- **Updated**: 2016-06-23 02:20:20+00:00
- **Authors**: Hyoseok Hwang, Hyun Sung Chang, Dongkyung Nam, In So Kweon
- **Comment**: 13 pages, 10 figures.submitted to IEEE Transactions on Image
  Processing
- **Journal**: None
- **Summary**: Nearly all 3D displays need calibration for correct rendering. More often than not, the optical elements in a 3D display are misaligned from the designed parameter setting. As a result, 3D magic does not perform well as intended. The observed images tend to get distorted. In this paper, we propose a novel display calibration method to fix the situation. In our method, a pattern image is displayed on the panel and a camera takes its pictures twice at different positions. Then, based on a quantitative model, we extract all display parameters (i.e., pitch, slanted angle, gap or thickness, offset) from the observed patterns in the captured images. For high accuracy and robustness, our method analyzes the patterns mostly in frequency domain. We conduct two types of experiments for validation; one with optical simulation for quantitative results and the other with real-life displays for qualitative assessment. Experimental results demonstrate that our method is quite accurate, about a half order of magnitude higher than prior work; is efficient, spending less than 2 s for computation; and is robust to noise, working well in the SNR regime as low as 6 dB.



### Deep Learning Markov Random Field for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1606.07230v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1606.07230v2)
- **Published**: 2016-06-23 08:52:39+00:00
- **Updated**: 2017-08-08 09:24:18+00:00
- **Authors**: Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen Change Loy, Xiaoou Tang
- **Comment**: To appear in IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI), 2017. Extended version of our previous ICCV 2015 paper
  (arXiv:1509.02634)
- **Journal**: None
- **Summary**: Semantic segmentation tasks can be well modeled by Markov Random Field (MRF). This paper addresses semantic segmentation by incorporating high-order relations and mixture of label contexts into MRF. Unlike previous works that optimized MRFs using iterative algorithm, we solve MRF by proposing a Convolutional Neural Network (CNN), namely Deep Parsing Network (DPN), which enables deterministic end-to-end computation in a single forward pass. Specifically, DPN extends a contemporary CNN to model unary terms and additional layers are devised to approximate the mean field (MF) algorithm for pairwise terms. It has several appealing properties. First, different from the recent works that required many iterations of MF during back-propagation, DPN is able to achieve high performance by approximating one iteration of MF. Second, DPN represents various types of pairwise terms, making many existing models as its special cases. Furthermore, pairwise terms in DPN provide a unified framework to encode rich contextual information in high-dimensional data, such as images and videos. Third, DPN makes MF easier to be parallelized and speeded up, thus enabling efficient inference. DPN is thoroughly evaluated on standard semantic image/video segmentation benchmarks, where a single DPN model yields state-of-the-art segmentation accuracies on PASCAL VOC 2012, Cityscapes dataset and CamVid dataset.



### Non Local Spatial and Angular Matching : Enabling higher spatial resolution diffusion MRI datasets through adaptive denoising
- **Arxiv ID**: http://arxiv.org/abs/1606.07239v1
- **DOI**: 10.1016/j.media.2016.02.010
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.07239v1)
- **Published**: 2016-06-23 09:28:29+00:00
- **Updated**: 2016-06-23 09:28:29+00:00
- **Authors**: Samuel St-Jean, Pierrick Coupé, Maxime Descoteaux
- **Comment**: Code available : https://github.com/samuelstjean/nlsam Datasets
  available : https://github.com/samuelstjean/nlsam_data, Medical Image
  Analysis, 2016
- **Journal**: Medical Image Analysis , Volume 32 , 115 - 130, 2016
- **Summary**: Diffusion magnetic resonance imaging datasets suffer from low Signal-to-Noise Ratio, especially at high b-values. Acquiring data at high b-values contains relevant information and is now of great interest for microstructural and connectomics studies. High noise levels bias the measurements due to the non-Gaussian nature of the noise, which in turn can lead to a false and biased estimation of the diffusion parameters. Additionally, the usage of in-plane acceleration techniques during the acquisition leads to a spatially varying noise distribution, which depends on the parallel acceleration method implemented on the scanner. This paper proposes a novel diffusion MRI denoising technique that can be used on all existing data, without adding to the scanning time. We first apply a statistical framework to convert the noise to Gaussian distributed noise, effectively removing the bias. We then introduce a spatially and angular adaptive denoising technique, the Non Local Spatial and Angular Matching (NLSAM) algorithm. Each volume is first decomposed in small 4D overlapping patches to capture the structure of the diffusion data and a dictionary of atoms is learned on those patches. A local sparse decomposition is then found by bounding the reconstruction error with the local noise variance. We compare against three other state-of-the-art denoising methods and show quantitative local and connectivity results on a synthetic phantom and on an in-vivo high resolution dataset. Overall, our method restores perceptual information, removes the noise bias in common diffusion metrics, restores the extracted peaks coherence and improves reproducibility of tractography. Our work paves the way for higher spatial resolution acquisition of diffusion MRI datasets, which could in turn reveal new anatomical details that are not discernible at the spatial resolution currently used by the diffusion MRI community.



### Human Computer Interaction Using Marker Based Hand Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/1606.07247v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1606.07247v1)
- **Published**: 2016-06-23 09:49:15+00:00
- **Updated**: 2016-06-23 09:49:15+00:00
- **Authors**: Sayem Mohammad Siam, Jahidul Adnan Sakel, Md. Hasanul Kabir
- **Comment**: 8 Pages, didn't submit to any conference yet
- **Journal**: None
- **Summary**: Human Computer Interaction (HCI) has been redefined in this era. People want to interact with their devices in such a way that has physical significance in the real world, in other words, they want ergonomic input devices. In this paper, we propose a new method of interaction with computing devices having a consumer grade camera, that uses two colored markers (red and green) worn on tips of the fingers to generate desired hand gestures, and for marker detection and tracking we used template matching with kalman filter. We have implemented all the usual system commands, i.e., cursor movement, right click, left click, double click, going forward and backward, zoom in and out through different hand gestures. Our system can easily recognize these gestures and give corresponding system commands. Our system is suitable for both desktop devices and devices where touch screen is not feasible like large screens or projected screens.



### Robust 3D Hand Pose Estimation in Single Depth Images: from Single-View CNN to Multi-View CNNs
- **Arxiv ID**: http://arxiv.org/abs/1606.07253v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.07253v3)
- **Published**: 2016-06-23 10:00:03+00:00
- **Updated**: 2016-12-27 14:22:54+00:00
- **Authors**: Liuhao Ge, Hui Liang, Junsong Yuan, Daniel Thalmann
- **Comment**: 9 pages, 9 figures, published at Computer Vision and Pattern
  Recognition (CVPR) 2016
- **Journal**: None
- **Summary**: Articulated hand pose estimation plays an important role in human-computer interaction. Despite the recent progress, the accuracy of existing methods is still not satisfactory, partially due to the difficulty of embedded high-dimensional and non-linear regression problem. Different from the existing discriminative methods that regress for the hand pose with a single depth image, we propose to first project the query depth image onto three orthogonal planes and utilize these multi-view projections to regress for 2D heat-maps which estimate the joint positions on each plane. These multi-view heat-maps are then fused to produce final 3D hand pose estimation with learned pose priors. Experiments show that the proposed method largely outperforms state-of-the-art on a challenging dataset. Moreover, a cross-dataset experiment also demonstrates the good generalization ability of the proposed method.



### Saliency Driven Object recognition in egocentric videos with deep CNN
- **Arxiv ID**: http://arxiv.org/abs/1606.07256v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.07256v1)
- **Published**: 2016-06-23 10:10:31+00:00
- **Updated**: 2016-06-23 10:10:31+00:00
- **Authors**: Philippe Pérez de San Roman, Jenny Benois-Pineau, Jean-Philippe Domenger, Florent Paclet, Daniel Cataert, Aymar de Rugy
- **Comment**: 20 pages, 8 figures, 3 tables, Submitted to the Journal of Computer
  Vision and Image Understanding
- **Journal**: None
- **Summary**: The problem of object recognition in natural scenes has been recently successfully addressed with Deep Convolutional Neuronal Networks giving a significant break-through in recognition scores. The computational efficiency of Deep CNNs as a function of their depth, allows for their use in real-time applications. One of the key issues here is to reduce the number of windows selected from images to be submitted to a Deep CNN. This is usually solved by preliminary segmentation and selection of specific windows, having outstanding "objectiveness" or other value of indicators of possible location of objects. In this paper we propose a Deep CNN approach and the general framework for recognition of objects in a real-time scenario and in an egocentric perspective. Here the window of interest is built on the basis of visual attention map computed over gaze fixations measured by a glass-worn eye-tracker. The application of this set-up is an interactive user-friendly environment for upper-limb amputees. Vision has to help the subject to control his worn neuro-prosthesis in case of a small amount of remaining muscles when the EMG control becomes unefficient. The recognition results on a specifically recorded corpus of 151 videos with simple geometrical objects show the mAP of 64,6\% and the computational time at the generalization lower than a time of a visual fixation on the object-of-interest.



### Identifying individual facial expressions by deconstructing a neural network
- **Arxiv ID**: http://arxiv.org/abs/1606.07285v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1606.07285v2)
- **Published**: 2016-06-23 12:24:45+00:00
- **Updated**: 2016-06-26 00:41:35+00:00
- **Authors**: Farhad Arbabzadah, Grégoire Montavon, Klaus-Robert Müller, Wojciech Samek
- **Comment**: 12 pages, 7 figures, Paper accepted for GCPR 2016
- **Journal**: None
- **Summary**: This paper focuses on the problem of explaining predictions of psychological attributes such as attractiveness, happiness, confidence and intelligence from face photographs using deep neural networks. Since psychological attribute datasets typically suffer from small sample sizes, we apply transfer learning with two base models to avoid overfitting. These models were trained on an age and gender prediction task, respectively. Using a novel explanation method we extract heatmaps that highlight the parts of the image most responsible for the prediction. We further observe that the explanation method provides important insights into the nature of features of the base model, which allow one to assess the aptitude of the base model for a given transfer learning task. Finally, we observe that the multiclass model is more feature rich than its binary counterpart. The experimental evaluation is performed on the 2222 images from the 10k US faces dataset containing psychological attribute labels as well as on a subset of KDEF images.



### Picture It In Your Mind: Generating High Level Visual Representations From Textual Descriptions
- **Arxiv ID**: http://arxiv.org/abs/1606.07287v1
- **DOI**: 10.1007/s10791-017-9318-6
- **Categories**: **cs.IR**, cs.CL, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1606.07287v1)
- **Published**: 2016-06-23 12:25:09+00:00
- **Updated**: 2016-06-23 12:25:09+00:00
- **Authors**: Fabio Carrara, Andrea Esuli, Tiziano Fagni, Fabrizio Falchi, Alejandro Moreo Fernández
- **Comment**: Neu-IR '16 SIGIR Workshop on Neural Information Retrieval, July 21,
  2016, Pisa, Italy
- **Journal**: None
- **Summary**: In this paper we tackle the problem of image search when the query is a short textual description of the image the user is looking for. We choose to implement the actual search process as a similarity search in a visual feature space, by learning to translate a textual query into a visual representation. Searching in the visual feature space has the advantage that any update to the translation model does not require to reprocess the, typically huge, image collection on which the search is performed. We propose Text2Vis, a neural network that generates a visual representation, in the visual feature space of the fc6-fc7 layers of ImageNet, from a short descriptive text. Text2Vis optimizes two loss functions, using a stochastic loss-selection method. A visual-focused loss is aimed at learning the actual text-to-visual feature mapping, while a text-focused loss is aimed at modeling the higher-level semantic concepts expressed in language and countering the overfit on non-relevant visual components of the visual loss. We report preliminary results on the MS-COCO dataset.



### Dynamical optical flow of saliency maps for predicting visual attention
- **Arxiv ID**: http://arxiv.org/abs/1606.07324v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.07324v1)
- **Published**: 2016-06-23 14:29:43+00:00
- **Updated**: 2016-06-23 14:29:43+00:00
- **Authors**: Aniello Raffaele Patrone, Christian Valuch, Ulrich Ansorge, Otmar Scherzer
- **Comment**: None
- **Journal**: None
- **Summary**: Saliency maps are used to understand human attention and visual fixation. However, while very well established for static images, there is no general agreement on how to compute a saliency map of dynamic scenes. In this paper we propose a mathematically rigorous approach to this prob- lem, including static saliency maps of each video frame for the calculation of the optical flow. Taking into account static saliency maps for calculating the optical flow allows for overcoming the aperture problem. Our ap- proach is able to explain human fixation behavior in situations which pose challenges to standard approaches, such as when a fixated object disappears behind an occlusion and reappears after several frames. In addition, we quantitatively compare our model against alternative solutions using a large eye tracking data set. Together, our results suggest that assessing optical flow information across a series of saliency maps gives a highly accurate and useful account of human overt attention in dynamic scenes.



### DropNeuron: Simplifying the Structure of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1606.07326v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1606.07326v3)
- **Published**: 2016-06-23 14:30:36+00:00
- **Updated**: 2016-07-03 09:39:30+00:00
- **Authors**: Wei Pan, Hao Dong, Yike Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning using multi-layer neural networks (NNs) architecture manifests superb power in modern machine learning systems. The trained Deep Neural Networks (DNNs) are typically large. The question we would like to address is whether it is possible to simplify the NN during training process to achieve a reasonable performance within an acceptable computational time. We presented a novel approach of optimising a deep neural network through regularisation of net- work architecture. We proposed regularisers which support a simple mechanism of dropping neurons during a network training process. The method supports the construction of a simpler deep neural networks with compatible performance with its simplified version. As a proof of concept, we evaluate the proposed method with examples including sparse linear regression, deep autoencoder and convolutional neural network. The valuations demonstrate excellent performance.   The code for this work can be found in http://www.github.com/panweihit/DropNeuron



### Analyzing the Behavior of Visual Question Answering Models
- **Arxiv ID**: http://arxiv.org/abs/1606.07356v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1606.07356v2)
- **Published**: 2016-06-23 16:05:16+00:00
- **Updated**: 2016-09-27 19:56:22+00:00
- **Authors**: Aishwarya Agrawal, Dhruv Batra, Devi Parikh
- **Comment**: 13 pages, 20 figures; To appear in EMNLP 2016
- **Journal**: None
- **Summary**: Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The performance of most models is clustered around 60-70%. In this paper we propose systematic methods to analyze the behavior of these models as a first step towards recognizing their strengths and weaknesses, and identifying the most fruitful directions for progress. We analyze two models, one each from two major classes of VQA models -- with-attention and without-attention and show the similarities and differences in the behavior of these models. We also analyze the winning entry of the VQA Challenge 2016.   Our behavior analysis reveals that despite recent progress, today's VQA models are "myopic" (tend to fail on sufficiently novel instances), often "jump to conclusions" (converge on a predicted answer after 'listening' to just half the question), and are "stubborn" (do not change their answers across images).



### Automatic Neuron Detection in Calcium Imaging Data Using Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1606.07372v2
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1606.07372v2)
- **Published**: 2016-06-23 16:49:40+00:00
- **Updated**: 2016-12-21 23:40:08+00:00
- **Authors**: Noah J. Apthorpe, Alexander J. Riordan, Rob E. Aguilar, Jan Homann, Yi Gu, David W. Tank, H. Sebastian Seung
- **Comment**: 9 pages, 5 figures, 2 ancillary files; minor changes for camera-ready
  version. appears in Advances in Neural Information Processing Systems 29
  (NIPS 2016)
- **Journal**: None
- **Summary**: Calcium imaging is an important technique for monitoring the activity of thousands of neurons simultaneously. As calcium imaging datasets grow in size, automated detection of individual neurons is becoming important. Here we apply a supervised learning approach to this problem and show that convolutional networks can achieve near-human accuracy and superhuman speed. Accuracy is superior to the popular PCA/ICA method based on precision and recall relative to ground truth annotation by a human expert. These results suggest that convolutional networks are an efficient and flexible tool for the analysis of large-scale calcium imaging data.



### VideoMCC: a New Benchmark for Video Comprehension
- **Arxiv ID**: http://arxiv.org/abs/1606.07373v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.07373v5)
- **Published**: 2016-06-23 16:53:22+00:00
- **Updated**: 2017-06-16 19:50:46+00:00
- **Authors**: Du Tran, Maksim Bolonkin, Manohar Paluri, Lorenzo Torresani
- **Comment**: None
- **Journal**: None
- **Summary**: While there is overall agreement that future technology for organizing, browsing and searching videos hinges on the development of methods for high-level semantic understanding of video, so far no consensus has been reached on the best way to train and assess models for this task. Casting video understanding as a form of action or event categorization is problematic as it is not fully clear what the semantic classes or abstractions in this domain should be. Language has been exploited to sidestep the problem of defining video categories, by formulating video understanding as the task of captioning or description. However, language is highly complex, redundant and sometimes ambiguous. Many different captions may express the same semantic concept. To account for this ambiguity, quantitative evaluation of video description requires sophisticated metrics, whose performance scores are typically hard to interpret by humans.   This paper provides four contributions to this problem. First, we formulate Video Multiple Choice Caption (VideoMCC) as a new well-defined task with an easy-to-interpret performance measure. Second, we describe a general semi-automatic procedure to create benchmarks for this task. Third, we publicly release a large-scale video benchmark created with an implementation of this procedure and we include a human study that assesses human performance on our dataset. Finally, we propose and test a varied collection of approaches on this benchmark for the purpose of gaining a better understanding of the new challenges posed by video comprehension.



### Fast Multi-Layer Laplacian Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1606.07396v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.07396v1)
- **Published**: 2016-06-23 18:30:05+00:00
- **Updated**: 2016-06-23 18:30:05+00:00
- **Authors**: Hossein Talebi, Peyman Milanfar
- **Comment**: None
- **Journal**: None
- **Summary**: A novel, fast and practical way of enhancing images is introduced in this paper. Our approach builds on Laplacian operators of well-known edge-aware kernels, such as bilateral and nonlocal means, and extends these filter's capabilities to perform more effective and fast image smoothing, sharpening and tone manipulation. We propose an approximation of the Laplacian, which does not require normalization of the kernel weights. Multiple Laplacians of the affinity weights endow our method with progressive detail decomposition of the input image from fine to coarse scale. These image components are blended by a structure mask, which avoids noise/artifact magnification or detail loss in the output image. Contributions of the proposed method to existing image editing tools are: (1) Low computational and memory requirements, making it appropriate for mobile device implementations (e.g. as a finish step in a camera pipeline), (2) A range of filtering applications from detail enhancement to denoising with only a few control parameters, enabling the user to apply a combination of various (and even opposite) filtering effects.



### Multiplierless 16-point DCT Approximation for Low-complexity Image and Video Coding
- **Arxiv ID**: http://arxiv.org/abs/1606.07414v1
- **DOI**: 10.1007/s11760-016-0923-4
- **Categories**: **cs.CV**, cs.MM, cs.NA, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/1606.07414v1)
- **Published**: 2016-06-23 19:26:01+00:00
- **Updated**: 2016-06-23 19:26:01+00:00
- **Authors**: T. L. T. Silveira, R. S. Oliveira, F. M. Bayer, R. J. Cintra, A. Madanayake
- **Comment**: 12 pages, 5 figures, 3 tables
- **Journal**: None
- **Summary**: An orthogonal 16-point approximate discrete cosine transform (DCT) is introduced. The proposed transform requires neither multiplications nor bit-shifting operations. A fast algorithm based on matrix factorization is introduced, requiring only 44 additions---the lowest arithmetic cost in literature. To assess the introduced transform, computational complexity, similarity with the exact DCT, and coding performance measures are computed. Classical and state-of-the-art 16-point low-complexity transforms were used in a comparative analysis. In the context of image compression, the proposed approximation was evaluated via PSNR and SSIM measurements, attaining the best cost-benefit ratio among the competitors. For video encoding, the proposed approximation was embedded into a HEVC reference software for direct comparison with the original HEVC standard. Physically realized and tested using FPGA hardware, the proposed transform showed 35% and 37% improvements of area-time and area-time-squared VLSI metrics when compared to the best competing transform in the literature.



### Find your Way by Observing the Sun and Other Semantic Cues
- **Arxiv ID**: http://arxiv.org/abs/1606.07415v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.07415v1)
- **Published**: 2016-06-23 19:28:22+00:00
- **Updated**: 2016-06-23 19:28:22+00:00
- **Authors**: Wei-Chiu Ma, Shenlong Wang, Marcus A. Brubaker, Sanja Fidler, Raquel Urtasun
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present a robust, efficient and affordable approach to self-localization which does not require neither GPS nor knowledge about the appearance of the world. Towards this goal, we utilize freely available cartographic maps and derive a probabilistic model that exploits semantic cues in the form of sun direction, presence of an intersection, road type, speed limit as well as the ego-car trajectory in order to produce very reliable localization results. Our experimental evaluation shows that our approach can localize much faster (in terms of driving time) with less computation and more robustly than competing approaches, which ignore semantic information.



### Learning to Poke by Poking: Experiential Learning of Intuitive Physics
- **Arxiv ID**: http://arxiv.org/abs/1606.07419v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1606.07419v2)
- **Published**: 2016-06-23 19:42:57+00:00
- **Updated**: 2017-02-15 22:53:52+00:00
- **Authors**: Pulkit Agrawal, Ashvin Nair, Pieter Abbeel, Jitendra Malik, Sergey Levine
- **Comment**: None
- **Journal**: NIPS 2016
- **Summary**: We investigate an experiential learning paradigm for acquiring an internal model of intuitive physics. Our model is evaluated on a real-world robotic manipulation task that requires displacing objects to target locations by poking. The robot gathered over 400 hours of experience by executing more than 100K pokes on different objects. We propose a novel approach based on deep neural networks for modeling the dynamics of robot's interactions directly from images, by jointly estimating forward and inverse models of dynamics. The inverse model objective provides supervision to construct informative visual features, which the forward model can then predict and in turn regularize the feature space for the inverse model. The interplay between these two objectives creates useful, accurate models that can then be used for multi-step decision making. This formulation has the additional benefit that it is possible to learn forward models in an abstract feature space and thus alleviate the need of predicting pixels. Our experiments show that this joint modeling approach outperforms alternative methods.



### Sort Story: Sorting Jumbled Images and Captions into Stories
- **Arxiv ID**: http://arxiv.org/abs/1606.07493v5
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1606.07493v5)
- **Published**: 2016-06-23 21:54:44+00:00
- **Updated**: 2016-11-07 18:48:13+00:00
- **Authors**: Harsh Agrawal, Arjun Chandrasekaran, Dhruv Batra, Devi Parikh, Mohit Bansal
- **Comment**: EMNLP 2016
- **Journal**: None
- **Summary**: Temporal common sense has applications in AI tasks such as QA, multi-document summarization, and human-AI communication. We propose the task of sequencing -- given a jumbled set of aligned image-caption pairs that belong to a story, the task is to sort them such that the output sequence forms a coherent story. We present multiple approaches, via unary (position) and pairwise (order) predictions, and their ensemble-based combinations, achieving strong results on this task. We use both text-based and image-based features, which depict complementary improvements. Using qualitative examples, we demonstrate that our models have learnt interesting aspects of temporal common sense.



### Is a Picture Worth Ten Thousand Words in a Review Dataset?
- **Arxiv ID**: http://arxiv.org/abs/1606.07496v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR, cs.LG, cs.NE, H.2.8; H.3.3; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1606.07496v1)
- **Published**: 2016-06-23 22:04:08+00:00
- **Updated**: 2016-06-23 22:04:08+00:00
- **Authors**: Roberto Camacho Barranco, Laura M. Rodriguez, Rebecca Urbina, M. Shahriar Hossain
- **Comment**: 10 pages, 11 figures, "for associated results, see
  http://http://auto-captioning.herokuapp.com/" "submitted to DLRS 2016
  workshop"
- **Journal**: None
- **Summary**: While textual reviews have become prominent in many recommendation-based systems, automated frameworks to provide relevant visual cues against text reviews where pictures are not available is a new form of task confronted by data mining and machine learning researchers. Suggestions of pictures that are relevant to the content of a review could significantly benefit the users by increasing the effectiveness of a review. We propose a deep learning-based framework to automatically: (1) tag the images available in a review dataset, (2) generate a caption for each image that does not have one, and (3) enhance each review by recommending relevant images that might not be uploaded by the corresponding reviewer. We evaluate the proposed framework using the Yelp Challenge Dataset. While a subset of the images in this particular dataset are correctly captioned, the majority of the pictures do not have any associated text. Moreover, there is no mapping between reviews and images. Each image has a corresponding business-tag where the picture was taken, though. The overall data setting and unavailability of crucial pieces required for a mapping make the problem of recommending images for reviews a major challenge. Qualitative and quantitative evaluations indicate that our proposed framework provides high quality enhancements through automatic captioning, tagging, and recommendation for mapping reviews and images.



### Convex Decomposition And Efficient Shape Representation Using Deformable Convex Polytopes
- **Arxiv ID**: http://arxiv.org/abs/1606.07509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.07509v1)
- **Published**: 2016-06-23 23:53:09+00:00
- **Updated**: 2016-06-23 23:53:09+00:00
- **Authors**: Fitsum Mesadi, Tolga Tasdizen
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Decomposition of shapes into (approximate) convex parts is essential for applications such as part-based shape representation, shape matching, and collision detection. In this paper, we propose a novel convex decomposition using a parametric implicit shape model called Disjunctive Normal Shape Model (DNSM). The DNSM is formed as a union of polytopes which themselves are formed by intersections of halfspaces. The key idea is by deforming the polytopes, which naturally remain convex during the evolution, the polytopes capture convex parts without the need to compute convexity. The major contributions of this paper include a robust convex decomposition which also results in an efficient part-based shape representation, and a novel shape convexity measure. The experimental results show the potential of the proposed method.



