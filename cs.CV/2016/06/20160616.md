# Arxiv Papers in cs.CV on 2016-06-16
### Zero-Shot Hashing via Transferring Supervised Knowledge
- **Arxiv ID**: http://arxiv.org/abs/1606.05032v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.05032v1)
- **Published**: 2016-06-16 02:56:39+00:00
- **Updated**: 2016-06-16 02:56:39+00:00
- **Authors**: Yang Yang, Weilun Chen, Yadan Luo, Fumin Shen, Jie Shao, Heng Tao Shen
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Hashing has shown its efficiency and effectiveness in facilitating large-scale multimedia applications. Supervised knowledge e.g. semantic labels or pair-wise relationship) associated to data is capable of significantly improving the quality of hash codes and hash functions. However, confronted with the rapid growth of newly-emerging concepts and multimedia data on the Web, existing supervised hashing approaches may easily suffer from the scarcity and validity of supervised information due to the expensive cost of manual labelling. In this paper, we propose a novel hashing scheme, termed \emph{zero-shot hashing} (ZSH), which compresses images of "unseen" categories to binary codes with hash functions learned from limited training data of "seen" categories. Specifically, we project independent data labels i.e. 0/1-form label vectors) into semantic embedding space, where semantic relationships among all the labels can be precisely characterized and thus seen supervised knowledge can be transferred to unseen classes. Moreover, in order to cope with the semantic shift problem, we rotate the embedded space to more suitably align the embedded semantics with the low-level visual feature space, thereby alleviating the influence of semantic gap. In the meantime, to exert positive effects on learning high-quality hash functions, we further propose to preserve local structural property and discrete nature in binary codes. Besides, we develop an efficient alternating algorithm to solve the ZSH model. Extensive experiments conducted on various real-life datasets show the superior zero-shot image retrieval performance of ZSH as compared to several state-of-the-art hashing methods.



### CLEAR: Covariant LEAst-square Re-fitting with applications to image restoration
- **Arxiv ID**: http://arxiv.org/abs/1606.05158v2
- **DOI**: None
- **Categories**: **math.ST**, cs.CV, stat.ML, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/1606.05158v2)
- **Published**: 2016-06-16 12:23:55+00:00
- **Updated**: 2016-09-14 20:45:02+00:00
- **Authors**: C-A. Deledalle, N. Papadakis, J. Salmon, S. Vaiter
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a new framework to remove parts of the systematic errors affecting popular restoration algorithms, with a special focus for image processing tasks. Generalizing ideas that emerged for $\ell_1$ regularization, we develop an approach re-fitting the results of standard methods towards the input data. Total variation regularizations and non-local means are special cases of interest. We identify important covariant information that should be preserved by the re-fitting method, and emphasize the importance of preserving the Jacobian (w.r.t. the observed signal) of the original estimator. Then, we provide an approach that has a "twicing" flavor and allows re-fitting the restored signal by adding back a local affine transformation of the residual term. We illustrate the benefits of our method on numerical simulations for image restoration tasks.



### How many faces can be recognized? Performance extrapolation for multi-class classification
- **Arxiv ID**: http://arxiv.org/abs/1606.05228v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1606.05228v1)
- **Published**: 2016-06-16 15:38:20+00:00
- **Updated**: 2016-06-16 15:38:20+00:00
- **Authors**: Charles Y. Zheng, Rakesh Achanta, Yuval Benjamini
- **Comment**: Submitted to NIPS 2016
- **Journal**: None
- **Summary**: The difficulty of multi-class classification generally increases with the number of classes. Using data from a subset of the classes, can we predict how well a classifier will scale with an increased number of classes? Under the assumption that the classes are sampled exchangeably, and under the assumption that the classifier is generative (e.g. QDA or Naive Bayes), we show that the expected accuracy when the classifier is trained on $k$ classes is the $k-1$st moment of a \emph{conditional accuracy distribution}, which can be estimated from data. This provides the theoretical foundation for performance extrapolation based on pseudolikelihood, unbiased estimation, and high-dimensional asymptotics. We investigate the robustness of our methods to non-generative classifiers in simulations and one optical character recognition example.



### Learning feed-forward one-shot learners
- **Arxiv ID**: http://arxiv.org/abs/1606.05233v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1606.05233v1)
- **Published**: 2016-06-16 15:49:26+00:00
- **Updated**: 2016-06-16 15:49:26+00:00
- **Authors**: Luca Bertinetto, Jo√£o F. Henriques, Jack Valmadre, Philip H. S. Torr, Andrea Vedaldi
- **Comment**: The first three authors contributed equally, and are listed in
  alphabetical order
- **Journal**: None
- **Summary**: One-shot learning is usually tackled by using generative models or discriminative embeddings. Discriminative methods based on deep learning, which are very effective in other learning scenarios, are ill-suited for one-shot learning as they need large amounts of training data. In this paper, we propose a method to learn the parameters of a deep model in one shot. We construct the learner as a second deep network, called a learnet, which predicts the parameters of a pupil network from a single exemplar. In this manner we obtain an efficient feed-forward one-shot learner, trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation. In order to make the construction feasible, we propose a number of factorizations of the parameters of the pupil network. We demonstrate encouraging results by learning characters from single exemplars in Omniglot, and by tracking visual objects from a single initial exemplar in the Visual Object Tracking benchmark.



### 3D zigzag for multislicing, multiband and video processing
- **Arxiv ID**: http://arxiv.org/abs/1606.05255v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.05255v1)
- **Published**: 2016-06-16 16:45:39+00:00
- **Updated**: 2016-06-16 16:45:39+00:00
- **Authors**: Mario Mastriani
- **Comment**: 13 pages, 4 figures
- **Journal**: None
- **Summary**: We present a 3D zigzag rafter (first in literature) which allows us to obtain the exact sequence of spectral components after application of Discrete Cosine Transform 3D (DCT-2D) over a cube. Such cube represents part of a video or eventually a group of images such as multislicing (e.g., Magnetic Resonance or Computed Tomography imaging) and multi or hyperspectral imagery (optical satellites). Besides, we present a new version of the traditional 2D zigzag, including the case of rectangular blocks. Finally, all the attached code is done in MATLAB, and that code serves both blocks of pixels or blocks of blocks.



### Convolutional Residual Memory Networks
- **Arxiv ID**: http://arxiv.org/abs/1606.05262v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.05262v3)
- **Published**: 2016-06-16 16:54:39+00:00
- **Updated**: 2016-07-14 18:40:24+00:00
- **Authors**: Joel Moniz, Christopher Pal
- **Comment**: None
- **Journal**: None
- **Summary**: Very deep convolutional neural networks (CNNs) yield state of the art results on a wide variety of visual recognition problems. A number of state of the the art methods for image recognition are based on networks with well over 100 layers and the performance vs. depth trend is moving towards networks in excess of 1000 layers. In such extremely deep architectures the vanishing or exploding gradient problem becomes a key issue. Recent evidence also indicates that convolutional networks could benefit from an interface to explicitly constructed memory mechanisms interacting with a CNN feature processing hierarchy. Correspondingly, we propose and evaluate a memory mechanism enhanced convolutional neural network architecture based on augmenting convolutional residual networks with a long short term memory mechanism. We refer to this as a convolutional residual memory network. To the best of our knowledge this approach can yield state of the art performance on the CIFAR-100 benchmark and compares well with other state of the art techniques on the CIFAR-10 and SVHN benchmarks. This is achieved using networks with more breadth, much less depth and much less overall computation relative to comparable deep ResNets without the memory mechanism. Our experiments and analysis explore the importance of the memory mechanism, network depth, breadth, and predictive performance.



### Holistic Features For Real-Time Crowd Behaviour Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/1606.05310v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.05310v1)
- **Published**: 2016-06-16 18:37:25+00:00
- **Updated**: 2016-06-16 18:37:25+00:00
- **Authors**: M. Marsden, K. McGuinness, S. Little, N. E. O'Connor
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: This paper presents a new approach to crowd behaviour anomaly detection that uses a set of efficiently computed, easily interpretable, scene-level holistic features. This low-dimensional descriptor combines two features from the literature: crowd collectiveness [1] and crowd conflict [2], with two newly developed crowd features: mean motion speed and a new formulation of crowd density. Two different anomaly detection approaches are investigated using these features. When only normal training data is available we use a Gaussian Mixture Model (GMM) for outlier detection. When both normal and abnormal training data is available we use a Support Vector Machine (SVM) for binary classification. We evaluate on two crowd behaviour anomaly detection datasets, achieving both state-of-the-art classification performance on the violent-flows dataset [3] as well as better than real-time processing performance (40 frames per second).



### Conditional Image Generation with PixelCNN Decoders
- **Arxiv ID**: http://arxiv.org/abs/1606.05328v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1606.05328v2)
- **Published**: 2016-06-16 19:40:56+00:00
- **Updated**: 2016-06-18 15:44:24+00:00
- **Authors**: Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, Koray Kavukcuoglu
- **Comment**: None
- **Journal**: None
- **Summary**: This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.



### Covariance of Motion and Appearance Featuresfor Spatio Temporal Recognition Tasks
- **Arxiv ID**: http://arxiv.org/abs/1606.05355v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.05355v1)
- **Published**: 2016-06-16 20:01:13+00:00
- **Updated**: 2016-06-16 20:01:13+00:00
- **Authors**: Subhabrata Bhattacharya, Nasim Souly, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce an end-to-end framework for video analysis focused towards practical scenarios built on theoretical foundations from sparse representation, including a novel descriptor for general purpose video analysis. In our approach, we compute kinematic features from optical flow and first and second-order derivatives of intensities to represent motion and appearance respectively. These features are then used to construct covariance matrices which capture joint statistics of both low-level motion and appearance features extracted from a video. Using an over-complete dictionary of the covariance based descriptors built from labeled training samples, we formulate low-level event recognition as a sparse linear approximation problem. Within this, we pose the sparse decomposition of a covariance matrix, which also conforms to the space of semi-positive definite matrices, as a determinant maximization problem. Also since covariance matrices lie on non-linear Riemannian manifolds, we compare our former approach with a sparse linear approximation alternative that is suitable for equivalent vector spaces of covariance matrices. This is done by searching for the best projection of the query data on a dictionary using an Orthogonal Matching pursuit algorithm. We show the applicability of our video descriptor in two different application domains - namely low-level event recognition in unconstrained scenarios and gesture recognition using one shot learning. Our experiments provide promising insights in large scale video analysis.



### Deep Image Set Hashing
- **Arxiv ID**: http://arxiv.org/abs/1606.05381v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.05381v2)
- **Published**: 2016-06-16 23:04:02+00:00
- **Updated**: 2016-10-01 02:14:51+00:00
- **Authors**: Jie Feng, Svebor Karaman, I-Hong Jhuo, Shih-Fu Chang
- **Comment**: None
- **Journal**: None
- **Summary**: In applications involving matching of image sets, the information from multiple images must be effectively exploited to represent each set. State-of-the-art methods use probabilistic distribution or subspace to model a set and use specific distance measure to compare two sets. These methods are slow to compute and not compact to use in a large scale scenario. Learning-based hashing is often used in large scale image retrieval as they provide a compact representation of each sample and the Hamming distance can be used to efficiently compare two samples. However, most hashing methods encode each image separately and discard knowledge that multiple images in the same set represent the same object or person. We investigate the set hashing problem by combining both set representation and hashing in a single deep neural network. An image set is first passed to a CNN module to extract image features, then these features are aggregated using two types of set feature to capture both set specific and database-wide distribution information. The computed set feature is then fed into a multilayer perceptron to learn a compact binary embedding. Triplet loss is used to train the network by forming set similarity relations using class labels. We extensively evaluate our approach on datasets used for image matching and show highly competitive performance compared to state-of-the-art methods.



