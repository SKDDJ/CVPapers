# Arxiv Papers in cs.CV on 2016-05-26
### Video Summarization with Long Short-term Memory
- **Arxiv ID**: http://arxiv.org/abs/1605.08110v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1605.08110v2)
- **Published**: 2016-05-26 00:46:35+00:00
- **Updated**: 2016-07-29 07:05:34+00:00
- **Authors**: Ke Zhang, Wei-Lun Chao, Fei Sha, Kristen Grauman
- **Comment**: To appear in ECCV 2016
- **Journal**: None
- **Summary**: We propose a novel supervised learning technique for summarizing videos by automatically selecting keyframes or key subshots. Casting the problem as a structured prediction problem on sequential data, our main idea is to use Long Short-Term Memory (LSTM), a special type of recurrent neural networks to model the variable-range dependencies entailed in the task of video summarization. Our learning models attain the state-of-the-art results on two benchmark video datasets. Detailed analysis justifies the design of the models. In particular, we show that it is crucial to take into consideration the sequential structures in videos and model them. Besides advances in modeling techniques, we introduce techniques to address the need of a large number of annotated data for training complex learning models. There, our main idea is to exploit the existence of auxiliary annotated video datasets, albeit heterogeneous in visual styles and contents. Specifically, we show domain adaptation techniques can improve summarization by reducing the discrepancies in statistical properties across those datasets.



### Automatic Action Annotation in Weakly Labeled Videos
- **Arxiv ID**: http://arxiv.org/abs/1605.08125v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.08125v1)
- **Published**: 2016-05-26 02:22:57+00:00
- **Updated**: 2016-05-26 02:22:57+00:00
- **Authors**: Waqas Sultani, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: Manual spatio-temporal annotation of human action in videos is laborious, requires several annotators and contains human biases. In this paper, we present a weakly supervised approach to automatically obtain spatio-temporal annotations of an actor in action videos. We first obtain a large number of action proposals in each video. To capture a few most representative action proposals in each video and evade processing thousands of them, we rank them using optical flow and saliency in a 3D-MRF based framework and select a few proposals using MAP based proposal subset selection method. We demonstrate that this ranking preserves the high quality action proposals. Several such proposals are generated for each video of the same action. Our next challenge is to iteratively select one proposal from each video so that all proposals are globally consistent. We formulate this as Generalized Maximum Clique Graph problem using shape, global and fine grained similarity of proposals across the videos. The output of our method is the most action representative proposals from each video. Our method can also annotate multiple instances of the same action in a video. We have validated our approach on three challenging action datasets: UCF Sport, sub-JHMDB and THUMOS'13 and have obtained promising results compared to several baseline methods. Moreover, on UCF Sports, we demonstrate that action classifiers trained on these automatically obtained spatio-temporal annotations have comparable performance to the classifiers trained on ground truth annotation.



### Learning Latent Sub-events in Activity Videos Using Temporal Attention Filters
- **Arxiv ID**: http://arxiv.org/abs/1605.08140v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.08140v3)
- **Published**: 2016-05-26 04:02:01+00:00
- **Updated**: 2016-12-26 11:16:33+00:00
- **Authors**: AJ Piergiovanni, Chenyou Fan, Michael S. Ryoo
- **Comment**: None
- **Journal**: AAAI 2017
- **Summary**: In this paper, we newly introduce the concept of temporal attention filters, and describe how they can be used for human activity recognition from videos. Many high-level activities are often composed of multiple temporal parts (e.g., sub-events) with different duration/speed, and our objective is to make the model explicitly learn such temporal structure using multiple attention filters and benefit from them. Our temporal filters are designed to be fully differentiable, allowing end-of-end training of the temporal filters together with the underlying frame-based or segment-based convolutional neural network architectures. This paper presents an approach of learning a set of optimal static temporal attention filters to be shared across different videos, and extends this approach to dynamically adjust attention filters per testing video using recurrent long short-term memory networks (LSTMs). This allows our temporal attention filters to learn latent sub-events specific to each activity. We experimentally confirm that the proposed concept of temporal attention filters benefits the activity recognition, and we visualize the learned latent sub-events.



### Predicting Visual Exemplars of Unseen Classes for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1605.08151v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.08151v2)
- **Published**: 2016-05-26 05:50:09+00:00
- **Updated**: 2017-08-20 05:18:39+00:00
- **Authors**: Soravit Changpinyo, Wei-Lun Chao, Fei Sha
- **Comment**: ICCV2017 camera-ready
- **Journal**: None
- **Summary**: Leveraging class semantic descriptions and examples of known objects, zero-shot learning makes it possible to train a recognition model for an object class whose examples are not available. In this paper, we propose a novel zero-shot learning model that takes advantage of clustering structures in the semantic embedding space. The key idea is to impose the structural constraint that semantic representations must be predictive of the locations of their corresponding visual exemplars. To this end, this reduces to training multiple kernel-based regressors from semantic representation-exemplar pairs from labeled data of the seen object categories. Despite its simplicity, our approach significantly outperforms existing zero-shot learning methods on standard benchmark datasets, including the ImageNet dataset with more than 20,000 unseen categories.



### DeepMovie: Using Optical Flow and Deep Neural Networks to Stylize Movies
- **Arxiv ID**: http://arxiv.org/abs/1605.08153v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1605.08153v1)
- **Published**: 2016-05-26 05:52:10+00:00
- **Updated**: 2016-05-26 05:52:10+00:00
- **Authors**: Alexander G. Anderson, Cory P. Berg, Daniel P. Mossing, Bruno A. Olshausen
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: A recent paper by Gatys et al. describes a method for rendering an image in the style of another image. First, they use convolutional neural network features to build a statistical model for the style of an image. Then they create a new image with the content of one image but the style statistics of another image. Here, we extend this method to render a movie in a given artistic style. The naive solution that independently renders each frame produces poor results because the features of the style move substantially from one frame to the next. The other naive method that initializes the optimization for the next frame using the rendered version of the previous frame also produces poor results because the features of the texture stay fixed relative to the frame of the movie instead of moving with objects in the scene. The main contribution of this paper is to use optical flow to initialize the style transfer optimization so that the texture features move with the objects in the video. Finally, we suggest a method to incorporate optical flow explicitly into the cost function.



### A single scale retinex based method for palm vein extraction
- **Arxiv ID**: http://arxiv.org/abs/1605.08154v1
- **DOI**: 10.1109/ITNEC.2016.7560322
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.08154v1)
- **Published**: 2016-05-26 06:09:24+00:00
- **Updated**: 2016-05-26 06:09:24+00:00
- **Authors**: Chongyang Wang, Ming Peng, Lingfeng Xu, Tong Chen
- **Comment**: 4 pages, 4 figures, received by 2016 IEEE Information
  Technology,Networking,Electronic and Automation Control Conference(ITNEC
  2016)
- **Journal**: None
- **Summary**: Palm vein recognition is a novel biometric identification technology. But how to gain a better vein extraction result from the raw palm image is still a challenging problem, especially when the raw data collection has the problem of asymmetric illumination. This paper proposes a method based on single scale Retinex algorithm to extract palm vein image when strong shadow presents due to asymmetric illumination and uneven geometry of the palm. We test our method on a multispectral palm image. The experimental result shows that the proposed method is robust to the influence of illumination angle and shadow. Compared to the traditional extraction methods, the proposed method can obtain palm vein lines with better visualization performance (the contrast ratio increases by 18.4%, entropy increases by 1.07%, and definition increases by 18.8%).



### Multiple target tracking based on sets of trajectories
- **Arxiv ID**: http://arxiv.org/abs/1605.08163v6
- **DOI**: 10.1109/TAES.2019.2921210
- **Categories**: **cs.CV**, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/1605.08163v6)
- **Published**: 2016-05-26 06:52:13+00:00
- **Updated**: 2020-06-11 15:05:00+00:00
- **Authors**: Ángel F. García-Fernández, Lennart Svensson, Mark R. Morelande
- **Comment**: MATLAB implementations of algorithms based on sets of trajectories
  can be found at https://github.com/Agarciafernandez
- **Journal**: in IEEE Transactions on Aerospace and Electronic Systems, vol. 56,
  no. 3, pp. 1685-1707, June 2020
- **Summary**: We propose a solution of the multiple target tracking (MTT) problem based on sets of trajectories and the random finite set framework. A full Bayesian approach to MTT should characterise the distribution of the trajectories given the measurements, as it contains all information about the trajectories. We attain this by considering multi-object density functions in which objects are trajectories. For the standard tracking models, we also describe a conjugate family of multitrajectory density functions.



### Discovering Causal Signals in Images
- **Arxiv ID**: http://arxiv.org/abs/1605.08179v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1605.08179v2)
- **Published**: 2016-05-26 07:36:56+00:00
- **Updated**: 2017-10-31 11:14:18+00:00
- **Authors**: David Lopez-Paz, Robert Nishihara, Soumith Chintala, Bernhard Schölkopf, Léon Bottou
- **Comment**: None
- **Journal**: None
- **Summary**: This paper establishes the existence of observable footprints that reveal the "causal dispositions" of the object categories appearing in collections of images. We achieve this goal in two steps. First, we take a learning approach to observational causal discovery, and build a classifier that achieves state-of-the-art performance on finding the causal direction between pairs of random variables, given samples from their joint distribution. Second, we use our causal direction classifier to effectively distinguish between features of objects and features of their contexts in collections of static images. Our experiments demonstrate the existence of a relation between the direction of causality and the difference between objects and their contexts, and by the same token, the existence of observable signals that reveal the causal dispositions of objects.



### cvpaper.challenge in 2015 - A review of CVPR2015 and DeepSurvey
- **Arxiv ID**: http://arxiv.org/abs/1605.08247v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1605.08247v1)
- **Published**: 2016-05-26 12:08:55+00:00
- **Updated**: 2016-05-26 12:08:55+00:00
- **Authors**: Hirokatsu Kataoka, Yudai Miyashita, Tomoaki Yamabe, Soma Shirakabe, Shin'ichi Sato, Hironori Hoshino, Ryo Kato, Kaori Abe, Takaaki Imanari, Naomichi Kobayashi, Shinichiro Morita, Akio Nakamura
- **Comment**: Survey Paper
- **Journal**: None
- **Summary**: The "cvpaper.challenge" is a group composed of members from AIST, Tokyo Denki Univ. (TDU), and Univ. of Tsukuba that aims to systematically summarize papers on computer vision, pattern recognition, and related fields. For this particular review, we focused on reading the ALL 602 conference papers presented at the CVPR2015, the premier annual computer vision event held in June 2015, in order to grasp the trends in the field. Further, we are proposing "DeepSurvey" as a mechanism embodying the entire process from the reading through all the papers, the generation of ideas, and to the writing of paper.



### Discrete Deep Feature Extraction: A Theory and New Architectures
- **Arxiv ID**: http://arxiv.org/abs/1605.08283v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IT, cs.NE, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1605.08283v1)
- **Published**: 2016-05-26 13:55:07+00:00
- **Updated**: 2016-05-26 13:55:07+00:00
- **Authors**: Thomas Wiatowski, Michael Tschannen, Aleksandar Stanić, Philipp Grohs, Helmut Bölcskei
- **Comment**: Proc. of International Conference on Machine Learning (ICML), New
  York, USA, June 2016, to appear
- **Journal**: Proc. of International Conference on Machine Learning (ICML), New
  York, USA, pp. 2149-2158, June 2016
- **Summary**: First steps towards a mathematical theory of deep convolutional neural networks for feature extraction were made---for the continuous-time case---in Mallat, 2012, and Wiatowski and B\"olcskei, 2015. This paper considers the discrete case, introduces new convolutional neural network architectures, and proposes a mathematical framework for their analysis. Specifically, we establish deformation and translation sensitivity results of local and global nature, and we investigate how certain structural properties of the input signal are reflected in the corresponding feature vectors. Our theory applies to general filters and general Lipschitz-continuous non-linearities and pooling operators. Experiments on handwritten digit classification and facial landmark detection---including feature importance evaluation---complement the theoretical findings.



### A Light-powered, Always-On, Smart Camera with Compressed Domain Gesture Detection
- **Arxiv ID**: http://arxiv.org/abs/1605.08313v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1605.08313v2)
- **Published**: 2016-05-26 14:52:19+00:00
- **Updated**: 2016-08-16 06:38:45+00:00
- **Authors**: Anvesha A, Shaojie Xu, Ningyuan Cao, Justin Romberg, Arijit Raychowdhury
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose an energy-efficient camera-based gesture recognition system powered by light energy for "always on" applications. Low energy consumption is achieved by directly extracting gesture features from the compressed measurements, which are the block averages and the linear combinations of the image sensor's pixel values. The gestures are recognized using a nearest-neighbour (NN) classifier followed by Dynamic Time Warping (DTW). The system has been implemented on an Analog Devices Black Fin ULP vision processor and powered by PV cells whose output is regulated by TI's DC-DC buck converter with Maximum Power Point Tracking (MPPT). Measured data reveals that with only 400 compressed measurements (768x compression ratio) per frame, the system is able to recognize key wake-up gestures with greater than 80% accuracy and only 95mJ of energy per frame. Owing to its fully self-powered operation, the proposed system can find wide applications in "always-on" vision systems such as in surveillance, robotics and consumer electronics with touch-less operation.



### Aerial image geolocalization from recognition and matching of roads and intersections
- **Arxiv ID**: http://arxiv.org/abs/1605.08323v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.08323v1)
- **Published**: 2016-05-26 15:11:09+00:00
- **Updated**: 2016-05-26 15:11:09+00:00
- **Authors**: Dragos Costea, Marius Leordeanu
- **Comment**: None
- **Journal**: None
- **Summary**: Aerial image analysis at a semantic level is important in many applications with strong potential impact in industry and consumer use, such as automated mapping, urban planning, real estate and environment monitoring, or disaster relief. The problem is enjoying a great interest in computer vision and remote sensing, due to increased computer power and improvement in automated image understanding algorithms. In this paper we address the task of automatic geolocalization of aerial images from recognition and matching of roads and intersections. Our proposed method is a novel contribution in the literature that could enable many applications of aerial image analysis when GPS data is not available. We offer a complete pipeline for geolocalization, from the detection of roads and intersections, to the identification of the enclosing geographic region by matching detected intersections to previously learned manually labeled ones, followed by accurate geometric alignment between the detected roads and the manually labeled maps. We test on a novel dataset with aerial images of two European cities and use the publicly available OpenStreetMap project for collecting ground truth roads annotations. We show in extensive experiments that our approach produces highly accurate localizations in the challenging case when we train on images from one city and test on the other and the quality of the aerial images is relatively poor. We also show that the the alignment between detected roads and pre-stored manual annotations can be effectively used for improving the quality of the road detection results.



### Benign-Malignant Lung Nodule Classification with Geometric and Appearance Histogram Features
- **Arxiv ID**: http://arxiv.org/abs/1605.08350v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.08350v1)
- **Published**: 2016-05-26 16:06:58+00:00
- **Updated**: 2016-05-26 16:06:58+00:00
- **Authors**: Tizita Nesibu Shewaye, Alhayat Ali Mekonnen
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: Lung cancer accounts for the highest number of cancer deaths globally. Early diagnosis of lung nodules is very important to reduce the mortality rate of patients by improving the diagnosis and treatment of lung cancer. This work proposes an automated system to classify lung nodules as malignant and benign in CT images. It presents extensive experimental results using a combination of geometric and histogram lung nodule image features and different linear and non-linear discriminant classifiers. The proposed approach is experimentally validated on the LIDC-IDRI public lung cancer screening thoracic computed tomography (CT) dataset containing nodule level diagnostic data. The obtained results are very encouraging correctly classifying 82% of malignant and 93% of benign nodules on unseen test data at best.



### Pairwise Decomposition of Image Sequences for Active Multi-View Recognition
- **Arxiv ID**: http://arxiv.org/abs/1605.08359v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1605.08359v1)
- **Published**: 2016-05-26 16:44:19+00:00
- **Updated**: 2016-05-26 16:44:19+00:00
- **Authors**: Edward Johns, Stefan Leutenegger, Andrew J. Davison
- **Comment**: CVPR 2016 (oral)
- **Journal**: None
- **Summary**: A multi-view image sequence provides a much richer capacity for object recognition than from a single image. However, most existing solutions to multi-view recognition typically adopt hand-crafted, model-based geometric methods, which do not readily embrace recent trends in deep learning. We propose to bring Convolutional Neural Networks to generic multi-view recognition, by decomposing an image sequence into a set of image pairs, classifying each pair independently, and then learning an object classifier by weighting the contribution of each pair. This allows for recognition over arbitrary camera trajectories, without requiring explicit training over the potentially infinite number of camera paths and lengths. Building these pairwise relationships then naturally extends to the next-best-view problem in an active recognition framework. To achieve this, we train a second Convolutional Neural Network to map directly from an observed image to next viewpoint. Finally, we incorporate this into a trajectory optimisation task, whereby the best recognition confidence is sought for a given trajectory length. We present state-of-the-art results in both guided and unguided multi-view recognition on the ModelNet dataset, and show how our method can be used with depth images, greyscale images, or both.



### Domain Transfer Multi-Instance Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/1605.08397v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.08397v1)
- **Published**: 2016-05-26 18:28:49+00:00
- **Updated**: 2016-05-26 18:28:49+00:00
- **Authors**: Ke Wang, Jiayong Liu, Daniel González
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we invest the domain transfer learning problem with multi-instance data. We assume we already have a well-trained multi-instance dictionary and its corresponding classifier from the source domain, which can be used to represent and classify the bags. But it cannot be directly used to the target domain. Thus we propose to adapt them to the target domain by adding an adaptive term to the source domain classifier. The adaptive function is a linear function based a domain transfer multi-instance dictionary. Given a target domain bag, we first map it to a bag-level feature space using the domain transfer dictionary, and then apply a the linear adaptive function to its bag-level feature vector. To learn the domain-transfer dictionary and the adaptive function parameter, we simultaneously minimize the average classification error of the target domain classifier over the target domain training set, and the complexities of both the adaptive function parameter and the domain transfer dictionary. The minimization problem is solved by an iterative algorithm which update the dictionary and the function parameter alternately. Experiments over several benchmark data sets show the advantage of the proposed method over existing state-of-the-art domain transfer multi-instance learning methods.



### Dense Volume-to-Volume Vascular Boundary Detection
- **Arxiv ID**: http://arxiv.org/abs/1605.08401v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.08401v1)
- **Published**: 2016-05-26 18:40:31+00:00
- **Updated**: 2016-05-26 18:40:31+00:00
- **Authors**: Jameson Merkow, David Kriegman, Alison Marsden, Zhuowen Tu
- **Comment**: Accepted to MICCAI2016
- **Journal**: None
- **Summary**: In this work, we present a novel 3D-Convolutional Neural Network (CNN) architecture called I2I-3D that predicts boundary location in volumetric data. Our fine-to-fine, deeply supervised framework addresses three critical issues to 3D boundary detection: (1) efficient, holistic, end-to-end volumetric label training and prediction (2) precise voxel-level prediction to capture fine scale structures prevalent in medical data and (3) directed multi-scale, multi-level feature learning. We evaluate our approach on a dataset consisting of 93 medical image volumes with a wide variety of anatomical regions and vascular structures. In the process, we also introduce HED-3D, a 3D extension of the state-of-the-art 2D edge detector (HED). We show that our deep learning approach out-performs, the current state-of-the-art in 3D vascular boundary detection (structured forests 3D), by a large margin, as well as HED applied to slices, and HED-3D while successfully localizing fine structures. With our approach, boundary detection takes about one minute on a typical 512x512x512 volume.



### CITlab ARGUS for historical handwritten documents
- **Arxiv ID**: http://arxiv.org/abs/1605.08412v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE, 68T10, 68T05
- **Links**: [PDF](http://arxiv.org/pdf/1605.08412v1)
- **Published**: 2016-05-26 19:19:43+00:00
- **Updated**: 2016-05-26 19:19:43+00:00
- **Authors**: Gundram Leifert, Tobias Strauß, Tobias Grüning, Roger Labahn
- **Comment**: Description of CITlab's System for the HTRtS 2015 Task : Handwritten
  Text Recognition on the tranScriptorium Dataset
- **Journal**: None
- **Summary**: We describe CITlab's recognition system for the HTRtS competition attached to the 13. International Conference on Document Analysis and Recognition, ICDAR 2015. The task comprises the recognition of historical handwritten documents. The core algorithms of our system are based on multi-dimensional recurrent neural networks (MDRNN) and connectionist temporal classification (CTC). The software modules behind that as well as the basic utility technologies are essentially powered by PLANET's ARGUS framework for intelligent text recognition and image processing.



### Low-Cost Scene Modeling using a Density Function Improves Segmentation Performance
- **Arxiv ID**: http://arxiv.org/abs/1605.08464v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1605.08464v1)
- **Published**: 2016-05-26 22:34:37+00:00
- **Updated**: 2016-05-26 22:34:37+00:00
- **Authors**: Vivek Sharma, Sule Yildirim-Yayilgan, Luc Van Gool
- **Comment**: accepted for publication at 25th IEEE International Symposium on
  Robot and Human Interactive Communication (RO-MAN), 2016
- **Journal**: None
- **Summary**: We propose a low cost and effective way to combine a free simulation software and free CAD models for modeling human-object interaction in order to improve human & object segmentation. It is intended for research scenarios related to safe human-robot collaboration (SHRC) and interaction (SHRI) in the industrial domain. The task of human and object modeling has been used for detecting activity, and for inferring and predicting actions, different from those works, we do human and object modeling in order to learn interactions in RGB-D data for improving segmentation. For this purpose, we define a novel density function to model a three dimensional (3D) scene in a virtual environment (VREP). This density function takes into account various possible configurations of human-object and object-object relationships and interactions governed by their affordances. Using this function, we synthesize a large, realistic and highly varied synthetic RGB-D dataset that we use for training. We train a random forest classifier, and the pixelwise predictions obtained is integrated as a unary term in a pairwise conditional random fields (CRF). Our evaluation shows that modeling these interactions improves segmentation performance by ~7\% in mean average precision and recall over state-of-the-art methods that ignore these interactions in real-world data. Our approach is computationally efficient, robust and can run real-time on consumer hardware.



### A Feature based Approach for Video Compression
- **Arxiv ID**: http://arxiv.org/abs/1605.08470v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1605.08470v1)
- **Published**: 2016-05-26 23:04:24+00:00
- **Updated**: 2016-05-26 23:04:24+00:00
- **Authors**: Rajer Sindhu
- **Comment**: Conference on Image Recognition, 2016
- **Journal**: None
- **Summary**: It is a high cost problem for panoramic image stitching via image matching algorithm and not practical for real-time performance. In this paper, we take full advantage ofHarris corner invariant characterization method light intensity parallel meaning, translation and rotation, and made a realtime panoramic image stitching algorithm. According to the basic characteristics and performance FPGA classical algorithm, several modules such as the feature point extraction, and matching description is to optimize the feature-based logic. Real-time optimization system to achieve high precision match. The new algorithm process the image from pixel domain and obtained from CCD camera Xilinx Spartan-6 hardware platform. After the image stitching algorithm, will eventually form a portable interface to output high-definition content on the display. The results showed that, the proposed algorithm has higher precision with good real-time performance and robustness.



