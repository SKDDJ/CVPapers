# Arxiv Papers in cs.CV on 2016-05-27
### SNN: Stacked Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1605.08512v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1605.08512v1)
- **Published**: 2016-05-27 06:02:48+00:00
- **Updated**: 2016-05-27 06:02:48+00:00
- **Authors**: Milad Mohammadi, Subhasis Das
- **Comment**: 8pages
- **Journal**: None
- **Summary**: It has been proven that transfer learning provides an easy way to achieve state-of-the-art accuracies on several vision tasks by training a simple classifier on top of features obtained from pre-trained neural networks. The goal of this work is to generate better features for transfer learning from multiple publicly available pre-trained neural networks. To this end, we propose a novel architecture called Stacked Neural Networks which leverages the fast training time of transfer learning while simultaneously being much more accurate. We show that using a stacked NN architecture can result in up to 8% improvements in accuracy over state-of-the-art techniques using only one pre-trained network for transfer learning. A second aim of this work is to make network fine- tuning retain the generalizability of the base network to unseen tasks. To this end, we propose a new technique called "joint fine-tuning" that is able to give accuracies comparable to finetuning the same network individually over two datasets. We also show that a jointly finetuned network generalizes better to unseen tasks when compared to a network finetuned over a single task.



### Lazy Evaluation of Convolutional Filters
- **Arxiv ID**: http://arxiv.org/abs/1605.08543v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1605.08543v1)
- **Published**: 2016-05-27 08:49:21+00:00
- **Updated**: 2016-05-27 08:49:21+00:00
- **Authors**: Sam Leroux, Steven Bohez, Cedric De Boom, Elias De Coninck, Tim Verbelen, Bert Vankeirsbilck, Pieter Simoens, Bart Dhoedt
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose a technique which avoids the evaluation of certain convolutional filters in a deep neural network. This allows to trade-off the accuracy of a deep neural network with the computational and memory requirements. This is especially important on a constrained device unable to hold all the weights of the network in memory.



### Extraction of clinical information from the non-invasive fetal electrocardiogram
- **Arxiv ID**: http://arxiv.org/abs/1606.01093v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.01093v1)
- **Published**: 2016-05-27 09:31:34+00:00
- **Updated**: 2016-05-27 09:31:34+00:00
- **Authors**: Joachim Behar
- **Comment**: Diss. University of Oxford, 2014
- **Journal**: None
- **Summary**: Estimation of the fetal heart rate (FHR) has gained interest in the last century, low heart rate variability has been studied to identify intrauterine growth restricted fetuses (prepartum), and abnormal FHR patterns have been associated with fetal distress during delivery (intrapartum). Several monitoring techniques have been proposed for FHR estimation, including auscultation and Doppler ultrasound. This thesis focuses on the extraction of the non-invasive fetal electrocardiogram (NI-FECG) recorded from a limited set of abdominal sensors. The main challenge with NI-FECG extraction techniques is the low signal-to-noise ratio of the FECG signal on the abdominal mixture signal which consists of a dominant maternal ECG component, FECG and noise. However the NI-FECG offers many advantages over the alternative fetal monitoring techniques, the most important one being the opportunity to enable morphological analysis of the FECG which is vital for determining whether an observed FHR event is normal or pathological. In order to advance the field of NI-FECG signal processing, the development of standardised public databases and benchmarking of a number of published and novel algorithms was necessary.



### Achieving stable subspace clustering by post-processing generic clustering results
- **Arxiv ID**: http://arxiv.org/abs/1605.08680v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.08680v1)
- **Published**: 2016-05-27 15:15:04+00:00
- **Updated**: 2016-05-27 15:15:04+00:00
- **Authors**: Duc-Son Pham, Ognjen Arandjelovic, Svetha Venkatesh
- **Comment**: International Joint Conference on Neural Networks, 2016
- **Journal**: None
- **Summary**: We propose an effective subspace selection scheme as a post-processing step to improve results obtained by sparse subspace clustering (SSC). Our method starts by the computation of stable subspaces using a novel random sampling scheme. Thus constructed preliminary subspaces are used to identify the initially incorrectly clustered data points and then to reassign them to more suitable clusters based on their goodness-of-fit to the preliminary model. To improve the robustness of the algorithm, we use a dominant nearest subspace classification scheme that controls the level of sensitivity against reassignment. We demonstrate that our algorithm is convergent and superior to the direct application of a generic alternative such as principal component analysis. On several popular datasets for motion segmentation and face clustering pervasively used in the sparse subspace clustering literature the proposed method is shown to reduce greatly the incidence of clustering errors while introducing negligible disturbance to the data points already correctly clustered.



### Stacking With Auxiliary Features
- **Arxiv ID**: http://arxiv.org/abs/1605.08764v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1605.08764v1)
- **Published**: 2016-05-27 19:31:54+00:00
- **Updated**: 2016-05-27 19:31:54+00:00
- **Authors**: Nazneen Fatema Rajani, Raymond J. Mooney
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1604.04802
- **Journal**: None
- **Summary**: Ensembling methods are well known for improving prediction accuracy. However, they are limited in the sense that they cannot discriminate among component models effectively. In this paper, we propose stacking with auxiliary features that learns to fuse relevant information from multiple systems to improve performance. Auxiliary features enable the stacker to rely on systems that not just agree on an output but also the provenance of the output. We demonstrate our approach on three very different and difficult problems -- the Cold Start Slot Filling, the Tri-lingual Entity Discovery and Linking and the ImageNet object detection tasks. We obtain new state-of-the-art results on the first two tasks and substantial improvements on the detection task, thus verifying the power and generality of our approach.



