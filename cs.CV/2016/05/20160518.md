# Arxiv Papers in cs.CV on 2016-05-18
### Are Facial Attributes Adversarially Robust?
- **Arxiv ID**: http://arxiv.org/abs/1605.05411v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.05411v3)
- **Published**: 2016-05-18 01:13:09+00:00
- **Updated**: 2016-09-16 21:49:14+00:00
- **Authors**: Andras Rozsa, Manuel GÃ¼nther, Ethan M. Rudd, Terrance E. Boult
- **Comment**: Pre-print of article accepted to the International Conference on
  Pattern Recognition (ICPR) 2016. 7 pages total
- **Journal**: None
- **Summary**: Facial attributes are emerging soft biometrics that have the potential to reject non-matches, for example, based on mismatching gender. To be usable in stand-alone systems, facial attributes must be extracted from images automatically and reliably. In this paper, we propose a simple yet effective solution for automatic facial attribute extraction by training a deep convolutional neural network (DCNN) for each facial attribute separately, without using any pre-training or dataset augmentation, and we obtain new state-of-the-art facial attribute classification results on the CelebA benchmark. To test the stability of the networks, we generated adversarial images -- formed by adding imperceptible non-random perturbations to original inputs which result in classification errors -- via a novel fast flipping attribute (FFA) technique. We show that FFA generates more adversarial examples than other related algorithms, and that DCNNs for certain attributes are generally robust to adversarial inputs, while DCNNs for other attributes are not. This result is surprising because no DCNNs tested to date have exhibited robustness to adversarial images without explicit augmentation in the training procedure to account for adversarial examples. Finally, we introduce the concept of natural adversarial samples, i.e., images that are misclassified but can be easily turned into correctly classified images by applying small perturbations. We demonstrate that natural adversarial samples commonly occur, even within the training set, and show that many of these images remain misclassified even with additional training epochs. This phenomenon is surprising because correcting the misclassification, particularly when guided by training data, should require only a small adjustment to the DCNN parameters.



### Relative distance features for gait recognition with Kinect
- **Arxiv ID**: http://arxiv.org/abs/1605.05415v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.05415v1)
- **Published**: 2016-05-18 01:38:37+00:00
- **Updated**: 2016-05-18 01:38:37+00:00
- **Authors**: Ke Yang, Yong Dou, Shaohe Lv, Fei Zhang, Qi Lv
- **Comment**: 28 pages,10 figures, under 2nd round review of JVIC
- **Journal**: None
- **Summary**: Gait and static body measurement are important biometric technologies for passive human recognition. Many previous works argue that recognition performance based completely on the gait feature is limited. The reason for this limited performance remains unclear. This study focuses on human recognition with gait feature obtained by Kinect and shows that gait feature can effectively distinguish from different human beings through a novel representation -- relative distance-based gait features. Experimental results show that the recognition accuracy with relative distance features reaches up to 85%, which is comparable with that of anthropometric features. The combination of relative distance features and anthropometric features can provide an accuracy of more than 95%. Results indicate that the relative distance feature is quite effective and worthy of further study in more general scenarios (e.g., without Kinect).



### Beyond Caption To Narrative: Video Captioning With Multiple Sentences
- **Arxiv ID**: http://arxiv.org/abs/1605.05440v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.05440v1)
- **Published**: 2016-05-18 05:00:12+00:00
- **Updated**: 2016-05-18 05:00:12+00:00
- **Authors**: Andrew Shin, Katsunori Ohnishi, Tatsuya Harada
- **Comment**: accepted to ICIP 2016
- **Journal**: None
- **Summary**: Recent advances in image captioning task have led to increasing interests in video captioning task. However, most works on video captioning are focused on generating single input of aggregated features, which hardly deviates from image captioning process and does not fully take advantage of dynamic contents present in videos. We attempt to generate video captions that convey richer contents by temporally segmenting the video with action localization, generating multiple captions from multiple frames, and connecting them with natural language processing techniques, in order to generate a story-like caption. We show that our proposed method can generate captions that are richer in contents and can compete with state-of-the-art method without explicitly using video-level features as input.



### Dual Local-Global Contextual Pathways for Recognition in Aerial Imagery
- **Arxiv ID**: http://arxiv.org/abs/1605.05462v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.05462v1)
- **Published**: 2016-05-18 07:37:22+00:00
- **Updated**: 2016-05-18 07:37:22+00:00
- **Authors**: Alina Marcu, Marius Leordeanu
- **Comment**: None
- **Journal**: None
- **Summary**: Visual context is important in object recognition and it is still an open problem in computer vision. Along with the advent of deep convolutional neural networks (CNN), using contextual information with such systems starts to receive attention in the literature. At the same time, aerial imagery is gaining momentum. While advances in deep learning make good progress in aerial image analysis, this problem still poses many great challenges. Aerial images are often taken under poor lighting conditions and contain low resolution objects, many times occluded by trees or taller buildings. In this domain, in particular, visual context could be of great help, but there are still very few papers that consider context in aerial image understanding. Here we introduce context as a complementary way of recognizing objects. We propose a dual-stream deep neural network model that processes information along two independent pathways, one for local and another for global visual reasoning. The two are later combined in the final layers of processing. Our model learns to combine local object appearance as well as information from the larger scene at the same time and in a complementary way, such that together they form a powerful classifier. We test our dual-stream network on the task of segmentation of buildings and roads in aerial images and obtain state-of-the-art results on the Massachusetts Buildings Dataset. We also introduce two new datasets, for buildings and road segmentation, respectively, and study the relative importance of local appearance vs. the larger scene, as well as their performance in combination. While our local-global model could also be useful in general recognition tasks, we clearly demonstrate the effectiveness of visual context in conjunction with deep nets for aerial image understanding.



### Image segmentation with superpixel-based covariance descriptors in low-rank representation
- **Arxiv ID**: http://arxiv.org/abs/1605.05466v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.05466v1)
- **Published**: 2016-05-18 07:44:38+00:00
- **Updated**: 2016-05-18 07:44:38+00:00
- **Authors**: Xianbin Gu, Jeremiah D. Deng, Martin K. Purvis
- **Comment**: 7 pages, 2 figures, 1 table
- **Journal**: None
- **Summary**: This paper investigates the problem of image segmentation using superpixels. We propose two approaches to enhance the discriminative ability of the superpixel's covariance descriptors. In the first one, we employ the Log-Euclidean distance as the metric on the covariance manifolds, and then use the RBF kernel to measure the similarities between covariance descriptors. The second method is focused on extracting the subspace structure of the set of covariance descriptors by extending a low rank representation algorithm on to the covariance manifolds. Experiments are carried out with the Berkly Segmentation Dataset, and compared with the state-of-the-art segmentation algorithms, both methods are competitive.



### Improving Weakly-Supervised Object Localization By Micro-Annotation
- **Arxiv ID**: http://arxiv.org/abs/1605.05538v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.05538v1)
- **Published**: 2016-05-18 12:06:35+00:00
- **Updated**: 2016-05-18 12:06:35+00:00
- **Authors**: Alexander Kolesnikov, Christoph H. Lampert
- **Comment**: None
- **Journal**: None
- **Summary**: Weakly-supervised object localization methods tend to fail for object classes that consistently co-occur with the same background elements, e.g. trains on tracks. We propose a method to overcome these failures by adding a very small amount of model-specific additional annotation. The main idea is to cluster a deep network's mid-level representations and assign object or distractor labels to each cluster. Experiments show substantially improved localization results on the challenging ILSVC2014 dataset for bounding box detection and the PASCAL VOC2012 dataset for semantic segmentation.



### A deep convolutional neural network approach to single-particle recognition in cryo-electron microscopy
- **Arxiv ID**: http://arxiv.org/abs/1605.05543v2
- **DOI**: 10.1186/s12859-017-1757-y
- **Categories**: **physics.data-an**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1605.05543v2)
- **Published**: 2016-05-18 12:15:10+00:00
- **Updated**: 2016-12-31 07:22:21+00:00
- **Authors**: Yanan Zhu, Qi Ouyang, Youdong Mao
- **Comment**: 26 pages, 6 figures, 1 table
- **Journal**: BMC Bioinformatics 18, 348 (2017)
- **Summary**: Background: Single-particle cryo-electron microscopy (cryo-EM) has become a popular tool for structural determination of biological macromolecular complexes. High-resolution cryo-EM reconstruction often requires hundreds of thousands of single-particle images. Particle extraction from experimental micrographs thus can be laborious and presents a major practical bottleneck in cryo-EM structural determination. Existing computational methods of particle picking often use low-resolution templates as inputs for particle matching, making it possible to cause reference-dependent bias. It is critical to develop a highly efficient template-free method to automatically recognize particle images from cryo-EM micrographs. Results: We developed a deep learning-based algorithmic framework, DeepEM, for single-particle recognition from noisy cryo-EM micrographs, enabling automated particle picking, selection and verification in an integrated fashion. The kernel of DeepEM is built upon a convolutional neural network (CNN) of eight layers, which can be recursively trained to be highly "knowledgeable". Our approach exhibits improved performance and high precision when tested on the standard KLH dataset. Application of DeepEM to several challenging experimental cryo-EM datasets demonstrates its capability in avoiding selection of un-wanted particles and non-particles even when true particles contain fewer features. Conclusions: The DeepEM method derived from a deep CNN allows automated particle extraction from raw cryo-EM micrographs in the absence of templates, which demonstrated improved performance, objectivity and accuracy. Application of this novel approach is expected to free the labor involved in single-particle verification, thus promoting the efficiency of cryo-EM data processing.



### Low-Rank Matrices on Graphs: Generalized Recovery & Applications
- **Arxiv ID**: http://arxiv.org/abs/1605.05579v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.05579v3)
- **Published**: 2016-05-18 13:50:04+00:00
- **Updated**: 2016-05-25 20:50:42+00:00
- **Authors**: Nauman Shahid, Nathanael Perraudin, Pierre Vandergheynst
- **Comment**: None
- **Journal**: None
- **Summary**: Many real world datasets subsume a linear or non-linear low-rank structure in a very low-dimensional space. Unfortunately, one often has very little or no information about the geometry of the space, resulting in a highly under-determined recovery problem. Under certain circumstances, state-of-the-art algorithms provide an exact recovery for linear low-rank structures but at the expense of highly inscalable algorithms which use nuclear norm. However, the case of non-linear structures remains unresolved. We revisit the problem of low-rank recovery from a totally different perspective, involving graphs which encode pairwise similarity between the data samples and features. Surprisingly, our analysis confirms that it is possible to recover many approximate linear and non-linear low-rank structures with recovery guarantees with a set of highly scalable and efficient algorithms. We call such data matrices as \textit{Low-Rank matrices on graphs} and show that many real world datasets satisfy this assumption approximately due to underlying stationarity. Our detailed theoretical and experimental analysis unveils the power of the simple, yet very novel recovery framework \textit{Fast Robust PCA on Graphs}



### Scalable low dimensional manifold model in the reconstruction of noisy and incomplete hyperspectral images
- **Arxiv ID**: http://arxiv.org/abs/1605.05652v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1605.05652v2)
- **Published**: 2016-05-18 16:47:53+00:00
- **Updated**: 2018-03-23 19:16:01+00:00
- **Authors**: Wei Zhu, Zuoqiang Shi, Stanley Osher
- **Comment**: None
- **Journal**: None
- **Summary**: We present a scalable low dimensional manifold model for the reconstruction of noisy and incomplete hyperspectral images. The model is based on the observation that the spatial-spectral blocks of a hyperspectral image typically lie close to a collection of low dimensional manifolds. To emphasize this, the dimension of the manifold is directly used as a regularizer in a variational functional, which is solved efficiently by alternating direction of minimization and weighted nonlocal Laplacian. Unlike general 3D images, the same similarity matrix can be shared across all spectral bands for a hyperspectral image, therefore the resulting algorithm is much more scalable than that for general 3D data. Numerical experiments on the reconstruction of hyperspectral images from sparse and noisy sampling demonstrate the superiority of our proposed algorithm in terms of both speed and accuracy.



### Robust Image Descriptors for Real-Time Inter-Examination Retargeting in Gastrointestinal Endoscopy
- **Arxiv ID**: http://arxiv.org/abs/1605.05757v2
- **DOI**: 10.1007/978-3-319-46720-7_52
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.05757v2)
- **Published**: 2016-05-18 21:07:35+00:00
- **Updated**: 2016-10-30 12:19:12+00:00
- **Authors**: Menglong Ye, Edward Johns, Benjamin Walter, Alexander Meining, Guang-Zhong Yang
- **Comment**: This paper was presented in MICCAI 2016 conference, and a DOI was
  linked to the publisher's version
- **Journal**: None
- **Summary**: For early diagnosis of malignancies in the gastrointestinal tract, surveillance endoscopy is increasingly used to monitor abnormal tissue changes in serial examinations of the same patient. Despite successes with optical biopsy for in vivo and in situ tissue characterisation, biopsy retargeting for serial examinations is challenging because tissue may change in appearance between examinations. In this paper, we propose an inter-examination retargeting framework for optical biopsy, based on an image descriptor designed for matching between endoscopic scenes over significant time intervals. Each scene is described by a hierarchy of regional intensity comparisons at various scales, offering tolerance to long-term change in tissue appearance whilst remaining discriminative. Binary coding is then used to compress the descriptor via a novel random forests approach, providing fast comparisons in Hamming space and real-time retargeting. Extensive validation conducted on 13 in vivo gastrointestinal videos, collected from six patients, show that our approach outperforms state-of-the-art methods.



