# Arxiv Papers in cs.CV on 2016-05-20
### Fine-Grained Classification of Pedestrians in Video: Benchmark and State of the Art
- **Arxiv ID**: http://arxiv.org/abs/1605.06177v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.06177v1)
- **Published**: 2016-05-20 00:03:42+00:00
- **Updated**: 2016-05-20 00:03:42+00:00
- **Authors**: David Hall, Pietro Perona
- **Comment**: CVPR 2015
- **Journal**: None
- **Summary**: A video dataset that is designed to study fine-grained categorisation of pedestrians is introduced. Pedestrians were recorded "in-the-wild" from a moving vehicle. Annotations include bounding boxes, tracks, 14 keypoints with occlusion information and the fine-grained categories of age (5 classes), sex (2 classes), weight (3 classes) and clothing style (4 classes). There are a total of 27,454 bounding box and pose labels across 4222 tracks. This dataset is designed to train and test algorithms for fine-grained categorisation of people, it is also useful for benchmarking tracking, detection and pose estimation of pedestrians. State-of-the-art algorithms for fine-grained classification and pose estimation were tested using the dataset and the results are reported as a useful performance baseline.



### Dimensionality Reduction on SPD Manifolds: The Emergence of Geometry-Aware Methods
- **Arxiv ID**: http://arxiv.org/abs/1605.06182v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.06182v1)
- **Published**: 2016-05-20 00:33:48+00:00
- **Updated**: 2016-05-20 00:33:48+00:00
- **Authors**: Mehrtash Harandi, Mathieu Salzmann, Richard Hartley
- **Comment**: arXiv admin note: text overlap with arXiv:1407.1120
- **Journal**: None
- **Summary**: Representing images and videos with Symmetric Positive Definite (SPD) matrices, and considering the Riemannian geometry of the resulting space, has been shown to yield high discriminative power in many visual recognition tasks. Unfortunately, computation on the Riemannian manifold of SPD matrices -especially of high-dimensional ones- comes at a high cost that limits the applicability of existing techniques. In this paper, we introduce algorithms able to handle high-dimensional SPD matrices by constructing a lower-dimensional SPD manifold. To this end, we propose to model the mapping from the high-dimensional SPD manifold to the low-dimensional one with an orthonormal projection. This lets us formulate dimensionality reduction as the problem of finding a projection that yields a low-dimensional manifold either with maximum discriminative power in the supervised scenario, or with maximum variance of the data in the unsupervised one. We show that learning can be expressed as an optimization problem on a Grassmann manifold and discuss fast solutions for special cases. Our evaluation on several classification tasks evidences that our approach leads to a significant accuracy gain over state-of-the-art methods.



### Fully Convolutional Networks for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1605.06211v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.06211v1)
- **Published**: 2016-05-20 04:30:16+00:00
- **Updated**: 2016-05-20 04:30:16+00:00
- **Authors**: Evan Shelhamer, Jonathan Long, Trevor Darrell
- **Comment**: to appear in PAMI (accepted May, 2016); journal edition of
  arXiv:1411.4038
- **Journal**: None
- **Summary**: Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves improved segmentation of PASCAL VOC (30% relative improvement to 67.2% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.



### Efficient Feature-based Image Registration by Mapping Sparsified Surfaces
- **Arxiv ID**: http://arxiv.org/abs/1605.06215v2
- **DOI**: 10.1016/j.jvcir.2018.07.005
- **Categories**: **cs.GR**, cs.CG, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1605.06215v2)
- **Published**: 2016-05-20 05:42:12+00:00
- **Updated**: 2020-04-23 03:32:25+00:00
- **Authors**: Chun Pang Yung, Gary P. T. Choi, Ke Chen, Lok Ming Lui
- **Comment**: None
- **Journal**: Journal of Visual Communication and Image Representation 55,
  561-571 (2018)
- **Summary**: With the advancement in the digital camera technology, the use of high resolution images and videos has been widespread in the modern society. In particular, image and video frame registration is frequently applied in computer graphics and film production. However, conventional registration approaches usually require long computational time for high resolution images and video frames. This hinders the application of the registration approaches in the modern industries. In this work, we first propose a new image representation method to accelerate the registration process by triangulating the images effectively. For each high resolution image or video frame, we compute an optimal coarse triangulation which captures the important features of the image. Then, we apply a surface registration algorithm to obtain a registration map which is used to compute the registration of the high resolution image. Experimental results suggest that our overall algorithm is efficient and capable to achieve a high compression rate while the accuracy of the registration is well retained when compared with the conventional grid-based approach. Also, the computational time of the registration is significantly reduced using our triangulation-based approach.



### Localizing by Describing: Attribute-Guided Attention Localization for Fine-Grained Recognition
- **Arxiv ID**: http://arxiv.org/abs/1605.06217v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.06217v2)
- **Published**: 2016-05-20 05:54:54+00:00
- **Updated**: 2016-05-23 03:37:54+00:00
- **Authors**: Xiao Liu, Jiang Wang, Shilei Wen, Errui Ding, Yuanqing Lin
- **Comment**: None
- **Journal**: None
- **Summary**: A key challenge in fine-grained recognition is how to find and represent discriminative local regions. Recent attention models are capable of learning discriminative region localizers only from category labels with reinforcement learning. However, not utilizing any explicit part information, they are not able to accurately find multiple distinctive regions. In this work, we introduce an attribute-guided attention localization scheme where the local region localizers are learned under the guidance of part attribute descriptions. By designing a novel reward strategy, we are able to learn to locate regions that are spatially and semantically distinctive with reinforcement learning algorithm. The attribute labeling requirement of the scheme is more amenable than the accurate part location annotation required by traditional part-based fine-grained recognition methods. Experimental results on the CUB-200-2011 dataset demonstrate the superiority of the proposed scheme on both fine-grained recognition and attribute recognition.



### FPNN: Field Probing Neural Networks for 3D Data
- **Arxiv ID**: http://arxiv.org/abs/1605.06240v3
- **DOI**: None
- **Categories**: **cs.CV**, I.5.1; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1605.06240v3)
- **Published**: 2016-05-20 08:15:57+00:00
- **Updated**: 2016-10-25 03:59:16+00:00
- **Authors**: Yangyan Li, Soeren Pirk, Hao Su, Charles R. Qi, Leonidas J. Guibas
- **Comment**: To appear in NIPS 2016
- **Journal**: None
- **Summary**: Building discriminative representations for 3D data has been an important task in computer graphics and computer vision research. Convolutional Neural Networks (CNNs) have shown to operate on 2D images with great success for a variety of tasks. Lifting convolution operators to 3D (3DCNNs) seems like a plausible and promising next step. Unfortunately, the computational complexity of 3D CNNs grows cubically with respect to voxel resolution. Moreover, since most 3D geometry representations are boundary based, occupied regions do not increase proportionately with the size of the discretization, resulting in wasted computation. In this work, we represent 3D spaces as volumetric fields, and propose a novel design that employs field probing filters to efficiently extract features from them. Each field probing filter is a set of probing points --- sensors that perceive the space. Our learning algorithm optimizes not only the weights associated with the probing points, but also their locations, which deforms the shape of the probing filters and adaptively distributes them in 3D space. The optimized probing points sense the 3D space "intelligently", rather than operating blindly over the entire domain. We show that field probing is significantly more efficient than 3DCNNs, while providing state-of-the-art performance, on classification tasks for 3D object recognition benchmark datasets.



### End-to-End Kernel Learning with Supervised Convolutional Kernel Networks
- **Arxiv ID**: http://arxiv.org/abs/1605.06265v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1605.06265v2)
- **Published**: 2016-05-20 09:52:14+00:00
- **Updated**: 2016-10-25 12:52:50+00:00
- **Authors**: Julien Mairal
- **Comment**: to appear in Advances in Neural Information Processing Systems (NIPS)
- **Journal**: None
- **Summary**: In this paper, we introduce a new image representation based on a multilayer kernel machine. Unlike traditional kernel methods where data representation is decoupled from the prediction task, we learn how to shape the kernel with supervision. We proceed by first proposing improvements of the recently-introduced convolutional kernel networks (CKNs) in the context of unsupervised learning; then, we derive backpropagation rules to take advantage of labeled training data. The resulting model is a new type of convolutional neural network, where optimizing the filters at each layer is equivalent to learning a linear subspace in a reproducing kernel Hilbert space (RKHS). We show that our method achieves reasonably competitive performance for image classification on some standard "deep learning" datasets such as CIFAR-10 and SVHN, and also for image super-resolution, demonstrating the applicability of our approach to a large variety of image-related tasks.



### Poisson multi-Bernoulli conjugate prior for multiple extended object filtering
- **Arxiv ID**: http://arxiv.org/abs/1605.06311v6
- **DOI**: 10.1109/TAES.2019.2920220
- **Categories**: **stat.CO**, cs.CV, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/1605.06311v6)
- **Published**: 2016-05-20 12:05:59+00:00
- **Updated**: 2019-12-06 09:12:45+00:00
- **Authors**: Karl Granstrom, Maryam Fatemi, Lennart Svensson
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a Poisson multi-Bernoulli mixture (PMBM) conjugate prior for multiple extended object filtering. A Poisson point process is used to describe the existence of yet undetected targets, while a multi-Bernoulli mixture describes the distribution of the targets that have been detected. The prediction and update equations are presented for the standard transition density and measurement likelihood. Both the prediction and the update preserve the PMBM form of the density, and in this sense the PMBM density is a conjugate prior. However, the unknown data associations lead to an intractably large number of terms in the PMBM density, and approximations are necessary for tractability. A gamma Gaussian inverse Wishart implementation is presented, along with methods to handle the data association problem. A simulation study shows that the extended target PMBM filter performs well in comparison to the extended target d-GLMB and LMB filters. An experiment with Lidar data illustrates the benefit of tracking both detected and undetected targets.



### Superpixel Hierarchy
- **Arxiv ID**: http://arxiv.org/abs/1605.06325v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.06325v1)
- **Published**: 2016-05-20 12:38:24+00:00
- **Updated**: 2016-05-20 12:38:24+00:00
- **Authors**: Xing Wei, Qingxiong Yang, Yihong Gong, Ming-Hsuan Yang, Narendra Ahuja
- **Comment**: None
- **Journal**: None
- **Summary**: Superpixel segmentation is becoming ubiquitous in computer vision. In practice, an object can either be represented by a number of segments in finer levels of detail or included in a surrounding region at coarser levels of detail, and thus a superpixel segmentation hierarchy is useful for applications that require different levels of image segmentation detail depending on the particular image objects segmented. Unfortunately, there is no method that can generate all scales of superpixels accurately in real-time. As a result, a simple yet effective algorithm named Super Hierarchy (SH) is proposed in this paper. It is as accurate as the state-of-the-art but 1-2 orders of magnitude faster. The proposed method can be directly integrated with recent efficient edge detectors like the structured forest edges to significantly outperforms the state-of-the-art in terms of segmentation accuracy. Quantitative and qualitative evaluation on a number of computer vision applications was conducted, demonstrating that the proposed method is the top performer.



### Ristretto: Hardware-Oriented Approximation of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1605.06402v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1605.06402v1)
- **Published**: 2016-05-20 15:22:29+00:00
- **Updated**: 2016-05-20 15:22:29+00:00
- **Authors**: Philipp Gysel
- **Comment**: Master's Thesis, University of California, Davis; 73 pages and 29
  figures
- **Journal**: None
- **Summary**: Convolutional neural networks (CNN) have achieved major breakthroughs in recent years. Their performance in computer vision have matched and in some areas even surpassed human capabilities. Deep neural networks can capture complex non-linear features; however this ability comes at the cost of high computational and memory requirements. State-of-art networks require billions of arithmetic operations and millions of parameters. To enable embedded devices such as smartphones, Google glasses and monitoring cameras with the astonishing power of deep learning, dedicated hardware accelerators can be used to decrease both execution time and power consumption. In applications where fast connection to the cloud is not guaranteed or where privacy is important, computation needs to be done locally. Many hardware accelerators for deep neural networks have been proposed recently. A first important step of accelerator design is hardware-oriented approximation of deep networks, which enables energy-efficient inference. We present Ristretto, a fast and automated framework for CNN approximation. Ristretto simulates the hardware arithmetic of a custom hardware accelerator. The framework reduces the bit-width of network parameters and outputs of resource-intense layers, which reduces the chip area for multiplication units significantly. Alternatively, Ristretto can remove the need for multipliers altogether, resulting in an adder-only arithmetic. The tool fine-tunes trimmed networks to achieve high classification accuracy. Since training of deep neural networks can be time-consuming, Ristretto uses highly optimized routines which run on the GPU. This enables fast compression of any given network. Given a maximum tolerance of 1%, Ristretto can successfully condense CaffeNet and SqueezeNet to 8-bit. The code for Ristretto is available.



### R-FCN: Object Detection via Region-based Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1605.06409v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.06409v2)
- **Published**: 2016-05-20 15:50:11+00:00
- **Updated**: 2016-06-21 15:28:57+00:00
- **Authors**: Jifeng Dai, Yi Li, Kaiming He, Jian Sun
- **Comment**: Tech report
- **Journal**: None
- **Summary**: We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets), for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20x faster than the Faster R-CNN counterpart. Code is made publicly available at: https://github.com/daijifeng001/r-fcn



### Shape Recognition by Bag of Skeleton-associated Contour Parts
- **Arxiv ID**: http://arxiv.org/abs/1605.06417v1
- **DOI**: 10.1007/978-3-662-45646-0_40
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.06417v1)
- **Published**: 2016-05-20 16:07:41+00:00
- **Updated**: 2016-05-20 16:07:41+00:00
- **Authors**: Wei Shen, Yuan Jiang, Wenjing Gao, Dan Zeng, Xinggang Wang
- **Comment**: 10 pages. Has been Accepted by Pattern Recognition Letters 2016
- **Journal**: None
- **Summary**: Contour and skeleton are two complementary representations for shape recognition. However combining them in a principal way is nontrivial, as they are generally abstracted by different structures (closed string vs graph), respectively. This paper aims at addressing the shape recognition problem by combining contour and skeleton according to the correspondence between them. The correspondence provides a straightforward way to associate skeletal information with a shape contour. More specifically, we propose a new shape descriptor. named Skeleton-associated Shape Context (SSC), which captures the features of a contour fragment associated with skeletal information. Benefited from the association, the proposed shape descriptor provides the complementary geometric information from both contour and skeleton parts, including the spatial distribution and the thickness change along the shape part. To form a meaningful shape feature vector for an overall shape, the Bag of Features framework is applied to the SSC descriptors extracted from it. Finally, the shape feature vector is fed into a linear SVM classifier to recognize the shape. The encouraging experimental results demonstrate that the proposed way to combine contour and skeleton is effective for shape recognition, which achieves the state-of-the-art performances on several standard shape benchmarks.



### Residual Networks Behave Like Ensembles of Relatively Shallow Networks
- **Arxiv ID**: http://arxiv.org/abs/1605.06431v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1605.06431v2)
- **Published**: 2016-05-20 16:44:03+00:00
- **Updated**: 2016-10-27 00:43:58+00:00
- **Authors**: Andreas Veit, Michael Wilber, Serge Belongie
- **Comment**: NIPS 2016
- **Journal**: None
- **Summary**: In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.



### Learning shape correspondence with anisotropic convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1605.06437v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.06437v1)
- **Published**: 2016-05-20 17:02:40+00:00
- **Updated**: 2016-05-20 17:02:40+00:00
- **Authors**: Davide Boscaini, Jonathan Masci, Emanuele Rodolà, Michael M. Bronstein
- **Comment**: None
- **Journal**: None
- **Summary**: Establishing correspondence between shapes is a fundamental problem in geometry processing, arising in a wide variety of applications. The problem is especially difficult in the setting of non-isometric deformations, as well as in the presence of topological noise and missing parts, mainly due to the limited capability to model such deformations axiomatically. Several recent works showed that invariance to complex shape transformations can be learned from examples. In this paper, we introduce an intrinsic convolutional neural network architecture based on anisotropic diffusion kernels, which we term Anisotropic Convolutional Neural Network (ACNN). In our construction, we generalize convolutions to non-Euclidean domains by constructing a set of oriented anisotropic diffusion kernels, creating in this way a local intrinsic polar representation of the data (`patch'), which is then correlated with a filter. Several cascades of such filters, linear, and non-linear operators are stacked to form a deep neural network whose parameters are learned by minimizing a task-specific cost. We use ACNNs to effectively learn intrinsic dense correspondences between deformable shapes in very challenging settings, achieving state-of-the-art results on some of the most difficult recent correspondence benchmarks.



### Virtual Worlds as Proxy for Multi-Object Tracking Analysis
- **Arxiv ID**: http://arxiv.org/abs/1605.06457v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1605.06457v1)
- **Published**: 2016-05-20 18:03:07+00:00
- **Updated**: 2016-05-20 18:03:07+00:00
- **Authors**: Adrien Gaidon, Qiao Wang, Yohann Cabon, Eleonora Vig
- **Comment**: CVPR 2016, Virtual KITTI dataset download at
  http://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds
- **Journal**: None
- **Summary**: Modern computer vision algorithms typically require expensive data acquisition and accurate manual labeling. In this work, we instead leverage the recent progress in computer graphics to generate fully labeled, dynamic, and photo-realistic proxy virtual worlds. We propose an efficient real-to-virtual world cloning method, and validate our approach by building and publicly releasing a new video dataset, called Virtual KITTI (see http://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds), automatically labeled with accurate ground truth for object detection, tracking, scene and instance segmentation, depth, and optical flow. We provide quantitative experimental evidence suggesting that (i) modern deep learning algorithms pre-trained on real data behave similarly in real and virtual worlds, and (ii) pre-training on virtual data improves performance. As the gap between real and virtual worlds is small, virtual worlds enable measuring the impact of various weather and imaging conditions on recognition performance, all other things being equal. We show these factors may affect drastically otherwise high-performing deep models for tracking.



### Swapout: Learning an ensemble of deep architectures
- **Arxiv ID**: http://arxiv.org/abs/1605.06465v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1605.06465v1)
- **Published**: 2016-05-20 18:39:33+00:00
- **Updated**: 2016-05-20 18:39:33+00:00
- **Authors**: Saurabh Singh, Derek Hoiem, David Forsyth
- **Comment**: Submitted to NIPS 2016
- **Journal**: None
- **Summary**: We describe Swapout, a new stochastic training method, that outperforms ResNets of identical network structure yielding impressive results on CIFAR-10 and CIFAR-100. Swapout samples from a rich set of architectures including dropout, stochastic depth and residual architectures as special cases. When viewed as a regularization method swapout not only inhibits co-adaptation of units in a layer, similar to dropout, but also across network layers. We conjecture that swapout achieves strong regularization by implicitly tying the parameters across layers. When viewed as an ensemble training method, it samples a much richer set of architectures than existing methods such as dropout or stochastic depth. We propose a parameterization that reveals connections to exiting architectures and suggests a much richer set of architectures to be explored. We show that our formulation suggests an efficient training method and validate our conclusions on CIFAR-10 and CIFAR-100 matching state of the art accuracy. Remarkably, our 32 layer wider model performs similar to a 1001 layer ResNet model.



### X-ray image separation via coupled dictionary learning
- **Arxiv ID**: http://arxiv.org/abs/1605.06474v1
- **DOI**: 10.1109/ICIP.2016.7533017
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.06474v1)
- **Published**: 2016-05-20 19:11:04+00:00
- **Updated**: 2016-05-20 19:11:04+00:00
- **Authors**: Nikos Deligiannis, João F. C. Mota, Bruno Cornelis, Miguel R. D. Rodrigues, Ingrid Daubechies
- **Comment**: To be presented at the IEEE International Conference on Image
  Processing (ICIP), 2016
- **Journal**: None
- **Summary**: In support of art investigation, we propose a new source sepa- ration method that unmixes a single X-ray scan acquired from double-sided paintings. Unlike prior source separation meth- ods, which are based on statistical or structural incoherence of the sources, we use visual images taken from the front- and back-side of the panel to drive the separation process. The coupling of the two imaging modalities is achieved via a new multi-scale dictionary learning method. Experimental results demonstrate that our method succeeds in the discrimination of the sources, while state-of-the-art methods fail to do so.



### Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups
- **Arxiv ID**: http://arxiv.org/abs/1605.06489v3
- **DOI**: 10.1109/CVPR.2017.633
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1605.06489v3)
- **Published**: 2016-05-20 19:51:37+00:00
- **Updated**: 2016-11-30 15:32:03+00:00
- **Authors**: Yani Ioannou, Duncan Robertson, Roberto Cipolla, Antonio Criminisi
- **Comment**: Updated full version of paper, in full letter paper two-column paper.
  Includes many textual changes, updated CIFAR10 results, and new analysis of
  inter/intra-layer correlation
- **Journal**: None
- **Summary**: We propose a new method for creating computationally efficient and compact convolutional neural networks (CNNs) using a novel sparse connection structure that resembles a tree root. This allows a significant reduction in computational cost and number of parameters compared to state-of-the-art deep CNNs, without compromising accuracy, by exploiting the sparsity of inter-layer filter dependencies. We validate our approach by using it to train more efficient variants of state-of-the-art CNN architectures, evaluated on the CIFAR10 and ILSVRC datasets. Our results show similar or higher accuracy than the baseline architectures with much less computation, as measured by CPU and GPU timings. For example, for ResNet 50, our model has 40% fewer parameters, 45% fewer floating point operations, and is 31% (12%) faster on a CPU (GPU). For the deeper ResNet 200 our model has 25% fewer floating point operations and 44% fewer parameters, while maintaining state-of-the-art accuracy. For GoogLeNet, our model has 7% fewer parameters and is 21% (16%) faster on a CPU (GPU).



