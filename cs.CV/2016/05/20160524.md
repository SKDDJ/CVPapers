# Arxiv Papers in cs.CV on 2016-05-24
### Dense CNN Learning with Equivalent Mappings
- **Arxiv ID**: http://arxiv.org/abs/1605.07251v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.07251v1)
- **Published**: 2016-05-24 01:24:26+00:00
- **Updated**: 2016-05-24 01:24:26+00:00
- **Authors**: Jianxin Wu, Chen-Wei Xie, Jian-Hao Luo
- **Comment**: submitted to NIPS 2016
- **Journal**: None
- **Summary**: Large receptive field and dense prediction are both important for achieving high accuracy in pixel labeling tasks such as semantic segmentation. These two properties, however, contradict with each other. A pooling layer (with stride 2) quadruples the receptive field size but reduces the number of predictions to 25\%. Some existing methods lead to dense predictions using computations that are not equivalent to the original model. In this paper, we propose the equivalent convolution (eConv) and equivalent pooling (ePool) layers, leading to predictions that are both dense and equivalent to the baseline CNN model. Dense prediction models learned using eConv and ePool can transfer the baseline CNN's parameters as a starting point, and can inverse transfer the learned parameters in a dense model back to the original one, which has both fast testing speed and high accuracy. The proposed eConv and ePool layers have achieved higher accuracy than baseline CNN in various tasks, including semantic segmentation, object localization, image categorization and apparent age estimation, not only in those tasks requiring dense pixel labeling.



### Measuring Neural Net Robustness with Constraints
- **Arxiv ID**: http://arxiv.org/abs/1605.07262v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1605.07262v2)
- **Published**: 2016-05-24 02:18:21+00:00
- **Updated**: 2017-06-16 11:58:51+00:00
- **Authors**: Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya Nori, Antonio Criminisi
- **Comment**: None
- **Journal**: None
- **Summary**: Despite having high accuracy, neural nets have been shown to be susceptible to adversarial examples, where a small perturbation to an input can cause it to become mislabeled. We propose metrics for measuring the robustness of a neural net and devise a novel algorithm for approximating these metrics based on an encoding of robustness as a linear program. We show how our metrics can be used to evaluate the robustness of deep neural nets with experiments on the MNIST and CIFAR-10 datasets. Our algorithm generates more informative estimates of robustness metrics compared to estimates based on existing algorithms. Furthermore, we show how existing approaches to improving robustness "overfit" to adversarial examples generated using a specific algorithm. Finally, we show that our techniques can be used to additionally improve neural net robustness both according to the metrics that we propose, but also according to previously proposed metrics.



### Trajectory probability hypothesis density filter
- **Arxiv ID**: http://arxiv.org/abs/1605.07264v2
- **DOI**: 10.23919/ICIF.2018.8455270
- **Categories**: **stat.AP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1605.07264v2)
- **Published**: 2016-05-24 02:19:11+00:00
- **Updated**: 2018-09-13 14:05:43+00:00
- **Authors**: Ángel F. García-Fernández, Lennart Svensson
- **Comment**: Published in the Proceedings of the 21st International Conference on
  Information Fusion (FUSION)
- **Journal**: None
- **Summary**: This paper presents the probability hypothesis density (PHD) filter for sets of trajectories: the trajectory probability density (TPHD) filter. The TPHD filter is capable of estimating trajectories in a principled way without requiring to evaluate all measurement-to-target association hypotheses. The TPHD filter is based on recursively obtaining the best Poisson approximation to the multitrajectory filtering density in the sense of minimising the Kullback-Leibler divergence. We also propose a Gaussian mixture implementation of the TPHD recursion. Finally, we include simulation results to show the performance of the proposed algorithm.



### Learning a Metric Embedding for Face Recognition using the Multibatch Method
- **Arxiv ID**: http://arxiv.org/abs/1605.07270v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.07270v1)
- **Published**: 2016-05-24 02:42:53+00:00
- **Updated**: 2016-05-24 02:42:53+00:00
- **Authors**: Oren Tadmor, Yonatan Wexler, Tal Rosenwein, Shai Shalev-Shwartz, Amnon Shashua
- **Comment**: None
- **Journal**: None
- **Summary**: This work is motivated by the engineering task of achieving a near state-of-the-art face recognition on a minimal computing budget running on an embedded system. Our main technical contribution centers around a novel training method, called Multibatch, for similarity learning, i.e., for the task of generating an invariant "face signature" through training pairs of "same" and "not-same" face images. The Multibatch method first generates signatures for a mini-batch of $k$ face images and then constructs an unbiased estimate of the full gradient by relying on all $k^2-k$ pairs from the mini-batch. We prove that the variance of the Multibatch estimator is bounded by $O(1/k^2)$, under some mild conditions. In contrast, the standard gradient estimator that relies on random $k/2$ pairs has a variance of order $1/k$. The smaller variance of the Multibatch estimator significantly speeds up the convergence rate of stochastic gradient descent. Using the Multibatch method we train a deep convolutional neural network that achieves an accuracy of $98.2\%$ on the LFW benchmark, while its prediction runtime takes only $30$msec on a single ARM Cortex A9 core. Furthermore, the entire training process took only 12 hours on a single Titan X GPU.



### EventNet Version 1.1 Technical Report
- **Arxiv ID**: http://arxiv.org/abs/1605.07289v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.07289v2)
- **Published**: 2016-05-24 05:06:29+00:00
- **Updated**: 2016-09-11 19:10:33+00:00
- **Authors**: Dongang Wang, Zheng Shou, Hongyi Liu, Shih-Fu Chang
- **Comment**: None
- **Journal**: None
- **Summary**: EventNet is a large-scale video corpus and event ontology consisting of 500 events associated with event-specific concepts. In order to improve the quality of the current EventNet, we conduct the following steps and introduce EventNet version 1.1: (1) manually verify the correctness of event labels for all videos; (2) remove the YouTube user bias by limiting the maximum number of videos in each event from the same YouTube user as 3; (3) remove the videos which are currently not accessible online; (4) remove the video belonging to multiple event categories. After the above procedure, some events may contain only a small number of videos, and therefore we crawl more videos for those events to ensure every event will contain more than 50 videos. Finally, EventNet version 1.1 contains 67,641 videos, 500 events, and 5,028 event-specific concepts. In addition, we train a Convolutional Neural Network (CNN) model for event classification via fine-tuning AlexNet using EventNet version 1.1. Then we use the trained CNN model to extract FC7 layer feature and train binary classifiers using linear SVM for each event-specific concept. We believe this new version of EventNet will significantly facilitate research in computer vision and multimedia, and will put it online for public downloading in the future.



### DeepText: A Unified Framework for Text Proposal Generation and Text Detection in Natural Images
- **Arxiv ID**: http://arxiv.org/abs/1605.07314v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.07314v1)
- **Published**: 2016-05-24 06:48:23+00:00
- **Updated**: 2016-05-24 06:48:23+00:00
- **Authors**: Zhuoyao Zhong, Lianwen Jin, Shuye Zhang, Ziyong Feng
- **Comment**: 12 pages, 4 figures, 3 tables
- **Journal**: None
- **Summary**: In this paper, we develop a novel unified framework called DeepText for text region proposal generation and text detection in natural images via a fully convolutional neural network (CNN). First, we propose the inception region proposal network (Inception-RPN) and design a set of text characteristic prior bounding boxes to achieve high word recall with only hundred level candidate proposals. Next, we present a powerful textdetection network that embeds ambiguous text category (ATC) information and multilevel region-of-interest pooling (MLRP) for text and non-text classification and accurate localization. Finally, we apply an iterative bounding box voting scheme to pursue high recall in a complementary manner and introduce a filtering algorithm to retain the most suitable bounding box, while removing redundant inner and outer boxes for each text instance. Our approach achieves an F-measure of 0.83 and 0.85 on the ICDAR 2011 and 2013 robust text detection benchmarks, outperforming previous state-of-the-art results.



### Spatio-Temporal Image Boundary Extrapolation
- **Arxiv ID**: http://arxiv.org/abs/1605.07363v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.07363v1)
- **Published**: 2016-05-24 10:22:33+00:00
- **Updated**: 2016-05-24 10:22:33+00:00
- **Authors**: Apratim Bhattacharyya, Mateusz Malinowski, Mario Fritz
- **Comment**: None
- **Journal**: None
- **Summary**: Boundary prediction in images as well as video has been a very active topic of research and organizing visual information into boundaries and segments is believed to be a corner stone of visual perception. While prior work has focused on predicting boundaries for observed frames, our work aims at predicting boundaries of future unobserved frames. This requires our model to learn about the fate of boundaries and extrapolate motion patterns. We experiment on established real-world video segmentation dataset, which provides a testbed for this new task. We show for the first time spatio-temporal boundary extrapolation in this challenging scenario. Furthermore, we show long-term prediction of boundaries in situations where the motion is governed by the laws of physics. We successfully predict boundaries in a billiard scenario without any assumptions of a strong parametric model or any object notion. We argue that our model has with minimalistic model assumptions derived a notion of 'intuitive physics' that can be applied to novel scenes.



### Quickest Moving Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1605.07369v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.07369v1)
- **Published**: 2016-05-24 10:40:13+00:00
- **Updated**: 2016-05-24 10:40:13+00:00
- **Authors**: Dong Lao, Ganesh Sundaramoorthi
- **Comment**: None
- **Journal**: None
- **Summary**: We present a general framework and method for simultaneous detection and segmentation of an object in a video that moves (or comes into view of the camera) at some unknown time in the video. The method is an online approach based on motion segmentation, and it operates under dynamic backgrounds caused by a moving camera or moving nuisances. The goal of the method is to detect and segment the object as soon as it moves. Due to stochastic variability in the video and unreliability of the motion signal, several frames are needed to reliably detect the object. The method is designed to detect and segment with minimum delay subject to a constraint on the false alarm rate. The method is derived as a problem of Quickest Change Detection. Experiments on a dataset show the effectiveness of our method in minimizing detection delay subject to false alarm constraints.



### Natural Scene Image Segmentation Based on Multi-Layer Feature Extraction
- **Arxiv ID**: http://arxiv.org/abs/1605.07586v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.07586v2)
- **Published**: 2016-05-24 19:03:54+00:00
- **Updated**: 2016-10-11 03:58:34+00:00
- **Authors**: Fariba Zohrizadeh, Mohsen Kheirandishfard, Farhad Kamangar
- **Comment**: This paper has been withdrawn by the author due to the fact that the
  contents need further research
- **Journal**: None
- **Summary**: This paper addresses the problem of natural image segmentation by extracting information from a multi-layer array which is constructed based on color, gradient, and statistical properties of the local neighborhoods in an image. A Gaussian Mixture Model (GMM) is used to improve the effectiveness of local spectral histogram features. Grouping these features leads to forming a rough initial over-segmented layer which contains coherent regions of pixels. The regions are merged by using two proposed functions for calculating the distance between two neighboring regions and making decisions about their merging. Extensive experiments are performed on the Berkeley Segmentation Dataset to evaluate the performance of our proposed method and compare the results with the recent state-of-the-art methods. The experimental results indicate that our method achieves higher level of accuracy for natural images compared to recent methods.



### FractalNet: Ultra-Deep Neural Networks without Residuals
- **Arxiv ID**: http://arxiv.org/abs/1605.07648v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.07648v4)
- **Published**: 2016-05-24 20:28:53+00:00
- **Updated**: 2017-05-26 18:53:56+00:00
- **Authors**: Gustav Larsson, Michael Maire, Gregory Shakhnarovich
- **Comment**: updated with ImageNet results; published as a conference paper at
  ICLR 2017; project page at http://people.cs.uchicago.edu/~larsson/fractalnet/
- **Journal**: None
- **Summary**: We introduce a design strategy for neural network macro-architecture based on self-similarity. Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals. These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers. In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks. Rather, the key may be the ability to transition, during training, from effectively shallow to deep. We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures. Such regularization allows extraction of high-performance fixed-depth subnetworks. Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.



### Blind Analysis of CT Image Noise Using Residual Denoised Images
- **Arxiv ID**: http://arxiv.org/abs/1605.07650v1
- **DOI**: 10.1109/NSSMIC.2015.7582055
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1605.07650v1)
- **Published**: 2016-05-24 20:31:39+00:00
- **Updated**: 2016-05-24 20:31:39+00:00
- **Authors**: Sohini Roychowdhury, Nathan Hollraft, Adam Alessio
- **Comment**: 4 pages, 6 figures, IEEE NSS/MIC 2015
- **Journal**: None
- **Summary**: CT protocol design and quality control would benefit from automated tools to estimate the quality of generated CT images. These tools could be used to identify erroneous CT acquisitions or refine protocols to achieve certain signal to noise characteristics. This paper investigates blind estimation methods to determine global signal strength and noise levels in chest CT images. Methods: We propose novel performance metrics corresponding to the accuracy of noise and signal estimation. We implement and evaluate the noise estimation performance of six spatial- and frequency- based methods, derived from conventional image filtering algorithms. Algorithms were tested on patient data sets from whole-body repeat CT acquisitions performed with a higher and lower dose technique over the same scan region. Results: The proposed performance metrics can evaluate the relative tradeoff of filter parameters and noise estimation performance. The proposed automated methods tend to underestimate CT image noise at low-flux levels. Initial application of methodology suggests that anisotropic diffusion and Wavelet-transform based filters provide optimal estimates of noise. Furthermore, methodology does not provide accurate estimates of absolute noise levels, but can provide estimates of relative change and/or trends in noise levels.



### Self Paced Deep Learning for Weakly Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1605.07651v3
- **DOI**: 10.1109/TPAMI.2018.2804907
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.07651v3)
- **Published**: 2016-05-24 20:34:03+00:00
- **Updated**: 2018-02-21 16:33:44+00:00
- **Authors**: Enver Sangineto, Moin Nabi, Dubravko Culibrk, Nicu Sebe
- **Comment**: To appear at IEEE Transactions on PAMI
- **Journal**: None
- **Summary**: In a weakly-supervised scenario object detectors need to be trained using image-level annotation alone. Since bounding-box-level ground truth is not available, most of the solutions proposed so far are based on an iterative, Multiple Instance Learning framework in which the current classifier is used to select the highest-confidence boxes in each image, which are treated as pseudo-ground truth in the next training iteration. However, the errors of an immature classifier can make the process drift, usually introducing many of false positives in the training dataset. To alleviate this problem, we propose in this paper a training protocol based on the self-paced learning paradigm. The main idea is to iteratively select a subset of images and boxes that are the most reliable, and use them for training. While in the past few years similar strategies have been adopted for SVMs and other classifiers, we are the first showing that a self-paced approach can be used with deep-network-based classifiers in an end-to-end training pipeline. The method we propose is built on the fully-supervised Fast-RCNN architecture and can be applied to similar architectures which represent the input image as a bag of boxes. We show state-of-the-art results on Pascal VOC 2007, Pascal VOC 2010 and ILSVRC 2013. On ILSVRC 2013 our results based on a low-capacity AlexNet network outperform even those weakly-supervised approaches which are based on much higher-capacity networks.



### An Analysis of Deep Neural Network Models for Practical Applications
- **Arxiv ID**: http://arxiv.org/abs/1605.07678v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.07678v4)
- **Published**: 2016-05-24 22:36:02+00:00
- **Updated**: 2017-04-14 23:40:21+00:00
- **Authors**: Alfredo Canziani, Adam Paszke, Eugenio Culurciello
- **Comment**: 7 pages, 10 figures, legend for Figure 2 got lost :/
- **Journal**: None
- **Summary**: Since the emergence of Deep Neural Networks (DNNs) as a prominent technique in the field of computer vision, the ImageNet classification challenge has played a major role in advancing the state-of-the-art. While accuracy figures have steadily increased, the resource utilisation of winning models has not been properly taken into account. In this work, we present a comprehensive analysis of important metrics in practical applications: accuracy, memory footprint, parameters, operations count, inference time and power consumption. Key findings are: (1) power consumption is independent of batch size and architecture; (2) accuracy and inference time are in a hyperbolic relationship; (3) energy constraint is an upper bound on the maximum achievable accuracy and model complexity; (4) the number of operations is a reliable estimate of the inference time. We believe our analysis provides a compelling set of information that helps design and engineer efficient DNNs.



### Convolutional Random Walk Networks for Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1605.07681v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.07681v3)
- **Published**: 2016-05-24 23:00:52+00:00
- **Updated**: 2017-05-08 17:49:21+00:00
- **Authors**: Gedas Bertasius, Lorenzo Torresani, Stella X. Yu, Jianbo Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Most current semantic segmentation methods rely on fully convolutional networks (FCNs). However, their use of large receptive fields and many pooling layers cause low spatial resolution inside the deep layers. This leads to predictions with poor localization around the boundaries. Prior work has attempted to address this issue by post-processing predictions with CRFs or MRFs. But such models often fail to capture semantic relationships between objects, which causes spatially disjoint predictions. To overcome these problems, recent methods integrated CRFs or MRFs into an FCN framework. The downside of these new models is that they have much higher complexity than traditional FCNs, which renders training and testing more challenging.   In this work we introduce a simple, yet effective Convolutional Random Walk Network (RWN) that addresses the issues of poor boundary localization and spatially fragmented predictions with very little increase in model complexity. Our proposed RWN jointly optimizes the objectives of pixelwise affinity and semantic segmentation. It combines these two objectives via a novel random walk layer that enforces consistent spatial grouping in the deep layers of the network. Our RWN is implemented using standard convolution and matrix multiplication. This allows an easy integration into existing FCN frameworks and it enables end-to-end training of the whole network via standard back-propagation. Our implementation of RWN requires just $131$ additional parameters compared to the traditional FCNs, and yet it consistently produces an improvement over the FCNs on semantic segmentation and scene labeling.



### Local Perturb-and-MAP for Structured Prediction
- **Arxiv ID**: http://arxiv.org/abs/1605.07686v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.07686v2)
- **Published**: 2016-05-24 23:25:23+00:00
- **Updated**: 2016-10-13 20:32:42+00:00
- **Authors**: Gedas Bertasius, Qiang Liu, Lorenzo Torresani, Jianbo Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Conditional random fields (CRFs) provide a powerful tool for structured prediction, but cast significant challenges in both the learning and inference steps. Approximation techniques are widely used in both steps, which should be considered jointly to guarantee good performance (a.k.a. "inferning"). Perturb-and-MAP models provide a promising alternative to CRFs, but require global combinatorial optimization and hence they are usable only on specific models. In this work, we present a new Local Perturb-and-MAP (locPMAP) framework that replaces the global optimization with a local optimization by exploiting our observed connection between locPMAP and the pseudolikelihood of the original CRF model. We test our approach on three different vision tasks and show that our method achieves consistently improved performance over other approximate inference techniques optimized to a pseudolikelihood objective. Additionally, we demonstrate that we can integrate our method in the fully convolutional network framework to increase our model's complexity. Finally, our observed connection between locPMAP and the pseudolikelihood leads to a novel perspective for understanding and using pseudolikelihood.



