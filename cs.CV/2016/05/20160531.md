# Arxiv Papers in cs.CV on 2016-05-31
### Quantitative Analysis of Saliency Models
- **Arxiv ID**: http://arxiv.org/abs/1605.09451v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1605.09451v1)
- **Published**: 2016-05-31 00:33:04+00:00
- **Updated**: 2016-05-31 00:33:04+00:00
- **Authors**: Flora Ponjou Tasse, Jiří Kosinka, Neil Anthony Dodgson
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Previous saliency detection research required the reader to evaluate performance qualitatively, based on renderings of saliency maps on a few shapes. This qualitative approach meant it was unclear which saliency models were better, or how well they compared to human perception. This paper provides a quantitative evaluation framework that addresses this issue. In the first quantitative analysis of 3D computational saliency models, we evaluate four computational saliency models and two baseline models against ground-truth saliency collected in previous work.



### Latent Bi-constraint SVM for Video-based Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1605.09452v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.09452v1)
- **Published**: 2016-05-31 00:34:37+00:00
- **Updated**: 2016-05-31 00:34:37+00:00
- **Authors**: Yang Liu, Minh Hoai, Mang Shao, Tae-Kyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: We address the task of recognizing objects from video input. This important problem is relatively unexplored, compared with image-based object recognition. To this end, we make the following contributions. First, we introduce two comprehensive datasets for video-based object recognition. Second, we propose Latent Bi-constraint SVM (LBSVM), a maximum-margin framework for video-based object recognition. LBSVM is based on Structured-Output SVM, but extends it to handle noisy video data and ensure consistency of the output decision throughout time. We apply LBSVM to recognize office objects and museum sculptures, and we demonstrate its benefits over image-based, set-based, and other video-based object recognition.



### Deep convolutional neural networks for predominant instrument recognition in polyphonic music
- **Arxiv ID**: http://arxiv.org/abs/1605.09507v3
- **DOI**: 10.1109/TASLP.2016.2632307
- **Categories**: **cs.SD**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1605.09507v3)
- **Published**: 2016-05-31 07:11:18+00:00
- **Updated**: 2016-12-26 12:29:26+00:00
- **Authors**: Yoonchang Han, Jaehun Kim, Kyogu Lee
- **Comment**: 13 pages, 7 figures, accepted for publication in IEEE/ACM
  Transactions on Audio, Speech, and Language Processing on 16-Nov-2016. This
  is initial submission version. Fully edited version is available at
  http://ieeexplore.ieee.org/document/7755799/
- **Journal**: Published in: IEEE/ACM Transactions on Audio, Speech, and Language
  Processing ( Volume: 25, Issue: 1, Jan. 2017 ) Page(s): 208 - 221
- **Summary**: Identifying musical instruments in polyphonic music recordings is a challenging but important problem in the field of music information retrieval. It enables music search by instrument, helps recognize musical genres, or can make music transcription easier and more accurate. In this paper, we present a convolutional neural network framework for predominant instrument recognition in real-world polyphonic music. We train our network from fixed-length music excerpts with a single-labeled predominant instrument and estimate an arbitrary number of predominant instruments from an audio signal with a variable length. To obtain the audio-excerpt-wise result, we aggregate multiple outputs from sliding windows over the test audio. In doing so, we investigated two different aggregation methods: one takes the average for each instrument and the other takes the instrument-wise sum followed by normalization. In addition, we conducted extensive experiments on several important factors that affect the performance, including analysis window size, identification threshold, and activation functions for neural networks to find the optimal set of parameters. Using a dataset of 10k audio excerpts from 11 instruments for evaluation, we found that convolutional neural networks are more robust than conventional methods that exploit spectral features and source separation with support vector machines. Experimental results showed that the proposed convolutional network architecture obtained an F1 measure of 0.602 for micro and 0.503 for macro, respectively, achieving 19.6% and 16.4% in performance improvement compared with other state-of-the-art algorithms.



### Predicting Human Intentions from Motion Only: A 2D+3D Fusion Approach
- **Arxiv ID**: http://arxiv.org/abs/1605.09526v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.09526v4)
- **Published**: 2016-05-31 08:43:25+00:00
- **Updated**: 2017-09-06 09:11:10+00:00
- **Authors**: Andrea Zunino, Jacopo Cavazza, Atesh Koul, Andrea Cavallo, Cristina Becchio, Vittorio Murino
- **Comment**: accepted as poster at the 25th ACM Multimedia (ACM MM) 2017, Mountain
  View, California, USA
- **Journal**: None
- **Summary**: In this paper, we address the new problem of the prediction of human intents. There is neuro-psychological evidence that actions performed by humans are anticipated by peculiar motor acts which are discriminant of the type of action going to be performed afterwards. In other words, an actual intent can be forecast by looking at the kinematics of the immediately preceding movement. To prove it in a computational and quantitative manner, we devise a new experimental setup where, without using contextual information, we predict human intents all originating from the same motor act. We posit the problem as a classification task and we introduce a new multi-modal dataset consisting of a set of motion capture marker 3D data and 2D video sequences, where, by only analysing very similar movements in both training and test phases, we are able to predict the underlying intent, i.e., the future, never observed action. We also present an extensive experimental evaluation as a baseline, customizing state-of-the-art techniques for either 3D and 2D data analysis. Realizing that video processing methods lead to inferior performance but show complementary information with respect to 3D data sequences, we developed a 2D+3D fusion analysis where we achieve better classification accuracies, attesting the superiority of the multimodal approach for the context-free prediction of human intents.



### Biconvex Relaxation for Semidefinite Programming in Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1605.09527v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1605.09527v2)
- **Published**: 2016-05-31 08:43:44+00:00
- **Updated**: 2016-08-08 23:23:28+00:00
- **Authors**: Sohil Shah, Abhay Kumar, Carlos Castillo, David Jacobs, Christoph Studer, Tom Goldstein
- **Comment**: None
- **Journal**: None
- **Summary**: Semidefinite programming is an indispensable tool in computer vision, but general-purpose solvers for semidefinite programs are often too slow and memory intensive for large-scale problems. We propose a general framework to approximately solve large-scale semidefinite problems (SDPs) at low complexity. Our approach, referred to as biconvex relaxation (BCR), transforms a general SDP into a specific biconvex optimization problem, which can then be solved in the original, low-dimensional variable space at low complexity. The resulting biconvex problem is solved using an efficient alternating minimization (AM) procedure. Since AM has the potential to get stuck in local minima, we propose a general initialization scheme that enables BCR to start close to a global optimum - this is key for our algorithm to quickly converge to optimal or near-optimal solutions. We showcase the efficacy of our approach on three applications in computer vision, namely segmentation, co-segmentation, and manifold metric learning. BCR achieves solution quality comparable to state-of-the-art SDP methods with speedups between 4X and 35X. At the same time, BCR handles a more general set of SDPs than previous approaches, which are more specialized.



### Robust Deep-Learning-Based Road-Prediction for Augmented Reality Navigation Systems
- **Arxiv ID**: http://arxiv.org/abs/1605.09533v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1605.09533v1)
- **Published**: 2016-05-31 09:00:33+00:00
- **Updated**: 2016-05-31 09:00:33+00:00
- **Authors**: Matthias Limmer, Julian Forster, Dennis Baudach, Florian Schüle, Roland Schweiger, Hendrik P. A. Lensch
- **Comment**: 8 pages, 12 figures, submitted to ITSC 2016
- **Journal**: None
- **Summary**: This paper proposes an approach that predicts the road course from camera sensors leveraging deep learning techniques. Road pixels are identified by training a multi-scale convolutional neural network on a large number of full-scene-labeled night-time road images including adverse weather conditions. A framework is presented that applies the proposed approach to longer distance road course estimation, which is the basis for an augmented reality navigation application. In this framework long range sensor data (radar) and data from a map database are fused with short range sensor data (camera) to produce a precise longitudinal and lateral localization and road course estimation. The proposed approach reliably detects roads with and without lane markings and thus increases the robustness and availability of road course estimations and augmented reality navigation. Evaluations on an extensive set of high precision ground truth data taken from a differential GPS and an inertial measurement unit show that the proposed approach reaches state-of-the-art performance without the limitation of requiring existing lane markings.



### Semantic-Aware Depth Super-Resolution in Outdoor Scenes
- **Arxiv ID**: http://arxiv.org/abs/1605.09546v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.09546v1)
- **Published**: 2016-05-31 09:37:55+00:00
- **Updated**: 2016-05-31 09:37:55+00:00
- **Authors**: Miaomiao Liu, Mathieu Salzmann, Xuming He
- **Comment**: None
- **Journal**: None
- **Summary**: While depth sensors are becoming increasingly popular, their spatial resolution often remains limited. Depth super-resolution therefore emerged as a solution to this problem. Despite much progress, state-of-the-art techniques suffer from two drawbacks: (i) they rely on the assumption that intensity edges coincide with depth discontinuities, which, unfortunately, is only true in controlled environments; and (ii) they typically exploit the availability of high-resolution training depth maps, which can often not be acquired in practice due to the sensors' limitations. By contrast, here, we introduce an approach to performing depth super-resolution in more challenging conditions, such as in outdoor scenes. To this end, we first propose to exploit semantic information to better constrain the super-resolution process. In particular, we design a co-sparse analysis model that learns filters from joint intensity, depth and semantic information. Furthermore, we show how low-resolution training depth maps can be employed in our learning strategy. We demonstrate the benefits of our approach over state-of-the-art depth super-resolution methods on two outdoor scene datasets.



### Attention Correctness in Neural Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1605.09553v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1605.09553v2)
- **Published**: 2016-05-31 10:04:20+00:00
- **Updated**: 2016-11-23 07:29:46+00:00
- **Authors**: Chenxi Liu, Junhua Mao, Fei Sha, Alan Yuille
- **Comment**: To appear in AAAI-17. See http://www.cs.jhu.edu/~cxliu/ for
  supplementary material
- **Journal**: None
- **Summary**: Attention mechanisms have recently been introduced in deep learning for various tasks in natural language processing and computer vision. But despite their popularity, the "correctness" of the implicitly-learned attention maps has only been assessed qualitatively by visualization of several examples. In this paper we focus on evaluating and improving the correctness of attention in neural image captioning models. Specifically, we propose a quantitative evaluation metric for the consistency between the generated attention maps and human annotations, using recently released datasets with alignment between regions in images and entities in captions. We then propose novel models with different levels of explicit supervision for learning attention maps during training. The supervision can be strong when alignment between regions and caption entities are available, or weak when only object segments and categories are provided. We show on the popular Flickr30k and COCO datasets that introducing supervision of attention maps during training solidly improves both attention correctness and caption quality, showing the promise of making machine perception more human-like.



### Modeling Photographic Composition via Triangles
- **Arxiv ID**: http://arxiv.org/abs/1605.09559v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.09559v1)
- **Published**: 2016-05-31 10:22:38+00:00
- **Updated**: 2016-05-31 10:22:38+00:00
- **Authors**: Zihan Zhou, Siqiong He, Jia Li, James Z. Wang
- **Comment**: 22 pages, 25 figures. The first two authors contributed equally
- **Journal**: None
- **Summary**: The capacity of automatically modeling photographic composition is valuable for many real-world machine vision applications such as digital photography, image retrieval, image understanding, and image aesthetics assessment. The triangle technique is among those indispensable composition methods on which professional photographers often rely. This paper proposes a system that can identify prominent triangle arrangements in two major categories of photographs: natural or urban scenes, and portraits. For the natural or urban scene pictures, the focus is on the effect of linear perspective. For portraits, we carefully examine the positioning of human subjects in a photo. We show that line analysis is highly advantageous for modeling composition in both categories. Based on the detected triangles, new mathematical descriptors for composition are formulated and used to retrieve similar images. Leveraging the rich source of high aesthetics photos online, similar approaches can potentially be incorporated in future smart cameras to enhance a person's photo composition skills.



### Model-driven Simulations for Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1605.09582v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.09582v1)
- **Published**: 2016-05-31 11:13:20+00:00
- **Updated**: 2016-05-31 11:13:20+00:00
- **Authors**: V S R Veeravasarapu, Constantin Rothkopf, Visvanathan Ramesh
- **Comment**: None
- **Journal**: None
- **Summary**: The use of simulated virtual environments to train deep convolutional neural networks (CNN) is a currently active practice to reduce the (real)data-hungriness of the deep CNN models, especially in application domains in which large scale real data and/or groundtruth acquisition is difficult or laborious. Recent approaches have attempted to harness the capabilities of existing video games, animated movies to provide training data with high precision groundtruth. However, a stumbling block is in how one can certify generalization of the learned models and their usefulness in real world data sets. This opens up fundamental questions such as: What is the role of photorealism of graphics simulations in training CNN models? Are the trained models valid in reality? What are possible ways to reduce the performance bias? In this work, we begin to address theses issues systematically in the context of urban semantic understanding with CNNs. Towards this end, we (a) propose a simple probabilistic urban scene model, (b) develop a parametric rendering tool to synthesize the data with groundtruth, followed by (c) a systematic exploration of the impact of level-of-realism on the generality of the trained CNN model to real world; and domain adaptation concepts to minimize the performance bias.



### A Sparse Representation of Complete Local Binary Pattern Histogram for Human Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1605.09584v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.09584v1)
- **Published**: 2016-05-31 11:19:01+00:00
- **Updated**: 2016-05-31 11:19:01+00:00
- **Authors**: Mawloud Guermoui, Mohamed L. Mekhalfi
- **Comment**: Accepted (but unattended) in IEEE-EMBS International Conferences on
  Biomedical and Health Informatics (BHI)
- **Journal**: None
- **Summary**: Human face recognition has been a long standing problem in computer vision and pattern recognition. Facial analysis can be viewed as a two-fold problem, namely (i) facial representation, and (ii) classification. So far, many face representations have been proposed, a well-known method is the Local Binary Pattern (LBP), which has witnessed a growing interest. In this respect, we treat in this paper the issues of face representation as well as classification in a novel manner. On the one hand, we use a variant to LBP, so-called Complete Local Binary Pattern (CLBP), which differs from the basic LBP by coding a given local region using a given central pixel and Sing_ Magnitude difference. Subsequently, most of LBPbased descriptors use a fixed grid to code a given facial image, which technique is, in most cases, not robust to pose variation and misalignment. To cope with such issue, a representative Multi-Resolution Histogram (MH) decomposition is adopted in our work. On the other hand, having the histograms of the considered images extracted, we exploit their sparsity to construct a so-called Sparse Representation Classifier (SRC) for further face classification. Experimental results have been conducted on ORL face database, and pointed out the superiority of our scheme over other popular state-of-the-art techniques.



### The use of deep learning in image segmentation, classification and detection
- **Arxiv ID**: http://arxiv.org/abs/1605.09612v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.09612v1)
- **Published**: 2016-05-31 13:09:40+00:00
- **Updated**: 2016-05-31 13:09:40+00:00
- **Authors**: M. S. Badea, I. I. Felea, L. M. Florea, C. Vertan
- **Comment**: None
- **Journal**: None
- **Summary**: Recent years have shown that deep learned neural networks are a valuable tool in the field of computer vision. This paper addresses the use of two different kinds of network architectures, namely LeNet and Network in Network (NiN). They will be compared in terms of both performance and computational efficiency by addressing the classification and detection problems. In this paper, multiple databases will be used to test the networks. One of them contains images depicting burn wounds from pediatric cases, another one contains an extensive number of art images and other facial databases were used for facial keypoints detection.



### A Systematic Evaluation and Benchmark for Person Re-Identification: Features, Metrics, and Datasets
- **Arxiv ID**: http://arxiv.org/abs/1605.09653v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.09653v5)
- **Published**: 2016-05-31 15:01:46+00:00
- **Updated**: 2018-02-14 16:27:31+00:00
- **Authors**: Srikrishna Karanam, Mengran Gou, Ziyan Wu, Angels Rates-Borras, Octavia Camps, Richard J. Radke
- **Comment**: Preliminary work on person Re-Id benchmark. S. Karanam and M. Gou
  contributed equally. 14 pages, 6 figures, 4 tables. For supplementary
  material, see
  http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/supmat/ReID_benchmark_supp.zip
- **Journal**: None
- **Summary**: Person re-identification (re-id) is a critical problem in video analytics applications such as security and surveillance. The public release of several datasets and code for vision algorithms has facilitated rapid progress in this area over the last few years. However, directly comparing re-id algorithms reported in the literature has become difficult since a wide variety of features, experimental protocols, and evaluation metrics are employed. In order to address this need, we present an extensive review and performance evaluation of single- and multi-shot re-id algorithms. The experimental protocol incorporates the most recent advances in both feature extraction and metric learning. To ensure a fair comparison, all of the approaches were implemented using a unified code library that includes 11 feature extraction algorithms and 22 metric learning and ranking techniques. All approaches were evaluated using a new large-scale dataset that closely mimics a real-world problem setting, in addition to 16 other publicly available datasets: VIPeR, GRID, CAVIAR, DukeMTMC4ReID, 3DPeS, PRID, V47, WARD, SAIVT-SoftBio, CUHK01, CHUK02, CUHK03, RAiD, iLIDSVID, HDA+ and Market1501. The evaluation codebase and results will be made publicly available for community use.



### Dynamic Filter Networks
- **Arxiv ID**: http://arxiv.org/abs/1605.09673v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1605.09673v2)
- **Published**: 2016-05-31 15:29:36+00:00
- **Updated**: 2016-06-06 15:39:10+00:00
- **Authors**: Bert De Brabandere, Xu Jia, Tinne Tuytelaars, Luc Van Gool
- **Comment**: submitted to NIPS16; X. Jia and B. De Brabandere contributed equally
  to this work and are listed in alphabetical order
- **Journal**: None
- **Summary**: In a traditional convolutional layer, the learned filters stay fixed after training. In contrast, we introduce a new framework, the Dynamic Filter Network, where filters are generated dynamically conditioned on an input. We show that this architecture is a powerful one, with increased flexibility thanks to its adaptive nature, yet without an excessive increase in the number of model parameters. A wide variety of filtering operations can be learned this way, including local spatial transformations, but also others like selective (de)blurring or adaptive feature extraction. Moreover, multiple such layers can be combined, e.g. in a recurrent architecture. We demonstrate the effectiveness of the dynamic filter network on the tasks of video and stereo prediction, and reach state-of-the-art performance on the moving MNIST dataset with a much smaller model. By visualizing the learned filters, we illustrate that the network has picked up flow information by only looking at unlabelled training data. This suggests that the network can be used to pretrain networks for various supervised tasks in an unsupervised way, like optical flow and depth estimation.



### Generalized Multi-view Embedding for Visual Recognition and Cross-modal Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1605.09696v3
- **DOI**: 10.1109/TCYB.2017.2742705
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1605.09696v3)
- **Published**: 2016-05-31 16:11:16+00:00
- **Updated**: 2017-08-31 09:17:50+00:00
- **Authors**: Guanqun Cao, Alexandros Iosifidis, Ke Chen, Moncef Gabbouj
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, the problem of multi-view embedding from different visual cues and modalities is considered. We propose a unified solution for subspace learning methods using the Rayleigh quotient, which is extensible for multiple views, supervised learning, and non-linear embeddings. Numerous methods including Canonical Correlation Analysis, Partial Least Sqaure regression and Linear Discriminant Analysis are studied using specific intrinsic and penalty graphs within the same framework. Non-linear extensions based on kernels and (deep) neural networks are derived, achieving better performance than the linear ones. Moreover, a novel Multi-view Modular Discriminant Analysis (MvMDA) is proposed by taking the view difference into consideration. We demonstrate the effectiveness of the proposed multi-view embedding methods on visual object recognition and cross-modal image retrieval, and obtain superior results in both applications compared to related methods.



### Towards ontology driven learning of visual concept detectors
- **Arxiv ID**: http://arxiv.org/abs/1605.09757v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1605.09757v1)
- **Published**: 2016-05-31 18:35:44+00:00
- **Updated**: 2016-05-31 18:35:44+00:00
- **Authors**: Sanchit Arora, Chuck Cho, Paul Fitzpatrick, Francois Scharffe
- **Comment**: unpublished
- **Journal**: None
- **Summary**: The maturity of deep learning techniques has led in recent years to a breakthrough in object recognition in visual media. While for some specific benchmarks, neural techniques seem to match if not outperform human judgement, challenges are still open for detecting arbitrary concepts in arbitrary videos. In this paper, we propose a system that combines neural techniques, a large scale visual concepts ontology, and an active learning loop, to provide on the fly model learning of arbitrary concepts. We give an overview of the system as a whole, and focus on the central role of the ontology for guiding and bootstrapping the learning of new concepts, improving the recall of concept detection, and, on the user end, providing semantic search on a library of annotated videos.



### Fast Zero-Shot Image Tagging
- **Arxiv ID**: http://arxiv.org/abs/1605.09759v1
- **DOI**: 10.1109/CVPR.2016.644
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.09759v1)
- **Published**: 2016-05-31 18:39:48+00:00
- **Updated**: 2016-05-31 18:39:48+00:00
- **Authors**: Yang Zhang, Boqing Gong, Mubarak Shah
- **Comment**: 9 pages (not including reference). This paper has been accepted by
  CVPR 2016
- **Journal**: CVPR 2016
- **Summary**: The well-known word analogy experiments show that the recent word vectors capture fine-grained linguistic regularities in words by linear vector offsets, but it is unclear how well the simple vector offsets can encode visual regularities over words. We study a particular image-word relevance relation in this paper. Our results show that the word vectors of relevant tags for a given image rank ahead of the irrelevant tags, along a principal direction in the word vector space. Inspired by this observation, we propose to solve image tagging by estimating the principal direction for an image. Particularly, we exploit linear mappings and nonlinear deep neural networks to approximate the principal direction from an input image. We arrive at a quite versatile tagging model. It runs fast given a test image, in constant time w.r.t.\ the training set size. It not only gives superior performance for the conventional tagging task on the NUS-WIDE dataset, but also outperforms competitive baselines on annotating images with previously unseen tags



### Adversarial Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/1605.09782v7
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1605.09782v7)
- **Published**: 2016-05-31 19:37:29+00:00
- **Updated**: 2017-04-03 20:34:36+00:00
- **Authors**: Jeff Donahue, Philipp Krähenbühl, Trevor Darrell
- **Comment**: Published as a conference paper at ICLR 2017. Changelog: (v7) Table 2
  results improved 1-2% due to averaging predictions over 10 crops at test
  time, as done in Noroozi & Favaro; Table 3 VOC classification results
  slightly improved due to minor bugfix. (See v6 changelog for previous
  versions.)
- **Journal**: None
- **Summary**: The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.



### Texture Synthesis Using Shallow Convolutional Networks with Random Filters
- **Arxiv ID**: http://arxiv.org/abs/1606.00021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.00021v1)
- **Published**: 2016-05-31 20:03:13+00:00
- **Updated**: 2016-05-31 20:03:13+00:00
- **Authors**: Ivan Ustyuzhaninov, Wieland Brendel, Leon A. Gatys, Matthias Bethge
- **Comment**: 9 pages, 4 figures
- **Journal**: None
- **Summary**: Here we demonstrate that the feature space of random shallow convolutional neural networks (CNNs) can serve as a surprisingly good model of natural textures. Patches from the same texture are consistently classified as being more similar then patches from different textures. Samples synthesized from the model capture spatial correlations on scales much larger then the receptive field size, and sometimes even rival or surpass the perceptual quality of state of the art texture models (but show less variability). The current state of the art in parametric texture synthesis relies on the multi-layer feature space of deep CNNs that were trained on natural images. Our finding suggests that such optimized multi-layer feature spaces are not imperative for texture modeling. Instead, much simpler shallow and convolutional networks can serve as the basis for novel texture synthesis algorithms.



### Hierarchical Question-Image Co-Attention for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1606.00061v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1606.00061v5)
- **Published**: 2016-05-31 22:02:01+00:00
- **Updated**: 2017-01-19 05:03:33+00:00
- **Authors**: Jiasen Lu, Jianwei Yang, Dhruv Batra, Devi Parikh
- **Comment**: 11 pages, 7 figures, 3 tables in 2016 Conference on Neural
  Information Processing Systems (NIPS)
- **Journal**: None
- **Summary**: A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling "where to look" or visual attention, it is equally important to model "what words to listen to" or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN). Our model improves the state-of-the-art on the VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA dataset. By using ResNet, the performance is further improved to 62.1% for VQA and 65.4% for COCO-QA.



