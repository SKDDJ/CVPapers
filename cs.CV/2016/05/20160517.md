# Arxiv Papers in cs.CV on 2016-05-17
### SemiContour: A Semi-supervised Learning Approach for Contour Detection
- **Arxiv ID**: http://arxiv.org/abs/1605.04996v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.04996v1)
- **Published**: 2016-05-17 01:33:20+00:00
- **Updated**: 2016-05-17 01:33:20+00:00
- **Authors**: Zizhao Zhang, Fuyong Xing, Xiaoshuang Shi, Lin Yang
- **Comment**: Accepted by Computer Vision and Pattern Recognition (CVPR) 2016
- **Journal**: None
- **Summary**: Supervised contour detection methods usually require many labeled training images to obtain satisfactory performance. However, a large set of annotated data might be unavailable or extremely labor intensive. In this paper, we investigate the usage of semi-supervised learning (SSL) to obtain competitive detection accuracy with very limited training data (three labeled images). Specifically, we propose a semi-supervised structured ensemble learning approach for contour detection built on structured random forests (SRF). To allow SRF to be applicable to unlabeled data, we present an effective sparse representation approach to capture inherent structure in image patches by finding a compact and discriminative low-dimensional subspace representation in an unsupervised manner, enabling the incorporation of abundant unlabeled patches with their estimated structured labels to help SRF perform better node splitting. We re-examine the role of sparsity and propose a novel and fast sparse coding algorithm to boost the overall learning efficiency. To the best of our knowledge, this is the first attempt to apply SSL for contour detection. Extensive experiments on the BSDS500 segmentation dataset and the NYU Depth dataset demonstrate the superiority of the proposed method.



### Image stitching with perspective-preserving warping
- **Arxiv ID**: http://arxiv.org/abs/1605.05019v1
- **DOI**: 10.5194/isprs-annals-III-3-287-2016
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.05019v1)
- **Published**: 2016-05-17 05:07:51+00:00
- **Updated**: 2016-05-17 05:07:51+00:00
- **Authors**: Tianzhu Xiang, Gui-Song Xia, Liangpei Zhang
- **Comment**: ISPRS 2016 - XXIII ISPRS Congress: Prague, Czech Republic, 2016
- **Journal**: None
- **Summary**: Image stitching algorithms often adopt the global transformation, such as homography, and work well for planar scenes or parallax free camera motions. However, these conditions are easily violated in practice. With casual camera motions, variable taken views, large depth change, or complex structures, it is a challenging task for stitching these images. The global transformation model often provides dreadful stitching results, such as misalignments or projective distortions, especially perspective distortion. To this end, we suggest a perspective-preserving warping for image stitching, which spatially combines local projective transformations and similarity transformation. By weighted combination scheme, our approach gradually extrapolates the local projective transformations of the overlapping regions into the non-overlapping regions, and thus the final warping can smoothly change from projective to similarity. The proposed method can provide satisfactory alignment accuracy as well as reduce the projective distortions and maintain the multi-perspective view. Experiments on a variety of challenging images confirm the efficiency of the approach.



### LIME: A Method for Low-light IMage Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1605.05034v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.05034v3)
- **Published**: 2016-05-17 06:50:55+00:00
- **Updated**: 2016-07-24 07:43:28+00:00
- **Authors**: Xiaojie Guo
- **Comment**: None
- **Journal**: None
- **Summary**: When one captures images in low-light conditions, the images often suffer from low visibility. This poor quality may significantly degrade the performance of many computer vision and multimedia algorithms that are primarily designed for high-quality inputs. In this paper, we propose a very simple and effective method, named as LIME, to enhance low-light images. More concretely, the illumination of each pixel is first estimated individually by finding the maximum value in R, G and B channels. Further, we refine the initial illumination map by imposing a structure prior on it, as the final illumination map. Having the well-constructed illumination map, the enhancement can be achieved accordingly. Experiments on a number of challenging real-world low-light images are present to reveal the efficacy of our LIME and show its superiority over several state-of-the-arts.



### Incremental Robot Learning of New Objects with Fixed Update Time
- **Arxiv ID**: http://arxiv.org/abs/1605.05045v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1605.05045v3)
- **Published**: 2016-05-17 07:50:58+00:00
- **Updated**: 2017-02-28 16:53:19+00:00
- **Authors**: Raffaello Camoriano, Giulia Pasquale, Carlo Ciliberto, Lorenzo Natale, Lorenzo Rosasco, Giorgio Metta
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: We consider object recognition in the context of lifelong learning, where a robotic agent learns to discriminate between a growing number of object classes as it accumulates experience about the environment. We propose an incremental variant of the Regularized Least Squares for Classification (RLSC) algorithm, and exploit its structure to seamlessly add new classes to the learned model. The presented algorithm addresses the problem of having an unbalanced proportion of training examples per class, which occurs when new objects are presented to the system for the first time.   We evaluate our algorithm on both a machine learning benchmark dataset and two challenging object recognition tasks in a robotic setting. Empirical evidence shows that our approach achieves comparable or higher classification performance than its batch counterpart when classes are unbalanced, while being significantly faster.



### HARRISON: A Benchmark on HAshtag Recommendation for Real-world Images in Social Networks
- **Arxiv ID**: http://arxiv.org/abs/1605.05054v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1605.05054v1)
- **Published**: 2016-05-17 08:21:07+00:00
- **Updated**: 2016-05-17 08:21:07+00:00
- **Authors**: Minseok Park, Hanxiang Li, Junmo Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Simple, short, and compact hashtags cover a wide range of information on social networks. Although many works in the field of natural language processing (NLP) have demonstrated the importance of hashtag recommendation, hashtag recommendation for images has barely been studied. In this paper, we introduce the HARRISON dataset, a benchmark on hashtag recommendation for real world images in social networks. The HARRISON dataset is a realistic dataset, composed of 57,383 photos from Instagram and an average of 4.5 associated hashtags for each photo. To evaluate our dataset, we design a baseline framework consisting of visual feature extractor based on convolutional neural network (CNN) and multi-label classifier based on neural network. Based on this framework, two single feature-based models, object-based and scene-based model, and an integrated model of them are evaluated on the HARRISON dataset. Our dataset shows that hashtag recommendation task requires a wide and contextual understanding of the situation conveyed in the image. As far as we know, this work is the first vision-only attempt at hashtag recommendation for real world images in social networks. We expect this benchmark to accelerate the advancement of hashtag recommendation.



### Detecting Violent and Abnormal Crowd activity using Temporal Analysis of Grey Level Co-occurrence Matrix (GLCM) Based Texture Measures
- **Arxiv ID**: http://arxiv.org/abs/1605.05106v2
- **DOI**: 10.1007/s00138-017-0830-x
- **Categories**: **cs.CV**, I.2.10; I.4.7; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1605.05106v2)
- **Published**: 2016-05-17 10:53:07+00:00
- **Updated**: 2017-04-03 10:39:02+00:00
- **Authors**: Kaelon Lloyd, David Marshall, Simon C. Moore, Paul L. Rosin
- **Comment**: Published under open access, 9 pages, 12 Figures
- **Journal**: Machine Vision and Applications (2017)
- **Summary**: The severity of sustained injury resulting from assault-related violence can be minimised by reducing detection time. However, it has been shown that human operators perform poorly at detecting events found in video footage when presented with simultaneous feeds. We utilise computer vision techniques to develop an automated method of abnormal crowd detection that can aid a human operator in the detection of violent behaviour. We observed that behaviour in city centre environments often occur in crowded areas, resulting in individual actions being occluded by other crowd members. We propose a real-time descriptor that models crowd dynamics by encoding changes in crowd texture using temporal summaries of Grey Level Co-Occurrence Matrix (GLCM) features. We introduce a measure of inter-frame uniformity (IFU) and demonstrate that the appearance of violent behaviour changes in a less uniform manner when compared to other types of crowd behaviour. Our proposed method is computationally cheap and offers real-time description. Evaluating our method using a privately held CCTV dataset and the publicly available Violent Flows, UCF Web Abnormality, and UMN Abnormal Crowd datasets, we report a receiver operating characteristic score of 0.9782, 0.9403, 0.8218 and 0.9956 respectively.



### Monocular Urban Localization using Street View
- **Arxiv ID**: http://arxiv.org/abs/1605.05157v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1605.05157v2)
- **Published**: 2016-05-17 13:33:25+00:00
- **Updated**: 2016-06-16 11:47:33+00:00
- **Authors**: Li Yu, Cyril Joly, Guillaume Bresson, Fabien Moutarde
- **Comment**: 6 pages, 6 figures, submitted to ICARCV2016
- **Journal**: None
- **Summary**: This paper presents a metric global localization in the urban environment only with a monocular camera and the Google Street View database. We fully leverage the abundant sources from the Street View and benefits from its topo-metric structure to build a coarse-to-fine positioning, namely a topological place recognition process and then a metric pose estimation by local bundle adjustment. Our method is tested on a 3 km urban environment and demonstrates both sub-meter accuracy and robustness to viewpoint changes, illumination and occlusion. To our knowledge, this is the first work that studies the global urban localization simply with a single camera and Street View.



### Structured Prediction of 3D Human Pose with Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1605.05180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.05180v1)
- **Published**: 2016-05-17 14:26:14+00:00
- **Updated**: 2016-05-17 14:26:14+00:00
- **Authors**: Bugra Tekin, Isinsu Katircioglu, Mathieu Salzmann, Vincent Lepetit, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: Most recent approaches to monocular 3D pose estimation rely on Deep Learning. They either train a Convolutional Neural Network to directly regress from image to 3D pose, which ignores the dependencies between human joints, or model these dependencies via a max-margin structured learning framework, which involves a high computational cost at inference time.   In this paper, we introduce a Deep Learning regression architecture for structured prediction of 3D human pose from monocular images that relies on an overcomplete auto-encoder to learn a high-dimensional latent pose representation and account for joint dependencies. We demonstrate that our approach outperforms state-of-the-art ones both in terms of structure preservation and prediction accuracy.



### Human Action Localization with Sparse Spatial Supervision
- **Arxiv ID**: http://arxiv.org/abs/1605.05197v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.05197v2)
- **Published**: 2016-05-17 14:55:03+00:00
- **Updated**: 2017-05-23 19:19:23+00:00
- **Authors**: Philippe Weinzaepfel, Xavier Martin, Cordelia Schmid
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce an approach for spatio-temporal human action localization using sparse spatial supervision. Our method leverages the large amount of annotated humans available today and extracts human tubes by combining a state-of-the-art human detector with a tracking-by-detection approach. Given these high-quality human tubes and temporal supervision, we select positive and negative tubes with very sparse spatial supervision, i.e., only one spatially annotated frame per instance. The selected tubes allow us to effectively learn a spatio-temporal action detector based on dense trajectories or CNNs. We conduct experiments on existing action localization benchmarks: UCF-Sports, J-HMDB and UCF-101. Our results show that our approach, despite using sparse spatial supervision, performs on par with methods using full supervision, i.e., one bounding box annotation per frame. To further validate our method, we introduce DALY (Daily Action Localization in YouTube), a dataset for realistic action localization in space and time. It contains high quality temporal and spatial annotations for 3.6k instances of 10 actions in 31 hours of videos (3.3M frames). It is an order of magnitude larger than existing datasets, with more diversity in appearance and long untrimmed videos.



### Multimodal Sparse Coding for Event Detection
- **Arxiv ID**: http://arxiv.org/abs/1605.05212v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1605.05212v1)
- **Published**: 2016-05-17 15:37:19+00:00
- **Updated**: 2016-05-17 15:37:19+00:00
- **Authors**: Youngjune Gwon, William Campbell, Kevin Brady, Douglas Sturim, Miriam Cha, H. T. Kung
- **Comment**: Multimodal Machine Learning Workshop at NIPS 2015
- **Journal**: None
- **Summary**: Unsupervised feature learning methods have proven effective for classification tasks based on a single modality. We present multimodal sparse coding for learning feature representations shared across multiple modalities. The shared representations are applied to multimedia event detection (MED) and evaluated in comparison to unimodal counterparts, as well as other feature learning methods such as GMM supervectors and sparse RBM. We report the cross-validated classification accuracy and mean average precision of the MED system trained on features learned from our unimodal and multimodal settings for a subset of the TRECVID MED 2014 dataset.



### Real-time Eye Gaze Direction Classification Using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1605.05258v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.05258v1)
- **Published**: 2016-05-17 17:33:18+00:00
- **Updated**: 2016-05-17 17:33:18+00:00
- **Authors**: Anjith George, Aurobinda Routray
- **Comment**: 5 pages, To appear in IEEE International Conference on Signal
  Processing and Communication, SPCOM 2016
- **Journal**: None
- **Summary**: Estimation eye gaze direction is useful in various human-computer interaction tasks. Knowledge of gaze direction can give valuable information regarding users point of attention. Certain patterns of eye movements known as eye accessing cues are reported to be related to the cognitive processes in the human brain. We propose a real-time framework for the classification of eye gaze direction and estimation of eye accessing cues. In the first stage, the algorithm detects faces using a modified version of the Viola-Jones algorithm. A rough eye region is obtained using geometric relations and facial landmarks. The eye region obtained is used in the subsequent stage to classify the eye gaze direction. A convolutional neural network is employed in this work for the classification of eye gaze direction. The proposed algorithm was tested on Eye Chimera database and found to outperform state of the art methods. The computational complexity of the algorithm is very less in the testing phase. The algorithm achieved an average frame rate of 24 fps in the desktop environment.



### Fast and Accurate Algorithm for Eye Localization for Gaze Tracking in Low Resolution Images
- **Arxiv ID**: http://arxiv.org/abs/1605.05272v1
- **DOI**: 10.1049/iet-cvi.2015.0316
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.05272v1)
- **Published**: 2016-05-17 18:10:43+00:00
- **Updated**: 2016-05-17 18:10:43+00:00
- **Authors**: Anjith George, Aurobinda Routray
- **Comment**: 12 pages, 10 figures, IET Computer Vision, 2016
- **Journal**: None
- **Summary**: Iris centre localization in low-resolution visible images is a challenging problem in computer vision community due to noise, shadows, occlusions, pose variations, eye blinks, etc. This paper proposes an efficient method for determining iris centre in low-resolution images in the visible spectrum. Even low-cost consumer-grade webcams can be used for gaze tracking without any additional hardware. A two-stage algorithm is proposed for iris centre localization. The proposed method uses geometrical characteristics of the eye. In the first stage, a fast convolution based approach is used for obtaining the coarse location of iris centre (IC). The IC location is further refined in the second stage using boundary tracing and ellipse fitting. The algorithm has been evaluated in public databases like BioID, Gi4E and is found to outperform the state of the art methods.



### Option Discovery in Hierarchical Reinforcement Learning using Spatio-Temporal Clustering
- **Arxiv ID**: http://arxiv.org/abs/1605.05359v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1605.05359v3)
- **Published**: 2016-05-17 20:44:19+00:00
- **Updated**: 2020-09-21 22:18:31+00:00
- **Authors**: Aravind Srinivas, Ramnandan Krishnamurthy, Peeyush Kumar, Balaraman Ravindran
- **Comment**: Revised version of ICML 16 Abstraction in Reinforcement Learning
  workshop paper
- **Journal**: None
- **Summary**: This paper introduces an automated skill acquisition framework in reinforcement learning which involves identifying a hierarchical description of the given task in terms of abstract states and extended actions between abstract states. Identifying such structures present in the task provides ways to simplify and speed up reinforcement learning algorithms. These structures also help to generalize such algorithms over multiple tasks without relearning policies from scratch. We use ideas from dynamical systems to find metastable regions in the state space and associate them with abstract states. The spectral clustering algorithm PCCA+ is used to identify suitable abstractions aligned to the underlying structure. Skills are defined in terms of the sequence of actions that lead to transitions between such abstract states. The connectivity information from PCCA+ is used to generate these skills or options. These skills are independent of the learning task and can be efficiently reused across a variety of tasks defined over the same model. This approach works well even without the exact model of the environment by using sample trajectories to construct an approximate estimate. We also present our approach to scaling the skill acquisition framework to complex tasks with large state spaces for which we perform state aggregation using the representation learned from an action conditional video prediction network and use the skill acquisition framework on the aggregated state space.



### Deep Action Sequence Learning for Causal Shape Transformation
- **Arxiv ID**: http://arxiv.org/abs/1605.05368v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1605.05368v3)
- **Published**: 2016-05-17 21:07:18+00:00
- **Updated**: 2016-11-08 20:48:47+00:00
- **Authors**: Kin Gwn Lore, Daniel Stoecklein, Michael Davies, Baskar Ganapathysubramanian, Soumik Sarkar
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning became the method of choice in recent year for solving a wide variety of predictive analytics tasks. For sequence prediction, recurrent neural networks (RNN) are often the go-to architecture for exploiting sequential information where the output is dependent on previous computation. However, the dependencies of the computation lie in the latent domain which may not be suitable for certain applications involving the prediction of a step-wise transformation sequence that is dependent on the previous computation only in the visible domain. We propose that a hybrid architecture of convolution neural networks (CNN) and stacked autoencoders (SAE) is sufficient to learn a sequence of actions that nonlinearly transforms an input shape or distribution into a target shape or distribution with the same support. While such a framework can be useful in a variety of problems such as robotic path planning, sequential decision-making in games, and identifying material processing pathways to achieve desired microstructures, the application of the framework is exemplified by the control of fluid deformations in a microfluidic channel by deliberately placing a sequence of pillars. Learning of a multistep topological transform has significant implications for rapid advances in material science and biomedical applications.



### Learning Deep Representations of Fine-grained Visual Descriptions
- **Arxiv ID**: http://arxiv.org/abs/1605.05395v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.05395v1)
- **Published**: 2016-05-17 23:08:46+00:00
- **Updated**: 2016-05-17 23:08:46+00:00
- **Authors**: Scott Reed, Zeynep Akata, Bernt Schiele, Honglak Lee
- **Comment**: CVPR 2016
- **Journal**: None
- **Summary**: State-of-the-art methods for zero-shot visual recognition formulate learning as a joint embedding problem of images and side information. In these formulations the current best complement to visual features are attributes: manually encoded vectors describing shared characteristics among categories. Despite good performance, attributes have limitations: (1) finer-grained recognition requires commensurately more attributes, and (2) attributes do not provide a natural language interface. We propose to overcome these limitations by training neural language models from scratch; i.e. without pre-training and only consuming words and characters. Our proposed models train end-to-end to align with the fine-grained and category-specific content of images. Natural language provides a flexible and compact way of encoding only the salient visual aspects for distinguishing categories. By training on raw text, our model can do inference on raw text as well, providing humans a familiar mode both for annotation and retrieval. Our model achieves strong performance on zero-shot text-based image retrieval and significantly outperforms the attribute-based state-of-the-art for zero-shot classification on the Caltech UCSD Birds 200-2011 dataset.



### Generative Adversarial Text to Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1605.05396v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1605.05396v2)
- **Published**: 2016-05-17 23:09:15+00:00
- **Updated**: 2016-06-05 13:39:27+00:00
- **Authors**: Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, Honglak Lee
- **Comment**: ICML 2016
- **Journal**: None
- **Summary**: Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image model- ing, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.



