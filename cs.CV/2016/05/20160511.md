# Arxiv Papers in cs.CV on 2016-05-11
### Deep Attributes Driven Multi-Camera Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1605.03259v2
- **DOI**: 10.1007/978-3-319-46475-6_30
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.03259v2)
- **Published**: 2016-05-11 02:05:22+00:00
- **Updated**: 2016-08-09 05:58:03+00:00
- **Authors**: Chi Su, Shiliang Zhang, Junliang Xing, Wen Gao, Qi Tian
- **Comment**: Person Re-identification; 17 pages; 5 figures; In IEEE ECCV 2016
- **Journal**: None
- **Summary**: The visual appearance of a person is easily affected by many factors like pose variations, viewpoint changes and camera parameter differences. This makes person Re-Identification (ReID) among multiple cameras a very challenging task. This work is motivated to learn mid-level human attributes which are robust to such visual appearance variations. And we propose a semi-supervised attribute learning framework which progressively boosts the accuracy of attributes only using a limited number of labeled data. Specifically, this framework involves a three-stage training. A deep Convolutional Neural Network (dCNN) is first trained on an independent dataset labeled with attributes. Then it is fine-tuned on another dataset only labeled with person IDs using our defined triplet loss. Finally, the updated dCNN predicts attribute labels for the target dataset, which is combined with the independent dataset for the final round of fine-tuning. The predicted attributes, namely \emph{deep attributes} exhibit superior generalization ability across different datasets. By directly using the deep attributes with simple Cosine distance, we have obtained surprisingly good accuracy on four person ReID datasets. Experiments also show that a simple metric learning modular further boosts our method, making it significantly outperform many recent works.



### Unsupervised Semantic Action Discovery from Video Collections
- **Arxiv ID**: http://arxiv.org/abs/1605.03324v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1605.03324v1)
- **Published**: 2016-05-11 08:22:06+00:00
- **Updated**: 2016-05-11 08:22:06+00:00
- **Authors**: Ozan Sener, Amir Roshan Zamir, Chenxia Wu, Silvio Savarese, Ashutosh Saxena
- **Comment**: First version of this paper arXiv:1506.08438 appeared in ICCV 2015.
  This extended version has more details on the learning algorithm and
  hierarchical clustering with full derivation, additional analysis on the
  robustness to the subtitle noise, and a novel application on robotics
- **Journal**: None
- **Summary**: Human communication takes many forms, including speech, text and instructional videos. It typically has an underlying structure, with a starting point, ending, and certain objective steps between them. In this paper, we consider instructional videos where there are tens of millions of them on the Internet.   We propose a method for parsing a video into such semantic steps in an unsupervised way. Our method is capable of providing a semantic "storyline" of the video composed of its objective steps. We accomplish this using both visual and language cues in a joint generative model. Our method can also provide a textual description for each of the identified semantic steps and video segments. We evaluate our method on a large number of complex YouTube videos and show that our method discovers semantically correct instructions for a variety of tasks.



### A robust particle detection algorithm based on symmetry
- **Arxiv ID**: http://arxiv.org/abs/1605.03328v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.03328v1)
- **Published**: 2016-05-11 08:38:32+00:00
- **Updated**: 2016-05-11 08:38:32+00:00
- **Authors**: Alvaro Rodriguez, Hanqing Zhang, Krister Wiklund, Tomas Brodin, Jonatan Klaminder, Patrik Andersson, Magnus Andersson
- **Comment**: Manuscript including supplementary materials
- **Journal**: None
- **Summary**: Particle tracking is common in many biophysical, ecological, and micro-fluidic applications. Reliable tracking information is heavily dependent on of the system under study and algorithms that correctly determines particle position between images. However, in a real environmental context with the presence of noise including particular or dissolved matter in water, and low and fluctuating light conditions, many algorithms fail to obtain reliable information. We propose a new algorithm, the Circular Symmetry algorithm (C-Sym), for detecting the position of a circular particle with high accuracy and precision in noisy conditions. The algorithm takes advantage of the spatial symmetry of the particle allowing for subpixel accuracy. We compare the proposed algorithm with four different methods using both synthetic and experimental datasets. The results show that C-Sym is the most accurate and precise algorithm when tracking micro-particles in all tested conditions and it has the potential for use in applications including tracking biota in their environment.



### Go-ICP: A Globally Optimal Solution to 3D ICP Point-Set Registration
- **Arxiv ID**: http://arxiv.org/abs/1605.03344v1
- **DOI**: 10.1109/TPAMI.2015.2513405
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.03344v1)
- **Published**: 2016-05-11 09:15:11+00:00
- **Updated**: 2016-05-11 09:15:11+00:00
- **Authors**: Jiaolong Yang, Hongdong Li, Dylan Campbell, Yunde Jia
- **Comment**: to appear in T-PAMI 2016 (IEEE Transactions on Pattern Analysis and
  Machine Intelligence)
- **Journal**: None
- **Summary**: The Iterative Closest Point (ICP) algorithm is one of the most widely used methods for point-set registration. However, being based on local iterative optimization, ICP is known to be susceptible to local minima. Its performance critically relies on the quality of the initialization and only local optimality is guaranteed. This paper presents the first globally optimal algorithm, named Go-ICP, for Euclidean (rigid) registration of two 3D point-sets under the L2 error metric defined in ICP. The Go-ICP method is based on a branch-and-bound (BnB) scheme that searches the entire 3D motion space SE(3). By exploiting the special structure of SE(3) geometry, we derive novel upper and lower bounds for the registration error function. Local ICP is integrated into the BnB scheme, which speeds up the new method while guaranteeing global optimality. We also discuss extensions, addressing the issue of outlier robustness. The evaluation demonstrates that the proposed method is able to produce reliable registration results regardless of the initialization. Go-ICP can be applied in scenarios where an optimal solution is desirable or where a good initialization is not always available.



### Efficiently Creating 3D Training Data for Fine Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1605.03389v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1605.03389v2)
- **Published**: 2016-05-11 11:40:27+00:00
- **Updated**: 2016-12-02 15:45:38+00:00
- **Authors**: Markus Oberweger, Gernot Riegler, Paul Wohlhart, Vincent Lepetit
- **Comment**: added link to source https://github.com/moberweger/semi-auto-anno.
  Appears in Proc. of CVPR 2016
- **Journal**: None
- **Summary**: While many recent hand pose estimation methods critically rely on a training set of labelled frames, the creation of such a dataset is a challenging task that has been overlooked so far. As a result, existing datasets are limited to a few sequences and individuals, with limited accuracy, and this prevents these methods from delivering their full potential. We propose a semi-automated method for efficiently and accurately labeling each frame of a hand depth video with the corresponding 3D locations of the joints: The user is asked to provide only an estimate of the 2D reprojections of the visible joints in some reference frames, which are automatically selected to minimize the labeling work by efficiently optimizing a sub-modular loss function. We then exploit spatial, temporal, and appearance constraints to retrieve the full 3D poses of the hand over the complete sequence. We show that this data can be used to train a recent state-of-the-art hand pose estimation method, leading to increased accuracy. The code and dataset can be found on our website https://cvarlab.icg.tugraz.at/projects/hand_detection/



### Image-level Classification in Hyperspectral Images using Feature Descriptors, with Application to Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1605.03428v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.03428v1)
- **Published**: 2016-05-11 13:18:22+00:00
- **Updated**: 2016-05-11 13:18:22+00:00
- **Authors**: Vivek Sharma, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we proposed a novel pipeline for image-level classification in the hyperspectral images. By doing this, we show that the discriminative spectral information at image-level features lead to significantly improved performance in a face recognition task. We also explored the potential of traditional feature descriptors in the hyperspectral images. From our evaluations, we observe that SIFT features outperform the state-of-the-art hyperspectral face recognition methods, and also the other descriptors. With the increasing deployment of hyperspectral sensors in a multitude of applications, we believe that our approach can effectively exploit the spectral information in hyperspectral images, thus beneficial to more accurate classification.



### On-the-fly Network Pruning for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1605.03477v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.03477v1)
- **Published**: 2016-05-11 15:27:43+00:00
- **Updated**: 2016-05-11 15:27:43+00:00
- **Authors**: Marc Masana, Joost van de Weijer, Andrew D. Bagdanov
- **Comment**: Accepted at ICLR 2016 workshop track as a poster presentation
- **Journal**: None
- **Summary**: Object detection with deep neural networks is often performed by passing a few thousand candidate bounding boxes through a deep neural network for each image. These bounding boxes are highly correlated since they originate from the same image. In this paper we investigate how to exploit feature occurrence at the image scale to prune the neural network which is subsequently applied to all bounding boxes. We show that removing units which have near-zero activation in the image allows us to significantly reduce the number of parameters in the network. Results on the PASCAL 2007 Object Detection Challenge demonstrate that up to 40% of units in some fully-connected layers can be entirely eliminated with little change in the detection result.



### Real-time 3D Tracking of Articulated Tools for Robotic Surgery
- **Arxiv ID**: http://arxiv.org/abs/1605.03483v3
- **DOI**: 10.1007/978-3-319-46720-7_45
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1605.03483v3)
- **Published**: 2016-05-11 15:35:23+00:00
- **Updated**: 2016-10-30 12:14:02+00:00
- **Authors**: Menglong Ye, Lin Zhang, Stamatia Giannarou, Guang-Zhong Yang
- **Comment**: This paper was presented in MICCAI 2016 conference, and a DOI was
  linked to the publisher's version
- **Journal**: None
- **Summary**: In robotic surgery, tool tracking is important for providing safe tool-tissue interaction and facilitating surgical skills assessment. Despite recent advances in tool tracking, existing approaches are faced with major difficulties in real-time tracking of articulated tools. Most algorithms are tailored for offline processing with pre-recorded videos. In this paper, we propose a real-time 3D tracking method for articulated tools in robotic surgery. The proposed method is based on the CAD model of the tools as well as robot kinematics to generate online part-based templates for efficient 2D matching and 3D pose estimation. A robust verification approach is incorporated to reject outliers in 2D detections, which is then followed by fusing inliers with robot kinematic readings for 3D pose estimation of the tool. The proposed method has been validated with phantom data, as well as ex vivo and in vivo experiments. The results derived clearly demonstrate the performance advantage of the proposed method when compared to the state-of-the-art.



### Deep Neural Networks Under Stress
- **Arxiv ID**: http://arxiv.org/abs/1605.03498v2
- **DOI**: 10.1109/ICIP.2016.7533200
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1605.03498v2)
- **Published**: 2016-05-11 16:22:23+00:00
- **Updated**: 2016-05-23 08:34:50+00:00
- **Authors**: Micael Carvalho, Matthieu Cord, Sandra Avila, Nicolas Thome, Eduardo Valle
- **Comment**: This article corresponds to the accepted version at IEEE ICIP 2016.
  We will link the DOI as soon as it is available
- **Journal**: None
- **Summary**: In recent years, deep architectures have been used for transfer learning with state-of-the-art performance in many datasets. The properties of their features remain, however, largely unstudied under the transfer perspective. In this work, we present an extensive analysis of the resiliency of feature vectors extracted from deep models, with special focus on the trade-off between performance and compression rate. By introducing perturbations to image descriptions extracted from a deep convolutional neural network, we change their precision and number of dimensions, measuring how it affects the final score. We show that deep features are more robust to these disturbances when compared to classical approaches, achieving a compression rate of 98.4%, while losing only 0.88% of their original score for Pascal VOC 2007.



### View Synthesis by Appearance Flow
- **Arxiv ID**: http://arxiv.org/abs/1605.03557v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.03557v3)
- **Published**: 2016-05-11 19:16:24+00:00
- **Updated**: 2017-02-11 20:33:50+00:00
- **Authors**: Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, Alexei A. Efros
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of novel view synthesis: given an input image, synthesizing new images of the same object or scene observed from arbitrary viewpoints. We approach this as a learning task but, critically, instead of learning to synthesize pixels from scratch, we learn to copy them from the input image. Our approach exploits the observation that the visual appearance of different views of the same instance is highly correlated, and such correlation could be explicitly learned by training a convolutional neural network (CNN) to predict appearance flows -- 2-D coordinate vectors specifying which pixels in the input view could be used to reconstruct the target view. Furthermore, the proposed framework easily generalizes to multiple input views by learning how to optimally combine single-view predictions. We show that for both objects and scenes, our approach is able to synthesize novel views of higher perceptual quality than previous CNN-based techniques.



### ASP Vision: Optically Computing the First Layer of Convolutional Neural Networks using Angle Sensitive Pixels
- **Arxiv ID**: http://arxiv.org/abs/1605.03621v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.03621v3)
- **Published**: 2016-05-11 21:11:30+00:00
- **Updated**: 2016-11-16 19:58:35+00:00
- **Authors**: Huaijin Chen, Suren Jayasuriya, Jiyue Yang, Judy Stephen, Sriram Sivaramakrishnan, Ashok Veeraraghavan, Alyosha Molnar
- **Comment**: Presented in CVPR 2016 (oral), 10 pages, 12 figures. This new version
  corrects the comparison between imaging power for ASPs and a regular image
  sensor
- **Journal**: None
- **Summary**: Deep learning using convolutional neural networks (CNNs) is quickly becoming the state-of-the-art for challenging computer vision applications. However, deep learning's power consumption and bandwidth requirements currently limit its application in embedded and mobile systems with tight energy budgets. In this paper, we explore the energy savings of optically computing the first layer of CNNs. To do so, we utilize bio-inspired Angle Sensitive Pixels (ASPs), custom CMOS diffractive image sensors which act similar to Gabor filter banks in the V1 layer of the human visual cortex. ASPs replace both image sensing and the first layer of a conventional CNN by directly performing optical edge filtering, saving sensing energy, data bandwidth, and CNN FLOPS to compute. Our experimental results (both on synthetic data and a hardware prototype) for a variety of vision tasks such as digit recognition, object recognition, and face identification demonstrate using ASPs while achieving similar performance compared to traditional deep learning pipelines.



### Modified Weibull distribution for Biomedical signals denoising
- **Arxiv ID**: http://arxiv.org/abs/1605.03624v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.03624v2)
- **Published**: 2016-05-11 21:26:50+00:00
- **Updated**: 2022-08-31 12:27:24+00:00
- **Authors**: A. M. Adam, R. M. Farouk, B. S . El-Desouky
- **Comment**: 7 pages, 7 figures, 4 tables. Neuroscience Informatics, Vol. 2, No.
  1, December 2021
- **Journal**: None
- **Summary**: A wide range of signs are acquired from the human body called Biomedical signs or biosignals, they can be at the cell level, organ level, or sub-atomic level. Electroencephalogramis the electrical activity from the cerebrum, the electrocardiogram is the electrical activity from the heart, electrical action from the muscle sound signals referred to as electromyogram, the electroretinogram from the eye, and so on. Studying these signals can be so helpful for doctors, it can help them examine and predict and cure many diseases.



### Facial Expression Recognition from World Wild Web
- **Arxiv ID**: http://arxiv.org/abs/1605.03639v3
- **DOI**: 10.1109/CVPRW.2016.188
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1605.03639v3)
- **Published**: 2016-05-11 23:45:00+00:00
- **Updated**: 2017-01-05 18:07:46+00:00
- **Authors**: Ali Mollahosseini, Behzad Hassani, Michelle J. Salvador, Hojjat Abdollahi, David Chan, Mohammad H. Mahoor
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing facial expression in a wild setting has remained a challenging task in computer vision. The World Wide Web is a good source of facial images which most of them are captured in uncontrolled conditions. In fact, the Internet is a Word Wild Web of facial images with expressions. This paper presents the results of a new study on collecting, annotating, and analyzing wild facial expressions from the web. Three search engines were queried using 1250 emotion related keywords in six different languages and the retrieved images were mapped by two annotators to six basic expressions and neutral. Deep neural networks and noise modeling were used in three different training scenarios to find how accurately facial expressions can be recognized when trained on noisy images collected from the web using query terms (e.g. happy face, laughing man, etc)? The results of our experiments show that deep neural networks can recognize wild facial expressions with an accuracy of 82.12%.



