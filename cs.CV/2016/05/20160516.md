# Arxiv Papers in cs.CV on 2016-05-16
### Heart Beat Characterization from Ballistocardiogram Signals using Extended Functions of Multiple Instances
- **Arxiv ID**: http://arxiv.org/abs/1605.04634v1
- **DOI**: 10.1109/EMBC.2016.7590812
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.04634v1)
- **Published**: 2016-05-16 02:30:28+00:00
- **Updated**: 2016-05-16 02:30:28+00:00
- **Authors**: Changzhe Jiao, Princess Lyons, Alina Zare, Licet Rosales, Marjorie Skubic
- **Comment**: IEEE EMBC 2016, pp. 1-5
- **Journal**: IEEE EMBC 2016, pp. 1-5
- **Summary**: A multiple instance learning (MIL) method, extended Function of Multiple Instances ($e$FUMI), is applied to ballistocardiogram (BCG) signals produced by a hydraulic bed sensor. The goal of this approach is to learn a personalized heartbeat "concept" for an individual. This heartbeat concept is a prototype (or "signature") that characterizes the heartbeat pattern for an individual in ballistocardiogram data. The $e$FUMI method models the problem of learning a heartbeat concept from a BCG signal as a MIL problem. This approach elegantly addresses the uncertainty inherent in a BCG signal e. g., misalignment between training data and ground truth, mis-collection of heartbeat by some transducers, etc. Given a BCG training signal coupled with a ground truth signal (e.g., a pulse finger sensor), training "bags" labeled with only binary labels denoting if a training bag contains a heartbeat signal or not can be generated. Then, using these bags, $e$FUMI learns a personalized concept of heartbeat for a subject as well as several non-heartbeat background concepts. After learning the heartbeat concept, heartbeat detection and heart rate estimation can be applied to test data. Experimental results show that the estimated heartbeat concept found by $e$FUMI is more representative and a more discriminative prototype of the heartbeat signals than those found by comparison MIL methods in the literature.



### Ternary Weight Networks
- **Arxiv ID**: http://arxiv.org/abs/1605.04711v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.04711v3)
- **Published**: 2016-05-16 10:21:25+00:00
- **Updated**: 2022-11-20 14:21:55+00:00
- **Authors**: Fengfu Li, Bin Liu, Xiaoxing Wang, Bo Zhang, Junchi Yan
- **Comment**: 5 pages, 3 fitures, conference
- **Journal**: None
- **Summary**: We present a memory and computation efficient ternary weight networks (TWNs) - with weights constrained to +1, 0 and -1. The Euclidian distance between full (float or double) precision weights and the ternary weights along with a scaling factor is minimized in training stage. Besides, a threshold-based ternary function is optimized to get an approximated solution which can be fast and easily computed. TWNs have shown better expressive abilities than binary precision counterparts. Meanwhile, TWNs achieve up to 16$\times$ model compression rate and need fewer multiplications compared with the float32 precision counterparts. Extensive experiments on MNIST, CIFAR-10, and ImageNet datasets show that the TWNs achieve much better result than the Binary-Weight-Networks (BWNs) and the classification performance on MNIST and CIFAR-10 is very close to the full precision networks. We also verify our method on object detection task and show that TWNs significantly outperforms BWN by more than 10\% mAP on PASCAL VOC dataset. The pytorch version of source code is available at: https://github.com/Thinklab-SJTU/twns.



### CNN based texture synthesize with Semantic segment
- **Arxiv ID**: http://arxiv.org/abs/1605.04731v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1605.04731v1)
- **Published**: 2016-05-16 11:24:03+00:00
- **Updated**: 2016-05-16 11:24:03+00:00
- **Authors**: Xianye Liang, Bocheng Zhuo, Peijie Li, Liangju He
- **Comment**: 7 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:1505.07376, arXiv:1604.04339, arXiv:1602.07188 by other authors
- **Journal**: None
- **Summary**: Deep learning algorithm display powerful ability in Computer Vision area, in recent year, the CNN has been applied to solve problems in the subarea of Image-generating, which has been widely applied in areas such as photo editing, image design, computer animation, real-time rendering for large scale of scenes and for visual effects in movies. However in the texture synthesize procedure. The state-of-art CNN can not capture the spatial location of texture in image, lead to significant distortion after texture synthesize, we propose a new way to generating-image by adding the semantic segment step with deep learning algorithm as Pre-Processing and analyze the outcome.



### Automatic Image Annotation via Label Transfer in the Semantic Space
- **Arxiv ID**: http://arxiv.org/abs/1605.04770v3
- **DOI**: 10.1016/j.patcog.2017.05.019
- **Categories**: **cs.CV**, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1605.04770v3)
- **Published**: 2016-05-16 13:45:15+00:00
- **Updated**: 2017-06-01 13:21:02+00:00
- **Authors**: Tiberio Uricchio, Lamberto Ballan, Lorenzo Seidenari, Alberto Del Bimbo
- **Comment**: To appear in Pattern Recognition
- **Journal**: None
- **Summary**: Automatic image annotation is among the fundamental problems in computer vision and pattern recognition, and it is becoming increasingly important in order to develop algorithms that are able to search and browse large-scale image collections. In this paper, we propose a label propagation framework based on Kernel Canonical Correlation Analysis (KCCA), which builds a latent semantic space where correlation of visual and textual features are well preserved into a semantic embedding. The proposed approach is robust and can work either when the training set is well annotated by experts, as well as when it is noisy such as in the case of user-generated tags in social media. We report extensive results on four popular datasets. Our results show that our KCCA-based framework can be applied to several state-of-the-art label transfer methods to obtain significant improvements. Our approach works even with the noisy tags of social users, provided that appropriate denoising is performed. Experiments on a large scale setting show that our method can provide some benefits even when the semantic space is estimated on a subset of training images.



### An Alternative Matting Laplacian
- **Arxiv ID**: http://arxiv.org/abs/1605.04785v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.04785v1)
- **Published**: 2016-05-16 14:42:34+00:00
- **Updated**: 2016-05-16 14:42:34+00:00
- **Authors**: François Pitié
- **Comment**: ICIP 2016
- **Journal**: None
- **Summary**: Cutting out and object and estimate its transparency mask is a key task in many applications. We take on the work on closed-form matting by Levin et al., that is used at the core of many matting techniques, and propose an alternative formulation that offers more flexible controls over the matting priors. We also show that this new approach is efficient at upscaling transparency maps from coarse estimates.



### Multilevel Thresholding Segmentation of T2 weighted Brain MRI images using Convergent Heterogeneous Particle Swarm Optimization
- **Arxiv ID**: http://arxiv.org/abs/1605.04806v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.04806v1)
- **Published**: 2016-05-16 15:30:05+00:00
- **Updated**: 2016-05-16 15:30:05+00:00
- **Authors**: Mohammad Hamed Mozaffari, Won-Sook Lee
- **Comment**: Journal
- **Journal**: None
- **Summary**: This paper proposes a new image thresholding segmentation approach using the heuristic method, Convergent Heterogeneous Particle Swarm Optimization algorithm. The proposed algorithm incorporates a new strategy of searching the problem space by dividing the swarm into subswarms. Each subswarm particles search for better solution separately lead to better exploitation while they cooperate with each other to find the best global position. The consequence of the aforementioned cooperation is better exploration, convergence and it able the algorithm to jump from local optimal solution to the better spots. A practical application of this method is demonstrated for the problem of medical image thresholding segmentation. We considered two classical thresholding techniques of Otsu and Kapur separately as the objective function for the optimization method and applied on a set of brain MR images. Comparative experimental results reveal that the proposed method outperforms another state of the art method from the literature in terms of accuracy, computation time and stable results.



### Video2GIF: Automatic Generation of Animated GIFs from Video
- **Arxiv ID**: http://arxiv.org/abs/1605.04850v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1605.04850v1)
- **Published**: 2016-05-16 17:44:31+00:00
- **Updated**: 2016-05-16 17:44:31+00:00
- **Authors**: Michael Gygli, Yale Song, Liangliang Cao
- **Comment**: Accepted to CVPR 2016
- **Journal**: None
- **Summary**: We introduce the novel problem of automatically generating animated GIFs from video. GIFs are short looping video with no sound, and a perfect combination between image and video that really capture our attention. GIFs tell a story, express emotion, turn events into humorous moments, and are the new wave of photojournalism. We pose the question: Can we automate the entirely manual and elaborate process of GIF creation by leveraging the plethora of user generated GIF content? We propose a Robust Deep RankNet that, given a video, generates a ranked list of its segments according to their suitability as GIF. We train our model to learn what visual content is often selected for GIFs by using over 100K user generated GIFs and their corresponding video sources. We effectively deal with the noisy web data by proposing a novel adaptive Huber loss in the ranking formulation. We show that our approach is robust to outliers and picks up several patterns that are frequently present in popular animated GIFs. On our new large-scale benchmark dataset, we show the advantage of our approach over several state-of-the-art methods.



### Classification of Big Data with Application to Imaging Genetics
- **Arxiv ID**: http://arxiv.org/abs/1605.04932v1
- **DOI**: None
- **Categories**: **physics.data-an**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1605.04932v1)
- **Published**: 2016-05-16 20:16:29+00:00
- **Updated**: 2016-05-16 20:16:29+00:00
- **Authors**: Magnus O. Ulfarsson, Frosti Palsson, Jakob Sigurdsson, Johannes R. Sveinsson
- **Comment**: None
- **Journal**: None
- **Summary**: Big data applications, such as medical imaging and genetics, typically generate datasets that consist of few observations n on many more variables p, a scenario that we denote as p>>n. Traditional data processing methods are often insufficient for extracting information out of big data. This calls for the development of new algorithms that can deal with the size, complexity, and the special structure of such datasets. In this paper, we consider the problem of classifying p>>n data and propose a classification method based on linear discriminant analysis (LDA). Traditional LDA depends on the covariance estimate of the data, but when p>>n the sample covariance estimate is singular. The proposed method estimates the covariance by using a sparse version of noisy principal component analysis (nPCA). The use of sparsity in this setting aims at automatically selecting variables that are relevant for classification. In experiments, the new method is compared to state-of-the art methods for big data problems using both simulated datasets and imaging genetics datasets.



### Viziometrics: Analyzing Visual Information in the Scientific Literature
- **Arxiv ID**: http://arxiv.org/abs/1605.04951v2
- **DOI**: None
- **Categories**: **cs.SI**, cs.CV, cs.DL, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1605.04951v2)
- **Published**: 2016-05-16 21:03:57+00:00
- **Updated**: 2016-05-27 17:26:22+00:00
- **Authors**: Po-shen Lee, Jevin D. West, Bill Howe
- **Comment**: None
- **Journal**: None
- **Summary**: Scientific results are communicated visually in the literature through diagrams, visualizations, and photographs. These information-dense objects have been largely ignored in bibliometrics and scientometrics studies when compared to citations and text. In this paper, we use techniques from computer vision and machine learning to classify more than 8 million figures from PubMed into 5 figure types and study the resulting patterns of visual information as they relate to impact. We find that the distribution of figures and figure types in the literature has remained relatively constant over time, but can vary widely across field and topic. Remarkably, we find a significant correlation between scientific impact and the use of visual information, where higher impact papers tend to include more diagrams, and to a lesser extent more plots and photographs. To explore these results and other ways of extracting this visual information, we have built a visual browser to illustrate the concept and explore design alternatives for supporting viziometric analysis and organizing visual information. We use these results to articulate a new research agenda -- viziometrics -- to study the organization and presentation of visual information in the scientific literature.



### Going Deeper into Action Recognition: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1605.04988v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.04988v2)
- **Published**: 2016-05-16 23:59:10+00:00
- **Updated**: 2017-02-01 03:27:52+00:00
- **Authors**: Samitha Herath, Mehrtash Harandi, Fatih Porikli
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding human actions in visual data is tied to advances in complementary research areas including object recognition, human dynamics, domain adaptation and semantic segmentation. Over the last decade, human action analysis evolved from earlier schemes that are often limited to controlled environments to nowadays advanced solutions that can learn from millions of videos and apply to almost all daily activities. Given the broad range of applications from video surveillance to human-computer interaction, scientific milestones in action recognition are achieved more rapidly, eventually leading to the demise of what used to be good in a short time. This motivated us to provide a comprehensive review of the notable steps taken towards recognizing human actions. To this end, we start our discussion with the pioneering methods that use handcrafted representations, and then, navigate into the realm of deep learning based approaches. We aim to remain objective throughout this survey, touching upon encouraging improvements as well as inevitable fallbacks, in the hope of raising fresh questions and motivating new research directions for the reader.



