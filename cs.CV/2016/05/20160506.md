# Arxiv Papers in cs.CV on 2016-05-06
### Shape from Mixed Polarization
- **Arxiv ID**: http://arxiv.org/abs/1605.02066v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.02066v2)
- **Published**: 2016-05-06 01:03:11+00:00
- **Updated**: 2016-06-11 11:56:26+00:00
- **Authors**: Vage Taamazyan, Achuta Kadambi, Ramesh Raskar
- **Comment**: 13 pages, 5 figures
- **Journal**: None
- **Summary**: Shape from Polarization (SfP) estimates surface normals using photos captured at different polarizer rotations. Fundamentally, the SfP model assumes that light is reflected either diffusely or specularly. However, this model is not valid for many real-world surfaces exhibiting a mixture of diffuse and specular properties. To address this challenge, previous methods have used a sequential solution: first, use an existing algorithm to separate the scene into diffuse and specular components, then apply the appropriate SfP model. In this paper, we propose a new method that jointly uses viewpoint and polarization data to holistically separate diffuse and specular components, recover refractive index, and ultimately recover 3D shape. By involving the physics of polarization in the separation process, we demonstrate competitive results with a benchmark method, while recovering additional information (e.g. refractive index).



### Estimating Sparse Signals with Smooth Support via Convex Programming and Block Sparsity
- **Arxiv ID**: http://arxiv.org/abs/1605.01813v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.01813v1)
- **Published**: 2016-05-06 03:58:25+00:00
- **Updated**: 2016-05-06 03:58:25+00:00
- **Authors**: Sohil Shah, Tom Goldstein, Christoph Studer
- **Comment**: To appear at CVPR 2016
- **Journal**: None
- **Summary**: Conventional algorithms for sparse signal recovery and sparse representation rely on $l_1$-norm regularized variational methods. However, when applied to the reconstruction of $\textit{sparse images}$, i.e., images where only a few pixels are non-zero, simple $l_1$-norm-based methods ignore potential correlations in the support between adjacent pixels. In a number of applications, one is interested in images that are not only sparse, but also have a support with smooth (or contiguous) boundaries. Existing algorithms that take into account such a support structure mostly rely on non-convex methods and---as a consequence---do not scale well to high-dimensional problems and/or do not converge to global optima. In this paper, we explore the use of new block $l_1$-norm regularizers, which enforce image sparsity while simultaneously promoting smooth support structure. By exploiting the convexity of our regularizers, we develop new computationally-efficient recovery algorithms that guarantee global optimality. We demonstrate the efficacy of our regularizers on a variety of imaging tasks including compressive image recovery, image restoration, and robust PCA.



### Robust Optical Flow Estimation of Double-Layer Images under Transparency or Reflection
- **Arxiv ID**: http://arxiv.org/abs/1605.01825v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.01825v1)
- **Published**: 2016-05-06 05:26:56+00:00
- **Updated**: 2016-05-06 05:26:56+00:00
- **Authors**: Jiaolong Yang, Hongdong Li, Yuchao Dai, Robby T. Tan
- **Comment**: to appear at CVPR 2016 (IEEE Conference on Computer Vision and
  Pattern Recognition)
- **Journal**: None
- **Summary**: This paper deals with a challenging, frequently encountered, yet not properly investigated problem in two-frame optical flow estimation. That is, the input frames are compounds of two imaging layers -- one desired background layer of the scene, and one distracting, possibly moving layer due to transparency or reflection. In this situation, the conventional brightness constancy constraint -- the cornerstone of most existing optical flow methods -- will no longer be valid. In this paper, we propose a robust solution to this problem. The proposed method performs both optical flow estimation, and image layer separation. It exploits a generalized double-layer brightness consistency constraint connecting these two tasks, and utilizes the priors for both of them. Experiments on both synthetic data and real images have confirmed the efficacy of the proposed method. To the best of our knowledge, this is the first attempt towards handling generic optical flow fields of two-frame images containing transparency or reflection.



### Beyond Local Search: Tracking Objects Everywhere with Instance-Specific Proposals
- **Arxiv ID**: http://arxiv.org/abs/1605.01839v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.01839v1)
- **Published**: 2016-05-06 06:53:48+00:00
- **Updated**: 2016-05-06 06:53:48+00:00
- **Authors**: Gao Zhu, Fatih Porikli, Hongdong Li
- **Comment**: CVPR'16. arXiv admin note: text overlap with arXiv:1507.08085
- **Journal**: None
- **Summary**: Most tracking-by-detection methods employ a local search window around the predicted object location in the current frame assuming the previous location is accurate, the trajectory is smooth, and the computational capacity permits a search radius that can accommodate the maximum speed yet small enough to reduce mismatches. These, however, may not be valid always, in particular for fast and irregularly moving objects. Here, we present an object tracker that is not limited to a local search window and has ability to probe efficiently the entire frame. Our method generates a small number of "high-quality" proposals by a novel instance-specific objectness measure and evaluates them against the object model that can be adopted from an existing tracking-by-detection approach as a core tracker. During the tracking process, we update the object model concentrating on hard false-positives supplied by the proposals, which help suppressing distractors caused by difficult background clutters, and learn how to re-rank proposals according to the object model. Since we reduce significantly the number of hypotheses the core tracker evaluates, we can use richer object descriptors and stronger detector. Our method outperforms most recent state-of-the-art trackers on popular tracking benchmarks, and provides improved robustness for fast moving objects as well as for ultra low-frame-rate videos.



### Perceptually Consistent Color-to-Gray Image Conversion
- **Arxiv ID**: http://arxiv.org/abs/1605.01843v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.01843v1)
- **Published**: 2016-05-06 07:13:48+00:00
- **Updated**: 2016-05-06 07:13:48+00:00
- **Authors**: Shaodi You, Nick Barnes, Janine Walker
- **Comment**: 18 pages
- **Journal**: None
- **Summary**: In this paper, we propose a color to grayscale image conversion algorithm (C2G) that aims to preserve the perceptual properties of the color image as much as possible. To this end, we propose measures for two perceptual properties based on contemporary research in vision science: brightness and multi-scale contrast. The brightness measurement is based on the idea that the brightness of a grayscale image will affect the perception of the probability of color information. The color contrast measurement is based on the idea that the contrast of a given pixel to its surroundings can be measured as a linear combination of color contrast at different scales. Based on these measures we propose a graph based optimization framework to balance the brightness and contrast measurements. To solve the optimization, an $\ell_1$-norm based method is provided which converts color discontinuities to brightness discontinuities. To validate our methods, we evaluate against the existing \cadik and Color250 datasets, and against NeoColor, a new dataset that improves over existing C2G datasets. NeoColor contains around 300 images from typical C2G scenarios, including: commercial photograph, printing, books, magazines, masterpiece artworks and computer designed graphics. We show improvements in metrics of performance, and further through a user study, we validate the performance of both the algorithm and the metric.



### UAV-based Autonomous Image Acquisition with Multi-View Stereo Quality Assurance by Confidence Prediction
- **Arxiv ID**: http://arxiv.org/abs/1605.01923v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1605.01923v1)
- **Published**: 2016-05-06 13:00:57+00:00
- **Updated**: 2016-05-06 13:00:57+00:00
- **Authors**: Christian Mostegel, Markus Rumpler, Friedrich Fraundorfer, Horst Bischof
- **Comment**: This paper was accepted to the 7th International Workshop on Computer
  Vision in Vehicle Technology (CVVT 2016) and will appear in IEEE Conference
  on Computer Vision and Pattern Recognition Workshops (CVPRW), 2016. The
  copyright was transferred to IEEE (ieee.org). The paper will be available on
  IEEE Xplore(ieeexplore.ieee.org). This version of the paper also contains the
  supplementary material
- **Journal**: None
- **Summary**: In this paper we present an autonomous system for acquiring close-range high-resolution images that maximize the quality of a later-on 3D reconstruction with respect to coverage, ground resolution and 3D uncertainty. In contrast to previous work, our system uses the already acquired images to predict the confidence in the output of a dense multi-view stereo approach without executing it. This confidence encodes the likelihood of a successful reconstruction with respect to the observed scene and potential camera constellations. Our prediction module runs in real-time and can be trained without any externally recorded ground truth. We use the confidence prediction for on-site quality assurance and for planning further views that are tailored for a specific multi-view stereo approach with respect to the given scene. We demonstrate the capabilities of our approach with an autonomous Unmanned Aerial Vehicle (UAV) in a challenging outdoor scenario.



### Visual Saliency Based on Scale-Space Analysis in the Frequency Domain
- **Arxiv ID**: http://arxiv.org/abs/1605.01999v1
- **DOI**: 10.1109/TPAMI.2012.147
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.01999v1)
- **Published**: 2016-05-06 16:32:33+00:00
- **Updated**: 2016-05-06 16:32:33+00:00
- **Authors**: Jian Li, Martin Levine, Xiangjing An, Xin Xu, Hangen He
- **Comment**: None
- **Journal**: Pattern Analysis and Machine Intelligence, IEEE Transactions on
  35.4 (2013): 996-1010
- **Summary**: We address the issue of visual saliency from three perspectives. First, we consider saliency detection as a frequency domain analysis problem. Second, we achieve this by employing the concept of {\it non-saliency}. Third, we simultaneously consider the detection of salient regions of different size. The paper proposes a new bottom-up paradigm for detecting visual saliency, characterized by a scale-space analysis of the amplitude spectrum of natural images. We show that the convolution of the {\it image amplitude spectrum} with a low-pass Gaussian kernel of an appropriate scale is equivalent to such an image saliency detector. The saliency map is obtained by reconstructing the 2-D signal using the original phase and the amplitude spectrum, filtered at a scale selected by minimizing saliency map entropy. A Hypercomplex Fourier Transform performs the analysis in the frequency domain. Using available databases, we demonstrate experimentally that the proposed model can predict human fixation data. We also introduce a new image database and use it to show that the saliency detector can highlight both small and large salient regions, as well as inhibit repeated distractors in cluttered images. In addition, we show that it is able to predict salient regions on which people focus their attention.



### Robust Bayesian Method for Simultaneous Block Sparse Signal Recovery with Applications to Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1605.02057v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.02057v2)
- **Published**: 2016-05-06 19:41:25+00:00
- **Updated**: 2016-05-10 20:52:12+00:00
- **Authors**: Igor Fedorov, Ritwik Giri, Bhaskar D. Rao, Truong Q. Nguyen
- **Comment**: To appear in ICIP 2016
- **Journal**: None
- **Summary**: In this paper, we present a novel Bayesian approach to recover simultaneously block sparse signals in the presence of outliers. The key advantage of our proposed method is the ability to handle non-stationary outliers, i.e. outliers which have time varying support. We validate our approach with empirical results showing the superiority of the proposed method over competing approaches in synthetic data experiments as well as the multiple measurement face recognition problem.



### Deformably Registering and Annotating Whole CLARITY Brains to an Atlas via Masked LDDMM
- **Arxiv ID**: http://arxiv.org/abs/1605.02060v1
- **DOI**: 10.1117/12.2227444
- **Categories**: **q-bio.QM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1605.02060v1)
- **Published**: 2016-05-06 19:51:27+00:00
- **Updated**: 2016-05-06 19:51:27+00:00
- **Authors**: Kwame S. Kutten, Joshua T. Vogelstein, Nicolas Charon, Li Ye, Karl Deisseroth, Michael I. Miller
- **Comment**: None
- **Journal**: Proc. SPIE 9896 Optics, Photonics and Digital Technologies for
  Imaging Applications IV (2016)
- **Summary**: The CLARITY method renders brains optically transparent to enable high-resolution imaging in the structurally intact brain. Anatomically annotating CLARITY brains is necessary for discovering which regions contain signals of interest. Manually annotating whole-brain, terabyte CLARITY images is difficult, time-consuming, subjective, and error-prone. Automatically registering CLARITY images to a pre-annotated brain atlas offers a solution, but is difficult for several reasons. Removal of the brain from the skull and subsequent storage and processing cause variable non-rigid deformations, thus compounding inter-subject anatomical variability. Additionally, the signal in CLARITY images arises from various biochemical contrast agents which only sparsely label brain structures. This sparse labeling challenges the most commonly used registration algorithms that need to match image histogram statistics to the more densely labeled histological brain atlases. The standard method is a multiscale Mutual Information B-spline algorithm that dynamically generates an average template as an intermediate registration target. We determined that this method performs poorly when registering CLARITY brains to the Allen Institute's Mouse Reference Atlas (ARA), because the image histogram statistics are poorly matched. Therefore, we developed a method (Mask-LDDMM) for registering CLARITY images, that automatically find the brain boundary and learns the optimal deformation between the brain and atlas masks. Using Mask-LDDMM without an average template provided better results than the standard approach when registering CLARITY brains to the ARA. The LDDMM pipelines developed here provide a fast automated way to anatomically annotate CLARITY images. Our code is available as open source software at http://NeuroData.io.



### ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1605.02097v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1605.02097v2)
- **Published**: 2016-05-06 20:46:34+00:00
- **Updated**: 2016-09-20 19:12:49+00:00
- **Authors**: Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, Wojciech Jaśkowski
- **Comment**: None
- **Journal**: Proceedings of IEEE Conference of Computational Intelligence in
  Games 2016
- **Summary**: The recent advances in deep neural networks have led to effective vision-based reinforcement learning methods that have been employed to obtain human-level controllers in Atari 2600 games from pixel data. Atari 2600 games, however, do not resemble real-world tasks since they involve non-realistic 2D environments and the third-person perspective. Here, we propose a novel test-bed platform for reinforcement learning research from raw visual information which employs the first-person perspective in a semi-realistic 3D world. The software, called ViZDoom, is based on the classical first-person shooter video game, Doom. It allows developing bots that play the game using the screen buffer. ViZDoom is lightweight, fast, and highly customizable via a convenient mechanism of user scenarios. In the experimental part, we test the environment by trying to learn bots for two scenarios: a basic move-and-shoot task and a more complex maze-navigation problem. Using convolutional deep neural networks with Q-learning and experience replay, for both scenarios, we were able to train competent bots, which exhibit human-like behaviors. The results confirm the utility of ViZDoom as an AI research platform and imply that visual reinforcement learning in 3D realistic first-person perspective environments is feasible.



### Attribute And-Or Grammar for Joint Parsing of Human Attributes, Part and Pose
- **Arxiv ID**: http://arxiv.org/abs/1605.02112v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.02112v2)
- **Published**: 2016-05-06 22:23:41+00:00
- **Updated**: 2016-07-07 20:10:52+00:00
- **Authors**: Seyoung Park, Bruce Xiaohan Nie, Song-Chun Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents an attribute and-or grammar (A-AOG) model for jointly inferring human body pose and human attributes in a parse graph with attributes augmented to nodes in the hierarchical representation. In contrast to other popular methods in the current literature that train separate classifiers for poses and individual attributes, our method explicitly represents the decomposition and articulation of body parts, and account for the correlations between poses and attributes. The A-AOG model is an amalgamation of three traditional grammar formulations: (i) Phrase structure grammar representing the hierarchical decomposition of the human body from whole to parts; (ii) Dependency grammar modeling the geometric articulation by a kinematic graph of the body pose; and (iii) Attribute grammar accounting for the compatibility relations between different parts in the hierarchy so that their appearances follow a consistent style. The parse graph outputs human detection, pose estimation, and attribute prediction simultaneously, which are intuitive and interpretable. We conduct experiments on two tasks on two datasets, and experimental results demonstrate the advantage of joint modeling in comparison with computing poses and attributes independently. Furthermore, our model obtains better performance over existing methods for both pose estimation and attribute prediction tasks.



