# Arxiv Papers in cs.CV on 2016-05-08
### Deeply Exploit Depth Information for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1605.02260v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.02260v1)
- **Published**: 2016-05-08 01:56:50+00:00
- **Updated**: 2016-05-08 01:56:50+00:00
- **Authors**: Saihui Hou, Zilei Wang, Feng Wu
- **Comment**: 9 pages, 3 figures, and 4 tables. Accepted by CVPR2016 Workshops
- **Journal**: None
- **Summary**: This paper addresses the issue on how to more effectively coordinate the depth with RGB aiming at boosting the performance of RGB-D object detection. Particularly, we investigate two primary ideas under the CNN model: property derivation and property fusion. Firstly, we propose that the depth can be utilized not only as a type of extra information besides RGB but also to derive more visual properties for comprehensively describing the objects of interest. So a two-stage learning framework consisting of property derivation and fusion is constructed. Here the properties can be derived either from the provided color/depth or their pairs (e.g. the geometry contour adopted in this paper). Secondly, we explore the fusion method of different properties in feature learning, which is boiled down to, under the CNN model, from which layer the properties should be fused together. The analysis shows that different semantic properties should be learned separately and combined before passing into the final classifier. Actually, such a detection way is in accordance with the mechanism of the primary neural cortex (V1) in brain. We experimentally evaluate the proposed method on the challenging dataset, and have achieved state-of-the-art performance.



### Laplacian Pyramid Reconstruction and Refinement for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1605.02264v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.02264v2)
- **Published**: 2016-05-08 02:25:12+00:00
- **Updated**: 2016-07-30 21:21:58+00:00
- **Authors**: Golnaz Ghiasi, Charless C. Fowlkes
- **Comment**: None
- **Journal**: None
- **Summary**: CNN architectures have terrific recognition performance but rely on spatial pooling which makes it difficult to adapt them to tasks that require dense, pixel-accurate labeling. This paper makes two contributions: (1) We demonstrate that while the apparent spatial resolution of convolutional feature maps is low, the high-dimensional feature representation contains significant sub-pixel localization information. (2) We describe a multi-resolution reconstruction architecture based on a Laplacian pyramid that uses skip connections from higher resolution feature maps and multiplicative gating to successively refine segment boundaries reconstructed from lower-resolution maps. This approach yields state-of-the-art semantic segmentation results on the PASCAL VOC and Cityscapes segmentation benchmarks without resorting to more complex random-field inference or instance detection driven architectures.



### Robust and Low-Rank Representation for Fast Face Identification with Occlusions
- **Arxiv ID**: http://arxiv.org/abs/1605.02266v2
- **DOI**: 10.1109/TIP.2017.2675206
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.02266v2)
- **Published**: 2016-05-08 02:45:33+00:00
- **Updated**: 2017-11-11 20:38:50+00:00
- **Authors**: Michael Iliadis, Haohong Wang, Rafael Molina, Aggelos K. Katsaggelos
- **Comment**: IEEE Transactions on Image Processing (TIP), 2017
- **Journal**: None
- **Summary**: In this paper we propose an iterative method to address the face identification problem with block occlusions. Our approach utilizes a robust representation based on two characteristics in order to model contiguous errors (e.g., block occlusion) effectively. The first fits to the errors a distribution described by a tailored loss function. The second describes the error image as having a specific structure (resulting in low-rank in comparison to image size). We will show that this joint characterization is effective for describing errors with spatial continuity. Our approach is computationally efficient due to the utilization of the Alternating Direction Method of Multipliers (ADMM). A special case of our fast iterative algorithm leads to the robust representation method which is normally used to handle non-contiguous errors (e.g., pixel corruption). Extensive results on representative face databases (in constrained and unconstrained environments) document the effectiveness of our method over existing robust representation methods with respect to both identification rates and computational time.   Code is available at Github, where you can find implementations of the F-LR-IRNNLS and F-IRNNLS (fast version of the RRC) : https://github.com/miliadis/FIRC



### Detecting Ground Control Points via Convolutional Neural Network for Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/1605.02289v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.02289v1)
- **Published**: 2016-05-08 07:38:40+00:00
- **Updated**: 2016-05-08 07:38:40+00:00
- **Authors**: Zhun Zhong, Songzhi Su, Donglin Cao, Shaozi Li
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: In this paper, we present a novel approach to detect ground control points (GCPs) for stereo matching problem. First of all, we train a convolutional neural network (CNN) on a large stereo set, and compute the matching confidence of each pixel by using the trained CNN model. Secondly, we present a ground control points selection scheme according to the maximum matching confidence of each pixel. Finally, the selected GCPs are used to refine the matching costs, and we apply the new matching costs to perform optimization with semi-global matching algorithm for improving the final disparity maps. We evaluate our approach on the KITTI 2012 stereo benchmark dataset. Our experiments show that the proposed approach significantly improves the accuracy of disparity maps.



### Estimating Depth from Monocular Images as Classification Using Deep Fully Convolutional Residual Networks
- **Arxiv ID**: http://arxiv.org/abs/1605.02305v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.02305v3)
- **Published**: 2016-05-08 10:38:06+00:00
- **Updated**: 2017-08-11 06:52:36+00:00
- **Authors**: Yuanzhouhan Cao, Zifeng Wu, Chunhua Shen
- **Comment**: Accepted to IEEE Transactions on Circuits and Systems for Video
  Technology
- **Journal**: None
- **Summary**: Depth estimation from single monocular images is a key component of scene understanding and has benefited largely from deep convolutional neural networks (CNN) recently. In this article, we take advantage of the recent deep residual networks and propose a simple yet effective approach to this problem. We formulate depth estimation as a pixel-wise classification task. Specifically, we first discretize the continuous depth values into multiple bins and label the bins according to their depth range. Then we train fully convolutional deep residual networks to predict the depth label of each pixel. Performing discrete depth label classification instead of continuous depth value regression allows us to predict a confidence in the form of probability distribution. We further apply fully-connected conditional random fields (CRF) as a post processing step to enforce local smoothness interactions, which improves the results. We evaluate our approach on both indoor and outdoor datasets and achieve state-of-the-art performance.



### Chained Predictions Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1605.02346v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.02346v2)
- **Published**: 2016-05-08 18:52:24+00:00
- **Updated**: 2016-10-23 17:08:29+00:00
- **Authors**: Georgia Gkioxari, Alexander Toshev, Navdeep Jaitly
- **Comment**: in submission to EECV 2016
- **Journal**: None
- **Summary**: In this paper, we present an adaptation of the sequence-to-sequence model for structured output prediction in vision tasks. In this model the output variables for a given input are predicted sequentially using neural networks. The prediction for each output variable depends not only on the input but also on the previously predicted output variables. The model is applied to spatial localization tasks and uses convolutional neural networks (CNNs) for processing input images and a multi-scale deconvolutional architecture for making spatial predictions at each time step. We explore the impact of weight sharing with a recurrent connection matrix between consecutive predictions, and compare it to a formulation where these weights are not tied. Untied weights are particularly suited for problems with a fixed sized structure, where different classes of output are predicted in different steps. We show that chained predictions achieve top performing results on human pose estimation from single images and videos.



