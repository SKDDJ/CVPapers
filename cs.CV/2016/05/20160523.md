# Arxiv Papers in cs.CV on 2016-05-23
### Mask-CNN: Localizing Parts and Selecting Descriptors for Fine-Grained Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1605.06878v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.06878v1)
- **Published**: 2016-05-23 02:46:47+00:00
- **Updated**: 2016-05-23 02:46:47+00:00
- **Authors**: Xiu-Shen Wei, Chen-Wei Xie, Jianxin Wu
- **Comment**: Submitted to NIPS 2016
- **Journal**: None
- **Summary**: Fine-grained image recognition is a challenging computer vision problem, due to the small inter-class variations caused by highly similar subordinate categories, and the large intra-class variations in poses, scales and rotations. In this paper, we propose a novel end-to-end Mask-CNN model without the fully connected layers for fine-grained recognition. Based on the part annotations of fine-grained images, the proposed model consists of a fully convolutional network to both locate the discriminative parts (e.g., head and torso), and more importantly generate object/part masks for selecting useful and meaningful convolutional descriptors. After that, a four-stream Mask-CNN model is built for aggregating the selected object- and part-level descriptors simultaneously. The proposed Mask-CNN model has the smallest number of parameters, lowest feature dimensionality and highest recognition accuracy when compared with state-of-the-arts fine-grained approaches.



### Bridging Category-level and Instance-level Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1605.06885v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.06885v1)
- **Published**: 2016-05-23 03:43:00+00:00
- **Updated**: 2016-05-23 03:43:00+00:00
- **Authors**: Zifeng Wu, Chunhua Shen, Anton van den Hengel
- **Comment**: 14 pages. arXiv admin note: substantial text overlap with
  arXiv:1604.04339
- **Journal**: None
- **Summary**: We propose an approach to instance-level image segmentation that is built on top of category-level segmentation. Specifically, for each pixel in a semantic category mask, its corresponding instance bounding box is predicted using a deep fully convolutional regression network. Thus it follows a different pipeline to the popular detect-then-segment approaches that first predict instances' bounding boxes, which are the current state-of-the-art in instance segmentation. We show that, by leveraging the strength of our state-of-the-art semantic segmentation models, the proposed method can achieve comparable or even better results to detect-then-segment approaches. We make the following contributions. (i) First, we propose a simple yet effective approach to semantic instance segmentation. (ii) Second, we propose an online bootstrapping method during training, which is critically important for achieving good performance for both semantic category segmentation and instance-level segmentation. (iii) As the performance of semantic category segmentation has a significant impact on the instance-level segmentation, which is the second step of our approach, we train fully convolutional residual networks to achieve the best semantic category segmentation accuracy. On the PASCAL VOC 2012 dataset, we obtain the currently best mean intersection-over-union score of 79.1%. (iv) We also achieve state-of-the-art results for instance-level segmentation.



### Embedding based on function approximation for large scale image search
- **Arxiv ID**: http://arxiv.org/abs/1605.06914v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.06914v3)
- **Published**: 2016-05-23 07:12:54+00:00
- **Updated**: 2017-04-04 02:54:44+00:00
- **Authors**: Thanh-Toan Do, Ngai-Man Cheung
- **Comment**: Accepted to TPAMI 2017. The implementation and precomputed features
  of the proposed F-FAemb are released at the following link:
  http://tinyurl.com/F-FAemb
- **Journal**: None
- **Summary**: The objective of this paper is to design an embedding method that maps local features describing an image (e.g. SIFT) to a higher dimensional representation useful for the image retrieval problem. First, motivated by the relationship between the linear approximation of a nonlinear function in high dimensional space and the stateof-the-art feature representation used in image retrieval, i.e., VLAD, we propose a new approach for the approximation. The embedded vectors resulted by the function approximation process are then aggregated to form a single representation for image retrieval. Second, in order to make the proposed embedding method applicable to large scale problem, we further derive its fast version in which the embedded vectors can be efficiently computed, i.e., in the closed-form. We compare the proposed embedding methods with the state of the art in the context of image search under various settings: when the images are represented by medium length vectors, short vectors, or binary vectors. The experimental results show that the proposed embedding methods outperform existing the state of the art on the standard public image retrieval benchmarks.



### Image Restoration with Locally Selected Class-Adapted Models
- **Arxiv ID**: http://arxiv.org/abs/1605.07003v2
- **DOI**: None
- **Categories**: **cs.CV**, 94A08, 68U10, 47N10, I.4.5; I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/1605.07003v2)
- **Published**: 2016-05-23 13:00:38+00:00
- **Updated**: 2016-08-02 09:37:11+00:00
- **Authors**: Afonso M. Teodoro, José M. Bioucas-Dias, Mário A. T. Figueiredo
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art algorithms for imaging inverse problems (namely deblurring and reconstruction) are typically iterative, involving a denoising operation as one of its steps. Using a state-of-the-art denoising method in this context is not trivial, and is the focus of current work. Recently, we have proposed to use a class-adapted denoiser (patch-based using Gaussian mixture models) in a so-called plug-and-play scheme, wherein a state-of-the-art denoiser is plugged into an iterative algorithm, leading to results that outperform the best general-purpose algorithms, when applied to an image of a known class (e.g. faces, text, brain MRI). In this paper, we extend that approach to handle situations where the image being processed is from one of a collection of possible classes or, more importantly, contains regions of different classes. More specifically, we propose a method to locally select one of a set of class-adapted Gaussian mixture patch priors, previously estimated from clean images of those classes. Our approach may be seen as simultaneously performing segmentation and restoration, thus contributing to bridging the gap between image restoration/reconstruction and analysis.



### Spontaneous vs. Posed smiles - can we tell the difference?
- **Arxiv ID**: http://arxiv.org/abs/1605.07026v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1605.07026v1)
- **Published**: 2016-05-23 14:21:30+00:00
- **Updated**: 2016-05-23 14:21:30+00:00
- **Authors**: Bappaditya Mandal, Nizar Ouarti
- **Comment**: 10 pages, 5 figures, 6 tables, International Conference on Computer
  Vision and Image Processing (CVIP 2016)
- **Journal**: None
- **Summary**: Smile is an irrefutable expression that shows the physical state of the mind in both true and deceptive ways. Generally, it shows happy state of the mind, however, `smiles' can be deceptive, for example people can give a smile when they feel happy and sometimes they might also give a smile (in a different way) when they feel pity for others. This work aims to distinguish spontaneous (felt) smile expressions from posed (deliberate) smiles by extracting and analyzing both global (macro) motion of the face and subtle (micro) changes in the facial expression features through both tracking a series of facial fiducial markers as well as using dense optical flow. Specifically the eyes and lips features are captured and used for analysis. It aims to automatically classify all smiles into either `spontaneous' or `posed' categories, by using support vector machines (SVM). Experimental results on large database show promising results as compared to other relevant methods.



### Depth from a Single Image by Harmonizing Overcomplete Local Network Predictions
- **Arxiv ID**: http://arxiv.org/abs/1605.07081v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.07081v2)
- **Published**: 2016-05-23 16:31:43+00:00
- **Updated**: 2016-09-07 18:20:19+00:00
- **Authors**: Ayan Chakrabarti, Jingyu Shao, Gregory Shakhnarovich
- **Comment**: NIPS 2016. Project page at http://www.ttic.edu/chakrabarti/mdepth/
- **Journal**: None
- **Summary**: A single color image can contain many cues informative towards different aspects of local geometric structure. We approach the problem of monocular depth estimation by using a neural network to produce a mid-level representation that summarizes these cues. This network is trained to characterize local scene geometry by predicting, at every image location, depth derivatives of different orders, orientations and scales. However, instead of a single estimate for each derivative, the network outputs probability distributions that allow it to express confidence about some coefficients, and ambiguity about others. Scene depth is then estimated by harmonizing this overcomplete set of network predictions, using a globalization procedure that finds a single consistent depth map that best matches all the local derivative distributions. We demonstrate the efficacy of this approach through evaluation on the NYU v2 depth data set.



### Generic Instance Search and Re-identification from One Example via Attributes and Categories
- **Arxiv ID**: http://arxiv.org/abs/1605.07104v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.07104v1)
- **Published**: 2016-05-23 17:25:40+00:00
- **Updated**: 2016-05-23 17:25:40+00:00
- **Authors**: Ran Tao, Arnold W. M. Smeulders, Shih-Fu Chang
- **Comment**: This technical report is an extended version of our previous
  conference paper 'Attributes and Categories for Generic Instance Search from
  One Example' (CVPR 2015)
- **Journal**: None
- **Summary**: This paper aims for generic instance search from one example where the instance can be an arbitrary object like shoes, not just near-planar and one-sided instances like buildings and logos. First, we evaluate state-of-the-art instance search methods on this problem. We observe that what works for buildings loses its generality on shoes. Second, we propose to use automatically learned category-specific attributes to address the large appearance variations present in generic instance search. Searching among instances from the same category as the query, the category-specific attributes outperform existing approaches by a large margin on shoes and cars and perform on par with the state-of-the-art on buildings. Third, we treat person re-identification as a special case of generic instance search. On the popular VIPeR dataset, we reach state-of-the-art performance with the same method. Fourth, we extend our method to search objects without restriction to the specifically known category. We show that the combination of category-level information and the category-specific attributes is superior to the alternative method combining category-level information with low-level features such as Fisher vector.



### A Formal Evaluation of PSNR as Quality Measurement Parameter for Image Segmentation Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1605.07116v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.07116v1)
- **Published**: 2016-05-23 17:54:57+00:00
- **Updated**: 2016-05-23 17:54:57+00:00
- **Authors**: Fernando A. Fardo, Victor H. Conforto, Francisco C. de Oliveira, Paulo S. Rodrigues
- **Comment**: 11 pages, 8 figures
- **Journal**: None
- **Summary**: Quality evaluation of image segmentation algorithms are still subject of debate and research. Currently, there is no generic metric that could be applied to any algorithm reliably. This article contains an evaluation for the PSRN (Peak Signal-To-Noise Ratio) as a metric which has been used to evaluate threshold level selection as well as the number of thresholds in the case of multi-level segmentation. The results obtained in this study suggest that the PSNR is not an adequate quality measurement for segmentation algorithms.



### Towards Multi-Agent Communication-Based Language Learning
- **Arxiv ID**: http://arxiv.org/abs/1605.07133v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1605.07133v1)
- **Published**: 2016-05-23 18:46:46+00:00
- **Updated**: 2016-05-23 18:46:46+00:00
- **Authors**: Angeliki Lazaridou, Nghia The Pham, Marco Baroni
- **Comment**: 9 pages, manuscript under submission
- **Journal**: None
- **Summary**: We propose an interactive multimodal framework for language learning. Instead of being passively exposed to large amounts of natural text, our learners (implemented as feed-forward neural networks) engage in cooperative referential games starting from a tabula rasa setup, and thus develop their own language from the need to communicate in order to succeed at the game. Preliminary experiments provide promising results, but also suggest that it is important to ensure that agents trained in this way do not develop an adhoc communication code only effective for the game they are playing



### Wide Residual Networks
- **Arxiv ID**: http://arxiv.org/abs/1605.07146v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1605.07146v4)
- **Published**: 2016-05-23 19:27:13+00:00
- **Updated**: 2017-06-14 06:06:48+00:00
- **Authors**: Sergey Zagoruyko, Nikos Komodakis
- **Comment**: None
- **Journal**: None
- **Summary**: Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at https://github.com/szagoruyko/wide-residual-networks



### Unsupervised Learning for Physical Interaction through Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/1605.07157v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1605.07157v4)
- **Published**: 2016-05-23 19:45:55+00:00
- **Updated**: 2016-10-17 20:09:56+00:00
- **Authors**: Chelsea Finn, Ian Goodfellow, Sergey Levine
- **Comment**: To appear in NIPS '16; Video results, code, and data available at:
  http://www.sites.google.com/site/robotprediction
- **Journal**: None
- **Summary**: A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about physical object motion without labels, we develop an action-conditioned video prediction model that explicitly models pixel motion, by predicting a distribution over pixel motion from previous frames. Because our model explicitly predicts motion, it is partially invariant to object appearance, enabling it to generalize to previously unseen objects. To explore video prediction for real-world interactive agents, we also introduce a dataset of 59,000 robot interactions involving pushing motions, including a test set with novel objects. In this dataset, accurate prediction of videos conditioned on the robot's future actions amounts to learning a "visual imagination" of different futures based on different courses of action. Our experiments show that our proposed method produces more accurate video predictions both quantitatively and qualitatively, when compared to prior methods.



