# Arxiv Papers in cs.CV on 2016-05-19
### A Generic Framework for Assessing the Performance Bounds of Image Feature Detectors
- **Arxiv ID**: http://arxiv.org/abs/1605.05791v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.05791v1)
- **Published**: 2016-05-19 01:02:22+00:00
- **Updated**: 2016-05-19 01:02:22+00:00
- **Authors**: Shoaib Ehsan, Adrian F. Clark, Ales Leonardis, Naveed ur Rehman, Klaus D. McDonald-Maier
- **Comment**: Journal version
- **Journal**: None
- **Summary**: Since local feature detection has been one of the most active research areas in computer vision during the last decade, a large number of detectors have been proposed. The interest in feature-based applications continues to grow and has thus rendered the task of characterizing the performance of various feature detection methods an important issue in vision research. Inspired by the good practices of electronic system design, a generic framework based on the repeatability measure is presented in this paper that allows assessment of the upper and lower bounds of detector performance and finds statistically significant performance differences between detectors as a function of image transformation amount by introducing a new variant of McNemars test in an effort to design more reliable and effective vision systems. The proposed framework is then employed to establish operating and guarantee regions for several state-of-the-art detectors and to identify their statistical performance differences for three specific image transformations: JPEG compression, uniform light changes and blurring. The results are obtained using a newly acquired, large image database (20482) images with 539 different scenes. These results provide new insights into the behaviour of detectors and are also useful from the vision systems design perspective.



### Bacterial foraging optimization based brain magnetic resonance image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1605.05815v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1605.05815v1)
- **Published**: 2016-05-19 05:39:13+00:00
- **Updated**: 2016-05-19 05:39:13+00:00
- **Authors**: Abdul kayom Md Khairuzzaman
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Segmentation partitions an image into its constituent parts. It is essentially the pre-processing stage of image analysis and computer vision. In this work, T1 and T2 weighted brain magnetic resonance images are segmented using multilevel thresholding and bacterial foraging optimization (BFO) algorithm. The thresholds are obtained by maximizing the between class variance (multilevel Otsu method) of the image. The BFO algorithm is used to optimize the threshold searching process. The edges are then obtained from the thresholded image by comparing the intensity of each pixel with its eight connected neighbourhood. Post processing is performed to remove spurious responses in the segmented image. The proposed segmentation technique is evaluated using edge detector evaluation parameters such as figure of merit, Rand Index and variation of information. The proposed brain MR image segmentation technique outperforms the traditional edge detectors such as canny and sobel.



### On the Sampling Strategy for Evaluation of Spectral-spatial Methods in Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1605.05829v1
- **DOI**: 10.1109/TGRS.2016.2616489
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.05829v1)
- **Published**: 2016-05-19 06:59:03+00:00
- **Updated**: 2016-05-19 06:59:03+00:00
- **Authors**: Jie Liang, Jun Zhou, Yuntao Qian, Lian Wen, Xiao Bai, Yongsheng Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Spectral-spatial processing has been increasingly explored in remote sensing hyperspectral image classification. While extensive studies have focused on developing methods to improve the classification accuracy, experimental setting and design for method evaluation have drawn little attention. In the scope of supervised classification, we find that traditional experimental designs for spectral processing are often improperly used in the spectral-spatial processing context, leading to unfair or biased performance evaluation. This is especially the case when training and testing samples are randomly drawn from the same image - a practice that has been commonly adopted in the experiments. Under such setting, the dependence caused by overlap between the training and testing samples may be artificially enhanced by some spatial information processing methods such as spatial filtering and morphological operation. Such interaction between training and testing sets has violated data independence assumption that is abided by supervised learning theory and performance evaluation mechanism. Therefore, the widely adopted pixel-based random sampling strategy is not always suitable to evaluate spectral-spatial classification algorithms because it is difficult to determine whether the improvement of classification accuracy is caused by incorporating spatial information into classifier or by increasing the overlap between training and testing samples. To partially solve this problem, we propose a novel controlled random sampling strategy for spectral-spatial methods. It can greatly reduce the overlap between training and testing samples and provides more objective and accurate evaluation.



### Siamese Instance Search for Tracking
- **Arxiv ID**: http://arxiv.org/abs/1605.05863v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.05863v1)
- **Published**: 2016-05-19 09:24:40+00:00
- **Updated**: 2016-05-19 09:24:40+00:00
- **Authors**: Ran Tao, Efstratios Gavves, Arnold W. M. Smeulders
- **Comment**: This paper is accepted to the IEEE Conference on Computer Vision and
  Pattern Recognition, 2016
- **Journal**: None
- **Summary**: In this paper we present a tracker, which is radically different from state-of-the-art trackers: we apply no model updating, no occlusion detection, no combination of trackers, no geometric matching, and still deliver state-of-the-art tracking performance, as demonstrated on the popular online tracking benchmark (OTB) and six very challenging YouTube videos. The presented tracker simply matches the initial patch of the target in the first frame with candidates in a new frame and returns the most similar patch by a learned matching function. The strength of the matching function comes from being extensively trained generically, i.e., without any data of the target, using a Siamese deep neural network, which we design for tracking. Once learned, the matching function is used as is, without any adapting, to track previously unseen targets. It turns out that the learned matching function is so powerful that a simple tracker built upon it, coined Siamese INstance search Tracker, SINT, which only uses the original observation of the target from the first frame, suffices to reach state-of-the-art performance. Further, we show the proposed tracker even allows for target re-identification after the target was absent for a complete video shot.



### Re-ranking Object Proposals for Object Detection in Automatic Driving
- **Arxiv ID**: http://arxiv.org/abs/1605.05904v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.05904v2)
- **Published**: 2016-05-19 11:56:55+00:00
- **Updated**: 2016-05-20 01:45:43+00:00
- **Authors**: Zhun Zhong, Mingyi Lei, Shaozi Li, Jianping Fan
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection often suffers from a plenty of bootless proposals, selecting high quality proposals remains a great challenge. In this paper, we propose a semantic, class-specific approach to re-rank object proposals, which can consistently improve the recall performance even with less proposals. We first extract features for each proposal including semantic segmentation, stereo information, contextual information, CNN-based objectness and low-level cue, and then score them using class-specific weights learnt by Structured SVM. The advantages of the proposed model are twofold: 1) it can be easily merged to existing generators with few computational costs, and 2) it can achieve high recall rate uner strict critical even using less proposals. Experimental evaluation on the KITTI benchmark demonstrates that our approach significantly improves existing popular generators on recall performance. Moreover, in the experiment conducted for object detection, even with 1,500 proposals, our approach can still have higher average precision (AP) than baselines with 5,000 proposals.



### Tongue contour extraction from ultrasound images based on deep neural network
- **Arxiv ID**: http://arxiv.org/abs/1605.05912v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.05912v1)
- **Published**: 2016-05-19 12:20:40+00:00
- **Updated**: 2016-05-19 12:20:40+00:00
- **Authors**: Aurore Jaumard-Hakoun, Kele Xu, Pierre Roussel-Ragot, GÃ©rard Dreyfus, Bruce Denby
- **Comment**: 5 pages, 3 figures, published in The International Congress of
  Phonetic Sciences, 2015
- **Journal**: None
- **Summary**: Studying tongue motion during speech using ultrasound is a standard procedure, but automatic ultrasound image labelling remains a challenge, as standard tongue shape extraction methods typically require human intervention. This article presents a method based on deep neural networks to automatically extract tongue contour from ultrasound images on a speech dataset. We use a deep autoencoder trained to learn the relationship between an image and its related contour, so that the model is able to automatically reconstruct contours from the ultrasound image alone. In this paper, we use an automatic labelling algorithm instead of time-consuming hand-labelling during the training process, and estimate the performances of both automatic labelling and contour extraction as compared to hand-labelling. Observed results show quality scores comparable to the state of the art.



### Matching Handwritten Document Images
- **Arxiv ID**: http://arxiv.org/abs/1605.05923v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.05923v1)
- **Published**: 2016-05-19 12:50:10+00:00
- **Updated**: 2016-05-19 12:50:10+00:00
- **Authors**: Praveen Krishnan, C. V. Jawahar
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: We address the problem of predicting similarity between a pair of handwritten document images written by different individuals. This has applications related to matching and mining in image collections containing handwritten content. A similarity score is computed by detecting patterns of text re-usages between document images irrespective of the minor variations in word morphology, word ordering, layout and paraphrasing of the content. Our method does not depend on an accurate segmentation of words and lines. We formulate the document matching problem as a structured comparison of the word distributions across two document images. To match two word images, we propose a convolutional neural network (CNN) based feature descriptor. Performance of this representation surpasses the state-of-the-art on handwritten word spotting. Finally, we demonstrate the applicability of our method on a practical problem of matching handwritten assignments.



### Hierarchical Piecewise-Constant Super-regions
- **Arxiv ID**: http://arxiv.org/abs/1605.05937v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.05937v1)
- **Published**: 2016-05-19 13:14:13+00:00
- **Updated**: 2016-05-19 13:14:13+00:00
- **Authors**: Imanol Luengo, Mark Basham, Andrew P. French
- **Comment**: None
- **Journal**: None
- **Summary**: Recent applications in computer vision have come to heavily rely on superpixel over-segmentation as a pre-processing step for higher level vision tasks, such as object recognition, image labelling or image segmentation. Here we present a new superpixel algorithm called Hierarchical Piecewise-Constant Super-regions (HPCS), which not only obtains superpixels comparable to the state-of-the-art, but can also be applied hierarchically to form what we call n-th order super-regions. In essence, a Markov Random Field (MRF)-based anisotropic denoising formulation over the quantized feature space is adopted to form piecewise-constant image regions, which are then combined with a graph-based split & merge post-processing step to form superpixels. The graph and quantized feature based formulation of the problem allows us to generalize it hierarchically to preserve boundary adherence with fewer superpixels. Experimental results show that, despite the simplicity of our framework, it is able to provide high quality superpixels, and to hierarchically apply them to form layers of over-segmentation, each with a decreasing number of superpixels, while maintaining the same desired properties (such as adherence to strong image edges). The algorithm is also memory efficient and has a low computational cost.



### Contour-based 3d tongue motion visualization using ultrasound image sequences
- **Arxiv ID**: http://arxiv.org/abs/1605.05967v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.05967v1)
- **Published**: 2016-05-19 14:17:46+00:00
- **Updated**: 2016-05-19 14:17:46+00:00
- **Authors**: Kele Xu, Yin Yang, ClÃ©mence Leboullenger, Pierre Roussel, Bruce Denby
- **Comment**: ICASSP 2016, Mar 2016, SHANGHAI, China
- **Journal**: None
- **Summary**: This article describes a contour-based 3D tongue deformation visualization framework using B-mode ultrasound image sequences. A robust, automatic tracking algorithm characterizes tongue motion via a contour, which is then used to drive a generic 3D Finite Element Model (FEM). A novel contour-based 3D dynamic modeling method is presented. Modal reduction and modal warping techniques are applied to model the deformation of the tongue physically and efficiently. This work can be helpful in a variety of fields, such as speech production, silent speech recognition, articulation training, speech disorder study, etc.



### A Geometric Approach to Color Image Regularization
- **Arxiv ID**: http://arxiv.org/abs/1605.05977v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.05977v1)
- **Published**: 2016-05-19 14:38:59+00:00
- **Updated**: 2016-05-19 14:38:59+00:00
- **Authors**: Freddie ÃstrÃ¶m, Christoph SchnÃ¶rr
- **Comment**: 30 pages
- **Journal**: None
- **Summary**: We present a new vectorial total variation method that addresses the problem of color consistent image filtering. Our approach is inspired from the double-opponent cell representation in the human visual cortex. Existing methods of vectorial total variation regularizers have insufficient (or no) coupling between the color channels and thus may introduce color artifacts. We address this problem by introducing a novel coupling between the color channels related to a pullback-metric from the opponent space to the data (RGB color) space. Our energy is a non-convex, non-smooth higher-order vectorial total variation approach and promotes color consistent image filtering via a coupling term. For a convex variant, we show well-posedness and existence of a solution in the space of vectorial bounded variation. For the higher-order scheme we employ a half-quadratic strategy, which model the non-convex energy terms as the infimum of a sequence of quadratic functions. In experiments, we elaborate on traditional image restoration applications of inpainting, deblurring and denoising. Regarding the latter, we demonstrate state of the art restoration quality with respect to structure coherence and color consistency.



### Hierarchical Clustering in Face Similarity Score Space
- **Arxiv ID**: http://arxiv.org/abs/1605.06052v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.06052v1)
- **Published**: 2016-05-19 17:08:16+00:00
- **Updated**: 2016-05-19 17:08:16+00:00
- **Authors**: Jason Grant, Patrick Flynn
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: Similarity scores in face recognition represent the proximity between pairs of images as computed by a matching algorithm. Given a large set of images and the proximities between all pairs, a similarity score space is defined. Cluster analysis was applied to the similarity score space to develop various taxonomies. Given the number of subjects in the dataset, we used hierarchical methods to aggregate images of the same subject. We also explored the hierarchy above and below the subject level, including clusters that reflect gender and ethnicity. Evidence supports the existence of clustering by race, gender, subject, and illumination condition.



### Stereotyping and Bias in the Flickr30K Dataset
- **Arxiv ID**: http://arxiv.org/abs/1605.06083v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1605.06083v1)
- **Published**: 2016-05-19 19:17:23+00:00
- **Updated**: 2016-05-19 19:17:23+00:00
- **Authors**: Emiel van Miltenburg
- **Comment**: In: Proceedings of the Workshop on Multimodal Corpora (MMC-2016),
  pages 1-4. Editors: Jens Edlund, Dirk Heylen and Patrizia Paggio
- **Journal**: None
- **Summary**: An untested assumption behind the crowdsourced descriptions of the images in the Flickr30K dataset (Young et al., 2014) is that they "focus only on the information that can be obtained from the image alone" (Hodosh et al., 2013, p. 859). This paper presents some evidence against this assumption, and provides a list of biases and unwarranted inferences that can be found in the Flickr30K dataset. Finally, it considers methods to find examples of these, and discusses how we should deal with stereotype-driven descriptions in future applications.



### Automatic Selection of the Optimal Local Feature Detector
- **Arxiv ID**: http://arxiv.org/abs/1605.06094v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.06094v1)
- **Published**: 2016-05-19 19:42:42+00:00
- **Updated**: 2016-05-19 19:42:42+00:00
- **Authors**: Bruno Ferrarini, Shoaib Ehsan, Naveed Ur Rehman, Ales Leonardis, Klaus D. McDonald-Maier
- **Comment**: pre-print version
- **Journal**: None
- **Summary**: A large number of different feature detectors has been proposed so far. Any existing approach presents strengths and weaknesses, which make a detector optimal only for a limited range of applications. A tool capable of selecting the optimal feature detector in relation to the operating conditions is presented in this paper. The input images are quickly analyzed to determine what type of image transformation is applied to them and at which amount. Finally, the detector that is expected to obtain the highest repeatability under such conditions, is chosen to extract features from the input images. The efficiency and the good accuracy in determining the optimal feature detector for any operating condition, make the proposed tool suitable to be utilized in real visual applications. %A large number of different feature detectors has been proposed so far. Any existing approach presents strengths and weaknesses, which make a detector optimal only for a limited range of applications. A large number of different local feature detectors have been proposed in the last few years. However, each feature detector has its own strengths ad weaknesses that limit its use to a specific range of applications. In this paper is presented a tool capable of quickly analysing input images to determine which type and amount of transformation is applied to them and then selecting the optimal feature detector, which is expected to perform the best. The results show that the performance and the fast execution time render the proposed tool suitable for real-world vision applications.



### Development of a 3D tongue motion visualization platform based on ultrasound image sequences
- **Arxiv ID**: http://arxiv.org/abs/1605.06106v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.06106v1)
- **Published**: 2016-05-19 19:56:38+00:00
- **Updated**: 2016-05-19 19:56:38+00:00
- **Authors**: Kele Xu, Yin Yang, Aurore Jaumard-Hakoun, Clemence Leboullenger, Gerard Dreyfus, Pierre Roussel, Maureen Stone, Bruce Denby
- **Comment**: 5 Pages, 5 figures, published in 18th International Congress of
  Phonetic Sciences, 2015
- **Journal**: None
- **Summary**: This article describes the development of a platform designed to visualize the 3D motion of the tongue using ultrasound image sequences. An overview of the system design is given and promising results are presented. Compared to the analysis of motion in 2D image sequences, such a system can provide additional visual information and a quantitative description of the tongue 3D motion. The platform can be useful in a variety of fields, such as speech production, articulation training, etc.



### Inter-Battery Topic Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1605.06155v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1605.06155v2)
- **Published**: 2016-05-19 21:44:12+00:00
- **Updated**: 2016-07-28 10:08:40+00:00
- **Authors**: Cheng Zhang, Hedvig Kjellstrom, Carl Henrik Ek
- **Comment**: ECCV 2016
- **Journal**: None
- **Summary**: In this paper, we present the Inter-Battery Topic Model (IBTM). Our approach extends traditional topic models by learning a factorized latent variable representation. The structured representation leads to a model that marries benefits traditionally associated with a discriminative approach, such as feature selection, with those of a generative model, such as principled regularization and ability to handle missing data. The factorization is provided by representing data in terms of aligned pairs of observations as different views. This provides means for selecting a representation that separately models topics that exist in both views from the topics that are unique to a single view. This structured consolidation allows for efficient and robust inference and provides a compact and efficient representation. Learning is performed in a Bayesian fashion by maximizing a rigorous bound on the log-likelihood. Firstly, we illustrate the benefits of the model on a synthetic dataset,. The model is then evaluated in both uni- and multi-modality settings on two different classification tasks with off-the-shelf convolutional neural network (CNN) features which generate state-of-the-art results with extremely compact representations.



