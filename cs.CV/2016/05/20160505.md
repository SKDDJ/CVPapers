# Arxiv Papers in cs.CV on 2016-05-05
### Classification of Human Whole-Body Motion using Hidden Markov Models
- **Arxiv ID**: http://arxiv.org/abs/1605.01569v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1605.01569v1)
- **Published**: 2016-05-05 12:38:18+00:00
- **Updated**: 2016-05-05 12:38:18+00:00
- **Authors**: Matthias Plappert
- **Comment**: None
- **Journal**: None
- **Summary**: Human motion plays an important role in many fields. Large databases exist that store and make available recordings of human motions. However, annotating each motion with multiple labels is a cumbersome and error-prone process. This bachelor's thesis presents different approaches to solve the multi-label classification problem using Hidden Markov Models (HMMs). First, different features that can be directly obtained from the raw data are introduced. Next, additional features are derived to improve classification performance. These features are then used to perform the multi-label classification using two different approaches. The first approach simply transforms the multi-label problem into a multi-class problem. The second, novel approach solves the same problem without the need to construct a transformation by predicting the labels directly from the likelihood scores. The second approach scales linearly with the number of labels whereas the first approach is subject to combinatorial explosion. All aspects of the classification process are evaluated on a data set that consists of 454 motions. System 1 achieves an accuracy of 98.02% and system 2 an accuracy of 93.39% on the test set.



### Patch-based Texture Synthesis for Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1605.01576v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.01576v1)
- **Published**: 2016-05-05 13:04:09+00:00
- **Updated**: 2016-05-05 13:04:09+00:00
- **Authors**: Tao Zhou, Brian Johnson, Rui Li
- **Comment**: in Computer Science and Applications, 2016
- **Journal**: None
- **Summary**: Image inpaiting is an important task in image processing and vision. In this paper, we develop a general method for patch-based image inpainting by synthesizing new textures from existing one. A novel framework is introduced to find several optimal candidate patches and generate a new texture patch in the process. We form it as an optimization problem that identifies the potential patches for synthesis from an coarse-to-fine manner. We use the texture descriptor as a clue in searching for matching patches from the known region. To ensure the structure faithful to the original image, a geometric constraint metric is formally defined that is applied directly to the patch synthesis procedure. We extensively conducted our experiments on a wide range of testing images on various scenarios and contents by arbitrarily specifying the target the regions for inference followed by using existing evaluation metrics to verify its texture coherency and structural consistency. Our results demonstrate the high accuracy and desirable output that can be potentially used for numerous applications: object removal, background subtraction, and image retrieval.



### AVEC 2016 - Depression, Mood, and Emotion Recognition Workshop and Challenge
- **Arxiv ID**: http://arxiv.org/abs/1605.01600v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1605.01600v4)
- **Published**: 2016-05-05 14:04:50+00:00
- **Updated**: 2016-11-22 15:19:24+00:00
- **Authors**: Michel Valstar, Jonathan Gratch, Bjorn Schuller, Fabien Ringeval, Denis Lalanne, Mercedes Torres Torres, Stefan Scherer, Guiota Stratou, Roddy Cowie, Maja Pantic
- **Comment**: Proceedings of the 6th International Workshop on Audio/Visual Emotion
  Challenge, AVEC'16, co-located with the 24th ACM International Conference on
  Multimedia, MM 2016, pages 3-10, Amsterdam, The Netherlands, October 2016.
  ACM
- **Journal**: None
- **Summary**: The Audio/Visual Emotion Challenge and Workshop (AVEC 2016) "Depression, Mood and Emotion" will be the sixth competition event aimed at comparison of multimedia processing and machine learning methods for automatic audio, visual and physiological depression and emotion analysis, with all participants competing under strictly the same conditions. The goal of the Challenge is to provide a common benchmark test set for multi-modal information processing and to bring together the depression and emotion recognition communities, as well as the audio, video and physiological processing communities, to compare the relative merits of the various approaches to depression and emotion recognition under well-defined and strictly comparable conditions and establish to what extent fusion of the approaches is possible and beneficial. This paper presents the challenge guidelines, the common data used, and the performance of the baseline system on the two tasks.



### Learning Action Maps of Large Environments via First-Person Vision
- **Arxiv ID**: http://arxiv.org/abs/1605.01679v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.01679v1)
- **Published**: 2016-05-05 18:22:30+00:00
- **Updated**: 2016-05-05 18:22:30+00:00
- **Authors**: Nicholas Rhinehart, Kris M. Kitani
- **Comment**: To appear at CVPR 2016
- **Journal**: None
- **Summary**: When people observe and interact with physical spaces, they are able to associate functionality to regions in the environment. Our goal is to automate dense functional understanding of large spaces by leveraging sparse activity demonstrations recorded from an ego-centric viewpoint. The method we describe enables functionality estimation in large scenes where people have behaved, as well as novel scenes where no behaviors are observed. Our method learns and predicts "Action Maps", which encode the ability for a user to perform activities at various locations. With the usage of an egocentric camera to observe human activities, our method scales with the size of the scene without the need for mounting multiple static surveillance cameras and is well-suited to the task of observing activities up-close. We demonstrate that by capturing appearance-based attributes of the environment and associating these attributes with activity demonstrations, our proposed mathematical framework allows for the prediction of Action Maps in new environments. Additionally, we offer a preliminary glance of the applicability of Action Maps by demonstrating a proof-of-concept application in which they are used in concert with activity detections to perform localization.



### Plug-and-Play ADMM for Image Restoration: Fixed Point Convergence and Applications
- **Arxiv ID**: http://arxiv.org/abs/1605.01710v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.01710v2)
- **Published**: 2016-05-05 19:49:19+00:00
- **Updated**: 2016-11-11 18:12:48+00:00
- **Authors**: Stanley H. Chan, Xiran Wang, Omar A. Elgendy
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Alternating direction method of multiplier (ADMM) is a widely used algorithm for solving constrained optimization problems in image restoration. Among many useful features, one critical feature of the ADMM algorithm is its modular structure which allows one to plug in any off-the-shelf image denoising algorithm for a subproblem in the ADMM algorithm. Because of the plug-in nature, this type of ADMM algorithms is coined the name "Plug-and-Play ADMM". Plug-and-Play ADMM has demonstrated promising empirical results in a number of recent papers. However, it is unclear under what conditions and by using what denoising algorithms would it guarantee convergence. Also, since Plug-and-Play ADMM uses a specific way to split the variables, it is unclear if fast implementation can be made for common Gaussian and Poissonian image restoration problems.   In this paper, we propose a Plug-and-Play ADMM algorithm with provable fixed point convergence. We show that for any denoising algorithm satisfying an asymptotic criteria, called bounded denoisers, Plug-and-Play ADMM converges to a fixed point under a continuation scheme. We also present fast implementations for two image restoration problems on super-resolution and single-photon imaging. We compare Plug-and-Play ADMM with state-of-the-art algorithms in each problem type, and demonstrate promising experimental results of the algorithm.



### Not Just a Black Box: Learning Important Features Through Propagating Activation Differences
- **Arxiv ID**: http://arxiv.org/abs/1605.01713v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1605.01713v3)
- **Published**: 2016-05-05 19:52:32+00:00
- **Updated**: 2017-04-11 15:58:48+00:00
- **Authors**: Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, Anshul Kundaje
- **Comment**: 6 pages, 3 figures, this is an older version; see
  https://arxiv.org/abs/1704.02685 for the newer version
- **Journal**: None
- **Summary**: Note: This paper describes an older version of DeepLIFT. See https://arxiv.org/abs/1704.02685 for the newer version. Original abstract follows: The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Learning Important FeaTures), an efficient and effective method for computing importance scores in a neural network. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. We apply DeepLIFT to models trained on natural images and genomic data, and show significant advantages over gradient-based methods.



### Adversarial Diversity and Hard Positive Generation
- **Arxiv ID**: http://arxiv.org/abs/1605.01775v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.01775v2)
- **Published**: 2016-05-05 22:09:35+00:00
- **Updated**: 2016-05-17 02:46:39+00:00
- **Authors**: Andras Rozsa, Ethan M. Rudd, Terrance E. Boult
- **Comment**: Accepted to CVPR 2016 DeepVision Workshop
- **Journal**: None
- **Summary**: State-of-the-art deep neural networks suffer from a fundamental problem - they misclassify adversarial examples formed by applying small perturbations to inputs. In this paper, we present a new psychometric perceptual adversarial similarity score (PASS) measure for quantifying adversarial images, introduce the notion of hard positive generation, and use a diverse set of adversarial perturbations - not just the closest ones - for data augmentation. We introduce a novel hot/cold approach for adversarial example generation, which provides multiple possible adversarial perturbations for every single image. The perturbations generated by our novel approach often correspond to semantically meaningful image structures, and allow greater flexibility to scale perturbation-amplitudes, which yields an increased diversity of adversarial images. We present adversarial images on several network topologies and datasets, including LeNet on the MNIST dataset, and GoogLeNet and ResidualNet on the ImageNet dataset. Finally, we demonstrate on LeNet and GoogLeNet that fine-tuning with a diverse set of hard positives improves the robustness of these networks compared to training with prior methods of generating adversarial images.



### Robust SAR STAP via Kronecker Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1605.01790v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.01790v1)
- **Published**: 2016-05-05 23:53:32+00:00
- **Updated**: 2016-05-05 23:53:32+00:00
- **Authors**: Kristjan Greenewald, Edmund Zelnio, Alfred Hero
- **Comment**: to appear at IEEE AES. arXiv admin note: text overlap with
  arXiv:1604.03622, arXiv:1501.07481
- **Journal**: None
- **Summary**: This paper proposes a spatio-temporal decomposition for the detection of moving targets in multiantenna SAR. As a high resolution radar imaging modality, SAR detects and localizes non-moving targets accurately, giving it an advantage over lower resolution GMTI radars. Moving target detection is more challenging due to target smearing and masking by clutter. Space-time adaptive processing (STAP) is often used to remove the stationary clutter and enhance the moving targets. In this work, it is shown that the performance of STAP can be improved by modeling the clutter covariance as a space vs. time Kronecker product with low rank factors. Based on this model, a low-rank Kronecker product covariance estimation algorithm is proposed, and a novel separable clutter cancelation filter based on the Kronecker covariance estimate is introduced. The proposed method provides orders of magnitude reduction in the required number of training samples, as well as improved robustness to corruption of the training data. Simulation results and experiments using the Gotcha SAR GMTI challenge dataset are presented that confirm the advantages of our approach relative to existing techniques.



