# Arxiv Papers in cs.CV on 2016-05-04
### Mining Discriminative Triplets of Patches for Fine-Grained Classification
- **Arxiv ID**: http://arxiv.org/abs/1605.01130v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.01130v1)
- **Published**: 2016-05-04 02:34:18+00:00
- **Updated**: 2016-05-04 02:34:18+00:00
- **Authors**: Yaming Wang, Jonghyun Choi, Vlad I. Morariu, Larry S. Davis
- **Comment**: None
- **Journal**: None
- **Summary**: Fine-grained classification involves distinguishing between similar sub-categories based on subtle differences in highly localized regions; therefore, accurate localization of discriminative regions remains a major challenge. We describe a patch-based framework to address this problem. We introduce triplets of patches with geometric constraints to improve the accuracy of patch localization, and automatically mine discriminative geometrically-constrained triplets for classification. The resulting approach only requires object bounding boxes. Its effectiveness is demonstrated using four publicly available fine-grained datasets, on which it outperforms or achieves comparable performance to the state-of-the-art in classification.



### A Comparative Evaluation of Approximate Probabilistic Simulation and Deep Neural Networks as Accounts of Human Physical Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/1605.01138v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1605.01138v2)
- **Published**: 2016-05-04 04:26:06+00:00
- **Updated**: 2016-10-04 01:01:47+00:00
- **Authors**: Renqiao Zhang, Jiajun Wu, Chengkai Zhang, William T. Freeman, Joshua B. Tenenbaum
- **Comment**: Accepted to CogSci 2016 as an oral presentation
- **Journal**: None
- **Summary**: Humans demonstrate remarkable abilities to predict physical events in complex scenes. Two classes of models for physical scene understanding have recently been proposed: "Intuitive Physics Engines", or IPEs, which posit that people make predictions by running approximate probabilistic simulations in causal mental models similar in nature to video-game physics engines, and memory-based models, which make judgments based on analogies to stored experiences of previously encountered scenes and physical outcomes. Versions of the latter have recently been instantiated in convolutional neural network (CNN) architectures. Here we report four experiments that, to our knowledge, are the first rigorous comparisons of simulation-based and CNN-based models, where both approaches are concretely instantiated in algorithms that can run on raw image inputs and produce as outputs physical judgments such as whether a stack of blocks will fall. Both approaches can achieve super-human accuracy levels and can quantitatively predict human judgments to a similar degree, but only the simulation-based models generalize to novel situations in ways that people do, and are qualitatively consistent with systematic perceptual illusions and judgment asymmetries that people show.



### Texture Synthesis Through Convolutional Neural Networks and Spectrum Constraints
- **Arxiv ID**: http://arxiv.org/abs/1605.01141v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.01141v3)
- **Published**: 2016-05-04 04:37:52+00:00
- **Updated**: 2016-05-19 08:43:00+00:00
- **Authors**: Gang Liu, Yann Gousseau, Gui-Song Xia
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a significant improvement for the synthesis of texture images using convolutional neural networks (CNNs), making use of constraints on the Fourier spectrum of the results. More precisely, the texture synthesis is regarded as a constrained optimization problem, with constraints conditioning both the Fourier spectrum and statistical features learned by CNNs. In contrast with existing methods, the presented method inherits from previous CNN approaches the ability to depict local structures and fine scale details, and at the same time yields coherent large scale structures, even in the case of quasi-periodic images. This is done at no extra computational cost. Synthesis experiments on various images show a clear improvement compared to a recent state-of-the art method relying on CNN constraints only.



### Application of Deep Convolutional Neural Networks for Detecting Extreme Weather in Climate Datasets
- **Arxiv ID**: http://arxiv.org/abs/1605.01156v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.01156v1)
- **Published**: 2016-05-04 06:38:19+00:00
- **Updated**: 2016-05-04 06:38:19+00:00
- **Authors**: Yunjie Liu, Evan Racah, Prabhat, Joaquin Correa, Amir Khosrowshahi, David Lavers, Kenneth Kunkel, Michael Wehner, William Collins
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting extreme events in large datasets is a major challenge in climate science research. Current algorithms for extreme event detection are build upon human expertise in defining events based on subjective thresholds of relevant physical variables. Often, multiple competing methods produce vastly different results on the same dataset. Accurate characterization of extreme events in climate simulations and observational data archives is critical for understanding the trends and potential impacts of such events in a climate change content. This study presents the first application of Deep Learning techniques as alternative methodology for climate extreme events detection. Deep neural networks are able to learn high-level representations of a broad class of patterns from labeled data. In this work, we developed deep Convolutional Neural Network (CNN) classification system and demonstrated the usefulness of Deep Learning technique for tackling climate pattern detection problems. Coupled with Bayesian based hyper-parameter optimization scheme, our deep CNN system achieves 89\%-99\% of accuracy in detecting extreme events (Tropical Cyclones, Atmospheric Rivers and Weather Fronts



### A metric on the space of finite sets of trajectories for evaluation of multi-target tracking algorithms
- **Arxiv ID**: http://arxiv.org/abs/1605.01177v5
- **DOI**: 10.1109/TSP.2020.3005309
- **Categories**: **cs.CV**, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/1605.01177v5)
- **Published**: 2016-05-04 08:17:24+00:00
- **Updated**: 2020-09-14 11:22:32+00:00
- **Authors**: Ángel F. García-Fernández, Abu Sajana Rahmathullah, Lennart Svensson
- **Comment**: Matlab code for the metric is available at
  https://github.com/Agarciafernandez/MTT
- **Journal**: in IEEE Transactions on Signal Processing, vol. 68, pp. 3917-3928,
  2020
- **Summary**: In this paper, we propose a metric on the space of finite sets of trajectories for assessing multi-target tracking algorithms in a mathematically sound way. The main use of the metric is to compare estimates of trajectories from different algorithms with the ground truth of trajectories. The proposed metric includes intuitive costs associated to localization error for properly detected targets, missed and false targets and track switches at each time step. The metric computation is based on solving a multi-dimensional assignment problem. We also propose a lower bound for the metric, which is also a metric for sets of trajectories and is computable in polynomial time using linear programming. We also extend the proposed metrics on sets of trajectories to random finite sets of trajectories.



### A Generic Method for Automatic Ground Truth Generation of Camera-captured Documents
- **Arxiv ID**: http://arxiv.org/abs/1605.01189v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.01189v1)
- **Published**: 2016-05-04 09:25:04+00:00
- **Updated**: 2016-05-04 09:25:04+00:00
- **Authors**: Sheraz Ahmed, Muhammad Imran Malik, Muhammad Zeshan Afzal, Koichi Kise, Masakazu Iwamura, Andreas Dengel, Marcus Liwicki
- **Comment**: None
- **Journal**: None
- **Summary**: The contribution of this paper is fourfold. The first contribution is a novel, generic method for automatic ground truth generation of camera-captured document images (books, magazines, articles, invoices, etc.). It enables us to build large-scale (i.e., millions of images) labeled camera-captured/scanned documents datasets, without any human intervention. The method is generic, language independent and can be used for generation of labeled documents datasets (both scanned and cameracaptured) in any cursive and non-cursive language, e.g., English, Russian, Arabic, Urdu, etc. To assess the effectiveness of the presented method, two different datasets in English and Russian are generated using the presented method. Evaluation of samples from the two datasets shows that 99:98% of the images were correctly labeled. The second contribution is a large dataset (called C3Wi) of camera-captured characters and words images, comprising 1 million word images (10 million character images), captured in a real camera-based acquisition. This dataset can be used for training as well as testing of character recognition systems on camera-captured documents. The third contribution is a novel method for the recognition of cameracaptured document images. The proposed method is based on Long Short-Term Memory and outperforms the state-of-the-art methods for camera based OCRs. As a fourth contribution, various benchmark tests are performed to uncover the behavior of commercial (ABBYY), open source (Tesseract), and the presented camera-based OCR using the presented C3Wi dataset. Evaluation results reveal that the existing OCRs, which already get very high accuracies on scanned documents, have limited performance on camera-captured document images; where ABBYY has an accuracy of 75%, Tesseract an accuracy of 50.22%, while the presented character recognition system has an accuracy of 95.10%.



### Learning Covariant Feature Detectors
- **Arxiv ID**: http://arxiv.org/abs/1605.01224v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.01224v2)
- **Published**: 2016-05-04 11:11:07+00:00
- **Updated**: 2016-09-09 14:44:36+00:00
- **Authors**: Karel Lenc, Andrea Vedaldi
- **Comment**: None
- **Journal**: None
- **Summary**: Local covariant feature detection, namely the problem of extracting viewpoint invariant features from images, has so far largely resisted the application of machine learning techniques. In this paper, we propose the first fully general formulation for learning local covariant feature detectors. We propose to cast detection as a regression problem, enabling the use of powerful regressors such as deep neural networks. We then derive a covariance constraint that can be used to automatically learn which visual structures provide stable anchors for local feature detection. We support these ideas theoretically, proposing a novel analysis of local features in term of geometric transformations, and we show that all common and many uncommon detectors can be derived in this framework. Finally, we present empirical results on translation and rotation covariant detectors on standard feature benchmarks, showing the power and flexibility of the framework.



### Hierarchical Modeling of Multidimensional Data in Regularly Decomposed Spaces: Applications in Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1605.01242v1
- **DOI**: None
- **Categories**: **cs.CV**, H.2.8; H.3.1; H.3.3; I.2.10; I.4.1; I.4.6; I.4.7; I.4.8; I.4.9;
  I.4.10; I.5.2; I.5.3; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1605.01242v1)
- **Published**: 2016-05-04 12:15:45+00:00
- **Updated**: 2016-05-04 12:15:45+00:00
- **Authors**: Olivier Guye
- **Comment**: 172 pages, 52 figures, research report
- **Journal**: None
- **Summary**: This last document is showing the gradual introduction of hierarchical modeling techniques in image analysis. The first chapter is dealing with the first works carried out in the field of industrial applications of pattern recognition. The second chapter is focusing on the usage of these techniques in satellite imagery and on the development of a satellite data archiving system in the aim of using it in digital geography. The third chapter is about face recognition based on planar image analysis and about the recognition of partially hidden patterns. The present publication is ending with the description of a future system of self-descriptive coding of still or moving pictures in relation with the current video coding standards. As in the previous documents, it will be found in annex algorithms targeted on image analysis according two complementary approaches: - boundary-based approach for the industrial applications of artificial vision; - region-based approach for satellite image analysis.



### Unsupervised Total Variation Loss for Semi-supervised Deep Learning of Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1605.01368v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.01368v3)
- **Published**: 2016-05-04 18:17:25+00:00
- **Updated**: 2018-08-07 20:21:16+00:00
- **Authors**: Mehran Javanmardi, Mehdi Sajjadi, Ting Liu, Tolga Tasdizen
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a novel unsupervised loss function for learning semantic segmentation with deep convolutional neural nets (ConvNet) when densely labeled training images are not available. More specifically, the proposed loss function penalizes the L1-norm of the gradient of the label probability vector image , i.e. total variation, produced by the ConvNet. This can be seen as a regularization term that promotes piecewise smoothness of the label probability vector image produced by the ConvNet during learning. The unsupervised loss function is combined with a supervised loss in a semi-supervised setting to learn ConvNets that can achieve high semantic segmentation accuracy even when only a tiny percentage of the pixels in the training images are labeled. We demonstrate significant improvements over the purely supervised setting in the Weizmann horse, Stanford background and Sift Flow datasets. Furthermore, we show that using the proposed piecewise smoothness constraint in the learning phase significantly outperforms post-processing results from a purely supervised approach with Markov Random Fields (MRF). Finally, we note that the framework we introduce is general and can be used to learn to label other types of structures such as curvilinear structures by modifying the unsupervised loss function accordingly.



### Accelerating Deep Learning with Shrinkage and Recall
- **Arxiv ID**: http://arxiv.org/abs/1605.01369v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1605.01369v2)
- **Published**: 2016-05-04 18:17:37+00:00
- **Updated**: 2016-09-19 19:27:39+00:00
- **Authors**: Shuai Zheng, Abhinav Vishnu, Chris Ding
- **Comment**: The 22nd IEEE International Conference on Parallel and Distributed
  Systems (ICPADS 2016)
- **Journal**: None
- **Summary**: Deep Learning is a very powerful machine learning model. Deep Learning trains a large number of parameters for multiple layers and is very slow when data is in large scale and the architecture size is large. Inspired from the shrinking technique used in accelerating computation of Support Vector Machines (SVM) algorithm and screening technique used in LASSO, we propose a shrinking Deep Learning with recall (sDLr) approach to speed up deep learning computation. We experiment shrinking Deep Learning with recall (sDLr) using Deep Neural Network (DNN), Deep Belief Network (DBN) and Convolution Neural Network (CNN) on 4 data sets. Results show that the speedup using shrinking Deep Learning with recall (sDLr) can reach more than 2.0 while still giving competitive classification performance.



### Leveraging Visual Question Answering for Image-Caption Ranking
- **Arxiv ID**: http://arxiv.org/abs/1605.01379v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.01379v2)
- **Published**: 2016-05-04 18:54:09+00:00
- **Updated**: 2016-08-31 20:14:12+00:00
- **Authors**: Xiao Lin, Devi Parikh
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) is the task of taking as input an image and a free-form natural language question about the image, and producing an accurate answer. In this work we view VQA as a "feature extraction" module to extract image and caption representations. We employ these representations for the task of image-caption ranking. Each feature dimension captures (imagines) whether a fact (question-answer pair) could plausibly be true for the image and caption. This allows the model to interpret images and captions from a wide variety of perspectives. We propose score-level and representation-level fusion models to incorporate VQA knowledge in an existing state-of-the-art VQA-agnostic image-caption ranking model. We find that incorporating and reasoning about consistency between images and captions significantly improves performance. Concretely, our model improves state-of-the-art on caption retrieval by 7.1% and on image retrieval by 4.4% on the MSCOCO dataset.



### The embedding dimension of Laplacian eigenfunction maps
- **Arxiv ID**: http://arxiv.org/abs/1605.01643v1
- **DOI**: 10.1016/j.acha.2014.03.002
- **Categories**: **stat.ML**, cs.CV, math.DG
- **Links**: [PDF](http://arxiv.org/pdf/1605.01643v1)
- **Published**: 2016-05-04 19:18:51+00:00
- **Updated**: 2016-05-04 19:18:51+00:00
- **Authors**: Jonathan Bates
- **Comment**: 16 pages, 2 figures, 3 theorems, and a torus in a pear tree
- **Journal**: Appl. Comput. Harmon. Anal. 37 (3) (2014) 516-530
- **Summary**: Any closed, connected Riemannian manifold $M$ can be smoothly embedded by its Laplacian eigenfunction maps into $\mathbb{R}^m$ for some $m$. We call the smallest such $m$ the maximal embedding dimension of $M$. We show that the maximal embedding dimension of $M$ is bounded from above by a constant depending only on the dimension of $M$, a lower bound for injectivity radius, a lower bound for Ricci curvature, and a volume bound. We interpret this result for the case of surfaces isometrically immersed in $\mathbb{R}^3$, showing that the maximal embedding dimension only depends on bounds for the Gaussian curvature, mean curvature, and surface area. Furthermore, we consider the relevance of these results for shape registration.



### Skin Lesion Analysis toward Melanoma Detection: A Challenge at the International Symposium on Biomedical Imaging (ISBI) 2016, hosted by the International Skin Imaging Collaboration (ISIC)
- **Arxiv ID**: http://arxiv.org/abs/1605.01397v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.01397v1)
- **Published**: 2016-05-04 19:49:17+00:00
- **Updated**: 2016-05-04 19:49:17+00:00
- **Authors**: David Gutman, Noel C. F. Codella, Emre Celebi, Brian Helba, Michael Marchetti, Nabin Mishra, Allan Halpern
- **Comment**: None
- **Journal**: None
- **Summary**: In this article, we describe the design and implementation of a publicly accessible dermatology image analysis benchmark challenge. The goal of the challenge is to sup- port research and development of algorithms for automated diagnosis of melanoma, a lethal form of skin cancer, from dermoscopic images. The challenge was divided into sub-challenges for each task involved in image analysis, including lesion segmentation, dermoscopic feature detection within a lesion, and classification of melanoma. Training data included 900 images. A separate test dataset of 379 images was provided to measure resultant performance of systems developed with the training data. Ground truth for both training and test sets was generated by a panel of dermoscopic experts. In total, there were 79 submissions from a group of 38 participants, making this the largest standardized and comparative study for melanoma diagnosis in dermoscopic images to date. While the official challenge duration and ranking of participants has concluded, the datasets remain available for further research and development.



