# Arxiv Papers in cs.CV on 2016-05-09
### Learning Discriminative Features with Class Encoder
- **Arxiv ID**: http://arxiv.org/abs/1605.02424v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.02424v1)
- **Published**: 2016-05-09 06:04:14+00:00
- **Updated**: 2016-05-09 06:04:14+00:00
- **Authors**: Hailin Shi, Xiangyu Zhu, Zhen Lei, Shengcai Liao, Stan Z. Li
- **Comment**: Accepted by CVPR2016 Workshop of Robust Features for Computer Vision
- **Journal**: None
- **Summary**: Deep neural networks usually benefit from unsupervised pre-training, e.g. auto-encoders. However, the classifier further needs supervised fine-tuning methods for good discrimination. Besides, due to the limits of full-connection, the application of auto-encoders is usually limited to small, well aligned images. In this paper, we incorporate the supervised information to propose a novel formulation, namely class-encoder, whose training objective is to reconstruct a sample from another one of which the labels are identical. Class-encoder aims to minimize the intra-class variations in the feature space, and to learn a good discriminative manifolds on a class scale. We impose the class-encoder as a constraint into the softmax for better supervised training, and extend the reconstruction on feature-level to tackle the parameter size issue and translation issue. The experiments show that the class-encoder helps to improve the performance on benchmarks of classification and face recognition. This could also be a promising direction for fast training of face recognition models.



### Fuzzy Clustering Based Segmentation Of Vertebrae in T1-Weighted Spinal MR Images
- **Arxiv ID**: http://arxiv.org/abs/1605.02460v1
- **DOI**: 10.5121/ijfls.2016.6202
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.02460v1)
- **Published**: 2016-05-09 08:09:13+00:00
- **Updated**: 2016-05-09 08:09:13+00:00
- **Authors**: Jiyo. S. Athertya, G. Saravana Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: Image segmentation in the medical domain is a challenging field owing to poor resolution and limited contrast. The predominantly used conventional segmentation techniques and the thresholding methods suffer from limitations because of heavy dependence on user interactions. Uncertainties prevalent in an image cannot be captured by these techniques. The performance further deteriorates when the images are corrupted by noise, outliers and other artifacts. The objective of this paper is to develop an effective robust fuzzy C- means clustering for segmenting vertebral body from magnetic resonance image owing to its unsupervised form of learning. The motivation for this work is detection of spine geometry and proper localisation and labelling will enhance the diagnostic output of a physician. The method is compared with Otsu thresholding and K-means clustering to illustrate the robustness.The reference standard for validation was the annotated images from the radiologist, and the Dice coefficient and Hausdorff distance measures were used to evaluate the segmentation.



### Orientation Driven Bag of Appearances for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1605.02464v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.02464v1)
- **Published**: 2016-05-09 08:25:33+00:00
- **Updated**: 2016-05-09 08:25:33+00:00
- **Authors**: Liqian Ma, Hong Liu, Liang Hu, Can Wang, Qianru Sun
- **Comment**: 13 pages, 15 figures, 3 tables, submitted to IEEE Transactions on
  Circuits and Systems for Video Technology
- **Journal**: None
- **Summary**: Person re-identification (re-id) consists of associating individual across camera network, which is valuable for intelligent video surveillance and has drawn wide attention. Although person re-identification research is making progress, it still faces some challenges such as varying poses, illumination and viewpoints. For feature representation in re-identification, existing works usually use low-level descriptors which do not take full advantage of body structure information, resulting in low representation ability. %discrimination. To solve this problem, this paper proposes the mid-level body-structure based feature representation (BSFR) which introduces body structure pyramid for codebook learning and feature pooling in the vertical direction of human body. Besides, varying viewpoints in the horizontal direction of human body usually causes the data missing problem, $i.e.$, the appearances obtained in different orientations of the identical person could vary significantly. To address this problem, the orientation driven bag of appearances (ODBoA) is proposed to utilize person orientation information extracted by orientation estimation technic. To properly evaluate the proposed approach, we introduce a new re-identification dataset (Market-1203) based on the Market-1501 dataset and propose a new re-identification dataset (PKU-Reid). Both datasets contain multiple images captured in different body orientations for each person. Experimental results on three public datasets and two proposed datasets demonstrate the superiority of the proposed approach, indicating the effectiveness of body structure and orientation information for improving re-identification performance.



### Robust imaging of hippocampal inner structure at 7T: in vivo acquisition protocol and methodological choices
- **Arxiv ID**: http://arxiv.org/abs/1605.02559v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.02559v1)
- **Published**: 2016-05-09 12:38:44+00:00
- **Updated**: 2016-05-09 12:38:44+00:00
- **Authors**: Linda Marrakchi-Kacem, Alexandre Vignaud, Julien Sein, Johanne Germain, Thomas R Henry, Cyril Poupon, Lucie Hertz-Pannier, Stéphane Lehéricy, Olivier Colliot, Pierre-François Van de Moortele, Marie Chupin
- **Comment**: None
- **Journal**: Magnetic Resonance Materials in Physics, Biology and Medicine,
  Springer Verlag, 2016
- **Summary**: OBJECTIVE:Motion-robust multi-slab imaging of hippocampal inner structure in vivo at 7T.MATERIALS AND METHODS:Motion is a crucial issue for ultra-high resolution imaging, such as can be achieved with 7T MRI. An acquisition protocol was designed for imaging hippocampal inner structure at 7T. It relies on a compromise between anatomical details visibility and robustness to motion. In order to reduce acquisition time and motion artifacts, the full slab covering the hippocampus was split into separate slabs with lower acquisition time. A robust registration approach was implemented to combine the acquired slabs within a final 3D-consistent high-resolution slab covering the whole hippocampus. Evaluation was performed on 50 subjects overall, made of three groups of subjects acquired using three acquisition settings; it focused on three issues: visibility of hippocampal inner structure, robustness to motion artifacts and registration procedure performance.RESULTS:Overall, T2-weighted acquisitions with interleaved slabs proved robust. Multi-slab registration yielded high quality datasets in 96 % of the subjects, thus compatible with further analyses of hippocampal inner structure.CONCLUSION:Multi-slab acquisition and registration setting is efficient for reducing acquisition time and consequently motion artifacts for ultra-high resolution imaging of the inner structure of the hippocampus.



### Studying the brain from adolescence to adulthood through sparse multi-view matrix factorisations
- **Arxiv ID**: http://arxiv.org/abs/1605.02560v1
- **DOI**: None
- **Categories**: **stat.AP**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1605.02560v1)
- **Published**: 2016-05-09 12:40:22+00:00
- **Updated**: 2016-05-09 12:40:22+00:00
- **Authors**: Zi Wang, Vyacheslav Karolis, Chiara Nosarti, Giovanni Montana
- **Comment**: Submitted to the 6th International Workshop on Pattern Recognition in
  Neuroimaging (PRNI)
- **Journal**: None
- **Summary**: Men and women differ in specific cognitive abilities and in the expression of several neuropsychiatric conditions. Such findings could be attributed to sex hormones, brain differences, as well as a number of environmental variables. Existing research on identifying sex-related differences in brain structure have predominantly used cross-sectional studies to investigate, for instance, differences in average gray matter volumes (GMVs). In this article we explore the potential of a recently proposed multi-view matrix factorisation (MVMF) methodology to study structural brain changes in men and women that occur from adolescence to adulthood. MVMF is a multivariate variance decomposition technique that extends principal component analysis to "multi-view" datasets, i.e. where multiple and related groups of observations are available. In this application, each view represents a different age group. MVMF identifies latent factors explaining shared and age-specific contributions to the observed overall variability in GMVs over time. These latent factors can be used to produce low-dimensional visualisations of the data that emphasise age-specific effects once the shared effects have been accounted for. The analysis of two datasets consisting of individuals born prematurely as well as healthy controls provides evidence to suggest that the separation between males and females becomes increasingly larger as the brain transitions from adolescence to adulthood. We report on specific brain regions associated to these variance effects.



### Oracle Based Active Set Algorithm for Scalable Elastic Net Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/1605.02633v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1605.02633v1)
- **Published**: 2016-05-09 15:49:36+00:00
- **Updated**: 2016-05-09 15:49:36+00:00
- **Authors**: Chong You, Chun-Guang Li, Daniel P. Robinson, Rene Vidal
- **Comment**: 15 pages, 6 figures, accepted to CVPR 2016 for oral presentation
- **Journal**: None
- **Summary**: State-of-the-art subspace clustering methods are based on expressing each data point as a linear combination of other data points while regularizing the matrix of coefficients with $\ell_1$, $\ell_2$ or nuclear norms. $\ell_1$ regularization is guaranteed to give a subspace-preserving affinity (i.e., there are no connections between points from different subspaces) under broad theoretical conditions, but the clusters may not be connected. $\ell_2$ and nuclear norm regularization often improve connectivity, but give a subspace-preserving affinity only for independent subspaces. Mixed $\ell_1$, $\ell_2$ and nuclear norm regularizations offer a balance between the subspace-preserving and connectedness properties, but this comes at the cost of increased computational complexity. This paper studies the geometry of the elastic net regularizer (a mixture of the $\ell_1$ and $\ell_2$ norms) and uses it to derive a provably correct and scalable active set method for finding the optimal coefficients. Our geometric analysis also provides a theoretical justification and a geometric interpretation for the balance between the connectedness (due to $\ell_2$ regularization) and subspace-preserving (due to $\ell_1$ regularization) properties for elastic net subspace clustering. Our experiments show that the proposed active set method not only achieves state-of-the-art clustering performance, but also efficiently handles large-scale datasets.



### Building a Large Scale Dataset for Image Emotion Recognition: The Fine Print and The Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1605.02677v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1605.02677v1)
- **Published**: 2016-05-09 18:14:52+00:00
- **Updated**: 2016-05-09 18:14:52+00:00
- **Authors**: Quanzeng You, Jiebo Luo, Hailin Jin, Jianchao Yang
- **Comment**: 7 pages, 7 figures, AAAI 2016
- **Journal**: None
- **Summary**: Psychological research results have confirmed that people can have different emotional reactions to different visual stimuli. Several papers have been published on the problem of visual emotion analysis. In particular, attempts have been made to analyze and predict people's emotional reaction towards images. To this end, different kinds of hand-tuned features are proposed. The results reported on several carefully selected and labeled small image data sets have confirmed the promise of such features. While the recent successes of many computer vision related tasks are due to the adoption of Convolutional Neural Networks (CNNs), visual emotion analysis has not achieved the same level of success. This may be primarily due to the unavailability of confidently labeled and relatively large image data sets for visual emotion analysis. In this work, we introduce a new data set, which started from 3+ million weakly labeled images of different emotions and ended up 30 times as large as the current largest publicly available visual emotion data set. We hope that this data set encourages further research on visual emotion analysis. We also perform extensive benchmarking analyses on this large data set using the state of the art methods including CNNs.



### Unconstrained Still/Video-Based Face Verification with Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1605.02686v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.02686v3)
- **Published**: 2016-05-09 18:28:44+00:00
- **Updated**: 2017-07-18 02:45:38+00:00
- **Authors**: Jun-Cheng Chen, Rajeev Ranjan, Swami Sankaranarayanan, Amit Kumar, Ching-Hui Chen, Vishal M. Patel, Carlos D. Castillo, Rama Chellappa
- **Comment**: accepted by IJCV
- **Journal**: None
- **Summary**: Over the last five years, methods based on Deep Convolutional Neural Networks (DCNNs) have shown impressive performance improvements for object detection and recognition problems. This has been made possible due to the availability of large annotated datasets, a better understanding of the non-linear mapping between input images and class labels as well as the affordability of GPUs. In this paper, we present the design details of a deep learning system for unconstrained face recognition, including modules for face detection, association, alignment and face verification. The quantitative performance evaluation is conducted using the IARPA Janus Benchmark A (IJB-A), the JANUS Challenge Set 2 (JANUS CS2), and the LFW dataset. The IJB-A dataset includes real-world unconstrained faces of 500 subjects with significant pose and illumination variations which are much harder than the Labeled Faces in the Wild (LFW) and Youtube Face (YTF) datasets. JANUS CS2 is the extended version of IJB-A which contains not only all the images/frames of IJB-A but also includes the original videos for evaluating the video-based face verification system. Some open issues regarding DCNNs for face verification problems are then discussed.



### Ask Your Neurons: A Deep Learning Approach to Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1605.02697v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1605.02697v2)
- **Published**: 2016-05-09 19:04:23+00:00
- **Updated**: 2016-11-24 10:30:18+00:00
- **Authors**: Mateusz Malinowski, Marcus Rohrbach, Mario Fritz
- **Comment**: Improved version, it also has a final table from the VQA challenge,
  and more baselines on DAQUAR
- **Journal**: None
- **Summary**: We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Ask Your Neurons, a scalable, jointly trained, end-to-end formulation to this problem.   In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language inputs (image and question). We provide additional insights into the problem by analyzing how much information is contained only in the language part for which we provide a new human baseline. To study human consensus, which is related to the ambiguities inherent in this challenging task, we propose two novel metrics and collect additional answers which extend the original DAQUAR dataset to DAQUAR-Consensus.   Moreover, we also extend our analysis to VQA, a large-scale question answering about images dataset, where we investigate some particular design choices and show the importance of stronger visual models. At the same time, we achieve strong performance of our model that still uses a global image representation. Finally, based on such analysis, we refine our Ask Your Neurons on DAQUAR, which also leads to a better performance on this challenging task.



### A Theoretical Analysis of Deep Neural Networks for Texture Classification
- **Arxiv ID**: http://arxiv.org/abs/1605.02699v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1605.02699v2)
- **Published**: 2016-05-09 19:11:22+00:00
- **Updated**: 2016-06-21 19:32:06+00:00
- **Authors**: Saikat Basu, Manohar Karki, Robert DiBiano, Supratik Mukhopadhyay, Sangram Ganguly, Ramakrishna Nemani, Shreekant Gayaka
- **Comment**: Accepted in International Joint Conference on Neural Networks, IJCNN
  2016
- **Journal**: None
- **Summary**: We investigate the use of Deep Neural Networks for the classification of image datasets where texture features are important for generating class-conditional discriminative representations. To this end, we first derive the size of the feature space for some standard textural features extracted from the input dataset and then use the theory of Vapnik-Chervonenkis dimension to show that hand-crafted feature extraction creates low-dimensional representations which help in reducing the overall excess error rate. As a corollary to this analysis, we derive for the first time upper bounds on the VC dimension of Convolutional Neural Network as well as Dropout and Dropconnect networks and the relation between excess error rate of Dropout and Dropconnect networks. The concept of intrinsic dimension is used to validate the intuition that texture-based datasets are inherently higher dimensional as compared to handwritten digits or other object recognition datasets and hence more difficult to be shattered by neural networks. We then derive the mean distance from the centroid to the nearest and farthest sampling points in an n-dimensional manifold and show that the Relative Contrast of the sample data vanishes as dimensionality of the underlying vector space tends to infinity.



### LightNet: A Versatile, Standalone Matlab-based Environment for Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1605.02766v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1605.02766v3)
- **Published**: 2016-05-09 20:33:30+00:00
- **Updated**: 2016-08-02 04:00:30+00:00
- **Authors**: Chengxi Ye, Chen Zhao, Yezhou Yang, Cornelia Fermuller, Yiannis Aloimonos
- **Comment**: Accepted to ACM MULTIMEDIA 2016 Open Source Software Competition
- **Journal**: None
- **Summary**: LightNet is a lightweight, versatile and purely Matlab-based deep learning framework. The idea underlying its design is to provide an easy-to-understand, easy-to-use and efficient computational platform for deep learning research. The implemented framework supports major deep learning architectures such as Multilayer Perceptron Networks (MLP), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). The framework also supports both CPU and GPU computation, and the switch between them is straightforward. Different applications in computer vision, natural language processing and robotics are demonstrated as experiments.



### Image Classification of Grapevine Buds using Scale-Invariant Features Transform, Bag of Features and Support Vector Machines
- **Arxiv ID**: http://arxiv.org/abs/1605.02775v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.02775v1)
- **Published**: 2016-05-09 20:48:20+00:00
- **Updated**: 2016-05-09 20:48:20+00:00
- **Authors**: Diego Sebastián Pérez, Facundo Bromberg, Carlos Ariel Diaz
- **Comment**: 21 pages, 7 figures, 2 tables
- **Journal**: None
- **Summary**: In viticulture, there are several applications where bud detection in vineyard images is a necessary task, susceptible of being automated through the use of computer vision methods. A common and effective family of visual detection algorithms are the scanning-window type, that slide a (usually) fixed size window along the original image, classifying each resulting windowed-patch as containing or not containing the target object. The simplicity of these algorithms finds its most challenging aspect in the classification stage. Interested in grapevine buds detection in natural field conditions, this paper presents a classification method for images of grapevine buds ranging 100 to 1600 pixels in diameter, captured in outdoor, under natural field conditions, in winter (i.e., no grape bunches, very few leaves, and dormant buds), without artificial background, and with minimum equipment requirements. The proposed method uses well-known computer vision technologies: Scale-Invariant Feature Transform for calculating low-level features, Bag of Features for building an image descriptor, and Support Vector Machines for training a classifier. When evaluated over images containing buds of at least 100 pixels in diameter, the approach achieves a recall higher than 0.9 and a precision of 0.86 over all windowed-patches covering the whole bud and down to 60% of it, and scaled up to window patches containing a proportion of 20%-80% of bud versus background pixels. This robustness on the position and size of the window demonstrates its viability for use as the classification stage in a scanning-window detection algorithms.



### Estimacion de carga muscular mediante imagenes
- **Arxiv ID**: http://arxiv.org/abs/1605.02783v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.02783v2)
- **Published**: 2016-05-09 21:06:30+00:00
- **Updated**: 2016-06-02 14:13:28+00:00
- **Authors**: Leandro Abraham, Facundo Bromberg, Raymundo Forradellas
- **Comment**: None
- **Journal**: None
- **Summary**: Un problema de gran interes en disciplinas como la ocupacional, ergonomica y deportiva, es la medicion de variables biomecanicas involucradas en el movimiento humano (como las fuerzas musculares internas y torque de articulaciones). Actualmente este problema se resuelve en un proceso de dos pasos. Primero capturando datos con dispositivos poco pr\'acticos, intrusivos y costosos. Luego estos datos son usados como entrada en modelos complejos para obtener las variables biomecanicas como salida. El presente trabajo representa una alternativa automatizada, no intrusiva y economica al primer paso, proponiendo la captura de estos datos a traves de imagenes. En trabajos futuros la idea es automatizar todo el proceso de calculo de esas variables. En este trabajo elegimos un caso particular de medicion de variables biomecanicas: el problema de estimar el nivel discreto de carga muscular que estan ejerciendo los musculos de un brazo. Para estimar a partir de imagenes estaticas del brazo ejerciendo la fuerza de sostener la carga, el nivel de la misma, realizamos un proceso de clasificacion. Nuestro enfoque utiliza Support Vector Machines para clasificacion, combinada con una etapa de pre-procesamiento que extrae caracter{\i}sticas visuales utilizando variadas tecnicas (Bag of Keypoints, Local Binary Patterns, Histogramas de Color, Momentos de Contornos) En los mejores casos (Local Binary Patterns y Momentos de Contornos) obtenemos medidas de performance en la clasificacion (Precision, Recall, F-Measure y Accuracy) superiores al 90 %.



