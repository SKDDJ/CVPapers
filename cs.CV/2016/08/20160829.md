# Arxiv Papers in cs.CV on 2016-08-29
### Human Action Recognition without Human
- **Arxiv ID**: http://arxiv.org/abs/1608.07876v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1608.07876v1)
- **Published**: 2016-08-29 01:22:38+00:00
- **Updated**: 2016-08-29 01:22:38+00:00
- **Authors**: Yun He, Soma Shirakabe, Yutaka Satoh, Hirokatsu Kataoka
- **Comment**: None
- **Journal**: None
- **Summary**: The objective of this paper is to evaluate "human action recognition without human". Motion representation is frequently discussed in human action recognition. We have examined several sophisticated options, such as dense trajectories (DT) and the two-stream convolutional neural network (CNN). However, some features from the background could be too strong, as shown in some recent studies on human action recognition. Therefore, we considered whether a background sequence alone can classify human actions in current large-scale action datasets (e.g., UCF101).   In this paper, we propose a novel concept for human action analysis that is named "human action recognition without human". An experiment clearly shows the effect of a background sequence for understanding an action label.



### Using k-nearest neighbors to construct cancelable minutiae templates
- **Arxiv ID**: http://arxiv.org/abs/1608.07897v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.07897v2)
- **Published**: 2016-08-29 02:48:32+00:00
- **Updated**: 2016-08-30 12:32:20+00:00
- **Authors**: Qinghai Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Fingerprint is widely used in a variety of applications. Security measures have to be taken to protect the privacy of fingerprint data. Cancelable biometrics is proposed as an effective mechanism of using and protecting biometrics. In this paper we propose a new method of constructing cancelable fingerprint template by combining real template with synthetic template. Specifically, each user is given one synthetic minutia template generated with random number generator. Every minutia point from the real template is individually thrown into the synthetic template, from which its k-nearest neighbors are found. The verification template is constructed by combining an arbitrary set of the k-nearest neighbors. To prove the validity of the scheme, testing is carried out on three databases. The results show that the constructed templates satisfy the requirements of cancelable biometrics.



### Vehicle Detection from 3D Lidar Using Fully Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/1608.07916v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1608.07916v1)
- **Published**: 2016-08-29 05:57:36+00:00
- **Updated**: 2016-08-29 05:57:36+00:00
- **Authors**: Bo Li, Tianlei Zhang, Tian Xia
- **Comment**: Robotics: Science and Systems, 2016
- **Journal**: None
- **Summary**: Convolutional network techniques have recently achieved great success in vision based detection tasks. This paper introduces the recent development of our research on transplanting the fully convolutional network technique to the detection tasks on 3D range scan data. Specifically, the scenario is set as the vehicle detection task from the range data of Velodyne 64E lidar. We proposes to present the data in a 2D point map and use a single 2D end-to-end fully convolutional network to predict the objectness confidence and the bounding boxes simultaneously. By carefully design the bounding box encoding, it is able to predict full 3D bounding boxes even using a 2D convolutional network. Experiments on the KITTI dataset shows the state-of-the-art performance of the proposed method.



### Approaching the Computational Color Constancy as a Classification Problem through Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1608.07951v1
- **DOI**: 10.1016/j.patcog.2016.08.013
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.07951v1)
- **Published**: 2016-08-29 08:41:55+00:00
- **Updated**: 2016-08-29 08:41:55+00:00
- **Authors**: Seoung Wug Oh, Seon Joo Kim
- **Comment**: This is a preprint of an article accepted for publication in Pattern
  Recognition, ELSEVIER
- **Journal**: Pattern Recognition, Volume 61, January 2017, Pages 405 to 416
- **Summary**: Computational color constancy refers to the problem of computing the illuminant color so that the images of a scene under varying illumination can be normalized to an image under the canonical illumination. In this paper, we adopt a deep learning framework for the illumination estimation problem. The proposed method works under the assumption of uniform illumination over the scene and aims for the accurate illuminant color computation. Specifically, we trained the convolutional neural network to solve the problem by casting the color constancy problem as an illumination classification problem. We designed the deep learning architecture so that the output of the network can be directly used for computing the color of the illumination. Experimental results show that our deep network is able to extract useful features for the illumination estimation and our method outperforms all previous color constancy methods on multiple test datasets.



### Linking Image and Text with 2-Way Nets
- **Arxiv ID**: http://arxiv.org/abs/1608.07973v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.07973v3)
- **Published**: 2016-08-29 09:57:47+00:00
- **Updated**: 2017-02-10 20:38:46+00:00
- **Authors**: Aviv Eisenschtat, Lior Wolf
- **Comment**: 14 pages, 2 figures, 6 tables
- **Journal**: None
- **Summary**: Linking two data sources is a basic building block in numerous computer vision problems. Canonical Correlation Analysis (CCA) achieves this by utilizing a linear optimizer in order to maximize the correlation between the two views. Recent work makes use of non-linear models, including deep learning techniques, that optimize the CCA loss in some feature space. In this paper, we introduce a novel, bi-directional neural network architecture for the task of matching vectors from two data sources. Our approach employs two tied neural network channels that project the two views into a common, maximally correlated space using the Euclidean loss. We show a direct link between the correlation-based loss and Euclidean loss, enabling the use of Euclidean loss for correlation maximization. To overcome common Euclidean regression optimization problems, we modify well-known techniques to our problem, including batch normalization and dropout. We show state of the art results on a number of computer vision matching tasks including MNIST image matching and sentence-image matching on the Flickr8k, Flickr30k and COCO datasets.



### Correspondence Insertion for As-Projective-As-Possible Image Stitching
- **Arxiv ID**: http://arxiv.org/abs/1608.07997v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.07997v1)
- **Published**: 2016-08-29 11:09:18+00:00
- **Updated**: 2016-08-29 11:09:18+00:00
- **Authors**: William X. Liu, Tat-Jun Chin
- **Comment**: None
- **Journal**: None
- **Summary**: Spatially varying warps are increasingly popular for image alignment. In particular, as-projective-as-possible (APAP) warps have been proven effective for accurate panoramic stitching, especially in cases with significant depth parallax that defeat standard homographic warps. However, estimating spatially varying warps requires a sufficient number of feature matches. In image regions where feature detection or matching fail, the warp loses guidance and is unable to accurately model the true underlying warp, thus resulting in poor registration. In this paper, we propose a correspondence insertion method for APAP warps, with a focus on panoramic stitching. Our method automatically identifies misaligned regions, and inserts appropriate point correspondences to increase the flexibility of the warp and improve alignment. Unlike other warp varieties, the underlying projective regularization of APAP warps reduces overfitting and geometric distortion, despite increases to the warp complexity. Comparisons with recent techniques for parallax-tolerant image stitching demonstrate the effectiveness and simplicity of our approach.



### PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1608.08021v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.08021v3)
- **Published**: 2016-08-29 12:32:00+00:00
- **Updated**: 2016-09-30 07:17:13+00:00
- **Authors**: Kye-Hyeon Kim, Sanghoon Hong, Byungseok Roh, Yeongjae Cheon, Minje Park
- **Comment**: Full details about "PVANet 9.0" in the VOC2012 leaderboard
  (https://goo.gl/DuQBku). The test codes are available at
  https://github.com/sanghoon/pva-faster-rcnn
- **Journal**: None
- **Summary**: This paper presents how we can achieve the state-of-the-art accuracy in multi-category object detection task while minimizing the computational cost by adapting and combining recent technical innovations. Following the common pipeline of "CNN feature extraction + region proposal + RoI classification", we mainly redesign the feature extraction part, since region proposal part is not computationally expensive and classification part can be efficiently compressed with common techniques like truncated SVD. Our design principle is "less channels with more layers" and adoption of some building blocks including concatenated ReLU, Inception, and HyperNet. The designed network is deep and thin and trained with the help of batch normalization, residual connections, and learning rate scheduling based on plateau detection. We obtained solid results on well-known object detection benchmarks: 83.8% mAP (mean average precision) on VOC2007 and 82.5% mAP on VOC2012 (2nd place), while taking only 750ms/image on Intel i7-6700K CPU with a single core and 46ms/image on NVIDIA Titan X GPU. Theoretically, our network requires only 12.3% of the computational cost compared to ResNet-101, the winner on VOC2012.



### Edge Preserving and Multi-Scale Contextual Neural Network for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1608.08029v2
- **DOI**: 10.1109/TIP.2017.2756825
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.08029v2)
- **Published**: 2016-08-29 12:43:43+00:00
- **Updated**: 2017-09-22 14:48:25+00:00
- **Authors**: Xiang Wang, Huimin Ma, Xiaozhi Chen, Shaodi You
- **Comment**: None
- **Journal**: IEEE TIP 2017
- **Summary**: In this paper, we propose a novel edge preserving and multi-scale contextual neural network for salient object detection. The proposed framework is aiming to address two limits of the existing CNN based methods. First, region-based CNN methods lack sufficient context to accurately locate salient object since they deal with each region independently. Second, pixel-based CNN methods suffer from blurry boundaries due to the presence of convolutional and pooling layers. Motivated by these, we first propose an end-to-end edge-preserved neural network based on Fast R-CNN framework (named RegionNet) to efficiently generate saliency map with sharp object boundaries. Later, to further improve it, multi-scale spatial context is attached to RegionNet to consider the relationship between regions and the global scenes. Furthermore, our method can be generally applied to RGB-D saliency detection by depth refinement. The proposed framework achieves both clear detection boundary and multi-scale contextual robustness simultaneously for the first time, and thus achieves an optimized performance. Experiments on six RGB and two RGB-D benchmark datasets demonstrate that the proposed method achieves state-of-the-art performance.



### Curvature Integration in a 5D Kernel for Extracting Vessel Connections in Retinal Images
- **Arxiv ID**: http://arxiv.org/abs/1608.08049v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.08049v3)
- **Published**: 2016-08-29 13:57:58+00:00
- **Updated**: 2017-06-26 17:18:58+00:00
- **Authors**: Samaneh Abbasi-Sureshjani, Marta Favali, Giovanna Citti, Alessandro Sarti, Bart M. ter Haar Romeny
- **Comment**: None
- **Journal**: None
- **Summary**: Tree-like structures such as retinal images are widely studied in computer-aided diagnosis systems for large-scale screening programs. Despite several segmentation and tracking methods proposed in the literature, there still exist several limitations specifically when two or more curvilinear structures cross or bifurcate, or in the presence of interrupted lines or highly curved blood vessels. In this paper, we propose a novel approach based on multi-orientation scores augmented with a contextual affinity matrix, which both are inspired by the geometry of the primary visual cortex (V1) and their contextual connections. The connectivity is described with a five-dimensional kernel obtained as the fundamental solution of the Fokker-Planck equation modelling the cortical connectivity in the lifted space of positions, orientations, curvatures and intensity. It is further used in a self-tuning spectral clustering step to identify the main perceptual units in the stimuli. The proposed method has been validated on several easy and challenging structures in a set of artificial images and actual retinal patches. Supported by quantitative and qualitative results, the method is capable of overcoming the limitations of current state-of-the-art techniques.



### Constraint matrix factorization for space variant PSFs field restoration
- **Arxiv ID**: http://arxiv.org/abs/1608.08104v3
- **DOI**: 10.1088/0266-5611/32/12/124001
- **Categories**: **cs.CV**, astro-ph.IM, 00
- **Links**: [PDF](http://arxiv.org/pdf/1608.08104v3)
- **Published**: 2016-08-29 15:30:25+00:00
- **Updated**: 2016-08-31 07:10:41+00:00
- **Authors**: F. M. Ngolè Mboula, J. -L. Starck, K. Okumura, J. Amiaux, P. Hudelot
- **Comment**: 33 pages
- **Journal**: None
- **Summary**: Context: in large-scale spatial surveys, the Point Spread Function (PSF) varies across the instrument field of view (FOV). Local measurements of the PSFs are given by the isolated stars images. Yet, these estimates may not be directly usable for post-processings because of the observational noise and potentially the aliasing. Aims: given a set of aliased and noisy stars images from a telescope, we want to estimate well-resolved and noise-free PSFs at the observed stars positions, in particular, exploiting the spatial correlation of the PSFs across the FOV. Contributions: we introduce RCA (Resolved Components Analysis) which is a noise-robust dimension reduction and super-resolution method based on matrix factorization. We propose an original way of using the PSFs spatial correlation in the restoration process through sparsity. The introduced formalism can be applied to correlated data sets with respect to any euclidean parametric space. Results: we tested our method on simulated monochromatic PSFs of Euclid telescope (launch planned for 2020). The proposed method outperforms existing PSFs restoration and dimension reduction methods. We show that a coupled sparsity constraint on individual PSFs and their spatial distribution yields a significant improvement on both the restored PSFs shapes and the PSFs subspace identification, in presence of aliasing. Perspectives: RCA can be naturally extended to account for the wavelength dependency of the PSFs.



### Temporal Activity Detection in Untrimmed Videos with Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1608.08128v3
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1608.08128v3)
- **Published**: 2016-08-29 16:14:52+00:00
- **Updated**: 2017-03-02 23:07:00+00:00
- **Authors**: Alberto Montes, Amaia Salvador, Santiago Pascual, Xavier Giro-i-Nieto
- **Comment**: Best Poster Award at the 1st NIPS Workshop on Large Scale Computer
  Vision Systems (Barcelona, December 2016). Source code available at
  https://imatge-upc.github.io/activitynet-2016-cvprw/
- **Journal**: None
- **Summary**: This thesis explore different approaches using Convolutional and Recurrent Neural Networks to classify and temporally localize activities on videos, furthermore an implementation to achieve it has been proposed. As the first step, features have been extracted from video frames using an state of the art 3D Convolutional Neural Network. This features are fed in a recurrent neural network that solves the activity classification and temporally location tasks in a simple and flexible way. Different architectures and configurations have been tested in order to achieve the best performance and learning of the video dataset provided. In addition it has been studied different kind of post processing over the trained network's output to achieve a better results on the temporally localization of activities on the videos. The results provided by the neural network developed in this thesis have been submitted to the ActivityNet Challenge 2016 of the CVPR, achieving competitive results using a simple and flexible architecture.



### Where is my Phone ? Personal Object Retrieval from Egocentric Images
- **Arxiv ID**: http://arxiv.org/abs/1608.08139v2
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, H.3.3; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/1608.08139v2)
- **Published**: 2016-08-29 16:41:52+00:00
- **Updated**: 2017-03-02 23:13:09+00:00
- **Authors**: Cristian Reyes, Eva Mohedano, Kevin McGuinness, Noel E. O'Connor, Xavier Giro-i-Nieto
- **Comment**: Lifelogging Tools and Applications Workshop (LTA'16) at ACM
  Multimedia 2016
- **Journal**: None
- **Summary**: This work presents a retrieval pipeline and evaluation scheme for the problem of finding the last appearance of personal objects in a large dataset of images captured from a wearable camera. Each personal object is modelled by a small set of images that define a query for a visual search engine.The retrieved results are reranked considering the temporal timestamps of the images to increase the relevance of the later detections. Finally, a temporal interleaving of the results is introduced for robustness against false detections. The Mean Reciprocal Rank is proposed as a metric to evaluate this problem. This application could help into developing personal assistants capable of helping users when they do not remember where they left their personal belongings.



### ORBSLAM-based Endoscope Tracking and 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1608.08149v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.08149v1)
- **Published**: 2016-08-29 17:10:26+00:00
- **Updated**: 2016-08-29 17:10:26+00:00
- **Authors**: Nader Mahmoud, Iñigo Cirauqui, Alexandre Hostettler, Christophe Doignon, Luc Soler, Jacques Marescaux, J. M. M. Montiel
- **Comment**: None
- **Journal**: None
- **Summary**: We aim to track the endoscope location inside the surgical scene and provide 3D reconstruction, in real-time, from the sole input of the image sequence captured by the monocular endoscope. This information offers new possibilities for developing surgical navigation and augmented reality applications. The main benefit of this approach is the lack of extra tracking elements which can disturb the surgeon performance in the clinical routine. It is our first contribution to exploit ORBSLAM, one of the best performing monocular SLAM algorithms, to estimate both of the endoscope location, and 3D structure of the surgical scene. However, the reconstructed 3D map poorly describe textureless soft organ surfaces such as liver. It is our second contribution to extend ORBSLAM to be able to reconstruct a semi-dense map of soft organs. Experimental results on in-vivo pigs, shows a robust endoscope tracking even with organs deformations and partial instrument occlusions. It also shows the reconstruction density, and accuracy against ground truth surface obtained from CT.



### Tracking Completion
- **Arxiv ID**: http://arxiv.org/abs/1608.08171v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.08171v2)
- **Published**: 2016-08-29 18:33:23+00:00
- **Updated**: 2016-09-09 01:49:46+00:00
- **Authors**: Yao Sui, Guanghui Wang, Yafei Tang, Li Zhang
- **Comment**: Published at ECCV 2016
- **Journal**: None
- **Summary**: A fundamental component of modern trackers is an online learned tracking model, which is typically modeled either globally or locally. The two kinds of models perform differently in terms of effectiveness and robustness under different challenging situations. This work exploits the advantages of both models. A subspace model, from a global perspective, is learned from previously obtained targets via rank-minimization to address the tracking, and a pixel-level local observation is leveraged si- multaneously, from a local point of view, to augment the subspace model. A matrix completion method is employed to integrate the two models. Unlike previous tracking methods, which locate the target among all fully observed target candidates, the proposed approach first estimates an expected target via the matrix completion through partially observed target candidates, and then, identifies the target according to the estimation accuracy with respect to the target candidates. Specifically, the tracking is formulated as a problem of target appearance estimation. Extensive experiments on various challenging video sequences verify the effectiveness of the proposed approach and demonstrate that the proposed tracker outperforms other popular state-of-the-art trackers.



### Real-Time Visual Tracking: Promoting the Robustness of Correlation Filter Learning
- **Arxiv ID**: http://arxiv.org/abs/1608.08173v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.08173v2)
- **Published**: 2016-08-29 18:36:36+00:00
- **Updated**: 2016-09-09 01:50:14+00:00
- **Authors**: Yao Sui, Ziming Zhang, Guanghui Wang, Yafei Tang, Li Zhang
- **Comment**: Published at ECCV 2016
- **Journal**: None
- **Summary**: Correlation filtering based tracking model has received lots of attention and achieved great success in real-time tracking, however, the lost function in current correlation filtering paradigm could not reliably response to the appearance changes caused by occlusion and illumination variations. This study intends to promote the robustness of the correlation filter learning. By exploiting the anisotropy of the filter response, three sparsity related loss functions are proposed to alleviate the overfitting issue of previous methods and improve the overall tracking performance. As a result, three real-time trackers are implemented. Extensive experiments in various challenging situations demonstrate that the robustness of the learned correlation filter has been greatly improved via the designed loss functions. In addition, the study reveals, from an experimental perspective, how different loss functions essentially influence the tracking performance. An important conclusion is that the sensitivity of the peak values of the filter in successive frames is consistent with the tracking performance. This is a useful reference criterion in designing a robust correlation filter for visual tracking.



### Visual Question: Predicting If a Crowd Will Agree on the Answer
- **Arxiv ID**: http://arxiv.org/abs/1608.08188v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1608.08188v1)
- **Published**: 2016-08-29 19:24:25+00:00
- **Updated**: 2016-08-29 19:24:25+00:00
- **Authors**: Danna Gurari, Kristen Grauman
- **Comment**: None
- **Journal**: None
- **Summary**: Visual question answering (VQA) systems are emerging from a desire to empower users to ask any natural language question about visual content and receive a valid answer in response. However, close examination of the VQA problem reveals an unavoidable, entangled problem that multiple humans may or may not always agree on a single answer to a visual question. We train a model to automatically predict from a visual question whether a crowd would agree on a single answer. We then propose how to exploit this system in a novel application to efficiently allocate human effort to collect answers to visual questions. Specifically, we propose a crowdsourcing system that automatically solicits fewer human responses when answer agreement is expected and more human responses when answer disagreement is expected. Our system improves upon existing crowdsourcing systems, typically eliminating at least 20% of human effort with no loss to the information collected from the crowd.



### Temporal Convolutional Networks: A Unified Approach to Action Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1608.08242v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.08242v1)
- **Published**: 2016-08-29 20:48:15+00:00
- **Updated**: 2016-08-29 20:48:15+00:00
- **Authors**: Colin Lea, Rene Vidal, Austin Reiter, Gregory D. Hager
- **Comment**: Submitted to the ECCV workshop on "Brave new ideas for motion
  representations in videos" (http://bravenewmotion.github.io/)
- **Journal**: None
- **Summary**: The dominant paradigm for video-based action segmentation is composed of two steps: first, for each frame, compute low-level features using Dense Trajectories or a Convolutional Neural Network that encode spatiotemporal information locally, and second, input these features into a classifier that captures high-level temporal relationships, such as a Recurrent Neural Network (RNN). While often effective, this decoupling requires specifying two separate models, each with their own complexities, and prevents capturing more nuanced long-range spatiotemporal relationships. We propose a unified approach, as demonstrated by our Temporal Convolutional Network (TCN), that hierarchically captures relationships at low-, intermediate-, and high-level time-scales. Our model achieves superior or competitive performance using video or sensor data on three public action segmentation datasets and can be trained in a fraction of the time it takes to train an RNN.



### Construction of Convex Sets on Quadrilateral Ordered Tiles or Graphs with Propagation Neighborhood Operations. Dales, Concavity Structures. Application to Gray Image Analysis of Human-Readable Shapes
- **Arxiv ID**: http://arxiv.org/abs/1608.08251v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.08251v1)
- **Published**: 2016-08-29 21:12:49+00:00
- **Updated**: 2016-08-29 21:12:49+00:00
- **Authors**: Igor Polkovnikov
- **Comment**: 58 pages, more than 50 figures
- **Journal**: None
- **Summary**: An effort has been made to show mathematicians some new ideas applied to image analysis. Gray images are presented as tilings. Based on topological properties of the tiling, a number of gray convex hulls: maximal, minimal, and oriented ones are constructed and some are proved. They are constructed with only one operation. Two tilings are used in the Constraint and Allowance types of operations. New type of concavity described: a dale. All operations are parallel, possible to realize clock-less. Convexities define what is the background. They are treated as separate gray objects. There are multiple relations among them and their descendants. Via that, topological size of concavities is proposed. Constructed with the same type of operations, Rays and Angles in a tiling define possible spatial relations. Notions like "strokes" are defined through concavities. Unusual effects on levelized gray objects are shown. It is illustrated how alphabet and complex hieroglyphs can be described through concavities and their relations. A hypothesis of living organisms image analysis is proposed. A number of examples with symbols and a human face are calculated with new Asynchwave C++ software library.



