# Arxiv Papers in cs.CV on 2016-08-05
### Play and Learn: Using Video Games to Train Computer Vision Models
- **Arxiv ID**: http://arxiv.org/abs/1608.01745v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.01745v2)
- **Published**: 2016-08-05 03:16:07+00:00
- **Updated**: 2016-08-15 19:41:47+00:00
- **Authors**: Alireza Shafaei, James J. Little, Mark Schmidt
- **Comment**: To appear in the British Machine Vision Conference (BMVC), September
  2016. -v2: fixed a typo in the references
- **Journal**: None
- **Summary**: Video games are a compelling source of annotated data as they can readily provide fine-grained groundtruth for diverse tasks. However, it is not clear whether the synthetically generated data has enough resemblance to the real-world images to improve the performance of computer vision models in practice. We present experiments assessing the effectiveness on real-world data of systems trained on synthetic RGB images that are extracted from a video game. We collected over 60000 synthetic samples from a modern video game with similar conditions to the real-world CamVid and Cityscapes datasets. We provide several experiments to demonstrate that the synthetically generated RGB images can be used to improve the performance of deep neural networks on both image segmentation and depth estimation. These results show that a convolutional network trained on synthetic data achieves a similar test error to a network that is trained on real-world data for dense image classification. Furthermore, the synthetically generated RGB images can provide similar or better results compared to the real-world datasets if a simple domain adaptation technique is applied. Our results suggest that collaboration with game developers for an accessible interface to gather data is potentially a fruitful direction for future work in computer vision.



### Deep Learning the City : Quantifying Urban Perception At A Global Scale
- **Arxiv ID**: http://arxiv.org/abs/1608.01769v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.01769v2)
- **Published**: 2016-08-05 05:58:35+00:00
- **Updated**: 2016-09-12 18:48:37+00:00
- **Authors**: Abhimanyu Dubey, Nikhil Naik, Devi Parikh, Ramesh Raskar, CÃ©sar A. Hidalgo
- **Comment**: 23 pages, 8 figures. Accepted to the European Conference on Computer
  Vision (ECCV), 2016
- **Journal**: None
- **Summary**: Computer vision methods that quantify the perception of urban environment are increasingly being used to study the relationship between a city's physical appearance and the behavior and health of its residents. Yet, the throughput of current methods is too limited to quantify the perception of cities across the world. To tackle this challenge, we introduce a new crowdsourced dataset containing 110,988 images from 56 cities, and 1,170,000 pairwise comparisons provided by 81,630 online volunteers along six perceptual attributes: safe, lively, boring, wealthy, depressing, and beautiful. Using this data, we train a Siamese-like convolutional neural architecture, which learns from a joint classification and ranking loss, to predict human judgments of pairwise image comparisons. Our results show that crowdsourcing combined with neural networks can produce urban perception data at the global scale.



### Sparse Subspace Clustering via Diffusion Process
- **Arxiv ID**: http://arxiv.org/abs/1608.01793v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.01793v1)
- **Published**: 2016-08-05 08:05:24+00:00
- **Updated**: 2016-08-05 08:05:24+00:00
- **Authors**: Qilin Li, Ling Li, Wanquan Liu
- **Comment**: arXiv admin note: text overlap with arXiv:1203.1005, arXiv:1605.02633
  by other authors
- **Journal**: None
- **Summary**: Subspace clustering refers to the problem of clustering high-dimensional data that lie in a union of low-dimensional subspaces. State-of-the-art subspace clustering methods are based on the idea of expressing each data point as a linear combination of other data points while regularizing the matrix of coefficients with L1, L2 or nuclear norms for a sparse solution. L1 regularization is guaranteed to give a subspace-preserving affinity (i.e., there are no connections between points from different subspaces) under broad theoretical conditions, but the clusters may not be fully connected. L2 and nuclear norm regularization often improve connectivity, but give a subspace-preserving affinity only for independent subspaces. Mixed L1, L2 and nuclear norm regularization could offer a balance between the subspace-preserving and connectedness properties, but this comes at the cost of increased computational complexity. This paper focuses on using L1 norm and alleviating the corresponding connectivity problem by a simple yet efficient diffusion process on subspace affinity graphs. Without adding any tuning parameter , our method can achieve state-of-the-art clustering performance on Hopkins 155 and Extended Yale B data sets.



### SIFT Meets CNN: A Decade Survey of Instance Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1608.01807v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.01807v2)
- **Published**: 2016-08-05 08:50:58+00:00
- **Updated**: 2017-05-23 08:10:33+00:00
- **Authors**: Liang Zheng, Yi Yang, Qi Tian
- **Comment**: Accepted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence
- **Journal**: None
- **Summary**: In the early days, content-based image retrieval (CBIR) was studied with global features. Since 2003, image retrieval based on local descriptors (de facto SIFT) has been extensively studied for over a decade due to the advantage of SIFT in dealing with image transformations. Recently, image representations based on the convolutional neural network (CNN) have attracted increasing interest in the community and demonstrated impressive performance. Given this time of rapid evolution, this article provides a comprehensive survey of instance retrieval over the last decade. Two broad categories, SIFT-based and CNN-based methods, are presented. For the former, according to the codebook size, we organize the literature into using large/medium-sized/small codebooks. For the latter, we discuss three lines of methods, i.e., using pre-trained or fine-tuned CNN models, and hybrid methods. The first two perform a single-pass of an image to the network, while the last category employs a patch-based feature extraction scheme. This survey presents milestones in modern instance retrieval, reviews a broad selection of previous works in different categories, and provides insights on the connection between SIFT and CNN-based methods. After analyzing and comparing retrieval performance of different categories on several datasets, we discuss promising directions towards generic and specialized instance retrieval.



### Compartmental analysis of dynamic nuclear medicine data: regularization procedure and application to physiology
- **Arxiv ID**: http://arxiv.org/abs/1608.01825v1
- **DOI**: 10.1080/17415977.2018.1512603
- **Categories**: **physics.med-ph**, cs.CV, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1608.01825v1)
- **Published**: 2016-08-05 10:15:06+00:00
- **Updated**: 2016-08-05 10:15:06+00:00
- **Authors**: Delbary Fabrice, Garbarino Sara
- **Comment**: None
- **Journal**: Inverse Problems in Science and Engineering 2018
- **Summary**: Compartmental models based on tracer mass balance are extensively used in clinical and pre-clinical nuclear medicine in order to obtain quantitative information on tracer metabolism in the biological tissue. This paper is the second of a series of two that deal with the problem of tracer coefficient estimation via compartmental modelling in an inverse problem framework. While the previous work was devoted to the discussion of identifiability issues for 2, 3 and n-dimension compartmental systems, here we discuss the problem of numerically determining the tracer coefficients by means of a general regularized Multivariate Gauss Newton scheme. In this paper, applications concerning cerebral, hepatic and renal functions are considered, involving experimental measurements on FDG-PET data on different set of murine models.



### Fusing Deep Convolutional Networks for Large Scale Visual Concept Classification
- **Arxiv ID**: http://arxiv.org/abs/1608.01866v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.01866v1)
- **Published**: 2016-08-05 12:50:28+00:00
- **Updated**: 2016-08-05 12:50:28+00:00
- **Authors**: Hilal Ergun, Mustafa Sert
- **Comment**: To appear in The Second IEEE International Conference on Multimedia
  Big Data (IEEE BigMM 2016)
- **Journal**: None
- **Summary**: Deep learning architectures are showing great promise in various computer vision domains including image classification, object detection, event detection and action recognition. In this study, we investigate various aspects of convolutional neural networks (CNNs) from the big data perspective. We analyze recent studies and different network architectures both in terms of running time and accuracy. We present extensive empirical information along with best practices for big data practitioners. Using these best practices we propose efficient fusion mechanisms both for single and multiple network models. We present state-of-the art results on benchmark datasets while keeping computational costs at a lower level. Another contribution of our paper is that these state-of-the-art results can be reached without using extensive data augmentation techniques.



### Blind Deconvolution of PET Images using Anatomical Priors
- **Arxiv ID**: http://arxiv.org/abs/1608.01896v1
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1608.01896v1)
- **Published**: 2016-08-05 14:32:18+00:00
- **Updated**: 2016-08-05 14:32:18+00:00
- **Authors**: StÃ©phanie GuÃ©rit, Adriana GonzÃ¡lez, Anne Bol, John A. Lee, Laurent Jacques
- **Comment**: iTWIST2016, August 24-26, Aalborg, Denmark
- **Journal**: None
- **Summary**: Images from positron emission tomography (PET) provide metabolic information about the human body. They present, however, a spatial resolution that is limited by physical and instrumental factors often modeled by a blurring function. Since this function is typically unknown, blind deconvolution (BD) techniques are needed in order to produce a useful restored PET image. In this work, we propose a general BD technique that restores a low resolution blurry image using information from data acquired with a high resolution modality (e.g., CT-based delineation of regions with uniform activity in PET images). The proposed BD method is validated on synthetic and actual phantoms.



### Enhanced Directional Smoothing Algorithm for Edge-Preserving Smoothing of Synthetic-Aperture Radar Images
- **Arxiv ID**: http://arxiv.org/abs/1608.01993v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.01993v1)
- **Published**: 2016-08-05 18:18:38+00:00
- **Updated**: 2016-08-05 18:18:38+00:00
- **Authors**: Mario Mastriani, Alberto E. Giraldez
- **Comment**: 10 pages, 3 figures, 1 table. arXiv admin note: text overlap with
  arXiv:1608.00277, arXiv:1608.00273, arXiv:1608.00270, arXiv:1608.00279,
  arXiv:1608.00274
- **Journal**: None
- **Summary**: Synthetic aperture radar (SAR) images are subject to prominent speckle noise, which is generally considered a purely multiplicative noise process. In theory, this multiplicative noise is that the ratio of the standard deviation to the signal value, the "coefficient of variation," is theoretically constant at every point in a SAR image. Most of the filters for speckle reduction are based on this property. Such property is irrelevant for the new filter structure, which is based on directional smoothing (DS) theory, the enhanced directional smoothing (EDS) that removes speckle noise from SAR images without blurring edges. We demonstrate the effectiveness of this new filtering method by comparing it to established speckle noise removal techniques on SAR images.



### OpenCL-accelerated object classification in video streams using Spatial Pooler of Hierarchical Temporal Memory
- **Arxiv ID**: http://arxiv.org/abs/1608.01966v1
- **DOI**: 10.14569/IJACSA.2017.080245
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.01966v1)
- **Published**: 2016-08-05 18:25:21+00:00
- **Updated**: 2016-08-05 18:25:21+00:00
- **Authors**: Maciej Wielgosz, Marcin PietroÅ
- **Comment**: Submitted to Journal of Circuits, Systems, and Computers (JCSC)
- **Journal**: International Journal of Advanced Computer Science and
  Applications (IJACSA), 8 (2), 2017
- **Summary**: We present a method to classify objects in video streams using a brain-inspired Hierarchical Temporal Memory (HTM) algorithm. Object classification is a challenging task where humans still significantly outperform machine learning algorithms due to their unique capabilities. We have implemented a system which achieves very promising performance in terms of recognition accuracy. Unfortunately, conducting more advanced experiments is very computationally demanding; some of the trials run on a standard CPU may take as long as several days for 960x540 video streams frames. Therefore we have decided to accelerate selected parts of the system using OpenCL. In particular, we seek to determine to what extent porting selected and computationally demanding parts of a core may speed up calculations.   The classification accuracy of the system was examined through a series of experiments and the performance was given in terms of F1 score as a function of the number of columns, synapses, $min\_overlap$ and $winners\_set\_size$. The system achieves the highest F1 score of 0.95 and 0.91 for $min\_overlap=4$ and 256 synapses, respectively. We have also conduced a series of experiments with different hardware setups and measured CPU/GPU acceleration. The best kernel speed-up of 632x and 207x was reached for 256 synapses and 1024 columns. However, overall acceleration including transfer time was significantly lower and amounted to 6.5x and 3.2x for the same setup.



### Identifying Designs from Incomplete, Fragmented Cultural Heritage Objects by Curve-Pattern Matching
- **Arxiv ID**: http://arxiv.org/abs/1608.02023v2
- **DOI**: 10.1117/1.JEI.26.1.011022
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.02023v2)
- **Published**: 2016-08-05 21:09:53+00:00
- **Updated**: 2017-01-05 21:40:03+00:00
- **Authors**: Jun Zhou, Haozhou Yu, Karen Smith, Colin Wilder, Hongkai Yu, Song Wang
- **Comment**: In Journal of Electronic Imaging, 2017
- **Journal**: None
- **Summary**: Study of cultural-heritage objects with embellished realistic and abstract designs made up of connected and intertwined curves crosscuts a number of related disciplines, including archaeology, art history, and heritage management. However, many objects, such as pottery sherds found in the archaeological record, are fragmentary, making the underlying complete designs unknowable at the scale of the sherd fragment. The challenge to reconstruct and study complete designs is stymied because 1) most fragmentary cultural-heritage objects contain only a small portion of the underlying full design, 2) in the case of a stamping application, the same design may be applied multiple times with spatial overlap on one object, and 3) curve patterns detected on an object are usually incomplete and noisy. As a result, classical curve-pattern matching algorithms, such as Chamfer matching, may perform poorly in identifying the underlying design. In this paper, we develop a new partial-to-global curve matching algorithm to address these challenges and better identify the full design from a fragmented cultural heritage object. Specifically, we develop the algorithm to identify the designs of the carved wooden paddles of the Southeastern Woodlands from unearthed pottery sherds. A set of pottery sherds from the Snow Collection, curated at Georgia Southern University, are used to test the proposed algorithm, with promising results.



### Photometric Bundle Adjustment for Vision-Based SLAM
- **Arxiv ID**: http://arxiv.org/abs/1608.02026v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1608.02026v1)
- **Published**: 2016-08-05 21:27:11+00:00
- **Updated**: 2016-08-05 21:27:11+00:00
- **Authors**: Hatem Alismail, Brett Browning, Simon Lucey
- **Comment**: Under review
- **Journal**: None
- **Summary**: We propose a novel algorithm for the joint refinement of structure and motion parameters from image data directly without relying on fixed and known correspondences. In contrast to traditional bundle adjustment (BA) where the optimal parameters are determined by minimizing the reprojection error using tracked features, the proposed algorithm relies on maximizing the photometric consistency and estimates the correspondences implicitly. Since the proposed algorithm does not require correspondences, its application is not limited to corner-like structure; any pixel with nonvanishing gradient could be used in the estimation process. Furthermore, we demonstrate the feasibility of refining the motion and structure parameters simultaneously using the photometric in unconstrained scenes and without requiring restrictive assumptions such as planarity. The proposed algorithm is evaluated on range of challenging outdoor datasets, and it is shown to improve upon the accuracy of the state-of-the-art VSLAM methods obtained using the minimization of the reprojection error using traditional BA as well as loop closure.



