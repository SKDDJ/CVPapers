# Arxiv Papers in cs.CV on 2016-08-22
### Domain Separation Networks
- **Arxiv ID**: http://arxiv.org/abs/1608.06019v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.06019v1)
- **Published**: 2016-08-22 00:12:27+00:00
- **Updated**: 2016-08-22 00:12:27+00:00
- **Authors**: Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, Dumitru Erhan
- **Comment**: This work will be presented at NIPS 2016
- **Journal**: None
- **Summary**: The cost of large scale data collection and annotation often makes the application of machine learning algorithms to new tasks or datasets prohibitively expensive. One approach circumventing this cost is training models on synthetic data where annotations are provided automatically. Despite their appeal, such models often fail to generalize from synthetic to real images, necessitating domain adaptation algorithms to manipulate these models before they can be successfully applied. Existing approaches focus either on mapping representations from one domain to the other, or on learning to extract features that are invariant to the domain from which they were extracted. However, by focusing only on creating a mapping or shared representation between the two domains, they ignore the individual characteristics of each domain. We suggest that explicitly modeling what is unique to each domain can improve a model's ability to extract domain-invariant features. Inspired by work on private-shared component analysis, we explicitly learn to extract image representations that are partitioned into two subspaces: one component which is private to each domain and one which is shared across domains. Our model is trained not only to perform the task we care about in the source domain, but also to use the partitioned representation to reconstruct the images from both domains. Our novel architecture results in a model that outperforms the state-of-the-art on a range of unsupervised domain adaptation scenarios and additionally produces visualizations of the private and shared representations enabling interpretation of the domain adaptation process.



### Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures
- **Arxiv ID**: http://arxiv.org/abs/1608.06037v8
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1608.06037v8)
- **Published**: 2016-08-22 02:50:57+00:00
- **Updated**: 2023-04-27 16:20:03+00:00
- **Authors**: Seyyed Hossein Hasanpour, Mohammad Rouhani, Mohsen Fayyaz, Mohammad Sabokrou
- **Comment**: Added the long-overdue ImageNet results and updated the missed
  cifar10/100 results from 2018
- **Journal**: None
- **Summary**: Major winning Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet, ResNet, GoogleNet, include tens to hundreds of millions of parameters, which impose considerable computation and memory overhead. This limits their practical use for training, optimization and memory efficiency. On the contrary, light-weight architectures, being proposed to address this issue, mainly suffer from low accuracy. These inefficiencies mostly stem from following an ad hoc procedure. We propose a simple architecture, called SimpleNet, based on a set of designing principles, with which we empirically show, a well-crafted yet simple and reasonably deep architecture can perform on par with deeper and more complex architectures. SimpleNet provides a good tradeoff between the computation/memory efficiency and the accuracy. Our simple 13-layer architecture outperforms most of the deeper and complex architectures to date such as VGGNet, ResNet, and GoogleNet on several well-known benchmarks while having 2 to 25 times fewer number of parameters and operations. This makes it very handy for embedded systems or systems with computational and memory limitations. We achieved state-of-the-art result on CIFAR10 outperforming several heavier architectures, near state of the art on MNIST and competitive results on CIFAR100 and SVHN. We also outperformed the much larger and deeper architectures such as VGGNet and popular variants of ResNets among others on the ImageNet dataset. Models are made available at: https://github.com/Coderx7/SimpleNet



### Local Binary Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1608.06049v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1608.06049v2)
- **Published**: 2016-08-22 04:32:21+00:00
- **Updated**: 2017-07-01 17:02:44+00:00
- **Authors**: Felix Juefei-Xu, Vishnu Naresh Boddeti, Marios Savvides
- **Comment**: To appear in CVPR 2017 as Spotlight
- **Journal**: None
- **Summary**: We propose local binary convolution (LBC), an efficient alternative to convolutional layers in standard convolutional neural networks (CNN). The design principles of LBC are motivated by local binary patterns (LBP). The LBC layer comprises of a set of fixed sparse pre-defined binary convolutional filters that are not updated during the training process, a non-linear activation function and a set of learnable linear weights. The linear weights combine the activated filter responses to approximate the corresponding activated filter responses of a standard convolutional layer. The LBC layer affords significant parameter savings, 9x to 169x in the number of learnable parameters compared to a standard convolutional layer. Furthermore, the sparse and binary nature of the weights also results in up to 9x to 169x savings in model size compared to a standard convolutional layer. We demonstrate both theoretically and experimentally that our local binary convolution layer is a good approximation of a standard convolutional layer. Empirically, CNNs with LBC layers, called local binary convolutional neural networks (LBCNN), achieves performance parity with regular CNNs on a range of visual datasets (MNIST, SVHN, CIFAR-10, and ImageNet) while enjoying significant computational savings.



### Multiple objects tracking in surveillance video using color and Hu moments
- **Arxiv ID**: http://arxiv.org/abs/1608.06148v2
- **DOI**: 10.5121/sipij.2016.7302
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.06148v2)
- **Published**: 2016-08-22 12:42:46+00:00
- **Updated**: 2016-08-28 13:11:35+00:00
- **Authors**: Chandrajit M, Girisha R, Vasudev T
- **Comment**: 13 pages, Signal & Image Processing : An International Journal
  (SIPIJ) Vol.7, No.3, June 2016
- **Journal**: None
- **Summary**: Multiple objects tracking finds its applications in many high level vision analysis like object behaviour interpretation and gait recognition. In this paper, a feature based method to track the multiple moving objects in surveillance video sequence is proposed. Object tracking is done by extracting the color and Hu moments features from the motion segmented object blob and establishing the association of objects in the successive frames of the video sequence based on Chi-Square dissimilarity measure and nearest neighbor classifier. The benchmark IEEE PETS and IEEE Change Detection datasets has been used to show the robustness of the proposed method. The proposed method is assessed quantitatively using the precision and recall accuracy metrics. Further, comparative evaluation with related works has been carried out to exhibit the efficacy of the proposed method.



### Efficient Continuous Relaxations for Dense CRF
- **Arxiv ID**: http://arxiv.org/abs/1608.06192v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.06192v1)
- **Published**: 2016-08-22 15:24:25+00:00
- **Updated**: 2016-08-22 15:24:25+00:00
- **Authors**: Alban Desmaison, Rudy Bunel, Pushmeet Kohli, Philip H. S. Torr, M. Pawan Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: Dense conditional random fields (CRF) with Gaussian pairwise potentials have emerged as a popular framework for several computer vision applications such as stereo correspondence and semantic segmentation. By modeling long-range interactions, dense CRFs provide a more detailed labelling compared to their sparse counterparts. Variational inference in these dense models is performed using a filtering-based mean-field algorithm in order to obtain a fully-factorized distribution minimising the Kullback-Leibler divergence to the true distribution. In contrast to the continuous relaxation-based energy minimisation algorithms used for sparse CRFs, the mean-field algorithm fails to provide strong theoretical guarantees on the quality of its solutions. To address this deficiency, we show that it is possible to use the same filtering approach to speed-up the optimisation of several continuous relaxations. Specifically, we solve a convex quadratic programming (QP) relaxation using the efficient Frank-Wolfe algorithm. This also allows us to solve difference-of-convex relaxations via the iterative concave-convex procedure where each iteration requires solving a convex QP. Finally, we develop a novel divide-and-conquer method to compute the subgradients of a linear programming relaxation that provides the best theoretical bounds for energy minimisation. We demonstrate the advantage of continuous relaxations over the widely used mean-field algorithm on publicly available datasets.



### CrowdNet: A Deep Convolutional Network for Dense Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/1608.06197v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.06197v1)
- **Published**: 2016-08-22 15:43:29+00:00
- **Updated**: 2016-08-22 15:43:29+00:00
- **Authors**: Lokesh Boominathan, Srinivas S S Kruthiventi, R. Venkatesh Babu
- **Comment**: Accepted at ACM Multimedia (MM) 2016
- **Journal**: None
- **Summary**: Our work proposes a novel deep learning framework for estimating crowd density from static images of highly dense crowds. We use a combination of deep and shallow, fully convolutional networks to predict the density map for a given crowd image. Such a combination is used for effectively capturing both the high-level semantic information (face/body detectors) and the low-level features (blob detectors), that are necessary for crowd counting under large scale variations. As most crowd datasets have limited training samples (<100 images) and deep learning based approaches require large amounts of training data, we perform multi-scale data augmentation. Augmenting the training samples in such a manner helps in guiding the CNN to learn scale invariant representations. Our method is tested on the challenging UCF_CC_50 dataset, and shown to outperform the state of the art methods.



### Large-scale Continuous Gesture Recognition Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1608.06338v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.06338v2)
- **Published**: 2016-08-22 23:44:34+00:00
- **Updated**: 2016-09-12 11:11:52+00:00
- **Authors**: Pichao Wang, Wanqing Li, Song Liu, Yuyao Zhang, Zhimin Gao, Philip Ogunbona
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of continuous gesture recognition from sequences of depth maps using convolutional neutral networks (ConvNets). The proposed method first segments individual gestures from a depth sequence based on quantity of movement (QOM). For each segmented gesture, an Improved Depth Motion Map (IDMM), which converts the depth sequence into one image, is constructed and fed to a ConvNet for recognition. The IDMM effectively encodes both spatial and temporal information and allows the fine-tuning with existing ConvNet models for classification without introducing millions of parameters to learn. The proposed method is evaluated on the Large-scale Continuous Gesture Recognition of the ChaLearn Looking at People (LAP) challenge 2016. It achieved the performance of 0.2655 (Mean Jaccard Index) and ranked $3^{rd}$ place in this challenge.



