# Arxiv Papers in cs.CV on 2016-08-15
### Every Filter Extracts A Specific Texture In Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1608.04170v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.04170v2)
- **Published**: 2016-08-15 02:47:23+00:00
- **Updated**: 2016-08-18 02:56:25+00:00
- **Authors**: Zhiqiang Xia, Ce Zhu, Zhengtao Wang, Qi Guo, Yipeng Liu
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: Many works have concentrated on visualizing and understanding the inner mechanism of convolutional neural networks (CNNs) by generating images that activate some specific neurons, which is called deep visualization. However, it is still unclear what the filters extract from images intuitively. In this paper, we propose a modified code inversion algorithm, called feature map inversion, to understand the function of filter of interest in CNNs. We reveal that every filter extracts a specific texture. The texture from higher layer contains more colours and more intricate structures. We also demonstrate that style of images could be a combination of these texture primitives. Two methods are proposed to reallocate energy distribution of feature maps randomly and purposefully. Then, we inverse the modified code and generate images of diverse styles. With these results, we provide an explanation about why Gram matrix of feature maps \cite{Gatys_2016_CVPR} could represent image style.



### Occlusion-Model Guided Anti-Occlusion Depth Estimation in Light Field
- **Arxiv ID**: http://arxiv.org/abs/1608.04187v2
- **DOI**: 10.1109/JSTSP.2017.2730818
- **Categories**: **cs.CV**, I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1608.04187v2)
- **Published**: 2016-08-15 06:21:24+00:00
- **Updated**: 2016-08-18 06:03:09+00:00
- **Authors**: Hao Zhu, Qing Wang, Jingyi Yu
- **Comment**: 19 pages, 13 figures, pdflatex
- **Journal**: None
- **Summary**: Occlusion is one of the most challenging problems in depth estimation. Previous work has modeled the single-occluder occlusion in light field and get good results, however it is still difficult to obtain accurate depth for multi-occluder occlusion. In this paper, we explore the multi-occluder occlusion model in light field, and derive the occluder-consistency between the spatial and angular space which is used as a guidance to select the un-occluded views for each candidate occlusion point. Then an anti-occlusion energy function is built to regularize depth map. The experimental results on public light field datasets have demonstrated the advantages of the proposed algorithm compared with other state-of-the-art light field depth estimation algorithms, especially in multi-occluder areas.



### Face Alignment In-the-Wild: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1608.04188v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.04188v1)
- **Published**: 2016-08-15 06:32:07+00:00
- **Updated**: 2016-08-15 06:32:07+00:00
- **Authors**: Xin Jin, Xiaoyang Tan
- **Comment**: None
- **Journal**: None
- **Summary**: Over the last two decades, face alignment or localizing fiducial facial points has received increasing attention owing to its comprehensive applications in automatic face analysis. However, such a task has proven extremely challenging in unconstrained environments due to many confounding factors, such as pose, occlusions, expression and illumination. While numerous techniques have been developed to address these challenges, this problem is still far away from being solved. In this survey, we present an up-to-date critical review of the existing literatures on face alignment, focusing on those methods addressing overall difficulties and challenges of this topic under uncontrolled conditions. Specifically, we categorize existing face alignment techniques, present detailed descriptions of the prominent algorithms within each category, and discuss their advantages and disadvantages. Furthermore, we organize special discussions on the practical aspects of face alignment in-the-wild, towards the development of a robust face alignment system. In addition, we show performance statistics of the state of the art, and conclude this paper with several promising directions for future research.



### Cross Euclidean-to-Riemannian Metric Learning with Application to Face Recognition from Video
- **Arxiv ID**: http://arxiv.org/abs/1608.04200v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.04200v2)
- **Published**: 2016-08-15 07:54:55+00:00
- **Updated**: 2017-01-06 22:01:26+00:00
- **Authors**: Zhiwu Huang, Ruiping Wang, Shiguang Shan, Luc Van Gool, Xilin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Riemannian manifolds have been widely employed for video representations in visual classification tasks including video-based face recognition. The success mainly derives from learning a discriminant Riemannian metric which encodes the non-linear geometry of the underlying Riemannian manifolds. In this paper, we propose a novel metric learning framework to learn a distance metric across a Euclidean space and a Riemannian manifold to fuse the average appearance and pattern variation of faces within one video. The proposed metric learning framework can handle three typical tasks of video-based face recognition: Video-to-Still, Still-to-Video and Video-to-Video settings. To accomplish this new framework, by exploiting typical Riemannian geometries for kernel embedding, we map the source Euclidean space and Riemannian manifold into a common Euclidean subspace, each through a corresponding high-dimensional Reproducing Kernel Hilbert Space (RKHS). With this mapping, the problem of learning a cross-view metric between the two source heterogeneous spaces can be expressed as learning a single-view Euclidean distance metric in the target common Euclidean space. By learning information on heterogeneous data with the shared label, the discriminant metric in the common space improves face recognition from videos. Extensive experiments on four challenging video face databases demonstrate that the proposed framework has a clear advantage over the state-of-the-art methods in the three classical video-based face recognition tasks.



### Generating Synthetic Data for Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1608.04224v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.04224v1)
- **Published**: 2016-08-15 10:13:46+00:00
- **Updated**: 2016-08-15 10:13:46+00:00
- **Authors**: Praveen Krishnan, C. V. Jawahar
- **Comment**: 5 pages, 2 figures
- **Journal**: None
- **Summary**: Generating synthetic images is an art which emulates the natural process of image generation in a closest possible manner. In this work, we exploit such a framework for data generation in handwritten domain. We render synthetic data using open source fonts and incorporate data augmentation schemes. As part of this work, we release 9M synthetic handwritten word image corpus which could be useful for training deep network architectures and advancing the performance in handwritten word spotting and recognition tasks.



### A Riemannian Network for SPD Matrix Learning
- **Arxiv ID**: http://arxiv.org/abs/1608.04233v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.04233v2)
- **Published**: 2016-08-15 11:00:16+00:00
- **Updated**: 2016-12-22 15:43:55+00:00
- **Authors**: Zhiwu Huang, Luc Van Gool
- **Comment**: Revised arXiv version, AAAI-17 camera-ready
- **Journal**: None
- **Summary**: Symmetric Positive Definite (SPD) matrix learning methods have become popular in many image and video processing tasks, thanks to their ability to learn appropriate statistical representations while respecting Riemannian geometry of underlying SPD manifolds. In this paper we build a Riemannian network architecture to open up a new direction of SPD matrix non-linear learning in a deep model. In particular, we devise bilinear mapping layers to transform input SPD matrices to more desirable SPD matrices, exploit eigenvalue rectification layers to apply a non-linear activation function to the new SPD matrices, and design an eigenvalue logarithm layer to perform Riemannian computing on the resulting SPD matrices for regular output layers. For training the proposed deep network, we exploit a new backpropagation with a variant of stochastic gradient descent on Stiefel manifolds to update the structured connection weights and the involved SPD matrix data. We show through experiments that the proposed SPD matrix network can be simply trained and outperform existing SPD matrix learning and state-of-the-art methods in three typical visual classification tasks.



### Generative and Discriminative Voxel Modeling with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1608.04236v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1608.04236v2)
- **Published**: 2016-08-15 11:14:35+00:00
- **Updated**: 2016-08-16 08:06:24+00:00
- **Authors**: Andrew Brock, Theodore Lim, J. M. Ritchie, Nick Weston
- **Comment**: 9 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: When working with three-dimensional data, choice of representation is key. We explore voxel-based models, and present evidence for the viability of voxellated representations in applications including shape modeling and object classification. Our key contributions are methods for training voxel-based variational autoencoders, a user interface for exploring the latent space learned by the autoencoder, and a deep convolutional neural network architecture for object classification. We address challenges unique to voxel-based representations, and empirically evaluate our models on the ModelNet benchmark, where we demonstrate a 51.5% relative improvement in the state of the art for object classification.



### Detecting Dominant Vanishing Points in Natural Scenes with Application to Composition-Sensitive Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1608.04267v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1608.04267v2)
- **Published**: 2016-08-15 13:48:22+00:00
- **Updated**: 2017-05-13 14:58:05+00:00
- **Authors**: Zihan Zhou, Farshid Farhat, James Z. Wang
- **Comment**: 15 pages, 18 figures, to appear in IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Linear perspective is widely used in landscape photography to create the impression of depth on a 2D photo. Automated understanding of linear perspective in landscape photography has several real-world applications, including aesthetics assessment, image retrieval, and on-site feedback for photo composition, yet adequate automated understanding has been elusive. We address this problem by detecting the dominant vanishing point and the associated line structures in a photo. However, natural landscape scenes pose great technical challenges because often the inadequate number of strong edges converging to the dominant vanishing point is inadequate. To overcome this difficulty, we propose a novel vanishing point detection method that exploits global structures in the scene via contour detection. We show that our method significantly outperforms state-of-the-art methods on a public ground truth landscape image dataset that we have created. Based on the detection results, we further demonstrate how our approach to linear perspective understanding provides on-site guidance to amateur photographers on their work through a novel viewpoint-specific image retrieval system.



### Visual place recognition using landmark distribution descriptors
- **Arxiv ID**: http://arxiv.org/abs/1608.04274v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.04274v1)
- **Published**: 2016-08-15 14:13:27+00:00
- **Updated**: 2016-08-15 14:13:27+00:00
- **Authors**: Pilailuck Panphattarasap, Andrew Calway
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Recent work by Suenderhauf et al. [1] demonstrated improved visual place recognition using proposal regions coupled with features from convolutional neural networks (CNN) to match landmarks between views. In this work we extend the approach by introducing descriptors built from landmark features which also encode the spatial distribution of the landmarks within a view. Matching descriptors then enforces consistency of the relative positions of landmarks between views. This has a significant impact on performance. For example, in experiments on 10 image-pair datasets, each consisting of 200 urban locations with significant differences in viewing positions and conditions, we recorded average precision of around 70% (at 100% recall), compared with 58% obtained using whole image CNN features and 50% for the method in [1].



### Transitive Hashing Network for Heterogeneous Multimedia Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1608.04307v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.04307v1)
- **Published**: 2016-08-15 15:36:41+00:00
- **Updated**: 2016-08-15 15:36:41+00:00
- **Authors**: Zhangjie Cao, Mingsheng Long, Qiang Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Hashing has been widely applied to large-scale multimedia retrieval due to the storage and retrieval efficiency. Cross-modal hashing enables efficient retrieval from database of one modality in response to a query of another modality. Existing work on cross-modal hashing assumes heterogeneous relationship across modalities for hash function learning. In this paper, we relax the strong assumption by only requiring such heterogeneous relationship in an auxiliary dataset different from the query/database domain. We craft a hybrid deep architecture to simultaneously learn the cross-modal correlation from the auxiliary dataset, and align the dataset distributions between the auxiliary dataset and the query/database domain, which generates transitive hash codes for heterogeneous multimedia retrieval. Extensive experiments exhibit that the proposed approach yields state of the art multimedia retrieval performance on public datasets, i.e. NUS-WIDE, ImageNet-YahooQA.



### Weakly Supervised Object Localization Using Size Estimates
- **Arxiv ID**: http://arxiv.org/abs/1608.04314v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.04314v2)
- **Published**: 2016-08-15 16:07:24+00:00
- **Updated**: 2016-08-16 11:31:41+00:00
- **Authors**: Miaojing Shi, Vittorio Ferrari
- **Comment**: ECCV 2016 camera-ready
- **Journal**: None
- **Summary**: We present a technique for weakly supervised object localization (WSOL), building on the observation that WSOL algorithms usually work better on images with bigger objects. Instead of training the object detector on the entire training set at the same time, we propose a curriculum learning strategy to feed training images into the WSOL learning loop in an order from images containing bigger objects down to smaller ones. To automatically determine the order, we train a regressor to estimate the size of the object given the whole image as input. Furthermore, we use these size estimates to further improve the re-localization step of WSOL by assigning weights to object proposals according to how close their size matches the estimated object size. We demonstrate the effectiveness of using size order and size weighting on the challenging PASCAL VOC 2007 dataset, where we achieve a significant improvement over existing state-of-the-art WSOL techniques.



### Design of Efficient Convolutional Layers using Single Intra-channel Convolution, Topological Subdivisioning and Spatial "Bottleneck" Structure
- **Arxiv ID**: http://arxiv.org/abs/1608.04337v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.04337v2)
- **Published**: 2016-08-15 17:35:56+00:00
- **Updated**: 2017-01-24 12:26:19+00:00
- **Authors**: Min Wang, Baoyuan Liu, Hassan Foroosh
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks achieve remarkable visual recognition performance, at the cost of high computational complexity. In this paper, we have a new design of efficient convolutional layers based on three schemes. The 3D convolution operation in a convolutional layer can be considered as performing spatial convolution in each channel and linear projection across channels simultaneously. By unravelling them and arranging the spatial convolution sequentially, the proposed layer is composed of a single intra-channel convolution, of which the computation is negligible, and a linear channel projection. A topological subdivisioning is adopted to reduce the connection between the input channels and output channels. Additionally, we also introduce a spatial "bottleneck" structure that utilizes a convolution-projection-deconvolution pipeline to take advantage of the correlation between adjacent pixels in the input. Our experiments demonstrate that the proposed layers remarkably outperform the standard convolutional layers with regard to accuracy/complexity ratio. Our models achieve similar accuracy to VGG, ResNet-50, ResNet-101 while requiring 42, 4.5, 6.5 times less computation respectively.



### Depth2Action: Exploring Embedded Depth for Large-Scale Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1608.04339v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.04339v1)
- **Published**: 2016-08-15 17:42:36+00:00
- **Updated**: 2016-08-15 17:42:36+00:00
- **Authors**: Yi Zhu, Shawn Newsam
- **Comment**: ECCVW 2016, Web-scale Vision and Social Media (VSM) workshop
- **Journal**: None
- **Summary**: This paper performs the first investigation into depth for large-scale human action recognition in video where the depth cues are estimated from the videos themselves. We develop a new framework called depth2action and experiment thoroughly into how best to incorporate the depth information. We introduce spatio-temporal depth normalization (STDN) to enforce temporal consistency in our estimated depth sequences. We also propose modified depth motion maps (MDMM) to capture the subtle temporal changes in depth. These two components significantly improve the action recognition performance. We evaluate our depth2action framework on three large-scale action recognition video benchmarks. Our model achieves state-of-the-art performance when combined with appearance and motion information thus demonstrating that depth2action is indeed complementary to existing approaches.



### Intrinsic Light Field Images
- **Arxiv ID**: http://arxiv.org/abs/1608.04342v2
- **DOI**: 10.1111/cgf.13154
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.04342v2)
- **Published**: 2016-08-15 17:50:16+00:00
- **Updated**: 2017-04-12 15:22:59+00:00
- **Authors**: Elena Garces, Jose I. Echevarria, Wen Zhang, Hongzhi Wu, Kun Zhou, Diego Gutierrez
- **Comment**: None
- **Journal**: Computer Graphics Forum 2017
- **Summary**: We present a method to automatically decompose a light field into its intrinsic shading and albedo components. Contrary to previous work targeted to 2D single images and videos, a light field is a 4D structure that captures non-integrated incoming radiance over a discrete angular domain. This higher dimensionality of the problem renders previous state-of-the-art algorithms impractical either due to their cost of processing a single 2D slice, or their inability to enforce proper coherence in additional dimensions. We propose a new decomposition algorithm that jointly optimizes the whole light field data for proper angular coherence. For efficiency, we extend Retinex theory, working on the gradient domain, where new albedo and occlusion terms are introduced. Results show our method provides 4D intrinsic decompositions difficult to achieve with previous state-of-the-art algorithms. We further provide a comprehensive analysis and comparisons with existing intrinsic image/video decomposition methods on light field images.



### Anomaly detection and classification for streaming data using PDEs
- **Arxiv ID**: http://arxiv.org/abs/1608.04348v2
- **DOI**: 10.1137/17M1121184
- **Categories**: **cs.LG**, cs.CV, cs.DB, 35D40, 49L25, 65N06, 06A07, 35F21, 68Q87, I.5; G.3; H.2.8
- **Links**: [PDF](http://arxiv.org/pdf/1608.04348v2)
- **Published**: 2016-08-15 18:03:51+00:00
- **Updated**: 2017-03-15 19:50:07+00:00
- **Authors**: Bilal Abbasi, Jeff Calder, Adam M. Oberman
- **Comment**: None
- **Journal**: SIAM Journal on Applied Math, 78(2), 921--941, 2018
- **Summary**: Nondominated sorting, also called Pareto Depth Analysis (PDA), is widely used in multi-objective optimization and has recently found important applications in multi-criteria anomaly detection. Recently, a partial differential equation (PDE) continuum limit was discovered for nondominated sorting leading to a very fast approximate sorting algorithm called PDE-based ranking. We propose in this paper a fast real-time streaming version of the PDA algorithm for anomaly detection that exploits the computational advantages of PDE continuum limits. Furthermore, we derive new PDE continuum limits for sorting points within their nondominated layers and show how the new PDEs can be used to classify anomalies based on which criterion was more significantly violated. We also prove statistical convergence rates for PDE-based ranking, and present the results of numerical experiments with both synthetic and real data.



### Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification
- **Arxiv ID**: http://arxiv.org/abs/1608.04363v2
- **DOI**: 10.1109/LSP.2017.2657381
- **Categories**: **cs.SD**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1608.04363v2)
- **Published**: 2016-08-15 18:57:10+00:00
- **Updated**: 2016-11-28 17:48:04+00:00
- **Authors**: Justin Salamon, Juan Pablo Bello
- **Comment**: Accepted November 2016, IEEE Signal Processing Letters. Copyright
  IEEE. Personal use of this material is permitted. Permission from IEEE must
  be obtained for all other uses, in any current or future media, including
  reprinting/republishing this material, creating new collective works, for
  resale or redistribution, or reuse of any copyrighted component of this work
  in other works
- **Journal**: None
- **Summary**: The ability of deep convolutional neural networks (CNN) to learn discriminative spectro-temporal patterns makes them well suited to environmental sound classification. However, the relative scarcity of labeled data has impeded the exploitation of this family of high-capacity models. This study has two primary contributions: first, we propose a deep convolutional neural network architecture for environmental sound classification. Second, we propose the use of audio data augmentation for overcoming the problem of data scarcity and explore the influence of different augmentations on the performance of the proposed CNN architecture. Combined with data augmentation, the proposed model produces state-of-the-art results for environmental sound classification. We show that the improved performance stems from the combination of a deep, high-capacity model and an augmented training set: this combination outperforms both the proposed CNN without augmentation and a "shallow" dictionary learning model with augmentation. Finally, we examine the influence of each augmentation on the model's classification accuracy for each class, and observe that the accuracy for each class is influenced differently by each augmentation, suggesting that the performance of the model could be improved further by applying class-conditional data augmentation.



### Star-galaxy Classification Using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1608.04369v2
- **DOI**: 10.1093/mnras/stw2672
- **Categories**: **astro-ph.IM**, astro-ph.CO, astro-ph.GA, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1608.04369v2)
- **Published**: 2016-08-15 19:21:58+00:00
- **Updated**: 2016-10-13 15:03:38+00:00
- **Authors**: Edward J. Kim, Robert J. Brunner
- **Comment**: 13 page, 13 figures. Accepted for publication in the MNRAS. Code
  available at https://github.com/EdwardJKim/dl4astro
- **Journal**: None
- **Summary**: Most existing star-galaxy classifiers use the reduced summary information from catalogs, requiring careful feature extraction and selection. The latest advances in machine learning that use deep convolutional neural networks allow a machine to automatically learn the features directly from data, minimizing the need for input from human experts. We present a star-galaxy classification framework that uses deep convolutional neural networks (ConvNets) directly on the reduced, calibrated pixel values. Using data from the Sloan Digital Sky Survey (SDSS) and the Canada-France-Hawaii Telescope Lensing Survey (CFHTLenS), we demonstrate that ConvNets are able to produce accurate and well-calibrated probabilistic classifications that are competitive with conventional machine learning techniques. Future advances in deep learning may bring more success with current and forthcoming photometric surveys, such as the Dark Energy Survey (DES) and the Large Synoptic Survey Telescope (LSST), because deep neural networks require very little, manual feature engineering.



