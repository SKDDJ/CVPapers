# Arxiv Papers in cs.CV on 2016-08-25
### Densely Connected Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1608.06993v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1608.06993v5)
- **Published**: 2016-08-25 00:44:55+00:00
- **Updated**: 2018-01-28 17:12:02+00:00
- **Authors**: Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .



### Ambient Sound Provides Supervision for Visual Learning
- **Arxiv ID**: http://arxiv.org/abs/1608.07017v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.07017v2)
- **Published**: 2016-08-25 04:50:16+00:00
- **Updated**: 2016-12-05 19:14:26+00:00
- **Authors**: Andrew Owens, Jiajun Wu, Josh H. McDermott, William T. Freeman, Antonio Torralba
- **Comment**: ECCV 2016
- **Journal**: None
- **Summary**: The sound of crashing waves, the roar of fast-moving cars -- sound conveys important information about the objects in our surroundings. In this work, we show that ambient sounds can be used as a supervisory signal for learning visual models. To demonstrate this, we train a convolutional neural network to predict a statistical summary of the sound associated with a video frame. We show that, through this process, the network learns a representation that conveys information about objects and scenes. We evaluate this representation on several recognition tasks, finding that its performance is comparable to that of other state-of-the-art unsupervised learning methods. Finally, we show through visualizations that the network learns units that are selective to objects that are often associated with characteristic sounds.



### Title Generation for User Generated Videos
- **Arxiv ID**: http://arxiv.org/abs/1608.07068v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1608.07068v2)
- **Published**: 2016-08-25 09:49:23+00:00
- **Updated**: 2016-09-08 17:36:13+00:00
- **Authors**: Kuo-Hao Zeng, Tseng-Hung Chen, Juan Carlos Niebles, Min Sun
- **Comment**: 14 pages, 4 figures, ECCV2016
- **Journal**: None
- **Summary**: A great video title describes the most salient event compactly and captures the viewer's attention. In contrast, video captioning tends to generate sentences that describe the video as a whole. Although generating a video title automatically is a very useful task, it is much less addressed than video captioning. We address video title generation for the first time by proposing two methods that extend state-of-the-art video captioners to this new task. First, we make video captioners highlight sensitive by priming them with a highlight detector. Our framework allows for jointly training a model for title generation and video highlight localization. Second, we induce high sentence diversity in video captioners, so that the generated titles are also diverse and catchy. This means that a large number of sentences might be required to learn the sentence structure of titles. Hence, we propose a novel sentence augmentation method to train a captioner with additional sentence-only examples that come without corresponding videos. We collected a large-scale Video Titles in the Wild (VTW) dataset of 18100 automatically crawled user-generated videos and titles. On VTW, our methods consistently improve title prediction accuracy, and achieve the best performance in both automatic and human evaluation. Finally, our sentence augmentation method also outperforms the baselines on the M-VAD dataset.



### Sympathy for the Details: Dense Trajectories and Hybrid Classification Architectures for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1608.07138v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.07138v1)
- **Published**: 2016-08-25 13:37:15+00:00
- **Updated**: 2016-08-25 13:37:15+00:00
- **Authors**: César Roberto de Souza, Adrien Gaidon, Eleonora Vig, Antonio Manuel López
- **Comment**: Accepted for publication in the 14th European Conference on Computer
  Vision (ECCV), Amsterdam, 2016, plus supplementary material
- **Journal**: None
- **Summary**: Action recognition in videos is a challenging task due to the complexity of the spatio-temporal patterns to model and the difficulty to acquire and learn on large quantities of video data. Deep learning, although a breakthrough for image classification and showing promise for videos, has still not clearly superseded action recognition methods using hand-crafted features, even when training on massive datasets. In this paper, we introduce hybrid video classification architectures based on carefully designed unsupervised representations of hand-crafted spatio-temporal features classified by supervised deep networks. As we show in our experiments on five popular benchmarks for action recognition, our hybrid model combines the best of both worlds: it is data efficient (trained on 150 to 10000 short clips) and yet improves significantly on the state of the art, including recent deep models trained on millions of manually labelled images and videos.



### Modeling and Propagating CNNs in a Tree Structure for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1608.07242v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.07242v1)
- **Published**: 2016-08-25 18:29:53+00:00
- **Updated**: 2016-08-25 18:29:53+00:00
- **Authors**: Hyeonseob Nam, Mooyeol Baek, Bohyung Han
- **Comment**: 10 pages, Hyeonseob Nam and Mooyeol Baek have equal contribution
- **Journal**: None
- **Summary**: We present an online visual tracking algorithm by managing multiple target appearance models in a tree structure. The proposed algorithm employs Convolutional Neural Networks (CNNs) to represent target appearances, where multiple CNNs collaborate to estimate target states and determine the desirable paths for online model updates in the tree. By maintaining multiple CNNs in diverse branches of tree structure, it is convenient to deal with multi-modality in target appearances and preserve model reliability through smooth updates along tree paths. Since multiple CNNs share all parameters in convolutional layers, it takes advantage of multiple models with little extra cost by saving memory space and avoiding redundant network evaluations. The final target state is estimated by sampling target candidates around the state in the previous frame and identifying the best sample in terms of a weighted average score from a set of active CNNs. Our algorithm illustrates outstanding performance compared to the state-of-the-art techniques in challenging datasets such as online tracking benchmark and visual object tracking challenge.



### Fast Trajectory Simplification Algorithm for Natural User Interfaces in Robot Programming by Demonstration
- **Arxiv ID**: http://arxiv.org/abs/1608.07338v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.07338v1)
- **Published**: 2016-08-25 23:33:23+00:00
- **Updated**: 2016-08-25 23:33:23+00:00
- **Authors**: Daniel L. Marino, Milos Manic
- **Comment**: None
- **Journal**: None
- **Summary**: Trajectory simplification is a problem encountered in areas like Robot programming by demonstration, CAD/CAM, computer vision, and in GPS-based applications like traffic analysis. This problem entails reduction of the points in a given trajectory while keeping the relevant points which preserve important information. The benefits include storage reduction, computational expense, while making data more manageable. Common techniques formulate a minimization problem to be solved, where the solution is found iteratively under some error metric, which causes the algorithms to work in super-linear time. We present an algorithm called FastSTray, which selects the relevant points in the trajectory in linear time by following an open loop heuristic approach. While most current trajectory simplification algorithms are tailored for GPS trajectories, our approach focuses on smooth trajectories for robot programming by demonstration recorded using motion capture systems.Two variations of the algorithm are presented: 1. aims to preserve shape and temporal information; 2. preserves only shape information. Using the points in the simplified trajectory we use cubic splines to interpolate between these points and recreate the original trajectory. The presented algorithm was tested on trajectories recorded from a hand-tracking system. It was able to eliminate about 90% of the points in the original trajectories while maintaining errors between 0.78-2cm which corresponds to 1%-2.4% relative error with respect to the bounding box of the trajectories.



