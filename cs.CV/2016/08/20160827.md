# Arxiv Papers in cs.CV on 2016-08-27
### Learning to generalize to new compositions in image understanding
- **Arxiv ID**: http://arxiv.org/abs/1608.07639v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1608.07639v1)
- **Published**: 2016-08-27 00:34:00+00:00
- **Updated**: 2016-08-27 00:34:00+00:00
- **Authors**: Yuval Atzmon, Jonathan Berant, Vahid Kezami, Amir Globerson, Gal Chechik
- **Comment**: None
- **Journal**: None
- **Summary**: Recurrent neural networks have recently been used for learning to describe images using natural language. However, it has been observed that these models generalize poorly to scenes that were not observed during training, possibly depending too strongly on the statistics of the text in the training data. Here we propose to describe images using short structured representations, aiming to capture the crux of a description. These structured representations allow us to tease-out and evaluate separately two types of generalization: standard generalization to new images with similar scenes, and generalization to new combinations of known entities. We compare two learning approaches on the MS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show, Attend and Tell), and a simple structured prediction model on top of a deep network. We find that the structured model generalizes to new compositions substantially better than the LSTM, ~7 times the accuracy of predicting structured representations. By providing a concrete method to quantify generalization for unseen combinations, we argue that structured representations and compositional splits are a useful benchmark for image captioning, and advocate compositional models that capture linguistic and visual structure.



### Spatio-temporal Aware Non-negative Component Representation for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1608.07664v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.07664v1)
- **Published**: 2016-08-27 06:30:34+00:00
- **Updated**: 2016-08-27 06:30:34+00:00
- **Authors**: Jianhong Wang, Tian Lan, Xu Zhang, Limin Luo
- **Comment**: 11 pages, 5 figures, 6 tables
- **Journal**: None
- **Summary**: This paper presents a novel mid-level representation for action recognition, named spatio-temporal aware non-negative component representation (STANNCR). The proposed STANNCR is based on action component and incorporates the spatial-temporal information. We first introduce a spatial-temporal distribution vector (STDV) to model the distributions of local feature locations in a compact and discriminative manner. Then we employ non-negative matrix factorization (NMF) to learn the action components and encode the video samples. The action component considers the correlations of visual words, which effectively bridge the sematic gap in action recognition. To incorporate the spatial-temporal cues for final representation, the STDV is used as the part of graph regularization for NMF. The fusion of spatial-temporal information makes the STANNCR more discriminative, and our fusion manner is more compact than traditional method of concatenating vectors. The proposed approach is extensively evaluated on three public datasets. The experimental results demonstrate the effectiveness of STANNCR for action recognition.



### Multi-Path Feedback Recurrent Neural Network for Scene Parsing
- **Arxiv ID**: http://arxiv.org/abs/1608.07706v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.07706v3)
- **Published**: 2016-08-27 13:19:23+00:00
- **Updated**: 2016-11-22 11:44:06+00:00
- **Authors**: Xiaojie Jin, Yunpeng Chen, Jiashi Feng, Zequn Jie, Shuicheng Yan
- **Comment**: Accepted by AAAI-17. Camera-ready version
- **Journal**: None
- **Summary**: In this paper, we consider the scene parsing problem and propose a novel Multi-Path Feedback recurrent neural network (MPF-RNN) for parsing scene images. MPF-RNN can enhance the capability of RNNs in modeling long-range context information at multiple levels and better distinguish pixels that are easy to confuse. Different from feedforward CNNs and RNNs with only single feedback, MPF-RNN propagates the contextual features learned at top layer through \textit{multiple} weighted recurrent connections to learn bottom features. For better training MPF-RNN, we propose a new strategy that considers accumulative loss at multiple recurrent steps to improve performance of the MPF-RNN on parsing small objects. With these two novel components, MPF-RNN has achieved significant improvement over strong baselines (VGG16 and Res101) on five challenging scene parsing benchmarks, including traditional SiftFlow, Barcelona, CamVid, Stanford Background as well as the recently released large-scale ADE20K.



### 3D Object Proposals using Stereo Imagery for Accurate Object Class Detection
- **Arxiv ID**: http://arxiv.org/abs/1608.07711v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.07711v2)
- **Published**: 2016-08-27 13:39:04+00:00
- **Updated**: 2017-04-25 07:58:07+00:00
- **Authors**: Xiaozhi Chen, Kaustav Kundu, Yukun Zhu, Huimin Ma, Sanja Fidler, Raquel Urtasun
- **Comment**: 14 pages, 12 figures
- **Journal**: None
- **Summary**: The goal of this paper is to perform 3D object detection in the context of autonomous driving. Our method first aims at generating a set of high-quality 3D object proposals by exploiting stereo imagery. We formulate the problem as minimizing an energy function that encodes object size priors, placement of objects on the ground plane as well as several depth informed features that reason about free space, point cloud densities and distance to the ground. We then exploit a CNN on top of these proposals to perform object detection. In particular, we employ a convolutional neural net (CNN) that exploits context and depth information to jointly regress to 3D bounding box coordinates and object pose. Our experiments show significant performance gains over existing RGB and RGB-D object proposal methods on the challenging KITTI benchmark. When combined with the CNN, our approach outperforms all existing results in object detection and orientation estimation tasks for all three KITTI object classes. Furthermore, we experiment also with the setting where LIDAR information is available, and show that using both LIDAR and stereo leads to the best result.



### Learning Temporal Transformations From Time-Lapse Videos
- **Arxiv ID**: http://arxiv.org/abs/1608.07724v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.07724v1)
- **Published**: 2016-08-27 16:33:48+00:00
- **Updated**: 2016-08-27 16:33:48+00:00
- **Authors**: Yipin Zhou, Tamara L. Berg
- **Comment**: ECCV2016
- **Journal**: None
- **Summary**: Based on life-long observations of physical, chemical, and biologic phenomena in the natural world, humans can often easily picture in their minds what an object will look like in the future. But, what about computers? In this paper, we learn computational models of object transformations from time-lapse videos. In particular, we explore the use of generative models to create depictions of objects at future times. These models explore several different prediction tasks: generating a future state given a single depiction of an object, generating a future state given two depictions of an object at different times, and generating future states recursively in a recurrent framework. We provide both qualitative and quantitative evaluations of the generated results, and also conduct a human evaluation to compare variations of our models.



