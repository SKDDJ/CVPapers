# Arxiv Papers in cs.CV on 2016-08-21
### Online Feature Selection with Group Structure Analysis
- **Arxiv ID**: http://arxiv.org/abs/1608.05889v1
- **DOI**: 10.1109/TKDE.2015.2441716
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1608.05889v1)
- **Published**: 2016-08-21 02:39:48+00:00
- **Updated**: 2016-08-21 02:39:48+00:00
- **Authors**: Jing Wang, Meng Wang, Peipei Li, Luoqi Liu, Zhongqiu Zhao, Xuegang Hu, Xindong Wu
- **Comment**: IEEE Transactions on Knowledge and Data Engineering,2015
- **Journal**: None
- **Summary**: Online selection of dynamic features has attracted intensive interest in recent years. However, existing online feature selection methods evaluate features individually and ignore the underlying structure of feature stream. For instance, in image analysis, features are generated in groups which represent color, texture and other visual information. Simply breaking the group structure in feature selection may degrade performance. Motivated by this fact, we formulate the problem as an online group feature selection. The problem assumes that features are generated individually but there are group structure in the feature stream. To the best of our knowledge, this is the first time that the correlation among feature stream has been considered in the online feature selection process. To solve this problem, we develop a novel online group feature selection method named OGFS. Our proposed approach consists of two stages: online intra-group selection and online inter-group selection. In the intra-group selection, we design a criterion based on spectral analysis to select discriminative features in each group. In the inter-group selection, we utilize a linear regression model to select an optimal subset. This two-stage procedure continues until there are no more features arriving or some predefined stopping conditions are met. %Our method has been applied Finally, we apply our method to multiple tasks including image classification %, face verification and face verification. Extensive empirical studies performed on real-world and benchmark data sets demonstrate that our method outperforms other state-of-the-art online feature selection %method methods.



### VoxResNet: Deep Voxelwise Residual Networks for Volumetric Brain Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1608.05895v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.05895v1)
- **Published**: 2016-08-21 04:39:12+00:00
- **Updated**: 2016-08-21 04:39:12+00:00
- **Authors**: Hao Chen, Qi Dou, Lequan Yu, Pheng-Ann Heng
- **Comment**: None
- **Journal**: None
- **Summary**: Recently deep residual learning with residual units for training very deep neural networks advanced the state-of-the-art performance on 2D image recognition tasks, e.g., object detection and segmentation. However, how to fully leverage contextual representations for recognition tasks from volumetric data has not been well studied, especially in the field of medical image computing, where a majority of image modalities are in volumetric format. In this paper we explore the deep residual learning on the task of volumetric brain segmentation. There are at least two main contributions in our work. First, we propose a deep voxelwise residual network, referred as VoxResNet, which borrows the spirit of deep residual learning in 2D image recognition tasks, and is extended into a 3D variant for handling volumetric data. Second, an auto-context version of VoxResNet is proposed by seamlessly integrating the low-level image appearance features, implicit shape information and high-level context together for further improving the volumetric segmentation performance. Extensive experiments on the challenging benchmark of brain segmentation from magnetic resonance (MR) images corroborated the efficacy of our proposed method in dealing with volumetric data. We believe this work unravels the potential of 3D deep learning to advance the recognition performance on volumetric image segmentation.



### Congruences and Concurrent Lines in Multi-View Geometry
- **Arxiv ID**: http://arxiv.org/abs/1608.05924v2
- **DOI**: None
- **Categories**: **math.AG**, cs.CV, cs.SC
- **Links**: [PDF](http://arxiv.org/pdf/1608.05924v2)
- **Published**: 2016-08-21 12:07:14+00:00
- **Updated**: 2016-12-25 20:52:46+00:00
- **Authors**: Jean Ponce, Bernd Sturmfels, Matthew Trager
- **Comment**: 26 pages
- **Journal**: None
- **Summary**: We present a new framework for multi-view geometry in computer vision. A camera is a mapping between $\mathbb{P}^3$ and a line congruence. This model, which ignores image planes and measurements, is a natural abstraction of traditional pinhole cameras. It includes two-slit cameras, pushbroom cameras, catadioptric cameras, and many more. We study the concurrent lines variety, which consists of $n$-tuples of lines in $\mathbb{P}^3$ that intersect at a point. Combining its equations with those of various congruences, we derive constraints for corresponding images in multiple views. We also study photographic cameras which use image measurements and are modeled as rational maps from $\mathbb{P}^3$ to $\mathbb{P}^2$ or $\mathbb{P}^1\times \mathbb{P}^1$.



### STFCN: Spatio-Temporal FCN for Semantic Video Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1608.05971v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.05971v2)
- **Published**: 2016-08-21 17:34:08+00:00
- **Updated**: 2016-09-02 15:51:49+00:00
- **Authors**: Mohsen Fayyaz, Mohammad Hajizadeh Saffar, Mohammad Sabokrou, Mahmood Fathy, Reinhard Klette, Fay Huang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel method to involve both spatial and temporal features for semantic video segmentation. Current work on convolutional neural networks(CNNs) has shown that CNNs provide advanced spatial features supporting a very good performance of solutions for both image and video analysis, especially for the semantic segmentation task. We investigate how involving temporal features also has a good effect on segmenting video data. We propose a module based on a long short-term memory (LSTM) architecture of a recurrent neural network for interpreting the temporal characteristics of video frames over time. Our system takes as input frames of a video and produces a correspondingly-sized output; for segmenting the video our method combines the use of three components: First, the regional spatial features of frames are extracted using a CNN; then, using LSTM the temporal features are added; finally, by deconvolving the spatio-temporal features we produce pixel-wise predictions. Our key insight is to build spatio-temporal convolutional networks (spatio-temporal CNNs) that have an end-to-end architecture for semantic video segmentation. We adapted fully some known convolutional network architectures (such as FCN-AlexNet and FCN-VGG16), and dilated convolution into our spatio-temporal CNNs. Our spatio-temporal CNNs achieve state-of-the-art semantic segmentation, as demonstrated for the Camvid and NYUDv2 datasets.



### Feedback-Controlled Sequential Lasso Screening
- **Arxiv ID**: http://arxiv.org/abs/1608.06010v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1608.06010v2)
- **Published**: 2016-08-21 23:40:56+00:00
- **Updated**: 2016-08-25 22:52:30+00:00
- **Authors**: Yun Wang, Xu Chen, Peter J. Ramadge
- **Comment**: None
- **Journal**: None
- **Summary**: One way to solve lasso problems when the dictionary does not fit into available memory is to first screen the dictionary to remove unneeded features. Prior research has shown that sequential screening methods offer the greatest promise in this endeavor. Most existing work on sequential screening targets the context of tuning parameter selection, where one screens and solves a sequence of $N$ lasso problems with a fixed grid of geometrically spaced regularization parameters. In contrast, we focus on the scenario where a target regularization parameter has already been chosen via cross-validated model selection, and we then need to solve many lasso instances using this fixed value. In this context, we propose and explore a feedback controlled sequential screening scheme. Feedback is used at each iteration to select the next problem to be solved. This allows the sequence of problems to be adapted to the instance presented and the number of intermediate problems to be automatically selected. We demonstrate our feedback scheme using several datasets including a dictionary of approximate size 100,000 by 300,000.



### The Symmetry of a Simple Optimization Problem in Lasso Screening
- **Arxiv ID**: http://arxiv.org/abs/1608.06014v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1608.06014v2)
- **Published**: 2016-08-21 23:48:43+00:00
- **Updated**: 2016-08-25 22:05:24+00:00
- **Authors**: Yun Wang, Peter J. Ramadge
- **Comment**: None
- **Journal**: None
- **Summary**: Recently dictionary screening has been proposed as an effective way to improve the computational efficiency of solving the lasso problem, which is one of the most commonly used method for learning sparse representations. To address today's ever increasing large dataset, effective screening relies on a tight region bound on the solution to the dual lasso. Typical region bounds are in the form of an intersection of a sphere and multiple half spaces. One way to tighten the region bound is using more half spaces, which however, adds to the overhead of solving the high dimensional optimization problem in lasso screening. This paper reveals the interesting property that the optimization problem only depends on the projection of features onto the subspace spanned by the normals of the half spaces. This property converts an optimization problem in high dimension to much lower dimension, and thus sheds light on reducing the computation overhead of lasso screening based on tighter region bounds.



