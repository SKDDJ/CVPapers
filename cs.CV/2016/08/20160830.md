# Arxiv Papers in cs.CV on 2016-08-30
### Utilizing Large Scale Vision and Text Datasets for Image Segmentation from Referring Expressions
- **Arxiv ID**: http://arxiv.org/abs/1608.08305v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.08305v1)
- **Published**: 2016-08-30 02:27:41+00:00
- **Updated**: 2016-08-30 02:27:41+00:00
- **Authors**: Ronghang Hu, Marcus Rohrbach, Subhashini Venugopalan, Trevor Darrell
- **Comment**: None
- **Journal**: None
- **Summary**: Image segmentation from referring expressions is a joint vision and language modeling task, where the input is an image and a textual expression describing a particular region in the image; and the goal is to localize and segment the specific image region based on the given expression. One major difficulty to train such language-based image segmentation systems is the lack of datasets with joint vision and text annotations. Although existing vision datasets such as MS COCO provide image captions, there are few datasets with region-level textual annotations for images, and these are often smaller in scale. In this paper, we explore how existing large scale vision-only and text-only datasets can be utilized to train models for image segmentation from referring expressions. We propose a method to address this problem, and show in experiments that our method can help this joint vision and language modeling task with vision-only and text-only data and outperforms previous results.



### Egocentric Meets Top-view
- **Arxiv ID**: http://arxiv.org/abs/1608.08334v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.08334v2)
- **Published**: 2016-08-30 05:42:07+00:00
- **Updated**: 2016-09-14 18:51:14+00:00
- **Authors**: Shervin Ardeshir, Ali Borji
- **Comment**: None
- **Journal**: None
- **Summary**: Thanks to the availability and increasing popularity of Egocentric cameras such as GoPro cameras, glasses, and etc. we have been provided with a plethora of videos captured from the first person perspective. Surveillance cameras and Unmanned Aerial Vehicles(also known as drones) also offer tremendous amount of videos, mostly with top-down or oblique view-point. Egocentric vision and top-view surveillance videos have been studied extensively in the past in the computer vision community. However, the relationship between the two has yet to be explored thoroughly. In this effort, we attempt to explore this relationship by approaching two questions. First, having a set of egocentric videos and a top-view video, can we verify if the top-view video contains all, or some of the egocentric viewers present in the egocentric set? And second, can we identify the egocentric viewers in the content of the top-view video? In other words, can we find the cameramen in the surveillance videos? These problems can become more challenging when the videos are not time-synchronous. Thus we formalize the problem in a way which handles and also estimates the unknown relative time-delays between the egocentric videos and the top-view video. We formulate the problem as a spectral graph matching instance, and jointly seek the optimal assignments and relative time-delays of the videos. As a result, we spatiotemporally localize the egocentric observers in the top-view video. We model each view (egocentric or top) using a graph, and compute the assignment and time-delays in an iterative-alternative fashion.



### Low-rank Multi-view Clustering in Third-Order Tensor Space
- **Arxiv ID**: http://arxiv.org/abs/1608.08336v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.08336v2)
- **Published**: 2016-08-30 05:57:31+00:00
- **Updated**: 2016-08-31 23:37:37+00:00
- **Authors**: Ming Yin, Junbin Gao, Shengli Xie, Yi Guo
- **Comment**: None
- **Journal**: None
- **Summary**: The plenty information from multiple views data as well as the complementary information among different views are usually beneficial to various tasks, e.g., clustering, classification, de-noising. Multi-view subspace clustering is based on the fact that the multi-view data are generated from a latent subspace. To recover the underlying subspace structure, the success of the sparse and/or low-rank subspace clustering has been witnessed recently. Despite some state-of-the-art subspace clustering approaches can numerically handle multi-view data, by simultaneously exploring all possible pairwise correlation within views, the high order statistics is often disregarded which can only be captured by simultaneously utilizing all views. As a consequence, the clustering performance for multi-view data is compromised. To address this issue, in this paper, a novel multi-view clustering method is proposed by using \textit{t-product} in third-order tensor space. Based on the circular convolution operation, multi-view data can be effectively represented by a \textit{t-linear} combination with sparse and low-rank penalty using "self-expressiveness". Our extensive experimental results on facial, object, digits image and text data demonstrate that the proposed method outperforms the state-of-the-art methods in terms of many criteria.



### American Sign Language fingerspelling recognition from video: Methods for unrestricted recognition and signer-independence
- **Arxiv ID**: http://arxiv.org/abs/1608.08339v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1608.08339v1)
- **Published**: 2016-08-30 06:12:22+00:00
- **Updated**: 2016-08-30 06:12:22+00:00
- **Authors**: Taehwan Kim
- **Comment**: PhD Thesis
- **Journal**: None
- **Summary**: In this thesis, we study the problem of recognizing video sequences of fingerspelled letters in American Sign Language (ASL). Fingerspelling comprises a significant but relatively understudied part of ASL, and recognizing it is challenging for a number of reasons: It involves quick, small motions that are often highly coarticulated; it exhibits significant variation between signers; and there has been a dearth of continuous fingerspelling data collected. In this work, we propose several types of recognition approaches, and explore the signer variation problem. Our best-performing models are segmental (semi-Markov) conditional random fields using deep neural network-based features. In the signer-dependent setting, our recognizers achieve up to about 8% letter error rates. The signer-independent setting is much more challenging, but with neural network adaptation we achieve up to 17% letter error rates.



### Motion Representation with Acceleration Images
- **Arxiv ID**: http://arxiv.org/abs/1608.08395v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1608.08395v1)
- **Published**: 2016-08-30 10:23:07+00:00
- **Updated**: 2016-08-30 10:23:07+00:00
- **Authors**: Hirokatsu Kataoka, Yun He, Soma Shirakabe, Yutaka Satoh
- **Comment**: None
- **Journal**: None
- **Summary**: Information of time differentiation is extremely important cue for a motion representation. We have applied first-order differential velocity from a positional information, moreover we believe that second-order differential acceleration is also a significant feature in a motion representation. However, an acceleration image based on a typical optical flow includes motion noises. We have not employed the acceleration image because the noises are too strong to catch an effective motion feature in an image sequence. On one hand, the recent convolutional neural networks (CNN) are robust against input noises. In this paper, we employ acceleration-stream in addition to the spatial- and temporal-stream based on the two-stream CNN. We clearly show the effectiveness of adding the acceleration stream to the two-stream CNN.



### Multi-Class Multi-Object Tracking using Changing Point Detection
- **Arxiv ID**: http://arxiv.org/abs/1608.08434v1
- **DOI**: 10.1007/978-3-319-48881-3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.08434v1)
- **Published**: 2016-08-30 13:07:05+00:00
- **Updated**: 2016-08-30 13:07:05+00:00
- **Authors**: Byungjae Lee, Enkhbayar Erdenee, Songguo Jin, Phill Kyu Rhee
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a robust multi-class multi-object tracking (MCMOT) formulated by a Bayesian filtering framework. Multi-object tracking for unlimited object classes is conducted by combining detection responses and changing point detection (CPD) algorithm. The CPD model is used to observe abrupt or abnormal changes due to a drift and an occlusion based spatiotemporal characteristics of track states. The ensemble of convolutional neural network (CNN) based object detector and Lucas-Kanede Tracker (KLT) based motion detector is employed to compute the likelihoods of foreground regions as the detection responses of different object classes. Extensive experiments are performed using lately introduced challenging benchmark videos; ImageNet VID and MOT benchmark dataset. The comparison to state-of-the-art video tracking techniques shows very encouraging results.



### New Methods to Improve Large-Scale Microscopy Image Analysis with Prior Knowledge and Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/1608.08471v1
- **DOI**: 10.5445/IR/1000057821
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.08471v1)
- **Published**: 2016-08-30 14:21:55+00:00
- **Updated**: 2016-08-30 14:21:55+00:00
- **Authors**: Johannes Stegmaier
- **Comment**: 218 pages, 58 figures, PhD thesis, Department of Mechanical
  Engineering, Karlsruhe Institute of Technology, published online with KITopen
  (License: CC BY-SA 3.0, http://dx.doi.org/10.5445/IR/1000057821)
- **Journal**: None
- **Summary**: Multidimensional imaging techniques provide powerful ways to examine various kinds of scientific questions. The routinely produced datasets in the terabyte-range, however, can hardly be analyzed manually and require an extensive use of automated image analysis. The present thesis introduces a new concept for the estimation and propagation of uncertainty involved in image analysis operators and new segmentation algorithms that are suitable for terabyte-scale analyses of 3D+t microscopy images.



### Multi-Person Pose Estimation with Local Joint-to-Person Associations
- **Arxiv ID**: http://arxiv.org/abs/1608.08526v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.08526v2)
- **Published**: 2016-08-30 16:00:42+00:00
- **Updated**: 2016-08-31 09:26:57+00:00
- **Authors**: Umar Iqbal, Juergen Gall
- **Comment**: Accepted to European Conference on Computer Vision (ECCV) Workshops,
  Crowd Understanding, 2016
- **Journal**: None
- **Summary**: Despite of the recent success of neural networks for human pose estimation, current approaches are limited to pose estimation of a single person and cannot handle humans in groups or crowds. In this work, we propose a method that estimates the poses of multiple persons in an image in which a person can be occluded by another person or might be truncated. To this end, we consider multi-person pose estimation as a joint-to-person association problem. We construct a fully connected graph from a set of detected joint candidates in an image and resolve the joint-to-person association and outlier detection using integer linear programming. Since solving joint-to-person association jointly for all persons in an image is an NP-hard problem and even approximations are expensive, we solve the problem locally for each person. On the challenging MPII Human Pose Dataset for multiple persons, our approach achieves the accuracy of a state-of-the-art method, but it is 6,000 to 19,000 times faster.



### What makes ImageNet good for transfer learning?
- **Arxiv ID**: http://arxiv.org/abs/1608.08614v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1608.08614v2)
- **Published**: 2016-08-30 19:45:09+00:00
- **Updated**: 2016-12-10 13:37:06+00:00
- **Authors**: Minyoung Huh, Pulkit Agrawal, Alexei A. Efros
- **Comment**: None
- **Journal**: None
- **Summary**: The tremendous success of ImageNet-trained deep features on a wide range of transfer tasks begs the question: what are the properties of the ImageNet dataset that are critical for learning good, general-purpose features? This work provides an empirical investigation of various facets of this question: Is more pre-training data always better? How does feature quality depend on the number of training examples per class? Does adding more object classes improve performance? For the same data budget, how should the data be split into classes? Is fine-grained recognition necessary for learning good features? Given the same number of training classes, is it better to have coarse classes or fine-grained classes? Which is better: more classes or more examples per class? To answer these and related questions, we pre-trained CNN features on various subsets of the ImageNet dataset and evaluated transfer performance on PASCAL detection, PASCAL action classification, and SUN scene classification tasks. Our overall findings suggest that most changes in the choice of pre-training data long thought to be critical do not significantly affect transfer performance.? Given the same number of training classes, is it better to have coarse classes or fine-grained classes? Which is better: more classes or more examples per class?



