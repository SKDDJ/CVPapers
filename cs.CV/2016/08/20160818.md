# Arxiv Papers in cs.CV on 2016-08-18
### IM2CAD
- **Arxiv ID**: http://arxiv.org/abs/1608.05137v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.05137v2)
- **Published**: 2016-08-18 00:26:44+00:00
- **Updated**: 2017-04-24 00:28:58+00:00
- **Authors**: Hamid Izadinia, Qi Shan, Steven M. Seitz
- **Comment**: To appear at CVPR 2017
- **Journal**: None
- **Summary**: Given a single photo of a room and a large database of furniture CAD models, our goal is to reconstruct a scene that is as similar as possible to the scene depicted in the photograph, and composed of objects drawn from the database. We present a completely automatic system to address this IM2CAD problem that produces high quality results on challenging imagery from interior home design and remodeling websites. Our approach iteratively optimizes the placement and scale of objects in the room to best match scene renderings to the input photo, using image comparison metrics trained via deep convolutional neural nets. By operating jointly on the full scene at once, we account for inter-object occlusions. We also show the applicability of our method in standard scene understanding benchmarks where we obtain significant improvement.



### A Systematic Approach for Cross-source Point Cloud Registration by Preserving Macro and Micro Structures
- **Arxiv ID**: http://arxiv.org/abs/1608.05143v2
- **DOI**: 10.1109/TIP.2017.2695888
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.05143v2)
- **Published**: 2016-08-18 00:53:13+00:00
- **Updated**: 2016-08-22 23:04:03+00:00
- **Authors**: Xiaoshui Huang, Jian Zhang, Lixin Fan, Qiang Wu, Chun Yuan
- **Comment**: Cross-source point cloud registration
- **Journal**: None
- **Summary**: We propose a systematic approach for registering cross-source point clouds. The compelling need for cross-source point cloud registration is motivated by the rapid development of a variety of 3D sensing techniques, but many existing registration methods face critical challenges as a result of the large variations in cross-source point clouds. This paper therefore illustrates a novel registration method which successfully aligns two cross-source point clouds in the presence of significant missing data, large variations in point density, scale difference and so on. The robustness of the method is attributed to the extraction of macro and micro structures. Our work has three main contributions: (1) a systematic pipeline to deal with cross-source point cloud registration; (2) a graph construction method to maintain macro and micro structures; (3) a new graph matching method is proposed which considers the global geometric constraint to robustly register these variable graphs. Compared to most of the related methods, the experiments show that the proposed method successfully registers in cross-source datasets, while other methods have difficulty achieving satisfactory results. The proposed method also shows great ability in same-source datasets.



### Full Resolution Image Compression with Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1608.05148v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.05148v2)
- **Published**: 2016-08-18 01:05:09+00:00
- **Updated**: 2017-07-07 16:26:16+00:00
- **Authors**: George Toderici, Damien Vincent, Nick Johnston, Sung Jin Hwang, David Minnen, Joel Shor, Michele Covell
- **Comment**: Updated with content for CVPR and removed supplemental material to an
  external link for size limitations
- **Journal**: None
- **Summary**: This paper presents a set of full-resolution lossy image compression methods based on neural networks. Each of the architectures we describe can provide variable compression rates during deployment without requiring retraining of the network: each network need only be trained once. All of our architectures consist of a recurrent neural network (RNN)-based encoder and decoder, a binarizer, and a neural network for entropy coding. We compare RNN types (LSTM, associative LSTM) and introduce a new hybrid of GRU and ResNet. We also study "one-shot" versus additive reconstruction architectures and introduce a new scaled-additive framework. We compare to previous work, showing improvements of 4.3%-8.8% AUC (area under the rate-distortion curve), depending on the perceptual metric used. As far as we know, this is the first neural network architecture that is able to outperform JPEG at image compression across most bitrates on the rate-distortion curve on the Kodak dataset images, with and without the aid of entropy coding.



### Multi-stage Object Detection with Group Recursive Learning
- **Arxiv ID**: http://arxiv.org/abs/1608.05159v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.05159v1)
- **Published**: 2016-08-18 02:37:28+00:00
- **Updated**: 2016-08-18 02:37:28+00:00
- **Authors**: Jianan Li, Xiaodan Liang, Jianshu Li, Tingfa Xu, Jiashi Feng, Shuicheng Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Most of existing detection pipelines treat object proposals independently and predict bounding box locations and classification scores over them separately. However, the important semantic and spatial layout correlations among proposals are often ignored, which are actually useful for more accurate object detection. In this work, we propose a new EM-like group recursive learning approach to iteratively refine object proposals by incorporating such context of surrounding proposals and provide an optimal spatial configuration of object detections. In addition, we propose to incorporate the weakly-supervised object segmentation cues and region-based object detection into a multi-stage architecture in order to fully exploit the learned segmentation features for better object detection in an end-to-end way. The proposed architecture consists of three cascaded networks which respectively learn to perform weakly-supervised object segmentation, object proposal generation and recursive detection refinement. Combining the group recursive learning and the multi-stage architecture provides competitive mAPs of 78.6% and 74.9% on the PASCAL VOC2007 and VOC2012 datasets respectively, which outperforms many well-established baselines [10] [20] significantly.



### AID: A Benchmark Dataset for Performance Evaluation of Aerial Scene Classification
- **Arxiv ID**: http://arxiv.org/abs/1608.05167v1
- **DOI**: 10.1109/TGRS.2017.2685945
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.05167v1)
- **Published**: 2016-08-18 04:20:45+00:00
- **Updated**: 2016-08-18 04:20:45+00:00
- **Authors**: Gui-Song Xia, Jingwen Hu, Fan Hu, Baoguang Shi, Xiang Bai, Yanfei Zhong, Liangpei Zhang
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, Vol. 55, No.7,
  pp.3965-3981 (July 2017)
- **Summary**: Aerial scene classification, which aims to automatically label an aerial image with a specific semantic category, is a fundamental problem for understanding high-resolution remote sensing imagery. In recent years, it has become an active task in remote sensing area and numerous algorithms have been proposed for this task, including many machine learning and data-driven approaches. However, the existing datasets for aerial scene classification like UC-Merced dataset and WHU-RS19 are with relatively small sizes, and the results on them are already saturated. This largely limits the development of scene classification algorithms. This paper describes the Aerial Image Dataset (AID): a large-scale dataset for aerial scene classification. The goal of AID is to advance the state-of-the-arts in scene classification of remote sensing images. For creating AID, we collect and annotate more than ten thousands aerial scene images. In addition, a comprehensive review of the existing aerial scene classification techniques as well as recent widely-used deep learning methods is given. Finally, we provide a performance analysis of typical aerial scene classification and deep learning approaches on AID, which can be served as the baseline results on this benchmark.



### Deeply-Supervised Recurrent Convolutional Neural Network for Saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/1608.05177v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.05177v1)
- **Published**: 2016-08-18 05:08:16+00:00
- **Updated**: 2016-08-18 05:08:16+00:00
- **Authors**: Youbao Tang, Xiangqian Wu, Wei Bu
- **Comment**: 5 pages, 5 figures, accepted by ACMMM 2016
- **Journal**: None
- **Summary**: This paper proposes a novel saliency detection method by developing a deeply-supervised recurrent convolutional neural network (DSRCNN), which performs a full image-to-image saliency prediction. For saliency detection, the local, global, and contextual information of salient objects is important to obtain a high quality salient map. To achieve this goal, the DSRCNN is designed based on VGGNet-16. Firstly, the recurrent connections are incorporated into each convolutional layer, which can make the model more powerful for learning the contextual information. Secondly, side-output layers are added to conduct the deeply-supervised operation, which can make the model learn more discriminative and robust features by effecting the intermediate layers. Finally, all of the side-outputs are fused to integrate the local and global information to get the final saliency detection results. Therefore, the DSRCNN combines the advantages of recurrent convolutional neural networks and deeply-supervised nets. The DSRCNN model is tested on five benchmark datasets, and experimental results demonstrate that the proposed method significantly outperforms the state-of-the-art saliency detection approaches on all test datasets.



### A Holistic Approach for Data-Driven Object Cutout
- **Arxiv ID**: http://arxiv.org/abs/1608.05180v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.05180v2)
- **Published**: 2016-08-18 05:19:26+00:00
- **Updated**: 2016-09-16 13:00:21+00:00
- **Authors**: Huayong Xu, Yangyan Li, Wenzheng Chen, Dani Lischinski, Daniel Cohen-Or, Baoquan Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Object cutout is a fundamental operation for image editing and manipulation, yet it is extremely challenging to automate it in real-world images, which typically contain considerable background clutter. In contrast to existing cutout methods, which are based mainly on low-level image analysis, we propose a more holistic approach, which considers the entire shape of the object of interest by leveraging higher-level image analysis and learnt global shape priors. Specifically, we leverage a deep neural network (DNN) trained for objects of a particular class (chairs) for realizing this mechanism. Given a rectangular image region, the DNN outputs a probability map (P-map) that indicates for each pixel inside the rectangle how likely it is to be contained inside an object from the class of interest. We show that the resulting P-maps may be used to evaluate how likely a rectangle proposal is to contain an instance of the class, and further process good proposals to produce an accurate object cutout mask. This amounts to an automatic end-to-end pipeline for catergory-specific object cutout. We evaluate our approach on segmentation benchmark datasets, and show that it significantly outperforms the state-of-the-art on them.



### Saliency Detection via Combining Region-Level and Pixel-Level Predictions with CNNs
- **Arxiv ID**: http://arxiv.org/abs/1608.05186v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.05186v1)
- **Published**: 2016-08-18 06:00:18+00:00
- **Updated**: 2016-08-18 06:00:18+00:00
- **Authors**: Youbao Tang, Xiangqian Wu
- **Comment**: 18 pages, 9 figures, accepted by ECCV 2016
- **Journal**: None
- **Summary**: This paper proposes a novel saliency detection method by combining region-level saliency estimation and pixel-level saliency prediction with CNNs (denoted as CRPSD). For pixel-level saliency prediction, a fully convolutional neural network (called pixel-level CNN) is constructed by modifying the VGGNet architecture to perform multi-scale feature learning, based on which an image-to-image prediction is conducted to accomplish the pixel-level saliency detection. For region-level saliency estimation, an adaptive superpixel based region generation technique is first designed to partition an image into regions, based on which the region-level saliency is estimated by using a CNN model (called region-level CNN). The pixel-level and region-level saliencies are fused to form the final salient map by using another CNN (called fusion CNN). And the pixel-level CNN and fusion CNN are jointly learned. Extensive quantitative and qualitative experiments on four public benchmark datasets demonstrate that the proposed method greatly outperforms the state-of-the-art saliency detection approaches.



### Seeing with Humans: Gaze-Assisted Neural Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1608.05203v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.05203v1)
- **Published**: 2016-08-18 08:13:22+00:00
- **Updated**: 2016-08-18 08:13:22+00:00
- **Authors**: Yusuke Sugano, Andreas Bulling
- **Comment**: None
- **Journal**: None
- **Summary**: Gaze reflects how humans process visual scenes and is therefore increasingly used in computer vision systems. Previous works demonstrated the potential of gaze for object-centric tasks, such as object localization and recognition, but it remains unclear if gaze can also be beneficial for scene-centric tasks, such as image captioning. We present a new perspective on gaze-assisted image captioning by studying the interplay between human gaze and the attention mechanism of deep neural networks. Using a public large-scale gaze dataset, we first assess the relationship between state-of-the-art object and scene recognition models, bottom-up visual saliency, and human gaze. We then propose a novel split attention model for image captioning. Our model integrates human gaze information into an attention-based long short-term memory architecture, and allows the algorithm to allocate attention selectively to both fixated and non-fixated image regions. Through evaluation on the COCO/SALICON datasets we show that our method improves image captioning performance and that gaze can complement machine attention for semantic scene understanding tasks.



### Refining Geometry from Depth Sensors using IR Shading Images
- **Arxiv ID**: http://arxiv.org/abs/1608.05204v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.05204v1)
- **Published**: 2016-08-18 08:19:43+00:00
- **Updated**: 2016-08-18 08:19:43+00:00
- **Authors**: Gyeongmin Choe, Jaesik Park, Yu-Wing Tai, In So Kweon
- **Comment**: Accepted to the International Journal of Computer Vision (IJCV)
- **Journal**: None
- **Summary**: We propose a method to refine geometry of 3D meshes from a consumer level depth camera, e.g. Kinect, by exploiting shading cues captured from an infrared (IR) camera. A major benefit to using an IR camera instead of an RGB camera is that the IR images captured are narrow band images that filter out most undesired ambient light, which makes our system robust against natural indoor illumination. Moreover, for many natural objects with colorful textures in the visible spectrum, the subjects appear to have a uniform albedo in the IR spectrum. Based on our analyses on the IR projector light of the Kinect, we define a near light source IR shading model that describes the captured intensity as a function of surface normals, albedo, lighting direction, and distance between light source and surface points. To resolve the ambiguity in our model between the normals and distances, we utilize an initial 3D mesh from the Kinect fusion and multi-view information to reliably estimate surface details that were not captured and reconstructed by the Kinect fusion. Our approach directly operates on the mesh model for geometry refinement. We ran experiments on our algorithm for geometries captured by both the Kinect I and Kinect II, as the depth acquisition in Kinect I is based on a structured-light technique and that of the Kinect II is based on a time-of-flight (ToF) technology. The effectiveness of our approach is demonstrated through several challenging real-world examples. We have also performed a user study to evaluate the quality of the mesh models before and after our refinements.



### Efficient Multi-Frequency Phase Unwrapping using Kernel Density Estimation
- **Arxiv ID**: http://arxiv.org/abs/1608.05209v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.05209v1)
- **Published**: 2016-08-18 08:49:13+00:00
- **Updated**: 2016-08-18 08:49:13+00:00
- **Authors**: Felix Järemo Lawin, Per-Erik Forssén, Hannes Ovrén
- **Comment**: Accepted at ECCV 2016
- **Journal**: None
- **Summary**: In this paper we introduce an efficient method to unwrap multi-frequency phase estimates for time-of-flight ranging. The algorithm generates multiple depth hypotheses and uses a spatial kernel density estimate (KDE) to rank them. The confidence produced by the KDE is also an effective means to detect outliers. We also introduce a new closed-form expression for phase noise prediction, that better fits real data. The method is applied to depth decoding for the Kinect v2 sensor, and compared to the Microsoft Kinect SDK and to the open source driver libfreenect2. The intended Kinect v2 use case is scenes with less than 8m range, and for such cases we observe consistent improvements, while maintaining real-time performance. When extending the depth range to the maximal value of 8.75m, we get about 52% more valid measurements than libfreenect2. The effect is that the sensor can now be used in large depth scenes, where it was previously not a good choice. Code and supplementary material are available at http://www.cvl.isy.liu.se/research/datasets/kinect2-dataset.



### How Image Degradations Affect Deep CNN-based Face Recognition?
- **Arxiv ID**: http://arxiv.org/abs/1608.05246v1
- **DOI**: 10.1109/BIOSIG.2016.7736924
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.05246v1)
- **Published**: 2016-08-18 11:48:26+00:00
- **Updated**: 2016-08-18 11:48:26+00:00
- **Authors**: Samil Karahan, Merve Kilinc Yildirim, Kadir Kirtac, Ferhat Sukru Rende, Gultekin Butun, Hazim Kemal Ekenel
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: Face recognition approaches that are based on deep convolutional neural networks (CNN) have been dominating the field. The performance improvements they have provided in the so called in-the-wild datasets are significant, however, their performance under image quality degradations have not been assessed, yet. This is particularly important, since in real-world face recognition applications, images may contain various kinds of degradations due to motion blur, noise, compression artifacts, color distortions, and occlusion. In this work, we have addressed this problem and analyzed the influence of these image degradations on the performance of deep CNN-based face recognition approaches using the standard LFW closed-set identification protocol. We have evaluated three popular deep CNN models, namely, the AlexNet, VGG-Face, and GoogLeNet. Results have indicated that blur, noise, and occlusion cause a significant decrease in performance, while deep CNN models are found to be robust to distortions, such as color distortions and change in color balance.



### Leveraging Structural Context Models and Ranking Score Fusion for Human Interaction Prediction
- **Arxiv ID**: http://arxiv.org/abs/1608.05267v3
- **DOI**: 10.1109/TMM.2017.2778559
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.05267v3)
- **Published**: 2016-08-18 14:05:37+00:00
- **Updated**: 2017-06-07 08:35:49+00:00
- **Authors**: Qiuhong Ke, Mohammed Bennamoun, Senjian An, Farid Bossaid, Ferdous Sohel
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting an interaction before it is fully executed is very important in applications such as human-robot interaction and video surveillance. In a two-human interaction scenario, there often contextual dependency structure between the global interaction context of the two humans and the local context of the different body parts of each human. In this paper, we propose to learn the structure of the interaction contexts, and combine it with the spatial and temporal information of a video sequence for a better prediction of the interaction class. The structural models, including the spatial and the temporal models, are learned with Long Short Term Memory (LSTM) networks to capture the dependency of the global and local contexts of each RGB frame and each optical flow image, respectively. LSTM networks are also capable of detecting the key information from the global and local interaction contexts. Moreover, to effectively combine the structural models with the spatial and temporal models for interaction prediction, a ranking score fusion method is also introduced to automatically compute the optimal weight of each model for score fusion. Experimental results on the BIT Interaction and the UT-Interaction datasets clearly demonstrate the benefits of the proposed method.



### Photo Filter Recommendation by Category-Aware Aesthetic Learning
- **Arxiv ID**: http://arxiv.org/abs/1608.05339v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.05339v2)
- **Published**: 2016-08-18 17:22:54+00:00
- **Updated**: 2017-03-27 05:07:06+00:00
- **Authors**: Wei-Tse Sun, Ting-Hsuan Chao, Yin-Hsi Kuo, Winston H. Hsu
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: Nowadays, social media has become a popular platform for the public to share photos. To make photos more visually appealing, users usually apply filters on their photos without domain knowledge. However, due to the growing number of filter types, it becomes a major issue for users to choose the best filter type. For this purpose, filter recommendation for photo aesthetics takes an important role in image quality ranking problems. In these years, several works have declared that Convolutional Neural Networks (CNNs) outperform traditional methods in image aesthetic categorization, which classifies images into high or low quality. Most of them do not consider the effect on filtered images; hence, we propose a novel image aesthetic learning for filter recommendation. Instead of binarizing image quality, we adjust the state-of-the-art CNN architectures and design a pairwise loss function to learn the embedded aesthetic responses in hidden layers for filtered images. Based on our pilot study, we observe image categories (e.g., portrait, landscape, food) will affect user preference on filter selection. We further integrate category classification into our proposed aesthetic-oriented models. To the best of our knowledge, there is no public dataset for aesthetic judgment with filtered images. We create a new dataset called Filter Aesthetic Comparison Dataset (FACD). It contains 28,160 filtered images based on the AVA dataset and 42,240 reliable image pairs with aesthetic annotations using Amazon Mechanical Turk. It is the first dataset containing filtered images and user preference labels. We conduct experiments on the collected FACD for filter recommendation, and the results show that our proposed category-aware aesthetic learning outperforms aesthetic classification methods (e.g., 12% relative improvement).



### Semantic Understanding of Scenes through the ADE20K Dataset
- **Arxiv ID**: http://arxiv.org/abs/1608.05442v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.05442v2)
- **Published**: 2016-08-18 22:23:13+00:00
- **Updated**: 2018-10-16 04:41:24+00:00
- **Authors**: Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, Antonio Torralba
- **Comment**: IJCV extension
- **Journal**: None
- **Summary**: Scene parsing, or recognizing and segmenting objects and stuff in an image, is one of the key problems in computer vision. Despite the community's efforts in data collection, there are still few image datasets covering a wide range of scenes and object categories with dense and detailed annotations for scene parsing. In this paper, we introduce and analyze the ADE20K dataset, spanning diverse annotations of scenes, objects, parts of objects, and in some cases even parts of parts. A generic network design called Cascade Segmentation Module is then proposed to enable the segmentation networks to parse a scene into stuff, objects, and object parts in a cascade. We evaluate the proposed module integrated within two existing semantic segmentation networks, yielding significant improvements for scene parsing. We further show that the scene parsing networks trained on ADE20K can be applied to a wide variety of scenes and objects.



