# Arxiv Papers in cs.CV on 2016-08-31
### Pruning Filters for Efficient ConvNets
- **Arxiv ID**: http://arxiv.org/abs/1608.08710v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1608.08710v3)
- **Published**: 2016-08-31 02:29:59+00:00
- **Updated**: 2017-03-10 17:57:56+00:00
- **Authors**: Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, Hans Peter Graf
- **Comment**: Published as a conference paper at ICLR 2017
- **Journal**: None
- **Summary**: The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy. However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks.



### Engagement Detection in Meetings
- **Arxiv ID**: http://arxiv.org/abs/1608.08711v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1608.08711v1)
- **Published**: 2016-08-31 02:46:37+00:00
- **Updated**: 2016-08-31 02:46:37+00:00
- **Authors**: Maria Frank, Ghassem Tofighi, Haisong Gu, Renate Fruchter
- **Comment**: The paper has been published on ICCCBE 2016.
  http://www.see.eng.osaka-u.ac.jp/seeit/icccbe2016/
  http://www.see.eng.osaka-u.ac.jp/seeit/icccbe2016/download/Tentative_Time_Table_ICCCBE2016_2016-05-10.pdf
- **Journal**: None
- **Summary**: Group meetings are frequent business events aimed to develop and conduct project work, such as Big Room design and construction project meetings. To be effective in these meetings, participants need to have an engaged mental state. The mental state of participants however, is hidden from other participants, and thereby difficult to evaluate. Mental state is understood as an inner process of thinking and feeling, that is formed of a conglomerate of mental representations and propositional attitudes. There is a need to create transparency of these hidden states to understand, evaluate and influence them. Facilitators need to evaluate the meeting situation and adjust for higher engagement and productivity. This paper presents a framework that defines a spectrum of engagement states and an array of classifiers aimed to detect the engagement state of participants in real time. The Engagement Framework integrates multi-modal information from 2D and 3D imaging and sound. Engagement is detected and evaluated at participants and aggregated at group level. We use empirical data collected at the lab of Konica Minolta, Inc. to test initial applications of this framework. The paper presents examples of the tested engagement classifiers, which are based on research in psychology, communication, and human computer interaction. Their accuracy is illustrated in dyadic interaction for engagement detection. In closing we discuss the potential extension to complex group collaboration settings and future feedback implementations.



### Measuring Machine Intelligence Through Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1608.08716v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1608.08716v1)
- **Published**: 2016-08-31 02:56:00+00:00
- **Updated**: 2016-08-31 02:56:00+00:00
- **Authors**: C. Lawrence Zitnick, Aishwarya Agrawal, Stanislaw Antol, Margaret Mitchell, Dhruv Batra, Devi Parikh
- **Comment**: AI Magazine, 2016
- **Journal**: None
- **Summary**: As machines have become more intelligent, there has been a renewed interest in methods for measuring their intelligence. A common approach is to propose tasks for which a human excels, but one which machines find difficult. However, an ideal task should also be easy to evaluate and not be easily gameable. We begin with a case study exploring the recently popular task of image captioning and its limitations as a task for measuring machine intelligence. An alternative and more promising task is Visual Question Answering that tests a machine's ability to reason about language and vision. We describe a dataset unprecedented in size created for the task that contains over 760,000 human generated questions about images. Using around 10 million human generated answers, machines may be easily evaluated.



### CliqueCNN: Deep Unsupervised Exemplar Learning
- **Arxiv ID**: http://arxiv.org/abs/1608.08792v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.08792v1)
- **Published**: 2016-08-31 09:49:56+00:00
- **Updated**: 2016-08-31 09:49:56+00:00
- **Authors**: Miguel A. Bautista, Artsiom Sanakoyeu, Ekaterina Sutter, Björn Ommer
- **Comment**: Accepted for publication at NIPS 2016
- **Journal**: None
- **Summary**: Exemplar learning is a powerful paradigm for discovering visual similarities in an unsupervised manner. In this context, however, the recent breakthrough in deep learning could not yet unfold its full potential. With only a single positive sample, a great imbalance between one positive and many negatives, and unreliable relationships between most samples, training of Convolutional Neural networks is impaired. Given weak estimates of local distance we propose a single optimization problem to extract batches of samples with mutually consistent relations. Conflicting relations are distributed over different batches and similar samples are grouped into compact cliques. Learning exemplar similarities is framed as a sequence of clique categorization tasks. The CNN then consolidates transitivity relations within and between cliques and learns a single representation for all samples without the need for labels. The proposed unsupervised approach has shown competitive performance on detailed posture analysis and object classification.



### Spatio-Colour Asplünd 's Metric and Logarithmic Image Processing for Colour Images (LIPC)
- **Arxiv ID**: http://arxiv.org/abs/1608.08831v2
- **DOI**: 10.1007/978-3-319-52277-7_5
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.08831v2)
- **Published**: 2016-08-31 12:49:12+00:00
- **Updated**: 2017-02-27 16:08:29+00:00
- **Authors**: Guillaume Noyel, Michel Jourlin
- **Comment**: None
- **Journal**: C\'esar Beltr\'an-Casta\~n\'on, Ingela Nystr\"om, Fazel Famili
  CIARP2016 - XXI IberoAmerican Congress on Pattern Recognition, Nov 2016,
  Lima, Peru. Springer, 10125 2017, pp.36-43, 2016, Progress in Pattern
  Recognition, Image Analysis, Computer Vision, and Applications: 21st
  Iberoamerican Congress, CIARP 2016, Lima, Peru, November 8--11, 2016,
  Proceedings
- **Summary**: Aspl\"und 's metric, which is useful for pattern matching, consists in a double-sided probing, i.e. the over-graph and the sub-graph of a function are probed jointly. This paper extends the Aspl\"und 's metric we previously defined for colour and multivariate images using a marginal approach (i.e. component by component) to the first spatio-colour Aspl\"und 's metric based on the vectorial colour LIP model (LIPC). LIPC is a non-linear model with operations between colour images which are consistent with the human visual system. The defined colour metric is insensitive to lighting variations and a variant which is robust to noise is used for colour pattern matching.



### Efficient Two-Stream Motion and Appearance 3D CNNs for Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1608.08851v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.08851v2)
- **Published**: 2016-08-31 13:52:54+00:00
- **Updated**: 2016-09-02 10:39:24+00:00
- **Authors**: Ali Diba, Ali Mohammad Pazandeh, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: The video and action classification have extremely evolved by deep neural networks specially with two stream CNN using RGB and optical flow as inputs and they present outstanding performance in terms of video analysis. One of the shortcoming of these methods is handling motion information extraction which is done out side of the CNNs and relatively time consuming also on GPUs. So proposing end-to-end methods which are exploring to learn motion representation, like 3D-CNN can achieve faster and accurate performance. We present some novel deep CNNs using 3D architecture to model actions and motion representation in an efficient way to be accurate and also as fast as real-time. Our new networks learn distinctive models to combine deep motion features into appearance model via learning optical flow features inside the network.



### Facial Surface Analysis using Iso-Geodesic Curves in Three Dimensional Face Recognition System
- **Arxiv ID**: http://arxiv.org/abs/1608.08878v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.08878v1)
- **Published**: 2016-08-31 14:24:15+00:00
- **Updated**: 2016-08-31 14:24:15+00:00
- **Authors**: Rachid Ahdid, El Mahdi Barrah, Said Safi, Bouzid Manaut
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: In this paper, we present an automatic 3D face recognition system. This system is based on the representation of human faces surfaces as collections of Iso-Geodesic Curves (IGC) using 3D Fast Marching algorithm. To compare two facial surfaces, we compute a geodesic distance between a pair of facial curves using a Riemannian geometry. In the classifying step, we use: Neural Networks (NN), K-Nearest Neighbor (KNN) and Support Vector Machines (SVM). To test this method and evaluate its performance, a simulation series of experiments were performed on 3D Shape REtrieval Contest 2008 database (SHREC2008).



### Robustness of classifiers: from adversarial to random noise
- **Arxiv ID**: http://arxiv.org/abs/1608.08967v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1608.08967v1)
- **Published**: 2016-08-31 17:54:34+00:00
- **Updated**: 2016-08-31 17:54:34+00:00
- **Authors**: Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard
- **Comment**: Accepted to NIPS 2016
- **Journal**: None
- **Summary**: Several recent works have shown that state-of-the-art classifiers are vulnerable to worst-case (i.e., adversarial) perturbations of the datapoints. On the other hand, it has been empirically observed that these same classifiers are relatively robust to random noise. In this paper, we propose to study a \textit{semi-random} noise regime that generalizes both the random and worst-case noise regimes. We propose the first quantitative analysis of the robustness of nonlinear classifiers in this general noise regime. We establish precise theoretical bounds on the robustness of classifiers in this general regime, which depend on the curvature of the classifier's decision boundary. Our bounds confirm and quantify the empirical observations that classifiers satisfying curvature constraints are robust to random noise. Moreover, we quantify the robustness of classifiers in terms of the subspace dimension in the semi-random noise regime, and show that our bounds remarkably interpolate between the worst-case and random noise regimes. We perform experiments and show that the derived bounds provide very accurate estimates when applied to various state-of-the-art deep neural networks and datasets. This result suggests bounds on the curvature of the classifiers' decision boundaries that we support experimentally, and more generally offers important insights onto the geometry of high dimensional classification problems.



### Towards Transparent AI Systems: Interpreting Visual Question Answering Models
- **Arxiv ID**: http://arxiv.org/abs/1608.08974v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1608.08974v2)
- **Published**: 2016-08-31 18:11:29+00:00
- **Updated**: 2016-09-09 19:51:06+00:00
- **Authors**: Yash Goyal, Akrit Mohapatra, Devi Parikh, Dhruv Batra
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have shown striking progress and obtained state-of-the-art results in many AI research fields in the recent years. However, it is often unsatisfying to not know why they predict what they do. In this paper, we address the problem of interpreting Visual Question Answering (VQA) models. Specifically, we are interested in finding what part of the input (pixels in images or words in questions) the VQA model focuses on while answering the question. To tackle this problem, we use two visualization techniques -- guided backpropagation and occlusion -- to find important words in the question and important regions in the image. We then present qualitative and quantitative analyses of these importance maps. We found that even without explicit attention mechanisms, VQA models may sometimes be implicitly attending to relevant regions in the image, and often to appropriate words in the question.



### Measuring the Quality of Exercises
- **Arxiv ID**: http://arxiv.org/abs/1608.09005v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.09005v1)
- **Published**: 2016-08-31 19:28:49+00:00
- **Updated**: 2016-08-31 19:28:49+00:00
- **Authors**: Paritosh Parmar, Brendan Tran Morris
- **Comment**: EMBC'16 (The 38th Annual International Conference of the IEEE
  Engineering in Medicine and Biology Society)
- **Journal**: None
- **Summary**: This work explores the problem of exercise quality measurement since it is essential for effective management of diseases like cerebral palsy (CP). This work examines the assessment of quality of large amplitude movement (LAM) exercises designed to treat CP in an automated fashion. Exercise data was collected by trained participants to generate ideal examples to use as a positive samples for machine learning. Following that, subjects were asked to deliberately make subtle errors during the exercise, such as restricting movements, as is commonly seen in cases of patients suffering from CP. The quality measurement problem was then posed as a classification to determine whether an example exercise was either "good" or "bad". Popular machine learning techniques for classification, including support vector machines (SVM), single and doublelayered neural networks (NN), boosted decision trees, and dynamic time warping (DTW), were compared. The AdaBoosted tree performed best with an accuracy of 94.68% demonstrating the feasibility of assessing exercise quality.



### Radiation Search Operations using Scene Understanding with Autonomous UAV and UGV
- **Arxiv ID**: http://arxiv.org/abs/1609.00017v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1609.00017v1)
- **Published**: 2016-08-31 20:00:46+00:00
- **Updated**: 2016-08-31 20:00:46+00:00
- **Authors**: Gordon Christie, Adam Shoemaker, Kevin Kochersberger, Pratap Tokekar, Lance McLean, Alexander Leonessa
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomously searching for hazardous radiation sources requires the ability of the aerial and ground systems to understand the scene they are scouting. In this paper, we present systems, algorithms, and experiments to perform radiation search using unmanned aerial vehicles (UAV) and unmanned ground vehicles (UGV) by employing semantic scene segmentation. The aerial data is used to identify radiological points of interest, generate an orthophoto along with a digital elevation model (DEM) of the scene, and perform semantic segmentation to assign a category (e.g. road, grass) to each pixel in the orthophoto. We perform semantic segmentation by training a model on a dataset of images we collected and annotated, using the model to perform inference on images of the test area unseen to the model, and then refining the results with the DEM to better reason about category predictions at each pixel. We then use all of these outputs to plan a path for a UGV carrying a LiDAR to map the environment and avoid obstacles not present during the flight, and a radiation detector to collect more precise radiation measurements from the ground. Results of the analysis for each scenario tested favorably. We also note that our approach is general and has the potential to work for a variety of different sensing tasks.



### Human Pose Estimation in Space and Time using 3D CNN
- **Arxiv ID**: http://arxiv.org/abs/1609.00036v3
- **DOI**: 10.1007/978-3-319-49409-8_5
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1609.00036v3)
- **Published**: 2016-08-31 20:55:26+00:00
- **Updated**: 2016-10-19 12:44:15+00:00
- **Authors**: Agne Grinciunaite, Amogh Gudi, Emrah Tasli, Marten den Uyl
- **Comment**: Accepted at ECCV 2016 Workshop on: Brave new ideas for motion
  representations in videos
- **Journal**: None
- **Summary**: This paper explores the capabilities of convolutional neural networks to deal with a task that is easily manageable for humans: perceiving 3D pose of a human body from varying angles. However, in our approach, we are restricted to using a monocular vision system. For this purpose, we apply a convolutional neural network approach on RGB videos and extend it to three dimensional convolutions. This is done via encoding the time dimension in videos as the 3\ts{rd} dimension in convolutional space, and directly regressing to human body joint positions in 3D coordinate space. This research shows the ability of such a network to achieve state-of-the-art performance on the selected Human3.6M dataset, thus demonstrating the possibility of successfully representing temporal data with an additional dimension in the convolutional operation.



### Analysis of the Self Projected Matching Pursuit Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1609.00053v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1609.00053v3)
- **Published**: 2016-08-31 21:58:17+00:00
- **Updated**: 2020-06-08 15:36:34+00:00
- **Authors**: Laura Rebollo-Neira, Miroslav Rozloznik, Pradip Sasmal
- **Comment**: The routines for implementing the methods, as well as scripts to
  reproduce the examples in the manuscript, are available on the website:
  http://www.nonlinear-approx.info/examples/node04.html
- **Journal**: None
- **Summary**: The convergence and numerical analysis of a low memory implementation of the Orthogonal Matching Pursuit greedy strategy, which is termed Self Projected Matching Pursuit, is presented. This approach renders an iterative way of solving the least squares problem with much less storage requirement than direct linear algebra techniques. Hence, it appropriate for solving large linear systems. The analysis highlights its suitability within the class of well posed problems.



