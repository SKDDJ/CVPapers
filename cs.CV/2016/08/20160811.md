# Arxiv Papers in cs.CV on 2016-08-11
### Enabling My Robot To Play Pictionary : Recurrent Neural Networks For Sketch Recognition
- **Arxiv ID**: http://arxiv.org/abs/1608.03369v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.03369v1)
- **Published**: 2016-08-11 05:11:23+00:00
- **Updated**: 2016-08-11 05:11:23+00:00
- **Authors**: Ravi Kiran Sarvadevabhatla, Jogendra Kundu, Babu R. Venkatesh
- **Comment**: Accepted at ACMMM 2016. Code and models at
  https://github.com/val-iisc/sketch-object-recognition
- **Journal**: None
- **Summary**: Freehand sketching is an inherently sequential process. Yet, most approaches for hand-drawn sketch recognition either ignore this sequential aspect or exploit it in an ad-hoc manner. In our work, we propose a recurrent neural network architecture for sketch object recognition which exploits the long-term sequential and structural regularities in stroke data in a scalable manner. Specifically, we introduce a Gated Recurrent Unit based framework which leverages deep sketch features and weighted per-timestep loss to achieve state-of-the-art results on a large database of freehand object sketches across a large number of object categories. The inherently online nature of our framework is especially suited for on-the-fly recognition of objects as they are being drawn. Thus, our framework can enable interesting applications such as camera-equipped robots playing the popular party game Pictionary with human players and generating sparsified yet recognizable sketches of objects.



### Automatic text extraction and character segmentation using maximally stable extremal regions
- **Arxiv ID**: http://arxiv.org/abs/1608.03374v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.03374v1)
- **Published**: 2016-08-11 05:34:42+00:00
- **Updated**: 2016-08-11 05:34:42+00:00
- **Authors**: Nitigya Sambyal, Pawanesh Abrol
- **Comment**: None
- **Journal**: International Journal of Modern Computer Science,Vol 4,Issue
  3,pp.136-141,2016
- **Summary**: Text detection and segmentation is an important prerequisite for many content based image analysis tasks. The paper proposes a novel text extraction and character segmentation algorithm using Maximally Stable Extremal Regions as basic letter candidates. These regions are then subjected to thresholding and thereafter various connected components are determined to identify separate characters. The algorithm is tested along a set of various JPEG, PNG and BMP images over four different character sets; English, Russian, Hindi and Urdu. The algorithm gives good results for English and Russian character set; however character segmentation in Urdu and Hindi language is not much accurate. The algorithm is simple, efficient, involves no overhead as required in training and gives good results for even low quality images. The paper also proposes various challenges in text extraction and segmentation for multilingual inputs.



### A machine learning method for the large-scale evaluation of urban visual environment
- **Arxiv ID**: http://arxiv.org/abs/1608.03396v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1608.03396v1)
- **Published**: 2016-08-11 08:29:07+00:00
- **Updated**: 2016-08-11 08:29:07+00:00
- **Authors**: Lun Liu, Hui Wang, Chunyang Wu
- **Comment**: 16 pages, 6 figures
- **Journal**: None
- **Summary**: Given the size of modern cities in the urbanising age, it is beyond the perceptual capacity of most people to develop a good knowledge about the beauty and ugliness of the city at every street corner. Correspondingly, for planners, it is also difficult to accurately answer questions like 'where are the worst-looking places in the city that regeneration should give first consideration', or 'in the fast urbanising cities, how is the city appearance changing', etc. To address this issue, we here present a computer vision method for the large-scale and automatic evaluation of the urban visual environment, by leveraging state-of-the-art machine learning techniques and the wide-coverage street view images. From the various factors that are at work, we choose two key features, the visual quality of street facade and the continuity of street wall, as the starting point of this line of analysis. In order to test the validity of this method, we further compare the machine ratings with ratings collected on site from 752 passers-by on fifty-six locations. We show that the machine learning model can produce a good estimation of people's real visual experience, and it holds much potential for various tasks in terms of urban design evaluation, culture identification, etc.



### Solving Visual Madlibs with Multiple Cues
- **Arxiv ID**: http://arxiv.org/abs/1608.03410v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.03410v1)
- **Published**: 2016-08-11 09:51:21+00:00
- **Updated**: 2016-08-11 09:51:21+00:00
- **Authors**: Tatiana Tommasi, Arun Mallya, Bryan Plummer, Svetlana Lazebnik, Alexander C. Berg, Tamara L. Berg
- **Comment**: accepted at BMVC 2016
- **Journal**: None
- **Summary**: This paper focuses on answering fill-in-the-blank style multiple choice questions from the Visual Madlibs dataset. Previous approaches to Visual Question Answering (VQA) have mainly used generic image features from networks trained on the ImageNet dataset, despite the wide scope of questions. In contrast, our approach employs features derived from networks trained for specialized tasks of scene classification, person activity prediction, and person and object attribute prediction. We also present a method for selecting sub-regions of an image that are relevant for evaluating the appropriateness of a putative answer. Visual features are computed both from the whole image and from local regions, while sentences are mapped to a common space using a simple normalized canonical correlation analysis (CCA) model. Our results show a significant improvement over the previous state of the art, and indicate that answering different question types benefits from examining a variety of image cues and carefully choosing informative image sub-regions.



### Recurrent Neural Networks to Correct Satellite Image Classification Maps
- **Arxiv ID**: http://arxiv.org/abs/1608.03440v3
- **DOI**: 10.1109/TGRS.2017.2697453
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.03440v3)
- **Published**: 2016-08-11 12:49:46+00:00
- **Updated**: 2017-04-21 11:08:37+00:00
- **Authors**: Emmanuel Maggiori, Guillaume Charpiat, Yuliya Tarabalka, Pierre Alliez
- **Comment**: None
- **Journal**: None
- **Summary**: While initially devised for image categorization, convolutional neural networks (CNNs) are being increasingly used for the pixelwise semantic labeling of images. However, the proper nature of the most common CNN architectures makes them good at recognizing but poor at localizing objects precisely. This problem is magnified in the context of aerial and satellite image labeling, where a spatially fine object outlining is of paramount importance. Different iterative enhancement algorithms have been presented in the literature to progressively improve the coarse CNN outputs, seeking to sharpen object boundaries around real image edges. However, one must carefully design, choose and tune such algorithms. Instead, our goal is to directly learn the iterative process itself. For this, we formulate a generic iterative enhancement process inspired from partial differential equations, and observe that it can be expressed as a recurrent neural network (RNN). Consequently, we train such a network from manually labeled data for our enhancement task. In a series of experiments we show that our RNN effectively learns an iterative process that significantly improves the quality of satellite image classification maps.



### Multi-View Product Image Search Using Deep ConvNets Representations
- **Arxiv ID**: http://arxiv.org/abs/1608.03462v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1608.03462v2)
- **Published**: 2016-08-11 13:50:07+00:00
- **Updated**: 2017-05-01 08:08:28+00:00
- **Authors**: Muhammet Bastan, Ozgur Yilmaz
- **Comment**: 13 pages, 16 figures
- **Journal**: None
- **Summary**: Multi-view product image queries can improve retrieval performance over single view queries significantly. In this paper, we investigated the performance of deep convolutional neural networks (ConvNets) on multi-view product image search. First, we trained a VGG-like network to learn deep ConvNets representations of product images. Then, we computed the deep ConvNets representations of database and query images and performed single view queries, and multi-view queries using several early and late fusion approaches.   We performed extensive experiments on the publicly available Multi-View Object Image Dataset (MVOD 5K) with both clean background queries from the Internet and cluttered background queries from a mobile phone. We compared the performance of ConvNets to the classical bag-of-visual-words (BoWs). We concluded that (1) multi-view queries with deep ConvNets representations perform significantly better than single view queries, (2) ConvNets perform much better than BoWs and have room for further improvement, (3) pre-training of ConvNets on a different image dataset with background clutter is needed to obtain good performance on cluttered product image queries obtained with a mobile phone.



### Learning Dynamic Hierarchical Models for Anytime Scene Labeling
- **Arxiv ID**: http://arxiv.org/abs/1608.03474v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.03474v1)
- **Published**: 2016-08-11 14:19:31+00:00
- **Updated**: 2016-08-11 14:19:31+00:00
- **Authors**: Buyu Liu, Xuming He
- **Comment**: Accepted by ECCV 2016
- **Journal**: None
- **Summary**: With increasing demand for efficient image and video analysis, test-time cost of scene parsing becomes critical for many large-scale or time-sensitive vision applications. We propose a dynamic hierarchical model for anytime scene labeling that allows us to achieve flexible trade-offs between efficiency and accuracy in pixel-level prediction. In particular, our approach incorporates the cost of feature computation and model inference, and optimizes the model performance for any given test-time budget by learning a sequence of image-adaptive hierarchical models. We formulate this anytime representation learning as a Markov Decision Process with a discrete-continuous state-action space. A high-quality policy of feature and model selection is learned based on an approximate policy iteration method with action proposal mechanism. We demonstrate the advantages of our dynamic non-myopic anytime scene parsing on three semantic segmentation datasets, which achieves $90\%$ of the state-of-the-art performances by using $15\%$ of their overall costs.



### Clockwork Convnets for Video Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1608.03609v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.03609v1)
- **Published**: 2016-08-11 20:32:55+00:00
- **Updated**: 2016-08-11 20:32:55+00:00
- **Authors**: Evan Shelhamer, Kate Rakelly, Judy Hoffman, Trevor Darrell
- **Comment**: None
- **Journal**: None
- **Summary**: Recent years have seen tremendous progress in still-image segmentation; however the na\"ive application of these state-of-the-art algorithms to every video frame requires considerable computation and ignores the temporal continuity inherent in video. We propose a video recognition framework that relies on two key observations: 1) while pixels may change rapidly from frame to frame, the semantic content of a scene evolves more slowly, and 2) execution can be viewed as an aspect of architecture, yielding purpose-fit computation schedules for networks. We define a novel family of "clockwork" convnets driven by fixed or adaptive clock signals that schedule the processing of different layers at different update rates according to their semantic stability. We design a pipeline schedule to reduce latency for real-time recognition and a fixed-rate schedule to reduce overall computation. Finally, we extend clockwork scheduling to adaptive video processing by incorporating data-driven clocks that can be tuned on unlabeled video. The accuracy and efficiency of clockwork convnets are evaluated on the Youtube-Objects, NYUD, and Cityscapes video datasets.



### Automatic detection of moving objects in video surveillance
- **Arxiv ID**: http://arxiv.org/abs/1608.03617v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.03617v1)
- **Published**: 2016-08-11 20:53:48+00:00
- **Updated**: 2016-08-11 20:53:48+00:00
- **Authors**: Larbi Guezouli, Hanane Belhani
- **Comment**: None
- **Journal**: None
- **Summary**: This work is in the field of video surveillance including motion detection. The video surveillance is one of essential techniques for automatic video analysis to extract crucial information or relevant scenes in video surveillance systems. The aim of our work is to propose solutions for the automatic detection of moving objects in real time with a surveillance camera. The detected objects are objects that have some geometric shape (circle, ellipse, square, and rectangle).



### Distributed-memory large deformation diffeomorphic 3D image registration
- **Arxiv ID**: http://arxiv.org/abs/1608.03630v1
- **DOI**: 10.1109/SC.2016.71
- **Categories**: **cs.DC**, cs.CV, math.OC, 68U10, 49J20, 35Q93, 65K10, 76D55, 90C20, D.1.3; I.4.3; G.1.8; F.2.1; G.1.0
- **Links**: [PDF](http://arxiv.org/pdf/1608.03630v1)
- **Published**: 2016-08-11 22:52:27+00:00
- **Updated**: 2016-08-11 22:52:27+00:00
- **Authors**: Andreas Mang, Amir Gholami, George Biros
- **Comment**: accepted for publication at SC16 in Salt Lake City, Utah, USA;
  November 2016
- **Journal**: Proceedings of the International Conference for High Performance
  Computing, Networking, Storage and Analysis, Article No. 72, 2016
- **Summary**: We present a parallel distributed-memory algorithm for large deformation diffeomorphic registration of volumetric images that produces large isochoric deformations (locally volume preserving). Image registration is a key technology in medical image analysis. Our algorithm uses a partial differential equation constrained optimal control formulation. Finding the optimal deformation map requires the solution of a highly nonlinear problem that involves pseudo-differential operators, biharmonic operators, and pure advection operators both forward and back- ward in time. A key issue is the time to solution, which poses the demand for efficient optimization methods as well as an effective utilization of high performance computing resources. To address this problem we use a preconditioned, inexact, Gauss-Newton- Krylov solver. Our algorithm integrates several components: a spectral discretization in space, a semi-Lagrangian formulation in time, analytic adjoints, different regularization functionals (including volume-preserving ones), a spectral preconditioner, a highly optimized distributed Fast Fourier Transform, and a cubic interpolation scheme for the semi-Lagrangian time-stepping. We demonstrate the scalability of our algorithm on images with resolution of up to $1024^3$ on the "Maverick" and "Stampede" systems at the Texas Advanced Computing Center (TACC). The critical problem in the medical imaging application domain is strong scaling, that is, solving registration problems of a moderate size of $256^3$---a typical resolution for medical images. We are able to solve the registration problem for images of this size in less than five seconds on 64 x86 nodes of TACC's "Maverick" system.



