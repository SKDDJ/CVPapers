# Arxiv Papers in cs.CV on 2016-08-06
### Compressive Change Retrieval for Moving Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1608.02051v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.02051v1)
- **Published**: 2016-08-06 02:04:25+00:00
- **Updated**: 2016-08-06 02:04:25+00:00
- **Authors**: Tomoya Murase, Kanji Tanaka
- **Comment**: 6 pages, 6 figures, Draft of a paper submitted to an International
  Conference
- **Journal**: None
- **Summary**: Change detection, or anomaly detection, from street-view images acquired by an autonomous robot at multiple different times, is a major problem in robotic mapping and autonomous driving. Formulation as an image comparison task, which operates on a given pair of query and reference images is common to many existing approaches to this problem. Unfortunately, providing relevant reference images is not straightforward. In this paper, we propose a novel formulation for change detection, termed compressive change retrieval, which can operate on a query image and similar reference images retrieved from the web. Compared to previous formulations, there are two sources of difficulty. First, the retrieved reference images may frequently contain non-relevant reference images, because even state-of-the-art place-recognition techniques suffer from retrieval noise. Second, image comparison needs to be conducted in a compressed domain to minimize the storage cost of large collections of street-view images. To address the above issues, we also present a practical change detection algorithm that uses compressed bag-of-words (BoW) image representation as a scalable solution. The results of experiments conducted on a practical change detection task, "moving object detection (MOD)," using the publicly available Malaga dataset validate the effectiveness of the proposed approach.



### Multi-Model Hypothesize-and-Verify Approach for Incremental Loop Closure Verification
- **Arxiv ID**: http://arxiv.org/abs/1608.02052v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.02052v1)
- **Published**: 2016-08-06 02:06:23+00:00
- **Updated**: 2016-08-06 02:06:23+00:00
- **Authors**: Kanji Tanaka
- **Comment**: 6 pages, 8 figures, Draft of a paper submitted to an International
  Conference. arXiv admin note: text overlap with arXiv:1509.07611
- **Journal**: None
- **Summary**: Loop closure detection, which is the task of identifying locations revisited by a robot in a sequence of odometry and perceptual observations, is typically formulated as a visual place recognition (VPR) task. However, even state-of-the-art VPR techniques generate a considerable number of false positives as a result of confusing visual features and perceptual aliasing. In this paper, we propose a robust incremental framework for loop closure detection, termed incremental loop closure verification. Our approach reformulates the problem of loop closure detection as an instance of a multi-model hypothesize-and-verify framework, in which multiple loop closure hypotheses are generated and verified in terms of the consistency between loop closure hypotheses and VPR constraints at multiple viewpoints along the robot's trajectory. Furthermore, we consider the general incremental setting of loop closure detection, in which the system must update both the set of VPR constraints and that of loop closure hypotheses when new constraints or hypotheses arrive during robot navigation. Experimental results using a stereo SLAM system and DCNN features and visual odometry validate effectiveness of the proposed approach.



### Signs in time: Encoding human motion as a temporal image
- **Arxiv ID**: http://arxiv.org/abs/1608.02059v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.02059v1)
- **Published**: 2016-08-06 03:37:28+00:00
- **Updated**: 2016-08-06 03:37:28+00:00
- **Authors**: Joon Son Chung, Andrew Zisserman
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of this work is to recognise and localise short temporal signals in image time series, where strong supervision is not available for training.   To this end we propose an image encoding that concisely represents human motion in a video sequence in a form that is suitable for learning with a ConvNet. The encoding reduces the pose information from an image to a single column, dramatically diminishing the input requirements for the network, but retaining the essential information for recognition.   The encoding is applied to the task of recognizing and localizing signed gestures in British Sign Language (BSL) videos. We demonstrate that using the proposed encoding, signs as short as 10 frames duration can be learnt from clips lasting hundreds of frames using only weak (clip level) supervision and with considerable label noise.



### Spoofing 2D Face Detection: Machines See People Who Aren't There
- **Arxiv ID**: http://arxiv.org/abs/1608.02128v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1608.02128v1)
- **Published**: 2016-08-06 16:50:26+00:00
- **Updated**: 2016-08-06 16:50:26+00:00
- **Authors**: Michael McCoyd, David Wagner
- **Comment**: 9 pages, 19 figures, submitted to AISec
- **Journal**: None
- **Summary**: Machine learning is increasingly used to make sense of the physical world yet may suffer from adversarial manipulation. We examine the Viola-Jones 2D face detection algorithm to study whether images can be created that humans do not notice as faces yet the algorithm detects as faces. We show that it is possible to construct images that Viola-Jones recognizes as containing faces yet no human would consider a face. Moreover, we show that it is possible to construct images that fool facial detection even when they are printed and then photographed.



### Leveraging Union of Subspace Structure to Improve Constrained Clustering
- **Arxiv ID**: http://arxiv.org/abs/1608.02146v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1608.02146v2)
- **Published**: 2016-08-06 19:29:58+00:00
- **Updated**: 2017-09-13 21:17:33+00:00
- **Authors**: John Lipor, Laura Balzano
- **Comment**: 11 pages, 8 figures
- **Journal**: Proceedings of the 34th International Conference on Machine
  Learning, in PMLR 70:2130-2139 (2017)
- **Summary**: Many clustering problems in computer vision and other contexts are also classification problems, where each cluster shares a meaningful label. Subspace clustering algorithms in particular are often applied to problems that fit this description, for example with face images or handwritten digits. While it is straightforward to request human input on these datasets, our goal is to reduce this input as much as possible. We present a pairwise-constrained clustering algorithm that actively selects queries based on the union-of-subspaces model. The central step of the algorithm is in querying points of minimum margin between estimated subspaces; analogous to classifier margin, these lie near the decision boundary. We prove that points lying near the intersection of subspaces are points with low margin. Our procedure can be used after any subspace clustering algorithm that outputs an affinity matrix. We demonstrate on several datasets that our algorithm drives the clustering error down considerably faster than the state-of-the-art active query algorithms on datasets with subspace structure and is competitive on other datasets.



### Adapting Deep Network Features to Capture Psychological Representations
- **Arxiv ID**: http://arxiv.org/abs/1608.02164v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1608.02164v1)
- **Published**: 2016-08-06 23:49:48+00:00
- **Updated**: 2016-08-06 23:49:48+00:00
- **Authors**: Joshua C. Peterson, Joshua T. Abbott, Thomas L. Griffiths
- **Comment**: 6 pages, 4 figures, To appear in the Proceedings of the 38th Annual
  Conference of the Cognitive Science Society, Winner of the Computational
  Modeling Prize in Perception/Action
- **Journal**: None
- **Summary**: Deep neural networks have become increasingly successful at solving classic perception problems such as object recognition, semantic segmentation, and scene understanding, often reaching or surpassing human-level accuracy. This success is due in part to the ability of DNNs to learn useful representations of high-dimensional inputs, a problem that humans must also solve. We examine the relationship between the representations learned by these networks and human psychological representations recovered from similarity judgments. We find that deep features learned in service of object classification account for a significant amount of the variance in human similarity judgments for a set of animal images. However, these features do not capture some qualitative distinctions that are a key part of human representations. To remedy this, we develop a method for adapting deep features to align with human similarity judgments, resulting in image representations that can potentially be used to extend the scope of psychological experiments.



