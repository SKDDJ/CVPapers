# Arxiv Papers in cs.CV on 2016-08-07
### ShapeFit and ShapeKick for Robust, Scalable Structure from Motion
- **Arxiv ID**: http://arxiv.org/abs/1608.02165v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, math.NA, math.OC, 68T45, I.2.10; I.4
- **Links**: [PDF](http://arxiv.org/pdf/1608.02165v1)
- **Published**: 2016-08-07 00:29:53+00:00
- **Updated**: 2016-08-07 00:29:53+00:00
- **Authors**: Thomas Goldstein, Paul Hand, Choongbum Lee, Vladislav Voroninski, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a new method for location recovery from pair-wise directions that leverages an efficient convex program that comes with exact recovery guarantees, even in the presence of adversarial outliers. When pairwise directions represent scaled relative positions between pairs of views (estimated for instance with epipolar geometry) our method can be used for location recovery, that is the determination of relative pose up to a single unknown scale. For this task, our method yields performance comparable to the state-of-the-art with an order of magnitude speed-up. Our proposed numerical framework is flexible in that it accommodates other approaches to location recovery and can be used to speed up other methods. These properties are demonstrated by extensively testing against state-of-the-art methods for location recovery on 13 large, irregular collections of images of real scenes in addition to simulated data with ground truth.



### Multiview Cauchy Estimator Feature Embedding for Depth and Inertial Sensor-Based Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1608.02183v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.02183v2)
- **Published**: 2016-08-07 05:50:10+00:00
- **Updated**: 2016-09-04 08:37:23+00:00
- **Authors**: Yanan Guo, Lei Li, Weifeng Liu, Jun Cheng, Dapeng Tao
- **Comment**: This paper has been withdrawn by the author due to a crucial error
- **Journal**: None
- **Summary**: The ever-growing popularity of Kinect and inertial sensors has prompted intensive research efforts on human action recognition. Since human actions can be characterized by multiple feature representations extracted from Kinect and inertial sensors, multiview features must be encoded into a unified space optimal for human action recognition. In this paper, we propose a new unsupervised feature fusion method termed Multiview Cauchy Estimator Feature Embedding (MCEFE) for human action recognition. By minimizing empirical risk, MCEFE integrates the encoded complementary information in multiple views to find the unified data representation and the projection matrices. To enhance robustness to outliers, the Cauchy estimator is imposed on the reconstruction error. Furthermore, ensemble manifold regularization is enforced on the projection matrices to encode the correlations between different views and avoid overfitting. Experiments are conducted on the new Chinese Academy of Sciences - Yunnan University - Multimodal Human Action Database (CAS-YNU-MHAD) to demonstrate the effectiveness and robustness of MCEFE for human action recognition.



### Playing for Data: Ground Truth from Computer Games
- **Arxiv ID**: http://arxiv.org/abs/1608.02192v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1608.02192v1)
- **Published**: 2016-08-07 08:20:14+00:00
- **Updated**: 2016-08-07 08:20:14+00:00
- **Authors**: Stephan R. Richter, Vibhav Vineet, Stefan Roth, Vladlen Koltun
- **Comment**: Accepted to the 14th European Conference on Computer Vision (ECCV
  2016)
- **Journal**: None
- **Summary**: Recent progress in computer vision has been driven by high-capacity models trained on large datasets. Unfortunately, creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required. In this paper, we present an approach to rapidly creating pixel-accurate semantic label maps for images extracted from modern computer games. Although the source code and the internal operation of commercial games are inaccessible, we show that associations between image patches can be reconstructed from the communication between the game and the graphics hardware. This enables rapid propagation of semantic labels within and across images synthesized by the game, with no access to the source code or the content. We validate the presented approach by producing dense pixel-level semantic annotations for 25 thousand images synthesized by a photorealistic open-world computer game. Experiments on semantic segmentation datasets show that using the acquired data to supplement real-world images significantly increases accuracy and that the acquired data enables reducing the amount of hand-labeled real-world data: models trained with game data and just 1/3 of the CamVid training set outperform models trained on the complete CamVid training set.



### Residual CNDS
- **Arxiv ID**: http://arxiv.org/abs/1608.02201v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.02201v1)
- **Published**: 2016-08-07 10:34:02+00:00
- **Updated**: 2016-08-07 10:34:02+00:00
- **Authors**: Hussein A. Al-Barazanchi, Hussam Qassim, Abhishek Verma
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural networks nowadays are of tremendous importance for any image classification system. One of the most investigated methods to increase the accuracy of CNN is by increasing the depth of CNN. Increasing the depth by stacking more layers also increases the difficulty of training besides making it computationally expensive. Some research found that adding auxiliary forks after intermediate layers increases the accuracy. Specifying which intermediate layer shoud have the fork just addressed recently. Where a simple rule were used to detect the position of intermediate layers that needs the auxiliary supervision fork. This technique known as convolutional neural networks with deep supervision (CNDS). This technique enhanced the accuracy of classification over the straight forward CNN used on the MIT places dataset and ImageNet. In the other side, Residual Learning is another technique emerged recently to ease the training of very deep CNN. Residual Learning framwork changed the learning of layers from unreferenced functions to learning residual function with regard to the layer's input. Residual Learning achieved state of arts results on ImageNet 2015 and COCO competitions. In this paper, we study the effect of adding residual connections to CNDS network. Our experiments results show increasing of accuracy over using CNDS only.



### Bootstrapping Face Detection with Hard Negative Examples
- **Arxiv ID**: http://arxiv.org/abs/1608.02236v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.02236v1)
- **Published**: 2016-08-07 16:10:50+00:00
- **Updated**: 2016-08-07 16:10:50+00:00
- **Authors**: Shaohua Wan, Zhijun Chen, Tao Zhang, Bo Zhang, Kong-kat Wong
- **Comment**: None
- **Journal**: None
- **Summary**: Recently significant performance improvement in face detection was made possible by deeply trained convolutional networks. In this report, a novel approach for training state-of-the-art face detector is described. The key is to exploit the idea of hard negative mining and iteratively update the Faster R-CNN based face detector with the hard negatives harvested from a large set of background examples. We demonstrate that our face detector outperforms state-of-the-art detectors on the FDDB dataset, which is the de facto standard for evaluating face detection algorithms.



### Deep Learning a Grasp Function for Grasping under Gripper Pose Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/1608.02239v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1608.02239v1)
- **Published**: 2016-08-07 16:30:42+00:00
- **Updated**: 2016-08-07 16:30:42+00:00
- **Authors**: Edward Johns, Stefan Leutenegger, Andrew J. Davison
- **Comment**: IROS 2016
- **Journal**: None
- **Summary**: This paper presents a new method for parallel-jaw grasping of isolated objects from depth images, under large gripper pose uncertainty. Whilst most approaches aim to predict the single best grasp pose from an image, our method first predicts a score for every possible grasp pose, which we denote the grasp function. With this, it is possible to achieve grasping robust to the gripper's pose uncertainty, by smoothing the grasp function with the pose uncertainty function. Therefore, if the single best pose is adjacent to a region of poor grasp quality, that pose will no longer be chosen, and instead a pose will be chosen which is surrounded by a region of high grasp quality. To learn this function, we train a Convolutional Neural Network which takes as input a single depth image of an object, and outputs a score for each grasp pose across the image. Training data for this is generated by use of physics simulation and depth image simulation with 3D object meshes, to enable acquisition of sufficient data without requiring exhaustive real-world experiments. We evaluate with both synthetic and real experiments, and show that the learned grasp score is more robust to gripper pose uncertainty than when this uncertainty is not accounted for.



### Edge Based Grid Super-Imposition for Crowd Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/1610.05566v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1610.05566v1)
- **Published**: 2016-08-07 17:00:17+00:00
- **Updated**: 2016-08-07 17:00:17+00:00
- **Authors**: Amol Patwardhan
- **Comment**: 6 pages, 6 figure, 1 table, emotion, crowd, group, spontaneous,
  indoor, edge, grid, mesh, tracking, temporal feature
- **Journal**: None
- **Summary**: Numerous automatic continuous emotion detection system studies have examined mostly use of videos and images containing individual person expressing emotions. This study examines the detection of spontaneous emotions in a group and crowd settings. Edge detection was used with a grid of lines superimposition to extract the features. The feature movement in terms of movement from the reference point was used to track across sequences of images from the color channel. Additionally the video data capturing was done on spontaneous emotions invoked by watching sports events from group of participants. The method was view and occlusion independent and the results were not affected by presence of multiple people chaotically expressing various emotions. The edge thresholds of 0.2 and grid thresholds of 20 showed the best accuracy results. The overall accuracy of the group emotion classifier was 70.9%.



### Spontaneous Facial Micro-Expression Recognition using Discriminative Spatiotemporal Local Binary Pattern with an Improved Integral Projection
- **Arxiv ID**: http://arxiv.org/abs/1608.02255v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.02255v1)
- **Published**: 2016-08-07 18:17:17+00:00
- **Updated**: 2016-08-07 18:17:17+00:00
- **Authors**: Xiaohua Huang, Sujing Wang, Xin Liu, Guoying Zhao, Xiaoyi Feng, Matti Pietikainen
- **Comment**: 13pages, 8 figures, 5 tables, submitted to IEEE Transactions on Image
  Processing
- **Journal**: None
- **Summary**: Recently, there are increasing interests in inferring mirco-expression from facial image sequences. Due to subtle facial movement of micro-expressions, feature extraction has become an important and critical issue for spontaneous facial micro-expression recognition. Recent works usually used spatiotemporal local binary pattern for micro-expression analysis. However, the commonly used spatiotemporal local binary pattern considers dynamic texture information to represent face images while misses the shape attribute of face images. On the other hand, their works extracted the spatiotemporal features from the global face regions, which ignore the discriminative information between two micro-expression classes. The above-mentioned problems seriously limit the application of spatiotemporal local binary pattern on micro-expression recognition. In this paper, we propose a discriminative spatiotemporal local binary pattern based on an improved integral projection to resolve the problems of spatiotemporal local binary pattern for micro-expression recognition. Firstly, we develop an improved integral projection for preserving the shape attribute of micro-expressions. Furthermore, an improved integral projection is incorporated with local binary pattern operators across spatial and temporal domains. Specifically, we extract the novel spatiotemporal features incorporating shape attributes into spatiotemporal texture features. For increasing the discrimination of micro-expressions, we propose a new feature selection based on Laplacian method to extract the discriminative information for facial micro-expression recognition. Intensive experiments are conducted on three availably published micro-expression databases. We compare our method with the state-of-the-art algorithms. Experimental results demonstrate that our proposed method achieves promising performance for micro-expression recognition.



