# Arxiv Papers in cs.CV on 2016-08-03
### Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution
- **Arxiv ID**: http://arxiv.org/abs/1608.01041v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.01041v2)
- **Published**: 2016-08-03 01:26:32+00:00
- **Updated**: 2016-09-24 01:20:03+00:00
- **Authors**: Emad Barsoum, Cha Zhang, Cristian Canton Ferrer, Zhengyou Zhang
- **Comment**: Submitted to ICMI 2016
- **Journal**: None
- **Summary**: Crowd sourcing has become a widely adopted scheme to collect ground truth labels. However, it is a well-known problem that these labels can be very noisy. In this paper, we demonstrate how to learn a deep convolutional neural network (DCNN) from noisy labels, using facial expression recognition as an example. More specifically, we have 10 taggers to label each input image, and compare four different approaches to utilizing the multiple labels: majority voting, multi-label learning, probabilistic label drawing, and cross-entropy loss. We show that the traditional majority voting scheme does not perform as well as the last two approaches that fully leverage the label distribution. An enhanced FER+ data set with multiple labels for each face image will also be shared with the research community.



### Analyzing Linear Dynamical Systems: From Modeling to Coding and Learning
- **Arxiv ID**: http://arxiv.org/abs/1608.01059v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.01059v2)
- **Published**: 2016-08-03 02:44:57+00:00
- **Updated**: 2017-02-27 02:42:26+00:00
- **Authors**: Wenbing Huang, Fuchun Sun, Lele Cao, Mehrtash Harandi
- **Comment**: 38 pages
- **Journal**: None
- **Summary**: Encoding time-series with Linear Dynamical Systems (LDSs) leads to rich models with applications ranging from dynamical texture recognition to video segmentation to name a few. In this paper, we propose to represent LDSs with infinite-dimensional subspaces and derive an analytic solution to obtain stable LDSs. We then devise efficient algorithms to perform sparse coding and dictionary learning on the space of infinite-dimensional subspaces. In particular, two solutions are developed to sparsely encode an LDS. In the first method, we map the subspaces into a Reproducing Kernel Hilbert Space (RKHS) and achieve our goal through kernel sparse coding. As for the second solution, we propose to embed the infinite-dimensional subspaces into the space of symmetric matrices and formulate the sparse coding accordingly in the induced space. For dictionary learning, we encode time-series by introducing a novel concept, namely the two-fold LDSs. We then make use of the two-fold LDSs to derive an analytical form for updating atoms of an LDS dictionary, i.e., each atom is an LDS itself. Compared to several baselines and state-of-the-art methods, the proposed methods yield higher accuracies in various classification tasks including video classification and tactile recognition.



### FPGA system for real-time computational extended depth of field imaging using phase aperture coding
- **Arxiv ID**: http://arxiv.org/abs/1608.01074v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.01074v1)
- **Published**: 2016-08-03 05:28:18+00:00
- **Updated**: 2016-08-03 05:28:18+00:00
- **Authors**: Tal Remez, Or Litany, Shachar Yoseff, Harel Haim, Alex Bronstein
- **Comment**: None
- **Journal**: None
- **Summary**: We present a proof-of-concept end-to-end system for computational extended depth of field (EDOF) imaging. The acquisition is performed through a phase-coded aperture implemented by placing a thin wavelength-dependent optical mask inside the pupil of a conventional camera lens, as a result of which, each color channel is focused at a different depth. The reconstruction process receives the raw Bayer image as the input, and performs blind estimation of the output color image in focus at an extended range of depths using a patch-wise sparse prior. We present a fast non-iterative reconstruction algorithm operating with constant latency in fixed-point arithmetics and achieving real-time performance in a prototype FPGA implementation. The output of the system, on simulated and real-life scenes, is qualitatively and quantitatively better than the result of clear-aperture imaging followed by state-of-the-art blind deblurring.



### Challenges in video based object detection in maritime scenario using computer vision
- **Arxiv ID**: http://arxiv.org/abs/1608.01079v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.01079v1)
- **Published**: 2016-08-03 05:55:18+00:00
- **Updated**: 2016-08-03 05:55:18+00:00
- **Authors**: D. K. Prasad, C. K. Prasath, D. Rajan, L. Rachmawati, E. Rajabaly, C. Quek
- **Comment**: None
- **Journal**: None
- **Summary**: This paper discusses the technical challenges in maritime image processing and machine vision problems for video streams generated by cameras. Even well documented problems of horizon detection and registration of frames in a video are very challenging in maritime scenarios. More advanced problems of background subtraction and object detection in video streams are very challenging. Challenges arising from the dynamic nature of the background, unavailability of static cues, presence of small objects at distant backgrounds, illumination effects, all contribute to the challenges as discussed here.



### Learning Common and Specific Features for RGB-D Semantic Segmentation with Deconvolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1608.01082v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.01082v1)
- **Published**: 2016-08-03 06:05:16+00:00
- **Updated**: 2016-08-03 06:05:16+00:00
- **Authors**: Jinghua Wang, Zhenhua Wang, Dacheng Tao, Simon See, Gang Wang
- **Comment**: ECCV 2016, 16 pages, 3 figures
- **Journal**: None
- **Summary**: In this paper, we tackle the problem of RGB-D semantic segmentation of indoor images. We take advantage of deconvolutional networks which can predict pixel-wise class labels, and develop a new structure for deconvolution of multiple modalities. We propose a novel feature transformation network to bridge the convolutional networks and deconvolutional networks. In the feature transformation network, we correlate the two modalities by discovering common features between them, as well as characterize each modality by discovering modality specific features. With the common features, we not only closely correlate the two modalities, but also allow them to borrow features from each other to enhance the representation of shared information. With specific features, we capture the visual patterns that are only visible in one modality. The proposed network achieves competitive segmentation accuracy on NYU depth dataset V1 and V2.



### Autonomous Grounding of Visual Field Experience through Sensorimotor Prediction
- **Arxiv ID**: http://arxiv.org/abs/1608.01127v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1608.01127v1)
- **Published**: 2016-08-03 09:25:35+00:00
- **Updated**: 2016-08-03 09:25:35+00:00
- **Authors**: Alban Laflaquière
- **Comment**: 6 pages, 4 figures, ICDL-Epirob 2016
- **Journal**: None
- **Summary**: In a developmental framework, autonomous robots need to explore the world and learn how to interact with it. Without an a priori model of the system, this opens the challenging problem of having robots master their interface with the world: how to perceive their environment using their sensors, and how to act in it using their motors. The sensorimotor approach of perception claims that a naive agent can learn to master this interface by capturing regularities in the way its actions transform its sensory inputs. In this paper, we apply such an approach to the discovery and mastery of the visual field associated with a visual sensor. A computational model is formalized and applied to a simulated system to illustrate the approach.



### Cascaded Continuous Regression for Real-time Incremental Face Tracking
- **Arxiv ID**: http://arxiv.org/abs/1608.01137v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.01137v2)
- **Published**: 2016-08-03 10:17:22+00:00
- **Updated**: 2016-08-06 21:03:34+00:00
- **Authors**: Enrique Sánchez-Lozano, Brais Martinez, Georgios Tzimiropoulos, Michel Valstar
- **Comment**: ECCV 2016 accepted paper, with supplementary material included as
  appendices. References to Equations fixed
- **Journal**: None
- **Summary**: This paper introduces a novel real-time algorithm for facial landmark tracking. Compared to detection, tracking has both additional challenges and opportunities. Arguably the most important aspect in this domain is updating a tracker's models as tracking progresses, also known as incremental (face) tracking. While this should result in more accurate localisation, how to do this online and in real time without causing a tracker to drift is still an important open research question. We address this question in the cascaded regression framework, the state-of-the-art approach for facial landmark localisation. Because incremental learning for cascaded regression is costly, we propose a much more efficient yet equally accurate alternative using continuous regression. More specifically, we first propose cascaded continuous regression (CCR) and show its accuracy is equivalent to the Supervised Descent Method. We then derive the incremental learning updates for CCR (iCCR) and show that it is an order of magnitude faster than standard incremental learning for cascaded regression, bringing the time required for the update from seconds down to a fraction of a second, thus enabling real-time tracking. Finally, we evaluate iCCR and show the importance of incremental learning in achieving state-of-the-art performance. Code for our iCCR is available from http://www.cs.nott.ac.uk/~psxes1



### Detailed Garment Recovery from a Single-View Image
- **Arxiv ID**: http://arxiv.org/abs/1608.01250v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.01250v4)
- **Published**: 2016-08-03 16:42:04+00:00
- **Updated**: 2016-09-12 19:04:56+00:00
- **Authors**: Shan Yang, Tanya Ambert, Zherong Pan, Ke Wang, Licheng Yu, Tamara Berg, Ming C. Lin
- **Comment**: Comparison added. Algorithm added. Equations cleaned up
- **Journal**: None
- **Summary**: Most recent garment capturing techniques rely on acquiring multiple views of clothing, which may not always be readily available, especially in the case of pre-existing photographs from the web. As an alternative, we pro- pose a method that is able to compute a rich and realistic 3D model of a human body and its outfits from a single photograph with little human in- teraction. Our algorithm is not only able to capture the global shape and geometry of the clothing, it can also extract small but important details of cloth, such as occluded wrinkles and folds. Unlike previous methods using full 3D information (i.e. depth, multi-view images, or sampled 3D geom- etry), our approach achieves detailed garment recovery from a single-view image by using statistical, geometric, and physical priors and a combina- tion of parameter estimation, semantic parsing, shape recovery, and physics- based cloth simulation. We demonstrate the effectiveness of our algorithm by re-purposing the reconstructed garments for virtual try-on and garment transfer applications, as well as cloth animation for digital characters.



### Fuzzy-based Propagation of Prior Knowledge to Improve Large-Scale Image Analysis Pipelines
- **Arxiv ID**: http://arxiv.org/abs/1608.01276v1
- **DOI**: 10.1371/journal.pone.0187535
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1608.01276v1)
- **Published**: 2016-08-03 18:21:02+00:00
- **Updated**: 2016-08-03 18:21:02+00:00
- **Authors**: Johannes Stegmaier, Ralf Mikut
- **Comment**: 39 pages, 12 figures
- **Journal**: None
- **Summary**: Many automatically analyzable scientific questions are well-posed and offer a variety of information about the expected outcome a priori. Although often being neglected, this prior knowledge can be systematically exploited to make automated analysis operations sensitive to a desired phenomenon or to evaluate extracted content with respect to this prior knowledge. For instance, the performance of processing operators can be greatly enhanced by a more focused detection strategy and the direct information about the ambiguity inherent in the extracted data. We present a new concept for the estimation and propagation of uncertainty involved in image analysis operators. This allows using simple processing operators that are suitable for analyzing large-scale 3D+t microscopy images without compromising the result quality. On the foundation of fuzzy set theory, we transform available prior knowledge into a mathematical representation and extensively use it enhance the result quality of various processing operators. All presented concepts are illustrated on a typical bioimage analysis pipeline comprised of seed point detection, segmentation, multiview fusion and tracking. Furthermore, the functionality of the proposed approach is validated on a comprehensive simulated 3D+t benchmark data set that mimics embryonic development and on large-scale light-sheet microscopy data of a zebrafish embryo. The general concept introduced in this contribution represents a new approach to efficiently exploit prior knowledge to improve the result quality of image analysis pipelines. Especially, the automated analysis of terabyte-scale microscopy data will benefit from sophisticated and efficient algorithms that enable a quantitative and fast readout. The generality of the concept, however, makes it also applicable to practically any other field with processing strategies that are arranged as linear pipelines.



### Retinal Vessel Segmentation Using A New Topological Method
- **Arxiv ID**: http://arxiv.org/abs/1608.01339v1
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1608.01339v1)
- **Published**: 2016-08-03 20:26:22+00:00
- **Updated**: 2016-08-03 20:26:22+00:00
- **Authors**: Martin Brooks
- **Comment**: None
- **Journal**: None
- **Summary**: A novel topological segmentation of retinal images represents blood vessels as connected regions in the continuous image plane, having shape-related analytic and geometric properties. This paper presents topological segmentation results from the DRIVE retinal image database.



### Permutation NMF
- **Arxiv ID**: http://arxiv.org/abs/1608.01372v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA
- **Links**: [PDF](http://arxiv.org/pdf/1608.01372v1)
- **Published**: 2016-08-03 21:59:44+00:00
- **Updated**: 2016-08-03 21:59:44+00:00
- **Authors**: Giovanni Barbarino
- **Comment**: None
- **Journal**: None
- **Summary**: Nonnegative Matrix Factorization(NMF) is a common used technique in machine learning to extract features out of data such as text documents and images thanks to its natural clustering properties. In particular, it is popular in image processing since it can decompose several pictures and recognize common parts if they're located in the same position over the photos. This paper's aim is to present a way to add the translation invariance to the classical NMF, that is, the algorithms presented are able to detect common features, even when they're shifted, in different original images.



### Language free character recognition using character sketch and center of gravity shifting
- **Arxiv ID**: http://arxiv.org/abs/1608.01391v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.01391v1)
- **Published**: 2016-08-03 23:13:01+00:00
- **Updated**: 2016-08-03 23:13:01+00:00
- **Authors**: Masoud Nosrati, Fakhereh Rahimi, Ronak Karimi
- **Comment**: World Applied Programming, Vol (6), Issue (2), July 2016
- **Journal**: None
- **Summary**: In this research, we present a heuristic method for character recognition. For this purpose, a sketch is constructed from the image that contains the character to be recognized. This sketch contains the most important pixels of image that are representatives of original image. These points are the most probable points in pixel-by-pixel matching of image that adapt to target image. Furthermore, a technique called gravity shifting is utilized for taking over the problem of elongation of characters. The consequence of combining sketch and gravity techniques leaded to a language free character recognition method. This method can be implemented independently for real-time uses or in combination of other classifiers as a feature extraction algorithm. Low complexity and acceptable performance are the most impressive features of this method that let it to be simply implemented in mobile and battery-limited computing devices. Results show that in the best case 86% of accuracy is obtained and in the worst case 28% of recognized characters are accurate.



