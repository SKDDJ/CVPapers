# Arxiv Papers in cs.CV on 2016-08-12
### Deep Motif Dashboard: Visualizing and Understanding Genomic Sequences Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1608.03644v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1608.03644v4)
- **Published**: 2016-08-12 00:43:59+00:00
- **Updated**: 2016-10-18 20:20:22+00:00
- **Authors**: Jack Lanchantin, Ritambhara Singh, Beilun Wang, Yanjun Qi
- **Comment**: 11 pages, 2 figures. Updated for PSB submission, Pacific Symposium on
  Biocomputing (PSB) 2017
- **Journal**: None
- **Summary**: Deep neural network (DNN) models have recently obtained state-of-the-art prediction accuracy for the transcription factor binding (TFBS) site classification task. However, it remains unclear how these approaches identify meaningful DNA sequence signals and give insights as to why TFs bind to certain locations. In this paper, we propose a toolkit called the Deep Motif Dashboard (DeMo Dashboard) which provides a suite of visualization strategies to extract motifs, or sequence patterns from deep neural network models for TFBS classification. We demonstrate how to visualize and understand three important DNN models: convolutional, recurrent, and convolutional-recurrent networks. Our first visualization method is finding a test sequence's saliency map which uses first-order derivatives to describe the importance of each nucleotide in making the final prediction. Second, considering recurrent models make predictions in a temporal manner (from one end of a TFBS sequence to the other), we introduce temporal output scores, indicating the prediction score of a model over time for a sequential input. Lastly, a class-specific visualization strategy finds the optimal input sequence for a given TFBS positive class via stochastic gradient optimization. Our experimental results indicate that a convolutional-recurrent architecture performs the best among the three architectures. The visualization techniques indicate that CNN-RNN makes predictions by modeling both motifs as well as dependencies among them.



### Deep Hashing: A Joint Approach for Image Signature Learning
- **Arxiv ID**: http://arxiv.org/abs/1608.03658v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.03658v1)
- **Published**: 2016-08-12 02:00:08+00:00
- **Updated**: 2016-08-12 02:00:08+00:00
- **Authors**: Yadong Mu, Zhu Liu
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Similarity-based image hashing represents crucial technique for visual data storage reduction and expedited image search. Conventional hashing schemes typically feed hand-crafted features into hash functions, which separates the procedures of feature extraction and hash function learning. In this paper, we propose a novel algorithm that concurrently performs feature engineering and non-linear supervised hashing function learning. Our technical contributions in this paper are two-folds: 1) deep network optimization is often achieved by gradient propagation, which critically requires a smooth objective function. The discrete nature of hash codes makes them not amenable for gradient-based optimization. To address this issue, we propose an exponentiated hashing loss function and its bilinear smooth approximation. Effective gradient calculation and propagation are thereby enabled; 2) pre-training is an important trick in supervised deep learning. The impact of pre-training on the hash code quality has never been discussed in current deep hashing literature. We propose a pre-training scheme inspired by recent advance in deep network based image classification, and experimentally demonstrate its effectiveness. Comprehensive quantitative evaluations are conducted on several widely-used image benchmarks. On all benchmarks, our proposed deep hashing algorithm outperforms all state-of-the-art competitors by significant margins. In particular, our algorithm achieves a near-perfect 0.99 in terms of Hamming ranking accuracy with only 12 bits on MNIST, and a new record of 0.74 on the CIFAR10 dataset. In comparison, the best accuracies obtained on CIFAR10 by existing hashing algorithms without or with deep networks are known to be 0.36 and 0.58 respectively.



### Reasoning and Algorithm Selection Augmented Symbolic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1608.03667v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.03667v1)
- **Published**: 2016-08-12 03:42:28+00:00
- **Updated**: 2016-08-12 03:42:28+00:00
- **Authors**: Martin Lukac, Kamila Abdiyeva, Michitaka Kameyama
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1505.07934
- **Journal**: None
- **Summary**: In this paper we present an alternative method to symbolic segmentation: we approach symbolic segmentation as an algorithm selection problem. That is, let there be a set A of available algorithms for symbolic segmentation, a set of input features $F$, a set of image attribute $\mathbb{A}$ and a selection mechanism $S(F,\mathbb{A},A)$ that selects on a case by case basis the best algorithm. The semantic segmentation is then an optimization process that combines best component segments from multiple results into a single optimal result. The experiments compare three different algorithm selection mechanisms using three selected semantic segmentation algorithms. The results show that using the current state of art algorithms and relatively low accuracy of algorithm selection the accuracy of the semantic segmentation can be improved by 2\%.



### Speech Signal Analysis for the Estimation of Heart Rates Under Different Emotional States
- **Arxiv ID**: http://arxiv.org/abs/1608.03720v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1608.03720v1)
- **Published**: 2016-08-12 08:52:23+00:00
- **Updated**: 2016-08-12 08:52:23+00:00
- **Authors**: Aibek Ryskaliyev, Sanzhar Askaruly, Alex Pappachen James
- **Comment**: to appear in Proceedings of International Conference on Advances in
  Computing, Communications and Informatics, IEEE, 2016
- **Journal**: None
- **Summary**: A non-invasive method for the monitoring of heart activity can help to reduce the deaths caused by heart disorders such as stroke, arrhythmia and heart attack. The human voice can be considered as a biometric data that can be used for estimation of heart rate. In this paper, we propose a method for estimating the heart rate from human speech dynamically using voice signal analysis and by the development of an empirical linear predictor model. The correlation between the voice signal and heart rate are established by classifiers and prediction of the heart rates with or without emotions are done using linear models. The prediction accuracy was tested using the data collected from 15 subjects, it is about 4050 samples of speech signals and corresponding electrocardiogram samples. The proposed approach can use for early non-invasive detection of heart rate changes that can be correlated to an emotional state of the individual and also can be used as a tool for diagnosis of heart conditions in real-time situations.



### Self-paced Learning for Weakly Supervised Evidence Discovery in Multimedia Event Search
- **Arxiv ID**: http://arxiv.org/abs/1608.03748v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.03748v3)
- **Published**: 2016-08-12 11:01:31+00:00
- **Updated**: 2017-10-23 06:39:45+00:00
- **Authors**: Mengyi Liu, Lu Jiang, Shiguang Shan, Alexander G. Hauptmann
- **Comment**: This paper has been withdrawn by the author due to a crucial error in
  tables
- **Journal**: None
- **Summary**: Multimedia event detection has been receiving increasing attention in recent years. Besides recognizing an event, the discovery of evidences (which is refered to as "recounting") is also crucial for user to better understand the searching result. Due to the difficulty of evidence annotation, only limited supervision of event labels are available for training a recounting model. To deal with the problem, we propose a weakly supervised evidence discovery method based on self-paced learning framework, which follows a learning process from easy "evidences" to gradually more complex ones, and simultaneously exploit more and more positive evidence samples from numerous weakly annotated video segments. Moreover, to evaluate our method quantitatively, we also propose two metrics, \textit{PctOverlap} and \textit{F1-score}, for measuring the performance of evidence localization specifically. The experiments are conducted on a subset of TRECVID MED dataset and demonstrate the promising results obtained by our method.



### Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1608.03773v2
- **DOI**: 10.1007/978-3-319-46454-1_29
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.03773v2)
- **Published**: 2016-08-12 12:24:11+00:00
- **Updated**: 2016-08-29 10:33:17+00:00
- **Authors**: Martin Danelljan, Andreas Robinson, Fahad Shahbaz Khan, Michael Felsberg
- **Comment**: Accepted at ECCV 2016
- **Journal**: ECCV 2016, Part V, LNCS 9909, pp. 472-488. Springer (2016)
- **Summary**: Discriminative Correlation Filters (DCF) have demonstrated excellent performance for visual object tracking. The key to their success is the ability to efficiently exploit available negative data by including all shifted versions of a training sample. However, the underlying DCF formulation is restricted to single-resolution feature maps, significantly limiting its potential. In this paper, we go beyond the conventional DCF framework and introduce a novel formulation for training continuous convolution filters. We employ an implicit interpolation model to pose the learning problem in the continuous spatial domain. Our proposed formulation enables efficient integration of multi-resolution deep feature maps, leading to superior results on three object tracking benchmarks: OTB-2015 (+5.1% in mean OP), Temple-Color (+4.6% in mean OP), and VOT2015 (20% relative reduction in failure rate). Additionally, our approach is capable of sub-pixel localization, crucial for the task of accurate feature point tracking. We also demonstrate the effectiveness of our learning formulation in extensive feature point tracking experiments. Code and supplementary material are available at http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/index.html.



### Applying Deep Learning to Basketball Trajectories
- **Arxiv ID**: http://arxiv.org/abs/1608.03793v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1608.03793v2)
- **Published**: 2016-08-12 13:50:24+00:00
- **Updated**: 2016-08-16 18:36:44+00:00
- **Authors**: Rajiv Shah, Rob Romijnders
- **Comment**: KDD 2016, Large Scale Sports Analytic Workshop
- **Journal**: None
- **Summary**: One of the emerging trends for sports analytics is the growing use of player and ball tracking data. A parallel development is deep learning predictive approaches that use vast quantities of data with less reliance on feature engineering. This paper applies recurrent neural networks in the form of sequence modeling to predict whether a three-point shot is successful. The models are capable of learning the trajectory of a basketball without any knowledge of physics. For comparison, a baseline static machine learning model with a full set of features, such as angle and velocity, in addition to the positional data is also tested. Using a dataset of over 20,000 three pointers from NBA SportVu data, the models based simply on sequential positional data outperform a static feature rich machine learning model in predicting whether a three-point shot is successful. This suggests deep learning models may offer an improvement to traditional feature based machine learning methods for tracking data.



### DeepDiary: Automatic Caption Generation for Lifelogging Image Streams
- **Arxiv ID**: http://arxiv.org/abs/1608.03819v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.03819v1)
- **Published**: 2016-08-12 15:17:33+00:00
- **Updated**: 2016-08-12 15:17:33+00:00
- **Authors**: Chenyou Fan, David J. Crandall
- **Comment**: This is an expanded preprint of a paper appearing at the ECCV
  International Workshop on Egocentric Perception, Interaction, and Computing
- **Journal**: None
- **Summary**: Lifelogging cameras capture everyday life from a first-person perspective, but generate so much data that it is hard for users to browse and organize their image collections effectively. In this paper, we propose to use automatic image captioning algorithms to generate textual representations of these collections. We develop and explore novel techniques based on deep learning to generate captions for both individual images and image streams, using temporal consistency constraints to create summaries that are both more compact and less noisy. We evaluate our techniques with quantitative and qualitative results, and apply captioning to an image retrieval application for finding potentially private images. Our results suggest that our automatic captioning algorithms, while imperfect, may work well enough to help users manage lifelogging photo collections.



### On Minimal Accuracy Algorithm Selection in Computer Vision and Intelligent Systems
- **Arxiv ID**: http://arxiv.org/abs/1608.03832v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.03832v1)
- **Published**: 2016-08-12 15:55:12+00:00
- **Updated**: 2016-08-12 15:55:12+00:00
- **Authors**: Martin Lukac, Kamila Abdiyeva, Michitaka Kameyama
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we discuss certain theoretical properties of algorithm selection approach to image processing and to intelligent system in general. We analyze the theoretical limits of algorithm selection with respect to the algorithm selection accuracy. We show the theoretical formulation of a crisp bound on the algorithm selector precision guaranteeing to always obtain better than the best available algorithm result.



### Temporal Registration in In-Utero Volumetric MRI Time Series
- **Arxiv ID**: http://arxiv.org/abs/1608.03907v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.03907v1)
- **Published**: 2016-08-12 20:40:24+00:00
- **Updated**: 2016-08-12 20:40:24+00:00
- **Authors**: Ruizhi Liao, Esra Turk, Miaomiao Zhang, Jie Luo, Ellen Grant, Elfar Adalsteinsson, Polina Golland
- **Comment**: to appear in International Conference on Medical Image Computing and
  Computer Assisted Intervention, 2016
- **Journal**: None
- **Summary**: We present a robust method to correct for motion and deformations for in-utero volumetric MRI time series. Spatio-temporal analysis of dynamic MRI requires robust alignment across time in the presence of substantial and unpredictable motion. We make a Markov assumption on the nature of deformations to take advantage of the temporal structure in the image data. Forward message passing in the corresponding hidden Markov model (HMM) yields an estimation algorithm that only has to account for relatively small motion between consecutive frames. We demonstrate the utility of the temporal model by showing that its use improves the accuracy of the segmentation propagation through temporal registration. Our results suggest that the proposed model captures accurately the temporal dynamics of deformations in in-utero MRI time series.



### When was that made?
- **Arxiv ID**: http://arxiv.org/abs/1608.03914v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.03914v1)
- **Published**: 2016-08-12 22:03:38+00:00
- **Updated**: 2016-08-12 22:03:38+00:00
- **Authors**: Sirion Vittayakorn, Alexander C. Berg, Tamara L. Berg
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we explore deep learning methods for estimating when objects were made. Automatic methods for this task could potentially be useful for historians, collectors, or any individual interested in estimating when their artifact was created. Direct applications include large-scale data organization or retrieval. Toward this goal, we utilize features from existing deep networks and also fine-tune new networks for temporal estimation. In addition, we create two new datasets of 67,771 dated clothing items from Flickr and museum collections. Our method outperforms both a color-based baseline and previous state of the art methods for temporal estimation. We also provide several analyses of what our networks have learned, and demonstrate applications to identifying temporal inspiration in fashion collections.



