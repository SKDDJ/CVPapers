# Arxiv Papers in cs.CV on 2016-08-01
### Video Summarization in a Multi-View Camera Network
- **Arxiv ID**: http://arxiv.org/abs/1608.00310v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.00310v1)
- **Published**: 2016-08-01 03:42:07+00:00
- **Updated**: 2016-08-01 03:42:07+00:00
- **Authors**: Rameswar Panda, Abir Das, Amit K. Roy-Chowdhury
- **Comment**: Accepted in ICPR 2016
- **Journal**: None
- **Summary**: While most existing video summarization approaches aim to extract an informative summary of a single video, we propose a novel framework for summarizing multi-view videos by exploiting both intra- and inter-view content correlations in a joint embedding space. We learn the embedding by minimizing an objective function that has two terms: one due to intra-view correlations and another due to inter-view correlations across the multiple views. The solution can be obtained directly by solving one Eigen-value problem that is linear in the number of multi-view videos. We then employ a sparse representative selection approach over the learned embedding space to summarize the multi-view videos. Experimental results on several benchmark datasets demonstrate that our proposed approach clearly outperforms the state-of-the-art.



### Fast and robust pushbroom hyperspectral imaging via DMD-based scanning
- **Arxiv ID**: http://arxiv.org/abs/1608.00361v2
- **DOI**: 10.1117/12.2239107
- **Categories**: **cs.CV**, cs.DC, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1608.00361v2)
- **Published**: 2016-08-01 09:18:38+00:00
- **Updated**: 2017-01-30 03:26:49+00:00
- **Authors**: Reza Arablouei, Ethan Goan, Stephen Gensemer, Branislav Kusy
- **Comment**: None
- **Journal**: None
- **Summary**: We describe a new pushbroom hyperspectral imaging device that has no macro moving part. The main components of the proposed hyperspectral imager are a digital micromirror device (DMD), a CMOS image sensor with no filter as the spectral sensor, a CMOS color (RGB) image sensor as the auxiliary image sensor, and a diffraction grating. Using the image sensor pair, the device can simultaneously capture hyperspectral data as well as RGB images of the scene. The RGB images captured by the auxiliary image sensor can facilitate geometric co-registration of the hyperspectral image slices captured by the spectral sensor. In addition, the information discernible from the RGB images can lead to capturing the spectral data of only the regions of interest within the scene. The proposed hyperspectral imaging architecture is cost-effective, fast, and robust. It also enables a trade-off between resolution and speed. We have built an initial prototype based on the proposed design. The prototype can capture a hyperspectral image datacube with a spatial resolution of 192x192 pixels and a spectral resolution of 500 bands in less than thirty seconds.



### Accelerating the Super-Resolution Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1608.00367v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.00367v1)
- **Published**: 2016-08-01 09:44:41+00:00
- **Updated**: 2016-08-01 09:44:41+00:00
- **Authors**: Chao Dong, Chen Change Loy, Xiaoou Tang
- **Comment**: 17 pages, 8 figures, ECCV 2016
- **Journal**: None
- **Summary**: As a successful deep model applied in image super-resolution (SR), the Super-Resolution Convolutional Neural Network (SRCNN) has demonstrated superior performance to the previous hand-crafted models either in speed and restoration quality. However, the high computational cost still hinders it from practical usage that demands real-time performance (24 fps). In this paper, we aim at accelerating the current SRCNN, and propose a compact hourglass-shape CNN structure for faster and better SR. We re-design the SRCNN structure mainly in three aspects. First, we introduce a deconvolution layer at the end of the network, then the mapping is learned directly from the original low-resolution image (without interpolation) to the high-resolution one. Second, we reformulate the mapping layer by shrinking the input feature dimension before mapping and expanding back afterwards. Third, we adopt smaller filter sizes but more mapping layers. The proposed model achieves a speed up of more than 40 times with even superior restoration quality. Further, we present the parameter settings that can achieve real-time performance on a generic CPU while still maintaining good performance. A corresponding transfer strategy is also proposed for fast training and testing across different upscaling factors.



### Labeling Topics with Images using Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1608.00470v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1608.00470v2)
- **Published**: 2016-08-01 15:27:16+00:00
- **Updated**: 2017-01-03 16:49:39+00:00
- **Authors**: Nikolaos Aletras, Arpit Mittal
- **Comment**: None
- **Journal**: None
- **Summary**: Topics generated by topic models are usually represented by lists of $t$ terms or alternatively using short phrases and images. The current state-of-the-art work on labeling topics using images selects images by re-ranking a small set of candidates for a given topic. In this paper, we present a more generic method that can estimate the degree of association between any arbitrary pair of an unseen topic and image using a deep neural network. Our method has better runtime performance $O(n)$ compared to $O(n^2)$ for the current state-of-the-art method, and is also significantly more accurate.



### Exploiting Temporal Information for DCNN-based Fine-Grained Object Classification
- **Arxiv ID**: http://arxiv.org/abs/1608.00486v3
- **DOI**: 10.1109/DICTA.2016.7797039
- **Categories**: **cs.CV**, cs.MM, I.2.6, I.4, I.5
- **Links**: [PDF](http://arxiv.org/pdf/1608.00486v3)
- **Published**: 2016-08-01 16:34:16+00:00
- **Updated**: 2016-10-24 06:40:02+00:00
- **Authors**: ZongYuan Ge, Chris McCool, Conrad Sanderson, Peng Wang, Lingqiao Liu, Ian Reid, Peter Corke
- **Comment**: International Conference on Digital Image Computing: Techniques and
  Applications, 2016
- **Journal**: None
- **Summary**: Fine-grained classification is a relatively new field that has concentrated on using information from a single image, while ignoring the enormous potential of using video data to improve classification. In this work we present the novel task of video-based fine-grained object classification, propose a corresponding new video dataset, and perform a systematic study of several recent deep convolutional neural network (DCNN) based approaches, which we specifically adapt to the task. We evaluate three-dimensional DCNNs, two-stream DCNNs, and bilinear DCNNs. Two forms of the two-stream approach are used, where spatial and temporal data from two independent DCNNs are fused either via early fusion (combination of the fully-connected layers) and late fusion (concatenation of the softmax outputs of the DCNNs). For bilinear DCNNs, information from the convolutional layers of the spatial and temporal DCNNs is combined via local co-occurrences. We then fuse the bilinear DCNN and early fusion of the two-stream approach to combine the spatial and temporal information at the local and global level (Spatio-Temporal Co-occurrence). Using the new and challenging video dataset of birds, classification performance is improved from 23.1% (using single images) to 41.1% when using the Spatio-Temporal Co-occurrence system. Incorporating automatically detected bounding box location further improves the classification accuracy to 53.6%.



### Supervised Classification of RADARSAT-2 Polarimetric Data for Different Land Features
- **Arxiv ID**: http://arxiv.org/abs/1608.00501v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4; I.5.3
- **Links**: [PDF](http://arxiv.org/pdf/1608.00501v1)
- **Published**: 2016-08-01 17:24:30+00:00
- **Updated**: 2016-08-01 17:24:30+00:00
- **Authors**: Abhishek Maity
- **Comment**: 3 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: The pixel percentage belonging to the user defined area that are assigned to cluster in a confusion matrix for RADARSAT-2 over Vancouver area has been analysed for classification. In this study, supervised Wishart and Support Vector Machine (SVM) classifiers over RADARSAT-2 (RS2) fine quadpol mode Single Look Complex (SLC) product data is computed and compared. In comparison with conventional single channel or dual channel polarization, RADARSAT-2 is fully polarimetric, making it to offer better land feature contrast for classification operation.



### Top-down Neural Attention by Excitation Backprop
- **Arxiv ID**: http://arxiv.org/abs/1608.00507v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.00507v1)
- **Published**: 2016-08-01 17:49:57+00:00
- **Updated**: 2016-08-01 17:49:57+00:00
- **Authors**: Jianming Zhang, Zhe Lin, Jonathan Brandt, Xiaohui Shen, Stan Sclaroff
- **Comment**: A shorter version of this paper is accepted at ECCV, 2016 (oral)
- **Journal**: None
- **Summary**: We aim to model the top-down attention of a Convolutional Neural Network (CNN) classifier for generating task-specific attention maps. Inspired by a top-down human visual attention model, we propose a new backpropagation scheme, called Excitation Backprop, to pass along top-down signals downwards in the network hierarchy via a probabilistic Winner-Take-All process. Furthermore, we introduce the concept of contrastive attention to make the top-down attention maps more discriminative. In experiments, we demonstrate the accuracy and generalizability of our method in weakly supervised localization tasks on the MS COCO, PASCAL VOC07 and ImageNet datasets. The usefulness of our method is further validated in the text-to-region association task. On the Flickr30k Entities dataset, we achieve promising performance in phrase localization by leveraging the top-down attention of a CNN model that has been trained on weakly labeled web images.



### Modeling Context Between Objects for Referring Expression Understanding
- **Arxiv ID**: http://arxiv.org/abs/1608.00525v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.00525v1)
- **Published**: 2016-08-01 19:03:27+00:00
- **Updated**: 2016-08-01 19:03:27+00:00
- **Authors**: Varun K. Nagaraja, Vlad I. Morariu, Larry S. Davis
- **Comment**: To appear at ECCV 16
- **Journal**: None
- **Summary**: Referring expressions usually describe an object using properties of the object and relationships of the object with other objects. We propose a technique that integrates context between objects to understand referring expressions. Our approach uses an LSTM to learn the probability of a referring expression, with input features from a region and a context region. The context regions are discovered using multiple-instance learning (MIL) since annotations for context objects are generally not available for training. We utilize max-margin based MIL objective functions for training the LSTM. Experiments on the Google RefExp and UNC RefExp datasets show that modeling context between objects provides better performance than modeling only object properties. We also qualitatively show that our technique can ground a referring expression to its referred region along with the supporting context region.



### Early Methods for Detecting Adversarial Images
- **Arxiv ID**: http://arxiv.org/abs/1608.00530v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1608.00530v2)
- **Published**: 2016-08-01 19:13:58+00:00
- **Updated**: 2017-03-23 18:03:47+00:00
- **Authors**: Dan Hendrycks, Kevin Gimpel
- **Comment**: ICLR 2017 Workshop Contribution
- **Journal**: None
- **Summary**: Many machine learning classifiers are vulnerable to adversarial perturbations. An adversarial perturbation modifies an input to change a classifier's prediction without causing the input to seem substantially different to human perception. We deploy three methods to detect adversarial images. Adversaries trying to bypass our detectors must make the adversarial image less pathological or they will fail trying. Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA. Other detectors and a colorful saliency map are in an appendix.



### Attention Tree: Learning Hierarchies of Visual Features for Large-Scale Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1608.00611v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1608.00611v1)
- **Published**: 2016-08-01 20:51:29+00:00
- **Updated**: 2016-08-01 20:51:29+00:00
- **Authors**: Priyadarshini Panda, Kaushik Roy
- **Comment**: 11 pages, 8 figures, Under review in IEEE Transactions on Neural
  Networks and Learning systems
- **Journal**: None
- **Summary**: One of the key challenges in machine learning is to design a computationally efficient multi-class classifier while maintaining the output accuracy and performance. In this paper, we present a tree-based classifier: Attention Tree (ATree) for large-scale image classification that uses recursive Adaboost training to construct a visual attention hierarchy. The proposed attention model is inspired from the biological 'selective tuning mechanism for cortical visual processing'. We exploit the inherent feature similarity across images in datasets to identify the input variability and use recursive optimization procedure, to determine data partitioning at each node, thereby, learning the attention hierarchy. A set of binary classifiers is organized on top of the learnt hierarchy to minimize the overall test-time complexity. The attention model maximizes the margins for the binary classifiers for optimal decision boundary modelling, leading to better performance at minimal complexity. The proposed framework has been evaluated on both Caltech-256 and SUN datasets and achieves accuracy improvement over state-of-the-art tree-based methods at significantly lower computational cost.



### Interactive Image Segmentation Using Constrained Dominant Sets
- **Arxiv ID**: http://arxiv.org/abs/1608.00641v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.00641v2)
- **Published**: 2016-08-01 23:37:41+00:00
- **Updated**: 2016-08-03 17:32:04+00:00
- **Authors**: Eyasu Zemene, Marcello Pelillo
- **Comment**: Accepted at ECCV 2016
- **Journal**: None
- **Summary**: We propose a new approach to interactive image segmentation based on some properties of a family of quadratic optimization problems related to dominant sets, a well-known graph-theoretic notion of a cluster which generalizes the concept of a maximal clique to edge-weighted graphs. In particular, we show that by properly controlling a regularization parameter which determines the structure and the scale of the underlying problem, we are in a position to extract groups of dominant-set clusters which are constrained to contain user-selected elements. The resulting algorithm can deal naturally with any type of input modality, including scribbles, sloppy contours, and bounding boxes, and is able to robustly handle noisy annotations on the part of the user. Experiments on standard benchmark datasets show the effectiveness of our approach as compared to state-of-the-art algorithms on a variety of natural images under several input conditions.



