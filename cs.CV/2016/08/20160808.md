# Arxiv Papers in cs.CV on 2016-08-08
### Detecting Sarcasm in Multimodal Social Platforms
- **Arxiv ID**: http://arxiv.org/abs/1608.02289v1
- **DOI**: 10.1145/2964284.2964321
- **Categories**: **cs.CV**, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1608.02289v1)
- **Published**: 2016-08-08 00:59:03+00:00
- **Updated**: 2016-08-08 00:59:03+00:00
- **Authors**: Rossano Schifanella, Paloma de Juan, Joel Tetreault, Liangliang Cao
- **Comment**: 10 pages, 3 figures, final version published in the Proceedings of
  ACM Multimedia 2016
- **Journal**: None
- **Summary**: Sarcasm is a peculiar form of sentiment expression, where the surface sentiment differs from the implied sentiment. The detection of sarcasm in social media platforms has been applied in the past mainly to textual utterances where lexical indicators (such as interjections and intensifiers), linguistic markers, and contextual information (such as user profiles, or past conversations) were used to detect the sarcastic tone. However, modern social media platforms allow to create multimodal messages where audiovisual content is integrated with the text, making the analysis of a mode in isolation partial. In our work, we first study the relationship between the textual and visual aspects in multimodal posts from three major social media platforms, i.e., Instagram, Tumblr and Twitter, and we run a crowdsourcing task to quantify the extent to which images are perceived as necessary by human annotators. Moreover, we propose two different computational frameworks to detect sarcasm that integrate the textual and visual modalities. The first approach exploits visual semantics trained on an external dataset, and concatenates the semantics features with state-of-the-art textual features. The second method adapts a visual neural network initialized with parameters trained on ImageNet to multimodal sarcastic posts. Results show the positive effect of combining modalities for the detection of sarcasm across platforms and methods.



### SANTIAGO: Spine Association for Neuron Topology Improvement and Graph Optimization
- **Arxiv ID**: http://arxiv.org/abs/1608.02307v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1608.02307v1)
- **Published**: 2016-08-08 03:37:29+00:00
- **Updated**: 2016-08-08 03:37:29+00:00
- **Authors**: William Gray Roncal, Colin Lea, Akira Baruah, Gregory D. Hager
- **Comment**: 13 pp
- **Journal**: None
- **Summary**: Developing automated and semi-automated solutions for reconstructing wiring diagrams of the brain from electron micrographs is important for advancing the field of connectomics. While the ultimate goal is to generate a graph of neuron connectivity, most prior automated methods have focused on volume segmentation rather than explicit graph estimation. In these approaches, one of the key, commonly occurring error modes is dendritic shaft-spine fragmentation.   We posit that directly addressing this problem of connection identification may provide critical insight into estimating more accurate brain graphs. To this end, we develop a network-centric approach motivated by biological priors image grammars. We build a computer vision pipeline to reconnect fragmented spines to their parent dendrites using both fully-automated and semi-automated approaches. Our experiments show we can learn valid connections despite uncertain segmentation paths. We curate the first known reference dataset for analyzing the performance of various spine-shaft algorithms and demonstrate promising results that recover many previously lost connections. Our automated approach improves the local subgraph score by more than four times and the full graph score by 60 percent. These data, results, and evaluation tools are all available to the broader scientific community. This reframing of the connectomics problem illustrates a semantic, biologically inspired solution to remedy a major problem with neuron tracking.



### Discriminatively Trained Latent Ordinal Model for Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1608.02318v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.02318v2)
- **Published**: 2016-08-08 05:15:54+00:00
- **Updated**: 2017-08-15 03:25:31+00:00
- **Authors**: Karan Sikka, Gaurav Sharma
- **Comment**: Paper accepted in IEEE TPAMI. arXiv admin note: substantial text
  overlap with arXiv:1604.01500
- **Journal**: None
- **Summary**: We study the problem of video classification for facial analysis and human action recognition. We propose a novel weakly supervised learning method that models the video as a sequence of automatically mined, discriminative sub-events (eg. onset and offset phase for "smile", running and jumping for "highjump"). The proposed model is inspired by the recent works on Multiple Instance Learning and latent SVM/HCRF -- it extends such frameworks to model the ordinal aspect in the videos, approximately. We obtain consistent improvements over relevant competitive baselines on four challenging and publicly available video based facial analysis datasets for prediction of expression, clinical pain and intent in dyadic conversations and on three challenging human action datasets. We also validate the method with qualitative results and show that they largely support the intuitions behind the method.



### Learning Joint Representations of Videos and Sentences with Web Image Search
- **Arxiv ID**: http://arxiv.org/abs/1608.02367v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.02367v1)
- **Published**: 2016-08-08 09:54:15+00:00
- **Updated**: 2016-08-08 09:54:15+00:00
- **Authors**: Mayu Otani, Yuta Nakashima, Esa Rahtu, Janne Heikkil√§, Naokazu Yokoya
- **Comment**: 16 pages, 4th Workshop on Web-scale Vision and Social Media (VSM),
  ECCV 2016
- **Journal**: None
- **Summary**: Our objective is video retrieval based on natural language queries. In addition, we consider the analogous problem of retrieving sentences or generating descriptions given an input video. Recent work has addressed the problem by embedding visual and textual inputs into a common space where semantic similarities correlate to distances. We also adopt the embedding approach, and make the following contributions: First, we utilize web image search in sentence embedding process to disambiguate fine-grained visual concepts. Second, we propose embedding models for sentence, image, and video inputs whose parameters are learned simultaneously. Finally, we show how the proposed model can be applied to description generation. Overall, we observe a clear improvement over the state-of-the-art methods in the video and sentence retrieval tasks. In description generation, the performance level is comparable to the current state-of-the-art, although our embeddings were trained for the retrieval tasks.



### A combined Approach Based on Fuzzy Classification and Contextual Region Growing to Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1608.02373v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.02373v1)
- **Published**: 2016-08-08 10:26:53+00:00
- **Updated**: 2016-08-08 10:26:53+00:00
- **Authors**: Mahaman Sani Chaibou, Karim Kalti, Bassel Soulaiman, Mohamed Ali Mahjoub
- **Comment**: CGIV 2016
- **Journal**: None
- **Summary**: We present in this paper an image segmentation approach that combines a fuzzy semantic region classification and a context based region-growing. Input image is first over-segmented. Then, prior domain knowledge is used to perform a fuzzy classification of these regions to provide a fuzzy semantic labeling. This allows the proposed approach to operate at high level instead of using low-level features and consequently to remedy to the problem of the semantic gap. Each over-segmented region is represented by a vector giving its corresponding membership degrees to the different thematic labels and the whole image is therefore represented by a Regions Partition Matrix. The segmentation is achieved on this matrix instead of the image pixels through two main phases: focusing and propagation. The focusing aims at selecting seeds regions from which information propagation will be performed. Thepropagation phase allows to spread toward others regions and using fuzzy contextual information the needed knowledge ensuring the semantic segmentation. An application of the proposed approach on mammograms shows promising results



### Comparative study and enhancement of Camera Tampering Detection algorithms
- **Arxiv ID**: http://arxiv.org/abs/1608.02385v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.02385v1)
- **Published**: 2016-08-08 11:22:23+00:00
- **Updated**: 2016-08-08 11:22:23+00:00
- **Authors**: Mabrouka Hagui, Mohamed Ali Mahjoub, Ahmed Boukhris
- **Comment**: CGIV 2016
- **Journal**: None
- **Summary**: Recently the use of video surveillance systems is widely increasing. Different places are equipped by camera surveillances such as hospitals, schools, airports, museums and military places in order to ensure the safety and security of the persons and their property. Therefore it becomes significant to guarantee the proper working of these systems. Intelligent video surveillance systems equipped by sophisticated digital camera can analyze video information s and automatically detect doubtful actions. The camera tampering detection algorithms may indicate that accidental or suspicious activities have occurred and that causes the abnormality works of the video surveillance.   Camera Tampering Detection uses several techniques based on image processing and computer vision. In this paper, comparative study of performance of three algorithms that can detect abnormal disturbance for video surveillance is presented.



### Database of handwritten Arabic mathematical formulas images
- **Arxiv ID**: http://arxiv.org/abs/1608.02388v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.02388v1)
- **Published**: 2016-08-08 11:30:35+00:00
- **Updated**: 2016-08-08 11:30:35+00:00
- **Authors**: Ibtissem Hadj Ali, Mohammed Ali Mahjoub
- **Comment**: CGIV 2016
- **Journal**: None
- **Summary**: Although publicly available, ground-truthed database have proven useful for training, evaluating, and comparing recognition systems in many domains, the availability of such database for handwritten Arabic mathematical formula recognition in particular, is currently quite poor. In this paper, we present a new public database that contains mathematical expressions available in their off-line handwritten form. Here, we describe the different steps that allowed us to acquire this database, from the creation of the mathematical expression corpora to the transcription of the collected data. Currently, the dataset contains 4 238 off-line handwritten mathematical expressions written by 66 writers and 20 300 handwritten isolated symbol images. The ground truth is also provided for the handwritten expressions as XML files with the number of symbols, and the MATHML structure.



