# Arxiv Papers in cs.CV on 2016-08-04
### Faster CNNs with Direct Sparse Convolutions and Guided Pruning
- **Arxiv ID**: http://arxiv.org/abs/1608.01409v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.01409v5)
- **Published**: 2016-08-04 01:16:39+00:00
- **Updated**: 2017-07-28 22:26:27+00:00
- **Authors**: Jongsoo Park, Sheng Li, Wei Wen, Ping Tak Peter Tang, Hai Li, Yiran Chen, Pradeep Dubey
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: Phenomenally successful in practical inference problems, convolutional neural networks (CNN) are widely deployed in mobile devices, data centers, and even supercomputers. The number of parameters needed in CNNs, however, are often large and undesirable. Consequently, various methods have been developed to prune a CNN once it is trained. Nevertheless, the resulting CNNs offer limited benefits. While pruning the fully connected layers reduces a CNN's size considerably, it does not improve inference speed noticeably as the compute heavy parts lie in convolutions. Pruning CNNs in a way that increase inference speed often imposes specific sparsity structures, thus limiting the achievable sparsity levels.   We present a method to realize simultaneously size economy and speed improvement while pruning CNNs. Paramount to our success is an efficient general sparse-with-dense matrix multiplication implementation that is applicable to convolution of feature maps with kernels of arbitrary sparsity patterns. Complementing this, we developed a performance model that predicts sweet spots of sparsity levels for different layers and on different computer architectures. Together, these two allow us to demonstrate 3.1--7.3$\times$ convolution speedups over dense convolution in AlexNet, on Intel Atom, Xeon, and Xeon Phi processors, spanning the spectrum from mobile devices to supercomputers. We also open source our project at https://github.com/IntelLabs/SkimCaffe.



### An efficient iterative thresholding method for image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1608.01431v2
- **DOI**: 10.1016/j.jcp.2017.08.020
- **Categories**: **cs.CV**, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1608.01431v2)
- **Published**: 2016-08-04 05:37:35+00:00
- **Updated**: 2016-08-12 07:45:53+00:00
- **Authors**: Dong Wang, Haohan Li, Xiaoyu Wei, Xiaoping Wang
- **Comment**: 14 pages, 21 figures
- **Journal**: None
- **Summary**: We proposed an efficient iterative thresholding method for multi-phase image segmentation. The algorithm is based on minimizing piecewise constant Mumford-Shah functional in which the contour length (or perimeter) is approximated by a non-local multi-phase energy. The minimization problem is solved by an iterative method. Each iteration consists of computing simple convolutions followed by a thresholding step. The algorithm is easy to implement and has the optimal complexity $O(N \log N)$ per iteration. We also show that the iterative algorithm has the total energy decaying property. We present some numerical results to show the efficiency of our method.



### Improving Multi-label Learning with Missing Labels by Structured Semantic Correlations
- **Arxiv ID**: http://arxiv.org/abs/1608.01441v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.01441v1)
- **Published**: 2016-08-04 06:58:32+00:00
- **Updated**: 2016-08-04 06:58:32+00:00
- **Authors**: Hao Yang, Joey Tianyi Zhou, Jianfei Cai
- **Comment**: Accepted in ECCV 2016
- **Journal**: None
- **Summary**: Multi-label learning has attracted significant interests in computer vision recently, finding applications in many vision tasks such as multiple object recognition and automatic image annotation. Associating multiple labels to a complex image is very difficult, not only due to the intricacy of describing the image, but also because of the incompleteness nature of the observed labels. Existing works on the problem either ignore the label-label and instance-instance correlations or just assume these correlations are linear and unstructured. Considering that semantic correlations between images are actually structured, in this paper we propose to incorporate structured semantic correlations to solve the missing label problem of multi-label learning. Specifically, we project images to the semantic space with an effective semantic descriptor. A semantic graph is then constructed on these images to capture the structured correlations between them. We utilize the semantic graph Laplacian as a smooth term in the multi-label learning formulation to incorporate the structured semantic correlations. Experimental results demonstrate the effectiveness of the proposed semantic descriptor and the usefulness of incorporating the structured semantic correlations. We achieve better results than state-of-the-art multi-label learning methods on four benchmark datasets.



### UnitBox: An Advanced Object Detection Network
- **Arxiv ID**: http://arxiv.org/abs/1608.01471v1
- **DOI**: 10.1145/2964284.2967274
- **Categories**: **cs.CV**, 68U01, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/1608.01471v1)
- **Published**: 2016-08-04 09:06:15+00:00
- **Updated**: 2016-08-04 09:06:15+00:00
- **Authors**: Jiahui Yu, Yuning Jiang, Zhangyang Wang, Zhimin Cao, Thomas Huang
- **Comment**: To appear in ACM MM 2016, 5 pages, 6 figures
- **Journal**: None
- **Summary**: In present object detection systems, the deep convolutional neural networks (CNNs) are utilized to predict bounding boxes of object candidates, and have gained performance advantages over the traditional region proposal methods. However, existing deep CNN methods assume the object bounds to be four independent variables, which could be regressed by the $\ell_2$ loss separately. Such an oversimplified assumption is contrary to the well-received observation, that those variables are correlated, resulting to less accurate localization. To address the issue, we firstly introduce a novel Intersection over Union ($IoU$) loss function for bounding box prediction, which regresses the four bounds of a predicted box as a whole unit. By taking the advantages of $IoU$ loss and deep fully convolutional networks, the UnitBox is introduced, which performs accurate and efficient localization, shows robust to objects of varied shapes and scales, and converges fast. We apply UnitBox on face detection task and achieve the best performance among all published methods on the FDDB benchmark.



### Recoding Color Transfer as a Color Homography
- **Arxiv ID**: http://arxiv.org/abs/1608.01505v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.01505v1)
- **Published**: 2016-08-04 11:54:37+00:00
- **Updated**: 2016-08-04 11:54:37+00:00
- **Authors**: Han Gong, Graham D. Finlayson, Robert B. Fisher
- **Comment**: Accepted by BMVC 2016
- **Journal**: None
- **Summary**: Color transfer is an image editing process that adjusts the colors of a picture to match a target picture's color theme. A natural color transfer not only matches the color styles but also prevents after-transfer artifacts due to image compression, noise, and gradient smoothness change. The recently discovered color homography theorem proves that colors across a change in photometric viewing condition are related by a homography. In this paper, we propose a color-homography-based color transfer decomposition which encodes color transfer as a combination of chromaticity shift and shading adjustment. A powerful form of shading adjustment is shown to be a global shading curve by which the same shading homography can be applied elsewhere. Our experiments show that the proposed color transfer decomposition provides a very close approximation to many popular color transfer methods. The advantage of our approach is that the learned color transfer can be applied to many other images (e.g. other frames in a video), instead of a frame-to-frame basis. We demonstrate two applications for color transfer enhancement and video color grading re-application. This simple model of color transfer is also important for future color transfer algorithm design.



### Deep Learning for Detecting Multiple Space-Time Action Tubes in Videos
- **Arxiv ID**: http://arxiv.org/abs/1608.01529v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.01529v1)
- **Published**: 2016-08-04 13:38:38+00:00
- **Updated**: 2016-08-04 13:38:38+00:00
- **Authors**: Suman Saha, Gurkirt Singh, Michael Sapienza, Philip H. S. Torr, Fabio Cuzzolin
- **Comment**: Accepted by British Machine Vision Conference 2016
- **Journal**: None
- **Summary**: In this work, we propose an approach to the spatiotemporal localisation (detection) and classification of multiple concurrent actions within temporally untrimmed videos. Our framework is composed of three stages. In stage 1, appearance and motion detection networks are employed to localise and score actions from colour images and optical flow. In stage 2, the appearance network detections are boosted by combining them with the motion detection scores, in proportion to their respective spatial overlap. In stage 3, sequences of detection boxes most likely to be associated with a single action instance, called action tubes, are constructed by solving two energy maximisation problems via dynamic programming. While in the first pass, action paths spanning the whole video are built by linking detection boxes over time using their class-specific scores and their spatial overlap, in the second pass, temporal trimming is performed by ensuring label consistency for all constituting detection boxes. We demonstrate the performance of our algorithm on the challenging UCF101, J-HMDB-21 and LIRIS-HARL datasets, achieving new state-of-the-art results across the board and significantly increasing detection speed at test time. We achieve a huge leap forward in action detection performance and report a 20% and 11% gain in mAP (mean average precision) on UCF-101 and J-HMDB-21 datasets respectively when compared to the state-of-the-art.



### Saliency Integration: An Arbitrator Model
- **Arxiv ID**: http://arxiv.org/abs/1608.01536v2
- **DOI**: 10.1109/TMM.2018.2856126
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.01536v2)
- **Published**: 2016-08-04 13:54:16+00:00
- **Updated**: 2018-07-27 12:41:34+00:00
- **Authors**: Yingyue Xu, Xiaopeng Hong, Fatih Porikli, Xin Liu, Jie Chen, Guoying Zhao
- **Comment**: IEEE Transactions on Multimedia (2018)
- **Journal**: Y. Xu, X. Hong, F. Porikli, X. Liu, J. Chen, and G. Zhao,
  "Saliency Integration: An Arbitrator Model", IEEE Transactions on Multimedia.
  2018
- **Summary**: Saliency integration has attracted much attention on unifying saliency maps from multiple saliency models. Previous offline integration methods usually face two challenges: 1. if most of the candidate saliency models misjudge the saliency on an image, the integration result will lean heavily on those inferior candidate models; 2. an unawareness of the ground truth saliency labels brings difficulty in estimating the expertise of each candidate model. To address these problems, in this paper, we propose an arbitrator model (AM) for saliency integration. Firstly, we incorporate the consensus of multiple saliency models and the external knowledge into a reference map to effectively rectify the misleading by candidate models. Secondly, our quest for ways of estimating the expertise of the saliency models without ground truth labels gives rise to two distinct online model-expertise estimation methods. Finally, we derive a Bayesian integration framework to reconcile the saliency models of varying expertise and the reference map. To extensively evaluate the proposed AM model, we test twenty-seven state-of-the-art saliency models, covering both traditional and deep learning ones, on various combinations over four datasets. The evaluation results show that the AM model improves the performance substantially compared to the existing state-of-the-art integration methods, regardless of the chosen candidate saliency models.



### A Recursive Framework for Expression Recognition: From Web Images to Deep Models to Game Dataset
- **Arxiv ID**: http://arxiv.org/abs/1608.01647v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.01647v1)
- **Published**: 2016-08-04 19:07:08+00:00
- **Updated**: 2016-08-04 19:07:08+00:00
- **Authors**: Wei Li, Christina Tsangouri, Farnaz Abtahi, Zhigang Zhu
- **Comment**: Submit to Machine Vision Application Journal. arXiv admin note: text
  overlap with arXiv:1607.02678
- **Journal**: None
- **Summary**: In this paper, we propose a recursive framework to recognize facial expressions from images in real scenes. Unlike traditional approaches that typically focus on developing and refining algorithms for improving recognition performance on an existing dataset, we integrate three important components in a recursive manner: facial dataset generation, facial expression recognition model building, and interactive interfaces for testing and new data collection. To start with, we first create a candid-images-for-facial-expression (CIFE) dataset. We then apply a convolutional neural network (CNN) to CIFE and build a CNN model for web image expression classification. In order to increase the expression recognition accuracy, we also fine-tune the CNN model and thus obtain a better CNN facial expression recognition model. Based on the fine-tuned CNN model, we design a facial expression game engine and collect a new and more balanced dataset, GaMo. The images of this dataset are collected from the different expressions our game users make when playing the game. Finally, we evaluate the GaMo and CIFE datasets and show that our recursive framework can help build a better facial expression model for dealing with real scene facial expression tasks.



### Identifying Metastases in Sentinel Lymph Nodes with Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1608.01658v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.01658v1)
- **Published**: 2016-08-04 19:47:30+00:00
- **Updated**: 2016-08-04 19:47:30+00:00
- **Authors**: Richard Chen, Yating Jing, Hunter Jackson
- **Comment**: None
- **Journal**: None
- **Summary**: Metastatic presence in lymph nodes is one of the most important prognostic variables of breast cancer. The current diagnostic procedure for manually reviewing sentinel lymph nodes, however, is very time-consuming and subjective. Pathologists have to manually scan an entire digital whole-slide image (WSI) for regions of metastasis that are sometimes only detectable under high resolution or entirely hidden from the human visual cortex. From October 2015 to April 2016, the International Symposium on Biomedical Imaging (ISBI) held the Camelyon Grand Challenge 2016 to crowd-source ideas and algorithms for automatic detection of lymph node metastasis. Using a generalizable stain normalization technique and the Proscia Pathology Cloud computing platform, we trained a deep convolutional neural network on millions of tissue and tumor image tiles to perform slide-based evaluation on our testing set of whole-slide images images, with a sensitivity of 0.96, specificity of 0.89, and AUC score of 0.90. Our results indicate that our platform can automatically scan any WSI for metastatic regions without institutional calibration to respective stain profiles.



### Sparse Filtered SIRT for Electron Tomography
- **Arxiv ID**: http://arxiv.org/abs/1608.01686v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.3, I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/1608.01686v2)
- **Published**: 2016-08-04 20:08:53+00:00
- **Updated**: 2018-07-30 20:21:48+00:00
- **Authors**: Chen Mu, Chiwoo Park
- **Comment**: None
- **Journal**: None
- **Summary**: Electron tomographic reconstruction is a method for obtaining a three-dimensional image of a specimen with a series of two dimensional microscope images taken from different viewing angles. Filtered backprojection, one of the most popular tomographic reconstruction methods, does not work well under the existence of image noises and missing wedges. This paper presents a new approach to largely mitigate the effect of noises and missing wedges. We propose a novel filtered backprojection that optimizes the filter of the backprojection operator in terms of a reconstruction error. This data-dependent filter adaptively chooses the spectral domains of signals and noises, suppressing the noise frequency bands, so it is very effective in denoising. We also propose the new filtered backprojection embedded within the simultaneous iterative reconstruction iteration for mitigating the effect of missing wedges. Our numerical study is presented to show the performance gain of the proposed approach over the state-of-the-art.



