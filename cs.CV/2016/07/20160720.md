# Arxiv Papers in cs.CV on 2016-07-20
### Hierarchical Manifold Clustering on Diffusion Maps for Connectomics (MIT 18.S096 final project)
- **Arxiv ID**: http://arxiv.org/abs/1607.06318v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.06318v1)
- **Published**: 2016-07-20 00:38:48+00:00
- **Updated**: 2016-07-20 00:38:48+00:00
- **Authors**: Gergely Odor
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce a novel algorithm for segmentation of imperfect boundary probability maps (BPM) in connectomics. Our algorithm can be a considered as an extension of spectral clustering. Instead of clustering the diffusion maps with traditional clustering algorithms, we learn the manifold and compute an estimate of the minimum normalized cut. We proceed by divide and conquer. We also introduce a novel criterion for determining if further splits are necessary in a component based on it's topological properties. Our algorithm complements the currently popular agglomeration approaches in connectomics, which overlook the geometrical aspects of this segmentation problem.



### Improved Deep Learning of Object Category using Pose Information
- **Arxiv ID**: http://arxiv.org/abs/1607.05836v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.05836v3)
- **Published**: 2016-07-20 07:11:08+00:00
- **Updated**: 2017-01-22 23:53:15+00:00
- **Authors**: Jiaping Zhao, Laurent Itti
- **Comment**: 10 pages, accepted by WACV 2017
- **Journal**: None
- **Summary**: Despite significant recent progress, the best available computer vision algorithms still lag far behind human capabilities, even for recognizing individual discrete objects under various poses, illuminations, and backgrounds. Here we present a new approach to using object pose information to improve deep network learning. While existing large-scale datasets, e.g. ImageNet, do not have pose information, we leverage the newly published turntable dataset, iLab-20M, which has ~22M images of 704 object instances shot under different lightings, camera viewpoints and turntable rotations, to do more controlled object recognition experiments. We introduce a new convolutional neural network architecture, what/where CNN (2W-CNN), built on a linear-chain feedforward CNN (e.g., AlexNet), augmented by hierarchical layers regularized by object poses. Pose information is only used as feedback signal during training, in addition to category information; during test, the feedforward network only predicts category. To validate the approach, we train both 2W-CNN and AlexNet using a fraction of the dataset, and 2W-CNN achieves 6% performance improvement in category prediction. We show mathematically that 2W-CNN has inherent advantages over AlexNet under the stochastic gradient descent (SGD) optimization procedure. Further more, we fine-tune object recognition on ImageNet by using the pretrained 2W-CNN and AlexNet features on iLab-20M, results show that significant improvements have been achieved, compared with training AlexNet from scratch. Moreover, fine-tuning 2W-CNN features performs even better than fine-tuning the pretrained AlexNet features. These results show pretrained features on iLab- 20M generalizes well to natural image datasets, and 2WCNN learns even better features for object recognition than AlexNet.



### Superpixel-based Two-view Deterministic Fitting for Multiple-structure Data
- **Arxiv ID**: http://arxiv.org/abs/1607.05839v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.05839v1)
- **Published**: 2016-07-20 07:28:49+00:00
- **Updated**: 2016-07-20 07:28:49+00:00
- **Authors**: Guobao Xiao, Hanzi Wang, Yan Yan, David Suter
- **Comment**: Accepted by European Conference on Computer Vision (ECCV)
- **Journal**: None
- **Summary**: This paper proposes a two-view deterministic geometric model fitting method, termed Superpixel-based Deterministic Fitting (SDF), for multiple-structure data. SDF starts from superpixel segmentation, which effectively captures prior information of feature appearances. The feature appearances are beneficial to reduce the computational complexity for deterministic fitting methods. SDF also includes two original elements, i.e., a deterministic sampling algorithm and a novel model selection algorithm. The two algorithms are tightly coupled to boost the performance of SDF in both speed and accuracy. Specifically, the proposed sampling algorithm leverages the grouping cues of superpixels to generate reliable and consistent hypotheses. The proposed model selection algorithm further makes use of desirable properties of the generated hypotheses, to improve the conventional fit-and-remove framework for more efficient and effective performance. The key characteristic of SDF is that it can efficiently and deterministically estimate the parameters of model instances in multi-structure data. Experimental results demonstrate that the proposed SDF shows superiority over several state-of-the-art fitting methods for real images with single-structure and multiple-structure data.



### Learning to Recognize Objects by Retaining other Factors of Variation
- **Arxiv ID**: http://arxiv.org/abs/1607.05851v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.05851v3)
- **Published**: 2016-07-20 07:58:57+00:00
- **Updated**: 2017-01-22 23:56:42+00:00
- **Authors**: Jiaping Zhao, Chin-kai Chang, Laurent Itti
- **Comment**: 9 pages, accepted by WACV 2017
- **Journal**: None
- **Summary**: Natural images are generated under many factors, including shape, pose, illumination etc. Most existing ConvNets formulate object recognition from natural images as a single task classification problem, and attempt to learn features useful for object categories, but invariant to other factors of variation as much as possible. These architectures do not explicitly learn other factors, like pose and lighting, instead, they usually discard them by pooling and normalization. In this work, we take the opposite approach: we train ConvNets for object recognition by retaining other factors (pose in our case) and learn them jointly with object category. We design a new multi-task leaning (MTL) ConvNet, named disentangling CNN (disCNN), which explicitly enforces the disentangled representations of object identity and pose, and is trained to predict object categories and pose transformations. We show that disCNN achieves significantly better object recognition accuracies than AlexNet trained solely to predict object categories on the iLab-20M dataset, which is a large scale turntable dataset with detailed object pose and lighting information. We further show that the pretrained disCNN/AlexNet features on iLab- 20M generalize to object recognition on both Washington RGB-D and ImageNet datasets, and the pretrained disCNN features are significantly better than the pretrained AlexNet features for fine-tuning object recognition on the ImageNet dataset.



### Visual Question Answering: A Survey of Methods and Datasets
- **Arxiv ID**: http://arxiv.org/abs/1607.05910v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.05910v1)
- **Published**: 2016-07-20 10:53:29+00:00
- **Updated**: 2016-07-20 10:53:29+00:00
- **Authors**: Qi Wu, Damien Teney, Peng Wang, Chunhua Shen, Anthony Dick, Anton van den Hengel
- **Comment**: 25 pages
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) is a challenging task that has received increasing attention from both the computer vision and the natural language processing communities. Given an image and a question in natural language, it requires reasoning over visual elements of the image and general knowledge to infer the correct answer. In the first part of this survey, we examine the state of the art by comparing modern approaches to the problem. We classify methods by their mechanism to connect the visual and textual modalities. In particular, we examine the common approach of combining convolutional and recurrent neural networks to map images and questions to a common feature space. We also discuss memory-augmented and modular architectures that interface with structured knowledge bases. In the second part of this survey, we review the datasets available for training and evaluating VQA systems. The various datatsets contain questions at different levels of complexity, which require different capabilities and types of reasoning. We examine in depth the question/answer pairs from the Visual Genome project, and evaluate the relevance of the structured annotations of images with scene graphs for VQA. Finally, we discuss promising future directions for the field, in particular the connection to structured knowledge bases and the use of natural language processing models.



### Color Homography Color Correction
- **Arxiv ID**: http://arxiv.org/abs/1607.05947v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.05947v3)
- **Published**: 2016-07-20 13:39:16+00:00
- **Updated**: 2016-08-01 13:17:50+00:00
- **Authors**: Graham D. Finlayson, Han Gong, Robert B. Fisher
- **Comment**: Accepted by Color Imaging Conference 2016
- **Journal**: None
- **Summary**: Homographies -- a mathematical formalism for relating image points across different camera viewpoints -- are at the foundations of geometric methods in computer vision and are used in geometric camera calibration, image registration, and stereo vision and other tasks. In this paper, we show the surprising result that colors across a change in viewing condition (changing light color, shading and camera) are also related by a homography. We propose a new color correction method based on color homography. Experiments demonstrate that solving the color homography problem leads to more accurate calibration.



### Interactive Illumination Invariance
- **Arxiv ID**: http://arxiv.org/abs/1607.05967v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.05967v1)
- **Published**: 2016-07-20 14:15:12+00:00
- **Updated**: 2016-07-20 14:15:12+00:00
- **Authors**: Han Gong, Graham Finlayson
- **Comment**: None
- **Journal**: Color and Imaging Conference, Volume 2015, Number 1, October 2015,
  pp. 186-190(5)
- **Summary**: Illumination effects cause problems for many computer vision algorithms. We present a user-friendly interactive system for robust illumination-invariant image generation. Compared with the previous automated illumination-invariant image derivation approaches, our system enables users to specify a particular kind of illumination variation for removal. The derivation of illumination-invariant image is guided by the user input. The input is a stroke that defines an area covering a set of pixels whose intensities are influenced predominately by the illumination variation. This additional flexibility enhances the robustness for processing non-linearly rendered images and the images of the scenes where their illumination variations are difficult to estimate automatically. Finally, we present some evaluation results of our method.



### 4D Cardiac Ultrasound Standard Plane Location by Spatial-Temporal Correlation
- **Arxiv ID**: http://arxiv.org/abs/1607.05969v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.05969v1)
- **Published**: 2016-07-20 14:19:03+00:00
- **Updated**: 2016-07-20 14:19:03+00:00
- **Authors**: Yun Gu, Guang-Zhong Yang, Jie Yang, Kun Sun
- **Comment**: submitted to MICCAI 2016
- **Journal**: None
- **Summary**: Echocardiography plays an important part in diagnostic aid in cardiac diseases. A critical step in echocardiography-aided diagnosis is to extract the standard planes since they tend to provide promising views to present different structures that are benefit to diagnosis. To this end, this paper proposes a spatial-temporal embedding framework to extract the standard view planes from 4D STIC (spatial-temporal image corre- lation) volumes. The proposed method is comprised of three stages, the frame smoothing, spatial-temporal embedding and final classification. In first stage, an L 0 smoothing filter is used to preprocess the frames that removes the noise and preserves the boundary. Then a compact repre- sentation is learned via embedding spatial and temporal features into a latent space in the supervised scheme considering both standard plane information and diagnosis result. In last stage, the learned features are fed into support vector machine to identify the standard plane. We eval- uate the proposed method on a 4D STIC volume dataset with 92 normal cases and 93 abnormal cases in three standard planes. It demonstrates that our method outperforms the baselines in both classification accuracy and computational efficiency.



### Person Re-identification for Real-world Surveillance Systems
- **Arxiv ID**: http://arxiv.org/abs/1607.05975v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1607.05975v1)
- **Published**: 2016-07-20 14:34:23+00:00
- **Updated**: 2016-07-20 14:34:23+00:00
- **Authors**: Furqan M. Khan, Francois Bremond
- **Comment**: Person re-identification, Visual surveillance
- **Journal**: None
- **Summary**: Appearance based person re-identification in a real-world video surveillance system with non-overlapping camera views is a challenging problem for many reasons. Current state-of-the-art methods often address the problem by relying on supervised learning of similarity metrics or ranking functions to implicitly model appearance transformation between cameras for each camera pair, or group, in the system. This requires considerable human effort to annotate data. Furthermore, the learned models are camera specific and not transferable from one set of cameras to another. Therefore, the annotation process is required after every network expansion or camera replacement, which strongly limits their applicability. Alternatively, we propose a novel modeling approach to harness complementary appearance information without supervised learning that significantly outperforms current state-of-the-art unsupervised methods on multiple benchmark datasets.



### On the Modeling of Error Functions as High Dimensional Landscapes for Weight Initialization in Learning Networks
- **Arxiv ID**: http://arxiv.org/abs/1607.06011v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, physics.data-an, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1607.06011v1)
- **Published**: 2016-07-20 16:25:27+00:00
- **Updated**: 2016-07-20 16:25:27+00:00
- **Authors**: Julius, Gopinath Mahale, Sumana T., C. S. Adityakrishna
- **Comment**: None
- **Journal**: None
- **Summary**: Next generation deep neural networks for classification hosted on embedded platforms will rely on fast, efficient, and accurate learning algorithms. Initialization of weights in learning networks has a great impact on the classification accuracy. In this paper we focus on deriving good initial weights by modeling the error function of a deep neural network as a high-dimensional landscape. We observe that due to the inherent complexity in its algebraic structure, such an error function may conform to general results of the statistics of large systems. To this end we apply some results from Random Matrix Theory to analyse these functions. We model the error function in terms of a Hamiltonian in N-dimensions and derive some theoretical results about its general behavior. These results are further used to make better initial guesses of weights for the learning algorithm.



### Automatic Detection of Solar Photovoltaic Arrays in High Resolution Aerial Imagery
- **Arxiv ID**: http://arxiv.org/abs/1607.06029v1
- **DOI**: 10.1016/j.apenergy.2016.08.191
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.06029v1)
- **Published**: 2016-07-20 17:07:53+00:00
- **Updated**: 2016-07-20 17:07:53+00:00
- **Authors**: Jordan M. Malof, Kyle Bradbury, Leslie M. Collins, Richard G. Newell
- **Comment**: 11 Page manuscript, and 1 page of supplemental information, 10
  figures, currently under review as a journal publication
- **Journal**: None
- **Summary**: The quantity of small scale solar photovoltaic (PV) arrays in the United States has grown rapidly in recent years. As a result, there is substantial interest in high quality information about the quantity, power capacity, and energy generated by such arrays, including at a high spatial resolution (e.g., counties, cities, or even smaller regions). Unfortunately, existing methods for obtaining this information, such as surveys and utility interconnection filings, are limited in their completeness and spatial resolution. This work presents a computer algorithm that automatically detects PV panels using very high resolution color satellite imagery. The approach potentially offers a fast, scalable method for obtaining accurate information on PV array location and size, and at much higher spatial resolutions than are currently available. The method is validated using a very large (135 km^2) collection of publicly available [1] aerial imagery, with over 2,700 human annotated PV array locations. The results demonstrate the algorithm is highly effective on a per-pixel basis. It is likewise effective at object-level PV array detection, but with significant potential for improvement in estimating the precise shape/size of the PV arrays. These results are the first of their kind for the detection of solar PV in aerial imagery, demonstrating the feasibility of the approach and establishing a baseline performance for future investigations.



### Deep Learning of Local RGB-D Patches for 3D Object Detection and 6D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1607.06038v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.06038v1)
- **Published**: 2016-07-20 17:38:15+00:00
- **Updated**: 2016-07-20 17:38:15+00:00
- **Authors**: Wadim Kehl, Fausto Milletari, Federico Tombari, Slobodan Ilic, Nassir Navab
- **Comment**: To appear at ECCV 2016
- **Journal**: None
- **Summary**: We present a 3D object detection method that uses regressed descriptors of locally-sampled RGB-D patches for 6D vote casting. For regression, we employ a convolutional auto-encoder that has been trained on a large collection of random local patches. During testing, scene patch descriptors are matched against a database of synthetic model view patches and cast 6D object votes which are subsequently filtered to refined hypotheses. We evaluate on three datasets to show that our method generalizes well to previously unseen input data, delivers robust detection results that compete with and surpass the state-of-the-art while being scalable in the number of objects.



### Hashmod: A Hashing Method for Scalable 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1607.06062v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.06062v1)
- **Published**: 2016-07-20 19:06:30+00:00
- **Updated**: 2016-07-20 19:06:30+00:00
- **Authors**: Wadim Kehl, Federico Tombari, Nassir Navab, Slobodan Ilic, Vincent Lepetit
- **Comment**: BMVC 2015
- **Journal**: None
- **Summary**: We present a scalable method for detecting objects and estimating their 3D poses in RGB-D data. To this end, we rely on an efficient representation of object views and employ hashing techniques to match these views against the input frame in a scalable way. While a similar approach already exists for 2D detection, we show how to extend it to estimate the 3D pose of the detected objects. In particular, we explore different hashing strategies and identify the one which is more suitable to our problem. We show empirically that the complexity of our method is sublinear with the number of objects and we enable detection and pose estimation of many 3D objects with high accuracy while outperforming the state-of-the-art in terms of runtime.



### Mesh Denoising based on Normal Voting Tensor and Binary Optimization
- **Arxiv ID**: http://arxiv.org/abs/1607.07427v2
- **DOI**: 10.1109/TVCG.2017.2740384
- **Categories**: **cs.CV**, cs.GR, math.DG
- **Links**: [PDF](http://arxiv.org/pdf/1607.07427v2)
- **Published**: 2016-07-20 20:39:37+00:00
- **Updated**: 2017-07-17 14:52:53+00:00
- **Authors**: S. K. Yadav, U. Reitebuch, K. Polthier
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: This paper presents a tensor multiplication based smoothing algorithm that follows a two step denoising method. Unlike other traditional averaging approaches, our approach uses an element based normal voting tensor to compute smooth surfaces. By introducing a binary optimization on the proposed tensor together with a local binary neighborhood concept, our algorithm better retains sharp features and produces smoother umbilical regions than previous approaches. On top of that, we provide a stochastic analysis on the different kinds of noise based on the average edge length. The quantitative and visual results demonstrate the performance our method is better compared to state of the art smoothing approaches.



### Sequence to sequence learning for unconstrained scene text recognition
- **Arxiv ID**: http://arxiv.org/abs/1607.06125v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1607.06125v1)
- **Published**: 2016-07-20 21:02:16+00:00
- **Updated**: 2016-07-20 21:02:16+00:00
- **Authors**: Ahmed Mamdouh A. Hassanien
- **Comment**: It is my master thesis. The thesis was done at Sony Technology Center
  Stuttgart and presented to Nile University. The thesis supervisors are Mark
  Blaxall, Fabien Cardinaux, and Motaz Abdelwahab
- **Journal**: None
- **Summary**: In this work we present a state-of-the-art approach for unconstrained natural scene text recognition. We propose a cascade approach that incorporates a convolutional neural network (CNN) architecture followed by a long short term memory model (LSTM). The CNN learns visual features for the characters and uses them with a softmax layer to detect sequence of characters. While the CNN gives very good recognition results, it does not model relation between characters, hence gives rise to false positive and false negative cases (confusing characters due to visual similarities like "g" and "9", or confusing background patches with characters; either removing existing characters or adding non-existing ones) To alleviate these problems we leverage recent developments in LSTM architectures to encode contextual information. We show that the LSTM can dramatically reduce such errors and achieve state-of-the-art accuracy in the task of unconstrained natural scene text recognition. Moreover we manually remove all occurrences of the words that exist in the test set from our training set to test whether our approach will generalize to unseen data. We use the ICDAR 13 test set for evaluation and compare the results with the state of the art approaches [11, 18]. We finally present an application of the work in the domain of for traffic monitoring.



### A Haar Wavelet-Based Perceptual Similarity Index for Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/1607.06140v4
- **DOI**: 10.1016/j.image.2017.11.001
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.06140v4)
- **Published**: 2016-07-20 22:30:31+00:00
- **Updated**: 2017-11-06 01:33:21+00:00
- **Authors**: Rafael Reisenhofer, Sebastian Bosse, Gitta Kutyniok, Thomas Wiegand
- **Comment**: None
- **Journal**: Signal Processing: Image Communication 61 (2018) 33-43
- **Summary**: In most practical situations, the compression or transmission of images and videos creates distortions that will eventually be perceived by a human observer. Vice versa, image and video restoration techniques, such as inpainting or denoising, aim to enhance the quality of experience of human viewers. Correctly assessing the similarity between an image and an undistorted reference image as subjectively experienced by a human viewer can thus lead to significant improvements in any transmission, compression, or restoration system. This paper introduces the Haar wavelet-based perceptual similarity index (HaarPSI), a novel and computationally inexpensive similarity measure for full reference image quality assessment. The HaarPSI utilizes the coefficients obtained from a Haar wavelet decomposition to assess local similarities between two images, as well as the relative importance of image areas. The consistency of the HaarPSI with the human quality of experience was validated on four large benchmark databases containing thousands of differently distorted images. On these databases, the HaarPSI achieves higher correlations with human opinion scores than state-of-the-art full reference similarity measures like the structural similarity index (SSIM), the feature similarity index (FSIM), and the visual saliency-based index (VSI). Along with the simple computational structure and the short execution time, these experimental results suggest a high applicability of the HaarPSI in real world tasks.



### Learning the Roots of Visual Domain Shift
- **Arxiv ID**: http://arxiv.org/abs/1607.06144v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.06144v1)
- **Published**: 2016-07-20 22:43:44+00:00
- **Updated**: 2016-07-20 22:43:44+00:00
- **Authors**: Tatiana Tommasi, Martina Lanzi, Paolo Russo, Barbara Caputo
- **Comment**: Extended Abstract
- **Journal**: None
- **Summary**: In this paper we focus on the spatial nature of visual domain shift, attempting to learn where domain adaptation originates in each given image of the source and target set. We borrow concepts and techniques from the CNN visualization literature, and learn domainnes maps able to localize the degree of domain specificity in images. We derive from these maps features related to different domainnes levels, and we show that by considering them as a preprocessing step for a domain adaptation algorithm, the final classification performance is strongly improved. Combined with the whole image representation, these features provide state of the art results on the Office dataset.



