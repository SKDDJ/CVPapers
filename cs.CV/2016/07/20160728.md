# Arxiv Papers in cs.CV on 2016-07-28
### Incremental Noising and its Fractal Behavior
- **Arxiv ID**: http://arxiv.org/abs/1607.08362v2
- **DOI**: 10.1007/s11263-017-1034-6
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.08362v2)
- **Published**: 2016-07-28 08:51:02+00:00
- **Updated**: 2016-08-01 17:11:33+00:00
- **Authors**: Konstantinos A. Raftopoulos, Marin Ferecatu, Dionyssios D. Sourlas, Stefanos D. Kollias
- **Comment**: 10 pages, 5 figures
- **Journal**: Int J Comput Vis (2017)
- **Summary**: This manuscript is about further elucidating the concept of noising. The concept of noising first appeared in \cite{CVPR14}, in the context of curvature estimation and vertex localization on planar shapes. There are indications that noising can play for global methods the role smoothing plays for local methods in this task. This manuscript is about investigating this claim by introducing incremental noising, in a recursive deterministic manner, analogous to how smoothing is extended to progressive smoothing in similar tasks. As investigating the properties and behavior of incremental noising is the purpose of this manuscript, a surprising connection between incremental noising and progressive smoothing is revealed by the experiments. To explain this phenomenon, the fractal and the space filling properties of the two methods respectively, are considered in a unifying context.



### 25 years of CNNs: Can we compare to human abstraction capabilities?
- **Arxiv ID**: http://arxiv.org/abs/1607.08366v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.08366v1)
- **Published**: 2016-07-28 09:00:59+00:00
- **Updated**: 2016-07-28 09:00:59+00:00
- **Authors**: Sebastian Stabinger, Antonio Rodríguez-Sánchez, Justus Piater
- **Comment**: To appear in the proceedings of ICANN 2016, Springer
- **Journal**: None
- **Summary**: We try to determine the progress made by convolutional neural networks over the past 25 years in classifying images into abstractc lasses. For this purpose we compare the performance of LeNet to that of GoogLeNet at classifying randomly generated images which are differentiated by an abstract property (e.g., one class contains two objects of the same size, the other class two objects of different sizes). Our results show that there is still work to do in order to solve vision problems humans are able to solve without much difficulty.



### Local Feature Detectors, Descriptors, and Image Representations: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1607.08368v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.08368v1)
- **Published**: 2016-07-28 09:10:19+00:00
- **Updated**: 2016-07-28 09:10:19+00:00
- **Authors**: Yusuke Uchida
- **Comment**: 20 pages
- **Journal**: None
- **Summary**: With the advances in both stable interest region detectors and robust and distinctive descriptors, local feature-based image or object retrieval has become a popular research topic. %All of the local feature-based image retrieval system involves two important processes: local feature extraction and image representation. The other key technology for image retrieval systems is image representation such as the bag-of-visual words (BoVW), Fisher vector, or Vector of Locally Aggregated Descriptors (VLAD) framework. In this paper, we review local features and image representations for image retrieval. Because many and many methods are proposed in this area, these methods are grouped into several classes and summarized. In addition, recent deep learning-based approaches for image retrieval are briefly reviewed.



### Gated Siamese Convolutional Neural Network Architecture for Human Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1607.08378v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.08378v2)
- **Published**: 2016-07-28 09:40:18+00:00
- **Updated**: 2016-09-26 16:28:58+00:00
- **Authors**: Rahul Rama Varior, Mrinal Haloi, Gang Wang
- **Comment**: Accepted to ECCV2016
- **Journal**: None
- **Summary**: Matching pedestrians across multiple camera views, known as human re-identification, is a challenging research problem that has numerous applications in visual surveillance. With the resurgence of Convolutional Neural Networks (CNNs), several end-to-end deep Siamese CNN architectures have been proposed for human re-identification with the objective of projecting the images of similar pairs (i.e. same identity) to be closer to each other and those of dissimilar pairs to be distant from each other. However, current networks extract fixed representations for each image regardless of other images which are paired with it and the comparison with other images is done only at the final level. In this setting, the network is at risk of failing to extract finer local patterns that may be essential to distinguish positive pairs from hard negative pairs. In this paper, we propose a gating function to selectively emphasize such fine common local patterns by comparing the mid-level features across pairs of images. This produces flexible representations for the same image according to the images they are paired with. We conduct experiments on the CUHK03, Market-1501 and VIPeR datasets and demonstrate improved performance compared to a baseline Siamese CNN architecture.



### A Siamese Long Short-Term Memory Architecture for Human Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1607.08381v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.08381v1)
- **Published**: 2016-07-28 09:43:52+00:00
- **Updated**: 2016-07-28 09:43:52+00:00
- **Authors**: Rahul Rama Varior, Bing Shuai, Jiwen Lu, Dong Xu, Gang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Matching pedestrians across multiple camera views known as human re-identification (re-identification) is a challenging problem in visual surveillance. In the existing works concentrating on feature extraction, representations are formed locally and independent of other regions. We present a novel siamese Long Short-Term Memory (LSTM) architecture that can process image regions sequentially and enhance the discriminative capability of local feature representation by leveraging contextual information. The feedback connections and internal gating mechanism of the LSTM cells enable our model to memorize the spatial dependencies and selectively propagate relevant contextual information through the network. We demonstrate improved performance compared to the baseline algorithm with no LSTM units and promising results compared to state-of-the-art methods on Market-1501, CUHK03 and VIPeR datasets. Visualization of the internal mechanism of LSTM cells shows meaningful patterns can be learned by our method.



### SEMBED: Semantic Embedding of Egocentric Action Videos
- **Arxiv ID**: http://arxiv.org/abs/1607.08414v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.08414v2)
- **Published**: 2016-07-28 11:55:38+00:00
- **Updated**: 2016-07-29 09:40:37+00:00
- **Authors**: Michael Wray, Davide Moltisanti, Walterio Mayol-Cuevas, Dima Damen
- **Comment**: None
- **Journal**: None
- **Summary**: We present SEMBED, an approach for embedding an egocentric object interaction video in a semantic-visual graph to estimate the probability distribution over its potential semantic labels. When object interactions are annotated using unbounded choice of verbs, we embrace the wealth and ambiguity of these labels by capturing the semantic relationships as well as the visual similarities over motion and appearance features. We show how SEMBED can interpret a challenging dataset of 1225 freely annotated egocentric videos, outperforming SVM classification by more than 5%.



### Stereo Video Deblurring
- **Arxiv ID**: http://arxiv.org/abs/1607.08421v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.3
- **Links**: [PDF](http://arxiv.org/pdf/1607.08421v1)
- **Published**: 2016-07-28 12:13:10+00:00
- **Updated**: 2016-07-28 12:13:10+00:00
- **Authors**: Anita Sellent, Carsten Rother, Stefan Roth
- **Comment**: Accepted to the 14th European Conference on Computer Vision (ECCV
  2016). Includes supplemental material
- **Journal**: None
- **Summary**: Videos acquired in low-light conditions often exhibit motion blur, which depends on the motion of the objects relative to the camera. This is not only visually unpleasing, but can hamper further processing. With this paper we are the first to show how the availability of stereo video can aid the challenging video deblurring task. We leverage 3D scene flow, which can be estimated robustly even under adverse conditions. We go beyond simply determining the object motion in two ways: First, we show how a piecewise rigid 3D scene flow representation allows to induce accurate blur kernels via local homographies. Second, we exploit the estimated motion boundaries of the 3D scene flow to mitigate ringing artifacts using an iterative weighting scheme. Being aware of 3D object motion, our approach can deal robustly with an arbitrary number of independently moving objects. We demonstrate its benefit over state-of-the-art video deblurring using quantitative and qualitative experiments on rendered scenes and real videos.



### Video Registration in Egocentric Vision under Day and Night Illumination Changes
- **Arxiv ID**: http://arxiv.org/abs/1607.08434v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.08434v1)
- **Published**: 2016-07-28 13:00:03+00:00
- **Updated**: 2016-07-28 13:00:03+00:00
- **Authors**: Stefano Alletto, Giuseppe Serra, Rita Cucchiara
- **Comment**: None
- **Journal**: None
- **Summary**: With the spread of wearable devices and head mounted cameras, a wide range of application requiring precise user localization is now possible. In this paper we propose to treat the problem of obtaining the user position with respect to a known environment as a video registration problem. Video registration, i.e. the task of aligning an input video sequence to a pre-built 3D model, relies on a matching process of local keypoints extracted on the query sequence to a 3D point cloud. The overall registration performance is strictly tied to the actual quality of this 2D-3D matching, and can degrade if environmental conditions such as steep changes in lighting like the ones between day and night occur. To effectively register an egocentric video sequence under these conditions, we propose to tackle the source of the problem: the matching process. To overcome the shortcomings of standard matching techniques, we introduce a novel embedding space that allows us to obtain robust matches by jointly taking into account local descriptors, their spatial arrangement and their temporal robustness. The proposal is evaluated using unconstrained egocentric video sequences both in terms of matching quality and resulting registration performance using different 3D models of historical landmarks. The results show that the proposed method can outperform state of the art registration algorithms, in particular when dealing with the challenges of night and day sequences.



### Faceless Person Recognition; Privacy Implications in Social Media
- **Arxiv ID**: http://arxiv.org/abs/1607.08438v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1607.08438v1)
- **Published**: 2016-07-28 13:10:27+00:00
- **Updated**: 2016-07-28 13:10:27+00:00
- **Authors**: Seong Joon Oh, Rodrigo Benenson, Mario Fritz, Bernt Schiele
- **Comment**: Accepted to ECCV'16
- **Journal**: None
- **Summary**: As we shift more of our lives into the virtual domain, the volume of data shared on the web keeps increasing and presents a threat to our privacy. This works contributes to the understanding of privacy implications of such data sharing by analysing how well people are recognisable in social media data. To facilitate a systematic study we define a number of scenarios considering factors such as how many heads of a person are tagged and if those heads are obfuscated or not. We propose a robust person recognition system that can handle large variations in pose and clothing, and can be trained with few training samples. Our results indicate that a handful of images is enough to threaten users' privacy, even in the presence of obfuscation. We show detailed experimental results, and discuss their implications.



### SSDH: Semi-supervised Deep Hashing for Large Scale Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1607.08477v3
- **DOI**: 10.1109/TCSVT.2017.2771332
- **Categories**: **cs.CV**, H.3.1
- **Links**: [PDF](http://arxiv.org/pdf/1607.08477v3)
- **Published**: 2016-07-28 14:30:21+00:00
- **Updated**: 2017-11-07 03:23:28+00:00
- **Authors**: Jian Zhang, Yuxin Peng
- **Comment**: 14 pages, accepted by IEEE Transactions on Circuits and Systems for
  Video Technology
- **Journal**: None
- **Summary**: Hashing methods have been widely used for efficient similarity retrieval on large scale image database. Traditional hashing methods learn hash functions to generate binary codes from hand-crafted features, which achieve limited accuracy since the hand-crafted features cannot optimally represent the image content and preserve the semantic similarity. Recently, several deep hashing methods have shown better performance because the deep architectures generate more discriminative feature representations. However, these deep hashing methods are mainly designed for supervised scenarios, which only exploit the semantic similarity information, but ignore the underlying data structures. In this paper, we propose the semi-supervised deep hashing (SSDH) approach, to perform more effective hash function learning by simultaneously preserving semantic similarity and underlying data structures. The main contributions are as follows: (1) We propose a semi-supervised loss to jointly minimize the empirical error on labeled data, as well as the embedding error on both labeled and unlabeled data, which can preserve the semantic similarity and capture the meaningful neighbors on the underlying data structures for effective hashing. (2) A semi-supervised deep hashing network is designed to extensively exploit both labeled and unlabeled data, in which we propose an online graph construction method to benefit from the evolving deep features during training to better capture semantic neighbors. To the best of our knowledge, the proposed deep network is the first deep hashing method that can perform hash code learning and feature learning simultaneously in a semi-supervised fashion. Experimental results on 5 widely-used datasets show that our proposed approach outperforms the state-of-the-art hashing methods.



### A Nonlocal Denoising Algorithm for Manifold-Valued Images Using Second Order Statistics
- **Arxiv ID**: http://arxiv.org/abs/1607.08481v3
- **DOI**: None
- **Categories**: **cs.CV**, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1607.08481v3)
- **Published**: 2016-07-28 14:39:13+00:00
- **Updated**: 2016-12-12 09:22:32+00:00
- **Authors**: Friederike Laus, Mila Nikolova, Johannes Persch, Gabriele Steidl
- **Comment**: None
- **Journal**: None
- **Summary**: Nonlocal patch-based methods, in particular the Bayes' approach of Lebrun, Buades and Morel (2013), are considered as state-of-the-art methods for denoising (color) images corrupted by white Gaussian noise of moderate variance. This paper is the first attempt to generalize this technique to manifold-valued images. Such images, for example images with phase or directional entries or with values in the manifold of symmetric positive definite matrices, are frequently encountered in real-world applications. Generalizing the normal law to manifolds is not canonical and different attempts have been considered. Here we focus on a straightforward intrinsic model and discuss the relation to other approaches for specific manifolds. We reinterpret the Bayesian approach of Lebrun et al. (2013) in terms of minimum mean squared error estimation, which motivates our definition of a corresponding estimator on the manifold. With this estimator at hand we present a nonlocal patch-based method for the restoration of manifold-valued images. Various proof of concept examples demonstrate the potential of the proposed algorithm.



### Fine-To-Coarse Global Registration of RGB-D Scans
- **Arxiv ID**: http://arxiv.org/abs/1607.08539v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.08539v3)
- **Published**: 2016-07-28 17:19:46+00:00
- **Updated**: 2016-11-23 04:55:29+00:00
- **Authors**: Maciej Halber, Thomas Funkhouser
- **Comment**: None
- **Journal**: None
- **Summary**: RGB-D scanning of indoor environments is important for many applications, including real estate, interior design, and virtual reality. However, it is still challenging to register RGB-D images from a hand-held camera over a long video sequence into a globally consistent 3D model. Current methods often can lose tracking or drift and thus fail to reconstruct salient structures in large environments (e.g., parallel walls in different rooms). To address this problem, we propose a "fine-to-coarse" global registration algorithm that leverages robust registrations at finer scales to seed detection and enforcement of new correspondence and structural constraints at coarser scales. To test global registration algorithms, we provide a benchmark with 10,401 manually-clicked point correspondences in 25 scenes from the SUN3D dataset. During experiments with this benchmark, we find that our fine-to-coarse algorithm registers long RGB-D sequences better than previous methods.



### A Deep Primal-Dual Network for Guided Depth Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1607.08569v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.08569v1)
- **Published**: 2016-07-28 18:49:55+00:00
- **Updated**: 2016-07-28 18:49:55+00:00
- **Authors**: Gernot Riegler, David Ferstl, Matthias Rüther, Horst Bischof
- **Comment**: BMVC 2016
- **Journal**: None
- **Summary**: In this paper we present a novel method to increase the spatial resolution of depth images. We combine a deep fully convolutional network with a non-local variational method in a deep primal-dual network. The joint network computes a noise-free, high-resolution estimate from a noisy, low-resolution input depth map. Additionally, a high-resolution intensity image is used to guide the reconstruction in the network. By unrolling the optimization steps of a first-order primal-dual algorithm and formulating it as a network, we can train our joint method end-to-end. This not only enables us to learn the weights of the fully convolutional network, but also to optimize all parameters of the variational method and its optimization procedure. The training of such a deep network requires a large dataset for supervision. Therefore, we generate high-quality depth maps and corresponding color images with a physically based renderer. In an exhaustive evaluation we show that our method outperforms the state-of-the-art on multiple benchmarks.



### Connectionist Temporal Modeling for Weakly Supervised Action Labeling
- **Arxiv ID**: http://arxiv.org/abs/1607.08584v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.08584v1)
- **Published**: 2016-07-28 19:35:50+00:00
- **Updated**: 2016-07-28 19:35:50+00:00
- **Authors**: De-An Huang, Li Fei-Fei, Juan Carlos Niebles
- **Comment**: To appear in ECCV 2016
- **Journal**: None
- **Summary**: We propose a weakly-supervised framework for action labeling in video, where only the order of occurring actions is required during training time. The key challenge is that the per-frame alignments between the input (video) and label (action) sequences are unknown during training. We address this by introducing the Extended Connectionist Temporal Classification (ECTC) framework to efficiently evaluate all possible alignments via dynamic programming and explicitly enforce their consistency with frame-to-frame visual similarities. This protects the model from distractions of visually inconsistent or degenerated alignments without the need of temporal supervision. We further extend our framework to the semi-supervised case when a few frames are sparsely annotated in a video. With less than 1% of labeled frames per video, our method is able to outperform existing semi-supervised approaches and achieve comparable performance to that of fully supervised approaches.



### General Automatic Human Shape and Motion Capture Using Volumetric Contour Cues
- **Arxiv ID**: http://arxiv.org/abs/1607.08659v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.08659v2)
- **Published**: 2016-07-28 22:59:55+00:00
- **Updated**: 2016-10-21 11:23:31+00:00
- **Authors**: Helge Rhodin, Nadia Robertini, Dan Casas, Christian Richardt, Hans-Peter Seidel, Christian Theobalt
- **Comment**: Accepted to ECCV 2016, added additional references
- **Journal**: None
- **Summary**: Markerless motion capture algorithms require a 3D body with properly personalized skeleton dimension and/or body shape and appearance to successfully track a person. Unfortunately, many tracking methods consider model personalization a different problem and use manual or semi-automatic model initialization, which greatly reduces applicability. In this paper, we propose a fully automatic algorithm that jointly creates a rigged actor model commonly used for animation - skeleton, volumetric shape, appearance, and optionally a body surface - and estimates the actor's motion from multi-view video input only. The approach is rigorously designed to work on footage of general outdoor scenes recorded with very few cameras and without background subtraction. Our method uses a new image formation model with analytic visibility and analytically differentiable alignment energy. For reconstruction, 3D body shape is approximated as Gaussian density field. For pose and shape estimation, we minimize a new edge-based alignment energy inspired by volume raycasting in an absorbing medium. We further propose a new statistical human body model that represents the body surface, volumetric Gaussian density, as well as variability in skeleton shape. Given any multi-view sequence, our method jointly optimizes the pose and shape parameters of this model fully automatically in a spatiotemporal way.



### Introspective Perception: Learning to Predict Failures in Vision Systems
- **Arxiv ID**: http://arxiv.org/abs/1607.08665v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1607.08665v1)
- **Published**: 2016-07-28 23:27:13+00:00
- **Updated**: 2016-07-28 23:27:13+00:00
- **Authors**: Shreyansh Daftry, Sam Zeng, J. Andrew Bagnell, Martial Hebert
- **Comment**: IEEE/RSJ International Conference on Intelligent Robots and Systems
  (IROS 2016)
- **Journal**: None
- **Summary**: As robots aspire for long-term autonomous operations in complex dynamic environments, the ability to reliably take mission-critical decisions in ambiguous situations becomes critical. This motivates the need to build systems that have situational awareness to assess how qualified they are at that moment to make a decision. We call this self-evaluating capability as introspection. In this paper, we take a small step in this direction and propose a generic framework for introspective behavior in perception systems. Our goal is to learn a model to reliably predict failures in a given system, with respect to a task, directly from input sensor data. We present this in the context of vision-based autonomous MAV flight in outdoor natural environments, and show that it effectively handles uncertain situations.



