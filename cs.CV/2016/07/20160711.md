# Arxiv Papers in cs.CV on 2016-07-11
### Bounds on the Number of Measurements for Reliable Compressive Classification
- **Arxiv ID**: http://arxiv.org/abs/1607.02801v2
- **DOI**: 10.1109/TSP.2016.2599496
- **Categories**: **cs.IT**, cs.CV, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1607.02801v2)
- **Published**: 2016-07-11 01:08:14+00:00
- **Updated**: 2016-08-02 17:59:32+00:00
- **Authors**: Hugo Reboredo, Francesco Renna, Robert Calderbank, Miguel R. D. Rodrigues
- **Comment**: 16 pages, 5 figures, 4 tables. Submitted to the IEEE Transactions on
  Signal Processing
- **Journal**: None
- **Summary**: This paper studies the classification of high-dimensional Gaussian signals from low-dimensional noisy, linear measurements. In particular, it provides upper bounds (sufficient conditions) on the number of measurements required to drive the probability of misclassification to zero in the low-noise regime, both for random measurements and designed ones. Such bounds reveal two important operational regimes that are a function of the characteristics of the source: i) when the number of classes is less than or equal to the dimension of the space spanned by signals in each class, reliable classification is possible in the low-noise regime by using a one-vs-all measurement design; ii) when the dimension of the spaces spanned by signals in each class is lower than the number of classes, reliable classification is guaranteed in the low-noise regime by using a simple random measurement design. Simulation results both with synthetic and real data show that our analysis is sharp, in the sense that it is able to gauge the number of measurements required to drive the misclassification probability to zero in the low-noise regime.



### Efficient Activity Detection in Untrimmed Video with Max-Subgraph Search
- **Arxiv ID**: http://arxiv.org/abs/1607.02815v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.02815v1)
- **Published**: 2016-07-11 03:48:21+00:00
- **Updated**: 2016-07-11 03:48:21+00:00
- **Authors**: Chao-Yeh Chen, Kristen Grauman
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an efficient approach for activity detection in video that unifies activity categorization with space-time localization. The main idea is to pose activity detection as a maximum-weight connected subgraph problem. Offline, we learn a binary classifier for an activity category using positive video exemplars that are "trimmed" in time to the activity of interest. Then, given a novel \emph{untrimmed} video sequence, we decompose it into a 3D array of space-time nodes, which are weighted based on the extent to which their component features support the learned activity model. To perform detection, we then directly localize instances of the activity by solving for the maximum-weight connected subgraph in the test video's space-time graph. We show that this detection strategy permits an efficient branch-and-cut solution for the best-scoring---and possibly non-cubically shaped---portion of the video for a given activity classifier. The upshot is a fast method that can search a broader space of space-time region candidates than was previously practical, which we find often leads to more accurate detection. We demonstrate the proposed algorithm on four datasets, and we show its speed and accuracy advantages over multiple existing search strategies.



### Hypergraph Modelling for Geometric Model Fitting
- **Arxiv ID**: http://arxiv.org/abs/1607.02829v1
- **DOI**: 10.1016/J.PATCOG.2016.06.026
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.02829v1)
- **Published**: 2016-07-11 06:16:37+00:00
- **Updated**: 2016-07-11 06:16:37+00:00
- **Authors**: Guobao Xiao, Hanzi Wang, Taotao Lai, David Suter
- **Comment**: Pattern Recognition, 2016
- **Journal**: None
- **Summary**: In this paper, we propose a novel hypergraph based method (called HF) to fit and segment multi-structural data. The proposed HF formulates the geometric model fitting problem as a hypergraph partition problem based on a novel hypergraph model. In the hypergraph model, vertices represent data points and hyperedges denote model hypotheses. The hypergraph, with large and "data-determined" degrees of hyperedges, can express the complex relationships between model hypotheses and data points. In addition, we develop a robust hypergraph partition algorithm to detect sub-hypergraphs for model fitting. HF can effectively and efficiently estimate the number of, and the parameters of, model instances in multi-structural data heavily corrupted with outliers simultaneously. Experimental results show the advantages of the proposed method over previous methods on both synthetic data and real images.



### Inference of Haemoglobin Concentration From Stereo RGB
- **Arxiv ID**: http://arxiv.org/abs/1607.02936v2
- **DOI**: 10.1007/978-3-319-43775-0_5
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.02936v2)
- **Published**: 2016-07-11 13:29:54+00:00
- **Updated**: 2017-06-22 10:23:09+00:00
- **Authors**: Geoffrey Jones, Neil T. Clancy, Yusuf Helo, Simon Arridge, Daniel S. Elson, Danail Stoyanov
- **Comment**: To appear at the 6th International Conference on Medical Imaging and
  Augmented Reality, MIAR 2016, held in Bern, Switzerland during August 2016,
  and in the corresponding proceedings
- **Journal**: None
- **Summary**: Multispectral imaging (MSI) can provide information about tissue oxygenation, perfusion and potentially function during surgery. In this paper we present a novel, near real-time technique for intrinsic measurements of total haemoglobin (THb) and blood oxygenation (SO2) in tissue using only RGB images from a stereo laparoscope. The high degree of spectral overlap between channels makes inference of haemoglobin concentration challenging, non-linear and under constrained. We decompose the problem into two constrained linear sub-problems and show that with Tikhonov regularisation the estimation significantly improves, giving robust estimation of the Thb. We demonstrate by using the co-registered stereo image data from two cameras it is possible to get robust SO2 estimation as well. Our method is closed from, providing computational efficiency even with multiple cameras. The method we present requires only spectral response calibration of each camera, without modification of existing laparoscopic imaging hardware. We validate our technique on synthetic data from Monte Carlo simulation % of light transport through soft tissue containing submerged blood vessels and further, in vivo, on a multispectral porcine data set.



### Benchmark for License Plate Character Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1607.02937v2
- **DOI**: 10.1117/1.JEI.25.5.053034
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.02937v2)
- **Published**: 2016-07-11 13:32:19+00:00
- **Updated**: 2016-10-31 16:11:21+00:00
- **Authors**: Gabriel Resende Gon√ßalves, Sirlene Pio Gomes da Silva, David Menotti, William Robson Schwartz
- **Comment**: 32 pages, single column
- **Journal**: J. Electron. Imaging. 25(5), 053034 (Oct 24, 2016)
- **Summary**: Automatic License Plate Recognition (ALPR) has been the focus of many researches in the past years. In general, ALPR is divided into the following problems: detection of on-track vehicles, license plates detection, segmention of license plate characters and optical character recognition (OCR). Even though commercial solutions are available for controlled acquisition conditions, e.g., the entrance of a parking lot, ALPR is still an open problem when dealing with data acquired from uncontrolled environments, such as roads and highways when relying only on imaging sensors. Due to the multiple orientations and scales of the license plates captured by the camera, a very challenging task of the ALPR is the License Plate Character Segmentation (LPCS) step, which effectiveness is required to be (near) optimal to achieve a high recognition rate by the OCR. To tackle the LPCS problem, this work proposes a novel benchmark composed of a dataset designed to focus specifically on the character segmentation step of the ALPR within an evaluation protocol. Furthermore, we propose the Jaccard-Centroid coefficient, a new evaluation measure more suitable than the Jaccard coefficient regarding the location of the bounding box within the ground-truth annotation. The dataset is composed of 2,000 Brazilian license plates consisting of 14,000 alphanumeric symbols and their corresponding bounding box annotations. We also present a new straightforward approach to perform LPCS efficiently. Finally, we provide an experimental evaluation for the dataset based on four LPCS approaches and demonstrate the importance of character segmentation for achieving an accurate OCR.



### Systholic Boolean Orthonormalizer Network in Wavelet Domain for SAR Image Despeckling
- **Arxiv ID**: http://arxiv.org/abs/1607.03105v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.03105v1)
- **Published**: 2016-07-11 16:08:25+00:00
- **Updated**: 2016-07-11 16:08:25+00:00
- **Authors**: Mario Mastriani
- **Comment**: 11 pages, 9 figures, 1 table. arXiv admin note: text overlap with
  arXiv:1405.0632
- **Journal**: None
- **Summary**: We describe a novel method for removing speckle (in wavelet domain) of unknown variance from SAR images. The me-thod is based on the following procedure: We apply 1) Bidimentional Discrete Wavelet Transform (DWT-2D) to the speckled image, 2) scaling and rounding to the coefficients of the highest subbands (to obtain integer and positive coefficients), 3) bit-slicing to the new highest subbands (to obtain bit-planes), 4) then we apply the Systholic Boolean Orthonormalizer Network (SBON) to the input bit-plane set and we obtain two orthonormal output bit-plane sets (in a Boolean sense), we project a set on the other one, by means of an AND operation, and then, 5) we apply re-assembling, and, 6) re-sca-ling. Finally, 7) we apply Inverse DWT-2D and reconstruct a SAR image from the modified wavelet coefficients. Despeckling results compare favorably to the most of methods in use at the moment.



### Salient Region Detection and Segmentation in Images using Dynamic Mode Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1607.03021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.03021v1)
- **Published**: 2016-07-11 16:30:06+00:00
- **Updated**: 2016-07-11 16:30:06+00:00
- **Authors**: Sikha O K, Sachin Kumar S, K P Soman
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Visual Saliency is the capability of vision system to select distinctive parts of scene and reduce the amount of visual data that need to be processed. The presentpaper introduces (1) a novel approach to detect salient regions by considering color and luminance based saliency scores using Dynamic Mode Decomposition (DMD), (2) a new interpretation to use DMD approach in static image processing. This approach integrates two data analysis methods: (1) Fourier Transform, (2) Principle Component Analysis.The key idea of our work is to create a color based saliency map. This is based on the observation thatsalient part of an image usually have distinct colors compared to the remaining portion of the image. We have exploited the power of different color spaces to model the complex and nonlinear behavior of human visual system to generate a color based saliency map. To further improve the effect of final saliency map, weutilized luminance information exploiting the fact that human eye is more sensitive towards brightness than color.The experimental results shows that our method based on DMD theory is effective in comparison with previous state-of-art saliency estimation approaches. The approach presented in this paperis evaluated using ROC curve, F-measure rate, Precision-Recall rate, AUC score etc.



### Fast Cosine Transform to increase speed-up and efficiency of Karhunen-Loeve Transform for lossy image compression
- **Arxiv ID**: http://arxiv.org/abs/1607.03164v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.03164v1)
- **Published**: 2016-07-11 21:18:24+00:00
- **Updated**: 2016-07-11 21:18:24+00:00
- **Authors**: Mario Mastriani, Juliana Gambini
- **Comment**: 10 pages, 20 figures, 2 tables
- **Journal**: Intern. Journal of Engineering and Mathematical Sciences. v.6(2),
  pp.82-92 (2010)
- **Summary**: In this work, we present a comparison between two techniques of image compression. In the first case, the image is divided in blocks which are collected according to zig-zag scan. In the second one, we apply the Fast Cosine Transform to the image, and then the transformed image is divided in blocks which are collected according to zig-zag scan too. Later, in both cases, the Karhunen-Loeve transform is applied to mentioned blocks. On the other hand, we present three new metrics based on eigenvalues for a better comparative evaluation of the techniques. Simulations show that the combined version is the best, with minor Mean Absolute Error (MAE) and Mean Squared Error (MSE), higher Peak Signal to Noise Ratio (PSNR) and better image quality. Finally, new technique was far superior to JPEG and JPEG2000.



