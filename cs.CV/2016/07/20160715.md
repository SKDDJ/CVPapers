# Arxiv Papers in cs.CV on 2016-07-15
### DSD: Dense-Sparse-Dense Training for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1607.04381v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.04381v2)
- **Published**: 2016-07-15 04:56:27+00:00
- **Updated**: 2017-02-21 20:51:05+00:00
- **Authors**: Song Han, Jeff Pool, Sharan Narang, Huizi Mao, Enhao Gong, Shijian Tang, Erich Elsen, Peter Vajda, Manohar Paluri, John Tran, Bryan Catanzaro, William J. Dally
- **Comment**: Published as a conference paper at ICLR 2017
- **Journal**: None
- **Summary**: Modern deep neural networks have a large number of parameters, making them very hard to train. We propose DSD, a dense-sparse-dense training flow, for regularizing deep neural networks and achieving better optimization performance. In the first D (Dense) step, we train a dense network to learn connection weights and importance. In the S (Sparse) step, we regularize the network by pruning the unimportant connections with small weights and retraining the network given the sparsity constraint. In the final D (re-Dense) step, we increase the model capacity by removing the sparsity constraint, re-initialize the pruned parameters from zero and retrain the whole dense network. Experiments show that DSD training can improve the performance for a wide range of CNNs, RNNs and LSTMs on the tasks of image classification, caption generation and speech recognition. On ImageNet, DSD improved the Top1 accuracy of GoogLeNet by 1.1%, VGG-16 by 4.3%, ResNet-18 by 1.2% and ResNet-50 by 1.1%, respectively. On the WSJ'93 dataset, DSD improved DeepSpeech and DeepSpeech2 WER by 2.0% and 1.1%. On the Flickr-8K dataset, DSD improved the NeuralTalk BLEU score by over 1.7. DSD is easy to use in practice: at training time, DSD incurs only one extra hyper-parameter: the sparsity ratio in the S step. At testing time, DSD doesn't change the network architecture or incur any inference overhead. The consistent and significant performance gain of DSD experiments shows the inadequacy of the current training methods for finding the best local optimum, while DSD effectively achieves superior optimization performance for finding a better solution. DSD models are available to download at https://songhan.github.io/DSD.



### Low-complexity feedback-channel-free distributed video coding using Local Rank Transform
- **Arxiv ID**: http://arxiv.org/abs/1607.07697v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1607.07697v1)
- **Published**: 2016-07-15 06:13:21+00:00
- **Updated**: 2016-07-15 06:13:21+00:00
- **Authors**: P Raj Bhagath, Kallol Mallick, Jayanta Mukherjee, Sudipta Mukopadhayay
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a new feedback-channel-free Distributed Video Coding (DVC) algorithm using Local Rank Transform (LRT). The encoder computes LRT by considering selected neighborhood pixels of Wyner-Ziv frame. The ranks from the modified LRT are merged, and their positions are entropy coded and sent to the decoder. In addition, means of each block of Wyner-Ziv frame are also transmitted to assist motion estimation. Using these measurements, the decoder generates side information (SI) by implementing motion estimation and compensation in LRT domain. An iterative algorithm is executed on SI using LRT to reconstruct the Wyner-Ziv frame. Experimental results show that the coding efficiency of our codec is close to the efficiency of pixel domain distributed video coders based on Low-Density Parity Check and Accumulate (LDPCA) or turbo codes, with less encoder complexity.



### Model-Driven Feed-Forward Prediction for Manipulation of Deformable Objects
- **Arxiv ID**: http://arxiv.org/abs/1607.04411v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1607.04411v1)
- **Published**: 2016-07-15 08:01:13+00:00
- **Updated**: 2016-07-15 08:01:13+00:00
- **Authors**: Yinxiao Li, Yan Wang, Yonghao Yue, Danfei Xu, Michael Case, Shih-Fu Chang, Eitan Grinspun, Peter Allen
- **Comment**: 21 pages, 27 figures
- **Journal**: None
- **Summary**: Robotic manipulation of deformable objects is a difficult problem especially because of the complexity of the many different ways an object can deform. Searching such a high dimensional state space makes it difficult to recognize, track, and manipulate deformable objects. In this paper, we introduce a predictive, model-driven approach to address this challenge, using a pre-computed, simulated database of deformable object models. Mesh models of common deformable garments are simulated with the garments picked up in multiple different poses under gravity, and stored in a database for fast and efficient retrieval. To validate this approach, we developed a comprehensive pipeline for manipulating clothing as in a typical laundry task. First, the database is used for category and pose estimation for a garment in an arbitrary position. A fully featured 3D model of the garment is constructed in real-time and volumetric features are then used to obtain the most similar model in the database to predict the object category and pose. Second, the database can significantly benefit the manipulation of deformable objects via non-rigid registration, providing accurate correspondences between the reconstructed object model and the database models. Third, the accurate model simulation can also be used to optimize the trajectories for manipulation of deformable objects, such as the folding of garments. Extensive experimental results are shown for the tasks above using a variety of different clothing.



### End-to-End Learning for Image Burst Deblurring
- **Arxiv ID**: http://arxiv.org/abs/1607.04433v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.04433v2)
- **Published**: 2016-07-15 09:46:49+00:00
- **Updated**: 2016-09-06 18:06:15+00:00
- **Authors**: Patrick Wieschollek, Bernhard Sch√∂lkopf, Hendrik P. A. Lensch, Michael Hirsch
- **Comment**: None
- **Journal**: None
- **Summary**: We present a neural network model approach for multi-frame blind deconvolution. The discriminative approach adopts and combines two recent techniques for image deblurring into a single neural network architecture. Our proposed hybrid-architecture combines the explicit prediction of a deconvolution filter and non-trivial averaging of Fourier coefficients in the frequency domain. In order to make full use of the information contained in all images in one burst, the proposed network embeds smaller networks, which explicitly allow the model to transfer information between images in early layers. Our system is trained end-to-end using standard backpropagation on a set of artificially generated training examples, enabling competitive performance in multi-frame blind deconvolution, both with respect to quality and runtime.



### A Real-Time Deep Learning Pedestrian Detector for Robot Navigation
- **Arxiv ID**: http://arxiv.org/abs/1607.04436v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1607.04436v2)
- **Published**: 2016-07-15 09:58:08+00:00
- **Updated**: 2017-09-19 09:31:28+00:00
- **Authors**: David Ribeiro, Andre Mateus, Pedro Miraldo, Jacinto C. Nascimento
- **Comment**: None
- **Journal**: IEEE Int'l Conf. Autonomous Robot Systems and Competitions
  (ICARSC), 2017
- **Summary**: A real-time Deep Learning based method for Pedestrian Detection (PD) is applied to the Human-Aware robot navigation problem. The pedestrian detector combines the Aggregate Channel Features (ACF) detector with a deep Convolutional Neural Network (CNN) in order to obtain fast and accurate performance. Our solution is firstly evaluated using a set of real images taken from onboard and offboard cameras and, then, it is validated in a typical robot navigation environment with pedestrians (two distinct experiments are conducted). The results on both tests show that our pedestrian detector is robust and fast enough to be used on robot navigation applications.



### Efficient and Robust Pedestrian Detection using Deep Learning for Human-Aware Navigation
- **Arxiv ID**: http://arxiv.org/abs/1607.04441v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1607.04441v3)
- **Published**: 2016-07-15 10:16:45+00:00
- **Updated**: 2018-12-13 11:43:14+00:00
- **Authors**: Andre Mateus, David Ribeiro, Pedro Miraldo, Jacinto C. Nascimento
- **Comment**: Accepted in Robotics and Autonomous Systems
- **Journal**: None
- **Summary**: This paper addresses the problem of Human-Aware Navigation (HAN), using multi camera sensors to implement a vision-based person tracking system. The main contributions of this paper are as follows: a novel and efficient Deep Learning person detection and a standardization of human-aware constraints. In the first stage of the approach, we propose to cascade the Aggregate Channel Features (ACF) detector with a deep Convolutional Neural Network (CNN) to achieve fast and accurate Pedestrian Detection (PD). Regarding the human awareness (that can be defined as constraints associated with the robot's motion), we use a mixture of asymmetric Gaussian functions, to define the cost functions associated to each constraint. Both methods proposed herein are evaluated individually to measure the impact of each of the components. The final solution (including both the proposed pedestrian detection and the human-aware constraints) is tested in a typical domestic indoor scenario, in four distinct experiments. The results show that the robot is able to cope with human-aware constraints, defined after common proxemics and social rules.



### Multi-body Non-rigid Structure-from-Motion
- **Arxiv ID**: http://arxiv.org/abs/1607.04515v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.04515v1)
- **Published**: 2016-07-15 14:04:30+00:00
- **Updated**: 2016-07-15 14:04:30+00:00
- **Authors**: Suryansh Kumar, Yuchao Dai, Hongdong Li
- **Comment**: 21 pages, 16 figures
- **Journal**: None
- **Summary**: Conventional structure-from-motion (SFM) research is primarily concerned with the 3D reconstruction of a single, rigidly moving object seen by a static camera, or a static and rigid scene observed by a moving camera --in both cases there are only one relative rigid motion involved. Recent progress have extended SFM to the areas of {multi-body SFM} (where there are {multiple rigid} relative motions in the scene), as well as {non-rigid SFM} (where there is a single non-rigid, deformable object or scene). Along this line of thinking, there is apparently a missing gap of "multi-body non-rigid SFM", in which the task would be to jointly reconstruct and segment multiple 3D structures of the multiple, non-rigid objects or deformable scenes from images. Such a multi-body non-rigid scenario is common in reality (e.g. two persons shaking hands, multi-person social event), and how to solve it represents a natural {next-step} in SFM research. By leveraging recent results of subspace clustering, this paper proposes, for the first time, an effective framework for multi-body NRSFM, which simultaneously reconstructs and segments each 3D trajectory into their respective low-dimensional subspace. Under our formulation, 3D trajectories for each non-rigid structure can be well approximated with a sparse affine combination of other 3D trajectories from the same structure (self-expressiveness). We solve the resultant optimization with the alternating direction method of multipliers (ADMM). We demonstrate the efficacy of the proposed framework through extensive experiments on both synthetic and real data sequences. Our method clearly outperforms other alternative methods, such as first clustering the 2D feature tracks to groups and then doing non-rigid reconstruction in each group or first conducting 3D reconstruction by using single subspace assumption and then clustering the 3D trajectories into groups.



### DAVE: A Unified Framework for Fast Vehicle Detection and Annotation
- **Arxiv ID**: http://arxiv.org/abs/1607.04564v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.04564v3)
- **Published**: 2016-07-15 15:58:16+00:00
- **Updated**: 2016-08-01 08:52:55+00:00
- **Authors**: Yi Zhou, Li Liu, Ling Shao, Matt Mellor
- **Comment**: This paper has been accepted by ECCV 2016
- **Journal**: None
- **Summary**: Vehicle detection and annotation for streaming video data with complex scenes is an interesting but challenging task for urban traffic surveillance. In this paper, we present a fast framework of Detection and Annotation for Vehicles (DAVE), which effectively combines vehicle detection and attributes annotation. DAVE consists of two convolutional neural networks (CNNs): a fast vehicle proposal network (FVPN) for vehicle-like objects extraction and an attributes learning network (ALN) aiming to verify each proposal and infer each vehicle's pose, color and type simultaneously. These two nets are jointly optimized so that abundant latent knowledge learned from the ALN can be exploited to guide FVPN training. Once the system is trained, it can achieve efficient vehicle detection and annotation for real-world traffic surveillance data. We evaluate DAVE on a new self-collected UTS dataset and the public PASCAL VOC2007 car and LISA 2010 datasets, with consistent improvements over existing algorithms.



### Analyzing features learned for Offline Signature Verification using Deep CNNs
- **Arxiv ID**: http://arxiv.org/abs/1607.04573v2
- **DOI**: 10.1109/ICPR.2016.7900092
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1607.04573v2)
- **Published**: 2016-07-15 16:35:20+00:00
- **Updated**: 2016-08-26 14:52:55+00:00
- **Authors**: Luiz G. Hafemann, Robert Sabourin, Luiz S. Oliveira
- **Comment**: Accepted as a conference paper to ICPR 2016
- **Journal**: None
- **Summary**: Research on Offline Handwritten Signature Verification explored a large variety of handcrafted feature extractors, ranging from graphology, texture descriptors to interest points. In spite of advancements in the last decades, performance of such systems is still far from optimal when we test the systems against skilled forgeries - signature forgeries that target a particular individual. In previous research, we proposed a formulation of the problem to learn features from data (signature images) in a Writer-Independent format, using Deep Convolutional Neural Networks (CNNs), seeking to improve performance on the task. In this research, we push further the performance of such method, exploring a range of architectures, and obtaining a large improvement in state-of-the-art performance on the GPDS dataset, the largest publicly available dataset on the task. In the GPDS-160 dataset, we obtained an Equal Error Rate of 2.74%, compared to 6.97% in the best result published in literature (that used a combination of multiple classifiers). We also present a visual analysis of the feature space learned by the model, and an analysis of the errors made by the classifier. Our analysis shows that the model is very effective in separating signatures that have a different global appearance, while being particularly vulnerable to forgeries that very closely resemble genuine signatures, even if their line quality is bad, which is the case of slowly-traced forgeries.



### Spatial Context based Angular Information Preserving Projection for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1607.04593v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.04593v1)
- **Published**: 2016-07-15 17:38:34+00:00
- **Updated**: 2016-07-15 17:38:34+00:00
- **Authors**: Minshan Cui, Saurabh Prasad
- **Comment**: None
- **Journal**: None
- **Summary**: Dimensionality reduction is a crucial preprocessing for hyperspectral data analysis - finding an appropriate subspace is often required for subsequent image classification. In recent work, we proposed supervised angular information based dimensionality reduction methods to find effective subspaces. Since unlabeled data are often more readily available compared to labeled data, we propose an unsupervised projection that finds a lower dimensional subspace where local angular information is preserved. To exploit spatial information from the hyperspectral images, we further extend our unsupervised projection to incorporate spatial contextual information around each pixel in the image. Additionally, we also propose a sparse representation based classifier which is optimized to exploit spatial information during classification - we hence assert that our proposed projection is particularly suitable for classifiers where local similarity and spatial context are both important. Experimental results with two real-world hyperspectral datasets demonstrate that our proposed methods provide a robust classification performance.



### Person Re-identification with Hyperspectral Multi-Camera Systems --- A Pilot Study
- **Arxiv ID**: http://arxiv.org/abs/1607.04609v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.04609v1)
- **Published**: 2016-07-15 18:38:38+00:00
- **Updated**: 2016-07-15 18:38:38+00:00
- **Authors**: Saurabh Prasad, Tanu Priya, Minshan Cui, Shishir Shah
- **Comment**: Accepted for presentation at the 8'th Workshop on Hyperspectral Image
  and Signal Processing, UCLA, August 2016
- **Journal**: None
- **Summary**: Person re-identification in a multi-camera environment is an important part of modern surveillance systems. Person re-identification from color images has been the focus of much active research, due to the numerous challenges posed with such analysis tasks, such as variations in illumination, pose and viewpoints. In this paper, we suggest that hyperspectral imagery has the potential to provide unique information that is expected to be beneficial for the re-identification task. Specifically, we assert that by accurately characterizing the unique spectral signature for each person's skin, hyperspectral imagery can provide very useful descriptors (e.g. spectral signatures from skin pixels) for re-identification. Towards this end, we acquired proof-of-concept hyperspectral re-identification data under challenging (practical) conditions from 15 people. Our results indicate that hyperspectral data result in a substantially enhanced re-identification performance compared to color (RGB) images, when using spectral signatures over skin as the feature descriptor.



### Context Matters: Refining Object Detection in Video with Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1607.04648v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.04648v2)
- **Published**: 2016-07-15 20:02:25+00:00
- **Updated**: 2016-07-19 03:00:35+00:00
- **Authors**: Subarna Tripathi, Zachary C. Lipton, Serge Belongie, Truong Nguyen
- **Comment**: To appear in BMVC 2016
- **Journal**: None
- **Summary**: Given the vast amounts of video available online, and recent breakthroughs in object detection with static images, object detection in video offers a promising new frontier. However, motion blur and compression artifacts cause substantial frame-level variability, even in videos that appear smooth to the eye. Additionally, video datasets tend to have sparsely annotated frames. We present a new framework for improving object detection in videos that captures temporal context and encourages consistency of predictions. First, we train a pseudo-labeler, that is, a domain-adapted convolutional neural network for object detection. The pseudo-labeler is first trained individually on the subset of labeled frames, and then subsequently applied to all frames. Then we train a recurrent neural network that takes as input sequences of pseudo-labeled frames and optimizes an objective that encourages both accuracy on the target frame and consistency across consecutive frames. The approach incorporates strong supervision of target frames, weak-supervision on context frames, and regularization via a smoothness penalty. Our approach achieves mean Average Precision (mAP) of 68.73, an improvement of 7.1 over the strongest image-based baselines for the Youtube-Video Objects dataset. Our experiments demonstrate that neighboring frames can provide valuable information, even absent labels.



### Unifying Registration based Tracking: A Case Study with Structural Similarity
- **Arxiv ID**: http://arxiv.org/abs/1607.04673v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.04673v4)
- **Published**: 2016-07-15 22:25:46+00:00
- **Updated**: 2017-01-30 14:50:49+00:00
- **Authors**: Abhineet Singh, Mennatullah Siam, Martin Jagersand
- **Comment**: Accepted at WACV 2017. Supplementary available at:
  http://webdocs.cs.ualberta.ca/~vis/mtf/ssim_supplementary.pdf arXiv admin
  note: text overlap with arXiv:1603.01292
- **Journal**: None
- **Summary**: This paper adapts a popular image quality measure called structural similarity for high precision registration based tracking while also introducing a simpler and faster variant of the same. Further, these are evaluated comprehensively against existing measures using a unified approach to study registration based trackers that decomposes them into three constituent sub modules - appearance model, state space model and search method. Several popular trackers in literature are broken down using this method so that their contributions - as of this paper - are shown to be limited to only one or two of these submodules. An open source tracking framework is made available that follows this decomposition closely through extensive use of generic programming. It is used to perform all experiments on four publicly available datasets so the results are easily reproducible. This framework provides a convenient interface to plug in a new method for any sub module and combine it with existing methods for the other two. It can also serve as a fast and flexible solution for practical tracking needs due to its highly efficient implementation.



