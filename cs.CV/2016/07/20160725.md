# Arxiv Papers in cs.CV on 2016-07-25
### Exploiting Symmetry and/or Manhattan Properties for 3D Object Structure Estimation from Single and Multiple Images
- **Arxiv ID**: http://arxiv.org/abs/1607.07129v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/1607.07129v3)
- **Published**: 2016-07-25 02:36:51+00:00
- **Updated**: 2017-03-29 08:15:16+00:00
- **Authors**: Yuan Gao, Alan L. Yuille
- **Comment**: Accepted to CVPR 2017
- **Journal**: None
- **Summary**: Many man-made objects have intrinsic symmetries and Manhattan structure. By assuming an orthographic projection model, this paper addresses the estimation of 3D structures and camera projection using symmetry and/or Manhattan structure cues, which occur when the input is single- or multiple-image from the same category, e.g., multiple different cars. Specifically, analysis on the single image case implies that Manhattan alone is sufficient to recover the camera projection, and then the 3D structure can be reconstructed uniquely exploiting symmetry. However, Manhattan structure can be difficult to observe from a single image due to occlusion. To this end, we extend to the multiple-image case which can also exploit symmetry but does not require Manhattan axes. We propose a novel rigid structure from motion method, exploiting symmetry and using multiple images from the same category as input. Experimental results on the Pascal3D+ dataset show that our method significantly outperforms baseline methods.



### A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1607.07155v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.07155v1)
- **Published**: 2016-07-25 05:15:31+00:00
- **Updated**: 2016-07-25 05:15:31+00:00
- **Authors**: Zhaowei Cai, Quanfu Fan, Rogerio S. Feris, Nuno Vasconcelos
- **Comment**: None
- **Journal**: None
- **Summary**: A unified deep neural network, denoted the multi-scale CNN (MS-CNN), is proposed for fast multi-scale object detection. The MS-CNN consists of a proposal sub-network and a detection sub-network. In the proposal sub-network, detection is performed at multiple output layers, so that receptive fields match objects of different scales. These complementary scale-specific detectors are combined to produce a strong multi-scale object detector. The unified network is learned end-to-end, by optimizing a multi-task loss. Feature upsampling by deconvolution is also explored, as an alternative to input upsampling, to reduce the memory and computation costs. State-of-the-art object detection performance, at up to 15 fps, is reported on datasets, such as KITTI and Caltech, containing a substantial number of small objects.



### Large-Scale Video Search with Efficient Temporal Voting Structure
- **Arxiv ID**: http://arxiv.org/abs/1607.07160v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.07160v1)
- **Published**: 2016-07-25 06:41:43+00:00
- **Updated**: 2016-07-25 06:41:43+00:00
- **Authors**: Ersin Esen, Savas Ozkan, Ilkay Atil
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a fast content-based video querying system for large-scale video search. The proposed system is distinguished from similar works with two major contributions. First contribution is superiority of joint usage of repeated content representation and efficient hashing mechanisms. Repeated content representation is utilized with a simple yet robust feature, which is based on edge energy of frames. Each of the representation is converted into hash code with Hamming Embedding method for further queries. Second contribution is novel queue-based voting scheme that leads to modest memory requirements with gradual memory allocation capability, contrary to complete brute-force temporal voting schemes. This aspect enables us to make queries on large video databases conveniently, even on commodity computers with limited memory capacity. Our results show that the system can respond to video queries on a large video database with fast query times, high recall rate and very low memory and disk requirements.



### DeepWarp: Photorealistic Image Resynthesis for Gaze Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1607.07215v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.07215v2)
- **Published**: 2016-07-25 11:27:46+00:00
- **Updated**: 2016-07-26 07:04:43+00:00
- **Authors**: Yaroslav Ganin, Daniil Kononenko, Diana Sungatullina, Victor Lempitsky
- **Comment**: Fixed typos, 14 + 2 + 2 pages, ECCV 2016
- **Journal**: None
- **Summary**: In this work, we consider the task of generating highly-realistic images of a given face with a redirected gaze. We treat this problem as a specific instance of conditional image generation and suggest a new deep architecture that can handle this task very well as revealed by numerical comparison with prior art and a user study. Our deep architecture performs coarse-to-fine warping with an additional intensity correction of individual pixels. All these operations are performed in a feed-forward manner, and the parameters associated with different operations are learned jointly in the end-to-end fashion. After learning, the resulting neural network can synthesize images with manipulated gaze, while the redirection angle can be selected arbitrarily from a certain range and provided as an input to the network.



### Temporal Model Adaptation for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1607.07216v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.07216v1)
- **Published**: 2016-07-25 11:30:03+00:00
- **Updated**: 2016-07-25 11:30:03+00:00
- **Authors**: Niki Martinel, Abir Das, Christian Micheloni, Amit K. Roy-Chowdhury
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification is an open and challenging problem in computer vision. Majority of the efforts have been spent either to design the best feature representation or to learn the optimal matching metric. Most approaches have neglected the problem of adapting the selected features or the learned model over time. To address such a problem, we propose a temporal model adaptation scheme with human in the loop. We first introduce a similarity-dissimilarity learning method which can be trained in an incremental fashion by means of a stochastic alternating directions methods of multipliers optimization procedure. Then, to achieve temporal adaptation with limited human effort, we exploit a graph-based approach to present the user only the most informative probe-gallery matches that should be used to update the model. Results on three datasets have shown that our approach performs on par or even better than state-of-the-art approaches while reducing the manual pairwise labeling effort by about 80%.



### Local- and Holistic- Structure Preserving Image Super Resolution via Deep Joint Component Learning
- **Arxiv ID**: http://arxiv.org/abs/1607.07220v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.07220v1)
- **Published**: 2016-07-25 11:45:48+00:00
- **Updated**: 2016-07-25 11:45:48+00:00
- **Authors**: Yukai Shi, Keze Wang, Li Xu, Liang Lin
- **Comment**: Published on ICME 2016 (oral), 6 pages, 6 figures
- **Journal**: None
- **Summary**: Recently, machine learning based single image super resolution (SR) approaches focus on jointly learning representations for high-resolution (HR) and low-resolution (LR) image patch pairs to improve the quality of the super-resolved images. However, due to treat all image pixels equally without considering the salient structures, these approaches usually fail to produce visual pleasant images with sharp edges and fine details. To address this issue, in this work we present a new novel SR approach, which replaces the main building blocks of the classical interpolation pipeline by a flexible, content-adaptive deep neural networks. In particular, two well-designed structure-aware components, respectively capturing local- and holistic- image contents, are naturally incorporated into the fully-convolutional representation learning to enhance the image sharpness and naturalness. Extensively evaluations on several standard benchmarks (e.g., Set5, Set14 and BSD200) demonstrate that our approach can achieve superior results, especially on the image with salient structures, over many existing state-of-the-art SR methods under both quantitative and qualitative measures.



### Automatic Attribute Discovery with Neural Activations
- **Arxiv ID**: http://arxiv.org/abs/1607.07262v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.07262v1)
- **Published**: 2016-07-25 13:30:10+00:00
- **Updated**: 2016-07-25 13:30:10+00:00
- **Authors**: Sirion Vittayakorn, Takayuki Umeda, Kazuhiko Murasaki, Kyoko Sudo, Takayuki Okatani, Kota Yamaguchi
- **Comment**: ECCV 2016
- **Journal**: None
- **Summary**: How can a machine learn to recognize visual attributes emerging out of online community without a definitive supervised dataset? This paper proposes an automatic approach to discover and analyze visual attributes from a noisy collection of image-text data on the Web. Our approach is based on the relationship between attributes and neural activations in the deep network. We characterize the visual property of the attribute word as a divergence within weakly-annotated set of images. We show that the neural activations are useful for discovering and learning a classifier that well agrees with human perception from the noisy real-world Web data. The empirical study suggests the layered structure of the deep neural networks also gives us insights into the perceptual depth of the given word. Finally, we demonstrate that we can utilize highly-activating neurons for finding semantically relevant regions.



### A Statistical Test for Joint Distributions Equivalence
- **Arxiv ID**: http://arxiv.org/abs/1607.07270v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1607.07270v1)
- **Published**: 2016-07-25 13:48:20+00:00
- **Updated**: 2016-07-25 13:48:20+00:00
- **Authors**: Francesco Solera, Andrea Palazzi
- **Comment**: None
- **Journal**: None
- **Summary**: We provide a distribution-free test that can be used to determine whether any two joint distributions $p$ and $q$ are statistically different by inspection of a large enough set of samples. Following recent efforts from Long et al. [1], we rely on joint kernel distribution embedding to extend the kernel two-sample test of Gretton et al. [2] to the case of joint probability distributions. Our main result can be directly applied to verify if a dataset-shift has occurred between training and test distributions in a learning framework, without further assuming the shift has occurred only in the input, in the target or in the conditional distribution.



### Learning Aligned Cross-Modal Representations from Weakly Aligned Data
- **Arxiv ID**: http://arxiv.org/abs/1607.07295v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.07295v1)
- **Published**: 2016-07-25 14:38:36+00:00
- **Updated**: 2016-07-25 14:38:36+00:00
- **Authors**: Lluis Castrejon, Yusuf Aytar, Carl Vondrick, Hamed Pirsiavash, Antonio Torralba
- **Comment**: Conference paper at CVPR 2016
- **Journal**: None
- **Summary**: People can recognize scenes across many different modalities beyond natural images. In this paper, we investigate how to learn cross-modal scene representations that transfer across modalities. To study this problem, we introduce a new cross-modal scene dataset. While convolutional neural networks can categorize cross-modal scenes well, they also learn an intermediate representation not aligned across modalities, which is undesirable for cross-modal transfer applications. We present methods to regularize cross-modal convolutional neural networks so that they have a shared representation that is agnostic of the modality. Our experiments suggest that our scene representation can help transfer representations across modalities for retrieval. Moreover, our visualizations suggest that units emerge in the shared representation that tend to activate on consistent concepts independently of the modality.



### Tracking with multi-level features
- **Arxiv ID**: http://arxiv.org/abs/1607.07304v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.07304v1)
- **Published**: 2016-07-25 15:07:45+00:00
- **Updated**: 2016-07-25 15:07:45+00:00
- **Authors**: Roberto Henschel, Laura Leal-Taixé, Bodo Rosenhahn, Konrad Schindler
- **Comment**: Submitted as an IEEE PAMI short article
- **Journal**: None
- **Summary**: We present a novel formulation of the multiple object tracking problem which integrates low and mid-level features. In particular, we formulate the tracking problem as a quadratic program coupling detections and dense point trajectories. Due to the computational complexity of the initial QP, we propose an approximation by two auxiliary problems, a temporal and spatial association, where the temporal subproblem can be efficiently solved by a linear program and the spatial association by a clustering algorithm. The objective function of the QP is used in order to find the optimal number of clusters, where each cluster ideally represents one person. Evaluation is provided for multiple scenarios, showing the superiority of our method with respect to classic tracking-by-detection methods and also other methods that greedily integrate low-level features.



### Symmetry-free SDP Relaxations for Affine Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/1607.07387v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.DM, stat.ML, 62H30 and 90C22
- **Links**: [PDF](http://arxiv.org/pdf/1607.07387v1)
- **Published**: 2016-07-25 18:01:17+00:00
- **Updated**: 2016-07-25 18:01:17+00:00
- **Authors**: Francesco Silvestri, Gerhard Reinelt, Christoph Schnörr
- **Comment**: 23 pages
- **Journal**: None
- **Summary**: We consider clustering problems where the goal is to determine an optimal partition of a given point set in Euclidean space in terms of a collection of affine subspaces. While there is vast literature on heuristics for this kind of problem, such approaches are known to be susceptible to poor initializations and getting trapped in bad local optima. We alleviate these issues by introducing a semidefinite relaxation based on Lasserre's method of moments. While a similiar approach is known for classical Euclidean clustering problems, a generalization to our more general subspace scenario is not straightforward, due to the high symmetry of the objective function that weakens any convex relaxation. We therefore introduce a new mechanism for symmetry breaking based on covering the feasible region with polytopes. Additionally, we introduce and analyze a deterministic rounding heuristic.



### gvnn: Neural Network Library for Geometric Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1607.07405v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1607.07405v3)
- **Published**: 2016-07-25 18:57:17+00:00
- **Updated**: 2016-08-12 17:28:24+00:00
- **Authors**: Ankur Handa, Michael Bloesch, Viorica Patraucean, Simon Stent, John McCormac, Andrew Davison
- **Comment**: Submitted to ECCV Workshop on Deep Geometry
- **Journal**: None
- **Summary**: We introduce gvnn, a neural network library in Torch aimed towards bridging the gap between classic geometric computer vision and deep learning. Inspired by the recent success of Spatial Transformer Networks, we propose several new layers which are often used as parametric transformations on the data in geometric computer vision. These layers can be inserted within a neural network much in the spirit of the original spatial transformers and allow backpropagation to enable end-to-end learning of a network involving any domain knowledge in geometric computer vision. This opens up applications in learning invariance to 3D geometric transformation for place recognition, end-to-end visual odometry, depth estimation and unsupervised learning through warping with a parametric transformation for image reconstruction error.



### Much Ado About Time: Exhaustive Annotation of Temporal Data
- **Arxiv ID**: http://arxiv.org/abs/1607.07429v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1607.07429v2)
- **Published**: 2016-07-25 19:51:42+00:00
- **Updated**: 2016-10-03 01:20:32+00:00
- **Authors**: Gunnar A. Sigurdsson, Olga Russakovsky, Ali Farhadi, Ivan Laptev, Abhinav Gupta
- **Comment**: HCOMP 2016 Camera Ready
- **Journal**: None
- **Summary**: Large-scale annotated datasets allow AI systems to learn from and build upon the knowledge of the crowd. Many crowdsourcing techniques have been developed for collecting image annotations. These techniques often implicitly rely on the fact that a new input image takes a negligible amount of time to perceive. In contrast, we investigate and determine the most cost-effective way of obtaining high-quality multi-label annotations for temporal data such as videos. Watching even a short 30-second video clip requires a significant time investment from a crowd worker; thus, requesting multiple annotations following a single viewing is an important cost-saving strategy. But how many questions should we ask per video? We conclude that the optimal strategy is to ask as many questions as possible in a HIT (up to 52 binary questions after watching a 30-second video clip in our experiments). We demonstrate that while workers may not correctly answer all questions, the cost-benefit analysis nevertheless favors consensus from multiple such cheap-yet-imperfect iterations over more complex alternatives. When compared with a one-question-per-video baseline, our method is able to achieve a 10% improvement in recall 76.7% ours versus 66.7% baseline) at comparable precision (83.8% ours versus 83.0% baseline) in about half the annotation time (3.8 minutes ours compared to 7.1 minutes baseline). We demonstrate the effectiveness of our method by collecting multi-label annotations of 157 human activities on 1,815 videos.



