# Arxiv Papers in cs.CV on 2016-07-19
### A Multi-task Deep Network for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1607.05369v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.05369v3)
- **Published**: 2016-07-19 01:59:02+00:00
- **Updated**: 2016-11-25 06:22:57+00:00
- **Authors**: Weihua Chen, Xiaotang Chen, Jianguo Zhang, Kaiqi Huang
- **Comment**: Accepted by AAAI2017
- **Journal**: None
- **Summary**: Person re-identification (ReID) focuses on identifying people across different scenes in video surveillance, which is usually formulated as a binary classification task or a ranking task in current person ReID approaches. In this paper, we take both tasks into account and propose a multi-task deep network (MTDnet) that makes use of their own advantages and jointly optimize the two tasks simultaneously for person ReID. To the best of our knowledge, we are the first to integrate both tasks in one network to solve the person ReID. We show that our proposed architecture significantly boosts the performance. Furthermore, deep architecture in general requires a sufficient dataset for training, which is usually not met in person ReID. To cope with this situation, we further extend the MTDnet and propose a cross-domain architecture that is capable of using an auxiliary set to assist training on small target sets. In the experiments, our approach outperforms most of existing person ReID algorithms on representative datasets including CUHK03, CUHK01, VIPeR, iLIDS and PRID2011, which clearly demonstrates the effectiveness of the proposed approach.



### Generating Images Part by Part with Composite Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1607.05387v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1607.05387v2)
- **Published**: 2016-07-19 03:09:31+00:00
- **Updated**: 2016-11-14 07:32:35+00:00
- **Authors**: Hanock Kwak, Byoung-Tak Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Image generation remains a fundamental problem in artificial intelligence in general and deep learning in specific. The generative adversarial network (GAN) was successful in generating high quality samples of natural images. We propose a model called composite generative adversarial network, that reveals the complex structure of images with multiple generators in which each generator generates some part of the image. Those parts are combined by alpha blending process to create a new single image. It can generate, for example, background and face sequentially with two generators, after training on face dataset. Training was done in an unsupervised way without any labels about what each generator should generate. We found possibilities of learning the structure by using this generative model empirically.



### Binary Hashing with Semidefinite Relaxation and Augmented Lagrangian
- **Arxiv ID**: http://arxiv.org/abs/1607.05396v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.05396v1)
- **Published**: 2016-07-19 04:20:24+00:00
- **Updated**: 2016-07-19 04:20:24+00:00
- **Authors**: Thanh-Toan Do, Anh-Dzung Doan, Duc-Thanh Nguyen, Ngai-Man Cheung
- **Comment**: Appearing in European Conference on Computer Vision (ECCV) 2016
- **Journal**: None
- **Summary**: This paper proposes two approaches for inferencing binary codes in two-step (supervised, unsupervised) hashing. We first introduce an unified formulation for both supervised and unsupervised hashing. Then, we cast the learning of one bit as a Binary Quadratic Problem (BQP). We propose two approaches to solve BQP. In the first approach, we relax BQP as a semidefinite programming problem which its global optimum can be achieved. We theoretically prove that the objective value of the binary solution achieved by this approach is well bounded. In the second approach, we propose an augmented Lagrangian based approach to solve BQP directly without relaxing the binary constraint. Experimental results on three benchmark datasets show that our proposed methods compare favorably with the state of the art.



### Runtime Configurable Deep Neural Networks for Energy-Accuracy Trade-off
- **Arxiv ID**: http://arxiv.org/abs/1607.05418v2
- **DOI**: 10.1145/2968456.2968458
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1607.05418v2)
- **Published**: 2016-07-19 06:27:05+00:00
- **Updated**: 2016-07-20 20:42:51+00:00
- **Authors**: Hokchhay Tann, Soheil Hashemi, R. Iris Bahar, Sherief Reda
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel dynamic configuration technique for deep neural networks that permits step-wise energy-accuracy trade-offs during runtime. Our configuration technique adjusts the number of channels in the network dynamically depending on response time, power, and accuracy targets. To enable this dynamic configuration technique, we co-design a new training algorithm, where the network is incrementally trained such that the weights in channels trained in earlier steps are fixed. Our technique provides the flexibility of multiple networks while storing and utilizing one set of weights. We evaluate our techniques using both an ASIC-based hardware accelerator as well as a low-power embedded GPGPU and show that our approach leads to only a small or negligible loss in the final network accuracy. We analyze the performance of our proposed methodology using three well-known networks for MNIST, CIFAR-10, and SVHN datasets, and we show that we are able to achieve up to 95% energy reduction with less than 1% accuracy loss across the three benchmarks. In addition, compared to prior work on dynamic network reconfiguration, we show that our approach leads to approximately 50% savings in storage requirements, while achieving similar accuracy.



### Training Skinny Deep Neural Networks with Iterative Hard Thresholding Methods
- **Arxiv ID**: http://arxiv.org/abs/1607.05423v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.05423v1)
- **Published**: 2016-07-19 06:41:31+00:00
- **Updated**: 2016-07-19 06:41:31+00:00
- **Authors**: Xiaojie Jin, Xiaotong Yuan, Jiashi Feng, Shuicheng Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have achieved remarkable success in a wide range of practical problems. However, due to the inherent large parameter space, deep models are notoriously prone to overfitting and difficult to be deployed in portable devices with limited memory. In this paper, we propose an iterative hard thresholding (IHT) approach to train Skinny Deep Neural Networks (SDNNs). An SDNN has much fewer parameters yet can achieve competitive or even better performance than its full CNN counterpart. More concretely, the IHT approach trains an SDNN through following two alternative phases: (I) perform hard thresholding to drop connections with small activations and fine-tune the other significant filters; (II)~re-activate the frozen connections and train the entire network to improve its overall discriminative capability. We verify the superiority of SDNNs in terms of efficiency and classification performance on four benchmark object recognition datasets, including CIFAR-10, CIFAR-100, MNIST and ImageNet. Experimental results clearly demonstrate that IHT can be applied for training SDNN based on various CNN architectures such as NIN and AlexNet.



### Trunk-Branch Ensemble Convolutional Neural Networks for Video-based Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1607.05427v2
- **DOI**: 10.1109/TPAMI.2017.2700390
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.05427v2)
- **Published**: 2016-07-19 07:14:28+00:00
- **Updated**: 2017-05-17 09:12:19+00:00
- **Authors**: Changxing Ding, Dacheng Tao
- **Comment**: Accepted Version to IEEE T-PAMI
- **Journal**: None
- **Summary**: Human faces in surveillance videos often suffer from severe image blur, dramatic pose variations, and occlusion. In this paper, we propose a comprehensive framework based on Convolutional Neural Networks (CNN) to overcome challenges in video-based face recognition (VFR). First, to learn blur-robust face representations, we artificially blur training data composed of clear still images to account for a shortfall in real-world video training data. Using training data composed of both still images and artificially blurred data, CNN is encouraged to learn blur-insensitive features automatically. Second, to enhance robustness of CNN features to pose variations and occlusion, we propose a Trunk-Branch Ensemble CNN model (TBE-CNN), which extracts complementary information from holistic face images and patches cropped around facial components. TBE-CNN is an end-to-end model that extracts features efficiently by sharing the low- and middle-level convolutional layers between the trunk and branch networks. Third, to further promote the discriminative power of the representations learnt by TBE-CNN, we propose an improved triplet loss function. Systematic experiments justify the effectiveness of the proposed techniques. Most impressively, TBE-CNN achieves state-of-the-art performance on three popular video face databases: PaSC, COX Face, and YouTube Faces. With the proposed techniques, we also obtain the first place in the BTAS 2016 Video Person Recognition Evaluation.



### Collaborative Layer-wise Discriminative Learning in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1607.05440v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.05440v1)
- **Published**: 2016-07-19 07:56:37+00:00
- **Updated**: 2016-07-19 07:56:37+00:00
- **Authors**: Xiaojie Jin, Yunpeng Chen, Jian Dong, Jiashi Feng, Shuicheng Yan
- **Comment**: To appear in ECCV 2016. Maybe subject to minor changes before
  camera-ready version
- **Journal**: None
- **Summary**: Intermediate features at different layers of a deep neural network are known to be discriminative for visual patterns of different complexities. However, most existing works ignore such cross-layer heterogeneities when classifying samples of different complexities. For example, if a training sample has already been correctly classified at a specific layer with high confidence, we argue that it is unnecessary to enforce rest layers to classify this sample correctly and a better strategy is to encourage those layers to focus on other samples.   In this paper, we propose a layer-wise discriminative learning method to enhance the discriminative capability of a deep network by allowing its layers to work collaboratively for classification. Towards this target, we introduce multiple classifiers on top of multiple layers. Each classifier not only tries to correctly classify the features from its input layer, but also coordinates with other classifiers to jointly maximize the final classification performance. Guided by the other companion classifiers, each classifier learns to concentrate on certain training examples and boosts the overall performance. Allowing for end-to-end training, our method can be conveniently embedded into state-of-the-art deep networks. Experiments with multiple popular deep networks, including Network in Network, GoogLeNet and VGGNet, on scale-various object classification benchmarks, including CIFAR100, MNIST and ImageNet, and scene classification benchmarks, including MIT67, SUN397 and Places205, demonstrate the effectiveness of our method. In addition, we also analyze the relationship between the proposed method and classical conditional random fields models.



### On Differentiating Parameterized Argmin and Argmax Problems with Application to Bi-level Optimization
- **Arxiv ID**: http://arxiv.org/abs/1607.05447v2
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1607.05447v2)
- **Published**: 2016-07-19 08:09:30+00:00
- **Updated**: 2016-07-21 03:43:35+00:00
- **Authors**: Stephen Gould, Basura Fernando, Anoop Cherian, Peter Anderson, Rodrigo Santa Cruz, Edison Guo
- **Comment**: 16 pages, 6 figures
- **Journal**: None
- **Summary**: Some recent works in machine learning and computer vision involve the solution of a bi-level optimization problem. Here the solution of a parameterized lower-level problem binds variables that appear in the objective of an upper-level problem. The lower-level problem typically appears as an argmin or argmax optimization problem. Many techniques have been proposed to solve bi-level optimization problems, including gradient descent, which is popular with current end-to-end learning approaches. In this technical report we collect some results on differentiating argmin and argmax optimization problems with and without constraints and provide some insightful motivating examples.



### Supervised Transformer Network for Efficient Face Detection
- **Arxiv ID**: http://arxiv.org/abs/1607.05477v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.05477v1)
- **Published**: 2016-07-19 09:15:38+00:00
- **Updated**: 2016-07-19 09:15:38+00:00
- **Authors**: Dong Chen, Gang Hua, Fang Wen, Jian Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Large pose variations remain to be a challenge that confronts real-word face detection. We propose a new cascaded Convolutional Neural Network, dubbed the name Supervised Transformer Network, to address this challenge. The first stage is a multi-task Region Proposal Network (RPN), which simultaneously predicts candidate face regions along with associated facial landmarks. The candidate regions are then warped by mapping the detected facial landmarks to their canonical positions to better normalize the face patterns. The second stage, which is a RCNN, then verifies if the warped candidate regions are valid faces or not. We conduct end-to-end learning of the cascaded network, including optimizing the canonical positions of the facial landmarks. This supervised learning of the transformations automatically selects the best scale to differentiate face/non-face patterns. By combining feature maps from both stages of the network, we achieve state-of-the-art detection accuracies on several public benchmarks. For real-time performance, we run the cascaded network only on regions of interests produced from a boosting cascade face detector. Our detector runs at 30 FPS on a single CPU core for a VGA-resolution image.



### Dendritic Spine Shape Analysis: A Clustering Perspective
- **Arxiv ID**: http://arxiv.org/abs/1607.05523v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.05523v1)
- **Published**: 2016-07-19 11:18:52+00:00
- **Updated**: 2016-07-19 11:18:52+00:00
- **Authors**: Muhammad Usman Ghani, Ertunc Erdil, Sumeyra Demir Kanik, Ali Ozgur Argunsah, Anna Felicity Hobbiss, Inbal Israely, Devrim Unay, Tolga Tasdizen, Mujdat Cetin
- **Comment**: Accepted for BioImageComputing workshop at ECCV 2016
- **Journal**: None
- **Summary**: Functional properties of neurons are strongly coupled with their morphology. Changes in neuronal activity alter morphological characteristics of dendritic spines. First step towards understanding the structure-function relationship is to group spines into main spine classes reported in the literature. Shape analysis of dendritic spines can help neuroscientists understand the underlying relationships. Due to unavailability of reliable automated tools, this analysis is currently performed manually which is a time-intensive and subjective task. Several studies on spine shape classification have been reported in the literature, however, there is an on-going debate on whether distinct spine shape classes exist or whether spines should be modeled through a continuum of shape variations. Another challenge is the subjectivity and bias that is introduced due to the supervised nature of classification approaches. In this paper, we aim to address these issues by presenting a clustering perspective. In this context, clustering may serve both confirmation of known patterns and discovery of new ones. We perform cluster analysis on two-photon microscopic images of spines using morphological, shape, and appearance based features and gain insights into the spine shape analysis problem. We use histogram of oriented gradients (HOG), disjunctive normal shape models (DNSM), morphological features, and intensity profile based features for cluster analysis. We use x-means to perform cluster analysis that selects the number of clusters automatically using the Bayesian information criterion (BIC). For all features, this analysis produces 4 clusters and we observe the formation of at least one cluster consisting of spines which are difficult to be assigned to a known class. This observation supports the argument of intermediate shape types.



### Dual Purpose Hashing
- **Arxiv ID**: http://arxiv.org/abs/1607.05529v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.05529v1)
- **Published**: 2016-07-19 11:37:00+00:00
- **Updated**: 2016-07-19 11:37:00+00:00
- **Authors**: Haomiao Liu, Ruiping Wang, Shiguang Shan, Xilin Chen
- **Comment**: With supplementary materials added to the end
- **Journal**: None
- **Summary**: Recent years have seen more and more demand for a unified framework to address multiple realistic image retrieval tasks concerning both category and attributes. Considering the scale of modern datasets, hashing is favorable for its low complexity. However, most existing hashing methods are designed to preserve one single kind of similarity, thus improper for dealing with the different tasks simultaneously. To overcome this limitation, we propose a new hashing method, named Dual Purpose Hashing (DPH), which jointly preserves the category and attribute similarities by exploiting the Convolutional Neural Network (CNN) models to hierarchically capture the correlations between category and attributes. Since images with both category and attribute labels are scarce, our method is designed to take the abundant partially labelled images on the Internet as training inputs. With such a framework, the binary codes of new-coming images can be readily obtained by quantizing the network outputs of a binary-like layer, and the attributes can be recovered from the codes easily. Experiments on two large-scale datasets show that our dual purpose hash codes can achieve comparable or even better performance than those state-of-the-art methods specifically designed for each individual retrieval task, while being more compact than the compared methods.



### A Local-Global Approach to Semantic Segmentation in Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/1607.05620v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.05620v1)
- **Published**: 2016-07-19 15:02:57+00:00
- **Updated**: 2016-07-19 15:02:57+00:00
- **Authors**: Alina Elena Marcu
- **Comment**: 50 pages, 18 figures. Master's Thesis, University Politehnica of
  Bucharest
- **Journal**: None
- **Summary**: Aerial images are often taken under poor lighting conditions and contain low resolution objects, many times occluded by other objects. In this domain, visual context could be of great help, but there are still very few papers that consider context in aerial image understanding and still remains an open problem in computer vision. We propose a dual-stream deep neural network that processes information along two independent pathways. Our model learns to combine local and global appearance in a complementary way, such that together form a powerful classifier. We test our dual-stream network on the task of buildings segmentation in aerial images and obtain state-of-the-art results on the Massachusetts Buildings Dataset. We study the relative importance of local appearance versus the larger scene, as well as their performance in combination on three new buildings datasets. We clearly demonstrate the effectiveness of visual context in conjunction with deep neural networks for aerial image understanding.



### Information-theoretical label embeddings for large-scale image classification
- **Arxiv ID**: http://arxiv.org/abs/1607.05691v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1607.05691v1)
- **Published**: 2016-07-19 18:40:01+00:00
- **Updated**: 2016-07-19 18:40:01+00:00
- **Authors**: François Chollet
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for training multi-label, massively multi-class image classification models, that is faster and more accurate than supervision via a sigmoid cross-entropy loss (logistic regression). Our method consists in embedding high-dimensional sparse labels onto a lower-dimensional dense sphere of unit-normed vectors, and treating the classification problem as a cosine proximity regression problem on this sphere. We test our method on a dataset of 300 million high-resolution images with 17,000 labels, where it yields considerably faster convergence, as well as a 7% higher mean average precision compared to logistic regression.



### FusionNet: 3D Object Classification Using Multiple Data Representations
- **Arxiv ID**: http://arxiv.org/abs/1607.05695v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.05695v4)
- **Published**: 2016-07-19 18:50:23+00:00
- **Updated**: 2016-11-27 01:00:22+00:00
- **Authors**: Vishakh Hegde, Reza Zadeh
- **Comment**: None
- **Journal**: None
- **Summary**: High-quality 3D object recognition is an important component of many vision and robotics systems. We tackle the object recognition problem using two data representations, to achieve leading results on the Princeton ModelNet challenge. The two representations: 1. Volumetric representation: the 3D object is discretized spatially as binary voxels - $1$ if the voxel is occupied and $0$ otherwise. 2. Pixel representation: the 3D object is represented as a set of projected 2D pixel images. Current leading submissions to the ModelNet Challenge use Convolutional Neural Networks (CNNs) on pixel representations. However, we diverge from this trend and additionally, use Volumetric CNNs to bridge the gap between the efficiency of the above two representations. We combine both representations and exploit them to learn new features, which yield a significantly better classifier than using either of the representations in isolation. To do this, we introduce new Volumetric CNN (V-CNN) architectures.



### Spatially Supervised Recurrent Convolutional Neural Networks for Visual Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1607.05781v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.05781v1)
- **Published**: 2016-07-19 23:27:56+00:00
- **Updated**: 2016-07-19 23:27:56+00:00
- **Authors**: Guanghan Ning, Zhi Zhang, Chen Huang, Zhihai He, Xiaobo Ren, Haohong Wang
- **Comment**: 10 pages, 9 figures, conference
- **Journal**: None
- **Summary**: In this paper, we develop a new approach of spatially supervised recurrent convolutional neural networks for visual object tracking. Our recurrent convolutional network exploits the history of locations as well as the distinctive visual features learned by the deep neural networks. Inspired by recent bounding box regression methods for object detection, we study the regression capability of Long Short-Term Memory (LSTM) in the temporal domain, and propose to concatenate high-level visual features produced by convolutional networks with region information. In contrast to existing deep learning based trackers that use binary classification for region candidates, we use regression for direct prediction of the tracking locations both at the convolutional layer and at the recurrent unit. Our extensive experimental results and performance comparison with state-of-the-art tracking methods on challenging benchmark video tracking datasets shows that our tracker is more accurate and robust while maintaining low computational cost. For most test video sequences, our method achieves the best tracking performance, often outperforms the second best by a large margin.



