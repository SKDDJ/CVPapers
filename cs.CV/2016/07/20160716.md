# Arxiv Papers in cs.CV on 2016-07-16
### Spatio-Temporal Saliency Networks for Dynamic Saliency Prediction
- **Arxiv ID**: http://arxiv.org/abs/1607.04730v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.04730v2)
- **Published**: 2016-07-16 11:46:38+00:00
- **Updated**: 2017-11-15 06:56:11+00:00
- **Authors**: Cagdas Bak, Aysun Kocak, Erkut Erdem, Aykut Erdem
- **Comment**: None
- **Journal**: None
- **Summary**: Computational saliency models for still images have gained significant popularity in recent years. Saliency prediction from videos, on the other hand, has received relatively little interest from the community. Motivated by this, in this work, we study the use of deep learning for dynamic saliency prediction and propose the so-called spatio-temporal saliency networks. The key to our models is the architecture of two-stream networks where we investigate different fusion mechanisms to integrate spatial and temporal information. We evaluate our models on the DIEM and UCF-Sports datasets and present highly competitive results against the existing state-of-the-art models. We also carry out some experiments on a number of still images from the MIT300 dataset by exploiting the optical flow maps predicted from these images. Our results show that considering inherent motion information in this way can be helpful for static saliency estimation.



### Weakly supervised object detection using pseudo-strong labels
- **Arxiv ID**: http://arxiv.org/abs/1607.04731v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.04731v1)
- **Published**: 2016-07-16 11:49:18+00:00
- **Updated**: 2016-07-16 11:49:18+00:00
- **Authors**: Ke Yang, Dongsheng Li, Yong Dou, Shaohe Lv, Qiang Wang
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Object detection is an import task of computer vision.A variety of methods have been proposed,but methods using the weak labels still do not have a satisfactory result.In this paper,we propose a new framework that using the weakly supervised method's output as the pseudo-strong labels to train a strongly supervised model.One weakly supervised method is treated as black-box to generate class-specific bounding boxes on train dataset.A de-noise method is then applied to the noisy bounding boxes.Then the de-noised pseudo-strong labels are used to train a strongly object detection network.The whole framework is still weakly supervised because the entire process only uses the image-level labels.The experiment results on PASCAL VOC 2007 prove the validity of our framework, and we get result 43.4% on mean average precision compared to 39.5% of the previous best result and 34.5% of the initial method,respectively.And this frame work is simple and distinct,and is promising to be applied to other method easily.



### New version of Gram-Schmidt Process with inverse for Signal and Image Processing
- **Arxiv ID**: http://arxiv.org/abs/1607.04759v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.04759v1)
- **Published**: 2016-07-16 16:29:15+00:00
- **Updated**: 2016-07-16 16:29:15+00:00
- **Authors**: Mario Mastriani
- **Comment**: 12 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: The Gram-Schmidt Process (GSP) is used to convert a non-orthogonal basis (a set of linearly independent vectors, matrices, etc) into an orthonormal basis (a set of orthogonal, unit-length vectors, bi or tri dimensional matrices). The process consists of taking each array and then subtracting the projections in common with the previous arrays. This paper introduces an enhanced version of the Gram-Schmidt Process (EGSP) with inverse, which is useful for Digital Signal and Image Processing, among others applications.



### Design and implementation of image processing system for Lumen social robot-humanoid as an exhibition guide for Electrical Engineering Days 2015
- **Arxiv ID**: http://arxiv.org/abs/1607.04760v1
- **DOI**: 10.13140/RG.2.1.3432.0889/1
- **Categories**: **cs.CV**, cs.HC, cs.RO, I.2.10; H.5.2
- **Links**: [PDF](http://arxiv.org/pdf/1607.04760v1)
- **Published**: 2016-07-16 16:34:09+00:00
- **Updated**: 2016-07-16 16:34:09+00:00
- **Authors**: Setyaki Sholata Sya, Ary Setijadi Prihatmanto
- **Comment**: Lumen, image processing system, face detection, face recognition,
  face tracking, human detection
- **Journal**: None
- **Summary**: Lumen Social Robot is a humanoid robot development with the purpose that it could be a good friend to all people. In this year, the Lumen Social Robot is being developed into a guide in the exhibition and in the seminar of the Final Exam of undergraduate and graduate students in Electrical Engineering ITB, named Electrical Engineering Days 2015. In order to be the guide in that occasion, Lumen is supported by several things. They are Nao robot components, servers, and multiple processor systems. The image processing system is a processing application system that allows Lumen to recognize and determine an object from the image taken from the camera eye. The image processing system is provided with four modules. They are face detection module to detect a person's face, face recognition module to recognize a person's face, face tracking module to follow a person's face, and human detection module to detect humans based on the upper parts of person's body. Face detection module and human detection module are implemented by using the library harcascade.xml on EMGU CV. Face recognition module is implemented by adding the database for the face that has been detected and store it in that database. Face tracking module is implemented by using the Smooth Gaussian filter to the image.   -----   Lumen Sosial Robot merupakan sebuah pengembangan robot humanoid agar dapat menjadi teman bagi banyak orang. Sistem pengolahan citra merupakan sistem aplikasi pengolah yang bertujuan Lumen dapat mengenali dan mengetahui suatu objek pada citra yang diambil dari camera mata Lumen. System pengolahan citra dilengkapi dengan empat buah modul, yaitu modul face detection untuk mendeteksi wajah seseorang, modul face recognition untuk mengenali wajah orang tersebut, modul face tracking untuk mengikuti wajah seseorang, dan modul human detection untuk mendeteksi manusia berdasarkan bagian tubuh atas orang



### Construction of extended 3D field of views of the internal bladder wall surface: a proof of concept
- **Arxiv ID**: http://arxiv.org/abs/1607.04773v1
- **DOI**: 10.1007/s13319-016-0095-6
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.04773v1)
- **Published**: 2016-07-16 17:32:07+00:00
- **Updated**: 2016-07-16 17:32:07+00:00
- **Authors**: Achraf Ben-Hamadou, Christian Daul, Charles Soussen
- **Comment**: 23 pages, Springer 3D Research, 2016
- **Journal**: None
- **Summary**: 3D extended field of views (FOVs) of the internal bladder wall facilitate lesion diagnosis, patient follow-up and treatment traceability. In this paper, we propose a 3D image mosaicing algorithm guided by 2D cystoscopic video-image registration for obtaining textured FOV mosaics. In this feasibility study, the registration makes use of data from a 3D cystoscope prototype providing, in addition to each small FOV image, some 3D points located on the surface. This proof of concept shows that textured surfaces can be constructed with minimally modified cystoscopes. The potential of the method is demonstrated on numerical and real phantoms reproducing various surface shapes. Pig and human bladder textures are superimposed on phantoms with known shape and dimensions. These data allow for quantitative assessment of the 3D mosaicing algorithm based on the registration of images simulating bladder textures.



### Exploiting Multi-modal Curriculum in Noisy Web Data for Large-scale Concept Learning
- **Arxiv ID**: http://arxiv.org/abs/1607.04780v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1607.04780v1)
- **Published**: 2016-07-16 18:14:51+00:00
- **Updated**: 2016-07-16 18:14:51+00:00
- **Authors**: Junwei Liang, Lu Jiang, Deyu Meng, Alexander Hauptmann
- **Comment**: None
- **Journal**: None
- **Summary**: Learning video concept detectors automatically from the big but noisy web data with no additional manual annotations is a novel but challenging area in the multimedia and the machine learning community. A considerable amount of videos on the web are associated with rich but noisy contextual information, such as the title, which provides weak annotations or labels about the video content. To leverage the big noisy web labels, this paper proposes a novel method called WEbly-Labeled Learning (WELL), which is established on the state-of-the-art machine learning algorithm inspired by the learning process of human. WELL introduces a number of novel multi-modal approaches to incorporate meaningful prior knowledge called curriculum from the noisy web videos. To investigate this problem, we empirically study the curriculum constructed from the multi-modal features of the videos collected from YouTube and Flickr. The efficacy and the scalability of WELL have been extensively demonstrated on two public benchmarks, including the largest multimedia dataset and the largest manually-labeled video set. The comprehensive experimental results demonstrate that WELL outperforms state-of-the-art studies by a statically significant margin on learning concepts from noisy web video data. In addition, the results also verify that WELL is robust to the level of noisiness in the video data. Notably, WELL trained on sufficient noisy web labels is able to achieve a comparable accuracy to supervised learning methods trained on the clean manually-labeled data.



