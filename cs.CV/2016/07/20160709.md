# Arxiv Papers in cs.CV on 2016-07-09
### A Photometrically Calibrated Benchmark For Monocular Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/1607.02555v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.02555v2)
- **Published**: 2016-07-09 00:11:14+00:00
- **Updated**: 2016-10-08 20:06:10+00:00
- **Authors**: Jakob Engel, Vladyslav Usenko, Daniel Cremers
- **Comment**: * Corrected a bug in the evaluation setup, which caused the real-time
  results for ORB-SLAM (dashed lines in Figure 8) to be much worse than they
  should be. * https://vision.in.tum.de/data/datasets/mono-dataset
- **Journal**: None
- **Summary**: We present a dataset for evaluating the tracking accuracy of monocular visual odometry and SLAM methods. It contains 50 real-world sequences comprising more than 100 minutes of video, recorded across dozens of different environments -- ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position. This allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated. We provide exposure times for each frame as reported by the sensor, the camera response function, and dense lens attenuation factors. We also propose a novel, simple approach to non-parametric vignette calibration, which requires minimal set-up and is easy to reproduce. Finally, we thoroughly evaluate two existing methods (ORB-SLAM and DSO) on the dataset, including an analysis of the effect of image resolution, camera field of view, and the camera motion direction.



### Action Recognition with Joint Attention on Multi-Level Deep Features
- **Arxiv ID**: http://arxiv.org/abs/1607.02556v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.02556v1)
- **Published**: 2016-07-09 01:25:24+00:00
- **Updated**: 2016-07-09 01:25:24+00:00
- **Authors**: Jialin Wu, Gu Wang, Wukui Yang, Xiangyang Ji
- **Comment**: 13 pages, submitted to BMVC
- **Journal**: None
- **Summary**: We propose a novel deep supervised neural network for the task of action recognition in videos, which implicitly takes advantage of visual tracking and shares the robustness of both deep Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN). In our method, a multi-branch model is proposed to suppress noise from background jitters. Specifically, we firstly extract multi-level deep features from deep CNNs and feed them into 3d-convolutional network. After that we feed those feature cubes into our novel joint LSTM module to predict labels and to generate attention regularization. We evaluate our model on two challenging datasets: UCF101 and HMDB51. The results show that our model achieves the state-of-art by only using convolutional features.



### Direct Sparse Odometry
- **Arxiv ID**: http://arxiv.org/abs/1607.02565v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.02565v2)
- **Published**: 2016-07-09 04:02:31+00:00
- **Updated**: 2016-10-07 04:31:21+00:00
- **Authors**: Jakob Engel, Vladlen Koltun, Daniel Cremers
- **Comment**: ** Corrected a bug which caused the real-time results for ORB-SLAM
  (dashed lines in Fig. 10 and 12) to be much worse than they should be **
  Added references [12], [13],[19], and Fig. 11. ** Partly re-formulated and
  extended [5. Conclusion]. ** Fixed typos and minor re-formulations
- **Journal**: None
- **Summary**: We propose a novel direct sparse visual odometry formulation. It combines a fully direct probabilistic model (minimizing a photometric error) with consistent, joint optimization of all model parameters, including geometry -- represented as inverse depth in a reference frame -- and camera motion. This is achieved in real time by omitting the smoothness prior used in other direct methods and instead sampling pixels evenly throughout the images. Since our method does not depend on keypoint detectors or descriptors, it can naturally sample pixels from across all image regions that have intensity gradient, including edges or smooth intensity variations on mostly white walls. The proposed model integrates a full photometric calibration, accounting for exposure time, lens vignetting, and non-linear response functions. We thoroughly evaluate our method on three different datasets comprising several hours of video. The experiments show that the presented approach significantly outperforms state-of-the-art direct and indirect methods in a variety of real-world settings, both in terms of tracking accuracy and robustness.



### Deep Learning of Appearance Models for Online Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1607.02568v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.02568v1)
- **Published**: 2016-07-09 06:15:20+00:00
- **Updated**: 2016-07-09 06:15:20+00:00
- **Authors**: Mengyao Zhai, Mehrsan Javan Roshtkhari, Greg Mori
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a novel deep learning based approach for vision based single target tracking. We address this problem by proposing a network architecture which takes the input video frames and directly computes the tracking score for any candidate target location by estimating the probability distributions of the positive and negative examples. This is achieved by combining a deep convolutional neural network with a Bayesian loss layer in a unified framework. In order to deal with the limited number of positive training examples, the network is pre-trained offline for a generic image feature representation and then is fine-tuned in multiple steps. An online fine-tuning step is carried out at every frame to learn the appearance of the target. We adopt a two-stage iterative algorithm to adaptively update the network parameters and maintain a probability density for target/non-target regions. The tracker has been tested on the standard tracking benchmark and the results indicate that the proposed solution achieves state-of-the-art tracking results.



### Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1607.02586v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1607.02586v1)
- **Published**: 2016-07-09 08:41:40+00:00
- **Updated**: 2016-07-09 08:41:40+00:00
- **Authors**: Tianfan Xue, Jiajun Wu, Katherine L. Bouman, William T. Freeman
- **Comment**: The first two authors contributed equally to this work
- **Journal**: None
- **Summary**: We study the problem of synthesizing a number of likely future frames from a single input image. In contrast to traditional methods, which have tackled this problem in a deterministic or non-parametric way, we propose a novel approach that models future frames in a probabilistic manner. Our probabilistic model makes it possible for us to sample and synthesize many possible future frames from a single input image. Future frame synthesis is challenging, as it involves low- and high-level image and motion understanding. We propose a novel network structure, namely a Cross Convolutional Network to aid in synthesizing future frames; this network structure encodes image and motion information as feature maps and convolutional kernels, respectively. In experiments, our model performs well on synthetic data, such as 2D shapes and animated game sprites, as well as on real-wold videos. We also show that our model can be applied to tasks such as visual analogy-making, and present an analysis of the learned network representations.



### Hierarchical Deep Temporal Models for Group Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1607.02643v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.02643v1)
- **Published**: 2016-07-09 18:23:36+00:00
- **Updated**: 2016-07-09 18:23:36+00:00
- **Authors**: Mostafa S. Ibrahim, Srikanth Muralidharan, Zhiwei Deng, Arash Vahdat, Greg Mori
- **Comment**: arXiv admin note: text overlap with arXiv:1511.06040
- **Journal**: None
- **Summary**: In this paper we present an approach for classifying the activity performed by a group of people in a video sequence. This problem of group activity recognition can be addressed by examining individual person actions and their relations. Temporal dynamics exist both at the level of individual person actions as well as at the level of group activity. Given a video sequence as input, methods can be developed to capture these dynamics at both person-level and group-level detail. We build a deep model to capture these dynamics based on LSTM (long short-term memory) models. In order to model both person-level and group-level dynamics, we present a 2-stage deep temporal model for the group activity recognition problem. In our approach, one LSTM model is designed to represent action dynamics of individual people in a video sequence and another LSTM model is designed to aggregate person-level information for group activity recognition. We collected a new dataset consisting of volleyball videos labeled with individual and group activities in order to evaluate our method. Experimental results on this new Volleyball Dataset and the standard benchmark Collective Activity Dataset demonstrate the efficacy of the proposed models.



### Multimodal Affect Recognition using Kinect
- **Arxiv ID**: http://arxiv.org/abs/1607.02652v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1607.02652v1)
- **Published**: 2016-07-09 20:01:33+00:00
- **Updated**: 2016-07-09 20:01:33+00:00
- **Authors**: Amol Patwardhan, Gerald Knapp
- **Comment**: 9 pages, 2 tables, 1 figure, Peer reviewed in ACM TIST
- **Journal**: None
- **Summary**: Affect (emotion) recognition has gained significant attention from researchers in the past decade. Emotion-aware computer systems and devices have many applications ranging from interactive robots, intelligent online tutor to emotion based navigation assistant. In this research data from multiple modalities such as face, head, hand, body and speech was utilized for affect recognition. The research used color and depth sensing device such as Kinect for facial feature extraction and tracking human body joints. Temporal features across multiple frames were used for affect recognition. Event driven decision level fusion was used to combine the results from each individual modality using majority voting to recognize the emotions. The study also implemented affect recognition by matching the features to the rule based emotion templates per modality. Experiments showed that multimodal affect recognition rates using combination of emotion templates and supervised learning were better compared to recognition rates based on supervised learning alone. Recognition rates obtained using temporal feature were higher compared to recognition rates obtained using position based features only.



### Combining multiple resolutions into hierarchical representations for kernel-based image classification
- **Arxiv ID**: http://arxiv.org/abs/1607.02654v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1607.02654v2)
- **Published**: 2016-07-09 20:07:37+00:00
- **Updated**: 2016-07-12 08:34:49+00:00
- **Authors**: Yanwei Cui, Sébastien Lefevre, Laetitia Chapel, Anne Puissant
- **Comment**: International Conference on Geographic Object-Based Image Analysis
  (GEOBIA 2016), University of Twente in Enschede, The Netherlands
- **Journal**: None
- **Summary**: Geographic object-based image analysis (GEOBIA) framework has gained increasing interest recently. Following this popular paradigm, we propose a novel multiscale classification approach operating on a hierarchical image representation built from two images at different resolutions. They capture the same scene with different sensors and are naturally fused together through the hierarchical representation, where coarser levels are built from a Low Spatial Resolution (LSR) or Medium Spatial Resolution (MSR) image while finer levels are generated from a High Spatial Resolution (HSR) or Very High Spatial Resolution (VHSR) image. Such a representation allows one to benefit from the context information thanks to the coarser levels, and subregions spatial arrangement information thanks to the finer levels. Two dedicated structured kernels are then used to perform machine learning directly on the constructed hierarchical representation. This strategy overcomes the limits of conventional GEOBIA classification procedures that can handle only one or very few pre-selected scales. Experiments run on an urban classification task show that the proposed approach can highly improve the classification accuracy w.r.t. conventional approaches working on a single scale.



### Augmenting Supervised Emotion Recognition with Rule-Based Decision Model
- **Arxiv ID**: http://arxiv.org/abs/1607.02660v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1607.02660v1)
- **Published**: 2016-07-09 20:34:48+00:00
- **Updated**: 2016-07-09 20:34:48+00:00
- **Authors**: Amol Patwardhan, Gerald Knapp
- **Comment**: 8 pages, 6 figures, 23 tables, IEEE TAC (in review)
- **Journal**: None
- **Summary**: The aim of this research is development of rule based decision model for emotion recognition. This research also proposes using the rules for augmenting inter-corporal recognition accuracy in multimodal systems that use supervised learning techniques. The classifiers for such learning based recognition systems are susceptible to over fitting and only perform well on intra-corporal data. To overcome the limitation this research proposes using rule based model as an additional modality. The rules were developed using raw feature data from visual channel, based on human annotator agreement and existing studies that have attributed movement and postures to emotions. The outcome of the rule evaluations was combined during the decision phase of emotion recognition system. The results indicate rule based emotion recognition augment recognition accuracy of learning based systems and also provide better recognition rate across inter corpus emotion test data.



