# Arxiv Papers in cs.CV on 2016-07-08
### Multi Channel-Kernel Canonical Correlation Analysis for Cross-View Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1607.02204v2
- **DOI**: 10.1145/3038916
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.02204v2)
- **Published**: 2016-07-08 00:40:38+00:00
- **Updated**: 2017-03-21 10:14:10+00:00
- **Authors**: Giuseppe Lisanti, Svebor Karaman, Iacopo Masi
- **Comment**: The latest/updated version of the manuscript with more experiments
  can be found at https://doi.org/10.1145/3038916. Please cite the paper using
  https://doi.org/10.1145/3038916
- **Journal**: ACM Transactions on Multimedia Computing, Communications, and
  Applications (TOMM), Volume 13 Issue 2, March 2017
- **Summary**: In this paper we introduce a method to overcome one of the main challenges of person re-identification in multi-camera networks, namely cross-view appearance changes. The proposed solution addresses the extreme variability of person appearance in different camera views by exploiting multiple feature representations. For each feature, Kernel Canonical Correlation Analysis (KCCA) with different kernels is exploited to learn several projection spaces in which the appearance correlation between samples of the same person observed from different cameras is maximized. An iterative logistic regression is finally used to select and weigh the contributions of each feature projections and perform the matching between the two views. Experimental evaluation shows that the proposed solution obtains comparable performance on VIPeR and PRID 450s datasets and improves on PRID and CUHK01 datasets with respect to the state of the art.



### From Collective Adaptive Systems to Human Centric Computation and Back: Spatial Model Checking for Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/1607.02235v1
- **DOI**: 10.4204/EPTCS.217.10
- **Categories**: **cs.LO**, cs.CV, D.2.4 model checking; F.4.1 modal logic; J.3 medical information
  systems
- **Links**: [PDF](http://arxiv.org/pdf/1607.02235v1)
- **Published**: 2016-07-08 05:37:08+00:00
- **Updated**: 2016-07-08 05:37:08+00:00
- **Authors**: Gina Belmonte, Vincenzo Ciancia, Diego Latella, Mieke Massink
- **Comment**: In Proceedings FORECAST 2016, arXiv:1607.02001
- **Journal**: EPTCS 217, 2016, pp. 81-92
- **Summary**: Recent research on formal verification for Collective Adaptive Systems (CAS) pushed advancements in spatial and spatio-temporal model checking, and as a side result provided novel image analysis methodologies, rooted in logical methods for topological spaces. Medical Imaging (MI) is a field where such technologies show potential for ground-breaking innovation. In this position paper, we present a preliminary investigation centred on applications of spatial model checking to MI. The focus is shifted from pure logics to a mixture of logical, statistical and algorithmic approaches, driven by the logical nature intrinsic to the specification of the properties of interest in the field. As a result, novel operators are introduced, that could as well be brought back to the setting of CAS.



### Overcoming Challenges in Fixed Point Training of Deep Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1607.02241v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1607.02241v1)
- **Published**: 2016-07-08 06:07:03+00:00
- **Updated**: 2016-07-08 06:07:03+00:00
- **Authors**: Darryl D. Lin, Sachin S. Talathi
- **Comment**: ICML2016 - Workshop on On-Device Intelligence
- **Journal**: None
- **Summary**: It is known that training deep neural networks, in particular, deep convolutional networks, with aggressively reduced numerical precision is challenging. The stochastic gradient descent algorithm becomes unstable in the presence of noisy gradient updates resulting from arithmetic with limited numeric precision. One of the well-accepted solutions facilitating the training of low precision fixed point networks is stochastic rounding. However, to the best of our knowledge, the source of the instability in training neural networks with noisy gradient updates has not been well investigated. This work is an attempt to draw a theoretical connection between low numerical precision and training algorithm stability. In doing so, we will also propose and verify through experiments methods that are able to improve the training performance of deep convolutional networks in fixed point.



### Siamese Regression Networks with Efficient mid-level Feature Extraction for 3D Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1607.02257v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.02257v1)
- **Published**: 2016-07-08 07:25:47+00:00
- **Updated**: 2016-07-08 07:25:47+00:00
- **Authors**: Andreas Doumanoglou, Vassileios Balntas, Rigas Kouskouridas, Tae-Kyun Kim
- **Comment**: 9 pages, paper submitted to NIPS 2016, project page:
  http://www.iis.ee.ic.ac.uk/rkouskou/research/SRN.html
- **Journal**: None
- **Summary**: In this paper we tackle the problem of estimating the 3D pose of object instances, using convolutional neural networks. State of the art methods usually solve the challenging problem of regression in angle space indirectly, focusing on learning discriminative features that are later fed into a separate architecture for 3D pose estimation. In contrast, we propose an end-to-end learning framework for directly regressing object poses by exploiting Siamese Networks. For a given image pair, we enforce a similarity measure between the representation of the sample images in the feature and pose space respectively, that is shown to boost regression performance. Furthermore, we argue that our pose-guided feature learning using our Siamese Regression Network generates more discriminative features that outperform the state of the art. Last, our feature learning formulation provides the ability of learning features that can perform under severe occlusions, demonstrating high performance on our novel hand-object dataset.



### Non-Central Catadioptric Cameras Pose Estimation using 3D Lines
- **Arxiv ID**: http://arxiv.org/abs/1607.02290v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1607.02290v1)
- **Published**: 2016-07-08 09:50:36+00:00
- **Updated**: 2016-07-08 09:50:36+00:00
- **Authors**: Andre Mateus, Pedro Miraldo, Pedro U. Lima
- **Comment**: None
- **Journal**: None
- **Summary**: In this article we purpose a novel method for planar pose estimation of mobile robots. This method is based on an analytic solution (which we derived) for the projection of 3D straight lines, onto the mirror of Non-Central Catadioptric Cameras (NCCS). The resulting solution is rewritten as a function of the rotation and translation parameters, which is then used as an error function for a set of mirror points. Those should be the result of the projection of a set of points incident with the respective 3D lines. The camera's pose is given by minimizing the error function, with the associated constraints. The method is validated by experiments both with synthetic and real data. The latter was collected from a mobile robot equipped with a NCCS.



### CNN-LTE: a Class of 1-X Pooling Convolutional Neural Networks on Label Tree Embeddings for Audio Scene Recognition
- **Arxiv ID**: http://arxiv.org/abs/1607.02303v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, cs.MM, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1607.02303v2)
- **Published**: 2016-07-08 10:39:05+00:00
- **Updated**: 2016-08-15 18:05:00+00:00
- **Authors**: Huy Phan, Lars Hertel, Marco Maass, Philipp Koch, Alfred Mertins
- **Comment**: Task1 technical report for the DCASE2016 challenge. arXiv admin note:
  text overlap with arXiv:1606.07908
- **Journal**: None
- **Summary**: We describe in this report our audio scene recognition system submitted to the DCASE 2016 challenge. Firstly, given the label set of the scenes, a label tree is automatically constructed. This category taxonomy is then used in the feature extraction step in which an audio scene instance is represented by a label tree embedding image. Different convolutional neural networks, which are tailored for the task at hand, are finally learned on top of the image features for scene recognition. Our system reaches an overall recognition accuracy of 81.2% and 83.3% and outperforms the DCASE 2016 baseline with absolute improvements of 8.7% and 6.1% on the development and test data, respectively.



### Enlightening Deep Neural Networks with Knowledge of Confounding Factors
- **Arxiv ID**: http://arxiv.org/abs/1607.02397v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.02397v1)
- **Published**: 2016-07-08 15:00:11+00:00
- **Updated**: 2016-07-08 15:00:11+00:00
- **Authors**: Yu Zhong, Gil Ettinger
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning techniques have demonstrated significant capacity in modeling some of the most challenging real world problems of high complexity. Despite the popularity of deep models, we still strive to better understand the underlying mechanism that drives their success. Motivated by observations that neurons in trained deep nets predict attributes indirectly related to the training tasks, we recognize that a deep network learns representations more general than the task at hand to disentangle impacts of multiple confounding factors governing the data, in order to isolate the effects of the concerning factors and optimize a given objective. Consequently, we propose a general framework to augment training of deep models with information on auxiliary explanatory data variables, in an effort to boost this disentanglement and train deep networks that comprehend the data interactions and distributions more accurately, and thus improve their generalizability. We incorporate information on prominent auxiliary explanatory factors of the data population into existing architectures as secondary objective/loss blocks that take inputs from hidden layers during training. Once trained, these secondary circuits can be removed to leave a model with the same architecture as the original, but more generalizable and discerning thanks to its comprehension of data interactions. Since pose is one of the most dominant confounding factors for object recognition, we apply this principle to instantiate a pose-aware deep convolutional neural network and demonstrate that auxiliary pose information indeed improves the classification accuracy in our experiments on SAR target classification tasks.



### Fast Predictive Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1607.02504v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.02504v1)
- **Published**: 2016-07-08 19:58:56+00:00
- **Updated**: 2016-07-08 19:58:56+00:00
- **Authors**: Xiao Yang, Roland Kwitt, Marc Niethammer
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method to predict image deformations based on patch-wise image appearance. Specifically, we design a patch-based deep encoder-decoder network which learns the pixel/voxel-wise mapping between image appearance and registration parameters. Our approach can predict general deformation parameterizations, however, we focus on the large deformation diffeomorphic metric mapping (LDDMM) registration model. By predicting the LDDMM momentum-parameterization we retain the desirable theoretical properties of LDDMM, while reducing computation time by orders of magnitude: combined with patch pruning, we achieve a 1500x/66x speed up compared to GPU-based optimization for 2D/3D image registration. Our approach has better prediction accuracy than predicting deformation or velocity fields and results in diffeomorphic transformations. Additionally, we create a Bayesian probabilistic version of our network, which allows evaluation of deformation field uncertainty through Monte Carlo sampling using dropout at test time. We show that deformation uncertainty highlights areas of ambiguous deformations. We test our method on the OASIS brain image dataset in 2D and 3D.



### Adversarial examples in the physical world
- **Arxiv ID**: http://arxiv.org/abs/1607.02533v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1607.02533v4)
- **Published**: 2016-07-08 21:12:11+00:00
- **Updated**: 2017-02-11 00:39:39+00:00
- **Authors**: Alexey Kurakin, Ian Goodfellow, Samy Bengio
- **Comment**: 14 pages, 6 figures. Demo available at https://youtu.be/zQ_uMenoBCk
- **Journal**: None
- **Summary**: Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.



### Multi-level Contextual RNNs with Attention Model for Scene Labeling
- **Arxiv ID**: http://arxiv.org/abs/1607.02537v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.02537v2)
- **Published**: 2016-07-08 21:51:53+00:00
- **Updated**: 2016-08-10 21:15:51+00:00
- **Authors**: Heng Fan, Xue Mei, Danil Prokhorov, Haibin Ling
- **Comment**: 8 pages, 8 figures
- **Journal**: None
- **Summary**: Context in image is crucial for scene labeling while existing methods only exploit local context generated from a small surrounding area of an image patch or a pixel, by contrast long-range and global contextual information is ignored. To handle this issue, we in this work propose a novel approach for scene labeling by exploring multi-level contextual recurrent neural networks (ML-CRNNs). Specifically, we encode three kinds of contextual cues, i.e., local context, global context and image topic context in structural recurrent neural networks (RNNs) to model long-range local and global dependencies in image. In this way, our method is able to `see' the image in terms of both long-range local and holistic views, and make a more reliable inference for image labeling. Besides, we integrate the proposed contextual RNNs into hierarchical convolutional neural networks (CNNs), and exploit dependence relationships in multiple levels to provide rich spatial and semantic information. Moreover, we novelly adopt an attention model to effectively merge multiple levels and show that it outperforms average- or max-pooling fusion strategies. Extensive experiments demonstrate that the proposed approach achieves new state-of-the-art results on the CamVid, SiftFlow and Stanford-background datasets.



### Graph Construction with Label Information for Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1607.02539v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.02539v3)
- **Published**: 2016-07-08 22:24:20+00:00
- **Updated**: 2017-02-12 22:20:26+00:00
- **Authors**: Liansheng Zhuang, Zihan Zhou, Jingwen Yin, Shenghua Gao, Zhouchen Lin, Yi Ma, Nenghai Yu
- **Comment**: This paper is withdrawn by the authors for some errors
- **Journal**: None
- **Summary**: In the literature, most existing graph-based semi-supervised learning (SSL) methods only use the label information of observed samples in the label propagation stage, while ignoring such valuable information when learning the graph. In this paper, we argue that it is beneficial to consider the label information in the graph learning stage. Specifically, by enforcing the weight of edges between labeled samples of different classes to be zero, we explicitly incorporate the label information into the state-of-the-art graph learning methods, such as the Low-Rank Representation (LRR), and propose a novel semi-supervised graph learning method called Semi-Supervised Low-Rank Representation (SSLRR). This results in a convex optimization problem with linear constraints, which can be solved by the linearized alternating direction method. Though we take LRR as an example, our proposed method is in fact very general and can be applied to any self-representation graph learning methods. Experiment results on both synthetic and real datasets demonstrate that the proposed graph learning method can better capture the global geometric structure of the data, and therefore is more effective for semi-supervised learning tasks.



### Screen Content Image Segmentation Using Robust Regression and Sparse Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1607.02547v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.02547v1)
- **Published**: 2016-07-08 23:16:45+00:00
- **Updated**: 2016-07-08 23:16:45+00:00
- **Authors**: Shervin Minaee, Yao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper considers how to separate text and/or graphics from smooth background in screen content and mixed document images and proposes two approaches to perform this segmentation task. The proposed methods make use of the fact that the background in each block is usually smoothly varying and can be modeled well by a linear combination of a few smoothly varying basis functions, while the foreground text and graphics create sharp discontinuity. The algorithms separate the background and foreground pixels by trying to fit background pixel values in the block into a smooth function using two different schemes. One is based on robust regression, where the inlier pixels will be considered as background, while remaining outlier pixels will be considered foreground. The second approach uses a sparse decomposition framework where the background and foreground layers are modeled with a smooth and sparse components respectively. These algorithms have been tested on images extracted from HEVC standard test sequences for screen content coding, and are shown to have superior performance over previous approaches. The proposed methods can be used in different applications such as text extraction, separate coding of background and foreground for compression of screen content, and medical image segmentation.



