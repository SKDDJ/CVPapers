# Arxiv Papers in cs.CV on 2016-07-10
### Towards an "In-the-Wild" Emotion Dataset Using a Game-based Framework
- **Arxiv ID**: http://arxiv.org/abs/1607.02678v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.02678v1)
- **Published**: 2016-07-10 02:16:10+00:00
- **Updated**: 2016-07-10 02:16:10+00:00
- **Authors**: Wei Li, Farnaz Abtahi, Christina Tsangouri, Zhigang Zhu
- **Comment**: This paper is accepted at CVPR 2016 Workshop
- **Journal**: None
- **Summary**: In order to create an "in-the-wild" dataset of facial emotions with large number of balanced samples, this paper proposes a game-based data collection framework. The framework mainly include three components: a game engine, a game interface, and a data collection and evaluation module. We use a deep learning approach to build an emotion classifier as the game engine. Then a emotion web game to allow gamers to enjoy the games, while the data collection module obtains automatically-labelled emotion images. Using our game, we have collected more than 15,000 images within a month of the test run and built an emotion dataset "GaMo". To evaluate the dataset, we compared the performance of two deep learning models trained on both GaMo and CIFE. The results of our experiments show that because of being large and balanced, GaMo can be used to build a more robust emotion detector than the emotion detector trained on CIFE, which was used in the game engine to collect the face images.



### Learning to Sketch Human Facial Portraits using Personal Styles by Case-Based Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1607.02715v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.02715v2)
- **Published**: 2016-07-10 08:48:09+00:00
- **Updated**: 2016-09-13 15:40:04+00:00
- **Authors**: Bingwen Jin, Songhua Xu, Weidong Geng
- **Comment**: None
- **Journal**: None
- **Summary**: This paper employs case-based reasoning (CBR) to capture the personal styles of individual artists and generate the human facial portraits from photos accordingly. For each human artist to be mimicked, a series of cases are firstly built-up from her/his exemplars of source facial photo and hand-drawn sketch, and then its stylization for facial photo is transformed as a style-transferring process of iterative refinement by looking-for and applying best-fit cases in a sense of style optimization. Two models, fitness evaluation model and parameter estimation model, are learned for case retrieval and adaptation respectively from these cases. The fitness evaluation model is to decide which case is best-fitted to the sketching of current interest, and the parameter estimation model is to automate case adaptation. The resultant sketch is synthesized progressively with an iterative loop of retrieval and adaptation of candidate cases until the desired aesthetic style is achieved. To explore the effectiveness and advantages of the novel approach, we experimentally compare the sketch portraits generated by the proposed method with that of a state-of-the-art example-based facial sketch generation algorithm as well as a couple commercial software packages. The comparisons reveal that our CBR based synthesis method for facial portraits is superior both in capturing and reproducing artists' personal illustration styles to the peer methods.



### Intra-layer Nonuniform Quantization for Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1607.02720v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.02720v2)
- **Published**: 2016-07-10 09:23:34+00:00
- **Updated**: 2016-08-06 08:41:25+00:00
- **Authors**: Fangxuan Sun, Jun Lin, Zhongfeng Wang
- **Comment**: submitted to WCSP 2016
- **Journal**: None
- **Summary**: Deep convolutional neural network (DCNN) has achieved remarkable performance on object detection and speech recognition in recent years. However, the excellent performance of a DCNN incurs high computational complexity and large memory requirement. In this paper, an equal distance nonuniform quantization (ENQ) scheme and a K-means clustering nonuniform quantization (KNQ) scheme are proposed to reduce the required memory storage when low complexity hardware or software implementations are considered. For the VGG-16 and the AlexNet, the proposed nonuniform quantization schemes reduce the number of required memory storage by approximately 50\% while achieving almost the same or even better classification accuracy compared to the state-of-the-art quantization method. Compared to the ENQ scheme, the proposed KNQ scheme provides a better tradeoff when higher accuracy is required.



### Transition Forests: Learning Discriminative Temporal Transitions for Action Recognition and Detection
- **Arxiv ID**: http://arxiv.org/abs/1607.02737v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.02737v3)
- **Published**: 2016-07-10 12:05:41+00:00
- **Updated**: 2017-03-31 15:39:45+00:00
- **Authors**: Guillermo Garcia-Hernando, Tae-Kyun Kim
- **Comment**: to appear in CVPR 2017
- **Journal**: None
- **Summary**: A human action can be seen as transitions between one's body poses over time, where the transition depicts a temporal relation between two poses. Recognizing actions thus involves learning a classifier sensitive to these pose transitions as well as to static poses. In this paper, we introduce a novel method called transitions forests, an ensemble of decision trees that both learn to discriminate static poses and transitions between pairs of two independent frames. During training, node splitting is driven by alternating two criteria: the standard classification objective that maximizes the discrimination power in individual frames, and the proposed one in pairwise frame transitions. Growing the trees tends to group frames that have similar associated transitions and share same action label incorporating temporal information that was not available otherwise. Unlike conventional decision trees where the best split in a node is determined independently of other nodes, the transition forests try to find the best split of nodes jointly (within a layer) for incorporating distant node transitions. When inferring the class label of a new frame, it is passed down the trees and the prediction is made based on previous frame predictions and the current one in an efficient and online manner. We apply our method on varied skeleton action recognition and online detection datasets showing its suitability over several baselines and state-of-the-art approaches.



### Adversarial Training For Sketch Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1607.02748v2
- **DOI**: 10.1007/978-3-319-46604-0_55
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.02748v2)
- **Published**: 2016-07-10 14:15:38+00:00
- **Updated**: 2016-08-23 11:24:37+00:00
- **Authors**: Antonia Creswell, Anil Anthony Bharath
- **Comment**: Accepted to ECCV2016 VisArt Workshop
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GAN) are able to learn excellent representations for unlabelled data which can be applied to image generation and scene classification. Representations learned by GANs have not yet been applied to retrieval. In this paper, we show that the representations learned by GANs can indeed be used for retrieval. We consider heritage documents that contain unlabelled Merchant Marks, sketch-like symbols that are similar to hieroglyphs. We introduce a novel GAN architecture with design features that make it suitable for sketch retrieval. The performance of this sketch-GAN is compared to a modified version of the original GAN architecture with respect to simple invariance properties. Experiments suggest that sketch-GANs learn representations that are suitable for retrieval and which also have increased stability to rotation, scale and translation compared to the standard GAN architecture.



### Annotation Methodologies for Vision and Language Dataset Creation
- **Arxiv ID**: http://arxiv.org/abs/1607.02769v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1607.02769v1)
- **Published**: 2016-07-10 18:11:27+00:00
- **Updated**: 2016-07-10 18:11:27+00:00
- **Authors**: Gitit Kehat, James Pustejovsky
- **Comment**: in Scene Understanding Workshop (SUNw) in CVPR 2016
- **Journal**: None
- **Summary**: Annotated datasets are commonly used in the training and evaluation of tasks involving natural language and vision (image description generation, action recognition and visual question answering). However, many of the existing datasets reflect problems that emerge in the process of data selection and annotation. Here we point out some of the difficulties and problems one confronts when creating and validating annotated vision and language datasets.



