# Arxiv Papers in cs.CV on 2016-07-21
### Local Multiple Directional Pattern of Palmprint Image
- **Arxiv ID**: http://arxiv.org/abs/1607.06166v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.06166v1)
- **Published**: 2016-07-21 01:27:27+00:00
- **Updated**: 2016-07-21 01:27:27+00:00
- **Authors**: Lunke Fei, Jie Wen, Zheng Zhang, Ke Yan, Zuofeng Zhong
- **Comment**: Accepted by ICPR 2016
- **Journal**: None
- **Summary**: Lines are the most essential and discriminative features of palmprint images, which motivate researches to propose various line direction based methods for palmprint recognition. Conventional methods usually capture the only one of the most dominant direction of palmprint images. However, a number of points in palmprint images have double or even more than two dominant directions because of a plenty of crossing lines of palmprint images. In this paper, we propose a local multiple directional pattern (LMDP) to effectively characterize the multiple direction features of palmprint images. LMDP can not only exactly denote the number and positions of dominant directions but also effectively reflect the confidence of each dominant direction. Then, a simple and effective coding scheme is designed to represent the LMDP and a block-wise LMDP descriptor is used as the feature space of palmprint images in palmprint recognition. Extensive experimental results demonstrate the superiority of the LMDP over the conventional powerful descriptors and the state-of-the-art direction based methods in palmprint recognition.



### Feature Descriptors for Tracking by Detection: a Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1607.06178v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.06178v1)
- **Published**: 2016-07-21 03:06:43+00:00
- **Updated**: 2016-07-21 03:06:43+00:00
- **Authors**: Alessandro Pieropan, Mårten Björkman, Niklas Bergström, Danica Kragic
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we provide an extensive evaluation of the performance of local descriptors for tracking applications. Many different descriptors have been proposed in the literature for a wide range of application in computer vision such as object recognition and 3D reconstruction. More recently, due to fast key-point detectors, local image features can be used in online tracking frameworks. However, while much effort has been spent on evaluating their performance in terms of distinctiveness and robustness to image transformations, very little has been done in the contest of tracking. Our evaluation is performed in terms of distinctiveness, tracking precision and tracking speed. Our results show that binary descriptors like ORB or BRISK have comparable results to SIFT or AKAZE due to a higher number of key-points.



### Detection of surface defects on ceramic tiles based on morphological techniques
- **Arxiv ID**: http://arxiv.org/abs/1607.06676v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.06676v1)
- **Published**: 2016-07-21 08:25:41+00:00
- **Updated**: 2016-07-21 08:25:41+00:00
- **Authors**: Grasha Jacob, R. Shenbagavalli, S. Karthika
- **Comment**: 9 pages, 11 figures
- **Journal**: None
- **Summary**: Ceramic tiles have become very popular and are used in the flooring of offices and shopping malls. As testing the quality of tiles manually in a highly polluted environment in the manufacturing industry is a labor-intensive and time consuming process, analysis is carried out on the tile images. This paper discusses an automated system to detect the defects on the surface of ceramic tiles based on dilation, erosion, SMEE and boundary detection techniques.



### Haze Visibility Enhancement: A Survey and Quantitative Benchmarking
- **Arxiv ID**: http://arxiv.org/abs/1607.06235v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.06235v1)
- **Published**: 2016-07-21 08:57:13+00:00
- **Updated**: 2016-07-21 08:57:13+00:00
- **Authors**: Yu Li, Shaodi You, Michael S. Brown, Robby T. Tan
- **Comment**: None
- **Journal**: None
- **Summary**: This paper provides a comprehensive survey of methods dealing with visibility enhancement of images taken in hazy or foggy scenes. The survey begins with discussing the optical models of atmospheric scattering media and image formation. This is followed by a survey of existing methods, which are grouped to multiple image methods, polarizing filters based methods, methods with known depth, and single-image methods. We also provide a benchmark of a number of well known single-image methods, based on a recent dataset provided by Fattal and our newly generated scattering media dataset that contains ground truth images for quantitative evaluation. To our knowledge, this is the first benchmark using numerical metrics to evaluate dehazing techniques. This benchmark allows us to objectively compare the results of existing methods and to better identify the strengths and limitations of each method.



### Dynamic Pose-Robust Facial Expression Recognition by Multi-View Pairwise Conditional Random Forests
- **Arxiv ID**: http://arxiv.org/abs/1607.06250v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.06250v1)
- **Published**: 2016-07-21 10:07:33+00:00
- **Updated**: 2016-07-21 10:07:33+00:00
- **Authors**: Arnaud Dapogny, Kévin Bailly, Séverine Dubuisson
- **Comment**: Extension of an ICCV 2015 paper
- **Journal**: None
- **Summary**: Automatic facial expression classification (FER) from videos is a critical problem for the development of intelligent human-computer interaction systems. Still, it is a challenging problem that involves capturing high-dimensional spatio-temporal patterns describing the variation of one's appearance over time. Such representation undergoes great variability of the facial morphology and environmental factors as well as head pose variations. In this paper, we use Conditional Random Forests to capture low-level expression transition patterns. More specifically, heterogeneous derivative features (e.g. feature point movements or texture variations) are evaluated upon pairs of images. When testing on a video frame, pairs are created between this current frame and previous ones and predictions for each previous frame are used to draw trees from Pairwise Conditional Random Forests (PCRF) whose pairwise outputs are averaged over time to produce robust estimates. Moreover, PCRF collections can also be conditioned on head pose estimation for multi-view dynamic FER. As such, our approach appears as a natural extension of Random Forests for learning spatio-temporal patterns, potentially from multiple viewpoints. Experiments on popular datasets show that our method leads to significant improvements over standard Random Forests as well as state-of-the-art approaches on several scenarios, including a novel multi-view video corpus generated from a publicly available database.



### Left/Right Hand Segmentation in Egocentric Videos
- **Arxiv ID**: http://arxiv.org/abs/1607.06264v1
- **DOI**: 10.1016/j.cviu.2016.09.005
- **Categories**: **cs.HC**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1607.06264v1)
- **Published**: 2016-07-21 11:06:05+00:00
- **Updated**: 2016-07-21 11:06:05+00:00
- **Authors**: Alejandro Betancourt, Pietro Morerio, Emilia Barakova, Lucio Marcenaro, Matthias Rauterberg, Carlo Regazzoni
- **Comment**: None
- **Journal**: None
- **Summary**: Wearable cameras allow people to record their daily activities from a user-centered (First Person Vision) perspective. Due to their favorable location, wearable cameras frequently capture the hands of the user, and may thus represent a promising user-machine interaction tool for different applications. Existent First Person Vision methods handle hand segmentation as a background-foreground problem, ignoring two important facts: i) hands are not a single "skin-like" moving element, but a pair of interacting cooperative entities, ii) close hand interactions may lead to hand-to-hand occlusions and, as a consequence, create a single hand-like segment. These facts complicate a proper understanding of hand movements and interactions. Our approach extends traditional background-foreground strategies, by including a hand-identification step (left-right) based on a Maxwell distribution of angle and position. Hand-to-hand occlusions are addressed by exploiting temporal superpixels. The experimental results show that, in addition to a reliable left/right hand-segmentation, our approach considerably improves the traditional background-foreground hand-segmentation.



### Real-Time Intensity-Image Reconstruction for Event Cameras Using Manifold Regularisation
- **Arxiv ID**: http://arxiv.org/abs/1607.06283v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.06283v2)
- **Published**: 2016-07-21 11:55:31+00:00
- **Updated**: 2016-08-04 14:11:45+00:00
- **Authors**: Christian Reinbacher, Gottfried Graber, Thomas Pock
- **Comment**: Accepted to BMVC 2016 as oral presentation, 12 pages
- **Journal**: None
- **Summary**: Event cameras or neuromorphic cameras mimic the human perception system as they measure the per-pixel intensity change rather than the actual intensity level. In contrast to traditional cameras, such cameras capture new information about the scene at MHz frequency in the form of sparse events. The high temporal resolution comes at the cost of losing the familiar per-pixel intensity information. In this work we propose a variational model that accurately models the behaviour of event cameras, enabling reconstruction of intensity images with arbitrary frame rate in real-time. Our method is formulated on a per-event-basis, where we explicitly incorporate information about the asynchronous nature of events via an event manifold induced by the relative timestamps of events. In our experiments we verify that solving the variational model on the manifold produces high-quality images without explicitly estimating optical flow.



### Confidence-Weighted Local Expression Predictions for Occlusion Handling in Expression Recognition and Action Unit detection
- **Arxiv ID**: http://arxiv.org/abs/1607.06290v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.06290v1)
- **Published**: 2016-07-21 12:17:34+00:00
- **Updated**: 2016-07-21 12:17:34+00:00
- **Authors**: Arnaud Dapogny, Kévin Bailly, Séverine Dubuisson
- **Comment**: None
- **Journal**: None
- **Summary**: Fully-Automatic Facial Expression Recognition (FER) from still images is a challenging task as it involves handling large interpersonal morphological differences, and as partial occlusions can occasionally happen. Furthermore, labelling expressions is a time-consuming process that is prone to subjectivity, thus the variability may not be fully covered by the training data. In this work, we propose to train Random Forests upon spatially defined local subspaces of the face. The output local predictions form a categorical expression-driven high-level representation that we call Local Expression Predictions (LEPs). LEPs can be combined to describe categorical facial expressions as well as Action Units (AUs). Furthermore, LEPs can be weighted by confidence scores provided by an autoencoder network. Such network is trained to locally capture the manifold of the non-occluded training data in a hierarchical way. Extensive experiments show that the proposed LEP representation yields high descriptive power for categorical expressions and AU occurrence prediction, and leads to interesting perspectives towards the design of occlusion-robust and confidence-aware FER systems.



### A Multi-cut Formulation for Joint Segmentation and Tracking of Multiple Objects
- **Arxiv ID**: http://arxiv.org/abs/1607.06317v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.06317v1)
- **Published**: 2016-07-21 13:41:32+00:00
- **Updated**: 2016-07-21 13:41:32+00:00
- **Authors**: Margret Keuper, Siyu Tang, Yu Zhongjie, Bjoern Andres, Thomas Brox, Bernt Schiele
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Minimum Cost Multicut Formulations have been proposed and proven to be successful in both motion trajectory segmentation and multi-target tracking scenarios. Both tasks benefit from decomposing a graphical model into an optimal number of connected components based on attractive and repulsive pairwise terms. The two tasks are formulated on different levels of granularity and, accordingly, leverage mostly local information for motion segmentation and mostly high-level information for multi-target tracking. In this paper we argue that point trajectories and their local relationships can contribute to the high-level task of multi-target tracking and also argue that high-level cues from object detection and tracking are helpful to solve motion segmentation. We propose a joint graphical model for point trajectories and object detections whose Multicuts are solutions to motion segmentation {\it and} multi-target tracking problems at once. Results on the FBMS59 motion segmentation benchmark as well as on pedestrian tracking sequences from the 2D MOT 2015 benchmark demonstrate the promise of this joint approach.



### Fast Robust Monocular Depth Estimation for Obstacle Detection with Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1607.06349v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1607.06349v1)
- **Published**: 2016-07-21 14:49:07+00:00
- **Updated**: 2016-07-21 14:49:07+00:00
- **Authors**: Michele Mancini, Gabriele Costante, Paolo Valigi, Thomas A. Ciarfuglia
- **Comment**: Accepted for publication in the Proceedings of the 2016 IEEE/RSJ
  International Conference on Intelligent Robots and Systems (IROS 2016)
- **Journal**: None
- **Summary**: Obstacle Detection is a central problem for any robotic system, and critical for autonomous systems that travel at high speeds in unpredictable environment. This is often achieved through scene depth estimation, by various means. When fast motion is considered, the detection range must be longer enough to allow for safe avoidance and path planning. Current solutions often make assumption on the motion of the vehicle that limit their applicability, or work at very limited ranges due to intrinsic constraints. We propose a novel appearance-based Object Detection system that is able to detect obstacles at very long range and at a very high speed (~300Hz), without making assumptions on the type of motion. We achieve these results using a Deep Neural Network approach trained on real and synthetic images and trading some depth accuracy for fast, robust and consistent operation. We show how photo-realistic synthetic images are able to solve the problem of training set dimension and variety typical of machine learning approaches, and how our system is robust to massive blurring of test images.



### Reasoning about Body-Parts Relations for Sign Language Recognition
- **Arxiv ID**: http://arxiv.org/abs/1607.06356v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.06356v1)
- **Published**: 2016-07-21 15:10:41+00:00
- **Updated**: 2016-07-21 15:10:41+00:00
- **Authors**: Marc Martínez-Camarena, Jose Oramas, Mario Montagud-Climent, Tinne Tuytelaars
- **Comment**: Under Review ( 15 Pages: 13 Figures, 6 Tables )
- **Journal**: None
- **Summary**: Over the years, hand gesture recognition has been mostly addressed considering hand trajectories in isolation. However, in most sign languages, hand gestures are defined on a particular context (body region). We propose a pipeline to perform sign language recognition which models hand movements in the context of other parts of the body captured in the 3D space using the MS Kinect sensor. In addition, we perform sign recognition based on the different hand postures that occur during a sign. Our experiments show that considering different body parts brings improved performance when compared to other methods which only consider global hand trajectories. Finally, we demonstrate that the combination of hand postures features with hand gestures features helps to improve the prediction of a given sign.



### Small-Variance Nonparametric Clustering on the Hypersphere
- **Arxiv ID**: http://arxiv.org/abs/1607.06407v1
- **DOI**: 10.1109/CVPR.2015.7298630
- **Categories**: **cs.CV**, math.ST, stat.AP, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/1607.06407v1)
- **Published**: 2016-07-21 17:52:08+00:00
- **Updated**: 2016-07-21 17:52:08+00:00
- **Authors**: Julian Straub, Trevor Campbell, Jonathan P. How, John W. Fisher III
- **Comment**: None
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
  (pp. 334-342). (2015)
- **Summary**: Structural regularities in man-made environments reflect in the distribution of their surface normals. Describing these surface normal distributions is important in many computer vision applications, such as scene understanding, plane segmentation, and regularization of 3D reconstructions. Based on the small-variance limit of Bayesian nonparametric von-Mises-Fisher (vMF) mixture distributions, we propose two new flexible and efficient k-means-like clustering algorithms for directional data such as surface normals. The first, DP-vMF-means, is a batch clustering algorithm derived from the Dirichlet process (DP) vMF mixture. Recognizing the sequential nature of data collection in many applications, we extend this algorithm to DDP-vMF-means, which infers temporally evolving cluster structure from streaming data. Both algorithms naturally respect the geometry of directional data, which lies on the unit sphere. We demonstrate their performance on synthetic directional data and real 3D surface normals from RGB-D sensors. While our experiments focus on 3D data, both algorithms generalize to high dimensional directional data such as protein backbone configurations and semantic word vectors.



### Multi-Camera Action Dataset for Cross-Camera Action Recognition Benchmarking
- **Arxiv ID**: http://arxiv.org/abs/1607.06408v3
- **DOI**: 10.1109/WACV.2017.28
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.06408v3)
- **Published**: 2016-07-21 17:58:19+00:00
- **Updated**: 2017-05-05 05:21:31+00:00
- **Authors**: Wenhui Li, Yongkang Wong, An-An Liu, Yang Li, Yu-Ting Su, Mohan Kankanhalli
- **Comment**: None
- **Journal**: None
- **Summary**: Action recognition has received increasing attention from the computer vision and machine learning communities in the last decade. To enable the study of this problem, there exist a vast number of action datasets, which are recorded under controlled laboratory settings, real-world surveillance environments, or crawled from the Internet. Apart from the "in-the-wild" datasets, the training and test split of conventional datasets often possess similar environments conditions, which leads to close to perfect performance on constrained datasets. In this paper, we introduce a new dataset, namely Multi-Camera Action Dataset (MCAD), which is designed to evaluate the open view classification problem under the surveillance environment. In total, MCAD contains 14,298 action samples from 18 action categories, which are performed by 20 subjects and independently recorded with 5 cameras. Inspired by the well received evaluation approach on the LFW dataset, we designed a standard evaluation protocol and benchmarked MCAD under several scenarios. The benchmark shows that while an average of 85% accuracy is achieved under the closed-view scenario, the performance suffers from a significant drop under the cross-view scenario. In the worst case scenario, the performance of 10-fold cross validation drops from 87.0% to 47.4%.



### Hierarchical Attention Network for Action Recognition in Videos
- **Arxiv ID**: http://arxiv.org/abs/1607.06416v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.06416v1)
- **Published**: 2016-07-21 18:16:39+00:00
- **Updated**: 2016-07-21 18:16:39+00:00
- **Authors**: Yilin Wang, Suhang Wang, Jiliang Tang, Neil O'Hare, Yi Chang, Baoxin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding human actions in wild videos is an important task with a broad range of applications. In this paper we propose a novel approach named Hierarchical Attention Network (HAN), which enables to incorporate static spatial information, short-term motion information and long-term video temporal structures for complex human action understanding. Compared to recent convolutional neural network based approaches, HAN has following advantages (1) HAN can efficiently capture video temporal structures in a longer range; (2) HAN is able to reveal temporal transitions between frame chunks with different time steps, i.e. it explicitly models the temporal transitions between frames as well as video segments and (3) with a multiple step spatial temporal attention mechanism, HAN automatically learns important regions in video frames and temporal segments in the video. The proposed model is trained and evaluated on the standard video action benchmarks, i.e., UCF-101 and HMDB-51, and it significantly outperforms the state-of-the arts



### Geometric Neural Phrase Pooling: Modeling the Spatial Co-occurrence of Neurons
- **Arxiv ID**: http://arxiv.org/abs/1607.06514v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.06514v1)
- **Published**: 2016-07-21 21:51:58+00:00
- **Updated**: 2016-07-21 21:51:58+00:00
- **Authors**: Lingxi Xie, Qi Tian, John Flynn, Jingdong Wang, Alan Yuille
- **Comment**: To appear, in ECCV 2016 (18 pages, 4 figures)
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (CNNs) are playing important roles in state-of-the-art visual recognition. This paper focuses on modeling the spatial co-occurrence of neuron responses, which is less studied in the previous work. For this, we consider the neurons in the hidden layer as neural words, and construct a set of geometric neural phrases on top of them. The idea that grouping neural words into neural phrases is borrowed from the Bag-of-Visual-Words (BoVW) model. Next, the Geometric Neural Phrase Pooling (GNPP) algorithm is proposed to efficiently encode these neural phrases. GNPP acts as a new type of hidden layer, which punishes the isolated neuron responses after convolution, and can be inserted into a CNN model with little extra computational overhead. Experimental results show that GNPP produces significant and consistent accuracy gain in image classification.



