# Arxiv Papers in cs.CV on 2016-07-26
### Salient Object Subitizing
- **Arxiv ID**: http://arxiv.org/abs/1607.07525v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.07525v1)
- **Published**: 2016-07-26 02:26:01+00:00
- **Updated**: 2016-07-26 02:26:01+00:00
- **Authors**: Jianming Zhang, Shugao Ma, Mehrnoosh Sameki, Stan Sclaroff, Margrit Betke, Zhe Lin, Xiaohui Shen, Brian Price, Radomir Mech
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of Salient Object Subitizing, i.e. predicting the existence and the number of salient objects in an image using holistic cues. This task is inspired by the ability of people to quickly and accurately identify the number of items within the subitizing range (1-4). To this end, we present a salient object subitizing image dataset of about 14K everyday images which are annotated using an online crowdsourcing marketplace. We show that using an end-to-end trained Convolutional Neural Network (CNN) model, we achieve prediction accuracy comparable to human performance in identifying images with zero or one salient object. For images with multiple salient objects, our model also provides significantly better than chance performance without requiring any localization process. Moreover, we propose a method to improve the training of the CNN subitizing model by leveraging synthetic images. In experiments, we demonstrate the accuracy and generalizability of our CNN subitizing model and its applications in salient object detection and image retrieval.



### Dynamic Probabilistic Network Based Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1610.06395v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1610.06395v1)
- **Published**: 2016-07-26 03:19:58+00:00
- **Updated**: 2016-07-26 03:19:58+00:00
- **Authors**: Anne Veenendaal, Eddie Jones, Zhao Gang, Elliot Daly, Sumalini Vartak, Rahul Patwardhan
- **Comment**: 7 pages, 4 figures
- **Journal**: None
- **Summary**: This paper examines use of dynamic probabilistic networks (DPN) for human action recognition. The actions of lifting objects and walking in the room, sitting in the room and neutral standing pose were used for testing the classification. The research used the dynamic interrelation between various different regions of interest (ROI) on the human body (face, body, arms, legs) and the time series based events related to the these ROIs. This dynamic links are then used to recognize the human behavioral aspects in the scene. First a model is developed to identify the human activities in an indoor scene and this model is dependent on the key features and interlinks between the various dynamic events using DPNs. The sub ROI are classified with DPN to associate the combined interlink with a specific human activity. The recognition accuracy performance between indoor (controlled lighting conditions) is compared with the outdoor lighting conditions. The accuracy in outdoor scenes was lower than the controlled environment.



### Semantic Image Inpainting with Deep Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1607.07539v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.07539v3)
- **Published**: 2016-07-26 04:52:48+00:00
- **Updated**: 2017-07-13 16:29:21+00:00
- **Authors**: Raymond A. Yeh, Chen Chen, Teck Yian Lim, Alexander G. Schwing, Mark Hasegawa-Johnson, Minh N. Do
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic image inpainting is a challenging task where large missing regions have to be filled based on the available visual data. Existing methods which extract information from only a single image generally produce unsatisfactory results due to the lack of high level context. In this paper, we propose a novel method for semantic image inpainting, which generates the missing content by conditioning on the available data. Given a trained generative model, we search for the closest encoding of the corrupted image in the latent image manifold using our context and prior losses. This encoding is then passed through the generative model to infer the missing content. In our method, inference is possible irrespective of how the missing content is structured, while the state-of-the-art learning based method requires specific information about the holes in the training phase. Experiments on three datasets show that our method successfully predicts information in large missing regions and achieves pixel-level photorealism, significantly outperforming the state-of-the-art methods.



### Generic 3D Convolutional Fusion for image restoration
- **Arxiv ID**: http://arxiv.org/abs/1607.07561v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.07561v1)
- **Published**: 2016-07-26 07:02:55+00:00
- **Updated**: 2016-07-26 07:02:55+00:00
- **Authors**: Jiqing Wu, Radu Timofte, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: Also recently, exciting strides forward have been made in the area of image restoration, particularly for image denoising and single image super-resolution. Deep learning techniques contributed to this significantly. The top methods differ in their formulations and assumptions, so even if their average performance may be similar, some work better on certain image types and image regions than others. This complementarity motivated us to propose a novel 3D convolutional fusion (3DCF) method. Unlike other methods adapted to different tasks, our method uses the exact same convolutional network architecture to address both image denois- ing and single image super-resolution. As a result, our 3DCF method achieves substantial improvements (0.1dB-0.4dB PSNR) over the state-of-the-art methods that it fuses, and this on standard benchmarks for both tasks. At the same time, the method still is computationally efficient.



### Generic Feature Learning for Wireless Capsule Endoscopy Analysis
- **Arxiv ID**: http://arxiv.org/abs/1607.07604v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.07604v1)
- **Published**: 2016-07-26 09:21:22+00:00
- **Updated**: 2016-07-26 09:21:22+00:00
- **Authors**: Santi Seguí, Michal Drozdzal, Guillem Pascual, Petia Radeva, Carolina Malagelada, Fernando Azpiroz, Jordi Vitrià
- **Comment**: None
- **Journal**: None
- **Summary**: The interpretation and analysis of the wireless capsule endoscopy recording is a complex task which requires sophisticated computer aided decision (CAD) systems in order to help physicians with the video screening and, finally, with the diagnosis. Most of the CAD systems in the capsule endoscopy share a common system design, but use very different image and video representations. As a result, each time a new clinical application of WCE appears, new CAD system has to be designed from scratch. This characteristic makes the design of new CAD systems a very time consuming. Therefore, in this paper we introduce a system for small intestine motility characterization, based on Deep Convolutional Neural Networks, which avoids the laborious step of designing specific features for individual motility events. Experimental results show the superiority of the learned features over alternative classifiers constructed by using state of the art hand-crafted features. In particular, it reaches a mean classification accuracy of 96% for six intestinal motility events, outperforming the other classifiers by a large margin (a 14% relative performance increase).



### Semantic Clustering for Robust Fine-Grained Scene Recognition
- **Arxiv ID**: http://arxiv.org/abs/1607.07614v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.07614v1)
- **Published**: 2016-07-26 09:46:48+00:00
- **Updated**: 2016-07-26 09:46:48+00:00
- **Authors**: Marian George, Mandar Dixit, Gábor Zogg, Nuno Vasconcelos
- **Comment**: Accepted at the European Conference on Computer Vision (ECCV), 2016
- **Journal**: None
- **Summary**: In domain generalization, the knowledge learnt from one or multiple source domains is transferred to an unseen target domain. In this work, we propose a novel domain generalization approach for fine-grained scene recognition. We first propose a semantic scene descriptor that jointly captures the subtle differences between fine-grained scenes, while being robust to varying object configurations across domains. We model the occurrence patterns of objects in scenes, capturing the informativeness and discriminability of each object for each scene. We then transform such occurrences into scene probabilities for each scene image. Second, we argue that scene images belong to hidden semantic topics that can be discovered by clustering our semantic descriptors. To evaluate the proposed method, we propose a new fine-grained scene dataset in cross-domain settings. Extensive experiments on the proposed dataset and three benchmark scene datasets show the effectiveness of the proposed approach for fine-grained scene transfer, where we outperform state-of-the-art scene recognition and domain generalization methods.



### Scale Invariant Interest Points with Shearlets
- **Arxiv ID**: http://arxiv.org/abs/1607.07639v1
- **DOI**: 10.1109/TIP.2017.2687122
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.07639v1)
- **Published**: 2016-07-26 11:03:20+00:00
- **Updated**: 2016-07-26 11:03:20+00:00
- **Authors**: Miguel A. Duval-Poo, Nicoletta Noceti, Francesca Odone, Ernesto De Vito
- **Comment**: None
- **Journal**: None
- **Summary**: Shearlets are a relatively new directional multi-scale framework for signal analysis, which have been shown effective to enhance signal discontinuities such as edges and corners at multiple scales. In this work we address the problem of detecting and describing blob-like features in the shearlets framework. We derive a measure which is very effective for blob detection and closely related to the Laplacian of Gaussian. We demonstrate the measure satisfies the perfect scale invariance property in the continuous case. In the discrete setting, we derive algorithms for blob detection and keypoint description. Finally, we provide qualitative justifications of our findings as well as a quantitative evaluation on benchmark data. We also report an experimental evidence that our method is very suitable to deal with compressed and noisy images, thanks to the sparsity property of shearlets.



### Emotion-Based Crowd Representation for Abnormality Detection
- **Arxiv ID**: http://arxiv.org/abs/1607.07646v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.07646v1)
- **Published**: 2016-07-26 11:26:44+00:00
- **Updated**: 2016-07-26 11:26:44+00:00
- **Authors**: Hamidreza Rabiee, Javad Haddadnia, Hossein Mousavi, Moin Nabi, Vittorio Murino, Nicu Sebe
- **Comment**: None
- **Journal**: None
- **Summary**: In crowd behavior understanding, a model of crowd behavior need to be trained using the information extracted from video sequences. Since there is no ground-truth available in crowd datasets except the crowd behavior labels, most of the methods proposed so far are just based on low-level visual features. However, there is a huge semantic gap between low-level motion/appearance features and high-level concept of crowd behaviors. In this paper we propose an attribute-based strategy to alleviate this problem. While similar strategies have been recently adopted for object and action recognition, as far as we know, we are the first showing that the crowd emotions can be used as attributes for crowd behavior understanding. The main idea is to train a set of emotion-based classifiers, which can subsequently be used to represent the crowd motion. For this purpose, we collect a big dataset of video clips and provide them with both annotations of "crowd behaviors" and "crowd emotions". We show the results of the proposed method on our dataset, which demonstrate that the crowd emotions enable the construction of more descriptive models for crowd behaviors. We aim at publishing the dataset with the article, to be used as a benchmark for the communities.



### Fundamental Matrices from Moving Objects Using Line Motion Barcodes
- **Arxiv ID**: http://arxiv.org/abs/1607.07660v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.07660v1)
- **Published**: 2016-07-26 12:16:51+00:00
- **Updated**: 2016-07-26 12:16:51+00:00
- **Authors**: Yoni Kasten, Gil Ben-Artzi, Shmuel Peleg, Michael Werman
- **Comment**: None
- **Journal**: ECCV'16, Amsterdam, Oct. 2016, Vol II, pp. 220-118
- **Summary**: Computing the epipolar geometry between cameras with very different viewpoints is often very difficult. The appearance of objects can vary greatly, and it is difficult to find corresponding feature points. Prior methods searched for corresponding epipolar lines using points on the convex hull of the silhouette of a single moving object. These methods fail when the scene includes multiple moving objects. This paper extends previous work to scenes having multiple moving objects by using the "Motion Barcodes", a temporal signature of lines. Corresponding epipolar lines have similar motion barcodes, and candidate pairs of corresponding epipoar lines are found by the similarity of their motion barcodes. As in previous methods we assume that cameras are relatively stationary and that moving objects have already been extracted using background subtraction.



### Region-based semantic segmentation with end-to-end training
- **Arxiv ID**: http://arxiv.org/abs/1607.07671v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.07671v1)
- **Published**: 2016-07-26 12:46:51+00:00
- **Updated**: 2016-07-26 12:46:51+00:00
- **Authors**: Holger Caesar, Jasper Uijlings, Vittorio Ferrari
- **Comment**: ECCV 2016 camera-ready
- **Journal**: None
- **Summary**: We propose a novel method for semantic segmentation, the task of labeling each pixel in an image with a semantic class. Our method combines the advantages of the two main competing paradigms. Methods based on region classification offer proper spatial support for appearance measurements, but typically operate in two separate stages, none of which targets pixel labeling performance at the end of the pipeline. More recent fully convolutional methods are capable of end-to-end training for the final pixel labeling, but resort to fixed patches as spatial support. We show how to modify modern region-based approaches to enable end-to-end training for semantic segmentation. This is achieved via a differentiable region-to-pixel layer and a differentiable free-form Region-of-Interest pooling layer. Our method improves the state-of-the-art in terms of class-average accuracy with 64.0% on SIFT Flow and 49.9% on PASCAL Context, and is particularly accurate at object boundaries.



### End-to-End Image Super-Resolution via Deep and Shallow Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1607.07680v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.07680v1)
- **Published**: 2016-07-26 13:15:53+00:00
- **Updated**: 2016-07-26 13:15:53+00:00
- **Authors**: Yifan Wang, Lijun Wang, Hongyu Wang, Peihua Li
- **Comment**: None
- **Journal**: None
- **Summary**: One impressive advantage of convolutional neural networks (CNNs) is their ability to automatically learn feature representation from raw pixels, eliminating the need for hand-designed procedures. However, recent methods for single image super-resolution (SR) fail to maintain this advantage. They utilize CNNs in two decoupled steps, i.e., first upsampling the low resolution (LR) image to the high resolution (HR) size with hand-designed techniques (e.g., bicubic interpolation), and then applying CNNs on the upsampled LR image to reconstruct HR results. In this paper, we seek an alternative and propose a new image SR method, which jointly learns the feature extraction, upsampling and HR reconstruction modules, yielding a completely end-to-end trainable deep CNN. As opposed to existing approaches, the proposed method conducts upsampling in the latent feature space with filters that are optimized for the task of image SR. In addition, the HR reconstruction is performed in a multi-scale manner to simultaneously incorporate both short- and long-range contextual information, ensuring more accurate restoration of HR images. To facilitate network training, a new training approach is designed, which jointly trains the proposed deep network with a relatively shallow network, leading to faster convergence and more superior performance. The proposed method is extensively evaluated on widely adopted data sets and improves the performance of state-of-the-art methods with a considerable margin. Moreover, in-depth ablation studies are conducted to verify the contribution of different network designs to image SR, providing additional insights for future research.



### Joint Optical Flow and Temporally Consistent Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1607.07716v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.07716v1)
- **Published**: 2016-07-26 14:25:37+00:00
- **Updated**: 2016-07-26 14:25:37+00:00
- **Authors**: Junhwa Hur, Stefan Roth
- **Comment**: 14 pages, Accepted for CVRSUAD workshop at ECCV 2016
- **Journal**: None
- **Summary**: The importance and demands of visual scene understanding have been steadily increasing along with the active development of autonomous systems. Consequently, there has been a large amount of research dedicated to semantic segmentation and dense motion estimation. In this paper, we propose a method for jointly estimating optical flow and temporally consistent semantic segmentation, which closely connects these two problem domains and leverages each other. Semantic segmentation provides information on plausible physical motion to its associated pixels, and accurate pixel-level temporal correspondences enhance the accuracy of semantic segmentation in the temporal domain. We demonstrate the benefits of our approach on the KITTI benchmark, where we observe performance gains for flow and segmentation. We achieve state-of-the-art optical flow results, and outperform all published algorithms by a large margin on challenging, but crucial dynamic objects.



### Approximate Policy Iteration for Budgeted Semantic Video Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1607.07770v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.07770v1)
- **Published**: 2016-07-26 15:58:32+00:00
- **Updated**: 2016-07-26 15:58:32+00:00
- **Authors**: Behrooz Mahasseni, Sinisa Todorovic, Alan Fern
- **Comment**: None
- **Journal**: None
- **Summary**: This paper formulates and presents a solution to the new problem of budgeted semantic video segmentation. Given a video, the goal is to accurately assign a semantic class label to every pixel in the video within a specified time budget. Typical approaches to such labeling problems, such as Conditional Random Fields (CRFs), focus on maximizing accuracy but do not provide a principled method for satisfying a time budget. For video data, the time required by CRF and related methods is often dominated by the time to compute low-level descriptors of supervoxels across the video. Our key contribution is the new budgeted inference framework for CRF models that intelligently selects the most useful subsets of descriptors to run on subsets of supervoxels within the time budget. The objective is to maintain an accuracy as close as possible to the CRF model with no time bound, while remaining within the time budget. Our second contribution is the algorithm for learning a policy for the sparse selection of supervoxels and their descriptors for budgeted CRF inference. This learning algorithm is derived by casting our problem in the framework of Markov Decision Processes, and then instantiating a state-of-the-art policy learning algorithm known as Classification-Based Approximate Policy Iteration. Our experiments on multiple video datasets show that our learning approach and framework is able to significantly reduce computation time, and maintain competitive accuracy under varying budgets.



