# Arxiv Papers in cs.CV on 2016-07-18
### Composite Kernel Local Angular Discriminant Analysis for Multi-Sensor Geospatial Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1607.04939v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.04939v1)
- **Published**: 2016-07-18 02:50:40+00:00
- **Updated**: 2016-07-18 02:50:40+00:00
- **Authors**: Saurabh Prasad, Minshan Cui, Lifeng Yan
- **Comment**: None
- **Journal**: None
- **Summary**: With the emergence of passive and active optical sensors available for geospatial imaging, information fusion across sensors is becoming ever more important. An important aspect of single (or multiple) sensor geospatial image analysis is feature extraction - the process of finding "optimal" lower dimensional subspaces that adequately characterize class-specific information for subsequent analysis tasks, such as classification, change and anomaly detection etc. In recent work, we proposed and developed an angle-based discriminant analysis approach that projected data onto subspaces with maximal "angular" separability in the input (raw) feature space and Reproducible Kernel Hilbert Space (RKHS). We also developed an angular locality preserving variant of this algorithm. In this letter, we advance this work and make it suitable for information fusion - we propose and validate a composite kernel local angular discriminant analysis projection, that can operate on an ensemble of feature sources (e.g. from different sources), and project the data onto a unified space through composite kernels where the data are maximally separated in an angular sense. We validate this method with the multi-sensor University of Houston hyperspectral and LiDAR dataset, and demonstrate that the proposed method significantly outperforms other composite kernel approaches to sensor (information) fusion.



### Sparse Representation-Based Classification: Orthogonal Least Squares or Orthogonal Matching Pursuit?
- **Arxiv ID**: http://arxiv.org/abs/1607.04942v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.04942v1)
- **Published**: 2016-07-18 03:05:07+00:00
- **Updated**: 2016-07-18 03:05:07+00:00
- **Authors**: Minshan Cui, Saurabh Prasad
- **Comment**: None
- **Journal**: None
- **Summary**: Spare representation of signals has received significant attention in recent years. Based on these developments, a sparse representation-based classification (SRC) has been proposed for a variety of classification and related tasks, including face recognition. Recently, a class dependent variant of SRC was proposed to overcome the limitations of SRC for remote sensing image classification. Traditionally, greedy pursuit based method such as orthogonal matching pursuit (OMP) are used for sparse coefficient recovery due to their simplicity as well as low time-complexity. However, orthogonal least square (OLS) has not yet been widely used in classifiers that exploit the sparse representation properties of data. Since OLS produces lower signal reconstruction error than OMP under similar conditions, we hypothesize that more accurate signal estimation will further improve the classification performance of classifiers that exploiting the sparsity of data. In this paper, we present a classification method based on OLS, which implements OLS in a classwise manner to perform the classification. We also develop and present its kernelized variant to handle nonlinearly separable data. Based on two real-world benchmarking hyperspectral datasets, we demonstrate that class dependent OLS based methods outperform several baseline methods including traditional SRC and the support vector machine classifier.



### Distributed Coding of Multiview Sparse Sources with Joint Recovery
- **Arxiv ID**: http://arxiv.org/abs/1607.04965v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, cs.MM, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1607.04965v1)
- **Published**: 2016-07-18 07:41:43+00:00
- **Updated**: 2016-07-18 07:41:43+00:00
- **Authors**: Huynh Van Luong, Nikos Deligiannis, Søren Forchhammer, André Kaup
- **Comment**: None
- **Journal**: None
- **Summary**: In support of applications involving multiview sources in distributed object recognition using lightweight cameras, we propose a new method for the distributed coding of sparse sources as visual descriptor histograms extracted from multiview images. The problem is challenging due to the computational and energy constraints at each camera as well as the limitations regarding inter-camera communication. Our approach addresses these challenges by exploiting the sparsity of the visual descriptor histograms as well as their intra- and inter-camera correlations. Our method couples distributed source coding of the sparse sources with a new joint recovery algorithm that incorporates multiple side information signals, where prior knowledge (low quality) of all the sparse sources is initially sent to exploit their correlations. Experimental evaluation using the histograms of shift-invariant feature transform (SIFT) descriptors extracted from multiview images shows that our method leads to bit-rate saving of up to 43% compared to the state-of-the-art distributed compressed sensing method with independent encoding of the sources.



### End-to-end optimization of nonlinear transform codes for perceptual quality
- **Arxiv ID**: http://arxiv.org/abs/1607.05006v2
- **DOI**: None
- **Categories**: **cs.IT**, cs.CV, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1607.05006v2)
- **Published**: 2016-07-18 10:41:17+00:00
- **Updated**: 2016-10-17 19:41:09+00:00
- **Authors**: Johannes Ballé, Valero Laparra, Eero P. Simoncelli
- **Comment**: Accepted as a conference contribution to Picture Coding Symposium
  2016
- **Journal**: Proc. 32nd Picture Coding Symposium, Nuremberg, Germany, Dec 2016.
  IEEE Signal Proc Society
- **Summary**: We introduce a general framework for end-to-end optimization of the rate--distortion performance of nonlinear transform codes assuming scalar quantization. The framework can be used to optimize any differentiable pair of analysis and synthesis transforms in combination with any differentiable perceptual metric. As an example, we consider a code built from a linear transform followed by a form of multi-dimensional local gain control. Distortion is measured with a state-of-the-art perceptual metric. When optimized over a large database of images, this representation offers substantial improvements in bitrate and perceptual appearance over fixed (DCT) codes, and over linear transform codes optimized for mean squared error.



### Deep Cascaded Bi-Network for Face Hallucination
- **Arxiv ID**: http://arxiv.org/abs/1607.05046v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.05046v1)
- **Published**: 2016-07-18 12:39:37+00:00
- **Updated**: 2016-07-18 12:39:37+00:00
- **Authors**: Shizhan Zhu, Sifei Liu, Chen Change Loy, Xiaoou Tang
- **Comment**: This paper is to appear in Proceedings of ECCV 2016
- **Journal**: None
- **Summary**: We present a novel framework for hallucinating faces of unconstrained poses and with very low resolution (face size as small as 5pxIOD). In contrast to existing studies that mostly ignore or assume pre-aligned face spatial configuration (e.g. facial landmarks localization or dense correspondence field), we alternatingly optimize two complementary tasks, namely face hallucination and dense correspondence field estimation, in a unified framework. In addition, we propose a new gated deep bi-network that contains two functionality-specialized branches to recover different levels of texture details. Extensive experiments demonstrate that such formulation allows exceptional hallucination quality on in-the-wild low-res faces with significant pose and illumination variations.



### Recycle deep features for better object detection
- **Arxiv ID**: http://arxiv.org/abs/1607.05066v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/1607.05066v1)
- **Published**: 2016-07-18 13:42:56+00:00
- **Updated**: 2016-07-18 13:42:56+00:00
- **Authors**: Wei Li, Matthias Breier, Dorit Merhof
- **Comment**: None
- **Journal**: None
- **Summary**: Aiming at improving the performance of existing detection algorithms developed for different applications, we propose a region regression-based multi-stage class-agnostic detection pipeline, whereby the existing algorithms are employed for providing the initial detection proposals. Better detection is obtained by exploiting the power of deep learning in the region regress scheme while avoiding the requirement on a huge amount of reference data for training deep neural networks. Additionally, a novel network architecture with recycled deep features is proposed, which provides superior regression results compared to the commonly used architectures. As demonstrated on a data set with ~1200 samples of different classes, it is feasible to successfully train a deep neural network in our proposed architecture and use it to obtain the desired detection performance. Since only slight modifications are required to common network architectures and since the deep neural network is trained using the standard hyperparameters, the proposed detection is well accessible and can be easily adopted to a broad variety of detection tasks.



### Deep Active Contours
- **Arxiv ID**: http://arxiv.org/abs/1607.05074v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.05074v1)
- **Published**: 2016-07-18 13:53:29+00:00
- **Updated**: 2016-07-18 13:53:29+00:00
- **Authors**: Christian Rupprecht, Elizabeth Huaroc, Maximilian Baust, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method for interactive boundary extraction which combines a deep, patch-based representation with an active contour framework. We train a class-specific convolutional neural network which predicts a vector pointing from the respective point on the evolving contour towards the closest point on the boundary of the object of interest. These predictions form a vector field which is then used for evolving the contour by the Sobolev active contour framework proposed by Sundaramoorthi et al. The resulting interactive segmentation method is very efficient in terms of required computational resources and can even be trained on comparatively small graphics cards. We evaluate the potential of the proposed method on both medical and non-medical challenge data sets, such as the STACOM data set and the PASCAL VOC 2012 data set.



### Learning to Hash with Binary Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1607.05140v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.05140v1)
- **Published**: 2016-07-18 15:48:58+00:00
- **Updated**: 2016-07-18 15:48:58+00:00
- **Authors**: Thanh-Toan Do, Anh-Dzung Doan, Ngai-Man Cheung
- **Comment**: Appearing in European Conference on Computer Vision (ECCV) 2016
- **Journal**: None
- **Summary**: This work proposes deep network models and learning algorithms for unsupervised and supervised binary hashing. Our novel network design constrains one hidden layer to directly output the binary codes. This addresses a challenging issue in some previous works: optimizing non-smooth objective functions due to binarization. Moreover, we incorporate independence and balance properties in the direct and strict forms in the learning. Furthermore, we include similarity preserving property in our objective function. Our resulting optimization with these binary, independence, and balance constraints is difficult to solve. We propose to attack it with alternating optimization and careful relaxation. Experimental results on three benchmark datasets show that our proposed methods compare favorably with the state of the art.



### Query-Focused Extractive Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/1607.05177v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.05177v1)
- **Published**: 2016-07-18 16:49:19+00:00
- **Updated**: 2016-07-18 16:49:19+00:00
- **Authors**: Aidean Sharghi, Boqing Gong, Mubarak Shah
- **Comment**: Accepted to ECCV 2016
- **Journal**: None
- **Summary**: Video data is explosively growing. As a result of the "big video data", intelligent algorithms for automatic video summarization have re-emerged as a pressing need. We develop a probabilistic model, Sequential and Hierarchical Determinantal Point Process (SH-DPP), for query-focused extractive video summarization. Given a user query and a long video sequence, our algorithm returns a summary by selecting key shots from the video. The decision to include a shot in the summary depends on the shot's relevance to the user query and importance in the context of the video, jointly. We verify our approach on two densely annotated video datasets. The query-focused video summarization is particularly useful for search engines, e.g., to display snippets of videos.



### HeMIS: Hetero-Modal Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1607.05194v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.05194v1)
- **Published**: 2016-07-18 17:11:57+00:00
- **Updated**: 2016-07-18 17:11:57+00:00
- **Authors**: Mohammad Havaei, Nicolas Guizard, Nicolas Chapados, Yoshua Bengio
- **Comment**: Accepted as an oral presentation at MICCAI 2016
- **Journal**: None
- **Summary**: We introduce a deep learning image segmentation framework that is extremely robust to missing imaging modalities. Instead of attempting to impute or synthesize missing data, the proposed approach learns, for each modality, an embedding of the input image into a single latent vector space for which arithmetic operations (such as taking the mean) are well defined. Points in that space, which are averaged over modalities available at inference time, can then be further processed to yield the desired segmentation. As such, any combinatorial subset of available modalities can be provided as input, without having to learn a combinatorial number of imputation models. Evaluated on two neurological MRI datasets (brain tumors and MS lesions), the approach yields state-of-the-art segmentation results when provided with all modalities; moreover, its performance degrades remarkably gracefully when modalities are removed, significantly more so than alternative mean-filling or other synthesis approaches.



### Bag of Attributes for Video Event Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1607.05208v2
- **DOI**: 10.1109/SIBGRAPI.2018.00064
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1607.05208v2)
- **Published**: 2016-07-18 17:24:23+00:00
- **Updated**: 2020-12-26 13:47:31+00:00
- **Authors**: Leonardo A. Duarte, Otávio A. B. Penatti, Jurandy Almeida
- **Comment**: None
- **Journal**: in 2018 31st SIBGRAPI Conference on Graphics, Patterns and Images
  (SIBGRAPI), Foz do Igua\c{c}u, Brazil, 2018, pp. 447-454
- **Summary**: In this paper, we present the Bag-of-Attributes (BoA) model for video representation aiming at video event retrieval. The BoA model is based on a semantic feature space for representing videos, resulting in high-level video feature vectors. For creating a semantic space, i.e., the attribute space, we can train a classifier using a labeled image dataset, obtaining a classification model that can be understood as a high-level codebook. This model is used to map low-level frame vectors into high-level vectors (e.g., classifier probability scores). Then, we apply pooling operations to the frame vectors to create the final bag of attributes for the video. In the BoA representation, each dimension corresponds to one category (or attribute) of the semantic space. Other interesting properties are: compactness, flexibility regarding the classifier, and ability to encode multiple semantic concepts in a single video representation. Our experiments considered the semantic space created by state-of-the-art convolutional neural networks pre-trained on 1000 object categories of ImageNet. Such deep neural networks were used to classify each video frame and then different coding strategies were used to encode the probability distribution from the softmax layer into a frame vector. Next, different pooling strategies were used to combine frame vectors in the BoA representation for a video. Results using BoA were comparable or superior to the baselines in the task of video event retrieval using the EVVE dataset, with the advantage of providing a much more compact representation.



### Deep learning trends for focal brain pathology segmentation in MRI
- **Arxiv ID**: http://arxiv.org/abs/1607.05258v3
- **DOI**: 10.1007/978-3-319-50478-0_6
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.05258v3)
- **Published**: 2016-07-18 19:52:00+00:00
- **Updated**: 2017-01-24 02:44:48+00:00
- **Authors**: Mohammad Havaei, Nicolas Guizard, Hugo Larochelle, Pierre-Marc Jodoin
- **Comment**: Published in Machine Learning for Health Informatics
- **Journal**: None
- **Summary**: Segmentation of focal (localized) brain pathologies such as brain tumors and brain lesions caused by multiple sclerosis and ischemic strokes are necessary for medical diagnosis, surgical planning and disease development as well as other applications such as tractography. Over the years, attempts have been made to automate this process for both clinical and research reasons. In this regard, machine learning methods have long been a focus of attention. Over the past two years, the medical imaging field has seen a rise in the use of a particular branch of machine learning commonly known as deep learning. In the non-medical computer vision world, deep learning based methods have obtained state-of-the-art results on many datasets. Recent studies in computer aided diagnostics have shown deep learning methods (and especially convolutional neural networks - CNN) to yield promising results. In this chapter, we provide a survey of CNN methods applied to medical imaging with a focus on brain pathology segmentation. In particular, we discuss their characteristic peculiarities and their specific configuration and adjustments that are best suited to segment medical images. We also underline the intrinsic differences deep learning methods have with other machine learning methods.



### Geometry-Informed Material Recognition
- **Arxiv ID**: http://arxiv.org/abs/1607.05338v1
- **DOI**: 10.1109/CVPR.2016.172
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.05338v1)
- **Published**: 2016-07-18 22:15:49+00:00
- **Updated**: 2016-07-18 22:15:49+00:00
- **Authors**: Joseph DeGol, Mani Golparvar-Fard, Derek Hoiem
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition 2016 (CVPR
  '16)
- **Journal**: None
- **Summary**: Our goal is to recognize material categories using images and geometry information. In many applications, such as construction management, coarse geometry information is available. We investigate how 3D geometry (surface normals, camera intrinsic and extrinsic parameters) can be used with 2D features (texture and color) to improve material classification. We introduce a new dataset, GeoMat, which is the first to provide both image and geometry data in the form of: (i) training and testing patches that were extracted at different scales and perspectives from real world examples of each material category, and (ii) a large scale construction site scene that includes 160 images and over 800,000 hand labeled 3D points. Our results show that using 2D and 3D features both jointly and independently to model materials improves classification accuracy across multiple scales and viewing directions for both material patches and images of a large scale construction site scene.



