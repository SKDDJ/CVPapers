# Arxiv Papers in cs.CV on 2016-07-04
### Coarse2Fine: Two-Layer Fusion For Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1607.00719v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1607.00719v1)
- **Published**: 2016-07-04 01:56:20+00:00
- **Updated**: 2016-07-04 01:56:20+00:00
- **Authors**: Gaipeng Kong, Le Dong, Wenpu Dong, Liang Zheng, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of large-scale image retrieval. We propose a two-layer fusion method which takes advantage of global and local cues and ranks database images from coarse to fine (C2F). Departing from the previous methods fusing multiple image descriptors simultaneously, C2F is featured by a layered procedure composed by filtering and refining. In particular, C2F consists of three components. 1) Distractor filtering. With holistic representations, noise images are filtered out from the database, so the number of candidate images to be used for comparison with the query can be greatly reduced. 2) Adaptive weighting. For a certain query, the similarity of candidate images can be estimated by holistic similarity scores in complementary to the local ones. 3) Candidate refining. Accurate retrieval is conducted via local features, combining the pre-computed adaptive weights. Experiments are presented on two benchmarks, \emph{i.e.,} Holidays and Ukbench datasets. We show that our method outperforms recent fusion methods in terms of storage consumption and computation complexity, and that the accuracy is competitive to the state-of-the-arts.



### A Two-Streamed Network for Estimating Fine-Scaled Depth Maps from Single RGB Images
- **Arxiv ID**: http://arxiv.org/abs/1607.00730v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.00730v4)
- **Published**: 2016-07-04 03:22:45+00:00
- **Updated**: 2017-12-04 01:52:43+00:00
- **Authors**: Jun Li, Reinhard Klein, Angela Yao
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating depth from a single RGB image is an ill-posed and inherently ambiguous problem. State-of-the-art deep learning methods can now estimate accurate 2D depth maps, but when the maps are projected into 3D, they lack local detail and are often highly distorted. We propose a fast-to-train two-streamed CNN that predicts depth and depth gradients, which are then fused together into an accurate and detailed depth map. We also define a novel set loss over multiple images; by regularizing the estimation between a common set of images, the network is less prone to over-fitting and achieves better accuracy than competing methods. Experiments on the NYU Depth v2 dataset shows that our depth predictions are competitive with state-of-the-art and lead to faithful 3D projections.



### Cell assemblies at multiple time scales with arbitrary lag constellations
- **Arxiv ID**: http://arxiv.org/abs/1607.00969v2
- **DOI**: 10.7554/eLife.19428
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1607.00969v2)
- **Published**: 2016-07-04 17:35:35+00:00
- **Updated**: 2017-02-14 14:24:54+00:00
- **Authors**: Eleonora Russo, Daniel Durstewitz
- **Comment**: None
- **Journal**: eLife 2017;6:e19428
- **Summary**: Hebb's idea of a cell assembly as the fundamental unit of neural information processing has dominated neuroscience like no other theoretical concept within the past 60 years. A range of different physiological phenomena, from precisely synchronized spiking to broadly simultaneous rate increases, has been subsumed under this term. Yet progress in this area is hampered by the lack of statistical tools that would enable to extract assemblies with arbitrary constellations of time lags, and at multiple temporal scales, partly due to the severe computational burden. Here we present such a unifying methodological and conceptual framework which detects assembly structure at many different time scales, levels of precision, and with arbitrary internal organization. Applying this methodology to multiple single unit recordings from various cortical areas, we find that there is no universal cortical coding scheme, but that assembly structure and precision significantly depends on brain area recorded and ongoing task demands.



### Can we unify monocular detectors for autonomous driving by using the pixel-wise semantic segmentation of CNNs?
- **Arxiv ID**: http://arxiv.org/abs/1607.00971v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.00971v1)
- **Published**: 2016-07-04 17:44:13+00:00
- **Updated**: 2016-07-04 17:44:13+00:00
- **Authors**: Eduardo Romera, Luis M. Bergasa, Roberto Arroyo
- **Comment**: Extended abstract presented in IV16-WS Deepdriving
  (http://iv2016.berkeleyvision.org/)
- **Journal**: None
- **Summary**: Autonomous driving is a challenging topic that requires complex solutions in perception tasks such as recognition of road, lanes, traffic signs or lights, vehicles and pedestrians. Through years of research, computer vision has grown capable of tackling these tasks with monocular detectors that can provide remarkable detection rates with relatively low processing times. However, the recent appearance of Convolutional Neural Networks (CNNs) has revolutionized the computer vision field and has made possible approaches to perform full pixel-wise semantic segmentation in times close to real time (even on hardware that can be carried on a vehicle). In this paper, we propose to use full image segmentation as an approach to simplify and unify most of the detection tasks required in the perception module of an autonomous vehicle, analyzing major concerns such as computation time and detection performance.



### Improving Sparse Representation-Based Classification Using Local Principal Component Analysis
- **Arxiv ID**: http://arxiv.org/abs/1607.01059v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.01059v6)
- **Published**: 2016-07-04 21:48:29+00:00
- **Updated**: 2018-06-02 05:27:13+00:00
- **Authors**: Chelsea Weaver, Naoki Saito
- **Comment**: Published in "Computational Intelligence for Pattern Recognition,"
  editors Shyi-Ming Chen and Witold Pedrycz. The original publication is
  available at http://www.springerlink.com
- **Journal**: None
- **Summary**: Sparse representation-based classification (SRC), proposed by Wright et al., seeks the sparsest decomposition of a test sample over the dictionary of training samples, with classification to the most-contributing class. Because it assumes test samples can be written as linear combinations of their same-class training samples, the success of SRC depends on the size and representativeness of the training set. Our proposed classification algorithm enlarges the training set by using local principal component analysis to approximate the basis vectors of the tangent hyperplane of the class manifold at each training sample. The dictionary in SRC is replaced by a local dictionary that adapts to the test sample and includes training samples and their corresponding tangent basis vectors. We use a synthetic data set and three face databases to demonstrate that this method can achieve higher classification accuracy than SRC in cases of sparse sampling, nonlinear class manifolds, and stringent dimension reduction.



