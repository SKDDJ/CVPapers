# Arxiv Papers in cs.CV on 2016-07-02
### NIST: An Image Classification Network to Image Semantic Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1607.00464v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.00464v1)
- **Published**: 2016-07-02 04:39:24+00:00
- **Updated**: 2016-07-02 04:39:24+00:00
- **Authors**: Le Dong, Xiuyuan Chen, Mengdie Mao, Qianni Zhang
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: This paper proposes a classification network to image semantic retrieval (NIST) framework to counter the image retrieval challenge. Our approach leverages the successful classification network GoogleNet based on Convolutional Neural Networks to obtain the semantic feature matrix which contains the serial number of classes and corresponding probabilities. Compared with traditional image retrieval using feature matching to compute the similarity between two images, NIST leverages the semantic information to construct semantic feature matrix and uses the semantic distance algorithm to compute the similarity. Besides, the fusion strategy can significantly reduce storage and time consumption due to less classes participating in the last semantic distance computation. Experiments demonstrate that our NIST framework produces state-of-the-art results in retrieval experiments on MIRFLICKR-25K dataset.



### Keyframe-based monocular SLAM: design, survey, and future directions
- **Arxiv ID**: http://arxiv.org/abs/1607.00470v2
- **DOI**: 10.1016/j.robot.2017.09.010
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1607.00470v2)
- **Published**: 2016-07-02 06:01:16+00:00
- **Updated**: 2018-01-07 20:47:25+00:00
- **Authors**: Georges Younes, Daniel Asmar, Elie Shammas, John Zelek
- **Comment**: None
- **Journal**: Robotics and Autonomous Systems, Volume 98, 2017, Pages 67-88
- **Summary**: Extensive research in the field of monocular SLAM for the past fifteen years has yielded workable systems that found their way into various applications in robotics and augmented reality. Although filter-based monocular SLAM systems were common at some time, the more efficient keyframe-based solutions are becoming the de facto methodology for building a monocular SLAM system. The objective of this paper is threefold: first, the paper serves as a guideline for people seeking to design their own monocular SLAM according to specific environmental constraints. Second, it presents a survey that covers the various keyframe-based monocular SLAM systems in the literature, detailing the components of their implementation, and critically assessing the specific strategies made in each proposed solution. Third, the paper provides insight into the direction of future research in this field, to address the major limitations still facing monocular SLAM; namely, in the issues of illumination changes, initialization, highly dynamic motion, poorly textured scenes, repetitive textures, map maintenance, and failure recovery.



### A Distributed Deep Representation Learning Model for Big Image Data Classification
- **Arxiv ID**: http://arxiv.org/abs/1607.00501v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.00501v1)
- **Published**: 2016-07-02 12:33:12+00:00
- **Updated**: 2016-07-02 12:33:12+00:00
- **Authors**: Le Dong, Na Lv, Qianni Zhang, Shanshan Xie, Ling He, Mengdie Mao
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes an effective and efficient image classification framework nominated distributed deep representation learning model (DDRL). The aim is to strike the balance between the computational intensive deep learning approaches (tuned parameters) which are intended for distributed computing, and the approaches that focused on the designed parameters but often limited by sequential computing and cannot scale up. In the evaluation of our approach, it is shown that DDRL is able to achieve state-of-art classification accuracy efficiently on both medium and large datasets. The result implies that our approach is more efficient than the conventional deep learning approaches, and can be applied to big data that is too complex for parameter designing focused approaches. More specifically, DDRL contains two main components, i.e., feature extraction and selection. A hierarchical distributed deep representation learning algorithm is designed to extract image statistics and a nonlinear mapping algorithm is used to map the inherent statistics into abstract features. Both algorithms are carefully designed to avoid millions of parameters tuning. This leads to a more compact solution for image classification of big data. We note that the proposed approach is designed to be friendly with parallel computing. It is generic and easy to be deployed to different distributed computing resources. In the experiments, the largescale image datasets are classified with a DDRM implementation on Hadoop MapReduce, which shows high scalability and resilience.



### Active Object Localization in Visual Situations
- **Arxiv ID**: http://arxiv.org/abs/1607.00548v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.00548v1)
- **Published**: 2016-07-02 18:43:07+00:00
- **Updated**: 2016-07-02 18:43:07+00:00
- **Authors**: Max H. Quinn, Anthony D. Rhodes, Melanie Mitchell
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: We describe a method for performing active localization of objects in instances of visual situations. A visual situation is an abstract concept---e.g., "a boxing match", "a birthday party", "walking the dog", "waiting for a bus"---whose image instantiations are linked more by their common spatial and semantic structure than by low-level visual similarity. Our system combines given and learned knowledge of the structure of a particular situation, and adapts that knowledge to a new situation instance as it actively searches for objects. More specifically, the system learns a set of probability distributions describing spatial and other relationships among relevant objects. The system uses those distributions to iteratively sample object proposals on a test image, but also continually uses information from those object proposals to adaptively modify the distributions based on what the system has detected. We test our approach's ability to efficiently localize objects, using a situation-specific image dataset created by our group. We compare the results with several baselines and variations on our method, and demonstrate the strong benefit of using situation knowledge and active context-driven localization. Finally, we contrast our method with several other approaches that use context as well as active search for object localization in images.



