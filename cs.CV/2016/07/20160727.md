# Arxiv Papers in cs.CV on 2016-07-27
### A Continuous Optimization Approach for Efficient and Accurate Scene Flow
- **Arxiv ID**: http://arxiv.org/abs/1607.07983v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.07983v1)
- **Published**: 2016-07-27 07:07:07+00:00
- **Updated**: 2016-07-27 07:07:07+00:00
- **Authors**: Zhaoyang Lv, Chris Beall, Pablo F. Alcantarilla, Fuxin Li, Zsolt Kira, Frank Dellaert
- **Comment**: Accepted in ECCV 2016. Please refer to the ECCV16-springer for
  detailed information
- **Journal**: None
- **Summary**: We propose a continuous optimization method for solving dense 3D scene flow problems from stereo imagery. As in recent work, we represent the dynamic 3D scene as a collection of rigidly moving planar segments. The scene flow problem then becomes the joint estimation of pixel-to-segment assignment, 3D position, normal vector and rigid motion parameters for each segment, leading to a complex and expensive discrete-continuous optimization problem. In contrast, we propose a purely continuous formulation which can be solved more efficiently. Using a fine superpixel segmentation that is fixed a-priori, we propose a factor graph formulation that decomposes the problem into photometric, geometric, and smoothing constraints. We initialize the solution with a novel, high-quality initialization method, then independently refine the geometry and motion of the scene, and finally perform a global non-linear refinement using Levenberg-Marquardt. We evaluate our method in the challenging KITTI Scene Flow benchmark, ranking in third position, while being 3 to 30 times faster than the top competitors.



### A Multiple Kernel Learning Approach for Human Behavioral Task Classification using STN-LFP Signal
- **Arxiv ID**: http://arxiv.org/abs/1607.07987v1
- **DOI**: 10.1109/EMBC.2016.7590878
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1607.07987v1)
- **Published**: 2016-07-27 07:28:40+00:00
- **Updated**: 2016-07-27 07:28:40+00:00
- **Authors**: Hosein M. Golshan, Adam O. Hebb, Sara J. Hanrahan, Joshua Nedrud, Mohammad H. Mahoor
- **Comment**: 38th Annual International Conference of the IEEE Engineering in
  Medicine and Biology Scociety
- **Journal**: None
- **Summary**: Deep Brain Stimulation (DBS) has gained increasing attention as an effective method to mitigate Parkinsons disease (PD) disorders. Existing DBS systems are open-loop such that the system parameters are not adjusted automatically based on patients behavior. Classification of human behavior is an important step in the design of the next generation of DBS systems that are closed-loop. This paper presents a classification approach to recognize such behavioral tasks using the subthalamic nucleus (STN) Local Field Potential (LFP) signals. In our approach, we use the time-frequency representation (spectrogram) of the raw LFP signals recorded from left and right STNs as the feature vectors. Then these features are combined together via Support Vector Machines (SVM) with Multiple Kernel Learning (MKL) formulation. The MKL-based classification method is utilized to classify different tasks: button press, mouth movement, speech, and arm movement. Our experiments show that the lp-norm MKL significantly outperforms single kernel SVM-based classifiers in classifying behavioral tasks of five subjects even using signals acquired with a low sampling rate of 10 Hz. This leads to a lower computational cost.



### ATGV-Net: Accurate Depth Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1607.07988v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.07988v1)
- **Published**: 2016-07-27 07:29:08+00:00
- **Updated**: 2016-07-27 07:29:08+00:00
- **Authors**: Gernot Riegler, Matthias RÃ¼ther, Horst Bischof
- **Comment**: ECCV 2016
- **Journal**: None
- **Summary**: In this work we present a novel approach for single depth map super-resolution. Modern consumer depth sensors, especially Time-of-Flight sensors, produce dense depth measurements, but are affected by noise and have a low lateral resolution. We propose a method that combines the benefits of recent advances in machine learning based single image super-resolution, i.e. deep convolutional networks, with a variational method to recover accurate high-resolution depth maps. In particular, we integrate a variational method that models the piecewise affine structures apparent in depth data via an anisotropic total generalized variation regularization term on top of a deep network. We call our method ATGV-Net and train it end-to-end by unrolling the optimization procedure of the variational method. To train deep networks, a large corpus of training data with accurate ground-truth is required. We demonstrate that it is feasible to train our method solely on synthetic data that we generate in large quantities for this task. Our evaluations show that we achieve state-of-the-art results on three different benchmarks, as well as on a challenging Time-of-Flight dataset, all without utilizing an additional intensity image as guidance.



### Instance Normalization: The Missing Ingredient for Fast Stylization
- **Arxiv ID**: http://arxiv.org/abs/1607.08022v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.08022v3)
- **Published**: 2016-07-27 10:23:00+00:00
- **Updated**: 2017-11-06 14:21:43+00:00
- **Authors**: Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky
- **Comment**: None
- **Journal**: None
- **Summary**: It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will is made available on github at https://github.com/DmitryUlyanov/texture_nets. Full paper can be found at arXiv:1701.02096.



### Visual Tracking via Shallow and Deep Collaborative Model
- **Arxiv ID**: http://arxiv.org/abs/1607.08040v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.08040v1)
- **Published**: 2016-07-27 11:18:12+00:00
- **Updated**: 2016-07-27 11:18:12+00:00
- **Authors**: Bohan Zhuang, Lijun Wang, Huchuan Lu
- **Comment**: Undergraduate Thesis, appearing in Pattern Recognition
- **Journal**: None
- **Summary**: In this paper, we propose a robust tracking method based on the collaboration of a generative model and a discriminative classifier, where features are learned by shallow and deep architectures, respectively. For the generative model, we introduce a block-based incremental learning scheme, in which a local binary mask is constructed to deal with occlusion. The similarity degrees between the local patches and their corresponding subspace are integrated to formulate a more accurate global appearance model. In the discriminative model, we exploit the advances of deep learning architectures to learn generic features which are robust to both background clutters and foreground appearance variations. To this end, we first construct a discriminative training set from auxiliary video sequences. A deep classification neural network is then trained offline on this training set. Through online fine-tuning, both the hierarchical feature extractor and the classifier can be adapted to the appearance change of the target for effective online tracking. The collaboration of these two models achieves a good balance in handling occlusion and target appearance change, which are two contradictory challenging factors in visual tracking. Both quantitative and qualitative evaluations against several state-of-the-art algorithms on challenging image sequences demonstrate the accuracy and the robustness of the proposed tracker.



### CNN-based Patch Matching for Optical Flow with Thresholded Hinge Embedding Loss
- **Arxiv ID**: http://arxiv.org/abs/1607.08064v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1607.08064v4)
- **Published**: 2016-07-27 12:41:00+00:00
- **Updated**: 2020-05-25 06:28:24+00:00
- **Authors**: Christian Bailer, Kiran Varanasi, Didier Stricker
- **Comment**: Fixed bracket error in equation 3 (it has no major influence in the
  approach, but on the optimal t value)
- **Journal**: None
- **Summary**: Learning based approaches have not yet achieved their full potential in optical flow estimation, where their performance still trails heuristic approaches. In this paper, we present a CNN based patch matching approach for optical flow estimation. An important contribution of our approach is a novel thresholded loss for Siamese networks. We demonstrate that our loss performs clearly better than existing losses. It also allows to speed up training by a factor of 2 in our tests. Furthermore, we present a novel way for calculating CNN based features for different image scales, which performs better than existing methods. We also discuss new ways of evaluating the robustness of trained features for the application of patch matching for optical flow. An interesting discovery in our paper is that low-pass filtering of feature maps can increase the robustness of features created by CNNs. We proved the competitive performance of our approach by submitting it to the KITTI 2012, KITTI 2015 and MPI-Sintel evaluation portals where we obtained state-of-the-art results on all three datasets.



### Improving Semantic Embedding Consistency by Metric Learning for Zero-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/1607.08085v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, math.ST, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/1607.08085v1)
- **Published**: 2016-07-27 13:35:16+00:00
- **Updated**: 2016-07-27 13:35:16+00:00
- **Authors**: Maxime Bucher, StÃ©phane Herbin, FrÃ©dÃ©ric Jurie
- **Comment**: in ECCV 2016, Oct 2016, amsterdam, Netherlands. 2016
- **Journal**: None
- **Summary**: This paper addresses the task of zero-shot image classification. The key contribution of the proposed approach is to control the semantic embedding of images -- one of the main ingredients of zero-shot learning -- by formulating it as a metric learning problem. The optimized empirical criterion associates two types of sub-task constraints: metric discriminating capacity and accurate attribute prediction. This results in a novel expression of zero-shot learning not requiring the notion of class in the training phase: only pairs of image/attributes, augmented with a consistency indicator, are given as ground truth. At test time, the learned model can predict the consistency of a test image with a given set of attributes , allowing flexible ways to produce recognition inferences. Despite its simplicity, the proposed approach gives state-of-the-art results on four challenging datasets used for zero-shot recognition evaluation.



### MLPnP - A Real-Time Maximum Likelihood Solution to the Perspective-n-Point Problem
- **Arxiv ID**: http://arxiv.org/abs/1607.08112v1
- **DOI**: 10.5194/isprs-annals-III-3-131-2016
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.08112v1)
- **Published**: 2016-07-27 14:23:52+00:00
- **Updated**: 2016-07-27 14:23:52+00:00
- **Authors**: Steffen Urban, Jens Leitloff, Stefan Hinz
- **Comment**: Submitted to the ISPRS congress (2016) in Prague. Oral Presentation.
  Published in ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., III-3,
  131-138
- **Journal**: None
- **Summary**: In this paper, a statistically optimal solution to the Perspective-n-Point (PnP) problem is presented. Many solutions to the PnP problem are geometrically optimal, but do not consider the uncertainties of the observations. In addition, it would be desirable to have an internal estimation of the accuracy of the estimated rotation and translation parameters of the camera pose. Thus, we propose a novel maximum likelihood solution to the PnP problem, that incorporates image observation uncertainties and remains real-time capable at the same time. Further, the presented method is general, as is works with 3D direction vectors instead of 2D image points and is thus able to cope with arbitrary central camera models. This is achieved by projecting (and thus reducing) the covariance matrices of the observations to the corresponding vector tangent space.



### Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1607.08128v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.08128v1)
- **Published**: 2016-07-27 14:46:36+00:00
- **Updated**: 2016-07-27 14:46:36+00:00
- **Authors**: Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, Michael J. Black
- **Comment**: To appear in ECCV 2016
- **Journal**: None
- **Summary**: We describe the first method to automatically estimate the 3D pose of the human body as well as its 3D shape from a single unconstrained image. We estimate a full 3D mesh and show that 2D joints alone carry a surprising amount of information about body shape. The problem is challenging because of the complexity of the human body, articulation, occlusion, clothing, lighting, and the inherent ambiguity in inferring 3D from 2D. To solve this, we first use a recently published CNN-based method, DeepCut, to predict (bottom-up) the 2D body joint locations. We then fit (top-down) a recently published statistical body shape model, called SMPL, to the 2D joints. We do so by minimizing an objective function that penalizes the error between the projected 3D model joints and detected 2D joints. Because SMPL captures correlations in human shape across the population, we are able to robustly fit it to very little data. We further leverage the 3D model to prevent solutions that cause interpenetration. We evaluate our method, SMPLify, on the Leeds Sports, HumanEva, and Human3.6M datasets, showing superior pose accuracy with respect to the state of the art.



### Spatial probabilistic pulsatility model for enhancing photoplethysmographic imaging systems
- **Arxiv ID**: http://arxiv.org/abs/1607.08129v1
- **DOI**: 10.1117/1.JBO.21.11.116010
- **Categories**: **q-bio.QM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1607.08129v1)
- **Published**: 2016-07-27 14:49:33+00:00
- **Updated**: 2016-07-27 14:49:33+00:00
- **Authors**: Robert Amelard, David A Clausi, Alexander Wong
- **Comment**: None
- **Journal**: None
- **Summary**: Photolethysmographic imaging (PPGI) is a widefield non-contact biophotonic technology able to remotely monitor cardiovascular function over anatomical areas. Though spatial context can provide increased physiological insight, existing PPGI systems rely on coarse spatial averaging with no anatomical priors for assessing arterial pulsatility. Here, we developed a continuous probabilistic pulsatility model for importance-weighted blood pulse waveform extraction. Using a data-driven approach, the model was constructed using a 23 participant sample with large demographic variation (11/12 female/male, age 11-60 years, BMI 16.4-35.1 kg$\cdot$m$^{-2}$). Using time-synchronized ground-truth waveforms, spatial correlation priors were computed and projected into a co-aligned importance-weighted Cartesian space. A modified Parzen-Rosenblatt kernel density estimation method was used to compute the continuous resolution-agnostic probabilistic pulsatility model. The model identified locations that consistently exhibited pulsatility across the sample. Blood pulse waveform signals extracted with the model exhibited significantly stronger temporal correlation ($W=35,p<0.01$) and spectral SNR ($W=31,p<0.01$) compared to uniform spatial averaging. Heart rate estimation was in strong agreement with true heart rate ($r^2=0.9619$, error $(\mu,\sigma)=(0.52,1.69)$ bpm).



### Adaptive foveated single-pixel imaging with dynamic super-sampling
- **Arxiv ID**: http://arxiv.org/abs/1607.08236v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1607.08236v1)
- **Published**: 2016-07-27 15:27:42+00:00
- **Updated**: 2016-07-27 15:27:42+00:00
- **Authors**: David B. Phillips, Ming-Jie Sun, Jonathan M. Taylor, Matthew P. Edgar, Stephen M. Barnett, Graham G. Gibson, Miles J. Padgett
- **Comment**: 13 pages, 5 figures
- **Journal**: None
- **Summary**: As an alternative to conventional multi-pixel cameras, single-pixel cameras enable images to be recorded using a single detector that measures the correlations between the scene and a set of patterns. However, to fully sample a scene in this way requires at least the same number of correlation measurements as there are pixels in the reconstructed image. Therefore single-pixel imaging systems typically exhibit low frame-rates. To mitigate this, a range of compressive sensing techniques have been developed which rely on a priori knowledge of the scene to reconstruct images from an under-sampled set of measurements. In this work we take a different approach and adopt a strategy inspired by the foveated vision systems found in the animal kingdom - a framework that exploits the spatio-temporal redundancy present in many dynamic scenes. In our single-pixel imaging system a high-resolution foveal region follows motion within the scene, but unlike a simple zoom, every frame delivers new spatial information from across the entire field-of-view. Using this approach we demonstrate a four-fold reduction in the time taken to record the detail of rapidly evolving features, whilst simultaneously accumulating detail of more slowly evolving regions over several consecutive frames. This tiered super-sampling technique enables the reconstruction of video streams in which both the resolution and the effective exposure-time spatially vary and adapt dynamically in response to the evolution of the scene. The methods described here can complement existing compressive sensing approaches and may be applied to enhance a variety of computational imagers that rely on sequential correlation measurements.



### Calorie Counter: RGB-Depth Visual Estimation of Energy Expenditure at Home
- **Arxiv ID**: http://arxiv.org/abs/1607.08196v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.08196v1)
- **Published**: 2016-07-27 17:47:44+00:00
- **Updated**: 2016-07-27 17:47:44+00:00
- **Authors**: Lili Tao, Tilo Burghardt, Majid Mirmehdi, Dima Damen, Ashley Cooper, Sion Hannuna, Massimo Camplani, Adeline Paiement, Ian Craddock
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new framework for vision-based estimation of calorific expenditure from RGB-D data - the first that is validated on physical gas exchange measurements and applied to daily living scenarios. Deriving a person's energy expenditure from sensors is an important tool in tracking physical activity levels for health and lifestyle monitoring. Most existing methods use metabolic lookup tables (METs) for a manual estimate or systems with inertial sensors which ultimately require users to wear devices. In contrast, the proposed pose-invariant and individual-independent vision framework allows for a remote estimation of calorific expenditure. We introduce, and evaluate our approach on, a new dataset called SPHERE-calorie, for which visual estimates can be compared against simultaneously obtained, indirect calorimetry measures based on gas exchange. % based on per breath gas exchange. We conclude from our experiments that the proposed vision pipeline is suitable for home monitoring in a controlled environment, with calorific expenditure estimates above accuracy levels of commonly used manual estimations via METs. With the dataset released, our work establishes a baseline for future research for this little-explored area of computer vision.



### MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1607.08221v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.08221v1)
- **Published**: 2016-07-27 19:18:16+00:00
- **Updated**: 2016-07-27 19:18:16+00:00
- **Authors**: Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, Jianfeng Gao
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we design a benchmark task and provide the associated datasets for recognizing face images and link them to corresponding entity keys in a knowledge base. More specifically, we propose a benchmark task to recognize one million celebrities from their face images, by using all the possibly collected face images of this individual on the web as training data. The rich information provided by the knowledge base helps to conduct disambiguation and improve the recognition accuracy, and contributes to various real-world applications, such as image captioning and news video analysis. Associated with this task, we design and provide concrete measurement set, evaluation protocol, as well as training data. We also present in details our experiment setup and report promising baseline results. Our benchmark task could lead to one of the largest classification problems in computer vision. To the best of our knowledge, our training dataset, which contains 10M images in version 1, is the largest publicly available one in the world.



### A 58.6mW Real-Time Programmable Object Detector with Multi-Scale Multi-Object Support Using Deformable Parts Model on 1920x1080 Video at 30fps
- **Arxiv ID**: http://arxiv.org/abs/1607.08635v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR
- **Links**: [PDF](http://arxiv.org/pdf/1607.08635v1)
- **Published**: 2016-07-27 19:20:33+00:00
- **Updated**: 2016-07-27 19:20:33+00:00
- **Authors**: Amr Suleiman, Zhengdong Zhang, Vivienne Sze
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a programmable, energy-efficient and real-time object detection accelerator using deformable parts models (DPM), with 2x higher accuracy than traditional rigid body models. With 8 deformable parts detection, three methods are used to address the high computational complexity: classification pruning for 33x fewer parts classification, vector quantization for 15x memory size reduction, and feature basis projection for 2x reduction of the cost of each classification. The chip is implemented in 65nm CMOS technology, and can process HD (1920x1080) images at 30fps without any off-chip storage while consuming only 58.6mW (0.94nJ/pixel, 1168 GOPS/W). The chip has two classification engines to simultaneously detect two different classes of objects. With a tested high throughput of 60fps, the classification engines can be time multiplexed to detect even more than two object classes. It is energy scalable by changing the pruning factor or disabling the parts classification.



