# Arxiv Papers in cs.CV on 2016-07-13
### Accelerating Eulerian Fluid Simulation With Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1607.03597v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.03597v7)
- **Published**: 2016-07-13 05:57:59+00:00
- **Updated**: 2022-11-08 20:03:56+00:00
- **Authors**: Jonathan Tompson, Kristofer Schlachter, Pablo Sprechmann, Ken Perlin
- **Comment**: Update website url
- **Journal**: None
- **Summary**: Efficient simulation of the Navier-Stokes equations for fluid flow is a long standing problem in applied mathematics, for which state-of-the-art methods require large compute resources. In this work, we propose a data-driven approach that leverages the approximation power of deep-learning with the precision of standard solvers to obtain fast and highly realistic simulations. Our method solves the incompressible Euler equations using the standard operator splitting method, in which a large sparse linear system with many free parameters must be solved. We use a Convolutional Network with a highly tailored architecture, trained using a novel unsupervised learning framework to solve the linear system. We present real-time 2D and 3D simulations that outperform recently proposed data-driven methods; the obtained results are realistic and show good generalization properties.



### Unsupervised Feature Learning Based on Deep Models for Environmental Audio Tagging
- **Arxiv ID**: http://arxiv.org/abs/1607.03681v2
- **DOI**: 10.1109/TASLP.2017.2690563
- **Categories**: **cs.SD**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1607.03681v2)
- **Published**: 2016-07-13 11:31:14+00:00
- **Updated**: 2016-11-29 15:56:36+00:00
- **Authors**: Yong Xu, Qiang Huang, Wenwu Wang, Peter Foster, Siddharth Sigtia, Philip J. B. Jackson, Mark D. Plumbley
- **Comment**: 10 pages, dcase 2016 challenge
- **Journal**: IEEE/ACM Transactions on Audio, Speech and Language Processing
  25(6):1230-1241, Jun 2017
- **Summary**: Environmental audio tagging aims to predict only the presence or absence of certain acoustic events in the interested acoustic scene. In this paper we make contributions to audio tagging in two parts, respectively, acoustic modeling and feature learning. We propose to use a shrinking deep neural network (DNN) framework incorporating unsupervised feature learning to handle the multi-label classification task. For the acoustic modeling, a large set of contextual frames of the chunk are fed into the DNN to perform a multi-label classification for the expected tags, considering that only chunk (or utterance) level rather than frame-level labels are available. Dropout and background noise aware training are also adopted to improve the generalization capability of the DNNs. For the unsupervised feature learning, we propose to use a symmetric or asymmetric deep de-noising auto-encoder (sDAE or aDAE) to generate new data-driven features from the Mel-Filter Banks (MFBs) features. The new features, which are smoothed against background noise and more compact with contextual information, can further improve the performance of the DNN baseline. Compared with the standard Gaussian Mixture Model (GMM) baseline of the DCASE 2016 audio tagging challenge, our proposed method obtains a significant equal error rate (EER) reduction from 0.21 to 0.13 on the development set. The proposed aDAE system can get a relative 6.7% EER reduction compared with the strong DNN baseline on the development set. Finally, the results also show that our approach obtains the state-of-the-art performance with 0.15 EER on the evaluation set of the DCASE 2016 audio tagging task while EER of the first prize of this challenge is 0.17.



### Hierarchical learning for DNN-based acoustic scene classification
- **Arxiv ID**: http://arxiv.org/abs/1607.03682v3
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1607.03682v3)
- **Published**: 2016-07-13 11:31:25+00:00
- **Updated**: 2016-08-13 10:37:53+00:00
- **Authors**: Yong Xu, Qiang Huang, Wenwu Wang, Mark D. Plumbley
- **Comment**: 5 pages, DCASE 2016 challenge workshop paper, poster
- **Journal**: None
- **Summary**: In this paper, we present a deep neural network (DNN)-based acoustic scene classification framework. Two hierarchical learning methods are proposed to improve the DNN baseline performance by incorporating the hierarchical taxonomy information of environmental sounds. Firstly, the parameters of the DNN are initialized by the proposed hierarchical pre-training. Multi-level objective function is then adopted to add more constraint on the cross-entropy based loss function. A series of experiments were conducted on the Task1 of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2016 challenge. The final DNN-based system achieved a 22.9% relative improvement on average scene classification error as compared with the Gaussian Mixture Model (GMM)-based benchmark system across four standard folds.



### Do semantic parts emerge in Convolutional Neural Networks?
- **Arxiv ID**: http://arxiv.org/abs/1607.03738v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.03738v5)
- **Published**: 2016-07-13 13:58:17+00:00
- **Updated**: 2017-09-20 18:09:33+00:00
- **Authors**: Abel Gonzalez-Garcia, Davide Modolo, Vittorio Ferrari
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic object parts can be useful for several visual recognition tasks. Lately, these tasks have been addressed using Convolutional Neural Networks (CNN), achieving outstanding results. In this work we study whether CNNs learn semantic parts in their internal representation. We investigate the responses of convolutional filters and try to associate their stimuli with semantic parts. We perform two extensive quantitative analyses. First, we use ground-truth part bounding-boxes from the PASCAL-Part dataset to determine how many of those semantic parts emerge in the CNN. We explore this emergence for different layers, network depths, and supervision levels. Second, we collect human judgements in order to study what fraction of all filters systematically fire on any semantic part, even if not annotated in PASCAL-Part. Moreover, we explore several connections between discriminative power and semantics. We find out which are the most discriminative filters for object recognition, and analyze whether they respond to semantic parts or to other image patches. We also investigate the other direction: we determine which semantic parts are the most discriminative and whether they correspond to those parts emerging in the network. This enables to gain an even deeper understanding of the role of semantic parts in the network.



### Application of Convolutional Neural Network for Image Classification on Pascal VOC Challenge 2012 dataset
- **Arxiv ID**: http://arxiv.org/abs/1607.03785v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.03785v1)
- **Published**: 2016-07-13 15:18:25+00:00
- **Updated**: 2016-07-13 15:18:25+00:00
- **Authors**: Suyash Shetty
- **Comment**: None
- **Journal**: None
- **Summary**: In this project we work on creating a model to classify images for the Pascal VOC Challenge 2012. We use convolutional neural networks trained on a single GPU instance provided by Amazon via their cloud service Amazon Web Services (AWS) to classify images in the Pascal VOC 2012 data set. We train multiple convolutional neural network models and finally settle on the best model which produced a validation accuracy of 85.6% and a testing accuracy of 85.24%.



### The KIT Motion-Language Dataset
- **Arxiv ID**: http://arxiv.org/abs/1607.03827v2
- **DOI**: 10.1089/big.2016.0028
- **Categories**: **cs.RO**, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1607.03827v2)
- **Published**: 2016-07-13 17:08:01+00:00
- **Updated**: 2018-08-09 14:24:47+00:00
- **Authors**: Matthias Plappert, Christian Mandery, Tamim Asfour
- **Comment**: 5 figures, 4 tables, submitted to Big Data journal, Special Issue on
  Robotics
- **Journal**: None
- **Summary**: Linking human motion and natural language is of great interest for the generation of semantic representations of human activities as well as for the generation of robot activities based on natural language input. However, while there have been years of research in this area, no standardized and openly available dataset exists to support the development and evaluation of such systems. We therefore propose the KIT Motion-Language Dataset, which is large, open, and extensible. We aggregate data from multiple motion capture databases and include them in our dataset using a unified representation that is independent of the capture system or marker set, making it easy to work with the data regardless of its origin. To obtain motion annotations in natural language, we apply a crowd-sourcing approach and a web-based tool that was specifically build for this purpose, the Motion Annotation Tool. We thoroughly document the annotation process itself and discuss gamification methods that we used to keep annotators motivated. We further propose a novel method, perplexity-based selection, which systematically selects motions for further annotation that are either under-represented in our dataset or that have erroneous annotations. We show that our method mitigates the two aforementioned problems and ensures a systematic annotation process. We provide an in-depth analysis of the structure and contents of our resulting dataset, which, as of October 10, 2016, contains 3911 motions with a total duration of 11.23 hours and 6278 annotations in natural language that contain 52,903 words. We believe this makes our dataset an excellent choice that enables more transparent and comparable research in this important area.



### Deep Structured-Output Regression Learning for Computational Color Constancy
- **Arxiv ID**: http://arxiv.org/abs/1607.03856v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.03856v2)
- **Published**: 2016-07-13 18:36:35+00:00
- **Updated**: 2016-08-11 17:32:22+00:00
- **Authors**: Yanlin Qian, Ke Chen, Joni-Kristian Kamarainen, Jarno Nikkanen, Jiri Matas
- **Comment**: None
- **Journal**: None
- **Summary**: Computational color constancy that requires esti- mation of illuminant colors of images is a fundamental yet active problem in computer vision, which can be formulated into a regression problem. To learn a robust regressor for color constancy, obtaining meaningful imagery features and capturing latent correlations across output variables play a vital role. In this work, we introduce a novel deep structured-output regression learning framework to achieve both goals simultaneously. By borrowing the power of deep convolutional neural networks (CNN) originally designed for visual recognition, the proposed framework can automatically discover strong features for white balancing over different illumination conditions and learn a multi-output regressor beyond underlying relationships between features and targets to find the complex interdependence of dif- ferent dimensions of target variables. Experiments on two public benchmarks demonstrate that our method achieves competitive performance in comparison with the state-of-the-art approaches.



### Large Scale SfM with the Distributed Camera Model
- **Arxiv ID**: http://arxiv.org/abs/1607.03949v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.03949v2)
- **Published**: 2016-07-13 22:39:11+00:00
- **Updated**: 2016-12-01 02:09:31+00:00
- **Authors**: Chris Sweeney, Victor Fragoso, Tobias Hollerer, Matthew Turk
- **Comment**: Published at 2016 3DV Conference
- **Journal**: None
- **Summary**: We introduce the distributed camera model, a novel model for Structure-from-Motion (SfM). This model describes image observations in terms of light rays with ray origins and directions rather than pixels. As such, the proposed model is capable of describing a single camera or multiple cameras simultaneously as the collection of all light rays observed. We show how the distributed camera model is a generalization of the standard camera model and describe a general formulation and solution to the absolute camera pose problem that works for standard or distributed cameras. The proposed method computes a solution that is up to 8 times more efficient and robust to rotation singularities in comparison with gDLS. Finally, this method is used in an novel large-scale incremental SfM pipeline where distributed cameras are accurately and robustly merged together. This pipeline is a direct generalization of traditional incremental SfM; however, instead of incrementally adding one camera at a time to grow the reconstruction the reconstruction is grown by adding a distributed camera. Our pipeline produces highly accurate reconstructions efficiently by avoiding the need for many bundle adjustment iterations and is capable of computing a 3D model of Rome from over 15,000 images in just 22 minutes.



### Deleting and Testing Forbidden Patterns in Multi-Dimensional Arrays
- **Arxiv ID**: http://arxiv.org/abs/1607.03961v3
- **DOI**: None
- **Categories**: **cs.DS**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1607.03961v3)
- **Published**: 2016-07-13 23:55:56+00:00
- **Updated**: 2017-03-26 12:43:59+00:00
- **Authors**: Omri Ben-Eliezer, Simon Korman, Daniel Reichman
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding the local behaviour of structured multi-dimensional data is a fundamental problem in various areas of computer science. As the amount of data is often huge, it is desirable to obtain sublinear time algorithms, and specifically property testers, to understand local properties of the data.   We focus on the natural local problem of testing pattern freeness: given a large $d$-dimensional array $A$ and a fixed $d$-dimensional pattern $P$ over a finite alphabet, we say that $A$ is $P$-free if it does not contain a copy of the forbidden pattern $P$ as a consecutive subarray. The distance of $A$ to $P$-freeness is the fraction of entries of $A$ that need to be modified to make it $P$-free. For any $\epsilon \in [0,1]$ and any large enough pattern $P$ over any alphabet, other than a very small set of exceptional patterns, we design a tolerant tester that distinguishes between the case that the distance is at least $\epsilon$ and the case that it is at most $a_d \epsilon$, with query complexity and running time $c_d \epsilon^{-1}$, where $a_d < 1$ and $c_d$ depend only on $d$.   To analyze the testers we establish several combinatorial results, including the following $d$-dimensional modification lemma, which might be of independent interest: for any large enough pattern $P$ over any alphabet (excluding a small set of exceptional patterns for the binary case), and any array $A$ containing a copy of $P$, one can delete this copy by modifying one of its locations without creating new $P$-copies in $A$.   Our results address an open question of Fischer and Newman, who asked whether there exist efficient testers for properties related to tight substructures in multi-dimensional structured data. They serve as a first step towards a general understanding of local properties of multi-dimensional arrays, as any such property can be characterized by a fixed family of forbidden patterns.



