# Arxiv Papers in cs.CV on 2016-07-07
### Iterative Multi-domain Regularized Deep Learning for Anatomical Structure Detection and Segmentation from Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/1607.01855v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.01855v1)
- **Published**: 2016-07-07 02:21:25+00:00
- **Updated**: 2016-07-07 02:21:25+00:00
- **Authors**: Hao Chen, Yefeng Zheng, Jin-Hyeong Park, Pheng-Ann Heng, S. Kevin Zhou
- **Comment**: MICCAI 2016
- **Journal**: None
- **Summary**: Accurate detection and segmentation of anatomical structures from ultrasound images are crucial for clinical diagnosis and biometric measurements. Although ultrasound imaging has been widely used with superiorities such as low cost and portability, the fuzzy border definition and existence of abounding artifacts pose great challenges for automatically detecting and segmenting the complex anatomical structures. In this paper, we propose a multi-domain regularized deep learning method to address this challenging problem. By leveraging the transfer learning from cross domains, the feature representations are effectively enhanced. The results are further improved by the iterative refinement. Moreover, our method is quite efficient by taking advantage of a fully convolutional network, which is formulated as an end-to-end learning framework of detection and segmentation. Extensive experimental results on a large-scale database corroborated that our method achieved a superior detection and segmentation accuracy, outperforming other methods by a significant margin and demonstrating competitive capability even compared to human performance.



### Random Walk Graph Laplacian based Smoothness Prior for Soft Decoding of JPEG Images
- **Arxiv ID**: http://arxiv.org/abs/1607.01895v1
- **DOI**: 10.1109/TIP.2016.2627807
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.01895v1)
- **Published**: 2016-07-07 07:31:22+00:00
- **Updated**: 2016-07-07 07:31:22+00:00
- **Authors**: Xianming Liu, Gene Cheung, Xiaolin Wu, Debin Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Given the prevalence of JPEG compressed images, optimizing image reconstruction from the compressed format remains an important problem. Instead of simply reconstructing a pixel block from the centers of indexed DCT coefficient quantization bins (hard decoding), soft decoding reconstructs a block by selecting appropriate coefficient values within the indexed bins with the help of signal priors. The challenge thus lies in how to define suitable priors and apply them effectively.   In this paper, we combine three image priors---Laplacian prior for DCT coefficients, sparsity prior and graph-signal smoothness prior for image patches---to construct an efficient JPEG soft decoding algorithm. Specifically, we first use the Laplacian prior to compute a minimum mean square error (MMSE) initial solution for each code block. Next, we show that while the sparsity prior can reduce block artifacts, limiting the size of the over-complete dictionary (to lower computation) would lead to poor recovery of high DCT frequencies. To alleviate this problem, we design a new graph-signal smoothness prior (desired signal has mainly low graph frequencies) based on the left eigenvectors of the random walk graph Laplacian matrix (LERaG). Compared to previous graph-signal smoothness priors, LERaG has desirable image filtering properties with low computation overhead. We demonstrate how LERaG can facilitate recovery of high DCT frequencies of a piecewise smooth (PWS) signal via an interpretation of low graph frequency components as relaxed solutions to normalized cut in spectral clustering. Finally, we construct a soft decoding algorithm using the three signal priors with appropriate prior weights. Experimental results show that our proposal outperforms state-of-the-art soft decoding algorithms in both objective and subjective evaluations noticeably.



### Superimposition of eye fundus images for longitudinal analysis from large public health databases
- **Arxiv ID**: http://arxiv.org/abs/1607.01971v3
- **DOI**: 10.1088/2057-1976/aa7d16
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.01971v3)
- **Published**: 2016-07-07 11:43:24+00:00
- **Updated**: 2018-07-18 13:07:32+00:00
- **Authors**: Guillaume Noyel, Rebecca Thomas, Gavin Bhakta, Andrew Crowder, David Owens, Peter Boyle
- **Comment**: This is an author-created, un-copyedited version of an article
  published in Biomedical Physics \& Engineering Express. IOP Publishing Ltd is
  not responsible for any errors or omissions in this version of the manuscript
  or any version derived from it. The Version of Record is available online at
  https://doi.org/10.1088/2057-1976/aa7d16
- **Journal**: Biomedical Physics \& Engineering Express, 2017, 3 (4)
- **Summary**: In this paper, a method is presented for superimposition (i.e. registration) of eye fundus images from persons with diabetes screened over many years for diabetic retinopathy. The method is fully automatic and robust to camera changes and colour variations across the images both in space and time. All the stages of the process are designed for longitudinal analysis of cohort public health databases where retinal examinations are made at approximately yearly intervals. The method relies on a model correcting two radial distortions and an affine transformation between pairs of images which is robustly fitted on salient points. Each stage involves linear estimators followed by non-linear optimisation. The model of image warping is also invertible for fast computation. The method has been validated (1) on a simulated montage and (2) on public health databases with 69 patients with high quality images (271 pairs acquired mostly with different types of camera and 268 pairs acquired mostly with the same type of camera) with success rates of 92% and 98%, and five patients (20 pairs) with low quality images with a success rate of 100%. Compared to two state-of-the-art methods, ours gives better results.



### Deep Depth Super-Resolution : Learning Depth Super-Resolution using Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1607.01977v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.01977v1)
- **Published**: 2016-07-07 12:01:59+00:00
- **Updated**: 2016-07-07 12:01:59+00:00
- **Authors**: Xibin Song, Yuchao Dai, Xueying Qin
- **Comment**: 13 pages, 10 figures
- **Journal**: None
- **Summary**: Depth image super-resolution is an extremely challenging task due to the information loss in sub-sampling. Deep convolutional neural network have been widely applied to color image super-resolution. Quite surprisingly, this success has not been matched to depth super-resolution. This is mainly due to the inherent difference between color and depth images. In this paper, we bridge up the gap and extend the success of deep convolutional neural network to depth super-resolution. The proposed deep depth super-resolution method learns the mapping from a low-resolution depth image to a high resolution one in an end-to-end style. Furthermore, to better regularize the learned depth map, we propose to exploit the depth field statistics and the local correlation between depth image and color image. These priors are integrated in an energy minimization formulation, where the deep neural network learns the unary term, the depth field statistics works as global model constraint and the color-depth correlation is utilized to enforce the local structure in depth images. Extensive experiments on various depth super-resolution benchmark datasets show that our method outperforms the state-of-the-art depth image super-resolution methods with a margin.



### Untrimmed Video Classification for Activity Detection: submission to ActivityNet Challenge
- **Arxiv ID**: http://arxiv.org/abs/1607.01979v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.01979v2)
- **Published**: 2016-07-07 12:08:04+00:00
- **Updated**: 2016-07-12 22:53:46+00:00
- **Authors**: Gurkirt Singh, Fabio Cuzzolin
- **Comment**: 3 pages, Presented at ActivityNet Large Scale Activity Recognition
  Challenge workshop at CVPR 2016, Second position in ActivityNet Detection
  challenge 2016
- **Journal**: None
- **Summary**: Current state-of-the-art human activity recognition is focused on the classification of temporally trimmed videos in which only one action occurs per frame. We propose a simple, yet effective, method for the temporal detection of activities in temporally untrimmed videos with the help of untrimmed classification. Firstly, our model predicts the top k labels for each untrimmed video by analysing global video-level features. Secondly, frame-level binary classification is combined with dynamic programming to generate the temporally trimmed activity proposals. Finally, each proposal is assigned a label based on the global label, and scored with the score of the temporal activity proposal and the global score. Ultimately, we show that untrimmed video classification models can be used as stepping stone for temporal detection.



### Tubelets: Unsupervised action proposals from spatiotemporal super-voxels
- **Arxiv ID**: http://arxiv.org/abs/1607.02003v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.02003v1)
- **Published**: 2016-07-07 13:30:17+00:00
- **Updated**: 2016-07-07 13:30:17+00:00
- **Authors**: Mihir Jain, Jan van Gemert, Hervé Jégou, Patrick Bouthemy, Cees G. M. Snoek
- **Comment**: submitted to International Journal of Computer Vision
- **Journal**: None
- **Summary**: This paper considers the problem of localizing actions in videos as a sequences of bounding boxes. The objective is to generate action proposals that are likely to include the action of interest, ideally achieving high recall with few proposals. Our contributions are threefold. First, inspired by selective search for object proposals, we introduce an approach to generate action proposals from spatiotemporal super-voxels in an unsupervised manner, we call them Tubelets. Second, along with the static features from individual frames our approach advantageously exploits motion. We introduce independent motion evidence as a feature to characterize how the action deviates from the background and explicitly incorporate such motion information in various stages of the proposal generation. Finally, we introduce spatiotemporal refinement of Tubelets, for more precise localization of actions, and pruning to keep the number of Tubelets limited. We demonstrate the suitability of our approach by extensive experiments for action proposal quality and action localization on three public datasets: UCF Sports, MSR-II and UCF101. For action proposal quality, our unsupervised proposals beat all other existing approaches on the three datasets. For action localization, we show top performance on both the trimmed videos of UCF Sports and UCF101 as well as the untrimmed videos of MSR-II.



### MoCap-guided Data Augmentation for 3D Pose Estimation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1607.02046v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.02046v2)
- **Published**: 2016-07-07 15:30:05+00:00
- **Updated**: 2016-10-28 12:43:51+00:00
- **Authors**: Grégory Rogez, Cordelia Schmid
- **Comment**: 9 pages, accepted to appear in NIPS 2016
- **Journal**: None
- **Summary**: This paper addresses the problem of 3D human pose estimation in the wild. A significant challenge is the lack of training data, i.e., 2D images of humans annotated with 3D poses. Such data is necessary to train state-of-the-art CNN architectures. Here, we propose a solution to generate a large set of photorealistic synthetic images of humans with 3D pose annotations. We introduce an image-based synthesis engine that artificially augments a dataset of real images with 2D human pose annotations using 3D Motion Capture (MoCap) data. Given a candidate 3D pose our algorithm selects for each joint an image whose 2D pose locally matches the projected 3D pose. The selected images are then combined to generate a new synthetic image by stitching local image patches in a kinematically constrained manner. The resulting images are used to train an end-to-end CNN for full-body 3D pose estimation. We cluster the training data into a large number of pose classes and tackle pose estimation as a K-way classification problem. Such an approach is viable only with large training sets such as ours. Our method outperforms the state of the art in terms of 3D pose estimation in controlled environments (Human3.6M) and shows promising results for in-the-wild images (LSP). This demonstrates that CNNs trained on artificial images generalize well to real images.



### Zero-Shot Visual Recognition via Bidirectional Latent Embedding
- **Arxiv ID**: http://arxiv.org/abs/1607.02104v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.02104v4)
- **Published**: 2016-07-07 17:48:21+00:00
- **Updated**: 2017-06-02 17:18:27+00:00
- **Authors**: Qian Wang, Ke Chen
- **Comment**: Technical report, School of Computer Science, The University of
  Manchester. Accepted by IJCV
- **Journal**: None
- **Summary**: Zero-shot learning for visual recognition, e.g., object and action recognition, has recently attracted a lot of attention. However, it still remains challenging in bridging the semantic gap between visual features and their underlying semantics and transferring knowledge to semantic categories unseen during learning. Unlike most of the existing zero-shot visual recognition methods, we propose a stagewise bidirectional latent embedding framework to two subsequent learning stages for zero-shot visual recognition. In the bottom-up stage, a latent embedding space is first created by exploring the topological and labeling information underlying training data of known classes via a proper supervised subspace learning algorithm and the latent embedding of training data are used to form landmarks that guide embedding semantics underlying unseen classes into this learned latent space. In the top-down stage, semantic representations of unseen-class labels in a given label vocabulary are then embedded to the same latent space to preserve the semantic relatedness between all different classes via our proposed semi-supervised Sammon mapping with the guidance of landmarks. Thus, the resultant latent embedding space allows for predicting the label of a test instance with a simple nearest-neighbor rule. To evaluate the effectiveness of the proposed framework, we have conducted extensive experiments on four benchmark datasets in object and action recognition, i.e., AwA, CUB-200-2011, UCF101 and HMDB51. The experimental results under comparative studies demonstrate that our proposed approach yields the state-of-the-art performance under inductive and transductive settings.



### Persistent Homology on Grassmann Manifolds for Analysis of Hyperspectral Movies
- **Arxiv ID**: http://arxiv.org/abs/1607.02196v2
- **DOI**: 10.1007/978-3-319-39441-1_21
- **Categories**: **cs.CV**, cs.CG, math.AT
- **Links**: [PDF](http://arxiv.org/pdf/1607.02196v2)
- **Published**: 2016-07-07 23:39:29+00:00
- **Updated**: 2016-07-11 13:53:15+00:00
- **Authors**: Sofya Chepushtanova, Michael Kirby, Chris Peterson, Lori Ziegelmeier
- **Comment**: version 2: typos correction
- **Journal**: Computational Topology in Image Context, Volume 9667 of the series
  Lecture Notes in Computer Science, pp. 228-239, June 2016
- **Summary**: The existence of characteristic structure, or shape, in complex data sets has been recognized as increasingly important for mathematical data analysis. This realization has motivated the development of new tools such as persistent homology for exploring topological invariants, or features, in large data sets. In this paper we apply persistent homology to the characterization of gas plumes in time dependent sequences of hyperspectral cubes, i.e. the analysis of 4-way arrays. We investigate hyperspectral movies of Long-Wavelength Infrared data monitoring an experimental release of chemical simulant into the air. Our approach models regions of interest within the hyperspectral data cubes as points on the real Grassmann manifold $G(k, n)$ (whose points parameterize the $k$-dimensional subspaces of $\mathbb{R}^n$), contrasting our approach with the more standard framework in Euclidean space. An advantage of this approach is that it allows a sequence of time slices in a hyperspectral movie to be collapsed to a sequence of points in such a way that some of the key structure within and between the slices is encoded by the points on the Grassmann manifold. This motivates the search for topological features, associated with the evolution of the frames of a hyperspectral movie, within the corresponding points on the Grassmann manifold. The proposed mathematical model affords the processing of large data sets while retaining valuable discriminatory information. In this paper, we discuss how embedding our data in the Grassmann manifold, together with topological data analysis, captures dynamical events that occur as the chemical plume is released and evolves.



