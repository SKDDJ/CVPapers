# Arxiv Papers in cs.CV on 2016-07-12
### Gland Instance Segmentation by Deep Multichannel Side Supervision
- **Arxiv ID**: http://arxiv.org/abs/1607.03222v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.03222v2)
- **Published**: 2016-07-12 03:08:43+00:00
- **Updated**: 2016-07-14 03:01:31+00:00
- **Authors**: Yan Xu, Yang Li, Mingyuan Liu, Yipei Wang, Maode Lai, Eric I-Chao Chang
- **Comment**: conditionally accepted at MICCAI 2016
- **Journal**: None
- **Summary**: In this paper, we propose a new image instance segmentation method that segments individual glands (instances) in colon histology images. This is a task called instance segmentation that has recently become increasingly important. The problem is challenging since not only do the glands need to be segmented from the complex background, they are also required to be individually identified. Here we leverage the idea of image-to-image prediction in recent deep learning by building a framework that automatically exploits and fuses complex multichannel information, regional and boundary patterns, with side supervision (deep supervision on side responses) in gland histology images. Our proposed system, deep multichannel side supervision (DMCS), alleviates heavy feature design due to the use of convolutional neural networks guided by side supervision. Compared to methods reported in the 2015 MICCAI Gland Segmentation Challenge, we observe state-of-the-art results based on a number of evaluation metrics.



### Local feature hierarchy for face recognition across pose and illumination
- **Arxiv ID**: http://arxiv.org/abs/1607.03226v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.03226v1)
- **Published**: 2016-07-12 03:52:30+00:00
- **Updated**: 2016-07-12 03:52:30+00:00
- **Authors**: Xiaoyue Jiang, Dong Zhang, Xiaoyi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Even though face recognition in frontal view and normal lighting condition works very well, the performance degenerates sharply in extreme conditions. Recently there are many work dealing with pose and illumination problems, respectively. However both the lighting and pose variation will always be encountered at the same time. Accordingly we propose an end-to-end face recognition method to deal with pose and illumination simultaneously based on convolutional networks where the discriminative nonlinear features that are invariant to pose and illumination are extracted. Normally the global structure for images taken in different views is quite diverse. Therefore we propose to use the 1*1 convolutional kernel to extract the local features. Furthermore the parallel multi-stream multi-layer 1*1 convolution network is developed to extract multi-hierarchy features. In the experiments we obtained the average face recognition rate of 96.9% on multiPIE dataset,which improves the state-of-the-art of face recognition across poses and illumination by 7.5%. Especially for profile-wise positions, the average recognition rate of our proposed network is 97.8%, which increases the state-of-the-art recognition rate by 19%.



### Weakly Supervised Learning of Heterogeneous Concepts in Videos
- **Arxiv ID**: http://arxiv.org/abs/1607.03240v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.03240v1)
- **Published**: 2016-07-12 06:49:49+00:00
- **Updated**: 2016-07-12 06:49:49+00:00
- **Authors**: Sohil Shah, Kuldeep Kulkarni, Arijit Biswas, Ankit Gandhi, Om Deshmukh, Larry Davis
- **Comment**: To appear at ECCV 2016
- **Journal**: None
- **Summary**: Typical textual descriptions that accompany online videos are 'weak': i.e., they mention the main concepts in the video but not their corresponding spatio-temporal locations. The concepts in the description are typically heterogeneous (e.g., objects, persons, actions). Certain location constraints on these concepts can also be inferred from the description. The goal of this paper is to present a generalization of the Indian Buffet Process (IBP) that can (a) systematically incorporate heterogeneous concepts in an integrated framework, and (b) enforce location constraints, for efficient classification and localization of the concepts in the videos. Finally, we develop posterior inference for the proposed formulation using mean-field variational approximation. Comparative evaluations on the Casablanca and the A2D datasets show that the proposed approach significantly outperforms other state-of-the-art techniques: 24% relative improvement for pairwise concept classification in the Casablanca dataset and 9% relative improvement for localization in the A2D dataset as compared to the most competitive baseline.



### Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures
- **Arxiv ID**: http://arxiv.org/abs/1607.03250v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1607.03250v1)
- **Published**: 2016-07-12 07:43:01+00:00
- **Updated**: 2016-07-12 07:43:01+00:00
- **Authors**: Hengyuan Hu, Rui Peng, Yu-Wing Tai, Chi-Keung Tang
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art neural networks are getting deeper and wider. While their performance increases with the increasing number of layers and neurons, it is crucial to design an efficient deep architecture in order to reduce computational and memory costs. Designing an efficient neural network, however, is labor intensive requiring many experiments, and fine-tunings. In this paper, we introduce network trimming which iteratively optimizes the network by pruning unimportant neurons based on analysis of their outputs on a large dataset. Our algorithm is inspired by an observation that the outputs of a significant portion of neurons in a large network are mostly zero, regardless of what inputs the network received. These zero activation neurons are redundant, and can be removed without affecting the overall accuracy of the network. After pruning the zero activation neurons, we retrain the network using the weights before pruning as initialization. We alternate the pruning and retraining to further reduce zero activations in a network. Our experiments on the LeNet and VGG-16 show that we can achieve high compression ratio of parameters without losing or even achieving higher accuracy than the original network.



### A Variational Model for Joint Motion Estimation and Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1607.03255v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, math.OC, 68U10, 65K10, 65M06
- **Links**: [PDF](http://arxiv.org/pdf/1607.03255v1)
- **Published**: 2016-07-12 08:29:40+00:00
- **Updated**: 2016-07-12 08:29:40+00:00
- **Authors**: Martin Burger, Hendrik Dirks, Carola-Bibiane Sch√∂nlieb
- **Comment**: None
- **Journal**: None
- **Summary**: The aim of this paper is to derive and analyze a variational model for the joint estimation of motion and reconstruction of image sequences, which is based on a time-continuous Eulerian motion model. The model can be set up in terms of the continuity equation or the brightness constancy equation. The analysis in this paper focuses on the latter for robust motion estimation on sequences of two-dimensional images. We rigorously prove the existence of a minimizer in a suitable function space setting. Moreover, we discuss the numerical solution of the model based on primal-dual algorithms and investigate several examples. Finally, the benefits of our model compared to existing techniques, such as sequential image reconstruction and motion estimation, are shown.



### City-Identification of Flickr Videos Using Semantic Acoustic Features
- **Arxiv ID**: http://arxiv.org/abs/1607.03257v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1607.03257v1)
- **Published**: 2016-07-12 08:30:45+00:00
- **Updated**: 2016-07-12 08:30:45+00:00
- **Authors**: Benjamin Elizalde, Guan-Lin Chao, Ming Zeng, Ian Lane
- **Comment**: None
- **Journal**: None
- **Summary**: City-identification of videos aims to determine the likelihood of a video belonging to a set of cities. In this paper, we present an approach using only audio, thus we do not use any additional modality such as images, user-tags or geo-tags. In this manner, we show to what extent the city-location of videos correlates to their acoustic information. Success in this task suggests improvements can be made to complement the other modalities. In particular, we present a method to compute and use semantic acoustic features to perform city-identification and the features show semantic evidence of the identification. The semantic evidence is given by a taxonomy of urban sounds and expresses the potential presence of these sounds in the city- soundtracks. We used the MediaEval Placing Task set, which contains Flickr videos labeled by city. In addition, we used the UrbanSound8K set containing audio clips labeled by sound- type. Our method improved the state-of-the-art performance and provides a novel semantic approach to this task



### A Machine learning approach for Shape From Shading
- **Arxiv ID**: http://arxiv.org/abs/1607.03284v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.03284v1)
- **Published**: 2016-07-12 09:37:00+00:00
- **Updated**: 2016-07-12 09:37:00+00:00
- **Authors**: Lyes Abada, Saliha Aouat
- **Comment**: 2nd International Conference on Signal, Image, Vision and their
  Applications (SIVA'13), November 18-20, 2013 - Guelma, Algeria
- **Journal**: None
- **Summary**: The aim of Shape From Shading (SFS) problem is to reconstruct the relief of an object from a single gray level image. In this paper we present a new method to solve the problem of SFS using Machine learning method. Our approach belongs to Local resolution category. The orientation of each part of the object is represented by the perpendicular vector to the surface (Normal Vector), this vector is defined by two angles SLANT and TILT, such as the TILT is the angle between the normal vector and Z-axis, and the SLANT is the angle between the the X-axis and the projection of the normal to the plane. The TILT can be determined from the gray level, the unknown is the SLANT. To calculate the normal of each part of the surface (pixel) a supervised Machine learning method has been proposed. This method divided into three steps: the first step is the preparation of the training data from 3D mathematical functions and synthetic objects. The second step is the creation of database of examples from 3D objects (off-line process). The third step is the application of test images (on-line process). The idea is to find for each pixel of the test image the most similar element in the examples database using a similarity value.



### Boundary conditions for Shape from Shading
- **Arxiv ID**: http://arxiv.org/abs/1607.03289v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.03289v1)
- **Published**: 2016-07-12 09:54:05+00:00
- **Updated**: 2016-07-12 09:54:05+00:00
- **Authors**: Lyes Abada, Saliha Aouat, Omar el farouk Bourahla
- **Comment**: International Conference on Pattern Analysis and Intelligent Systems
  (PAIS'15), October.26-27, 2015 - Tebessa, Algeria
- **Journal**: None
- **Summary**: The Shape From Shading is one of a computer vision field. It studies the 3D reconstruction of an object from a single grayscale image. The difficulty of this field can be expressed in the local ambiguity (convex / concave). J.Shi and Q.Zhu have proposed a method (Global View) to solve the local ambiguity. This method based on the graph theory and the relationship between the singular points. In this work we will show that the use of singular points is not sufficient and requires further information on the object to resolve this ambiguity.



### Camera Elevation Estimation from a Single Mountain Landscape Photograph
- **Arxiv ID**: http://arxiv.org/abs/1607.03305v1
- **DOI**: 10.5244/C.29.30
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.03305v1)
- **Published**: 2016-07-12 10:47:51+00:00
- **Updated**: 2016-07-12 10:47:51+00:00
- **Authors**: Martin Cadik, Jan Vasicek, Michal Hradis, Filip Radenovic, Ondrej Chum
- **Comment**: None
- **Journal**: In Xianghua Xie, Mark W. Jones, and Gary K. L. Tam, editors,
  Proceedings of the British Machine Vision Conference (BMVC), pages
  30.1-30.12. BMVA Press, September 2015
- **Summary**: This work addresses the problem of camera elevation estimation from a single photograph in an outdoor environment. We introduce a new benchmark dataset of one-hundred thousand images with annotated camera elevation called Alps100K. We propose and experimentally evaluate two automatic data-driven approaches to camera elevation estimation: one based on convolutional neural networks, the other on local features. To compare the proposed methods to human performance, an experiment with 100 subjects is conducted. The experimental results show that both proposed approaches outperform humans and that the best result is achieved by their combination.



### RGBD Salient Object Detection via Deep Fusion
- **Arxiv ID**: http://arxiv.org/abs/1607.03333v1
- **DOI**: 10.1109/TIP.2017.2682981
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.03333v1)
- **Published**: 2016-07-12 12:32:56+00:00
- **Updated**: 2016-07-12 12:32:56+00:00
- **Authors**: Liangqiong Qu, Shengfeng He, Jiawei Zhang, Jiandong Tian, Yandong Tang, Qingxiong Yang
- **Comment**: This paper has been submitted to IEEE Transactions on Image
  Processing
- **Journal**: None
- **Summary**: Numerous efforts have been made to design different low level saliency cues for the RGBD saliency detection, such as color or depth contrast features, background and color compactness priors. However, how these saliency cues interact with each other and how to incorporate these low level saliency cues effectively to generate a master saliency map remain a challenging problem. In this paper, we design a new convolutional neural network (CNN) to fuse different low level saliency cues into hierarchical features for automatically detecting salient objects in RGBD images. In contrast to the existing works that directly feed raw image pixels to the CNN, the proposed method takes advantage of the knowledge in traditional saliency detection by adopting various meaningful and well-designed saliency feature vectors as input. This can guide the training of CNN towards detecting salient object more effectively due to the reduced learning ambiguity. We then integrate a Laplacian propagation framework with the learned CNN to extract a spatially consistent saliency map by exploiting the intrinsic structure of the input image. Extensive quantitative and qualitative experimental evaluations on three datasets demonstrate that the proposed method consistently outperforms state-of-the-art methods.



### DeepBinaryMask: Learning a Binary Mask for Video Compressive Sensing
- **Arxiv ID**: http://arxiv.org/abs/1607.03343v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1607.03343v2)
- **Published**: 2016-07-12 13:14:02+00:00
- **Updated**: 2016-07-18 17:21:36+00:00
- **Authors**: Michael Iliadis, Leonidas Spinoulas, Aggelos K. Katsaggelos
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel encoder-decoder neural network model referred to as DeepBinaryMask for video compressive sensing. In video compressive sensing one frame is acquired using a set of coded masks (sensing matrix) from which a number of video frames is reconstructed, equal to the number of coded masks. The proposed framework is an end-to-end model where the sensing matrix is trained along with the video reconstruction. The encoder learns the binary elements of the sensing matrix and the decoder is trained to recover the unknown video sequence. The reconstruction performance is found to improve when using the trained sensing mask from the network as compared to other mask designs such as random, across a wide variety of compressive sensing reconstruction algorithms. Finally, our analysis and discussion offers insights into understanding the characteristics of the trained mask designs that lead to the improved reconstruction quality.



### Multi-modal image retrieval with random walk on multi-layer graphs
- **Arxiv ID**: http://arxiv.org/abs/1607.03406v1
- **DOI**: 10.1109/ISM.2016.0011
- **Categories**: **cs.IR**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1607.03406v1)
- **Published**: 2016-07-12 15:35:01+00:00
- **Updated**: 2016-07-12 15:35:01+00:00
- **Authors**: Renata Khasanova, Xiaowen Dong, Pascal Frossard
- **Comment**: None
- **Journal**: None
- **Summary**: The analysis of large collections of image data is still a challenging problem due to the difficulty of capturing the true concepts in visual data. The similarity between images could be computed using different and possibly multimodal features such as color or edge information or even text labels. This motivates the design of image analysis solutions that are able to effectively integrate the multi-view information provided by different feature sets. We therefore propose a new image retrieval solution that is able to sort images through a random walk on a multi-layer graph, where each layer corresponds to a different type of information about the image data. We study in depth the design of the image graph and propose in particular an effective method to select the edge weights for the multi-layer graph, such that the image ranking scores are optimised. We then provide extensive experiments in different real-world photo collections, which confirm the high performance of our new image retrieval algorithm that generally surpasses state-of-the-art solutions due to a more meaningful image similarity computation.



### Bayesian Inference of Bijective Non-Rigid Shape Correspondence
- **Arxiv ID**: http://arxiv.org/abs/1607.03425v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.03425v1)
- **Published**: 2016-07-12 16:04:54+00:00
- **Updated**: 2016-07-12 16:04:54+00:00
- **Authors**: Matthias Vestner, Roee Litman, Alex Bronstein, Emanuele Rodol√†, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: Many algorithms for the computation of correspondences between deformable shapes rely on some variant of nearest neighbor matching in a descriptor space. Such are, for example, various point-wise correspondence recovery algorithms used as a postprocessing stage in the functional correspondence framework. In this paper, we show that such frequently used techniques in practice suffer from lack of accuracy and result in poor surjectivity. We propose an alternative recovery technique guaranteeing a bijective correspondence and producing significantly higher accuracy. We derive the proposed method from a statistical framework of Bayesian inference and demonstrate its performance on several challenging deformable 3D shape matching datasets.



### DNA Image Pro -- A Tool for Generating Pixel Patterns using DNA Tile Assembly
- **Arxiv ID**: http://arxiv.org/abs/1607.03434v1
- **DOI**: None
- **Categories**: **cs.ET**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1607.03434v1)
- **Published**: 2016-07-12 16:46:25+00:00
- **Updated**: 2016-07-12 16:46:25+00:00
- **Authors**: Dixita Limbachiya, Dhaval Trivedi, Manish K Gupta
- **Comment**: 14 pages, draft
- **Journal**: None
- **Summary**: Self-assembly is a process found everywhere in the Nature. In particular, it is known that DNA self-assembly is Turing universal. Thus one can do arbitrary computations or build nano-structures using DNA self-assembly. In order to understand the DNA self-assembly process, many mathematical models have been proposed in the literature. In particular, abstract Tile Assembly Model (aTAM) received much attention. In this work, we investigate pixel pattern generation using aTAM. For a given image, a tile assembly system is given which can generate the image by self-assembly process. We also consider image blocks with specific cyclic pixel patterns (uniform shift and non uniform shift) self assembly. A software, DNA Image Pro, for generating pixel patterns using DNA tile assembly is also given.



### Hierarchical Multi-resolution Mesh Networks for Brain Decoding
- **Arxiv ID**: http://arxiv.org/abs/1607.07695v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1607.07695v2)
- **Published**: 2016-07-12 17:26:31+00:00
- **Updated**: 2017-01-11 20:42:47+00:00
- **Authors**: Itir Onal Ertugrul, Mete Ozay, Fatos Tunay Yarman Vural
- **Comment**: 18 pages
- **Journal**: None
- **Summary**: We propose a new framework, called Hierarchical Multi-resolution Mesh Networks (HMMNs), which establishes a set of brain networks at multiple time resolutions of fMRI signal to represent the underlying cognitive process. The suggested framework, first, decomposes the fMRI signal into various frequency subbands using wavelet transforms. Then, a brain network, called mesh network, is formed at each subband by ensembling a set of local meshes. The locality around each anatomic region is defined with respect to a neighborhood system based on functional connectivity. The arc weights of a mesh are estimated by ridge regression formed among the average region time series. In the final step, the adjacency matrices of mesh networks obtained at different subbands are ensembled for brain decoding under a hierarchical learning architecture, called, fuzzy stacked generalization (FSG). Our results on Human Connectome Project task-fMRI dataset reflect that the suggested HMMN model can successfully discriminate tasks by extracting complementary information obtained from mesh arc weights of multiple subbands. We study the topological properties of the mesh networks at different resolutions using the network measures, namely, node degree, node strength, betweenness centrality and global efficiency; and investigate the connectivity of anatomic regions, during a cognitive task. We observe significant variations among the network topologies obtained for different subbands. We, also, analyze the diversity properties of classifier ensemble, trained by the mesh networks in multiple subbands and observe that the classifiers in the ensemble collaborate with each other to fuse the complementary information freed at each subband. We conclude that the fMRI data, recorded during a cognitive task, embed diverse information across the anatomic regions at each resolution.



### A Representation Theory Perspective on Simultaneous Alignment and Classification
- **Arxiv ID**: http://arxiv.org/abs/1607.03464v1
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1607.03464v1)
- **Published**: 2016-07-12 18:50:20+00:00
- **Updated**: 2016-07-12 18:50:20+00:00
- **Authors**: Roy R. Lederman, Amit Singer
- **Comment**: None
- **Journal**: None
- **Summary**: One of the difficulties in 3D reconstruction of molecules from images in single particle Cryo-Electron Microscopy (Cryo-EM), in addition to high levels of noise and unknown image orientations, is heterogeneity in samples: in many cases, the samples contain a mixture of molecules, or multiple conformations of one molecule. Many algorithms for the reconstruction of molecules from images in heterogeneous Cryo-EM experiments are based on iterative approximations of the molecules in a non-convex optimization that is prone to reaching suboptimal local minima. Other algorithms require an alignment in order to perform classification, or vice versa. The recently introduced Non-Unique Games framework provides a representation theoretic approach to studying problems of alignment over compact groups, and offers convex relaxations for alignment problems which are formulated as semidefinite programs (SDPs) with certificates of global optimality under certain circumstances. In this manuscript, we propose to extend Non-Unique Games to the problem of simultaneous alignment and classification with the goal of simultaneously classifying Cryo-EM images and aligning them within their respective classes. Our proposed approach can also be extended to the case of continuous heterogeneity.



### Event-based, 6-DOF Camera Tracking from Photometric Depth Maps
- **Arxiv ID**: http://arxiv.org/abs/1607.03468v2
- **DOI**: 10.1109/TPAMI.2017.2769655
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1607.03468v2)
- **Published**: 2016-07-12 19:08:24+00:00
- **Updated**: 2017-10-31 18:00:23+00:00
- **Authors**: Guillermo Gallego, Jon E. A. Lund, Elias Mueggler, Henri Rebecq, Tobi Delbruck, Davide Scaramuzza
- **Comment**: 12 pages, 13 figures. 2 tables. (in press)
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  Vol. 40, No. 2, pp. 2402-2412, Oct. 2018
- **Summary**: Event cameras are bio-inspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. These cameras do not suffer from motion blur and have a very high dynamic range, which enables them to provide reliable visual information during high-speed motions or in scenes characterized by high dynamic range. These features, along with a very low power consumption, make event cameras an ideal complement to standard cameras for VR/AR and video game applications. With these applications in mind, this paper tackles the problem of accurate, low-latency tracking of an event camera from an existing photometric depth map (i.e., intensity plus depth information) built via classic dense reconstruction pipelines. Our approach tracks the 6-DOF pose of the event camera upon the arrival of each event, thus virtually eliminating latency. We successfully evaluate the method in both indoor and outdoor scenes and show that---because of the technological advantages of the event camera---our pipeline works in scenes characterized by high-speed motion, which are still unaccessible to standard cameras.



### End-to-end training of object class detectors for mean average precision
- **Arxiv ID**: http://arxiv.org/abs/1607.03476v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1607.03476v2)
- **Published**: 2016-07-12 19:45:12+00:00
- **Updated**: 2017-03-16 13:55:07+00:00
- **Authors**: Paul Henderson, Vittorio Ferrari
- **Comment**: This version has minor additions to results (ablation study) and
  discussion
- **Journal**: None
- **Summary**: We present a method for training CNN-based object class detectors directly using mean average precision (mAP) as the training loss, in a truly end-to-end fashion that includes non-maximum suppression (NMS) at training time. This contrasts with the traditional approach of training a CNN for a window classification loss, then applying NMS only at test time, when mAP is used as the evaluation metric in place of classification accuracy. However, mAP following NMS forms a piecewise-constant structured loss over thousands of windows, with gradients that do not convey useful information for gradient descent. Hence, we define new, general gradient-like quantities for piecewise constant functions, which have wide applicability. We describe how to calculate these efficiently for mAP following NMS, enabling to train a detector based on Fast R-CNN directly for mAP. This model achieves equivalent performance to the standard Fast R-CNN on the PASCAL VOC 2007 and 2012 datasets, while being conceptually more appealing as the very same model and loss are used at both training and test time.



### Deep Reconstruction-Classification Networks for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1607.03516v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1607.03516v2)
- **Published**: 2016-07-12 20:48:58+00:00
- **Updated**: 2016-08-01 09:58:13+00:00
- **Authors**: Muhammad Ghifary, W. Bastiaan Kleijn, Mengjie Zhang, David Balduzzi, Wen Li
- **Comment**: to appear in European Conference on Computer Vision (ECCV) 2016
- **Journal**: None
- **Summary**: In this paper, we propose a novel unsupervised domain adaptation algorithm based on deep learning for visual object recognition. Specifically, we design a new model called Deep Reconstruction-Classification Network (DRCN), which jointly learns a shared encoding representation for two tasks: i) supervised classification of labeled source data, and ii) unsupervised reconstruction of unlabeled target data.In this way, the learnt representation not only preserves discriminability, but also encodes useful information from the target domain. Our new DRCN model can be optimized by using backpropagation similarly as the standard neural networks.   We evaluate the performance of DRCN on a series of cross-domain object recognition tasks, where DRCN provides a considerable improvement (up to ~8% in accuracy) over the prior state-of-the-art algorithms. Interestingly, we also observe that the reconstruction pipeline of DRCN transforms images from the source domain into images whose appearance resembles the target dataset. This suggests that DRCN's performance is due to constructing a single composite representation that encodes information about both the structure of target images and the classification of source images. Finally, we provide a formal analysis to justify the algorithm's objective in domain adaptation context.



### Improved Multi-Class Cost-Sensitive Boosting via Estimation of the Minimum-Risk Class
- **Arxiv ID**: http://arxiv.org/abs/1607.03547v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1607.03547v2)
- **Published**: 2016-07-12 23:56:33+00:00
- **Updated**: 2016-11-15 19:29:30+00:00
- **Authors**: Ron Appel, Xavier Burgos-Artizzu, Pietro Perona
- **Comment**: Project website: http://www.vision.caltech.edu/~appel/projects/REBEL/
- **Journal**: None
- **Summary**: We present a simple unified framework for multi-class cost-sensitive boosting. The minimum-risk class is estimated directly, rather than via an approximation of the posterior distribution. Our method jointly optimizes binary weak learners and their corresponding output vectors, requiring classes to share features at each iteration. By training in a cost-sensitive manner, weak learners are invested in separating classes whose discrimination is important, at the expense of less relevant classification boundaries. Additional contributions are a family of loss functions along with proof that our algorithm is Boostable in the theoretical sense, as well as an efficient procedure for growing decision trees for use as weak learners. We evaluate our method on a variety of datasets: a collection of synthetic planar data, common UCI datasets, MNIST digits, SUN scenes, and CUB-200 birds. Results show state-of-the-art performance across all datasets against several strong baselines, including non-boosting multi-class approaches.



