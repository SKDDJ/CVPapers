# Arxiv Papers in cs.CV on 2016-10-31
### A deep convolutional neural network using directional wavelets for low-dose X-ray CT reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1610.09736v3
- **DOI**: 10.1002/mp.12344
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.09736v3)
- **Published**: 2016-10-31 00:02:56+00:00
- **Updated**: 2017-10-14 14:26:48+00:00
- **Authors**: Eunhee Kang, Junhong Min, Jong Chul Ye
- **Comment**: Will appear in Medical Physics (invited paper); 2016 AAPM low-dose CT
  Grand Challenge 2nd Place Award
- **Journal**: None
- **Summary**: Due to the potential risk of inducing cancers, radiation dose of X-ray CT should be reduced for routine patient scanning. However, in low-dose X-ray CT, severe artifacts usually occur due to photon starvation, beamhardening, etc, which decrease the reliability of diagnosis. Thus, high quality reconstruction from low-dose X-ray CT data has become one of the important research topics in CT community. Conventional model-based denoising approaches are, however, computationally very expensive, and image domain denoising approaches hardly deal with CT specific noise patterns. To address these issues, we propose an algorithm using a deep convolutional neural network (CNN), which is applied to wavelet transform coefficients of low-dose CT images. Specifically, by using a directional wavelet transform for extracting directional component of artifacts and exploiting the intra- and inter-band correlations, our deep network can effectively suppress CT specific noises. Moreover, our CNN is designed to have various types of residual learning architecture for faster network training and better denoising. Experimental results confirm that the proposed algorithm effectively removes complex noise patterns of CT images, originated from the reduced X-ray dose. In addition, we show that wavelet domain CNN is efficient in removing the noises from low-dose CT compared to an image domain CNN. Our results were rigorously evaluated by several radiologists and won the second place award in 2016 AAPM Low-Dose CT Grand Challenge. To the best of our knowledge, this work is the first deep learning architecture for low-dose CT reconstruction that has been rigorously evaluated and proven for its efficacy.



### A New Distance Measure for Non-Identical Data with Application to Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1610.09766v1
- **DOI**: 10.1016/j.patcog.2016.10.018
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.09766v1)
- **Published**: 2016-10-31 03:04:48+00:00
- **Updated**: 2016-10-31 03:04:48+00:00
- **Authors**: Muthukaruppan Swaminathan, Pankaj Kumar Yadav, Obdulio Piloto, Tobias Sj√∂blom, Ian Cheong
- **Comment**: None
- **Journal**: None
- **Summary**: Distance measures are part and parcel of many computer vision algorithms. The underlying assumption in all existing distance measures is that feature elements are independent and identically distributed. However, in real-world settings, data generally originate from heterogeneous sources even if they do possess a common data-generating mechanism. Since these sources are not identically distributed by necessity, the assumption of identical distribution is inappropriate. Here, we use statistical analysis to show that feature elements of local image descriptors are indeed non-identically distributed. To test the effect of omitting the unified distribution assumption, we created a new distance measure called the Poisson-Binomial Radius (PBR). PBR is a bin-to-bin distance which accounts for the dispersion of bin-to-bin information. PBR's performance was evaluated on twelve benchmark data sets covering six different classification and recognition applications: texture, material, leaf, scene, ear biometrics and category-level image classification. Results from these experiments demonstrate that PBR outperforms state-of-the-art distance measures for most of the data sets and achieves comparable performance on the rest, suggesting that accounting for different distributions in distance measures can improve performance in classification and recognition tasks.



### Robust Gait Recognition by Integrating Inertial and RGBD Sensors
- **Arxiv ID**: http://arxiv.org/abs/1610.09816v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.09816v1)
- **Published**: 2016-10-31 08:06:50+00:00
- **Updated**: 2016-10-31 08:06:50+00:00
- **Authors**: Qin Zou, Lihao Ni, Qian Wang, Qingquan Li, Song Wang
- **Comment**: None
- **Journal**: IEEE Transactions on Cybernetics, vol. 48, no. 4, pp. 1136-1150,
  2018
- **Summary**: Gait has been considered as a promising and unique biometric for person identification. Traditionally, gait data are collected using either color sensors, such as a CCD camera, depth sensors, such as a Microsoft Kinect, or inertial sensors, such as an accelerometer. However, a single type of sensors may only capture part of the dynamic gait features and make the gait recognition sensitive to complex covariate conditions, leading to fragile gait-based person identification systems. In this paper, we propose to combine all three types of sensors for gait data collection and gait recognition, which can be used for important identification applications, such as identity recognition to access a restricted building or area. We propose two new algorithms, namely EigenGait and TrajGait, to extract gait features from the inertial data and the RGBD (color and depth) data, respectively. Specifically, EigenGait extracts general gait dynamics from the accelerometer readings in the eigenspace and TrajGait extracts more detailed sub-dynamics by analyzing 3D dense trajectories. Finally, both extracted features are fed into a supervised classifier for gait recognition and person identification. Experiments on 50 subjects, with comparisons to several other state-of-the-art gait-recognition approaches, show that the proposed approach can achieve higher recognition accuracy and robustness.



### Temporal Matrix Completion with Locally Linear Latent Factors for Medical Applications
- **Arxiv ID**: http://arxiv.org/abs/1611.00800v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1611.00800v1)
- **Published**: 2016-10-31 12:02:53+00:00
- **Updated**: 2016-10-31 12:02:53+00:00
- **Authors**: Frodo Kin Sun Chan, Andy J Ma, Pong C Yuen, Terry Cheuk-Fung Yip, Yee-Kit Tse, Vincent Wai-Sun Wong, Grace Lai-Hung Wong
- **Comment**: None
- **Journal**: None
- **Summary**: Regular medical records are useful for medical practitioners to analyze and monitor patient health status especially for those with chronic disease, but such records are usually incomplete due to unpunctuality and absence of patients. In order to resolve the missing data problem over time, tensor-based model is suggested for missing data imputation in recent papers because this approach makes use of low rank tensor assumption for highly correlated data. However, when the time intervals between records are long, the data correlation is not high along temporal direction and such assumption is not valid. To address this problem, we propose to decompose a matrix with missing data into its latent factors. Then, the locally linear constraint is imposed on these factors for matrix completion in this paper. By using a publicly available dataset and two medical datasets collected from hospital, experimental results show that the proposed algorithm achieves the best performance by comparing with the existing methods.



### Joint Large-Scale Motion Estimation and Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1610.09908v1
- **DOI**: None
- **Categories**: **cs.CV**, math.OC, 68U10, 65K10, 65M06, I.4; G.1.6; G.4
- **Links**: [PDF](http://arxiv.org/pdf/1610.09908v1)
- **Published**: 2016-10-31 13:16:38+00:00
- **Updated**: 2016-10-31 13:16:38+00:00
- **Authors**: Hendrik Dirks
- **Comment**: 21 pages, 1 figure
- **Journal**: None
- **Summary**: This article describes the implementation of the joint motion estimation and image reconstruction framework presented by Burger, Dirks and Sch\"onlieb and extends this framework to large-scale motion between consecutive image frames. The variational framework uses displacements between consecutive frames based on the optical flow approach to improve the image reconstruction quality on the one hand and the motion estimation quality on the other. The energy functional consists of a data-fidelity term with a general operator that connects the input sequence to the solution, it has a total variation term for the image sequence and is connected to the underlying flow using an optical flow term. Additional spatial regularity for the flow is modeled by a total variation regularizer for both components of the flow. The numerical minimization is performed in an alternating manner using primal-dual techniques. The resulting schemes are presented as pseudo-code together with a short numerical evaluation.



### A Detailed Rubric for Motion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1610.10033v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.10033v1)
- **Published**: 2016-10-31 17:57:23+00:00
- **Updated**: 2016-10-31 17:57:23+00:00
- **Authors**: Pia Bideau, Erik Learned-Miller
- **Comment**: None
- **Journal**: None
- **Summary**: Motion segmentation is currently an active area of research in computer Vision. The task of comparing different methods of motion segmentation is complicated by the fact that researchers may use subtly different definitions of the problem. Questions such as "Which objects are moving?", "What is background?", and "How can we use motion of the camera to segment objects, whether they are static or moving?" are clearly related to each other, but lead to different algorithms, and imply different versions of the ground truth. This report has two goals. The first is to offer a precise definition of motion segmentation so that the intent of an algorithm is as well-defined as possible. The second is to report on new versions of three previously existing data sets that are compatible with this definition. We hope that this more detailed definition, and the three data sets that go with it, will allow more meaningful comparisons of certain motion segmentation methods.



### ConfocalGN : a minimalistic confocal image simulator
- **Arxiv ID**: http://arxiv.org/abs/1610.10042v2
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1610.10042v2)
- **Published**: 2016-10-31 18:08:54+00:00
- **Updated**: 2016-11-21 13:50:11+00:00
- **Authors**: Serge Dmitrieff, Fran√ßois N√©d√©lec
- **Comment**: None
- **Journal**: None
- **Summary**: SUMMARY : We developed a user-friendly software to generate synthetic confocal microscopy images from a ground truth specified as a 3D bitmap with pixels of arbitrary size. The software can analyze a real confocal stack to derivate noise parameters and will use them directly to generate new images with similar noise characteristics. Such synthetic images can then be used to assert the quality and robustness of an image analysis pipeline, as well as be used to train machine-learning image analysis procedures. We illustrate the approach with closed curves corresponding to the microtubule ring present in blood platelet. AVAILABILITY AND IMPLEMENTATION: ConfocalGN is written in Matlab but does not require any toolbox. The source code is distributed under the GPL 3.0 licence on https://github.com/SergeDmi/ConfocalGN.



### Bi-modal First Impressions Recognition using Temporally Ordered Deep Audio and Stochastic Visual Features
- **Arxiv ID**: http://arxiv.org/abs/1610.10048v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.10048v1)
- **Published**: 2016-10-31 18:21:13+00:00
- **Updated**: 2016-10-31 18:21:13+00:00
- **Authors**: Arulkumar Subramaniam, Vismay Patel, Ashish Mishra, Prashanth Balasubramanian, Anurag Mittal
- **Comment**: to be published in: ECCV 2016 Workshops proceedings (Apparent
  Personality Analysis)
- **Journal**: None
- **Summary**: We propose a novel approach for First Impressions Recognition in terms of the Big Five personality-traits from short videos. The Big Five personality traits is a model to describe human personality using five broad categories: Extraversion, Agreeableness, Conscientiousness, Neuroticism and Openness. We train two bi-modal end-to-end deep neural network architectures using temporally ordered audio and novel stochastic visual features from few frames, without over-fitting. We empirically show that the trained models perform exceptionally well, even after training from a small sub-portions of inputs. Our method is evaluated in ChaLearn LAP 2016 Apparent Personality Analysis (APA) competition using ChaLearn LAP APA2016 dataset and achieved excellent performance.



### Exploiting Spatio-Temporal Structure with Recurrent Winner-Take-All Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.00050v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1611.00050v2)
- **Published**: 2016-10-31 21:16:46+00:00
- **Updated**: 2017-03-15 16:01:43+00:00
- **Authors**: Eder Santana, Matthew Emigh, Pablo Zegers, Jose C Principe
- **Comment**: under review
- **Journal**: None
- **Summary**: We propose a convolutional recurrent neural network, with Winner-Take-All dropout for high dimensional unsupervised feature learning in multi-dimensional time series. We apply the proposedmethod for object recognition with temporal context in videos and obtain better results than comparable methods in the literature, including the Deep Predictive Coding Networks previously proposed by Chalasani and Principe.Our contributions can be summarized as a scalable reinterpretation of the Deep Predictive Coding Networks trained end-to-end with backpropagation through time, an extension of the previously proposed Winner-Take-All Autoencoders to sequences in time, and a new technique for initializing and regularizing convolutional-recurrent neural networks.



