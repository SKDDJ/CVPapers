# Arxiv Papers in cs.CV on 2016-10-06
### Exploiting Depth from Single Monocular Images for Object Detection and Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1610.01706v1
- **DOI**: 10.1109/TIP.2016.2621673
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.01706v1)
- **Published**: 2016-10-06 01:30:46+00:00
- **Updated**: 2016-10-06 01:30:46+00:00
- **Authors**: Yuanzhouhan Cao, Chunhua Shen, Heng Tao Shen
- **Comment**: 14 pages. Accepted to IEEE T. Image Processing
- **Journal**: None
- **Summary**: Augmenting RGB data with measured depth has been shown to improve the performance of a range of tasks in computer vision including object detection and semantic segmentation. Although depth sensors such as the Microsoft Kinect have facilitated easy acquisition of such depth information, the vast majority of images used in vision tasks do not contain depth information. In this paper, we show that augmenting RGB images with estimated depth can also improve the accuracy of both object detection and semantic segmentation. Specifically, we first exploit the recent success of depth estimation from monocular images and learn a deep depth estimation model. Then we learn deep depth features from the estimated depth and combine with RGB features for object detection and semantic segmentation. Additionally, we propose an RGB-D semantic segmentation method which applies a multi-task training scheme: semantic label prediction and depth value regression. We test our methods on several datasets and demonstrate that incorporating information from estimated depth improves the performance of object detection and semantic segmentation remarkably.



### A Deep Spatial Contextual Long-term Recurrent Convolutional Network for Saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/1610.01708v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.01708v1)
- **Published**: 2016-10-06 01:38:13+00:00
- **Updated**: 2016-10-06 01:38:13+00:00
- **Authors**: Nian Liu, Junwei Han
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional saliency models usually adopt hand-crafted image features and human-designed mechanisms to calculate local or global contrast. In this paper, we propose a novel computational saliency model, i.e., deep spatial contextual long-term recurrent convolutional network (DSCLRCN) to predict where people looks in natural scenes. DSCLRCN first automatically learns saliency related local features on each image location in parallel. Then, in contrast with most other deep network based saliency models which infer saliency in local contexts, DSCLRCN can mimic the cortical lateral inhibition mechanisms in human visual system to incorporate global contexts to assess the saliency of each image location by leveraging the deep spatial long short-term memory (DSLSTM) model. Moreover, we also integrate scene context modulation in DSLSTM for saliency inference, leading to a novel deep spatial contextual LSTM (DSCLSTM) model. The whole network can be trained end-to-end and works efficiently when testing. Experimental results on two benchmark datasets show that DSCLRCN can achieve state-of-the-art performance on saliency detection. Furthermore, the proposed DSCLSTM model can significantly boost the saliency detection performance by incorporating both global spatial interconnections and scene context modulation, which may uncover novel inspirations for studies on them in computational saliency models.



### PCA-aided Fully Convolutional Networks for Semantic Segmentation of Multi-channel fMRI
- **Arxiv ID**: http://arxiv.org/abs/1610.01732v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1610.01732v4)
- **Published**: 2016-10-06 05:08:15+00:00
- **Updated**: 2017-07-11 15:52:08+00:00
- **Authors**: Lei Tai, Haoyang Ye, Qiong Ye, Ming Liu
- **Comment**: ICAR 2017 - 18th International Conference on Advanced Robotics, Best
  Student Paper Award, 6 figures
- **Journal**: None
- **Summary**: Semantic segmentation of functional magnetic resonance imaging (fMRI) makes great sense for pathology diagnosis and decision system of medical robots. The multi-channel fMRI provides more information of the pathological features. But the increased amount of data causes complexity in feature detections. This paper proposes a principal component analysis (PCA)-aided fully convolutional network to particularly deal with multi-channel fMRI. We transfer the learned weights of contemporary classification networks to the segmentation task by fine-tuning. The results of the convolutional network are compared with various methods e.g. k-NN. A new labeling strategy is proposed to solve the semantic segmentation problem with unclear boundaries. Even with a small-sized training dataset, the test results demonstrate that our model outperforms other pathological feature detection methods. Besides, its forward inference only takes 90 milliseconds for a single set of fMRI data. To our knowledge, this is the first time to realize pixel-wise labeling of multi-channel magnetic resonance image using FCN.



### Multiple Regularizations Deep Learning for Paddy Growth Stages Classification from LANDSAT-8
- **Arxiv ID**: http://arxiv.org/abs/1610.01795v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1610.01795v1)
- **Published**: 2016-10-06 09:46:08+00:00
- **Updated**: 2016-10-06 09:46:08+00:00
- **Authors**: Ines Heidieni Ikasari, Vina Ayumi, Mohamad Ivan Fanany, Sidik Mulyono
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: This study uses remote sensing technology that can provide information about the condition of the earth's surface area, fast, and spatially. The study area was in Karawang District, lying in the Northern part of West Java-Indonesia. We address a paddy growth stages classification using LANDSAT 8 image data obtained from multi-sensor remote sensing image taken in October 2015 to August 2016. This study pursues a fast and accurate classification of paddy growth stages by employing multiple regularizations learning on some deep learning methods such as DNN (Deep Neural Networks) and 1-D CNN (1-D Convolutional Neural Networks). The used regularizations are Fast Dropout, Dropout, and Batch Normalization. To evaluate the effectiveness, we also compared our method with other machine learning methods such as (Logistic Regression, SVM, Random Forest, and XGBoost). The data used are seven bands of LANDSAT-8 spectral data samples that correspond to paddy growth stages data obtained from i-Sky (eye in the sky) Innovation system. The growth stages are determined based on paddy crop phenology profile from time series of LANDSAT-8 images. The classification results show that MLP using multiple regularization Dropout and Batch Normalization achieves the highest accuracy for this dataset.



### Searching Scenes by Abstracting Things
- **Arxiv ID**: http://arxiv.org/abs/1610.01801v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.01801v1)
- **Published**: 2016-10-06 10:03:45+00:00
- **Updated**: 2016-10-06 10:03:45+00:00
- **Authors**: Svetlana Kordumova, Jan C. van Gemert, Cees G. M. Snoek, Arnold W. M. Smeulders
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose to represent a scene as an abstraction of 'things'. We start from 'things' as generated by modern object proposals, and we investigate their immediately observable properties: position, size, aspect ratio and color, and those only. Where the recent successes and excitement of the field lie in object identification, we represent the scene composition independent of object identities. We make three contributions in this work. First, we study simple observable properties of 'things', and call it things syntax. Second, we propose translating the things syntax in linguistic abstract statements and study their descriptive effect to retrieve scenes. Thirdly, we propose querying of scenes with abstract block illustrations and study their effectiveness to discriminate among different types of scenes. The benefit of abstract statements and block illustrations is that we generate them directly from the images, without any learning beforehand as in the standard attribute learning. Surprisingly, we show that even though we use the simplest of features from 'things' layout and no learning at all, we can still retrieve scenes reasonably well.



### Do They All Look the Same? Deciphering Chinese, Japanese and Koreans by Fine-Grained Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1610.01854v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.01854v2)
- **Published**: 2016-10-06 13:14:34+00:00
- **Updated**: 2016-10-23 01:26:37+00:00
- **Authors**: Yu Wang, Haofu Liao, Yang Feng, Xiangyang Xu, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: We study to what extend Chinese, Japanese and Korean faces can be classified and which facial attributes offer the most important cues. First, we propose a novel way of obtaining large numbers of facial images with nationality labels. Then we train state-of-the-art neural networks with these labeled images. We are able to achieve an accuracy of 75.03% in the classification task, with chances being 33.33% and human accuracy 38.89% . Further, we train multiple facial attribute classifiers to identify the most distinctive features for each group. We find that Chinese, Japanese and Koreans do exhibit substantial differences in certain attributes, such as bangs, smiling, and bushy eyebrows. Along the way, we uncover several gender-related cross-country patterns as well. Our work, which complements existing APIs such as Microsoft Cognitive Services and Face++, could find potential applications in tourism, e-commerce, social media marketing, criminal justice and even counter-terrorism.



### Distortion Varieties
- **Arxiv ID**: http://arxiv.org/abs/1610.01860v1
- **DOI**: None
- **Categories**: **math.AG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1610.01860v1)
- **Published**: 2016-10-06 13:22:30+00:00
- **Updated**: 2016-10-06 13:22:30+00:00
- **Authors**: Joe Kileel, Zuzana Kukelova, Tomas Pajdla, Bernd Sturmfels
- **Comment**: 25 pages
- **Journal**: None
- **Summary**: The distortion varieties of a given projective variety are parametrized by duplicating coordinates and multiplying them with monomials. We study their degrees and defining equations. Exact formulas are obtained for the case of one-parameter distortions. These are based on Chow polytopes and Gr\"obner bases. Multi-parameter distortions are studied using tropical geometry. The motivation for distortion varieties comes from multi-view geometry in computer vision. Our theory furnishes a new framework for formulating and solving minimal problems for camera models with image distortion.



### Utilizing High-level Visual Feature for Indoor Shopping Mall Navigation
- **Arxiv ID**: http://arxiv.org/abs/1610.01906v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.01906v4)
- **Published**: 2016-10-06 15:14:47+00:00
- **Updated**: 2017-02-18 11:50:28+00:00
- **Authors**: Ziwei Xu, Haitian Zheng, Minjian Pang, Yangchun Zhu, Xiongfei Su, Guyue Zhou, Lu Fang
- **Comment**: None
- **Journal**: None
- **Summary**: Towards robust and convenient indoor shopping mall navigation, we propose a novel learning-based scheme to utilize the high-level visual information from the storefront images captured by personal devices of users. Specifically, we decompose the visual navigation problem into localization and map generation respectively. Given a storefront input image, a novel feature fusion scheme (denoted as FusionNet) is proposed by fusing the distinguishing DNN-based appearance feature and text feature for robust recognition of store brands, which serves for accurate localization. Regarding the map generation, we convert the user-captured indicator map of the shopping mall into a topological map by parsing the stores and their connectivity. Experimental results conducted on the real shopping malls demonstrate that the proposed system achieves robust localization and precise map generation, enabling accurate navigation.



### Metaheuristic Algorithms for Convolution Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1610.01925v1
- **DOI**: 10.1155/2016/1537325
- **Categories**: **cs.CV**, cs.AI, cs.NE, 68Txx, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1610.01925v1)
- **Published**: 2016-10-06 16:11:06+00:00
- **Updated**: 2016-10-06 16:11:06+00:00
- **Authors**: L. M. Rasdi Rere, Mohamad Ivan Fanany, Aniati Murni Arymurthy
- **Comment**: Article ID 1537325, 13 pages. Received 29 January 2016; Revised 15
  April 2016; Accepted 10 May 2016. Academic Editor: Martin Hagan. in Hindawi
  Publishing. Computational Intelligence and Neuroscience Volume 2016 (2016)
- **Journal**: Computational Intelligence and Neuroscience Volume 2016 (2016),
  Article ID 1537325, 13 pages
- **Summary**: A typical modern optimization technique is usually either heuristic or metaheuristic. This technique has managed to solve some optimization problems in the research area of science, engineering, and industry. However, implementation strategy of metaheuristic for accuracy improvement on convolution neural networks (CNN), a famous deep learning method, is still rarely investigated. Deep learning relates to a type of machine learning technique, where its aim is to move closer to the goal of artificial intelligence of creating a machine that could successfully perform any intellectual tasks that can be carried out by a human. In this paper, we propose the implementation strategy of three popular metaheuristic approaches, that is, simulated annealing, differential evolution, and harmony search, to optimize CNN. The performances of these metaheuristic methods in optimizing CNN on classifying MNIST and CIFAR dataset were evaluated and compared. Furthermore, the proposed methods are also compared with the original CNN. Although the proposed methods show an increase in the computation time, their accuracy has also been improved (up to 7.14 percent).



### PetroSurf3D - A Dataset for high-resolution 3D Surface Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1610.01944v3
- **DOI**: 10.1145/3095713.3095719
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.01944v3)
- **Published**: 2016-10-06 16:55:07+00:00
- **Updated**: 2017-03-01 13:40:08+00:00
- **Authors**: Georg Poier, Markus Seidl, Matthias Zeppelzauer, Christian Reinbacher, Martin Schaich, Giovanna Bellandi, Alberto Marretta, Horst Bischof
- **Comment**: CBMI Submission; Dataset and more information can be found at
  http://lrs.icg.tugraz.at/research/petroglyphsegmentation/
- **Journal**: None
- **Summary**: The development of powerful 3D scanning hardware and reconstruction algorithms has strongly promoted the generation of 3D surface reconstructions in different domains. An area of special interest for such 3D reconstructions is the cultural heritage domain, where surface reconstructions are generated to digitally preserve historical artifacts. While reconstruction quality nowadays is sufficient in many cases, the robust analysis (e.g. segmentation, matching, and classification) of reconstructed 3D data is still an open topic. In this paper, we target the automatic and interactive segmentation of high-resolution 3D surface reconstructions from the archaeological domain. To foster research in this field, we introduce a fully annotated and publicly available large-scale 3D surface dataset including high-resolution meshes, depth maps and point clouds as a novel benchmark dataset to the community. We provide baseline results for our existing random forest-based approach and for the first time investigate segmentation with convolutional neural networks (CNNs) on the data. Results show that both approaches have complementary strengths and weaknesses and that the provided dataset represents a challenge for future research.



### Driving in the Matrix: Can Virtual Worlds Replace Human-Generated Annotations for Real World Tasks?
- **Arxiv ID**: http://arxiv.org/abs/1610.01983v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1610.01983v2)
- **Published**: 2016-10-06 18:26:43+00:00
- **Updated**: 2017-02-25 13:20:49+00:00
- **Authors**: Matthew Johnson-Roberson, Charles Barto, Rounak Mehta, Sharath Nittur Sridhar, Karl Rosaen, Ram Vasudevan
- **Comment**: Proceedings of International Conference on Robotics and Automation
  (ICRA) 2017, 8 pages
- **Journal**: None
- **Summary**: Deep learning has rapidly transformed the state of the art algorithms used to address a variety of problems in computer vision and robotics. These breakthroughs have relied upon massive amounts of human annotated training data. This time consuming process has begun impeding the progress of these deep learning efforts. This paper describes a method to incorporate photo-realistic computer images from a simulation engine to rapidly generate annotated data that can be used for the training of machine learning algorithms. We demonstrate that a state of the art architecture, which is trained only using these synthetic annotations, performs better than the identical architecture trained on human annotated real-world data, when tested on the KITTI data set for vehicle detection. By training machine learning algorithms on a rich virtual world, real objects in real scenes can be learned and classified using synthetic data. This approach offers the possibility of accelerating deep learning's application to sensor-based classification problems like those that appear in self-driving cars. The source code and data to train and validate the networks described in this paper are made available for researchers.



### Places: An Image Database for Deep Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/1610.02055v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1610.02055v1)
- **Published**: 2016-10-06 20:14:13+00:00
- **Updated**: 2016-10-06 20:14:13+00:00
- **Authors**: Bolei Zhou, Aditya Khosla, Agata Lapedriza, Antonio Torralba, Aude Oliva
- **Comment**: None
- **Journal**: None
- **Summary**: The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification at tasks such as object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories and attributes, comprising a quasi-exhaustive list of the types of environments encountered in the world. Using state of the art Convolutional Neural Networks, we provide impressive baseline performances at scene classification. With its high-coverage and high-diversity of exemplars, the Places Database offers an ecosystem to guide future progress on currently intractable visual recognition problems.



