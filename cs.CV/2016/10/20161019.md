# Arxiv Papers in cs.CV on 2016-10-19
### Lensless Imaging with Compressive Ultrafast Sensing
- **Arxiv ID**: http://arxiv.org/abs/1610.05834v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.05834v2)
- **Published**: 2016-10-19 01:08:55+00:00
- **Updated**: 2017-03-30 02:04:49+00:00
- **Authors**: Guy Satat, Matthew Tancik, Ramesh Raskar
- **Comment**: None
- **Journal**: None
- **Summary**: Lensless imaging is an important and challenging problem. One notable solution to lensless imaging is a single pixel camera which benefits from ideas central to compressive sampling. However, traditional single pixel cameras require many illumination patterns which result in a long acquisition process. Here we present a method for lensless imaging based on compressive ultrafast sensing. Each sensor acquisition is encoded with a different illumination pattern and produces a time series where time is a function of the photon's origin in the scene. Currently available hardware with picosecond time resolution enables time tagging photons as they arrive to an omnidirectional sensor. This allows lensless imaging with significantly fewer patterns compared to regular single pixel imaging. To that end, we develop a framework for designing lensless imaging systems that use ultrafast detectors. We provide an algorithm for ideal sensor placement and an algorithm for optimized active illumination patterns. We show that efficient lensless imaging is possible with ultrafast measurement and compressive sensing. This paves the way for novel imaging architectures and remote sensing in extreme situations where imaging with a lens is not possible.



### Mixed context networks for semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/1610.05854v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.05854v1)
- **Published**: 2016-10-19 03:00:47+00:00
- **Updated**: 2016-10-19 03:00:47+00:00
- **Authors**: Haiming Sun, Di Xie, Shiliang Pu
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: Semantic segmentation is challenging as it requires both object-level information and pixel-level accuracy. Recently, FCN-based systems gained great improvement in this area. Unlike classification networks, combining features of different layers plays an important role in these dense prediction models, as these features contains information of different levels. A number of models have been proposed to show how to use these features. However, what is the best architecture to make use of features of different layers is still a question. In this paper, we propose a module, called mixed context network, and show that our presented system outperforms most existing semantic segmentation systems by making use of this module.



### StuffNet: Using 'Stuff' to Improve Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1610.05861v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.05861v2)
- **Published**: 2016-10-19 04:44:51+00:00
- **Updated**: 2017-01-30 03:10:20+00:00
- **Authors**: Samarth Brahmbhatt, Henrik I. Christensen, James Hays
- **Comment**: Camera-ready version for IEEE WACV 2017
- **Journal**: None
- **Summary**: We propose a Convolutional Neural Network (CNN) based algorithm - StuffNet - for object detection. In addition to the standard convolutional features trained for region proposal and object detection [31], StuffNet uses convolutional features trained for segmentation of objects and 'stuff' (amorphous categories such as ground and water). Through experiments on Pascal VOC 2010, we show the importance of features learnt from stuff segmentation for improving object detection performance. StuffNet improves performance from 18.8% mAP to 23.9% mAP for small objects. We also devise a method to train StuffNet on datasets that do not have stuff segmentation labels. Through experiments on Pascal VOC 2007 and 2012, we demonstrate the effectiveness of this method and show that StuffNet also significantly improves object detection performance on such datasets.



### A Robust 3D-2D Interactive Tool for Scene Segmentation and Annotation
- **Arxiv ID**: http://arxiv.org/abs/1610.05883v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.05883v1)
- **Published**: 2016-10-19 06:54:02+00:00
- **Updated**: 2016-10-19 06:54:02+00:00
- **Authors**: Duc Thanh Nguyen, Binh-Son Hua, Lap-Fai Yu, Sai-Kit Yeung
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Recent advances of 3D acquisition devices have enabled large-scale acquisition of 3D scene data. Such data, if completely and well annotated, can serve as useful ingredients for a wide spectrum of computer vision and graphics works such as data-driven modeling and scene understanding, object detection and recognition. However, annotating a vast amount of 3D scene data remains challenging due to the lack of an effective tool and/or the complexity of 3D scenes (e.g. clutter, varying illumination conditions). This paper aims to build a robust annotation tool that effectively and conveniently enables the segmentation and annotation of massive 3D data. Our tool works by coupling 2D and 3D information via an interactive framework, through which users can provide high-level semantic annotation for objects. We have experimented our tool and found that a typical indoor scene could be well segmented and annotated in less than 30 minutes by using the tool, as opposed to a few hours if done manually. Along with the tool, we created a dataset of over a hundred 3D scenes associated with complete annotations using our tool. The tool and dataset are available at www.scenenn.net.



### An automatic bad band preremoval algorithm for hyperspectral imagery
- **Arxiv ID**: http://arxiv.org/abs/1610.05929v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.05929v1)
- **Published**: 2016-10-19 09:31:31+00:00
- **Updated**: 2016-10-19 09:31:31+00:00
- **Authors**: Luyan Ji, Xiurui Geng, Yongchao Zhao, Fuxiang Wang
- **Comment**: 17 pages, 8 figures
- **Journal**: None
- **Summary**: For most hyperspectral remote sensing applications, removing bad bands, such as water absorption bands, is a required preprocessing step. Currently, the commonly applied method is by visual inspection, which is very time-consuming and it is easy to overlook some noisy bands. In this study, we find an inherent connection between target detection algorithms and the corrupted band removal. As an example, for the matched filter (MF), which is the most widely used target detection method for hyperspectral data, we present an automatic MF-based algorithm for bad band identification. The MF detector is a filter vector, and the resulting filter output is the sum of all bands weighted by the MF coefficients. Therefore, we can identify bad bands only by using the MF filter vector itself, the absolute value of whose entry accounts for the importance of each band for the target detection. For a specific target of interest, the bands with small MF weights correspond to the noisy or bad ones. Based on this fact, we develop an automatic bad band preremoval algorithm by utilizing the average absolute value of MF weights for multiple targets within a scene. Experiments with three well known hyperspectral datasets show that our method can always identify the water absorption and other low signal-to-noise (SNR) bands that are usually chosen as bad bands manually.



### Visual-Inertial Monocular SLAM with Map Reuse
- **Arxiv ID**: http://arxiv.org/abs/1610.05949v2
- **DOI**: 10.1109/LRA.2017.2653359
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1610.05949v2)
- **Published**: 2016-10-19 10:17:16+00:00
- **Updated**: 2017-01-17 15:45:14+00:00
- **Authors**: Raul Mur-Artal, Juan D. Tardos
- **Comment**: Accepted for publication in IEEE Robotics and Automation Letters
- **Journal**: None
- **Summary**: In recent years there have been excellent results in Visual-Inertial Odometry techniques, which aim to compute the incremental motion of the sensor with high accuracy and robustness. However these approaches lack the capability to close loops, and trajectory estimation accumulates drift even if the sensor is continually revisiting the same place. In this work we present a novel tightly-coupled Visual-Inertial Simultaneous Localization and Mapping system that is able to close loops and reuse its map to achieve zero-drift localization in already mapped areas. While our approach can be applied to any camera configuration, we address here the most general problem of a monocular camera, with its well-known scale ambiguity. We also propose a novel IMU initialization method, which computes the scale, the gravity direction, the velocity, and gyroscope and accelerometer biases, in a few seconds with high accuracy. We test our system in the 11 sequences of a recent micro-aerial vehicle public dataset achieving a typical scale factor error of 1% and centimeter precision. We compare to the state-of-the-art in visual-inertial odometry in sequences with revisiting, proving the better accuracy of our method due to map reuse and no drift accumulation.



### Learning Robust Video Synchronization without Annotations
- **Arxiv ID**: http://arxiv.org/abs/1610.05985v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.05985v3)
- **Published**: 2016-10-19 12:43:56+00:00
- **Updated**: 2017-09-16 00:32:02+00:00
- **Authors**: Patrick Wieschollek, Ido Freeman, Hendrik P. A. Lensch
- **Comment**: International Conference On Machine Learning And Applications (ICMLA
  2017)
- **Journal**: None
- **Summary**: Aligning video sequences is a fundamental yet still unsolved component for a broad range of applications in computer graphics and vision. Most classical image processing methods cannot be directly applied to related video problems due to the high amount of underlying data and their limit to small changes in appearance. We present a scalable and robust method for computing a non-linear temporal video alignment. The approach autonomously manages its training data for learning a meaningful representation in an iterative procedure each time increasing its own knowledge. It leverages on the nature of the videos themselves to remove the need for manually created labels. While previous alignment methods similarly consider weather conditions, season and illumination, our approach is able to align videos from data recorded months apart.



### Fast and Accurate Surface Normal Integration on Non-Rectangular Domains
- **Arxiv ID**: http://arxiv.org/abs/1610.06049v1
- **DOI**: None
- **Categories**: **cs.NA**, cs.CV, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/1610.06049v1)
- **Published**: 2016-10-19 15:01:09+00:00
- **Updated**: 2016-10-19 15:01:09+00:00
- **Authors**: Martin Bähr, Michael Breuß, Yvain Quéau, Ali Sharifi Boroujerdi, Jean-Denis Durou
- **Comment**: None
- **Journal**: None
- **Summary**: The integration of surface normals for the purpose of computing the shape of a surface in 3D space is a classic problem in computer vision. However, even nowadays it is still a challenging task to devise a method that combines the flexibility to work on non-trivial computational domains with high accuracy, robustness and computational efficiency. By uniting a classic approach for surface normal integration with modern computational techniques we construct a solver that fulfils these requirements. Building upon the Poisson integration model we propose to use an iterative Krylov subspace solver as a core step in tackling the task. While such a method can be very efficient, it may only show its full potential when combined with a suitable numerical preconditioning and a problem-specific initialisation. We perform a thorough numerical study in order to identify an appropriate preconditioner for our purpose. To address the issue of a suitable initialisation we propose to compute this initial state via a recently developed fast marching integrator. Detailed numerical experiments illuminate the benefits of this novel combination. In addition, we show on real-world photometric stereo datasets that the developed numerical framework is flexible enough to tackle modern computer vision applications.



### POI: Multiple Object Tracking with High Performance Detection and Appearance Feature
- **Arxiv ID**: http://arxiv.org/abs/1610.06136v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.06136v1)
- **Published**: 2016-10-19 18:10:21+00:00
- **Updated**: 2016-10-19 18:10:21+00:00
- **Authors**: Fengwei Yu, Wenbo Li, Quanquan Li, Yu Liu, Xiaohua Shi, Junjie Yan
- **Comment**: ECCV workshop BMTT 2016
- **Journal**: ECCV 2016 Workshops, Part II, LNCS 9914, paper approval (Chapter
  3, 978-3-319-48880-6, 434776_1_En
- **Summary**: Detection and learning based appearance feature play the central role in data association based multiple object tracking (MOT), but most recent MOT works usually ignore them and only focus on the hand-crafted feature and association algorithms. In this paper, we explore the high-performance detection and deep learning based appearance feature, and show that they lead to significantly better MOT results in both online and offline setting. We make our detection and appearance feature publicly available. In the following part, we first summarize the detection and appearance feature, and then introduce our tracker named Person of Interest (POI), which has both online and offline version.



### A Reinforcement Learning Approach to the View Planning Problem
- **Arxiv ID**: http://arxiv.org/abs/1610.06204v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.06204v2)
- **Published**: 2016-10-19 20:29:20+00:00
- **Updated**: 2016-11-18 20:26:51+00:00
- **Authors**: Mustafa Devrim Kaba, Mustafa Gokhan Uzunbas, Ser Nam Lim
- **Comment**: None
- **Journal**: None
- **Summary**: We present a Reinforcement Learning (RL) solution to the view planning problem (VPP), which generates a sequence of view points that are capable of sensing all accessible area of a given object represented as a 3D model. In doing so, the goal is to minimize the number of view points, making the VPP a class of set covering optimization problem (SCOP). The SCOP is NP-hard, and the inapproximability results tell us that the greedy algorithm provides the best approximation that runs in polynomial time. In order to find a solution that is better than the greedy algorithm, (i) we introduce a novel score function by exploiting the geometry of the 3D model, (ii) we model an intuitive human approach to VPP using this score function, and (iii) we cast VPP as a Markovian Decision Process (MDP), and solve the MDP in RL framework using well-known RL algorithms. In particular, we use SARSA, Watkins-Q and TD with function approximation to solve the MDP. We compare the results of our method with the baseline greedy algorithm in an extensive set of test objects, and show that we can out-perform the baseline in almost all cases.



