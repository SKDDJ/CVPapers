# Arxiv Papers in cs.CV on 2016-10-09
### Visual Closed-Loop Control for Pouring Liquids
- **Arxiv ID**: http://arxiv.org/abs/1610.02610v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1610.02610v3)
- **Published**: 2016-10-09 01:28:01+00:00
- **Updated**: 2017-02-25 23:05:47+00:00
- **Authors**: Connor Schenck, Dieter Fox
- **Comment**: To appear at ICRA 2017
- **Journal**: None
- **Summary**: Pouring a specific amount of liquid is a challenging task. In this paper we develop methods for robots to use visual feedback to perform closed-loop control for pouring liquids. We propose both a model-based and a model-free method utilizing deep learning for estimating the volume of liquid in a container. Our results show that the model-free method is better able to estimate the volume. We combine this with a simple PID controller to pour specific amounts of liquid, and show that the robot is able to achieve an average 38ml deviation from the target amount. To our knowledge, this is the first use of raw visual feedback to pour liquids in robotics.



### Learning Spatial-Semantic Context with Fully Convolutional Recurrent Network for Online Handwritten Chinese Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1610.02616v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.02616v2)
- **Published**: 2016-10-09 02:39:07+00:00
- **Updated**: 2017-05-25 15:33:19+00:00
- **Authors**: Zecheng Xie, Zenghui Sun, Lianwen Jin, Hao Ni, Terry Lyons
- **Comment**: 14 pages, 9 figures
- **Journal**: None
- **Summary**: Online handwritten Chinese text recognition (OHCTR) is a challenging problem as it involves a large-scale character set, ambiguous segmentation, and variable-length input sequences. In this paper, we exploit the outstanding capability of path signature to translate online pen-tip trajectories into informative signature feature maps using a sliding window-based method, successfully capturing the analytic and geometric properties of pen strokes with strong local invariance and robustness. A multi-spatial-context fully convolutional recurrent network (MCFCRN) is proposed to exploit the multiple spatial contexts from the signature feature maps and generate a prediction sequence while completely avoiding the difficult segmentation problem. Furthermore, an implicit language model is developed to make predictions based on semantic context within a predicting feature sequence, providing a new perspective for incorporating lexicon constraints and prior knowledge about a certain language in the recognition procedure. Experiments on two standard benchmarks, Dataset-CASIA and Dataset-ICDAR, yielded outstanding results, with correct rates of 97.10% and 97.15%, respectively, which are significantly better than the best result reported thus far in the literature.



### Spatial Relationship Based Features for Indian Sign Language Recognition
- **Arxiv ID**: http://arxiv.org/abs/1610.07995v1
- **DOI**: 10.15242/IJCCIE.IAE0516005
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.07995v1)
- **Published**: 2016-10-09 06:50:17+00:00
- **Updated**: 2016-10-09 06:50:17+00:00
- **Authors**: B. M. Chethana Kumara, H. S. Nagendraswamy, R. Lekha Chinmayi
- **Comment**: 7 Pages, 9 Figures, 4 Tables
- **Journal**: Int'l Journal of Computing, Communications & Instrumentation Engg.
  (IJCCIE) Vol. 3, Issue 2 (2016) ISSN 2349-1469 EISSN 2349-1477
- **Summary**: In this paper, the task of recognizing signs made by hearing impaired people at sentence level has been addressed. A novel method of extracting spatial features to capture hand movements of a signer has been proposed. Frames of a given video of a sign are preprocessed to extract face and hand components of a signer. The local centroids of the extracted components along with the global centroid are exploited to extract spatial features. The concept of interval valued type symbolic data has been explored to capture variations in the same sign made by the different signers at different instances of time. A suitable symbolic similarity measure is studied to establish matching between test and reference signs and a simple nearest neighbour classifier is used to recognize an unknown sign as one among the known signs by specifying a desired level of threshold. An extensive experimentation is conducted on a considerably large database of signs created by us during the course of research work in order to evaluate the performance of the proposed system



### Zero Shot Hashing
- **Arxiv ID**: http://arxiv.org/abs/1610.02651v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.02651v1)
- **Published**: 2016-10-09 09:46:02+00:00
- **Updated**: 2016-10-09 09:46:02+00:00
- **Authors**: Shubham Pachori, Shanmuganathan Raman
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: This paper provides a framework to hash images containing instances of unknown object classes. In many object recognition problems, we might have access to huge amount of data. It may so happen that even this huge data doesn't cover the objects belonging to classes that we see in our day to day life. Zero shot learning exploits auxiliary information (also called as signatures) in order to predict the labels corresponding to unknown classes. In this work, we attempt to generate the hash codes for images belonging to unseen classes, information of which is available only through the textual corpus. We formulate this as an unsupervised hashing formulation as the exact labels are not available for the instances of unseen classes. We show that the proposed solution is able to generate hash codes which can predict labels corresponding to unseen classes with appreciably good precision.



### Proposal for Automatic License and Number Plate Recognition System for Vehicle Identification
- **Arxiv ID**: http://arxiv.org/abs/1610.03341v1
- **DOI**: 10.1016/S0550-3213(01)00405-9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.03341v1)
- **Published**: 2016-10-09 11:33:34+00:00
- **Updated**: 2016-10-09 11:33:34+00:00
- **Authors**: Hamed Saghaei
- **Comment**: 5 pages, 3 figures, 2016 1st International Conference on New Research
  Achievements in Electrical and Computer Engineering
- **Journal**: None
- **Summary**: In this paper, we propose an automatic and mechanized license and number plate recognition (LNPR) system which can extract the license plate number of the vehicles passing through a given location using image processing algorithms. No additional devices such as GPS or radio frequency identification (RFID) need to be installed for implementing the proposed system. Using special cameras, the system takes pictures from each passing vehicle and forwards the image to the computer for being processed by the LPR software. Plate recognition software uses different algorithms such as localization, orientation, normalization, segmentation and finally optical character recognition (OCR). The resulting data is applied to compare with the records on a database. Experimental results reveal that the presented system successfully detects and recognizes the vehicle number plate on real images. This system can also be used for security and traffic control.



### Open-Ended Visual Question-Answering
- **Arxiv ID**: http://arxiv.org/abs/1610.02692v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1610.02692v1)
- **Published**: 2016-10-09 16:38:31+00:00
- **Updated**: 2016-10-09 16:38:31+00:00
- **Authors**: Issey Masuda, Santiago Pascual de la Puente, Xavier Giro-i-Nieto
- **Comment**: Bachelor thesis report graded with A with honours at ETSETB Telecom
  BCN school, Universitat Polit\`ecnica de Catalunya (UPC). June 2016. Source
  code and models are publicly available at
  http://imatge-upc.github.io/vqa-2016-cvprw/
- **Journal**: None
- **Summary**: This thesis report studies methods to solve Visual Question-Answering (VQA) tasks with a Deep Learning framework. As a preliminary step, we explore Long Short-Term Memory (LSTM) networks used in Natural Language Processing (NLP) to tackle Question-Answering (text based). We then modify the previous model to accept an image as an input in addition to the question. For this purpose, we explore the VGG-16 and K-CNN convolutional neural networks to extract visual features from the image. These are merged with the word embedding or with a sentence embedding of the question to predict the answer. This work was successfully submitted to the Visual Question Answering Challenge 2016, where it achieved a 53,62% of accuracy in the test dataset. The developed software has followed the best programming practices and Python code style, providing a consistent baseline in Keras for different configurations.



### Egocentric Height Estimation
- **Arxiv ID**: http://arxiv.org/abs/1610.02714v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.02714v1)
- **Published**: 2016-10-09 20:08:17+00:00
- **Updated**: 2016-10-09 20:08:17+00:00
- **Authors**: Jessica Finocchiaro, Aisha Urooj Khan, Ali Borji
- **Comment**: None
- **Journal**: None
- **Summary**: Egocentric, or first-person vision which became popular in recent years with an emerge in wearable technology, is different than exocentric (third-person) vision in some distinguishable ways, one of which being that the camera wearer is generally not visible in the video frames. Recent work has been done on action and object recognition in egocentric videos, as well as work on biometric extraction from first-person videos. Height estimation can be a useful feature for both soft-biometrics and object tracking. Here, we propose a method of estimating the height of an egocentric camera without any calibration or reference points. We used both traditional computer vision approaches and deep learning in order to determine the visual cues that results in best height estimation. Here, we introduce a framework inspired by two stream networks comprising of two Convolutional Neural Networks, one based on spatial information, and one based on information given by optical flow in a frame. Given an egocentric video as an input to the framework, our model yields a height estimate as an output. We also incorporate late fusion to learn a combination of temporal and spatial cues. Comparing our model with other methods we used as baselines, we achieve height estimates for videos with a Mean Average Error of 14.04 cm over a range of 103 cm of data, and classification accuracy for relative height (tall, medium or short) up to 93.75% where chance level is 33%.



