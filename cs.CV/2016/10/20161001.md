# Arxiv Papers in cs.CV on 2016-10-01
### Radial Velocity Retrieval for Multichannel SAR Moving Targets with Time-Space Doppler De-ambiguity
- **Arxiv ID**: http://arxiv.org/abs/1610.00070v3
- **DOI**: None
- **Categories**: **cs.IT**, cs.CV, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1610.00070v3)
- **Published**: 2016-10-01 01:39:15+00:00
- **Updated**: 2017-06-09 19:58:44+00:00
- **Authors**: Jia Xu, Zu-Zhen Huang, Zhi-Rui Wang, Li Xiao, Xiang-Gen Xia, Teng Long
- **Comment**: 14 double-column pages, 11 figures, 4 tables
- **Journal**: None
- **Summary**: In this paper, with respect to multichannel synthetic aperture radars (SAR), we first formulate the problems of Doppler ambiguities on the radial velocity (RV) estimation of a ground moving target in range-compressed domain, range-Doppler domain and image domain, respectively. It is revealed that in these problems, a cascaded time-space Doppler ambiguity (CTSDA) may encounter, i.e., time domain Doppler ambiguity (TDDA) in each channel arises first and then spatial domain Doppler ambiguity (SDDA) among multi-channels arises second. Accordingly, the multichannel SAR systems with different parameters are investigated in three different cases with diverse Doppler ambiguity properties, and a multi-frequency SAR is then proposed to obtain the RV estimation by solving the ambiguity problem based on Chinese remainder theorem (CRT). In the first two cases, the ambiguity problem can be solved by the existing closed-form robust CRT. In the third case, it is found that the problem is different from the conventional CRT problems and we call it a double remaindering problem in this paper. We then propose a sufficient condition under which the double remaindering problem, i.e., the CTSDA, can also be solved by the closed-form robust CRT. When the sufficient condition is not satisfied for a multi-channel SAR, a searching based method is proposed. Finally, some results of numerical experiments are provided to demonstrate the effectiveness of the proposed methods.



### How Transferable are CNN-based Features for Age and Gender Classification?
- **Arxiv ID**: http://arxiv.org/abs/1610.00134v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.00134v1)
- **Published**: 2016-10-01 13:28:39+00:00
- **Updated**: 2016-10-01 13:28:39+00:00
- **Authors**: Gökhan Özbulak, Yusuf Aytar, Hazım Kemal Ekenel
- **Comment**: 12 pages, 3 figures, 2 tables, International Conference of the
  Biometrics Special Interest Group (BIOSIG) 2016
- **Journal**: None
- **Summary**: Age and gender are complementary soft biometric traits for face recognition. Successful estimation of age and gender from facial images taken under real-world conditions can contribute improving the identification results in the wild. In this study, in order to achieve robust age and gender classification in the wild, we have benefited from Deep Convolutional Neural Networks based representation. We have explored transferability of existing deep convolutional neural network (CNN) models for age and gender classification. The generic AlexNet-like architecture and domain specific VGG-Face CNN model are employed and fine-tuned with the Adience dataset prepared for age and gender classification in uncontrolled environments. In addition, task specific GilNet CNN model has also been utilized and used as a baseline method in order to compare with transferred models. Experimental results show that both transferred deep CNN models outperform the GilNet CNN model, which is the state-of-the-art age and gender classification approach on the Adience dataset, by an absolute increase of 7% and 4.5% in accuracy, respectively. This outcome indicates that transferring a deep CNN model can provide better classification performance than a task specific CNN model, which has a limited number of layers and trained from scratch using a limited amount of data as in the case of GilNet. Domain specific VGG-Face CNN model has been found to be more useful and provided better performance for both age and gender classification tasks, when compared with generic AlexNet-like model, which shows that transfering from a closer domain is more useful.



### X-CNN: Cross-modal Convolutional Neural Networks for Sparse Datasets
- **Arxiv ID**: http://arxiv.org/abs/1610.00163v2
- **DOI**: 10.1109/SSCI.2016.7849978
- **Categories**: **stat.ML**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1610.00163v2)
- **Published**: 2016-10-01 18:01:35+00:00
- **Updated**: 2016-10-17 14:51:36+00:00
- **Authors**: Petar Veličković, Duo Wang, Nicholas D. Lane, Pietro Liò
- **Comment**: To appear in the 7th IEEE Symposium Series on Computational
  Intelligence (IEEE SSCI 2016), 8 pages, 6 figures. Minor revisions, in
  response to reviewers' comments
- **Journal**: None
- **Summary**: In this paper we propose cross-modal convolutional neural networks (X-CNNs), a novel biologically inspired type of CNN architectures, treating gradient descent-specialised CNNs as individual units of processing in a larger-scale network topology, while allowing for unconstrained information flow and/or weight sharing between analogous hidden layers of the network---thus generalising the already well-established concept of neural network ensembles (where information typically may flow only between the output layers of the individual networks). The constituent networks are individually designed to learn the output function on their own subset of the input data, after which cross-connections between them are introduced after each pooling operation to periodically allow for information exchange between them. This injection of knowledge into a model (by prior partition of the input data through domain knowledge or unsupervised methods) is expected to yield greatest returns in sparse data environments, which are typically less suitable for training CNNs. For evaluation purposes, we have compared a standard four-layer CNN as well as a sophisticated FitNet4 architecture against their cross-modal variants on the CIFAR-10 and CIFAR-100 datasets with differing percentages of the training data being removed, and find that at lower levels of data availability, the X-CNNs significantly outperform their baselines (typically providing a 2--6% benefit, depending on the dataset size and whether data augmentation is used), while still maintaining an edge on all of the full dataset tests.



### Near-Infrared Image Dehazing Via Color Regularization
- **Arxiv ID**: http://arxiv.org/abs/1610.00175v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.00175v1)
- **Published**: 2016-10-01 19:46:24+00:00
- **Updated**: 2016-10-01 19:46:24+00:00
- **Authors**: Chang-Hwan Son, Xiao-Ping Zhang
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Near-infrared imaging can capture haze-free near-infrared gray images and visible color images, according to physical scattering models, e.g., Rayleigh or Mie models. However, there exist serious discrepancies in brightness and image structures between the near-infrared gray images and the visible color images. The direct use of the near-infrared gray images brings about another color distortion problem in the dehazed images. Therefore, the color distortion should also be considered for near-infrared dehazing. To reflect this point, this paper presents an approach of adding a new color regularization to conventional dehazing framework. The proposed color regularization can model the color prior for unknown haze-free images from two captured images. Thus, natural-looking colors and fine details can be induced on the dehazed images. The experimental results show that the proposed color regularization model can help remove the color distortion and the haze at the same time. Also, the effectiveness of the proposed color regularization is verified by comparing with other conventional regularizations. It is also shown that the proposed color regularization can remove the edge artifacts which arise from the use of the conventional dark prior model.



