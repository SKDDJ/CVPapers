# Arxiv Papers in cs.CV on 2016-10-27
### Iterative Inversion of Deformation Vector Fields with Feedback Control
- **Arxiv ID**: http://arxiv.org/abs/1610.08589v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.08589v4)
- **Published**: 2016-10-27 01:32:57+00:00
- **Updated**: 2018-03-28 23:49:55+00:00
- **Authors**: Abhishek Kumar Dubey, Alexandros-Stavros Iliopoulos, Xiaobai Sun, Fang-Fang Yin, Lei Ren
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: Often, the inverse deformation vector field (DVF) is needed together with the corresponding forward DVF in 4D reconstruction and dose calculation, adaptive radiation therapy, and simultaneous deformable registration. This study aims at improving both accuracy and efficiency of iterative algorithms for DVF inversion, and advancing our understanding of divergence and latency conditions. Method: We introduce a framework of fixed-point iteration algorithms with active feedback control for DVF inversion. Based on rigorous convergence analysis, we design control mechanisms for modulating the inverse consistency (IC) residual of the current iterate, to be used as feedback into the next iterate. The control is designed adaptively to the input DVF with the objective to enlarge the convergence area and expedite convergence. Three particular settings of feedback control are introduced: constant value over the domain throughout the iteration; alternating values between iteration steps; and spatially variant values. We also introduce three spectral measures of the displacement Jacobian for characterizing a DVF. These measures reveal the critical role of what we term the non-translational displacement component (NTDC) of the DVF. We carry out inversion experiments with an analytical DVF pair, and with DVFs associated with thoracic CT images of 6 patients at end of expiration and end of inspiration. Results: NTDC-adaptive iterations are shown to attain a larger convergence region at a faster pace compared to previous non-adaptive DVF inversion iteration algorithms. By our numerical experiments, alternating control yields smaller IC residuals and inversion errors than constant control. Spatially variant control renders smaller residuals and errors by at least an order of magnitude, compared to other schemes, in no more than 10 steps. Inversion results also show remarkable quantitative agreement with analysis-based predictions. Conclusion: Our analysis captures properties of DVF data associated with clinical CT images, and provides new understanding of iterative DVF inversion algorithms with a simple residual feedback control. Adaptive control is necessary and highly effective in the presence of non-small NTDCs. The adaptive iterations or the spectral measures, or both, may potentially be incorporated into deformable image registration methods.



### Fast Low-rank Shared Dictionary Learning for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1610.08606v3
- **DOI**: 10.1109/TIP.2017.2729885
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1610.08606v3)
- **Published**: 2016-10-27 03:58:17+00:00
- **Updated**: 2017-07-16 02:39:50+00:00
- **Authors**: Tiep Vu, Vishal Monga
- **Comment**: Accepted version
- **Journal**: None
- **Summary**: Despite the fact that different objects possess distinct class-specific features, they also usually share common patterns. This observation has been exploited partially in a recently proposed dictionary learning framework by separating the particularity and the commonality (COPAR). Inspired by this, we propose a novel method to explicitly and simultaneously learn a set of common patterns as well as class-specific features for classification with more intuitive constraints. Our dictionary learning framework is hence characterized by both a shared dictionary and particular (class-specific) dictionaries. For the shared dictionary, we enforce a low-rank constraint, i.e. claim that its spanning subspace should have low dimension and the coefficients corresponding to this dictionary should be similar. For the particular dictionaries, we impose on them the well-known constraints stated in the Fisher discrimination dictionary learning (FDDL). Further, we develop new fast and accurate algorithms to solve the subproblems in the learning step, accelerating its convergence. The said algorithms could also be applied to FDDL and its extensions. The efficiencies of these algorithms are theoretically and experimentally verified by comparing their complexities and running time with those of other well-known dictionary learning methods. Experimental results on widely used image datasets establish the advantages of our method over state-of-the-art dictionary learning methods.



### Joint Target Detection and Tracking in Multipath Environment: A Variational Bayesian Approach
- **Arxiv ID**: http://arxiv.org/abs/1610.08616v2
- **DOI**: None
- **Categories**: **cs.CV**, 65K10
- **Links**: [PDF](http://arxiv.org/pdf/1610.08616v2)
- **Published**: 2016-10-27 05:05:42+00:00
- **Updated**: 2018-12-11 09:13:37+00:00
- **Authors**: Hua Lan, Shuai Sun, Zengfu Wang, Quan Pan, Zhishan Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: We consider multitarget detection and tracking problem for a class of multipath detection system where one target may generate multiple measurements via multiple propagation paths, and the association relationship among targets, measurements and propagation paths is unknown. In order to effectively utilize multipath measurements from one target to improve detection and tracking performance, a tracker has to handle high-dimensional estimation of latent variables including target active/dormant meta-state, target kinematic state, and multipath data association. Based on variational Bayesian inference, we propose a novel joint detection and tracking algorithm that incorporates multipath data association, target detection and target state estimation in a unified Bayesian framework. The posterior probabilities of these latent variables are derived in a closed-form iterative manner, which is effective for reducing the performance deterioration caused by the coupling between estimation errors and identification errors. Loopy belief propagation is exploited to approximately calculate the probability of multipath data association, saving the computational cost significantly. Simulation results of over-the-horizon radar multitarget tracking show that the proposed algorithm outperforms multihypothesis multipath track fusion and multi-detection (hypothesis-oriented) multiple hypothesis tracker, especially under low signal-to-noise ratio circumstance.



### Exploiting Structure Sparsity for Covariance-based Visual Representation
- **Arxiv ID**: http://arxiv.org/abs/1610.08619v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.08619v2)
- **Published**: 2016-10-27 05:17:14+00:00
- **Updated**: 2016-11-27 23:50:11+00:00
- **Authors**: Jianjia Zhang, Lei Wang, Luping Zhou, Wanqing Li
- **Comment**: None
- **Journal**: None
- **Summary**: The past few years have witnessed increasing research interest on covariance-based feature representation. A variety of methods have been proposed to boost its efficacy, with some recent ones resorting to nonlinear kernel technique. Noting that the essence of this feature representation is to characterise the underlying structure of visual features, this paper argues that an equally, if not more, important approach to boosting its efficacy shall be to improve the quality of this characterisation. Following this idea, we propose to exploit the structure sparsity of visual features in skeletal human action recognition, and compute sparse inverse covariance estimate (SICE) as feature representation. We discuss the advantage of this new representation on dealing with small sample, high dimensionality, and modelling capability. Furthermore, utilising the monotonicity property of SICE, we efficiently generate a hierarchy of SICE matrices to characterise the structure of visual features at different sparsity levels, and two discriminative learning algorithms are then developed to adaptively integrate them to perform recognition. As demonstrated by extensive experiments, the proposed representation leads to significantly improved recognition performance over the state-of-the-art comparable methods. In particular, as a method fully based on linear technique, it is comparable or even better than those employing nonlinear kernel technique. This result well demonstrates the value of exploiting structure sparsity for covariance-based feature representation.



### PCM and APCM Revisited: An Uncertainty Perspective
- **Arxiv ID**: http://arxiv.org/abs/1610.08624v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1610.08624v1)
- **Published**: 2016-10-27 05:41:23+00:00
- **Updated**: 2016-10-27 05:41:23+00:00
- **Authors**: Peixin Hou, Hao Deng, Jiguang Yue, Shuguang Liu
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: In this paper, we take a new look at the possibilistic c-means (PCM) and adaptive PCM (APCM) clustering algorithms from the perspective of uncertainty. This new perspective offers us insights into the clustering process, and also provides us greater degree of flexibility. We analyze the clustering behavior of PCM-based algorithms and introduce parameters $\sigma_v$ and $\alpha$ to characterize uncertainty of estimated bandwidth and noise level of the dataset respectively. Then uncertainty (fuzziness) of membership values caused by uncertainty of the estimated bandwidth parameter is modeled by a conditional fuzzy set, which is a new formulation of the type-2 fuzzy set. Experiments show that parameters $\sigma_v$ and $\alpha$ make the clustering process more easy to control, and main features of PCM and APCM are unified in this new clustering framework (UPCM). More specifically, UPCM reduces to PCM when we set a small $\alpha$ or a large $\sigma_v$, and UPCM reduces to APCM when clusters are confined in their physical clusters and possible cluster elimination are ensured. Finally we present further researches of this paper.



### Estimation of Bandlimited Grayscale Images From the Single Bit Observations of Pixels Affected by Additive Gaussian Noise
- **Arxiv ID**: http://arxiv.org/abs/1610.08627v1
- **DOI**: None
- **Categories**: **cs.CV**, math.ST, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/1610.08627v1)
- **Published**: 2016-10-27 06:15:26+00:00
- **Updated**: 2016-10-27 06:15:26+00:00
- **Authors**: Abhinav Kumar, Animesh Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: The estimation of grayscale images using their single-bit zero mean Gaussian noise-affected pixels is presented in this paper. The images are assumed to be bandlimited in the Fourier Cosine transform (FCT) domain. The images are oversampled over their Nyquist rate in the FCT domain. We propose a non-recursive approach based on first order approximation of Cumulative Distribution Function (CDF) to estimate the image from single bit pixels which itself is based on Banach's contraction theorem. The decay rate for mean squared error of estimating such images is found to be independent of the precision of the quantizer and it varies as $O(1/N)$ where $N$ is the "effective" oversampling ratio with respect to the Nyquist rate in the FCT domain.



### Single- and Multi-Task Architectures for Surgical Workflow Challenge at M2CAI 2016
- **Arxiv ID**: http://arxiv.org/abs/1610.08844v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.08844v2)
- **Published**: 2016-10-27 15:42:08+00:00
- **Updated**: 2016-10-28 09:12:57+00:00
- **Authors**: Andru P. Twinanda, Didier Mutter, Jacques Marescaux, Michel de Mathelin, Nicolas Padoy
- **Comment**: The dataset is available at http://camma.u-strasbg.fr/m2cai2016/
- **Journal**: None
- **Summary**: The surgical workflow challenge at M2CAI 2016 consists of identifying 8 surgical phases in cholecystectomy procedures. Here, we propose to use deep architectures that are based on our previous work where we presented several architectures to perform multiple recognition tasks on laparoscopic videos. In this technical report, we present the phase recognition results using two architectures: (1) a single-task architecture designed to perform solely the surgical phase recognition task and (2) a multi-task architecture designed to perform jointly phase recognition and tool presence detection. On top of these architectures we propose to use two different approaches to enforce the temporal constraints of the surgical workflow: (1) HMM-based and (2) LSTM-based pipelines. The results show that the LSTM-based approach is able to outperform the HMM-based approach and also to properly enforce the temporal constraints into the recognition process.



### Single- and Multi-Task Architectures for Tool Presence Detection Challenge at M2CAI 2016
- **Arxiv ID**: http://arxiv.org/abs/1610.08851v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.08851v1)
- **Published**: 2016-10-27 15:51:53+00:00
- **Updated**: 2016-10-27 15:51:53+00:00
- **Authors**: Andru P. Twinanda, Didier Mutter, Jacques Marescaux, Michel de Mathelin, Nicolas Padoy
- **Comment**: The dataset is available at http://camma.u-strasbg.fr/m2cai2016/ .
  arXiv admin note: text overlap with arXiv:1610.08844
- **Journal**: None
- **Summary**: The tool presence detection challenge at M2CAI 2016 consists of identifying the presence/absence of seven surgical tools in the images of cholecystectomy videos. Here, we propose to use deep architectures that are based on our previous work where we presented several architectures to perform multiple recognition tasks on laparoscopic videos. In this technical report, we present the tool presence detection results using two architectures: (1) a single-task architecture designed to perform solely the tool presence detection task and (2) a multi-task architecture designed to perform jointly phase recognition and tool presence detection. The results show that the multi-task network only slightly improves the tool presence detection results. In constrast, a significant improvement is obtained when there are more data available to train the networks. This significant improvement can be regarded as a call for action for other institutions to start working toward publishing more datasets into the community, so that better models could be generated to perform the task.



### Tool and Phase recognition using contextual CNN features
- **Arxiv ID**: http://arxiv.org/abs/1610.08854v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.08854v1)
- **Published**: 2016-10-27 15:54:41+00:00
- **Updated**: 2016-10-27 15:54:41+00:00
- **Authors**: Manish Sahu, Anirban Mukhopadhyay, Angelika Szengel, Stefan Zachow
- **Comment**: MICCAI M2CAI 2016 Surgical tool & phase detection challenge report
- **Journal**: None
- **Summary**: A transfer learning method for generating features suitable for surgical tools and phase recognition from the ImageNet classification features [1] is proposed here. In addition, methods are developed for generating contextual features and combining them with time series analysis for final classification using multi-class random forest. The proposed pipeline is tested over the training and testing datasets of M2CAI16 challenges: tool and phase detection. Encouraging results are obtained by leave-one-out cross validation evaluation on the training dataset.



### Detecting People in Artwork with CNNs
- **Arxiv ID**: http://arxiv.org/abs/1610.08871v1
- **DOI**: 10.1007/978-3-319-46604-0_57
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.08871v1)
- **Published**: 2016-10-27 16:30:15+00:00
- **Updated**: 2016-10-27 16:30:15+00:00
- **Authors**: Nicholas Westlake, Hongping Cai, Peter Hall
- **Comment**: 14 pages, plus 3 pages of references; 7 figures in ECCV 2016
  Workshops
- **Journal**: None
- **Summary**: CNNs have massively improved performance in object detection in photographs. However research into object detection in artwork remains limited. We show state-of-the-art performance on a challenging dataset, People-Art, which contains people from photos, cartoons and 41 different artwork movements. We achieve this high performance by fine-tuning a CNN for this task, thus also demonstrating that training CNNs on photos results in overfitting for photos: only the first three or four layers transfer from photos to artwork. Although the CNN's performance is the highest yet, it remains less than 60\% AP, suggesting further work is needed for the cross-depiction problem. The final publication is available at Springer via http://dx.doi.org/10.1007/978-3-319-46604-0_57



### Local Similarity-Aware Deep Feature Embedding
- **Arxiv ID**: http://arxiv.org/abs/1610.08904v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1610.08904v1)
- **Published**: 2016-10-27 17:51:18+00:00
- **Updated**: 2016-10-27 17:51:18+00:00
- **Authors**: Chen Huang, Chen Change Loy, Xiaoou Tang
- **Comment**: 9 pages, 4 figures, 2 tables. Accepted to NIPS 2016
- **Journal**: None
- **Summary**: Existing deep embedding methods in vision tasks are capable of learning a compact Euclidean space from images, where Euclidean distances correspond to a similarity metric. To make learning more effective and efficient, hard sample mining is usually employed, with samples identified through computing the Euclidean feature distance. However, the global Euclidean distance cannot faithfully characterize the true feature similarity in a complex visual feature space, where the intraclass distance in a high-density region may be larger than the interclass distance in low-density regions. In this paper, we introduce a Position-Dependent Deep Metric (PDDM) unit, which is capable of learning a similarity metric adaptive to local feature structure. The metric can be used to select genuinely hard samples in a local neighborhood to guide the deep embedding learning in an online and robust manner. The new layer is appealing in that it is pluggable to any convolutional networks and is trained end-to-end. Our local similarity-aware feature embedding not only demonstrates faster convergence and boosted performance on two complex image retrieval datasets, its large margin nature also leads to superior generalization results under the large and open set scenarios of transfer learning and zero-shot learning on ImageNet 2010 and ImageNet-10K datasets.



### SoundNet: Learning Sound Representations from Unlabeled Video
- **Arxiv ID**: http://arxiv.org/abs/1610.09001v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1610.09001v1)
- **Published**: 2016-10-27 20:23:39+00:00
- **Updated**: 2016-10-27 20:23:39+00:00
- **Authors**: Yusuf Aytar, Carl Vondrick, Antonio Torralba
- **Comment**: NIPS 2016
- **Journal**: None
- **Summary**: We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge. Our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification. Visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels.



### Cross-Modal Scene Networks
- **Arxiv ID**: http://arxiv.org/abs/1610.09003v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1610.09003v1)
- **Published**: 2016-10-27 20:24:36+00:00
- **Updated**: 2016-10-27 20:24:36+00:00
- **Authors**: Yusuf Aytar, Lluis Castrejon, Carl Vondrick, Hamed Pirsiavash, Antonio Torralba
- **Comment**: See more at http://cmplaces.csail.mit.edu/. arXiv admin note: text
  overlap with arXiv:1607.07295
- **Journal**: None
- **Summary**: People can recognize scenes across many different modalities beyond natural images. In this paper, we investigate how to learn cross-modal scene representations that transfer across modalities. To study this problem, we introduce a new cross-modal scene dataset. While convolutional neural networks can categorize scenes well, they also learn an intermediate representation not aligned across modalities, which is undesirable for cross-modal transfer applications. We present methods to regularize cross-modal convolutional neural networks so that they have a shared representation that is agnostic of the modality. Our experiments suggest that our scene representation can help transfer representations across modalities for retrieval. Moreover, our visualizations suggest that units emerge in the shared representation that tend to activate on consistent concepts independently of the modality.



### Compressive Holographic Video
- **Arxiv ID**: http://arxiv.org/abs/1610.09013v1
- **DOI**: 10.1364/OE.25.000250
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1610.09013v1)
- **Published**: 2016-10-27 20:57:42+00:00
- **Updated**: 2016-10-27 20:57:42+00:00
- **Authors**: Zihao Wang, Leonidas Spinoulas, Kuan He, Huaijin Chen, Lei Tian, Aggelos K. Katsaggelos, Oliver Cossairt
- **Comment**: 12 pages, 6 figures
- **Journal**: None
- **Summary**: Compressed sensing has been discussed separately in spatial and temporal domains. Compressive holography has been introduced as a method that allows 3D tomographic reconstruction at different depths from a single 2D image. Coded exposure is a temporal compressed sensing method for high speed video acquisition. In this work, we combine compressive holography and coded exposure techniques and extend the discussion to 4D reconstruction in space and time from one coded captured image. In our prototype, digital in-line holography was used for imaging macroscopic, fast moving objects. The pixel-wise temporal modulation was implemented by a digital micromirror device. In this paper we demonstrate $10\times$ temporal super resolution with multiple depths recovery from a single image. Two examples are presented for the purpose of recording subtle vibrations and tracking small particles within 5 ms.



### Icon: An Interactive Approach to Train Deep Neural Networks for Segmentation of Neuronal Structures
- **Arxiv ID**: http://arxiv.org/abs/1610.09032v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.09032v1)
- **Published**: 2016-10-27 23:23:56+00:00
- **Updated**: 2016-10-27 23:23:56+00:00
- **Authors**: Felix Gonda, Verena Kaynig, Ray Thouis, Daniel Haehn, Jeff Lichtman, Toufiq Parag, Hanspeter Pfister
- **Comment**: None
- **Journal**: None
- **Summary**: We present an interactive approach to train a deep neural network pixel classifier for the segmentation of neuronal structures. An interactive training scheme reduces the extremely tedious manual annotation task that is typically required for deep networks to perform well on image segmentation problems. Our proposed method employs a feedback loop that captures sparse annotations using a graphical user interface, trains a deep neural network based on recent and past annotations, and displays the prediction output to users in almost real-time. Our implementation of the algorithm also allows multiple users to provide annotations in parallel and receive feedback from the same classifier. Quick feedback on classifier performance in an interactive setting enables users to identify and label examples that are more important than others for segmentation purposes. Our experiments show that an interactively-trained pixel classifier produces better region segmentation results on Electron Microscopy (EM) images than those generated by a network of the same architecture trained offline on exhaustive ground-truth labels.



