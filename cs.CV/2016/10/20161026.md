# Arxiv Papers in cs.CV on 2016-10-26
### Incremental Nonparametric Weighted Feature Extraction for OnlineSubspace Pattern Classification
- **Arxiv ID**: http://arxiv.org/abs/1610.08133v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.08133v1)
- **Published**: 2016-10-26 01:02:01+00:00
- **Updated**: 2016-10-26 01:02:01+00:00
- **Authors**: Hamid Abrishami Moghaddam, Elaheh Raisi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a new online method based on nonparametric weighted feature extraction (NWFE) is proposed. NWFE was introduced to enjoy optimum characteristics of linear discriminant analysis (LDA) and nonparametric discriminant analysis (NDA) while rectifying their drawbacks. It emphasizes the points near decision boundary by putting greater weights on them and deemphasizes other points. Incremental nonparametric weighted feature extraction (INWFE) is the online version of NWFE. INWFE has advantages of NWFE method such as extracting more than L-1 features in contrast to LDA. It is independent of the class distribution and performs well in complex distributed data. The effects of outliers are reduced due to the nature of its nonparametric scatter matrix. Furthermore, it is possible to add new samples asynchronously, i.e. whenever a new sample becomes available at any given time, it can be added to the algorithm. This is useful for many real world applications since all data cannot be available in advance. This method is implemented on Gaussian and non-Gaussian multidimensional data, a number of UCI datasets and Indian Pine dataset. Results are compared with NWFE in terms of classification accuracy and execution time. For nearest neighbour classifier it shows that this technique converges to NWFE at the end of learning process. In addition, the computational complexity is reduced in comparison with NWFE in terms of execution time.



### The Event-Camera Dataset and Simulator: Event-based Data for Pose Estimation, Visual Odometry, and SLAM
- **Arxiv ID**: http://arxiv.org/abs/1610.08336v4
- **DOI**: 10.1177/0278364917691115
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1610.08336v4)
- **Published**: 2016-10-26 13:59:39+00:00
- **Updated**: 2017-11-08 08:40:14+00:00
- **Authors**: Elias Mueggler, Henri Rebecq, Guillermo Gallego, Tobi Delbruck, Davide Scaramuzza
- **Comment**: 7 pages, 4 figures, 3 tables
- **Journal**: International Journal of Robotics Research, Vol. 36, Issue 2, pp.
  142-149, Feb. 2017
- **Summary**: New vision sensors, such as the Dynamic and Active-pixel Vision sensor (DAVIS), incorporate a conventional global-shutter camera and an event-based sensor in the same pixel array. These sensors have great potential for high-speed robotics and computer vision because they allow us to combine the benefits of conventional cameras with those of event-based sensors: low latency, high temporal resolution, and very high dynamic range. However, new algorithms are required to exploit the sensor characteristics and cope with its unconventional output, which consists of a stream of asynchronous brightness changes (called "events") and synchronous grayscale frames. For this purpose, we present and release a collection of datasets captured with a DAVIS in a variety of synthetic and real environments, which we hope will motivate research on new algorithms for high-speed and high-dynamic-range robotics and computer-vision applications. In addition to global-shutter intensity images and asynchronous events, we provide inertial measurements and ground-truth camera poses from a motion-capture system. The latter allows comparing the pose accuracy of ego-motion estimation algorithms quantitatively. All the data are released both as standard text files and binary files (i.e., rosbag). This paper provides an overview of the available data and describes a simulator that we release open-source to create synthetic event-camera data.



### Structured illumination microscopy with unknown patterns and a statistical prior
- **Arxiv ID**: http://arxiv.org/abs/1611.00287v2
- **DOI**: 10.1364/BOE.8.000695
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1611.00287v2)
- **Published**: 2016-10-26 14:55:42+00:00
- **Updated**: 2017-01-12 07:12:09+00:00
- **Authors**: Li-Hao Yeh, Lei Tian, Laura Waller
- **Comment**: None
- **Journal**: Biomed. Opt. Express 8, 695-711 (2017)
- **Summary**: Structured illumination microscopy (SIM) improves resolution by down-modulating high-frequency information of an object to fit within the passband of the optical system. Generally, the reconstruction process requires prior knowledge of the illumination patterns, which implies a well-calibrated and aberration-free system. Here, we propose a new \textit{algorithmic self-calibration} strategy for SIM that does not need to know the exact patterns {\it a priori}, but only their covariance. The algorithm, termed PE-SIMS, includes a Pattern-Estimation (PE) step requiring the uniformity of the sum of the illumination patterns and a SIM reconstruction procedure using a Statistical prior (SIMS). Additionally, we perform a pixel reassignment process (SIMS-PR) to enhance the reconstruction quality. We achieve 2$\times$ better resolution than a conventional widefield microscope, while remaining insensitive to aberration-induced pattern distortion and robust against parameter tuning.



### Video Analysis of "YouTube Funnies" to Aid the Study of Human Gait and Falls - Preliminary Results and Proof of Concept
- **Arxiv ID**: http://arxiv.org/abs/1610.08400v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.08400v1)
- **Published**: 2016-10-26 16:28:15+00:00
- **Updated**: 2016-10-26 16:28:15+00:00
- **Authors**: Babak Taati, Pranay Lohia, Avril Mansfield, Ahmed Ashraf
- **Comment**: 4 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: Because falls are funny, YouTube and other video sharing sites contain a large repository of real-life falls. We propose extracting gait and balance information from these videos to help us better understand some of the factors that contribute to falls. Proof-of-concept is explored in a single video containing multiple (n=14) falls/non-falls in the presence of an unexpected obstacle. The analysis explores: computing spatiotemporal parameters of gait in a video captured from an arbitrary viewpoint; the relationship between parameters of gait from the last few steps before the obstacle and falling vs. not falling; and the predictive capacity of a multivariate model in predicting a fall in the presence of an unexpected obstacle. Homography transformations correct the perspective projection distortion and allow for the consistent tracking of gait parameters as an individual walks in an arbitrary direction in the scene. A synthetic top view allows for computing the average stride length and a synthetic side view allows for measuring up and down motions of the head. In leave-one-out cross-validation, we were able to correctly predict whether a person would fall or not in 11 out of the 14 cases (78.6%), just by looking at the average stride length and the range of vertical head motion during the 1-4 most recent steps prior to reaching the obstacle.



### Universal adversarial perturbations
- **Arxiv ID**: http://arxiv.org/abs/1610.08401v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1610.08401v3)
- **Published**: 2016-10-26 16:30:45+00:00
- **Updated**: 2017-03-09 17:01:25+00:00
- **Authors**: Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal Frossard
- **Comment**: Accepted at IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR), 2017
- **Journal**: None
- **Summary**: Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images.



### Estimating the concentration of gold nanoparticles incorporated on Natural Rubber membranes using Multi-Level Starlet Optimal Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1610.08436v1
- **DOI**: 10.1007/s11051-014-2809-0
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.08436v1)
- **Published**: 2016-10-26 17:49:49+00:00
- **Updated**: 2016-10-26 17:49:49+00:00
- **Authors**: Alexandre Fioravante de Siqueira, Fl√°vio Camargo Cabrera, Aylton Pagamisse, Aldo Eloizo Job
- **Comment**: 22 pages, 8 figures
- **Journal**: J Nanopart Res (2014) 16: 2809
- **Summary**: This study consolidates Multi-Level Starlet Segmentation (MLSS) and Multi-Level Starlet Optimal Segmentation (MLSOS), techniques for photomicrograph segmentation that use starlet wavelet detail levels to separate areas of interest in an input image. Several segmentation levels can be obtained using Multi-Level Starlet Segmentation; after that, Matthews correlation coefficient (MCC) is used to choose an optimal segmentation level, giving rise to Multi-Level Starlet Optimal Segmentation. In this paper, MLSOS is employed to estimate the concentration of gold nanoparticles with diameter around 47 nm, reducted on natural rubber membranes. These samples were used on the construction of SERS/SERRS substrates and in the study of natural rubber membranes with incorporated gold nanoparticles influence on Leishmania braziliensis physiology. Precision, recall and accuracy are used to evaluate the segmentation performance, and MLSOS presents accuracy greater than 88% for this application.



### Volumetric Light-field Encryption at the Microscopic Scale
- **Arxiv ID**: http://arxiv.org/abs/1610.08762v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, physics.bio-ph, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1610.08762v1)
- **Published**: 2016-10-26 19:41:19+00:00
- **Updated**: 2016-10-26 19:41:19+00:00
- **Authors**: Haoyu Li, Changliang Guo, Inbarasan Muniraj, Bryce C. Schroeder, John T. Sheridan, Shu Jia
- **Comment**: None
- **Journal**: None
- **Summary**: We report a light-field based method that allows the optical encryption of three-dimensional (3D) volumetric information at the microscopic scale in a single 2D light-field image. The system consists of a microlens array and an array of random phase/amplitude masks. The method utilizes a wave optics model to account for the dominant diffraction effect at this new scale, and the system point-spread function (PSF) serves as the key for encryption and decryption. We successfully developed and demonstrated a deconvolution algorithm to retrieve spatially multiplexed discrete and continuous volumetric data from 2D light-field images. Showing that the method is practical for data transmission and storage, we obtained a faithful reconstruction of the 3D volumetric information from a digital copy of the encrypted light-field image. The method represents a new level of optical encryption, paving the way for broad industrial and biomedical applications in processing and securing 3D data at the microscopic scale.



### Mask-off: Synthesizing Face Images in the Presence of Head-mounted Displays
- **Arxiv ID**: http://arxiv.org/abs/1610.08481v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.08481v2)
- **Published**: 2016-10-26 19:41:37+00:00
- **Updated**: 2016-10-27 05:44:38+00:00
- **Authors**: Yajie Zhao, Qingguo Xu, Xinyu Huang, Ruigang Yang
- **Comment**: 12 pages,15 figures
- **Journal**: None
- **Summary**: A head-mounted display (HMD) could be an important component of augmented reality system. However, as the upper face region is seriously occluded by the device, the user experience could be affected in applications such as telecommunication and multi-player video games. In this paper, we first present a novel experimental setup that consists of two near-infrared (NIR) cameras to point to the eye regions and one visible-light RGB camera to capture the visible face region. The main purpose of this paper is to synthesize realistic face images without occlusions based on the images captured by these cameras. To this end, we propose a novel synthesis framework that contains four modules: 3D head reconstruction, face alignment and tracking, face synthesis, and eye synthesis. In face synthesis, we propose a novel algorithm that can robustly align and track a personalized 3D head model given a face that is severely occluded by the HMD. In eye synthesis, in order to generate accurate eye movements and dynamic wrinkle variations around eye regions, we propose another novel algorithm to colorize the NIR eye images and further remove the "red eye" effects caused by the colorization. Results show that both hardware setup and system framework are robust to synthesize realistic face images in video sequences.



### Robust Cardiac Motion Estimation using Ultrafast Ultrasound Data: A Low-Rank-Topology-Preserving Approach
- **Arxiv ID**: http://arxiv.org/abs/1611.02730v2
- **DOI**: 10.1088/1361-6560/aa6914
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.02730v2)
- **Published**: 2016-10-26 22:38:48+00:00
- **Updated**: 2017-04-25 13:24:48+00:00
- **Authors**: Angelica I. Aviles, Thomas Widlak, Alicia Casals, Maartje M. Nillesen, Habib Ammari
- **Comment**: 15 pages, 10 figures, Physics in Medicine and Biology, 2017
- **Journal**: None
- **Summary**: Cardiac motion estimation is an important diagnostic tool to detect heart diseases and it has been explored with modalities such as MRI and conventional ultrasound (US) sequences. US cardiac motion estimation still presents challenges because of the complex motion patterns and the presence of noise. In this work, we propose a novel approach to estimate the cardiac motion using ultrafast ultrasound data. -- Our solution is based on a variational formulation characterized by the L2-regularized class. The displacement is represented by a lattice of b-splines and we ensure robustness by applying a maximum likelihood type estimator. While this is an important part of our solution, the main highlight of this paper is to combine a low-rank data representation with topology preservation. Low-rank data representation (achieved by finding the k-dominant singular values of a Casorati Matrix arranged from the data sequence) speeds up the global solution and achieves noise reduction. On the other hand, topology preservation (achieved by monitoring the Jacobian determinant) allows to radically rule out distortions while carefully controlling the size of allowed expansions and contractions. Our variational approach is carried out on a realistic dataset as well as on a simulated one. We demonstrate how our proposed variational solution deals with complex deformations through careful numerical experiments. While maintaining the accuracy of the solution, the low-rank preprocessing is shown to speed up the convergence of the variational problem. Beyond cardiac motion estimation, our approach is promising for the analysis of other organs that experience motion.



