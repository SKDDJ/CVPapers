# Arxiv Papers in cs.CV on 2016-10-07
### A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1610.02136v3
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1610.02136v3)
- **Published**: 2016-10-07 04:06:01+00:00
- **Updated**: 2018-10-03 07:32:57+00:00
- **Authors**: Dan Hendrycks, Kevin Gimpel
- **Comment**: Published as a conference paper at ICLR 2017. 1 Figure in 1 Appendix.
  Minor changes from the previous version
- **Journal**: International Conference on Learning Representations 2017
- **Summary**: We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.



### Automatic Liver and Lesion Segmentation in CT Using Cascaded Fully Convolutional Neural Networks and 3D Conditional Random Fields
- **Arxiv ID**: http://arxiv.org/abs/1610.02177v1
- **DOI**: 10.1007/978-3-319-46723-8_48
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.02177v1)
- **Published**: 2016-10-07 08:23:32+00:00
- **Updated**: 2016-10-07 08:23:32+00:00
- **Authors**: Patrick Ferdinand Christ, Mohamed Ezzeldin A. Elshaer, Florian Ettlinger, Sunil Tatavarty, Marc Bickel, Patrick Bilic, Markus Rempfler, Marco Armbruster, Felix Hofmann, Melvin D'Anastasi, Wieland H. Sommer, Seyed-Ahmad Ahmadi, Bjoern H. Menze
- **Comment**: Accepted at MICCAI 2016. Source code available on
  https://github.com/IBBM/Cascaded-FCN
- **Journal**: None
- **Summary**: Automatic segmentation of the liver and its lesion is an important step towards deriving quantitative biomarkers for accurate clinical diagnosis and computer-aided decision support systems. This paper presents a method to automatically segment liver and lesions in CT abdomen images using cascaded fully convolutional neural networks (CFCNs) and dense 3D conditional random fields (CRFs). We train and cascade two FCNs for a combined segmentation of the liver and its lesions. In the first step, we train a FCN to segment the liver as ROI input for a second FCN. The second FCN solely segments lesions from the predicted liver ROIs of step 1. We refine the segmentations of the CFCN using a dense 3D CRF that accounts for both spatial coherence and appearance. CFCN models were trained in a 2-fold cross-validation on the abdominal CT dataset 3DIRCAD comprising 15 hepatic tumor volumes. Our results show that CFCN-based semantic liver and lesion segmentation achieves Dice scores over 94% for liver with computation times below 100s per volume. We experimentally demonstrate the robustness of the proposed method as a decision support system with a high accuracy and speed for usage in daily clinical routine.



### Weakly supervised learning of actions from transcripts
- **Arxiv ID**: http://arxiv.org/abs/1610.02237v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.02237v2)
- **Published**: 2016-10-07 12:00:08+00:00
- **Updated**: 2017-06-19 09:25:13+00:00
- **Authors**: Hilde Kuehne, Alexander Richard, Juergen Gall
- **Comment**: 33 pages, 9 figures, to appear in CVIU
- **Journal**: None
- **Summary**: We present an approach for weakly supervised learning of human actions from video transcriptions. Our system is based on the idea that, given a sequence of input data and a transcript, i.e. a list of the order the actions occur in the video, it is possible to infer the actions within the video stream, and thus, learn the related action models without the need for any frame-based annotation. Starting from the transcript information at hand, we split the given data sequences uniformly based on the number of expected actions. We then learn action models for each class by maximizing the probability that the training video sequences are generated by the action models given the sequence order as defined by the transcripts. The learned model can be used to temporally segment an unseen video with or without transcript. We evaluate our approach on four distinct activity datasets, namely Hollywood Extended, MPII Cooking, Breakfast and CRIM13. We show that our system is able to align the scripted actions with the video data and that the learned models localize and classify actions competitively in comparison to models trained with full supervision, i.e. with frame level annotations, and that they outperform any current state-of-the-art approach for aligning transcripts with video data.



### Automated Detection of Individual Micro-calcifications from Mammograms using a Multi-stage Cascade Approach
- **Arxiv ID**: http://arxiv.org/abs/1610.02251v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.02251v1)
- **Published**: 2016-10-07 12:36:21+00:00
- **Updated**: 2016-10-07 12:36:21+00:00
- **Authors**: Zhi Lu, Gustavo Carneiro, Neeraj Dhungel, Andrew P. Bradley
- **Comment**: 5 Pages, ISBI 2017 Submission
- **Journal**: None
- **Summary**: In mammography, the efficacy of computer-aided detection methods depends, in part, on the robust localisation of micro-calcifications ($\mu$C). Currently, the most effective methods are based on three steps: 1) detection of individual $\mu$C candidates, 2) clustering of individual $\mu$C candidates, and 3) classification of $\mu$C clusters. Where the second step is motivated both to reduce the number of false positive detections from the first step and on the evidence that malignancy depends on a relatively large number of $\mu$C detections within a certain area. In this paper, we propose a novel approach to $\mu$C detection, consisting of the detection \emph{and} classification of individual $\mu$C candidates, using shape and appearance features, using a cascade of boosting classifiers. The final step in our approach then clusters the remaining individual $\mu$C candidates. The main advantage of this approach lies in its ability to reject a significant number of false positive $\mu$C candidates compared to previously proposed methods. Specifically, on the INbreast dataset, we show that our approach has a true positive rate (TPR) for individual $\mu$Cs of 40\% at one false positive per image (FPI) and a TPR of 80\% at 10 FPI. These results are significantly more accurate than the current state of the art, which has a TPR of less than 1\% at one FPI and a TPR of 10\% at 10 FPI. Our results are competitive with the state of the art at the subsequent stage of detecting clusters of $\mu$Cs.



### Learning Grimaces by Watching TV
- **Arxiv ID**: http://arxiv.org/abs/1610.02255v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.02255v1)
- **Published**: 2016-10-07 12:42:47+00:00
- **Updated**: 2016-10-07 12:42:47+00:00
- **Authors**: Samuel Albanie, Andrea Vedaldi
- **Comment**: British Machine Vision Conference (BMVC) 2016
- **Journal**: None
- **Summary**: Differently from computer vision systems which require explicit supervision, humans can learn facial expressions by observing people in their environment. In this paper, we look at how similar capabilities could be developed in machine vision. As a starting point, we consider the problem of relating facial expressions to objectively measurable events occurring in videos. In particular, we consider a gameshow in which contestants play to win significant sums of money. We extract events affecting the game and corresponding facial expressions objectively and automatically from the videos, obtaining large quantities of labelled data for our study. We also develop, using benchmarks such as FER and SFEW 2.0, state-of-the-art deep neural networks for facial expression recognition, showing that pre-training on face verification data can be highly beneficial for this task. Then, we extend these models to use facial expressions to predict events in videos and learn nameable expressions from them. The dataset and emotion recognition models are available at http://www.robots.ox.ac.uk/~vgg/data/facevalue



### ILGNet: Inception Modules with Connected Local and Global Features for Efficient Image Aesthetic Quality Classification using Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1610.02256v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/1610.02256v3)
- **Published**: 2016-10-07 12:46:45+00:00
- **Updated**: 2018-04-29 23:43:04+00:00
- **Authors**: Xin Jin, Le Wu, Xiaodong Li, Xiaokun Zhang, Jingying Chi, Siwei Peng, Shiming Ge, Geng Zhao, Shuying Li
- **Comment**: under review, IET-Computer Vision, Previous WCSP2016 paper
- **Journal**: None
- **Summary**: In this paper, we address a challenging problem of aesthetic image classification, which is to label an input image as high or low aesthetic quality. We take both the local and global features of images into consideration. A novel deep convolutional neural network named ILGNet is proposed, which combines both the Inception modules and an connected layer of both Local and Global features. The ILGnet is based on GoogLeNet. Thus, it is easy to use a pre-trained GoogLeNet for large-scale image classification problem and fine tune our connected layers on an large scale database of aesthetic related images: AVA, i.e. \emph{domain adaptation}. The experiments reveal that our model achieves the state of the arts in AVA database. Both the training and testing speeds of our model are higher than those of the original GoogLeNet.



### Optimization of Convolutional Neural Network using Microcanonical Annealing Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1610.02306v1
- **DOI**: None
- **Categories**: **cs.CV**, 68Txx, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1610.02306v1)
- **Published**: 2016-10-07 14:39:50+00:00
- **Updated**: 2016-10-07 14:39:50+00:00
- **Authors**: Vina Ayumi, L. M. Rasdi Rere, Mohamad Ivan Fanany, Aniati Murni Arymurthy
- **Comment**: Accepted to be published at IEEE ICACSIS 2016. arXiv admin note: text
  overlap with arXiv:1610.01925
- **Journal**: None
- **Summary**: Convolutional neural network (CNN) is one of the most prominent architectures and algorithm in Deep Learning. It shows a remarkable improvement in the recognition and classification of objects. This method has also been proven to be very effective in a variety of computer vision and machine learning problems. As in other deep learning, however, training the CNN is interesting yet challenging. Recently, some metaheuristic algorithms have been used to optimize CNN using Genetic Algorithm, Particle Swarm Optimization, Simulated Annealing and Harmony Search. In this paper, another type of metaheuristic algorithms with different strategy has been proposed, i.e. Microcanonical Annealing to optimize Convolutional Neural Network. The performance of the proposed method is tested using the MNIST and CIFAR-10 datasets. Although experiment results of MNIST dataset indicate the increase in computation time (1.02x - 1.38x), nevertheless this proposed method can considerably enhance the performance of the original CNN (up to 4.60\%). On the CIFAR10 dataset, currently, state of the art is 96.53\% using fractional pooling, while this proposed method achieves 99.14\%.



### Xception: Deep Learning with Depthwise Separable Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1610.02357v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.02357v3)
- **Published**: 2016-10-07 17:51:51+00:00
- **Updated**: 2017-04-04 18:40:27+00:00
- **Authors**: François Chollet
- **Comment**: None
- **Journal**: None
- **Summary**: We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.



### Distributed Averaging CNN-ELM for Big Data
- **Arxiv ID**: http://arxiv.org/abs/1610.02373v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DC, 68T05, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1610.02373v1)
- **Published**: 2016-10-07 18:59:23+00:00
- **Updated**: 2016-10-07 18:59:23+00:00
- **Authors**: Arif Budiman, Mohamad Ivan Fanany, Chan Basaruddin
- **Comment**: Submitted to IEEE Transactions on Systems, Man and Cybernetics:
  Systems
- **Journal**: None
- **Summary**: Increasing the scalability of machine learning to handle big volume of data is a challenging task. The scale up approach has some limitations. In this paper, we proposed a scale out approach for CNN-ELM based on MapReduce on classifier level. Map process is the CNN-ELM training for certain partition of data. It involves many CNN-ELM models that can be trained asynchronously. Reduce process is the averaging of all CNN-ELM weights as final training result. This approach can save a lot of training time than single CNN-ELM models trained alone. This approach also increased the scalability of machine learning by combining scale out and scale up approaches. We verified our method in extended MNIST data set and not-MNIST data set experiment. However, it has some drawbacks by additional iteration learning parameters that need to be carefully taken and training data distribution that need to be carefully selected. Further researches to use more complex image data set are required.



### Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization
- **Arxiv ID**: http://arxiv.org/abs/1610.02391v4
- **DOI**: 10.1007/s11263-019-01228-7
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1610.02391v4)
- **Published**: 2016-10-07 19:54:24+00:00
- **Updated**: 2019-12-03 02:13:03+00:00
- **Authors**: Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra
- **Comment**: This version was published in International Journal of Computer
  Vision (IJCV) in 2019; A previous version of the paper was published at
  International Conference on Computer Vision (ICCV'17)
- **Journal**: None
- **Summary**: We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.



### Indoor Space Recognition using Deep Convolutional Neural Network: A Case Study at MIT Campus
- **Arxiv ID**: http://arxiv.org/abs/1610.02414v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.02414v1)
- **Published**: 2016-10-07 20:24:04+00:00
- **Updated**: 2016-10-07 20:24:04+00:00
- **Authors**: Fan Zhang, Fabio Duarte, Ruixian Ma, Dimitrios Milioris, Hui Lin, Carlo Ratti
- **Comment**: 22 pages; 14 figures
- **Journal**: None
- **Summary**: In this paper, we propose a robust and parsimonious approach using Deep Convolutional Neural Network (DCNN) to recognize and interpret interior space. DCNN has achieved incredible success in object and scene recognition. In this study we design and train a DCNN to classify a pre-zoning indoor space, and from a single phone photo to recognize the learned space features, with no need of additional assistive technology. We collect more than 600,000 images inside MIT campus buildings to train our DCNN model, and achieved 97.9% accuracy in validation dataset and 81.7% accuracy in test dataset based on spatial-scale fixed model. Furthermore, the recognition accuracy and spatial resolution can be potentially improved through multiscale classification model. We identify the discriminative image regions through Class Activating Mapping (CAM) technique, to observe the model's behavior in how to recognize space and interpret it in an abstract way. By evaluating the results with misclassification matrix, we investigate the visual spatial feature of interior space by looking into its visual similarity and visual distinctiveness, giving insights into interior design and human indoor perception and wayfinding research. The contribution of this paper is threefold. First, we propose a robust and parsimonious approach for indoor navigation using DCNN. Second, we demonstrate that DCNN also has a potential capability in space feature learning and recognition, even under severe appearance changes. Third, we introduce a DCNN based approach to look into the visual similarity and visual distinctiveness of interior space.



### Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models
- **Arxiv ID**: http://arxiv.org/abs/1610.02424v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1610.02424v2)
- **Published**: 2016-10-07 20:56:47+00:00
- **Updated**: 2018-10-22 13:48:32+00:00
- **Authors**: Ashwin K Vijayakumar, Michael Cogswell, Ramprasath R. Selvaraju, Qing Sun, Stefan Lee, David Crandall, Dhruv Batra
- **Comment**: 16 pages; accepted at AAAI 2018
- **Journal**: None
- **Summary**: Neural sequence models are widely used to model time-series data. Equally ubiquitous is the usage of beam search (BS) as an approximate inference algorithm to decode output sequences from these models. BS explores the search space in a greedy left-right fashion retaining only the top-B candidates - resulting in sequences that differ only slightly from each other. Producing lists of nearly identical sequences is not only computationally wasteful but also typically fails to capture the inherent ambiguity of complex AI tasks. To overcome this problem, we propose Diverse Beam Search (DBS), an alternative to BS that decodes a list of diverse outputs by optimizing for a diversity-augmented objective. We observe that our method finds better top-1 solutions by controlling for the exploration and exploitation of the search space - implying that DBS is a better search algorithm. Moreover, these gains are achieved with minimal computational or memory over- head as compared to beam search. To demonstrate the broad applicability of our method, we present results on image captioning, machine translation and visual question generation using both standard quantitative metrics and qualitative human studies. Further, we study the role of diversity for image-grounded language generation tasks as the complexity of the image changes. We observe that our method consistently outperforms BS and previously proposed techniques for diverse decoding from neural sequence models.



### ResearchDoom and CocoDoom: Learning Computer Vision with Games
- **Arxiv ID**: http://arxiv.org/abs/1610.02431v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.02431v1)
- **Published**: 2016-10-07 21:35:02+00:00
- **Updated**: 2016-10-07 21:35:02+00:00
- **Authors**: A. Mahendran, H. Bilen, J. F. Henriques, A. Vedaldi
- **Comment**: None
- **Journal**: None
- **Summary**: In this short note we introduce ResearchDoom, an implementation of the Doom first-person shooter that can extract detailed metadata from the game. We also introduce the CocoDoom dataset, a collection of pre-recorded data extracted from Doom gaming sessions along with annotations in the MS Coco format. ResearchDoom and CocoDoom can be used to train and evaluate a variety of computer vision methods such as object recognition, detection and segmentation at the level of instances and categories, tracking, ego-motion estimation, monocular depth estimation and scene segmentation. The code and data are available at http://www.robots.ox.ac.uk/~vgg/research/researchdoom.



