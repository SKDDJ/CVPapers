# Arxiv Papers in cs.CV on 2016-10-18
### ARTiS: Appearance-based Action Recognition in Task Space for Real-Time Human-Robot Collaboration
- **Arxiv ID**: http://arxiv.org/abs/1610.05432v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1610.05432v2)
- **Published**: 2016-10-18 04:45:03+00:00
- **Updated**: 2017-03-07 03:58:38+00:00
- **Authors**: Markus Eich, Sareh Shirazi, Gordon Wyeth
- **Comment**: None
- **Journal**: None
- **Summary**: To have a robot actively supporting a human during a collaborative task, it is crucial that robots are able to identify the current action in order to predict the next one. Common approaches make use of high-level knowledge, such as object affordances, semantics or understanding of actions in terms of pre- and post-conditions. These approaches often require hand-coded a priori knowledge, time- and resource-intensive or supervised learning techniques.   We propose to reframe this problem as an appearance-based place recognition problem. In our framework, we regard sequences of visual images of human actions as a map in analogy to the visual place recognition problem. Observing the task for the second time, our approach is able to recognize pre-observed actions in a one-shot learning approach and is thereby able to recognize the current observation in the task space. We propose two new methods for creating and aligning action observations within a task map. We compare and verify our approaches with real data of humans assembling several types of IKEA flat packs.



### Real-time analysis of cataract surgery videos using statistical models
- **Arxiv ID**: http://arxiv.org/abs/1610.05465v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.05465v1)
- **Published**: 2016-10-18 07:55:48+00:00
- **Updated**: 2016-10-18 07:55:48+00:00
- **Authors**: Katia Charrière, Gwenolé Quellec, Mathieu Lamard, David Martiano, Guy Cazuguel, Gouenou Coatrieux, Béatrice Cochener
- **Comment**: This is an extended version of a paper presented at the CBMI 2016
  conference
- **Journal**: None
- **Summary**: The automatic analysis of the surgical process, from videos recorded during surgeries, could be very useful to surgeons, both for training and for acquiring new techniques. The training process could be optimized by automatically providing some targeted recommendations or warnings, similar to the expert surgeon's guidance. In this paper, we propose to reuse videos recorded and stored during cataract surgeries to perform the analysis. The proposed system allows to automatically recognize, in real time, what the surgeon is doing: what surgical phase or, more precisely, what surgical step he or she is performing. This recognition relies on the inference of a multilevel statistical model which uses 1) the conditional relations between levels of description (steps and phases) and 2) the temporal relations among steps and among phases. The model accepts two types of inputs: 1) the presence of surgical tools, manually provided by the surgeons, or 2) motion in videos, automatically analyzed through the Content Based Video retrieval (CBVR) paradigm. Different data-driven statistical models are evaluated in this paper. For this project, a dataset of 30 cataract surgery videos was collected at Brest University hospital. The system was evaluated in terms of area under the ROC curve. Promising results were obtained using either the presence of surgical tools ($A_z$ = 0.983) or motion analysis ($A_z$ = 0.759). The generality of the method allows to adapt it to any kinds of surgeries. The proposed solution could be used in a computer assisted surgery tool to support surgeons during the surgery.



### Shape-based defect classification for Non Destructive Testing
- **Arxiv ID**: http://arxiv.org/abs/1610.05518v1
- **DOI**: 10.1109/MetroAeroSpace.2015.7180691
- **Categories**: **cs.CV**, cs.CE
- **Links**: [PDF](http://arxiv.org/pdf/1610.05518v1)
- **Published**: 2016-10-18 10:03:25+00:00
- **Updated**: 2016-10-18 10:03:25+00:00
- **Authors**: Gianni D'Angelo, Salvatore Rampone
- **Comment**: 5 pages, IEEE International Workshop
- **Journal**: IEEE International Workshop on Metrology for Aerospace, Benevento,
  Italy, June 4-5, 2015
- **Summary**: The aim of this work is to classify the aerospace structure defects detected by eddy current non-destructive testing. The proposed method is based on the assumption that the defect is bound to the reaction of the probe coil impedance during the test. Impedance plane analysis is used to extract a feature vector from the shape of the coil impedance in the complex plane, through the use of some geometric parameters. Shape recognition is tested with three different machine-learning based classifiers: decision trees, neural networks and Naive Bayes. The performance of the proposed detection system are measured in terms of accuracy, sensitivity, specificity, precision and Matthews correlation coefficient. Several experiments are performed on dataset of eddy current signal samples for aircraft structures. The obtained results demonstrate the usefulness of our approach and the competiveness against existing descriptors.



### M2CAI Workflow Challenge: Convolutional Neural Networks with Time Smoothing and Hidden Markov Model for Video Frames Classification
- **Arxiv ID**: http://arxiv.org/abs/1610.05541v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.05541v2)
- **Published**: 2016-10-18 11:35:22+00:00
- **Updated**: 2016-12-02 11:07:39+00:00
- **Authors**: Rémi Cadène, Thomas Robert, Nicolas Thome, Matthieu Cord
- **Comment**: None
- **Journal**: None
- **Summary**: Our approach is among the three best to tackle the M2CAI Workflow challenge. The latter consists in recognizing the operation phase for each frames of endoscopic videos. In this technical report, we compare several classification models and temporal smoothing methods. Our submitted solution is a fine tuned Residual Network-200 on 80% of the training set with temporal smoothing using simple temporal averaging of the predictions and a Hidden Markov Model modeling the sequence.



### Master's Thesis : Deep Learning for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1610.05567v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.05567v1)
- **Published**: 2016-10-18 12:26:49+00:00
- **Updated**: 2016-10-18 12:26:49+00:00
- **Authors**: Rémi Cadène, Nicolas Thome, Matthieu Cord
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of our research is to develop methods advancing automatic visual recognition. In order to predict the unique or multiple labels associated to an image, we study different kind of Deep Neural Networks architectures and methods for supervised features learning. We first draw up a state-of-the-art review of the Convolutional Neural Networks aiming to understand the history behind this family of statistical models, the limit of modern architectures and the novel techniques currently used to train deep CNNs. The originality of our work lies in our approach focusing on tasks with a low amount of data. We introduce different models and techniques to achieve the best accuracy on several kind of datasets, such as a medium dataset of food recipes (100k images) for building a web API, or a small dataset of satellite images (6,000) for the DSG online challenge that we've won. We also draw up the state-of-the-art in Weakly Supervised Learning, introducing different kind of CNNs able to localize regions of interest. Our last contribution is a framework, build on top of Torch7, for training and testing deep models on any visual recognition tasks and on datasets of any scale.



### Deep Identity-aware Transfer of Facial Attributes
- **Arxiv ID**: http://arxiv.org/abs/1610.05586v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.05586v2)
- **Published**: 2016-10-18 12:56:47+00:00
- **Updated**: 2018-12-06 13:36:08+00:00
- **Authors**: Mu Li, Wangmeng Zuo, David Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a Deep convolutional network model for Identity-Aware Transfer (DIAT) of facial attributes. Given the source input image and the reference attribute, DIAT aims to generate a facial image that owns the reference attribute as well as keeps the same or similar identity to the input image. In general, our model consists of a mask network and an attribute transform network which work in synergy to generate a photo-realistic facial image with the reference attribute. Considering that the reference attribute may be only related to some parts of the image, the mask network is introduced to avoid the incorrect editing on attribute irrelevant region. Then the estimated mask is adopted to combine the input and transformed image for producing the transfer result. For joint training of transform network and mask network, we incorporate the adversarial attribute loss, identity-aware adaptive perceptual loss, and VGG-FACE based identity loss. Furthermore, a denoising network is presented to serve for perceptual regularization to suppress the artifacts in transfer result, while an attribute ratio regularization is introduced to constrain the size of attribute relevant region. Our DIAT can provide a unified solution for several representative facial attribute transfer tasks, e.g., expression transfer, accessory removal, age progression, and gender transfer, and can be extended for other face enhancement tasks such as face hallucination. The experimental results validate the effectiveness of the proposed method. Even for the identity-related attribute (e.g., gender), our DIAT can obtain visually impressive results by changing the attribute while retaining most identity-aware features.



### From Traditional to Modern : Domain Adaptation for Action Classification in Short Social Video Clips
- **Arxiv ID**: http://arxiv.org/abs/1610.05613v1
- **DOI**: 10.1007/978-3-319-45886-1_20
- **Categories**: **cs.CV**, I.5.4; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1610.05613v1)
- **Published**: 2016-10-18 13:45:32+00:00
- **Updated**: 2016-10-18 13:45:32+00:00
- **Authors**: Aditya Singh, Saurabh Saini, Rajvi Shah, P J Narayanan
- **Comment**: 9 pages, GCPR, 2016
- **Journal**: Pattern Recognition,38th German Conference, GCPR 2016, Hannover,
  Germany, September 12-15, 2016, Proceedings,pp 245-257
- **Summary**: Short internet video clips like vines present a significantly wild distribution compared to traditional video datasets. In this paper, we focus on the problem of unsupervised action classification in wild vines using traditional labeled datasets. To this end, we use a data augmentation based simple domain adaptation strategy. We utilise semantic word2vec space as a common subspace to embed video features from both, labeled source domain and unlablled target domain. Our method incrementally augments the labeled source with target samples and iteratively modifies the embedding function to bring the source and target distributions together. Additionally, we utilise a multi-modal representation that incorporates noisy semantic information available in form of hash-tags. We show the effectiveness of this simple adaptation technique on a test set of vines and achieve notable improvements in performance.



### Semantic Decomposition and Recognition of Long and Complex Manipulation Action Sequences
- **Arxiv ID**: http://arxiv.org/abs/1610.05693v1
- **DOI**: 10.1007/s11263-016-0956-8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.05693v1)
- **Published**: 2016-10-18 16:13:59+00:00
- **Updated**: 2016-10-18 16:13:59+00:00
- **Authors**: Eren Erdal Aksoy, Adil Orhan, Florentin Woergoetter
- **Comment**: IJCV preprint manuscript
- **Journal**: International Journal of Computer Vision, 2017
- **Summary**: Understanding continuous human actions is a non-trivial but important problem in computer vision. Although there exists a large corpus of work in the recognition of action sequences, most approaches suffer from problems relating to vast variations in motions, action combinations, and scene contexts. In this paper, we introduce a novel method for semantic segmentation and recognition of long and complex manipulation action tasks, such as "preparing a breakfast" or "making a sandwich". We represent manipulations with our recently introduced "Semantic Event Chain" (SEC) concept, which captures the underlying spatiotemporal structure of an action invariant to motion, velocity, and scene context. Solely based on the spatiotemporal interactions between manipulated objects and hands in the extracted SEC, the framework automatically parses individual manipulation streams performed either sequentially or concurrently. Using event chains, our method further extracts basic primitive elements of each parsed manipulation. Without requiring any prior object knowledge, the proposed framework can also extract object-like scene entities that exhibit the same role in semantically similar manipulations. We conduct extensive experiments on various recent datasets to validate the robustness of the framework.



### Fast L1-NMF for Multiple Parametric Model Estimation
- **Arxiv ID**: http://arxiv.org/abs/1610.05712v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1610.05712v2)
- **Published**: 2016-10-18 17:20:38+00:00
- **Updated**: 2016-11-11 15:54:14+00:00
- **Authors**: Mariano Tepper, Guillermo Sapiro
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we introduce a comprehensive algorithmic pipeline for multiple parametric model estimation. The proposed approach analyzes the information produced by a random sampling algorithm (e.g., RANSAC) from a machine learning/optimization perspective, using a \textit{parameterless} biclustering algorithm based on L1 nonnegative matrix factorization (L1-NMF). The proposed framework exploits consistent patterns that naturally arise during the RANSAC execution, while explicitly avoiding spurious inconsistencies. Contrarily to the main trends in the literature, the proposed technique does not impose non-intersecting parametric models. A new accelerated algorithm to compute L1-NMFs allows to handle medium-sized problems faster while also extending the usability of the algorithm to much larger datasets. This accelerated algorithm has applications in any other context where an L1-NMF is needed, beyond the biclustering approach to parameter estimation here addressed. We accompany the algorithmic presentation with theoretical foundations and numerous and diverse examples.



### Robot Vision Architecture for Autonomous Clothes Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1610.05824v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1610.05824v1)
- **Published**: 2016-10-18 22:56:16+00:00
- **Updated**: 2016-10-18 22:56:16+00:00
- **Authors**: Li Sun, Gerardo Aragon-Camarasa, Simon Rogers, J. Paul Siebert
- **Comment**: 14 pages, under review
- **Journal**: None
- **Summary**: This paper presents a novel robot vision architecture for perceiving generic 3D clothes configurations. Our architecture is hierarchically structured, starting from low-level curvatures, across mid-level geometric shapes \& topology descriptions; and finally approaching high-level semantic surface structure descriptions. We demonstrate our robot vision architecture in a customised dual-arm industrial robot with our self-designed, off-the-self stereo vision system, carrying out autonomous grasping and dual-arm flattening. It is worth noting that the proposed dual-arm flattening approach is unique among the state-of-the-art robot autonomous system, which is the major contribution of this paper. The experimental results show that the proposed dual-arm flattening using stereo vision system remarkably outperforms the single-arm flattening and widely-cited Kinect-based sensing system for dexterous manipulation tasks. In addition, the proposed grasping approach achieves satisfactory performance on grasping various kind of garments, verifying the capability of proposed visual perception architecture to be adapted to more than one clothing manipulation tasks.



