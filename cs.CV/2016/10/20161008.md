# Arxiv Papers in cs.CV on 2016-10-08
### Learning What and Where to Draw
- **Arxiv ID**: http://arxiv.org/abs/1610.02454v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1610.02454v1)
- **Published**: 2016-10-08 00:27:57+00:00
- **Updated**: 2016-10-08 00:27:57+00:00
- **Authors**: Scott Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka, Bernt Schiele, Honglak Lee
- **Comment**: In NIPS 2016
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have recently demonstrated the capability to synthesize compelling real-world images, such as room interiors, album covers, manga, faces, birds, and flowers. While existing models can synthesize images based on global constraints such as a class label or caption, they do not provide control over pose or object location. We propose a new model, the Generative Adversarial What-Where Network (GAWWN), that synthesizes images given instructions describing what content to draw in which location. We show high-quality 128 x 128 image synthesis on the Caltech-UCSD Birds dataset, conditioned on both informal text descriptions and also object location. Our system exposes control over both the bounding box around the bird and its constituent parts. By modeling the conditional distributions over part locations, our system also enables conditioning on arbitrary subsets of parts (e.g. only the beak and tail), yielding an efficient interface for picking part locations. We also show preliminary results on the more challenging domain of text- and location-controllable synthesis of images of human actions on the MPII Human Pose dataset.



### Deep Convolutional Networks as Models of Generalization and Blending Within Visual Creativity
- **Arxiv ID**: http://arxiv.org/abs/1610.02478v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1610.02478v2)
- **Published**: 2016-10-08 04:15:26+00:00
- **Updated**: 2019-07-15 21:02:30+00:00
- **Authors**: Graeme McCaig, Steve DiPaola, Liane Gabora
- **Comment**: 8 pages, In Proceedings of the 7th International Conference on
  Computational Creativity. Palo Alto: Association for the Advancement of
  Artificial Intelligence (AAAI) Press (2016)
- **Journal**: In Proceedings of the 7th International Conference on
  Computational Creativity (pp. 156-163). Palo Alto, CA: Association for the
  Advancement of Artificial Intelligence (AAAI) Press. (2016)
- **Summary**: We examine two recent artificial intelligence (AI) based deep learning algorithms for visual blending in convolutional neural networks (Mordvintsev et al. 2015, Gatys et al. 2015). To investigate the potential value of these algorithms as tools for computational creativity research, we explain and schematize the essential aspects of the algorithms' operation and give visual examples of their output. We discuss the relationship of the two algorithms to human cognitive science theories of creativity such as conceptual blending theory and honing theory, and characterize the algorithms with respect to generation of novelty and aesthetic quality.



### 4D Crop Monitoring: Spatio-Temporal Reconstruction for Agriculture
- **Arxiv ID**: http://arxiv.org/abs/1610.02482v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1610.02482v1)
- **Published**: 2016-10-08 04:34:25+00:00
- **Updated**: 2016-10-08 04:34:25+00:00
- **Authors**: Jing Dong, John Gary Burnham, Byron Boots, Glen C. Rains, Frank Dellaert
- **Comment**: Submitted to IEEE International Conference on Robotics and Automation
  (ICRA) 2017
- **Journal**: None
- **Summary**: Autonomous crop monitoring at high spatial and temporal resolution is a critical problem in precision agriculture. While Structure from Motion and Multi-View Stereo algorithms can finely reconstruct the 3D structure of a field with low-cost image sensors, these algorithms fail to capture the dynamic nature of continuously growing crops. In this paper we propose a 4D reconstruction approach to crop monitoring, which employs a spatio-temporal model of dynamic scenes that is useful for precision agriculture applications. Additionally, we provide a robust data association algorithm to address the problem of large appearance changes due to scenes being viewed from different angles at different points in time, which is critical to achieving 4D reconstruction. Finally, we collected a high quality dataset with ground truth statistics to evaluate the performance of our method. We demonstrate that our 4D reconstruction approach provides models that are qualitatively correct with respect to visual appearance and quantitatively accurate when measured against the ground truth geometric properties of the monitored crops.



### Boost K-Means
- **Arxiv ID**: http://arxiv.org/abs/1610.02483v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DB
- **Links**: [PDF](http://arxiv.org/pdf/1610.02483v2)
- **Published**: 2016-10-08 04:36:42+00:00
- **Updated**: 2016-12-04 07:32:37+00:00
- **Authors**: Wan-Lei Zhao, Cheng-Hao Deng, Chong-Wah Ngo
- **Comment**: 11 pages, 6 figures
- **Journal**: None
- **Summary**: Due to its simplicity and versatility, k-means remains popular since it was proposed three decades ago. The performance of k-means has been enhanced from different perspectives over the years. Unfortunately, a good trade-off between quality and efficiency is hardly reached. In this paper, a novel k-means variant is presented. Different from most of k-means variants, the clustering procedure is driven by an explicit objective function, which is feasible for the whole l2-space. The classic egg-chicken loop in k-means has been simplified to a pure stochastic optimization procedure. The procedure of k-means becomes simpler and converges to a considerably better local optima. The effectiveness of this new variant has been studied extensively in different contexts, such as document clustering, nearest neighbor search and image clustering. Superior performance is observed across different scenarios.



### Content-Based Image Retrieval Using Multiresolution Analysis Of Shape-Based Classified Images
- **Arxiv ID**: http://arxiv.org/abs/1610.02509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.02509v1)
- **Published**: 2016-10-08 10:27:58+00:00
- **Updated**: 2016-10-08 10:27:58+00:00
- **Authors**: I. M. El-Henawy, Kareem Ahmed
- **Comment**: None
- **Journal**: Global Journal of Computers & Technology 1, no. 1 (2014)
- **Summary**: Content-Based Image Retrieval (CBIR) systems have been widely used for a wide range of applications such as Art collections, Crime prevention and Intellectual property. In this paper, a novel CBIR system, which utilizes visual contents (color, texture and shape) of an image to retrieve images, is proposed. The proposed system builds three feature vectors and stores them into MySQL database. The first feature vector uses descriptive statistics to describe the distribution of data in each channel of RGB channels of the image. The second feature vector describes the texture using eigenvalues of the 39 sub-bands that are generated after applying four levels 2D DWT in each channel (red, green and blue channels) of the image. These wavelets sub-bands perfectly describes the horizontal, vertical and diagonal edges that exist in the multi-resolution analysis of the image. The third feature vector describes the basic shapes that exist in the skeletonization version of the black and white representation of the image. Experimental results on a private MYSQL database that consists of 10000 images, using color, texture, shape and stored relevance feedbacks, showed 96.4% average correct retrieval rate in an efficient recovery time.



### Crafting GBD-Net for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1610.02579v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.02579v1)
- **Published**: 2016-10-08 20:36:20+00:00
- **Updated**: 2016-10-08 20:36:20+00:00
- **Authors**: Xingyu Zeng, Wanli Ouyang, Junjie Yan, Hongsheng Li, Tong Xiao, Kun Wang, Yu Liu, Yucong Zhou, Bin Yang, Zhe Wang, Hui Zhou, Xiaogang Wang
- **Comment**: This paper shows the details of our approach in wining the ImageNet
  object detection challenge of 2016, with source code provided on
  \url{https://github.com/craftGBD/craftGBD}. The preliminary version of this
  paper is presented at the ECCV. Xingyu Zeng and Wanli Ouyang contributed
  equally
- **Journal**: None
- **Summary**: The visual cues from multiple support regions of different sizes and resolutions are complementary in classifying a candidate box in object detection. Effective integration of local and contextual visual cues from these regions has become a fundamental problem in object detection.   In this paper, we propose a gated bi-directional CNN (GBD-Net) to pass messages among features from different support regions during both feature learning and feature extraction. Such message passing can be implemented through convolution between neighboring support regions in two directions and can be conducted in various layers. Therefore, local and contextual visual patterns can validate the existence of each other by learning their nonlinear relationships and their close interactions are modeled in a more complex way. It is also shown that message passing is not always helpful but dependent on individual samples. Gated functions are therefore needed to control message transmission, whose on-or-offs are controlled by extra visual evidence from the input sample. The effectiveness of GBD-Net is shown through experiments on three object detection datasets, ImageNet, Pascal VOC2007 and Microsoft COCO. This paper also shows the details of our approach in wining the ImageNet object detection challenge of 2016, with source code provided on \url{https://github.com/craftGBD/craftGBD}.



