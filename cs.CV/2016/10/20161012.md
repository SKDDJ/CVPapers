# Arxiv Papers in cs.CV on 2016-10-12
### Subspace clustering based on low rank representation and weighted nuclear norm minimization
- **Arxiv ID**: http://arxiv.org/abs/1610.03604v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.03604v3)
- **Published**: 2016-10-12 06:11:34+00:00
- **Updated**: 2016-10-14 03:08:52+00:00
- **Authors**: Yu Song, Yiquan Wu
- **Comment**: 17 pages, 3 figures, 5 tables This paper is also submitted to the
  journal 'pattern recognition'. arXiv admin note: text overlap with
  arXiv:1203.1005 by other authors
- **Journal**: None
- **Summary**: Subspace clustering refers to the problem of segmenting a set of data points approximately drawn from a union of multiple linear subspaces. Aiming at the subspace clustering problem, various subspace clustering algorithms have been proposed and low rank representation based subspace clustering is a very promising and efficient subspace clustering algorithm. Low rank representation method seeks the lowest rank representation among all the candidates that can represent the data points as linear combinations of the bases in a given dictionary. Nuclear norm minimization is adopted to minimize the rank of the representation matrix. However, nuclear norm is not a very good approximation of the rank of a matrix and the representation matrix thus obtained can be of high rank which will affect the final clustering accuracy. Weighted nuclear norm (WNN) is a better approximation of the rank of a matrix and WNN is adopted in this paper to describe the rank of the representation matrix. The convex program is solved via conventional alternation direction method of multipliers (ADMM) and linearized alternating direction method of multipliers (LADMM) and they are respectively refer to as WNNM-LRR and WNNM-LRR(L). Experimental results show that, compared with low rank representation method and several other state-of-the-art subspace clustering methods, WNNM-LRR and WNNM-LRR(L) can get higher clustering accuracy.



### The Analysis of Local Motion and Deformation in Image Sequences Inspired by Physical Electromagnetic Interaction
- **Arxiv ID**: http://arxiv.org/abs/1610.03612v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.03612v1)
- **Published**: 2016-10-12 06:39:15+00:00
- **Updated**: 2016-10-12 06:39:15+00:00
- **Authors**: Xiaodong Zhuang, N. E. Mastorakis
- **Comment**: 15 pages, 23 figures. arXiv admin note: substantial text overlap with
  arXiv:1610.03615, arXiv:1610.02762
- **Journal**: WSEAS TRANSACTIONS on COMPUTERS, pp. 231-245, Volume 14, 2015
- **Summary**: In order to analyze the moving and deforming of the objects in image sequence, a novel way is presented to analyze the local changes of object edges between two related images (such as two adjacent frames in a video sequence), which is inspired by the physical electromagnetic interaction. The changes of edge between adjacent frames in sequences are analyzed by simulation of virtual current interaction, which can reflect the change of the object's position or shape. The virtual current along the main edge line is proposed based on the significant edge extraction. Then the virtual interaction between the current elements in the two related images is studied by imitating the interaction between physical current-carrying wires. The experimental results prove that the distribution of magnetic forces on the current elements in one image applied by the other can reflect the local change of edge lines from one image to the other, which is important in further analysis.



### A Model of Virtual Carrier Immigration in Digital Images for Region Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1610.03614v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.03614v1)
- **Published**: 2016-10-12 06:43:34+00:00
- **Updated**: 2016-10-12 06:43:34+00:00
- **Authors**: Xiaodong Zhuang, N. E. Mastorakis
- **Comment**: 11 pages, 17 figures. arXiv admin note: text overlap with
  arXiv:1610.02760
- **Journal**: WSEAS TRANSACTIONS on COMPUTERS, pp. 708-718, Volume 14, 2015
- **Summary**: A novel model for image segmentation is proposed, which is inspired by the carrier immigration mechanism in physical P-N junction. The carrier diffusing and drifting are simulated in the proposed model, which imitates the physical self-balancing mechanism in P-N junction. The effect of virtual carrier immigration in digital images is analyzed and studied by experiments on test images and real world images. The sign distribution of net carrier at the model's balance state is exploited for region segmentation. The experimental results for both test images and real-world images demonstrate self-adaptive and meaningful gathering of pixels to suitable regions, which prove the effectiveness of the proposed method for image region segmentation.



### The Virtual Electromagnetic Interaction between Digital Images for Image Matching with Shifting Transformation
- **Arxiv ID**: http://arxiv.org/abs/1610.03615v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.03615v1)
- **Published**: 2016-10-12 06:48:04+00:00
- **Updated**: 2016-10-12 06:48:04+00:00
- **Authors**: Xiaodong Zhuang, N. E. Mastorakis
- **Comment**: 17 pages, 39 figures. arXiv admin note: substantial text overlap with
  arXiv:1610.03612, arXiv:1610.02762
- **Journal**: WSEAS Transactions on Computers, pp. 107-123, Volume 14, 2015
- **Summary**: A novel way of matching two images with shifting transformation is studied. The approach is based on the presentation of the virtual edge current in images, and also the study of virtual electromagnetic interaction between two related images inspired by electromagnetism. The edge current in images is proposed as a discrete simulation of the physical current, which is based on the significant edge line extracted by Canny-like edge detection. Then the virtual interaction of the edge currents between related images is studied by imitating the electro-magnetic interaction between current-carrying wires. Based on the virtual interaction force between two related images, a novel method is presented and applied in image matching for shifting transformation. The preliminary experimental results indicate the effectiveness of the proposed method.



### Fast Training of Convolutional Neural Networks via Kernel Rescaling
- **Arxiv ID**: http://arxiv.org/abs/1610.03623v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.03623v1)
- **Published**: 2016-10-12 07:25:34+00:00
- **Updated**: 2016-10-12 07:25:34+00:00
- **Authors**: Pedro Porto Buarque de Gusmão, Gianluca Francini, Skjalg Lepsøy, Enrico Magli
- **Comment**: None
- **Journal**: None
- **Summary**: Training deep Convolutional Neural Networks (CNN) is a time consuming task that may take weeks to complete. In this article we propose a novel, theoretically founded method for reducing CNN training time without incurring any loss in accuracy. The basic idea is to begin training with a pre-train network using lower-resolution kernels and input images, and then refine the results at the full resolution by exploiting the spatial scaling property of convolutions. We apply our method to the ImageNet winner OverFeat and to the more recent ResNet architecture and show a reduction in training time of nearly 20% while test set accuracy is preserved in both cases.



### RetiNet: Automatic AMD identification in OCT volumetric data
- **Arxiv ID**: http://arxiv.org/abs/1610.03628v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1610.03628v1)
- **Published**: 2016-10-12 07:56:24+00:00
- **Updated**: 2016-10-12 07:56:24+00:00
- **Authors**: Stefanos Apostolopoulos, Carlos Ciller, Sandro I. De Zanet, Sebastian Wolf, Raphael Sznitman
- **Comment**: 14 pages, 10 figures, Code available
- **Journal**: None
- **Summary**: Optical Coherence Tomography (OCT) provides a unique ability to image the eye retina in 3D at micrometer resolution and gives ophthalmologist the ability to visualize retinal diseases such as Age-Related Macular Degeneration (AMD). While visual inspection of OCT volumes remains the main method for AMD identification, doing so is time consuming as each cross-section within the volume must be inspected individually by the clinician. In much the same way, acquiring ground truth information for each cross-section is expensive and time consuming. This fact heavily limits the ability to acquire large amounts of ground truth, which subsequently impacts the performance of learning-based methods geared at automatic pathology identification. To avoid this burden, we propose a novel strategy for automatic analysis of OCT volumes where only volume labels are needed. That is, we train a classifier in a semi-supervised manner to conduct this task. Our approach uses a novel Convolutional Neural Network (CNN) architecture, that only needs volume-level labels to be trained to automatically asses whether an OCT volume is healthy or contains AMD. Our architecture involves first learning a cross-section pathology classifier using pseudo-labels that could be corrupted and then leverage these towards a more accurate volume-level classification. We then show that our approach provides excellent performances on a publicly available dataset and outperforms a number of existing automatic techniques.



### Theory and computer simulation of the moiré patterns in single-layer cylindrical particles
- **Arxiv ID**: http://arxiv.org/abs/1610.04156v1
- **DOI**: None
- **Categories**: **cond-mat.mes-hall**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1610.04156v1)
- **Published**: 2016-10-12 08:09:34+00:00
- **Updated**: 2016-10-12 08:09:34+00:00
- **Authors**: Vladimir Saveljev, Irina Palchikova
- **Comment**: 8 pages, 14 figures, 45 equations; first written on July 3, 2016,
  last modified on October 12, 2016
- **Journal**: None
- **Summary**: Basing on the theory for arbitrary oriented surfaces, we developed the theory of the moir\'e effect for cylindrical single-layer objects in the paraxial approximation. With using the dual grids, the moir\'e effect in the plane gratings is simulated, as well as the near-axis moir\'e effect in cylinders including the chiral layouts. The results can be applied to the graphene layers, to single-walled nanotubes, and to cylinders in general.



### Analyzing the Affect of a Group of People Using Multi-modal Framework
- **Arxiv ID**: http://arxiv.org/abs/1610.03640v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.03640v2)
- **Published**: 2016-10-12 08:45:52+00:00
- **Updated**: 2016-10-13 21:43:39+00:00
- **Authors**: Xiaohua Huang, Abhinav Dhall, Xin Liu, Guoying Zhao, Jingang Shi, Roland Goecke, Matti Pietikainen
- **Comment**: 11 pages. Submitted to the IEEE Transactions on Cybernetics
- **Journal**: None
- **Summary**: Millions of images on the web enable us to explore images from social events such as a family party, thus it is of interest to understand and model the affect exhibited by a group of people in images. But analysis of the affect expressed by multiple people is challenging due to varied indoor and outdoor settings, and interactions taking place between various numbers of people. A few existing works on Group-level Emotion Recognition (GER) have investigated on face-level information. Due to the challenging environments, face may not provide enough information to GER. Relatively few studies have investigated multi-modal GER. Therefore, we propose a novel multi-modal approach based on a new feature description for understanding emotional state of a group of people in an image. In this paper, we firstly exploit three kinds of rich information containing face, upperbody and scene in a group-level image. Furthermore, in order to integrate multiple person's information in a group-level image, we propose an information aggregation method to generate three features for face, upperbody and scene, respectively. We fuse face, upperbody and scene information for robustness of GER against the challenging environments. Intensive experiments are performed on two challenging group-level emotion databases to investigate the role of face, upperbody and scene as well as multi-modal framework. Experimental results demonstrate that our framework achieves very promising performance for GER.



### Image Based Camera Localization: an Overview
- **Arxiv ID**: http://arxiv.org/abs/1610.03660v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.03660v4)
- **Published**: 2016-10-12 10:19:36+00:00
- **Updated**: 2018-05-03 11:09:03+00:00
- **Authors**: Yihong Wu, Fulin Tang, Heping Li
- **Comment**: None
- **Journal**: Invited Paper by Visual Computing for Industry, Biomedicine and
  Art, 2018
- **Summary**: Recently, virtual reality, augmented reality, robotics, autonomous driving et al attract much attention of both academic and industrial community, in which image based camera localization is a key task. However, there has not been a complete review on image-based camera localization. It is urgent to map this topic to help people enter the field quickly. In this paper, an overview of image based camera localization is presented. A new and complete kind of classifications for image based camera localization is provided and the related techniques are introduced. Trends for the future development are also discussed. It will be useful to not only researchers but also engineers and other people interested.



### Multi-Task Curriculum Transfer Deep Learning of Clothing Attributes
- **Arxiv ID**: http://arxiv.org/abs/1610.03670v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.03670v4)
- **Published**: 2016-10-12 11:17:16+00:00
- **Updated**: 2016-12-25 23:43:22+00:00
- **Authors**: Qi Dong, Shaogang Gong, Xiatian Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Recognising detailed clothing characteristics (fine-grained attributes) in unconstrained images of people in-the-wild is a challenging task for computer vision, especially when there is only limited training data from the wild whilst most data available for model learning are captured in well-controlled environments using fashion models (well lit, no background clutter, frontal view, high-resolution). In this work, we develop a deep learning framework capable of model transfer learning from well-controlled shop clothing images collected from web retailers to in-the-wild images from the street. Specifically, we formulate a novel Multi-Task Curriculum Transfer (MTCT) deep learning method to explore multiple sources of different types of web annotations with multi-labelled fine-grained attributes. Our multi-task loss function is designed to extract more discriminative representations in training by jointly learning all attributes, and our curriculum strategy exploits the staged easy-to-complex transfer learning motivated by cognitive studies. We demonstrate the advantages of the MTCT model over the state-of-the-art methods on the X-Domain benchmark, a large scale clothing attribute dataset. Moreover, we show that the MTCT model has a notable advantage over contemporary models when the training data size is small.



### Deep Fruit Detection in Orchards
- **Arxiv ID**: http://arxiv.org/abs/1610.03677v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1610.03677v2)
- **Published**: 2016-10-12 11:40:24+00:00
- **Updated**: 2017-09-18 01:03:55+00:00
- **Authors**: Suchet Bargoti, James Underwood
- **Comment**: Submitted to the IEEE International Conference on Robotics and
  Automation 2017
- **Journal**: None
- **Summary**: An accurate and reliable image based fruit detection system is critical for supporting higher level agriculture tasks such as yield mapping and robotic harvesting. This paper presents the use of a state-of-the-art object detection framework, Faster R-CNN, in the context of fruit detection in orchards, including mangoes, almonds and apples. Ablation studies are presented to better understand the practical deployment of the detection network, including how much training data is required to capture variability in the dataset. Data augmentation techniques are shown to yield significant performance gains, resulting in a greater than two-fold reduction in the number of training images required. In contrast, transferring knowledge between orchards contributed to negligible performance gain over initialising the Deep Convolutional Neural Network directly from ImageNet features. Finally, to operate over orchard data containing between 100-1000 fruit per image, a tiling approach is introduced for the Faster R-CNN framework. The study has resulted in the best yet detection performance for these orchards relative to previous works, with an F1-score of >0.9 achieved for apples and mangoes.



### Light Field Compression with Disparity Guided Sparse Coding based on Structural Key Views
- **Arxiv ID**: http://arxiv.org/abs/1610.03684v2
- **DOI**: 10.1109/TIP.2017.2750413
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.03684v2)
- **Published**: 2016-10-12 12:23:49+00:00
- **Updated**: 2017-05-10 07:30:34+00:00
- **Authors**: Jie Chen, Junhui Hou, Lap-Pui Chau
- **Comment**: None
- **Journal**: None
- **Summary**: Recent imaging technologies are rapidly evolving for sampling richer and more immersive representations of the 3D world. And one of the emerging technologies are light field (LF) cameras based on micro-lens arrays. To record the directional information of the light rays, a much larger storage space and transmission bandwidth are required by a LF image as compared with a conventional 2D image of similar spatial dimension, and the compression of LF data becomes a vital part of its application.   In this paper, we propose a LF codec that fully exploits the intrinsic geometry between the LF sub-views by first approximating the LF with disparity guided sparse coding over a perspective shifted light field dictionary. The sparse coding is only based on several optimized Structural Key Views (SKV); however the entire LF can be recovered from the coding coefficients. By keeping the approximation identical between encoder and decoder, only the residuals of the non-key views, disparity map and the SKVs need to be compressed into the bit stream. An optimized SKV selection method is proposed such that most LF spatial information could be preserved. And to achieve optimum dictionary efficiency, the LF is divided into several Coding Regions (CR), over which the reconstruction works individually. Experiments and comparisons have been carried out over benchmark LF dataset, which show that the proposed SC-SKV codec produces convincing compression results in terms of both rate-distortion performance and visual quality compared with High Efficiency Video Coding (HEVC): with 47.87% BD-rate reduction and 1.59 dB BD-PSNR improvement achieved on average, especially with up to 4 dB improvement for low bit rate scenarios.



### Generating captions without looking beyond objects
- **Arxiv ID**: http://arxiv.org/abs/1610.03708v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1610.03708v2)
- **Published**: 2016-10-12 13:42:03+00:00
- **Updated**: 2016-10-18 09:35:03+00:00
- **Authors**: Hendrik Heuer, Christof Monz, Arnold W. M. Smeulders
- **Comment**: This paper was presented at the ECCV2016 2nd Workshop on Storytelling
  with Images and Videos (VisStory)
- **Journal**: None
- **Summary**: This paper explores new evaluation perspectives for image captioning and introduces a noun translation task that achieves comparative image caption generation performance by translating from a set of nouns to captions. This implies that in image captioning, all word categories other than nouns can be evoked by a powerful language model without sacrificing performance on n-gram precision. The paper also investigates lower and upper bounds of how much individual word categories in the captions contribute to the final BLEU score. A large possible improvement exists for nouns, verbs, and prepositions.



### Detecting Unseen Falls from Wearable Devices using Channel-wise Ensemble of Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1610.03761v3
- **DOI**: 10.1016/j.eswa.2017.06.011
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1610.03761v3)
- **Published**: 2016-10-12 15:55:06+00:00
- **Updated**: 2017-03-22 20:51:59+00:00
- **Authors**: Shehroz S. Khan, Babak Taati
- **Comment**: 25 pages, 6 figures, 4 Tables
- **Journal**: Expert Systems with Applications, Volume 87, 30 November 2017,
  Pages 280-290
- **Summary**: A fall is an abnormal activity that occurs rarely, so it is hard to collect real data for falls. It is, therefore, difficult to use supervised learning methods to automatically detect falls. Another challenge in using machine learning methods to automatically detect falls is the choice of engineered features. In this paper, we propose to use an ensemble of autoencoders to extract features from different channels of wearable sensor data trained only on normal activities. We show that the traditional approach of choosing a threshold as the maximum of the reconstruction error on the training normal data is not the right way to identify unseen falls. We propose two methods for automatic tightening of reconstruction error from only the normal activities for better identification of unseen falls. We present our results on two activity recognition datasets and show the efficacy of our proposed method against traditional autoencoder models and two standard one-class classification methods.



### Deep disentangled representations for volumetric reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1610.03777v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.03777v1)
- **Published**: 2016-10-12 16:36:37+00:00
- **Updated**: 2016-10-12 16:36:37+00:00
- **Authors**: Edward Grant, Pushmeet Kohli, Marcel van Gerven
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a convolutional neural network for inferring a compact disentangled graphical description of objects from 2D images that can be used for volumetric reconstruction. The network comprises an encoder and a twin-tailed decoder. The encoder generates a disentangled graphics code. The first decoder generates a volume, and the second decoder reconstructs the input image using a novel training regime that allows the graphics code to learn a separate representation of the 3D object and a description of its lighting and pose conditions. We demonstrate this method by generating volumes and disentangled graphical descriptions from images and videos of faces and chairs.



### Video Depth-From-Defocus
- **Arxiv ID**: http://arxiv.org/abs/1610.03782v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.03782v1)
- **Published**: 2016-10-12 16:43:10+00:00
- **Updated**: 2016-10-12 16:43:10+00:00
- **Authors**: Hyeongwoo Kim, Christian Richardt, Christian Theobalt
- **Comment**: 13 pages, supplemental document included as appendix, 3DV 2016
- **Journal**: None
- **Summary**: Many compelling video post-processing effects, in particular aesthetic focus editing and refocusing effects, are feasible if per-frame depth information is available. Existing computational methods to capture RGB and depth either purposefully modify the optics (coded aperture, light-field imaging), or employ active RGB-D cameras. Since these methods are less practical for users with normal cameras, we present an algorithm to capture all-in-focus RGB-D video of dynamic scenes with an unmodified commodity video camera. Our algorithm turns the often unwanted defocus blur into a valuable signal. The input to our method is a video in which the focus plane is continuously moving back and forth during capture, and thus defocus blur is provoked and strongly visible. This can be achieved by manually turning the focus ring of the lens during recording. The core algorithmic ingredient is a new video-based depth-from-defocus algorithm that computes space-time-coherent depth maps, deblurred all-in-focus video, and the focus distance for each frame. We extensively evaluate our approach, and show that it enables compelling video post-processing effects, such as different types of refocusing.



### Recursive Diffeomorphism-Based Regression for Shape Functions
- **Arxiv ID**: http://arxiv.org/abs/1610.03819v2
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, math.ST, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/1610.03819v2)
- **Published**: 2016-10-12 18:43:51+00:00
- **Updated**: 2017-07-29 04:44:15+00:00
- **Authors**: Jieren Xu, Haizhao Yang, Ingrid Daubechies
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a recursive diffeomorphism based regression method for one-dimensional generalized mode decomposition problem that aims at extracting generalized modes $\alpha_k(t)s_k(2\pi N_k\phi_k(t))$ from their superposition $\sum_{k=1}^K \alpha_k(t)s_k(2\pi N_k\phi_k(t))$. First, a one-dimensional synchrosqueezed transform is applied to estimate instantaneous information, e.g., $\alpha_k(t)$ and $N_k\phi_k(t)$. Second, a novel approach based on diffeomorphisms and nonparametric regression is proposed to estimate wave shape functions $s_k(t)$. These two methods lead to a framework for the generalized mode decomposition problem under a weak well-separation condition. Numerical examples of synthetic and real data are provided to demonstrate the fruitful applications of these methods.



### Semi-Coupled Two-Stream Fusion ConvNets for Action Recognition at Extremely Low Resolutions
- **Arxiv ID**: http://arxiv.org/abs/1610.03898v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.03898v2)
- **Published**: 2016-10-12 23:19:57+00:00
- **Updated**: 2018-10-05 19:00:02+00:00
- **Authors**: Jiawei Chen, Jonathan Wu, Janusz Konrad, Prakash Ishwar
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (ConvNets) have been recently shown to attain state-of-the-art performance for action recognition on standard-resolution videos. However, less attention has been paid to recognition performance at extremely low resolutions (eLR) (e.g., 16 x 12 pixels). Reliable action recognition using eLR cameras would address privacy concerns in various application environments such as private homes, hospitals, nursing/rehabilitation facilities, etc. In this paper, we propose a semi-coupled filter-sharing network that leverages high resolution (HR) videos during training in order to assist an eLR ConvNet. We also study methods for fusing spatial and temporal ConvNets customized for eLR videos in order to take advantage of appearance and motion information. Our method outperforms state-of-the-art methods at extremely low resolutions on IXMAS (93.7%) and HMDB (29.2%) datasets.



