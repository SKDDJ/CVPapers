# Arxiv Papers in cs.CV on 2016-10-10
### Image Segmentation Based on the Self-Balancing Mechanism in Virtual 3D Elastic Mesh
- **Arxiv ID**: http://arxiv.org/abs/1610.02760v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.02760v1)
- **Published**: 2016-10-10 03:13:29+00:00
- **Updated**: 2016-10-10 03:13:29+00:00
- **Authors**: Xiaodong Zhuang, N. E. Mastorakis, Jieru Chi, Hanping Wang
- **Comment**: 14 pages, 21 figures
- **Journal**: WSEAS Transactions on Computers, pp. 805-818, Volume 14, 2015
- **Summary**: In this paper, a novel model of 3D elastic mesh is presented for image segmentation. The model is inspired by stress and strain in physical elastic objects, while the repulsive force and elastic force in the model are defined slightly different from the physical force to suit the segmentation problem well. The self-balancing mechanism in the model guarantees the stability of the method in segmentation. The shape of the elastic mesh at balance state is used for region segmentation, in which the sign distribution of the points'z coordinate values is taken as the basis for segmentation. The effectiveness of the proposed method is proved by analysis and experimental results for both test images and real world images.



### Matching of Images with Rotation Transformation Based on the Virtual Electromagnetic Interaction
- **Arxiv ID**: http://arxiv.org/abs/1610.02762v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.02762v1)
- **Published**: 2016-10-10 03:21:35+00:00
- **Updated**: 2016-10-10 03:21:35+00:00
- **Authors**: Xiaodong Zhuang, N. E. Mastorakis
- **Comment**: 19 pages, 26 figures
- **Journal**: WSEAS Transactions On Computers, pp. 679-697, Volume 14, 2015
- **Summary**: A novel approach of image matching for rotating transformation is presented and studied. The approach is inspired by electromagnetic interaction force between physical currents. The virtual current in images is proposed based on the significant edge lines extracted as the fundamental structural feature of images. The virtual electromagnetic force and the corresponding moment is studied between two images after the extraction of the virtual currents in the images. Then image matching for rotating transformation is implemented by exploiting the interaction between the virtual currents in the two images to be matched. The experimental results prove the effectiveness of the novel idea, which indicates the promising application of the proposed method in image registration.



### Impatient DNNs - Deep Neural Networks with Dynamic Time Budgets
- **Arxiv ID**: http://arxiv.org/abs/1610.02850v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.02850v1)
- **Published**: 2016-10-10 11:11:06+00:00
- **Updated**: 2016-10-10 11:11:06+00:00
- **Authors**: Manuel Amthor, Erik Rodner, Joachim Denzler
- **Comment**: British Machine Vision Conference (BMVC) 2016
- **Journal**: None
- **Summary**: We propose Impatient Deep Neural Networks (DNNs) which deal with dynamic time budgets during application. They allow for individual budgets given a priori for each test example and for anytime prediction, i.e., a possible interruption at multiple stages during inference while still providing output estimates. Our approach can therefore tackle the computational costs and energy demands of DNNs in an adaptive manner, a property essential for real-time applications. Our Impatient DNNs are based on a new general framework of learning dynamic budget predictors using risk minimization, which can be applied to current DNN architectures by adding early prediction and additional loss layers. A key aspect of our method is that all of the intermediate predictors are learned jointly. In experiments, we evaluate our approach for different budget distributions, architectures, and datasets. Our results show a significant gain in expected accuracy compared to common baselines.



### Content Based Image Retrieval (CBIR) in Remote Clinical Diagnosis and Healthcare
- **Arxiv ID**: http://arxiv.org/abs/1610.02902v1
- **DOI**: 10.4018/978-1-4666-9978-6.ch039
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.02902v1)
- **Published**: 2016-10-10 13:22:28+00:00
- **Updated**: 2016-10-10 13:22:28+00:00
- **Authors**: Albany E. Herrmann, Vania Vieira Estrela
- **Comment**: 28 pages, 6 figures, Book Chapter from "Encyclopedia of E-Health and
  Telemedicine"
- **Journal**: Encyclopedia of E-Health and Telemedicine. IGI Global, 2016.
  495-520. Web. 10 Oct. 2016
- **Summary**: Content-Based Image Retrieval (CBIR) locates, retrieves and displays images alike to one given as a query, using a set of features. It demands accessible data in medical archives and from medical equipment, to infer meaning after some processing. A problem similar in some sense to the target image can aid clinicians. CBIR complements text-based retrieval and improves evidence-based diagnosis, administration, teaching, and research in healthcare. It facilitates visual/automatic diagnosis and decision-making in real-time remote consultation/screening, store-and-forward tests, home care assistance and overall patient surveillance. Metrics help comparing visual data and improve diagnostic. Specially designed architectures can benefit from the application scenario. CBIR use calls for file storage standardization, querying procedures, efficient image transmission, realistic databases, global availability, access simplicity, and Internet-based structures. This chapter recommends important and complex aspects required to handle visual content in healthcare.



### Deep Pyramidal Residual Networks
- **Arxiv ID**: http://arxiv.org/abs/1610.02915v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.02915v4)
- **Published**: 2016-10-10 13:47:13+00:00
- **Updated**: 2017-09-06 10:18:53+00:00
- **Authors**: Dongyoon Han, Jiwhan Kim, Junmo Kim
- **Comment**: Accepted to CVPR 2017
- **Journal**: None
- **Summary**: Deep convolutional neural networks (DCNNs) have shown remarkable performance in image classification tasks in recent years. Generally, deep neural network architectures are stacks consisting of a large number of convolutional layers, and they perform downsampling along the spatial dimension via pooling to reduce memory usage. Concurrently, the feature map dimension (i.e., the number of channels) is sharply increased at downsampling locations, which is essential to ensure effective performance because it increases the diversity of high-level attributes. This also applies to residual networks and is very closely related to their performance. In this research, instead of sharply increasing the feature map dimension at units that perform downsampling, we gradually increase the feature map dimension at all units to involve as many locations as possible. This design, which is discussed in depth together with our new insights, has proven to be an effective means of improving generalization ability. Furthermore, we propose a novel residual unit capable of further improving the classification accuracy with our new network architecture. Experiments on benchmark CIFAR-10, CIFAR-100, and ImageNet datasets have shown that our network architecture has superior generalization ability compared to the original residual networks. Code is available at https://github.com/jhkim89/PyramidNet}



### EM-Based Mixture Models Applied to Video Event Detection
- **Arxiv ID**: http://arxiv.org/abs/1610.02923v1
- **DOI**: 10.5772/38129
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.02923v1)
- **Published**: 2016-10-10 14:07:49+00:00
- **Updated**: 2016-10-10 14:07:49+00:00
- **Authors**: Alessandra Martins Coelho, Vania V. Estrela
- **Comment**: 25 pages, 8 figures, Available from:
  http://www.intechopen.com/books/principal-component-analysis-engineering-applications/em-based-mixture-models-applied-to-video-event-detection,
  Chapter from book "Principal Component Analysis - Engineering Applications",
  Dr. Parinya Sanguansat (Ed.), InTech, 2012. arXiv admin note: text overlap
  with arXiv:1404.1100 by other authors
- **Journal**: None
- **Summary**: Surveillance system (SS) development requires hi-tech support to prevail over the shortcomings related to the massive quantity of visual information from SSs. Anything but reduced human monitoring became impossible by means of its physical and economic implications, and an advance towards an automated surveillance becomes the only way out. When it comes to a computer vision system, automatic video event comprehension is a challenging task due to motion clutter, event understanding under complex scenes, multilevel semantic event inference, contextualization of events and views obtained from multiple cameras, unevenness of motion scales, shape changes, occlusions and object interactions among lots of other impairments. In recent years, state-of-the-art models for video event classification and recognition include modeling events to discern context, detecting incidents with only one camera, low-level feature extraction and description, high-level semantic event classification, and recognition. Even so, it is still very burdensome to recuperate or label a specific video part relying solely on its content. Principal component analysis (PCA) has been widely known and used, but when combined with other techniques such as the expectation-maximization (EM) algorithm its computation becomes more efficient. This chapter introduces advances associated with the concept of Probabilistic PCA (PPCA) analysis of video event and it also aims at looking closely to ways and metrics to evaluate these less intensive EM implementations of PCA and KPCA.



### End-to-end Concept Word Detection for Video Captioning, Retrieval, and Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1610.02947v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.02947v3)
- **Published**: 2016-10-10 15:03:15+00:00
- **Updated**: 2017-07-25 10:12:31+00:00
- **Authors**: Youngjae Yu, Hyungjin Ko, Jongwook Choi, Gunhee Kim
- **Comment**: In CVPR 2017. Winner of three (fill-in-the-blank, multiple-choice
  test, and movie retrieval) out of four tasks of the LSMDC 2016 Challenge. 22
  pages
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2017, pp. 3165-3173
- **Summary**: We propose a high-level concept word detector that can be integrated with any video-to-language models. It takes a video as input and generates a list of concept words as useful semantic priors for language generation models. The proposed word detector has two important properties. First, it does not require any external knowledge sources for training. Second, the proposed word detector is trainable in an end-to-end manner jointly with any video-to-language models. To maximize the values of detected words, we also develop a semantic attention mechanism that selectively focuses on the detected concept words and fuse them with the word encoding and decoding in the language model. In order to demonstrate that the proposed approach indeed improves the performance of multiple video-to-language tasks, we participate in four tasks of LSMDC 2016. Our approach achieves the best accuracies in three of them, including fill-in-the-blank, multiple-choice test, and movie retrieval. We also attain comparable performance for the other task, movie description.



### Person Re-identification: Past, Present and Future
- **Arxiv ID**: http://arxiv.org/abs/1610.02984v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.02984v1)
- **Published**: 2016-10-10 16:19:21+00:00
- **Updated**: 2016-10-10 16:19:21+00:00
- **Authors**: Liang Zheng, Yi Yang, Alexander G. Hauptmann
- **Comment**: 20 pages, 5 tables, 10 images
- **Journal**: None
- **Summary**: Person re-identification (re-ID) has become increasingly popular in the community due to its application and research significance. It aims at spotting a person of interest in other cameras. In the early days, hand-crafted algorithms and small-scale evaluation were predominantly reported. Recent years have witnessed the emergence of large-scale datasets and deep learning systems which make use of large data volumes. Considering different tasks, we classify most current re-ID methods into two classes, i.e., image-based and video-based; in both tasks, hand-crafted and deep learning systems will be reviewed. Moreover, two new re-ID tasks which are much closer to real-world applications are described and discussed, i.e., end-to-end re-ID and fast re-ID in very large galleries. This paper: 1) introduces the history of person re-ID and its relationship with image classification and instance retrieval; 2) surveys a broad selection of the hand-crafted systems and the large-scale methods in both image- and video-based re-ID; 3) describes critical future directions in end-to-end re-ID and fast retrieval in large galleries; and 4) finally briefs some important yet under-developed issues.



### Learning Low Dimensional Convolutional Neural Networks for High-Resolution Remote Sensing Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1610.03023v2
- **DOI**: 10.3390/rs9050489
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.03023v2)
- **Published**: 2016-10-10 18:45:30+00:00
- **Updated**: 2016-12-30 19:04:58+00:00
- **Authors**: Weixun Zhou, Shawn Newsam, Congmin Li, Zhenfeng Shao
- **Comment**: None
- **Journal**: Remote Sens., 9(5), 489 (2017)
- **Summary**: Learning powerful feature representations for image retrieval has always been a challenging task in the field of remote sensing. Traditional methods focus on extracting low-level hand-crafted features which are not only time-consuming but also tend to achieve unsatisfactory performance due to the content complexity of remote sensing images. In this paper, we investigate how to extract deep feature representations based on convolutional neural networks (CNN) for high-resolution remote sensing image retrieval (HRRSIR). To this end, two effective schemes are proposed to generate powerful feature representations for HRRSIR. In the first scheme, the deep features are extracted from the fully-connected and convolutional layers of the pre-trained CNN models, respectively; in the second scheme, we propose a novel CNN architecture based on conventional convolution layers and a three-layer perceptron. The novel CNN model is then trained on a large remote sensing dataset to learn low dimensional features. The two schemes are evaluated on several public and challenging datasets, and the results indicate that the proposed schemes and in particular the novel CNN are able to achieve state-of-the-art performance.



### Tangled Splines
- **Arxiv ID**: http://arxiv.org/abs/1610.03129v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/1610.03129v1)
- **Published**: 2016-10-10 23:31:18+00:00
- **Updated**: 2016-10-10 23:31:18+00:00
- **Authors**: Aditya Tatu
- **Comment**: 12 pages, To be sent to a Journal/Conference
- **Journal**: None
- **Summary**: Extracting shape information from object bound- aries is a well studied problem in vision, and has found tremen- dous use in applications like object recognition. Conversely, studying the space of shapes represented by curves satisfying certain constraints is also intriguing. In this paper, we model and analyze the space of shapes represented by a 3D curve (space curve) formed by connecting n pieces of quarter of a unit circle. Such a space curve is what we call a Tangle, the name coming from a toy built on the same principle. We provide two models for the shape space of n-link open and closed tangles, and we show that tangles are a subset of trigonometric splines of a certain order. We give algorithms for curve approximation using open/closed tangles, computing geodesics on these shape spaces, and to find the deformation that takes one given tangle to another given tangle, i.e., the Log map. The algorithms provided yield tangles upto a small and acceptable tolerance, as shown by the results given in the paper.



