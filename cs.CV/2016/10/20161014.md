# Arxiv Papers in cs.CV on 2016-10-14
### Recurrent 3D Attentional Networks for End-to-End Active Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1610.04308v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.04308v4)
- **Published**: 2016-10-14 01:25:09+00:00
- **Updated**: 2022-01-11 12:37:22+00:00
- **Authors**: Min Liu, Yifei Shi, Lintao Zheng, Kai Xu, Hui Huang, Dinesh Manocha
- **Comment**: None
- **Journal**: None
- **Summary**: Active vision is inherently attention-driven: The agent actively selects views to attend in order to fast achieve the vision task while improving its internal representation of the scene being observed. Inspired by the recent success of attention-based models in 2D vision tasks based on single RGB images, we propose to address the multi-view depth-based active object recognition using attention mechanism, through developing an end-to-end recurrent 3D attentional network. The architecture takes advantage of a recurrent neural network (RNN) to store and update an internal representation. Our model, trained with 3D shape datasets, is able to iteratively attend to the best views targeting an object of interest for recognizing it. To realize 3D view selection, we derive a 3D spatial transformer network which is differentiable for training with backpropagation, achieving much faster convergence than the reinforcement learning employed by most existing attention-based models. Experiments show that our method, with only depth input, achieves state-of-the-art next-best-view performance in time efficiency and recognition accuracy.



### Learning and Fusing Multimodal Features from and for Multi-task Facial Computing
- **Arxiv ID**: http://arxiv.org/abs/1610.04322v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.04322v1)
- **Published**: 2016-10-14 04:17:13+00:00
- **Updated**: 2016-10-14 04:17:13+00:00
- **Authors**: Wei Li, Zhigang Zhu
- **Comment**: An experiment to feature fusion in deep learning
- **Journal**: None
- **Summary**: We propose a deep learning-based feature fusion approach for facial computing including face recognition as well as gender, race and age detection. Instead of training a single classifier on face images to classify them based on the features of the person whose face appears in the image, we first train four different classifiers for classifying face images based on race, age, gender and identification (ID). Multi-task features are then extracted from the trained models and cross-task-feature training is conducted which shows the value of fusing multimodal features extracted from multi-tasks. We have found that features trained for one task can be used for other related tasks. More interestingly, the features trained for a task with more classes (e.g. ID) and then used in another task with fewer classes (e.g. race) outperforms the features trained for the other task itself. The final feature fusion is performed by combining the four types of features extracted from the images by the four classifiers. The feature fusion approach improves the classifications accuracy by a 7.2%, 20.1%, 22.2%, 21.8% margin, respectively, for ID, age, race and gender recognition, over the results of single classifiers trained only on their individual features. The proposed method can be applied to applications in which different types of data or features can be extracted.



### Hadamard Product for Low-rank Bilinear Pooling
- **Arxiv ID**: http://arxiv.org/abs/1610.04325v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1610.04325v4)
- **Published**: 2016-10-14 04:29:52+00:00
- **Updated**: 2017-03-26 16:22:47+00:00
- **Authors**: Jin-Hwa Kim, Kyoung-Woon On, Woosang Lim, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang
- **Comment**: 13 pages, 1 figure, & appendix. ICLR 2017 accepted
- **Journal**: None
- **Summary**: Bilinear models provide rich representations compared with linear models. They have been applied in various visual tasks, such as object recognition, segmentation, and visual question-answering, to get state-of-the-art performances taking advantage of the expanded representations. However, bilinear representations tend to be high-dimensional, limiting the applicability to computationally complex tasks. We propose low-rank bilinear pooling using Hadamard product for an efficient attention mechanism of multimodal learning. We show that our model outperforms compact bilinear pooling in visual question-answering tasks with the state-of-the-art results on the VQA dataset, having a better parsimonious property.



### On the Existence of a Sample Mean in Dynamic Time Warping Spaces
- **Arxiv ID**: http://arxiv.org/abs/1610.04460v3
- **DOI**: None
- **Categories**: **cs.CV**, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1610.04460v3)
- **Published**: 2016-10-14 13:42:47+00:00
- **Updated**: 2018-03-05 08:57:17+00:00
- **Authors**: Brijnesh J. Jain, David Schultz
- **Comment**: None
- **Journal**: None
- **Summary**: The concept of sample mean in dynamic time warping (DTW) spaces has been successfully applied to improve pattern recognition systems and generalize centroid-based clustering algorithms. Its existence has neither been proved nor challenged. This article presents sufficient conditions for existence of a sample mean in DTW spaces. The proposed result justifies prior work on approximate mean algorithms, sets the stage for constructing exact mean algorithms, and is a first step towards a statistical theory of DTW spaces.



### Amortised MAP Inference for Image Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/1610.04490v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1610.04490v3)
- **Published**: 2016-10-14 14:58:44+00:00
- **Updated**: 2017-02-21 13:08:24+00:00
- **Authors**: Casper Kaae Sønderby, Jose Caballero, Lucas Theis, Wenzhe Shi, Ferenc Huszár
- **Comment**: None
- **Journal**: None
- **Summary**: Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high-resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss. However, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Furthermore, MAP inference is often performed via optimisation-based iterative algorithms which don't compare well with the efficiency of neural-network-based alternatives. Here we introduce new methods for amortised MAP inference whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data. Lastly, we establish a connection between GANs and amortised variational inference as in e.g. variational autoencoders.



### Numerical Inversion of SRNF Maps for Elastic Shape Analysis of Genus-Zero Surfaces
- **Arxiv ID**: http://arxiv.org/abs/1610.04531v1
- **DOI**: 10.1109/TPAMI.2016.2647596
- **Categories**: **cs.GR**, cs.CG, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1610.04531v1)
- **Published**: 2016-10-14 16:56:49+00:00
- **Updated**: 2016-10-14 16:56:49+00:00
- **Authors**: Hamid Laga, Qian Xie, Ian H. Jermyn, Anuj Srivastava
- **Comment**: None
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2017
- **Summary**: Recent developments in elastic shape analysis (ESA) are motivated by the fact that it provides comprehensive frameworks for simultaneous registration, deformation, and comparison of shapes. These methods achieve computational efficiency using certain square-root representations that transform invariant elastic metrics into Euclidean metrics, allowing for applications of standard algorithms and statistical tools. For analyzing shapes of embeddings of $\mathbb{S}^2$ in $\mathbb{R}^3$, Jermyn et al. introduced square-root normal fields (SRNFs) that transformed an elastic metric, with desirable invariant properties, into the $\mathbb{L}^2$ metric. These SRNFs are essentially surface normals scaled by square-roots of infinitesimal area elements. A critical need in shape analysis is to invert solutions (deformations, averages, modes of variations, etc) computed in the SRNF space, back to the original surface space for visualizations and inferences. Due to the lack of theory for understanding SRNFs maps and their inverses, we take a numerical approach and derive an efficient multiresolution algorithm, based on solving an optimization problem in the surface space, that estimates surfaces corresponding to given SRNFs. This solution is found effective, even for complex shapes, e.g. human bodies and animals, that undergo significant deformations including bending and stretching. Specifically, we use this inversion for computing elastic shape deformations, transferring deformations, summarizing shapes, and for finding modes of variability in a given collection, while simultaneously registering the surfaces. We demonstrate the proposed algorithms using a statistical analysis of human body shapes, classification of generic surfaces and analysis of brain structures.



### On Duality Of Multiple Target Tracking and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1610.04542v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.04542v1)
- **Published**: 2016-10-14 17:18:46+00:00
- **Updated**: 2016-10-14 17:18:46+00:00
- **Authors**: Yicong Tian, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: Traditionally, object tracking and segmentation are treated as two separate problems and solved independently. However, in this paper, we argue that tracking and segmentation are actually closely related and solving one should help the other. On one hand, the object track, which is a set of bounding boxes with one bounding box in every frame, would provide strong high-level guidance for the target/background segmentation task. On the other hand, the object segmentation would separate object from other objects and background, which will be useful for determining track locations in every frame. We propose a novel framework which combines online multiple target tracking and segmentation in a video. In our approach, the tracking and segmentation problems are coupled by Lagrange dual decomposition, which leads to more accurate segmentation results and also \emph{helps resolve typical difficulties in multiple target tracking, such as occlusion handling, ID-switch and track drifting}. To track targets, an individual appearance model is learned for each target via structured learning and network flow is employed to generate tracks from densely sampled candidates. For segmentation, multi-label Conditional Random Field (CRF) is applied to a superpixel based spatio-temporal graph in a segment of video to assign background or target labels to every superpixel. The experiments on diverse sequences show that our method outperforms state-of-the-art approaches for multiple target tracking as well as segmentation.



### Are Accuracy and Robustness Correlated?
- **Arxiv ID**: http://arxiv.org/abs/1610.04563v2
- **DOI**: 10.1109/ICMLA.2016.0045
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.04563v2)
- **Published**: 2016-10-14 18:10:04+00:00
- **Updated**: 2016-12-01 00:54:14+00:00
- **Authors**: Andras Rozsa, Manuel Günther, Terrance E. Boult
- **Comment**: Accepted for publication at ICMLA 2016
- **Journal**: None
- **Summary**: Machine learning models are vulnerable to adversarial examples formed by applying small carefully chosen perturbations to inputs that cause unexpected classification errors. In this paper, we perform experiments on various adversarial example generation approaches with multiple deep convolutional neural networks including Residual Networks, the best performing models on ImageNet Large-Scale Visual Recognition Challenge 2015. We compare the adversarial example generation techniques with respect to the quality of the produced images, and measure the robustness of the tested machine learning models to adversarial examples. Finally, we conduct large-scale experiments on cross-model adversarial portability. We find that adversarial examples are mostly transferable across similar network topologies, and we demonstrate that better machine learning models are less vulnerable to adversarial examples.



### Generalization Error of Invariant Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1610.04574v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1610.04574v3)
- **Published**: 2016-10-14 18:40:52+00:00
- **Updated**: 2017-07-02 18:58:21+00:00
- **Authors**: Jure Sokolic, Raja Giryes, Guillermo Sapiro, Miguel R. D. Rodrigues
- **Comment**: Accepted to AISTATS. This version has updated references
- **Journal**: Conference on Artificial Intelligence and Statistics (AISTATS),
  2017, pp. 1094-1103
- **Summary**: This paper studies the generalization error of invariant classifiers. In particular, we consider the common scenario where the classification task is invariant to certain transformations of the input, and that the classifier is constructed (or learned) to be invariant to these transformations. Our approach relies on factoring the input space into a product of a base space and a set of transformations. We show that whereas the generalization error of a non-invariant classifier is proportional to the complexity of the input space, the generalization error of an invariant classifier is proportional to the complexity of the base space. We also derive a set of sufficient conditions on the geometry of the base space and the set of transformations that ensure that the complexity of the base space is much smaller than the complexity of the input space. Our analysis applies to general classifiers such as convolutional neural networks. We demonstrate the implications of the developed theory for such classifiers with experiments on the MNIST and CIFAR-10 datasets.



### Automatically tracking neurons in a moving and deforming brain
- **Arxiv ID**: http://arxiv.org/abs/1610.04579v1
- **DOI**: 10.1371/journal.pcbi.1005517
- **Categories**: **q-bio.NC**, cs.CV, physics.bio-ph
- **Links**: [PDF](http://arxiv.org/pdf/1610.04579v1)
- **Published**: 2016-10-14 18:51:30+00:00
- **Updated**: 2016-10-14 18:51:30+00:00
- **Authors**: Jeffrey P. Nguyen, Ashley N. Linder, George S. Plummer, Joshua W. Shaevitz, Andrew M. Leifer
- **Comment**: 33 pages, 7 figures, code available
- **Journal**: PLoS Comput Biol 13(5): e1005517 (2017)
- **Summary**: Advances in optical neuroimaging techniques now allow neural activity to be recorded with cellular resolution in awake and behaving animals. Brain motion in these recordings pose a unique challenge. The location of individual neurons must be tracked in 3D over time to accurately extract single neuron activity traces. Recordings from small invertebrates like C. elegans are especially challenging because they undergo very large brain motion and deformation during animal movement. Here we present an automated computer vision pipeline to reliably track populations of neurons with single neuron resolution in the brain of a freely moving C. elegans undergoing large motion and deformation. 3D volumetric fluorescent images of the animal's brain are straightened, aligned and registered, and the locations of neurons in the images are found via segmentation. Each neuron is then assigned an identity using a new time-independent machine-learning approach we call Neuron Registration Vector Encoding. In this approach, non-rigid point-set registration is used to match each segmented neuron in each volume with a set of reference volumes taken from throughout the recording. The way each neuron matches with the references defines a feature vector which is clustered to assign an identity to each neuron in each volume. Finally, thin-plate spline interpolation is used to correct errors in segmentation and check consistency of assigned identities. The Neuron Registration Vector Encoding approach proposed here is uniquely well suited for tracking neurons in brains undergoing large deformations. When applied to whole-brain calcium imaging recordings in freely moving C. elegans, this analysis pipeline located 150 neurons for the duration of an 8 minute recording and consistently found more neurons more quickly than manual or semi-automated approaches.



### Message-passing algorithms for synchronization problems over compact groups
- **Arxiv ID**: http://arxiv.org/abs/1610.04583v1
- **DOI**: 10.1002/cpa.21750
- **Categories**: **cs.IT**, cs.CV, cs.DS, math.IT, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1610.04583v1)
- **Published**: 2016-10-14 19:05:32+00:00
- **Updated**: 2016-10-14 19:05:32+00:00
- **Authors**: Amelia Perry, Alexander S. Wein, Afonso S. Bandeira, Ankur Moitra
- **Comment**: 35 pages, 11 figures
- **Journal**: None
- **Summary**: Various alignment problems arising in cryo-electron microscopy, community detection, time synchronization, computer vision, and other fields fall into a common framework of synchronization problems over compact groups such as Z/L, U(1), or SO(3). The goal of such problems is to estimate an unknown vector of group elements given noisy relative observations. We present an efficient iterative algorithm to solve a large class of these problems, allowing for any compact group, with measurements on multiple 'frequency channels' (Fourier modes, or more generally, irreducible representations of the group). Our algorithm is a highly efficient iterative method following the blueprint of approximate message passing (AMP), which has recently arisen as a central technique for inference problems such as structured low-rank estimation and compressed sensing. We augment the standard ideas of AMP with ideas from representation theory so that the algorithm can work with distributions over compact groups. Using standard but non-rigorous methods from statistical physics we analyze the behavior of our algorithm on a Gaussian noise model, identifying phases where the problem is easy, (computationally) hard, and (statistically) impossible. In particular, such evidence predicts that our algorithm is information-theoretically optimal in many cases, and that the remaining cases show evidence of statistical-to-computational gaps.



### A Harmonic Mean Linear Discriminant Analysis for Robust Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1610.04631v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1610.04631v2)
- **Published**: 2016-10-14 20:36:57+00:00
- **Updated**: 2016-10-24 16:38:29+00:00
- **Authors**: Shuai Zheng, Feiping Nie, Chris Ding, Heng Huang
- **Comment**: IEEE 28th International Conference on Tools with Artificial
  Intelligence, ICTAI 2016
- **Journal**: None
- **Summary**: Linear Discriminant Analysis (LDA) is a widely-used supervised dimensionality reduction method in computer vision and pattern recognition. In null space based LDA (NLDA), a well-known LDA extension, between-class distance is maximized in the null space of the within-class scatter matrix. However, there are some limitations in NLDA. Firstly, for many data sets, null space of within-class scatter matrix does not exist, thus NLDA is not applicable to those datasets. Secondly, NLDA uses arithmetic mean of between-class distances and gives equal consideration to all between-class distances, which makes larger between-class distances can dominate the result and thus limits the performance of NLDA. In this paper, we propose a harmonic mean based Linear Discriminant Analysis, Multi-Class Discriminant Analysis (MCDA), for image classification, which minimizes the reciprocal of weighted harmonic mean of pairwise between-class distance. More importantly, MCDA gives higher priority to maximize small between-class distances. MCDA can be extended to multi-label dimension reduction. Results on 7 single-label data sets and 4 multi-label data sets show that MCDA has consistently better performance than 10 other single-label approaches and 4 other multi-label approaches in terms of classification accuracy, macro and micro average F1 score.



### Deep Learning Ensembles for Melanoma Recognition in Dermoscopy Images
- **Arxiv ID**: http://arxiv.org/abs/1610.04662v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.04662v2)
- **Published**: 2016-10-14 22:31:34+00:00
- **Updated**: 2016-10-18 00:25:35+00:00
- **Authors**: Noel Codella, Quoc-Bao Nguyen, Sharath Pankanti, David Gutman, Brian Helba, Allan Halpern, John R. Smith
- **Comment**: URL for the IBM Journal of Research and Development:
  http://www.research.ibm.com/journal/
- **Journal**: IBM Journal of Research and Development, vol. 61, no. 4/5, 2017
- **Summary**: Melanoma is the deadliest form of skin cancer. While curable with early detection, only highly trained specialists are capable of accurately recognizing the disease. As expertise is in limited supply, automated systems capable of identifying disease could save lives, reduce unnecessary biopsies, and reduce costs. Toward this goal, we propose a system that combines recent developments in deep learning with established machine learning approaches, creating ensembles of methods that are capable of segmenting skin lesions, as well as analyzing the detected area and surrounding tissue for melanoma detection. The system is evaluated using the largest publicly available benchmark dataset of dermoscopic images, containing 900 training and 379 testing images. New state-of-the-art performance levels are demonstrated, leading to an improvement in the area under receiver operating characteristic curve of 7.5% (0.843 vs. 0.783), in average precision of 4% (0.649 vs. 0.624), and in specificity measured at the clinically relevant 95% sensitivity operating point 2.9 times higher than the previous state-of-the-art (36.8% specificity compared to 12.5%). Compared to the average of 8 expert dermatologists on a subset of 100 test images, the proposed system produces a higher accuracy (76% vs. 70.5%), and specificity (62% vs. 59%) evaluated at an equivalent sensitivity (82%).



