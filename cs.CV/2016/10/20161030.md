# Arxiv Papers in cs.CV on 2016-10-30
### Conditional Image Synthesis With Auxiliary Classifier GANs
- **Arxiv ID**: http://arxiv.org/abs/1610.09585v4
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1610.09585v4)
- **Published**: 2016-10-30 00:29:31+00:00
- **Updated**: 2017-07-20 20:23:31+00:00
- **Authors**: Augustus Odena, Christopher Olah, Jonathon Shlens
- **Comment**: None
- **Journal**: None
- **Summary**: Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.



### A Scalable and Robust Framework for Intelligent Real-time Video Surveillance
- **Arxiv ID**: http://arxiv.org/abs/1610.09590v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1610.09590v1)
- **Published**: 2016-10-30 01:22:52+00:00
- **Updated**: 2016-10-30 01:22:52+00:00
- **Authors**: Shreenath Dutt, Ankita Kalra
- **Comment**: 4 pages, 3 figures, Presented in International Conference on Advances
  in Computing, Communications and Informatics (ICACCI-2016), September 2016
- **Journal**: None
- **Summary**: In this paper, we present an intelligent, reliable and storage-efficient video surveillance system using Apache Storm and OpenCV. As a Storm topology, we have added multiple information extraction modules that only write important content to the disk. Our topology is extensible, capable of adding novel algorithms as per the use case without affecting the existing ones, since all the processing is independent of each other. This framework is also highly scalable and fault tolerant, which makes it a best option for organisations that need to monitor a large network of surveillance cameras.



### Generalized Haar Filter based Deep Networks for Real-Time Object Detection in Traffic Scene
- **Arxiv ID**: http://arxiv.org/abs/1610.09609v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1610.09609v1)
- **Published**: 2016-10-30 07:02:57+00:00
- **Updated**: 2016-10-30 07:02:57+00:00
- **Authors**: Keyu Lu, Jian Li, Xiangjing An, Hangen He
- **Comment**: None
- **Journal**: None
- **Summary**: Vision-based object detection is one of the fundamental functions in numerous traffic scene applications such as self-driving vehicle systems and advance driver assistance systems (ADAS). However, it is also a challenging task due to the diversity of traffic scene and the storage, power and computing source limitations of the platforms for traffic scene applications. This paper presents a generalized Haar filter based deep network which is suitable for the object detection tasks in traffic scene. In this approach, we first decompose a object detection task into several easier local regression tasks. Then, we handle the local regression tasks by using several tiny deep networks which simultaneously output the bounding boxes, categories and confidence scores of detected objects. To reduce the consumption of storage and computing resources, the weights of the deep networks are constrained to the form of generalized Haar filter in training phase. Additionally, we introduce the strategy of sparse windows generation to improve the efficiency of the algorithm. Finally, we perform several experiments to validate the performance of our proposed approach. Experimental results demonstrate that the proposed approach is both efficient and effective in traffic scene compared with the state-of-the-art.



### Compressed Learning: A Deep Neural Network Approach
- **Arxiv ID**: http://arxiv.org/abs/1610.09615v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.09615v1)
- **Published**: 2016-10-30 07:54:19+00:00
- **Updated**: 2016-10-30 07:54:19+00:00
- **Authors**: Amir Adler, Michael Elad, Michael Zibulevsky
- **Comment**: None
- **Journal**: None
- **Summary**: Compressed Learning (CL) is a joint signal processing and machine learning framework for inference from a signal, using a small number of measurements obtained by linear projections of the signal. In this paper we present an end-to-end deep learning approach for CL, in which a network composed of fully-connected layers followed by convolutional layers perform the linear sensing and non-linear inference stages. During the training phase, the sensing matrix and the non-linear inference operator are jointly optimized, and the proposed approach outperforms state-of-the-art for the task of image classification. For example, at a sensing rate of 1% (only 8 measurements of 28 X 28 pixels images), the classification error for the MNIST handwritten digits dataset is 6.46% compared to 41.06% with state-of-the-art.



### Discovering containment: from infants to machines
- **Arxiv ID**: http://arxiv.org/abs/1610.09625v1
- **DOI**: 10.1016/j.cognition.2018.11.001
- **Categories**: **q-bio.NC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1610.09625v1)
- **Published**: 2016-10-30 10:26:22+00:00
- **Updated**: 2016-10-30 10:26:22+00:00
- **Authors**: Shimon Ullman, Nimrod Dorfman, Daniel Harari
- **Comment**: None
- **Journal**: Cognition 183 (2019) 67-81
- **Summary**: Current artificial learning systems can recognize thousands of visual categories, or play Go at a champion"s level, but cannot explain infants learning, in particular the ability to learn complex concepts without guidance, in a specific order. A notable example is the category of 'containers' and the notion of containment, one of the earliest spatial relations to be learned, starting already at 2.5 months, and preceding other common relations (e.g., support). Such spontaneous unsupervised learning stands in contrast with current highly successful computational models, which learn in a supervised manner, that is, by using large data sets of labeled examples. How can meaningful concepts be learned without guidance, and what determines the trajectory of infant learning, making some notions appear consistently earlier than others?



### Accurate Deep Representation Quantization with Gradient Snapping Layer for Similarity Search
- **Arxiv ID**: http://arxiv.org/abs/1610.09645v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.09645v1)
- **Published**: 2016-10-30 13:24:54+00:00
- **Updated**: 2016-10-30 13:24:54+00:00
- **Authors**: Shicong Liu, Hongtao Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advance of large scale similarity search involves using deeply learned representations to improve the search accuracy and use vector quantization methods to increase the search speed. However, how to learn deep representations that strongly preserve similarities between data pairs and can be accurately quantized via vector quantization remains a challenging task. Existing methods simply leverage quantization loss and similarity loss, which result in unexpectedly biased back-propagating gradients and affect the search performances. To this end, we propose a novel gradient snapping layer (GSL) to directly regularize the back-propagating gradient towards a neighboring codeword, the generated gradients are un-biased for reducing similarity loss and also propel the learned representations to be accurately quantized. Joint deep representation and vector quantization learning can be easily performed by alternatively optimize the quantization codebook and the deep neural network. The proposed framework is compatible with various existing vector quantization approaches. Experimental results demonstrate that the proposed framework is effective, flexible and outperforms the state-of-the-art large scale similarity search methods.



### Visual Tracking via Boolean Map Representations
- **Arxiv ID**: http://arxiv.org/abs/1610.09652v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.09652v1)
- **Published**: 2016-10-30 14:17:05+00:00
- **Updated**: 2016-10-30 14:17:05+00:00
- **Authors**: Kaihua Zhang, Qingshan Liu, Ming-Hsuan Yang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a simple yet effective Boolean map based representation that exploits connectivity cues for visual tracking. We describe a target object with histogram of oriented gradients and raw color features, of which each one is characterized by a set of Boolean maps generated by uniformly thresholding their values. The Boolean maps effectively encode multi-scale connectivity cues of the target with different granularities. The fine-grained Boolean maps capture spatially structural details that are effective for precise target localization while the coarse-grained ones encode global shape information that are robust to large target appearance variations. Finally, all the Boolean maps form together a robust representation that can be approximated by an explicit feature map of the intersection kernel, which is fed into a logistic regression classifier with online update, and the target location is estimated within a particle filter framework. The proposed representation scheme is computationally efficient and facilitates achieving favorable performance in terms of accuracy and robustness against the state-of-the-art tracking methods on a large benchmark dataset of 50 image sequences.



### Real-Time Image Distortion Correction: Analysis and Evaluation of FPGA-Compatible Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1610.09712v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.09712v1)
- **Published**: 2016-10-30 21:14:15+00:00
- **Updated**: 2016-10-30 21:14:15+00:00
- **Authors**: Paolo Di Febbo, Stefano Mattoccia, Carlo Dal Mutto
- **Comment**: To be published in Proceedings of the International Conference on
  Reconfigurable Computing and FPGAs, Cancun, Mexico, 30 November, 2 December
  2016
- **Journal**: None
- **Summary**: Image distortion correction is a critical pre-processing step for a variety of computer vision and image processing algorithms. Standard real-time software implementations are generally not suited for direct hardware porting, so appropriated versions need to be designed in order to obtain implementations deployable on FPGAs. In this paper, hardware-compatible techniques for image distortion correction are introduced and analyzed in details. The considered solutions are compared in terms of output quality by using a geometrical-error-based approach, with particular emphasis on robustness with respect to increasing lens distortion. The required amount of hardware resources is also estimated for each considered approach.



