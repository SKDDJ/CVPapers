# Arxiv Papers in cs.CV on 2016-10-24
### A coarse-to-fine algorithm for registration in 3D street-view cross-source point clouds
- **Arxiv ID**: http://arxiv.org/abs/1610.07324v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.07324v1)
- **Published**: 2016-10-24 08:22:32+00:00
- **Updated**: 2016-10-24 08:22:32+00:00
- **Authors**: Xiaoshui Huang, Jian Zhang, Qiang Wu, Lixin Fan, Chun Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: With the development of numerous 3D sensing technologies, object registration on cross-source point cloud has aroused researchers' interests. When the point clouds are captured from different kinds of sensors, there are large and different kinds of variations. In this study, we address an even more challenging case in which the differently-source point clouds are acquired from a real street view. One is produced directly by the LiDAR system and the other is generated by using VSFM software on image sequence captured from RGB cameras. When it confronts to large scale point clouds, previous methods mostly focus on point-to-point level registration, and the methods have many limitations.The reason is that the least mean error strategy shows poor ability in registering large variable cross-source point clouds. In this paper, different from previous ICP-based methods, and from a statistic view, we propose a effective coarse-to-fine algorithm to detect and register a small scale SFM point cloud in a large scale Lidar point cloud. Seen from the experimental results, the model can successfully run on LiDAR and SFM point clouds, hence it can make a contribution to many applications, such as robotics and smart city development.



### MultiCol-SLAM - A Modular Real-Time Multi-Camera SLAM System
- **Arxiv ID**: http://arxiv.org/abs/1610.07336v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.07336v1)
- **Published**: 2016-10-24 09:27:47+00:00
- **Updated**: 2016-10-24 09:27:47+00:00
- **Authors**: Steffen Urban, Stefan Hinz
- **Comment**: 15 pages, 8 figures, 2 tables
- **Journal**: None
- **Summary**: The basis for most vision based applications like robotics, self-driving cars and potentially augmented and virtual reality is a robust, continuous estimation of the position and orientation of a camera system w.r.t the observed environment (scene). In recent years many vision based systems that perform simultaneous localization and mapping (SLAM) have been presented and released as open source. In this paper, we extend and improve upon a state-of-the-art SLAM to make it applicable to arbitrary, rigidly coupled multi-camera systems (MCS) using the MultiCol model. In addition, we include a performance evaluation on accurate ground truth and compare the robustness of the proposed method to a single camera version of the SLAM system. An open source implementation of the proposed multi-fisheye camera SLAM system can be found on-line https://github.com/urbste/MultiCol-SLAM.



### Theoretical Analysis of Active Contours on Graphs
- **Arxiv ID**: http://arxiv.org/abs/1610.07381v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/1610.07381v1)
- **Published**: 2016-10-24 12:20:55+00:00
- **Updated**: 2016-10-24 12:20:55+00:00
- **Authors**: Christos Sakaridis, Kimon Drakopoulos, Petros Maragos
- **Comment**: 20 pages
- **Journal**: None
- **Summary**: Active contour models based on partial differential equations have proved successful in image segmentation, yet the study of their geometric formulation on arbitrary geometric graphs is still at an early stage. In this paper, we introduce geometric approximations of gradient and curvature, which are used in the geodesic active contour model. We prove convergence in probability of our gradient approximation to the true gradient value and derive an asymptotic upper bound for the error of this approximation for the class of random geometric graphs. Two different approaches for the approximation of curvature are presented and both are also proved to converge in probability in the case of random geometric graphs. We propose neighborhood-based filtering on graphs to improve the accuracy of the aforementioned approximations and define two variants of Gaussian smoothing on graphs which include normalization in order to adapt to graph non-uniformities. The performance of our active contour framework on graphs is demonstrated in the segmentation of regular images and geographical data defined on arbitrary graphs.



### Record Counting in Historical Handwritten Documents with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1610.07393v2
- **DOI**: 10.1016/j.patrec.2017.10.023
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.07393v2)
- **Published**: 2016-10-24 12:56:20+00:00
- **Updated**: 2016-10-25 10:23:02+00:00
- **Authors**: Samuele Capobianco, Simone Marinai
- **Comment**: Accepted to ICPR workshop on Deep Learning for Pattern Recognition
  (DLPR 2016)
- **Journal**: None
- **Summary**: In this paper, we investigate the use of Convolutional Neural Networks for counting the number of records in historical handwritten documents. With this work we demonstrate that training the networks only with synthetic images allows us to perform a near perfect evaluation of the number of records printed on historical documents. The experiments have been performed on a benchmark dataset composed by marriage records and outperform previous results on this dataset.



### Savu: A Python-based, MPI Framework for Simultaneous Processing of Multiple, N-dimensional, Large Tomography Datasets
- **Arxiv ID**: http://arxiv.org/abs/1610.08015v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV, cs.DB
- **Links**: [PDF](http://arxiv.org/pdf/1610.08015v1)
- **Published**: 2016-10-24 13:22:09+00:00
- **Updated**: 2016-10-24 13:22:09+00:00
- **Authors**: Nicola Wadeson, Mark Basham
- **Comment**: 10 pages, 10 figures, 1 table
- **Journal**: None
- **Summary**: Diamond Light Source (DLS), the UK synchrotron facility, attracts scientists from across the world to perform ground-breaking x-ray experiments. With over 3000 scientific users per year, vast amounts of data are collected across the experimental beamlines, with the highest volume of data collected during tomographic imaging experiments. A growing interest in tomography as an imaging technique, has led to an expansion in the range of experiments performed, in addition to a growth in the size of the data per experiment.   Savu is a portable, flexible, scientific processing pipeline capable of processing multiple, n-dimensional datasets in serial on a PC, or in parallel across a cluster. Developed at DLS, and successfully deployed across the beamlines, it uses a modular plugin format to enable experiment-specific processing and utilises parallel HDF5 to remove RAM restrictions. The Savu design, described throughout this paper, focuses on easy integration of existing and new functionality, flexibility and ease of use for users and developers alike.



### Virtual Embodiment: A Scalable Long-Term Strategy for Artificial Intelligence Research
- **Arxiv ID**: http://arxiv.org/abs/1610.07432v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, 68T01, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1610.07432v1)
- **Published**: 2016-10-24 14:37:27+00:00
- **Updated**: 2016-10-24 14:37:27+00:00
- **Authors**: Douwe Kiela, Luana Bulat, Anita L. Vero, Stephen Clark
- **Comment**: None
- **Journal**: None
- **Summary**: Meaning has been called the "holy grail" of a variety of scientific disciplines, ranging from linguistics to philosophy, psychology and the neurosciences. The field of Artifical Intelligence (AI) is very much a part of that list: the development of sophisticated natural language semantics is a sine qua non for achieving a level of intelligence comparable to humans. Embodiment theories in cognitive science hold that human semantic representation depends on sensori-motor experience; the abundant evidence that human meaning representation is grounded in the perception of physical reality leads to the conclusion that meaning must depend on a fusion of multiple (perceptual) modalities. Despite this, AI research in general, and its subdisciplines such as computational linguistics and computer vision in particular, have focused primarily on tasks that involve a single modality. Here, we propose virtual embodiment as an alternative, long-term strategy for AI research that is multi-modal in nature and that allows for the kind of scalability required to develop the field coherently and incrementally, in an ethically responsible fashion.



### Deep Multi-scale Location-aware 3D Convolutional Neural Networks for Automated Detection of Lacunes of Presumed Vascular Origin
- **Arxiv ID**: http://arxiv.org/abs/1610.07442v2
- **DOI**: 10.1016/j.nicl.2017.01.033
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.07442v2)
- **Published**: 2016-10-24 14:51:47+00:00
- **Updated**: 2016-10-29 13:14:32+00:00
- **Authors**: Mohsen Ghafoorian, Nico Karssemeijer, Tom Heskes, Mayra Bergkamp, Joost Wissink, Jiri Obels, Karlijn Keizer, Frank-Erik de Leeuw, Bram van Ginneken, Elena Marchiori, Bram Platel
- **Comment**: 11 pages, 7 figures
- **Journal**: Neuroimage Clin 14 (2017) 391-399
- **Summary**: Lacunes of presumed vascular origin (lacunes) are associated with an increased risk of stroke, gait impairment, and dementia and are a primary imaging feature of the small vessel disease. Quantification of lacunes may be of great importance to elucidate the mechanisms behind neuro-degenerative disorders and is recommended as part of study standards for small vessel disease research. However, due to the different appearance of lacunes in various brain regions and the existence of other similar-looking structures, such as perivascular spaces, manual annotation is a difficult, elaborative and subjective task, which can potentially be greatly improved by reliable and consistent computer-aided detection (CAD) routines.   In this paper, we propose an automated two-stage method using deep convolutional neural networks (CNN). We show that this method has good performance and can considerably benefit readers. We first use a fully convolutional neural network to detect initial candidates. In the second step, we employ a 3D CNN as a false positive reduction tool. As the location information is important to the analysis of candidate structures, we further equip the network with contextual information using multi-scale analysis and integration of explicit location features. We trained, validated and tested our networks on a large dataset of 1075 cases obtained from two different studies. Subsequently, we conducted an observer study with four trained observers and compared our method with them using a free-response operating characteristic analysis. Shown on a test set of 111 cases, the resulting CAD system exhibits performance similar to the trained human observers and achieves a sensitivity of 0.974 with 0.13 false positives per slice. A feasibility study also showed that a trained human observer would considerably benefit once aided by the CAD system.



### Feature Sensitive Label Fusion with Random Walker for Atlas-based Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1610.07475v2
- **DOI**: 10.1109/TIP.2017.2691799
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.07475v2)
- **Published**: 2016-10-24 16:22:07+00:00
- **Updated**: 2017-11-09 15:13:11+00:00
- **Authors**: Siqi Bao, Albert C. S. Chung
- **Comment**: This manuscript has been accepted for TIP2017
- **Journal**: None
- **Summary**: In this paper, a novel label fusion method is proposed for brain magnetic resonance image segmentation. This label fusion method is formulated on a graph, which embraces both label priors from atlases and anatomical priors from target image. To represent a pixel in a comprehensive way, three kinds of feature vectors are generated, including intensity, gradient and structural signature. To select candidate atlas nodes for fusion, rather than exact searching, randomized k-d tree with spatial constraint is introduced as an efficient approximation for high-dimensional feature matching. Feature Sensitive Label Prior (FSLP), which takes both the consistency and variety of different features into consideration, is proposed to gather atlas priors. As FSLP is a non-convex problem, one heuristic approach is further designed to solve it efficiently. Moreover, based on the anatomical knowledge, parts of the target pixels are also employed as graph seeds to assist the label fusion process and an iterative strategy is utilized to gradually update the label map. The comprehensive experiments carried out on two publicly available databases give results to demonstrate that the proposed method can obtain better segmentation quality.



### Laplacian regularized low rank subspace clustering
- **Arxiv ID**: http://arxiv.org/abs/1610.07488v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.07488v2)
- **Published**: 2016-10-24 16:51:05+00:00
- **Updated**: 2016-10-28 01:24:55+00:00
- **Authors**: Yu Song, Yiquan Wu
- **Comment**: 17 pages, 4 figures, 5 tables. The paper is submitted to "computer
  vision and image understanding". arXiv admin note: text overlap with
  arXiv:1610.03604; text overlap with arXiv:1203.1005 by other authors
- **Journal**: None
- **Summary**: The problem of fitting a union of subspaces to a collection of data points drawn from multiple subspaces is considered in this paper. In the traditional low rank representation model, the dictionary used to represent the data points is chosen as the data points themselves and thus the dictionary is corrupted with noise. This problem is solved in the low rank subspace clustering model which decomposes the corrupted data matrix as the sum of a clean and self-expressive dictionary plus a matrix of noise and gross errors. Also, the clustering results of the low rank representation model can be enhanced by using a graph of data similarity. This model is called Laplacian regularized low rank representation model with a graph regularization term added to the objective function. Inspired from the above two ideas, in this paper a Laplacian regularized low rank subspace clustering model is proposed. This model uses a clean dictionary to represent the data points and a graph regularization term is also incorporated in the objective function. Experimental results show that, compared with the traditional low rank representation model, low rank subspace clustering model and several other state-of-the-art subspace clustering model, the model proposed in this paper can get better subspace clustering results with lower clustering error.



### Automatic and Manual Segmentation of Hippocampus in Epileptic Patients MRI
- **Arxiv ID**: http://arxiv.org/abs/1610.07557v2
- **DOI**: None
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1610.07557v2)
- **Published**: 2016-10-24 19:17:18+00:00
- **Updated**: 2016-10-26 15:54:42+00:00
- **Authors**: Mohammad-Parsa Hosseini, Mohammad-Reza Nazem-Zadeh, Dario Pompili, Kourosh Jafari-Khouzani, Kost Elisevich, Hamid Soltanian-Zadeh
- **Comment**: Presented in the 6th Annual New York Medical Imaging Informatics
  Symposium (NYMIIS), New York, NY, USA, Sept. 2015
- **Journal**: None
- **Summary**: The hippocampus is a seminal structure in the most common surgically-treated form of epilepsy. Accurate segmentation of the hippocampus aids in establishing asymmetry regarding size and signal characteristics in order to disclose the likely site of epileptogenicity. With sufficient refinement, it may ultimately aid in the avoidance of invasive monitoring with its expense and risk for the patient. To this end, a reliable and consistent method for segmentation of the hippocampus from magnetic resonance imaging (MRI) is needed. In this work, we present a systematic and statistical analysis approach for evaluation of automated segmentation methods in order to establish one that reliably approximates the results achieved by manual tracing of the hippocampus.



### Automated OCT Segmentation for Images with DME
- **Arxiv ID**: http://arxiv.org/abs/1610.07560v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.07560v1)
- **Published**: 2016-10-24 19:24:38+00:00
- **Updated**: 2016-10-24 19:24:38+00:00
- **Authors**: Sohini Roychowdhury, Dara D. Koozekanani, Michael Reinsbach, Keshab K. Parhi
- **Comment**: 31 pages, 7 figures, CRC Press Book Chapter, 2016
- **Journal**: None
- **Summary**: This paper presents a novel automated system that segments six sub-retinal layers from optical coherence tomography (OCT) image stacks of healthy patients and patients with diabetic macular edema (DME). First, each image in the OCT stack is denoised using a Wiener deconvolution algorithm that estimates the additive speckle noise variance using a novel Fourier-domain based structural error. This denoising method enhances the image SNR by an average of 12dB. Next, the denoised images are subjected to an iterative multi-resolution high-pass filtering algorithm that detects seven sub-retinal surfaces in six iterative steps. The thicknesses of each sub-retinal layer for all scans from a particular OCT stack are then compared to the manually marked groundtruth. The proposed system uses adaptive thresholds for denoising and segmenting each image and hence it is robust to disruptions in the retinal micro-structure due to DME. The proposed denoising and segmentation system has an average error of 1.2-5.8 $\mu m$ and 3.5-26$\mu m$ for segmenting sub-retinal surfaces in normal and abnormal images with DME, respectively. For estimating the sub-retinal layer thicknesses, the proposed system has an average error of 0.2-2.5 $\mu m$ and 1.8-18 $\mu m$ in normal and abnormal images, respectively. Additionally, the average inner sub-retinal layer thickness in abnormal images is estimated as 275$\mu m (r=0.92)$ with an average error of 9.3 $\mu m$, while the average thickness of the outer layers in abnormal images is estimated as 57.4$\mu m (r=0.74)$ with an average error of 3.5 $\mu m$. The proposed system can be useful for tracking the disease progression for DME over a period of time.



### A data augmentation methodology for training machine/deep learning gait recognition algorithms
- **Arxiv ID**: http://arxiv.org/abs/1610.07570v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.07570v1)
- **Published**: 2016-10-24 19:35:35+00:00
- **Updated**: 2016-10-24 19:35:35+00:00
- **Authors**: Christoforos C. Charalambous, Anil A. Bharath
- **Comment**: The paper and supplementary material are available on
  http://www.bmva.org/bmvc/2016/papers/paper110/index.html Dataset is available
  on http://www.bicv.org/datasets/m Proceedings of the BMVC 2016
- **Journal**: None
- **Summary**: There are several confounding factors that can reduce the accuracy of gait recognition systems. These factors can reduce the distinctiveness, or alter the features used to characterise gait, they include variations in clothing, lighting, pose and environment, such as the walking surface. Full invariance to all confounding factors is challenging in the absence of high-quality labelled training data. We introduce a simulation-based methodology and a subject-specific dataset which can be used for generating synthetic video frames and sequences for data augmentation. With this methodology, we generated a multi-modal dataset. In addition, we supply simulation files that provide the ability to simultaneously sample from several confounding variables. The basis of the data is real motion capture data of subjects walking and running on a treadmill at different speeds. Results from gait recognition experiments suggest that information about the identity of subjects is retained within synthetically generated examples. The dataset and methodology allow studies into fully-invariant identity recognition spanning a far greater number of observation conditions than would otherwise be possible.



### Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling
- **Arxiv ID**: http://arxiv.org/abs/1610.07584v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1610.07584v2)
- **Published**: 2016-10-24 19:53:41+00:00
- **Updated**: 2017-01-04 18:35:52+00:00
- **Authors**: Jiajun Wu, Chengkai Zhang, Tianfan Xue, William T. Freeman, Joshua B. Tenenbaum
- **Comment**: NIPS 2016. The first two authors contributed equally to this work
- **Journal**: None
- **Summary**: We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.



### A Learned Representation For Artistic Style
- **Arxiv ID**: http://arxiv.org/abs/1610.07629v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1610.07629v5)
- **Published**: 2016-10-24 20:06:54+00:00
- **Updated**: 2017-02-09 16:29:09+00:00
- **Authors**: Vincent Dumoulin, Jonathon Shlens, Manjunath Kudlur
- **Comment**: 9 pages. 15 pages of Appendix, International Conference on Learning
  Representations (ICLR) 2017
- **Journal**: None
- **Summary**: The diversity of painting styles represents a rich visual vocabulary for the construction of an image. The degree to which one may learn and parsimoniously capture this visual vocabulary measures our understanding of the higher level features of paintings, if not images in general. In this work we investigate the construction of a single, scalable deep network that can parsimoniously capture the artistic style of a diversity of paintings. We demonstrate that such a network generalizes across a diversity of artistic styles by reducing a painting to a point in an embedding space. Importantly, this model permits a user to explore new painting styles by arbitrarily combining the styles learned from individual paintings. We hope that this work provides a useful step towards building rich models of paintings and offers a window on to the structure of the learned representation of artistic style.



