# Arxiv Papers in cs.CV on 2016-10-13
### Predicting the dynamics of 2d objects with a deep residual network
- **Arxiv ID**: http://arxiv.org/abs/1610.04032v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1610.04032v2)
- **Published**: 2016-10-13 11:27:07+00:00
- **Updated**: 2016-11-24 11:12:52+00:00
- **Authors**: François Fleuret
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate how a residual network can learn to predict the dynamics of interacting shapes purely as an image-to-image regression task.   With a simple 2d physics simulator, we generate short sequences composed of rectangles put in motion by applying a pulling force at a point picked at random. The network is trained with a quadratic loss to predict the image of the resulting configuration, given the image of the starting configuration and an image indicating the point of grasping.   Experiments show that the network learns to predict accurately the resulting image, which implies in particular that (1) it segments rectangles as distinct components, (2) it infers which one contains the grasping point, (3) it models properly the dynamic of a single rectangle, including the torque, (4) it detects and handles collisions to some extent, and (5) it re-synthesizes properly the entire scene with displaced rectangles.



### Stroke Sequence-Dependent Deep Convolutional Neural Network for Online Handwritten Chinese Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/1610.04057v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.04057v1)
- **Published**: 2016-10-13 12:54:37+00:00
- **Updated**: 2016-10-13 12:54:37+00:00
- **Authors**: Baotian Hu, Xin Liu, Xiangping Wu, Qingcai Chen
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel model, named Stroke Sequence-dependent Deep Convolutional Neural Network (SSDCNN), using the stroke sequence information and eight-directional features for Online Handwritten Chinese Character Recognition (OLHCCR). On one hand, SSDCNN can learn the representation of Online Handwritten Chinese Character (OLHCC) by incorporating the natural sequence information of the strokes. On the other hand, SSDCNN can incorporate eight-directional features in a natural way. In order to train SSDCNN, we divide the process of training into two stages: 1) The training data is used to pre-train the whole architecture until the performance tends to converge. 2) Fully-connected neural network which is used to combine the stroke sequence-dependent representation with eight-directional features and softmax layer are further trained. Experiments were conducted on the OLHCCR competition tasks of ICDAR 2013. Results show that, SSDCNN can reduce the recognition error by 50\% (5.13\% vs 2.56\%) compared to the model which only use eight-directional features. The proposed SSDCNN achieves 97.44\% accuracy which reduces the recognition error by about 1.9\% compared with the best submitted system on ICDAR2013 competition. These results indicate that SSDCNN can exploit the stroke sequence information to learn high-quality representation of OLHCC. It also shows that the learnt representation and the classical eight-directional features complement each other within the SSDCNN architecture.



### Video Fill in the Blank with Merging LSTMs
- **Arxiv ID**: http://arxiv.org/abs/1610.04062v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.04062v1)
- **Published**: 2016-10-13 13:05:41+00:00
- **Updated**: 2016-10-13 13:05:41+00:00
- **Authors**: Amir Mazaheri, Dong Zhang, Mubarak Shah
- **Comment**: for Large Scale Movie Description and Understanding Challenge (LSMDC)
  2016, "Movie fill-in-the-blank" Challenge, UCF_CRCV
- **Journal**: None
- **Summary**: Given a video and its incomplete textural description with missing words, the Video-Fill-in-the-Blank (ViFitB) task is to automatically find the missing word. The contextual information of the sentences are important to infer the missing words; the visual cues are even more crucial to get a more accurate inference. In this paper, we presents a new method which intuitively takes advantage of the structure of the sentences and employs merging LSTMs (to merge two LSTMs) to tackle the problem with embedded textural and visual cues. In the experiments, we have demonstrated the superior performance of the proposed method on the challenging "Movie Fill-in-the-Blank" dataset.



### Towards end-to-end optimisation of functional image analysis pipelines
- **Arxiv ID**: http://arxiv.org/abs/1610.04079v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1610.04079v1)
- **Published**: 2016-10-13 13:57:55+00:00
- **Updated**: 2016-10-13 13:57:55+00:00
- **Authors**: Albert Vilamala, Kristoffer Hougaard Madsen, Lars Kai Hansen
- **Comment**: 7 pages, 2 figures
- **Journal**: None
- **Summary**: The study of neurocognitive tasks requiring accurate localisation of activity often rely on functional Magnetic Resonance Imaging, a widely adopted technique that makes use of a pipeline of data processing modules, each involving a variety of parameters. These parameters are frequently set according to the local goal of each specific module, not accounting for the rest of the pipeline. Given recent success of neural network research in many different domains, we propose to convert the whole data pipeline into a deep neural network, where the parameters involved are jointly optimised by the network to best serve a common global goal. As a proof of concept, we develop a module able to adaptively apply the most suitable spatial smoothing to every brain volume for each specific neuroimaging task, and we validate its results in a standard brain decoding experiment.



### Automatic View-Point Selection for Inter-Operative Endoscopic Surveillance
- **Arxiv ID**: http://arxiv.org/abs/1610.04097v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.04097v1)
- **Published**: 2016-10-13 14:24:33+00:00
- **Updated**: 2016-10-13 14:24:33+00:00
- **Authors**: Anant S. Vemuri, Stephane A. Nicolau, Jacques Marescaux, Luc Soler, Nicholas Ayache
- **Comment**: Medical Content-based Retrieval for Clinical Decision Support and
  Treatment Planning, MICCAI Conference
- **Journal**: None
- **Summary**: Esophageal adenocarcinoma arises from Barrett's esophagus, which is the most serious complication of gastroesophageal reflux disease. Strategies for screening involve periodic surveillance and tissue biopsies. A major challenge in such regular examinations is to record and track the disease evolution and re-localization of biopsied sites to provide targeted treatments. In this paper, we extend our original inter-operative relocalization framework to provide a constrained image based search for obtaining the best view-point match to the live view. Within this context we investigate the effect of: the choice of feature descriptors and color-space; filtering of uninformative frames and endoscopic modality, for view-point localization. Our experiments indicate an improvement in the best view-point retrieval rate to [92%,87%] from [73%,76%] (in our previous approach) for NBI and WL.



### Embedded real-time stereo estimation via Semi-Global Matching on the GPU
- **Arxiv ID**: http://arxiv.org/abs/1610.04121v1
- **DOI**: 10.1016/j.procs.2016.05.305
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.04121v1)
- **Published**: 2016-10-13 15:15:11+00:00
- **Updated**: 2016-10-13 15:15:11+00:00
- **Authors**: Daniel Hernandez-Juarez, Alejandro Chacón, Antonio Espinosa, David Vázquez, Juan Carlos Moure, Antonio Manuel López
- **Comment**: None
- **Journal**: None
- **Summary**: Dense, robust and real-time computation of depth information from stereo-camera systems is a computationally demanding requirement for robotics, advanced driver assistance systems (ADAS) and autonomous vehicles. Semi-Global Matching (SGM) is a widely used algorithm that propagates consistency constraints along several paths across the image. This work presents a real-time system producing reliable disparity estimation results on the new embedded energy-efficient GPU devices. Our design runs on a Tegra X1 at 42 frames per second (fps) for an image size of 640x480, 128 disparity levels, and using 4 path directions for the SGM method.



### GPU-accelerated real-time stixel computation
- **Arxiv ID**: http://arxiv.org/abs/1610.04124v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.04124v1)
- **Published**: 2016-10-13 15:20:40+00:00
- **Updated**: 2016-10-13 15:20:40+00:00
- **Authors**: Daniel Hernandez-Juarez, Antonio Espinosa, David Vázquez, Antonio Manuel López, Juan Carlos Moure
- **Comment**: None
- **Journal**: None
- **Summary**: The Stixel World is a medium-level, compact representation of road scenes that abstracts millions of disparity pixels into hundreds or thousands of stixels. The goal of this work is to implement and evaluate a complete multi-stixel estimation pipeline on an embedded, energy-efficient, GPU-accelerated device. This work presents a full GPU-accelerated implementation of stixel estimation that produces reliable results at 26 frames per second (real-time) on the Tegra X1 for disparity images of 1024x440 pixels and stixel widths of 5 pixels, and achieves more than 400 frames per second on a high-end Titan X GPU card.



### Assessing Threat of Adversarial Examples on Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1610.04256v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.04256v1)
- **Published**: 2016-10-13 20:34:48+00:00
- **Updated**: 2016-10-13 20:34:48+00:00
- **Authors**: Abigail Graese, Andras Rozsa, Terrance E. Boult
- **Comment**: This is a pre-print version to appear in IEEE ICMLA 2016
- **Journal**: None
- **Summary**: Deep neural networks are facing a potential security threat from adversarial examples, inputs that look normal but cause an incorrect classification by the deep neural network. For example, the proposed threat could result in hand-written digits on a scanned check being incorrectly classified but looking normal when humans see them. This research assesses the extent to which adversarial examples pose a security threat, when one considers the normal image acquisition process. This process is mimicked by simulating the transformations that normally occur in acquiring the image in a real world application, such as using a scanner to acquire digits for a check amount or using a camera in an autonomous car. These small transformations negate the effect of the carefully crafted perturbations of adversarial examples, resulting in a correct classification by the deep neural network. Thus just acquiring the image decreases the potential impact of the proposed security threat. We also show that the already widely used process of averaging over multiple crops neutralizes most adversarial examples. Normal preprocessing, such as text binarization, almost completely neutralizes adversarial examples. This is the first paper to show that for text driven classification, adversarial examples are an academic curiosity, not a security threat.



