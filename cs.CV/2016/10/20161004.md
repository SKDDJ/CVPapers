# Arxiv Papers in cs.CV on 2016-10-04
### Real Time Fine-Grained Categorization with Accuracy and Interpretability
- **Arxiv ID**: http://arxiv.org/abs/1610.00824v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.00824v1)
- **Published**: 2016-10-04 02:20:18+00:00
- **Updated**: 2016-10-04 02:20:18+00:00
- **Authors**: Shaoli Huang, Dacheng Tao
- **Comment**: arXiv admin note: text overlap with arXiv:1512.08086
- **Journal**: None
- **Summary**: A well-designed fine-grained categorization system usually has three contradictory requirements: accuracy (the ability to identify objects among subordinate categories); interpretability (the ability to provide human-understandable explanation of recognition system behavior); and efficiency (the speed of the system). To handle the trade-off between accuracy and interpretability, we propose a novel "Deeper Part-Stacked CNN" architecture armed with interpretability by modeling subtle differences between object parts. The proposed architecture consists of a part localization network, a two-stream classification network that simultaneously encodes object-level and part-level cues, and a feature vectors fusion component. Specifically, the part localization network is implemented by exploring a new paradigm for key point localization that first samples a small number of representable pixels and then determine their labels via a convolutional layer followed by a softmax layer. We also use a cropping layer to extract part features and propose a scale mean-max layer for feature fusion learning. Experimentally, our proposed method outperform state-of-the-art approaches both in part localization task and classification task on Caltech-UCSD Birds-200-2011. Moreover, by adopting a set of sharing strategies between the computation of multiple object parts, our single model is fairly efficient running at 32 frames/sec.



### Image Aesthetic Assessment: An Experimental Survey
- **Arxiv ID**: http://arxiv.org/abs/1610.00838v2
- **DOI**: 10.1109/MSP.2017.2696576
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.00838v2)
- **Published**: 2016-10-04 04:09:41+00:00
- **Updated**: 2017-04-20 03:32:41+00:00
- **Authors**: Yubin Deng, Chen Change Loy, Xiaoou Tang
- **Comment**: None
- **Journal**: None
- **Summary**: This survey aims at reviewing recent computer vision techniques used in the assessment of image aesthetic quality. Image aesthetic assessment aims at computationally distinguishing high-quality photos from low-quality ones based on photographic rules, typically in the form of binary classification or quality scoring. A variety of approaches has been proposed in the literature trying to solve this challenging problem. In this survey, we present a systematic listing of the reviewed approaches based on visual feature types (hand-crafted features and deep features) and evaluation criteria (dataset characteristics and evaluation metrics). Main contributions and novelties of the reviewed approaches are highlighted and discussed. In addition, following the emergence of deep learning techniques, we systematically evaluate recent deep learning settings that are useful for developing a robust deep model for aesthetic scoring. Experiments are conducted using simple yet solid baselines that are competitive with the current state-of-the-arts. Moreover, we discuss the possibility of manipulating the aesthetics of images through computational approaches. We hope that our survey could serve as a comprehensive reference source for future research on the study of image aesthetic assessment.



### Cardea: Context-Aware Visual Privacy Protection from Pervasive Cameras
- **Arxiv ID**: http://arxiv.org/abs/1610.00889v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1610.00889v1)
- **Published**: 2016-10-04 08:01:27+00:00
- **Updated**: 2016-10-04 08:01:27+00:00
- **Authors**: Jiayu Shu, Rui Zheng, Pan Hui
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: The growing popularity of mobile and wearable devices with built-in cameras, the bright prospect of camera related applications such as augmented reality and life-logging system, the increased ease of taking and sharing photos, and advances in computer vision techniques have greatly facilitated people's lives in many aspects, but have also inevitably raised people's concerns about visual privacy at the same time. Motivated by recent user studies that people's privacy concerns are dependent on the context, in this paper, we propose Cardea, a context-aware and interactive visual privacy protection framework that enforces privacy protection according to people's privacy preferences. The framework provides people with fine-grained visual privacy protection using: i) personal privacy profiles, with which people can define their context-dependent privacy preferences; and ii) visual indicators: face features, for devices to automatically locate individuals who request privacy protection; and iii) hand gestures, for people to flexibly interact with cameras to temporarily change their privacy preferences. We design and implement the framework consisting of the client app on Android devices and the cloud server. Our evaluation results confirm this framework is practical and effective with 86% overall accuracy, showing promising future for context-aware visual privacy protection from pervasive cameras.



### Adaptive Graph-based Total Variation for Tomographic Reconstructions
- **Arxiv ID**: http://arxiv.org/abs/1610.00893v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.00893v3)
- **Published**: 2016-10-04 08:13:07+00:00
- **Updated**: 2018-03-14 12:18:34+00:00
- **Authors**: Faisal Mahmood, Nauman Shahid, Ulf Skoglund, Pierre Vandergheynst
- **Comment**: 8 Pages, 5 page letter, 3 page supplement, 8 Figures, Accepted for
  publication: IEEE Signal Processing Letters
- **Journal**: None
- **Summary**: Sparsity exploiting image reconstruction (SER) methods have been extensively used with Total Variation (TV) regularization for tomographic reconstructions. Local TV methods fail to preserve texture details and often create additional artefacts due to over-smoothing. Non-Local TV (NLTV) methods have been proposed as a solution to this but they either lack continuous updates due to computational constraints or limit the locality to a small region. In this paper, we propose Adaptive Graph-based TV (AGTV). The proposed method goes beyond spatial similarity between different regions of an image being reconstructed by establishing a connection between similar regions in the entire image regardless of spatial distance. As compared to NLTV the proposed method is computationally efficient and involves updating the graph prior during every iteration making the connection between similar regions stronger. Moreover, it promotes sparsity in the wavelet and graph gradient domains. Since TV is a special case of graph TV the proposed method can also be seen as a generalization of SER and TV methods.



### A novel and effective scoring scheme for structure classification and pairwise similarity measurement
- **Arxiv ID**: http://arxiv.org/abs/1610.01052v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.01052v1)
- **Published**: 2016-10-04 15:47:45+00:00
- **Updated**: 2016-10-04 15:47:45+00:00
- **Authors**: Rezaul Karim, Md. Momin Al Aziz, Swakkhar Shatabda, M. Sohel Rahman
- **Comment**: None
- **Journal**: None
- **Summary**: Protein tertiary structure defines its functions, classification and binding sites. Similar structural characteristics between two proteins often lead to the similar characteristics thereof. Determining structural similarity accurately in real time is a crucial research issue. In this paper, we present a novel and effective scoring scheme that is dependent on novel features extracted from protein alpha carbon distance matrices. Our scoring scheme is inspired from pattern recognition and computer vision. Our method is significantly better than the current state of the art methods in terms of family match of pairs of protein structures and other statistical measurements. The effectiveness of our method is tested on standard benchmark structures. A web service is available at http://research.buet.ac.bd:8080/Comograd/score.html where you can get the similarity measurement score between two protein structures based on our method.



### Sparsity-based Color Image Super Resolution via Exploiting Cross Channel Constraints
- **Arxiv ID**: http://arxiv.org/abs/1610.01066v1
- **DOI**: 10.1109/TIP.2017.2704443
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.01066v1)
- **Published**: 2016-10-04 16:15:36+00:00
- **Updated**: 2016-10-04 16:15:36+00:00
- **Authors**: Hojjat S. Mousavi, Vishal Monga
- **Comment**: None
- **Journal**: None
- **Summary**: Sparsity constrained single image super-resolution (SR) has been of much recent interest. A typical approach involves sparsely representing patches in a low-resolution (LR) input image via a dictionary of example LR patches, and then using the coefficients of this representation to generate the high-resolution (HR) output via an analogous HR dictionary. However, most existing sparse representation methods for super resolution focus on the luminance channel information and do not capture interactions between color channels. In this work, we extend sparsity based super-resolution to multiple color channels by taking color information into account. Edge similarities amongst RGB color bands are exploited as cross channel correlation constraints. These additional constraints lead to a new optimization problem which is not easily solvable; however, a tractable solution is proposed to solve it efficiently. Moreover, to fully exploit the complementary information among color channels, a dictionary learning method is also proposed specifically to learn color dictionaries that encourage edge similarities. Merits of the proposed method over state of the art are demonstrated both visually and quantitatively using image quality metrics.



### Fast Image Classification by Boosting Fuzzy Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1610.01068v1
- **DOI**: 10.1016/j.ins.2015.08.030
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.01068v1)
- **Published**: 2016-10-04 16:18:57+00:00
- **Updated**: 2016-10-04 16:18:57+00:00
- **Authors**: Marcin Korytkowski, Leszek Rutkowski, Rafał Scherer
- **Comment**: 1 figure
- **Journal**: Inf. Sci. (327) 2016 175-182
- **Summary**: This paper presents a novel approach to visual objects classification based on generating simple fuzzy classifiers using local image features to distinguish between one known class and other classes. Boosting meta learning is used to find the most representative local features. The proposed approach is tested on a state-of-the-art image dataset and compared with the bag-of-features image representation model combined with the Support Vector Machine classification. The novel method gives better classification accuracy and the time of learning and testing process is more than 30% shorter.



### Tutorial on Answering Questions about Images with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1610.01076v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1610.01076v1)
- **Published**: 2016-10-04 16:29:28+00:00
- **Updated**: 2016-10-04 16:29:28+00:00
- **Authors**: Mateusz Malinowski, Mario Fritz
- **Comment**: The tutorial was presented at '2nd Summer School on Integrating
  Vision and Language: Deep Learning' in Malta, 2016
- **Journal**: None
- **Summary**: Together with the development of more accurate methods in Computer Vision and Natural Language Understanding, holistic architectures that answer on questions about the content of real-world images have emerged. In this tutorial, we build a neural-based approach to answer questions about images. We base our tutorial on two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the models that we present here can achieve a competitive performance on both datasets, in fact, they are among the best methods that use a combination of LSTM with a global, full frame CNN representation of an image. We hope that after reading this tutorial, the reader will be able to use Deep Learning frameworks, such as Keras and introduced Kraino, to build various architectures that will lead to a further performance improvement on this challenging task.



### Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs
- **Arxiv ID**: http://arxiv.org/abs/1610.01119v2
- **DOI**: 10.1109/TIP.2017.2675339
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.01119v2)
- **Published**: 2016-10-04 18:33:32+00:00
- **Updated**: 2017-02-21 21:00:55+00:00
- **Authors**: Limin Wang, Sheng Guo, Weilin Huang, Yuanjun Xiong, Yu Qiao
- **Comment**: To appear in IEEE Transactions on Image Processing. Code and models
  are available at https://github.com/wanglimin/MRCNN-Scene-Recognition
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have made remarkable progress on scene recognition, partially due to these recent large-scale scene datasets, such as the Places and Places2. Scene categories are often defined by multi-level information, including local objects, global layout, and background environment, thus leading to large intra-class variations. In addition, with the increasing number of scene categories, label ambiguity has become another crucial issue in large-scale classification. This paper focuses on large-scale scene recognition and makes two major contributions to tackle these issues. First, we propose a multi-resolution CNN architecture that captures visual content and structure at multiple levels. The multi-resolution CNNs are composed of coarse resolution CNNs and fine resolution CNNs, which are complementary to each other. Second, we design two knowledge guided disambiguation techniques to deal with the problem of label ambiguity. (i) We exploit the knowledge from the confusion matrix computed on validation data to merge ambiguous classes into a super category. (ii) We utilize the knowledge of extra networks to produce a soft label for each image. Then the super categories or soft labels are employed to guide CNN training on the Places2. We conduct extensive experiments on three large-scale image datasets (ImageNet, Places, and Places2), demonstrating the effectiveness of our approach. Furthermore, our method takes part in two major scene recognition challenges, and achieves the second place at the Places2 challenge in ILSVRC 2015, and the first place at the LSUN challenge in CVPR 2016. Finally, we directly test the learned representations on other scene benchmarks, and obtain the new state-of-the-art results on the MIT Indoor67 (86.7\%) and SUN397 (72.0\%). We release the code and models at~\url{https://github.com/wanglimin/MRCNN-Scene-Recognition}.



### Feature Learning from Spectrograms for Assessment of Personality Traits
- **Arxiv ID**: http://arxiv.org/abs/1610.01223v1
- **DOI**: 10.1109/TAFFC.2017.2763132
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.01223v1)
- **Published**: 2016-10-04 22:38:35+00:00
- **Updated**: 2016-10-04 22:38:35+00:00
- **Authors**: Marc-André Carbonneau, Eric Granger, Yazid Attabi, Ghyslain Gagnon
- **Comment**: 12 pages, 3 figures
- **Journal**: None
- **Summary**: Several methods have recently been proposed to analyze speech and automatically infer the personality of the speaker. These methods often rely on prosodic and other hand crafted speech processing features extracted with off-the-shelf toolboxes. To achieve high accuracy, numerous features are typically extracted using complex and highly parameterized algorithms. In this paper, a new method based on feature learning and spectrogram analysis is proposed to simplify the feature extraction process while maintaining a high level of accuracy. The proposed method learns a dictionary of discriminant features from patches extracted in the spectrogram representations of training speech segments. Each speech segment is then encoded using the dictionary, and the resulting feature set is used to perform classification of personality traits. Experiments indicate that the proposed method achieves state-of-the-art results with a significant reduction in complexity when compared to the most recent reference methods. The number of features, and difficulties linked to the feature extraction process are greatly reduced as only one type of descriptors is used, for which the 6 parameters can be tuned automatically. In contrast, the simplest reference method uses 4 types of descriptors to which 6 functionals are applied, resulting in over 20 parameters to be tuned.



