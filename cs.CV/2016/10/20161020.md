# Arxiv Papers in cs.CV on 2016-10-20
### Adaptive Substring Extraction and Modified Local NBNN Scoring for Binary Feature-based Local Mobile Visual Search without False Positives
- **Arxiv ID**: http://arxiv.org/abs/1610.06266v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.06266v1)
- **Published**: 2016-10-20 02:13:50+00:00
- **Updated**: 2016-10-20 02:13:50+00:00
- **Authors**: Yusuke Uchida, Shigeyuki Sakazawa, Shin'ichi Satoh
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a stand-alone mobile visual search system based on binary features and the bag-of-visual words framework. The contribution of this study is three-fold: (1) We propose an adaptive substring extraction method that adaptively extracts informative bits from the original binary vector and stores them in the inverted index. These substrings are used to refine visual word-based matching. (2) A modified local NBNN scoring method is proposed in the context of image retrieval, which considers the density of binary features in scoring each feature matching. (3) In order to suppress false positives, we introduce a convexity check step that imposes a convexity constraint on the configuration of a transformed reference image. The proposed system improves retrieval accuracy by 11% compared with a conventional method without increasing the database size. Furthermore, our system with the convexity check does not lead to false positive results.



### Retrieving challenging vessel connections in retinal images by line co-occurrence statistics
- **Arxiv ID**: http://arxiv.org/abs/1610.06368v1
- **DOI**: 10.1007/s00422-017-0718-x
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.06368v1)
- **Published**: 2016-10-20 11:31:06+00:00
- **Updated**: 2016-10-20 11:31:06+00:00
- **Authors**: Samaneh Abbasi-Sureshjani, Jiong Zhang, Remco Duits, Bart ter Haar Romeny
- **Comment**: None
- **Journal**: None
- **Summary**: Natural images contain often curvilinear structures, which might be disconnected, or partly occluded. Recovering the missing connection of disconnected structures is an open issue and needs appropriate geometric reasoning. We propose to find line co-occurrence statistics from the centerlines of blood vessels in retinal images and show its remarkable similarity to a well-known probabilistic model for the connectivity pattern in the primary visual cortex. Furthermore, the probabilistic model is trained from the data via statistics and used for automated grouping of interrupted vessels in a spectral clustering based approach. Several challenging image patches are investigated around junction points, where successful results indicate the perfect match of the trained model to the profiles of blood vessels in retinal images. Also, comparisons among several statistical models obtained from different datasets reveals their high similarity i.e., they are independent of the dataset. On top of that, the best approximation of the statistical model with the symmetrized extension of the probabilistic model on the projective line bundle is found with a least square error smaller than 2%. Apparently, the direction process on the projective line bundle is a good continuation model for vessels in retinal images.



### Exploiting inter-image similarity and ensemble of extreme learners for fixation prediction using deep features
- **Arxiv ID**: http://arxiv.org/abs/1610.06449v1
- **DOI**: 10.1016/j.neucom.2017.03.018
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1610.06449v1)
- **Published**: 2016-10-20 14:55:29+00:00
- **Updated**: 2016-10-20 14:55:29+00:00
- **Authors**: Hamed R. -Tavakoli, Ali Borji, Jorma Laaksonen, Esa Rahtu
- **Comment**: None
- **Journal**: Neurocomputing 244 (2017) 10-18
- **Summary**: This paper presents a novel fixation prediction and saliency modeling framework based on inter-image similarities and ensemble of Extreme Learning Machines (ELM). The proposed framework is inspired by two observations, 1) the contextual information of a scene along with low-level visual cues modulates attention, 2) the influence of scene memorability on eye movement patterns caused by the resemblance of a scene to a former visual experience. Motivated by such observations, we develop a framework that estimates the saliency of a given image using an ensemble of extreme learners, each trained on an image similar to the input image. That is, after retrieving a set of similar images for a given image, a saliency predictor is learnt from each of the images in the retrieved image set using an ELM, resulting in an ensemble. The saliency of the given image is then measured in terms of the mean of predicted saliency value by the ensemble's members.



### Change-point Detection Methods for Body-Worn Video
- **Arxiv ID**: http://arxiv.org/abs/1610.06453v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1610.06453v1)
- **Published**: 2016-10-20 15:11:42+00:00
- **Updated**: 2016-10-20 15:11:42+00:00
- **Authors**: Stephanie Allen, David Madras, Ye Ye, Greg Zanotti
- **Comment**: None
- **Journal**: None
- **Summary**: Body-worn video (BWV) cameras are increasingly utilized by police departments to provide a record of police-public interactions. However, large-scale BWV deployment produces terabytes of data per week, necessitating the development of effective computational methods to identify salient changes in video. In work carried out at the 2016 RIPS program at IPAM, UCLA, we present a novel two-stage framework for video change-point detection. First, we employ state-of-the-art machine learning methods including convolutional neural networks and support vector machines for scene classification. We then develop and compare change-point detection algorithms utilizing mean squared-error minimization, forecasting methods, hidden Markov models, and maximum likelihood estimation to identify noteworthy changes. We test our framework on detection of vehicle exits and entrances in a BWV data set provided by the Los Angeles Police Department and achieve over 90% recall and nearly 70% precision -- demonstrating robustness to rapid scene changes, extreme luminance differences, and frequent camera occlusions.



### Efficient Estimation of Compressible State-Space Models with Application to Calcium Signal Deconvolution
- **Arxiv ID**: http://arxiv.org/abs/1610.06461v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.IT, math.DS, math.IT, math.ST, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/1610.06461v1)
- **Published**: 2016-10-20 15:37:53+00:00
- **Updated**: 2016-10-20 15:37:53+00:00
- **Authors**: Abbas Kazemipour, Ji Liu, Patrick Kanold, Min Wu, Behtash Babadi
- **Comment**: 2016 IEEE Global Conference on Signal and Information Processing
  (GlobalSIP), Dec. 7-9, 2016, Washington D.C
- **Journal**: None
- **Summary**: In this paper, we consider linear state-space models with compressible innovations and convergent transition matrices in order to model spatiotemporally sparse transient events. We perform parameter and state estimation using a dynamic compressed sensing framework and develop an efficient solution consisting of two nested Expectation-Maximization (EM) algorithms. Under suitable sparsity assumptions on the innovations, we prove recovery guarantees and derive confidence bounds for the state estimates. We provide simulation studies as well as application to spike deconvolution from calcium imaging data which verify our theoretical results and show significant improvement over existing algorithms.



### ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras
- **Arxiv ID**: http://arxiv.org/abs/1610.06475v2
- **DOI**: 10.1109/TRO.2017.2705103
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1610.06475v2)
- **Published**: 2016-10-20 16:04:31+00:00
- **Updated**: 2017-06-19 04:44:33+00:00
- **Authors**: Raul Mur-Artal, Juan D. Tardos
- **Comment**: Accepted for publication in IEEE Transactions on Robotics
- **Journal**: None
- **Summary**: We present ORB-SLAM2 a complete SLAM system for monocular, stereo and RGB-D cameras, including map reuse, loop closing and relocalization capabilities. The system works in real-time on standard CPUs in a wide variety of environments from small hand-held indoors sequences, to drones flying in industrial environments and cars driving around a city. Our back-end based on bundle adjustment with monocular and stereo observations allows for accurate trajectory estimation with metric scale. Our system includes a lightweight localization mode that leverages visual odometry tracks for unmapped regions and matches to map points that allow for zero-drift localization. The evaluation on 29 popular public sequences shows that our method achieves state-of-the-art accuracy, being in most cases the most accurate SLAM solution. We publish the source code, not only for the benefit of the SLAM community, but with the aim of being an out-of-the-box SLAM solution for researchers in other fields.



### Utilization of Deep Reinforcement Learning for saccadic-based object visual search
- **Arxiv ID**: http://arxiv.org/abs/1610.06492v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1610.06492v1)
- **Published**: 2016-10-20 16:34:08+00:00
- **Updated**: 2016-10-20 16:34:08+00:00
- **Authors**: Tomasz Kornuta, Kamil Rocki
- **Comment**: Paper submitted to special session on Machine Intelligence organized
  during 23rd International AUTOMATION Conference
- **Journal**: None
- **Summary**: The paper focuses on the problem of learning saccades enabling visual object search. The developed system combines reinforcement learning with a neural network for learning to predict the possible outcomes of its actions. We validated the solution in three types of environment consisting of (pseudo)-randomly generated matrices of digits. The experimental verification is followed by the discussion regarding elements required by systems mimicking the fovea movement and possible further research directions.



### An Image Dataset of Text Patches in Everyday Scenes
- **Arxiv ID**: http://arxiv.org/abs/1610.06494v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.06494v1)
- **Published**: 2016-10-20 16:38:42+00:00
- **Updated**: 2016-10-20 16:38:42+00:00
- **Authors**: Ahmed Ibrahim, A. Lynn Abbott, Mohamed E. Hussein
- **Comment**: Accepted in the 12th International Symposium on Visual Computing
  (ISVC'16)
- **Journal**: None
- **Summary**: This paper describes a dataset containing small images of text from everyday scenes. The purpose of the dataset is to support the development of new automated systems that can detect and analyze text. Although much research has been devoted to text detection and recognition in scanned documents, relatively little attention has been given to text detection in other types of images, such as photographs that are posted on social-media sites. This new dataset, known as COCO-Text-Patch, contains approximately 354,000 small images that are each labeled as "text" or "non-text". This dataset particularly addresses the problem of text verification, which is an essential stage in the end-to-end text detection and recognition pipeline. In order to evaluate the utility of this dataset, it has been used to train two deep convolution neural networks to distinguish text from non-text. One network is inspired by the GoogLeNet architecture, and the second one is based on CaffeNet. Accuracy levels of 90.2% and 90.9% were obtained using the two networks, respectively. All of the images, source code, and deep-learning trained models described in this paper will be publicly available



### Proposing Plausible Answers for Open-ended Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1610.06620v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1610.06620v2)
- **Published**: 2016-10-20 22:01:36+00:00
- **Updated**: 2016-10-24 00:12:29+00:00
- **Authors**: Omid Bakhshandeh, Trung Bui, Zhe Lin, Walter Chang
- **Comment**: None
- **Journal**: None
- **Summary**: Answering open-ended questions is an essential capability for any intelligent agent. One of the most interesting recent open-ended question answering challenges is Visual Question Answering (VQA) which attempts to evaluate a system's visual understanding through its answers to natural language questions about images. There exist many approaches to VQA, the majority of which do not exhibit deeper semantic understanding of the candidate answers they produce. We study the importance of generating plausible answers to a given question by introducing the novel task of `Answer Proposal': for a given open-ended question, a system should generate a ranked list of candidate answers informed by the semantics of the question. We experiment with various models including a neural generative model as well as a semantic graph matching one. We provide both intrinsic and extrinsic evaluations for the task of Answer Proposal, showing that our best model learns to propose plausible answers with a high recall and performs competitively with some other solutions to VQA.



### Bit-pragmatic Deep Neural Network Computing
- **Arxiv ID**: http://arxiv.org/abs/1610.06920v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.AR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1610.06920v1)
- **Published**: 2016-10-20 22:16:05+00:00
- **Updated**: 2016-10-20 22:16:05+00:00
- **Authors**: J. Albericio, P. Judd, A. Delmás, S. Sharify, A. Moshovos
- **Comment**: None
- **Journal**: None
- **Summary**: We quantify a source of ineffectual computations when processing the multiplications of the convolutional layers in Deep Neural Networks (DNNs) and propose Pragmatic (PRA), an architecture that exploits it improving performance and energy efficiency. The source of these ineffectual computations is best understood in the context of conventional multipliers which generate internally multiple terms, that is, products of the multiplicand and powers of two, which added together produce the final product [1]. At runtime, many of these terms are zero as they are generated when the multiplicand is combined with the zero-bits of the multiplicator. While conventional bit-parallel multipliers calculate all terms in parallel to reduce individual product latency, PRA calculates only the non-zero terms using a) on-the-fly conversion of the multiplicator representation into an explicit list of powers of two, and b) hybrid bit-parallel multplicand/bit-serial multiplicator processing units. PRA exploits two sources of ineffectual computations: 1) the aforementioned zero product terms which are the result of the lack of explicitness in the multiplicator representation, and 2) the excess in the representation precision used for both multiplicants and multiplicators, e.g., [2]. Measurements demonstrate that for the convolutional layers, a straightforward variant of PRA improves performance by 2.6x over the DaDiaNao (DaDN) accelerator [3] and by 1.4x over STR [4]. Similarly, PRA improves energy efficiency by 28% and 10% on average compared to DaDN and STR. An improved cross lane synchronication scheme boosts performance improvements to 3.1x over DaDN. Finally, Pragmatic benefits persist even with an 8-bit quantized representation [5].



