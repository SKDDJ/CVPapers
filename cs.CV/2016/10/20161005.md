# Arxiv Papers in cs.CV on 2016-10-05
### Find Your Own Way: Weakly-Supervised Segmentation of Path Proposals for Urban Autonomy
- **Arxiv ID**: http://arxiv.org/abs/1610.01238v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1610.01238v3)
- **Published**: 2016-10-05 00:44:49+00:00
- **Updated**: 2017-11-17 16:54:44+00:00
- **Authors**: Dan Barnes, Will Maddern, Ingmar Posner
- **Comment**: International Conference on Robotics and Automation (ICRA), 2017.
  Video summary: http://youtu.be/rbZ8ck_1nZk
- **Journal**: None
- **Summary**: We present a weakly-supervised approach to segmenting proposed drivable paths in images with the goal of autonomous driving in complex urban environments. Using recorded routes from a data collection vehicle, our proposed method generates vast quantities of labelled images containing proposed paths and obstacles without requiring manual annotation, which we then use to train a deep semantic segmentation network. With the trained network we can segment proposed paths and obstacles at run-time using a vehicle equipped with only a monocular camera without relying on explicit modelling of road or lane markings. We evaluate our method on the large-scale KITTI and Oxford RobotCar datasets and demonstrate reliable path proposal and obstacle segmentation in a wide variety of environments under a range of lighting, weather and traffic conditions. We illustrate how the method can generalise to multiple path proposals at intersections and outline plans to incorporate the system into a framework for autonomous urban driving.



### ECAT: Event Capture Annotation Tool
- **Arxiv ID**: http://arxiv.org/abs/1610.01247v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1610.01247v1)
- **Published**: 2016-10-05 01:24:42+00:00
- **Updated**: 2016-10-05 01:24:42+00:00
- **Authors**: Tuan Do, Nikhil Krishnaswamy, James Pustejovsky
- **Comment**: 4 pages, 4 figures, ISA workshop 2015
- **Journal**: None
- **Summary**: This paper introduces the Event Capture Annotation Tool (ECAT), a user-friendly, open-source interface tool for annotating events and their participants in video, capable of extracting the 3D positions and orientations of objects in video captured by Microsoft's Kinect(R) hardware. The modeling language VoxML (Pustejovsky and Krishnaswamy, 2016) underlies ECAT's object, program, and attribute representations, although ECAT uses its own spec for explicit labeling of motion instances. The demonstration will show the tool's workflow and the options available for capturing event-participant relations and browsing visual data. Mapping ECAT's output to VoxML will also be addressed.



### Mobility Map Computations for Autonomous Navigation using an RGBD Sensor
- **Arxiv ID**: http://arxiv.org/abs/1610.01326v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1610.01326v1)
- **Published**: 2016-10-05 09:22:32+00:00
- **Updated**: 2016-10-05 09:22:32+00:00
- **Authors**: Nicol√≤ Genesio, Tariq Abuhashim, Fabio Solari, Manuela Chessa, Lorenzo Natale
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, the numbers of life-size humanoids as well as their mobile capabilities have steadily grown. Stable walking motion and control for humanoid robots are active fields of research. In this scenario an open question is how to model and analyse the scene so that a motion planning algorithm can generate an appropriate walking pattern. This paper presents the current work towards scene modelling and understanding, using an RGBD sensor. The main objective is to provide the humanoid robot iCub with capabilities to navigate safely and interact with various parts of the environment. In this sense we address the problem of traversability analysis of the scene, focusing on classification of point clouds as a function of mobility, and hence walking safety.



### Domain Adaptation with Soft-margin multiple feature-kernel learning beats Deep Learning for surveillance face recognition
- **Arxiv ID**: http://arxiv.org/abs/1610.01374v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1610.01374v2)
- **Published**: 2016-10-05 11:48:56+00:00
- **Updated**: 2016-10-27 13:14:49+00:00
- **Authors**: Samik Banerjee, Sukhendu Das
- **Comment**: This is an extended version of the paper accepted in CVPR Biometric
  Workshop, 2016. arXiv admin note: text overlap with arXiv:1610.00660
- **Journal**: None
- **Summary**: Face recognition (FR) is the most preferred mode for biometric-based surveillance, due to its passive nature of detecting subjects, amongst all different types of biometric traits. FR under surveillance scenario does not give satisfactory performance due to low contrast, noise and poor illumination conditions on probes, as compared to the training samples. A state-of-the-art technology, Deep Learning, even fails to perform well in these scenarios. We propose a novel soft-margin based learning method for multiple feature-kernel combinations, followed by feature transformed using Domain Adaptation, which outperforms many recent state-of-the-art techniques, when tested using three real-world surveillance face datasets.



### Recognizing and Presenting the Storytelling Video Structure with Deep Multimodal Networks
- **Arxiv ID**: http://arxiv.org/abs/1610.01376v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.01376v2)
- **Published**: 2016-10-05 11:55:33+00:00
- **Updated**: 2016-11-10 14:09:38+00:00
- **Authors**: Lorenzo Baraldi, Costantino Grana, Rita Cucchiara
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel approach for temporal and semantic segmentation of edited videos into meaningful segments, from the point of view of the storytelling structure. The objective is to decompose a long video into more manageable sequences, which can in turn be used to retrieve the most significant parts of it given a textual query and to provide an effective summarization. Previous video decomposition methods mainly employed perceptual cues, tackling the problem either as a story change detection, or as a similarity grouping task, and the lack of semantics limited their ability to identify story boundaries. Our proposal connects together perceptual, audio and semantic cues in a specialized deep network architecture designed with a combination of CNNs which generate an appropriate embedding, and clusters shots into connected sequences of semantic scenes, i.e. stories. A retrieval presentation strategy is also proposed, by selecting the semantically and aesthetically "most valuable" thumbnails to present, considering the query in order to improve the storytelling presentation. Finally, the subjective nature of the task is considered, by conducting experiments with different annotators and by proposing an algorithm to maximize the agreement between automatic results and human annotators.



### Reliability of PET/CT shape and heterogeneity features in functional and morphological components of Non-Small Cell Lung Cancer tumors: a repeatability analysis in a prospective multi-center cohort
- **Arxiv ID**: http://arxiv.org/abs/1610.01390v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1610.01390v1)
- **Published**: 2016-10-05 12:51:09+00:00
- **Updated**: 2016-10-05 12:51:09+00:00
- **Authors**: Marie-Charlotte Desseroit, Florent Tixier, Wolfgang Weber, Barry A Siegel, Catherine Cheze Le Rest, Dimitris Visvikis, Mathieu Hatt
- **Comment**: Journal of Nuclear Medicine, Society of Nuclear Medicine, 2016
- **Journal**: None
- **Summary**: Purpose: The main purpose of this study was to assess the reliability of shape and heterogeneity features in both Positron Emission Tomography (PET) and low-dose Computed Tomography (CT) components of PET/CT. A secondary objective was to investigate the impact of image quantization.Material and methods: A Health Insurance Portability and Accountability Act -compliant secondary analysis of deidentified prospectively acquired PET/CT test-retest datasets of 74 patients from multi-center Merck and ACRIN trials was performed. Metabolically active volumes were automatically delineated on PET with Fuzzy Locally Adaptive Bayesian algorithm. 3DSlicerTM was used to semi-automatically delineate the anatomical volumes on low-dose CT components. Two quantization methods were considered: a quantization into a set number of bins (quantizationB) and an alternative quantization with bins of fixed width (quantizationW). Four shape descriptors, ten first-order metrics and 26 textural features were computed. Bland-Altman analysis was used to quantify repeatability. Features were subsequently categorized as very reliable, reliable, moderately reliable and poorly reliable with respect to the corresponding volume variability. Results: Repeatability was highly variable amongst features. Numerous metrics were identified as poorly or moderately reliable. Others were (very) reliable in both modalities, and in all categories (shape, 1st-, 2nd- and 3rd-order metrics). Image quantization played a major role in the features repeatability. Features were more reliable in PET with quantizationB, whereas quantizationW showed better results in CT.Conclusion: The test-retest repeatability of shape and heterogeneity features in PET and low-dose CT varied greatly amongst metrics. The level of repeatability also depended strongly on the quantization step, with different optimal choices for each modality. The repeatability of PET and low-dose CT features should be carefully taken into account when selecting metrics to build multiparametric models.



### Learning Optimal Parameters for Multi-target Tracking with Contextual Interactions
- **Arxiv ID**: http://arxiv.org/abs/1610.01394v1
- **DOI**: 10.1007/s11263-016-0960-z
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.01394v1)
- **Published**: 2016-10-05 13:04:48+00:00
- **Updated**: 2016-10-05 13:04:48+00:00
- **Authors**: Shaofei Wang, Charless C. Fowlkes
- **Comment**: arXiv admin note: text overlap with arXiv:1412.2066
- **Journal**: None
- **Summary**: We describe an end-to-end framework for learning parameters of min-cost flow multi-target tracking problem with quadratic trajectory interactions including suppression of overlapping tracks and contextual cues about cooccurrence of different objects. Our approach utilizes structured prediction with a tracking-specific loss function to learn the complete set of model parameters. In this learning framework, we evaluate two different approaches to finding an optimal set of tracks under a quadratic model objective, one based on an LP relaxation and the other based on novel greedy variants of dynamic programming that handle pairwise interactions. We find the greedy algorithms achieve almost equivalent accuracy to the LP relaxation while being up to 10x faster than a commercial LP solver. We evaluate trained models on three challenging benchmarks. Surprisingly, we find that with proper parameter learning, our simple data association model without explicit appearance/motion reasoning is able to achieve comparable or better accuracy than many state-of-the-art methods that use far more complex motion features or appearance affinity metric learning.



### Convex Histogram-Based Joint Image Segmentation with Regularized Optimal Transport Cost
- **Arxiv ID**: http://arxiv.org/abs/1610.01400v1
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1610.01400v1)
- **Published**: 2016-10-05 13:13:21+00:00
- **Updated**: 2016-10-05 13:13:21+00:00
- **Authors**: Nicolas Papadakis, Julien Rabin
- **Comment**: Technical report
- **Journal**: None
- **Summary**: We investigate in this work a versatile convex framework for multiple image segmentation, relying on the regularized optimal mass transport theory. In this setting, several transport cost functions are considered and used to match statistical distributions of features. In practice, global multidimensional histograms are estimated from the segmented image regions, and are compared to referring models that are either fixed histograms given a priori, or directly inferred in the non-supervised case. The different convex problems studied are solved efficiently using primal-dual algorithms. The proposed approach is generic and enables multi-phase segmentation as well as co-segmentation of multiple images.



### Markov Chain Modeling and Simulation of Breathing Patterns
- **Arxiv ID**: http://arxiv.org/abs/1610.01444v1
- **DOI**: 10.1016/j.bspc.2016.12.002
- **Categories**: **stat.AP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1610.01444v1)
- **Published**: 2016-10-05 14:32:21+00:00
- **Updated**: 2016-10-05 14:32:21+00:00
- **Authors**: Davide Alinovi, Gianluigi Ferrari, Francesco Pisani, Riccardo Raheli
- **Comment**: submitted for publication; 19 pages, 9 figures, 4 tables
- **Journal**: None
- **Summary**: The lack of large video databases obtained from real patients with respiratory disorders makes the design and optimization of video-based monitoring systems quite critical. The purpose of this study is the development of suitable models and simulators of breathing behaviors and disorders, such as respiratory pauses and apneas, in order to allow efficient design and test of video-based monitoring systems. More precisely, a novel Continuous-Time Markov Chain (CTMC) statistical model of breathing patterns is presented. The Respiratory Rate (RR) pattern, estimated by measured vital signs of hospital-monitored patients, is approximated as a CTMC, whose states and parameters are selected through an appropriate statistical analysis. Then, two simulators, software- and hardware-based, are proposed. After validation of the CTMC model, the proposed simulators are tested with previously developed video-based algorithms for the estimation of the RR and the detection of apnea events. Examples of application to assess the performance of systems for video-based RR estimation and apnea detection are presented. The results, in terms of Kullback-Leibler divergence, show that realistic breathing patterns, including specific respiratory disorders, can be accurately described by the proposed model; moreover, the simulators are able to reproduce practical breathing patterns for video analysis. The presented CTMC statistical model can be strategic to describe realistic breathing patterns and devise simulators useful to develop and test novel and effective video processing-based monitoring systems.



### Compressive Imaging with Iterative Forward Models
- **Arxiv ID**: http://arxiv.org/abs/1610.01852v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1610.01852v1)
- **Published**: 2016-10-05 14:44:24+00:00
- **Updated**: 2016-10-05 14:44:24+00:00
- **Authors**: Hsiou-Yuan Liu, Ulugbek S. Kamilov, Dehong Liu, Hassan Mansour, Petros T. Boufounos
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new compressive imaging method for reconstructing 2D or 3D objects from their scattered wave-field measurements. Our method relies on a novel, nonlinear measurement model that can account for the multiple scattering phenomenon, which makes the method preferable in applications where linear measurement models are inaccurate. We construct the measurement model by expanding the scattered wave-field with an accelerated-gradient method, which is guaranteed to converge and is suitable for large-scale problems. We provide explicit formulas for computing the gradient of our measurement model with respect to the unknown image, which enables image formation with a sparsity- driven numerical optimization algorithm. We validate the method both analytically and with numerical simulations.



### Visual Question Answering: Datasets, Algorithms, and Future Challenges
- **Arxiv ID**: http://arxiv.org/abs/1610.01465v4
- **DOI**: 10.1016/j.cviu.2017.06.005
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1610.01465v4)
- **Published**: 2016-10-05 14:58:36+00:00
- **Updated**: 2017-06-15 01:52:59+00:00
- **Authors**: Kushal Kafle, Christopher Kanan
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) is a recent problem in computer vision and natural language processing that has garnered a large amount of interest from the deep learning, computer vision, and natural language processing communities. In VQA, an algorithm needs to answer text-based questions about images. Since the release of the first VQA dataset in 2014, additional datasets have been released and many algorithms have been proposed. In this review, we critically examine the current state of VQA in terms of problem formulation, existing datasets, evaluation metrics, and algorithms. In particular, we discuss the limitations of current datasets with regard to their ability to properly train and assess VQA algorithms. We then exhaustively review existing algorithms for VQA. Finally, we discuss possible future directions for VQA and image understanding research.



### DeepGaze II: Reading fixations from deep features trained on object recognition
- **Arxiv ID**: http://arxiv.org/abs/1610.01563v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1610.01563v1)
- **Published**: 2016-10-05 18:47:28+00:00
- **Updated**: 2016-10-05 18:47:28+00:00
- **Authors**: Matthias K√ºmmerer, Thomas S. A. Wallis, Matthias Bethge
- **Comment**: None
- **Journal**: None
- **Summary**: Here we present DeepGaze II, a model that predicts where people look in images. The model uses the features from the VGG-19 deep neural network trained to identify objects in images. Contrary to other saliency models that use deep features, here we use the VGG features for saliency prediction with no additional fine-tuning (rather, a few readout layers are trained on top of the VGG features to predict saliency). The model is therefore a strong test of transfer learning. After conservative cross-validation, DeepGaze II explains about 87% of the explainable information gain in the patterns of fixations and achieves top performance in area under the curve metrics on the MIT300 hold-out benchmark. These results corroborate the finding from DeepGaze I (which explained 56% of the explainable information gain), that deep features trained on object recognition provide a versatile feature space for performing related visual tasks. We explore the factors that contribute to this success and present several informative image examples. A web service is available to compute model predictions at http://deepgaze.bethgelab.org.



### A new algorithm for identity verification based on the analysis of a handwritten dynamic signature
- **Arxiv ID**: http://arxiv.org/abs/1610.01578v1
- **DOI**: 10.1016/j.asoc.2016.02.017
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1610.01578v1)
- **Published**: 2016-10-05 19:32:55+00:00
- **Updated**: 2016-10-05 19:32:55+00:00
- **Authors**: Krzysztof Cpalka, Marcin Zalasinski, Leszek Rutkowski
- **Comment**: 34 pages, 7 figures
- **Journal**: Applied Soft Computing, vol. 43, pp. 47-56, 2016
- **Summary**: Identity verification based on authenticity assessment of a handwritten signature is an important issue in biometrics. There are many effective methods for signature verification taking into account dynamics of a signing process. Methods based on partitioning take a very important place among them. In this paper we propose a new approach to signature partitioning. Its most important feature is the possibility of selecting and processing of hybrid partitions in order to increase a precision of the test signature analysis. Partitions are formed by a combination of vertical and horizontal sections of the signature. Vertical sections correspond to the initial, middle, and final time moments of the signing process. In turn, horizontal sections correspond to the signature areas associated with high and low pen velocity and high and low pen pressure on the surface of a graphics tablet. Our previous research on vertical and horizontal sections of the dynamic signature (created independently) led us to develop the algorithm presented in this paper. Selection of sections, among others, allows us to define the stability of the signing process in the partitions, promoting signature areas of greater stability (and vice versa). In the test of the proposed method two databases were used: public MCYT-100 and paid BioSecure.



### Supervision via Competition: Robot Adversaries for Learning Tasks
- **Arxiv ID**: http://arxiv.org/abs/1610.01685v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1610.01685v1)
- **Published**: 2016-10-05 23:28:12+00:00
- **Updated**: 2016-10-05 23:28:12+00:00
- **Authors**: Lerrel Pinto, James Davidson, Abhinav Gupta
- **Comment**: Submission to ICRA 2017
- **Journal**: None
- **Summary**: There has been a recent paradigm shift in robotics to data-driven learning for planning and control. Due to large number of experiences required for training, most of these approaches use a self-supervised paradigm: using sensors to measure success/failure. However, in most cases, these sensors provide weak supervision at best. In this work, we propose an adversarial learning framework that pits an adversary against the robot learning the task. In an effort to defeat the adversary, the original robot learns to perform the task with more robustness leading to overall improved performance. We show that this adversarial framework forces the the robot to learn a better grasping model in order to overcome the adversary. By grasping 82% of presented novel objects compared to 68% without an adversary, we demonstrate the utility of creating adversaries. We also demonstrate via experiments that having robots in adversarial setting might be a better learning strategy as compared to having collaborative multiple robots.



