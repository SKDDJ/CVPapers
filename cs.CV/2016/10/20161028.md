# Arxiv Papers in cs.CV on 2016-10-28
### Recent advances in content based video copy detection
- **Arxiv ID**: http://arxiv.org/abs/1610.09087v1
- **DOI**: 10.1109/PERVASIVE.2015.7087093
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.09087v1)
- **Published**: 2016-10-28 06:21:06+00:00
- **Updated**: 2016-10-28 06:21:06+00:00
- **Authors**: Sanket Shinde, Girija Chiddarwar
- **Comment**: None
- **Journal**: None
- **Summary**: With the immense number of videos being uploaded to the video sharing sites, issue of copyright infringement arises with uploading of illicit copies or transformed versions of original video. Thus safeguarding copyright of digital media has become matter of concern. To address this concern, it is obliged to have a video copy detection system which is sufficiently robust to detect these transformed videos with ability to pinpoint location of copied segments. This paper outlines recent advancement in content based video copy detection, mainly focusing on different visual features employed by video copy detection systems. Finally we evaluate performance of existing video copy detection systems.



### Towards automatic pulmonary nodule management in lung cancer screening with deep learning
- **Arxiv ID**: http://arxiv.org/abs/1610.09157v2
- **DOI**: 10.1038/srep46479
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.09157v2)
- **Published**: 2016-10-28 10:25:11+00:00
- **Updated**: 2017-05-23 12:53:49+00:00
- **Authors**: Francesco Ciompi, Kaman Chung, Sarah J. van Riel, Arnaud Arindra Adiyoso Setio, Paul K. Gerke, Colin Jacobs, Ernst Th. Scholten, Cornelia Schaefer-Prokop, Mathilde M. W. Wille, Alfonso Marchiano, Ugo Pastorino, Mathias Prokop, Bram van Ginneken
- **Comment**: Published on Scientific Reports
- **Journal**: Sci. Rep. 7, 46479; (2017)
- **Summary**: The introduction of lung cancer screening programs will produce an unprecedented amount of chest CT scans in the near future, which radiologists will have to read in order to decide on a patient follow-up strategy. According to the current guidelines, the workup of screen-detected nodules strongly relies on nodule size and nodule type. In this paper, we present a deep learning system based on multi-stream multi-scale convolutional networks, which automatically classifies all nodule types relevant for nodule workup. The system processes raw CT data containing a nodule without the need for any additional information such as nodule segmentation or nodule size and learns a representation of 3D data by analyzing an arbitrary number of 2D views of a given nodule. The deep learning system was trained with data from the Italian MILD screening trial and validated on an independent set of data from the Danish DLCST screening trial. We analyze the advantage of processing nodules at multiple scales with a multi-stream convolutional network architecture, and we show that the proposed deep learning system achieves performance at classifying nodule type that surpasses the one of classical machine learning approaches and is within the inter-observer variability among four experienced human observers.



### Judging a Book By its Cover
- **Arxiv ID**: http://arxiv.org/abs/1610.09204v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.09204v3)
- **Published**: 2016-10-28 13:26:55+00:00
- **Updated**: 2017-10-13 03:26:08+00:00
- **Authors**: Brian Kenji Iwana, Syed Tahseen Raza Rizvi, Sheraz Ahmed, Andreas Dengel, Seiichi Uchida
- **Comment**: 6 pages, 9 figures
- **Journal**: None
- **Summary**: Book covers communicate information to potential readers, but can that same information be learned by computers? We propose using a deep Convolutional Neural Network (CNN) to predict the genre of a book based on the visual clues provided by its cover. The purpose of this research is to investigate whether relationships between books and their covers can be learned. However, determining the genre of a book is a difficult task because covers can be ambiguous and genres can be overarching. Despite this, we show that a CNN can extract features and learn underlying design rules set by the designer to define a genre. Using machine learning, we can bring the large amount of resources available to the book cover design process. In addition, we present a new challenging dataset that can be used for many pattern recognition tasks.



### Learnable Visual Markers
- **Arxiv ID**: http://arxiv.org/abs/1610.09237v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.09237v1)
- **Published**: 2016-10-28 14:31:02+00:00
- **Updated**: 2016-10-28 14:31:02+00:00
- **Authors**: Oleg Grinchuk, Vadim Lebedev, Victor Lempitsky
- **Comment**: NIPS 2016
- **Journal**: None
- **Summary**: We propose a new approach to designing visual markers (analogous to QR-codes, markers for augmented reality, and robotic fiducial tags) based on the advances in deep generative networks. In our approach, the markers are obtained as color images synthesized by a deep network from input bit strings, whereas another deep network is trained to recover the bit strings back from the photos of these markers. The two networks are trained simultaneously in a joint backpropagation process that takes characteristic photometric and geometric distortions associated with marker fabrication and marker scanning into account. Additionally, a stylization loss based on statistics of activations in a pretrained classification network can be inserted into the learning in order to shift the marker appearance towards some texture prototype. In the experiments, we demonstrate that the markers obtained using our approach are capable of retaining bit strings that are long enough to be practical. The ability to automatically adapt markers according to the usage scenario and the desired capacity as well as the ability to combine information encoding with artistic stylization are the unique properties of our approach. As a byproduct, our approach provides an insight on the structure of patterns that are most suitable for recognition by ConvNets and on their ability to distinguish composite patterns.



### The TUM LapChole dataset for the M2CAI 2016 workflow challenge
- **Arxiv ID**: http://arxiv.org/abs/1610.09278v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.09278v2)
- **Published**: 2016-10-28 15:36:58+00:00
- **Updated**: 2017-08-31 14:27:37+00:00
- **Authors**: Ralf Stauder, Daniel Ostler, Michael Kranzfelder, Sebastian Koller, Hubertus Feu√üner, Nassir Navab
- **Comment**: 5 pages, 2 figures, preliminary reference for published dataset
  (until larger comparison study of workshop organizers is published)
- **Journal**: None
- **Summary**: In this technical report we present our collected dataset of laparoscopic cholecystectomies (LapChole). Laparoscopic videos of a total of 20 surgeries were recorded and annotated with surgical phase labels, of which 15 were randomly pre-determined as training data, while the remaining 5 videos are selected as test data. This dataset was later included as part of the M2CAI 2016 workflow detection challenge during MICCAI 2016 in Athens.



### Real-time Online Action Detection Forests using Spatio-temporal Contexts
- **Arxiv ID**: http://arxiv.org/abs/1610.09334v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.09334v1)
- **Published**: 2016-10-28 18:15:31+00:00
- **Updated**: 2016-10-28 18:15:31+00:00
- **Authors**: Seungryul Baek, Kwang In Kim, Tae-Kyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Online action detection (OAD) is challenging since 1) robust yet computationally expensive features cannot be straightforwardly used due to the real-time processing requirements and 2) the localization and classification of actions have to be performed even before they are fully observed. We propose a new random forest (RF)-based online action detection framework that addresses these challenges. Our algorithm uses computationally efficient skeletal joint features. High accuracy is achieved by using robust convolutional neural network (CNN)-based features which are extracted from the raw RGBD images, plus the temporal relationships between the current frame of interest, and the past and future frames. While these high-quality features are not available in real-time testing scenario, we demonstrate that they can be effectively exploited in training RF classifiers: We use these spatio-temporal contexts to craft RF's new split functions improving RFs' leaf node statistics. Experiments with challenging MSRAction3D, G3D, and OAD datasets demonstrate that our algorithm significantly improves the accuracy over the state-of-the-art online action detection algorithms while achieving the real-time efficiency of existing skeleton-based RF classifiers.



### Detecting Breast Cancer using a Compressive Sensing Unmixing Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1610.09386v1
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1610.09386v1)
- **Published**: 2016-10-28 20:01:55+00:00
- **Updated**: 2016-10-28 20:01:55+00:00
- **Authors**: Richard Obermeier, Jose Angel Martinez-Lorenzo
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional breast cancer imaging methods using microwave Nearfield Radar Imaging (NRI) seek to recover the complex permittivity of the tissues at each voxel in the imaging region. This approach is suboptimal, in that it does not directly consider the permittivity values that healthy and cancerous breast tissues typically have. In this paper, we describe a novel unmixing algorithm for detecting breast cancer. In this approach, the breast tissue is separated into three components, low water content (LWC), high water content (HWC), and cancerous tissues, and the goal of the optimization procedure is to recover the mixture proportions for each component. By utilizing this approach in a hybrid DBT / NRI system, the unmixing reconstruction process can be posed as a sparse recovery problem, such that compressive sensing (CS) techniques can be employed. A numerical analysis is performed, which demonstrates that cancerous lesions can be detected from their mixture proportion under the appropriate conditions.



### Learning Adaptive Parameter Tuning for Image Processing
- **Arxiv ID**: http://arxiv.org/abs/1610.09414v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.09414v2)
- **Published**: 2016-10-28 21:56:52+00:00
- **Updated**: 2017-12-27 21:18:08+00:00
- **Authors**: Jingming Dong, Iuri Frosio, Jan Kautz
- **Comment**: None
- **Journal**: Jinming Dong, Iuri Frosio, Jan Kautz, Learning Adaptive Parameter
  Tuning for Image Processing, Proc. EI 2018, Image Processing: Algorithms and
  Systems XVI, Burlingame, USA, 28 Jan - 2 Feb 2018
- **Summary**: The non-stationary nature of image characteristics calls for adaptive processing, based on the local image content. We propose a simple and flexible method to learn local tuning of parameters in adaptive image processing: we extract simple local features from an image and learn the relation between these features and the optimal filtering parameters. Learning is performed by optimizing a user defined cost function (any image quality metric) on a training set. We apply our method to three classical problems (denoising, demosaicing and deblurring) and we show the effectiveness of the learned parameter modulation strategies. We also show that these strategies are consistent with theoretical results from the literature.



