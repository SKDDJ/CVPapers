# Arxiv Papers in cs.CV on 2016-01-05
### Weakly-supervised Disentangling with Recurrent Transformations for 3D View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1601.00706v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1601.00706v1)
- **Published**: 2016-01-05 00:08:09+00:00
- **Updated**: 2016-01-05 00:08:09+00:00
- **Authors**: Jimei Yang, Scott Reed, Ming-Hsuan Yang, Honglak Lee
- **Comment**: This was published in NIPS 2015 conference
- **Journal**: None
- **Summary**: An important problem for both graphics and vision is to synthesize novel views of a 3D object from a single image. This is particularly challenging due to the partial observability inherent in projecting a 3D object onto the image space, and the ill-posedness of inferring object shape and pose. However, we can train a neural network to address the problem if we restrict our attention to specific object categories (in our case faces and chairs) for which we can gather ample training data. In this paper, we propose a novel recurrent convolutional encoder-decoder network that is trained end-to-end on the task of rendering rotated objects starting from a single image. The recurrent structure allows our model to capture long-term dependencies along a sequence of transformations. We demonstrate the quality of its predictions for human faces on the Multi-PIE dataset and for a dataset of 3D chair models, and also show its ability to disentangle latent factors of variation (e.g., identity and pose) without using full supervision.



### Matrix Variate RBM and Its Applications
- **Arxiv ID**: http://arxiv.org/abs/1601.00722v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.00722v1)
- **Published**: 2016-01-05 02:55:17+00:00
- **Updated**: 2016-01-05 02:55:17+00:00
- **Authors**: Guanglei Qi, Yanfeng Sun, Junbin Gao, Yongli Hu, Jinghua Li
- **Comment**: None
- **Journal**: None
- **Summary**: Restricted Boltzmann Machine (RBM) is an importan- t generative model modeling vectorial data. While applying an RBM in practice to images, the data have to be vec- torized. This results in high-dimensional data and valu- able spatial information has got lost in vectorization. In this paper, a Matrix-Variate Restricted Boltzmann Machine (MVRBM) model is proposed by generalizing the classic RBM to explicitly model matrix data. In the new RBM model, both input and hidden variables are in matrix forms which are connected by bilinear transforms. The MVRBM has much less model parameters, resulting in a faster train- ing algorithm while retaining comparable performance as the classic RBM. The advantages of the MVRBM have been demonstrated on two real-world applications: Image super- resolution and handwritten digit recognition.



### Low-Rank Representation over the Manifold of Curves
- **Arxiv ID**: http://arxiv.org/abs/1601.00732v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1601.00732v2)
- **Published**: 2016-01-05 04:21:45+00:00
- **Updated**: 2016-01-06 09:50:20+00:00
- **Authors**: Stephen Tierney, Junbin Gao, Yi Guo, Zhengwu Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In machine learning it is common to interpret each data point as a vector in Euclidean space. However the data may actually be functional i.e.\ each data point is a function of some variable such as time and the function is discretely sampled. The naive treatment of functional data as traditional multivariate data can lead to poor performance since the algorithms are ignoring the correlation in the curvature of each function. In this paper we propose a method to analyse subspace structure of the functional data by using the state of the art Low-Rank Representation (LRR). Experimental evaluation on synthetic and real data reveals that this method massively outperforms conventional LRR in tasks concerning functional data.



### Brain4Cars: Car That Knows Before You Do via Sensory-Fusion Deep Learning Architecture
- **Arxiv ID**: http://arxiv.org/abs/1601.00740v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1601.00740v1)
- **Published**: 2016-01-05 05:25:14+00:00
- **Updated**: 2016-01-05 05:25:14+00:00
- **Authors**: Ashesh Jain, Hema S Koppula, Shane Soh, Bharad Raghavan, Avi Singh, Ashutosh Saxena
- **Comment**: Journal Version (ICCV and ICRA combination with more system details)
  http://brain4cars.com
- **Journal**: None
- **Summary**: Advanced Driver Assistance Systems (ADAS) have made driving safer over the last decade. They prepare vehicles for unsafe road conditions and alert drivers if they perform a dangerous maneuver. However, many accidents are unavoidable because by the time drivers are alerted, it is already too late. Anticipating maneuvers beforehand can alert drivers before they perform the maneuver and also give ADAS more time to avoid or prepare for the danger.   In this work we propose a vehicular sensor-rich platform and learning algorithms for maneuver anticipation. For this purpose we equip a car with cameras, Global Positioning System (GPS), and a computing device to capture the driving context from both inside and outside of the car. In order to anticipate maneuvers, we propose a sensory-fusion deep learning architecture which jointly learns to anticipate and fuse multiple sensory streams. Our architecture consists of Recurrent Neural Networks (RNNs) that use Long Short-Term Memory (LSTM) units to capture long temporal dependencies. We propose a novel training procedure which allows the network to predict the future given only a partial temporal context. We introduce a diverse data set with 1180 miles of natural freeway and city driving, and show that we can anticipate maneuvers 3.5 seconds before they occur in real-time with a precision and recall of 90.5\% and 87.4\% respectively.



### Robust Method of Vote Aggregation and Proposition Verification for Invariant Local Features
- **Arxiv ID**: http://arxiv.org/abs/1601.00781v1
- **DOI**: 10.5220/0005267002520259
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.00781v1)
- **Published**: 2016-01-05 10:18:13+00:00
- **Updated**: 2016-01-05 10:18:13+00:00
- **Authors**: Grzegorz Kurzejamski, Jacek Zawistowski, Grzegorz Sarwas
- **Comment**: 8 pages Short Paper, presented at VISAPP 2015 Conference in Berlin,
  March. Proceedings of the 10th International Conference on Computer Vision
  Theory and Applications, 252-259, 2015, Berlin, Germany, ISBN
  978-989-758-090-1
- **Journal**: None
- **Summary**: This paper presents a method for analysis of the vote space created from the local features extraction process in a multi-detection system. The method is opposed to the classic clustering approach and gives a high level of control over the clusters composition for further verification steps. Proposed method comprises of the graphical vote space presentation, the proposition generation, the two-pass iterative vote aggregation and the cascade filters for verification of the propositions. Cascade filters contain all of the minor algorithms needed for effective object detection verification. The new approach does not have the drawbacks of the classic clustering approaches and gives a substantial control over process of detection. Method exhibits an exceptionally high detection rate in conjunction with a low false detection chance in comparison to alternative methods.



### Gamifying Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1601.00825v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.00825v1)
- **Published**: 2016-01-05 13:48:05+00:00
- **Updated**: 2016-01-05 13:48:05+00:00
- **Authors**: Simone Palazzo, Concetto Spampinato, Daniela Giordano
- **Comment**: Submitted to PAMI
- **Journal**: None
- **Summary**: Video object segmentation can be considered as one of the most challenging computer vision problems. Indeed, so far, no existing solution is able to effectively deal with the peculiarities of real-world videos, especially in cases of articulated motion and object occlusions; limitations that appear more evident when we compare their performance with the human one. However, manually segmenting objects in videos is largely impractical as it requires a lot of human time and concentration. To address this problem, in this paper we propose an interactive video object segmentation method, which exploits, on one hand, the capability of humans to identify correctly objects in visual scenes, and on the other hand, the collective human brainpower to solve challenging tasks. In particular, our method relies on a web game to collect human inputs on object locations, followed by an accurate segmentation phase achieved by optimizing an energy function encoding spatial and temporal constraints between object regions as well as human-provided input. Performance analysis carried out on challenging video datasets with some users playing the game demonstrated that our method shows a better trade-off between annotation times and segmentation accuracy than interactive video annotation and automated video object segmentation approaches.



### Crater Detection via Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1601.00978v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.00978v1)
- **Published**: 2016-01-05 21:03:59+00:00
- **Updated**: 2016-01-05 21:03:59+00:00
- **Authors**: Joseph Paul Cohen, Henry Z. Lo, Tingting Lu, Wei Ding
- **Comment**: 2 Pages. Submitted to 47th Lunar and Planetary Science Conference
  (LPSC 2016)
- **Journal**: None
- **Summary**: Craters are among the most studied geomorphic features in the Solar System because they yield important information about the past and present geological processes and provide information about the relative ages of observed geologic formations. We present a method for automatic crater detection using advanced machine learning to deal with the large amount of satellite imagery collected. The challenge of automatically detecting craters comes from their is complex surface because their shape erodes over time to blend into the surface. Bandeira provided a seminal dataset that embodied this challenge that is still an unsolved pattern recognition problem to this day. There has been work to solve this challenge based on extracting shape and contrast features and then applying classification models on those features. The limiting factor in this existing work is the use of hand crafted filters on the image such as Gabor or Sobel filters or Haar features. These hand crafted methods rely on domain knowledge to construct. We would like to learn the optimal filters and features based on training examples. In order to dynamically learn filters and features we look to Convolutional Neural Networks (CNNs) which have shown their dominance in computer vision. The power of CNNs is that they can learn image filters which generate features for high accuracy classification.



### Forecasting Social Navigation in Crowded Complex Scenes
- **Arxiv ID**: http://arxiv.org/abs/1601.00998v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1601.00998v1)
- **Published**: 2016-01-05 22:10:15+00:00
- **Updated**: 2016-01-05 22:10:15+00:00
- **Authors**: Alexandre Robicquet, Alexandre Alahi, Amir Sadeghian, Bryan Anenberg, John Doherty, Eli Wu, Silvio Savarese
- **Comment**: None
- **Journal**: None
- **Summary**: When humans navigate a crowed space such as a university campus or the sidewalks of a busy street, they follow common sense rules based on social etiquette. In this paper, we argue that in order to enable the design of new algorithms that can take fully advantage of these rules to better solve tasks such as target tracking or trajectory forecasting, we need to have access to better data in the first place. To that end, we contribute the very first large scale dataset (to the best of our knowledge) that collects images and videos of various types of targets (not just pedestrians, but also bikers, skateboarders, cars, buses, golf carts) that navigate in a real-world outdoor environment such as a university campus. We present an extensive evaluation where different methods for trajectory forecasting are evaluated and compared. Moreover, we present a new algorithm for trajectory prediction that exploits the complexity of our new dataset and allows to: i) incorporate inter-class interactions into trajectory prediction models (e.g, pedestrian vs bike) as opposed to just intra-class interactions (e.g., pedestrian vs pedestrian); ii) model the degree to which the social forces are regulating an interaction. We call the latter "social sensitivity"and it captures the sensitivity to which a target is responding to a certain interaction. An extensive experimental evaluation demonstrates the effectiveness of our novel approach.



### Space-Time Representation of People Based on 3D Skeletal Data: A Review
- **Arxiv ID**: http://arxiv.org/abs/1601.01006v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.01006v3)
- **Published**: 2016-01-05 22:38:36+00:00
- **Updated**: 2017-02-04 01:08:55+00:00
- **Authors**: Fei Han, Brian Reily, William Hoff, Hao Zhang
- **Comment**: Our paper has been accepted by the journal Computer Vision and Image
  Understanding, see
  http://www.sciencedirect.com/science/article/pii/S1077314217300279, Computer
  Vision and Image Understanding, 2017
- **Journal**: None
- **Summary**: Spatiotemporal human representation based on 3D visual perception data is a rapidly growing research area. Based on the information sources, these representations can be broadly categorized into two groups based on RGB-D information or 3D skeleton data. Recently, skeleton-based human representations have been intensively studied and kept attracting an increasing attention, due to their robustness to variations of viewpoint, human body scale and motion speed as well as the realtime, online performance. This paper presents a comprehensive survey of existing space-time representations of people based on 3D skeletal data, and provides an informative categorization and analysis of these methods from the perspectives, including information modality, representation encoding, structure and transition, and feature engineering. We also provide a brief overview of skeleton acquisition devices and construction methods, enlist a number of public benchmark datasets with skeleton data, and discuss potential future research directions.



