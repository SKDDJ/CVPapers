# Arxiv Papers in cs.CV on 2016-01-07
### Stochastic Dykstra Algorithms for Metric Learning on Positive Semi-Definite Cone
- **Arxiv ID**: http://arxiv.org/abs/1601.01422v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.01422v1)
- **Published**: 2016-01-07 07:29:45+00:00
- **Updated**: 2016-01-07 07:29:45+00:00
- **Authors**: Tomoki Matsuzawa, Raissa Relator, Jun Sese, Tsuyoshi Kato
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, covariance descriptors have received much attention as powerful representations of set of points. In this research, we present a new metric learning algorithm for covariance descriptors based on the Dykstra algorithm, in which the current solution is projected onto a half-space at each iteration, and runs at O(n^3) time. We empirically demonstrate that randomizing the order of half-spaces in our Dykstra-based algorithm significantly accelerates the convergence to the optimal solution. Furthermore, we show that our approach yields promising experimental results on pattern recognition tasks.



### Mixture of Bilateral-Projection Two-dimensional Probabilistic Principal Component Analysis
- **Arxiv ID**: http://arxiv.org/abs/1601.01431v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.01431v1)
- **Published**: 2016-01-07 07:48:41+00:00
- **Updated**: 2016-01-07 07:48:41+00:00
- **Authors**: Fujiao Ju, Yanfeng Sun, Junbin Gao, Simeng Liu, Yongli Hu
- **Comment**: None
- **Journal**: None
- **Summary**: The probabilistic principal component analysis (PPCA) is built upon a global linear mapping, with which it is insufficient to model complex data variation. This paper proposes a mixture of bilateral-projection probabilistic principal component analysis model (mixB2DPPCA) on 2D data. With multi-components in the mixture, this model can be seen as a soft cluster algorithm and has capability of modeling data with complex structures. A Bayesian inference scheme has been proposed based on the variational EM (Expectation-Maximization) approach for learning model parameters. Experiments on some publicly available databases show that the performance of mixB2DPPCA has been largely improved, resulting in more accurate reconstruction errors and recognition rates than the existing PCA-based algorithms.



### Block-Diagonal Sparse Representation by Learning a Linear Combination Dictionary for Recognition
- **Arxiv ID**: http://arxiv.org/abs/1601.01432v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.01432v2)
- **Published**: 2016-01-07 08:01:56+00:00
- **Updated**: 2016-11-28 00:31:37+00:00
- **Authors**: Xinglin Piao, Yongli Hu, Yanfeng Sun, Junbin Gao, Baocai Yin
- **Comment**: We want to withdraw this paper because we need more mathematical
  derivation and experiments to support our method. Therefore, we think this
  paper is not suitable to be published in this period
- **Journal**: None
- **Summary**: In a sparse representation based recognition scheme, it is critical to learn a desired dictionary, aiming both good representational power and discriminative performance. In this paper, we propose a new dictionary learning model for recognition applications, in which three strategies are adopted to achieve these two objectives simultaneously. First, a block-diagonal constraint is introduced into the model to eliminate the correlation between classes and enhance the discriminative performance. Second, a low-rank term is adopted to model the coherence within classes for refining the sparse representation of each class. Finally, instead of using the conventional over-complete dictionary, a specific dictionary constructed from the linear combination of the training samples is proposed to enhance the representational power of the dictionary and to improve the robustness of the sparse representation model. The proposed method is tested on several public datasets. The experimental results show the method outperforms most state-of-the-art methods.



### On Some Properties of Calibrated Trifocal Tensors
- **Arxiv ID**: http://arxiv.org/abs/1601.01467v3
- **DOI**: 10.1007/s10851-017-0712-x
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.01467v3)
- **Published**: 2016-01-07 10:24:03+00:00
- **Updated**: 2016-05-15 10:08:55+00:00
- **Authors**: Evgeniy Martyushev
- **Comment**: 18 pages, 1 figure
- **Journal**: Journal of Mathematical Imaging and Vision, Volume 58, Issue 2, pp
  321-332, 2017
- **Summary**: In two-view geometry, the essential matrix describes the relative position and orientation of two calibrated images. In three views, a similar role is assigned to the calibrated trifocal tensor. It is a particular case of the (uncalibrated) trifocal tensor and thus it inherits all its properties but, due to the smaller degrees of freedom, satisfies a number of additional algebraic constraints. Some of them are described in this paper. More specifically, we define a new notion --- the trifocal essential matrix. On the one hand, it is a generalization of the ordinary (bifocal) essential matrix, and, on the other hand, it is closely related to the calibrated trifocal tensor. We prove the two necessary and sufficient conditions that characterize the set of trifocal essential matrices. Based on these characterizations, we propose three necessary conditions on a calibrated trifocal tensor. They have a form of 15 quartic and 99 quintic polynomial equations. We show that in the practically significant real case the 15 quartic constraints are also sufficient.



### Learning to Compose Neural Networks for Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1601.01705v4
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1601.01705v4)
- **Published**: 2016-01-07 21:21:59+00:00
- **Updated**: 2016-06-07 23:25:51+00:00
- **Authors**: Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein
- **Comment**: None
- **Journal**: None
- **Summary**: We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural model network, achieves state-of-the-art results on benchmark datasets in both visual and structured domains.



