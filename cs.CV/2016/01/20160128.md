# Arxiv Papers in cs.CV on 2016-01-28
### Combining Maps and Street Level Images for Building Height and Facade Estimation
- **Arxiv ID**: http://arxiv.org/abs/1601.07630v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.07630v2)
- **Published**: 2016-01-28 02:58:04+00:00
- **Updated**: 2016-12-13 18:47:44+00:00
- **Authors**: Jiangye Yuan, Anil M. Cheriyadat
- **Comment**: UrbanGIS '16 Proceedings of the 2nd ACM SIGSPATIAL Workshop on Smart
  Cities and Urban Analytics
- **Journal**: None
- **Summary**: We propose a method that integrates two widely available data sources, building footprints from 2D maps and street level images, to derive valuable information that is generally difficult to acquire -- building heights and building facade masks in images. Building footprints are elevated in world coordinates and projected onto images. Building heights are estimated by scoring projected footprints based on their alignment with building features in images. Building footprints with estimated heights can be converted to simple 3D building models, which are projected back to images to identify buildings. In this procedure, accurate camera projections are critical. However, camera position errors inherited from external sensors commonly exist, which adversely affect results. We derive a solution to precisely locate cameras on maps using correspondence between image features and building footprints. Experiments on real-world datasets show the promise of our method.



### A Grassmannian Graph Approach to Affine Invariant Feature Matching
- **Arxiv ID**: http://arxiv.org/abs/1601.07648v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.07648v2)
- **Published**: 2016-01-28 05:17:17+00:00
- **Updated**: 2016-02-04 05:18:52+00:00
- **Authors**: Mark Moyou, John Corring, Adrian Peter, Anand Rangarajan
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a novel and practical approach to address one of the longstanding problems in computer vision: 2D and 3D affine invariant feature matching. Our Grassmannian Graph (GrassGraph) framework employs a two stage procedure that is capable of robustly recovering correspondences between two unorganized, affinely related feature (point) sets. The first stage maps the feature sets to an affine invariant Grassmannian representation, where the features are mapped into the same subspace. It turns out that coordinate representations extracted from the Grassmannian differ by an arbitrary orthonormal matrix. In the second stage, by approximating the Laplace-Beltrami operator (LBO) on these coordinates, this extra orthonormal factor is nullified, providing true affine-invariant coordinates which we then utilize to recover correspondences via simple nearest neighbor relations. The resulting GrassGraph algorithm is empirically shown to work well in non-ideal scenarios with noise, outliers, and occlusions. Our validation benchmarks use an unprecedented 440,000+ experimental trials performed on 2D and 3D datasets, with a variety of parameter settings and competing methods. State-of-the-art performance in the majority of these extensive evaluations confirm the utility of our method.



### Discriminative Training of Deep Fully-connected Continuous CRF with Task-specific Loss
- **Arxiv ID**: http://arxiv.org/abs/1601.07649v1
- **DOI**: 10.1109/TIP.2017.2675166
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.07649v1)
- **Published**: 2016-01-28 05:30:50+00:00
- **Updated**: 2016-01-28 05:30:50+00:00
- **Authors**: Fayao Liu, Guosheng Lin, Chunhua Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Recent works on deep conditional random fields (CRF) have set new records on many vision tasks involving structured predictions. Here we propose a fully-connected deep continuous CRF model for both discrete and continuous labelling problems. We exemplify the usefulness of the proposed model on multi-class semantic labelling (discrete) and the robust depth estimation (continuous) problems.   In our framework, we model both the unary and the pairwise potential functions as deep convolutional neural networks (CNN), which are jointly learned in an end-to-end fashion. The proposed method possesses the main advantage of continuously-valued CRF, which is a closed-form solution for the Maximum a posteriori (MAP) inference.   To better adapt to different tasks, instead of using the commonly employed maximum likelihood CRF parameter learning protocol, we propose task-specific loss functions for learning the CRF parameters.   It enables direct optimization of the quality of the MAP estimates during the course of learning.   Specifically, we optimize the multi-class classification loss for the semantic labelling task and the Turkey's biweight loss for the robust depth estimation problem.   Experimental results on the semantic labelling and robust depth estimation tasks demonstrate that the proposed method compare favorably against both baseline and state-of-the-art methods.   In particular, we show that although the proposed deep CRF model is continuously valued, with the equipment of task-specific loss, it achieves impressive results even on discrete labelling tasks.



### DehazeNet: An End-to-End System for Single Image Haze Removal
- **Arxiv ID**: http://arxiv.org/abs/1601.07661v2
- **DOI**: 10.1109/TIP.2016.2598681
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.07661v2)
- **Published**: 2016-01-28 06:32:22+00:00
- **Updated**: 2016-05-17 08:03:18+00:00
- **Authors**: Bolun Cai, Xiangmin Xu, Kui Jia, Chunmei Qing, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Single image haze removal is a challenging ill-posed problem. Existing methods use various constraints/priors to get plausible dehazing solutions. The key to achieve haze removal is to estimate a medium transmission map for an input hazy image. In this paper, we propose a trainable end-to-end system called DehazeNet, for medium transmission estimation. DehazeNet takes a hazy image as input, and outputs its medium transmission map that is subsequently used to recover a haze-free image via atmospheric scattering model. DehazeNet adopts Convolutional Neural Networks (CNN) based deep architecture, whose layers are specially designed to embody the established assumptions/priors in image dehazing. Specifically, layers of Maxout units are used for feature extraction, which can generate almost all haze-relevant features. We also propose a novel nonlinear activation function in DehazeNet, called Bilateral Rectified Linear Unit (BReLU), which is able to improve the quality of recovered haze-free image. We establish connections between components of the proposed DehazeNet and those used in existing methods. Experiments on benchmark images show that DehazeNet achieves superior performance over existing methods, yet keeps efficient and easy to use.



### An Overview of Melanoma Detection in Dermoscopy Images Using Image Processing and Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1601.07843v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML, I.4; I.5.4; J.3
- **Links**: [PDF](http://arxiv.org/pdf/1601.07843v1)
- **Published**: 2016-01-28 17:33:48+00:00
- **Updated**: 2016-01-28 17:33:48+00:00
- **Authors**: Nabin K. Mishra, M. Emre Celebi
- **Comment**: 15 pages, 3 figures
- **Journal**: None
- **Summary**: The incidence of malignant melanoma continues to increase worldwide. This cancer can strike at any age; it is one of the leading causes of loss of life in young persons. Since this cancer is visible on the skin, it is potentially detectable at a very early stage when it is curable. New developments have converged to make fully automatic early melanoma detection a real possibility. First, the advent of dermoscopy has enabled a dramatic boost in clinical diagnostic ability to the point that melanoma can be detected in the clinic at the very earliest stages. The global adoption of this technology has allowed accumulation of large collections of dermoscopy images of melanomas and benign lesions validated by histopathology. The development of advanced technologies in the areas of image processing and machine learning have given us the ability to allow distinction of malignant melanoma from the many benign mimics that require no biopsy. These new technologies should allow not only earlier detection of melanoma, but also reduction of the large number of needless and costly biopsy procedures. Although some of the new systems reported for these technologies have shown promise in preliminary trials, widespread implementation must await further technical progress in accuracy and reproducibility. In this paper, we provide an overview of computerized detection of melanoma in dermoscopy images. First, we discuss the various aspects of lesion segmentation. Then, we provide a brief overview of clinical feature segmentation. Finally, we discuss the classification stage where machine learning algorithms are applied to the attributes generated from the segmented features to predict the existence of melanoma.



### Towards the Design of an End-to-End Automated System for Image and Video-based Recognition
- **Arxiv ID**: http://arxiv.org/abs/1601.07883v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.07883v1)
- **Published**: 2016-01-28 20:06:16+00:00
- **Updated**: 2016-01-28 20:06:16+00:00
- **Authors**: Rama Chellappa, Jun-Cheng Chen, Rajeev Ranjan, Swami Sankaranarayanan, Amit Kumar, Vishal M. Patel, Carlos D. Castillo
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Over many decades, researchers working in object recognition have longed for an end-to-end automated system that will simply accept 2D or 3D image or videos as inputs and output the labels of objects in the input data. Computer vision methods that use representations derived based on geometric, radiometric and neural considerations and statistical and structural matchers and artificial neural network-based methods where a multi-layer network learns the mapping from inputs to class labels have provided competing approaches for image recognition problems. Over the last four years, methods based on Deep Convolutional Neural Networks (DCNNs) have shown impressive performance improvements on object detection/recognition challenge problems. This has been made possible due to the availability of large annotated data, a better understanding of the non-linear mapping between image and class labels as well as the affordability of GPUs. In this paper, we present a brief history of developments in computer vision and artificial neural networks over the last forty years for the problem of image-based recognition. We then present the design details of a deep learning system for end-to-end unconstrained face verification/recognition. Some open issues regarding DCNNs for object recognition problems are then discussed. We caution the readers that the views expressed in this paper are from the authors and authors only!



### Geo-distinctive Visual Element Matching for Location Estimation of Images
- **Arxiv ID**: http://arxiv.org/abs/1601.07884v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1601.07884v1)
- **Published**: 2016-01-28 20:13:01+00:00
- **Updated**: 2016-01-28 20:13:01+00:00
- **Authors**: Xinchao Li, Martha A. Larson, Alan Hanjalic
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an image representation and matching approach that substantially improves visual-based location estimation for images. The main novelty of the approach, called distinctive visual element matching (DVEM), is its use of representations that are specific to the query image whose location is being predicted. These representations are based on visual element clouds, which robustly capture the connection between the query and visual evidence from candidate locations. We then maximize the influence of visual elements that are geo-distinctive because they do not occur in images taken at many other locations. We carry out experiments and analysis for both geo-constrained and geo-unconstrained location estimation cases using two large-scale, publicly-available datasets: the San Francisco Landmark dataset with $1.06$ million street-view images and the MediaEval '15 Placing Task dataset with $5.6$ million geo-tagged images from Flickr. We present examples that illustrate the highly-transparent mechanics of the approach, which are based on common sense observations about the visual patterns in image collections. Our results show that the proposed method delivers a considerable performance improvement compared to the state of the art.



