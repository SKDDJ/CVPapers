# Arxiv Papers in cs.CV on 2016-01-14
### Quantification of Ultrasonic Texture heterogeneity via Volumetric Stochastic Modeling for Tissue Characterization
- **Arxiv ID**: http://arxiv.org/abs/1601.03531v1
- **DOI**: 10.1016/j.media.2014.12.004
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.03531v1)
- **Published**: 2016-01-14 09:51:37+00:00
- **Updated**: 2016-01-14 09:51:37+00:00
- **Authors**: O. S. Al-Kadi, Daniel Y. F. Chung, Robert C. Carlisle, Constantin C. Coussios, J. Alison Noble
- **Comment**: Supplementary data associated with this article can be found, in the
  online version, at http://dx.doi.org/10.1016/j.media.2014.12. 004
- **Journal**: Medical Image Analysis, vol. 21(1), pp. 59-71, 2015
- **Summary**: Intensity variations in image texture can provide powerful quantitative information about physical properties of biological tissue. However, tissue patterns can vary according to the utilized imaging system and are intrinsically correlated to the scale of analysis. In the case of ultrasound, the Nakagami distribution is a general model of the ultrasonic backscattering envelope under various scattering conditions and densities where it can be employed for characterizing image texture, but the subtle intra-heterogeneities within a given mass are difficult to capture via this model as it works at a single spatial scale. This paper proposes a locally adaptive 3D multi-resolution Nakagami-based fractal feature descriptor that extends Nakagami-based texture analysis to accommodate subtle speckle spatial frequency tissue intensity variability in volumetric scans. Local textural fractal descriptors - which are invariant to affine intensity changes - are extracted from volumetric patches at different spatial resolutions from voxel lattice-based generated shape and scale Nakagami parameters. Using ultrasound radio-frequency datasets we found that after applying an adaptive fractal decomposition label transfer approach on top of the generated Nakagami voxels, tissue characterization results were superior to the state of art. Experimental results on real 3D ultrasonic pre-clinical and clinical datasets suggest that describing tumor intra-heterogeneity via this descriptor may facilitate improved prediction of therapy response and disease characterization.



### Dynamic Concept Composition for Zero-Example Event Detection
- **Arxiv ID**: http://arxiv.org/abs/1601.03679v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.03679v1)
- **Published**: 2016-01-14 17:40:09+00:00
- **Updated**: 2016-01-14 17:40:09+00:00
- **Authors**: Xiaojun Chang, Yi Yang, Guodong Long, Chengqi Zhang, Alexander G. Hauptmann
- **Comment**: 7 pages, AAAI 2016
- **Journal**: None
- **Summary**: In this paper, we focus on automatically detecting events in unconstrained videos without the use of any visual training exemplars. In principle, zero-shot learning makes it possible to train an event detection model based on the assumption that events (e.g. \emph{birthday party}) can be described by multiple mid-level semantic concepts (e.g. "blowing candle", "birthday cake"). Towards this goal, we first pre-train a bundle of concept classifiers using data from other sources. Then we evaluate the semantic correlation of each concept \wrt the event of interest and pick up the relevant concept classifiers, which are applied on all test videos to get multiple prediction score vectors. While most existing systems combine the predictions of the concept classifiers with fixed weights, we propose to learn the optimal weights of the concept classifiers for each testing video by exploring a set of online available videos with free-form text descriptions of their content. To validate the effectiveness of the proposed approach, we have conducted extensive experiments on the latest TRECVID MEDTest 2014, MEDTest 2013 and CCV dataset. The experimental results confirm the superiority of the proposed approach.



