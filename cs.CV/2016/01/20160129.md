# Arxiv Papers in cs.CV on 2016-01-29
### Face Alignment by Local Deep Descriptor Regression
- **Arxiv ID**: http://arxiv.org/abs/1601.07950v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.07950v1)
- **Published**: 2016-01-29 00:00:16+00:00
- **Updated**: 2016-01-29 00:00:16+00:00
- **Authors**: Amit Kumar, Rajeev Ranjan, Vishal Patel, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: We present an algorithm for extracting key-point descriptors using deep convolutional neural networks (CNN). Unlike many existing deep CNNs, our model computes local features around a given point in an image. We also present a face alignment algorithm based on regression using these local descriptors. The proposed method called Local Deep Descriptor Regression (LDDR) is able to localize face landmarks of varying sizes, poses and occlusions with high accuracy. Deep Descriptors presented in this paper are able to uniquely and efficiently describe every pixel in the image and therefore can potentially replace traditional descriptors such as SIFT and HOG. Extensive evaluations on five publicly available unconstrained face alignment datasets show that our deep descriptor network is able to capture strong local features around a given landmark and performs significantly better than many competitive and state-of-the-art face alignment algorithms.



### Hybrid CNN and Dictionary-Based Models for Scene Recognition and Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1601.07977v1
- **DOI**: 10.1109/TCSVT.2015.2511543
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.07977v1)
- **Published**: 2016-01-29 05:32:52+00:00
- **Updated**: 2016-01-29 05:32:52+00:00
- **Authors**: Guo-Sen Xie, Xu-Yao Zhang, Shuicheng Yan, Cheng-Lin Liu
- **Comment**: Accepted by TCSVT on Sep.2015
- **Journal**: None
- **Summary**: Convolutional neural network (CNN) has achieved state-of-the-art performance in many different visual tasks. Learned from a large-scale training dataset, CNN features are much more discriminative and accurate than the hand-crafted features. Moreover, CNN features are also transferable among different domains. On the other hand, traditional dictionarybased features (such as BoW and SPM) contain much more local discriminative and structural information, which is implicitly embedded in the images. To further improve the performance, in this paper, we propose to combine CNN with dictionarybased models for scene recognition and visual domain adaptation. Specifically, based on the well-tuned CNN models (e.g., AlexNet and VGG Net), two dictionary-based representations are further constructed, namely mid-level local representation (MLR) and convolutional Fisher vector representation (CFV). In MLR, an efficient two-stage clustering method, i.e., weighted spatial and feature space spectral clustering on the parts of a single image followed by clustering all representative parts of all images, is used to generate a class-mixture or a classspecific part dictionary. After that, the part dictionary is used to operate with the multi-scale image inputs for generating midlevel representation. In CFV, a multi-scale and scale-proportional GMM training strategy is utilized to generate Fisher vectors based on the last convolutional layer of CNN. By integrating the complementary information of MLR, CFV and the CNN features of the fully connected layer, the state-of-the-art performance can be achieved on scene recognition and domain adaptation problems. An interested finding is that our proposed hybrid representation (from VGG net trained on ImageNet) is also complementary with GoogLeNet and/or VGG-11 (trained on Place205) greatly.



### Efficient Robust Mean Value Calculation of 1D Features
- **Arxiv ID**: http://arxiv.org/abs/1601.08003v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.08003v1)
- **Published**: 2016-01-29 08:54:22+00:00
- **Updated**: 2016-01-29 08:54:22+00:00
- **Authors**: Erik Jonsson, Michael Felsberg
- **Comment**: Presented at the SSBA Symposium 2005, Malm\"o, Sweden
- **Journal**: None
- **Summary**: A robust mean value is often a good alternative to the standard mean value when dealing with data containing many outliers. An efficient method for samples of one-dimensional features and the truncated quadratic error norm is presented and compared to the method of channel averaging (soft histograms).



### Mapping Tractography Across Subjects
- **Arxiv ID**: http://arxiv.org/abs/1601.08165v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1601.08165v1)
- **Published**: 2016-01-29 15:50:20+00:00
- **Updated**: 2016-01-29 15:50:20+00:00
- **Authors**: Thien Bao Nguyen, Emanuele Olivetti, Paolo Avesani
- **Comment**: None
- **Journal**: None
- **Summary**: Diffusion magnetic resonance imaging (dMRI) and tractography provide means to study the anatomical structures within the white matter of the brain. When studying tractography data across subjects, it is usually necessary to align, i.e. to register, tractographies together. This registration step is most often performed by applying the transformation resulting from the registration of other volumetric images (T1, FA). In contrast with registration methods that "transform" tractographies, in this work, we try to find which streamline in one tractography correspond to which streamline in the other tractography, without any transformation. In other words, we try to find a "mapping" between the tractographies. We propose a graph-based solution for the tractography mapping problem and we explain similarities and differences with the related well-known graph matching problem. Specifically, we define a loss function based on the pairwise streamline distance and reformulate the mapping problem as combinatorial optimization of that loss function. We show preliminary promising results where we compare the proposed method, implemented with simulated annealing, against a standard registration techniques in a task of segmentation of the corticospinal tract.



### Joint System and Algorithm Design for Computationally Efficient Fan Beam Coded Aperture X-ray Coherent Scatter Imaging
- **Arxiv ID**: http://arxiv.org/abs/1603.06400v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/1603.06400v1)
- **Published**: 2016-01-29 16:35:57+00:00
- **Updated**: 2016-01-29 16:35:57+00:00
- **Authors**: Ikenna Odinaka, Joseph A. O'Sullivan, David G. Politte, Kenneth P. MacCabe, Yan Kaganovsky, Joel A. Greenberg, Manu Lakshmanan, Kalyani Krishnamurthy, Anuj Kapadia, Lawrence Carin, David J. Brady
- **Comment**: This paper has been submitted to IEEE Transactions on Computational
  Imaging for consideration. 18 pages, 6 figures
- **Journal**: None
- **Summary**: In x-ray coherent scatter tomography, tomographic measurements of the forward scatter distribution are used to infer scatter densities within a volume. A radiopaque 2D pattern placed between the object and the detector array enables the disambiguation between different scatter events. The use of a fan beam source illumination to speed up data acquisition relative to a pencil beam presents computational challenges. To facilitate the use of iterative algorithms based on a penalized Poisson log-likelihood function, efficient computational implementation of the forward and backward models are needed. Our proposed implementation exploits physical symmetries and structural properties of the system and suggests a joint system-algorithm design, where the system design choices are influenced by computational considerations, and in turn lead to reduced reconstruction time. Computational-time speedups of approximately 146 and 32 are achieved in the computation of the forward and backward models, respectively. Results validating the forward model and reconstruction algorithm are presented on simulated analytic and Monte Carlo data.



### Lipreading with Long Short-Term Memory
- **Arxiv ID**: http://arxiv.org/abs/1601.08188v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1601.08188v1)
- **Published**: 2016-01-29 16:48:07+00:00
- **Updated**: 2016-01-29 16:48:07+00:00
- **Authors**: Michael Wand, Jan Koutník, Jürgen Schmidhuber
- **Comment**: Accepted for publication at ICASSP 2016
- **Journal**: None
- **Summary**: Lipreading, i.e. speech recognition from visual-only recordings of a speaker's face, can be achieved with a processing pipeline based solely on neural networks, yielding significantly better accuracy than conventional methods. Feed-forward and recurrent neural network layers (namely Long Short-Term Memory; LSTM) are stacked to form a single structure which is trained by back-propagating error gradients through all the layers. The performance of such a stacked network was experimentally evaluated and compared to a standard Support Vector Machine classifier using conventional computer vision features (Eigenlips and Histograms of Oriented Gradients). The evaluation was performed on data from 19 speakers of the publicly available GRID corpus. With 51 different words to classify, we report a best word accuracy on held-out evaluation speakers of 79.6% using the end-to-end neural network-based solution (11.6% improvement over the best feature-based solution evaluated).



### Spectrally Grouped Total Variation Reconstruction for Scatter Imaging Using ADMM
- **Arxiv ID**: http://arxiv.org/abs/1601.08201v1
- **DOI**: 10.1109/NSSMIC.2015.7582220
- **Categories**: **math.NA**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1601.08201v1)
- **Published**: 2016-01-29 17:27:37+00:00
- **Updated**: 2016-01-29 17:27:37+00:00
- **Authors**: Ikenna Odinaka, Yan Kaganovsky, Joel A. Greenberg, Mehadi Hassan, David G. Politte, Joseph A. O'Sullivan, Lawrence Carin, David J. Brady
- **Comment**: Presented at IEEE Nuclear Science Symposium and Medical Imaging
  Conference (NSS/MIC) 2015. 4 pages, 2 figures
- **Journal**: None
- **Summary**: We consider X-ray coherent scatter imaging, where the goal is to reconstruct momentum transfer profiles (spectral distributions) at each spatial location from multiplexed measurements of scatter. Each material is characterized by a unique momentum transfer profile (MTP) which can be used to discriminate between different materials. We propose an iterative image reconstruction algorithm based on a Poisson noise model that can account for photon-limited measurements as well as various second order statistics of the data. To improve image quality, previous approaches use edge-preserving regularizers to promote piecewise constancy of the image in the spatial domain while treating each spectral bin separately. Instead, we propose spectrally grouped regularization that promotes piecewise constant images along the spatial directions but also ensures that the MTPs of neighboring spatial bins are similar, if they contain the same material. We demonstrate that this group regularization results in improvement of both spectral and spatial image quality. We pursue an optimization transfer approach where convex decompositions are used to lift the problem such that all hyper-voxels can be updated in parallel and in closed-form. The group penalty introduces a challenge since it is not directly amendable to these decompositions. We use the alternating directions method of multipliers (ADMM) to replace the original problem with an equivalent sequence of sub-problems that are amendable to convex decompositions, leading to a highly parallel algorithm. We demonstrate the performance on real data.



### Deep convolutional networks for automated detection of posterior-element fractures on spine CT
- **Arxiv ID**: http://arxiv.org/abs/1602.00020v1
- **DOI**: 10.1117/12.2217146
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.00020v1)
- **Published**: 2016-01-29 21:48:13+00:00
- **Updated**: 2016-01-29 21:48:13+00:00
- **Authors**: Holger R. Roth, Yinong Wang, Jianhua Yao, Le Lu, Joseph E. Burns, Ronald M. Summers
- **Comment**: To be presented at SPIE Medical Imaging, 2016, San Diego
- **Journal**: None
- **Summary**: Injuries of the spine, and its posterior elements in particular, are a common occurrence in trauma patients, with potentially devastating consequences. Computer-aided detection (CADe) could assist in the detection and classification of spine fractures. Furthermore, CAD could help assess the stability and chronicity of fractures, as well as facilitate research into optimization of treatment paradigms.   In this work, we apply deep convolutional networks (ConvNets) for the automated detection of posterior element fractures of the spine. First, the vertebra bodies of the spine with its posterior elements are segmented in spine CT using multi-atlas label fusion. Then, edge maps of the posterior elements are computed. These edge maps serve as candidate regions for predicting a set of probabilities for fractures along the image edges using ConvNets in a 2.5D fashion (three orthogonal patches in axial, coronal and sagittal planes). We explore three different methods for training the ConvNet using 2.5D patches along the edge maps of 'positive', i.e. fractured posterior-elements and 'negative', i.e. non-fractured elements.   An experienced radiologist retrospectively marked the location of 55 displaced posterior-element fractures in 18 trauma patients. We randomly split the data into training and testing cases. In testing, we achieve an area-under-the-curve of 0.857. This corresponds to 71% or 81% sensitivities at 5 or 10 false-positives per patient, respectively. Analysis of our set of trauma patients demonstrates the feasibility of detecting posterior-element fractures in spine CT images using computer vision techniques such as deep convolutional networks.



### What Can I Do Around Here? Deep Functional Scene Understanding for Cognitive Robots
- **Arxiv ID**: http://arxiv.org/abs/1602.00032v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1602.00032v2)
- **Published**: 2016-01-29 22:55:53+00:00
- **Updated**: 2016-02-02 16:28:01+00:00
- **Authors**: Chengxi Ye, Yezhou Yang, Cornelia Fermuller, Yiannis Aloimonos
- **Comment**: None
- **Journal**: None
- **Summary**: For robots that have the capability to interact with the physical environment through their end effectors, understanding the surrounding scenes is not merely a task of image classification or object recognition. To perform actual tasks, it is critical for the robot to have a functional understanding of the visual scene. Here, we address the problem of localizing and recognition of functional areas from an arbitrary indoor scene, formulated as a two-stage deep learning based detection pipeline. A new scene functionality testing-bed, which is complied from two publicly available indoor scene datasets, is used for evaluation. Our method is evaluated quantitatively on the new dataset, demonstrating the ability to perform efficient recognition of functional areas from arbitrary indoor scenes. We also demonstrate that our detection model can be generalized onto novel indoor scenes by cross validating it with the images from two different datasets.



