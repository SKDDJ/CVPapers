# Arxiv Papers in cs.CV on 2016-01-13
### Blind Image Denoising via Dependent Dirichlet Process Tree
- **Arxiv ID**: http://arxiv.org/abs/1601.03117v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1601.03117v1)
- **Published**: 2016-01-13 02:44:36+00:00
- **Updated**: 2016-01-13 02:44:36+00:00
- **Authors**: Fengyuan Zhu, Guangyong Chen, Jianye Hao, Pheng-Ann Heng
- **Comment**: 25 pages, 11 figures
- **Journal**: None
- **Summary**: Most existing image denoising approaches assumed the noise to be homogeneous white Gaussian distributed with known intensity. However, in real noisy images, the noise models are usually unknown beforehand and can be much more complex. This paper addresses this problem and proposes a novel blind image denoising algorithm to recover the clean image from noisy one with the unknown noise model. To model the empirical noise of an image, our method introduces the mixture of Gaussian distribution, which is flexible enough to approximate different continuous distributions. The problem of blind image denoising is reformulated as a learning problem. The procedure is to first build a two-layer structural model for noisy patches and consider the clean ones as latent variable. To control the complexity of the noisy patch model, this work proposes a novel Bayesian nonparametric prior called "Dependent Dirichlet Process Tree" to build the model. Then, this study derives a variational inference algorithm to estimate model parameters and recover clean patches. We apply our method on synthesis and real noisy images with different noise models. Comparing with previous approaches, ours achieves better performance. The experimental results indicate the efficiency of the proposed algorithm to cope with practical image denoising tasks.



### Enhancing Energy Minimization Framework for Scene Text Recognition with Top-Down Cues
- **Arxiv ID**: http://arxiv.org/abs/1601.03128v1
- **DOI**: 10.1016/j.cviu.2016.01.002
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.03128v1)
- **Published**: 2016-01-13 04:47:28+00:00
- **Updated**: 2016-01-13 04:47:28+00:00
- **Authors**: Anand Mishra, Karteek Alahari, C. V. Jawahar
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing scene text is a challenging problem, even more so than the recognition of scanned documents. This problem has gained significant attention from the computer vision community in recent years, and several methods based on energy minimization frameworks and deep learning approaches have been proposed. In this work, we focus on the energy minimization framework and propose a model that exploits both bottom-up and top-down cues for recognizing cropped words extracted from street images. The bottom-up cues are derived from individual character detections from an image. We build a conditional random field model on these detections to jointly model the strength of the detections and the interactions between them. These interactions are top-down cues obtained from a lexicon-based prior, i.e., language statistics. The optimal word represented by the text image is obtained by minimizing the energy function corresponding to the random field model. We evaluate our proposed algorithm extensively on a number of cropped scene text benchmark datasets, namely Street View Text, ICDAR 2003, 2011 and 2013 datasets, and IIIT 5K-word, and show better performance than comparable methods. We perform a rigorous analysis of all the steps in our approach and analyze the results. We also show that state-of-the-art convolutional neural network features can be integrated in our framework to further improve the recognition performance.



### Digital Image Forensics vs. Image Composition: An Indirect Arms Race
- **Arxiv ID**: http://arxiv.org/abs/1601.03239v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1601.03239v1)
- **Published**: 2016-01-13 13:38:36+00:00
- **Updated**: 2016-01-13 13:38:36+00:00
- **Authors**: Victor Schetinger, Massimo Iuliani, Alessandro Piva, Manuel M. Oliveira
- **Comment**: None
- **Journal**: None
- **Summary**: The field of image composition is constantly trying to improve the ways in which an image can be altered and enhanced. While this is usually done in the name of aesthetics and practicality, it also provides tools that can be used to maliciously alter images. In this sense, the field of digital image forensics has to be prepared to deal with the influx of new technology, in a constant arms-race. In this paper, the current state of this arms-race is analyzed, surveying the state-of-the-art and providing means to compare both sides. A novel scale to classify image forensics assessments is proposed, and experiments are performed to test composition techniques in regards to different forensics traces. We show that even though research in forensics seems unaware of the advanced forms of image composition, it possesses the basic tools to detect it.



### Document image classification, with a specific view on applications of patent images
- **Arxiv ID**: http://arxiv.org/abs/1601.03295v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.03295v1)
- **Published**: 2016-01-13 16:02:13+00:00
- **Updated**: 2016-01-13 16:02:13+00:00
- **Authors**: Gabriela Csurka
- **Comment**: Paper submitted in 2014 as book chapter of Current Challenges in
  Patent Information Retrieval, Second edition by M. Lupu et al (eds.). To
  appear in 2016
- **Journal**: None
- **Summary**: The main focus of this paper is document image classification and retrieval, where we analyze and compare different parameters for the RunLeght Histogram (RL) and Fisher Vector (FV) based image representations. We do an exhaustive experimental study using different document image datasets, including the MARG benchmarks, two datasets built on customer data and the images from the Patent Image Classification task of the Clef-IP 2011. The aim of the study is to give guidelines on how to best choose the parameters such that the same features perform well on different tasks. As an example of such need, we describe the Image-based Patent Retrieval task's of Clef-IP 2011, where we used the same image representation to predict the image type and retrieve relevant patents.



### Localized Dictionary design for Geometrically Robust Sonar ATR
- **Arxiv ID**: http://arxiv.org/abs/1601.03323v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.03323v1)
- **Published**: 2016-01-13 17:35:19+00:00
- **Updated**: 2016-01-13 17:35:19+00:00
- **Authors**: John McKay, Vishal Monga, Raghu Raj
- **Comment**: Submitted to IGARSS 2016
- **Journal**: None
- **Summary**: Advancements in Sonar image capture have opened the door to powerful classification schemes for automatic target recognition (ATR. Recent work has particularly seen the application of sparse reconstruction-based classification (SRC) to sonar ATR, which provides compelling accuracy rates even in the presence of noise and blur. Existing sparsity based sonar ATR techniques however assume that the test images exhibit geometric pose that is consistent with respect to the training set. This work addresses the outstanding open challenge of handling inconsistently posed test sonar images relative to training. We develop a new localized block-based dictionary design that can enable geometric, i.e. pose robustness. Further, a dictionary learning method is incorporated to increase performance and efficiency. The proposed SRC with Localized Pose Management (LPM), is shown to outperform the state of the art SIFT feature and SVM approach, due to its power to discern background clutter in Sonar images.



### A Score-level Fusion Method for Eye Movement Biometrics
- **Arxiv ID**: http://arxiv.org/abs/1601.03333v1
- **DOI**: 10.1016/j.patrec.2015.11.020
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.03333v1)
- **Published**: 2016-01-13 18:06:57+00:00
- **Updated**: 2016-01-13 18:06:57+00:00
- **Authors**: Anjith George, Aurobinda Routray
- **Comment**: 11 pages, 6 figures, In press, Pattern Recognition Letters
- **Journal**: None
- **Summary**: This paper proposes a novel framework for the use of eye movement patterns for biometric applications. Eye movements contain abundant information about cognitive brain functions, neural pathways, etc. In the proposed method, eye movement data is classified into fixations and saccades. Features extracted from fixations and saccades are used by a Gaussian Radial Basis Function Network (GRBFN) based method for biometric authentication. A score fusion approach is adopted to classify the data in the output layer. In the evaluation stage, the algorithm has been tested using two types of stimuli: random dot following on a screen and text reading. The results indicate the strength of eye movement pattern as a biometric modality. The algorithm has been evaluated on BioEye 2015 database and found to outperform all the other methods. Eye movements are generated by a complex oculomotor plant which is very hard to spoof by mechanical replicas. Use of eye movement dynamics along with iris recognition technology may lead to a robust counterfeit-resistant person identification system.



### Multi-Atlas Segmentation with Joint Label Fusion of Osteoporotic Vertebral Compression Fractures on CT
- **Arxiv ID**: http://arxiv.org/abs/1601.03375v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.03375v1)
- **Published**: 2016-01-13 20:30:15+00:00
- **Updated**: 2016-01-13 20:30:15+00:00
- **Authors**: Yinong Wang, Jianhua Yao, Holger R. Roth, Joseph E. Burns, Ronald M. Summers
- **Comment**: MICCAI 2015 Computational Methods and Clinical Applications for Spine
  Imaging Workshop
- **Journal**: None
- **Summary**: The precise and accurate segmentation of the vertebral column is essential in the diagnosis and treatment of various orthopedic, neurological, and oncological traumas and pathologies. Segmentation is especially challenging in the presence of pathology such as vertebral compression fractures. In this paper, we propose a method to produce segmentations for osteoporotic compression fractured vertebrae by applying a multi-atlas joint label fusion technique for clinical CT images. A total of 170 thoracic and lumbar vertebrae were evaluated using atlases from five patients with varying degrees of spinal degeneration. In an osteoporotic cohort of bundled atlases, registration provided an average Dice coefficient and mean absolute surface distance of 2.7$\pm$4.5% and 0.32$\pm$0.13mm for osteoporotic vertebrae, respectively, and 90.9$\pm$3.0% and 0.36$\pm$0.11mm for compression fractured vertebrae.



