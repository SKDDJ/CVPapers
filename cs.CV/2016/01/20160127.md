# Arxiv Papers in cs.CV on 2016-01-27
### Font Identification in Historical Documents Using Active Learning
- **Arxiv ID**: http://arxiv.org/abs/1601.07252v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.DL, stat.AP, stat.ML, I.5; I.2
- **Links**: [PDF](http://arxiv.org/pdf/1601.07252v1)
- **Published**: 2016-01-27 03:24:05+00:00
- **Updated**: 2016-01-27 03:24:05+00:00
- **Authors**: Anshul Gupta, Ricardo Gutierrez-Osuna, Matthew Christy, Richard Furuta, Laura Mandell
- **Comment**: None
- **Journal**: None
- **Summary**: Identifying the type of font (e.g., Roman, Blackletter) used in historical documents can help optical character recognition (OCR) systems produce more accurate text transcriptions. Towards this end, we present an active-learning strategy that can significantly reduce the number of labeled samples needed to train a font classifier. Our approach extracts image-based features that exploit geometric differences between fonts at the word level, and combines them into a bag-of-word representation for each page in a document. We evaluate six sampling strategies based on uncertainty, dissimilarity and diversity criteria, and test them on a database containing over 3,000 historical documents with Blackletter, Roman and Mixed fonts. Our results show that a combination of uncertainty and diversity achieves the highest predictive accuracy (89% of test cases correctly classified) while requiring only a small fraction of the data (17%) to be labeled. We discuss the implications of this result for mass digitization projects of historical documents.



### PersonNet: Person Re-identification with Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1601.07255v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.07255v2)
- **Published**: 2016-01-27 03:49:34+00:00
- **Updated**: 2016-06-20 06:43:54+00:00
- **Authors**: Lin Wu, Chunhua Shen, Anton van den Hengel
- **Comment**: 7 pages. Fixed Figure 4 (a)
- **Journal**: None
- **Summary**: In this paper, we propose a deep end-to-end neu- ral network to simultaneously learn high-level features and a corresponding similarity metric for person re-identification. The network takes a pair of raw RGB images as input, and outputs a similarity value indicating whether the two input images depict the same person. A layer of computing neighborhood range differences across two input images is employed to capture local relationship between patches. This operation is to seek a robust feature from input images. By increasing the depth to 10 weight layers and using very small (3$\times$3) convolution filters, our architecture achieves a remarkable improvement on the prior-art configurations. Meanwhile, an adaptive Root- Mean-Square (RMSProp) gradient decent algorithm is integrated into our architecture, which is beneficial to deep nets. Our method consistently outperforms state-of-the-art on two large datasets (CUHK03 and Market-1501), and a medium-sized data set (CUHK01).



### Fast Integral Image Estimation at 1% measurement rate
- **Arxiv ID**: http://arxiv.org/abs/1601.07258v1
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1601.07258v1)
- **Published**: 2016-01-27 04:32:20+00:00
- **Updated**: 2016-01-27 04:32:20+00:00
- **Authors**: Kuldeep Kulkarni, Pavan Turaga
- **Comment**: Submitted to TPAMI
- **Journal**: None
- **Summary**: We propose a framework called ReFInE to directly obtain integral image estimates from a very small number of spatially multiplexed measurements of the scene without iterative reconstruction of any auxiliary image, and demonstrate their practical utility in visual object tracking. Specifically, we design measurement matrices which are tailored to facilitate extremely fast estimation of the integral image, by using a single-shot linear operation on the measured vector. Leveraging a prior model for the images, we formulate a nuclear norm minimization problem with second order conic constraints to jointly obtain the measurement matrix and the linear operator. Through qualitative and quantitative experiments, we show that high quality integral image estimates can be obtained using our framework at very low measurement rates. Further, on a standard dataset of 50 videos, we present object tracking results which are comparable to the state-of-the-art methods, even at an extremely low measurement rate of 1%.



### Deep Learning Driven Visual Path Prediction from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1601.07265v1
- **DOI**: 10.1109/TIP.2016.2613686
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.07265v1)
- **Published**: 2016-01-27 05:04:31+00:00
- **Updated**: 2016-01-27 05:04:31+00:00
- **Authors**: Siyu Huang, Xi Li, Zhongfei Zhang, Zhouzhou He, Fei Wu, Wei Liu, Jinhui Tang, Yueting Zhuang
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing, vol. 25, no. 12, pp.
  5892-5904, Dec. 2016
- **Summary**: Capabilities of inference and prediction are significant components of visual systems. In this paper, we address an important and challenging task of them: visual path prediction. Its goal is to infer the future path for a visual object in a static scene. This task is complicated as it needs high-level semantic understandings of both the scenes and motion patterns underlying video sequences. In practice, cluttered situations have also raised higher demands on the effectiveness and robustness of the considered models. Motivated by these observations, we propose a deep learning framework which simultaneously performs deep feature learning for visual representation in conjunction with spatio-temporal context modeling. After that, we propose a unified path planning scheme to make accurate future path prediction based on the analytic results of the context models. The highly effective visual representation and deep context models ensure that our framework makes a deep semantic understanding of the scene and motion pattern, consequently improving the performance of the visual path prediction task. In order to comprehensively evaluate the model's performance on the visual path prediction task, we construct two large benchmark datasets from the adaptation of video tracking datasets. The qualitative and quantitative experimental results show that our approach outperforms the existing approaches and owns a better generalization capability.



### Comprehensive Feature-based Robust Video Fingerprinting Using Tensor Model
- **Arxiv ID**: http://arxiv.org/abs/1601.07270v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.07270v1)
- **Published**: 2016-01-27 06:02:59+00:00
- **Updated**: 2016-01-27 06:02:59+00:00
- **Authors**: Xiushan Nie, Yilong Yin, Jiande Sun
- **Comment**: 13pages
- **Journal**: None
- **Summary**: Content-based near-duplicate video detection (NDVD) is essential for effective search and retrieval, and robust video fingerprinting is a good solution for NDVD. Most existing video fingerprinting methods use a single feature or concatenating different features to generate video fingerprints, and show a good performance under single-mode modifications such as noise addition and blurring. However, when they suffer combined modifications, the performance is degraded to a certain extent because such features cannot characterize the video content completely. By contrast, the assistance and consensus among different features can improve the performance of video fingerprinting. Therefore, in the present study, we mine the assistance and consensus among different features based on tensor model, and present a new comprehensive feature to fully use them in the proposed video fingerprinting framework. We also analyze what the comprehensive feature really is for representing the original video. In this framework, the video is initially set as a high-order tensor that consists of different features, and the video tensor is decomposed via the Tucker model with a solution that determines the number of components. Subsequently, the comprehensive feature is generated by the low-order tensor obtained from tensor decomposition. Finally, the video fingerprint is computed using this feature. A matching strategy used for narrowing the search is also proposed based on the core tensor. The robust video fingerprinting framework is resistant not only to single-mode modifications, but also to the combination of them.



### Neighborhood Preserved Sparse Representation for Robust Classification on Symmetric Positive Definite Matrices
- **Arxiv ID**: http://arxiv.org/abs/1601.07336v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.07336v1)
- **Published**: 2016-01-27 12:05:22+00:00
- **Updated**: 2016-01-27 12:05:22+00:00
- **Authors**: Ming Yin, Shengli Xie, Yi Guo, Junbin Gao, Yun Zhang
- **Comment**: arXiv admin note: text overlap with arXiv:1601.00414
- **Journal**: None
- **Summary**: Due to its promising classification performance, sparse representation based classification(SRC) algorithm has attracted great attention in the past few years. However, the existing SRC type methods apply only to vector data in Euclidean space. As such, there is still no satisfactory approach to conduct classification task for symmetric positive definite (SPD) matrices which is very useful in computer vision. To address this problem, in this paper, a neighborhood preserved kernel SRC method is proposed on SPD manifolds. Specifically, by embedding the SPD matrices into a Reproducing Kernel Hilbert Space (RKHS), the proposed method can perform classification on SPD manifolds through an appropriate Log-Euclidean kernel. Through exploiting the geodesic distance between SPD matrices, our method can effectively characterize the intrinsic local Riemannian geometry within data so as to well unravel the underlying sub-manifold structure. Despite its simplicity, experimental results on several famous database demonstrate that the proposed method achieves better classification results than the state-of-the-art approaches.



### Shape Distributions of Nonlinear Dynamical Systems for Video-based Inference
- **Arxiv ID**: http://arxiv.org/abs/1601.07471v1
- **DOI**: 10.1109/TPAMI.2016.2533388
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.07471v1)
- **Published**: 2016-01-27 18:01:38+00:00
- **Updated**: 2016-01-27 18:01:38+00:00
- **Authors**: Vinay Venkataraman, Pavan Turaga
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence
- **Journal**: None
- **Summary**: This paper presents a shape-theoretic framework for dynamical analysis of nonlinear dynamical systems which appear frequently in several video-based inference tasks. Traditional approaches to dynamical modeling have included linear and nonlinear methods with their respective drawbacks. A novel approach we propose is the use of descriptors of the shape of the dynamical attractor as a feature representation of nature of dynamics. The proposed framework has two main advantages over traditional approaches: a) representation of the dynamical system is derived directly from the observational data, without any inherent assumptions, and b) the proposed features show stability under different time-series lengths where traditional dynamical invariants fail. We illustrate our idea using nonlinear dynamical models such as Lorenz and Rossler systems, where our feature representations (shape distribution) support our hypothesis that the local shape of the reconstructed phase space can be used as a discriminative feature. Our experimental analyses on these models also indicate that the proposed framework show stability for different time-series lengths, which is useful when the available number of samples are small/variable. The specific applications of interest in this paper are: 1) activity recognition using motion capture and RGBD sensors, 2) activity quality assessment for applications in stroke rehabilitation, and 3) dynamical scene classification. We provide experimental validation through action and gesture recognition experiments on motion capture and Kinect datasets. In all these scenarios, we show experimental evidence of the favorable properties of the proposed representation.



### Learning to Extract Motion from Videos in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1601.07532v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.07532v1)
- **Published**: 2016-01-27 20:19:14+00:00
- **Updated**: 2016-01-27 20:19:14+00:00
- **Authors**: Damien Teney, Martial Hebert
- **Comment**: None
- **Journal**: None
- **Summary**: This paper shows how to extract dense optical flow from videos with a convolutional neural network (CNN). The proposed model constitutes a potential building block for deeper architectures to allow using motion without resorting to an external algorithm, \eg for recognition in videos. We derive our network architecture from signal processing principles to provide desired invariances to image contrast, phase and texture. We constrain weights within the network to enforce strict rotation invariance and substantially reduce the number of parameters to learn. We demonstrate end-to-end training on only 8 sequences of the Middlebury dataset, orders of magnitude less than competing CNN-based motion estimation methods, and obtain comparable performance to classical methods on the Middlebury benchmark. Importantly, our method outputs a distributed representation of motion that allows representing multiple, transparent motions, and dynamic textures. Our contributions on network design and rotation invariance offer insights nonspecific to motion estimation.



### Osteoporotic and Neoplastic Compression Fracture Classification on Longitudinal CT
- **Arxiv ID**: http://arxiv.org/abs/1601.07533v1
- **DOI**: 10.1109/ISBI.2016.7493477
- **Categories**: **cs.CV**, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/1601.07533v1)
- **Published**: 2016-01-27 20:20:05+00:00
- **Updated**: 2016-01-27 20:20:05+00:00
- **Authors**: Yinong Wang, Jianhua Yao, Joseph E. Burns, Ronald M. Summers
- **Comment**: Contributed 4-Page Paper to be presented at the 2016 IEEE
  International Symposium on Biomedical Imaging (ISBI), April 13-16, 2016,
  Prague, Czech Republic
- **Journal**: None
- **Summary**: Classification of vertebral compression fractures (VCF) having osteoporotic or neoplastic origin is fundamental to the planning of treatment. We developed a fracture classification system by acquiring quantitative morphologic and bone density determinants of fracture progression through the use of automated measurements from longitudinal studies. A total of 250 CT studies were acquired for the task, each having previously identified VCFs with osteoporosis or neoplasm. Thirty-six features or each identified VCF were computed and classified using a committee of support vector machines. Ten-fold cross validation on 695 identified fractured vertebrae showed classification accuracies of 0.812, 0.665, and 0.820 for the measured, longitudinal, and combined feature sets respectively.



### Locally-Supervised Deep Hybrid Model for Scene Recognition
- **Arxiv ID**: http://arxiv.org/abs/1601.07576v2
- **DOI**: 10.1109/TIP.2016.2629443
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.07576v2)
- **Published**: 2016-01-27 21:32:15+00:00
- **Updated**: 2016-12-15 21:30:09+00:00
- **Authors**: Sheng Guo, Weilin Huang, Limin Wang, Yu Qiao
- **Comment**: To appear in IEEE Trans. on Image Processing, 2017
- **Journal**: None
- **Summary**: Convolutional neural networks (CNN) have recently achieved remarkable successes in various image classification and understanding tasks. The deep features obtained at the top fully-connected layer of the CNN (FC-features) exhibit rich global semantic information and are extremely effective in image classification. On the other hand, the convolutional features in the middle layers of the CNN also contain meaningful local information, but are not fully explored for image representation. In this paper, we propose a novel Locally-Supervised Deep Hybrid Model (LS-DHM) that effectively enhances and explores the convolutional features for scene recognition. Firstly, we notice that the convolutional features capture local objects and fine structures of scene images, which yield important cues for discriminating ambiguous scenes, whereas these features are significantly eliminated in the highly-compressed FC representation. Secondly, we propose a new Local Convolutional Supervision (LCS) layer to enhance the local structure of the image by directly propagating the label information to the convolutional layers. Thirdly, we propose an efficient Fisher Convolutional Vector (FCV) that successfully rescues the orderless mid-level semantic information (e.g. objects and textures) of scene image. The FCV encodes the large-sized convolutional maps into a fixed-length mid-level representation, and is demonstrated to be strongly complementary to the high-level FC-features. Finally, both the FCV and FC-features are collaboratively employed in the LSDHM representation, which achieves outstanding performance in our experiments. It obtains 83.75% and 67.56% accuracies respectively on the heavily benchmarked MIT Indoor67 and SUN397 datasets, advancing the stat-of-the-art substantially.



