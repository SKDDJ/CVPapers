# Arxiv Papers in cs.CV on 2016-01-26
### ReconNet: Non-Iterative Reconstruction of Images from Compressively Sensed Random Measurements
- **Arxiv ID**: http://arxiv.org/abs/1601.06892v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.06892v2)
- **Published**: 2016-01-26 05:17:14+00:00
- **Updated**: 2016-03-07 23:31:08+00:00
- **Authors**: Kuldeep Kulkarni, Suhas Lohit, Pavan Turaga, Ronan Kerviche, Amit Ashok
- **Comment**: Accepted at IEEE International Conference on Computer Vision and
  Pattern Recognition (CVPR), 2016
- **Journal**: None
- **Summary**: The goal of this paper is to present a non-iterative and more importantly an extremely fast algorithm to reconstruct images from compressively sensed (CS) random measurements. To this end, we propose a novel convolutional neural network (CNN) architecture which takes in CS measurements of an image as input and outputs an intermediate reconstruction. We call this network, ReconNet. The intermediate reconstruction is fed into an off-the-shelf denoiser to obtain the final reconstructed image. On a standard dataset of images we show significant improvements in reconstruction results (both in terms of PSNR and time complexity) over state-of-the-art iterative CS reconstruction algorithms at various measurement rates. Further, through qualitative experiments on real data collected using our block single pixel camera (SPC), we show that our network is highly robust to sensor noise and can recover visually better quality images than competitive algorithms at extremely low sensing rates of 0.1 and 0.04. To demonstrate that our algorithm can recover semantically informative images even at a low measurement rate of 0.01, we present a very robust proof of concept real-time visual tracking application.



### Classification and Verification of Online Handwritten Signatures with Time Causal Information Theory Quantifiers
- **Arxiv ID**: http://arxiv.org/abs/1601.06925v1
- **DOI**: 10.1371/journal.pone.0166868
- **Categories**: **cs.IT**, cs.CV, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1601.06925v1)
- **Published**: 2016-01-26 08:40:55+00:00
- **Updated**: 2016-01-26 08:40:55+00:00
- **Authors**: Osvaldo A. Rosso, Raydonal Ospina, Alejandro C. Frery
- **Comment**: Submitted to PLOS One
- **Journal**: None
- **Summary**: We present a new approach for online handwritten signature classification and verification based on descriptors stemming from Information Theory. The proposal uses the Shannon Entropy, the Statistical Complexity, and the Fisher Information evaluated over the Bandt and Pompe symbolization of the horizontal and vertical coordinates of signatures. These six features are easy and fast to compute, and they are the input to an One-Class Support Vector Machine classifier. The results produced surpass state-of-the-art techniques that employ higher-dimensional feature spaces which often require specialized software and hardware. We assess the consistency of our proposal with respect to the size of the training sample, and we also use it to classify the signatures into meaningful groups.



### Fisher Motion Descriptor for Multiview Gait Recognition
- **Arxiv ID**: http://arxiv.org/abs/1601.06931v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1601.06931v1)
- **Published**: 2016-01-26 09:05:26+00:00
- **Updated**: 2016-01-26 09:05:26+00:00
- **Authors**: F. M. Castro, M. J. Marín-Jiménez, N. Guil, R. Muñoz-Salinas
- **Comment**: This paper extends with new experiments the one published at
  ICPR'2014
- **Journal**: None
- **Summary**: The goal of this paper is to identify individuals by analyzing their gait. Instead of using binary silhouettes as input data (as done in many previous works) we propose and evaluate the use of motion descriptors based on densely sampled short-term trajectories. We take advantage of state-of-the-art people detectors to define custom spatial configurations of the descriptors around the target person, obtaining a rich representation of the gait motion. The local motion features (described by the Divergence-Curl-Shear descriptor) extracted on the different spatial areas of the person are combined into a single high-level gait descriptor by using the Fisher Vector encoding. The proposed approach, coined Pyramidal Fisher Motion, is experimentally validated on `CASIA' dataset (parts B and C), `TUM GAID' dataset, `CMU MoBo' dataset and the recent `AVA Multiview Gait' dataset. The results show that this new approach achieves state-of-the-art results in the problem of gait recognition, allowing to recognize walking people from diverse viewpoints on single and multiple camera setups, wearing different clothes, carrying bags, walking at diverse speeds and not limited to straight walking paths.



### Virtual Rephotography: Novel View Prediction Error for 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1601.06950v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, I.3.7
- **Links**: [PDF](http://arxiv.org/pdf/1601.06950v1)
- **Published**: 2016-01-26 09:57:34+00:00
- **Updated**: 2016-01-26 09:57:34+00:00
- **Authors**: Michael Waechter, Mate Beljan, Simon Fuhrmann, Nils Moehrle, Johannes Kopf, Michael Goesele
- **Comment**: 10 pages, 12 figures, paper was submitted to ACM Transactions on
  Graphics for review
- **Journal**: None
- **Summary**: The ultimate goal of many image-based modeling systems is to render photo-realistic novel views of a scene without visible artifacts. Existing evaluation metrics and benchmarks focus mainly on the geometric accuracy of the reconstructed model, which is, however, a poor predictor of visual accuracy. Furthermore, using only geometric accuracy by itself does not allow evaluating systems that either lack a geometric scene representation or utilize coarse proxy geometry. Examples include light field or image-based rendering systems. We propose a unified evaluation approach based on novel view prediction error that is able to analyze the visual quality of any method that can render novel views from input images. One of the key advantages of this approach is that it does not require ground truth geometry. This dramatically simplifies the creation of test datasets and benchmarks. It also allows us to evaluate the quality of an unknown scene during the acquisition and reconstruction process, which is useful for acquisition planning. We evaluate our approach on a range of methods including standard geometry-plus-texture pipelines as well as image-based rendering techniques, compare it to existing geometry-based benchmarks, and demonstrate its utility for a range of use cases.



### Hough-CNN: Deep Learning for Segmentation of Deep Brain Regions in MRI and Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/1601.07014v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.07014v3)
- **Published**: 2016-01-26 13:25:01+00:00
- **Updated**: 2016-01-31 19:35:15+00:00
- **Authors**: Fausto Milletari, Seyed-Ahmad Ahmadi, Christine Kroll, Annika Plate, Verena Rozanski, Juliana Maiostre, Johannes Levin, Olaf Dietrich, Birgit Ertl-Wagner, Kai Bötzel, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we propose a novel approach to perform segmentation by leveraging the abstraction capabilities of convolutional neural networks (CNNs). Our method is based on Hough voting, a strategy that allows for fully automatic localisation and segmentation of the anatomies of interest. This approach does not only use the CNN classification outcomes, but it also implements voting by exploiting the features produced by the deepest portion of the network. We show that this learning-based segmentation method is robust, multi-region, flexible and can be easily adapted to different modalities. In the attempt to show the capabilities and the behaviour of CNNs when they are applied to medical image analysis, we perform a systematic study of the performances of six different network architectures, conceived according to state-of-the-art criteria, in various situations. We evaluate the impact of both different amount of training data and different data dimensionality (2D, 2.5D and 3D) on the final results. We show results on both MRI and transcranial US volumes depicting respectively 26 regions of the basal ganglia and the midbrain.



### Polyhedron Volume-Ratio-based Classification for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1601.07021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.07021v1)
- **Published**: 2016-01-26 13:41:48+00:00
- **Updated**: 2016-01-26 13:41:48+00:00
- **Authors**: Qingxiang Feng, Jeng-Shyang Pan, Jar-Ferr Yang, Yang-Ting Chou
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a novel method, called polyhedron volume ratio classification (PVRC) is proposed for image recognition



### COCO-Text: Dataset and Benchmark for Text Detection and Recognition in Natural Images
- **Arxiv ID**: http://arxiv.org/abs/1601.07140v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.07140v2)
- **Published**: 2016-01-26 19:30:34+00:00
- **Updated**: 2016-06-19 23:52:14+00:00
- **Authors**: Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, Serge Belongie
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes the COCO-Text dataset. In recent years large-scale datasets like SUN and Imagenet drove the advancement of scene understanding and object recognition. The goal of COCO-Text is to advance state-of-the-art in text detection and recognition in natural images. The dataset is based on the MS COCO dataset, which contains images of complex everyday scenes. The images were not collected with text in mind and thus contain a broad variety of text instances. To reflect the diversity of text in natural scenes, we annotate text with (a) location in terms of a bounding box, (b) fine-grained classification into machine printed text and handwritten text, (c) classification into legible and illegible text, (d) script of the text and (e) transcriptions of legible text. The dataset contains over 173k text annotations in over 63k images. We provide a statistical analysis of the accuracy of our annotations. In addition, we present an analysis of three leading state-of-the-art photo Optical Character Recognition (OCR) approaches on our dataset. While scene text detection and recognition enjoys strong advances in recent years, we identify significant shortcomings motivating future work.



