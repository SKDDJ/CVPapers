# Arxiv Papers in cs.CV on 2016-01-31
### Tracing liquid level and material boundaries in transparent vessels using the graph cut computer vision approach
- **Arxiv ID**: http://arxiv.org/abs/1602.00177v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.00177v1)
- **Published**: 2016-01-31 00:20:39+00:00
- **Updated**: 2016-01-31 00:20:39+00:00
- **Authors**: Sagi Eppel
- **Comment**: None
- **Journal**: None
- **Summary**: Detection of boundaries of materials stored in transparent vessels is essential for identifying properties such as liquid level and phase boundaries, which are vital for controlling numerous processes in the industry and chemistry laboratory. This work presents a computer vision method for identifying the boundary of materials in transparent vessels using the graph-cut algorithm. The method receives an image of a transparent vessel containing a material and the contour of the vessel in the image. The boundary of the material in the vessel is found by the graph cut method. In general the method uses the vessel region of the image to create a graph, where pixels are vertices, and the cost of an edge between two pixels is inversely correlated with their intensity difference. The bottom 10% of the vessel region in the image is assumed to correspond to the material phase and defined as the graph and source. The top 10% of the pixels in the vessels are assumed to correspond to the air phase and defined as the graph sink. The minimal cut that splits the resulting graph between the source and sink (hence, material and air) is traced using the max-flow/min-cut approach. This cut corresponds to the boundary of the material in the image. The method gave high accuracy in boundary recognition for a wide range of liquid, solid, granular and powder materials in various glass vessels from everyday life and the chemistry laboratory, such as bottles, jars, Glasses, Chromotography colums and separatory funnels.



### Unsupervised Deep Hashing for Large-scale Visual Search
- **Arxiv ID**: http://arxiv.org/abs/1602.00206v1
- **DOI**: 10.1109/IPTA.2016.7821007
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1602.00206v1)
- **Published**: 2016-01-31 06:36:47+00:00
- **Updated**: 2016-01-31 06:36:47+00:00
- **Authors**: Zhaoqiang Xia, Xiaoyi Feng, Jinye Peng, Abdenour Hadid
- **Comment**: None
- **Journal**: 2016 6th International Conference on Image Processing Theory Tools
  and Applications (IPTA)
- **Summary**: Learning based hashing plays a pivotal role in large-scale visual search. However, most existing hashing algorithms tend to learn shallow models that do not seek representative binary codes. In this paper, we propose a novel hashing approach based on unsupervised deep learning to hierarchically transform features into hash codes. Within the heterogeneous deep hashing framework, the autoencoder layers with specific constraints are considered to model the nonlinear mapping between features and binary codes. Then, a Restricted Boltzmann Machine (RBM) layer with constraints is utilized to reduce the dimension in the hamming space. Extensive experiments on the problem of visual search demonstrate the competitiveness of our proposed approach compared to state-of-the-art.



### Trainlets: Dictionary Learning in High Dimensions
- **Arxiv ID**: http://arxiv.org/abs/1602.00212v4
- **DOI**: 10.1109/TSP.2016.2540599
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.00212v4)
- **Published**: 2016-01-31 08:24:24+00:00
- **Updated**: 2016-05-12 16:37:09+00:00
- **Authors**: Jeremias Sulam, Boaz Ophir, Michael Zibulevsky, Michael Elad
- **Comment**: None
- **Journal**: None
- **Summary**: Sparse representations has shown to be a very powerful model for real world signals, and has enabled the development of applications with notable performance. Combined with the ability to learn a dictionary from signal examples, sparsity-inspired algorithms are often achieving state-of-the-art results in a wide variety of tasks. Yet, these methods have traditionally been restricted to small dimensions mainly due to the computational constraints that the dictionary learning problem entails. In the context of image processing, this implies handling small image patches. In this work we show how to efficiently handle bigger dimensions and go beyond the small patches in sparsity-based signal and image processing methods. We build our approach based on a new cropped wavelet decomposition, which enables a multi-scale analysis with virtually no border effects. We then employ this as the base dictionary within a double sparsity model to enable the training of adaptive dictionaries. To cope with the increase of training data, while at the same time improving the training performance, we present an Online Sparse Dictionary Learning (OSDL) algorithm to train this model effectively, enabling it to handle millions of examples. This work shows that dictionary learning can be up-scaled to tackle a new level of signal dimensions, obtaining large adaptable atoms that we call trainlets.



### Dimensionality Reduction via Regression in Hyperspectral Imagery
- **Arxiv ID**: http://arxiv.org/abs/1602.00214v1
- **DOI**: 10.1109/JSTSP.2015.2417833
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1602.00214v1)
- **Published**: 2016-01-31 09:34:58+00:00
- **Updated**: 2016-01-31 09:34:58+00:00
- **Authors**: Valero Laparra, Jesus Malo, Gustau Camps-Valls
- **Comment**: 12 pages, 6 figures, 62 references
- **Journal**: J. Sel. Topics Signal Processing 9(6): 1026-1036 (2015)
- **Summary**: This paper introduces a new unsupervised method for dimensionality reduction via regression (DRR). The algorithm belongs to the family of invertible transforms that generalize Principal Component Analysis (PCA) by using curvilinear instead of linear features. DRR identifies the nonlinear features through multivariate regression to ensure the reduction in redundancy between he PCA coefficients, the reduction of the variance of the scores, and the reduction in the reconstruction error. More importantly, unlike other nonlinear dimensionality reduction methods, the invertibility, volume-preservation, and straightforward out-of-sample extension, makes DRR interpretable and easy to apply. The properties of DRR enable learning a more broader class of data manifolds than the recently proposed Non-linear Principal Components Analysis (NLPCA) and Principal Polynomial Analysis (PPA). We illustrate the performance of the representation in reducing the dimensionality of remote sensing data. In particular, we tackle two common problems: processing very high dimensional spectral information such as in hyperspectral image sounding data, and dealing with spatial-spectral image patches of multispectral images. Both settings pose collinearity and ill-determination problems. Evaluation of the expressive power of the features is assessed in terms of truncation error, estimating atmospheric variables, and surface land cover classification error. Results show that DRR outperforms linear PCA and recently proposed invertible extensions based on neural networks (NLPCA) and univariate regressions (PPA).



### Image Denoising with Kernels based on Natural Image Relations
- **Arxiv ID**: http://arxiv.org/abs/1602.00217v1
- **DOI**: 10.1145/1756006.1756035
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1602.00217v1)
- **Published**: 2016-01-31 10:02:14+00:00
- **Updated**: 2016-01-31 10:02:14+00:00
- **Authors**: Valero Laparra, Juan Gutiérrez, Gustavo Camps-Valls, Jesús Malo
- **Comment**: None
- **Journal**: Journal of Machine Learning Research 11: 873-903 (2010)
- **Summary**: A successful class of image denoising methods is based on Bayesian approaches working in wavelet representations. However, analytical estimates can be obtained only for particular combinations of analytical models of signal and noise, thus precluding its straightforward extension to deal with other arbitrary noise sources. In this paper, we propose an alternative non-explicit way to take into account the relations among natural image wavelet coefficients for denoising: we use support vector regression (SVR) in the wavelet domain to enforce these relations in the estimated signal. Since relations among the coefficients are specific to the signal, the regularization property of SVR is exploited to remove the noise, which does not share this feature. The specific signal relations are encoded in an anisotropic kernel obtained from mutual information measures computed on a representative image database. Training considers minimizing the Kullback-Leibler divergence (KLD) between the estimated and actual probability functions of signal and noise in order to enforce similarity. Due to its non-parametric nature, the method can eventually cope with different noise sources without the need of an explicit re-formulation, as it is strictly necessary under parametric Bayesian formalisms. Results under several noise levels and noise sources show that: (1) the proposed method outperforms conventional wavelet methods that assume coefficient independence, (2) it is similar to state-of-the-art methods that do explicitly include these relations when the noise source is Gaussian, and (3) it gives better numerical and visual performance when more complex, realistic noise sources are considered. Therefore, the proposed machine learning approach can be seen as a more flexible (model-free) alternative to the explicit description of wavelet coefficient relations for image denoising.



### Order-aware Convolutional Pooling for Video Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1602.00224v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.00224v1)
- **Published**: 2016-01-31 10:58:11+00:00
- **Updated**: 2016-01-31 10:58:11+00:00
- **Authors**: Peng Wang, Lingqiao Liu, Chunhua Shen, Heng Tao Shen
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Most video based action recognition approaches create the video-level representation by temporally pooling the features extracted at each frame. The pooling methods that they adopt, however, usually completely or partially neglect the dynamic information contained in the temporal domain, which may undermine the discriminative power of the resulting video representation since the video sequence order could unveil the evolution of a specific event or action. To overcome this drawback and explore the importance of incorporating the temporal order information, in this paper we propose a novel temporal pooling approach to aggregate the frame-level features. Inspired by the capacity of Convolutional Neural Networks (CNN) in making use of the internal structure of images for information abstraction, we propose to apply the temporal convolution operation to the frame-level representations to extract the dynamic information. However, directly implementing this idea on the original high-dimensional feature would inevitably result in parameter explosion.   To tackle this problem, we view the temporal evolution of the feature value at each feature dimension as a 1D signal and learn a unique convolutional filter bank for each of these 1D signals. We conduct experiments on two challenging video-based action recognition datasets, HMDB51 and UCF101; and demonstrate that the proposed method is superior to the conventional pooling methods.



### Bit-Planes: Dense Subpixel Alignment of Binary Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1602.00307v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.00307v1)
- **Published**: 2016-01-31 19:51:11+00:00
- **Updated**: 2016-01-31 19:51:11+00:00
- **Authors**: Hatem Alismail, Brett Browning, Simon Lucey
- **Comment**: 10 pages. In submission
- **Journal**: None
- **Summary**: Binary descriptors have been instrumental in the recent evolution of computationally efficient sparse image alignment algorithms. Increasingly, however, the vision community is interested in dense image alignment methods, which are more suitable for estimating correspondences from high frame rate cameras as they do not rely on exhaustive search. However, classic dense alignment approaches are sensitive to illumination change. In this paper, we propose an easy to implement and low complexity dense binary descriptor, which we refer to as bit-planes, that can be seamlessly integrated within a multi-channel Lucas & Kanade framework. This novel approach combines the robustness of binary descriptors with the speed and accuracy of dense alignment methods. The approach is demonstrated on a template tracking problem achieving state-of-the-art robustness and faster than real-time performance on consumer laptops (400+ fps on a single core Intel i7) and hand-held mobile devices (100+ fps on an iPad Air 2).



### Learning a low-rank shared dictionary for object classification
- **Arxiv ID**: http://arxiv.org/abs/1602.00310v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.00310v2)
- **Published**: 2016-01-31 19:55:08+00:00
- **Updated**: 2016-05-18 00:36:39+00:00
- **Authors**: Tiep H. Vu, Vishal Monga
- **Comment**: 4 page + 1 reference page
- **Journal**: None
- **Summary**: Despite the fact that different objects possess distinct class-specific features, they also usually share common patterns. Inspired by this observation, we propose a novel method to explicitly and simultaneously learn a set of common patterns as well as class-specific features for classification. Our dictionary learning framework is hence characterized by both a shared dictionary and particular (class-specific) dictionaries. For the shared dictionary, we enforce a low-rank constraint, i.e. claim that its spanning subspace should have low dimension and the coefficients corresponding to this dictionary should be similar. For the particular dictionaries, we impose on them the well-known constraints stated in the Fisher discrimination dictionary learning (FDDL). Further, we propose a new fast and accurate algorithm to solve the sparse coding problems in the learning step, accelerating its convergence. The said algorithm could also be applied to FDDL and its extensions. Experimental results on widely used image databases establish the advantages of our method over state-of-the-art dictionary learning methods.



### Novel Views of Objects from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1602.00328v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1602.00328v2)
- **Published**: 2016-01-31 21:43:13+00:00
- **Updated**: 2016-08-15 03:03:50+00:00
- **Authors**: Konstantinos Rematas, Chuong Nguyen, Tobias Ritschel, Mario Fritz, Tinne Tuytelaars
- **Comment**: to appear in PAMI 2016
- **Journal**: None
- **Summary**: Taking an image of an object is at its core a lossy process. The rich information about the three-dimensional structure of the world is flattened to an image plane and decisions such as viewpoint and camera parameters are final and not easily revertible. As a consequence, possibilities of changing viewpoint are limited. Given a single image depicting an object, novel-view synthesis is the task of generating new images that render the object from a different viewpoint than the one given. The main difficulty is to synthesize the parts that are disoccluded; disocclusion occurs when parts of an object are hidden by the object itself under a specific viewpoint. In this work, we show how to improve novel-view synthesis by making use of the correlations observed in 3D models and applying them to new image instances. We propose a technique to use the structural information extracted from a 3D model that matches the image object in terms of viewpoint and shape. For the latter part, we propose an efficient 2D-to-3D alignment method that associates precisely the image appearance with the 3D model geometry with minimal user interaction. Our technique is able to simulate plausible viewpoint changes for a variety of object classes within seconds. Additionally, we show that our synthesized images can be used as additional training data that improves the performance of standard object detectors.



