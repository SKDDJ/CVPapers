# Arxiv Papers in cs.CV on 2016-03-31
### Accurate Text Localization in Natural Image with Cascaded Convolutional Text Network
- **Arxiv ID**: http://arxiv.org/abs/1603.09423v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.09423v1)
- **Published**: 2016-03-31 00:16:31+00:00
- **Updated**: 2016-03-31 00:16:31+00:00
- **Authors**: Tong He, Weilin Huang, Yu Qiao, Jian Yao
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a new top-down pipeline for scene text detection. We propose a novel Cascaded Convolutional Text Network (CCTN) that joints two customized convolutional networks for coarse-to-fine text localization. The CCTN fast detects text regions roughly from a low-resolution image, and then accurately localizes text lines from each enlarged region. We cast previous character based detection into direct text region estimation, avoiding multiple bottom- up post-processing steps. It exhibits surprising robustness and discriminative power by considering whole text region as detection object which provides strong semantic information. We customize convolutional network by develop- ing rectangle convolutions and multiple in-network fusions. This enables it to handle multi-shape and multi-scale text efficiently. Furthermore, the CCTN is computationally efficient by sharing convolutional computations, and high-level property allows it to be invariant to various languages and multiple orientations. It achieves 0.84 and 0.86 F-measures on the ICDAR 2011 and ICDAR 2013, delivering substantial improvements over state-of-the-art results [23, 1].



### The Open World of Micro-Videos
- **Arxiv ID**: http://arxiv.org/abs/1603.09439v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.09439v2)
- **Published**: 2016-03-31 02:19:53+00:00
- **Updated**: 2016-04-01 01:53:32+00:00
- **Authors**: Phuc Xuan Nguyen, Gregory Rogez, Charless Fowlkes, Deva Ramanan
- **Comment**: None
- **Journal**: None
- **Summary**: Micro-videos are six-second videos popular on social media networks with several unique properties. Firstly, because of the authoring process, they contain significantly more diversity and narrative structure than existing collections of video "snippets". Secondly, because they are often captured by hand-held mobile cameras, they contain specialized viewpoints including third-person, egocentric, and self-facing views seldom seen in traditional produced video. Thirdly, due to to their continuous production and publication on social networks, aggregate micro-video content contains interesting open-world dynamics that reflects the temporal evolution of tag topics. These aspects make micro-videos an appealing well of visual data for developing large-scale models for video understanding. We analyze a novel dataset of micro-videos labeled with 58 thousand tags. To analyze this data, we introduce viewpoint-specific and temporally-evolving models for video understanding, defined over state-of-the-art motion and deep visual features. We conclude that our dataset opens up new research opportunities for large-scale video analysis, novel viewpoints, and open-world dynamics.



### Object Skeleton Extraction in Natural Images by Fusing Scale-associated Deep Side Outputs
- **Arxiv ID**: http://arxiv.org/abs/1603.09446v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.09446v2)
- **Published**: 2016-03-31 03:21:33+00:00
- **Updated**: 2016-04-06 05:51:33+00:00
- **Authors**: Wei Shen, Kai Zhao, Yuan Jiang, Yan Wang, Zhijiang Zhang, Xiang Bai
- **Comment**: Accepted by CVPR2016
- **Journal**: None
- **Summary**: Object skeleton is a useful cue for object detection, complementary to the object contour, as it provides a structural representation to describe the relationship among object parts. While object skeleton extraction in natural images is a very challenging problem, as it requires the extractor to be able to capture both local and global image context to determine the intrinsic scale of each skeleton pixel. Existing methods rely on per-pixel based multi-scale feature computation, which results in difficult modeling and high time consumption. In this paper, we present a fully convolutional network with multiple scale-associated side outputs to address this problem. By observing the relationship between the receptive field sizes of the sequential stages in the network and the skeleton scales they can capture, we introduce a scale-associated side output to each stage. We impose supervision to different stages by guiding the scale-associated side outputs toward groundtruth skeletons of different scales. The responses of the multiple scale-associated side outputs are then fused in a scale-specific way to localize skeleton pixels with multiple scales effectively. Our method achieves promising results on two skeleton extraction datasets, and significantly outperforms other competitors.



### Exemplar-AMMs: Recognizing Crowd Movements from Pedestrian Trajectories
- **Arxiv ID**: http://arxiv.org/abs/1603.09454v1
- **DOI**: 10.1109/TMM.2016.2598091
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.09454v1)
- **Published**: 2016-03-31 04:58:25+00:00
- **Updated**: 2016-03-31 04:58:25+00:00
- **Authors**: Wenxi Liu, Rynson W. H. Lau, Xiaogang Wang, Dinesh Manocha
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel method to recognize the types of crowd movement from crowd trajectories using agent-based motion models (AMMs). Our idea is to apply a number of AMMs, referred to as exemplar-AMMs, to describe the crowd movement. Specifically, we propose an optimization framework that filters out the unknown noise in the crowd trajectories and measures their similarity to the exemplar-AMMs to produce a crowd motion feature. We then address our real-world crowd movement recognition problem as a multi-label classification problem. Our experiments show that the proposed feature outperforms the state-of-the-art methods in recognizing both simulated and real-world crowd movements from their trajectories. Finally, we have created a synthetic dataset, SynCrowd, which contains 2D crowd trajectories in various scenarios, generated by various crowd simulators. This dataset can serve as a training set or benchmark for crowd analysis work.



### Robust Uncalibrated Stereo Rectification with Constrained Geometric Distortions (USR-CGD)
- **Arxiv ID**: http://arxiv.org/abs/1603.09462v1
- **DOI**: 10.1016/j.imavis.2017.01.001
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.09462v1)
- **Published**: 2016-03-31 06:02:23+00:00
- **Updated**: 2016-03-31 06:02:23+00:00
- **Authors**: Hyunsuk Ko, Han Suk Shim, Ouk Choi, C. -C. Jay Kuo
- **Comment**: None
- **Journal**: Image and Vision Computing, 2017
- **Summary**: A novel algorithm for uncalibrated stereo image-pair rectification under the constraint of geometric distortion, called USR-CGD, is presented in this work. Although it is straightforward to define a rectifying transformation (or homography) given the epipolar geometry, many existing algorithms have unwanted geometric distortions as a side effect. To obtain rectified images with reduced geometric distortions while maintaining a small rectification error, we parameterize the homography by considering the influence of various kinds of geometric distortions. Next, we define several geometric measures and incorporate them into a new cost function for parameter optimization. Finally, we propose a constrained adaptive optimization scheme to allow a balanced performance between the rectification error and the geometric error. Extensive experimental results are provided to demonstrate the superb performance of the proposed USR-CGD method, which outperforms existing algorithms by a significant margin.



### A ParaBoost Stereoscopic Image Quality Assessment (PBSIQA) System
- **Arxiv ID**: http://arxiv.org/abs/1603.09469v1
- **DOI**: 10.1016/j.jvcir.2017.02.014
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1603.09469v1)
- **Published**: 2016-03-31 06:55:25+00:00
- **Updated**: 2016-03-31 06:55:25+00:00
- **Authors**: Hyunsuk Ko, Rui Song, C. -C. Jay Kuo
- **Comment**: None
- **Journal**: Journal of Visual Communication and Image Representation, 2017
- **Summary**: The problem of stereoscopic image quality assessment, which finds applications in 3D visual content delivery such as 3DTV, is investigated in this work. Specifically, we propose a new ParaBoost (parallel-boosting) stereoscopic image quality assessment (PBSIQA) system. The system consists of two stages. In the first stage, various distortions are classified into a few types, and individual quality scorers targeting at a specific distortion type are developed. These scorers offer complementary performance in face of a database consisting of heterogeneous distortion types. In the second stage, scores from multiple quality scorers are fused to achieve the best overall performance, where the fuser is designed based on the parallel boosting idea borrowed from machine learning. Extensive experimental results are conducted to compare the performance of the proposed PBSIQA system with those of existing stereo image quality assessment (SIQA) metrics. The developed quality metric can serve as an objective function to optimize the performance of a 3D content delivery system.



### Learning Compatibility Across Categories for Heterogeneous Item Recommendation
- **Arxiv ID**: http://arxiv.org/abs/1603.09473v3
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1603.09473v3)
- **Published**: 2016-03-31 07:22:30+00:00
- **Updated**: 2016-09-29 00:43:21+00:00
- **Authors**: Ruining He, Charles Packer, Julian McAuley
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: Identifying relationships between items is a key task of an online recommender system, in order to help users discover items that are functionally complementary or visually compatible. In domains like clothing recommendation, this task is particularly challenging since a successful system should be capable of handling a large corpus of items, a huge amount of relationships among them, as well as the high-dimensional and semantically complicated features involved. Furthermore, the human notion of "compatibility" to capture goes beyond mere similarity: For two items to be compatible---whether jeans and a t-shirt, or a laptop and a charger---they should be similar in some ways, but systematically different in others.   In this paper we propose a novel method, Monomer, to learn complicated and heterogeneous relationships between items in product recommendation settings. Recently, scalable methods have been developed that address this task by learning similarity metrics on top of the content of the products involved. Here our method relaxes the metricity assumption inherent in previous work and models multiple localized notions of 'relatedness,' so as to uncover ways in which related items should be systematically similar, and systematically different. Quantitatively, we show that our system achieves state-of-the-art performance on large-scale compatibility prediction tasks, especially in cases where there is substantial heterogeneity between related items. Qualitatively, we demonstrate that richer notions of compatibility can be learned that go beyond similarity, and that our model can make effective recommendations of heterogeneous content.



### Sub-pixel accuracy edge fitting by means of B-spline
- **Arxiv ID**: http://arxiv.org/abs/1603.09558v1
- **DOI**: 10.1109/MMSP.2009.5293265
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.09558v1)
- **Published**: 2016-03-31 12:37:58+00:00
- **Updated**: 2016-03-31 12:37:58+00:00
- **Authors**: R. L. B. Breder, Vania V. Estrela, J. T. de Assis
- **Comment**: 5 pages, Proceedings of the MMSP '09. IEEE International Workshop on
  Multimedia Signal Processing, ISBN 978-1-4244-4463-2
- **Journal**: None
- **Summary**: Local perturbations around contours strongly disturb the final result of computer vision tasks. It is common to introduce a priori information in the estimation process. Improvement can be achieved via a deformable model such as the snake model. In recent works, the deformable contour is modeled by means of B-spline snakes which allows local control, concise representation, and the use of fewer parameters. The estimation of the sub-pixel edges using a global B-spline model relies on the contour global determination according to a maximum likelihood framework and using the observed data likelihood. This procedure guarantees that the noisiest data will be filtered out. The data likelihood is computed as a consequence of the observation model which includes both orientation and position information. Comparative experiments of this algorithm and the classical spline interpolation have shown that the proposed algorithm outperforms the classical approach for Gaussian and Salt & Pepper noise.



### Total Variation Applications in Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1603.09599v1
- **DOI**: 10.4018/978-1-4666-8654-0.ch002
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.09599v1)
- **Published**: 2016-03-31 14:08:53+00:00
- **Updated**: 2016-03-31 14:08:53+00:00
- **Authors**: Vania V. Estrela, Hermes Aguiar Magalhaes, Osamu Saotome
- **Comment**: 24 pages, Book Title: Handbook of Research on Emerging Perspectives
  in Intelligent Pattern Recognition, Analysis, and Image Processing, Editor
  Narendra Kumar Kamila, IGI Global, 2016,
  http://www.igi-global.com/chapter/total-variation-applications-in-computer-vision/141626
- **Journal**: None
- **Summary**: The objectives of this chapter are: (i) to introduce a concise overview of regularization; (ii) to define and to explain the role of a particular type of regularization called total variation norm (TV-norm) in computer vision tasks; (iii) to set up a brief discussion on the mathematical background of TV methods; and (iv) to establish a relationship between models and a few existing methods to solve problems cast as TV-norm. For the most part, image-processing algorithms blur the edges of the estimated images, however TV regularization preserves the edges with no prior information on the observed and the original images. The regularization scalar parameter {\lambda} controls the amount of regularization allowed and it is an essential to obtain a high-quality regularized output. A wide-ranging review of several ways to put into practice TV regularization as well as its advantages and limitations are discussed.



### Large Scale Deep Convolutional Neural Network Features Search with Lucene
- **Arxiv ID**: http://arxiv.org/abs/1603.09687v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1603.09687v4)
- **Published**: 2016-03-31 17:11:43+00:00
- **Updated**: 2016-07-20 09:29:57+00:00
- **Authors**: Claudio Gennaro
- **Comment**: This paper has been withdrawn by the author due to many errors
- **Journal**: None
- **Summary**: In this work, we propose an approach to index Deep Convolutional Neural Network Features to support efficient content-based retrieval on large image databases. To this aim, we have converted the these features into a textual form, to index them into an inverted index by means of Lucene. In this way, we were able to set up a robust retrieval system that combines full-text search with content-based image retrieval capabilities. We evaluated different strategies of textual representation in order to optimize the index occupation and the query response time. In order to show that our approach is able to handle large datasets, we have developed a web-based prototype that provides an interface for combined textual and visual searching into a dataset of about 100 million of images.



### Audio-Visual Speaker Diarization Based on Spatiotemporal Bayesian Fusion
- **Arxiv ID**: http://arxiv.org/abs/1603.09725v2
- **DOI**: 10.1109/TPAMI.2017.2648793
- **Categories**: **cs.CV**, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1603.09725v2)
- **Published**: 2016-03-31 19:15:01+00:00
- **Updated**: 2016-11-03 16:40:24+00:00
- **Authors**: Israel D. Gebru, Silèye Ba, Xiaofei Li, Radu Horaud
- **Comment**: 14 pages, 6 figures, 5 tables
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  40(6), 1086 - 1099, 2018
- **Summary**: Speaker diarization consists of assigning speech signals to people engaged in a dialogue. An audio-visual spatiotemporal diarization model is proposed. The model is well suited for challenging scenarios that consist of several participants engaged in multi-party interaction while they move around and turn their heads towards the other participants rather than facing the cameras and the microphones. Multiple-person visual tracking is combined with multiple speech-source localization in order to tackle the speech-to-person association problem. The latter is solved within a novel audio-visual fusion method on the following grounds: binaural spectral features are first extracted from a microphone pair, then a supervised audio-visual alignment technique maps these features onto an image, and finally a semi-supervised clustering method assigns binaural spectral features to visible persons. The main advantage of this method over previous work is that it processes in a principled way speech signals uttered simultaneously by multiple persons. The diarization itself is cast into a latent-variable temporal graphical model that infers speaker identities and speech turns, based on the output of an audio-visual association process, executed at each time slice, and on the dynamics of the diarization variable itself. The proposed formulation yields an efficient exact inference procedure. A novel dataset, that contains audio-visual training data as well as a number of scenarios involving several participants engaged in formal and informal dialogue, is introduced. The proposed method is thoroughly tested and benchmarked with respect to several state-of-the art diarization algorithms.



### Robust Head-Pose Estimation Based on Partially-Latent Mixture of Linear Regressions
- **Arxiv ID**: http://arxiv.org/abs/1603.09732v3
- **DOI**: 10.1109/TIP.2017.2654165
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.09732v3)
- **Published**: 2016-03-31 19:32:52+00:00
- **Updated**: 2017-03-06 11:18:47+00:00
- **Authors**: Vincent Drouard, Radu Horaud, Antoine Deleforge, Silèye Ba, Georgios Evangelidis
- **Comment**: 12 pages, 5 figures, 3 tables
- **Journal**: IEEE Transactions on Image Processing, volume 26, Issue 3,
  1428-1440, 2017
- **Summary**: Head-pose estimation has many applications, such as social event analysis, human-robot and human-computer interaction, driving assistance, and so forth. Head-pose estimation is challenging because it must cope with changing illumination conditions, variabilities in face orientation and in appearance, partial occlusions of facial landmarks, as well as bounding-box-to-face alignment errors. We propose tu use a mixture of linear regressions with partially-latent output. This regression method learns to map high-dimensional feature vectors (extracted from bounding boxes of faces) onto the joint space of head-pose angles and bounding-box shifts, such that they are robustly predicted in the presence of unobservable phenomena. We describe in detail the mapping method that combines the merits of unsupervised manifold learning techniques and of mixtures of regressions. We validate our method with three publicly available datasets and we thoroughly benchmark four variants of the proposed algorithm with several state-of-the-art head-pose estimation methods.



### Object Boundary Guided Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1603.09742v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.09742v4)
- **Published**: 2016-03-31 19:51:05+00:00
- **Updated**: 2016-07-06 23:51:40+00:00
- **Authors**: Qin Huang, Chunyang Xia, Wenchao Zheng, Yuhang Song, Hao Xu, C. -C. Jay Kuo
- **Comment**: The results in the first version of this paper are mistaken due to
  overlapping validation data and incorrect benchmark methods
- **Journal**: None
- **Summary**: Semantic segmentation is critical to image content understanding and object localization. Recent development in fully-convolutional neural network (FCN) has enabled accurate pixel-level labeling. One issue in previous works is that the FCN based method does not exploit the object boundary information to delineate segmentation details since the object boundary label is ignored in the network training. To tackle this problem, we introduce a double branch fully convolutional neural network, which separates the learning of the desirable semantic class labeling with mask-level object proposals guided by relabeled boundaries. This network, called object boundary guided FCN (OBG-FCN), is able to integrate the distinct properties of object shape and class features elegantly in a fully convolutional way with a designed masking architecture. We conduct experiments on the PASCAL VOC segmentation benchmark, and show that the end-to-end trainable OBG-FCN system offers great improvement in optimizing the target semantic segmentation quality.



### Modeling Visual Compatibility through Hierarchical Mid-level Elements
- **Arxiv ID**: http://arxiv.org/abs/1604.00036v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.00036v1)
- **Published**: 2016-03-31 20:18:16+00:00
- **Updated**: 2016-03-31 20:18:16+00:00
- **Authors**: Jose Oramas, Tinne Tuytelaars
- **Comment**: 29 pages, 19 Figures
- **Journal**: None
- **Summary**: In this paper we present a hierarchical method to discover mid-level elements with the objective of modeling visual compatibility between objects. At the base-level, our method identifies patterns of CNN activations with the aim of modeling different variations/styles in which objects of the classes of interest may occur. At the top-level, the proposed method discovers patterns of co-occurring activations of base-level elements that define visual compatibility between pairs of object classes. Experiments on the massive Amazon dataset show the strength of our method at describing object classes and the characteristics that drive the compatibility between them.



### To Fall Or Not To Fall: A Visual Approach to Physical Stability Prediction
- **Arxiv ID**: http://arxiv.org/abs/1604.00066v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1604.00066v1)
- **Published**: 2016-03-31 21:53:32+00:00
- **Updated**: 2016-03-31 21:53:32+00:00
- **Authors**: Wenbin Li, Seyedmajid Azimi, Aleš Leonardis, Mario Fritz
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding physical phenomena is a key competence that enables humans and animals to act and interact under uncertain perception in previously unseen environments containing novel object and their configurations. Developmental psychology has shown that such skills are acquired by infants from observations at a very early stage.   In this paper, we contrast a more traditional approach of taking a model-based route with explicit 3D representations and physical simulation by an end-to-end approach that directly predicts stability and related quantities from appearance. We ask the question if and to what extent and quality such a skill can directly be acquired in a data-driven way bypassing the need for an explicit simulation.   We present a learning-based approach based on simulated data that predicts stability of towers comprised of wooden blocks under different conditions and quantities related to the potential fall of the towers. The evaluation is carried out on synthetic data and compared to human judgments on the same stimuli.



