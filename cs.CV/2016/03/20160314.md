# Arxiv Papers in cs.CV on 2016-03-14
### Learning Binary Codes and Binary Weights for Efficient Classification
- **Arxiv ID**: http://arxiv.org/abs/1603.04116v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.04116v1)
- **Published**: 2016-03-14 03:05:53+00:00
- **Updated**: 2016-03-14 03:05:53+00:00
- **Authors**: Fumin Shen, Yadong Mu, Wei Liu, Yang Yang, Heng Tao Shen
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: This paper proposes a generic formulation that significantly expedites the training and deployment of image classification models, particularly under the scenarios of many image categories and high feature dimensions. As a defining property, our method represents both the images and learned classifiers using binary hash codes, which are simultaneously learned from the training data. Classifying an image thereby reduces to computing the Hamming distance between the binary codes of the image and classifiers and selecting the class with minimal Hamming distance. Conventionally, compact hash codes are primarily used for accelerating image search. Our work is first of its kind to represent classifiers using binary codes. Specifically, we formulate multi-class image classification as an optimization problem over binary variables. The optimization alternatively proceeds over the binary classifiers and image hash codes. Profiting from the special property of binary codes, we show that the sub-problems can be efficiently solved through either a binary quadratic program (BQP) or linear program. In particular, for attacking the BQP problem, we propose a novel bit-flipping procedure which enjoys high efficacy and local optimality guarantee. Our formulation supports a large family of empirical loss functions and is here instantiated by exponential / hinge losses. Comprehensive evaluations are conducted on several representative image benchmarks. The experiments consistently observe reduced complexities of model training and deployment, without sacrifice of accuracies.



### Multi-modal Tracking for Object based SLAM
- **Arxiv ID**: http://arxiv.org/abs/1603.04117v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.04117v1)
- **Published**: 2016-03-14 03:06:39+00:00
- **Updated**: 2016-03-14 03:06:39+00:00
- **Authors**: Prateek Singhal, Ruffin White, Henrik Christensen
- **Comment**: Submitted to IROS 2016
- **Journal**: None
- **Summary**: We present an on-line 3D visual object tracking framework for monocular cameras by incorporating spatial knowledge and uncertainty from semantic mapping along with high frequency measurements from visual odometry. Using a combination of vision and odometry that are tightly integrated we can increase the overall performance of object based tracking for semantic mapping. We present a framework for integration of the two data-sources into a coherent framework through information based fusion/arbitration. We demonstrate the framework in the context of OmniMapper[1] and present results on 6 challenging sequences over multiple objects compared to data obtained from a motion capture systems. We are able to achieve a mean error of 0.23m for per frame tracking showing 9% relative error less than state of the art tracker.



### A Novel Method for the Extrinsic Calibration of a 2D Laser Rangefinder and a Camera
- **Arxiv ID**: http://arxiv.org/abs/1603.04132v4
- **DOI**: 10.1109/JSEN.2018.2819082
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1603.04132v4)
- **Published**: 2016-03-14 04:15:54+00:00
- **Updated**: 2018-08-31 23:21:55+00:00
- **Authors**: Wenbo Dong, Volkan Isler
- **Comment**: 12 pages, 12 figures
- **Journal**: IEEE Sensors Journal, 18(10): 4200-4211, 2018
- **Summary**: We present a novel method for extrinsically calibrating a camera and a 2D Laser Rangefinder (LRF) whose beams are invisible from the camera image. We show that point-to-plane constraints from a single observation of a V-shaped calibration pattern composed of two non-coplanar triangles suffice to uniquely constrain the relative pose between two sensors. Next, we present an approach to obtain analytical solutions using point-to-plane constraints from single or multiple observations. Along the way, we also show that previous solutions, in contrast to our method, have inherent ambiguities and therefore must rely on a good initial estimate. Real and synthetic experiments validate our method and show that it achieves better accuracy than previous methods.



### RISAS: A Novel Rotation, Illumination, Scale Invariant Appearance and Shape Feature
- **Arxiv ID**: http://arxiv.org/abs/1603.04134v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1603.04134v2)
- **Published**: 2016-03-14 04:39:49+00:00
- **Updated**: 2016-09-19 09:09:37+00:00
- **Authors**: Kanzhi Wu, Xiaoyang Li, Ravindra Ranasinghe, Gamini Dissanayake, Yong Liu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel appearance and shape feature, RISAS, which is robust to viewpoint, illumination, scale and rotation variations. RISAS consists of a keypoint detector and a feature descriptor both of which utilise texture and geometric information present in the appearance and shape channels. A novel response function based on the surface normals is used in combination with the Harris corner detector for selecting keypoints in the scene. A strategy that uses the depth information for scale estimation and background elimination is proposed to select the neighbourhood around the keypoints in order to build precise invariant descriptors. Proposed descriptor relies on the ordering of both grayscale intensity and shape information in the neighbourhood. Comprehensive experiments which confirm the effectiveness of the proposed RGB-D feature when compared with CSHOT and LOIND are presented. Furthermore, we highlight the utility of incorporating texture and shape information in the design of both the detector and the descriptor by demonstrating the enhanced performance of CSHOT and LOIND when combined with RISAS detector.



### SSSC-AM: A Unified Framework for Video Co-Segmentation by Structured Sparse Subspace Clustering with Appearance and Motion Features
- **Arxiv ID**: http://arxiv.org/abs/1603.04139v2
- **DOI**: 10.1109/ICIP.2016.7533102
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.04139v2)
- **Published**: 2016-03-14 05:36:40+00:00
- **Updated**: 2016-09-28 22:05:15+00:00
- **Authors**: Junlin Yao, Frank Nielsen
- **Comment**: 19 pages, 6 figures, 5 tables, extend ICIP 2016
- **Journal**: IEEE International Conference on Image Processing (ICIP), 2016
- **Summary**: Video co-segmentation refers to the task of jointly segmenting common objects appearing in a given group of videos. In practice, high-dimensional data such as videos can be conceptually thought as being drawn from a union of subspaces corresponding to categories rather than from a smooth manifold. Therefore, segmenting data into respective subspaces --- subspace clustering --- finds widespread applications in computer vision, including co-segmentation. State-of-the-art methods via subspace clustering seek to solve the problem in two steps:   First, an affinity matrix is built from data, with appearance features or motion patterns. Second, the data are segmented by applying spectral clustering to the affinity matrix. However, this process is insufficient to obtain an optimal solution since it does not take into account the {\em interdependence} of the affinity matrix with the segmentation. In this work, we present a novel unified video co-segmentation framework inspired by the recent Structured Sparse Subspace Clustering ($\mathrm{S^{3}C}$) based on the {\em self-expressiveness} model. Our method yields more consistent segmentation results. In order to improve the detectability of motion features with missing trajectories due to occlusion or tracked points moving out of frames, we add an extra-dimensional signature to the motion trajectories. Moreover, we reformulate the $\mathrm{S^{3}C}$ algorithm by adding the affine subspace constraint in order to make it more suitable to segment rigid motions lying in affine subspaces of dimension at most $3$. Our experiments on MOViCS dataset show that our framework achieves the highest overall performance among baseline algorithms and demonstrate its robustness to heavy noise.



### Saliency Detection for Improving Object Proposals
- **Arxiv ID**: http://arxiv.org/abs/1603.04146v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.04146v3)
- **Published**: 2016-03-14 06:44:43+00:00
- **Updated**: 2016-10-17 06:30:08+00:00
- **Authors**: Shuhan Chen, Jindong Li, Xuelong Hu, Ping Zhou
- **Comment**: IEEE DSP 2016
- **Journal**: None
- **Summary**: Object proposals greatly benefit object detection task in recent state-of-the-art works. However, the existing object proposals usually have low localization accuracy at high intersection over union threshold. To address it, we apply saliency detection to each bounding box to improve their quality in this paper. We first present a geodesic saliency detection method in contour, which is designed to find closed contours. Then, we apply it to each candidate box with multi-sizes, and refined boxes can be easily produced in the obtained saliency maps which are further used to calculate saliency scores for proposal ranking. Experiments on PASCAL VOC 2007 test dataset demonstrate the proposed refinement approach can greatly improve existing models.



### Regression-based Hypergraph Learning for Image Clustering and Classification
- **Arxiv ID**: http://arxiv.org/abs/1603.04150v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.04150v1)
- **Published**: 2016-03-14 06:54:39+00:00
- **Updated**: 2016-03-14 06:54:39+00:00
- **Authors**: Sheng Huang, Dan Yang, Bo Liu, Xiaohong Zhang
- **Comment**: 11pages
- **Journal**: None
- **Summary**: Inspired by the recently remarkable successes of Sparse Representation (SR), Collaborative Representation (CR) and sparse graph, we present a novel hypergraph model named Regression-based Hypergraph (RH) which utilizes the regression models to construct the high quality hypergraphs. Moreover, we plug RH into two conventional hypergraph learning frameworks, namely hypergraph spectral clustering and hypergraph transduction, to present Regression-based Hypergraph Spectral Clustering (RHSC) and Regression-based Hypergraph Transduction (RHT) models for addressing the image clustering and classification issues. Sparse Representation and Collaborative Representation are employed to instantiate two RH instances and their RHSC and RHT algorithms. The experimental results on six popular image databases demonstrate that the proposed RH learning algorithms achieve promising image clustering and classification performances, and also validate that RH can inherit the desirable properties from both hypergraph models and regression models.



### Extended Object Tracking: Introduction, Overview and Applications
- **Arxiv ID**: http://arxiv.org/abs/1604.00970v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.SY, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1604.00970v3)
- **Published**: 2016-03-14 09:58:49+00:00
- **Updated**: 2017-02-21 16:23:16+00:00
- **Authors**: Karl Granstrom, Marcus Baum, Stephan Reuter
- **Comment**: 30 pages, 19 figures
- **Journal**: Journal of Advances in Information Fusion, Volume 12, Number 2,
  Pages 139-174, December 2016, ISSN 1557-6418
- **Summary**: This article provides an elaborate overview of current research in extended object tracking. We provide a clear definition of the extended object tracking problem and discuss its delimitation to other types of object tracking. Next, different aspects of extended object modelling are extensively discussed. Subsequently, we give a tutorial introduction to two basic and well used extended object tracking approaches - the random matrix approach and the Kalman filter-based approach for star-convex shapes. The next part treats the tracking of multiple extended objects and elaborates how the large number of feasible association hypotheses can be tackled using both Random Finite Set (RFS) and Non-RFS multi-object trackers. The article concludes with a summary of current applications, where four example applications involving camera, X-band radar, light detection and ranging (lidar), red-green-blue-depth (RGB-D) sensors are highlighted.



### Visual Concept Recognition and Localization via Iterative Introspection
- **Arxiv ID**: http://arxiv.org/abs/1603.04186v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1603.04186v2)
- **Published**: 2016-03-14 10:18:03+00:00
- **Updated**: 2016-05-25 13:27:37+00:00
- **Authors**: Amir Rosenfeld, Shimon Ullman
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks have been shown to develop internal representations, which correspond closely to semantically meaningful objects and parts, although trained solely on class labels. Class Activation Mapping (CAM) is a recent method that makes it possible to easily highlight the image regions contributing to a network's classification decision. We build upon these two developments to enable a network to re-examine informative image regions, which we term introspection. We propose a weakly-supervised iterative scheme, which shifts its center of attention to increasingly discriminative regions as it progresses, by alternating stages of classification and introspection. We evaluate our method and show its effectiveness over a range of several datasets, where we obtain competitive or state-of-the-art results: on Stanford-40 Actions, we set a new state-of the art of 81.74%. On FGVC-Aircraft and the Stanford Dogs dataset, we show consistent improvements over baselines, some of which include significantly more supervision.



### Graph Based Sinogram Denoising for Tomographic Reconstructions
- **Arxiv ID**: http://arxiv.org/abs/1603.04203v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.04203v1)
- **Published**: 2016-03-14 10:58:23+00:00
- **Updated**: 2016-03-14 10:58:23+00:00
- **Authors**: Faisal Mahmood, Nauman Shahid, Pierre Vandergheynst, Ulf Skoglund
- **Comment**: None
- **Journal**: None
- **Summary**: Limited data and low dose constraints are common problems in a variety of tomographic reconstruction paradigms which lead to noisy and incomplete data. Over the past few years sinogram denoising has become an essential pre-processing step for low dose Computed Tomographic (CT) reconstructions. We propose a novel sinogram denoising algorithm inspired by the modern field of signal processing on graphs. Graph based methods often perform better than standard filtering operations since they can exploit the signal structure. This makes the sinogram an ideal candidate for graph based denoising since it generally has a piecewise smooth structure. We test our method with a variety of phantoms and different reconstruction methods. Our numerical study shows that the proposed algorithm improves the performance of analytical filtered back-projection (FBP) and iterative methods ART (Kaczmarz) and SIRT (Cimmino).We observed that graph denoised sinogram always minimizes the error measure and improves the accuracy of the solution as compared to regular reconstructions.



### Investigation of event-based memory surfaces for high-speed tracking, unsupervised feature extraction and object recognition
- **Arxiv ID**: http://arxiv.org/abs/1603.04223v3
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1603.04223v3)
- **Published**: 2016-03-14 11:54:15+00:00
- **Updated**: 2017-11-08 10:39:46+00:00
- **Authors**: Saeed Afshar, Gregory Cohen, Tara Julia Hamilton, Jonathan Tapson, Andre van Schaik
- **Comment**: This is an updated version of a previously submitted manuscript
- **Journal**: None
- **Summary**: In this paper we compare event-based decaying and time based-decaying memory surfaces for high-speed eventbased tracking, feature extraction, and object classification using an event-based camera. The high-speed recognition task involves detecting and classifying model airplanes that are dropped free-hand close to the camera lens so as to generate a challenging dataset exhibiting significant variance in target velocity. This variance motivated the investigation of event-based decaying memory surfaces in comparison to time-based decaying memory surfaces to capture the temporal aspect of the event-based data. These surfaces are then used to perform unsupervised feature extraction, tracking and recognition. In order to generate the memory surfaces, event binning, linearly decaying kernels, and exponentially decaying kernels were investigated with exponentially decaying kernels found to perform best. Event-based decaying memory surfaces were found to outperform time-based decaying memory surfaces in recognition especially when invariance to target velocity was made a requirement. A range of network and receptive field sizes were investigated. The system achieves 98.75% recognition accuracy within 156 milliseconds of an airplane entering the field of view, using only twenty-five event-based feature extracting neurons in series with a linear classifier. By comparing the linear classifier results to an ELM classifier, we find that a small number of event-based feature extractors can effectively project the complex spatio-temporal event patterns of the dataset to an almost linearly separable representation in feature space.



### Dynamic Scene Deblurring using a Locally Adaptive Linear Blur Model
- **Arxiv ID**: http://arxiv.org/abs/1603.04265v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.04265v1)
- **Published**: 2016-03-14 13:57:19+00:00
- **Updated**: 2016-03-14 13:57:19+00:00
- **Authors**: Tae Hyun Kim, Seungjun Nah, Kyoung Mu Lee
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art video deblurring methods cannot handle blurry videos recorded in dynamic scenes, since they are built under a strong assumption that the captured scenes are static. Contrary to the existing methods, we propose a video deblurring algorithm that can deal with general blurs inherent in dynamic scenes. To handle general and locally varying blurs caused by various sources, such as moving objects, camera shake, depth variation, and defocus, we estimate pixel-wise non-uniform blur kernels. We infer bidirectional optical flows to handle motion blurs, and also estimate Gaussian blur maps to remove optical blur from defocus in our new blur model. Therefore, we propose a single energy model that jointly estimates optical flows, defocus blur maps and latent frames. We also provide a framework and efficient solvers to minimize the proposed energy model. By optimizing the energy model, we achieve significant improvements in removing general blurs, estimating optical flows, and extending depth-of-field in blurry frames. Moreover, in this work, to evaluate the performance of non-uniform deblurring methods objectively, we have constructed a new realistic dataset with ground truths. In addition, extensive experimental on publicly available challenging video data demonstrate that the proposed method produces qualitatively superior performance than the state-of-the-art methods which often fail in either deblurring or optical flow estimation.



### Diversity in Object Proposals
- **Arxiv ID**: http://arxiv.org/abs/1603.04308v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.04308v1)
- **Published**: 2016-03-14 15:40:38+00:00
- **Updated**: 2016-03-14 15:40:38+00:00
- **Authors**: Anton Winschel, Rainer Lienhart, Christian Eggert
- **Comment**: None
- **Journal**: None
- **Summary**: Current top performing object recognition systems build on object proposals as a preprocessing step. Object proposal algorithms are designed to generate candidate regions for generic objects, yet current approaches are limited in capturing the vast variety of object characteristics. In this paper we analyze the error modes of the state-of-the-art Selective Search object proposal algorithm and suggest extensions to broaden its feature diversity in order to mitigate its error modes. We devise an edge grouping algorithm for handling objects without clear boundaries. To further enhance diversity, we incorporate the Edge Boxes proposal algorithm, which is based on fundamentally different principles than Selective Search. The combination of segmentations and edges provides rich image information and feature diversity which is essential for obtaining high quality object proposals for generic objects. For a preset amount of object proposals we achieve considerably better results by using our combination of different strategies than using any single strategy alone.



### Automatic Discrimination of Color Retinal Images using the Bag of Words Approach
- **Arxiv ID**: http://arxiv.org/abs/1603.04327v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.04327v1)
- **Published**: 2016-03-14 16:26:32+00:00
- **Updated**: 2016-03-14 16:26:32+00:00
- **Authors**: Ibrahim Sadek
- **Comment**: None
- **Journal**: None
- **Summary**: Diabetic retinopathy (DR) and age related macular degeneration (ARMD) are among the major causes of visual impairment worldwide. DR is mainly characterized by red spots, namely microaneurysms and bright lesions, specifically exudates whereas ARMD is mainly identified by tiny yellow or white deposits called drusen. Since exudates might be the only manifestation of the early diabetic retinopathy, there is an increase demand for automatic retinopathy diagnosis. Exudates and drusen may share similar appearances, thus discriminating between them is of interest to enhance screening performance. In this research, we investigative the role of bag of words approach in the automatic diagnosis of retinopathy diabetes. We proposed to use a single based and multiple based methods for the construction of the visual dictionary by combining the histogram of word occurrences from each dictionary and building a single histogram. The introduced approach is evaluated for automatic diagnosis of normal and abnormal color fundus images with bright lesions. This approach has been implemented on 430 fundus images, including six publicly available datasets, in addition to one local dataset. The mean accuracies reported are 97.2% and 99.77% for single based and multiple based dictionaries respectively.



### Rapid building detection using machine learning
- **Arxiv ID**: http://arxiv.org/abs/1603.04392v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.04392v1)
- **Published**: 2016-03-14 19:03:53+00:00
- **Updated**: 2016-03-14 19:03:53+00:00
- **Authors**: Joseph Paul Cohen, Wei Ding, Caitlin Kuhlman, Aijun Chen, Liping Di
- **Comment**: Accepted to be published in Applied Intelligence 2016
- **Journal**: None
- **Summary**: This work describes algorithms for performing discrete object detection, specifically in the case of buildings, where usually only low quality RGB-only geospatial reflective imagery is available. We utilize new candidate search and feature extraction techniques to reduce the problem to a machine learning (ML) classification task. Here we can harness the complex patterns of contrast features contained in training data to establish a model of buildings. We avoid costly sliding windows to generate candidates; instead we innovatively stitch together well known image processing techniques to produce candidates for building detection that cover 80-85% of buildings. Reducing the number of possible candidates is important due to the scale of the problem. Each candidate is subjected to classification which, although linear, costs time and prohibits large scale evaluation. We propose a candidate alignment algorithm to boost classification performance to 80-90% precision with a linear time algorithm and show it has negligible cost. Also, we propose a new concept called a Permutable Haar Mesh (PHM) which we use to form and traverse a search space to recover candidate buildings which were lost in the initial preprocessing phase.



### U-CATCH: Using Color ATtribute of image patCHes in binary descriptors
- **Arxiv ID**: http://arxiv.org/abs/1603.04408v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.04408v3)
- **Published**: 2016-03-14 19:32:57+00:00
- **Updated**: 2019-08-22 05:17:34+00:00
- **Authors**: Alisher Abdulkhaev, Ozgur Yilmaz
- **Comment**: None
- **Journal**: None
- **Summary**: In this study, we propose a simple yet very effective method for extracting color information through binary feature description framework. Our method expands the dimension of binary comparisons into RGB and YCbCr spaces, showing more than 100% matching improve ment compared to non-color binary descriptors for a wide range of hard-to-match cases. The proposed method is general and can be applied to any binary descriptor to make it color sensitive. It is faster than classical binary descriptors for RGB sampling due to the abandonment of grayscale conversion and has almost identical complexity (insignificant compared to smoothing operation) for YCbCr sampling.



