# Arxiv Papers in cs.CV on 2016-03-11
### Watch-n-Patch: Unsupervised Learning of Actions and Relations
- **Arxiv ID**: http://arxiv.org/abs/1603.03541v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1603.03541v1)
- **Published**: 2016-03-11 07:13:59+00:00
- **Updated**: 2016-03-11 07:13:59+00:00
- **Authors**: Chenxia Wu, Jiemi Zhang, Ozan Sener, Bart Selman, Silvio Savarese, Ashutosh Saxena
- **Comment**: arXiv admin note: text overlap with arXiv:1512.04208
- **Journal**: None
- **Summary**: There is a large variation in the activities that humans perform in their everyday lives. We consider modeling these composite human activities which comprises multiple basic level actions in a completely unsupervised setting. Our model learns high-level co-occurrence and temporal relations between the actions. We consider the video as a sequence of short-term action clips, which contains human-words and object-words. An activity is about a set of action-topics and object-topics indicating which actions are present and which objects are interacting with. We then propose a new probabilistic model relating the words and the topics. It allows us to model long-range action relations that commonly exist in the composite activities, which is challenging in previous works. We apply our model to the unsupervised action segmentation and clustering, and to a novel application that detects forgotten actions, which we call action patching. For evaluation, we contribute a new challenging RGB-D activity video dataset recorded by the new Kinect v2, which contains several human daily activities as compositions of multiple actions interacting with different objects. Moreover, we develop a robotic system that watches people and reminds people by applying our action patching algorithm. Our robotic setup can be easily deployed on any assistive robot.



### Fast Optical Flow using Dense Inverse Search
- **Arxiv ID**: http://arxiv.org/abs/1603.03590v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1603.03590v1)
- **Published**: 2016-03-11 10:55:07+00:00
- **Updated**: 2016-03-11 10:55:07+00:00
- **Authors**: Till Kroeger, Radu Timofte, Dengxin Dai, Luc Van Gool
- **Comment**: 9 pages main paper + 16 pages supplementary material
- **Journal**: None
- **Summary**: Most recent works in optical flow extraction focus on the accuracy and neglect the time complexity. However, in real-life visual applications, such as tracking, activity detection and recognition, the time complexity is critical.   We propose a solution with very low time complexity and competitive accuracy for the computation of dense optical flow. It consists of three parts: 1) inverse search for patch correspondences; 2) dense displacement field creation through patch aggregation along multiple scales; 3) variational refinement. At the core of our Dense Inverse Search-based method (DIS) is the efficient search of correspondences inspired by the inverse compositional image alignment proposed by Baker and Matthews in 2001.   DIS is competitive on standard optical flow benchmarks with large displacements. DIS runs at 300Hz up to 600Hz on a single CPU core, reaching the temporal resolution of human's biological vision system. It is order(s) of magnitude faster than state-of-the-art methods in the same range of accuracy, making DIS ideal for visual applications.



### Efficient forward propagation of time-sequences in convolutional neural networks using Deep Shifting
- **Arxiv ID**: http://arxiv.org/abs/1603.03657v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1603.03657v1)
- **Published**: 2016-03-11 15:16:09+00:00
- **Updated**: 2016-03-11 15:16:09+00:00
- **Authors**: Koen Groenland, Sander Bohte
- **Comment**: None
- **Journal**: None
- **Summary**: When a Convolutional Neural Network is used for on-the-fly evaluation of continuously updating time-sequences, many redundant convolution operations are performed. We propose the method of Deep Shifting, which remembers previously calculated results of convolution operations in order to minimize the number of calculations. The reduction in complexity is at least a constant and in the best case quadratic. We demonstrate that this method does indeed save significant computation time in a practical implementation, especially when the networks receives a large number of time-frames.



### Learning Gaze Transitions from Depth to Improve Video Saliency Estimation
- **Arxiv ID**: http://arxiv.org/abs/1603.03669v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.03669v1)
- **Published**: 2016-03-11 15:53:58+00:00
- **Updated**: 2016-03-11 15:53:58+00:00
- **Authors**: G. Leifman, D. Rudoy, T. Swedish, E. Bayro-Corrochano, R. Raskar
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we introduce a novel Depth-Aware Video Saliency approach to predict human focus of attention when viewing RGBD videos on regular 2D screens. We train a generative convolutional neural network which predicts a saliency map for a frame, given the fixation map of the previous frame. Saliency estimation in this scenario is highly important since in the near future 3D video content will be easily acquired and yet hard to display. This can be explained, on the one hand, by the dramatic improvement of 3D-capable acquisition equipment. On the other hand, despite the considerable progress in 3D display technologies, most of the 3D displays are still expensive and require wearing special glasses. To evaluate the performance of our approach, we present a new comprehensive database of eye-fixation ground-truth for RGBD videos. Our experiments indicate that integrating depth into video saliency calculation is beneficial. We demonstrate that our approach outperforms state-of-the-art methods for video saliency, achieving 15% relative improvement.



### Region Graph Based Method for Multi-Object Detection and Tracking using Depth Cameras
- **Arxiv ID**: http://arxiv.org/abs/1603.03783v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.03783v1)
- **Published**: 2016-03-11 21:06:35+00:00
- **Updated**: 2016-03-11 21:06:35+00:00
- **Authors**: Sachin Mehta, Balakrishnan Prabhakaran
- **Comment**: Accepted in IEEE Winter Conference in Computer Vision (WACV'16)
- **Journal**: None
- **Summary**: In this paper, we propose a multi-object detection and tracking method using depth cameras. Depth maps are very noisy and obscure in object detection. We first propose a region-based method to suppress high magnitude noise which cannot be filtered using spatial filters. Second, the proposed method detect Region of Interests by temporal learning which are then tracked using weighted graph-based approach. We demonstrate the performance of the proposed method on standard depth camera datasets with and without object occlusions. Experimental results show that the proposed method is able to suppress high magnitude noise in depth maps and detect/track the objects (with and without occlusion).



