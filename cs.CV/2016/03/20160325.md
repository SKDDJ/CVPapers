# Arxiv Papers in cs.CV on 2016-03-25
### Quadratic Projection Based Feature Extraction with Its Application to Biometric Recognition
- **Arxiv ID**: http://arxiv.org/abs/1603.07797v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07797v1)
- **Published**: 2016-03-25 01:10:53+00:00
- **Updated**: 2016-03-25 01:10:53+00:00
- **Authors**: Yan Yan, Hanzi Wang, Si Chen, Xiaochun Cao, David Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel quadratic projection based feature extraction framework, where a set of quadratic matrices is learned to distinguish each class from all other classes. We formulate quadratic matrix learning (QML) as a standard semidefinite programming (SDP) problem. However, the con- ventional interior-point SDP solvers do not scale well to the problem of QML for high-dimensional data. To solve the scalability of QML, we develop an efficient algorithm, termed DualQML, based on the Lagrange duality theory, to extract nonlinear features. To evaluate the feasibility and effectiveness of the proposed framework, we conduct extensive experiments on biometric recognition. Experimental results on three representative biometric recogni- tion tasks, including face, palmprint, and ear recognition, demonstrate the superiority of the DualQML-based feature extraction algorithm compared to the current state-of-the-art algorithms



### An Effective Unconstrained Correlation Filter and Its Kernelization for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1603.07800v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07800v1)
- **Published**: 2016-03-25 01:36:41+00:00
- **Updated**: 2016-03-25 01:36:41+00:00
- **Authors**: Yan Yan, Hanzi Wang, Cuihua Li, Chenhui Yang, Bineng Zhong
- **Comment**: None
- **Journal**: Neurocomputing, 119, pp.201-211, 2013
- **Summary**: In this paper, an effective unconstrained correlation filter called Uncon- strained Optimal Origin Tradeoff Filter (UOOTF) is presented and applied to robust face recognition. Compared with the conventional correlation filters in Class-dependence Feature Analysis (CFA), UOOTF improves the overall performance for unseen patterns by removing the hard constraints on the origin correlation outputs during the filter design. To handle non-linearly separable distributions between different classes, we further develop a non- linear extension of UOOTF based on the kernel technique. The kernel ex- tension of UOOTF allows for higher flexibility of the decision boundary due to a wider range of non-linearity properties. Experimental results demon- strate the effectiveness of the proposed unconstrained correlation filter and its kernelization in the task of face recognition.



### Mode-Seeking on Hypergraphs for Robust Geometric Model Fitting
- **Arxiv ID**: http://arxiv.org/abs/1603.07807v1
- **DOI**: 10.1109/ICCV.2015.332
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07807v1)
- **Published**: 2016-03-25 02:29:40+00:00
- **Updated**: 2016-03-25 02:29:40+00:00
- **Authors**: Hanzi Wang, Guobao Xiao, Yan Yan, David Suter
- **Comment**: Proceedings of the IEEE International Conference on Computer Vision,
  pp. 2902-2910, 2015
- **Journal**: None
- **Summary**: In this paper, we propose a novel geometric model fitting method, called Mode-Seeking on Hypergraphs (MSH),to deal with multi-structure data even in the presence of severe outliers. The proposed method formulates geometric model fitting as a mode seeking problem on a hypergraph in which vertices represent model hypotheses and hyperedges denote data points. MSH intuitively detects model instances by a simple and effective mode seeking algorithm. In addition to the mode seeking algorithm, MSH includes a similarity measure between vertices on the hypergraph and a weight-aware sampling technique. The proposed method not only alleviates sensitivity to the data distribution, but also is scalable to large scale problems. Experimental results further demonstrate that the proposed method has significant superiority over the state-of-the-art fitting methods on both synthetic data and real images.



### Conditional Similarity Networks
- **Arxiv ID**: http://arxiv.org/abs/1603.07810v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1603.07810v3)
- **Published**: 2016-03-25 02:52:02+00:00
- **Updated**: 2017-04-10 15:18:21+00:00
- **Authors**: Andreas Veit, Serge Belongie, Theofanis Karaletsos
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: What makes images similar? To measure the similarity between images, they are typically embedded in a feature-vector space, in which their distance preserve the relative dissimilarity. However, when learning such similarity embeddings the simplifying assumption is commonly made that images are only compared to one unique measure of similarity. A main reason for this is that contradicting notions of similarities cannot be captured in a single space. To address this shortcoming, we propose Conditional Similarity Networks (CSNs) that learn embeddings differentiated into semantically distinct subspaces that capture the different notions of similarities. CSNs jointly learn a disentangled embedding where features for different similarities are encoded in separate dimensions as well as masks that select and reweight relevant dimensions to induce a subspace that encodes a specific similarity notion. We show that our approach learns interpretable image representations with visually relevant semantic subspaces. Further, when evaluating on triplet questions from multiple similarity notions our model even outperforms the accuracy obtained by training individual specialized networks for each notion separately.



### Training-Free Synthesized Face Sketch Recognition Using Image Quality Assessment Metrics
- **Arxiv ID**: http://arxiv.org/abs/1603.07823v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07823v1)
- **Published**: 2016-03-25 05:20:08+00:00
- **Updated**: 2016-03-25 05:20:08+00:00
- **Authors**: Nannan Wang, Jie Li, Leiyu Sun, Bin Song, Xinbo Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Face sketch synthesis has wide applications ranging from digital entertainments to law enforcements. Objective image quality assessment scores and face recognition accuracy are two mainly used tools to evaluate the synthesis performance. In this paper, we proposed a synthesized face sketch recognition framework based on full-reference image quality assessment metrics. Synthesized sketches generated from four state-of-the-art methods are utilized to test the performance of the proposed recognition framework. For the image quality assessment metrics, we employed the classical structured similarity index metric and other three prevalent metrics: visual information fidelity, feature similarity index metric and gradient magnitude similarity deviation. Extensive experiments compared with baseline methods illustrate the effectiveness of the proposed synthesized face sketch recognition framework. Data and implementation code in this paper are available online at www.ihitworld.com/WNN/IQA_Sketch.zip.



### An end-to-end convolutional selective autoencoder approach to Soybean Cyst Nematode eggs detection
- **Arxiv ID**: http://arxiv.org/abs/1603.07834v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1603.07834v1)
- **Published**: 2016-03-25 07:12:32+00:00
- **Updated**: 2016-03-25 07:12:32+00:00
- **Authors**: Adedotun Akintayo, Nigel Lee, Vikas Chawla, Mark Mullaney, Christopher Marett, Asheesh Singh, Arti Singh, Greg Tylka, Baskar Ganapathysubramaniam, Soumik Sarkar
- **Comment**: A 10 pages, 8 figures International Conference on Machine
  Leaning(ICML) Submission
- **Journal**: None
- **Summary**: This paper proposes a novel selective autoencoder approach within the framework of deep convolutional networks. The crux of the idea is to train a deep convolutional autoencoder to suppress undesired parts of an image frame while allowing the desired parts resulting in efficient object detection. The efficacy of the framework is demonstrated on a critical plant science problem. In the United States, approximately $1 billion is lost per annum due to a nematode infection on soybean plants. Currently, plant-pathologists rely on labor-intensive and time-consuming identification of Soybean Cyst Nematode (SCN) eggs in soil samples via manual microscopy. The proposed framework attempts to significantly expedite the process by using a series of manually labeled microscopic images for training followed by automated high-throughput egg detection. The problem is particularly difficult due to the presence of a large population of non-egg particles (disturbances) in the image frames that are very similar to SCN eggs in shape, pose and illumination. Therefore, the selective autoencoder is trained to learn unique features related to the invariant shapes and sizes of the SCN eggs without handcrafting. After that, a composite non-maximum suppression and differencing is applied at the post-processing stage.



### Early Detection of Combustion Instabilities using Deep Convolutional Selective Autoencoders on Hi-speed Flame Video
- **Arxiv ID**: http://arxiv.org/abs/1603.07839v1
- **DOI**: 10.1145/1235
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1603.07839v1)
- **Published**: 2016-03-25 08:02:41+00:00
- **Updated**: 2016-03-25 08:02:41+00:00
- **Authors**: Adedotun Akintayo, Kin Gwn Lore, Soumalya Sarkar, Soumik Sarkar
- **Comment**: A 10 pages, 10 figures submission for Applied Data Science Track of
  KDD16
- **Journal**: None
- **Summary**: This paper proposes an end-to-end convolutional selective autoencoder approach for early detection of combustion instabilities using rapidly arriving flame image frames. The instabilities arising in combustion processes cause significant deterioration and safety issues in various human-engineered systems such as land and air based gas turbine engines. These properties are described as self-sustaining, large amplitude pressure oscillations and show varying spatial scales periodic coherent vortex structure shedding. However, such instability is extremely difficult to detect before a combustion process becomes completely unstable due to its sudden (bifurcation-type) nature. In this context, an autoencoder is trained to selectively mask stable flame and allow unstable flame image frames. In that process, the model learns to identify and extract rich descriptive and explanatory flame shape features. With such a training scheme, the selective autoencoder is shown to be able to detect subtle instability features as a combustion process makes transition from stable to unstable region. As a consequence, the deep learning tool-chain can perform as an early detection framework for combustion instabilities that will have a transformative impact on the safety and performance of modern engines.



### A Novel Biologically Mechanism-Based Visual Cognition Model--Automatic Extraction of Semantics, Formation of Integrated Concepts and Re-selection Features for Ambiguity
- **Arxiv ID**: http://arxiv.org/abs/1603.07886v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1603.07886v1)
- **Published**: 2016-03-25 11:47:16+00:00
- **Updated**: 2016-03-25 11:47:16+00:00
- **Authors**: Peijie Yin, Hong Qiao, Wei Wu, Lu Qi, YinLin Li, Shanlin Zhong, Bo Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Integration between biology and information science benefits both fields. Many related models have been proposed, such as computational visual cognition models, computational motor control models, integrations of both and so on. In general, the robustness and precision of recognition is one of the key problems for object recognition models.   In this paper, inspired by features of human recognition process and their biological mechanisms, a new integrated and dynamic framework is proposed to mimic the semantic extraction, concept formation and feature re-selection in human visual processing. The main contributions of the proposed model are as follows:   (1) Semantic feature extraction: Local semantic features are learnt from episodic features that are extracted from raw images through a deep neural network;   (2) Integrated concept formation: Concepts are formed with local semantic information and structural information learnt through network.   (3) Feature re-selection: When ambiguity is detected during recognition process, distinctive features according to the difference between ambiguous candidates are re-selected for recognition.   Experimental results on hand-written digits and facial shape dataset show that, compared with other methods, the new proposed model exhibits higher robustness and precision for visual recognition, especially in the condition when input samples are smantic ambiguous. Meanwhile, the introduced biological mechanisms further strengthen the interaction between neuroscience and information science.



### Object Recognition Based on Amounts of Unlabeled Data
- **Arxiv ID**: http://arxiv.org/abs/1603.07957v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07957v1)
- **Published**: 2016-03-25 16:44:35+00:00
- **Updated**: 2016-03-25 16:44:35+00:00
- **Authors**: Fuqiang Liu, Fukun Bi, Liang Chen
- **Comment**: 16 pages, 6 figures, 2 tables
- **Journal**: None
- **Summary**: This paper proposes a novel semi-supervised method on object recognition. First, based on Boost Picking, a universal algorithm, Boost Picking Teaching (BPT), is proposed to train an effective binary-classifier just using a few labeled data and amounts of unlabeled data. Then, an ensemble strategy is detailed to synthesize multiple BPT-trained binary-classifiers to be a high-performance multi-classifier. The rationality of the strategy is also analyzed in theory. Finally, the proposed method is tested on two databases, CIFAR-10 and CIFAR-100. Using 2% labeled data and 98% unlabeled data, the accuracies of the proposed method on the two data sets are 78.39% and 50.77% respectively.



### Unsupervised Category Discovery via Looped Deep Pseudo-Task Optimization Using a Large Scale Radiology Image Database
- **Arxiv ID**: http://arxiv.org/abs/1603.07965v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07965v1)
- **Published**: 2016-03-25 17:16:00+00:00
- **Updated**: 2016-03-25 17:16:00+00:00
- **Authors**: Xiaosong Wang, Le Lu, Hoo-chang Shin, Lauren Kim, Isabella Nogues, Jianhua Yao, Ronald Summers
- **Comment**: None
- **Journal**: None
- **Summary**: Obtaining semantic labels on a large scale radiology image database (215,786 key images from 61,845 unique patients) is a prerequisite yet bottleneck to train highly effective deep convolutional neural network (CNN) models for image recognition. Nevertheless, conventional methods for collecting image labels (e.g., Google search followed by crowd-sourcing) are not applicable due to the formidable difficulties of medical annotation tasks for those who are not clinically trained. This type of image labeling task remains non-trivial even for radiologists due to uncertainty and possible drastic inter-observer variation or inconsistency.   In this paper, we present a looped deep pseudo-task optimization procedure for automatic category discovery of visually coherent and clinically semantic (concept) clusters. Our system can be initialized by domain-specific (CNN trained on radiology images and text report derived labels) or generic (ImageNet based) CNN models. Afterwards, a sequence of pseudo-tasks are exploited by the looped deep image feature clustering (to refine image labels) and deep CNN training/classification using new labels (to obtain more task representative deep features). Our method is conceptually simple and based on the hypothesized "convergence" of better labels leading to better trained CNN models which in turn feed more effective deep image features to facilitate more meaningful clustering/labels. We have empirically validated the convergence and demonstrated promising quantitative and qualitative results. Category labels of significantly higher quality than those in previous work are discovered. This allows for further investigation of the hierarchical semantic nature of the given large-scale radiology image database.



### Friction from Reflectance: Deep Reflectance Codes for Predicting Physical Surface Properties from One-Shot In-Field Reflectance
- **Arxiv ID**: http://arxiv.org/abs/1603.07998v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07998v2)
- **Published**: 2016-03-25 19:57:03+00:00
- **Updated**: 2016-07-10 20:36:37+00:00
- **Authors**: Hang Zhang, Kristin Dana, Ko Nishino
- **Comment**: None
- **Journal**: None
- **Summary**: Images are the standard input for vision algorithms, but one-shot infield reflectance measurements are creating new opportunities for recognition and scene understanding. In this work, we address the question of what reflectance can reveal about materials in an efficient manner. We go beyond the question of recognition and labeling and ask the question: What intrinsic physical properties of the surface can be estimated using reflectance? We introduce a framework that enables prediction of actual friction values for surfaces using one-shot reflectance measurements. This work is a first of its kind vision-based friction estimation. We develop a novel representation for reflectance disks that capture partial BRDF measurements instantaneously. Our method of deep reflectance codes combines CNN features and fisher vector pooling with optimal binary embedding to create codes that have sufficient discriminatory power and have important properties of illumination and spatial invariance. The experimental results demonstrate that reflectance can play a new role in deciphering the underlying physical properties of real-world scenes.



### Resnet in Resnet: Generalizing Residual Architectures
- **Arxiv ID**: http://arxiv.org/abs/1603.08029v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1603.08029v1)
- **Published**: 2016-03-25 20:55:40+00:00
- **Updated**: 2016-03-25 20:55:40+00:00
- **Authors**: Sasha Targ, Diogo Almeida, Kevin Lyman
- **Comment**: None
- **Journal**: None
- **Summary**: Residual networks (ResNets) have recently achieved state-of-the-art on challenging computer vision tasks. We introduce Resnet in Resnet (RiR): a deep dual-stream architecture that generalizes ResNets and standard CNNs and is easily implemented with no computational overhead. RiR consistently improves performance over ResNets, outperforms architectures with similar amounts of augmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100.



### An Empirical Study of Dimensional Reduction Techniques for Facial Action Units Detection
- **Arxiv ID**: http://arxiv.org/abs/1603.08039v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.08039v1)
- **Published**: 2016-03-25 21:27:31+00:00
- **Updated**: 2016-03-25 21:27:31+00:00
- **Authors**: Zhuo Hui, Wen-Sheng Chu
- **Comment**: Report on DR
- **Journal**: None
- **Summary**: Biologically inspired features, such as Gabor filters, result in very high dimensional measurement. Does reducing the dimensionality of the feature space afford advantages beyond computational efficiency? Do some approaches to dimensionality reduction (DR) yield improved action unit detection? To answer these questions, we compared DR approaches in two relatively large databases of spontaneous facial behavior (45 participants in total with over 2 minutes of FACS-coded video per participant). Facial features were tracked and aligned using active appearance models (AAM). SIFT and Gabor features were extracted from local facial regions. We compared linear (PCA and KPCA), manifold (LPP and LLE), supervised (LDA and KDA) and hybrid approaches (LSDA) to DR with respect to AU detection. For further comparison, a no-DR control condition was included as well. Linear support vector machine classifiers with independent train and test sets were used for AU detection. AU detection was quantified using area under the ROC curve and F1. Baseline results for PCA with Gabor features were comparable with previous research. With some notable exceptions, DR improved AU detection relative to no-DR. Locality embedding approaches proved vulnerable to \emph{out-of-sample} problems. Gradient-based SIFT lead to better AU detection than the filter-based Gabor features. For area under the curve, few differences were found between linear and other DR approaches. For F1, results were mixed. For both metrics, the pattern of results varied among action units. These findings suggest that action unit detection may be optimized by using specific DR for specific action units. PCA and LDA were the most efficient approaches; KDA was the least efficient.



