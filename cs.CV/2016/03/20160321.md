# Arxiv Papers in cs.CV on 2016-03-21
### Deep Self-Convolutional Activations Descriptor for Dense Cross-Modal Correspondence
- **Arxiv ID**: http://arxiv.org/abs/1603.06327v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06327v1)
- **Published**: 2016-03-21 05:30:48+00:00
- **Updated**: 2016-03-21 05:30:48+00:00
- **Authors**: Seungryong Kim, Dongbo Min, Stephen Lin, Kwanghoon Sohn
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel descriptor, called deep self-convolutional activations (DeSCA), designed for establishing dense correspondences between images taken under different imaging modalities, such as different spectral ranges or lighting conditions. Motivated by descriptors based on local self-similarity (LSS), we formulate a novel descriptor by leveraging LSS in a deep architecture, leading to better discriminative power and greater robustness to non-rigid image deformations than state-of-the-art cross-modality descriptors. The DeSCA first computes self-convolutions over a local support window for randomly sampled patches, and then builds self-convolution activations by performing an average pooling through a hierarchical formulation within a deep convolutional architecture. Finally, the feature responses on the self-convolution activations are encoded through a spatial pyramid pooling in a circular configuration. In contrast to existing convolutional neural networks (CNNs) based descriptors, the DeSCA is training-free (i.e., randomly sampled patches are utilized as the convolution kernels), is robust to cross-modal imaging, and can be densely computed in an efficient manner that significantly reduces computational redundancy. The state-of-the-art performance of DeSCA on challenging cases of cross-modal image pairs is demonstrated through extensive experiments.



### Unified Depth Prediction and Intrinsic Image Decomposition from a Single Image via Joint Convolutional Neural Fields
- **Arxiv ID**: http://arxiv.org/abs/1603.06359v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06359v1)
- **Published**: 2016-03-21 09:10:38+00:00
- **Updated**: 2016-03-21 09:10:38+00:00
- **Authors**: Seungryong Kim, Kihong Park, Kwanghoon Sohn, Stephen Lin
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for jointly predicting a depth map and intrinsic images from single-image input. The two tasks are formulated in a synergistic manner through a joint conditional random field (CRF) that is solved using a novel convolutional neural network (CNN) architecture, called the joint convolutional neural field (JCNF) model. Tailored to our joint estimation problem, JCNF differs from previous CNNs in its sharing of convolutional activations and layers between networks for each task, its inference in the gradient domain where there exists greater correlation between depth and intrinsic images, and the incorporation of a gradient scale network that learns the confidence of estimated gradients in order to effectively balance them in the solution. This approach is shown to surpass state-of-the-art methods both on single-image depth estimation and on intrinsic image decomposition.



### Appearance Harmonization for Single Image Shadow Removal
- **Arxiv ID**: http://arxiv.org/abs/1603.06398v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06398v1)
- **Published**: 2016-03-21 12:01:36+00:00
- **Updated**: 2016-03-21 12:01:36+00:00
- **Authors**: Liqian Ma, Jue Wang, Eli Shechtman, Kalyan Sunkavalli, Shimin Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Shadows often create unwanted artifacts in photographs, and removing them can be very challenging. Previous shadow removal methods often produce de-shadowed regions that are visually inconsistent with the rest of the image. In this work we propose a fully automatic shadow region harmonization approach that improves the appearance compatibility of the de-shadowed region as typically produced by previous methods. It is based on a shadow-guided patch-based image synthesis approach that reconstructs the shadow region using patches sampled from non-shadowed regions. The result is then refined based on the reconstruction confidence to handle unique image patterns. Many shadow removal results and comparisons are show the effectiveness of our improvement. Quantitative evaluation on a benchmark dataset suggests that our automatic shadow harmonization approach effectively improves upon the state-of-the-art.



### Beyond Sharing Weights for Deep Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1603.06432v2
- **DOI**: 10.1109/TPAMI.2018.2814042
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06432v2)
- **Published**: 2016-03-21 14:20:41+00:00
- **Updated**: 2016-11-17 13:51:31+00:00
- **Authors**: Artem Rozantsev, Mathieu Salzmann, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: The performance of a classifier trained on data coming from a specific domain typically degrades when applied to a related but different one. While annotating many samples from the new domain would address this issue, it is often too expensive or impractical. Domain Adaptation has therefore emerged as a solution to this problem; It leverages annotated data from a source domain, in which it is abundant, to train a classifier to operate in a target domain, in which it is either sparse or even lacking altogether. In this context, the recent trend consists of learning deep architectures whose weights are shared for both domains, which essentially amounts to learning domain invariant features.   Here, we show that it is more effective to explicitly model the shift from one domain to the other. To this end, we introduce a two-stream architecture, where one operates in the source domain and the other in the target domain. In contrast to other approaches, the weights in corresponding layers are related but not shared. We demonstrate that this both yields higher accuracy than state-of-the-art methods on several object recognition and detection tasks and consistently outperforms networks with shared weights in both supervised and unsupervised settings.



### Illumination-invariant image mosaic calculation based on logarithmic search
- **Arxiv ID**: http://arxiv.org/abs/1603.06433v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06433v1)
- **Published**: 2016-03-21 14:23:00+00:00
- **Updated**: 2016-03-21 14:23:00+00:00
- **Authors**: Wolfgang Konen
- **Comment**: 8 pages, 1 figure
- **Journal**: None
- **Summary**: This technical report describes an improved image mosaicking algorithm. It is based on Jain's logarithmic search algorithm [Jain 1981] which is coupled to the method of Kourogi (1999} for matching images in a video sequence. Logarithmic search has a better invariance against illumination changes than the original optical-flow-based method of Kourogi.



### Controlling Explanatory Heatmap Resolution and Semantics via Decomposition Depth
- **Arxiv ID**: http://arxiv.org/abs/1603.06463v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06463v3)
- **Published**: 2016-03-21 15:42:22+00:00
- **Updated**: 2016-04-04 08:38:26+00:00
- **Authors**: Sebastian Bach, Alexander Binder, Klaus-Robert MÃ¼ller, Wojciech Samek
- **Comment**: 5 pages, 1 table, 1 figure with 40 embedded images
- **Journal**: None
- **Summary**: We present an application of the Layer-wise Relevance Propagation (LRP) algorithm to state of the art deep convolutional neural networks and Fisher Vector classifiers to compare the image perception and prediction strategies of both classifiers with the use of visualized heatmaps. Layer-wise Relevance Propagation (LRP) is a method to compute scores for individual components of an input image, denoting their contribution to the prediction of the classifier for one particular test point. We demonstrate the impact of different choices of decomposition cut-off points during the LRP-process, controlling the resolution and semantics of the heatmap on test images from the PASCAL VOC 2007 test data set.



### Frankenstein: Learning Deep Face Representations using Small Data
- **Arxiv ID**: http://arxiv.org/abs/1603.06470v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06470v3)
- **Published**: 2016-03-21 15:55:30+00:00
- **Updated**: 2017-09-21 15:25:54+00:00
- **Authors**: Guosheng Hu, Xiaojiang Peng, Yongxin Yang, Timothy Hospedales, Jakob Verbeek
- **Comment**: IEEE TIP
- **Journal**: None
- **Summary**: Deep convolutional neural networks have recently proven extremely effective for difficult face recognition problems in uncontrolled settings. To train such networks, very large training sets are needed with millions of labeled images. For some applications, such as near-infrared (NIR) face recognition, such large training datasets are not publicly available and difficult to collect. In this work, we propose a method to generate very large training datasets of synthetic images by compositing real face images in a given dataset. We show that this method enables to learn models from as few as 10,000 training images, which perform on par with models trained from 500,000 images. Using our approach we also obtain state-of-the-art results on the CASIA NIR-VIS2.0 heterogeneous face recognition dataset.



### Instance Influence Estimation for Hyperspectral Target Signature Characterization using Extended Functions of Multiple Instances
- **Arxiv ID**: http://arxiv.org/abs/1603.06496v1
- **DOI**: 10.1117/12.2228154
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06496v1)
- **Published**: 2016-03-21 16:54:41+00:00
- **Updated**: 2016-03-21 16:54:41+00:00
- **Authors**: Sheng Zou, Alina Zare
- **Comment**: Published, Proceedings of the SPIE, 2016
- **Journal**: None
- **Summary**: The Extended Functions of Multiple Instances (eFUMI) algorithm is a generalization of Multiple Instance Learning (MIL). In eFUMI, only bag level (i.e. set level) labels are needed to estimate target signatures from mixed data. The training bags in eFUMI are labeled positive if any data point in a bag contains or represents any proportion of the target signature and are labeled as a negative bag if all data points in the bag do not represent any target. From these imprecise labels, eFUMI has been shown to be effective at estimating target signatures in hyperspectral subpixel target detection problems. One motivating scenario for the use of eFUMI is where an analyst circles objects/regions of interest in a hyperspectral scene such that the target signatures of these objects can be estimated and be used to determine whether other instances of the object appear elsewhere in the image collection. The regions highlighted by the analyst serve as the imprecise labels for eFUMI. Often, an analyst may want to iteratively refine their imprecise labels. In this paper, we present an approach for estimating the influence on the estimated target signature if the label for a particular input data point is modified. This "instance influence estimation" guides an analyst to focus on (re-)labeling the data points that provide the largest change in the resulting estimated target signature and, thus, reduce the amount of time an analyst needs to spend refining the labels for a hyperspectral scene. Results are shown on real hyperspectral sub-pixel target detection data sets.



### Deep video gesture recognition using illumination invariants
- **Arxiv ID**: http://arxiv.org/abs/1603.06531v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1603.06531v1)
- **Published**: 2016-03-21 18:33:29+00:00
- **Updated**: 2016-03-21 18:33:29+00:00
- **Authors**: Otkrist Gupta, Dan Raviv, Ramesh Raskar
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present architectures based on deep neural nets for gesture recognition in videos, which are invariant to local scaling. We amalgamate autoencoder and predictor architectures using an adaptive weighting scheme coping with a reduced size labeled dataset, while enriching our models from enormous unlabeled sets. We further improve robustness to lighting conditions by introducing a new adaptive filer based on temporal local scale normalization. We provide superior results over known methods, including recent reported approaches based on neural nets.



### Action-Affect Classification and Morphing using Multi-Task Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1603.06554v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1603.06554v1)
- **Published**: 2016-03-21 19:38:07+00:00
- **Updated**: 2016-03-21 19:38:07+00:00
- **Authors**: Timothy J. Shields, Mohamed R. Amer, Max Ehrlich, Amir Tamrakar
- **Comment**: None
- **Journal**: None
- **Summary**: Most recent work focused on affect from facial expressions, and not as much on body. This work focuses on body affect analysis. Affect does not occur in isolation. Humans usually couple affect with an action in natural interactions; for example, a person could be talking and smiling. Recognizing body affect in sequences requires efficient algorithms to capture both the micro movements that differentiate between happy and sad and the macro variations between different actions. We depart from traditional approaches for time-series data analytics by proposing a multi-task learning model that learns a shared representation that is well-suited for action-affect classification as well as generation. For this paper we choose Conditional Restricted Boltzmann Machines to be our building block. We propose a new model that enhances the CRBM model with a factored multi-task component to become Multi-Task Conditional Restricted Boltzmann Machines (MTCRBMs). We evaluate our approach on two publicly available datasets, the Body Affect dataset and the Tower Game dataset, and show superior classification performance improvement over the state-of-the-art, as well as the generative abilities of our model.



