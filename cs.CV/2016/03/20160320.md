# Arxiv Papers in cs.CV on 2016-03-20
### Towards Automatic Wild Animal Monitoring: Identification of Animal Species in Camera-trap Images using Very Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1603.06169v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06169v2)
- **Published**: 2016-03-20 00:47:46+00:00
- **Updated**: 2016-03-22 00:53:37+00:00
- **Authors**: Alexander Gomez, Augusto Salazar, Francisco Vargas
- **Comment**: Submitted to ECCV16
- **Journal**: None
- **Summary**: Non intrusive monitoring of animals in the wild is possible using camera trapping framework, which uses cameras triggered by sensors to take a burst of images of animals in their habitat. However camera trapping framework produces a high volume of data (in the order on thousands or millions of images), which must be analyzed by a human expert. In this work, a method for animal species identification in the wild using very deep convolutional neural networks is presented. Multiple versions of the Snapshot Serengeti dataset were used in order to probe the ability of the method to cope with different challenges that camera-trap images demand. The method reached 88.9% of accuracy in Top-1 and 98.1% in Top-5 in the evaluation set using a residual network topology. Also, the results show that the proposed method outperforms previous approximations and proves that recognition in camera-trap images can be automated.



### Segmentation from Natural Language Expressions
- **Arxiv ID**: http://arxiv.org/abs/1603.06180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06180v1)
- **Published**: 2016-03-20 04:10:53+00:00
- **Updated**: 2016-03-20 04:10:53+00:00
- **Authors**: Ronghang Hu, Marcus Rohrbach, Trevor Darrell
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we approach the novel problem of segmenting an image based on a natural language expression. This is different from traditional semantic segmentation over a predefined set of semantic classes, as e.g., the phrase "two men sitting on the right bench" requires segmenting only the two people on the right bench and no one standing or sitting on another bench. Previous approaches suitable for this task were limited to a fixed set of categories and/or rectangular regions. To produce pixelwise segmentation for the language expression, we propose an end-to-end trainable recurrent and convolutional network model that jointly learns to process visual and linguistic information. In our model, a recurrent LSTM network is used to encode the referential expression into a vector representation, and a fully convolutional network is used to a extract a spatial feature map from the image and output a spatial response map for the target object. We demonstrate on a benchmark dataset that our model can produce quality segmentation output from the natural language expression, and outperforms baseline methods by a large margin.



### Modelling Temporal Information Using Discrete Fourier Transform for Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1603.06182v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06182v5)
- **Published**: 2016-03-20 04:28:21+00:00
- **Updated**: 2016-08-17 00:48:55+00:00
- **Authors**: Haimin Zhang
- **Comment**: to be revised
- **Journal**: None
- **Summary**: Recently, video classification attracts intensive research efforts. However, most existing works are based on framelevel visual features, which might fail to model the temporal information, e.g. characteristics accumulated along time. In order to capture video temporal information, we propose to analyse features in frequency domain transformed by discrete Fourier transform (DFT features). Frame-level features are firstly extract by a pre-trained deep convolutional neural network (CNN). Then, time domain features are transformed and interpolated into DFT features. CNN and DFT features are further encoded by using different pooling methods and fused for video classification. In this way, static image features extracted from a pre-trained deep CNN and temporal information represented by DFT features are jointly considered for video classification. We test our method for video emotion classification and action recognition. Experimental results demonstrate that combining DFT features can effectively capture temporal information and therefore improve the performance of both video emotion classification and action recognition. Our approach has achieved a state-of-the-art performance on the largest video emotion dataset (VideoEmotion-8 dataset) and competitive results on UCF-101.



### Modelling Temporal Information Using Discrete Fourier Transform for Recognizing Emotions in User-generated Videos
- **Arxiv ID**: http://arxiv.org/abs/1603.06568v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06568v2)
- **Published**: 2016-03-20 04:46:00+00:00
- **Updated**: 2016-08-03 00:53:23+00:00
- **Authors**: Haimin Zhang, Min Xu
- **Comment**: 5 pages. arXiv admin note: substantial text overlap with
  arXiv:1603.06182
- **Journal**: None
- **Summary**: With the widespread of user-generated Internet videos, emotion recognition in those videos attracts increasing research efforts. However, most existing works are based on framelevel visual features and/or audio features, which might fail to model the temporal information, e.g. characteristics accumulated along time. In order to capture video temporal information, in this paper, we propose to analyse features in frequency domain transformed by discrete Fourier transform (DFT features). Frame-level features are firstly extract by a pre-trained deep convolutional neural network (CNN). Then, time domain features are transferred and interpolated into DFT features. CNN and DFT features are further encoded and fused for emotion classification. By this way, static image features extracted from a pre-trained deep CNN and temporal information represented by DFT features are jointly considered for video emotion recognition. Experimental results demonstrate that combining DFT features can effectively capture temporal information and therefore improve emotion recognition performance. Our approach has achieved a state-of-the-art performance on the largest video emotion dataset (VideoEmotion-8 dataset), improving accuracy from 51.1% to 62.6%.



### A Survey on Object Detection in Optical Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/1603.06201v2
- **DOI**: 10.1016/j.isprsjprs.2016.03.014
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06201v2)
- **Published**: 2016-03-20 11:09:30+00:00
- **Updated**: 2016-03-23 03:13:29+00:00
- **Authors**: Gong Cheng, Junwei Han
- **Comment**: This manuscript is the accepted version for ISPRS Journal of
  Photogrammetry and Remote Sensing
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing, 117: 11-28,
  2016
- **Summary**: Object detection in optical remote sensing images, being a fundamental but challenging problem in the field of aerial and satellite image analysis, plays an important role for a wide range of applications and is receiving significant attention in recent years. While enormous methods exist, a deep review of the literature concerning generic object detection is still lacking. This paper aims to provide a review of the recent progress in this field. Different from several previously published surveys that focus on a specific object class such as building and road, we concentrate on more generic object categories including, but are not limited to, road, building, tree, vehicle, ship, airport, urban-area. Covering about 270 publications we survey 1) template matching-based object detection methods, 2) knowledge-based object detection methods, 3) object-based image analysis (OBIA)-based object detection methods, 4) machine learning-based object detection methods, and 5) five publicly available datasets and three standard evaluation metrics. We also discuss the challenges of current studies and propose two promising research directions, namely deep learning-based feature representation and weakly supervised learning-based geospatial object detection. It is our hope that this survey will be beneficial for the researchers to have better understanding of this research field.



### RotationNet: Joint Object Categorization and Pose Estimation Using Multiviews from Unsupervised Viewpoints
- **Arxiv ID**: http://arxiv.org/abs/1603.06208v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06208v4)
- **Published**: 2016-03-20 12:20:06+00:00
- **Updated**: 2018-03-23 09:52:06+00:00
- **Authors**: Asako Kanezaki, Yasuyuki Matsushita, Yoshifumi Nishida
- **Comment**: 24 pages, 23 figures. Accepted to CVPR 2018
- **Journal**: None
- **Summary**: We propose a Convolutional Neural Network (CNN)-based model "RotationNet," which takes multi-view images of an object as input and jointly estimates its pose and object category. Unlike previous approaches that use known viewpoint labels for training, our method treats the viewpoint labels as latent variables, which are learned in an unsupervised manner during the training using an unaligned object dataset. RotationNet is designed to use only a partial set of multi-view images for inference, and this property makes it useful in practical scenarios where only partial views are available. Moreover, our pose alignment strategy enables one to obtain view-specific feature representations shared across classes, which is important to maintain high accuracy in both object categorization and pose estimation. Effectiveness of RotationNet is demonstrated by its superior performance to the state-of-the-art methods of 3D object classification on 10- and 40-class ModelNet datasets. We also show that RotationNet, even trained without known poses, achieves the state-of-the-art performance on an object pose estimation dataset. The code is available on https://github.com/kanezaki/rotationnet



