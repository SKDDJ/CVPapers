# Arxiv Papers in cs.CV on 2016-03-10
### Exploring Context with Deep Structured models for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1603.03183v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.03183v3)
- **Published**: 2016-03-10 08:34:19+00:00
- **Updated**: 2017-05-02 08:06:42+00:00
- **Authors**: Guosheng Lin, Chunhua Shen, Anton van den Hengel, Ian Reid
- **Comment**: 16 pages. Accepted to IEEE T. Pattern Analysis & Machine
  Intelligence, 2017. Extended version of arXiv:1504.01013
- **Journal**: None
- **Summary**: State-of-the-art semantic image segmentation methods are mostly based on training deep convolutional neural networks (CNNs). In this work, we proffer to improve semantic segmentation with the use of contextual information. In particular, we explore `patch-patch' context and `patch-background' context in deep CNNs. We formulate deep structured models by combining CNNs and Conditional Random Fields (CRFs) for learning the patch-patch context between image regions. Specifically, we formulate CNN-based pairwise potential functions to capture semantic correlations between neighboring patches. Efficient piecewise training of the proposed deep structured model is then applied in order to avoid repeated expensive CRF inference during the course of back propagation. For capturing the patch-background context, we show that a network design with traditional multi-scale image inputs and sliding pyramid pooling is very effective for improving performance. We perform comprehensive evaluation of the proposed method. We achieve new state-of-the-art performance on a number of challenging semantic segmentation datasets including $NYUDv2$, $PASCAL$-$VOC2012$, $Cityscapes$, $PASCAL$-$Context$, $SUN$-$RGBD$, $SIFT$-$flow$, and $KITTI$ datasets. Particularly, we report an intersection-over-union score of $77.8$ on the $PASCAL$-$VOC2012$ dataset.



### Instance-Aware Hashing for Multi-Label Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1603.03234v1
- **DOI**: 10.1109/TIP.2016.2545300
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.03234v1)
- **Published**: 2016-03-10 12:21:50+00:00
- **Updated**: 2016-03-10 12:21:50+00:00
- **Authors**: Hanjiang Lai, Pan Yan, Xiangbo Shu, Yunchao Wei, Shuicheng Yan
- **Comment**: has been accepted as a regular paper in the IEEE Transactions on
  Image Processing, 2016
- **Journal**: None
- **Summary**: Similarity-preserving hashing is a commonly used method for nearest neighbour search in large-scale image retrieval. For image retrieval, deep-networks-based hashing methods are appealing since they can simultaneously learn effective image representations and compact hash codes. This paper focuses on deep-networks-based hashing for multi-label images, each of which may contain objects of multiple categories. In most existing hashing methods, each image is represented by one piece of hash code, which is referred to as semantic hashing. This setting may be suboptimal for multi-label image retrieval. To solve this problem, we propose a deep architecture that learns \textbf{instance-aware} image representations for multi-label image data, which are organized in multiple groups, with each group containing the features for one category. The instance-aware representations not only bring advantages to semantic hashing, but also can be used in category-aware hashing, in which an image is represented by multiple pieces of hash codes and each piece of code corresponds to a category. Extensive evaluations conducted on several benchmark datasets demonstrate that, for both semantic hashing and category-aware hashing, the proposed method shows substantial improvement over the state-of-the-art supervised and unsupervised hashing methods.



### UTSig: A Persian Offline Signature Dataset
- **Arxiv ID**: http://arxiv.org/abs/1603.03235v4
- **DOI**: 10.1049/iet-bmt.2015.0058
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.03235v4)
- **Published**: 2016-03-10 12:23:03+00:00
- **Updated**: 2016-08-12 06:58:59+00:00
- **Authors**: Amir Soleimani, Kazim Fouladi, Babak N. Araabi
- **Comment**: 15 pages, 6 figures
- **Journal**: None
- **Summary**: The pivotal role of datasets in signature verification systems motivates researchers to collect signature samples. Distinct characteristics of Persian signature demands for richer and culture-dependent offline signature datasets. This paper introduces a new and public Persian offline signature dataset, UTSig, that consists of 8280 images from 115 classes. Each class has 27 genuine signatures, 3 opposite-hand signatures, and 42 skilled forgeries made by 6 forgers. Compared with the other public datasets, UTSig has more samples, more classes, and more forgers. We considered various variables including signing period, writing instrument, signature box size, and number of observable samples for forgers in the data collection procedure. By careful examination of main characteristics of offline signature datasets, we observe that Persian signatures have fewer numbers of branch points and end points. We propose and evaluate four different training and test setups for UTSig. Results of our experiments show that training genuine samples along with opposite-hand samples and random forgeries can improve the performance in terms of equal error rate and minimum cost of log likelihood ratio.



### Template Matching via Densities on the Roto-Translation Group
- **Arxiv ID**: http://arxiv.org/abs/1603.03304v5
- **DOI**: 10.1109/TPAMI.2017.2652452
- **Categories**: **cs.CV**, math.GR
- **Links**: [PDF](http://arxiv.org/pdf/1603.03304v5)
- **Published**: 2016-03-10 15:46:57+00:00
- **Updated**: 2017-03-09 15:05:21+00:00
- **Authors**: Erik J. Bekkers, Marco Loog, Bart M. ter Haar Romeny, Remco Duits
- **Comment**: 28 pages, 11 figures, 5 appendices. Paper accepted at ieee
  transaction on Pattern Analysis and Machine Inteligence (ieee tPAMI)
- **Journal**: None
- **Summary**: We propose a template matching method for the detection of 2D image objects that are characterized by orientation patterns. Our method is based on data representations via orientation scores, which are functions on the space of positions and orientations, and which are obtained via a wavelet-type transform. This new representation allows us to detect orientation patterns in an intuitive and direct way, namely via cross-correlations. Additionally, we propose a generalized linear regression framework for the construction of suitable templates using smoothing splines. Here, it is important to recognize a curved geometry on the position-orientation domain, which we identify with the Lie group SE(2): the roto-translation group. Templates are then optimized in a B-spline basis, and smoothness is defined with respect to the curved geometry. We achieve state-of-the-art results on three different applications: detection of the optic nerve head in the retina (99.83% success rate on 1737 images), of the fovea in the retina (99.32% success rate on 1616 images), and of the pupil in regular camera images (95.86% on 1521 images). The high performance is due to inclusion of both intensity and orientation features with effective geometric priors in the template matching. Moreover, our method is fast due to a cross-correlation based matching approach.



### Summary Transfer: Exemplar-based Subset Selection for Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/1603.03369v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.03369v3)
- **Published**: 2016-03-10 18:59:14+00:00
- **Updated**: 2016-04-29 08:11:32+00:00
- **Authors**: Ke Zhang, Wei-Lun Chao, Fei Sha, Kristen Grauman
- **Comment**: CVPR 2016 camera ready
- **Journal**: None
- **Summary**: Video summarization has unprecedented importance to help us digest, browse, and search today's ever-growing video collections. We propose a novel subset selection technique that leverages supervision in the form of human-created summaries to perform automatic keyframe-based video summarization. The main idea is to nonparametrically transfer summary structures from annotated videos to unseen test videos. We show how to extend our method to exploit semantic side information about the video's category/genre to guide the transfer process by those training videos semantically consistent with the test input. We also show how to generalize our method to subshot-based summarization, which not only reduces computational costs but also provides more flexible ways of defining visual similarity across subshots spanning several frames. We conduct extensive evaluation on several benchmarks and demonstrate promising results, outperforming existing methods in several settings.



### Temporally coherent 4D reconstruction of complex dynamic scenes
- **Arxiv ID**: http://arxiv.org/abs/1603.03381v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.03381v2)
- **Published**: 2016-03-10 19:16:43+00:00
- **Updated**: 2016-03-28 11:17:48+00:00
- **Authors**: Armin Mustafa, Hansung Kim, Jean-Yves Guillemaut, Adrian Hilton
- **Comment**: To appear in The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR) 2016 . Video available at:
  https://www.youtube.com/watch?v=bm_P13_-DsQ
- **Journal**: None
- **Summary**: This paper presents an approach for reconstruction of 4D temporally coherent models of complex dynamic scenes. No prior knowledge is required of scene structure or camera calibration allowing reconstruction from multiple moving cameras. Sparse-to-dense temporal correspondence is integrated with joint multi-view segmentation and reconstruction to obtain a complete 4D representation of static and dynamic objects. Temporal coherence is exploited to overcome visual ambiguities resulting in improved reconstruction of complex scenes. Robust joint segmentation and reconstruction of dynamic objects is achieved by introducing a geodesic star convexity constraint. Comparative evaluation is performed on a variety of unstructured indoor and outdoor dynamic scenes with hand-held cameras and multiple people. This demonstrates reconstruction of complete temporally coherent 4D scene models with improved nonrigid object segmentation and shape reconstruction.



### Texture Networks: Feed-forward Synthesis of Textures and Stylized Images
- **Arxiv ID**: http://arxiv.org/abs/1603.03417v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.03417v1)
- **Published**: 2016-03-10 20:45:40+00:00
- **Updated**: 2016-03-10 20:45:40+00:00
- **Authors**: Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, Victor Lempitsky
- **Comment**: None
- **Journal**: None
- **Summary**: Gatys et al. recently demonstrated that deep networks can generate beautiful textures and stylized images from a single texture example. However, their methods requires a slow and memory-consuming optimization process. We propose here an alternative approach that moves the computational burden to a learning stage. Given a single example of a texture, our approach trains compact feed-forward convolutional networks to generate multiple samples of the same texture of arbitrary size and to transfer artistic style from a given image to any other image. The resulting networks are remarkably light-weight and can generate textures of quality comparable to Gatys~et~al., but hundreds of times faster. More generally, our approach highlights the power and flexibility of generative feed-forward models trained with complex and expressive loss functions.



