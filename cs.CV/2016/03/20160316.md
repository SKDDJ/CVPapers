# Arxiv Papers in cs.CV on 2016-03-16
### DeepContext: Context-Encoding Neural Pathways for 3D Holistic Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/1603.04922v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.04922v4)
- **Published**: 2016-03-16 00:09:41+00:00
- **Updated**: 2017-08-16 04:12:50+00:00
- **Authors**: Yinda Zhang, Mingru Bai, Pushmeet Kohli, Shahram Izadi, Jianxiong Xiao
- **Comment**: Accepted by ICCV2017
- **Journal**: None
- **Summary**: While deep neural networks have led to human-level performance on computer vision tasks, they have yet to demonstrate similar gains for holistic scene understanding. In particular, 3D context has been shown to be an extremely important cue for scene understanding - yet very little research has been done on integrating context information with deep models. This paper presents an approach to embed 3D context into the topology of a neural network trained to perform holistic scene understanding. Given a depth image depicting a 3D scene, our network aligns the observed scene with a predefined 3D scene template, and then reasons about the existence and location of each object within the scene template. In doing so, our model recognizes multiple objects in a single forward pass of a 3D convolutional neural network, capturing both global scene and local object information simultaneously. To create training data for this 3D network, we generate partly hallucinated depth images which are rendered by replacing real objects with a repository of CAD models of the same object category. Extensive experiments demonstrate the effectiveness of our algorithm compared to the state-of-the-arts. Source code and data are available at http://deepcontext.cs.princeton.edu.



### Deep Fully-Connected Networks for Video Compressive Sensing
- **Arxiv ID**: http://arxiv.org/abs/1603.04930v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1603.04930v2)
- **Published**: 2016-03-16 01:15:35+00:00
- **Updated**: 2017-12-16 23:26:43+00:00
- **Authors**: Michael Iliadis, Leonidas Spinoulas, Aggelos K. Katsaggelos
- **Comment**: 14 pages, to appear in Elsevier Digital Signal Processing
- **Journal**: None
- **Summary**: In this work we present a deep learning framework for video compressive sensing. The proposed formulation enables recovery of video frames in a few seconds at significantly improved reconstruction quality compared to previous approaches. Our investigation starts by learning a linear mapping between video sequences and corresponding measured frames which turns out to provide promising results. We then extend the linear formulation to deep fully-connected networks and explore the performance gains using deeper architectures. Our analysis is always driven by the applicability of the proposed framework on existing compressive video architectures. Extensive simulations on several video sequences document the superiority of our approach both quantitatively and qualitatively. Finally, our analysis offers insights into understanding how dataset sizes and number of layers affect reconstruction performance while raising a few points for future investigation.   Code is available at Github: https://github.com/miliadis/DeepVideoCS



### Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue
- **Arxiv ID**: http://arxiv.org/abs/1603.04992v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.04992v2)
- **Published**: 2016-03-16 08:57:15+00:00
- **Updated**: 2016-07-29 03:20:46+00:00
- **Authors**: Ravi Garg, Vijay Kumar BG, Gustavo Carneiro, Ian Reid
- **Comment**: Accepted for publication at ECCV 2016
- **Journal**: None
- **Summary**: A significant weakness of most current deep Convolutional Neural Networks is the need to train them using vast amounts of manu- ally labelled data. In this work we propose a unsupervised framework to learn a deep convolutional neural network for single view depth predic- tion, without requiring a pre-training stage or annotated ground truth depths. We achieve this by training the network in a manner analogous to an autoencoder. At training time we consider a pair of images, source and target, with small, known camera motion between the two such as a stereo pair. We train the convolutional encoder for the task of predicting the depth map for the source image. To do so, we explicitly generate an inverse warp of the target image using the predicted depth and known inter-view displacement, to reconstruct the source image; the photomet- ric error in the reconstruction is the reconstruction loss for the encoder. The acquisition of this training data is considerably simpler than for equivalent systems, requiring no manual annotation, nor calibration of depth sensor to camera. We show that our network trained on less than half of the KITTI dataset (without any further augmentation) gives com- parable performance to that of the state of art supervised methods for single view depth estimation.



### Non-linear Dimensionality Regularizer for Solving Inverse Problems
- **Arxiv ID**: http://arxiv.org/abs/1603.05015v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.05015v1)
- **Published**: 2016-03-16 10:04:38+00:00
- **Updated**: 2016-03-16 10:04:38+00:00
- **Authors**: Ravi Garg, Anders Eriksson, Ian Reid
- **Comment**: None
- **Journal**: None
- **Summary**: Consider an ill-posed inverse problem of estimating causal factors from observations, one of which is known to lie near some (un- known) low-dimensional, non-linear manifold expressed by a predefined Mercer-kernel. Solving this problem requires simultaneous estimation of these factors and learning the low-dimensional representation for them. In this work, we introduce a novel non-linear dimensionality regulariza- tion technique for solving such problems without pre-training. We re-formulate Kernel-PCA as an energy minimization problem in which low dimensionality constraints are introduced as regularization terms in the energy. To the best of our knowledge, ours is the first at- tempt to create a dimensionality regularizer in the KPCA framework. Our approach relies on robustly penalizing the rank of the recovered fac- tors directly in the implicit feature space to create their low-dimensional approximations in closed form. Our approach performs robust KPCA in the presence of missing data and noise. We demonstrate state-of-the-art results on predicting missing entries in the standard oil flow dataset. Additionally, we evaluate our method on the challenging problem of Non-Rigid Structure from Motion and our approach delivers promising results on CMU mocap dataset despite the presence of significant occlusions and noise.



### Identity Mappings in Deep Residual Networks
- **Arxiv ID**: http://arxiv.org/abs/1603.05027v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1603.05027v3)
- **Published**: 2016-03-16 10:53:56+00:00
- **Updated**: 2016-07-25 15:18:32+00:00
- **Authors**: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
- **Comment**: ECCV 2016 camera-ready
- **Journal**: None
- **Summary**: Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers



### Descriptor transition tables for object retrieval using unconstrained cluttered video acquired using a consumer level handheld mobile device
- **Arxiv ID**: http://arxiv.org/abs/1603.05073v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.05073v2)
- **Published**: 2016-03-16 12:57:10+00:00
- **Updated**: 2016-03-21 12:02:40+00:00
- **Authors**: Warren Rieutort-Louis, Ognjen Arandjelovic
- **Comment**: 2016
- **Journal**: None
- **Summary**: Visual recognition and vision based retrieval of objects from large databases are tasks with a wide spectrum of potential applications. In this paper we propose a novel recognition method from video sequences suitable for retrieval from databases acquired in highly unconstrained conditions e.g. using a mobile consumer-level device such as a phone. On the lowest level, we represent each sequence as a 3D mesh of densely packed local appearance descriptors. While image plane geometry is captured implicitly by a large overlap of neighbouring regions from which the descriptors are extracted, 3D information is extracted by means of a descriptor transition table, learnt from a single sequence for each known gallery object. These allow us to connect local descriptors along the 3rd dimension (which corresponds to viewpoint changes), thus resulting in a set of variable length Markov chains for each video. The matching of two sets of such chains is formulated as a statistical hypothesis test, whereby a subset of each is chosen to maximize the likelihood that the corresponding video sequences show the same object. The effectiveness of the proposed algorithm is empirically evaluated on the Amsterdam Library of Object Images and a new highly challenging video data set acquired using a mobile phone. On both data sets our method is shown to be successful in recognition in the presence of background clutter and large viewpoint changes.



### Suppressing the Unusual: towards Robust CNNs using Symmetric Activation Functions
- **Arxiv ID**: http://arxiv.org/abs/1603.05145v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1603.05145v1)
- **Published**: 2016-03-16 15:35:07+00:00
- **Updated**: 2016-03-16 15:35:07+00:00
- **Authors**: Qiyang Zhao, Lewis D Griffin
- **Comment**: 11 pages, 12 figures
- **Journal**: None
- **Summary**: Many deep Convolutional Neural Networks (CNN) make incorrect predictions on adversarial samples obtained by imperceptible perturbations of clean samples. We hypothesize that this is caused by a failure to suppress unusual signals within network layers. As remedy we propose the use of Symmetric Activation Functions (SAF) in non-linear signal transducer units. These units suppress signals of exceptional magnitude. We prove that SAF networks can perform classification tasks to arbitrary precision in a simplified situation. In practice, rather than use SAFs alone, we add them into CNNs to improve their robustness. The modified CNNs can be easily trained using popular strategies with the moderate training load. Our experiments on MNIST and CIFAR-10 show that the modified CNNs perform similarly to plain ones on clean samples, and are remarkably more robust against adversarial and nonsense samples.



### 2D Discrete Fourier Transform with Simultaneous Edge Artifact Removal for Real-Time Applications
- **Arxiv ID**: http://arxiv.org/abs/1603.05154v1
- **DOI**: 10.1109/FPT.2015.7393157
- **Categories**: **cs.CV**, cs.AR
- **Links**: [PDF](http://arxiv.org/pdf/1603.05154v1)
- **Published**: 2016-03-16 15:52:13+00:00
- **Updated**: 2016-03-16 15:52:13+00:00
- **Authors**: Faisal Mahmood, Märt Toots, Lars-Göran Öfverstedt, Ulf Skoglund
- **Comment**: IEEE 2015 International Conference on Field Programmable Technology
  (FPT), Queenstown, New Zealand
- **Journal**: None
- **Summary**: Two-Dimensional (2D) Discrete Fourier Transform (DFT) is a basic and computationally intensive algorithm, with a vast variety of applications. 2D images are, in general, non-periodic, but are assumed to be periodic while calculating their DFTs. This leads to cross-shaped artifacts in the frequency domain due to spectral leakage. These artifacts can have critical consequences if the DFTs are being used for further processing. In this paper we present a novel FPGA-based design to calculate high-throughput 2D DFTs with simultaneous edge artifact removal. Standard approaches for removing these artifacts using apodization functions or mirroring, either involve removing critical frequencies or a surge in computation by increasing image size. We use a periodic-plus-smooth decomposition based artifact removal algorithm optimized for FPGA implementation, while still achieving real-time ($\ge$23 frames per second) performance for a 512$\times$512 size image stream. Our optimization approach leads to a significant decrease in external memory utilization thereby avoiding memory conflicts and simplifies the design. We have tested our design on a PXIe based Xilinx Kintex 7 FPGA system communicating with a host PC which gives us the advantage to further expand the design for industrial applications.



### Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units
- **Arxiv ID**: http://arxiv.org/abs/1603.05201v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1603.05201v2)
- **Published**: 2016-03-16 18:17:36+00:00
- **Updated**: 2016-07-19 05:18:36+00:00
- **Authors**: Wenling Shang, Kihyuk Sohn, Diogo Almeida, Honglak Lee
- **Comment**: ICML 2016
- **Journal**: None
- **Summary**: Recently, convolutional neural networks (CNNs) have been used as a powerful tool to solve many problems of machine learning and computer vision. In this paper, we aim to provide insight on the property of convolutional neural networks, as well as a generic method to improve the performance of many CNN architectures. Specifically, we first examine existing CNN models and observe an intriguing property that the filters in the lower layers form pairs (i.e., filters with opposite phase). Inspired by our observation, we propose a novel, simple yet effective activation scheme called concatenated ReLU (CRelu) and theoretically analyze its reconstruction property in CNNs. We integrate CRelu into several state-of-the-art CNN architectures and demonstrate improvement in their recognition performance on CIFAR-10/100 and ImageNet datasets with fewer trainable parameters. Our results suggest that better understanding of the properties of CNNs can lead to significant performance improvement with a simple modification.



### XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1603.05279v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.05279v4)
- **Published**: 2016-03-16 20:56:21+00:00
- **Updated**: 2016-08-02 20:59:14+00:00
- **Authors**: Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, Ali Farhadi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values resulting in 32x memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58x faster convolutional operations and 32x memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is only 2.9% less than the full-precision AlexNet (in top-1 measure). We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and outperform these methods by large margins on ImageNet, more than 16% in top-1 accuracy.



### Image Labeling by Assignment
- **Arxiv ID**: http://arxiv.org/abs/1603.05285v1
- **DOI**: 10.1007/s10851-016-0702-4
- **Categories**: **cs.CV**, math.OC, 62H35, 65K05, 68U10, 62M40
- **Links**: [PDF](http://arxiv.org/pdf/1603.05285v1)
- **Published**: 2016-03-16 21:15:49+00:00
- **Updated**: 2016-03-16 21:15:49+00:00
- **Authors**: Freddie Åström, Stefania Petra, Bernhard Schmitzer, Christoph Schnörr
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a novel geometric approach to the image labeling problem. Abstracting from specific labeling applications, a general objective function is defined on a manifold of stochastic matrices, whose elements assign prior data that are given in any metric space, to observed image measurements. The corresponding Riemannian gradient flow entails a set of replicator equations, one for each data point, that are spatially coupled by geometric averaging on the manifold. Starting from uniform assignments at the barycenter as natural initialization, the flow terminates at some global maximum, each of which corresponds to an image labeling that uniquely assigns the prior data. Our geometric variational approach constitutes a smooth non-convex inner approximation of the general image labeling problem, implemented with sparse interior-point numerics in terms of parallel multiplicative updates that converge efficiently.



### Persistent Homology of Attractors For Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1603.05310v1
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1603.05310v1)
- **Published**: 2016-03-16 23:15:50+00:00
- **Updated**: 2016-03-16 23:15:50+00:00
- **Authors**: Vinay Venkataraman, Karthikeyan Natesan Ramamurthy, Pavan Turaga
- **Comment**: 5 pages, Under review in International Conference on Image Processing
- **Journal**: None
- **Summary**: In this paper, we propose a novel framework for dynamical analysis of human actions from 3D motion capture data using topological data analysis. We model human actions using the topological features of the attractor of the dynamical system. We reconstruct the phase-space of time series corresponding to actions using time-delay embedding, and compute the persistent homology of the phase-space reconstruction. In order to better represent the topological properties of the phase-space, we incorporate the temporal adjacency information when computing the homology groups. The persistence of these homology groups encoded using persistence diagrams are used as features for the actions. Our experiments with action recognition using these features demonstrate that the proposed approach outperforms other baseline methods.



