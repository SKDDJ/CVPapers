# Arxiv Papers in cs.CV on 2016-03-29
### The Conditional Lucas & Kanade Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1603.08597v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.08597v1)
- **Published**: 2016-03-29 00:34:07+00:00
- **Updated**: 2016-03-29 00:34:07+00:00
- **Authors**: Chen-Hsuan Lin, Rui Zhu, Simon Lucey
- **Comment**: 17 pages, 11 figures
- **Journal**: None
- **Summary**: The Lucas & Kanade (LK) algorithm is the method of choice for efficient dense image and object alignment. The approach is efficient as it attempts to model the connection between appearance and geometric displacement through a linear relationship that assumes independence across pixel coordinates. A drawback of the approach, however, is its generative nature. Specifically, its performance is tightly coupled with how well the linear model can synthesize appearance from geometric displacement, even though the alignment task itself is associated with the inverse problem. In this paper, we present a new approach, referred to as the Conditional LK algorithm, which: (i) directly learns linear models that predict geometric displacement as a function of appearance, and (ii) employs a novel strategy for ensuring that the generative pixel independence assumption can still be taken advantage of. We demonstrate that our approach exhibits superior performance to classical generative forms of the LK algorithm. Furthermore, we demonstrate its comparable performance to state-of-the-art methods such as the Supervised Descent Method with substantially less training examples, as well as the unique ability to "swap" geometric warp functions without having to retrain from scratch. Finally, from a theoretical perspective, our approach hints at possible redundancies that exist in current state-of-the-art methods for alignment that could be leveraged in vision systems of the future.



### Classification of Alzheimer's Disease using fMRI Data and Deep Learning Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1603.08631v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.08631v1)
- **Published**: 2016-03-29 04:30:07+00:00
- **Updated**: 2016-03-29 04:30:07+00:00
- **Authors**: Saman Sarraf, Ghassem Tofighi
- **Comment**: None
- **Journal**: None
- **Summary**: Over the past decade, machine learning techniques especially predictive modeling and pattern recognition in biomedical sciences from drug delivery system to medical imaging has become one of the important methods which are assisting researchers to have deeper understanding of entire issue and to solve complex medical problems. Deep learning is power learning machine learning algorithm in classification while extracting high-level features. In this paper, we used convolutional neural network to classify Alzheimer's brain from normal healthy brain. The importance of classifying this kind of medical data is to potentially develop a predict model or system in order to recognize the type disease from normal subjects or to estimate the stage of the disease. Classification of clinical data such as Alzheimer's disease has been always challenging and most problematic part has been always selecting the most discriminative features. Using Convolutional Neural Network (CNN) and the famous architecture LeNet-5, we successfully classified functional MRI data of Alzheimer's subjects from normal controls where the accuracy of test data on trained data reached 96.85%. This experiment suggests us the shift and scale invariant features extracted by CNN followed by deep learning classification is most powerful method to distinguish clinical data from healthy data in fMRI. This approach also enables us to expand our methodology to predict more complicated systems.



### Learning a Predictable and Generative Vector Representation for Objects
- **Arxiv ID**: http://arxiv.org/abs/1603.08637v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.08637v2)
- **Published**: 2016-03-29 04:36:18+00:00
- **Updated**: 2016-08-31 15:39:29+00:00
- **Authors**: Rohit Girdhar, David F. Fouhey, Mikel Rodriguez, Abhinav Gupta
- **Comment**: To appear in ECCV 2016. Project webpage:
  rohitgirdhar.github.io/GenerativePredictableVoxels/
- **Journal**: None
- **Summary**: What is a good vector representation of an object? We believe that it should be generative in 3D, in the sense that it can produce new 3D objects; as well as be predictable from 2D, in the sense that it can be perceived from 2D images. We propose a novel architecture, called the TL-embedding network, to learn an embedding space with these properties. The network consists of two components: (a) an autoencoder that ensures the representation is generative; and (b) a convolutional network that ensures the representation is predictable. This enables tackling a number of tasks including voxel prediction from 2D images and 3D model retrieval. Extensive experimental analysis demonstrates the usefulness and versatility of this embedding.



### Instance-sensitive Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1603.08678v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.08678v1)
- **Published**: 2016-03-29 08:37:26+00:00
- **Updated**: 2016-03-29 08:37:26+00:00
- **Authors**: Jifeng Dai, Kaiming He, Yi Li, Shaoqing Ren, Jian Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Fully convolutional networks (FCNs) have been proven very successful for semantic segmentation, but the FCN outputs are unaware of object instances. In this paper, we develop FCNs that are capable of proposing instance-level segment candidates. In contrast to the previous FCN that generates one score map, our FCN is designed to compute a small set of instance-sensitive score maps, each of which is the outcome of a pixel-wise classifier of a relative position to instances. On top of these instance-sensitive score maps, a simple assembling module is able to output instance candidate at each position. In contrast to the recent DeepMask method for segmenting instances, our method does not have any high-dimensional layer related to the mask resolution, but instead exploits image local coherence for estimating instances. We present competitive results of instance segment proposal on both PASCAL VOC and MS COCO.



### Learning to Refine Object Segments
- **Arxiv ID**: http://arxiv.org/abs/1603.08695v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.08695v2)
- **Published**: 2016-03-29 09:33:44+00:00
- **Updated**: 2016-07-26 20:40:51+00:00
- **Authors**: Pedro O. Pinheiro, Tsung-Yi Lin, Ronan Collobert, Piotr Doll√†r
- **Comment**: extended version of ECCV camera-ready (figures 6-9 only in arXiv)
- **Journal**: None
- **Summary**: Object segmentation requires both object-level information and low-level pixel data. This presents a challenge for feedforward networks: lower layers in convolutional nets capture rich spatial information, while upper layers encode object-level knowledge but are invariant to factors such as pose and appearance. In this work we propose to augment feedforward nets for object segmentation with a novel top-down refinement approach. The resulting bottom-up/top-down architecture is capable of efficiently generating high-fidelity object masks. Similarly to skip connections, our approach leverages features at all layers of the net. Unlike skip connections, our approach does not attempt to output independent predictions at each layer. Instead, we first output a coarse `mask encoding' in a feedforward pass, then refine this mask encoding in a top-down pass utilizing features at successively lower layers. The approach is simple, fast, and effective. Building on the recent DeepMask network for generating object proposals, we show accuracy improvements of 10-20% in average recall for various setups. Additionally, by optimizing the overall network architecture, our approach, which we call SharpMask, is 50% faster than the original DeepMask network (under .8s per image).



### Multi-Band Image Fusion Based on Spectral Unmixing
- **Arxiv ID**: http://arxiv.org/abs/1603.08720v1
- **DOI**: 10.1109/TGRS.2016.2598784
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.08720v1)
- **Published**: 2016-03-29 10:54:21+00:00
- **Updated**: 2016-03-29 10:54:21+00:00
- **Authors**: Qi Wei, Jose Bioucas-Dias, Nicolas Dobigeon, Jean-Yves Tourneret, Marcus Chen, Simon Godsill
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a multi-band image fusion algorithm based on unsupervised spectral unmixing for combining a high-spatial low-spectral resolution image and a low-spatial high-spectral resolution image. The widely used linear observation model (with additive Gaussian noise) is combined with the linear spectral mixture model to form the likelihoods of the observations. The non-negativity and sum-to-one constraints resulting from the intrinsic physical properties of the abundances are introduced as prior information to regularize this ill-posed problem. The joint fusion and unmixing problem is then formulated as maximizing the joint posterior distribution with respect to the endmember signatures and abundance maps, This optimization problem is attacked with an alternating optimization strategy. The two resulting sub-problems are convex and are solved efficiently using the alternating direction method of multipliers. Experiments are conducted for both synthetic and semi-real data. Simulation results show that the proposed unmixing based fusion scheme improves both the abundance and endmember estimation comparing with the state-of-the-art joint fusion and unmixing algorithms.



### Multi-Cue Zero-Shot Learning with Strong Supervision
- **Arxiv ID**: http://arxiv.org/abs/1603.08754v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.08754v1)
- **Published**: 2016-03-29 13:04:21+00:00
- **Updated**: 2016-03-29 13:04:21+00:00
- **Authors**: Zeynep Akata, Mateusz Malinowski, Mario Fritz, Bernt Schiele
- **Comment**: 2016 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)
- **Journal**: None
- **Summary**: Scaling up visual category recognition to large numbers of classes remains challenging. A promising research direction is zero-shot learning, which does not require any training data to recognize new classes, but rather relies on some form of auxiliary information describing the new classes. Ultimately, this may allow to use textbook knowledge that humans employ to learn about new classes by transferring knowledge from classes they know well. The most successful zero-shot learning approaches currently require a particular type of auxiliary information -- namely attribute annotations performed by humans -- that is not readily available for most classes. Our goal is to circumvent this bottleneck by substituting such annotations by extracting multiple pieces of information from multiple unstructured text sources readily available on the web. To compensate for the weaker form of auxiliary information, we incorporate stronger supervision in the form of semantic part annotations on the classes from which we transfer knowledge. We achieve our goal by a joint embedding framework that maps multiple text parts as well as multiple semantic parts into a common space. Our results consistently and significantly improve on the state-of-the-art in zero-short recognition and retrieval.



### Scalable Solution for Approximate Nearest Subspace Search
- **Arxiv ID**: http://arxiv.org/abs/1603.08810v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.08810v1)
- **Published**: 2016-03-29 15:24:43+00:00
- **Updated**: 2016-03-29 15:24:43+00:00
- **Authors**: Masakazu Iwamura, Masataka Konishi, Koichi Kise
- **Comment**: None
- **Journal**: None
- **Summary**: Finding the nearest subspace is a fundamental problem and influential to many applications. In particular, a scalable solution that is fast and accurate for a large problem has a great impact. The existing methods for the problem are, however, useless in a large-scale problem with a large number of subspaces and high dimensionality of the feature space. A cause is that they are designed based on the traditional idea to represent a subspace by a single point. In this paper, we propose a scalable solution for the approximate nearest subspace search (ANSS) problem. Intuitively, the proposed method represents a subspace by multiple points unlike the existing methods. This makes a large-scale ANSS problem tractable. In the experiment with 3036 subspaces in the 1024-dimensional space, we confirmed that the proposed method was 7.3 times faster than the previous state-of-the-art without loss of accuracy.



### Face Image Analysis using AAM, Gabor, LBP and WD features for Gender, Age, Expression and Ethnicity Classification
- **Arxiv ID**: http://arxiv.org/abs/1604.01684v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01684v1)
- **Published**: 2016-03-29 17:49:14+00:00
- **Updated**: 2016-03-29 17:49:14+00:00
- **Authors**: N. S. Lakshmiprabha
- **Comment**: None
- **Journal**: None
- **Summary**: The growth in electronic transactions and human machine interactions rely on the information such as gender, age, expression and ethnicity provided by the face image. In order to obtain these information, feature extraction plays a major role. In this paper, retrieval of age, gender, expression and race information from an individual face image is analysed using different feature extraction methods. The performance of four major feature extraction methods such as Active Appearance Model (AAM), Gabor wavelets, Local Binary Pattern (LBP) and Wavelet Decomposition (WD) are analyzed for gender recognition, age estimation, expression recognition and racial recognition in terms of accuracy (recognition rate), time for feature extraction, neural training and time to test an image. Each of this recognition system is compared with four feature extractors on same dataset (training and validation set) to get a better understanding in its performance. Experiments carried out on FG-NET, Cohn-Kanade, PAL face database shows that each method has its own merits and demerits. Hence it is practically impossible to define a method which is best at all circumstances with less computational complexity. Further, a detailed comparison of age estimation and age estimation using gender information is provided along with a solution to overcome aging effect in case of gender recognition. An attempt has been made in obtaining all (i.e. gender, age range, expression and ethnicity) information from a test image in a single go.



### Fusing Face and Periocular biometrics using Canonical correlation analysis
- **Arxiv ID**: http://arxiv.org/abs/1604.01683v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01683v1)
- **Published**: 2016-03-29 18:23:49+00:00
- **Updated**: 2016-03-29 18:23:49+00:00
- **Authors**: N. S. Lakshmiprabha
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel face and periocular biometric fusion at feature level using canonical correlation analysis. Face recognition itself has limitations such as illumination, pose, expression, occlusion etc. Also, periocular biometrics has spectacles, head angle, hair and expression as its limitations. Unimodal biometrics cannot surmount all these limitations. The recognition accuracy can be increased by fusing dual information (face and periocular) from a single source (face image) using canonical correlation analysis (CCA). This work also proposes a new wavelet decomposed local binary pattern (WD-LBP) feature extractor which provides sufficient features for fusion. A detailed analysis on face and periocular biometrics shows that WD-LBP features are more accurate and faster than local binary pattern (LBP) and gabor wavelet. The experimental results using Muct face database reveals that the proposed multimodal biometrics performs better than the unimodal biometrics.



### Latent Embeddings for Zero-shot Classification
- **Arxiv ID**: http://arxiv.org/abs/1603.08895v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.08895v2)
- **Published**: 2016-03-29 19:24:38+00:00
- **Updated**: 2016-04-10 10:33:02+00:00
- **Authors**: Yongqin Xian, Zeynep Akata, Gaurav Sharma, Quynh Nguyen, Matthias Hein, Bernt Schiele
- **Comment**: 2016 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)
- **Journal**: None
- **Summary**: We present a novel latent embedding model for learning a compatibility function between image and class embeddings, in the context of zero-shot classification. The proposed method augments the state-of-the-art bilinear compatibility model by incorporating latent variables. Instead of learning a single bilinear map, it learns a collection of maps with the selection, of which map to use, being a latent variable for the current image-class pair. We train the model with a ranking based objective function which penalizes incorrect rankings of the true class for a given image. We empirically demonstrate that our model improves the state-of-the-art for various class embeddings consistently on three challenging publicly available datasets for the zero-shot setting. Moreover, our method leads to visually highly interpretable results with clear clusters of different fine-grained object properties that correspond to different latent variable maps.



### Cross-modal Supervision for Learning Active Speaker Detection in Video
- **Arxiv ID**: http://arxiv.org/abs/1603.08907v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.08907v1)
- **Published**: 2016-03-29 19:47:46+00:00
- **Updated**: 2016-03-29 19:47:46+00:00
- **Authors**: Punarjay Chakravarty, Tinne Tuytelaars
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: In this paper, we show how to use audio to supervise the learning of active speaker detection in video. Voice Activity Detection (VAD) guides the learning of the vision-based classifier in a weakly supervised manner. The classifier uses spatio-temporal features to encode upper body motion - facial expressions and gesticulations associated with speaking. We further improve a generic model for active speaker detection by learning person specific models. Finally, we demonstrate the online adaptation of generic models learnt on one dataset, to previously unseen people in a new dataset, again using audio (VAD) for weak supervision. The use of temporal continuity overcomes the lack of clean training data. We are the first to present an active speaker detection system that learns on one audio-visual dataset and automatically adapts to speakers in a new dataset. This work can be seen as an example of how the availability of multi-modal data allows us to learn a model without the need for supervision, by transferring knowledge from one modality to another.



### Sweep Distortion Removal from THz Images via Blind Demodulation
- **Arxiv ID**: http://arxiv.org/abs/1604.03426v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1604.03426v1)
- **Published**: 2016-03-29 20:01:16+00:00
- **Updated**: 2016-03-29 20:01:16+00:00
- **Authors**: Alireza Aghasi, Barmak Heshmat, Albert Redo-Sanchez, Justin Romberg, Ramesh Raskar
- **Comment**: None
- **Journal**: None
- **Summary**: Heavy sweep distortion induced by alignments and inter-reflections of layers of a sample is a major burden in recovering 2D and 3D information in time resolved spectral imaging. This problem cannot be addressed by conventional denoising and signal processing techniques as it heavily depends on the physics of the acquisition. Here we propose and implement an algorithmic framework based on low-rank matrix recovery and alternating minimization that exploits the forward model for THz acquisition. The method allows recovering the original signal in spite of the presence of temporal-spatial distortions. We address a blind-demodulation problem, where based on several observations of the sample texture modulated by an undesired sweep pattern, the two classes of signals are separated. The performance of the method is examined in both synthetic and experimental data, and the successful reconstructions are demonstrated. The proposed general scheme can be implemented to advance inspection and imaging applications in THz and other time-resolved sensing modalities.



### FAST: A Framework to Accelerate Super-Resolution Processing on Compressed Videos
- **Arxiv ID**: http://arxiv.org/abs/1603.08968v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.08968v2)
- **Published**: 2016-03-29 21:07:16+00:00
- **Updated**: 2017-08-04 18:20:25+00:00
- **Authors**: Zhengdong Zhang, Vivienne Sze
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art super-resolution (SR) algorithms require significant computational resources to achieve real-time throughput (e.g., 60Mpixels/s for HD video). This paper introduces FAST (Free Adaptive Super-resolution via Transfer), a framework to accelerate any SR algorithm applied to compressed videos. FAST exploits the temporal correlation between adjacent frames such that SR is only applied to a subset of frames; SR pixels are then transferred to the other frames. The transferring process has negligible computation cost as it uses information already embedded in the compressed video (e.g., motion vectors and residual). Adaptive processing is used to retain accuracy when the temporal correlation is not present (e.g., occlusions). FAST accelerates state-of-the-art SR algorithms by up to 15x with a visual quality loss of 0.2dB. FAST is an important step towards real-time SR algorithms for ultra-HD displays and energy constrained devices (e.g., phones and tablets).



### SMASH: Physics-guided Reconstruction of Collisions from Videos
- **Arxiv ID**: http://arxiv.org/abs/1603.08984v2
- **DOI**: 10.1145/2980179.2982421
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1603.08984v2)
- **Published**: 2016-03-29 22:18:29+00:00
- **Updated**: 2017-04-10 11:09:34+00:00
- **Authors**: Aron Monszpart, Nils Thuerey, Niloy J. Mitra
- **Comment**: SIGGRAPH Asia 2016
- **Journal**: ACM Trans. Graph. 35, 6, Article 199 (November 2016), 14 pages
  (2016)
- **Summary**: Collision sequences are commonly used in games and entertainment to add drama and excitement. Authoring even two body collisions in the real world can be difficult, as one has to get timing and the object trajectories to be correctly synchronized. After tedious trial-and-error iterations, when objects can actually be made to collide, then they are difficult to capture in 3D. In contrast, synthetically generating plausible collisions is difficult as it requires adjusting different collision parameters (e.g., object mass ratio, coefficient of restitution, etc.) and appropriate initial parameters. We present SMASH to directly read off appropriate collision parameters directly from raw input video recordings. Technically we enable this by utilizing laws of rigid body collision to regularize the problem of lifting 2D trajectories to a physically valid 3D reconstruction of the collision. The reconstructed sequences can then be modified and combined to easily author novel and plausible collisions. We evaluate our system on a range of synthetic scenes and demonstrate the effectiveness of our method by accurately reconstructing several complex real world collision events.



