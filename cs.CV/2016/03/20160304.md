# Arxiv Papers in cs.CV on 2016-03-04
### Learning deep representation of multityped objects and tasks
- **Arxiv ID**: http://arxiv.org/abs/1603.01359v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1603.01359v1)
- **Published**: 2016-03-04 06:34:24+00:00
- **Updated**: 2016-03-04 06:34:24+00:00
- **Authors**: Truyen Tran, Dinh Phung, Svetha Venkatesh
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a deep multitask architecture to integrate multityped representations of multimodal objects. This multitype exposition is less abstract than the multimodal characterization, but more machine-friendly, and thus is more precise to model. For example, an image can be described by multiple visual views, which can be in the forms of bag-of-words (counts) or color/texture histograms (real-valued). At the same time, the image may have several social tags, which are best described using a sparse binary vector. Our deep model takes as input multiple type-specific features, narrows the cross-modality semantic gaps, learns cross-type correlation, and produces a high-level homogeneous representation. At the same time, the model supports heterogeneously typed tasks. We demonstrate the capacity of the model on two applications: social image retrieval and multiple concept prediction. The deep architecture produces more compact representation, naturally integrates multiviews and multimodalities, exploits better side information, and most importantly, performs competitively against baselines.



### Dynamic Memory Networks for Visual and Textual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1603.01417v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1603.01417v1)
- **Published**: 2016-03-04 10:40:28+00:00
- **Updated**: 2016-03-04 10:40:28+00:00
- **Authors**: Caiming Xiong, Stephen Merity, Richard Socher
- **Comment**: None
- **Journal**: None
- **Summary**: Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering. One such architecture, the dynamic memory network (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the \babi-10k text question-answering dataset without supporting fact supervision.



### Depth Superresolution using Motion Adaptive Regularization
- **Arxiv ID**: http://arxiv.org/abs/1603.01633v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.01633v1)
- **Published**: 2016-03-04 21:16:21+00:00
- **Updated**: 2016-03-04 21:16:21+00:00
- **Authors**: Ulugbek S. Kamilov, Petros T. Boufounos
- **Comment**: None
- **Journal**: None
- **Summary**: Spatial resolution of depth sensors is often significantly lower compared to that of conventional optical cameras. Recent work has explored the idea of improving the resolution of depth using higher resolution intensity as a side information. In this paper, we demonstrate that further incorporating temporal information in videos can significantly improve the results. In particular, we propose a novel approach that improves depth resolution, exploiting the space-time redundancy in the depth and intensity using motion-adaptive low-rank regularization. Experiments confirm that the proposed approach substantially improves the quality of the estimated high-resolution depth. Our approach can be a first component in systems using vision techniques that rely on high resolution depth information.



