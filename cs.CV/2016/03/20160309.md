# Arxiv Papers in cs.CV on 2016-03-09
### Image Captioning and Visual Question Answering Based on Attributes and External Knowledge
- **Arxiv ID**: http://arxiv.org/abs/1603.02814v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.02814v2)
- **Published**: 2016-03-09 08:56:45+00:00
- **Updated**: 2016-12-16 11:44:34+00:00
- **Authors**: Qi Wu, Chunhua Shen, Anton van den Hengel, Peng Wang, Anthony Dick
- **Comment**: 14 pages. arXiv admin note: text overlap with arXiv:1511.06973
- **Journal**: None
- **Summary**: Much recent progress in Vision-to-Language problems has been achieved through a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). This approach does not explicitly represent high-level semantic concepts, but rather seeks to progress directly from image features to text. In this paper we first propose a method of incorporating high-level concepts into the successful CNN-RNN approach, and show that it achieves a significant improvement on the state-of-the-art in both image captioning and visual question answering. We further show that the same mechanism can be used to incorporate external knowledge, which is critically important for answering high level visual questions. Specifically, we design a visual question answering model that combines an internal representation of the content of an image with information extracted from a general knowledge base to answer a broad range of image-based questions. It particularly allows questions to be asked about the contents of an image, even when the image itself does not contain a complete answer. Our final model achieves the best reported results on both image captioning and visual question answering on several benchmark datasets.



### Fast Training of Triplet-based Deep Binary Embedding Networks
- **Arxiv ID**: http://arxiv.org/abs/1603.02844v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.02844v2)
- **Published**: 2016-03-09 11:10:12+00:00
- **Updated**: 2016-08-01 01:52:57+00:00
- **Authors**: Bohan Zhuang, Guosheng Lin, Chunhua Shen, Ian Reid
- **Comment**: Apeparing in Proc. IEEE Conf. Computer Vision and Pattern Recognition
  2016. Code is at
  https://bitbucket.org/jingruixiaozhuang/fast-training-of-triplet-based-deep-binary-embedding-networks
- **Journal**: None
- **Summary**: In this paper, we aim to learn a mapping (or embedding) from images to a compact binary space in which Hamming distances correspond to a ranking measure for the image retrieval task.   We make use of a triplet loss because this has been shown to be most effective for ranking problems.   However, training in previous works can be prohibitively expensive due to the fact that optimization is directly performed on the triplet space, where the number of possible triplets for training is cubic in the number of training examples.   To address this issue, we propose to formulate high-order binary codes learning as a multi-label classification problem by explicitly separating learning into two interleaved stages.   To solve the first stage, we design a large-scale high-order binary codes inference algorithm to reduce the high-order objective to a standard binary quadratic problem such that graph cuts can be used to efficiently infer the binary code which serve as the label of each training datum.   In the second stage we propose to map the original image to compact binary codes via carefully designed deep convolutional neural networks (CNNs) and the hashing function fitting can be solved by training binary CNN classifiers.   An incremental/interleaved optimization strategy is proffered to ensure that these two steps are interactive with each other during training for better accuracy.   We conduct experiments on several benchmark datasets, which demonstrate both improved training time (by as much as two orders of magnitude) as well as producing state-of-the-art hashing for various retrieval tasks.



### Recursive Recurrent Nets with Attention Modeling for OCR in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1603.03101v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.03101v1)
- **Published**: 2016-03-09 23:49:51+00:00
- **Updated**: 2016-03-09 23:49:51+00:00
- **Authors**: Chen-Yu Lee, Simon Osindero
- **Comment**: accepted at CVPR 2016
- **Journal**: None
- **Summary**: We present recursive recurrent neural networks with attention modeling (R$^2$AM) for lexicon-free optical character recognition in natural scene images. The primary advantages of the proposed method are: (1) use of recursive convolutional neural networks (CNNs), which allow for parametrically efficient and effective image feature extraction; (2) an implicitly learned character-level language model, embodied in a recurrent neural network which avoids the need to use N-grams; and (3) the use of a soft-attention mechanism, allowing the model to selectively exploit image features in a coordinated way, and allowing for end-to-end training within a standard backpropagation framework. We validate our method with state-of-the-art performance on challenging benchmark datasets: Street View Text, IIIT5k, ICDAR and Synth90k.



