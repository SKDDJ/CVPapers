# Arxiv Papers in cs.CV on 2016-03-24
### A Diagram Is Worth A Dozen Images
- **Arxiv ID**: http://arxiv.org/abs/1603.07396v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1603.07396v1)
- **Published**: 2016-03-24 00:02:58+00:00
- **Updated**: 2016-03-24 00:02:58+00:00
- **Authors**: Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi
- **Comment**: None
- **Journal**: None
- **Summary**: Diagrams are common tools for representing complex concepts, relationships and events, often when it would be difficult to portray the same information with natural images. Understanding natural images has been extensively studied in computer vision, while diagram understanding has received little attention. In this paper, we study the problem of diagram interpretation and reasoning, the challenging task of identifying the structure of a diagram and the semantics of its constituents and their relationships. We introduce Diagram Parse Graphs (DPG) as our representation to model the structure of diagrams. We define syntactic parsing of diagrams as learning to infer DPGs for diagrams and study semantic interpretation and reasoning of diagrams in the context of diagram question answering. We devise an LSTM-based method for syntactic parsing of diagrams and introduce a DPG-based attention model for diagram question answering. We compile a new dataset of diagrams with exhaustive annotations of constituents and relationships for over 5,000 diagrams and 15,000 questions and answers. Our results show the significance of our models for syntactic parsing and question answering in diagrams using DPGs.



### Attentive Contexts for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1603.07415v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07415v1)
- **Published**: 2016-03-24 02:18:37+00:00
- **Updated**: 2016-03-24 02:18:37+00:00
- **Authors**: Jianan Li, Yunchao Wei, Xiaodan Liang, Jian Dong, Tingfa Xu, Jiashi Feng, Shuicheng Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Modern deep neural network based object detection methods typically classify candidate proposals using their interior features. However, global and local surrounding contexts that are believed to be valuable for object detection are not fully exploited by existing methods yet. In this work, we take a step towards understanding what is a robust practice to extract and utilize contextual information to facilitate object detection in practice. Specifically, we consider the following two questions: "how to identify useful global contextual information for detecting a certain object?" and "how to exploit local context surrounding a proposal for better inferring its contents?". We provide preliminary answers to these questions through developing a novel Attention to Context Convolution Neural Network (AC-CNN) based object detection model. AC-CNN effectively incorporates global and local contextual information into the region-based CNN (e.g. Fast RCNN) detection model and provides better object detection performance. It consists of one attention-based global contextualized (AGC) sub-network and one multi-scale local contextualized (MLC) sub-network. To capture global context, the AGC sub-network recurrently generates an attention map for an input image to highlight useful global contextual locations, through multiple stacked Long Short-Term Memory (LSTM) layers. For capturing surrounding local context, the MLC sub-network exploits both the inside and outside contextual information of each specific proposal at multiple scales. The global and local context are then fused together for making the final decision for detection. Extensive experiments on PASCAL VOC 2007 and VOC 2012 well demonstrate the superiority of the proposed AC-CNN over well-established baselines. In particular, AC-CNN outperforms the popular Fast-RCNN by 2.0% and 2.2% on VOC 2007 and VOC 2012 in terms of mAP, respectively.



### Pixel-Level Domain Transfer
- **Arxiv ID**: http://arxiv.org/abs/1603.07442v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1603.07442v3)
- **Published**: 2016-03-24 05:20:59+00:00
- **Updated**: 2016-11-28 13:17:40+00:00
- **Authors**: Donggeun Yoo, Namil Kim, Sunggyun Park, Anthony S. Paek, In So Kweon
- **Comment**: Published in ECCV 2016. Code and dataset available at dgyoo.github.io
- **Journal**: None
- **Summary**: We present an image-conditional image generation model. The model transfers an input domain to a target domain in semantic level, and generates the target image in pixel level. To generate realistic target images, we employ the real/fake-discriminator as in Generative Adversarial Nets, but also introduce a novel domain-discriminator to make the generated image relevant to the input image. We verify our model through a challenging task of generating a piece of clothing from an input image of a dressed person. We present a high quality clothing dataset containing the two domains, and succeed in demonstrating decent results.



### Fine-scale Surface Normal Estimation using a Single NIR Image
- **Arxiv ID**: http://arxiv.org/abs/1603.07475v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07475v1)
- **Published**: 2016-03-24 08:43:14+00:00
- **Updated**: 2016-03-24 08:43:14+00:00
- **Authors**: Youngjin Yoon, Gyeongmin Choe, Namil Kim, Joon-Young Lee, In So Kweon
- **Comment**: None
- **Journal**: None
- **Summary**: We present surface normal estimation using a single near infrared (NIR) image. We are focusing on fine-scale surface geometry captured with an uncalibrated light source. To tackle this ill-posed problem, we adopt a generative adversarial network which is effective in recovering a sharp output, which is also essential for fine-scale surface normal estimation. We incorporate angular error and integrability constraint into the objective function of the network to make estimated normals physically meaningful. We train and validate our network on a recent NIR dataset, and also evaluate the generality of our trained model by using new external datasets which are captured with a different camera under different environment.



### Simple Does It: Weakly Supervised Instance and Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1603.07485v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07485v2)
- **Published**: 2016-03-24 09:04:16+00:00
- **Updated**: 2016-11-23 09:53:29+00:00
- **Authors**: Anna Khoreva, Rodrigo Benenson, Jan Hosang, Matthias Hein, Bernt Schiele
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic labelling and instance segmentation are two tasks that require particularly costly annotations. Starting from weak supervision in the form of bounding box detection annotations, we propose a new approach that does not require modification of the segmentation training procedure. We show that when carefully designing the input labels from given bounding boxes, even a single round of training is enough to improve over previously reported weakly supervised results. Overall, our weak supervision approach reaches ~95% of the quality of the fully supervised model, both for semantic labelling and instance segmentation.



### Multi-Subregion Based Correlation Filter Bank for Robust Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1603.07604v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07604v1)
- **Published**: 2016-03-24 14:45:51+00:00
- **Updated**: 2016-03-24 14:45:51+00:00
- **Authors**: Yan Yan, Hanzi Wang, David Suter
- **Comment**: None
- **Journal**: Pattern Recognition, volume 47, 11, pages3487--3501, (2014)
- **Summary**: In this paper, we propose an effective feature extraction algorithm, called Multi-Subregion based Correlation Filter Bank (MS-CFB), for robust face recognition. MS-CFB combines the benefits of global-based and local-based feature extraction algorithms, where multiple correlation filters correspond- ing to different face subregions are jointly designed to optimize the overall correlation outputs. Furthermore, we reduce the computational complexi- ty of MS-CFB by designing the correlation filter bank in the spatial domain and improve its generalization capability by capitalizing on the unconstrained form during the filter bank design process. MS-CFB not only takes the d- ifferences among face subregions into account, but also effectively exploits the discriminative information in face subregions. Experimental results on various public face databases demonstrate that the proposed algorithm pro- vides a better feature representation for classification and achieves higher recognition rates compared with several state-of-the-art algorithms.



### Position and Vector Detection of Blind Spot motion with the Horn-Schunck Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1603.07625v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07625v1)
- **Published**: 2016-03-24 15:28:26+00:00
- **Updated**: 2016-03-24 15:28:26+00:00
- **Authors**: Stephen Yu, Mike Wu
- **Comment**: None
- **Journal**: None
- **Summary**: The proposed method uses live image footage which, based on calculations of pixel motion, decides whether or not an object is in the blind-spot. If found, the driver is notified by a sensory light or noise built into the vehicle's CPU. The new technology incorporates optical vectors and flow fields rather than expensive radar-waves, creating cheaper detection systems that retain the needed accuracy while adapting to the current processor speeds.



### Joint Projection and Dictionary Learning using Low-rank Regularization and Graph Constraints
- **Arxiv ID**: http://arxiv.org/abs/1603.07697v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07697v2)
- **Published**: 2016-03-24 18:35:41+00:00
- **Updated**: 2016-12-06 00:08:19+00:00
- **Authors**: Homa Foroughi, Nilanjan Ray, Hong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we aim at learning simultaneously a discriminative dictionary and a robust projection matrix from noisy data. The joint learning, makes the learned projection and dictionary a better fit for each other, so a more accurate classification can be obtained. However, current prevailing joint dimensionality reduction and dictionary learning methods, would fail when the training samples are noisy or heavily corrupted. To address this issue, we propose a joint projection and dictionary learning using low-rank regularization and graph constraints (JPDL-LR). Specifically, the discrimination of the dictionary is achieved by imposing Fisher criterion on the coding coefficients. In addition, our method explicitly encodes the local structure of data by incorporating a graph regularization term, that further improves the discriminative ability of the projection matrix. Inspired by recent advances of low-rank representation for removing outliers and noise, we enforce a low-rank constraint on sub-dictionaries of all classes to make them more compact and robust to noise. Experimental results on several benchmark datasets verify the effectiveness and robustness of our method for both dimensionality reduction and image classification, especially when the data contains considerable noise or variations.



### Coarse-to-Fine Segmentation With Shape-Tailored Scale Spaces
- **Arxiv ID**: http://arxiv.org/abs/1603.07745v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07745v1)
- **Published**: 2016-03-24 20:39:24+00:00
- **Updated**: 2016-03-24 20:39:24+00:00
- **Authors**: Ganesh Sundaramoorthi, Naeemullah Khan, Byung-Woo Hong
- **Comment**: None
- **Journal**: None
- **Summary**: We formulate a general energy and method for segmentation that is designed to have preference for segmenting the coarse structure over the fine structure of the data, without smoothing across boundaries of regions. The energy is formulated by considering data terms at a continuum of scales from the scale space computed from the Heat Equation within regions, and integrating these terms over all time. We show that the energy may be approximately optimized without solving for the entire scale space, but rather solving time-independent linear equations at the native scale of the image, making the method computationally feasible. We provide a multi-region scheme, and apply our method to motion segmentation. Experiments on a benchmark dataset shows that our method is less sensitive to clutter or other undesirable fine-scale structure, and leads to better performance in motion segmentation.



### Seeing Invisible Poses: Estimating 3D Body Pose from Egocentric Video
- **Arxiv ID**: http://arxiv.org/abs/1603.07763v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07763v1)
- **Published**: 2016-03-24 21:46:49+00:00
- **Updated**: 2016-03-24 21:46:49+00:00
- **Authors**: Hao Jiang, Kristen Grauman
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding the camera wearer's activity is central to egocentric vision, yet one key facet of that activity is inherently invisible to the camera--the wearer's body pose. Prior work focuses on estimating the pose of hands and arms when they come into view, but this 1) gives an incomplete view of the full body posture, and 2) prevents any pose estimate at all in many frames, since the hands are only visible in a fraction of daily life activities. We propose to infer the "invisible pose" of a person behind the egocentric camera. Given a single video, our efficient learning-based approach returns the full body 3D joint positions for each frame. Our method exploits cues from the dynamic motion signatures of the surrounding scene--which changes predictably as a function of body pose--as well as static scene structures that reveal the viewpoint (e.g., sitting vs. standing). We further introduce a novel energy minimization scheme to infer the pose sequence. It uses soft predictions of the poses per time instant together with a non-parametric model of human pose dynamics over longer windows. Our method outperforms an array of possible alternatives, including deep learning approaches for direct pose regression from images.



### Co-occurrence Feature Learning for Skeleton based Action Recognition using Regularized Deep LSTM Networks
- **Arxiv ID**: http://arxiv.org/abs/1603.07772v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1603.07772v1)
- **Published**: 2016-03-24 22:43:55+00:00
- **Updated**: 2016-03-24 22:43:55+00:00
- **Authors**: Wentao Zhu, Cuiling Lan, Junliang Xing, Wenjun Zeng, Yanghao Li, Li Shen, Xiaohui Xie
- **Comment**: AAAI 2016 conference
- **Journal**: None
- **Summary**: Skeleton based action recognition distinguishes human actions using the trajectories of skeleton joints, which provide a very good representation for describing actions. Considering that recurrent neural networks (RNNs) with Long Short-Term Memory (LSTM) can learn feature representations and model long-term temporal dependencies automatically, we propose an end-to-end fully connected deep LSTM network for skeleton based action recognition. Inspired by the observation that the co-occurrences of the joints intrinsically characterize human actions, we take the skeleton as the input at each time slot and introduce a novel regularization scheme to learn the co-occurrence features of skeleton joints. To train the deep LSTM network effectively, we propose a new dropout algorithm which simultaneously operates on the gates, cells, and output responses of the LSTM neurons. Experimental results on three human action recognition datasets consistently demonstrate the effectiveness of the proposed model.



