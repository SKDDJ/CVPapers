# Arxiv Papers in cs.CV on 2016-03-12
### Real-time 3D scene description using Spheres, Cones and Cylinders
- **Arxiv ID**: http://arxiv.org/abs/1603.03856v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.03856v1)
- **Published**: 2016-03-12 04:03:46+00:00
- **Updated**: 2016-03-12 04:03:46+00:00
- **Authors**: Kristiyan Georgiev, Motaz Al-Hami, Rolf Lakaemper
- **Comment**: 8 Pages, 16th International Conference on Advanced Robotics (ICAR
  2013). Montevideo, Uruguay, November 2013
- **Journal**: None
- **Summary**: The paper describes a novel real-time algorithm for finding 3D geometric primitives (cylinders, cones and spheres) from 3D range data. In its core, it performs a fast model fitting with a model update in constant time (O(1)) for each new data point added to the model. We use a three stage approach.The first step inspects 1.5D sub spaces, to find ellipses. The next stage uses these ellipses as input by examining their neighborhood structure to form sets of candidates for the 3D geometric primitives. Finally, candidate ellipses are fitted to the geometric primitives. The complexity for point processing is O(n); additional time of lower order is needed for working on significantly smaller amount of mid-level objects. This allows the approach to process 30 frames per second on Kinect depth data, which suggests this approach as a pre-processing step for 3D real-time higher level tasks in robotics, like tracking or feature based mapping.



### Towards Building an RGBD-M Scanner
- **Arxiv ID**: http://arxiv.org/abs/1603.03875v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.03875v1)
- **Published**: 2016-03-12 09:00:24+00:00
- **Updated**: 2016-03-12 09:00:24+00:00
- **Authors**: Zhe Wu, Sai-Kit Yeung, Ping Tan
- **Comment**: None
- **Journal**: None
- **Summary**: We present a portable device to capture both shape and reflectance of an indoor scene. Consisting of a Kinect, an IR camera and several IR LEDs, our device allows the user to acquire data in a similar way as he/she scans with a single Kinect. Scene geometry is reconstructed by KinectFusion. To estimate reflectance from incomplete and noisy observations, 3D vertices of the same material are identified by our material segmentation propagation algorithm. Then BRDF observations at these vertices are merged into a more complete and accurate BRDF for the material. Effectiveness of our device is demonstrated by quality results on real-world scenes.



### Optical Flow with Semantic Segmentation and Localized Layers
- **Arxiv ID**: http://arxiv.org/abs/1603.03911v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.03911v2)
- **Published**: 2016-03-12 13:34:09+00:00
- **Updated**: 2016-04-11 13:00:13+00:00
- **Authors**: Laura Sevilla-Lara, Deqing Sun, Varun Jampani, Michael J. Black
- **Comment**: None
- **Journal**: None
- **Summary**: Existing optical flow methods make generic, spatially homogeneous, assumptions about the spatial structure of the flow. In reality, optical flow varies across an image depending on object class. Simply put, different objects move differently. Here we exploit recent advances in static semantic scene segmentation to segment the image into objects of different types. We define different models of image motion in these regions depending on the type of object. For example, we model the motion on roads with homographies, vegetation with spatially smooth flow, and independently moving objects like cars and planes with affine motion plus deviations. We then pose the flow estimation problem using a novel formulation of localized layers, which addresses limitations of traditional layered models for dealing with complex scene motion. Our semantic flow method achieves the lowest error of any published monocular method in the KITTI-2015 flow benchmark and produces qualitatively better flow and segmentation than recent top methods on a wide range of natural videos.



### Robust Scene Text Recognition with Automatic Rectification
- **Arxiv ID**: http://arxiv.org/abs/1603.03915v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.03915v2)
- **Published**: 2016-03-12 13:58:27+00:00
- **Updated**: 2016-04-19 14:44:54+00:00
- **Authors**: Baoguang Shi, Xinggang Wang, Pengyuan Lyu, Cong Yao, Xiang Bai
- **Comment**: Accepted by CVPR 2016
- **Journal**: None
- **Summary**: Recognizing text in natural images is a challenging task with many unsolved problems. Different from those in documents, words in natural images often possess irregular shapes, which are caused by perspective distortion, curved character placement, etc. We propose RARE (Robust text recognizer with Automatic REctification), a recognition model that is robust to irregular text. RARE is a specially-designed deep neural network, which consists of a Spatial Transformer Network (STN) and a Sequence Recognition Network (SRN). In testing, an image is firstly rectified via a predicted Thin-Plate-Spline (TPS) transformation, into a more "readable" image for the following SRN, which recognizes text through a sequence recognition approach. We show that the model is able to recognize several types of irregular text, including perspective text and curved text. RARE is end-to-end trainable, requiring only images and associated text labels, making it convenient to train and deploy the model in practical systems. State-of-the-art or highly-competitive performance achieved on several benchmarks well demonstrates the effectiveness of the proposed model.



### Image Captioning with Semantic Attention
- **Arxiv ID**: http://arxiv.org/abs/1603.03925v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.03925v1)
- **Published**: 2016-03-12 15:11:43+00:00
- **Updated**: 2016-03-12 15:11:43+00:00
- **Authors**: Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, Jiebo Luo
- **Comment**: 10 pages, 5 figures, CVPR16
- **Journal**: None
- **Summary**: Automatically generating a natural language description of an image has attracted interests recently both because of its importance in practical applications and because it connects two major artificial intelligence fields: computer vision and natural language processing. Existing approaches are either top-down, which start from a gist of an image and convert it into words, or bottom-up, which come up with words describing various aspects of an image and then combine them. In this paper, we propose a new algorithm that combines both approaches through a model of semantic attention. Our algorithm learns to selectively attend to semantic concept proposals and fuse them into hidden states and outputs of recurrent neural networks. The selection and fusion form a feedback connecting the top-down and bottom-up computation. We evaluate our algorithm on two public benchmarks: Microsoft COCO and Flickr30K. Experimental results show that our algorithm significantly outperforms the state-of-the-art approaches consistently across different evaluation metrics.



### Template Adaptation for Face Verification and Identification
- **Arxiv ID**: http://arxiv.org/abs/1603.03958v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.03958v3)
- **Published**: 2016-03-12 19:57:17+00:00
- **Updated**: 2016-04-06 02:11:02+00:00
- **Authors**: Nate Crosswhite, Jeffrey Byrne, Omkar M. Parkhi, Chris Stauffer, Qiong Cao, Andrew Zisserman
- **Comment**: None
- **Journal**: None
- **Summary**: Face recognition performance evaluation has traditionally focused on one-to-one verification, popularized by the Labeled Faces in the Wild dataset for imagery and the YouTubeFaces dataset for videos. In contrast, the newly released IJB-A face recognition dataset unifies evaluation of one-to-many face identification with one-to-one face verification over templates, or sets of imagery and videos for a subject. In this paper, we study the problem of template adaptation, a form of transfer learning to the set of media in a template. Extensive performance evaluations on IJB-A show a surprising result, that perhaps the simplest method of template adaptation, combining deep convolutional network features with template specific linear SVMs, outperforms the state-of-the-art by a wide margin. We study the effects of template size, negative set construction and classifier fusion on performance, then compare template adaptation to convolutional networks with metric learning, 2D and 3D alignment. Our unexpected conclusion is that these other methods, when combined with template adaptation, all achieve nearly the same top performance on IJB-A for template-based face verification and identification.



### Temporally Robust Global Motion Compensation by Keypoint-based Congealing
- **Arxiv ID**: http://arxiv.org/abs/1603.03968v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.03968v1)
- **Published**: 2016-03-12 21:42:18+00:00
- **Updated**: 2016-03-12 21:42:18+00:00
- **Authors**: S. Morteza Safdarnejad, Yousef Atoum, Xiaoming Liu
- **Comment**: 14 Pages
- **Journal**: None
- **Summary**: Global motion compensation (GMC) removes the impact of camera motion and creates a video in which the background appears static over the progression of time. Various vision problems, such as human activity recognition, background reconstruction, and multi-object tracking can benefit from GMC. Existing GMC algorithms rely on sequentially processing consecutive frames, by estimating the transformation mapping the two frames, and obtaining a composite transformation to a global motion compensated coordinate. Sequential GMC suffers from temporal drift of frames from the accurate global coordinate, due to either error accumulation or sporadic failures of motion estimation at a few frames. We propose a temporally robust global motion compensation (TRGMC) algorithm which performs accurate and stable GMC, despite complicated and long-term camera motion. TRGMC densely connects pairs of frames, by matching local keypoints of each frame. A joint alignment of these frames is formulated as a novel keypoint-based congealing problem, where the transformation of each frame is updated iteratively, such that the spatial coordinates for the start and end points of matched keypoints are identical. Experimental results demonstrate that TRGMC has superior performance in a wide range of scenarios.



