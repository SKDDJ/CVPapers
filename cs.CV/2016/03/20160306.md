# Arxiv Papers in cs.CV on 2016-03-06
### Fast calculation of correlations in recognition systems
- **Arxiv ID**: http://arxiv.org/abs/1603.01772v1
- **DOI**: None
- **Categories**: **cs.CV**, 62H30, 65F05, 65F10, 65F30, 65F50, 68T05, 68T10, 94A11, 94A12,
  94A13, 94A15, F.2.1; G.1.0; G.1.3; G.4; H.4.2; I.1.2; I.2.2; I.5.2; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1603.01772v1)
- **Published**: 2016-03-06 00:30:34+00:00
- **Updated**: 2016-03-06 00:30:34+00:00
- **Authors**: Pavel Dourbal, Mikhail Pekker
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Computationally efficient classification system architecture is proposed. It utilizes fast tensor-vector multiplication algorithm to apply linear operators upon input signals . The approach is applicable to wide variety of recognition system architectures ranging from single stage matched filter bank classifiers to complex neural networks with unlimited number of hidden layers.



### Variational methods for Conditional Multimodal Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1603.01801v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1603.01801v2)
- **Published**: 2016-03-06 07:33:03+00:00
- **Updated**: 2016-08-26 10:57:41+00:00
- **Authors**: Gaurav Pandey, Ambedkar Dukkipati
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: In this paper, we address the problem of conditional modality learning, whereby one is interested in generating one modality given the other. While it is straightforward to learn a joint distribution over multiple modalities using a deep multimodal architecture, we observe that such models aren't very effective at conditional generation. Hence, we address the problem by learning conditional distributions between the modalities. We use variational methods for maximizing the corresponding conditional log-likelihood. The resultant deep model, which we refer to as conditional multimodal autoencoder (CMMA), forces the latent representation obtained from a single modality alone to be `close' to the joint representation obtained from multiple modalities. We use the proposed model to generate faces from attributes. We show that the faces generated from attributes using the proposed model, are qualitatively and quantitatively more representative of the attributes from which they were generated, than those obtained by other deep generative models. We also propose a secondary task, whereby the existing faces are modified by modifying the corresponding attributes. We observe that the modifications in face introduced by the proposed model are representative of the corresponding modifications in attributes.



### Proximal groupoid patterns In digital images
- **Arxiv ID**: http://arxiv.org/abs/1603.01842v1
- **DOI**: None
- **Categories**: **cs.CV**, 54E40
- **Links**: [PDF](http://arxiv.org/pdf/1603.01842v1)
- **Published**: 2016-03-06 16:39:34+00:00
- **Updated**: 2016-03-06 16:39:34+00:00
- **Authors**: Enoch A-iyeh, James F. Peters
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: The focus of this article is on the detection and classification of patterns based on groupoids. The approach hinges on descriptive proximity of points in a set based on the neighborliness property. This approach lends support to image analysis and understanding and in studying nearness of image segments. A practical application of the approach is in terms of the analysis of natural images for pattern identification and classification.



### Single Image Restoration for Participating Media Based on Prior Fusion
- **Arxiv ID**: http://arxiv.org/abs/1603.01864v2
- **DOI**: 10.1109/MCG.2018.2881388
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.01864v2)
- **Published**: 2016-03-06 19:54:18+00:00
- **Updated**: 2017-01-11 19:19:36+00:00
- **Authors**: Joel D. O. Gaya, Felipe Codevilla, Amanda C. Duarte, Paulo L. Drews-Jr, Silvia S. Botelho
- **Comment**: This paper is under consideration at Pattern Recognition Letters
- **Journal**: None
- **Summary**: This paper describes a method to restore degraded images captured in a participating media -- fog, turbid water, sand storm, etc. Differently from the related work that only deal with a medium, we obtain generality by using an image formation model and a fusion of new image priors. The model considers the image color variation produced by the medium. The proposed restoration method is based on the fusion of these priors and supported by statistics collected on images acquired in both non-participating and participating media. The key of the method is to fuse two complementary measures --- local contrast and color data. The obtained results on underwater and foggy images demonstrate the capabilities of the proposed method. Moreover, we evaluated our method using a special dataset for which a ground-truth image is available.



