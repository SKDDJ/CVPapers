# Arxiv Papers in cs.CV on 2016-03-27
### Perceptual Losses for Real-Time Style Transfer and Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1603.08155v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1603.08155v1)
- **Published**: 2016-03-27 01:04:27+00:00
- **Updated**: 2016-03-27 01:04:27+00:00
- **Authors**: Justin Johnson, Alexandre Alahi, Li Fei-Fei
- **Comment**: None
- **Journal**: None
- **Summary**: We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a \emph{per-pixel} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing \emph{perceptual} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.



### VolumeDeform: Real-time Volumetric Non-rigid Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1603.08161v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.08161v2)
- **Published**: 2016-03-27 02:09:03+00:00
- **Updated**: 2016-07-30 06:07:24+00:00
- **Authors**: Matthias Innmann, Michael Zollhöfer, Matthias Nießner, Christian Theobalt, Marc Stamminger
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel approach for the reconstruction of dynamic geometric shapes using a single hand-held consumer-grade RGB-D sensor at real-time rates. Our method does not require a pre-defined shape template to start with and builds up the scene model from scratch during the scanning process. Geometry and motion are parameterized in a unified manner by a volumetric representation that encodes a distance field of the surface geometry as well as the non-rigid space deformation. Motion tracking is based on a set of extracted sparse color features in combination with a dense depth-based constraint formulation. This enables accurate tracking and drastically reduces drift inherent to standard model-to-depth alignment. We cast finding the optimal deformation of space as a non-linear regularized variational optimization problem by enforcing local smoothness and proximity to the input constraints. The problem is tackled in real-time at the camera's capture rate using a data-parallel flip-flop optimization strategy. Our results demonstrate robust tracking even for fast motion and scenes that lack geometric features.



### 3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions
- **Arxiv ID**: http://arxiv.org/abs/1603.08182v3
- **DOI**: None
- **Categories**: **cs.CV**, I.4.7; I.4.5; I.3.5; I.2.10; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1603.08182v3)
- **Published**: 2016-03-27 06:43:52+00:00
- **Updated**: 2017-04-09 19:56:05+00:00
- **Authors**: Andy Zeng, Shuran Song, Matthias Nießner, Matthew Fisher, Jianxiong Xiao, Thomas Funkhouser
- **Comment**: To appear at the Conference on Computer Vision and Pattern
  Recognition (CVPR) 2017. Project webpage: http://3dmatch.cs.princeton.edu
- **Journal**: None
- **Summary**: Matching local geometric features on real-world depth images is a challenging task due to the noisy, low-resolution, and incomplete nature of 3D scan data. These difficulties limit the performance of current state-of-art methods, which are typically based on histograms over geometric properties. In this paper, we present 3DMatch, a data-driven model that learns a local volumetric patch descriptor for establishing correspondences between partial 3D data. To amass training data for our model, we propose a self-supervised feature learning method that leverages the millions of correspondence labels found in existing RGB-D reconstructions. Experiments show that our descriptor is not only able to match local geometry in new scenes for reconstruction, but also generalize to different tasks and spatial scales (e.g. instance-level object model alignment for the Amazon Picking Challenge, and mesh surface correspondence). Results show that 3DMatch consistently outperforms other state-of-the-art approaches by a significant margin. Code, data, benchmarks, and pre-trained models are available online at http://3dmatch.cs.princeton.edu



### Recurrent Mixture Density Network for Spatiotemporal Visual Attention
- **Arxiv ID**: http://arxiv.org/abs/1603.08199v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.08199v4)
- **Published**: 2016-03-27 10:34:22+00:00
- **Updated**: 2017-02-11 10:05:06+00:00
- **Authors**: Loris Bazzani, Hugo Larochelle, Lorenzo Torresani
- **Comment**: ICLR 2017
- **Journal**: None
- **Summary**: In many computer vision tasks, the relevant information to solve the problem at hand is mixed to irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, e.g., by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to express the probability of saliency for each pixel. Time consistency in videos is modeled hierarchically by: 1) deep 3D convolutional features to represent spatial and short-term time relations and 2) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds. The parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data, without knowledge of the action in each video. Our experiments on Hollywood2 show state-of-the-art performance on saliency prediction for video. We also show that our attentional model trained on Hollywood2 generalizes well to UCF101 and it can be leveraged to improve action classification accuracy on both datasets.



### Human Pose Estimation using Deep Consensus Voting
- **Arxiv ID**: http://arxiv.org/abs/1603.08212v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1603.08212v1)
- **Published**: 2016-03-27 12:45:33+00:00
- **Updated**: 2016-03-27 12:45:33+00:00
- **Authors**: Ita Lifshitz, Ethan Fetaya, Shimon Ullman
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we consider the problem of human pose estimation from a single still image. We propose a novel approach where each location in the image votes for the position of each keypoint using a convolutional neural net. The voting scheme allows us to utilize information from the whole image, rather than rely on a sparse set of keypoint locations. Using dense, multi-target votes, not only produces good keypoint predictions, but also enables us to compute image-dependent joint keypoint probabilities by looking at consensus voting. This differs from most previous methods where joint probabilities are learned from relative keypoint locations and are independent of the image. We finally combine the keypoints votes and joint probabilities in order to identify the optimal pose configuration. We show our competitive performance on the MPII Human Pose and Leeds Sports Pose datasets.



### Evolution of active categorical image classification via saccadic eye movement
- **Arxiv ID**: http://arxiv.org/abs/1603.08233v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1603.08233v2)
- **Published**: 2016-03-27 16:36:43+00:00
- **Updated**: 2016-06-16 21:00:53+00:00
- **Authors**: Randal S. Olson, Jason H. Moore, Christoph Adami
- **Comment**: 10 pages, 5 figures, to appear in PPSN 2016 conference proceedings
- **Journal**: Lecture Notes in Computer Science 9921 (2016) 581-590
- **Summary**: Pattern recognition and classification is a central concern for modern information processing systems. In particular, one key challenge to image and video classification has been that the computational cost of image processing scales linearly with the number of pixels in the image or video. Here we present an intelligent machine (the "active categorical classifier," or ACC) that is inspired by the saccadic movements of the eye, and is capable of classifying images by selectively scanning only a portion of the image. We harness evolutionary computation to optimize the ACC on the MNIST hand-written digit classification task, and provide a proof-of-concept that the ACC works on noisy multi-class data. We further analyze the ACC and demonstrate its ability to classify images after viewing only a fraction of the pixels, and provide insight on future research paths to further improve upon the ACC presented here.



### DeLight-Net: Decomposing Reflectance Maps into Specular Materials and Natural Illumination
- **Arxiv ID**: http://arxiv.org/abs/1603.08240v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.08240v1)
- **Published**: 2016-03-27 18:03:28+00:00
- **Updated**: 2016-03-27 18:03:28+00:00
- **Authors**: Stamatios Georgoulis, Konstantinos Rematas, Tobias Ritschel, Mario Fritz, Luc Van Gool, Tinne Tuytelaars
- **Comment**: Stamatios Georgoulis and Konstantinos Rematas contributed equally to
  this work
- **Journal**: None
- **Summary**: In this paper we are extracting surface reflectance and natural environmental illumination from a reflectance map, i.e. from a single 2D image of a sphere of one material under one illumination. This is a notoriously difficult problem, yet key to various re-rendering applications. With the recent advances in estimating reflectance maps from 2D images their further decomposition has become increasingly relevant.   To this end, we propose a Convolutional Neural Network (CNN) architecture to reconstruct both material parameters (i.e. Phong) as well as illumination (i.e. high-resolution spherical illumination maps), that is solely trained on synthetic data. We demonstrate that decomposition of synthetic as well as real photographs of reflectance maps, both in High Dynamic Range (HDR), and, for the first time, on Low Dynamic Range (LDR) as well. Results are compared to previous approaches quantitatively as well as qualitatively in terms of re-renderings where illumination, material, view or shape are changed.



