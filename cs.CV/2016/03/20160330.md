# Arxiv Papers in cs.CV on 2016-03-30
### Rich Image Captioning in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1603.09016v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.09016v2)
- **Published**: 2016-03-30 01:55:33+00:00
- **Updated**: 2016-03-31 01:45:31+00:00
- **Authors**: Kenneth Tran, Xiaodong He, Lei Zhang, Jian Sun, Cornelia Carapcea, Chris Thrasher, Chris Buehler, Chris Sienkiewicz
- **Comment**: None
- **Journal**: None
- **Summary**: We present an image caption system that addresses new challenges of automatically describing images in the wild. The challenges include high quality caption quality with respect to human judgments, out-of-domain data handling, and low latency required in many applications. Built on top of a state-of-the-art framework, we developed a deep vision model that detects a broad range of visual concepts, an entity recognition model that identifies celebrities and landmarks, and a confidence model for the caption output. Experimental results show that our caption engine outperforms previous state-of-the-art systems significantly on both in-domain dataset (i.e. MS COCO) and out of-domain datasets.



### Palmprint Recognition Using Deep Scattering Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/1603.09027v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.09027v1)
- **Published**: 2016-03-30 03:09:15+00:00
- **Updated**: 2016-03-30 03:09:15+00:00
- **Authors**: Shervin Minaee, Yao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Palmprint recognition has drawn a lot of attention during the recent years. Many algorithms have been proposed for palmprint recognition in the past, majority of them being based on features extracted from the transform domain. Many of these transform domain features are not translation or rotation invariant, and therefore a great deal of preprocessing is needed to align the images. In this paper, a powerful image representation, called scattering network/transform, is used for palmprint recognition. Scattering network is a convolutional network where its architecture and filters are predefined wavelet transforms. The first layer of scattering network captures similar features to SIFT descriptors and the higher-layer features capture higher-frequency content of the signal which are lost in SIFT and other similar descriptors. After extraction of the scattering features, their dimensionality is reduced by applying principal component analysis (PCA) which reduces the computational complexity of the recognition task. Two different classifiers are used for recognition: multi-class SVM and minimum-distance classifier. The proposed scheme has been tested on a well-known palmprint database and achieved accuracy rate of 99.95% and 100% using minimum distance classifier and SVM respectively.



### Vector Quantization for Machine Vision
- **Arxiv ID**: http://arxiv.org/abs/1603.09037v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.09037v1)
- **Published**: 2016-03-30 04:40:31+00:00
- **Updated**: 2016-03-30 04:40:31+00:00
- **Authors**: Vincenzo Liguori
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: This paper shows how to reduce the computational cost for a variety of common machine vision tasks by operating directly in the compressed domain, particularly in the context of hardware acceleration. Pyramid Vector Quantization (PVQ) is the compression technique of choice and its properties are exploited to simplify Support Vector Machines (SVM), Convolutional Neural Networks(CNNs), Histogram of Oriented Gradients (HOG) features, interest points matching and other algorithms.



### MÃ¶bius Invariants of Shapes and Images
- **Arxiv ID**: http://arxiv.org/abs/1603.09335v2
- **DOI**: 10.3842/SIGMA.2016.080
- **Categories**: **cs.CV**, math.MG
- **Links**: [PDF](http://arxiv.org/pdf/1603.09335v2)
- **Published**: 2016-03-30 05:13:59+00:00
- **Updated**: 2016-08-11 05:36:45+00:00
- **Authors**: Stephen Marsland, Robert McLachlan
- **Comment**: None
- **Journal**: SIGMA 12 (2016), 080, 29 pages
- **Summary**: Identifying when different images are of the same object despite changes caused by imaging technologies, or processes such as growth, has many applications in fields such as computer vision and biological image analysis. One approach to this problem is to identify the group of possible transformations of the object and to find invariants to the action of that group, meaning that the object has the same values of the invariants despite the action of the group. In this paper we study the invariants of planar shapes and images under the M\"obius group $\mathrm{PSL}(2,\mathbb{C})$, which arises in the conformal camera model of vision and may also correspond to neurological aspects of vision, such as grouping of lines and circles. We survey properties of invariants that are important in applications, and the known M\"obius invariants, and then develop an algorithm by which shapes can be recognised that is M\"obius- and reparametrization-invariant, numerically stable, and robust to noise. We demonstrate the efficacy of this new invariant approach on sets of curves, and then develop a M\"obius-invariant signature of grey-scale images.



### Dense Image Representation with Spatial Pyramid VLAD Coding of CNN for Locally Robust Captioning
- **Arxiv ID**: http://arxiv.org/abs/1603.09046v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.09046v1)
- **Published**: 2016-03-30 05:48:05+00:00
- **Updated**: 2016-03-30 05:48:05+00:00
- **Authors**: Andrew Shin, Masataka Yamaguchi, Katsunori Ohnishi, Tatsuya Harada
- **Comment**: submitted to ECCV2016
- **Journal**: None
- **Summary**: The workflow of extracting features from images using convolutional neural networks (CNN) and generating captions with recurrent neural networks (RNN) has become a de-facto standard for image captioning task. However, since CNN features are originally designed for classification task, it is mostly concerned with the main conspicuous element of the image, and often fails to correctly convey information on local, secondary elements. We propose to incorporate coding with vector of locally aggregated descriptors (VLAD) on spatial pyramid for CNN features of sub-regions in order to generate image representations that better reflect the local information of the images. Our results show that our method of compact VLAD coding can match CNN features with as little as 3% of dimensionality and, when combined with spatial pyramid, it results in image captions that more accurately take local elements into account.



### Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections
- **Arxiv ID**: http://arxiv.org/abs/1603.09056v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.09056v2)
- **Published**: 2016-03-30 07:16:05+00:00
- **Updated**: 2016-09-01 01:15:42+00:00
- **Authors**: Xiao-Jiao Mao, Chunhua Shen, Yu-Bin Yang
- **Comment**: Accepted to Proc. Advances in Neural Information Processing Systems
  (NIPS'16). Content of the final version may be slightly different. Extended
  version is available at http://arxiv.org/abs/1606.08921
- **Journal**: None
- **Summary**: In this paper, we propose a very deep fully convolutional encoding-decoding framework for image restoration such as denoising and super-resolution. The network is composed of multiple layers of convolution and de-convolution operators, learning end-to-end mappings from corrupted images to the original ones. The convolutional layers act as the feature extractor, which capture the abstraction of image contents while eliminating noises/corruptions. De-convolutional layers are then used to recover the image details. We propose to symmetrically link convolutional and de-convolutional layers with skip-layer connections, with which the training converges much faster and attains a higher-quality local optimum. First, The skip connections allow the signal to be back-propagated to bottom layers directly, and thus tackles the problem of gradient vanishing, making training deep networks easier and achieving restoration performance gains consequently. Second, these skip connections pass image details from convolutional layers to de-convolutional layers, which is beneficial in recovering the original image. Significantly, with the large capacity, we can handle different levels of noises using a single model. Experimental results show that our network achieves better performance than all previously reported state-of-the-art methods.



### Structured Feature Learning for Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1603.09065v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.09065v1)
- **Published**: 2016-03-30 07:52:22+00:00
- **Updated**: 2016-03-30 07:52:22+00:00
- **Authors**: Xiao Chu, Wanli Ouyang, Hongsheng Li, Xiaogang Wang
- **Comment**: Accepted by CVPR2016
- **Journal**: None
- **Summary**: In this paper, we propose a structured feature learning framework to reason the correlations among body joints at the feature level in human pose estimation. Different from existing approaches of modelling structures on score maps or predicted labels, feature maps preserve substantially richer descriptions of body joints. The relationships between feature maps of joints are captured with the introduced geometrical transform kernels, which can be easily implemented with a convolution layer. Features and their relationships are jointly learned in an end-to-end learning system. A bi-directional tree structured model is proposed, so that the feature channels at a body joint can well receive information from other joints. The proposed framework improves feature learning substantially. With very simple post processing, it reaches the best mean PCP on the LSP and FLIC datasets. Compared with the baseline of learning features at each joint separately with ConvNet, the mean PCP has been improved by 18% on FLIC. The code is released to the public.



### Learning Local Descriptors by Optimizing the Keypoint-Correspondence Criterion: Applications to Face Matching, Learning from Unlabeled Videos and 3D-Shape Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1603.09095v6
- **DOI**: 10.1109/TIP.2018.2867270
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.09095v6)
- **Published**: 2016-03-30 09:24:40+00:00
- **Updated**: 2019-05-07 11:41:01+00:00
- **Authors**: Nenad MarkuÅ¡, Igor S. PandÅ¾iÄ, JÃ¶rgen Ahlberg
- **Comment**: This version has been accepted for publication in IEEE Transactions
  on Image Processing (presents methodological and experimental improvements of
  our ICPR2016 paper)
- **Journal**: None
- **Summary**: Current best local descriptors are learned on a large dataset of matching and non-matching keypoint pairs. However, data of this kind is not always available since detailed keypoint correspondences can be hard to establish. On the other hand, we can often obtain labels for pairs of keypoint bags. For example, keypoint bags extracted from two images of the same object under different views form a matching pair, and keypoint bags extracted from images of different objects form a non-matching pair. On average, matching pairs should contain more corresponding keypoints than non-matching pairs. We describe an end-to-end differentiable architecture that enables the learning of local keypoint descriptors from such weakly-labeled data. Additionally, we discuss how to improve the method by incorporating the procedure of mining hard negatives. We also show how can our approach be used to learn convolutional features from unlabeled video signals and 3D models.   Our implementation is available at https://github.com/nenadmarkus/wlrn



### LIFT: Learned Invariant Feature Transform
- **Arxiv ID**: http://arxiv.org/abs/1603.09114v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.09114v2)
- **Published**: 2016-03-30 10:33:18+00:00
- **Updated**: 2016-07-29 15:29:39+00:00
- **Authors**: Kwang Moo Yi, Eduard Trulls, Vincent Lepetit, Pascal Fua
- **Comment**: Accepted to ECCV 2016 (spotlight)
- **Journal**: None
- **Summary**: We introduce a novel Deep Network architecture that implements the full feature point handling pipeline, that is, detection, orientation estimation, and feature description. While previous works have successfully tackled each one of these problems individually, we show how to learn to do all three in a unified manner while preserving end-to-end differentiability. We then demonstrate that our Deep pipeline outperforms state-of-the-art methods on a number of benchmark datasets, without the need of retraining.



### Exploiting Facial Landmarks for Emotion Recognition in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1603.09129v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1603.09129v1)
- **Published**: 2016-03-30 11:11:29+00:00
- **Updated**: 2016-03-30 11:11:29+00:00
- **Authors**: Matthew Day
- **Comment**: 4 pages, ICMI 2015
- **Journal**: None
- **Summary**: In this paper, we describe an entry to the third Emotion Recognition in the Wild Challenge, EmotiW2015. We detail the associated experiments and show that, through more accurately locating the facial landmarks, and considering only the distances between them, we can achieve a surprising level of performance. The resulting system is not only more accurate than the challenge baseline, but also much simpler.



### Unsupervised Visual Sense Disambiguation for Verbs using Multimodal Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1603.09188v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1603.09188v1)
- **Published**: 2016-03-30 13:43:38+00:00
- **Updated**: 2016-03-30 13:43:38+00:00
- **Authors**: Spandana Gella, Mirella Lapata, Frank Keller
- **Comment**: 11 pages, NAACL-HLT 2016
- **Journal**: None
- **Summary**: We introduce a new task, visual sense disambiguation for verbs: given an image and a verb, assign the correct sense of the verb, i.e., the one that describes the action depicted in the image. Just as textual word sense disambiguation is useful for a wide range of NLP tasks, visual sense disambiguation can be useful for multimodal tasks such as image retrieval, image description, and text illustration. We introduce VerSe, a new dataset that augments existing multimodal datasets (COCO and TUHOI) with sense labels. We propose an unsupervised algorithm based on Lesk which performs visual sense disambiguation using textual, visual, or multimodal embeddings. We find that textual embeddings perform well when gold-standard textual annotations (object labels and image descriptions) are available, while multimodal embeddings perform well on unannotated images. We also verify our findings by using the textual and multimodal embeddings as features in a supervised setting and analyse the performance of visual sense disambiguation task. VerSe is made publicly available and can be downloaded at: https://github.com/spandanagella/verse.



### Unsupervised Understanding of Location and Illumination Changes in Egocentric Videos
- **Arxiv ID**: http://arxiv.org/abs/1603.09200v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.09200v2)
- **Published**: 2016-03-30 14:03:18+00:00
- **Updated**: 2017-03-27 22:27:43+00:00
- **Authors**: Alejandro Betancourt, Natalia DÃ­az-RodrÃ­guez, Emilia Barakova, Lucio Marcenaro, Matthias Rauterberg, Carlo Regazzoni
- **Comment**: Submitted for publication
- **Journal**: None
- **Summary**: Wearable cameras stand out as one of the most promising devices for the upcoming years, and as a consequence, the demand of computer algorithms to automatically understand the videos recorded with them is increasing quickly. An automatic understanding of these videos is not an easy task, and its mobile nature implies important challenges to be faced, such as the changing light conditions and the unrestricted locations recorded. This paper proposes an unsupervised strategy based on global features and manifold learning to endow wearable cameras with contextual information regarding the light conditions and the location captured. Results show that non-linear manifold methods can capture contextual patterns from global features without compromising large computational resources. The proposed strategy is used, as an application case, as a switching mechanism to improve the hand-detection problem in egocentric videos.



### Binary Quadratic Programing for Online Tracking of Hundreds of People in Extremely Crowded Scenes
- **Arxiv ID**: http://arxiv.org/abs/1603.09240v1
- **DOI**: 10.1109/TPAMI.2017.2687462
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1603.09240v1)
- **Published**: 2016-03-30 15:11:38+00:00
- **Updated**: 2016-03-30 15:11:38+00:00
- **Authors**: Afshin Dehghan, Mubarak Shah
- **Comment**: 14 pages, 15 figures
- **Journal**: None
- **Summary**: Multi-object tracking has been studied for decades. However, when it comes to tracking pedestrians in extremely crowded scenes, we are limited to only few works. This is an important problem which gives rise to several challenges. Pre-trained object detectors fail to localize targets in crowded sequences. This consequently limits the use of data-association based multi-target tracking methods which rely on the outcome of an object detector. Additionally, the small apparent target size makes it challenging to extract features to discriminate targets from their surroundings. Finally, the large number of targets greatly increases computational complexity which in turn makes it hard to extend existing multi-target tracking approaches to high-density crowd scenarios. In this paper, we propose a tracker that addresses the aforementioned problems and is capable of tracking hundreds of people efficiently. We formulate online crowd tracking as Binary Quadratic Programing. Our formulation employs target's individual information in the form of appearance and motion as well as contextual cues in the form of neighborhood motion, spatial proximity and grouping constraints, and solves detection and data association simultaneously. In order to solve the proposed quadratic optimization efficiently, where state-of art commercial quadratic programing solvers fail to find the answer in a reasonable amount of time, we propose to use the most recent version of the Modified Frank Wolfe algorithm, which takes advantage of SWAP-steps to speed up the optimization. We show that the proposed formulation can track hundreds of targets efficiently and improves state-of-art results by significant margins on eleven challenging high density crowd sequences.



### Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles
- **Arxiv ID**: http://arxiv.org/abs/1603.09246v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.09246v3)
- **Published**: 2016-03-30 15:27:37+00:00
- **Updated**: 2017-08-22 17:32:19+00:00
- **Authors**: Mehdi Noroozi, Paolo Favaro
- **Comment**: ECCV 2016
- **Journal**: None
- **Summary**: In this paper we study the problem of image representation learning without human annotation. By following the principles of self-supervision, we build a convolutional neural network (CNN) that can be trained to solve Jigsaw puzzles as a pretext task, which requires no manual labeling, and then later repurposed to solve object classification and detection. To maintain the compatibility across tasks we introduce the context-free network (CFN), a siamese-ennead CNN. The CFN takes image tiles as input and explicitly limits the receptive field (or context) of its early processing units to one tile at a time. We show that the CFN includes fewer parameters than AlexNet while preserving the same semantic learning capabilities. By training the CFN to solve Jigsaw puzzles, we learn both a feature mapping of object parts as well as their correct spatial arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. Our proposed method for learning visual representations outperforms state of the art methods in several transfer learning benchmarks.



### Confidence driven TGV fusion
- **Arxiv ID**: http://arxiv.org/abs/1603.09302v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.09302v2)
- **Published**: 2016-03-30 18:27:22+00:00
- **Updated**: 2016-04-29 17:25:58+00:00
- **Authors**: Valsamis Ntouskos, Fiora Pirri
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a novel model for spatially varying variational data fusion, driven by point-wise confidence values. The proposed model allows for the joint estimation of the data and the confidence values based on the spatial coherence of the data. We discuss the main properties of the introduced model as well as suitable algorithms for estimating the solution of the corresponding biconvex minimization problem and their convergence. The performance of the proposed model is evaluated considering the problem of depth image fusion by using both synthetic and real data from publicly available datasets.



### Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs
- **Arxiv ID**: http://arxiv.org/abs/1603.09320v4
- **DOI**: None
- **Categories**: **cs.DS**, cs.CV, cs.IR, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1603.09320v4)
- **Published**: 2016-03-30 19:29:44+00:00
- **Updated**: 2018-08-14 19:29:07+00:00
- **Authors**: Yu. A. Malkov, D. A. Yashunin
- **Comment**: 13 pages, 15 figures
- **Journal**: None
- **Summary**: We present a new approach for the approximate K-nearest neighbor search based on navigable small world graphs with controllable hierarchy (Hierarchical NSW, HNSW). The proposed solution is fully graph-based, without any need for additional search structures, which are typically used at the coarse search stage of the most proximity graph techniques. Hierarchical NSW incrementally builds a multi-layer structure consisting from hierarchical set of proximity graphs (layers) for nested subsets of the stored elements. The maximum layer in which an element is present is selected randomly with an exponentially decaying probability distribution. This allows producing graphs similar to the previously studied Navigable Small World (NSW) structures while additionally having the links separated by their characteristic distance scales. Starting search from the upper layer together with utilizing the scale separation boosts the performance compared to NSW and allows a logarithmic complexity scaling. Additional employment of a heuristic for selecting proximity graph neighbors significantly increases performance at high recall and in case of highly clustered data. Performance evaluation has demonstrated that the proposed general metric space search index is able to strongly outperform previous opensource state-of-the-art vector-only approaches. Similarity of the algorithm to the skip list structure allows straightforward balanced distributed implementation.



### Partial Face Detection for Continuous Authentication
- **Arxiv ID**: http://arxiv.org/abs/1603.09364v1
- **DOI**: 10.1109/ICIP.2016.7532908
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.09364v1)
- **Published**: 2016-03-30 20:15:08+00:00
- **Updated**: 2016-03-30 20:15:08+00:00
- **Authors**: Upal Mahbub, Vishal M. Patel, Deepak Chandra, Brandon Barbello, Rama Chellappa
- **Comment**: None
- **Journal**: 2016 IEEE International Conference on Image Processing (ICIP),
  Phoenix, AZ, USA, 2016, pp. 2991-2995
- **Summary**: In this paper, a part-based technique for real time detection of users' faces on mobile devices is proposed. This method is specifically designed for detecting partially cropped and occluded faces captured using a smartphone's front-facing camera for continuous authentication. The key idea is to detect facial segments in the frame and cluster the results to obtain the region which is most likely to contain a face. Extensive experimentation on a mobile dataset of 50 users shows that our method performs better than many state-of-the-art face detection methods in terms of accuracy and processing speed.



### Deep Networks with Stochastic Depth
- **Arxiv ID**: http://arxiv.org/abs/1603.09382v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1603.09382v3)
- **Published**: 2016-03-30 20:58:07+00:00
- **Updated**: 2016-07-28 23:24:16+00:00
- **Authors**: Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, Kilian Weinberger
- **Comment**: first two authors contributed equally
- **Journal**: None
- **Summary**: Very deep convolutional networks with hundreds of layers have led to significant reductions in error on competitive benchmarks. Although the unmatched expressiveness of the many layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes, and the training time can be painfully slow. To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. This simple approach complements the recent success of residual networks. It reduces training time substantially and improves the test error significantly on almost all data sets that we used for evaluation. With stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91% on CIFAR-10).



