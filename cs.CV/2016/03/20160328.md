# Arxiv Papers in cs.CV on 2016-03-28
### Audio Visual Emotion Recognition with Temporal Alignment and Perception Attention
- **Arxiv ID**: http://arxiv.org/abs/1603.08321v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1603.08321v1)
- **Published**: 2016-03-28 06:06:10+00:00
- **Updated**: 2016-03-28 06:06:10+00:00
- **Authors**: Linlin Chao, Jianhua Tao, Minghao Yang, Ya Li, Zhengqi Wen
- **Comment**: None
- **Journal**: None
- **Summary**: This paper focuses on two key problems for audio-visual emotion recognition in the video. One is the audio and visual streams temporal alignment for feature level fusion. The other one is locating and re-weighting the perception attentions in the whole audio-visual stream for better recognition. The Long Short Term Memory Recurrent Neural Network (LSTM-RNN) is employed as the main classification architecture. Firstly, soft attention mechanism aligns the audio and visual streams. Secondly, seven emotion embedding vectors, which are corresponding to each classification emotion type, are added to locate the perception attentions. The locating and re-weighting process is also based on the soft attention mechanism. The experiment results on EmotiW2015 dataset and the qualitative analysis show the efficiency of the proposed two techniques.



### Hierarchy of Groups Evaluation Using Different F-score Variants
- **Arxiv ID**: http://arxiv.org/abs/1603.08323v1
- **DOI**: 10.1007/978-3-662-49381-6_63
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.08323v1)
- **Published**: 2016-03-28 06:38:56+00:00
- **Updated**: 2016-03-28 06:38:56+00:00
- **Authors**: Michał Spytkowski, Łukasz P. Olech, Halina Kwaśnicka
- **Comment**: Presented on ACIIDS2016 conference https://aciids.pwr.edu.pl/. The
  final publication is available at Springer via
  http://dx.doi.org/10.1007/978-3-662-49381-6_63
- **Journal**: ACIIDS 2016, Da Nang, Vietnam, March 14-16, 2016, pp. 654
  (Springer Berlin Heidelberg)
- **Summary**: The paper presents a cursory examination of clustering, focusing on a rarely explored field of hierarchy of clusters. Based on this, a short discussion of clustering quality measures is presented and the F-score measure is examined more deeply. As there are no attempts to assess the quality for hierarchies of clusters, three variants of the F-Score based index are presented: classic, hierarchical and partial order. The partial order index is the authors' approach to the subject. Conducted experiments show the properties of the considered measures. In conclusions, the strong and weak sides of each variant are presented.



### Continuous 3D Label Stereo Matching using Local Expansion Moves
- **Arxiv ID**: http://arxiv.org/abs/1603.08328v3
- **DOI**: 10.1109/TPAMI.2017.2766072
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.08328v3)
- **Published**: 2016-03-28 07:27:49+00:00
- **Updated**: 2017-10-17 10:31:50+00:00
- **Authors**: Tatsunori Taniai, Yasuyuki Matsushita, Yoichi Sato, Takeshi Naemura
- **Comment**: 14 pages. An extended version of our preliminary conference paper
  [39], Taniai et al. "Graph Cut based Continuous Stereo Matching using Locally
  Shared Labels" in the proceedings of IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR 2014). Our results were submitted to Middlebury
  Stereo Benchmark Version 2 on April 22, 2015, and to Version 3 on July 4,
  2017
- **Journal**: IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, no. 11, pp.
  2725-2739, 2018
- **Summary**: We present an accurate stereo matching method using local expansion moves based on graph cuts. This new move-making scheme is used to efficiently infer per-pixel 3D plane labels on a pairwise Markov random field (MRF) that effectively combines recently proposed slanted patch matching and curvature regularization terms. The local expansion moves are presented as many alpha-expansions defined for small grid regions. The local expansion moves extend traditional expansion moves by two ways: localization and spatial propagation. By localization, we use different candidate alpha-labels according to the locations of local alpha-expansions. By spatial propagation, we design our local alpha-expansions to propagate currently assigned labels for nearby regions. With this localization and spatial propagation, our method can efficiently infer MRF models with a continuous label space using randomized search. Our method has several advantages over previous approaches that are based on fusion moves or belief propagation; it produces submodular moves deriving a subproblem optimality; it helps find good, smooth, piecewise linear disparity maps; it is suitable for parallelization; it can use cost-volume filtering techniques for accelerating the matching cost computations. Even using a simple pairwise MRF, our method is shown to have best performance in the Middlebury stereo benchmark V2 and V3.



### Hierarchical Gaussian Mixture Model with Objects Attached to Terminal and Non-terminal Dendrogram Nodes
- **Arxiv ID**: http://arxiv.org/abs/1603.08342v1
- **DOI**: 10.1007/978-3-319-26227-7_18
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1603.08342v1)
- **Published**: 2016-03-28 08:54:03+00:00
- **Updated**: 2016-03-28 08:54:03+00:00
- **Authors**: Łukasz P. Olech, Mariusz Paradowski
- **Comment**: This article was presented on CORES2015 conference
  http://cores.pwr.wroc.pl/ . The final publication is available at Springer
  via http://dx.doi.org/10.1007/978-3-319-26227-7_18
- **Journal**: Proceedings of the CORES 2015 conf., pp. 191-201. Springer
  International Publishing, Cham (2016)
- **Summary**: A hierarchical clustering algorithm based on Gaussian mixture model is presented. The key difference to regular hierarchical mixture models is the ability to store objects in both terminal and nonterminal nodes. Upper levels of the hierarchy contain sparsely distributed objects, while lower levels contain densely represented ones. As it was shown by experiments, this ability helps in noise detection (modelling). Furthermore, compared to regular hierarchical mixture model, the presented method generates more compact dendrograms with higher quality measured by adopted F-measure.



### Fast, Exact and Multi-Scale Inference for Semantic Image Segmentation with Deep Gaussian CRFs
- **Arxiv ID**: http://arxiv.org/abs/1603.08358v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1603.08358v4)
- **Published**: 2016-03-28 10:55:20+00:00
- **Updated**: 2016-11-29 14:52:20+00:00
- **Authors**: Siddhartha Chandra, Iasonas Kokkinos
- **Comment**: Our code is available at https://github.com/siddharthachandra/gcrf
- **Journal**: None
- **Summary**: In this work we propose a structured prediction technique that combines the virtues of Gaussian Conditional Random Fields (G-CRF) with Deep Learning: (a) our structured prediction task has a unique global optimum that is obtained exactly from the solution of a linear system (b) the gradients of our model parameters are analytically computed using closed form expressions, in contrast to the memory-demanding contemporary deep structured prediction approaches that rely on back-propagation-through-time, (c) our pairwise terms do not have to be simple hand-crafted expressions, as in the line of works building on the DenseCRF, but can rather be `discovered' from data through deep architectures, and (d) out system can trained in an end-to-end manner. Building on standard tools from numerical analysis we develop very efficient algorithms for inference and learning, as well as a customized technique adapted to the semantic segmentation task. This efficiency allows us to explore more sophisticated architectures for structured prediction in deep learning: we introduce multi-resolution architectures to couple information across scales in a joint optimization framework, yielding systematic improvements. We demonstrate the utility of our approach on the challenging VOC PASCAL 2012 image segmentation benchmark, showing substantial improvements over strong baselines. We make all of our code and experiments available at {https://github.com/siddharthachandra/gcrf}



### Sparse Activity and Sparse Connectivity in Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1603.08367v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CG, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1603.08367v1)
- **Published**: 2016-03-28 12:06:49+00:00
- **Updated**: 2016-03-28 12:06:49+00:00
- **Authors**: Markus Thom, Günther Palm
- **Comment**: See http://jmlr.org/papers/v14/thom13a.html for the authoritative
  version
- **Journal**: Journal of Machine Learning Research, vol. 14, pp. 1091-1143, 2013
- **Summary**: Sparseness is a useful regularizer for learning in a wide range of applications, in particular in neural networks. This paper proposes a model targeted at classification tasks, where sparse activity and sparse connectivity are used to enhance classification capabilities. The tool for achieving this is a sparseness-enforcing projection operator which finds the closest vector with a pre-defined sparseness for any given vector. In the theoretical part of this paper, a comprehensive theory for such a projection is developed. In conclusion, it is shown that the projection is differentiable almost everywhere and can thus be implemented as a smooth neuronal transfer function. The entire model can hence be tuned end-to-end using gradient-based methods. Experiments on the MNIST database of handwritten digits show that classification performance can be boosted by sparse activity or sparse connectivity. With a combination of both, performance can be significantly better compared to classical non-sparse approaches.



### A Generic Inverted Index Framework for Similarity Search on the GPU - Technical Report
- **Arxiv ID**: http://arxiv.org/abs/1603.08390v3
- **DOI**: None
- **Categories**: **cs.DB**, cs.CV, cs.DC, cs.DS
- **Links**: [PDF](http://arxiv.org/pdf/1603.08390v3)
- **Published**: 2016-03-28 14:44:34+00:00
- **Updated**: 2018-08-14 08:49:16+00:00
- **Authors**: Jingbo Zhou, Qi Guo, H. V. Jagadish, Luboš Krčál, Siyuan Liu, Wenhao Luan, Anthony K. H. Tung, Yueji Yang, Yuxin Zheng
- **Comment**: 18 pages, technical report for the ICDE 2018 paper
- **Journal**: None
- **Summary**: We propose a novel generic inverted index framework on the GPU (called GENIE), aiming to reduce the programming complexity of the GPU for parallel similarity search of different data types. Not every data type and similarity measure are supported by GENIE, but many popular ones are. We present the system design of GENIE, and demonstrate similarity search with GENIE on several data types along with a theoretical analysis of search results. A new concept of locality sensitive hashing (LSH) named $\tau$-ANN search, and a novel data structure c-PQ on the GPU are also proposed for achieving this purpose. Extensive experiments on different real-life datasets demonstrate the efficiency and effectiveness of our framework. The implemented system has been released as open source.



### Deep Embedding for Spatial Role Labeling
- **Arxiv ID**: http://arxiv.org/abs/1603.08474v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1603.08474v1)
- **Published**: 2016-03-28 18:38:46+00:00
- **Updated**: 2016-03-28 18:38:46+00:00
- **Authors**: Oswaldo Ludwig, Xiao Liu, Parisa Kordjamshidi, Marie-Francine Moens
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces the visually informed embedding of word (VIEW), a continuous vector representation for a word extracted from a deep neural model trained using the Microsoft COCO data set to forecast the spatial arrangements between visual objects, given a textual description. The model is composed of a deep multilayer perceptron (MLP) stacked on the top of a Long Short Term Memory (LSTM) network, the latter being preceded by an embedding layer. The VIEW is applied to transferring multimodal background knowledge to Spatial Role Labeling (SpRL) algorithms, which recognize spatial relations between objects mentioned in the text. This work also contributes with a new method to select complementary features and a fine-tuning method for MLP that improves the $F1$ measure in classifying the words into spatial roles. The VIEW is evaluated with the Task 3 of SemEval-2013 benchmark data set, SpaceEval.



### Learning to Read Chest X-Rays: Recurrent Neural Cascade Model for Automated Image Annotation
- **Arxiv ID**: http://arxiv.org/abs/1603.08486v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.08486v1)
- **Published**: 2016-03-28 19:02:07+00:00
- **Updated**: 2016-03-28 19:02:07+00:00
- **Authors**: Hoo-Chang Shin, Kirk Roberts, Le Lu, Dina Demner-Fushman, Jianhua Yao, Ronald M Summers
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the recent advances in automatically describing image contents, their applications have been mostly limited to image caption datasets containing natural images (e.g., Flickr 30k, MSCOCO). In this paper, we present a deep learning model to efficiently detect a disease from an image and annotate its contexts (e.g., location, severity and the affected organs). We employ a publicly available radiology dataset of chest x-rays and their reports, and use its image annotations to mine disease names to train convolutional neural networks (CNNs). In doing so, we adopt various regularization techniques to circumvent the large normal-vs-diseased cases bias. Recurrent neural networks (RNNs) are then trained to describe the contexts of a detected disease, based on the deep CNN features. Moreover, we introduce a novel approach to use the weights of the already trained pair of CNN/RNN on the domain-specific image/text dataset, to infer the joint image/text contexts for composite image labeling. Significantly improved image annotation results are demonstrated using the recurrent neural cascade model by taking the joint image/text contexts into account.



### Generating Visual Explanations
- **Arxiv ID**: http://arxiv.org/abs/1603.08507v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1603.08507v1)
- **Published**: 2016-03-28 19:54:12+00:00
- **Updated**: 2016-03-28 19:54:12+00:00
- **Authors**: Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, Trevor Darrell
- **Comment**: None
- **Journal**: None
- **Summary**: Clearly explaining a rationale for a classification decision to an end-user can be as important as the decision itself. Existing approaches for deep visual recognition are generally opaque and do not output any justification text; contemporary vision-language models can describe image content but fail to take into account class-discriminative image aspects which justify visual predictions. We propose a new model that focuses on the discriminating properties of the visible object, jointly predicts a class label, and explains why the predicted label is appropriate for the image. We propose a novel loss function based on sampling and reinforcement learning that learns to generate sentences that realize a global sentence property, such as class specificity. Our results on a fine-grained bird species classification dataset show that our model is able to generate explanations which are not only consistent with an image but also more discriminative than descriptions produced by existing captioning methods.



### Colorful Image Colorization
- **Arxiv ID**: http://arxiv.org/abs/1603.08511v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.08511v5)
- **Published**: 2016-03-28 19:58:19+00:00
- **Updated**: 2016-10-05 18:01:05+00:00
- **Authors**: Richard Zhang, Phillip Isola, Alexei A. Efros
- **Comment**: None
- **Journal**: None
- **Summary**: Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a "colorization Turing test," asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32% of the trials, significantly higher than previous methods. Moreover, we show that colorization can be a powerful pretext task for self-supervised feature learning, acting as a cross-channel encoder. This approach results in state-of-the-art performance on several feature learning benchmarks.



### Shuffle and Learn: Unsupervised Learning using Temporal Order Verification
- **Arxiv ID**: http://arxiv.org/abs/1603.08561v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1603.08561v2)
- **Published**: 2016-03-28 21:00:43+00:00
- **Updated**: 2016-07-26 17:26:01+00:00
- **Authors**: Ishan Misra, C. Lawrence Zitnick, Martial Hebert
- **Comment**: Accepted at ECCV 2016
- **Journal**: None
- **Summary**: In this paper, we present an approach for learning a visual representation from the raw spatiotemporal signals in videos. Our representation is learned without supervision from semantic labels. We formulate our method as an unsupervised sequential verification task, i.e., we determine whether a sequence of frames from a video is in the correct temporal order. With this simple task and no semantic labels, we learn a powerful visual representation using a Convolutional Neural Network (CNN). The representation contains complementary information to that learned from supervised image datasets like ImageNet. Qualitative results show that our method captures information that is temporally varying, such as human pose. When used as pre-training for action recognition, our method gives significant gains over learning without external data on benchmark datasets like UCF101 and HMDB51. To demonstrate its sensitivity to human pose, we show results for pose estimation on the FLIC and MPII datasets that are competitive, or better than approaches using significantly more supervision. Our method can be combined with supervised representations to provide an additional boost in accuracy.



### Kernelized Weighted SUSAN based Fuzzy C-Means Clustering for Noisy Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1603.08564v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1603.08564v1)
- **Published**: 2016-03-28 21:09:52+00:00
- **Updated**: 2016-03-28 21:09:52+00:00
- **Authors**: Satrajit Mukherjee, Bodhisattwa Prasad Majumder, Aritran Piplai, Swagatam Das
- **Comment**: Journal Version
- **Journal**: None
- **Summary**: The paper proposes a novel Kernelized image segmentation scheme for noisy images that utilizes the concept of Smallest Univalue Segment Assimilating Nucleus (SUSAN) and incorporates spatial constraints by computing circular colour map induced weights. Fuzzy damping coefficients are obtained for each nucleus or center pixel on the basis of the corresponding weighted SUSAN area values, the weights being equal to the inverse of the number of horizontal and vertical moves required to reach a neighborhood pixel from the center pixel. These weights are used to vary the contributions of the different nuclei in the Kernel based framework. The paper also presents an edge quality metric obtained by fuzzy decision based edge candidate selection and final computation of the blurriness of the edges after their selection. The inability of existing algorithms to preserve edge information and structural details in their segmented maps necessitates the computation of the edge quality factor (EQF) for all the competing algorithms. Qualitative and quantitative analysis have been rendered with respect to state-of-the-art algorithms and for images ridden with varying types of noises. Speckle noise ridden SAR images and Rician noise ridden Magnetic Resonance Images have also been considered for evaluating the effectiveness of the proposed algorithm in extracting important segmentation information.



### Attend, Infer, Repeat: Fast Scene Understanding with Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1603.08575v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1603.08575v3)
- **Published**: 2016-03-28 21:59:08+00:00
- **Updated**: 2016-08-12 16:05:08+00:00
- **Authors**: S. M. Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Koray Kavukcuoglu, Geoffrey E. Hinton
- **Comment**: None
- **Journal**: None
- **Summary**: We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects - counting, locating and classifying the elements of a scene - without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network. We further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization.



### Exploring Local Context for Multi-target Tracking in Wide Area Aerial Surveillance
- **Arxiv ID**: http://arxiv.org/abs/1603.08592v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.08592v1)
- **Published**: 2016-03-28 23:47:25+00:00
- **Updated**: 2016-03-28 23:47:25+00:00
- **Authors**: Bor-Jeng Chen, Gerard Medioni
- **Comment**: None
- **Journal**: None
- **Summary**: Tracking many vehicles in wide coverage aerial imagery is crucial for understanding events in a large field of view. Most approaches aim to associate detections from frame differencing into tracks. However, slow or stopped vehicles result in long-term missing detections and further cause tracking discontinuities. Relying merely on appearance clue to recover missing detections is difficult as targets are extremely small and in grayscale. In this paper, we address the limitations of detection association methods by coupling it with a local context tracker (LCT), which does not rely on motion detections. On one hand, our LCT learns neighboring spatial relation and tracks each target in consecutive frames using graph optimization. It takes the advantage of context constraints to avoid drifting to nearby targets. We generate hypotheses from sparse and dense flow efficiently to keep solutions tractable. On the other hand, we use detection association strategy to extract short tracks in batch processing. We explicitly handle merged detections by generating additional hypotheses from them. Our evaluation on wide area aerial imagery sequences shows significant improvement over state-of-the-art methods.



