# Arxiv Papers in cs.CV on 2016-03-17
### Saliency Detection with Spaces of Background-based Distribution
- **Arxiv ID**: http://arxiv.org/abs/1603.05335v1
- **DOI**: 10.1109/LSP.2016.2544781
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.05335v1)
- **Published**: 2016-03-17 02:18:30+00:00
- **Updated**: 2016-03-17 02:18:30+00:00
- **Authors**: Tong Zhao, Lin Li, Xinghao Ding, Yue Huang, Delu Zeng
- **Comment**: 5 pages, 6 figures, Accepted by IEEE Signal Processing Letters in
  March 2016
- **Journal**: None
- **Summary**: In this letter, an effective image saliency detection method is proposed by constructing some novel spaces to model the background and redefine the distance of the salient patches away from the background. Concretely, given the backgroundness prior, eigendecomposition is utilized to create four spaces of background-based distribution (SBD) to model the background, in which a more appropriate metric (Mahalanobis distance) is quoted to delicately measure the saliency of every image patch away from the background. After that, a coarse saliency map is obtained by integrating the four adjusted Mahalanobis distance maps, each of which is formed by the distances between all the patches and background in the corresponding SBD. To be more discriminative, the coarse saliency map is further enhanced into the posterior probability map within Bayesian perspective. Finally, the final saliency map is generated by properly refining the posterior probability map with geodesic distance. Experimental results on two usual datasets show that the proposed method is effective compared with the state-of-the-art algorithms.



### Variable-Length Hashing
- **Arxiv ID**: http://arxiv.org/abs/1603.05414v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1603.05414v1)
- **Published**: 2016-03-17 10:19:50+00:00
- **Updated**: 2016-03-17 10:19:50+00:00
- **Authors**: Honghai Yu, Pierre Moulin, Hong Wei Ng, Xiaoli Li
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Hashing has emerged as a popular technique for large-scale similarity search. Most learning-based hashing methods generate compact yet correlated hash codes. However, this redundancy is storage-inefficient. Hence we propose a lossless variable-length hashing (VLH) method that is both storage- and search-efficient. Storage efficiency is achieved by converting the fixed-length hash code into a variable-length code. Search efficiency is obtained by using a multiple hash table structure. With VLH, we are able to deliberately add redundancy into hash codes to improve retrieval performance with little sacrifice in storage efficiency or search complexity. In particular, we propose a block K-means hashing (B-KMH) method to obtain significantly improved retrieval performance with no increase in storage and marginal increase in computational cost.



### Neural Aggregation Network for Video Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1603.05474v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1603.05474v4)
- **Published**: 2016-03-17 13:30:45+00:00
- **Updated**: 2017-08-02 08:08:14+00:00
- **Authors**: Jiaolong Yang, Peiran Ren, Dongqing Zhang, Dong Chen, Fang Wen, Hongdong Li, Gang Hua
- **Comment**: Post CVPR2017 version with minor typo fix
- **Journal**: None
- **Summary**: This paper presents a Neural Aggregation Network (NAN) for video face recognition. The network takes a face video or face image set of a person with a variable number of face images as its input, and produces a compact, fixed-dimension feature representation for recognition. The whole network is composed of two modules. The feature embedding module is a deep Convolutional Neural Network (CNN) which maps each face image to a feature vector. The aggregation module consists of two attention blocks which adaptively aggregate the feature vectors to form a single feature inside the convex hull spanned by them. Due to the attention mechanism, the aggregation is invariant to the image order. Our NAN is trained with a standard classification or verification loss without any extra supervision signal, and we found that it automatically learns to advocate high-quality face images while repelling low-quality ones such as blurred, occluded and improperly exposed faces. The experiments on IJB-A, YouTube Face, Celebrity-1000 video face recognition benchmarks show that it consistently outperforms naive aggregation methods and achieves the state-of-the-art accuracy.



### Tracking multiple moving objects in images using Markov Chain Monte Carlo
- **Arxiv ID**: http://arxiv.org/abs/1603.05522v1
- **DOI**: None
- **Categories**: **stat.AP**, cs.CV, stat.CO
- **Links**: [PDF](http://arxiv.org/pdf/1603.05522v1)
- **Published**: 2016-03-17 15:03:10+00:00
- **Updated**: 2016-03-17 15:03:10+00:00
- **Authors**: Lan Jiang, Sumeetpal S. Singh
- **Comment**: None
- **Journal**: None
- **Summary**: A new Bayesian state and parameter learning algorithm for multiple target tracking (MTT) models with image observations is proposed. Specifically, a Markov chain Monte Carlo algorithm is designed to sample from the posterior distribution of the unknown number of targets, their birth and death times, states and model parameters, which constitutes the complete solution to the tracking problem. The conventional approach is to pre-process the images to extract point observations and then perform tracking. We model the image generation process directly to avoid potential loss of information when extracting point observations. Numerical examples show that our algorithm has improved tracking performance over commonly used techniques, for both synthetic examples and real florescent microscopy data, especially in the case of dim targets with overlapping illuminated regions.



### "What happens if..." Learning to Predict the Effect of Forces in Images
- **Arxiv ID**: http://arxiv.org/abs/1603.05600v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.05600v1)
- **Published**: 2016-03-17 18:12:33+00:00
- **Updated**: 2016-03-17 18:12:33+00:00
- **Authors**: Roozbeh Mottaghi, Mohammad Rastegari, Abhinav Gupta, Ali Farhadi
- **Comment**: None
- **Journal**: None
- **Summary**: What happens if one pushes a cup sitting on a table toward the edge of the table? How about pushing a desk against a wall? In this paper, we study the problem of understanding the movements of objects as a result of applying external forces to them. For a given force vector applied to a specific location in an image, our goal is to predict long-term sequential movements caused by that force. Doing so entails reasoning about scene geometry, objects, their attributes, and the physical rules that govern the movements of objects. We design a deep neural network model that learns long-term sequential dependencies of object movements while taking into account the geometry and appearance of the scene by combining Convolutional and Recurrent Neural Networks. Training our model requires a large-scale dataset of object movements caused by external forces. To build a dataset of forces in scenes, we reconstructed all images in SUN RGB-D dataset in a physics simulator to estimate the physical movements of objects caused by external forces applied to them. Our Forces in Scenes (ForScene) dataset contains 10,335 images in which a variety of external forces are applied to different types of objects resulting in more than 65,000 object movements represented in 3D. Our experimental evaluations show that the challenging task of predicting long-term movements of objects as their reaction to external forces is possible from a single image.



### Generative Image Modeling using Style and Structure Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1603.05631v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.05631v2)
- **Published**: 2016-03-17 19:33:20+00:00
- **Updated**: 2016-07-26 03:54:23+00:00
- **Authors**: Xiaolong Wang, Abhinav Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: Current generative frameworks use end-to-end learning and generate images by sampling from uniform noise distribution. However, these approaches ignore the most basic principle of image formation: images are product of: (a) Structure: the underlying 3D model; (b) Style: the texture mapped onto structure. In this paper, we factorize the image generation process and propose Style and Structure Generative Adversarial Network (S^2-GAN). Our S^2-GAN has two components: the Structure-GAN generates a surface normal map; the Style-GAN takes the surface normal map as input and generates the 2D image. Apart from a real vs. generated loss function, we use an additional loss with computed surface normals from generated images. The two GANs are first trained independently, and then merged together via joint learning. We show our S^2-GAN model is interpretable, generates more realistic images and can be used to learn unsupervised RGBD representations.



