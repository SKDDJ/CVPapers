# Arxiv Papers in cs.CV on 2016-03-07
### A Two-Stage Shape Retrieval (TSR) Method with Global and Local Features
- **Arxiv ID**: http://arxiv.org/abs/1603.01942v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.01942v3)
- **Published**: 2016-03-07 05:33:00+00:00
- **Updated**: 2016-05-03 04:22:41+00:00
- **Authors**: Xiaqing Pan, Sachin Chachada, C. -C. Jay Kuo
- **Comment**: None
- **Journal**: None
- **Summary**: A robust two-stage shape retrieval (TSR) method is proposed to address the 2D shape retrieval problem. Most state-of-the-art shape retrieval methods are based on local features matching and ranking. Their retrieval performance is not robust since they may retrieve globally dissimilar shapes in high ranks. To overcome this challenge, we decompose the decision process into two stages. In the first irrelevant cluster filtering (ICF) stage, we consider both global and local features and use them to predict the relevance of gallery shapes with respect to the query. Irrelevant shapes are removed from the candidate shape set. After that, a local-features-based matching and ranking (LMR) method follows in the second stage. We apply the proposed TSR system to MPEG-7, Kimia99 and Tari1000 three datasets and show that it outperforms all other existing methods. The robust retrieval performance of the TSR system is demonstrated.



### Deep Contrast Learning for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1603.01976v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.01976v1)
- **Published**: 2016-03-07 08:50:33+00:00
- **Updated**: 2016-03-07 08:50:33+00:00
- **Authors**: Guanbin Li, Yizhou Yu
- **Comment**: To appear in CVPR 2016
- **Journal**: None
- **Summary**: Salient object detection has recently witnessed substantial progress due to powerful features extracted using deep convolutional neural networks (CNNs). However, existing CNN-based methods operate at the patch level instead of the pixel level. Resulting saliency maps are typically blurry, especially near the boundary of salient objects. Furthermore, image patches are treated as independent samples even when they are overlapping, giving rise to significant redundancy in computation and storage. In this CVPR 2016 paper, we propose an end-to-end deep contrast network to overcome the aforementioned limitations. Our deep network consists of two complementary components, a pixel-level fully convolutional stream and a segment-wise spatial pooling stream. The first stream directly produces a saliency map with pixel-level accuracy from an input image. The second stream extracts segment-wise features very efficiently, and better models saliency discontinuities along object boundaries. Finally, a fully connected CRF model can be optionally incorporated to improve spatial coherence and contour localization in the fused result from these two streams. Experimental results demonstrate that our deep model significantly improves the state of the art.



### From A to Z: Supervised Transfer of Style and Content Using Deep Neural Network Generators
- **Arxiv ID**: http://arxiv.org/abs/1603.02003v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.02003v1)
- **Published**: 2016-03-07 11:04:17+00:00
- **Updated**: 2016-03-07 11:04:17+00:00
- **Authors**: Paul Upchurch, Noah Snavely, Kavita Bala
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new neural network architecture for solving single-image analogies - the generation of an entire set of stylistically similar images from just a single input image. Solving this problem requires separating image style from content. Our network is a modified variational autoencoder (VAE) that supports supervised training of single-image analogies and in-network evaluation of outputs with a structured similarity objective that captures pixel covariances. On the challenging task of generating a 62-letter font from a single example letter we produce images with 22.4% lower dissimilarity to the ground truth than state-of-the-art.



### Position paper: Towards an observer-oriented theory of shape comparison
- **Arxiv ID**: http://arxiv.org/abs/1603.02008v1
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV, math.AT, 55N35 (Primary), 22F99, 47H09, 54H15, 57S10, 68U05, 65D18
  (Secondary), I.4.7; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/1603.02008v1)
- **Published**: 2016-03-07 11:16:28+00:00
- **Updated**: 2016-03-07 11:16:28+00:00
- **Authors**: Patrizio Frosini
- **Comment**: Preprint of the position paper submitted to the Eurographics Workshop
  on 3D Object Retrieval (2016)
- **Journal**: None
- **Summary**: In this position paper we suggest a possible metric approach to shape comparison that is based on a mathematical formalization of the concept of observer, seen as a collection of suitable operators acting on a metric space of functions. These functions represent the set of data that are accessible to the observer, while the operators describe the way the observer elaborates the data and enclose the invariance that he/she associates with them. We expose this model and illustrate some theoretical reasons that justify its possible use for shape comparison.



### Adaptive Visualisation System for Construction Building Information Models Using Saliency
- **Arxiv ID**: http://arxiv.org/abs/1603.02028v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1603.02028v1)
- **Published**: 2016-03-07 12:25:33+00:00
- **Updated**: 2016-03-07 12:25:33+00:00
- **Authors**: Hugo Martin, Sylvain Chevallier, Eric Monacelli
- **Comment**: 10 pages, 5 figures, to be submitted
- **Journal**: None
- **Summary**: Building Information Modeling (BIM) is a recent construction process based on a 3D model, containing every component related to the building achievement. Architects, structure engineers, method engineers, and others participant to the building process work on this model through the design-to-construction cycle. The high complexity and the large amount of information included in these models raise several issues, delaying its wide adoption in the industrial world. One of the most important is the visualization: professionals have difficulties to find out the relevant information for their job. Actual solutions suffer from two limitations: the BIM models information are processed manually and insignificant information are simply hidden, leading to inconsistencies in the building model. This paper describes a system relying on an ontological representation of the building information to label automatically the building elements. Depending on the user's department, the visualization is modified according to these labels by automatically adjusting the colors and image properties based on a saliency model. The proposed saliency model incorporates several adaptations to fit the specificities of architectural images.



### A novel learning-based frame pooling method for Event Detection
- **Arxiv ID**: http://arxiv.org/abs/1603.02078v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.02078v2)
- **Published**: 2016-03-07 14:15:55+00:00
- **Updated**: 2016-08-19 02:59:56+00:00
- **Authors**: Lan Wang, Chenqiang Gao, Jiang Liu, Deyu Meng
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting complex events in a large video collection crawled from video websites is a challenging task. When applying directly good image-based feature representation, e.g., HOG, SIFT, to videos, we have to face the problem of how to pool multiple frame feature representations into one feature representation. In this paper, we propose a novel learning-based frame pooling method. We formulate the pooling weight learning as an optimization problem and thus our method can automatically learn the best pooling weight configuration for each specific event category. Experimental results conducted on TRECVID MED 2011 reveal that our method outperforms the commonly used average pooling and max pooling strategies on both high-level and low-level 2D image features.



### Learning a Discriminative Null Space for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1603.02139v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.02139v1)
- **Published**: 2016-03-07 16:26:07+00:00
- **Updated**: 2016-03-07 16:26:07+00:00
- **Authors**: Li Zhang, Tao Xiang, Shaogang Gong
- **Comment**: accepted by CVPR2016
- **Journal**: None
- **Summary**: Most existing person re-identification (re-id) methods focus on learning the optimal distance metrics across camera views. Typically a person's appearance is represented using features of thousands of dimensions, whilst only hundreds of training samples are available due to the difficulties in collecting matched training images. With the number of training samples much smaller than the feature dimension, the existing methods thus face the classic small sample size (SSS) problem and have to resort to dimensionality reduction techniques and/or matrix regularisation, which lead to loss of discriminative power. In this work, we propose to overcome the SSS problem in re-id distance metric learning by matching people in a discriminative null space of the training data. In this null space, images of the same person are collapsed into a single point thus minimising the within-class scatter to the extreme and maximising the relative between-class separation simultaneously. Importantly, it has a fixed dimension, a closed-form solution and is very efficient to compute. Extensive experiments carried out on five person re-identification benchmarks including VIPeR, PRID2011, CUHK01, CUHK03 and Market1501 show that such a simple approach beats the state-of-the-art alternatives, often by a big margin.



### Gaussian Process Regression for Out-of-Sample Extension
- **Arxiv ID**: http://arxiv.org/abs/1603.02194v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1603.02194v2)
- **Published**: 2016-03-07 18:35:51+00:00
- **Updated**: 2016-06-05 16:56:21+00:00
- **Authors**: Oren Barkan, Jonathan Weill, Amir Averbuch
- **Comment**: None
- **Journal**: None
- **Summary**: Manifold learning methods are useful for high dimensional data analysis. Many of the existing methods produce a low dimensional representation that attempts to describe the intrinsic geometric structure of the original data. Typically, this process is computationally expensive and the produced embedding is limited to the training data. In many real life scenarios, the ability to produce embedding of unseen samples is essential. In this paper we propose a Bayesian non-parametric approach for out-of-sample extension. The method is based on Gaussian Process Regression and independent of the manifold learning algorithm. Additionally, the method naturally provides a measure for the degree of abnormality for a newly arrived data point that did not participate in the training process. We derive the mathematical connection between the proposed method and the Nystrom extension and show that the latter is a special case of the former. We present extensive experimental results that demonstrate the performance of the proposed method and compare it to other existing out-of-sample extension methods.



### Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection
- **Arxiv ID**: http://arxiv.org/abs/1603.02199v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1603.02199v4)
- **Published**: 2016-03-07 18:53:00+00:00
- **Updated**: 2016-08-28 23:32:37+00:00
- **Authors**: Sergey Levine, Peter Pastor, Alex Krizhevsky, Deirdre Quillen
- **Comment**: This is an extended version of "Learning Hand-Eye Coordination for
  Robotic Grasping with Large-Scale Data Collection," ISER 2016. Draft modified
  to correct typo in Algorithm 1 and add a link to the publicly available
  dataset
- **Journal**: None
- **Summary**: We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware. Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing.



### Elastic Functional Coding of Riemannian Trajectories
- **Arxiv ID**: http://arxiv.org/abs/1603.02200v1
- **DOI**: 10.1109/TPAMI.2016.2564409
- **Categories**: **cs.CV**, math.DG
- **Links**: [PDF](http://arxiv.org/pdf/1603.02200v1)
- **Published**: 2016-03-07 18:53:25+00:00
- **Updated**: 2016-03-07 18:53:25+00:00
- **Authors**: Rushil Anirudh, Pavan Turaga, Jingyong Su, Anuj Srivastava
- **Comment**: Under major revision at IEEE T-PAMI, 2016
- **Journal**: None
- **Summary**: Visual observations of dynamic phenomena, such as human actions, are often represented as sequences of smoothly-varying features . In cases where the feature spaces can be structured as Riemannian manifolds, the corresponding representations become trajectories on manifolds. Analysis of these trajectories is challenging due to non-linearity of underlying spaces and high-dimensionality of trajectories. In vision problems, given the nature of physical systems involved, these phenomena are better characterized on a low-dimensional manifold compared to the space of Riemannian trajectories. For instance, if one does not impose physical constraints of the human body, in data involving human action analysis, the resulting representation space will have highly redundant features. Learning an effective, low-dimensional embedding for action representations will have a huge impact in the areas of search and retrieval, visualization, learning, and recognition. The difficulty lies in inherent non-linearity of the domain and temporal variability of actions that can distort any traditional metric between trajectories. To overcome these issues, we use the framework based on transported square-root velocity fields (TSRVF); this framework has several desirable properties, including a rate-invariant metric and vector space representations. We propose to learn an embedding such that each action trajectory is mapped to a single point in a low-dimensional Euclidean space, and the trajectories that differ only in temporal rates map to the same point. We utilize the TSRVF representation, and accompanying statistical summaries of Riemannian trajectories, to extend existing coding methods such as PCA, KSVD and Label Consistent KSVD to Riemannian trajectories or more generally to Riemannian functions.



### Authenticating users through their arm movement patterns
- **Arxiv ID**: http://arxiv.org/abs/1603.02211v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, K.6.5
- **Links**: [PDF](http://arxiv.org/pdf/1603.02211v1)
- **Published**: 2016-03-07 19:15:39+00:00
- **Updated**: 2016-03-07 19:15:39+00:00
- **Authors**: Rajesh Kumar, Vir V Phoha, Rahul Raina
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose four continuous authentication designs by using the characteristics of arm movements while individuals walk. The first design uses acceleration of arms captured by a smartwatch's accelerometer sensor, the second design uses the rotation of arms captured by a smartwatch's gyroscope sensor, third uses the fusion of both acceleration and rotation at the feature-level and fourth uses the fusion at score-level. Each of these designs is implemented by using four classifiers, namely, k nearest neighbors (k-NN) with Euclidean distance, Logistic Regression, Multilayer Perceptrons, and Random Forest resulting in a total of sixteen authentication mechanisms. These authentication mechanisms are tested under three different environments, namely an intra-session, inter-session on a dataset of 40 users and an inter-phase on a dataset of 12 users. The sessions of data collection were separated by at least ten minutes, whereas the phases of data collection were separated by at least three months. Under the intra-session environment, all of the twelve authentication mechanisms achieve a mean dynamic false accept rate (DFAR) of 0% and dynamic false reject rate (DFRR) of 0%. For the inter-session environment, feature level fusion-based design with classifier k-NN achieves the best error rates that are a mean DFAR of 2.2% and DFRR of 4.2%. The DFAR and DFRR increased from 5.68% and 4.23% to 15.03% and 14.62% respectively when feature level fusion-based design with classifier k-NN was tested under the inter-phase environment on a dataset of 12 users.



### Drift Robust Non-rigid Optical Flow Enhancement for Long Sequences
- **Arxiv ID**: http://arxiv.org/abs/1603.02252v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.02252v1)
- **Published**: 2016-03-07 20:51:15+00:00
- **Updated**: 2016-03-07 20:51:15+00:00
- **Authors**: Wenbin Li, Darren Cosker, Matthew Brown
- **Comment**: Preprint version of our paper accepted by Journal of Intelligent and
  Fuzzy Systems
- **Journal**: None
- **Summary**: It is hard to densely track a nonrigid object in long term, which is a fundamental research issue in the computer vision community. This task often relies on estimating pairwise correspondences between images over time where the error is accumulated and leads to a drift issue. In this paper, we introduce a novel optimization framework with an Anchor Patch constraint. It is supposed to significantly reduce overall errors given long sequences containing non-rigidly deformable objects. Our framework can be applied to any dense tracking algorithm, e.g. optical flow. We demonstrate the success of our approach by showing significant error reduction on 6 popular optical flow algorithms applied to a range of real-world nonrigid benchmarks. We also provide quantitative analysis of our approach given synthetic occlusions and image noise.



### Blur Robust Optical Flow using Motion Channel
- **Arxiv ID**: http://arxiv.org/abs/1603.02253v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.02253v1)
- **Published**: 2016-03-07 20:53:20+00:00
- **Updated**: 2016-03-07 20:53:20+00:00
- **Authors**: Wenbin Li, Yang Chen, JeeHang Lee, Gang Ren, Darren Cosker
- **Comment**: Preprint of our paper accepted by Neurocomputing
- **Journal**: None
- **Summary**: It is hard to estimate optical flow given a realworld video sequence with camera shake and other motion blur. In this paper, we first investigate the blur parameterization for video footage using near linear motion elements. we then combine a commercial 3D pose sensor with an RGB camera, in order to film video footage of interest together with the camera motion. We illustrates that this additional camera motion/trajectory channel can be embedded into a hybrid framework by interleaving an iterative blind deconvolution and warping based optical flow scheme. Our method yields improved accuracy within three other state-of-the-art baselines given our proposed ground truth blurry sequences; and several other realworld sequences filmed by our imaging system.



