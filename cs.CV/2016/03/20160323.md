# Arxiv Papers in cs.CV on 2016-03-23
### A Richly Annotated Dataset for Pedestrian Attribute Recognition
- **Arxiv ID**: http://arxiv.org/abs/1603.07054v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07054v3)
- **Published**: 2016-03-23 02:41:59+00:00
- **Updated**: 2016-04-27 06:42:25+00:00
- **Authors**: Dangwei Li, Zhang Zhang, Xiaotang Chen, Haibin Ling, Kaiqi Huang
- **Comment**: 16 pages, 8 figures
- **Journal**: None
- **Summary**: In this paper, we aim to improve the dataset foundation for pedestrian attribute recognition in real surveillance scenarios. Recognition of human attributes, such as gender, and clothes types, has great prospects in real applications. However, the development of suitable benchmark datasets for attribute recognition remains lagged behind. Existing human attribute datasets are collected from various sources or an integration of pedestrian re-identification datasets. Such heterogeneous collection poses a big challenge on developing high quality fine-grained attribute recognition algorithms. Furthermore, human attribute recognition are generally severely affected by environmental or contextual factors, such as viewpoints, occlusions and body parts, while existing attribute datasets barely care about them. To tackle these problems, we build a Richly Annotated Pedestrian (RAP) dataset from real multi-camera surveillance scenarios with long term collection, where data samples are annotated with not only fine-grained human attributes but also environmental and contextual factors. RAP has in total 41,585 pedestrian samples, each of which is annotated with 72 attributes as well as viewpoints, occlusions, body parts information. To our knowledge, the RAP dataset is the largest pedestrian attribute dataset, which is expected to greatly promote the study of large-scale attribute recognition systems. Furthermore, we empirically analyze the effects of different environmental and contextual factors on pedestrian attribute recognition. Experimental results demonstrate that viewpoints, occlusions and body parts information could assist attribute recognition a lot in real applications.



### Do We Really Need to Collect Millions of Faces for Effective Face Recognition?
- **Arxiv ID**: http://arxiv.org/abs/1603.07057v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07057v2)
- **Published**: 2016-03-23 02:57:15+00:00
- **Updated**: 2016-04-11 02:25:35+00:00
- **Authors**: Iacopo Masi, Anh Tuan Tran, Jatuporn Toy Leksut, Tal Hassner, Gerard Medioni
- **Comment**: None
- **Journal**: None
- **Summary**: Face recognition capabilities have recently made extraordinary leaps. Though this progress is at least partially due to ballooning training set sizes -- huge numbers of face images downloaded and labeled for identity -- it is not clear if the formidable task of collecting so many images is truly necessary. We propose a far more accessible means of increasing training data sizes for face recognition systems. Rather than manually harvesting and labeling more faces, we simply synthesize them. We describe novel methods of enriching an existing dataset with important facial appearance variations by manipulating the faces it contains. We further apply this synthesis approach when matching query images represented using a standard convolutional neural network. The effect of training and testing with synthesized images is extensively tested on the LFW and IJB-A (verification and identification) benchmarks and Janus CS2. The performances obtained by our approach match state of the art results reported by systems trained on millions of downloaded images.



### Semantic Object Parsing with Graph LSTM
- **Arxiv ID**: http://arxiv.org/abs/1603.07063v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07063v1)
- **Published**: 2016-03-23 03:31:02+00:00
- **Updated**: 2016-03-23 03:31:02+00:00
- **Authors**: Xiaodan Liang, Xiaohui Shen, Jiashi Feng, Liang Lin, Shuicheng Yan
- **Comment**: 18 pages
- **Journal**: None
- **Summary**: By taking the semantic object parsing task as an exemplar application scenario, we propose the Graph Long Short-Term Memory (Graph LSTM) network, which is the generalization of LSTM from sequential data or multi-dimensional data to general graph-structured data. Particularly, instead of evenly and fixedly dividing an image to pixels or patches in existing multi-dimensional LSTM structures (e.g., Row, Grid and Diagonal LSTMs), we take each arbitrary-shaped superpixel as a semantically consistent node, and adaptively construct an undirected graph for each image, where the spatial relations of the superpixels are naturally used as edges. Constructed on such an adaptive graph topology, the Graph LSTM is more naturally aligned with the visual patterns in the image (e.g., object boundaries or appearance similarities) and provides a more economical information propagation route. Furthermore, for each optimization step over Graph LSTM, we propose to use a confidence-driven scheme to update the hidden and memory states of nodes progressively till all nodes are updated. In addition, for each node, the forgets gates are adaptively learned to capture different degrees of semantic correlation with neighboring nodes. Comprehensive evaluations on four diverse semantic object parsing datasets well demonstrate the significant superiority of our Graph LSTM over other state-of-the-art solutions.



### Towards Viewpoint Invariant 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1603.07076v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07076v3)
- **Published**: 2016-03-23 06:24:19+00:00
- **Updated**: 2016-07-26 06:59:37+00:00
- **Authors**: Albert Haque, Boya Peng, Zelun Luo, Alexandre Alahi, Serena Yeung, Li Fei-Fei
- **Comment**: European Conference on Computer Vision (ECCV) 2016
- **Journal**: None
- **Summary**: We propose a viewpoint invariant model for 3D human pose estimation from a single depth image. To achieve this, our discriminative model embeds local regions into a learned viewpoint invariant feature space. Formulated as a multi-task learning problem, our model is able to selectively predict partial poses in the presence of noise and occlusion. Our approach leverages a convolutional and recurrent network architecture with a top-down error feedback mechanism to self-correct previous pose estimates in an end-to-end manner. We evaluate our model on a previously published depth dataset and a newly collected human pose dataset containing 100K annotated depth images from extreme viewpoints. Experiments show that our model achieves competitive performance on frontal views while achieving state-of-the-art performance on alternate viewpoints.



### Deep Multimodal Feature Analysis for Action Recognition in RGB+D Videos
- **Arxiv ID**: http://arxiv.org/abs/1603.07120v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07120v2)
- **Published**: 2016-03-23 10:22:12+00:00
- **Updated**: 2016-12-26 05:31:52+00:00
- **Authors**: Amir Shahroudy, Tian-Tsong Ng, Yihong Gong, Gang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Single modality action recognition on RGB or depth sequences has been extensively explored recently. It is generally accepted that each of these two modalities has different strengths and limitations for the task of action recognition. Therefore, analysis of the RGB+D videos can help us to better study the complementary properties of these two types of modalities and achieve higher levels of performance. In this paper, we propose a new deep autoencoder based shared-specific feature factorization network to separate input multimodal signals into a hierarchy of components. Further, based on the structure of the features, a structured sparsity learning machine is proposed which utilizes mixed norms to apply regularization within components and group selection between them for better classification performance. Our experimental results show the effectiveness of our cross-modality feature analysis framework by achieving state-of-the-art accuracy for action classification on five challenging benchmark datasets.



### Robust cDNA microarray image segmentation and analysis technique based on Hough circle transform
- **Arxiv ID**: http://arxiv.org/abs/1603.07123v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07123v1)
- **Published**: 2016-03-23 10:25:20+00:00
- **Updated**: 2016-03-23 10:25:20+00:00
- **Authors**: R. M. Farouk, M. A. SayedElahl
- **Comment**: 13 Pages,12 figures,FSP JOURNAL ISSN:1955-2068,Vol.9, Issue.7, part.1
- **Journal**: HFSP JOURNAL ISSN:1955-2068,Vol.9, Issue.7, part.1 Human Frontier
  Science Program , HFSP JOURNAL ISSN:1955-2068
- **Summary**: One of the most challenging tasks in microarray image analysis is spot segmentation. A solution to this problem is to provide an algorithm than can be used to find any spot within the microarray image. Circular Hough Transformation (CHT) is a powerful feature extraction technique used in image analysis, computer vision, and digital image processing. CHT algorithm is applied on the cDNA microarray images to develop the accuracy and the efficiency of the spots localization, addressing and segmentation process. The purpose of the applied technique is to find imperfect instances of spots within a certain class of circles by applying a voting procedure on the cDNA microarray images for spots localization, addressing and characterizing the pixels of each spot into foreground pixels and background simultaneously. Intensive experiments on the University of North Carolina (UNC) microarray database indicate that the proposed method is superior to the K-means method and the Support vector machine (SVM). Keywords: Hough circle transformation, cDNA microarray image analysis, cDNA microarray image segmentation, spots localization and addressing, spots segmentation



### BreakingNews: Article Annotation by Image and Text Processing
- **Arxiv ID**: http://arxiv.org/abs/1603.07141v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07141v1)
- **Published**: 2016-03-23 11:30:24+00:00
- **Updated**: 2016-03-23 11:30:24+00:00
- **Authors**: Arnau Ramisa, Fei Yan, Francesc Moreno-Noguer, Krystian Mikolajczyk
- **Comment**: None
- **Journal**: None
- **Summary**: Building upon recent Deep Neural Network architectures, current approaches lying in the intersection of computer vision and natural language processing have achieved unprecedented breakthroughs in tasks like automatic captioning or image retrieval. Most of these learning methods, though, rely on large training sets of images associated with human annotations that specifically describe the visual content. In this paper we propose to go a step further and explore the more complex cases where textual descriptions are loosely related to the images. We focus on the particular domain of News articles in which the textual content often expresses connotative and ambiguous relations that are only suggested but not directly inferred from images. We introduce new deep learning methods that address source detection, popularity prediction, article illustration and geolocation of articles. An adaptive CNN architecture is proposed, that shares most of the structure for all the tasks, and is suitable for multitask and transfer learning. Deep Canonical Correlation Analysis is deployed for article illustration, and a new loss function based on Great Circle Distance is proposed for geolocation. Furthermore, we present BreakingNews, a novel dataset with approximately 100K news articles including images, text and captions, and enriched with heterogeneous meta-data (such as GPS coordinates and popularity metrics). We show this dataset to be appropriate to explore all aforementioned problems, for which we provide a baseline performance using various Deep Learning architectures, and different representations of the textual and visual features. We report very promising results and bring to light several limitations of current state-of-the-art in this kind of domain, which we hope will help spur progress in the field.



### Weakly-Supervised Semantic Segmentation using Motion Cues
- **Arxiv ID**: http://arxiv.org/abs/1603.07188v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07188v3)
- **Published**: 2016-03-23 14:01:03+00:00
- **Updated**: 2017-04-21 08:16:06+00:00
- **Authors**: Pavel Tokmakov, Karteek Alahari, Cordelia Schmid
- **Comment**: Extended version of our ECCV 2016 paper
- **Journal**: None
- **Summary**: Fully convolutional neural networks (FCNNs) trained on a large number of images with strong pixel-level annotations have become the new state of the art for the semantic segmentation task. While there have been recent attempts to learn FCNNs from image-level weak annotations, they need additional constraints, such as the size of an object, to obtain reasonable performance. To address this issue, we present motion-CNN (M-CNN), a novel FCNN framework which incorporates motion cues and is learned from video-level weak annotations. Our learning scheme to train the network uses motion segments as soft constraints, thereby handling noisy motion information. When trained on weakly-annotated videos, our method outperforms the state-of-the-art EM-Adapt approach on the PASCAL VOC 2012 image segmentation benchmark. We also demonstrate that the performance of M-CNN learned with 150 weak video annotations is on par with state-of-the-art weakly-supervised methods trained with thousands of images. Finally, M-CNN substantially outperforms recent approaches in a related task of video co-localization on the YouTube-Objects dataset.



### Lightweight Unsupervised Domain Adaptation by Convolutional Filter Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1603.07234v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07234v1)
- **Published**: 2016-03-23 15:28:29+00:00
- **Updated**: 2016-03-23 15:28:29+00:00
- **Authors**: Rahaf Aljundi, Tinne Tuytelaars
- **Comment**: None
- **Journal**: None
- **Summary**: End-to-end learning methods have achieved impressive results in many areas of computer vision. At the same time, these methods still suffer from a degradation in performance when testing on new datasets that stem from a different distribution. This is known as the domain shift effect. Recently proposed adaptation methods focus on retraining the network parameters. However, this requires access to all (labeled) source data, a large amount of (unlabeled) target data, and plenty of computational resources. In this work, we propose a lightweight alternative, that allows adapting to the target domain based on a limited number of target samples in a matter of minutes rather than hours, days or even weeks. To this end, we first analyze the output of each convolutional layer from a domain adaptation perspective. Surprisingly, we find that already at the very first layer, domain shift effects pop up. We then propose a new domain adaptation method, where first layer convolutional filters that are badly affected by the domain shift are reconstructed based on less affected ones. This improves the performance of the deep network on various benchmark datasets.



### Global-Local Face Upsampling Network
- **Arxiv ID**: http://arxiv.org/abs/1603.07235v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1603.07235v2)
- **Published**: 2016-03-23 15:29:09+00:00
- **Updated**: 2016-04-27 15:31:01+00:00
- **Authors**: Oncel Tuzel, Yuichi Taguchi, John R. Hershey
- **Comment**: None
- **Journal**: None
- **Summary**: Face hallucination, which is the task of generating a high-resolution face image from a low-resolution input image, is a well-studied problem that is useful in widespread application areas. Face hallucination is particularly challenging when the input face resolution is very low (e.g., 10 x 12 pixels) and/or the image is captured in an uncontrolled setting with large pose and illumination variations. In this paper, we revisit the algorithm introduced in [1] and present a deep interpretation of this framework that achieves state-of-the-art under such challenging scenarios. In our deep network architecture the global and local constraints that define a face can be efficiently modeled and learned end-to-end using training data. Conceptually our network design can be partitioned into two sub-networks: the first one implements the holistic face reconstruction according to global constraints, and the second one enhances face-specific details and enforces local patch statistics. We optimize the deep network using a new loss function for super-resolution that combines reconstruction error with a learned face quality measure in adversarial setting, producing improved visual results. We conduct extensive experiments in both controlled and uncontrolled setups and show that our algorithm improves the state of the art both numerically and visually.



### Gaussian Process Morphable Models
- **Arxiv ID**: http://arxiv.org/abs/1603.07254v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07254v1)
- **Published**: 2016-03-23 16:19:29+00:00
- **Updated**: 2016-03-23 16:19:29+00:00
- **Authors**: Marcel Lüthi, Christoph Jud, Thomas Gerig, Thomas Vetter
- **Comment**: None
- **Journal**: None
- **Summary**: Statistical shape models (SSMs) represent a class of shapes as a normal distribution of point variations, whose parameters are estimated from example shapes. Principal component analysis (PCA) is applied to obtain a low-dimensional representation of the shape variation in terms of the leading principal components. In this paper, we propose a generalization of SSMs, called Gaussian Process Morphable Models (GPMMs). We model the shape variations with a Gaussian process, which we represent using the leading components of its Karhunen-Loeve expansion. To compute the expansion, we make use of an approximation scheme based on the Nystrom method. The resulting model can be seen as a continuous analogon of an SSM. However, while for SSMs the shape variation is restricted to the span of the example data, with GPMMs we can define the shape variation using any Gaussian process. For example, we can build shape models that correspond to classical spline models, and thus do not require any example data. Furthermore, Gaussian processes make it possible to combine different models. For example, an SSM can be extended with a spline model, to obtain a model that incorporates learned shape characteristics, but is flexible enough to explain shapes that cannot be represented by the SSM. We introduce a simple algorithm for fitting a GPMM to a surface or image. This results in a non-rigid registration approach, whose regularization properties are defined by a GPMM. We show how we can obtain different registration schemes,including methods for multi-scale, spatially-varying or hybrid registration, by constructing an appropriate GPMM. As our approach strictly separates modelling from the fitting process, this is all achieved without changes to the fitting algorithm. We show the applicability and versatility of GPMMs on a clinical use case, where the goal is the model-based segmentation of 3D forearm images.



### Face Recognition Using Deep Multi-Pose Representations
- **Arxiv ID**: http://arxiv.org/abs/1603.07388v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07388v1)
- **Published**: 2016-03-23 23:16:40+00:00
- **Updated**: 2016-03-23 23:16:40+00:00
- **Authors**: Wael AbdAlmageed, Yue Wua, Stephen Rawlsa, Shai Harel, Tal Hassner, Iacopo Masi, Jongmoo Choi, Jatuporn Toy Leksut, Jungyeon Kim, Prem Natarajan, Ram Nevatia, Gerard Medioni
- **Comment**: WACV 2016
- **Journal**: None
- **Summary**: We introduce our method and system for face recognition using multiple pose-aware deep learning models. In our representation, a face image is processed by several pose-specific deep convolutional neural network (CNN) models to generate multiple pose-specific features. 3D rendering is used to generate multiple face poses from the input image. Sensitivity of the recognition system to pose variations is reduced since we use an ensemble of pose-specific CNN features. The paper presents extensive experimental results on the effect of landmark detection, CNN layer selection and pose model selection on the performance of the recognition pipeline. Our novel representation achieves better results than the state-of-the-art on IARPA's CS2 and NIST's IJB-A in both verification and identification (i.e. search) tasks.



