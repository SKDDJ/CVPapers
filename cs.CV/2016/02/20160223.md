# Arxiv Papers in cs.CV on 2016-02-23
### A survey of sparse representation: algorithms and applications
- **Arxiv ID**: http://arxiv.org/abs/1602.07017v1
- **DOI**: 10.1109/ACCESS.2015.2430359
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1602.07017v1)
- **Published**: 2016-02-23 02:44:53+00:00
- **Updated**: 2016-02-23 02:44:53+00:00
- **Authors**: Zheng Zhang, Yong Xu, Jian Yang, Xuelong Li, David Zhang
- **Comment**: Published on IEEE Access, Vol. 3, pp. 490-530, 2015
- **Journal**: None
- **Summary**: Sparse representation has attracted much attention from researchers in fields of signal processing, image processing, computer vision and pattern recognition. Sparse representation also has a good reputation in both theoretical research and practical applications. Many different algorithms have been proposed for sparse representation. The main purpose of this article is to provide a comprehensive study and an updated review on sparse representation and to supply a guidance for researchers. The taxonomy of sparse representation methods can be studied from various viewpoints. For example, in terms of different norm minimizations used in sparsity constraints, the methods can be roughly categorized into five groups: sparse representation with $l_0$-norm minimization, sparse representation with $l_p$-norm (0$<$p$<$1) minimization, sparse representation with $l_1$-norm minimization and sparse representation with $l_{2,1}$-norm minimization. In this paper, a comprehensive overview of sparse representation is provided. The available sparse representation algorithms can also be empirically categorized into four groups: greedy strategy approximation, constrained optimization, proximity algorithm-based optimization, and homotopy algorithm-based sparse representation. The rationales of different algorithms in each category are analyzed and a wide range of sparse representation applications are summarized, which could sufficiently reveal the potential nature of the sparse representation theory. Specifically, an experimentally comparative study of these sparse representation algorithms was presented. The Matlab code used in this paper can be available at: http://www.yongxu.org/lunwen.html.



### Computer Aided Restoration of Handwritten Character Strokes
- **Arxiv ID**: http://arxiv.org/abs/1602.07038v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, math.NA, 68U07, 68U10, 65D18, 94A08, I.7.5; I.5.4; I.4.5; J.6; I.3.3; I.3.5
- **Links**: [PDF](http://arxiv.org/pdf/1602.07038v2)
- **Published**: 2016-02-23 04:47:28+00:00
- **Updated**: 2016-07-06 23:24:19+00:00
- **Authors**: Barak Sober, David Levin
- **Comment**: 11 pages, 17 figures
- **Journal**: None
- **Summary**: This work suggests a new variational approach to the task of computer aided restoration of incomplete characters, residing in a highly noisy document. We model character strokes as the movement of a pen with a varying radius. Following this model, a cubic spline representation is being utilized to perform gradient descent steps, while maintaining interpolation at some initial (manually sampled) points. The proposed algorithm was utilized in the process of restoring approximately 1000 ancient Hebrew characters (dating to ca. 8th-7th century BCE), some of which are presented herein and show that the algorithm yields plausible results when applied on deteriorated documents.



### Learning Shapes by Convex Composition
- **Arxiv ID**: http://arxiv.org/abs/1602.07613v2
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1602.07613v2)
- **Published**: 2016-02-23 08:01:58+00:00
- **Updated**: 2016-07-04 08:37:00+00:00
- **Authors**: Alireza Aghasi, Justin Romberg
- **Comment**: None
- **Journal**: None
- **Summary**: We present a mathematical and algorithmic scheme for learning the principal geometric elements in an image or 3D object. We build on recent work that convexifies the basic problem of finding a combination of a small number shapes that overlap and occlude one another in such a way that they "match" a given scene as closely as possible. This paper derives general sufficient conditions under which this convex shape composition identifies a target composition. From a computational standpoint, we present two different methods for solving the associated optimization programs. The first method simply recasts the problem as a linear program, while the second uses the alternating direction method of multipliers with a series of easily computed proximal operators.



### The ImageNet Shuffle: Reorganized Pre-training for Video Event Detection
- **Arxiv ID**: http://arxiv.org/abs/1602.07119v1
- **DOI**: 10.1145/2911996.2912036
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.07119v1)
- **Published**: 2016-02-23 11:12:55+00:00
- **Updated**: 2016-02-23 11:12:55+00:00
- **Authors**: Pascal Mettes, Dennis C. Koelma, Cees G. M. Snoek
- **Comment**: None
- **Journal**: None
- **Summary**: This paper strives for video event detection using a representation learned from deep convolutional neural networks. Different from the leading approaches, who all learn from the 1,000 classes defined in the ImageNet Large Scale Visual Recognition Challenge, we investigate how to leverage the complete ImageNet hierarchy for pre-training deep networks. To deal with the problems of over-specific classes and classes with few images, we introduce a bottom-up and top-down approach for reorganization of the ImageNet hierarchy based on all its 21,814 classes and more than 14 million images. Experiments on the TRECVID Multimedia Event Detection 2013 and 2015 datasets show that video representations derived from the layers of a deep neural network pre-trained with our reorganized hierarchy i) improves over standard pre-training, ii) is complementary among different reorganizations, iii) maintains the benefits of fusion with other modalities, and iv) leads to state-of-the-art event detection results. The reorganized hierarchies and their derived Caffe models are publicly available at http://tinyurl.com/imagenetshuffle.



### Car Type Recognition with Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1602.07125v2
- **DOI**: 10.1109/IVS.2016.7535529
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.07125v2)
- **Published**: 2016-02-23 11:34:58+00:00
- **Updated**: 2016-04-22 07:19:43+00:00
- **Authors**: Heikki Huttunen, Fatemeh Shokrollahi Yancheshmeh, Ke Chen
- **Comment**: To appear in proceedings of IEEE Intelligent Vehicles Symposium 2016
- **Journal**: In proceedings of IEEE Intelligent Vehicles Symposium 2016
- **Summary**: In this paper we study automatic recognition of cars of four types: Bus, Truck, Van and Small car. For this problem we consider two data driven frameworks: a deep neural network and a support vector machine using SIFT features. The accuracy of the methods is validated with a database of over 6500 images, and the resulting prediction accuracy is over 97 %. This clearly exceeds the accuracies of earlier studies that use manually engineered feature extraction pipelines.



### Exploring the Neural Algorithm of Artistic Style
- **Arxiv ID**: http://arxiv.org/abs/1602.07188v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.07188v2)
- **Published**: 2016-02-23 15:17:55+00:00
- **Updated**: 2016-03-13 21:13:57+00:00
- **Authors**: Yaroslav Nikulin, Roman Novak
- **Comment**: A short class project report (14 pages, 14 figures)
- **Journal**: None
- **Summary**: We explore the method of style transfer presented in the article "A Neural Algorithm of Artistic Style" by Leon A. Gatys, Alexander S. Ecker and Matthias Bethge (arXiv:1508.06576).   We first demonstrate the power of the suggested style space on a few examples. We then vary different hyper-parameters and program properties that were not discussed in the original paper, among which are the recognition network used, starting point of the gradient descent and different ways to partition style and content layers. We also give a brief comparison of some of the existing algorithm implementations and deep learning frameworks used.   To study the style space further we attempt to generate synthetic images by maximizing a single entry in one of the Gram matrices $\mathcal{G}_l$ and some interesting results are observed. Next, we try to mimic the sparsity and intensity distribution of Gram matrices obtained from a real painting and generate more complex textures.   Finally, we propose two new style representations built on top of network's features and discuss how one could be used to achieve local and potentially content-aware style transfer.



### Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning
- **Arxiv ID**: http://arxiv.org/abs/1602.07261v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.07261v2)
- **Published**: 2016-02-23 18:44:39+00:00
- **Updated**: 2016-08-23 16:42:29+00:00
- **Authors**: Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi
- **Comment**: None
- **Journal**: None
- **Summary**: Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge



### Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations
- **Arxiv ID**: http://arxiv.org/abs/1602.07332v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1602.07332v1)
- **Published**: 2016-02-23 22:00:40+00:00
- **Updated**: 2016-02-23 22:00:40+00:00
- **Authors**: Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, Fei-Fei Li
- **Comment**: 44 pages, 37 figures
- **Journal**: None
- **Summary**: Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked "What vehicle is the person riding?", computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) in order to answer correctly that "the person is riding a horse-drawn carriage".   In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 100K images where each image has an average of 21 objects, 18 attributes, and 18 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answers.



