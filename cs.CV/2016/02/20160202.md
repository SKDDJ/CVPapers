# Arxiv Papers in cs.CV on 2016-02-02
### Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects
- **Arxiv ID**: http://arxiv.org/abs/1602.00753v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1602.00753v1)
- **Published**: 2016-02-02 00:16:39+00:00
- **Updated**: 2016-02-02 00:16:39+00:00
- **Authors**: Hessam Bagherinezhad, Hannaneh Hajishirzi, Yejin Choi, Ali Farhadi
- **Comment**: To appear in AAAI 2016
- **Journal**: None
- **Summary**: Human vision greatly benefits from the information about sizes of objects. The role of size in several visual reasoning tasks has been thoroughly explored in human perception and cognition. However, the impact of the information about sizes of objects is yet to be determined in AI. We postulate that this is mainly attributed to the lack of a comprehensive repository of size information. In this paper, we introduce a method to automatically infer object sizes, leveraging visual and textual information from web. By maximizing the joint likelihood of textual and visual observations, our method learns reliable relative size estimates, with no explicit human supervision. We introduce the relative size dataset and show that our method outperforms competitive textual and visual baselines in reasoning about size comparisons.



### Simple Online and Realtime Tracking
- **Arxiv ID**: http://arxiv.org/abs/1602.00763v2
- **DOI**: 10.1109/ICIP.2016.7533003
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.00763v2)
- **Published**: 2016-02-02 01:39:28+00:00
- **Updated**: 2017-07-07 11:59:38+00:00
- **Authors**: Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, Ben Upcroft
- **Comment**: Presented at ICIP 2016, code is available at
  https://github.com/abewley/sort
- **Journal**: None
- **Summary**: This paper explores a pragmatic approach to multiple object tracking where the main focus is to associate objects efficiently for online and realtime applications. To this end, detection quality is identified as a key factor influencing tracking performance, where changing the detector can improve tracking by up to 18.9%. Despite only using a rudimentary combination of familiar techniques such as the Kalman Filter and Hungarian algorithm for the tracking components, this approach achieves an accuracy comparable to state-of-the-art online trackers. Furthermore, due to the simplicity of our tracking method, the tracker updates at a rate of 260 Hz which is over 20x faster than other state-of-the-art trackers.



### Learning a Deep Model for Human Action Recognition from Novel Viewpoints
- **Arxiv ID**: http://arxiv.org/abs/1602.00828v1
- **DOI**: 10.1103/PhysRevD.94.065007
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.00828v1)
- **Published**: 2016-02-02 08:42:44+00:00
- **Updated**: 2016-02-02 08:42:44+00:00
- **Authors**: Hossein Rahmani, Ajmal Mian, Mubarak Shah
- **Comment**: None
- **Journal**: Phys. Rev. D 94, 065007 (2016)
- **Summary**: Recognizing human actions from unknown and unseen (novel) views is a challenging problem. We propose a Robust Non-Linear Knowledge Transfer Model (R-NKTM) for human action recognition from novel views. The proposed R-NKTM is a deep fully-connected neural network that transfers knowledge of human actions from any unknown view to a shared high-level virtual view by finding a non-linear virtual path that connects the views. The R-NKTM is learned from dense trajectories of synthetic 3D human models fitted to real motion capture data and generalizes to real videos of human actions. The strength of our technique is that we learn a single R-NKTM for all actions and all viewpoints for knowledge transfer of any real human action video without the need for re-training or fine-tuning the model. Thus, R-NKTM can efficiently scale to incorporate new action classes. R-NKTM is learned with dummy labels and does not require knowledge of the camera viewpoint at any stage. Experiments on three benchmark cross-view human action datasets show that our method outperforms existing state-of-the-art.



### On distances, paths and connections for hyperspectral image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1603.08497v1
- **DOI**: None
- **Categories**: **cs.CV**, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1603.08497v1)
- **Published**: 2016-02-02 09:17:06+00:00
- **Updated**: 2016-02-02 09:17:06+00:00
- **Authors**: Guillaume Noyel, Jesus Angulo, Dominique Jeulin
- **Comment**: None
- **Journal**: Proceedings of the 8th International Symposium on Mathematical
  Morphology: Volume 1, pp.399-410, 2007, 978-85-17-00032-5
- **Summary**: The present paper introduces the $\eta$ and {\eta} connections in order to add regional information on $\lambda$-flat zones, which only take into account a local information. A top-down approach is considered. First $\lambda$-flat zones are built in a way leading to a sub-segmentation. Then a finer segmentation is obtained by computing $\eta$-bounded regions and $\mu$-geodesic balls inside the $\lambda$-flat zones. The proposed algorithms for the construction of new partitions are based on queues with an ordered selection of seeds using the cumulative distance. $\eta$-bounded regions offers a control on the variations of amplitude in the class from a point, called center, and $\mu$-geodesic balls controls the "size" of the class. These results are applied to hyperspectral images.



### Comparative evaluation of state-of-the-art algorithms for SSVEP-based BCIs
- **Arxiv ID**: http://arxiv.org/abs/1602.00904v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1602.00904v2)
- **Published**: 2016-02-02 12:31:48+00:00
- **Updated**: 2016-02-03 09:59:44+00:00
- **Authors**: Vangelis P. Oikonomou, Georgios Liaros, Kostantinos Georgiadis, Elisavet Chatzilari, Katerina Adam, Spiros Nikolopoulos, Ioannis Kompatsiaris
- **Comment**: None
- **Journal**: None
- **Summary**: Brain-computer interfaces (BCIs) have been gaining momentum in making human-computer interaction more natural, especially for people with neuro-muscular disabilities. Among the existing solutions the systems relying on electroencephalograms (EEG) occupy the most prominent place due to their non-invasiveness. However, the process of translating EEG signals into computer commands is far from trivial, since it requires the optimization of many different parameters that need to be tuned jointly. In this report, we focus on the category of EEG-based BCIs that rely on Steady-State-Visual-Evoked Potentials (SSVEPs) and perform a comparative evaluation of the most promising algorithms existing in the literature. More specifically, we define a set of algorithms for each of the various different parameters composing a BCI system (i.e. filtering, artifact removal, feature extraction, feature selection and classification) and study each parameter independently by keeping all other parameters fixed. The results obtained from this evaluation process are provided together with a dataset consisting of the 256-channel, EEG signals of 11 subjects, as well as a processing toolbox for reproducing the results and supporting further experimentation. In this way, we manage to make available for the community a state-of-the-art baseline for SSVEP-based BCIs that can be used as a basis for introducing novel methods and approaches.



### Unsupervised High-level Feature Learning by Ensemble Projection for Semi-supervised Image Classification and Image Clustering
- **Arxiv ID**: http://arxiv.org/abs/1602.00955v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.00955v2)
- **Published**: 2016-02-02 14:53:36+00:00
- **Updated**: 2016-02-04 13:58:00+00:00
- **Authors**: Dengxin Dai, Luc Van Gool
- **Comment**: 22 pages, 8 figures
- **Journal**: None
- **Summary**: This paper investigates the problem of image classification with limited or no annotations, but abundant unlabeled data. The setting exists in many tasks such as semi-supervised image classification, image clustering, and image retrieval. Unlike previous methods, which develop or learn sophisticated regularizers for classifiers, our method learns a new image representation by exploiting the distribution patterns of all available data for the task at hand. Particularly, a rich set of visual prototypes are sampled from all available data, and are taken as surrogate classes to train discriminative classifiers; images are projected via the classifiers; the projected values, similarities to the prototypes, are stacked to build the new feature vector. The training set is noisy. Hence, in the spirit of ensemble learning we create a set of such training sets which are all diverse, leading to diverse classifiers. The method is dubbed Ensemble Projection (EP). EP captures not only the characteristics of individual images, but also the relationships among images. It is conceptually simple and computationally efficient, yet effective and flexible. Experiments on eight standard datasets show that: (1) EP outperforms previous methods for semi-supervised image classification; (2) EP produces promising results for self-taught image classification, where unlabeled samples are a random collection of images rather than being from the same distribution as the labeled ones; and (3) EP improves over the original features for image clustering. The code of the method is available on the project page.



### Visual descriptors for content-based retrieval of remote sensing images
- **Arxiv ID**: http://arxiv.org/abs/1602.00970v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.00970v5)
- **Published**: 2016-02-02 15:19:16+00:00
- **Updated**: 2017-08-08 09:36:07+00:00
- **Authors**: Paolo Napoletano
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present an extensive evaluation of visual descriptors for the content-based retrieval of remote sensing (RS) images. The evaluation includes global hand-crafted, local hand-crafted, and Convolutional Neural Network (CNNs) features coupled with four different Content-Based Image Retrieval schemes. We conducted all the experiments on two publicly available datasets: the 21-class UC Merced Land Use/Land Cover (LandUse) dataset and 19-class High-resolution Satellite Scene dataset (SceneSat). The content of RS images might be quite heterogeneous, ranging from images containing fine grained textures, to coarse grained ones or to images containing objects. It is therefore not obvious in this domain, which descriptor should be employed to describe images having such a variability. Results demonstrate that CNN-based features perform better than both global and and local hand-crafted features whatever is the retrieval scheme adopted. Features extracted from SatResNet-50, a residual CNN suitable fine-tuned on the RS domain, shows much better performance than a residual CNN pre-trained on multimedia scene and object images. Features extracted from NetVLAD, a CNN that considers both CNN and local features, works better than others CNN solutions on those images that contain fine-grained textures and objects.



### Mental State Recognition via Wearable EEG
- **Arxiv ID**: http://arxiv.org/abs/1602.00985v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1602.00985v2)
- **Published**: 2016-02-02 15:55:20+00:00
- **Updated**: 2016-06-05 14:18:48+00:00
- **Authors**: Pouya Bashivan, Irina Rish, Steve Heisig
- **Comment**: Presented at MLINI-2015 workshop, 2015 (arXiv:cs/0101200)
- **Journal**: Proceedings of 5th NIPS workshop on Machine Learning and
  Interpretation in Neuroimaging (MLINI15) (2015) 5-1
- **Summary**: The increasing quality and affordability of consumer electroencephalogram (EEG) headsets make them attractive for situations where medical grade devices are impractical. Predicting and tracking cognitive states is possible for tasks that were previously not conducive to EEG monitoring. For instance, monitoring operators for states inappropriate to the task (e.g. drowsy drivers), tracking mental health (e.g. anxiety) and productivity (e.g. tiredness) are among possible applications for the technology. Consumer grade EEG headsets are affordable and relatively easy to use, but they lack the resolution and quality of signal that can be achieved using medical grade EEG devices. Thus, the key questions remain: to what extent are wearable EEG devices capable of mental state recognition, and what kind of mental states can be accurately recognized with these devices? In this work, we examined responses to two different types of input: instructional (logical) versus recreational (emotional) videos, using a range of machine-learning methods. We tried SVMs, sparse logistic regression, and Deep Belief Networks, to discriminate between the states of mind induced by different types of video input, that can be roughly labeled as logical vs. emotional. Our results demonstrate a significant potential of wearable EEG devices in differentiating cognitive states between situations with large contextual but subtle apparent differences.



### Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1602.00991v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, cs.RO, I.2.6; I.2.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1602.00991v2)
- **Published**: 2016-02-02 16:10:16+00:00
- **Updated**: 2016-03-08 22:09:05+00:00
- **Authors**: Peter Ondruska, Ingmar Posner
- **Comment**: Published in The Thirtieth AAAI Conference on Artificial Intelligence
  (AAAI-16), Video: https://youtu.be/cdeWCpfUGWc, Code:
  http://mrg.robots.ox.ac.uk/mrg_people/peter-ondruska/
- **Journal**: None
- **Summary**: This paper presents to the best of our knowledge the first end-to-end object tracking approach which directly maps from raw sensor input to object tracks in sensor space without requiring any feature engineering or system identification in the form of plant or sensor models. Specifically, our system accepts a stream of raw sensor data at one end and, in real-time, produces an estimate of the entire environment state at the output including even occluded objects. We achieve this by framing the problem as a deep learning task and exploit sequence models in the form of recurrent neural networks to learn a mapping from sensor measurements to object tracks. In particular, we propose a learning method based on a form of input dropout which allows learning in an unsupervised manner, only based on raw, occluded sensor data without access to ground-truth annotations. We demonstrate our approach using a synthetic dataset designed to mimic the task of tracking objects in 2D laser data -- as commonly encountered in robotics applications -- and show that it learns to track many dynamic objects despite occlusions and the presence of sensor noise.



### Head Pose Estimation of Occluded Faces using Regularized Regression
- **Arxiv ID**: http://arxiv.org/abs/1602.00997v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.00997v1)
- **Published**: 2016-02-02 16:27:18+00:00
- **Updated**: 2016-02-02 16:27:18+00:00
- **Authors**: Amit Kumar, Rishabh Bindal, Soumya Indela, Michael Rotkowitz
- **Comment**: Submitted to ICIP'16
- **Journal**: None
- **Summary**: This paper presents regression methods for estimation of head pose from occluded 2-D face images. The process primarily involves reconstructing a face from its occluded image, followed by classification. Typical methods for reconstruction assume that the pixel errors of the occluded regions are independent. However, such an assumption is not true in the case of occlusion, because of its inherent contiguous nature. Hence, we use nuclear norm as a metric that can describe well the structure of the error. We also use LASSO Regression based l1 - regularization to improve reconstruction. Next, we implement Nuclear Norm Regularized Regression (NR), and also our proposed method, for reconstruction and subsequent classification. Finally, we compare the performance of the methods in terms of accuracy of head pose estimation of occluded faces.



### A-expansion for multiple "hedgehog" shapes
- **Arxiv ID**: http://arxiv.org/abs/1602.01006v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.01006v1)
- **Published**: 2016-02-02 16:42:27+00:00
- **Updated**: 2016-02-02 16:42:27+00:00
- **Authors**: Hossam Isack, Yuri Boykov, Olga Veksler
- **Comment**: None
- **Journal**: None
- **Summary**: Overlapping colors and cluttered or weak edges are common segmentation problems requiring additional regularization. For example, star-convexity is popular for interactive single object segmentation due to simplicity and amenability to exact graph cut optimization. This paper proposes an approach to multiobject segmentation where objects could be restricted to separate "hedgehog" shapes. We show that a-expansion moves are submodular for our multi-shape constraints. Each "hedgehog" shape has its surface normals constrained by some vector field, e.g. gradients of a distance transform for user scribbles. Tight constraint give an extreme case of a shape prior enforcing skeleton consistency with the scribbles. Wider cones of allowed normals gives more relaxed hedgehog shapes. A single click and +/-90 degrees normal orientation constraints reduce our hedgehog prior to star-convexity. If all hedgehogs come from single clicks then our approach defines multi-star prior. Our general method has significantly more applications than standard one-star segmentation. For example, in medical data we can separate multiple non-star organs with similar appearances and weak or noisy edges.



### Development of an Ideal Observer that Incorporates Nuisance Parameters and Processes List-Mode Data
- **Arxiv ID**: http://arxiv.org/abs/1602.01449v1
- **DOI**: 10.1364/JOSAA.33.000689
- **Categories**: **physics.data-an**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1602.01449v1)
- **Published**: 2016-02-02 20:08:40+00:00
- **Updated**: 2016-02-02 20:08:40+00:00
- **Authors**: Christopher J. MacGahan, Matthew A. Kupinski, Nathan R. Hilton, Erik M. Brubaker, William C. Johnson
- **Comment**: None
- **Journal**: None
- **Summary**: Observer models were developed to process data in list-mode format in order to perform binary discrimination tasks for use in an arms-control-treaty context. Data used in this study was generated using GEANT4 Monte Carlo simulations for photons using custom models of plutonium inspection objects and a radiation imaging system. Observer model performance was evaluated and presented using the area under the receiver operating characteristic curve. The ideal observer was studied under both signal-known-exactly conditions and in the presence of unknowns such as object orientation and absolute count-rate variability; when these additional sources of randomness were present, their incorporation into the observer yielded superior performance.



### Fitting a 3D Morphable Model to Edges: A Comparison Between Hard and Soft Correspondences
- **Arxiv ID**: http://arxiv.org/abs/1602.01125v2
- **DOI**: 10.1007/978-3-319-54427-4_28
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.01125v2)
- **Published**: 2016-02-02 21:43:15+00:00
- **Updated**: 2016-10-03 13:25:00+00:00
- **Authors**: Anil Bas, William A. P. Smith, Timo Bolkart, Stefanie Wuhrer
- **Comment**: To appear in ACCV 2016 Workshop on Facial Informatics
- **Journal**: None
- **Summary**: We propose a fully automatic method for fitting a 3D morphable model to single face images in arbitrary pose and lighting. Our approach relies on geometric features (edges and landmarks) and, inspired by the iterated closest point algorithm, is based on computing hard correspondences between model vertices and edge pixels. We demonstrate that this is superior to previous work that uses soft correspondences to form an edge-derived cost surface that is minimised by nonlinear optimisation.



