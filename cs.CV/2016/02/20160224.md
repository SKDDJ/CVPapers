# Arxiv Papers in cs.CV on 2016-02-24
### SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size
- **Arxiv ID**: http://arxiv.org/abs/1602.07360v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1602.07360v4)
- **Published**: 2016-02-24 00:09:45+00:00
- **Updated**: 2016-11-04 21:26:08+00:00
- **Authors**: Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, Kurt Keutzer
- **Comment**: In ICLR Format
- **Journal**: None
- **Summary**: Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).   The SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet



### On Study of the Binarized Deep Neural Network for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1602.07373v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1602.07373v1)
- **Published**: 2016-02-24 02:39:47+00:00
- **Updated**: 2016-02-24 02:39:47+00:00
- **Authors**: Song Wang, Dongchun Ren, Li Chen, Wei Fan, Jun Sun, Satoshi Naoi
- **Comment**: 9 pages, 6 figures. Rejected conference (CVPR 2015) submission.
  Submission date: November, 2014. This work is patented in China (NO.
  201410647710.3)
- **Journal**: None
- **Summary**: Recently, the deep neural network (derived from the artificial neural network) has attracted many researchers' attention by its outstanding performance. However, since this network requires high-performance GPUs and large storage, it is very hard to use it on individual devices. In order to improve the deep neural network, many trials have been made by refining the network structure or training strategy. Unlike those trials, in this paper, we focused on the basic propagation function of the artificial neural network and proposed the binarized deep neural network. This network is a pure binary system, in which all the values and calculations are binarized. As a result, our network can save a lot of computational resource and storage. Therefore, it is possible to use it on various devices. Moreover, the experimental results proved the feasibility of the proposed network.



### How Deep Neural Networks Can Improve Emotion Recognition on Video Data
- **Arxiv ID**: http://arxiv.org/abs/1602.07377v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.07377v5)
- **Published**: 2016-02-24 03:10:32+00:00
- **Updated**: 2017-01-10 04:50:47+00:00
- **Authors**: Pooya Khorrami, Tom Le Paine, Kevin Brady, Charlie Dagli, Thomas S. Huang
- **Comment**: Accepted at ICIP 2016. Fixed typo in Experiments section
- **Journal**: None
- **Summary**: We consider the task of dimensional emotion recognition on video data using deep learning. While several previous methods have shown the benefits of training temporal neural network models such as recurrent neural networks (RNNs) on hand-crafted features, few works have considered combining convolutional neural networks (CNNs) with RNNs. In this work, we present a system that performs emotion recognition on video data using both CNNs and RNNs, and we also analyze how much each neural network component contributes to the system's overall performance. We present our findings on videos from the Audio/Visual+Emotion Challenge (AV+EC2015). In our experiments, we analyze the effects of several hyperparameters on overall performance while also achieving superior performance to the baseline and other competing methods.



### Automatic Moth Detection from Trap Images for Pest Management
- **Arxiv ID**: http://arxiv.org/abs/1602.07383v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1602.07383v1)
- **Published**: 2016-02-24 03:35:42+00:00
- **Updated**: 2016-02-24 03:35:42+00:00
- **Authors**: Weiguang Ding, Graham Taylor
- **Comment**: Preprints accepted by Computers and electronics in agriculture
- **Journal**: None
- **Summary**: Monitoring the number of insect pests is a crucial component in pheromone-based pest management systems. In this paper, we propose an automatic detection pipeline based on deep learning for identifying and counting pests in images taken inside field traps. Applied to a commercial codling moth dataset, our method shows promising performance both qualitatively and quantitatively. Compared to previous attempts at pest detection, our approach uses no pest-specific engineering which enables it to adapt to other species and environments with minimal human effort. It is amenable to implementation on parallel hardware and therefore capable of deployment in settings where real-time performance is required.



### Learning to Generate with Memory
- **Arxiv ID**: http://arxiv.org/abs/1602.07416v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1602.07416v2)
- **Published**: 2016-02-24 06:57:14+00:00
- **Updated**: 2016-05-28 03:41:27+00:00
- **Authors**: Chongxuan Li, Jun Zhu, Bo Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Memory units have been widely used to enrich the capabilities of deep networks on capturing long-term dependencies in reasoning and prediction tasks, but little investigation exists on deep generative models (DGMs) which are good at inferring high-level invariant representations from unlabeled data. This paper presents a deep generative model with a possibly large external memory and an attention mechanism to capture the local detail information that is often lost in the bottom-up abstraction process in representation learning. By adopting a smooth attention model, the whole network is trained end-to-end by optimizing a variational bound of data likelihood via auto-encoding variational Bayesian methods, where an asymmetric recognition network is learnt jointly to infer high-level invariant representations. The asymmetric architecture can reduce the competition between bottom-up invariant feature extraction and top-down generation of instance details. Our experiments on several datasets demonstrate that memory can significantly boost the performance of DGMs and even achieve state-of-the-art results on various tasks, including density estimation, image generation, and missing value imputation.



### A fine-grained approach to scene text script identification
- **Arxiv ID**: http://arxiv.org/abs/1602.07475v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.07475v1)
- **Published**: 2016-02-24 12:12:07+00:00
- **Updated**: 2016-02-24 12:12:07+00:00
- **Authors**: Lluis Gomez, Dimosthenis Karatzas
- **Comment**: None
- **Journal**: None
- **Summary**: This paper focuses on the problem of script identification in unconstrained scenarios. Script identification is an important prerequisite to recognition, and an indispensable condition for automatic text understanding systems designed for multi-language environments. Although widely studied for document images and handwritten documents, it remains an almost unexplored territory for scene text images.   We detail a novel method for script identification in natural images that combines convolutional features and the Naive-Bayes Nearest Neighbor classifier. The proposed framework efficiently exploits the discriminative power of small stroke-parts, in a fine-grained classification framework.   In addition, we propose a new public benchmark dataset for the evaluation of joint text detection and script identification in natural scenes. Experiments done in this new dataset demonstrate that the proposed method yields state of the art results, while it generalizes well to different datasets and variable number of scripts. The evidence provided shows that multi-lingual scene text recognition in the wild is a viable proposition. Source code of the proposed method is made available online.



### Improving patch-based scene text script identification with ensembles of conjoined networks
- **Arxiv ID**: http://arxiv.org/abs/1602.07480v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.07480v2)
- **Published**: 2016-02-24 12:33:25+00:00
- **Updated**: 2017-02-01 13:17:57+00:00
- **Authors**: Lluis Gomez, Anguelos Nicolaou, Dimosthenis Karatzas
- **Comment**: None
- **Journal**: None
- **Summary**: This paper focuses on the problem of script identification in scene text images. Facing this problem with state of the art CNN classifiers is not straightforward, as they fail to address a key characteristic of scene text instances: their extremely variable aspect ratio. Instead of resizing input images to a fixed aspect ratio as in the typical use of holistic CNN classifiers, we propose here a patch-based classification framework in order to preserve discriminative parts of the image that are characteristic of its class. We describe a novel method based on the use of ensembles of conjoined networks to jointly learn discriminative stroke-parts representations and their relative importance in a patch-based classification scheme. Our experiments with this learning procedure demonstrate state-of-the-art results in two public script identification datasets. In addition, we propose a new public benchmark dataset for the evaluation of multi-lingual scene text end-to-end reading systems. Experiments done in this dataset demonstrate the key role of script identification in a complete end-to-end system that combines our script identification method with a previously published text detector and an off-the-shelf OCR engine.



### SHAPE: Linear-Time Camera Pose Estimation With Quadratic Error-Decay
- **Arxiv ID**: http://arxiv.org/abs/1602.07535v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.07535v1)
- **Published**: 2016-02-24 14:53:29+00:00
- **Updated**: 2016-02-24 14:53:29+00:00
- **Authors**: Alireza Ghasemi, Adam Scholefield, Martin Vetterli
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel camera pose estimation or perspective-n-point (PnP) algorithm, based on the idea of consistency regions and half-space intersections. Our algorithm has linear time-complexity and a squared reconstruction error that decreases at least quadratically, as the number of feature point correspondences increase.   Inspired by ideas from triangulation and frame quantisation theory, we define consistent reconstruction and then present SHAPE, our proposed consistent pose estimation algorithm. We compare this algorithm with state-of-the-art pose estimation techniques in terms of accuracy and error decay rate. The experimental results verify our hypothesis on the optimal worst-case quadratic decay and demonstrate its promising performance compared to other approaches.



### On the Accuracy of Point Localisation in a Circular Camera-Array
- **Arxiv ID**: http://arxiv.org/abs/1602.07542v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.07542v1)
- **Published**: 2016-02-24 15:02:53+00:00
- **Updated**: 2016-02-24 15:02:53+00:00
- **Authors**: Alireza Ghasemi, Adam Scholefield, Martin Vetterli
- **Comment**: None
- **Journal**: None
- **Summary**: Although many advances have been made in light-field and camera-array image processing, there is still a lack of thorough analysis of the localisation accuracy of different multi-camera systems. By considering the problem from a frame-quantisation perspective, we are able to quantify the point localisation error of circular camera configurations. Specifically, we obtain closed form expressions bounding the localisation error in terms of the parameters describing the acquisition setup.   These theoretical results are independent of the localisation algorithm and thus provide fundamental limits on performance. Furthermore, the new frame-quantisation perspective is general enough to be extended to more complex camera configurations.



