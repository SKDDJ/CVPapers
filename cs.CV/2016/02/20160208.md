# Arxiv Papers in cs.CV on 2016-02-08
### A Large Dataset of Object Scans
- **Arxiv ID**: http://arxiv.org/abs/1602.02481v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1602.02481v3)
- **Published**: 2016-02-08 07:20:52+00:00
- **Updated**: 2016-05-05 05:35:48+00:00
- **Authors**: Sungjoon Choi, Qian-Yi Zhou, Stephen Miller, Vladlen Koltun
- **Comment**: Technical report
- **Journal**: None
- **Summary**: We have created a dataset of more than ten thousand 3D scans of real objects. To create the dataset, we recruited 70 operators, equipped them with consumer-grade mobile 3D scanning setups, and paid them to scan objects in their environments. The operators scanned objects of their choosing, outside the laboratory and without direct supervision by computer vision professionals. The result is a large and diverse collection of object scans: from shoes, mugs, and toys to grand pianos, construction vehicles, and large outdoor sculptures. We worked with an attorney to ensure that data acquisition did not violate privacy constraints. The acquired data was irrevocably placed in the public domain and is available freely at http://redwood-data.org/3dscan .



### A Semi-Automated Method for Object Segmentation in Infant's Egocentric Videos to Study Object Perception
- **Arxiv ID**: http://arxiv.org/abs/1602.02522v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.02522v1)
- **Published**: 2016-02-08 10:56:22+00:00
- **Updated**: 2016-02-08 10:56:22+00:00
- **Authors**: Qazaleh Mirsharif, Sidharth Sadani, Shishir Shah, Hanako Yoshida, Joseph Burling
- **Comment**: Accepted at CVIP 2016
- **Journal**: None
- **Summary**: Object segmentation in infant's egocentric videos is a fundamental step in studying how children perceive objects in early stages of development. From the computer vision perspective, object segmentation in such videos pose quite a few challenges because the child's view is unfocused, often with large head movements, effecting in sudden changes in the child's point of view which leads to frequent change in object properties such as size, shape and illumination. In this paper, we develop a semi-automated, domain specific, method to address these concerns and facilitate the object annotation process for cognitive scientists allowing them to select and monitor the object under segmentation. The method starts with an annotation from the user of the desired object and employs graph cut segmentation and optical flow computation to predict the object mask for subsequent video frames automatically. To maintain accuracy, we use domain specific heuristic rules to re-initialize the program with new user input whenever object properties change dramatically. The evaluations demonstrate the high speed and accuracy of the presented method for object segmentation in voluminous egocentric videos. We apply the proposed method to investigate potential patterns in object distribution in child's view at progressive ages.



### Homogeneity of Cluster Ensembles
- **Arxiv ID**: http://arxiv.org/abs/1602.02543v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1602.02543v1)
- **Published**: 2016-02-08 12:28:57+00:00
- **Updated**: 2016-02-08 12:28:57+00:00
- **Authors**: Brijnesh J. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: The expectation and the mean of partitions generated by a cluster ensemble are not unique in general. This issue poses challenges in statistical inference and cluster stability. In this contribution, we state sufficient conditions for uniqueness of expectation and mean. The proposed conditions show that a unique mean is neither exceptional nor generic. To cope with this issue, we introduce homogeneity as a measure of how likely is a unique mean for a sample of partitions. We show that homogeneity is related to cluster stability. This result points to a possible conflict between cluster stability and diversity in consensus clustering. To assess homogeneity in a practical setting, we propose an efficient way to compute a lower bound of homogeneity. Empirical results using the k-means algorithm suggest that uniqueness of the mean partition is not exceptional for real-world data. Moreover, for samples of high homogeneity, uniqueness can be enforced by increasing the number of data points or by removing outlier partitions. In a broader context, this contribution can be placed as a further step towards a statistical theory of partitions.



### Tumour ROI Estimation in Ultrasound Images via Radon Barcodes in Patients with Locally Advanced Breast Cancer
- **Arxiv ID**: http://arxiv.org/abs/1602.02586v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.02586v1)
- **Published**: 2016-02-08 14:39:01+00:00
- **Updated**: 2016-02-08 14:39:01+00:00
- **Authors**: Hamid R. Tizhoosh, Mehrdad J. Gangeh, Hadi Tadayyon, Gregory J. Czarnota
- **Comment**: To appear in proceedings of The International Symposium on Biomedical
  Imaging (ISBI), April 13-16, 2016, Prague, Czech Republic
- **Journal**: None
- **Summary**: Quantitative ultrasound (QUS) methods provide a promising framework that can non-invasively and inexpensively be used to predict or assess the tumour response to cancer treatment. The first step in using the QUS methods is to select a region of interest (ROI) inside the tumour in ultrasound images. Manual segmentation, however, is very time consuming and tedious. In this paper, a semi-automated approach will be proposed to roughly localize an ROI for a tumour in ultrasound images of patients with locally advanced breast cancer (LABC). Content-based barcodes, a recently introduced binary descriptor based on Radon transform, were used in order to find similar cases and estimate a bounding box surrounding the tumour. Experiments with 33 B-scan images resulted in promising results with an accuracy of $81\%$.



### Generating Images with Perceptual Similarity Metrics based on Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1602.02644v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1602.02644v2)
- **Published**: 2016-02-08 16:50:28+00:00
- **Updated**: 2016-02-09 09:36:36+00:00
- **Authors**: Alexey Dosovitskiy, Thomas Brox
- **Comment**: minor corrections
- **Journal**: None
- **Summary**: Image-generating machine learning models are typically trained with loss functions based on distance in the image space. This often leads to over-smoothed results. We propose a class of loss functions, which we call deep perceptual similarity metrics (DeePSiM), that mitigate this problem. Instead of computing distances in the image space, we compute distances between image features extracted by deep neural networks. This metric better reflects perceptually similarity of images and thus leads to better results. We show three applications: autoencoder training, a modification of a variational autoencoder, and inversion of deep convolutional networks. In all cases, the generated images look sharp and resemble natural images.



### Automatic Face Reenactment
- **Arxiv ID**: http://arxiv.org/abs/1602.02651v1
- **DOI**: 10.1109/CVPR.2014.537
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1602.02651v1)
- **Published**: 2016-02-08 17:05:37+00:00
- **Updated**: 2016-02-08 17:05:37+00:00
- **Authors**: Pablo Garrido, Levi Valgaerts, Ole Rehmsen, Thorsten Thormaehlen, Patrick Perez, Christian Theobalt
- **Comment**: Proceedings of the 2014 IEEE Conference on Computer Vision and
  Pattern Recognition (8 pages)
- **Journal**: None
- **Summary**: We propose an image-based, facial reenactment system that replaces the face of an actor in an existing target video with the face of a user from a source video, while preserving the original target performance. Our system is fully automatic and does not require a database of source expressions. Instead, it is able to produce convincing reenactment results from a short source video captured with an off-the-shelf camera, such as a webcam, where the user performs arbitrary facial gestures. Our reenactment pipeline is conceived as part image retrieval and part face transfer: The image retrieval is based on temporal clustering of target frames and a novel image matching metric that combines appearance and motion to select candidate frames from the source video, while the face transfer uses a 2D warping strategy that preserves the user's identity. Our system excels in simplicity as it does not rely on a 3D face model, it is robust under head motion and does not require the source and target performance to be similar. We show convincing reenactment results for videos that we recorded ourselves and for low-quality footage taken from the Internet.



### Exploiting Cyclic Symmetry in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1602.02660v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1602.02660v2)
- **Published**: 2016-02-08 17:37:16+00:00
- **Updated**: 2016-05-26 11:47:18+00:00
- **Authors**: Sander Dieleman, Jeffrey De Fauw, Koray Kavukcuoglu
- **Comment**: 10 pages, 6 figures, accepted for publication at ICML 2016
- **Journal**: None
- **Summary**: Many classes of images exhibit rotational symmetry. Convolutional neural networks are sometimes trained using data augmentation to exploit this, but they are still required to learn the rotation equivariance properties from the data. Encoding these properties into the network architecture, as we are already used to doing for translation equivariance by using convolutional layers, could result in a more efficient use of the parameter budget by relieving the model from learning them. We introduce four operations which can be inserted into neural network models as layers, and which can be combined to make these models partially equivariant to rotations. They also enable parameter sharing across different orientations. We evaluate the effect of these architectural modifications on three datasets which exhibit rotational symmetry and demonstrate improved performance with smaller models.



### Multimodal Remote Sensing Image Registration with Accuracy Estimation at Local and Global Scales
- **Arxiv ID**: http://arxiv.org/abs/1602.02720v2
- **DOI**: 10.1109/TGRS.2016.2587321
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.02720v2)
- **Published**: 2016-02-08 20:05:42+00:00
- **Updated**: 2016-05-25 20:16:54+00:00
- **Authors**: M. L. Uss, B. Vozel, V. V. Lukin, K. Chehdi
- **Comment**: 48 pages, 8 figures, 5 tables, 51 references Revised arguments in
  sections 2 and 3. Additional test cases added in Section 4; comparison with
  the state-of-the-art improved. References added. Conclusions unchanged.
  Proofread
- **Journal**: None
- **Summary**: This paper focuses on potential accuracy of remote sensing images registration. We investigate how this accuracy can be estimated without ground truth available and used to improve registration quality of mono- and multi-modal pair of images. At the local scale of image fragments, the Cramer-Rao lower bound (CRLB) on registration error is estimated for each local correspondence between coarsely registered pair of images. This CRLB is defined by local image texture and noise properties. Opposite to the standard approach, where registration accuracy is only evaluated at the output of the registration process, such valuable information is used by us as an additional input knowledge. It greatly helps detecting and discarding outliers and refining the estimation of geometrical transformation model parameters. Based on these ideas, a new area-based registration method called RAE (Registration with Accuracy Estimation) is proposed. In addition to its ability to automatically register very complex multimodal image pairs with high accuracy, the RAE method provides registration accuracy at the global scale as covariance matrix of estimation error of geometrical transformation model parameters or as point-wise registration Standard Deviation. This accuracy does not depend on any ground truth availability and characterizes each pair of registered images individually. Thus, the RAE method can identify image areas for which a predefined registration accuracy is guaranteed. The RAE method is proved successful with reaching subpixel accuracy while registering eight complex mono/multimodal and multitemporal image pairs including optical to optical, optical to radar, optical to Digital Elevation Model (DEM) images and DEM to radar cases. Other methods employed in comparisons fail to provide in a stable manner accurate results on the same test cases.



