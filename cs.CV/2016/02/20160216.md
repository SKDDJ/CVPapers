# Arxiv Papers in cs.CV on 2016-02-16
### Deep Feature-based Face Detection on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/1602.04868v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.04868v1)
- **Published**: 2016-02-16 00:14:22+00:00
- **Updated**: 2016-02-16 00:14:22+00:00
- **Authors**: Sayantan Sarkar, Vishal M. Patel, Rama Chellappa
- **Comment**: ISBA 2016
- **Journal**: None
- **Summary**: We propose a deep feature-based face detector for mobile devices to detect user's face acquired by the front facing camera. The proposed method is able to detect faces in images containing extreme pose and illumination variations as well as partial faces. The main challenge in developing deep feature-based algorithms for mobile devices is the constrained nature of the mobile platform and the non-availability of CUDA enabled GPUs on such devices. Our implementation takes into account the special nature of the images captured by the front-facing camera of mobile devices and exploits the GPUs present in mobile devices without CUDA-based frameorks, to meet these challenges.



### Fast, Robust, Continuous Monocular Egomotion Computation
- **Arxiv ID**: http://arxiv.org/abs/1602.04886v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1602.04886v1)
- **Published**: 2016-02-16 02:18:04+00:00
- **Updated**: 2016-02-16 02:18:04+00:00
- **Authors**: Andrew Jaegle, Stephen Phillips, Kostas Daniilidis
- **Comment**: Accepted as a conference paper at ICRA 2016. Main paper: 8 pages, 7
  figures. Supplement: 4 pages, 2 figures
- **Journal**: None
- **Summary**: We propose robust methods for estimating camera egomotion in noisy, real-world monocular image sequences in the general case of unknown observer rotation and translation with two views and a small baseline. This is a difficult problem because of the nonconvex cost function of the perspective camera motion equation and because of non-Gaussian noise arising from noisy optical flow estimates and scene non-rigidity. To address this problem, we introduce the expected residual likelihood method (ERL), which estimates confidence weights for noisy optical flow data using likelihood distributions of the residuals of the flow field under a range of counterfactual model parameters. We show that ERL is effective at identifying outliers and recovering appropriate confidence weights in many settings. We compare ERL to a novel formulation of the perspective camera motion equation using a lifted kernel, a recently proposed optimization framework for joint parameter and confidence weight estimation with good empirical properties. We incorporate these strategies into a motion estimation pipeline that avoids falling into local minima. We find that ERL outperforms the lifted kernel method and baseline monocular egomotion estimation strategies on the challenging KITTI dataset, while adding almost no runtime cost over baseline egomotion methods.



### Segmentation Rectification for Video Cutout via One-Class Structured Learning
- **Arxiv ID**: http://arxiv.org/abs/1602.04906v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1602.04906v1)
- **Published**: 2016-02-16 04:31:20+00:00
- **Updated**: 2016-02-16 04:31:20+00:00
- **Authors**: Junyan Wang, Sai-kit Yeung, Jue Wang, Kun Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Recent works on interactive video object cutout mainly focus on designing dynamic foreground-background (FB) classifiers for segmentation propagation. However, the research on optimally removing errors from the FB classification is sparse, and the errors often accumulate rapidly, causing significant errors in the propagated frames. In this work, we take the initial steps to addressing this problem, and we call this new task \emph{segmentation rectification}. Our key observation is that the possibly asymmetrically distributed false positive and false negative errors were handled equally in the conventional methods. We, alternatively, propose to optimally remove these two types of errors. To this effect, we propose a novel bilayer Markov Random Field (MRF) model for this new task. We also adopt the well-established structured learning framework to learn the optimal model from data. Additionally, we propose a novel one-class structured SVM (OSSVM) which greatly speeds up the structured learning process. Our method naturally extends to RGB-D videos as well. Comprehensive experiments on both RGB and RGB-D data demonstrate that our simple and effective method significantly outperforms the segmentation propagation methods adopted in the state-of-the-art video cutout systems, and the results also suggest the potential usefulness of our method in image cutout system.



### A diffusion and clustering-based approach for finding coherent motions and understanding crowd scenes
- **Arxiv ID**: http://arxiv.org/abs/1602.04921v1
- **DOI**: 10.1109/TIP.2016.2531281
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1602.04921v1)
- **Published**: 2016-02-16 06:25:30+00:00
- **Updated**: 2016-02-16 06:25:30+00:00
- **Authors**: Weiyao Lin, Yang Mi, Weiyue Wang, Jianxin Wu, Jingdong Wang, Tao Mei
- **Comment**: This manuscript is the accepted version for TIP (IEEE Transactions on
  Image Processing), 2016
- **Journal**: None
- **Summary**: This paper addresses the problem of detecting coherent motions in crowd scenes and presents its two applications in crowd scene understanding: semantic region detection and recurrent activity mining. It processes input motion fields (e.g., optical flow fields) and produces a coherent motion filed, named as thermal energy field. The thermal energy field is able to capture both motion correlation among particles and the motion trends of individual particles which are helpful to discover coherency among them. We further introduce a two-step clustering process to construct stable semantic regions from the extracted time-varying coherent motions. These semantic regions can be used to recognize pre-defined activities in crowd scenes. Finally, we introduce a cluster-and-merge process which automatically discovers recurrent activities in crowd scenes by clustering and merging the extracted coherent motions. Experiments on various videos demonstrate the effectiveness of our approach.



### Optimizing Gaze Direction in a Visual Navigation Task
- **Arxiv ID**: http://arxiv.org/abs/1602.04981v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1602.04981v1)
- **Published**: 2016-02-16 11:00:56+00:00
- **Updated**: 2016-02-16 11:00:56+00:00
- **Authors**: Tuomas Välimäki, Risto Ritala
- **Comment**: None
- **Journal**: None
- **Summary**: Navigation in an unknown environment consists of multiple separable subtasks, such as collecting information about the surroundings and navigating to the current goal. In the case of pure visual navigation, all these subtasks need to utilize the same vision system, and therefore a way to optimally control the direction of focus is needed. We present a case study, where we model the active sensing problem of directing the gaze of a mobile robot with three machine vision cameras as a partially observable Markov decision process (POMDP) using a mutual information (MI) based reward function. The key aspect of the solution is that the cameras are dynamically used either in monocular or stereo configuration. The benefits of using the proposed active sensing implementation are demonstrated with simulations and experiments on a real robot.



### Contextual Media Retrieval Using Natural Language Queries
- **Arxiv ID**: http://arxiv.org/abs/1602.04983v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.AI, cs.CL, cs.CV, cs.HC, H.3.3; H.5.1; I.2.6; I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/1602.04983v1)
- **Published**: 2016-02-16 11:04:29+00:00
- **Updated**: 2016-02-16 11:04:29+00:00
- **Authors**: Sreyasi Nag Chowdhury, Mateusz Malinowski, Andreas Bulling, Mario Fritz
- **Comment**: 8 pages, 9 figures, 1 table
- **Journal**: None
- **Summary**: The widespread integration of cameras in hand-held and head-worn devices as well as the ability to share content online enables a large and diverse visual capture of the world that millions of users build up collectively every day. We envision these images as well as associated meta information, such as GPS coordinates and timestamps, to form a collective visual memory that can be queried while automatically taking the ever-changing context of mobile users into account. As a first step towards this vision, in this work we present Xplore-M-Ego: a novel media retrieval system that allows users to query a dynamic database of images and videos using spatio-temporal natural language queries. We evaluate our system using a new dataset of real user queries as well as through a usability study. One key finding is that there is a considerable amount of inter-user variability, for example in the resolution of spatial relations in natural language utterances. We show that our retrieval system can cope with this variability using personalisation through an online learning-based retrieval formulation.



### Deconvolutional Feature Stacking for Weakly-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1602.04984v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.04984v3)
- **Published**: 2016-02-16 11:05:24+00:00
- **Updated**: 2016-03-12 08:22:30+00:00
- **Authors**: Hyo-Eun Kim, Sangheum Hwang
- **Comment**: None
- **Journal**: None
- **Summary**: A weakly-supervised semantic segmentation framework with a tied deconvolutional neural network is presented. Each deconvolution layer in the framework consists of unpooling and deconvolution operations. 'Unpooling' upsamples the input feature map based on unpooling switches defined by corresponding convolution layer's pooling operation. 'Deconvolution' convolves the input unpooled features by using convolutional weights tied with the corresponding convolution layer's convolution operation. The unpooling-deconvolution combination helps to eliminate less discriminative features in a feature extraction stage, since output features of the deconvolution layer are reconstructed from the most discriminative unpooled features instead of the raw one. This results in reduction of false positives in a pixel-level inference stage. All the feature maps restored from the entire deconvolution layers can constitute a rich discriminative feature set according to different abstraction levels. Those features are stacked to be selectively used for generating class-specific activation maps. Under the weak supervision (image-level labels), the proposed framework shows promising results on lesion segmentation in medical images (chest X-rays) and achieves state-of-the-art performance on the PASCAL VOC segmentation dataset in the same experimental condition.



### Generating images with recurrent adversarial networks
- **Arxiv ID**: http://arxiv.org/abs/1602.05110v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1602.05110v5)
- **Published**: 2016-02-16 17:51:39+00:00
- **Updated**: 2016-12-13 03:21:03+00:00
- **Authors**: Daniel Jiwoong Im, Chris Dongjoo Kim, Hui Jiang, Roland Memisevic
- **Comment**: None
- **Journal**: None
- **Summary**: Gatys et al. (2015) showed that optimizing pixels to match features in a convolutional network with respect reference image features is a way to render images of high visual quality. We show that unrolling this gradient-based optimization yields a recurrent computation that creates images by incrementally adding onto a visual "canvas". We propose a recurrent generative model inspired by this view, and show that it can be trained using adversarial training to generate very good image samples. We also propose a way to quantitatively compare adversarial networks by having the generators and discriminators of these networks compete against each other.



### An Approach for Noise Removal on Depth Images
- **Arxiv ID**: http://arxiv.org/abs/1602.05168v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.05168v1)
- **Published**: 2016-02-16 20:28:16+00:00
- **Updated**: 2016-02-16 20:28:16+00:00
- **Authors**: Rashi Chaudhary, Himanshu Dasgupta
- **Comment**: None
- **Journal**: None
- **Summary**: Image based rendering is a fundamental problem in computer vision and graphics. Modern techniques often rely on depth image for the 3D construction. However for most of the existing depth cameras, the large and unpredictable noises can be problematic, which can cause noticeable artifacts in the rendered results. In this paper, we proposed an efficacious method for depth image noise removal that can be applied for most RGBD systems. The proposed solution will benefit many subsequent vision problems such as 3D reconstruction, novel view rendering, object recognition. Our experimental results demonstrate the efficacy and accuracy.



