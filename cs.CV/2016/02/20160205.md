# Arxiv Papers in cs.CV on 2016-02-05
### Search Tracker: Human-derived object tracking in-the-wild through large-scale search and retrieval
- **Arxiv ID**: http://arxiv.org/abs/1602.01890v1
- **DOI**: 10.1109/TCSVT.2016.2555718
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1602.01890v1)
- **Published**: 2016-02-05 00:01:13+00:00
- **Updated**: 2016-02-05 00:01:13+00:00
- **Authors**: Archith J. Bency, S. Karthikeyan, Carter De Leo, Santhoshkumar Sunderrajan, B. S. Manjunath
- **Comment**: Under review with the IEEE Transactions on Circuits and Systems for
  Video Technology
- **Journal**: None
- **Summary**: Humans use context and scene knowledge to easily localize moving objects in conditions of complex illumination changes, scene clutter and occlusions. In this paper, we present a method to leverage human knowledge in the form of annotated video libraries in a novel search and retrieval based setting to track objects in unseen video sequences. For every video sequence, a document that represents motion information is generated. Documents of the unseen video are queried against the library at multiple scales to find videos with similar motion characteristics. This provides us with coarse localization of objects in the unseen video. We further adapt these retrieved object locations to the new video using an efficient warping scheme. The proposed method is validated on in-the-wild video surveillance datasets where we outperform state-of-the-art appearance-based trackers. We also introduce a new challenging dataset with complex object appearance changes.



### Generate Image Descriptions based on Deep RNN and Memory Cells for Images Features
- **Arxiv ID**: http://arxiv.org/abs/1602.01895v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1602.01895v1)
- **Published**: 2016-02-05 00:17:18+00:00
- **Updated**: 2016-02-05 00:17:18+00:00
- **Authors**: Shijian Tang, Song Han
- **Comment**: None
- **Journal**: None
- **Summary**: Generating natural language descriptions for images is a challenging task. The traditional way is to use the convolutional neural network (CNN) to extract image features, followed by recurrent neural network (RNN) to generate sentences. In this paper, we present a new model that added memory cells to gate the feeding of image features to the deep neural network. The intuition is enabling our model to memorize how much information from images should be fed at each stage of the RNN. Experiments on Flickr8K and Flickr30K datasets showed that our model outperforms other state-of-the-art models with higher BLEU scores.



### Recognition of Visually Perceived Compositional Human Actions by Multiple Spatio-Temporal Scales Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1602.01921v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1602.01921v3)
- **Published**: 2016-02-05 04:00:16+00:00
- **Updated**: 2017-02-22 16:33:49+00:00
- **Authors**: Haanvid Lee, Minju Jung, Jun Tani
- **Comment**: 10 pages, 9 figures, 5 tables
- **Journal**: None
- **Summary**: The current paper proposes a novel neural network model for recognizing visually perceived human actions. The proposed multiple spatio-temporal scales recurrent neural network (MSTRNN) model is derived by introducing multiple timescale recurrent dynamics to the conventional convolutional neural network model. One of the essential characteristics of the MSTRNN is that its architecture imposes both spatial and temporal constraints simultaneously on the neural activity which vary in multiple scales among different layers. As suggested by the principle of the upward and downward causation, it is assumed that the network can develop meaningful structures such as functional hierarchy by taking advantage of such constraints during the course of learning. To evaluate the characteristics of the model, the current study uses three types of human action video dataset consisting of different types of primitive actions and different levels of compositionality on them. The performance of the MSTRNN in testing with these dataset is compared with the ones by other representative deep learning models used in the field. The analysis of the internal representation obtained through the learning with the dataset clarifies what sorts of functional hierarchy can be developed by extracting the essential compositionality underlying the dataset.



### On Feature based Delaunay Triangulation for Palmprint Recognition
- **Arxiv ID**: http://arxiv.org/abs/1602.01927v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.01927v1)
- **Published**: 2016-02-05 05:31:41+00:00
- **Updated**: 2016-02-05 05:31:41+00:00
- **Authors**: Zanobya N. Khan, Rashid Jalal Qureshi, Jamil Ahmad
- **Comment**: None
- **Journal**: Journal of Platform Technology, 3(4), 9-18 (2015)
- **Summary**: Authentication of individuals via palmprint based biometric system is becoming very popular due to its reliability as it contains unique and stable features. In this paper, we present a novel approach for palmprint recognition and its representation. To extract the palm lines, local thresholding technique Niblack binarization algorithm is adopted. The endpoints of these lines are determined and a connection is created among them using the Delaunay triangulation thereby generating a distinct topological structure of each palmprint. Next, we extract different geometric as well as quantitative features from the triangles of the Delaunay triangulation that assist in identifying different individuals. To ensure that the proposed approach is invariant to rotation and scaling, features were made relative to topological and geometrical structure of the palmprint. The similarity of the two palmprints is computed using the weighted sum approach and compared with the k-nearest neighbor. The experimental results obtained reflect the effectiveness of the proposed approach to discriminate between different palmprint images and thus achieved a recognition rate of 90% over large databases.



### Automatic and Quantitative evaluation of attribute discovery methods
- **Arxiv ID**: http://arxiv.org/abs/1602.01940v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.01940v1)
- **Published**: 2016-02-05 07:43:08+00:00
- **Updated**: 2016-02-05 07:43:08+00:00
- **Authors**: Liangchen Liu, Arnold Wiliem, Shaokang Chen, Brian C. Lovell
- **Comment**: 9 pages, WACV 2016
- **Journal**: None
- **Summary**: Many automatic attribute discovery methods have been developed to extract a set of visual attributes from images for various tasks. However, despite good performance in some image classification tasks, it is difficult to evaluate whether these methods discover meaningful attributes and which one is the best to find the attributes for image descriptions. An intuitive way to evaluate this is to manually verify whether consistent identifiable visual concepts exist to distinguish between positive and negative images of an attribute. This manual checking is tedious, labor intensive and expensive and it is very hard to get quantitative comparisons between different methods. In this work, we tackle this problem by proposing an attribute meaningfulness metric, that can perform automatic evaluation on the meaningfulness of attribute sets as well as achieving quantitative comparisons. We apply our proposed metric to recent automatic attribute discovery methods and popular hashing methods on three attribute datasets. A user study is also conducted to validate the effectiveness of the metric. In our evaluation, we gleaned some insights that could be beneficial in developing automatic attribute discovery methods to generate meaningful attributes. To the best of our knowledge, this is the first work to quantitatively measure the semantic content of automatically discovered attributes.



### Preoperative Volume Determination for Pituitary Adenoma
- **Arxiv ID**: http://arxiv.org/abs/1602.02022v1
- **DOI**: 10.1117/12.877660
- **Categories**: **cs.CV**, cs.CG, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1602.02022v1)
- **Published**: 2016-02-05 14:08:21+00:00
- **Updated**: 2016-02-05 14:08:21+00:00
- **Authors**: Dzenan Zukic, Jan Egger, Miriam H. A. Bauer, Daniela Kuhnt, Barbara Carl, Bernd Freisleben, Andreas Kolb, Christopher Nimsky
- **Comment**: 7 pages, 6 figures, 1 table, 16 references in Proc. SPIE 7963,
  Medical Imaging 2011: Computer-Aided Diagnosis, 79632T (9 March 2011). arXiv
  admin note: text overlap with arXiv:1103.1778
- **Journal**: None
- **Summary**: The most common sellar lesion is the pituitary adenoma, and sellar tumors are approximately 10-15% of all intracranial neoplasms. Manual slice-by-slice segmentation takes quite some time that can be reduced by using the appropriate algorithms. In this contribution, we present a segmentation method for pituitary adenoma. The method is based on an algorithm that we have applied recently to segmenting glioblastoma multiforme. A modification of this scheme is used for adenoma segmentation that is much harder to perform, due to lack of contrast-enhanced boundaries. In our experimental evaluation, neurosurgeons performed manual slice-by-slice segmentation of ten magnetic resonance imaging (MRI) cases. The segmentations were compared to the segmentation results of the proposed method using the Dice Similarity Coefficient (DSC). The average DSC for all datasets was 75.92% +/- 7.24%. A manual segmentation took about four minutes and our algorithm required about one second.



### Efficient Multi-view Performance Capture of Fine-Scale Surface Detail
- **Arxiv ID**: http://arxiv.org/abs/1602.02023v1
- **DOI**: 10.1109/3DV.2014.46
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1602.02023v1)
- **Published**: 2016-02-05 14:08:47+00:00
- **Updated**: 2016-02-05 14:08:47+00:00
- **Authors**: Nadia Robertini, Edilson De Aguiar, Thomas Helten, Christian Theobalt
- **Comment**: 3D Vision (3DV), 2014 2nd International Conference on
- **Journal**: None
- **Summary**: We present a new effective way for performance capture of deforming meshes with fine-scale time-varying surface detail from multi-view video. Our method builds up on coarse 4D surface reconstructions, as obtained with commonly used template-based methods. As they only capture models of coarse-to-medium scale detail, fine scale deformation detail is often done in a second pass by using stereo constraints, features, or shading-based refinement. In this paper, we propose a new effective and stable solution to this second step. Our framework creates an implicit representation of the deformable mesh using a dense collection of 3D Gaussian functions on the surface, and a set of 2D Gaussians for the images. The fine scale deformation of all mesh vertices that maximizes photo-consistency can be efficiently found by densely optimizing a new model-to-image consistency energy on all vertex positions. A principal advantage is that our problem formulation yields a smooth closed form energy with implicit occlusion handling and analytic derivatives. Error-prone correspondence finding, or discrete sampling of surface displacement values are also not needed. We show several reconstructions of human subjects wearing loose clothing, and we qualitatively and quantitatively show that we robustly capture more detail than related methods.



### Sub-cortical brain structure segmentation using F-CNN's
- **Arxiv ID**: http://arxiv.org/abs/1602.02130v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.02130v1)
- **Published**: 2016-02-05 19:32:39+00:00
- **Updated**: 2016-02-05 19:32:39+00:00
- **Authors**: Mahsa Shakeri, Stavros Tsogkas, Enzo Ferrante, Sarah Lippe, Samuel Kadoury, Nikos Paragios, Iasonas Kokkinos
- **Comment**: ISBI 2016: International Symposium on Biomedical Imaging, Apr 2016,
  Prague, Czech Republic
- **Journal**: None
- **Summary**: In this paper we propose a deep learning approach for segmenting sub-cortical structures of the human brain in Magnetic Resonance (MR) image data. We draw inspiration from a state-of-the-art Fully-Convolutional Neural Network (F-CNN) architecture for semantic segmentation of objects in natural images, and adapt it to our task. Unlike previous CNN-based methods that operate on image patches, our model is applied on a full blown 2D image, without any alignment or registration steps at testing time. We further improve segmentation results by interpreting the CNN output as potentials of a Markov Random Field (MRF), whose topology corresponds to a volumetric grid. Alpha-expansion is used to perform approximate inference imposing spatial volumetric homogeneity to the CNN priors. We compare the performance of the proposed pipeline with a similar system using Random Forest-based priors, as well as state-of-art segmentation algorithms, and show promising results on two different brain MRI datasets.



