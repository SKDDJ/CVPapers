# Arxiv Papers in cs.CV on 2016-04-16
### Evolutionary Projection Selection for Radon Barcodes
- **Arxiv ID**: http://arxiv.org/abs/1604.04673v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04673v1)
- **Published**: 2016-04-16 00:48:52+00:00
- **Updated**: 2016-04-16 00:48:52+00:00
- **Authors**: Hamid R. Tizhoosh, Shahryar Rahnamayan
- **Comment**: To appear in proceedings of The 2016 IEEE Congress on Evolutionary
  Computation (IEEE CEC 2016), July 24-29, 2016, Vancouver, Canada
- **Journal**: None
- **Summary**: Recently, Radon transformation has been used to generate barcodes for tagging medical images. The under-sampled image is projected in certain directions, and each projection is binarized using a local threshold. The concatenation of the thresholded projections creates a barcode that can be used for tagging or annotating medical images. A small number of equidistant projections, e.g., 4 or 8, is generally used to generate short barcodes. However, due to the diverse nature of digital images, and since we are only working with a small number of projections (to keep the barcode short), taking equidistant projections may not be the best course of action. In this paper, we proposed to find $n$ optimal projections, whereas $n\!<\!180$, in order to increase the expressiveness of Radon barcodes. We show examples for the exhaustive search for the simple case when we attempt to find 4 best projections out of 16 equidistant projections and compare it with the evolutionary approach in order to establish the benefit of the latter when operating on a small population size as in the case of micro-DE. We randomly selected 10 different classes from IRMA dataset (14,400 x-ray images in 58 classes) and further randomly selected 5 images per class for our tests.



### Radon Features and Barcodes for Medical Image Retrieval via SVM
- **Arxiv ID**: http://arxiv.org/abs/1604.04675v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04675v1)
- **Published**: 2016-04-16 01:13:23+00:00
- **Updated**: 2016-04-16 01:13:23+00:00
- **Authors**: Shujin Zhu, H. R. Tizhoosh
- **Comment**: To appear in proceedings of The 2016 IEEE International Joint
  Conference on Neural Networks (IJCNN 2016), July 24-29, 2016, Vancouver,
  Canada
- **Journal**: None
- **Summary**: For more than two decades, research has been performed on content-based image retrieval (CBIR). By combining Radon projections and the support vector machines (SVM), a content-based medical image retrieval method is presented in this work. The proposed approach employs the normalized Radon projections with corresponding image category labels to build an SVM classifier, and the Radon barcode database which encodes every image in a binary format is also generated simultaneously to tag all images. To retrieve similar images when a query image is given, Radon projections and the barcode of the query image are generated. Subsequently, the k-nearest neighbor search method is applied to find the images with minimum Hamming distance of the Radon barcode within the same class predicted by the trained SVM classifier that uses Radon features. The performance of the proposed method is validated by using the IRMA 2009 dataset with 14,410 x-ray images in 57 categories. The results demonstrate that our method has the capacity to retrieve similar responses for the correctly identified query image and even for those mistakenly classified by SVM. The approach further is very fast and has low memory requirement.



### Generating Binary Tags for Fast Medical Image Retrieval Based on Convolutional Nets and Radon Transform
- **Arxiv ID**: http://arxiv.org/abs/1604.04676v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04676v1)
- **Published**: 2016-04-16 01:30:01+00:00
- **Updated**: 2016-04-16 01:30:01+00:00
- **Authors**: Xinran Liu, Hamid R. Tizhoosh, Jonathan Kofman
- **Comment**: To appear in proceedings of The 2016 IEEE International Joint
  Conference on Neural Networks (IJCNN 2016), July 24-29, 2016, Vancouver,
  Canada
- **Journal**: None
- **Summary**: Content-based image retrieval (CBIR) in large medical image archives is a challenging and necessary task. Generally, different feature extraction methods are used to assign expressive and invariant features to each image such that the search for similar images comes down to feature classification and/or matching. The present work introduces a new image retrieval method for medical applications that employs a convolutional neural network (CNN) with recently introduced Radon barcodes. We combine neural codes for global classification with Radon barcodes for the final retrieval. We also examine image search based on regions of interest (ROI) matching after image retrieval. The IRMA dataset with more than 14,000 x-rays images is used to evaluate the performance of our method. Experimental results show that our approach is superior to many published works.



### Anatomy-Aware Measurement of Segmentation Accuracy
- **Arxiv ID**: http://arxiv.org/abs/1604.04678v1
- **DOI**: 10.1117/12.2214869
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04678v1)
- **Published**: 2016-04-16 01:49:22+00:00
- **Updated**: 2016-04-16 01:49:22+00:00
- **Authors**: Hamid R. Tizhoosh, Ahmed A. Othman
- **Comment**: To appear in SPIE Medical Imaging 2016
- **Journal**: None
- **Summary**: Quantifying the accuracy of segmentation and manual delineation of organs, tissue types and tumors in medical images is a necessary measurement that suffers from multiple problems. One major shortcoming of all accuracy measures is that they neglect the anatomical significance or relevance of different zones within a given segment. Hence, existing accuracy metrics measure the overlap of a given segment with a ground-truth without any anatomical discrimination inside the segment. For instance, if we understand the rectal wall or urethral sphincter as anatomical zones, then current accuracy measures ignore their significance when they are applied to assess the quality of the prostate gland segments. In this paper, we propose an anatomy-aware measurement scheme for segmentation accuracy of medical images. The idea is to create a ``master gold'' based on a consensus shape containing not just the outline of the segment but also the outlines of the internal zones if existent or relevant. To apply this new approach to accuracy measurement, we introduce the anatomy-aware extensions of both Dice coefficient and Jaccard index and investigate their effect using 500 synthetic prostate ultrasound images with 20 different segments for each image. We show that through anatomy-sensitive calculation of segmentation accuracy, namely by considering relevant anatomical zones, not only the measurement of individual users can change but also the ranking of users' segmentation skills may require reordering.



### Subcategory-aware Convolutional Neural Networks for Object Proposals and Detection
- **Arxiv ID**: http://arxiv.org/abs/1604.04693v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04693v3)
- **Published**: 2016-04-16 05:05:32+00:00
- **Updated**: 2017-03-09 01:19:53+00:00
- **Authors**: Yu Xiang, Wongun Choi, Yuanqing Lin, Silvio Savarese
- **Comment**: Published in WACV 2017
- **Journal**: None
- **Summary**: In CNN-based object detection methods, region proposal becomes a bottleneck when objects exhibit significant scale variation, occlusion or truncation. In addition, these methods mainly focus on 2D object detection and cannot estimate detailed properties of objects. In this paper, we propose subcategory-aware CNNs for object detection. We introduce a novel region proposal network that uses subcategory information to guide the proposal generating process, and a new detection network for joint detection and subcategory classification. By using subcategories related to object pose, we achieve state-of-the-art performance on both detection and pose estimation on commonly used benchmarks.



### Automatic Segmentation of Dynamic Objects from an Image Pair
- **Arxiv ID**: http://arxiv.org/abs/1604.04724v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04724v1)
- **Published**: 2016-04-16 11:00:24+00:00
- **Updated**: 2016-04-16 11:00:24+00:00
- **Authors**: Sri Raghu Malireddi, Shanmuganathan Raman
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Automatic segmentation of objects from a single image is a challenging problem which generally requires training on large number of images. We consider the problem of automatically segmenting only the dynamic objects from a given pair of images of a scene captured from different positions. We exploit dense correspondences along with saliency measures in order to first localize the interest points on the dynamic objects from the two images. We propose a novel approach based on techniques from computational geometry in order to automatically segment the dynamic objects from both the images using a top-down segmentation strategy. We discuss how the proposed approach is unique in novelty compared to other state-of-the-art segmentation algorithms. We show that the proposed approach for segmentation is efficient in handling large motions and is able to achieve very good segmentation of the objects for different scenes. We analyse the results with respect to the manually marked ground truth segmentation masks created using our own dataset and provide key observations in order to improve the work in future.



### Efficient Dictionary Learning with Sparseness-Enforcing Projections
- **Arxiv ID**: http://arxiv.org/abs/1604.04767v1
- **DOI**: 10.1007/s11263-015-0799-8
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1604.04767v1)
- **Published**: 2016-04-16 15:42:12+00:00
- **Updated**: 2016-04-16 15:42:12+00:00
- **Authors**: Markus Thom, Matthias Rapp, Günther Palm
- **Comment**: The final publication is available at Springer via
  http://dx.doi.org/10.1007/s11263-015-0799-8
- **Journal**: International Journal of Computer Vision, vol. 114, no. 2, pp.
  168-194, 2015
- **Summary**: Learning dictionaries suitable for sparse coding instead of using engineered bases has proven effective in a variety of image processing tasks. This paper studies the optimization of dictionaries on image data where the representation is enforced to be explicitly sparse with respect to a smooth, normalized sparseness measure. This involves the computation of Euclidean projections onto level sets of the sparseness measure. While previous algorithms for this optimization problem had at least quasi-linear time complexity, here the first algorithm with linear time complexity and constant space complexity is proposed. The key for this is the mathematically rigorous derivation of a characterization of the projection's result based on a soft-shrinkage function. This theory is applied in an original algorithm called Easy Dictionary Learning (EZDL), which learns dictionaries with a simple and fast-to-compute Hebbian-like learning rule. The new algorithm is efficient, expressive and particularly simple to implement. It is demonstrated that despite its simplicity, the proposed learning algorithm is able to generate a rich variety of dictionaries, in particular a topographic organization of atoms or separable atoms. Further, the dictionaries are as expressive as those of benchmark learning algorithms in terms of the reproduction quality on entire images, and result in an equivalent denoising performance. EZDL learns approximately 30 % faster than the already very efficient Online Dictionary Learning algorithm, and is therefore eligible for rapid data set analysis and problems with vast quantities of learning samples.



### ACD: Action Concept Discovery from Image-Sentence Corpora
- **Arxiv ID**: http://arxiv.org/abs/1604.04784v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04784v1)
- **Published**: 2016-04-16 18:26:13+00:00
- **Updated**: 2016-04-16 18:26:13+00:00
- **Authors**: Jiyang Gao, Chen Sun, Ram Nevatia
- **Comment**: 8 pages, accepted by ICMR 2016
- **Journal**: None
- **Summary**: Action classification in still images is an important task in computer vision. It is challenging as the appearances of ac- tions may vary depending on their context (e.g. associated objects). Manually labeling of context information would be time consuming and difficult to scale up. To address this challenge, we propose a method to automatically discover and cluster action concepts, and learn their classifiers from weakly supervised image-sentence corpora. It obtains candidate action concepts by extracting verb-object pairs from sentences and verifies their visualness with the associated images. Candidate action concepts are then clustered by using a multi-modal representation with image embeddings from deep convolutional networks and text embeddings from word2vec. More than one hundred human action concept classifiers are learned from the Flickr 30k dataset with no additional human effort and promising classification results are obtained. We further apply the AdaBoost algorithm to automatically select and combine relevant action concepts given an action query. Promising results have been shown on the PASCAL VOC 2012 action classification benchmark, which has zero overlap with Flickr30k.



### Learning Models for Actions and Person-Object Interactions with Transfer to Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1604.04808v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04808v2)
- **Published**: 2016-04-16 22:54:05+00:00
- **Updated**: 2016-07-28 04:44:36+00:00
- **Authors**: Arun Mallya, Svetlana Lazebnik
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes deep convolutional network models that utilize local and global context to make human activity label predictions in still images, achieving state-of-the-art performance on two recent datasets with hundreds of labels each. We use multiple instance learning to handle the lack of supervision on the level of individual person instances, and weighted loss to handle unbalanced training data. Further, we show how specialized features trained on these datasets can be used to improve accuracy on the Visual Question Answering (VQA) task, in the form of multiple choice fill-in-the-blank questions (Visual Madlibs). Specifically, we tackle two types of questions on person activity and person-object relationship and show improvements over generic features trained on the ImageNet classification task.



