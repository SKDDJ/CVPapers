# Arxiv Papers in cs.CV on 2016-04-25
### Makeup like a superstar: Deep Localized Makeup Transfer Network
- **Arxiv ID**: http://arxiv.org/abs/1604.07102v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1604.07102v1)
- **Published**: 2016-04-25 01:01:51+00:00
- **Updated**: 2016-04-25 01:01:51+00:00
- **Authors**: Si Liu, Xinyu Ou, Ruihe Qian, Wei Wang, Xiaochun Cao
- **Comment**: 7pages, 11 figures, to appear in IJCAI 2016
- **Journal**: None
- **Summary**: In this paper, we propose a novel Deep Localized Makeup Transfer Network to automatically recommend the most suitable makeup for a female and synthesis the makeup on her face. Given a before-makeup face, her most suitable makeup is determined automatically. Then, both the beforemakeup and the reference faces are fed into the proposed Deep Transfer Network to generate the after-makeup face. Our end-to-end makeup transfer network have several nice properties including: (1) with complete functions: including foundation, lip gloss, and eye shadow transfer; (2) cosmetic specific: different cosmetics are transferred in different manners; (3) localized: different cosmetics are applied on different facial regions; (4) producing naturally looking results without obvious artifacts; (5) controllable makeup lightness: various results from light makeup to heavy makeup can be generated. Qualitative and quantitative experiments show that our network performs much better than the methods of [Guo and Sim, 2009] and two variants of NerualStyle [Gatys et al., 2015a].



### Actionness Estimation Using Hybrid Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1604.07279v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.07279v1)
- **Published**: 2016-04-25 14:32:28+00:00
- **Updated**: 2016-04-25 14:32:28+00:00
- **Authors**: Limin Wang, Yu Qiao, Xiaoou Tang, Luc Van Gool
- **Comment**: accepted by CVPR16
- **Journal**: None
- **Summary**: Actionness was introduced to quantify the likelihood of containing a generic action instance at a specific location. Accurate and efficient estimation of actionness is important in video analysis and may benefit other relevant tasks such as action recognition and action detection. This paper presents a new deep architecture for actionness estimation, called hybrid fully convolutional network (H-FCN), which is composed of appearance FCN (A-FCN) and motion FCN (M-FCN). These two FCNs leverage the strong capacity of deep models to estimate actionness maps from the perspectives of static appearance and dynamic motion, respectively. In addition, the fully convolutional nature of H-FCN allows it to efficiently process videos with arbitrary sizes. Experiments are conducted on the challenging datasets of Stanford40, UCF Sports, and JHMDB to verify the effectiveness of H-FCN on actionness estimation, which demonstrate that our method achieves superior performance to previous ones. Moreover, we apply the estimated actionness maps on action proposal generation and action detection. Our actionness maps advance the current state-of-the-art performance of these tasks substantially.



### End to End Learning for Self-Driving Cars
- **Arxiv ID**: http://arxiv.org/abs/1604.07316v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1604.07316v1)
- **Published**: 2016-04-25 16:03:56+00:00
- **Updated**: 2016-04-25 16:03:56+00:00
- **Authors**: Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao, Karol Zieba
- **Comment**: None
- **Journal**: None
- **Summary**: We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads.   The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads.   Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps.   We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS).



### Semi-supervised Dictionary Learning Based on Hilbert-Schmidt Independence Criterion
- **Arxiv ID**: http://arxiv.org/abs/1604.07319v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.07319v1)
- **Published**: 2016-04-25 16:25:38+00:00
- **Updated**: 2016-04-25 16:25:38+00:00
- **Authors**: Mehrdad J. Gangeh, Safaa M. A. Bedawi, Ali Ghodsi, Fakhri Karray
- **Comment**: Accepted at International conference on Image analysis and
  Recognition (ICIAR) 2016
- **Journal**: None
- **Summary**: In this paper, a novel semi-supervised dictionary learning and sparse representation (SS-DLSR) is proposed. The proposed method benefits from the supervisory information by learning the dictionary in a space where the dependency between the data and class labels is maximized. This maximization is performed using Hilbert-Schmidt independence criterion (HSIC). On the other hand, the global distribution of the underlying manifolds were learned from the unlabeled data by minimizing the distances between the unlabeled data and the corresponding nearest labeled data in the space of the dictionary learned. The proposed SS-DLSR algorithm has closed-form solutions for both the dictionary and sparse coefficients, and therefore does not have to learn the two iteratively and alternately as is common in the literature of the DLSR. This makes the solution for the proposed algorithm very fast. The experiments confirm the improvement in classification performance on benchmark datasets by including the information from both labeled and unlabeled data, particularly when there are many unlabeled data.



### Scalable Gaussian Processes for Supervised Hashing
- **Arxiv ID**: http://arxiv.org/abs/1604.07335v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.07335v1)
- **Published**: 2016-04-25 17:30:20+00:00
- **Updated**: 2016-04-25 17:30:20+00:00
- **Authors**: Bahadir Ozdemir, Larry S. Davis
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: We propose a flexible procedure for large-scale image search by hash functions with kernels. Our method treats binary codes and pairwise semantic similarity as latent and observed variables, respectively, in a probabilistic model based on Gaussian processes for binary classification. We present an efficient inference algorithm with the sparse pseudo-input Gaussian process (SPGP) model and parallelization. Experiments on three large-scale image dataset demonstrate the effectiveness of the proposed hashing method, Gaussian Process Hashing (GPH), for short binary codes and the datasets without predefined classes in comparison to the state-of-the-art supervised hashing methods.



### Supervised Incremental Hashing
- **Arxiv ID**: http://arxiv.org/abs/1604.07342v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.07342v2)
- **Published**: 2016-04-25 17:50:05+00:00
- **Updated**: 2016-06-09 17:24:25+00:00
- **Authors**: Bahadir Ozdemir, Mahyar Najibi, Larry S. Davis
- **Comment**: 14 pages, 6 figures
- **Journal**: None
- **Summary**: We propose an incremental strategy for learning hash functions with kernels for large-scale image search. Our method is based on a two-stage classification framework that treats binary codes as intermediate variables between the feature space and the semantic space. In the first stage of classification, binary codes are considered as class labels by a set of binary SVMs; each corresponds to one bit. In the second stage, binary codes become the input space of a multi-class SVM. Hash functions are learned by an efficient algorithm where the NP-hard problem of finding optimal binary codes is solved via cyclic coordinate descent and SVMs are trained in a parallelized incremental manner. For modifications like adding images from a previously unseen class, we describe an incremental procedure for effective and efficient updates to the previous hash functions. Experiments on three large-scale image datasets demonstrate the effectiveness of the proposed hashing method, Supervised Incremental Hashing (SIH), over the state-of-the-art supervised hashing methods.



### Attributes for Improved Attributes: A Multi-Task Network for Attribute Classification
- **Arxiv ID**: http://arxiv.org/abs/1604.07360v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.07360v1)
- **Published**: 2016-04-25 18:49:55+00:00
- **Updated**: 2016-04-25 18:49:55+00:00
- **Authors**: Emily M. Hand, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: Attributes, or semantic features, have gained popularity in the past few years in domains ranging from activity recognition in video to face verification. Improving the accuracy of attribute classifiers is an important first step in any application which uses these attributes. In most works to date, attributes have been considered to be independent. However, we know this not to be the case. Many attributes are very strongly related, such as heavy makeup and wearing lipstick. We propose to take advantage of attribute relationships in three ways: by using a multi-task deep convolutional neural network (MCNN) sharing the lowest layers amongst all attributes, sharing the higher layers for related attributes, and by building an auxiliary network on top of the MCNN which utilizes the scores from all attributes to improve the final classification of each attribute. We demonstrate the effectiveness of our method by producing results on two challenging publicly available datasets.



### Persistence Lenses: Segmentation, Simplification, Vectorization, Scale Space and Fractal Analysis of Images
- **Arxiv ID**: http://arxiv.org/abs/1604.07361v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, math.GN
- **Links**: [PDF](http://arxiv.org/pdf/1604.07361v3)
- **Published**: 2016-04-25 18:50:19+00:00
- **Updated**: 2016-06-21 17:18:40+00:00
- **Authors**: Martin Brooks
- **Comment**: None
- **Journal**: None
- **Summary**: A persistence lens is a hierarchy of disjoint upper and lower level sets of a continuous luminance image's Reeb graph. The boundary components of a persistence lens's interior components are Jordan curves that serve as a hierarchical segmentation of the image, and may be rendered as vector graphics. A persistence lens determines a varilet basis for the luminance image, in which image simplification is a realized by subspace projection. Image scale space, and image fractal analysis, result from applying a scale measure to each basis function.



### Context Encoders: Feature Learning by Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1604.07379v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1604.07379v2)
- **Published**: 2016-04-25 19:42:46+00:00
- **Updated**: 2016-11-21 20:56:42+00:00
- **Authors**: Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, Alexei A. Efros
- **Comment**: New results on ImageNet Generation
- **Journal**: CVPR 2016
- **Summary**: We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.



### Balancing Appearance and Context in Sketch Interpretation
- **Arxiv ID**: http://arxiv.org/abs/1604.07429v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1604.07429v1)
- **Published**: 2016-04-25 20:14:35+00:00
- **Updated**: 2016-04-25 20:14:35+00:00
- **Authors**: Yale Song, Randall Davis, Kaichen Ma, Dana L. Penny
- **Comment**: None
- **Journal**: None
- **Summary**: We describe a sketch interpretation system that detects and classifies clock numerals created by subjects taking the Clock Drawing Test, a clinical tool widely used to screen for cognitive impairments (e.g., dementia). We describe how it balances appearance and context, and document its performance on some 2,000 drawings (about 24K clock numerals) produced by a wide spectrum of patients. We calibrate the utility of different forms of context, describing experiments with Conditional Random Fields trained and tested using a variety of features. We identify context that contributes to interpreting otherwise ambiguous or incomprehensible strokes. We describe ST-slices, a novel representation that enables "unpeeling" the layers of ink that result when people overwrite, which often produces ink impossible to analyze if only the final drawing is examined. We characterize when ST-slices work, calibrate their impact on performance, and consider their breadth of applicability.



### Modeling the Contribution of Central Versus Peripheral Vision in Scene, Object, and Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1604.07457v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1604.07457v1)
- **Published**: 2016-04-25 22:01:50+00:00
- **Updated**: 2016-04-25 22:01:50+00:00
- **Authors**: Panqu Wang, Garrison Cottrell
- **Comment**: CogSci 2016 Conference Paper
- **Journal**: None
- **Summary**: It is commonly believed that the central visual field is important for recognizing objects and faces, and the peripheral region is useful for scene recognition. However, the relative importance of central versus peripheral information for object, scene, and face recognition is unclear. In a behavioral study, Larson and Loschky (2009) investigated this question by measuring the scene recognition accuracy as a function of visual angle, and demonstrated that peripheral vision was indeed more useful in recognizing scenes than central vision. In this work, we modeled and replicated the result of Larson and Loschky (2009), using deep convolutional neural networks. Having fit the data for scenes, we used the model to predict future data for large-scale scene recognition as well as for objects and faces. Our results suggest that the relative order of importance of using central visual field information is face recognition>object recognition>scene recognition, and vice-versa for peripheral information.



### Long-Term Identity-Aware Multi-Person Tracking for Surveillance Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/1604.07468v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.07468v2)
- **Published**: 2016-04-25 22:53:55+00:00
- **Updated**: 2017-04-11 01:21:20+00:00
- **Authors**: Shoou-I Yu, Yi Yang, Xuanchong Li, Alexander G. Hauptmann
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-person tracking plays a critical role in the analysis of surveillance video. However, most existing work focus on shorter-term (e.g. minute-long or hour-long) video sequences. Therefore, we propose a multi-person tracking algorithm for very long-term (e.g. month-long) multi-camera surveillance scenarios. Long-term tracking is challenging because 1) the apparel/appearance of the same person will vary greatly over multiple days and 2) a person will leave and re-enter the scene numerous times. To tackle these challenges, we leverage face recognition information, which is robust to apparel change, to automatically reinitialize our tracker over multiple days of recordings. Unfortunately, recognized faces are unavailable oftentimes. Therefore, our tracker propagates identity information to frames without recognized faces by uncovering the appearance and spatial manifold formed by person detections. We tested our algorithm on a 23-day 15-camera data set (4,935 hours total), and we were able to localize a person 53.2% of the time with 69.8% precision. We further performed video summarization experiments based on our tracking output. Results on 116.25 hours of video showed that we were able to generate a reasonable visual diary (i.e. a summary of what a person did) for different people, thus potentially opening the door to automatic summarization of the vast amount of surveillance video generated every day.



### Joint Semantic Segmentation and Depth Estimation with Deep Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1604.07480v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.07480v3)
- **Published**: 2016-04-25 23:58:00+00:00
- **Updated**: 2016-09-19 21:57:28+00:00
- **Authors**: Arsalan Mousavian, Hamed Pirsiavash, Jana Kosecka
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-scale deep CNNs have been used successfully for problems mapping each pixel to a label, such as depth estimation and semantic segmentation. It has also been shown that such architectures are reusable and can be used for multiple tasks. These networks are typically trained independently for each task by varying the output layer(s) and training objective. In this work we present a new model for simultaneous depth estimation and semantic segmentation from a single RGB image. Our approach demonstrates the feasibility of training parts of the model for each task and then fine tuning the full, combined model on both tasks simultaneously using a single loss function. Furthermore we couple the deep CNN with fully connected CRF, which captures the contextual relationships and interactions between the semantic and depth cues improving the accuracy of the final results. The proposed model is trained and evaluated on NYUDepth V2 dataset outperforming the state of the art methods on semantic segmentation and achieving comparable results on the task of depth estimation.



