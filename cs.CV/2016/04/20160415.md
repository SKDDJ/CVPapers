# Arxiv Papers in cs.CV on 2016-04-15
### Improving the Robustness of Deep Neural Networks via Stability Training
- **Arxiv ID**: http://arxiv.org/abs/1604.04326v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1604.04326v1)
- **Published**: 2016-04-15 01:15:18+00:00
- **Updated**: 2016-04-15 01:15:18+00:00
- **Authors**: Stephan Zheng, Yang Song, Thomas Leung, Ian Goodfellow
- **Comment**: Published in CVPR 2016
- **Journal**: None
- **Summary**: In this paper we address the issue of output instability of deep neural networks: small perturbations in the visual input can significantly distort the feature embeddings and output of a neural network. Such instability affects many deep architectures with state-of-the-art performance on a wide range of computer vision tasks. We present a general stability training method to stabilize deep networks against small input distortions that result from various types of common image processing, such as compression, rescaling, and cropping. We validate our method by stabilizing the state-of-the-art Inception architecture against these types of distortions. In addition, we demonstrate that our stabilized model gives robust state-of-the-art performance on large-scale near-duplicate detection, similar-image ranking, and classification on noisy datasets.



### Invariant feature extraction from event based stimuli
- **Arxiv ID**: http://arxiv.org/abs/1604.04327v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04327v3)
- **Published**: 2016-04-15 01:18:29+00:00
- **Updated**: 2016-06-21 04:38:31+00:00
- **Authors**: Thusitha N. Chandrapala, Bertram E. Shi
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: We propose a novel architecture, the event-based GASSOM for learning and extracting invariant representations from event streams originating from neuromorphic vision sensors. The framework is inspired by feed-forward cortical models for visual processing. The model, which is based on the concepts of sparsity and temporal slowness, is able to learn feature extractors that resemble neurons in the primary visual cortex. Layers of units in the proposed model can be cascaded to learn feature extractors with different levels of complexity and selectivity. We explore the applicability of the framework on real world tasks by using the learned network for object recognition. The proposed model achieve higher classification accuracy compared to other state-of-the-art event based processing methods. Our results also demonstrate the generality and robustness of the method, as the recognizers for different data sets and different tasks all used the same set of learned feature detectors, which were trained on data collected independently of the testing data.



### Latent Model Ensemble with Auto-localization
- **Arxiv ID**: http://arxiv.org/abs/1604.04333v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04333v2)
- **Published**: 2016-04-15 02:07:42+00:00
- **Updated**: 2016-10-11 01:57:19+00:00
- **Authors**: Miao Sun, Tony X. Han, Xun Xu, Ming-Chang Liu, Ahmad Khodayari-Rostamabad
- **Comment**: International Conference on Pattern Recognition (ICPR) 2016
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (CNN) have exhibited superior performance in many visual recognition tasks including image classification, object detection, and scene label- ing, due to their large learning capacity and resistance to overfit. For the image classification task, most of the current deep CNN- based approaches take the whole size-normalized image as input and have achieved quite promising results. Compared with the previously dominating approaches based on feature extraction, pooling, and classification, the deep CNN-based approaches mainly rely on the learning capability of deep CNN to achieve superior results: the burden of minimizing intra-class variation while maximizing inter-class difference is entirely dependent on the implicit feature learning component of deep CNN; we rely upon the implicitly learned filters and pooling component to select the discriminative regions, which correspond to the activated neurons. However, if the irrelevant regions constitute a large portion of the image of interest, the classification performance of the deep CNN, which takes the whole image as input, can be heavily affected. To solve this issue, we propose a novel latent CNN framework, which treats the most discriminate region as a latent variable. We can jointly learn the global CNN with the latent CNN to avoid the aforementioned big irrelevant region issue, and our experimental results show the evident advantage of the proposed latent CNN over traditional deep CNN: latent CNN outperforms the state-of-the-art performance of deep CNN on standard benchmark datasets including the CIFAR-10, CIFAR- 100, MNIST and PASCAL VOC 2007 Classification dataset.



### Recognition of facial expressions based on salient geometric features and support vector machines
- **Arxiv ID**: http://arxiv.org/abs/1604.04334v1
- **DOI**: 10.1007/s11042-016-3428-9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04334v1)
- **Published**: 2016-04-15 02:18:11+00:00
- **Updated**: 2016-04-15 02:18:11+00:00
- **Authors**: Deepak Ghimire, Joonwhoan Lee, Ze-Nian Li, Sunghwan Jeong
- **Comment**: Facial points, Geometric features, AdaBoost, Extreme learning
  machine, Support vector machines, Facial expression recognitions
- **Journal**: Multimedia Tools and Applications (2016): 1-26
- **Summary**: Facial expressions convey nonverbal cues which play an important role in interpersonal relations, and are widely used in behavior interpretation of emotions, cognitive science, and social interactions. In this paper we analyze different ways of representing geometric feature and present a fully automatic facial expression recognition (FER) system using salient geometric features. In geometric feature-based FER approach, the first important step is to initialize and track dense set of facial points as the expression evolves over time in consecutive frames. In the proposed system, facial points are initialized using elastic bunch graph matching (EBGM) algorithm and tracking is performed using Kanade-Lucas-Tomaci (KLT) tracker. We extract geometric features from point, line and triangle composed of tracking results of facial points. The most discriminative line and triangle features are extracted using feature selective multi-class AdaBoost with the help of extreme learning machine (ELM) classification. Finally the geometric features for FER are extracted from the boosted line, and triangles composed of facial points. The recognition accuracy using features from point, line and triangle are analyzed independently. The performance of the proposed FER system is evaluated on three different data sets: namely CK+, MMI and MUG facial expression data sets.



### Facial expression recognition based on local region specific features and support vector machines
- **Arxiv ID**: http://arxiv.org/abs/1604.04337v1
- **DOI**: 10.1007/s11042-016-3418-y
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04337v1)
- **Published**: 2016-04-15 02:27:21+00:00
- **Updated**: 2016-04-15 02:27:21+00:00
- **Authors**: Deepak Ghimire, Sunghwan Jeong, Joonwhoan Lee, Sang Hyun Park
- **Comment**: Facial expressions, Local representation, Appearance features,
  Geometric features, Support vector machines
- **Journal**: Multimedia Tools and Applications, pp 1-19, Online: 16 March 2016
- **Summary**: Facial expressions are one of the most powerful, natural and immediate means for human being to communicate their emotions and intensions. Recognition of facial expression has many applications including human-computer interaction, cognitive science, human emotion analysis, personality development etc. In this paper, we propose a new method for the recognition of facial expressions from single image frame that uses combination of appearance and geometric features with support vector machines classification. In general, appearance features for the recognition of facial expressions are computed by dividing face region into regular grid (holistic representation). But, in this paper we extracted region specific appearance features by dividing the whole face region into domain specific local regions. Geometric features are also extracted from corresponding domain specific regions. In addition, important local regions are determined by using incremental search approach which results in the reduction of feature dimension and improvement in recognition accuracy. The results of facial expressions recognition using features from domain specific regions are also compared with the results obtained using holistic representation. The performance of the proposed facial expression recognition system has been validated on publicly available extended Cohn-Kanade (CK+) facial expression data sets.



### High-performance Semantic Segmentation Using Very Deep Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1604.04339v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04339v1)
- **Published**: 2016-04-15 02:52:46+00:00
- **Updated**: 2016-04-15 02:52:46+00:00
- **Authors**: Zifeng Wu, Chunhua Shen, Anton van den Hengel
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: We propose a method for high-performance semantic image segmentation (or semantic pixel labelling) based on very deep residual networks, which achieves the state-of-the-art performance. A few design factors are carefully considered to this end.   We make the following contributions. (i) First, we evaluate different variations of a fully convolutional residual network so as to find the best configuration, including the number of layers, the resolution of feature maps, and the size of field-of-view. Our experiments show that further enlarging the field-of-view and increasing the resolution of feature maps are typically beneficial, which however inevitably leads to a higher demand for GPU memories. To walk around the limitation, we propose a new method to simulate a high resolution network with a low resolution network, which can be applied during training and/or testing. (ii) Second, we propose an online bootstrapping method for training. We demonstrate that online bootstrapping is critically important for achieving good accuracy. (iii) Third we apply the traditional dropout to some of the residual blocks, which further improves the performance. (iv) Finally, our method achieves the currently best mean intersection-over-union 78.3\% on the PASCAL VOC 2012 dataset, as well as on the recent dataset Cityscapes.



### The Chow Form of the Essential Variety in Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1604.04372v2
- **DOI**: None
- **Categories**: **math.AG**, cs.CV, math.AC, 14M12, 14C05, 14Q15, 13D02, 13C14, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1604.04372v2)
- **Published**: 2016-04-15 06:57:09+00:00
- **Updated**: 2016-07-14 18:09:12+00:00
- **Authors**: Gunnar Fløystad, Joe Kileel, Giorgio Ottaviani
- **Comment**: 27 pages, 1 figure, 6 Macaulay2 ancillary files. v2: edits to Theorem
  1.1, references, acknowledgements
- **Journal**: None
- **Summary**: The Chow form of the essential variety in computer vision is calculated. Our derivation uses secant varieties, Ulrich sheaves and representation theory. Numerical experiments show that our formula can detect noisy point correspondences between two images.



### DARI: Distance metric And Representation Integration for Person Verification
- **Arxiv ID**: http://arxiv.org/abs/1604.04377v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04377v1)
- **Published**: 2016-04-15 07:21:26+00:00
- **Updated**: 2016-04-15 07:21:26+00:00
- **Authors**: Guangrun Wang, Liang Lin, Shengyong Ding, Ya Li, Qing Wang
- **Comment**: To appear in Proceedings of AAAI Conference on Artificial
  Intelligence (AAAI), 2016
- **Journal**: None
- **Summary**: The past decade has witnessed the rapid development of feature representation learning and distance metric learning, whereas the two steps are often discussed separately. To explore their interaction, this work proposes an end-to-end learning framework called DARI, i.e. Distance metric And Representation Integration, and validates the effectiveness of DARI in the challenging task of person verification. Given the training images annotated with the labels, we first produce a large number of triplet units, and each one contains three images, i.e. one person and the matched/mismatch references. For each triplet unit, the distance disparity between the matched pair and the mismatched pair tends to be maximized. We solve this objective by building a deep architecture of convolutional neural networks. In particular, the Mahalanobis distance matrix is naturally factorized as one top fully-connected layer that is seamlessly integrated with other bottom layers representing the image feature. The image feature and the distance metric can be thus simultaneously optimized via the one-shot backward propagation. On several public datasets, DARI shows very promising performance on re-identifying individuals cross cameras against various challenges, and outperforms other state-of-the-art approaches.



### Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1604.04382v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04382v1)
- **Published**: 2016-04-15 07:32:18+00:00
- **Updated**: 2016-04-15 07:32:18+00:00
- **Authors**: Chuan Li, Michael Wand
- **Comment**: 17 pages, 15 figures
- **Journal**: None
- **Summary**: This paper proposes Markovian Generative Adversarial Networks (MGANs), a method for training generative neural networks for efficient texture synthesis. While deep neural network approaches have recently demonstrated remarkable results in terms of synthesis quality, they still come at considerable computational costs (minutes of run-time for low-res images). Our paper addresses this efficiency issue. Instead of a numerical deconvolution in previous work, we precompute a feed-forward, strided convolutional network that captures the feature statistics of Markovian patches and is able to directly generate outputs of arbitrary dimensions. Such network can directly decode brown noise to realistic texture, or photos to artistic paintings. With adversarial training, we obtain quality comparable to recent neural texture synthesis methods. As no optimization is required any longer at generation time, our run-time performance (0.25M pixel images at 25Hz) surpasses previous neural texture synthesizers by a significant margin (at least 500 times faster). We apply this idea to texture synthesis, style transfer, and video stylization.



### Unsupervised Image Segmentation using the Deffuant-Weisbuch Model from Social Dynamics
- **Arxiv ID**: http://arxiv.org/abs/1604.04393v3
- **DOI**: 10.1007/s11760-017-1100-0
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04393v3)
- **Published**: 2016-04-15 08:01:15+00:00
- **Updated**: 2016-06-02 21:01:42+00:00
- **Authors**: Subhradeep Kayal
- **Comment**: This paper is under consideration at Signal Image and Video
  Processing journal
- **Journal**: None
- **Summary**: Unsupervised image segmentation algorithms aim at identifying disjoint homogeneous regions in an image, and have been subject to considerable attention in the machine vision community. In this paper, a popular theoretical model with it's origins in statistical physics and social dynamics, known as the Deffuant-Weisbuch model, is applied to the image segmentation problem. The Deffuant-Weisbuch model has been found to be useful in modelling the evolution of a closed system of interacting agents characterised by their opinions or beliefs, leading to the formation of clusters of agents who share a similar opinion or belief at steady state. In the context of image segmentation, this paper considers a pixel as an agent and it's colour property as it's opinion, with opinion updates as per the Deffuant-Weisbuch model. Apart from applying the basic model to image segmentation, this paper incorporates adjacency and neighbourhood information in the model, which factors in the local similarity and smoothness properties of images. Convergence is reached when the number of unique pixel opinions, i.e., the number of colour centres, matches the pre-specified number of clusters. Experiments are performed on a set of images from the Berkeley Image Segmentation Dataset and the results are analysed both qualitatively and quantitatively, which indicate that this simple and intuitive method is promising for image segmentation. To the best of the knowledge of the author, this is the first work where a theoretical model from statistical physics and social dynamics has been successfully applied to image processing.



### Low-Rank Matrix Recovery using Gabidulin Codes in Characteristic Zero
- **Arxiv ID**: http://arxiv.org/abs/1604.04397v2
- **DOI**: None
- **Categories**: **cs.IT**, cs.CV, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1604.04397v2)
- **Published**: 2016-04-15 08:19:49+00:00
- **Updated**: 2016-09-15 07:30:30+00:00
- **Authors**: Sven Müelich, Sven Puchinger, Martin Bossert
- **Comment**: 6 pages, presented at the International Workshop on Algebraic and
  Combinatorial Coding Theory (ACCT) 2016, submitted to Electronic Notes in
  Discrete Mathematics (volume devoted to ACCT 2016)
- **Journal**: None
- **Summary**: We present a new approach on low-rank matrix recovery (LRMR) based on Gabidulin Codes. Since most applications of LRMR deal with matrices over infinite fields, we use the recently introduced generalization of Gabidulin codes to fields of characterstic zero. We show that LRMR can be reduced to decoding of Gabidulin codes and discuss which field extensions can be used in the code construction.



### Probing the Intra-Component Correlations within Fisher Vector for Material Classification
- **Arxiv ID**: http://arxiv.org/abs/1604.04473v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04473v1)
- **Published**: 2016-04-15 12:55:00+00:00
- **Updated**: 2016-04-15 12:55:00+00:00
- **Authors**: Xiaopeng Hong, Xianbiao Qi, Guoying Zhao, Matti Pietikäinen
- **Comment**: It is manuscript submitted to Neurocomputing on the end of April,
  2015 (!). One year past but no review comments we received yet!
- **Journal**: None
- **Summary**: Fisher vector (FV) has become a popular image representation. One notable underlying assumption of the FV framework is that local descriptors are well decorrelated within each cluster so that the covariance matrix for each Gaussian can be simplified to be diagonal. Though the FV usually relies on the Principal Component Analysis (PCA) to decorrelate local features, the PCA is applied to the entire training data and hence it only diagonalizes the \textit{universal} covariance matrix, rather than those w.r.t. the local components. As a result, the local decorrelation assumption is usually not supported in practice.   To relax this assumption, this paper proposes a completed model of the Fisher vector, which is termed as the Completed Fisher vector (CFV). The CFV is a more general framework of the FV, since it encodes not only the variances but also the correlations of the whitened local descriptors. The CFV thus leads to improved discriminative power. We take the task of material categorization as an example and experimentally show that: 1) the CFV outperforms the FV under all parameter settings; 2) the CFV is robust to the changes in the number of components in the mixture; 3) even with a relatively small visual vocabulary the CFV still works well on two challenging datasets.



### Long-term Temporal Convolutions for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1604.04494v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04494v2)
- **Published**: 2016-04-15 13:33:13+00:00
- **Updated**: 2017-06-02 12:08:57+00:00
- **Authors**: Gül Varol, Ivan Laptev, Cordelia Schmid
- **Comment**: None
- **Journal**: None
- **Summary**: Typical human actions last several seconds and exhibit characteristic spatio-temporal structure. Recent methods attempt to capture this structure and learn action representations with convolutional neural networks. Such representations, however, are typically learned at the level of a few video frames failing to model actions at their full temporal extent. In this work we learn video representations using neural networks with long-term temporal convolutions (LTC). We demonstrate that LTC-CNN models with increased temporal extents improve the accuracy of action recognition. We also study the impact of different low-level representations, such as raw values of video pixels and optical flow vector fields and demonstrate the importance of high-quality optical flow estimation for learning accurate action models. We report state-of-the-art results on two challenging benchmarks for human action recognition UCF101 (92.7%) and HMDB51 (67.2%).



### Tracking Human-like Natural Motion Using Deep Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1604.04528v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1604.04528v1)
- **Published**: 2016-04-15 14:55:27+00:00
- **Updated**: 2016-04-15 14:55:27+00:00
- **Authors**: Youngbin Park, Sungphill Moon, Il Hong Suh
- **Comment**: submitted to ECCV 2016
- **Journal**: None
- **Summary**: Kinect skeleton tracker is able to achieve considerable human body tracking performance in convenient and a low-cost manner. However, The tracker often captures unnatural human poses such as discontinuous and vibrated motions when self-occlusions occur. A majority of approaches tackle this problem by using multiple Kinect sensors in a workspace. Combination of the measurements from different sensors is then conducted in Kalman filter framework or optimization problem is formulated for sensor fusion. However, these methods usually require heuristics to measure reliability of measurements observed from each Kinect sensor. In this paper, we developed a method to improve Kinect skeleton using single Kinect sensor, in which supervised learning technique was employed to correct unnatural tracking motions. Specifically, deep recurrent neural networks were used for improving joint positions and velocities of Kinect skeleton, and three methods were proposed to integrate the refined positions and velocities for further enhancement. Moreover, we suggested a novel measure to evaluate naturalness of captured motions. We evaluated the proposed approach by comparison with the ground truth obtained using a commercial optical maker-based motion capture system.



### Unsupervised single-particle deep clustering via statistical manifold learning
- **Arxiv ID**: http://arxiv.org/abs/1604.04539v2
- **DOI**: 10.1371/journal.pone.0182130
- **Categories**: **physics.data-an**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1604.04539v2)
- **Published**: 2016-04-15 15:28:39+00:00
- **Updated**: 2016-12-31 07:43:07+00:00
- **Authors**: Jiayi Wu, Yong-Bei Ma, Charles Congdon, Bevin Brett, Shuobing Chen, Qi Ouyang, Youdong Mao
- **Comment**: 29 pages, 5 figures
- **Journal**: PLoS ONE 12, e0182130 (2017)
- **Summary**: Motivation: Structural heterogeneity in single-particle cryo-electron microscopy (cryo-EM) data represents a major challenge for high-resolution structure determination. Unsupervised classification may serve as the first step in the assessment of structural heterogeneity. Traditional algorithms for unsupervised classification, such as K-means clustering and maximum likelihood optimization, may classify images into wrong classes with decreasing signal-to-noise-ratio (SNR) in the image data, yet demand increased cost in computation. Overcoming these limitations requires further development on clustering algorithms for high-performance cryo-EM data analysis. Results: Here we introduce a statistical manifold learning algorithm for unsupervised single-particle deep clustering. We show that statistical manifold learning improves classification accuracy by about 40% in the absence of input references for lower SNR data. Applications to several experimental datasets suggest that our deep clustering approach can detect subtle structural difference among classes. Through code optimization over the Intel high-performance computing (HPC) processors, our software implementation can generate thousands of reference-free class averages within several hours from hundreds of thousands of single-particle cryo-EM images, which allows significant improvement in ab initio 3D reconstruction resolution and quality. Our approach has been successfully applied in several structural determination projects. We expect that it provides a powerful computational tool in analyzing highly heterogeneous structural data and assisting in computational purification of single-particle datasets for high-resolution reconstruction.



### CNN-RNN: A Unified Framework for Multi-label Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1604.04573v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1604.04573v1)
- **Published**: 2016-04-15 17:10:54+00:00
- **Updated**: 2016-04-15 17:10:54+00:00
- **Authors**: Jiang Wang, Yi Yang, Junhua Mao, Zhiheng Huang, Chang Huang, Wei Xu
- **Comment**: CVPR 2016
- **Journal**: None
- **Summary**: While deep convolutional neural networks (CNNs) have shown a great success in single-label image classification, it is important to note that real world images generally contain multiple labels, which could correspond to different objects, scenes, actions and attributes in an image. Traditional approaches to multi-label image classification learn independent classifiers for each category and employ ranking or thresholding on the classification results. These techniques, although working well, fail to explicitly exploit the label dependencies in an image. In this paper, we utilize recurrent neural networks (RNNs) to address this problem. Combined with CNNs, the proposed CNN-RNN framework learns a joint image-label embedding to characterize the semantic label dependency as well as the image-label relevance, and it can be trained end-to-end from scratch to integrate both information in a unified framework. Experimental results on public benchmark datasets demonstrate that the proposed architecture achieves better performance than the state-of-the-art multi-label classification model



### Learning Temporal Regularity in Video Sequences
- **Arxiv ID**: http://arxiv.org/abs/1604.04574v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04574v1)
- **Published**: 2016-04-15 17:20:01+00:00
- **Updated**: 2016-04-15 17:20:01+00:00
- **Authors**: Mahmudul Hasan, Jonghyun Choi, Jan Neumann, Amit K. Roy-Chowdhury, Larry S. Davis
- **Comment**: CVPR 2016
- **Journal**: None
- **Summary**: Perceiving meaningful activities in a long video sequence is a challenging problem due to ambiguous definition of 'meaningfulness' as well as clutters in the scene. We approach this problem by learning a generative model for regular motion patterns, termed as regularity, using multiple sources with very limited supervision. Specifically, we propose two methods that are built upon the autoencoders for their ability to work with little to no supervision. We first leverage the conventional handcrafted spatio-temporal local features and learn a fully connected autoencoder on them. Second, we build a fully convolutional feed-forward autoencoder to learn both the local features and the classifiers as an end-to-end learning framework. Our model can capture the regularities from multiple datasets. We evaluate our methods in both qualitative and quantitative ways - showing the learned regularity of videos in various aspects and demonstrating competitive performance on anomaly detection datasets as an application.



### Non-contact hemodynamic imaging reveals the jugular venous pulse waveform
- **Arxiv ID**: http://arxiv.org/abs/1604.05213v2
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1604.05213v2)
- **Published**: 2016-04-15 19:21:35+00:00
- **Updated**: 2016-04-21 14:50:01+00:00
- **Authors**: Robert Amelard, Richard L Hughson, Danielle K Greaves, Kaylen J Pfisterer, Jason Leung, David A Clausi, Alexander Wong
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: Cardiovascular monitoring is important to prevent diseases from progressing. The jugular venous pulse (JVP) waveform offers important clinical information about cardiac health, but is not routinely examined due to its invasive catheterisation procedure. Here, we demonstrate for the first time that the JVP can be consistently observed in a non-contact manner using a novel light-based photoplethysmographic imaging system, coded hemodynamic imaging (CHI). While traditional monitoring methods measure the JVP at a single location, CHI's wide-field imaging capabilities were able to observe the jugular venous pulse's spatial flow profile for the first time. The important inflection points in the JVP were observed, meaning that cardiac abnormalities can be assessed through JVP distortions. CHI provides a new way to assess cardiac health through non-contact light-based JVP monitoring, and can be used in non-surgical environments for cardiac assessment.



### Bags of Local Convolutional Features for Scalable Instance Search
- **Arxiv ID**: http://arxiv.org/abs/1604.04653v1
- **DOI**: 10.1145/2911996.2912061
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1604.04653v1)
- **Published**: 2016-04-15 22:02:22+00:00
- **Updated**: 2016-04-15 22:02:22+00:00
- **Authors**: Eva Mohedano, Amaia Salvador, Kevin McGuinness, Ferran Marques, Noel E. O'Connor, Xavier Giro-i-Nieto
- **Comment**: Preprint of a short paper accepted in the ACM International
  Conference on Multimedia Retrieval (ICMR) 2016 (New York City, NY, USA)
- **Journal**: None
- **Summary**: This work proposes a simple instance retrieval pipeline based on encoding the convolutional features of CNN using the bag of words aggregation scheme (BoW). Assigning each local array of activations in a convolutional layer to a visual word produces an \textit{assignment map}, a compact representation that relates regions of an image with a visual word. We use the assignment map for fast spatial reranking, obtaining object localizations that are used for query expansion. We demonstrate the suitability of the BoW representation based on local CNN features for instance retrieval, achieving competitive performance on the Oxford and Paris buildings benchmarks. We show that our proposed system for CNN feature aggregation with BoW outperforms state-of-the-art techniques using sum pooling at a subset of the challenging TRECVid INS benchmark.



