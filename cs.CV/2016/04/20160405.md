# Arxiv Papers in cs.CV on 2016-04-05
### BundleFusion: Real-time Globally Consistent 3D Reconstruction using On-the-fly Surface Re-integration
- **Arxiv ID**: http://arxiv.org/abs/1604.01093v3
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1604.01093v3)
- **Published**: 2016-04-05 00:06:39+00:00
- **Updated**: 2017-02-07 19:00:46+00:00
- **Authors**: Angela Dai, Matthias Nießner, Michael Zollhöfer, Shahram Izadi, Christian Theobalt
- **Comment**: None
- **Journal**: None
- **Summary**: Real-time, high-quality, 3D scanning of large-scale scenes is key to mixed reality and robotic applications. However, scalability brings challenges of drift in pose estimation, introducing significant errors in the accumulated model. Approaches often require hours of offline processing to globally correct model errors. Recent online methods demonstrate compelling results, but suffer from: (1) needing minutes to perform online correction preventing true real-time use; (2) brittle frame-to-frame (or frame-to-model) pose estimation resulting in many tracking failures; or (3) supporting only unstructured point-based representations, which limit scan quality and applicability. We systematically address these issues with a novel, real-time, end-to-end reconstruction framework. At its core is a robust pose estimation strategy, optimizing per frame for a global set of camera poses by considering the complete history of RGB-D input with an efficient hierarchical approach. We remove the heavy reliance on temporal tracking, and continually localize to the globally optimized frames instead. We contribute a parallelizable optimization framework, which employs correspondences based on sparse features and dense geometric and photometric matching. Our approach estimates globally optimized (i.e., bundle adjusted) poses in real-time, supports robust tracking with recovery from gross tracking failures (i.e., relocalization), and re-estimates the 3D model in real-time to ensure global consistency; all within a single framework. Our approach outperforms state-of-the-art online systems with quality on par to offline methods, but with unprecedented speed and scan completeness. Our framework leads to a comprehensive online scanning solution for large indoor environments, enabling ease of use and high-quality results.



### Counting Grid Aggregation for Event Retrieval and Recognition
- **Arxiv ID**: http://arxiv.org/abs/1604.01109v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01109v3)
- **Published**: 2016-04-05 01:38:07+00:00
- **Updated**: 2016-10-11 12:11:47+00:00
- **Authors**: Zhanning Gao, Gang Hua, Dongqing Zhang, Jianru Xue, Nanning Zheng
- **Comment**: This paper has been withdrawn by the author because this work will be
  part of another object which will be released soon
- **Journal**: None
- **Summary**: Event retrieval and recognition in a large corpus of videos necessitates a holistic fixed-size visual representation at the video clip level that is comprehensive, compact, and yet discriminative. It shall comprehensively aggregate information across relevant video frames, while suppress redundant information, leading to a compact representation that can effectively differentiate among different visual events. In search for such a representation, we propose to build a spatially consistent counting grid model to aggregate together deep features extracted from different video frames. The spatial consistency of the counting grid model is achieved by introducing a prior model estimated from a large corpus of video data. The counting grid model produces an intermediate tensor representation for each video, which automatically identifies and removes the feature redundancy across the different frames. The tensor representation is subsequently reduced to a fixed-size vector representation by averaging over the counting grid. When compared to existing methods on both event retrieval and event classification benchmarks, we achieve significantly better accuracy with much more compact representation.



### Less is more: zero-shot learning from online textual documents with noise suppression
- **Arxiv ID**: http://arxiv.org/abs/1604.01146v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01146v1)
- **Published**: 2016-04-05 06:13:06+00:00
- **Updated**: 2016-04-05 06:13:06+00:00
- **Authors**: Ruizhi Qiao, Lingqiao Liu, Chunhua Shen, Anton van den Hengel
- **Comment**: Accepted to Int. Conf. Computer Vision and Pattern Recognition (CVPR)
  2016
- **Journal**: None
- **Summary**: Classifying a visual concept merely from its associated online textual source, such as a Wikipedia article, is an attractive research topic in zero-shot learning because it alleviates the burden of manually collecting semantic attributes. Several recent works have pursued this approach by exploring various ways of connecting the visual and text domains. This paper revisits this idea by stepping further to consider one important factor: the textual representation is usually too noisy for the zero-shot learning application. This consideration motivates us to design a simple-but-effective zero-shot learning method capable of suppressing noise in the text.   More specifically, we propose an $l_{2,1}$-norm based objective function which can simultaneously suppress the noisy signal in the text and learn a function to match the text document and visual features. We also develop an optimization algorithm to efficiently solve the resulting problem. By conducting experiments on two large datasets, we demonstrate that the proposed method significantly outperforms the competing methods which rely on online information sources but without explicit noise suppression. We further make an in-depth analysis of the proposed method and provide insight as to what kind of information in documents is useful for zero-shot learning.



### Comparative Deep Learning of Hybrid Representations for Image Recommendations
- **Arxiv ID**: http://arxiv.org/abs/1604.01252v1
- **DOI**: 10.1109/CVPR.2016.279
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01252v1)
- **Published**: 2016-04-05 13:34:28+00:00
- **Updated**: 2016-04-05 13:34:28+00:00
- **Authors**: Chenyi Lei, Dong Liu, Weiping Li, Zheng-Jun Zha, Houqiang Li
- **Comment**: CVPR 2016
- **Journal**: None
- **Summary**: In many image-related tasks, learning expressive and discriminative representations of images is essential, and deep learning has been studied for automating the learning of such representations. Some user-centric tasks, such as image recommendations, call for effective representations of not only images but also preferences and intents of users over images. Such representations are termed \emph{hybrid} and addressed via a deep learning approach in this paper. We design a dual-net deep network, in which the two sub-networks map input images and preferences of users into a same latent semantic space, and then the distances between images and users in the latent space are calculated to make decisions. We further propose a comparative deep learning (CDL) method to train the deep network, using a pair of images compared against one user to learn the pattern of their relative distances. The CDL embraces much more training data than naive deep learning, and thus achieves superior performance than the latter, with no cost of increasing network complexity. Experimental results with real-world data sets for image recommendations have shown the proposed dual-net network and CDL greatly outperform other state-of-the-art image recommendation solutions.



### Cohomology of Cryo-Electron Microscopy
- **Arxiv ID**: http://arxiv.org/abs/1604.01319v2
- **DOI**: None
- **Categories**: **cs.CV**, math.AT, 92E10, 46M20, 94A08, 68U10, 44A12, 55R35
- **Links**: [PDF](http://arxiv.org/pdf/1604.01319v2)
- **Published**: 2016-04-05 16:26:47+00:00
- **Updated**: 2017-04-22 10:08:39+00:00
- **Authors**: Ke Ye, Lek-Heng Lim
- **Comment**: 27 pages, 5 figures
- **Journal**: None
- **Summary**: The goal of cryo-electron microscopy (EM) is to reconstruct the 3-dimensional structure of a molecule from a collection of its 2-dimensional projected images. In this article, we show that the basic premise of cryo-EM --- patching together 2-dimensional projections to reconstruct a 3-dimensional object --- is naturally one of Cech cohomology with SO(2)-coefficients. We deduce that every cryo-EM reconstruction problem corresponds to an oriented circle bundle on a simplicial complex, allowing us to classify cryo-EM problems via principal bundles. In practice, the 2-dimensional images are noisy and a main task in cryo-EM is to denoise them. We will see how the aforementioned insights can be used towards this end.



### Deep Image Retrieval: Learning global representations for image search
- **Arxiv ID**: http://arxiv.org/abs/1604.01325v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01325v2)
- **Published**: 2016-04-05 16:48:17+00:00
- **Updated**: 2016-07-28 10:44:17+00:00
- **Authors**: Albert Gordo, Jon Almazan, Jerome Revaud, Diane Larlus
- **Comment**: ECCV 2016 version + additional results
- **Journal**: None
- **Summary**: We propose a novel approach for instance-level image retrieval. It produces a global and compact fixed-length representation for each image by aggregating many region-wise descriptors. In contrast to previous works employing pre-trained deep networks as a black box to produce features, our method leverages a deep architecture trained for the specific task of image retrieval. Our contribution is twofold: (i) we leverage a ranking framework to learn convolution and projection weights that are used to build the region features; and (ii) we employ a region proposal network to learn which regions should be pooled to form the final global descriptor. We show that using clean training data is key to the success of our approach. To that aim, we use a large scale but noisy landmark dataset and develop an automatic cleaning approach. The proposed architecture produces a global image representation in a single forward pass. Our approach significantly outperforms previous approaches based on global descriptors on standard datasets. It even surpasses most prior works based on costly local descriptor indexing and spatial verification. Additional material is available at www.xrce.xerox.com/Deep-Image-Retrieval.



### Deep Cross Residual Learning for Multitask Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1604.01335v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM, I.2.6; I.5.1; I.5.4; H.5.1
- **Links**: [PDF](http://arxiv.org/pdf/1604.01335v2)
- **Published**: 2016-04-05 17:08:14+00:00
- **Updated**: 2016-07-20 01:55:12+00:00
- **Authors**: Brendan Jou, Shih-Fu Chang
- **Comment**: 10 pages, 6 figures, To appear in ACM Multimedia
- **Journal**: None
- **Summary**: Residual learning has recently surfaced as an effective means of constructing very deep neural networks for object recognition. However, current incarnations of residual networks do not allow for the modeling and integration of complex relations between closely coupled recognition tasks or across domains. Such problems are often encountered in multimedia applications involving large-scale content recognition. We propose a novel extension of residual learning for deep networks that enables intuitive learning across multiple related tasks using cross-connections called cross-residuals. These cross-residuals connections can be viewed as a form of in-network regularization and enables greater network generalization. We show how cross-residual learning (CRL) can be integrated in multitask networks to jointly train and detect visual concepts across several tasks. We present a single multitask cross-residual network with >40% less parameters that is able to achieve competitive, or even better, detection performance on a visual sentiment concept detection problem normally requiring multiple specialized single-task networks. The resulting multitask cross-residual network also achieves better detection performance by about 10.4% over a standard multitask residual network without cross-residuals with even a small amount of cross-task weighting.



### Integrating Local Material Recognition with Large-Scale Perceptual Attribute Discovery
- **Arxiv ID**: http://arxiv.org/abs/1604.01345v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01345v4)
- **Published**: 2016-04-05 17:40:57+00:00
- **Updated**: 2017-04-12 04:03:39+00:00
- **Authors**: Gabriel Schwartz, Ko Nishino
- **Comment**: None
- **Journal**: None
- **Summary**: Material attributes have been shown to provide a discriminative intermediate representation for recognizing materials, especially for the challenging task of recognition from local material appearance (i.e., regardless of object and scene context). In the past, however, material attributes have been recognized separately preceding category recognition. In contrast, neuroscience studies on material perception and computer vision research on object and place recognition have shown that attributes are produced as a by-product during the category recognition process. Does the same hold true for material attribute and category recognition? In this paper, we introduce a novel material category recognition network architecture to show that perceptual attributes can, in fact, be automatically discovered inside a local material recognition framework. The novel material-attribute-category convolutional neural network (MAC-CNN) produces perceptual material attributes from the intermediate pooling layers of an end-to-end trained category recognition network using an auxiliary loss function that encodes human material perception. To train this model, we introduce a novel large-scale database of local material appearance organized under a canonical material category taxonomy and careful image patch extraction that avoids unwanted object and scene context. We show that the discovered attributes correspond well with semantically-meaningful visual material traits via Boolean algebra, and enable recognition of previously unseen material categories given only a few examples. These results have strong implications in how perceptually meaningful attributes can be learned in other recognition tasks.



### Marr Revisited: 2D-3D Alignment via Surface Normal Prediction
- **Arxiv ID**: http://arxiv.org/abs/1604.01347v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01347v1)
- **Published**: 2016-04-05 17:51:39+00:00
- **Updated**: 2016-04-05 17:51:39+00:00
- **Authors**: Aayush Bansal, Bryan Russell, Abhinav Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce an approach that leverages surface normal predictions, along with appearance cues, to retrieve 3D models for objects depicted in 2D still images from a large CAD object library. Critical to the success of our approach is the ability to recover accurate surface normals for objects in the depicted scene. We introduce a skip-network model built on the pre-trained Oxford VGG convolutional neural network (CNN) for surface normal prediction. Our model achieves state-of-the-art accuracy on the NYUv2 RGB-D dataset for surface normal prediction, and recovers fine object detail compared to previous methods. Furthermore, we develop a two-stream network over the input image and predicted surface normals that jointly learns pose and style for CAD model retrieval. When using the predicted surface normals, our two-stream network matches prior work using surface normals computed from RGB-D images on the task of pose prediction, and achieves state of the art when using RGB-D input. Finally, our two-stream network allows us to retrieve CAD models that better match the style and pose of a depicted object compared with baseline approaches.



### Radiometric Scene Decomposition: Scene Reflectance, Illumination, and Geometry from RGB-D Images
- **Arxiv ID**: http://arxiv.org/abs/1604.01354v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01354v1)
- **Published**: 2016-04-05 18:18:41+00:00
- **Updated**: 2016-04-05 18:18:41+00:00
- **Authors**: Stephen Lombardi, Ko Nishino
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: Recovering the radiometric properties of a scene (i.e., the reflectance, illumination, and geometry) is a long-sought ability of computer vision that can provide invaluable information for a wide range of applications. Deciphering the radiometric ingredients from the appearance of a real-world scene, as opposed to a single isolated object, is particularly challenging as it generally consists of various objects with different material compositions exhibiting complex reflectance and light interactions that are also part of the illumination. We introduce the first method for radiometric scene decomposition that handles those intricacies. We use RGB-D images to bootstrap geometry recovery and simultaneously recover the complex reflectance and natural illumination while refining the noisy initial geometry and segmenting the scene into different material regions. Most important, we handle real-world scenes consisting of multiple objects of unknown materials, which necessitates the modeling of spatially-varying complex reflectance, natural illumination, texture, interreflection and shadows. We systematically evaluate the effectiveness of our method on synthetic scenes and demonstrate its application to real-world scenes. The results show that rich radiometric information can be recovered from RGB-D images and demonstrate a new role RGB-D sensors can play for general scene understanding tasks.



### The Curious Robot: Learning Visual Representations via Physical Interactions
- **Arxiv ID**: http://arxiv.org/abs/1604.01360v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1604.01360v2)
- **Published**: 2016-04-05 18:47:15+00:00
- **Updated**: 2016-07-26 03:30:44+00:00
- **Authors**: Lerrel Pinto, Dhiraj Gandhi, Yuanfeng Han, Yong-Lae Park, Abhinav Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: What is the right supervisory signal to train visual representations? Current approaches in computer vision use category labels from datasets such as ImageNet to train ConvNets. However, in case of biological agents, visual representation learning does not require millions of semantic labels. We argue that biological agents use physical interactions with the world to learn visual representations unlike current vision systems which just use passive observations (images and videos downloaded from web). For example, babies push objects, poke them, put them in their mouth and throw them to learn representations. Towards this goal, we build one of the first systems on a Baxter platform that pushes, pokes, grasps and observes objects in a tabletop environment. It uses four different types of physical interactions to collect more than 130K datapoints, with each datapoint providing supervision to a shared ConvNet architecture allowing us to learn visual representations. We show the quality of learned representations by observing neuron activations and performing nearest neighbor retrieval on this learned representation. Quantitatively, we evaluate our learned ConvNet on image classification tasks and show improvements compared to learning without external data. Finally, on the task of instance retrieval, our network outperforms the ImageNet network on recall@1 by 3%



### Highly accurate gaze estimation using a consumer RGB-depth sensor
- **Arxiv ID**: http://arxiv.org/abs/1604.01420v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01420v1)
- **Published**: 2016-04-05 20:50:40+00:00
- **Updated**: 2016-04-05 20:50:40+00:00
- **Authors**: Reza Shoja Ghiass, Ognjen Arandjelovic
- **Comment**: International Joint Conference on Artificial Intelligence, 2016
- **Journal**: None
- **Summary**: Determining the direction in which a person is looking is an important problem in a wide range of HCI applications. In this paper we describe a highly accurate algorithm that performs gaze estimation using an affordable and widely available device such as Kinect. The method we propose starts by performing accurate head pose estimation achieved by fitting a person specific morphable model of the face to depth data. The ordinarily competing requirements of high accuracy and high speed are met concurrently by formulating the fitting objective function as a combination of terms which excel either in accurate or fast fitting, and then by adaptively adjusting their relative contributions throughout fitting. Following pose estimation, pose normalization is done by re-rendering the fitted model as a frontal face. Finally gaze estimates are obtained through regression from the appearance of the eyes in synthetic, normalized images. Using EYEDIAP, the standard public dataset for the evaluation of gaze estimation algorithms from RGB-D data, we demonstrate that our method greatly outperforms the state of the art.



### Forecasting Interactive Dynamics of Pedestrians with Fictitious Play
- **Arxiv ID**: http://arxiv.org/abs/1604.01431v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01431v3)
- **Published**: 2016-04-05 21:13:32+00:00
- **Updated**: 2017-03-28 16:31:01+00:00
- **Authors**: Wei-Chiu Ma, De-An Huang, Namhoon Lee, Kris M. Kitani
- **Comment**: Accepted to CVPR 2017
- **Journal**: None
- **Summary**: We develop predictive models of pedestrian dynamics by encoding the coupled nature of multi-pedestrian interaction using game theory, and deep learning-based visual analysis to estimate person-specific behavior parameters. Building predictive models for multi-pedestrian interactions however, is very challenging due to two reasons: (1) the dynamics of interaction are complex interdependent processes, where the predicted behavior of one pedestrian can affect the actions taken by others and (2) dynamics are variable depending on an individuals physical characteristics (e.g., an older person may walk slowly while the younger person may walk faster). To address these challenges, we (1) utilize concepts from game theory to model the interdependent decision making process of multiple pedestrians and (2) use visual classifiers to learn a mapping from pedestrian appearance to behavior parameters. We evaluate our proposed model on several public multiple pedestrian interaction video datasets. Results show that our strategic planning model explains human interactions 25% better when compared to state-of-the-art methods.



### A Convolutional Neural Network Neutrino Event Classifier
- **Arxiv ID**: http://arxiv.org/abs/1604.01444v3
- **DOI**: 10.1088/1748-0221/11/09/P09001
- **Categories**: **hep-ex**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1604.01444v3)
- **Published**: 2016-04-05 22:41:13+00:00
- **Updated**: 2016-08-12 19:31:15+00:00
- **Authors**: A. Aurisano, A. Radovic, D. Rocco, A. Himmel, M. D. Messier, E. Niner, G. Pawloski, F. Psihas, A. Sousa, P. Vahle
- **Comment**: 23 pages, 12 figures
- **Journal**: 2016 JINST 11 P09001
- **Summary**: Convolutional neural networks (CNNs) have been widely applied in the computer vision community to solve complex problems in image recognition and analysis. We describe an application of the CNN technology to the problem of identifying particle interactions in sampling calorimeters used commonly in high energy physics and high energy neutrino physics in particular. Following a discussion of the core concepts of CNNs and recent innovations in CNN architectures related to the field of deep learning, we outline a specific application to the NOvA neutrino detector. This algorithm, CVN (Convolutional Visual Network) identifies neutrino interactions based on their topology without the need for detailed reconstruction and outperforms algorithms currently in use by the NOvA collaboration.



