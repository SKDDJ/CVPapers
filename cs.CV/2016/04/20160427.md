# Arxiv Papers in cs.CV on 2016-04-27
### Image Colorization Using a Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1604.07904v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, I.2.6; I.4.9; J.5
- **Links**: [PDF](http://arxiv.org/pdf/1604.07904v1)
- **Published**: 2016-04-27 02:16:43+00:00
- **Updated**: 2016-04-27 02:16:43+00:00
- **Authors**: Tung Nguyen, Kazuki Mori, Ruck Thawonmas
- **Comment**: None
- **Journal**: Proc. of ASIAGRAPH 2016, Toyama, Japan, pp. 49-50, Mar. 5-6, 2016
- **Summary**: In this paper, we present a novel approach that uses deep learning techniques for colorizing grayscale images. By utilizing a pre-trained convolutional neural network, which is originally designed for image classification, we are able to separate content and style of different images and recombine them into a single image. We then propose a method that can add colors to a grayscale image by combining its content with style of a color image having semantic similarity with the grayscale one. As an application, to our knowledge the first of its kind, we use the proposed method to colorize images of ukiyo-e a genre of Japanese painting?and obtain interesting results, showing the potential of this method in the growing field of computer assisted art.



### DASC: Robust Dense Descriptor for Multi-modal and Multi-spectral Correspondence Estimation
- **Arxiv ID**: http://arxiv.org/abs/1604.07944v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.07944v1)
- **Published**: 2016-04-27 06:35:13+00:00
- **Updated**: 2016-04-27 06:35:13+00:00
- **Authors**: Seungryong Kim, Dongbo Min, Bumsub Ham, Minh N. Do, Kwanghoon Sohn
- **Comment**: None
- **Journal**: None
- **Summary**: Establishing dense correspondences between multiple images is a fundamental task in many applications. However, finding a reliable correspondence in multi-modal or multi-spectral images still remains unsolved due to their challenging photometric and geometric variations. In this paper, we propose a novel dense descriptor, called dense adaptive self-correlation (DASC), to estimate multi-modal and multi-spectral dense correspondences. Based on an observation that self-similarity existing within images is robust to imaging modality variations, we define the descriptor with a series of an adaptive self-correlation similarity measure between patches sampled by a randomized receptive field pooling, in which a sampling pattern is obtained using a discriminative learning. The computational redundancy of dense descriptors is dramatically reduced by applying fast edge-aware filtering. Furthermore, in order to address geometric variations including scale and rotation, we propose a geometry-invariant DASC (GI-DASC) descriptor that effectively leverages the DASC through a superpixel-based representation. For a quantitative evaluation of the GI-DASC, we build a novel multi-modal benchmark as varying photometric and geometric conditions. Experimental results demonstrate the outstanding performance of the DASC and GI-DASC in many cases of multi-modal and multi-spectral dense correspondences.



### Graph Laplacian Regularization for Image Denoising: Analysis in the Continuous Domain
- **Arxiv ID**: http://arxiv.org/abs/1604.07948v2
- **DOI**: 10.1109/TIP.2017.2651400
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.07948v2)
- **Published**: 2016-04-27 06:53:12+00:00
- **Updated**: 2017-08-30 12:11:26+00:00
- **Authors**: Jiahao Pang, Gene Cheung
- **Comment**: More discussions and results are provided
- **Journal**: None
- **Summary**: Inverse imaging problems are inherently under-determined, and hence it is important to employ appropriate image priors for regularization. One recent popular prior---the graph Laplacian regularizer---assumes that the target pixel patch is smooth with respect to an appropriately chosen graph. However, the mechanisms and implications of imposing the graph Laplacian regularizer on the original inverse problem are not well understood. To address this problem, in this paper we interpret neighborhood graphs of pixel patches as discrete counterparts of Riemannian manifolds and perform analysis in the continuous domain, providing insights into several fundamental aspects of graph Laplacian regularization for image denoising. Specifically, we first show the convergence of the graph Laplacian regularizer to a continuous-domain functional, integrating a norm measured in a locally adaptive metric space. Focusing on image denoising, we derive an optimal metric space assuming non-local self-similarity of pixel patches, leading to an optimal graph Laplacian regularizer for denoising in the discrete domain. We then interpret graph Laplacian regularization as an anisotropic diffusion scheme to explain its behavior during iterations, e.g., its tendency to promote piecewise smooth signals under certain settings. To verify our analysis, an iterative image denoising algorithm is developed. Experimental results show that our algorithm performs competitively with state-of-the-art denoising methods such as BM3D for natural images, and outperforms them significantly for piecewise smooth images.



### Zero-shot object prediction using semantic scene knowledge
- **Arxiv ID**: http://arxiv.org/abs/1604.07952v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.07952v3)
- **Published**: 2016-04-27 07:16:56+00:00
- **Updated**: 2016-12-22 12:30:59+00:00
- **Authors**: Rene Grzeszick, Gernot A. Fink
- **Comment**: This version extends the related work
- **Journal**: None
- **Summary**: This work focuses on the semantic relations between scenes and objects for visual object recognition. Semantic knowledge can be a powerful source of information especially in scenarios with few or no annotated training samples. These scenarios are referred to as zero-shot or few-shot recognition and often build on visual attributes. Here, instead of relying on various visual attributes, a more direct way is pursued: after recognizing the scene that is depicted in an image, semantic relations between scenes and objects are used for predicting the presence of objects in an unsupervised manner. Most importantly, relations between scenes and objects can easily be obtained from external sources such as large scale text corpora from the web and, therefore, do not require tremendous manual labeling efforts. It will be shown that in cluttered scenes, where visual recognition is difficult, scene knowledge is an important cue for predicting objects.



### Simultaneous Food Localization and Recognition
- **Arxiv ID**: http://arxiv.org/abs/1604.07953v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.07953v2)
- **Published**: 2016-04-27 07:20:55+00:00
- **Updated**: 2017-01-19 12:49:11+00:00
- **Authors**: Marc Bola√±os, Petia Radeva
- **Comment**: 6 pages, 6 figures, 2 tables. International Conference on Pattern
  Recognition (ICPR) 2016 (in press)
- **Journal**: None
- **Summary**: The development of automatic nutrition diaries, which would allow to keep track objectively of everything we eat, could enable a whole new world of possibilities for people concerned about their nutrition patterns. With this purpose, in this paper we propose the first method for simultaneous food localization and recognition. Our method is based on two main steps, which consist in, first, produce a food activation map on the input image (i.e. heat map of probabilities) for generating bounding boxes proposals and, second, recognize each of the food types or food-related objects present in each bounding box. We demonstrate that our proposal, compared to the most similar problem nowadays - object localization, is able to obtain high precision and reasonable recall levels with only a few bounding boxes. Furthermore, we show that it is applicable to both conventional and egocentric images.



### Deep Learning for Saliency Prediction in Natural Video
- **Arxiv ID**: http://arxiv.org/abs/1604.08010v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.08010v1)
- **Published**: 2016-04-27 10:34:21+00:00
- **Updated**: 2016-04-27 10:34:21+00:00
- **Authors**: Souad Chaabouni, Jenny Benois-Pineau, Ofer Hadar, Chokri Ben Amar
- **Comment**: None
- **Journal**: None
- **Summary**: The purpose of this paper is the detection of salient areas in natural video by using the new deep learning techniques. Salient patches in video frames are predicted first. Then the predicted visual fixation maps are built upon them. We design the deep architecture on the basis of CaffeNet implemented with Caffe toolkit. We show that changing the way of data selection for optimisation of network parameters, we can save computation cost up to 12 times. We extend deep learning approaches for saliency prediction in still images with RGB values to specificity of video using the sensitivity of the human visual system to residual motion. Furthermore, we complete primary colour pixel values by contrast features proposed in classical visual attention prediction models. The experiments are conducted on two publicly available datasets. The first is IRCCYN video database containing 31 videos with an overall amount of 7300 frames and eye fixations of 37 subjects. The second one is HOLLYWOOD2 provided 2517 movie clips with the eye fixations of 19 subjects. On IRCYYN dataset, the accuracy obtained is of 89.51%. On HOLLYWOOD2 dataset, results in prediction of saliency of patches show the improvement up to 2% with regard to RGB use only. The resulting accuracy of 76, 6% is obtained. The AUC metric in comparison of predicted saliency maps with visual fixation maps shows the increase up to 16% on a sample of video clips from this dataset.



### Detecting Violence in Video using Subclasses
- **Arxiv ID**: http://arxiv.org/abs/1604.08088v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1604.08088v1)
- **Published**: 2016-04-27 14:32:16+00:00
- **Updated**: 2016-04-27 14:32:16+00:00
- **Authors**: Xirong Li, Yujia Huo, Jieping Xu, Qin Jin
- **Comment**: None
- **Journal**: None
- **Summary**: This paper attacks the challenging problem of violence detection in videos. Different from existing works focusing on combining multi-modal features, we go one step further by adding and exploiting subclasses visually related to violence. We enrich the MediaEval 2015 violence dataset by \emph{manually} labeling violence videos with respect to the subclasses. Such fine-grained annotations not only help understand what have impeded previous efforts on learning to fuse the multi-modal features, but also enhance the generalization ability of the learned fusion to novel test data. The new subclass based solution, with AP of 0.303 and P100 of 0.55 on the MediaEval 2015 test set, outperforms several state-of-the-art alternatives. Notice that our solution does not require fine-grained annotations on the test set, so it can be directly applied on novel and fully unlabeled videos. Interestingly, our study shows that motion related features, though being essential part in previous systems, are dispensable.



### Laser light-field fusion for wide-field lensfree on-chip phase contrast nanoscopy
- **Arxiv ID**: http://arxiv.org/abs/1604.08145v2
- **DOI**: None
- **Categories**: **physics.optics**, cs.CV, physics.ins-det
- **Links**: [PDF](http://arxiv.org/pdf/1604.08145v2)
- **Published**: 2016-04-27 17:06:17+00:00
- **Updated**: 2016-05-30 19:56:34+00:00
- **Authors**: Farnoud Kazemzadeh, Alexander Wong
- **Comment**: 15 pages, 4 figures
- **Journal**: None
- **Summary**: Wide-field lensfree on-chip microscopy, which leverages holography principles to capture interferometric light-field encodings without lenses, is an emerging imaging modality with widespread interest given the large field-of-view compared to lens-based techniques. In this study, we introduce the idea of laser light-field fusion for lensfree on-chip phase contrast nanoscopy, where interferometric laser light-field encodings acquired using an on-chip setup with laser pulsations at different wavelengths are fused to produce marker-free phase contrast images of superior quality with resolving power more than five times below the pixel pitch of the sensor array and more than 40% beyond the diffraction limit. As a proof of concept, we demonstrate, for the first time, a wide-field lensfree on-chip instrument successfully detecting 300 nm particles, resulting in a numerical aperture of 1.1, across a large field-of-view of $\sim$ 30 mm$^2$ without any specialized or intricate sample preparation, or the use of synthetic aperture- or shift-based techniques.



### Unsupervised Classification in Hyperspectral Imagery with Nonlocal Total Variation and Primal-Dual Hybrid Gradient Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1604.08182v2
- **DOI**: 10.1109/TGRS.2017.2654486
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.08182v2)
- **Published**: 2016-04-27 19:11:10+00:00
- **Updated**: 2017-02-13 21:45:19+00:00
- **Authors**: Wei Zhu, Victoria Chayes, Alexandre Tiard, Stephanie Sanchez, Devin Dahlberg, Andrea L. Bertozzi, Stanley Osher, Dominique Zosso, Da Kuang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a graph-based nonlocal total variation method (NLTV) is proposed for unsupervised classification of hyperspectral images (HSI). The variational problem is solved by the primal-dual hybrid gradient (PDHG) algorithm. By squaring the labeling function and using a stable simplex clustering routine, an unsupervised clustering method with random initialization can be implemented. The effectiveness of this proposed algorithm is illustrated on both synthetic and real-world HSI, and numerical results show that the proposed algorithm outperforms other standard unsupervised clustering methods such as spherical K-means, nonnegative matrix factorization (NMF), and the graph-based Merriman-Bence-Osher (MBO) scheme.



### Amodal Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1604.08202v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.08202v2)
- **Published**: 2016-04-27 19:56:11+00:00
- **Updated**: 2016-08-17 19:13:04+00:00
- **Authors**: Ke Li, Jitendra Malik
- **Comment**: 23 pages, 14 figures; European Conference on Computer Vision (ECCV),
  2016
- **Journal**: None
- **Summary**: We consider the problem of amodal instance segmentation, the objective of which is to predict the region encompassing both visible and occluded parts of each object. Thus far, the lack of publicly available amodal segmentation annotations has stymied the development of amodal segmentation methods. In this paper, we sidestep this issue by relying solely on standard modal instance segmentation annotations to train our model. The result is a new method for amodal instance segmentation, which represents the first such method to the best of our knowledge. We demonstrate the proposed method's effectiveness both qualitatively and quantitatively.



### Diving deeper into mentee networks
- **Arxiv ID**: http://arxiv.org/abs/1604.08220v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1604.08220v1)
- **Published**: 2016-04-27 20:05:45+00:00
- **Updated**: 2016-04-27 20:05:45+00:00
- **Authors**: Ragav Venkatesan, Baoxin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Modern computer vision is all about the possession of powerful image representations. Deeper and deeper convolutional neural networks have been built using larger and larger datasets and are made publicly available. A large swath of computer vision scientists use these pre-trained networks with varying degrees of successes in various tasks. Even though there is tremendous success in copying these networks, the representational space is not learnt from the target dataset in a traditional manner. One of the reasons for opting to use a pre-trained network over a network learnt from scratch is that small datasets provide less supervision and require meticulous regularization, smaller and careful tweaking of learning rates to even achieve stable learning without weight explosion. It is often the case that large deep networks are not portable, which necessitates the ability to learn mid-sized networks from scratch.   In this article, we dive deeper into training these mid-sized networks on small datasets from scratch by drawing additional supervision from a large pre-trained network. Such learning also provides better generalization accuracies than networks trained with common regularization techniques such as l2, l1 and dropouts. We show that features learnt thus, are more general than those learnt independently. We studied various characteristics of such networks and found some interesting behaviors.



### Multiview Differential Geometry of Curves
- **Arxiv ID**: http://arxiv.org/abs/1604.08256v1
- **DOI**: 10.1007/s11263-016-0912-7
- **Categories**: **cs.CV**, cs.CG, cs.GR, math.DG, 53A04, 53A17, 53A20, I.4.8; I.3.5
- **Links**: [PDF](http://arxiv.org/pdf/1604.08256v1)
- **Published**: 2016-04-27 21:55:39+00:00
- **Updated**: 2016-04-27 21:55:39+00:00
- **Authors**: Ricardo Fabbri, Benjamin Kimia
- **Comment**: International Journal of Computer Vision Final Accepted version.
  International Journal of Computer Vision, 2016. The final publication is
  available at Springer via http://dx.doi.org/10.1007/s11263-016-0912-7
- **Journal**: None
- **Summary**: The field of multiple view geometry has seen tremendous progress in reconstruction and calibration due to methods for extracting reliable point features and key developments in projective geometry. Point features, however, are not available in certain applications and result in unstructured point cloud reconstructions. General image curves provide a complementary feature when keypoints are scarce, and result in 3D curve geometry, but face challenges not addressed by the usual projective geometry of points and algebraic curves. We address these challenges by laying the theoretical foundations of a framework based on the differential geometry of general curves, including stationary curves, occluding contours, and non-rigid curves, aiming at stereo correspondence, camera estimation (including calibration, pose, and multiview epipolar geometry), and 3D reconstruction given measured image curves. By gathering previous results into a cohesive theory, novel results were made possible, yielding three contributions. First we derive the differential geometry of an image curve (tangent, curvature, curvature derivative) from that of the underlying space curve (tangent, curvature, curvature derivative, torsion). Second, we derive the differential geometry of a space curve from that of two corresponding image curves. Third, the differential motion of an image curve is derived from camera motion and the differential geometry and motion of the space curve. The availability of such a theory enables novel curve-based multiview reconstruction and camera estimation systems to augment existing point-based approaches. This theory has been used to reconstruct a "3D curve sketch", to determine camera pose from local curve geometry, and tracking; other developments are underway.



### Efficient Optimization for Rank-based Loss Functions
- **Arxiv ID**: http://arxiv.org/abs/1604.08269v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.08269v3)
- **Published**: 2016-04-27 23:33:19+00:00
- **Updated**: 2018-02-28 09:27:30+00:00
- **Authors**: Pritish Mohapatra, Michal Rolinek, C. V. Jawahar, Vladimir Kolmogorov, M. Pawan Kumar
- **Comment**: 15 pages, 2 figures
- **Journal**: None
- **Summary**: The accuracy of information retrieval systems is often measured using complex loss functions such as the average precision (AP) or the normalized discounted cumulative gain (NDCG). Given a set of positive and negative samples, the parameters of a retrieval system can be estimated by minimizing these loss functions. However, the non-differentiability and non-decomposability of these loss functions does not allow for simple gradient based optimization algorithms. This issue is generally circumvented by either optimizing a structured hinge-loss upper bound to the loss function or by using asymptotic methods like the direct-loss minimization framework. Yet, the high computational complexity of loss-augmented inference, which is necessary for both the frameworks, prohibits its use in large training data sets. To alleviate this deficiency, we present a novel quicksort flavored algorithm for a large class of non-decomposable loss functions. We provide a complete characterization of the loss functions that are amenable to our algorithm, and show that it includes both AP and NDCG based loss functions. Furthermore, we prove that no comparison based algorithm can improve upon the computational complexity of our approach asymptotically. We demonstrate the effectiveness of our approach in the context of optimizing the structured hinge loss upper bound of AP and NDCG loss for learning models for a variety of vision tasks. We show that our approach provides significantly better results than simpler decomposable loss functions, while requiring a comparable training time.



