# Arxiv Papers in cs.CV on 2016-04-01
### Variational reaction-diffusion systems for semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/1604.00092v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1604.00092v1)
- **Published**: 2016-04-01 01:04:31+00:00
- **Updated**: 2016-04-01 01:04:31+00:00
- **Authors**: Paul Vernaza
- **Comment**: None
- **Journal**: None
- **Summary**: A novel global energy model for multi-class semantic image segmentation is proposed that admits very efficient exact inference and derivative calculations for learning. Inference in this model is equivalent to MAP inference in a particular kind of vector-valued Gaussian Markov random field, and ultimately reduces to solving a linear system of linear PDEs known as a reaction-diffusion system. Solving this system can be achieved in time scaling near-linearly in the number of image pixels by reducing it to sequential FFTs, after a linear change of basis. The efficiency and differentiability of the model make it especially well-suited for integration with convolutional neural networks, even allowing it to be used in interior, feature-generating layers and stacked multiple times. Experimental results are shown demonstrating that the model can be employed profitably in conjunction with different convolutional net architectures, and that doing so compares favorably to joint training of a fully-connected CRF with a convolutional net.



### Good Practice in CNN Feature Transfer
- **Arxiv ID**: http://arxiv.org/abs/1604.00133v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.00133v1)
- **Published**: 2016-04-01 05:31:57+00:00
- **Updated**: 2016-04-01 05:31:57+00:00
- **Authors**: Liang Zheng, Yali Zhao, Shengjin Wang, Jingdong Wang, Qi Tian
- **Comment**: 9 pages. It will be submitted to an appropriate journal
- **Journal**: None
- **Summary**: The objective of this paper is the effective transfer of the Convolutional Neural Network (CNN) feature in image search and classification. Systematically, we study three facts in CNN transfer. 1) We demonstrate the advantage of using images with a properly large size as input to CNN instead of the conventionally resized one. 2) We benchmark the performance of different CNN layers improved by average/max pooling on the feature maps. Our observation suggests that the Conv5 feature yields very competitive accuracy under such pooling step. 3) We find that the simple combination of pooled features extracted across various CNN layers is effective in collecting evidences from both low and high level descriptors. Following these good practices, we are capable of improving the state of the art on a number of benchmarks to a large margin.



### It's Moving! A Probabilistic Model for Causal Motion Segmentation in Moving Camera Videos
- **Arxiv ID**: http://arxiv.org/abs/1604.00136v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.00136v1)
- **Published**: 2016-04-01 05:37:26+00:00
- **Updated**: 2016-04-01 05:37:26+00:00
- **Authors**: Pia Bideau, Erik Learned-Miller
- **Comment**: None
- **Journal**: None
- **Summary**: The human ability to detect and segment moving objects works in the presence of multiple objects, complex background geometry, motion of the observer, and even camouflage. In addition to all of this, the ability to detect motion is nearly instantaneous. While there has been much recent progress in motion segmentation, it still appears we are far from human capabilities. In this work, we derive from first principles a new likelihood function for assessing the probability of an optical flow vector given the 3D motion direction of an object. This likelihood uses a novel combination of the angle and magnitude of the optical flow to maximize the information about the true motions of objects. Using this new likelihood and several innovations in initialization, we develop a motion segmentation algorithm that beats current state-of-the-art methods by a large margin. We compare to five state-of-the-art methods on two established benchmarks, and a third new data set of camouflaged animals, which we introduce to push motion segmentation to the next level.



### Learning a Pose Lexicon for Semantic Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1604.00147v1
- **DOI**: 10.1109/ICME.2016.7552882
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.00147v1)
- **Published**: 2016-04-01 06:24:31+00:00
- **Updated**: 2016-04-01 06:24:31+00:00
- **Authors**: Lijuan Zhou, Wanqing Li, Philip Ogunbona
- **Comment**: Accepted by the 2016 IEEE International Conference on Multimedia and
  Expo (ICME 2016). 6 pages paper and 4 pages supplementary material
- **Journal**: None
- **Summary**: This paper presents a novel method for learning a pose lexicon comprising semantic poses defined by textual instructions and their associated visual poses defined by visual features. The proposed method simultaneously takes two input streams, semantic poses and visual pose candidates, and statistically learns a mapping between them to construct the lexicon. With the learned lexicon, action recognition can be cast as the problem of finding the maximum translation probability of a sequence of semantic poses given a stream of visual pose candidates. Experiments evaluating pre-trained and zero-shot action recognition conducted on MSRC-12 gesture and WorkoutSu-10 exercise datasets were used to verify the efficacy of the proposed method.



### PHOCNet: A Deep Convolutional Neural Network for Word Spotting in Handwritten Documents
- **Arxiv ID**: http://arxiv.org/abs/1604.00187v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.00187v3)
- **Published**: 2016-04-01 10:11:38+00:00
- **Updated**: 2017-12-05 06:58:02+00:00
- **Authors**: Sebastian Sudholt, Gernot A. Fink
- **Comment**: published as conference paper at the International Conference on
  Frontiers in Handwriting Recognition 2016
- **Journal**: None
- **Summary**: In recent years, deep convolutional neural networks have achieved state of the art performance in various computer vision task such as classification, detection or segmentation. Due to their outstanding performance, CNNs are more and more used in the field of document image analysis as well. In this work, we present a CNN architecture that is trained with the recently proposed PHOC representation. We show empirically that our CNN architecture is able to outperform state of the art results for various word spotting benchmarks while exhibiting short training and test times.



### Tensor Representations via Kernel Linearization for Action Recognition from 3D Skeletons (Extended Version)
- **Arxiv ID**: http://arxiv.org/abs/1604.00239v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.00239v2)
- **Published**: 2016-04-01 13:41:49+00:00
- **Updated**: 2016-07-28 08:35:38+00:00
- **Authors**: Piotr Koniusz, Anoop Cherian, Fatih Porikli
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we explore tensor representations that can compactly capture higher-order relationships between skeleton joints for 3D action recognition. We first define RBF kernels on 3D joint sequences, which are then linearized to form kernel descriptors. The higher-order outer-products of these kernel descriptors form our tensor representations. We present two different kernels for action recognition, namely (i) a sequence compatibility kernel that captures the spatio-temporal compatibility of joints in one sequence against those in the other, and (ii) a dynamics compatibility kernel that explicitly models the action dynamics of a sequence. Tensors formed from these kernels are then used to train an SVM. We present experiments on several benchmark datasets and demonstrate state of the art results, substantiating the effectiveness of our representations.



### Building Machines That Learn and Think Like People
- **Arxiv ID**: http://arxiv.org/abs/1604.00289v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1604.00289v3)
- **Published**: 2016-04-01 15:37:57+00:00
- **Updated**: 2016-11-02 17:26:50+00:00
- **Authors**: Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, Samuel J. Gershman
- **Comment**: In press at Behavioral and Brain Sciences. Open call for commentary
  proposals (until Nov. 22, 2016).
  https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/information/calls-for-commentary/open-calls-for-commentary
- **Journal**: None
- **Summary**: Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.



### Automated Alertness and Emotion Detection for Empathic Feedback During E-Learning
- **Arxiv ID**: http://arxiv.org/abs/1604.00312v1
- **DOI**: 10.1109/T4E.2013.19
- **Categories**: **cs.CV**, cs.CY, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1604.00312v1)
- **Published**: 2016-04-01 16:13:05+00:00
- **Updated**: 2016-04-01 16:13:05+00:00
- **Authors**: S L Happy, A. Dasgupta, P. Patnaik, A. Routray
- **Comment**: IEEE International Conference on Technology for Education, Kharagpur,
  India, 2013
- **Journal**: IEEE International Conference on Technology for Education, 2013
- **Summary**: In the context of education technology, empathic interaction with the user and feedback by the learning system using multiple inputs such as video, voice and text inputs is an important area of research. In this paper, a nonintrusive, standalone model for intelligent assessment of alertness and emotional state as well as generation of appropriate feedback has been proposed. Using the non-intrusive visual cues, the system classifies emotion and alertness state of the user, and provides appropriate feedback according to the detected cognitive state using facial expressions, ocular parameters, postures, and gestures. Assessment of alertness level using ocular parameters such as PERCLOS and saccadic parameters, emotional state from facial expression analysis, and detection of both relevant cognitive and emotional states from upper body gestures and postures has been proposed. Integration of such a system in e-learning environment is expected to enhance students performance through interaction, feedback, and positive mood induction.



### How to Transfer? Zero-Shot Object Recognition via Hierarchical Transfer of Semantic Attributes
- **Arxiv ID**: http://arxiv.org/abs/1604.00326v1
- **DOI**: 10.1109/WACV.2015.116
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.00326v1)
- **Published**: 2016-04-01 16:51:56+00:00
- **Updated**: 2016-04-01 16:51:56+00:00
- **Authors**: Ziad Al-Halah, Rainer Stiefelhagen
- **Comment**: Published as a conference paper at WACV 2015, modifications include
  new results with GoogLeNet features
- **Journal**: None
- **Summary**: Attribute based knowledge transfer has proven very successful in visual object analysis and learning previously unseen classes. However, the common approach learns and transfers attributes without taking into consideration the embedded structure between the categories in the source set. Such information provides important cues on the intra-attribute variations. We propose to capture these variations in a hierarchical model that expands the knowledge source with additional abstraction levels of attributes. We also provide a novel transfer approach that can choose the appropriate attributes to be shared with an unseen class. We evaluate our approach on three public datasets: aPascal, Animals with Attributes and CUB-200-2011 Birds. The experiments demonstrate the effectiveness of our model with significant improvement over state-of-the-art.



### Person Re-identification in Appearance Impaired Scenarios
- **Arxiv ID**: http://arxiv.org/abs/1604.00367v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.00367v1)
- **Published**: 2016-04-01 19:20:03+00:00
- **Updated**: 2016-04-01 19:20:03+00:00
- **Authors**: Mengran Gou, Xikang Zhang, Angels Rates-Borras, Sadjad Asghari-Esfeden, Mario Sznaier, Octavia Camps
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Person re-identification is critical in surveillance applications. Current approaches rely on appearance based features extracted from a single or multiple shots of the target and candidate matches. These approaches are at a disadvantage when trying to distinguish between candidates dressed in similar colors or when targets change their clothing. In this paper we propose a dynamics-based feature to overcome this limitation. The main idea is to capture soft biometrics from gait and motion patterns by gathering dense short trajectories (tracklets) which are Fisher vector encoded. To illustrate the merits of the proposed features we introduce three new "appearance-impaired" datasets. Our experiments on the original and the appearance impaired datasets demonstrate the benefits of incorporating dynamics-based information with appearance-based information to re-identification algorithms.



### Large-Scale Electron Microscopy Image Segmentation in Spark
- **Arxiv ID**: http://arxiv.org/abs/1604.00385v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1604.00385v1)
- **Published**: 2016-04-01 19:53:30+00:00
- **Updated**: 2016-04-01 19:53:30+00:00
- **Authors**: Stephen M. Plaza, Stuart E. Berg
- **Comment**: None
- **Journal**: None
- **Summary**: The emerging field of connectomics aims to unlock the mysteries of the brain by understanding the connectivity between neurons. To map this connectivity, we acquire thousands of electron microscopy (EM) images with nanometer-scale resolution. After aligning these images, the resulting dataset has the potential to reveal the shapes of neurons and the synaptic connections between them. However, imaging the brain of even a tiny organism like the fruit fly yields terabytes of data. It can take years of manual effort to examine such image volumes and trace their neuronal connections. One solution is to apply image segmentation algorithms to help automate the tracing tasks. In this paper, we propose a novel strategy to apply such segmentation on very large datasets that exceed the capacity of a single machine. Our solution is robust to potential segmentation errors which could otherwise severely compromise the quality of the overall segmentation, for example those due to poor classifier generalizability or anomalies in the image dataset. We implement our algorithms in a Spark application which minimizes disk I/O, and apply them to a few large EM datasets, revealing both their effectiveness and scalability. We hope this work will encourage external contributions to EM segmentation by providing 1) a flexible plugin architecture that deploys easily on different cluster environments and 2) an in-memory representation of segmentation that could be conducive to new advances.



### Structure from Motion on a Sphere
- **Arxiv ID**: http://arxiv.org/abs/1604.00409v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.00409v2)
- **Published**: 2016-04-01 21:01:53+00:00
- **Updated**: 2016-09-05 15:11:43+00:00
- **Authors**: Jonathan Ventura
- **Comment**: in European Conference on Computer Vision (ECCV) 2016
- **Journal**: None
- **Summary**: We describe a special case of structure from motion where the camera rotates on a sphere. The camera's optical axis lies perpendicular to the sphere's surface. In this case, the camera's pose is minimally represented by three rotation parameters. From analysis of the epipolar geometry we derive a novel and efficient solution for the essential matrix relating two images, requiring only three point correspondences in the minimal case. We apply this solver in a structure-from-motion pipeline that aggregates pairwise relations by rotation averaging followed by bundle adjustment with an inverse depth parameterization. Our methods enable scene modeling with an outward-facing camera and object scanning with an inward-facing camera.



### Leaving Some Stones Unturned: Dynamic Feature Prioritization for Activity Detection in Streaming Video
- **Arxiv ID**: http://arxiv.org/abs/1604.00427v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.00427v1)
- **Published**: 2016-04-01 22:37:28+00:00
- **Updated**: 2016-04-01 22:37:28+00:00
- **Authors**: Yu-Chuan Su, Kristen Grauman
- **Comment**: None
- **Journal**: None
- **Summary**: Current approaches for activity recognition often ignore constraints on computational resources: 1) they rely on extensive feature computation to obtain rich descriptors on all frames, and 2) they assume batch-mode access to the entire test video at once. We propose a new active approach to activity recognition that prioritizes "what to compute when" in order to make timely predictions. The main idea is to learn a policy that dynamically schedules the sequence of features to compute on selected frames of a given test video. In contrast to traditional static feature selection, our approach continually re-prioritizes computation based on the accumulated history of observations and accounts for the transience of those observations in ongoing video. We develop variants to handle both the batch and streaming settings. On two challenging datasets, our method provides significantly better accuracy than alternative techniques for a wide range of computational budgets.



### Adapting Models to Signal Degradation using Distillation
- **Arxiv ID**: http://arxiv.org/abs/1604.00433v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.00433v2)
- **Published**: 2016-04-01 23:24:17+00:00
- **Updated**: 2017-08-29 17:14:25+00:00
- **Authors**: Jong-Chyi Su, Subhransu Maji
- **Comment**: BMVC 2017
- **Journal**: None
- **Summary**: Model compression and knowledge distillation have been successfully applied for cross-architecture and cross-domain transfer learning. However, a key requirement is that training examples are in correspondence across the domains. We show that in many scenarios of practical importance such aligned data can be synthetically generated using computer graphics pipelines allowing domain adaptation through distillation. We apply this technique to learn models for recognizing low-resolution images using labeled high-resolution images, non-localized objects using labeled localized objects, line-drawings using labeled color images, etc. Experiments on various fine-grained recognition datasets demonstrate that the technique improves recognition performance on the low-quality data and beats strong baselines for domain adaptation. Finally, we present insights into workings of the technique through visualizations and relating it to existing literature.



