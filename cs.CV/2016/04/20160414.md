# Arxiv Papers in cs.CV on 2016-04-14
### Understanding How Image Quality Affects Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1604.04004v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04004v2)
- **Published**: 2016-04-14 00:47:50+00:00
- **Updated**: 2016-04-21 20:44:52+00:00
- **Authors**: Samuel Dodge, Lina Karam
- **Comment**: Final version will appear in IEEE Xplore in the Proceedings of the
  Conference on the Quality of Multimedia Experience (QoMEX), June 6-8, 2016
- **Journal**: None
- **Summary**: Image quality is an important practical challenge that is often overlooked in the design of machine vision systems. Commonly, machine vision systems are trained and tested on high quality image datasets, yet in practical applications the input images can not be assumed to be of high quality. Recently, deep neural networks have obtained state-of-the-art performance on many machine vision tasks. In this paper we provide an evaluation of 4 state-of-the-art deep neural network models for image classification under quality distortions. We consider five types of quality distortions: blur, noise, contrast, JPEG, and JPEG2000 compression. We show that the existing networks are susceptible to these quality distortions, particularly to blur and noise. These results enable future work in developing deep neural networks that are more invariant to quality distortions.



### Multi-Oriented Text Detection with Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1604.04018v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04018v2)
- **Published**: 2016-04-14 02:37:05+00:00
- **Updated**: 2016-04-16 13:22:03+00:00
- **Authors**: Zheng Zhang, Chengquan Zhang, Wei Shen, Cong Yao, Wenyu Liu, Xiang Bai
- **Comment**: Accepted by CVPR2016
- **Journal**: None
- **Summary**: In this paper, we propose a novel approach for text detec- tion in natural images. Both local and global cues are taken into account for localizing text lines in a coarse-to-fine pro- cedure. First, a Fully Convolutional Network (FCN) model is trained to predict the salient map of text regions in a holistic manner. Then, text line hypotheses are estimated by combining the salient map and character components. Fi- nally, another FCN classifier is used to predict the centroid of each character, in order to remove the false hypotheses. The framework is general for handling text in multiple ori- entations, languages and fonts. The proposed method con- sistently achieves the state-of-the-art performance on three text detection benchmarks: MSRA-TD500, ICDAR2015 and ICDAR2013.



### Towards Automated Melanoma Screening: Proper Computer Vision & Reliable Results
- **Arxiv ID**: http://arxiv.org/abs/1604.04024v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04024v3)
- **Published**: 2016-04-14 03:26:28+00:00
- **Updated**: 2016-05-06 21:00:22+00:00
- **Authors**: Michel Fornaciali, Micael Carvalho, Fl√°via Vasques Bittencourt, Sandra Avila, Eduardo Valle
- **Comment**: Minor corrections on State of the Art and Conclusion
- **Journal**: None
- **Summary**: In this paper we survey, analyze and criticize current art on automated melanoma screening, reimplementing a baseline technique, and proposing two novel ones. Melanoma, although highly curable when detected early, ends as one of the most dangerous types of cancer, due to delayed diagnosis and treatment. Its incidence is soaring, much faster than the number of trained professionals able to diagnose it. Automated screening appears as an alternative to make the most of those professionals, focusing their time on the patients at risk while safely discharging the other patients. However, the potential of automated melanoma diagnosis is currently unfulfilled, due to the emphasis of current literature on outdated computer vision models. Even more problematic is the irreproducibility of current art. We show how streamlined pipelines based upon current Computer Vision outperform conventional models - a model based on an advanced bags of words reaches an AUC of 84.6%, and a model based on deep neural networks reaches 89.3%, while the baseline (a classical bag of words) stays at 81.2%. We also initiate a dialog to improve reproducibility in our community



### Deep Feature Based Contextual Model for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1604.04048v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04048v1)
- **Published**: 2016-04-14 07:01:23+00:00
- **Updated**: 2016-04-14 07:01:23+00:00
- **Authors**: Wenqing Chu, Deng Cai
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection is one of the most active areas in computer vision, which has made significant improvement in recent years. Current state-of-the-art object detection methods mostly adhere to the framework of regions with convolutional neural network (R-CNN) and only use local appearance features inside object bounding boxes. Since these approaches ignore the contextual information around the object proposals, the outcome of these detectors may generate a semantically incoherent interpretation of the input image. In this paper, we propose an ensemble object detection system which incorporates the local appearance, the contextual information in term of relationships among objects and the global scene based contextual feature generated by a convolutional neural network. The system is formulated as a fully connected conditional random field (CRF) defined on object proposals and the contextual constraints among object proposals are modeled as edges naturally. Furthermore, a fast mean field approximation method is utilized to inference in this CRF model efficiently. The experimental results demonstrate that our approach achieves a higher mean average precision (mAP) on PASCAL VOC 2007 datasets compared to the baseline algorithm Faster R-CNN.



### Object Detection from Video Tubelets with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1604.04053v1
- **DOI**: 10.1109/CVPR.2016.95
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04053v1)
- **Published**: 2016-04-14 07:22:44+00:00
- **Updated**: 2016-04-14 07:22:44+00:00
- **Authors**: Kai Kang, Wanli Ouyang, Hongsheng Li, Xiaogang Wang
- **Comment**: Accepted in CVPR 2016 as a Spotlight paper
- **Journal**: Computer Vision and Pattern Recognition (CVPR), 2016 IEEE
  Conference on (pp. 817-825)
- **Summary**: Deep Convolution Neural Networks (CNNs) have shown impressive performance in various vision tasks such as image classification, object detection and semantic segmentation. For object detection, particularly in still images, the performance has been significantly increased last year thanks to powerful deep networks (e.g. GoogleNet) and detection frameworks (e.g. Regions with CNN features (R-CNN)). The lately introduced ImageNet task on object detection from video (VID) brings the object detection task into the video domain, in which objects' locations at each frame are required to be annotated with bounding boxes. In this work, we introduce a complete framework for the VID task based on still-image object detection and general object tracking. Their relations and contributions in the VID task are thoroughly studied and evaluated. In addition, a temporal convolution network is proposed to incorporate temporal information to regularize the detection results and shows its effectiveness for the task.



### Deep Residual Networks with Exponential Linear Unit
- **Arxiv ID**: http://arxiv.org/abs/1604.04112v4
- **DOI**: 10.1145/2983402.2983406
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04112v4)
- **Published**: 2016-04-14 11:09:09+00:00
- **Updated**: 2016-10-05 07:14:27+00:00
- **Authors**: Anish Shah, Eashan Kadam, Hena Shah, Sameer Shinde, Sandip Shingade
- **Comment**: submitted in Vision Net 2016, Jaipur, India
- **Journal**: None
- **Summary**: Very deep convolutional neural networks introduced new problems like vanishing gradient and degradation. The recent successful contributions towards solving these problems are Residual and Highway Networks. These networks introduce skip connections that allow the information (from the input or those learned in earlier layers) to flow more into the deeper layers. These very deep models have lead to a considerable decrease in test errors, on benchmarks like ImageNet and COCO. In this paper, we propose the use of exponential linear unit instead of the combination of ReLU and Batch Normalization in Residual Networks. We show that this not only speeds up learning in Residual Networks but also improves the accuracy as the depth increases. It improves the test error on almost all data sets, like CIFAR-10 and CIFAR-100



### Filling in the details: Perceiving from low fidelity images
- **Arxiv ID**: http://arxiv.org/abs/1604.04125v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1604.04125v1)
- **Published**: 2016-04-14 12:10:23+00:00
- **Updated**: 2016-04-14 12:10:23+00:00
- **Authors**: Farahnaz Ahmed Wick, Michael L. Wick, Marc Pomplun
- **Comment**: None
- **Journal**: None
- **Summary**: Humans perceive their surroundings in great detail even though most of our visual field is reduced to low-fidelity color-deprived (e.g. dichromatic) input by the retina. In contrast, most deep learning architectures are computationally wasteful in that they consider every part of the input when performing an image processing task. Yet, the human visual system is able to perform visual reasoning despite having only a small fovea of high visual acuity. With this in mind, we wish to understand the extent to which connectionist architectures are able to learn from and reason with low acuity, distorted inputs. Specifically, we train autoencoders to generate full-detail images from low-detail "foveations" of those images and then measure their ability to reconstruct the full-detail images from the foveated versions. By varying the type of foveation, we can study how well the architectures can cope with various types of distortion. We find that the autoencoder compensates for lower detail by learning increasingly global feature functions. In many cases, the learnt features are suitable for reconstructing the original full-detail image. For example, we find that the networks accurately perceive color in the periphery, even when 75\% of the input is achromatic.



### On Reducing the Number of Visual Words in the Bag-of-Features Representation
- **Arxiv ID**: http://arxiv.org/abs/1604.04142v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1604.04142v1)
- **Published**: 2016-04-14 13:08:57+00:00
- **Updated**: 2016-04-14 13:08:57+00:00
- **Authors**: Giuseppe Amato, Fabrizio Falchi, Claudio Gennaro
- **Comment**: None
- **Journal**: VISAPP (1). 2013. p. 657-662
- **Summary**: A new class of applications based on visual search engines are emerging, especially on smart-phones that have evolved into powerful tools for processing images and videos. The state-of-the-art algorithms for large visual content recognition and content based similarity search today use the "Bag of Features" (BoF) or "Bag of Words" (BoW) approach. The idea, borrowed from text retrieval, enables the use of inverted files. A very well known issue with this approach is that the query images, as well as the stored data, are described with thousands of words. This poses obvious efficiency problems when using inverted files to perform efficient image matching. In this paper, we propose and compare various techniques to reduce the number of words describing an image to improve efficiency and we study the effects of this reduction on effectiveness in landmark recognition and retrieval scenarios. We show that very relevant improvement in performance are achievable still preserving the advantages of the BoF base approach.



### Self-taught learning of a deep invariant representation for visual tracking via temporal slowness principle
- **Arxiv ID**: http://arxiv.org/abs/1604.04144v1
- **DOI**: 10.1016/j.patcog.2015.02.012
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1604.04144v1)
- **Published**: 2016-04-14 13:12:07+00:00
- **Updated**: 2016-04-14 13:12:07+00:00
- **Authors**: Jason Kuen, Kian Ming Lim, Chin Poo Lee
- **Comment**: Pattern Recognition (Elsevier), 2015
- **Journal**: None
- **Summary**: Visual representation is crucial for a visual tracking method's performances. Conventionally, visual representations adopted in visual tracking rely on hand-crafted computer vision descriptors. These descriptors were developed generically without considering tracking-specific information. In this paper, we propose to learn complex-valued invariant representations from tracked sequential image patches, via strong temporal slowness constraint and stacked convolutional autoencoders. The deep slow local representations are learned offline on unlabeled data and transferred to the observational model of our proposed tracker. The proposed observational model retains old training samples to alleviate drift, and collect negative samples which are coherent with target's motion pattern for better discriminative tracking. With the learned representation and online training samples, a logistic regression classifier is adopted to distinguish target from background, and retrained online to adapt to appearance changes. Subsequently, the observational model is integrated into a particle filter framework to peform visual tracking. Experimental results on various challenging benchmark sequences demonstrate that the proposed tracker performs favourably against several state-of-the-art trackers.



### Learning Visual Storylines with Skipping Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1604.04279v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04279v2)
- **Published**: 2016-04-14 19:56:33+00:00
- **Updated**: 2016-07-26 23:18:23+00:00
- **Authors**: Gunnar A. Sigurdsson, Xinlei Chen, Abhinav Gupta
- **Comment**: European Conference on Computer Vision (ECCV) 2016
- **Journal**: None
- **Summary**: What does a typical visit to Paris look like? Do people first take photos of the Louvre and then the Eiffel Tower? Can we visually model a temporal event like "Paris Vacation" using current frameworks? In this paper, we explore how we can automatically learn the temporal aspects, or storylines of visual concepts from web data. Previous attempts focus on consecutive image-to-image transitions and are unsuccessful at recovering the long-term underlying story. Our novel Skipping Recurrent Neural Network (S-RNN) model does not attempt to predict each and every data point in the sequence, like classic RNNs. Rather, S-RNN uses a framework that skips through the images in the photo stream to explore the space of all ordered subsets of the albums via an efficient sampling procedure. This approach reduces the negative impact of strong short-term correlations, and recovers the latent story more accurately. We show how our learned storylines can be used to analyze, predict, and summarize photo albums from Flickr. Our experimental results provide strong qualitative and quantitative evidence that S-RNN is significantly better than other candidate methods such as LSTMs on learning long-term correlations and recovering latent storylines. Moreover, we show how storylines can help machines better understand and summarize photo streams by inferring a brief personalized story of each individual album.



### Unsupervised Nonlinear Spectral Unmixing based on a Multilinear Mixing Model
- **Arxiv ID**: http://arxiv.org/abs/1604.04293v1
- **DOI**: 10.1109/TGRS.2017.2693366
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04293v1)
- **Published**: 2016-04-14 20:09:22+00:00
- **Updated**: 2016-04-14 20:09:22+00:00
- **Authors**: Qi Wei, Marcus Chen, Jean-Yves Tourneret, Simon Godsill
- **Comment**: None
- **Journal**: None
- **Summary**: In the community of remote sensing, nonlinear mixing models have recently received particular attention in hyperspectral image processing. In this paper, we present a novel nonlinear spectral unmixing method following the recent multilinear mixing model of [1], which includes an infinite number of terms related to interactions between different endmembers. The proposed unmixing method is unsupervised in the sense that the endmembers are estimated jointly with the abundances and other parameters of interest, i.e., the transition probability of undergoing further interactions. Non-negativity and sum-to one constraints are imposed on abundances while only nonnegativity is considered for endmembers. The resulting unmixing problem is formulated as a constrained nonlinear optimization problem, which is solved by a block coordinate descent strategy, consisting of updating the endmembers, abundances and transition probability iteratively. The proposed method is evaluated and compared with linear unmixing methods for synthetic and real hyperspectral datasets acquired by the AVIRIS sensor. The advantage of using non-linear unmixing as opposed to linear unmixing is clearly shown in these examples.



