# Arxiv Papers in cs.CV on 2016-04-13
### Joint Unsupervised Learning of Deep Representations and Image Clusters
- **Arxiv ID**: http://arxiv.org/abs/1604.03628v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1604.03628v3)
- **Published**: 2016-04-13 01:24:59+00:00
- **Updated**: 2016-06-20 19:56:16+00:00
- **Authors**: Jianwei Yang, Devi Parikh, Dhruv Batra
- **Comment**: 19 pages, 11 figures, 14 tables, 2016 IEEE Conference on Computer
  Vision and Pattern Recognition (CVPR)
- **Journal**: None
- **Summary**: In this paper, we propose a recurrent framework for Joint Unsupervised LEarning (JULE) of deep representations and image clusters. In our framework, successive operations in a clustering algorithm are expressed as steps in a recurrent process, stacked on top of representations output by a Convolutional Neural Network (CNN). During training, image clusters and representations are updated jointly: image clustering is conducted in the forward pass, while representation learning in the backward pass. Our key idea behind this framework is that good representations are beneficial to image clustering and clustering results provide supervisory signals to representation learning. By integrating two processes into a single model with a unified weighted triplet loss and optimizing it end-to-end, we can obtain not only more powerful representations, but also more precise image clusters. Extensive experiments show that our method outperforms the state-of-the-art on image clustering across a variety of image datasets. Moreover, the learned representations generalize well when transferred to other tasks.



### Quantifying mesoscale neuroanatomy using X-ray microtomography
- **Arxiv ID**: http://arxiv.org/abs/1604.03629v2
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1604.03629v2)
- **Published**: 2016-04-13 01:46:54+00:00
- **Updated**: 2016-07-26 19:56:59+00:00
- **Authors**: Eva L. Dyer, William Gray Roncal, Hugo L. Fernandes, Doga Gürsoy, Vincent De Andrade, Rafael Vescovi, Kamel Fezzaa, Xianghui Xiao, Joshua T. Vogelstein, Chris Jacobsen, Konrad P. Körding, Narayanan Kasthuri
- **Comment**: 28 pages, 9 figures
- **Journal**: None
- **Summary**: Methods for resolving the 3D microstructure of the brain typically start by thinly slicing and staining the brain, and then imaging each individual section with visible light photons or electrons. In contrast, X-rays can be used to image thick samples, providing a rapid approach for producing large 3D brain maps without sectioning. Here we demonstrate the use of synchrotron X-ray microtomography ($\mu$CT) for producing mesoscale $(1~\mu m^3)$ resolution brain maps from millimeter-scale volumes of mouse brain. We introduce a pipeline for $\mu$CT-based brain mapping that combines methods for sample preparation, imaging, automated segmentation of image volumes into cells and blood vessels, and statistical analysis of the resulting brain structures. Our results demonstrate that X-ray tomography promises rapid quantification of large brain volumes, complementing other brain mapping and connectomics efforts.



### Online Multi-Target Tracking Using Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1604.03635v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.03635v2)
- **Published**: 2016-04-13 02:41:43+00:00
- **Updated**: 2016-12-07 03:09:30+00:00
- **Authors**: Anton Milan, Seyed Hamid Rezatofighi, Anthony Dick, Ian Reid, Konrad Schindler
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel approach to online multi-target tracking based on recurrent neural networks (RNNs). Tracking multiple objects in real-world scenes involves many challenges, including a) an a-priori unknown and time-varying number of targets, b) a continuous state estimation of all present targets, and c) a discrete combinatorial problem of data association. Most previous methods involve complex models that require tedious tuning of parameters. Here, we propose for the first time, an end-to-end learning approach for online multi-target tracking. Existing deep learning methods are not designed for the above challenges and cannot be trivially applied to the task. Our solution addresses all of the above points in a principled way. Experiments on both synthetic and real data show promising results obtained at ~300 Hz on a standard CPU, and pave the way towards future research in this direction.



### Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1604.03650v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.03650v1)
- **Published**: 2016-04-13 04:35:07+00:00
- **Updated**: 2016-04-13 04:35:07+00:00
- **Authors**: Junyuan Xie, Ross Girshick, Ali Farhadi
- **Comment**: None
- **Journal**: None
- **Summary**: As 3D movie viewing becomes mainstream and Virtual Reality (VR) market emerges, the demand for 3D contents is growing rapidly. Producing 3D videos, however, remains challenging. In this paper we propose to use deep neural networks for automatically converting 2D videos and images to stereoscopic 3D format. In contrast to previous automatic 2D-to-3D conversion algorithms, which have separate stages and need ground truth depth map as supervision, our approach is trained end-to-end directly on stereo pairs extracted from 3D movies. This novel training scheme makes it possible to exploit orders of magnitude more data and significantly increases performance. Indeed, Deep3D outperforms baselines in both quantitative and human subject evaluations.



### Learning Social Affordance for Human-Robot Interaction
- **Arxiv ID**: http://arxiv.org/abs/1604.03692v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1604.03692v2)
- **Published**: 2016-04-13 08:40:06+00:00
- **Updated**: 2016-04-20 21:02:02+00:00
- **Authors**: Tianmin Shu, M. S. Ryoo, Song-Chun Zhu
- **Comment**: International Joint Conference on Artificial Intelligence (IJCAI),
  2016
- **Journal**: None
- **Summary**: In this paper, we present an approach for robot learning of social affordance from human activity videos. We consider the problem in the context of human-robot interaction: Our approach learns structural representations of human-human (and human-object-human) interactions, describing how body-parts of each agent move with respect to each other and what spatial relations they should maintain to complete each sub-event (i.e., sub-goal). This enables the robot to infer its own movement in reaction to the human body motion, allowing it to naturally replicate such interactions.   We introduce the representation of social affordance and propose a generative model for its weakly supervised learning from human demonstration videos. Our approach discovers critical steps (i.e., latent sub-events) in an interaction and the typical motion associated with them, learning what body-parts should be involved and how. The experimental results demonstrate that our Markov Chain Monte Carlo (MCMC) based learning algorithm automatically discovers semantically meaningful interactive affordance from RGB-D videos, which allows us to generate appropriate full body motion for an agent.



### A Novel Method to Study Bottom-up Visual Saliency and its Neural Mechanism
- **Arxiv ID**: http://arxiv.org/abs/1604.08426v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1604.08426v1)
- **Published**: 2016-04-13 12:14:31+00:00
- **Updated**: 2016-04-13 12:14:31+00:00
- **Authors**: Cheng Chen, Xilin Zhang, Yizhou Wang, Fang Fang
- **Comment**: None
- **Journal**: None
- **Summary**: In this study, we propose a novel method to measure bottom-up saliency maps of natural images. In order to eliminate the influence of top-down signals, backward masking is used to make stimuli (natural images) subjectively invisible to subjects, however, the bottom-up saliency can still orient the subjects attention. To measure this orientation/attention effect, we adopt the cueing effect paradigm by deploying discrimination tasks at each location of an image, and measure the discrimination performance variation across the image as the attentional effect of the bottom-up saliency. Such attentional effects are combined to construct a final bottomup saliency map. Based on the proposed method, we introduce a new bottom-up saliency map dataset of natural images to benchmark computational models. We compare several state-of-the-art saliency models on the dataset. Moreover, the proposed paradigm is applied to investigate the neural basis of the bottom-up visual saliency map by analyzing psychophysical and fMRI experimental results. Our findings suggest that the bottom-up saliency maps of natural images are constructed in V1. It provides a strong scientific evidence to resolve the long standing dispute in neuroscience about where the bottom-up saliency map is constructed in human brain.



### DENSER Cities: A System for Dense Efficient Reconstructions of Cities
- **Arxiv ID**: http://arxiv.org/abs/1604.03734v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1604.03734v1)
- **Published**: 2016-04-13 12:37:59+00:00
- **Updated**: 2016-04-13 12:37:59+00:00
- **Authors**: Michael Tanner, Pedro Pinies, Lina Maria Paz, Paul Newman
- **Comment**: None
- **Journal**: None
- **Summary**: This paper is about the efficient generation of dense, colored models of city-scale environments from range data and in particular, stereo cameras. Better maps make for better understanding; better understanding leads to better robots, but this comes at a cost. The computational and memory requirements of large dense models can be prohibitive. We provide the theory and the system needed to create city-scale dense reconstructions. To do so, we apply a regularizer over a compressed 3D data structure while dealing with the complex boundary conditions this induces during the data-fusion stage. We show that only with these considerations can we swiftly create neat, large, "well behaved" reconstructions. We evaluate our system using the KITTI dataset and provide statistics for the metric errors in all surfaces created compared to those measured with 3D laser. Our regularizer reduces the median error by 40% in 3.4 km of dense reconstructions with a median accuracy of 6 cm. For subjective analysis, we provide a qualitative review of 6.1 km of our dense reconstructions in an attached video. These are the largest dense reconstructions from a single passive camera we are aware of in the literature.



### VConv-DAE: Deep Volumetric Shape Learning Without Object Labels
- **Arxiv ID**: http://arxiv.org/abs/1604.03755v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1604.03755v3)
- **Published**: 2016-04-13 13:14:53+00:00
- **Updated**: 2016-09-09 20:36:36+00:00
- **Authors**: Abhishek Sharma, Oliver Grau, Mario Fritz
- **Comment**: None
- **Journal**: None
- **Summary**: With the advent of affordable depth sensors, 3D capture becomes more and more ubiquitous and already has made its way into commercial products. Yet, capturing the geometry or complete shapes of everyday objects using scanning devices (e.g. Kinect) still comes with several challenges that result in noise or even incomplete shapes. Recent success in deep learning has shown how to learn complex shape distributions in a data-driven way from large scale 3D CAD Model collections and to utilize them for 3D processing on volumetric representations and thereby circumventing problems of topology and tessellation. Prior work has shown encouraging results on problems ranging from shape completion to recognition. We provide an analysis of such approaches and discover that training as well as the resulting representation are strongly and unnecessarily tied to the notion of object labels. Thus, we propose a full convolutional volumetric auto encoder that learns volumetric representation from noisy data by estimating the voxel occupancy grids. The proposed method outperforms prior work on challenging tasks like denoising and shape completion. We also show that the obtained deep embedding gives competitive performance when used for classification and promising results for shape interpolation.



### Reversible Image Merging for Low-level Machine Vision
- **Arxiv ID**: http://arxiv.org/abs/1604.03832v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.03832v1)
- **Published**: 2016-04-13 15:31:24+00:00
- **Updated**: 2016-04-13 15:31:24+00:00
- **Authors**: Mikhail Kharinov
- **Comment**: 5 pages, 3 figures, 6 formulas, submitted to the 13th International
  Conference on Pattern Recognition and Information Processing October 3-5,
  2016, Minsk, Belarus
- **Journal**: Proc of the 13th International Conference on Pattern Recognition
  and Information Processing (PRIP'2016), Oct 3-5, 2016, Minsk, Belarus,
  pp.25-29
- **Summary**: In this paper a hierarchical model for pixel clustering and image segmentation is developed. In the model an image is hierarchically structured. The original image is treated as a set of nested images, which are capable to reversibly merge with each other. An object is defined as a structural element of an image, so that, an image is regarded as a maximal object. The simulating of none-hierarchical optimal pixel clustering by hierarchical clustering is studied. To generate a hierarchy of optimized piecewise constant image approximations, estimated by the standard deviation of approximation from the image, the conversion of any hierarchy of approximations into the hierarchy described in relation to the number of intensity levels by convex sequence of total squared errors is proposed.



### Detangling People: Individuating Multiple Close People and Their Body Parts via Region Assembly
- **Arxiv ID**: http://arxiv.org/abs/1604.03880v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.03880v1)
- **Published**: 2016-04-13 17:35:05+00:00
- **Updated**: 2016-04-13 17:35:05+00:00
- **Authors**: Hao Jiang, Kristen Grauman
- **Comment**: None
- **Journal**: None
- **Summary**: Today's person detection methods work best when people are in common upright poses and appear reasonably well spaced out in the image. However, in many real images, that's not what people do. People often appear quite close to each other, e.g., with limbs linked or heads touching, and their poses are often not pedestrian-like. We propose an approach to detangle people in multi-person images. We formulate the task as a region assembly problem. Starting from a large set of overlapping regions from body part semantic segmentation and generic object proposals, our optimization approach reassembles those pieces together into multiple person instances. It enforces that the composed body part regions of each person instance obey constraints on relative sizes, mutual spatial relationships, foreground coverage, and exclusive label assignments when overlapping. Since optimal region assembly is a challenging combinatorial problem, we present a Lagrangian relaxation method to accelerate the lower bound estimation, thereby enabling a fast branch and bound solution for the global optimum. As output, our method produces a pixel-level map indicating both 1) the body part labels (arm, leg, torso, and head), and 2) which parts belong to which individual person. Our results on three challenging datasets show our method is robust to clutter, occlusion, and complex poses. It outperforms a variety of competing methods, including existing detector CRF methods and region CNN approaches. In addition, we demonstrate its impact on a proxemics recognition task, which demands a precise representation of "whose body part is where" in crowded images.



### The Effect of Distortions on the Prediction of Visual Attention
- **Arxiv ID**: http://arxiv.org/abs/1604.03882v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.03882v1)
- **Published**: 2016-04-13 17:37:54+00:00
- **Updated**: 2016-04-13 17:37:54+00:00
- **Authors**: Milind S. Gide, Samuel F. Dodge, Lina J. Karam
- **Comment**: 14 pages, 2 column, 14 figures
- **Journal**: None
- **Summary**: Existing saliency models have been designed and evaluated for predicting the saliency in distortion-free images. However, in practice, the image quality is affected by a host of factors at several stages of the image processing pipeline such as acquisition, compression and transmission. Several studies have explored the effect of distortion on human visual attention; however, none of them have considered the performance of visual saliency models in the presence of distortion. Furthermore, given that one potential application of visual saliency prediction is to aid pooling of objective visual quality metrics, it is important to compare the performance of existing saliency models on distorted images. In this paper, we evaluate several state-of-the-art visual attention models over different databases consisting of distorted images with various types of distortions such as blur, noise and compression with varying levels of distortion severity. This paper also introduces new improved performance evaluation metrics that are shown to overcome shortcomings in existing performance metrics. We find that the performance of most models improves with moderate and high levels of distortions as compared to the near distortion-free case. In addition, model performance is also found to decrease with an increase in image complexity.



### Single-Image Depth Perception in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1604.03901v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1604.03901v2)
- **Published**: 2016-04-13 18:19:35+00:00
- **Updated**: 2017-01-06 16:05:35+00:00
- **Authors**: Weifeng Chen, Zhao Fu, Dawei Yang, Jia Deng
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies single-image depth perception in the wild, i.e., recovering depth from a single image taken in unconstrained settings. We introduce a new dataset "Depth in the Wild" consisting of images in the wild annotated with relative depth between pairs of random points. We also propose a new algorithm that learns to estimate metric depth using annotations of relative depth. Compared to the state of the art, our algorithm is simpler and performs better. Experiments show that our algorithm, combined with existing RGB-D data and our new relative depth annotations, significantly improves single-image depth perception in the wild.



### Removing Clouds and Recovering Ground Observations in Satellite Image Sequences via Temporally Contiguous Robust Matrix Completion
- **Arxiv ID**: http://arxiv.org/abs/1604.03915v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1604.03915v1)
- **Published**: 2016-04-13 19:13:17+00:00
- **Updated**: 2016-04-13 19:13:17+00:00
- **Authors**: Jialei Wang, Peder A. Olsen, Andrew R. Conn, Aurelie C. Lozano
- **Comment**: To Appear In Conference on Computer Vision and Pattern Recognition
  (CVPR 2016)
- **Journal**: None
- **Summary**: We consider the problem of removing and replacing clouds in satellite image sequences, which has a wide range of applications in remote sensing. Our approach first detects and removes the cloud-contaminated part of the image sequences. It then recovers the missing scenes from the clean parts using the proposed "TECROMAC" (TEmporally Contiguous RObust MAtrix Completion) objective. The objective function balances temporal smoothness with a low rank solution while staying close to the original observations. The matrix whose the rows are pixels and columnsare days corresponding to the image, has low-rank because the pixels reflect land-types such as vegetation, roads and lakes and there are relatively few variations as a result. We provide efficient optimization algorithms for TECROMAC, so we can exploit images containing millions of pixels. Empirical results on real satellite image sequences, as well as simulated data, demonstrate that our approach is able to recover underlying images from heavily cloud-contaminated observations.



### Visual Storytelling
- **Arxiv ID**: http://arxiv.org/abs/1604.03968v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1604.03968v1)
- **Published**: 2016-04-13 20:27:43+00:00
- **Updated**: 2016-04-13 20:27:43+00:00
- **Authors**: Ting-Hao, Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, Lucy Vanderwende, Michel Galley, Margaret Mitchell
- **Comment**: to appear in NAACL 2016
- **Journal**: None
- **Summary**: We introduce the first dataset for sequential vision-to-language, and explore how this data may be used for the task of visual storytelling. The first release of this dataset, SIND v.1, includes 81,743 unique photos in 20,211 sequences, aligned to both descriptive (caption) and story language. We establish several strong baselines for the storytelling task, and motivate an automatic metric to benchmark progress. Modelling concrete description as well as figurative and social language, as provided in this dataset and the storytelling task, has the potential to move artificial intelligence from basic understandings of typical visual scenes towards more and more human-like understanding of grounded event structure and subjective expression.



