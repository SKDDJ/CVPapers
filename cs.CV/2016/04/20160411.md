# Arxiv Papers in cs.CV on 2016-04-11
### Capturing Dynamic Textured Surfaces of Moving Targets
- **Arxiv ID**: http://arxiv.org/abs/1604.02801v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.02801v1)
- **Published**: 2016-04-11 06:03:09+00:00
- **Updated**: 2016-04-11 06:03:09+00:00
- **Authors**: Ruizhe Wang, Lingyu Wei, Etienne Vouga, Qixing Huang, Duygu Ceylan, Gerard Medioni, Hao Li
- **Comment**: 22 pages, 12 figures
- **Journal**: None
- **Summary**: We present an end-to-end system for reconstructing complete watertight and textured models of moving subjects such as clothed humans and animals, using only three or four handheld sensors. The heart of our framework is a new pairwise registration algorithm that minimizes, using a particle swarm strategy, an alignment error metric based on mutual visibility and occlusion. We show that this algorithm reliably registers partial scans with as little as 15% overlap without requiring any initial correspondences, and outperforms alternative global registration algorithms. This registration algorithm allows us to reconstruct moving subjects from free-viewpoint video produced by consumer-grade sensors, without extensive sensor calibration, constrained capture volume, expensive arrays of cameras, or templates of the subject geometry.



### NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis
- **Arxiv ID**: http://arxiv.org/abs/1604.02808v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.02808v1)
- **Published**: 2016-04-11 06:44:53+00:00
- **Updated**: 2016-04-11 06:44:53+00:00
- **Authors**: Amir Shahroudy, Jun Liu, Tian-Tsong Ng, Gang Wang
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Recent approaches in depth-based human activity analysis achieved outstanding performance and proved the effectiveness of 3D representation for classification of action classes. Currently available depth-based and RGB+D-based action recognition benchmarks have a number of limitations, including the lack of training samples, distinct class labels, camera views and variety of subjects. In this paper we introduce a large-scale dataset for RGB+D human action recognition with more than 56 thousand video samples and 4 million frames, collected from 40 distinct subjects. Our dataset contains 60 different action classes including daily, mutual, and health-related actions. In addition, we propose a new recurrent neural network structure to model the long-term temporal correlation of the features for each body part, and utilize them for better action classification. Experimental results show the advantages of applying deep learning methods over state-of-the-art hand-crafted features on the suggested cross-subject and cross-view evaluation criteria for our dataset. The introduction of this large scale dataset will enable the community to apply, develop and adapt various data-hungry learning techniques for the task of depth-based and RGB+D-based human activity analysis.



### Beyond Brightness Constancy: Learning Noise Models for Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1604.02815v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.02815v1)
- **Published**: 2016-04-11 07:23:44+00:00
- **Updated**: 2016-04-11 07:23:44+00:00
- **Authors**: Dan Rosenbaum, Yair Weiss
- **Comment**: None
- **Journal**: None
- **Summary**: Optical flow is typically estimated by minimizing a "data cost" and an optional regularizer. While there has been much work on different regularizers many modern algorithms still use a data cost that is not very different from the ones used over 30 years ago: a robust version of brightness constancy or gradient constancy. In this paper we leverage the recent availability of ground-truth optical flow databases in order to learn a data cost. Specifically we take a generative approach in which the data cost models the distribution of noise after warping an image according to the flow and we measure the "goodness" of a data cost by how well it matches the true distribution of flow warp error. Consistent with current practice, we find that robust versions of gradient constancy are better models than simple brightness constancy but a learned GMM that models the density of patches of warp error gives a much better fit than any existing assumption of constancy. This significant advantage of the GMM is due to an explicit modeling of the spatial structure of warp errors, a feature which is missing from almost all existing data costs in optical flow. Finally, we show how a good density model of warp error patches can be used for optical flow estimation on whole images. We replace the data cost by the expected patch log-likelihood (EPLL), and show how this cost can be optimized iteratively using an additional step of denoising the warp error image. The results of our experiments are promising and show that patch models with higher likelihood lead to better optical flow estimation.



### Active Learning for Online Recognition of Human Activities from Streaming Videos
- **Arxiv ID**: http://arxiv.org/abs/1604.02855v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1604.02855v1)
- **Published**: 2016-04-11 09:32:51+00:00
- **Updated**: 2016-04-11 09:32:51+00:00
- **Authors**: Rocco De Rosa, Ilaria Gori, Fabio Cuzzolin, Barbara Caputo, Nicol√≤ Cesa-Bianchi
- **Comment**: None
- **Journal**: None
- **Summary**: Recognising human activities from streaming videos poses unique challenges to learning algorithms: predictive models need to be scalable, incrementally trainable, and must remain bounded in size even when the data stream is arbitrarily long. Furthermore, as parameter tuning is problematic in a streaming setting, suitable approaches should be parameterless, and make no assumptions on what class labels may occur in the stream. We present here an approach to the recognition of human actions from streaming data which meets all these requirements by: (1) incrementally learning a model which adaptively covers the feature space with simple local classifiers; (2) employing an active learning strategy to reduce annotation requests; (3) achieving promising accuracy within a fixed model size. Extensive experiments on standard benchmarks show that our approach is competitive with state-of-the-art non-incremental methods, and outperforms the existing active incremental baselines.



### Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1604.02878v1
- **DOI**: 10.1109/LSP.2016.2603342
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.02878v1)
- **Published**: 2016-04-11 10:47:14+00:00
- **Updated**: 2016-04-11 10:47:14+00:00
- **Authors**: Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, Yu Qiao
- **Comment**: Submitted to IEEE Signal Processing Letters
- **Journal**: None
- **Summary**: Face detection and alignment in unconstrained environment are challenging due to various poses, illuminations and occlusions. Recent studies show that deep learning approaches can achieve impressive performance on these two tasks. In this paper, we propose a deep cascaded multi-task framework which exploits the inherent correlation between them to boost up their performance. In particular, our framework adopts a cascaded structure with three stages of carefully designed deep convolutional networks that predict face and landmark location in a coarse-to-fine manner. In addition, in the learning process, we propose a new online hard sample mining strategy that can improve the performance automatically without manual sample selection. Our method achieves superior accuracy over the state-of-the-art techniques on the challenging FDDB and WIDER FACE benchmark for face detection, and AFLW benchmark for face alignment, while keeps real time performance.



### Semantic 3D Reconstruction with Continuous Regularization and Ray Potentials Using a Visibility Consistency Constraint
- **Arxiv ID**: http://arxiv.org/abs/1604.02885v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.02885v3)
- **Published**: 2016-04-11 11:12:24+00:00
- **Updated**: 2019-08-26 14:53:16+00:00
- **Authors**: Nikolay Savinov, Christian Haene, Lubor Ladicky, Marc Pollefeys
- **Comment**: Accepted as a spotlight oral paper by CVPR 2016. Code at
  https://github.com/nsavinov/ray_potentials/
- **Journal**: None
- **Summary**: We propose an approach for dense semantic 3D reconstruction which uses a data term that is defined as potentials over viewing rays, combined with continuous surface area penalization. Our formulation is a convex relaxation which we augment with a crucial non-convex constraint that ensures exact handling of visibility. To tackle the non-convex minimization problem, we propose a majorize-minimize type strategy which converges to a critical point. We demonstrate the benefits of using the non-convex constraint experimentally. For the geometry-only case, we set a new state of the art on two datasets of the commonly used Middlebury multi-view stereo benchmark. Moreover, our general-purpose formulation directly reconstructs thin objects, which are usually treated with specialized algorithms. A qualitative evaluation on the dense semantic 3D reconstruction task shows that we improve significantly over previous methods.



### Sparse Coding for Alpha Matting
- **Arxiv ID**: http://arxiv.org/abs/1604.02898v1
- **DOI**: 10.1109/TIP.2016.2555705
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.02898v1)
- **Published**: 2016-04-11 11:48:18+00:00
- **Updated**: 2016-04-11 11:48:18+00:00
- **Authors**: Jubin Johnson, Ehsan Shahrian Varnousfaderani, Hisham Cholakkal, Deepu Rajan
- **Comment**: To appear in IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Existing color sampling based alpha matting methods use the compositing equation to estimate alpha at a pixel from pairs of foreground (F) and background (B) samples. The quality of the matte depends on the selected (F,B) pairs. In this paper, the matting problem is reinterpreted as a sparse coding of pixel features, wherein the sum of the codes gives the estimate of the alpha matte from a set of unpaired F and B samples. A non-parametric probabilistic segmentation provides a certainty measure on the pixel belonging to foreground or background, based on which a dictionary is formed for use in sparse coding. By removing the restriction to conform to (F,B) pairs, this method allows for better alpha estimation from multiple F and B samples. The same framework is extended to videos, where the requirement of temporal coherence is handled effectively. Here, the dictionary is formed by samples from multiple frames. A multi-frame graph model, as opposed to a single image as for image matting, is proposed that can be solved efficiently in closed form. Quantitative and qualitative evaluations on a benchmark dataset are provided to show that the proposed method outperforms current state-of-the-art in image and video matting.



### Statistics of RGBD Images
- **Arxiv ID**: http://arxiv.org/abs/1604.02902v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.02902v1)
- **Published**: 2016-04-11 12:00:57+00:00
- **Updated**: 2016-04-11 12:00:57+00:00
- **Authors**: Dan Rosenbaum, Yair Weiss
- **Comment**: None
- **Journal**: None
- **Summary**: Cameras that can measure the depth of each pixel in addition to its color have become easily available and are used in many consumer products worldwide. Often the depth channel is captured at lower quality compared to the RGB channels and different algorithms have been proposed to improve the quality of the D channel given the RGB channels. Typically these approaches work by assuming that edges in RGB are correlated with edges in D.   In this paper we approach this problem from the standpoint of natural image statistics. We obtain examples of high quality RGBD images from a computer graphics generated movie (MPI-Sintel) and we use these examples to compare different probabilistic generative models of RGBD image patches. We then use the generative models together with a degradation model and obtain a Bayes Least Squares (BLS) estimator of the D channel given the RGB channels. Our results show that learned generative models outperform the state-of-the-art in improving the quality of depth channels given the color channels in natural images even when training is performed on artificially generated images.



### Gaussian Process Domain Experts for Model Adaptation in Facial Behavior Analysis
- **Arxiv ID**: http://arxiv.org/abs/1604.02917v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1604.02917v2)
- **Published**: 2016-04-11 12:37:36+00:00
- **Updated**: 2016-05-02 18:54:08+00:00
- **Authors**: Stefanos Eleftheriadis, Ognjen Rudovic, Marc P. Deisenroth, Maja Pantic
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel approach for supervised domain adaptation that is based upon the probabilistic framework of Gaussian processes (GPs). Specifically, we introduce domain-specific GPs as local experts for facial expression classification from face images. The adaptation of the classifier is facilitated in probabilistic fashion by conditioning the target expert on multiple source experts. Furthermore, in contrast to existing adaptation approaches, we also learn a target expert from available target data solely. Then, a single and confident classifier is obtained by combining the predictions from multiple experts based on their confidence. Learning of the model is efficient and requires no retraining/reweighting of the source classifiers. We evaluate the proposed approach on two publicly available datasets for multi-class (MultiPIE) and multi-label (DISFA) facial expression classification. To this end, we perform adaptation of two contextual factors: 'where' (view) and 'who' (subject). We show in our experiments that the proposed approach consistently outperforms both source and target classifiers, while using as few as 30 target examples. It also outperforms the state-of-the-art approaches for supervised domain adaptation.



### Kernel-based Sensor Fusion with Application to Audio-Visual Voice Activity Detection
- **Arxiv ID**: http://arxiv.org/abs/1604.02946v1
- **DOI**: 10.1109/TSP.2016.2605068
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.02946v1)
- **Published**: 2016-04-11 13:33:43+00:00
- **Updated**: 2016-04-11 13:33:43+00:00
- **Authors**: David Dov, Ronen Talmon, Israel Cohen
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the problem of multiple view data fusion in the presence of noise and interferences. Recent studies have approached this problem using kernel methods, by relying particularly on a product of kernels constructed separately for each view. From a graph theory point of view, we analyze this fusion approach in a discrete setting. More specifically, based on a statistical model for the connectivity between data points, we propose an algorithm for the selection of the kernel bandwidth, a parameter, which, as we show, has important implications on the robustness of this fusion approach to interferences. Then, we consider the fusion of audio-visual speech signals measured by a single microphone and by a video camera pointed to the face of the speaker. Specifically, we address the task of voice activity detection, i.e., the detection of speech and non-speech segments, in the presence of structured interferences such as keyboard taps and office noise. We propose an algorithm for voice activity detection based on the audio-visual signal. Simulation results show that the proposed algorithm outperforms competing fusion and voice activity detection approaches. In addition, we demonstrate that a proper selection of the kernel bandwidth indeed leads to improved performance.



### CP-mtML: Coupled Projection multi-task Metric Learning for Large Scale Face Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1604.02975v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.02975v1)
- **Published**: 2016-04-11 14:30:38+00:00
- **Updated**: 2016-04-11 14:30:38+00:00
- **Authors**: Binod Bhattarai, Gaurav Sharma, Frederic Jurie
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel Coupled Projection multi-task Metric Learning (CP-mtML) method for large scale face retrieval. In contrast to previous works which were limited to low dimensional features and small datasets, the proposed method scales to large datasets with high dimensional face descriptors. It utilises pairwise (dis-)similarity constraints as supervision and hence does not require exhaustive class annotation for every training image. While, traditionally, multi-task learning methods have been validated on same dataset but different tasks, we work on the more challenging setting with heterogeneous datasets and different tasks. We show empirical validation on multiple face image datasets of different facial traits, e.g. identity, age and expression. We use classic Local Binary Pattern (LBP) descriptors along with the recent Deep Convolutional Neural Network (CNN) features. The experiments clearly demonstrate the scalability and improved performance of the proposed method on the tasks of identity and age based face image retrieval compared to competitive existing methods, on the standard datasets and with the presence of a million distractor face images.



### Semi-supervised learning of local structured output predictors
- **Arxiv ID**: http://arxiv.org/abs/1604.03010v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1604.03010v1)
- **Published**: 2016-04-11 15:53:04+00:00
- **Updated**: 2016-04-11 15:53:04+00:00
- **Authors**: Xin Du
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the problem of semi-supervised structured output prediction, which aims to learn predictors for structured outputs, such as sequences, tree nodes, vectors, etc., from a set of data points of both input-output pairs and single inputs without outputs. The traditional methods to solve this problem usually learns one single predictor for all the data points, and ignores the variety of the different data points. Different parts of the data set may have different local distributions, and requires different optimal local predictors. To overcome this disadvantage of existing methods, we propose to learn different local predictors for neighborhoods of different data points, and the missing structured outputs simultaneously. In the neighborhood of each data point, we proposed to learn a linear predictor by minimizing both the complexity of the predictor and the upper bound of the structured prediction loss. The minimization is conducted by gradient descent algorithms. Experiments over four benchmark data sets, including DDSM mammography medical images, SUN natural image data set, Cora research paper data set, and Spanish news wire article sentence data set, show the advantages of the proposed method.



### Binarized Neural Networks on the ImageNet Classification Task
- **Arxiv ID**: http://arxiv.org/abs/1604.03058v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1604.03058v5)
- **Published**: 2016-04-11 18:39:33+00:00
- **Updated**: 2016-11-19 01:37:40+00:00
- **Authors**: Xundong Wu, Yong Wu, Yong Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: We trained Binarized Neural Networks (BNNs) on the high resolution ImageNet ILSVRC-2102 dataset classification task and achieved a good performance. With a moderate size network of 13 layers, we obtained top-5 classification accuracy rate of 84.1 % on validation set through network distillation, much better than previous published results of 73.2% on XNOR network and 69.1% on binarized GoogleNET. We expect networks of better performance can be obtained by following our current strategies. We provide a detailed discussion and preliminary analysis on strategies used in the network training.



### Reservoir computing for spatiotemporal signal classification without trained output weights
- **Arxiv ID**: http://arxiv.org/abs/1604.03073v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1604.03073v2)
- **Published**: 2016-04-11 19:14:05+00:00
- **Updated**: 2016-07-19 13:28:17+00:00
- **Authors**: Ashley Prater
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: Reservoir computing is a recently introduced machine learning paradigm that has been shown to be well-suited for the processing of spatiotemporal data. Rather than training the network node connections and weights via backpropagation in traditional recurrent neural networks, reservoirs instead have fixed connections and weights among the `hidden layer' nodes, and traditionally only the weights to the output layer of neurons are trained using linear regression. We claim that for signal classification tasks one may forgo the weight training step entirely and instead use a simple supervised clustering method based upon principal components of norms of reservoir states. The proposed method is mathematically analyzed and explored through numerical experiments on real-world data. The examples demonstrate that the proposed may outperform the traditional trained output weight approach in terms of classification accuracy and sensitivity to reservoir parameters.



### Fully-Automatic Synapse Prediction and Validation on a Large Data Set
- **Arxiv ID**: http://arxiv.org/abs/1604.03075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.03075v1)
- **Published**: 2016-04-11 19:25:44+00:00
- **Updated**: 2016-04-11 19:25:44+00:00
- **Authors**: Gary B. Huang, Louis K. Scheffer, Stephen M. Plaza
- **Comment**: None
- **Journal**: None
- **Summary**: Extracting a connectome from an electron microscopy (EM) data set requires identification of neurons and determination of synapses between neurons. As manual extraction of this information is very time-consuming, there has been extensive research effort to automatically segment the neurons to help guide and eventually replace manual tracing. Until recently, there has been comparatively less research on automatically detecting the actual synapses between neurons. This discrepancy can, in part, be attributed to several factors: obtaining neuronal shapes is a prerequisite first step in extracting a connectome, manual tracing is much more time-consuming than annotating synapses, and neuronal contact area can be used as a proxy for synapses in determining connections.   However, recent research has demonstrated that contact area alone is not a sufficient predictor of synaptic connection. Moreover, as segmentation has improved, we have observed that synapse annotation is consuming a more significant fraction of overall reconstruction time. This ratio will only get worse as segmentation improves, gating overall possible speed-up. Therefore, we address this problem by developing algorithms that automatically detect pre-synaptic neurons and their post-synaptic partners. In particular, pre-synaptic structures are detected using a Deep and Wide Multiscale Recursive Network, and post-synaptic partners are detected using a MLP with features conditioned on the local segmentation.   This work is novel because it requires minimal amount of training, leverages advances in image segmentation directly, and provides a complete solution for polyadic synapse detection. We further introduce novel metrics to evaluate our algorithm on connectomes of meaningful size. These metrics demonstrate that complete automatic prediction can be used to effectively characterize most connectivity correctly.



### Hardware-oriented Approximation of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1604.03168v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.03168v3)
- **Published**: 2016-04-11 22:43:21+00:00
- **Updated**: 2016-10-20 15:09:39+00:00
- **Authors**: Philipp Gysel, Mohammad Motamedi, Soheil Ghiasi
- **Comment**: 8 pages, 4 figures, Accepted as a workshop contribution at ICLR 2016.
  Updated comparison to other works
- **Journal**: None
- **Summary**: High computational complexity hinders the widespread usage of Convolutional Neural Networks (CNNs), especially in mobile devices. Hardware accelerators are arguably the most promising approach for reducing both execution time and power consumption. One of the most important steps in accelerator development is hardware-oriented model approximation. In this paper we present Ristretto, a model approximation framework that analyzes a given CNN with respect to numerical resolution used in representing weights and outputs of convolutional and fully connected layers. Ristretto can condense models by using fixed point arithmetic and representation instead of floating point. Moreover, Ristretto fine-tunes the resulting fixed point network. Given a maximum error tolerance of 1%, Ristretto can successfully condense CaffeNet and SqueezeNet to 8-bit. The code for Ristretto is available.



### Using Deep Learning for Image-Based Plant Disease Detection
- **Arxiv ID**: http://arxiv.org/abs/1604.03169v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.03169v2)
- **Published**: 2016-04-11 22:44:20+00:00
- **Updated**: 2016-04-15 14:05:34+00:00
- **Authors**: Sharada Prasanna Mohanty, David Hughes, Marcel Salathe
- **Comment**: None
- **Journal**: None
- **Summary**: Crop diseases are a major threat to food security, but their rapid identification remains difficult in many parts of the world due to the lack of the necessary infrastructure. The combination of increasing global smartphone penetration and recent advances in computer vision made possible by deep learning has paved the way for smartphone-assisted disease diagnosis. Using a public dataset of 54,306 images of diseased and healthy plant leaves collected under controlled conditions, we train a deep convolutional neural network to identify 14 crop species and 26 diseases (or absence thereof). The trained model achieves an accuracy of 99.35% on a held-out test set, demonstrating the feasibility of this approach. When testing the model on a set of images collected from trusted online sources - i.e. taken under conditions different from the images used for training - the model still achieves an accuracy of 31.4%. While this accuracy is much higher than the one based on random selection (2.6%), a more diverse set of training data is needed to improve the general accuracy. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path towards smartphone-assisted crop disease diagnosis on a massive global scale.



