# Arxiv Papers in cs.CV on 2016-04-30
### InterActive: Inter-Layer Activeness Propagation
- **Arxiv ID**: http://arxiv.org/abs/1605.00052v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.00052v1)
- **Published**: 2016-04-30 02:28:11+00:00
- **Updated**: 2016-04-30 02:28:11+00:00
- **Authors**: Lingxi Xie, Liang Zheng, Jingdong Wang, Alan Yuille, Qi Tian
- **Comment**: To appear, in CVPR 2016 (10 pages, 3 figures)
- **Journal**: None
- **Summary**: An increasing number of computer vision tasks can be tackled with deep features, which are the intermediate outputs of a pre-trained Convolutional Neural Network. Despite the astonishing performance, deep features extracted from low-level neurons are still below satisfaction, arguably because they cannot access the spatial context contained in the higher layers. In this paper, we present InterActive, a novel algorithm which computes the activeness of neurons and network connections. Activeness is propagated through a neural network in a top-down manner, carrying high-level context and improving the descriptive power of low-level and mid-level neurons. Visualization indicates that neuron activeness can be interpreted as spatial-weighted neuron responses. We achieve state-of-the-art classification performance on a wide range of image datasets.



### DisturbLabel: Regularizing CNN on the Loss Layer
- **Arxiv ID**: http://arxiv.org/abs/1605.00055v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.00055v1)
- **Published**: 2016-04-30 02:44:48+00:00
- **Updated**: 2016-04-30 02:44:48+00:00
- **Authors**: Lingxi Xie, Jingdong Wang, Zhen Wei, Meng Wang, Qi Tian
- **Comment**: To appear in CVPR 2016 (10 pages, 10 figures)
- **Journal**: None
- **Summary**: During a long period of time we are combating over-fitting in the CNN training process with model regularization, including weight decay, model averaging, data augmentation, etc. In this paper, we present DisturbLabel, an extremely simple algorithm which randomly replaces a part of labels as incorrect values in each iteration. Although it seems weird to intentionally generate incorrect training labels, we show that DisturbLabel prevents the network training from over-fitting by implicitly averaging over exponentially many networks which are trained with different label sets. To the best of our knowledge, DisturbLabel serves as the first work which adds noises on the loss layer. Meanwhile, DisturbLabel cooperates well with Dropout to provide complementary regularization functions. Experiments demonstrate competitive recognition results on several popular image recognition datasets.



### Deep Colorization
- **Arxiv ID**: http://arxiv.org/abs/1605.00075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.00075v1)
- **Published**: 2016-04-30 07:58:05+00:00
- **Updated**: 2016-04-30 07:58:05+00:00
- **Authors**: Zezhou Cheng, Qingxiong Yang, Bin Sheng
- **Comment**: This is a low-resolution version. Please contact the authors to
  obtain the high-resolution one if you need it. Preliminary version of this
  work was published in ICCV 2015
- **Journal**: None
- **Summary**: This paper investigates into the colorization problem which converts a grayscale image to a colorful version. This is a very difficult problem and normally requires manual adjustment to achieve artifact-free quality. For instance, it normally requires human-labelled color scribbles on the grayscale target image or a careful selection of colorful reference images (e.g., capturing the same scene in the grayscale target image). Unlike the previous methods, this paper aims at a high-quality fully-automatic colorization method. With the assumption of a perfect patch matching technique, the use of an extremely large-scale reference database (that contains sufficient color images) is the most reliable solution to the colorization problem. However, patch matching noise will increase with respect to the size of the reference database in practice. Inspired by the recent success in deep learning techniques which provide amazing modeling of large-scale data, this paper re-formulates the colorization problem so that deep learning techniques can be directly employed. To ensure artifact-free quality, a joint bilateral filtering based post-processing step is proposed. We further develop an adaptive image clustering technique to incorporate the global image information. Numerous experiments demonstrate that our method outperforms the state-of-art algorithms both in terms of quality and speed.



### 3D Keypoint Detection Based on Deep Neural Network with Sparse Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1605.00129v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.00129v1)
- **Published**: 2016-04-30 15:47:28+00:00
- **Updated**: 2016-04-30 15:47:28+00:00
- **Authors**: Xinyu Lin, Ce Zhu, Qian Zhang, Yipeng Liu
- **Comment**: 13 pages, 6 figures
- **Journal**: None
- **Summary**: Researchers have proposed various methods to extract 3D keypoints from the surface of 3D mesh models over the last decades, but most of them are based on geometric methods, which lack enough flexibility to meet the requirements for various applications. In this paper, we propose a new method on the basis of deep learning by formulating the 3D keypoint detection as a regression problem using deep neural network (DNN) with sparse autoencoder (SAE) as our regression model. Both local information and global information of a 3D mesh model in multi-scale space are fully utilized to detect whether a vertex is a keypoint or not. SAE can effectively extract the internal structure of these two kinds of information and formulate high-level features for them, which is beneficial to the regression model. Three SAEs are used to formulate the hidden layers of the DNN and then a logistic regression layer is trained to process the high-level features produced in the third SAE. Numerical experiments show that the proposed DNN based 3D keypoint detection algorithm outperforms current five state-of-the-art methods for various 3D mesh models.



### Look-ahead before you leap: end-to-end active recognition by forecasting the effect of motion
- **Arxiv ID**: http://arxiv.org/abs/1605.00164v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1605.00164v2)
- **Published**: 2016-04-30 20:39:16+00:00
- **Updated**: 2016-08-05 22:15:48+00:00
- **Authors**: Dinesh Jayaraman, Kristen Grauman
- **Comment**: A preliminary version of the material in this document was filed as
  University of Texas technical report no. UT AI15-06, December, 2015, at:
  http://apps.cs.utexas.edu/tech_reports/reports/ai/AI-2214.pdf, ECCV 2016
- **Journal**: None
- **Summary**: Visual recognition systems mounted on autonomous moving agents face the challenge of unconstrained data, but simultaneously have the opportunity to improve their performance by moving to acquire new views of test data. In this work, we first show how a recurrent neural network-based system may be trained to perform end-to-end learning of motion policies suited for this "active recognition" setting. Further, we hypothesize that active vision requires an agent to have the capacity to reason about the effects of its motions on its view of the world. To verify this hypothesis, we attempt to induce this capacity in our active recognition pipeline, by simultaneously learning to forecast the effects of the agent's motions on its internal representation of the environment conditional on all past views. Results across two challenging datasets confirm both that our end-to-end system successfully learns meaningful policies for active category recognition, and that "learning to look ahead" further boosts recognition performance.



### Enforcing Template Representability and Temporal Consistency for Adaptive Sparse Tracking
- **Arxiv ID**: http://arxiv.org/abs/1605.00170v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.00170v1)
- **Published**: 2016-04-30 21:53:23+00:00
- **Updated**: 2016-04-30 21:53:23+00:00
- **Authors**: Xue Yang, Fei Han, Hua Wang, Hao Zhang
- **Comment**: 8 pages. It has been accepted for publication in 25th International
  Joint Conference on Artificial Intelligence (IJCAI-16)
- **Journal**: None
- **Summary**: Sparse representation has been widely studied in visual tracking, which has shown promising tracking performance. Despite a lot of progress, the visual tracking problem is still a challenging task due to appearance variations over time. In this paper, we propose a novel sparse tracking algorithm that well addresses temporal appearance changes, by enforcing template representability and temporal consistency (TRAC). By modeling temporal consistency, our algorithm addresses the issue of drifting away from a tracking target. By exploring the templates' long-term-short-term representability, the proposed method adaptively updates the dictionary using the most descriptive templates, which significantly improves the robustness to target appearance changes. We compare our TRAC algorithm against the state-of-the-art approaches on 12 challenging benchmark image sequences. Both qualitative and quantitative results demonstrate that our algorithm significantly outperforms previous state-of-the-art trackers.



