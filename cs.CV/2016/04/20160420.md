# Arxiv Papers in cs.CV on 2016-04-20
### Sherlock: Sparse Hierarchical Embeddings for Visually-aware One-class Collaborative Filtering
- **Arxiv ID**: http://arxiv.org/abs/1604.05813v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1604.05813v1)
- **Published**: 2016-04-20 04:36:57+00:00
- **Updated**: 2016-04-20 04:36:57+00:00
- **Authors**: Ruining He, Chunbin Lin, Jianguo Wang, Julian McAuley
- **Comment**: 7 pages, 3 figures
- **Journal**: None
- **Summary**: Building successful recommender systems requires uncovering the underlying dimensions that describe the properties of items as well as users' preferences toward them. In domains like clothing recommendation, explaining users' preferences requires modeling the visual appearance of the items in question. This makes recommendation especially challenging, due to both the complexity and subtlety of people's 'visual preferences,' as well as the scale and dimensionality of the data and features involved. Ultimately, a successful model should be capable of capturing considerable variance across different categories and styles, while still modeling the commonalities explained by `global' structures in order to combat the sparsity (e.g. cold-start), variability, and scale of real-world datasets. Here, we address these challenges by building such structures to model the visual dimensions across different product categories. With a novel hierarchical embedding architecture, our method accounts for both high-level (colorfulness, darkness, etc.) and subtle (e.g. casualness) visual characteristics simultaneously.



### Deep CNNs for HEp-2 Cells Classification : A Cross-specimen Analysis
- **Arxiv ID**: http://arxiv.org/abs/1604.05816v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.05816v3)
- **Published**: 2016-04-20 04:43:20+00:00
- **Updated**: 2018-02-14 16:29:32+00:00
- **Authors**: Hongwei Li, Wei-Shi Zheng, Jianguo Zhang
- **Comment**: rejected by MICCAI
- **Journal**: None
- **Summary**: Automatic classification of Human Epithelial Type-2 (HEp-2) cells staining patterns is an important and yet a challenging problem. Although both shallow and deep methods have been applied, the study of deep convolutional networks (CNNs) on this topic is shallow to date, thus failed to claim its top position for this problem. In this paper, we propose a novel study of using CNNs for HEp-2 cells classification focusing on cross-specimen analysis, a key evaluation for generalization. For the first time, our study reveals several key factors of using CNNs for HEp-2 cells classification. Our proposed system achieves state-of-the-art classification accuracy on public benchmark dataset. Comparative experiments on different training data reveals that adding different specimens,rather than increasing in numbers by affine transformations, helps to train a good deep model. This opens a new avenue for adopting deep CNNs to HEp-2 cells classification.



### Depth Image Inpainting: Improving Low Rank Matrix Completion with Low Gradient Regularization
- **Arxiv ID**: http://arxiv.org/abs/1604.05817v1
- **DOI**: 10.1109/TIP.2017.2718183
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.05817v1)
- **Published**: 2016-04-20 04:56:06+00:00
- **Updated**: 2016-04-20 04:56:06+00:00
- **Authors**: Hongyang Xue, Shengming Zhang, Deng Cai
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the case of inpainting single depth images. Without corresponding color images, previous or next frames, depth image inpainting is quite challenging. One natural solution is to regard the image as a matrix and adopt the low rank regularization just as inpainting color images. However, the low rank assumption does not make full use of the properties of depth images.   A shallow observation may inspire us to penalize the non-zero gradients by sparse gradient regularization. However, statistics show that though most pixels have zero gradients, there is still a non-ignorable part of pixels whose gradients are equal to 1. Based on this specific property of depth images , we propose a low gradient regularization method in which we reduce the penalty for gradient 1 while penalizing the non-zero gradients to allow for gradual depth changes. The proposed low gradient regularization is integrated with the low rank regularization into the low rank low gradient approach for depth image inpainting. We compare our proposed low gradient regularization with sparse gradient regularization. The experimental results show the effectiveness of our proposed approach.



### Scene Parsing with Integration of Parametric and Non-parametric Models
- **Arxiv ID**: http://arxiv.org/abs/1604.05848v1
- **DOI**: 10.1109/TIP.2016.2533862
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.05848v1)
- **Published**: 2016-04-20 07:38:15+00:00
- **Updated**: 2016-04-20 07:38:15+00:00
- **Authors**: Bing Shuai, Zhen Zuo, Gang Wang, Bing Wang
- **Comment**: 13 Pages, 6 figures, IEEE Transactions on Image Processing (T-IP)
  2016
- **Journal**: None
- **Summary**: We adopt Convolutional Neural Networks (CNNs) to be our parametric model to learn discriminative features and classifiers for local patch classification. Based on the occurrence frequency distribution of classes, an ensemble of CNNs (CNN-Ensemble) are learned, in which each CNN component focuses on learning different and complementary visual patterns. The local beliefs of pixels are output by CNN-Ensemble. Considering that visually similar pixels are indistinguishable under local context, we leverage the global scene semantics to alleviate the local ambiguity. The global scene constraint is mathematically achieved by adding a global energy term to the labeling energy function, and it is practically estimated in a non-parametric framework. A large margin based CNN metric learning method is also proposed for better global belief estimation. In the end, the integration of local and global beliefs gives rise to the class likelihood of pixels, based on which maximum marginal inference is performed to generate the label prediction maps. Even without any post-processing, we achieve state-of-the-art results on the challenging SiftFlow and Barcelona benchmarks.



### Estimating 3D Trajectories from 2D Projections via Disjunctive Factored Four-Way Conditional Restricted Boltzmann Machines
- **Arxiv ID**: http://arxiv.org/abs/1604.05865v2
- **DOI**: 10.1016/j.patcog.2017.04.017
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1604.05865v2)
- **Published**: 2016-04-20 09:08:50+00:00
- **Updated**: 2017-04-29 06:11:10+00:00
- **Authors**: Decebal Constantin Mocanu, Haitham Bou Ammar, Luis Puig, Eric Eaton, Antonio Liotta
- **Comment**: Pattern Recognition, ISSN 0031-3203, Elsevier, 2017
- **Journal**: None
- **Summary**: Estimation, recognition, and near-future prediction of 3D trajectories based on their two dimensional projections available from one camera source is an exceptionally difficult problem due to uncertainty in the trajectories and environment, high dimensionality of the specific trajectory states, lack of enough labeled data and so on. In this article, we propose a solution to solve this problem based on a novel deep learning model dubbed Disjunctive Factored Four-Way Conditional Restricted Boltzmann Machine (DFFW-CRBM). Our method improves state-of-the-art deep learning techniques for high dimensional time-series modeling by introducing a novel tensor factorization capable of driving forth order Boltzmann machines to considerably lower energy levels, at no computational costs. DFFW-CRBMs are capable of accurately estimating, recognizing, and performing near-future prediction of three-dimensional trajectories from their 2D projections while requiring limited amount of labeled data. We evaluate our method on both simulated and real-world data, showing its effectiveness in predicting and classifying complex ball trajectories and human activities.



### Local Binary Pattern for Word Spotting in Handwritten Historical Document
- **Arxiv ID**: http://arxiv.org/abs/1604.05907v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.05907v2)
- **Published**: 2016-04-20 11:58:36+00:00
- **Updated**: 2016-04-21 09:50:47+00:00
- **Authors**: Sounak Dey, Anguelos Nicolaou, Josep Llados, Umapada Pal
- **Comment**: None
- **Journal**: None
- **Summary**: Digital libraries store images which can be highly degraded and to index this kind of images we resort to word spot- ting as our information retrieval system. Information retrieval for handwritten document images is more challenging due to the difficulties in complex layout analysis, large variations of writing styles, and degradation or low quality of historical manuscripts. This paper presents a simple innovative learning-free method for word spotting from large scale historical documents combining Local Binary Pattern (LBP) and spatial sampling. This method offers three advantages: firstly, it operates in completely learning free paradigm which is very different from unsupervised learning methods, secondly, the computational time is significantly low because of the LBP features which are very fast to compute, and thirdly, the method can be used in scenarios where annotations are not available. Finally we compare the results of our proposed retrieval method with the other methods in the literature.



### Jansen-MIDAS: a multi-level photomicrograph segmentation software based on isotropic undecimated wavelets
- **Arxiv ID**: http://arxiv.org/abs/1604.05921v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1604.05921v2)
- **Published**: 2016-04-20 12:40:50+00:00
- **Updated**: 2016-05-25 10:21:28+00:00
- **Authors**: Alexandre Fioravante de Siqueira, Fl√°vio Camargo Cabrera, Wagner Massayuki Nakasuga, Aylton Pagamisse, Aldo Eloizo Job
- **Comment**: arXiv version: 25 pages, 10 figures
- **Journal**: None
- **Summary**: Image segmentation, the process of separating the elements within an image, is frequently used for obtaining information from photomicrographs. However, segmentation methods should be used with reservations: incorrect segmentation can mislead when interpreting regions of interest (ROI), thus decreasing the success rate of additional procedures. Multi-Level Starlet Segmentation (MLSS) and Multi-Level Starlet Optimal Segmentation (MLSOS) were developed to address the photomicrograph segmentation deficiency on general tools. These methods gave rise to Jansen-MIDAS, an open-source software which a scientist can use to obtain a multi-level threshold segmentation of his/hers photomicrographs. This software is presented in two versions: a text-based version, for GNU Octave, and a graphical user interface (GUI) version, for MathWorks MATLAB. It can be used to process several types of images, becoming a reliable alternative to the scientist.



### Parametric Object Motion from Blur
- **Arxiv ID**: http://arxiv.org/abs/1604.05933v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.05933v1)
- **Published**: 2016-04-20 13:00:30+00:00
- **Updated**: 2016-04-20 13:00:30+00:00
- **Authors**: Jochen Gast, Anita Sellent, Stefan Roth
- **Comment**: Accepted to IEEE Conference on Computer Vision and Pattern
  Recognition 2016. Includes supplemental material
- **Journal**: None
- **Summary**: Motion blur can adversely affect a number of vision tasks, hence it is generally considered a nuisance. We instead treat motion blur as a useful signal that allows to compute the motion of objects from a single image. Drawing on the success of joint segmentation and parametric motion models in the context of optical flow estimation, we propose a parametric object motion model combined with a segmentation mask to exploit localized, non-uniform motion blur. Our parametric image formation model is differentiable w.r.t. the motion parameters, which enables us to generalize marginal-likelihood techniques from uniform blind deblurring to localized, non-uniform blur. A two-stage pipeline, first in derivative space and then in image space, allows to estimate both parametric object motion as well as a motion segmentation from a single image alone. Our experiments demonstrate its ability to cope with very challenging cases of object motion blur.



### Labeled Multi-Bernoulli Tracking for Industrial Mobile Platform Safety
- **Arxiv ID**: http://arxiv.org/abs/1604.05966v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.05966v2)
- **Published**: 2016-04-20 14:00:09+00:00
- **Updated**: 2016-05-11 00:00:09+00:00
- **Authors**: Tharindu Rathnayake, Reza Hoseinnezhad, Ruwan Tennakoon, Alireza Bab-Hadiashar
- **Comment**: The conference which this paper was submitted, has rejected this
  paper. Thus, we are in the process of enhancing the content of the paper and
  submit it to another conference/journal
- **Journal**: None
- **Summary**: This paper presents a track-before-detect labeled multi-Bernoulli filter tailored for industrial mobile platform safety applications. We derive two application specific separable likelihood functions that capture the geometric shape and colour information of the human targets who are wearing a high visible vest. These likelihoods are then used in a labeled multi-Bernoulli filter with a novel two step Bayesian update. Preliminary simulation results show that the proposed solution can successfully track human workers wearing a luminous yellow colour vest in an industrial environment.



### Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation
- **Arxiv ID**: http://arxiv.org/abs/1604.06057v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1604.06057v2)
- **Published**: 2016-04-20 18:47:48+00:00
- **Updated**: 2016-05-31 14:45:58+00:00
- **Authors**: Tejas D. Kulkarni, Karthik R. Narasimhan, Ardavan Saeedi, Joshua B. Tenenbaum
- **Comment**: 14 pages, 7 figures
- **Journal**: None
- **Summary**: Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'.



### Symmetry-aware Depth Estimation using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1604.06079v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.06079v2)
- **Published**: 2016-04-20 19:50:52+00:00
- **Updated**: 2016-06-09 22:39:49+00:00
- **Authors**: Guilin Liu, Chao Yang, Zimo Li, Duygu Ceylan, Qixing Huang
- **Comment**: 19 pages
- **Journal**: None
- **Summary**: Due to the abundance of 2D product images from the Internet, developing efficient and scalable algorithms to recover the missing depth information is central to many applications. Recent works have addressed the single-view depth estimation problem by utilizing convolutional neural networks. In this paper, we show that exploring symmetry information, which is ubiquitous in man made objects, can significantly boost the quality of such depth predictions. Specifically, we propose a new convolutional neural network architecture to first estimate dense symmetric correspondences in a product image and then propose an optimization which utilizes this information explicitly to significantly improve the quality of single-view depth estimations. We have evaluated our approach extensively, and experimental results show that this approach outperforms state-of-the-art depth estimation techniques.



### Automatic Graphic Logo Detection via Fast Region-based Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1604.06083v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.06083v1)
- **Published**: 2016-04-20 19:54:01+00:00
- **Updated**: 2016-04-20 19:54:01+00:00
- **Authors**: Gon√ßalo Oliveira, Xavier Fraz√£o, Andr√© Pimentel, Bernardete Ribeiro
- **Comment**: 7 pages, 9 figures, IJCNN 2016
- **Journal**: None
- **Summary**: Brand recognition is a very challenging topic with many useful applications in localization recognition, advertisement and marketing. In this paper we present an automatic graphic logo detection system that robustly handles unconstrained imaging conditions. Our approach is based on Fast Region-based Convolutional Networks (FRCN) proposed by Ross Girshick, which have shown state-of-the-art performance in several generic object recognition tasks (PASCAL Visual Object Classes challenges). In particular, we use two CNN models pre-trained with the ILSVRC ImageNet dataset and we look at the selective search of windows `proposals' in the pre-processing stage and data augmentation to enhance the logo recognition rate. The novelty lies in the use of transfer learning to leverage powerful Convolutional Neural Network models trained with large-scale datasets and repurpose them in the context of graphic logo detection. Another benefit of this framework is that it allows for multiple detections of graphic logos using regions that are likely to have an object. Experimental results with the FlickrLogos-32 dataset show not only the promising performance of our developed models with respect to noise and other transformations a graphic logo can be subject to, but also its superiority over state-of-the-art systems with hand-crafted models and features.



### Network of Experts for Large-Scale Image Categorization
- **Arxiv ID**: http://arxiv.org/abs/1604.06119v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.06119v3)
- **Published**: 2016-04-20 20:34:44+00:00
- **Updated**: 2017-04-19 16:25:14+00:00
- **Authors**: Karim Ahmed, Mohammad Haris Baig, Lorenzo Torresani
- **Comment**: ECCV 2016
- **Journal**: None
- **Summary**: We present a tree-structured network architecture for large scale image classification. The trunk of the network contains convolutional layers optimized over all classes. At a given depth, the trunk splits into separate branches, each dedicated to discriminate a different subset of classes. Each branch acts as an expert classifying a set of categories that are difficult to tell apart, while the trunk provides common knowledge to all experts in the form of shared features. The training of our "network of experts" is completely end-to-end: the partition of categories into disjoint subsets is learned simultaneously with the parameters of the network trunk and the experts are trained jointly by minimizing a single learning objective over all classes. The proposed structure can be built from any existing convolutional neural network (CNN). We demonstrate its generality by adapting 4 popular CNNs for image categorization into the form of networks of experts. Our experiments on CIFAR100 and ImageNet show that in every case our method yields a substantial improvement in accuracy over the base CNN, and gives the best result achieved so far on CIFAR100. Finally, the improvement in accuracy comes at little additional cost: compared to the base network, the training time is only moderately increased and the number of parameters is comparable or in some cases even lower.



