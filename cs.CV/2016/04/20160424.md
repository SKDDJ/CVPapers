# Arxiv Papers in cs.CV on 2016-04-24
### Bayesian Inference of Recursive Sequences of Group Activities from Tracks
- **Arxiv ID**: http://arxiv.org/abs/1604.06970v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1604.06970v1)
- **Published**: 2016-04-24 00:55:27+00:00
- **Updated**: 2016-04-24 00:55:27+00:00
- **Authors**: Ernesto Brau, Colin Dawson, Alfredo Carrillo, David Sidi, Clayton T. Morrison
- **Comment**: 10 pages, 6 figures, in Proceedings of the 30th AAAI Conference on
  Artificial Intelligence (AAAI'16), Phoenix, AZ, 2016
- **Journal**: None
- **Summary**: We present a probabilistic generative model for inferring a description of coordinated, recursively structured group activities at multiple levels of temporal granularity based on observations of individuals' trajectories. The model accommodates: (1) hierarchically structured groups, (2) activities that are temporally and compositionally recursive, (3) component roles assigning different subactivity dynamics to subgroups of participants, and (4) a nonparametric Gaussian Process model of trajectories. We present an MCMC sampling framework for performing joint inference over recursive activity descriptions and assignment of trajectories to groups, integrating out continuous parameters. We demonstrate the model's expressive power in several simulated and complex real-world scenarios from the VIRAT and UCLA Aerial Event video data sets.



### Cardiac Motion Analysis by Temporal Flow Graphs
- **Arxiv ID**: http://arxiv.org/abs/1604.06979v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.06979v1)
- **Published**: 2016-04-24 03:35:15+00:00
- **Updated**: 2016-04-24 03:35:15+00:00
- **Authors**: V S R Veeravasarapu, Jayanthi Sivaswamy, Vishanji Karani
- **Comment**: None
- **Journal**: None
- **Summary**: Cardiac motion analysis from B-mode ultrasound sequence is a key task in assessing the health of the heart. The paper proposes a new methodology for cardiac motion analysis based on the temporal behaviour of points of interest on the myocardium. We define a new signal called the Temporal Flow Graph (TFG) which depicts the movement of a point of interest over time. It is a graphical representation derived from a flow field and describes the temporal evolution of a point. We prove that TFG for an object undergoing periodic motion is also periodic. This principle can be utilized to derive both global and local information from a given sequence. We demonstrate this for detecting motion irregularities at the sequence, as well as regional levels on real and synthetic data. A coarse localisation of anatomical landmarks such as centres of left/right cavities and valve points is also demonstrated using TFGs.



### Towards Better Analysis of Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1604.07043v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.07043v3)
- **Published**: 2016-04-24 15:53:22+00:00
- **Updated**: 2016-05-04 08:17:19+00:00
- **Authors**: Mengchen Liu, Jiaxin Shi, Zhen Li, Chongxuan Li, Jun Zhu, Shixia Liu
- **Comment**: Submitted to VIS 2016
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) have achieved breakthrough performance in many pattern recognition tasks such as image classification. However, the development of high-quality deep models typically relies on a substantial amount of trial-and-error, as there is still no clear understanding of when and why a deep model works. In this paper, we present a visual analytics approach for better understanding, diagnosing, and refining deep CNNs. We formulate a deep CNN as a directed acyclic graph. Based on this formulation, a hybrid visualization is developed to disclose the multiple facets of each neuron and the interactions between them. In particular, we introduce a hierarchical rectangle packing algorithm and a matrix reordering algorithm to show the derived features of a neuron cluster. We also propose a biclustering-based edge bundling method to reduce visual clutter caused by a large number of connections between neurons. We evaluated our method on a set of CNNs and the results are generally favorable.



### Rotation-Invariant Restricted Boltzmann Machine Using Shared Gradient Filters
- **Arxiv ID**: http://arxiv.org/abs/1604.07045v2
- **DOI**: 10.1007/978-3-319-44781-0_57
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.07045v2)
- **Published**: 2016-04-24 15:56:18+00:00
- **Updated**: 2016-06-23 09:59:47+00:00
- **Authors**: Mario Valerio Giuffrida, Sotirios A. Tsaftaris
- **Comment**: 8 pages, 3 figures, 1 table
- **Journal**: None
- **Summary**: Finding suitable features has been an essential problem in computer vision. We focus on Restricted Boltzmann Machines (RBMs), which, despite their versatility, cannot accommodate transformations that may occur in the scene. As a result, several approaches have been proposed that consider a set of transformations, which are used to either augment the training set or transform the actual learned filters. In this paper, we propose the Explicit Rotation-Invariant Restricted Boltzmann Machine, which exploits prior information coming from the dominant orientation of images. Our model extends the standard RBM, by adding a suitable number of weight matrices, associated with each dominant gradient. We show that our approach is able to learn rotation-invariant features, comparing it with the classic formulation of RBM on the MNIST benchmark dataset. Overall, requiring less hidden units, our method learns compact features, which are robust to rotations.



### Multi-Fold Gabor, PCA and ICA Filter Convolution Descriptor for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1604.07057v3
- **DOI**: 10.1109/TCSVT.2017.2761829
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.07057v3)
- **Published**: 2016-04-24 17:30:34+00:00
- **Updated**: 2017-10-19 06:39:45+00:00
- **Authors**: Cheng Yaw Low, Andrew Beng Jin Teoh, Cong Jie Ng
- **Comment**: 14 pages, 10 figures, 10 tables
- **Journal**: None
- **Summary**: This paper devises a new means of filter diversification, dubbed multi-fold filter convolution (M-FFC), for face recognition. On the assumption that M-FFC receives single-scale Gabor filters of varying orientations as input, these filters are self-cross convolved by M-fold to instantiate a filter offspring set. The M-FFC flexibility also permits cross convolution amongst Gabor filters and other filter banks of profoundly dissimilar traits, e.g., principal component analysis (PCA) filters, and independent component analysis (ICA) filters. The 2-FFC of Gabor, PCA and ICA filters thus yields three offspring sets: (1) Gabor filters solely, (2) Gabor-PCA filters, and (3) Gabor-ICA filters, to render the learning-free and the learning-based 2-FFC descriptors. To facilitate a sensible Gabor filter selection for M-FFC, the 40 multi-scale, multi-orientation Gabor filters are condensed into 8 elementary filters. Aside from that, an average histogram pooling operator is employed to leverage the 2-FFC histogram features, prior to the final whitening PCA compression. The empirical results substantiate that the 2-FFC descriptors prevail over, or on par with, other face descriptors on both identification and verification tasks.



### Binary Codes for Tagging X-Ray Images via Deep De-Noising Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1604.07060v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.07060v1)
- **Published**: 2016-04-24 17:44:30+00:00
- **Updated**: 2016-04-24 17:44:30+00:00
- **Authors**: Antonio Sze-To, Hamid R. Tizhoosh, Andrew K. C. Wong
- **Comment**: To appear in proceedings of The 2016 International Joint Conference
  on Neural Networks (IJCNN 2016), July 24-29, 2016, Vancouver, Canada
- **Journal**: None
- **Summary**: A Content-Based Image Retrieval (CBIR) system which identifies similar medical images based on a query image can assist clinicians for more accurate diagnosis. The recent CBIR research trend favors the construction and use of binary codes to represent images. Deep architectures could learn the non-linear relationship among image pixels adaptively, allowing the automatic learning of high-level features from raw pixels. However, most of them require class labels, which are expensive to obtain, particularly for medical images. The methods which do not need class labels utilize a deep autoencoder for binary hashing, but the code construction involves a specific training algorithm and an ad-hoc regularization technique. In this study, we explored using a deep de-noising autoencoder (DDA), with a new unsupervised training scheme using only backpropagation and dropout, to hash images into binary codes. We conducted experiments on more than 14,000 x-ray images. By using class labels only for evaluating the retrieval results, we constructed a 16-bit DDA and a 512-bit DDA independently. Comparing to other unsupervised methods, we succeeded to obtain the lowest total error by using the 512-bit codes for retrieval via exhaustive search, and speed up 9.27 times with the use of the 16-bit codes while keeping a comparable total error. We found that our new training scheme could reduce the total retrieval error significantly by 21.9%. To further boost the image retrieval performance, we developed Radon Autoencoder Barcode (RABC) which are learned from the Radon projections of images using a de-noising autoencoder. Experimental results demonstrated its superior performance in retrieval when it was combined with DDA binary codes.



### A Review of Co-saliency Detection Technique: Fundamentals, Applications, and Challenges
- **Arxiv ID**: http://arxiv.org/abs/1604.07090v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.07090v5)
- **Published**: 2016-04-24 22:36:38+00:00
- **Updated**: 2017-07-09 15:44:10+00:00
- **Authors**: Dingwen Zhang, Huazhu Fu, Junwei Han, Ali Borji, Xuelong Li
- **Comment**: 28 pages, 12 figures, 3 tables
- **Journal**: None
- **Summary**: Co-saliency detection is a newly emerging and rapidly growing research area in computer vision community. As a novel branch of visual saliency, co-saliency detection refers to the discovery of common and salient foregrounds from two or more relevant images, and can be widely used in many computer vision tasks. The existing co-saliency detection algorithms mainly consist of three components: extracting effective features to represent the image regions, exploring the informative cues or factors to characterize co-saliency, and designing effective computational frameworks to formulate co-saliency. Although numerous methods have been developed, the literature is still lacking a deep review and evaluation of co-saliency detection techniques. In this paper, we aim at providing a comprehensive review of the fundamentals, challenges, and applications of co-saliency detection. Specifically, we provide an overview of some related computer vision works, review the history of co-saliency detection, summarize and categorize the major algorithms in this research area, discuss some open issues in this area, present the potential applications of co-saliency detection, and finally point out some unsolved challenges and promising future works. We expect this review to be beneficial to both fresh and senior researchers in this field, and give insights to researchers in other related areas regarding the utility of co-saliency detection algorithms.



### Semi-supervised Vocabulary-informed Learning
- **Arxiv ID**: http://arxiv.org/abs/1604.07093v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1604.07093v1)
- **Published**: 2016-04-24 23:36:36+00:00
- **Updated**: 2016-04-24 23:36:36+00:00
- **Authors**: Yanwei Fu, Leonid Sigal
- **Comment**: 10 pages, Accepted by CVPR 2016 as an oral presentation
- **Journal**: None
- **Summary**: Despite significant progress in object categorization, in recent years, a number of important challenges remain, mainly, ability to learn from limited labeled data and ability to recognize object classes within large, potentially open, set of labels. Zero-shot learning is one way of addressing these challenges, but it has only been shown to work with limited sized class vocabularies and typically requires separation between supervised and unsupervised classes, allowing former to inform the latter but not vice versa. We propose the notion of semi-supervised vocabulary-informed learning to alleviate the above mentioned challenges and address problems of supervised, zero-shot and open set recognition using a unified framework. Specifically, we propose a maximum margin framework for semantic manifold-based recognition that incorporates distance constraints from (both supervised and unsupervised) vocabulary atoms, ensuring that labeled samples are projected closest to their correct prototypes, in the embedding space, than to others. We show that resulting model shows improvements in supervised, zero-shot, and large open set recognition, with up to 310K class vocabulary on AwA and ImageNet datasets.



