# Arxiv Papers in cs.CV on 2016-04-26
### Modern Physiognomy: An Investigation on Predicting Personality Traits and Intelligence from the Human Face
- **Arxiv ID**: http://arxiv.org/abs/1604.07499v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.07499v1)
- **Published**: 2016-04-26 02:58:08+00:00
- **Updated**: 2016-04-26 02:58:08+00:00
- **Authors**: Rizhen Qin, Wei Gao, Huarong Xu, Zhanyi Hu
- **Comment**: None
- **Journal**: None
- **Summary**: The human behavior of evaluating other individuals with respect to their personality traits and intelligence by evaluating their faces plays a crucial role in human relations. These trait judgments might influence important social outcomes in our lives such as elections and court sentences. Previous studies have reported that human can make valid inferences for at least four personality traits. In addition, some studies have demonstrated that facial trait evaluation can be learned using machine learning methods accurately. In this work, we experimentally explore whether self-reported personality traits and intelligence can be predicted reliably from a facial image. More specifically, the prediction problem is separately cast in two parts: a classification task and a regression task. A facial structural feature is constructed from the relations among facial salient points, and an appearance feature is built by five texture descriptors. In addition, a minutia-based fingerprint feature from a fingerprint image is also explored. The classification results show that the personality traits "Rule-consciousness" and "Vigilance" can be predicted reliably, and that the traits of females can be predicted more accurately than those of male. However, the regression experiments show that it is difficult to predict scores for individual personality traits and intelligence. The residual plots and the correlation results indicate no evident linear correlation between the measured scores and the predicted scores. Both the classification and the regression results reveal that "Rule-consciousness" and "Tension" can be reliably predicted from the facial features, while "Social boldness" gets the worst prediction results. The experiments results show that it is difficult to predict intelligence from either the facial features or the fingerprint feature, a finding that is in agreement with previous studies.



### Once for All: a Two-flow Convolutional Neural Network for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1604.07507v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.07507v1)
- **Published**: 2016-04-26 03:34:39+00:00
- **Updated**: 2016-04-26 03:34:39+00:00
- **Authors**: Kai Chen, Wenbing Tao
- **Comment**: None
- **Journal**: None
- **Summary**: One of the main challenges of visual object tracking comes from the arbitrary appearance of objects. Most existing algorithms try to resolve this problem as an object-specific task, i.e., the model is trained to regenerate or classify a specific object. As a result, the model need to be initialized and retrained for different objects. In this paper, we propose a more generic approach utilizing a novel two-flow convolutional neural network (named YCNN). The YCNN takes two inputs (one is object image patch, the other is search image patch), then outputs a response map which predicts how likely the object appears in a specific location. Unlike those object-specific approach, the YCNN is trained to measure the similarity between two image patches. Thus it will not be confined to any specific object. Furthermore the network can be end-to-end trained to extract both shallow and deep convolutional features which are dedicated for visual tracking. And once properly trained, the YCNN can be applied to track all kinds of objects without further training and updating. Benefiting from the once-for-all model, our algorithm is able to run at a very high speed of 45 frames-per-second. The experiments on 51 sequences also show that our algorithm achieves an outstanding performance.



### Semantic Change Detection with Hypermaps
- **Arxiv ID**: http://arxiv.org/abs/1604.07513v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1604.07513v2)
- **Published**: 2016-04-26 04:31:31+00:00
- **Updated**: 2017-03-16 01:46:37+00:00
- **Authors**: Teppei Suzuki, Soma Shirakabe, Yudai Miyashita, Akio Nakamura, Yutaka Satoh, Hirokatsu Kataoka
- **Comment**: None
- **Journal**: None
- **Summary**: Change detection is the study of detecting changes between two different images of a scene taken at different times. By the detected change areas, however, a human cannot understand how different the two images. Therefore, a semantic understanding is required in the change detection research such as disaster investigation. The paper proposes the concept of semantic change detection, which involves intuitively inserting semantic meaning into detected change areas. We mainly focus on the novel semantic segmentation in addition to a conventional change detection approach. In order to solve this problem and obtain a high-level of performance, we propose an improvement to the hypercolumns representation, hereafter known as hypermaps, which effectively uses convolutional maps obtained from convolutional neural networks (CNNs). We also employ multi-scale feature representation captured by different image patches. We applied our method to the TSUNAMI Panoramic Change Detection dataset, and re-annotated the changed areas of the dataset via semantic classes. The results show that our multi-scale hypermaps provided outstanding performance on the re-annotated TSUNAMI dataset.



### Learning Deep Feature Representations with Domain Guided Dropout for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1604.07528v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.07528v1)
- **Published**: 2016-04-26 05:39:53+00:00
- **Updated**: 2016-04-26 05:39:53+00:00
- **Authors**: Tong Xiao, Hongsheng Li, Wanli Ouyang, Xiaogang Wang
- **Comment**: To appear in CVPR2016
- **Journal**: None
- **Summary**: Learning generic and robust feature representations with data from multiple domains for the same problem is of great value, especially for the problems that have multiple datasets but none of them are large enough to provide abundant data variations. In this work, we present a pipeline for learning deep feature representations from multiple domains with Convolutional Neural Networks (CNNs). When training a CNN with data from all the domains, some neurons learn representations shared across several domains, while some others are effective only for a specific one. Based on this important observation, we propose a Domain Guided Dropout algorithm to improve the feature learning procedure. Experiments show the effectiveness of our pipeline and the proposed algorithm. Our methods on the person re-identification problem outperform state-of-the-art methods on multiple datasets by large margins.



### Towards Miss Universe Automatic Prediction: The Evening Gown Competition
- **Arxiv ID**: http://arxiv.org/abs/1604.07547v2
- **DOI**: 10.1109/ICPR.2016.7899781
- **Categories**: **cs.CV**, cs.CY, cs.MM, 68T45, I.4; I.4.8; I.4.9; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1604.07547v2)
- **Published**: 2016-04-26 07:02:16+00:00
- **Updated**: 2016-09-12 03:22:01+00:00
- **Authors**: Johanna Carvajal, Arnold Wiliem, Conrad Sanderson, Brian Lovell
- **Comment**: None
- **Journal**: International Conference on Pattern Recognition, 2016
- **Summary**: Can we predict the winner of Miss Universe after watching how they stride down the catwalk during the evening gown competition? Fashion gurus say they can! In our work, we study this question from the perspective of computer vision. In particular, we want to understand whether existing computer vision approaches can be used to automatically extract the qualities exhibited by the Miss Universe winners during their catwalk. This study can pave the way towards new vision-based applications for the fashion industry. To this end, we propose a novel video dataset, called the Miss Universe dataset, comprising 10 years of the evening gown competition selected between 1996-2010. We further propose two ranking-related problems: (1) Miss Universe Listwise Ranking and (2) Miss Universe Pairwise Ranking. In addition, we also develop an approach that simultaneously addresses the two proposed problems. To describe the videos we employ the recently proposed Stacked Fisher Vectors in conjunction with robust local spatio-temporal features. From our evaluation we found that although the addressed problems are extremely challenging, the proposed system is able to rank the winner in the top 3 best predicted scores for 5 out of 10 Miss Universe competitions.



### A New Approach in Persian Handwritten Letters Recognition Using Error Correcting Output Coding
- **Arxiv ID**: http://arxiv.org/abs/1604.07554v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1604.07554v1)
- **Published**: 2016-04-26 07:43:59+00:00
- **Updated**: 2016-04-26 07:43:59+00:00
- **Authors**: Maziar Kazemi, Muhammad Yousefnezhad, Saber Nourian
- **Comment**: Journal of Advances in Computer Research
- **Journal**: None
- **Summary**: Classification Ensemble, which uses the weighed polling of outputs, is the art of combining a set of basic classifiers for generating high-performance, robust and more stable results. This study aims to improve the results of identifying the Persian handwritten letters using Error Correcting Output Coding (ECOC) ensemble method. Furthermore, the feature selection is used to reduce the costs of errors in our proposed method. ECOC is a method for decomposing a multi-way classification problem into many binary classification tasks; and then combining the results of the subtasks into a hypothesized solution to the original problem. Firstly, the image features are extracted by Principal Components Analysis (PCA). After that, ECOC is used for identification the Persian handwritten letters which it uses Support Vector Machine (SVM) as the base classifier. The empirical results of applying this ensemble method using 10 real-world data sets of Persian handwritten letters indicate that this method has better results in identifying the Persian handwritten letters than other ensemble methods and also single classifications. Moreover, by testing a number of different features, this paper found that we can reduce the additional cost in feature selection stage by using this method.



### Spot On: Action Localization from Pointly-Supervised Proposals
- **Arxiv ID**: http://arxiv.org/abs/1604.07602v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.07602v2)
- **Published**: 2016-04-26 10:09:18+00:00
- **Updated**: 2016-07-25 20:45:18+00:00
- **Authors**: Pascal Mettes, Jan C. van Gemert, Cees G. M. Snoek
- **Comment**: None
- **Journal**: None
- **Summary**: We strive for spatio-temporal localization of actions in videos. The state-of-the-art relies on action proposals at test time and selects the best one with a classifier trained on carefully annotated box annotations. Annotating action boxes in video is cumbersome, tedious, and error prone. Rather than annotating boxes, we propose to annotate actions in video with points on a sparse subset of frames only. We introduce an overlap measure between action proposals and points and incorporate them all into the objective of a non-convex Multiple Instance Learning optimization. Experimental evaluation on the UCF Sports and UCF 101 datasets shows that (i) spatio-temporal proposals can be used to train classifiers while retaining the localization performance, (ii) point annotations yield results comparable to box annotations while being significantly faster to annotate, (iii) with a minimum amount of supervision our approach is competitive to the state-of-the-art. Finally, we introduce spatio-temporal action annotations on the train and test videos of Hollywood2, resulting in Hollywood2Tubes, available at http://tinyurl.com/hollywood2tubes.



### $\ell_p$-Box ADMM: A Versatile Framework for Integer Programming
- **Arxiv ID**: http://arxiv.org/abs/1604.07666v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.DS
- **Links**: [PDF](http://arxiv.org/pdf/1604.07666v3)
- **Published**: 2016-04-26 13:15:17+00:00
- **Updated**: 2016-06-19 12:53:02+00:00
- **Authors**: Baoyuan Wu, Bernard Ghanem
- **Comment**: both authors share first-authorship. Integer programming, Lp-box
  intersection, ADMM
- **Journal**: None
- **Summary**: This paper revisits the integer programming (IP) problem, which plays a fundamental role in many computer vision and machine learning applications. The literature abounds with many seminal works that address this problem, some focusing on continuous approaches (e.g. linear program relaxation) while others on discrete ones (e.g., min-cut). However, a limited number of them are designed to handle the general IP form and even these methods cannot adequately satisfy the simultaneous requirements of accuracy, feasibility, and scalability. To this end, we propose a novel and versatile framework called $\ell_p$-box ADMM, which is based on two parts. (1) The discrete constraint is equivalently replaced by the intersection of a box and a $(n-1)$-dimensional sphere (defined through the $\ell_p$ norm). (2) We infuse this equivalence into the ADMM (Alternating Direction Method of Multipliers) framework to handle these continuous constraints separately and to harness its attractive properties. More importantly, the ADMM update steps can lead to manageable sub-problems in the continuous domain. To demonstrate its efficacy, we consider an instance of the framework, namely $\ell_2$-box ADMM applied to binary quadratic programming (BQP). Here, the ADMM steps are simple, computationally efficient, and theoretically guaranteed to converge to a KKT point. We demonstrate the applicability of $\ell_2$-box ADMM on three important applications: MRF energy minimization, graph matching, and clustering. Results clearly show that it significantly outperforms existing generic IP solvers both in runtime and objective. It also achieves very competitive performance vs. state-of-the-art methods specific to these applications.



### Real-time Action Recognition with Enhanced Motion Vector CNNs
- **Arxiv ID**: http://arxiv.org/abs/1604.07669v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.07669v1)
- **Published**: 2016-04-26 13:21:37+00:00
- **Updated**: 2016-04-26 13:21:37+00:00
- **Authors**: Bowen Zhang, Limin Wang, Zhe Wang, Yu Qiao, Hanli Wang
- **Comment**: accepted by CVPR16
- **Journal**: None
- **Summary**: The deep two-stream architecture exhibited excellent performance on video based action recognition. The most computationally expensive step in this approach comes from the calculation of optical flow which prevents it to be real-time. This paper accelerates this architecture by replacing optical flow with motion vector which can be obtained directly from compressed videos without extra calculation. However, motion vector lacks fine structures, and contains noisy and inaccurate motion patterns, leading to the evident degradation of recognition performance. Our key insight for relieving this problem is that optical flow and motion vector are inherent correlated. Transferring the knowledge learned with optical flow CNN to motion vector CNN can significantly boost the performance of the latter. Specifically, we introduce three strategies for this, initialization transfer, supervision transfer and their combination. Experimental results show that our method achieves comparable recognition performance to the state-of-the-art, while our method can process 390.7 frames per second, which is 27 times faster than the original two-stream method.



### Efficient Splitting-based Method for Global Image Smoothing
- **Arxiv ID**: http://arxiv.org/abs/1604.07681v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.07681v1)
- **Published**: 2016-04-26 14:05:41+00:00
- **Updated**: 2016-04-26 14:05:41+00:00
- **Authors**: Youngjung Kim, Dongbo Min, Bumsub Ham, Kwanghoon Sohn
- **Comment**: None
- **Journal**: None
- **Summary**: Edge-preserving smoothing (EPS) can be formulated as minimizing an objective function that consists of data and prior terms. This global EPS approach shows better smoothing performance than a local one that typically has a form of weighted averaging, at the price of high computational cost. In this paper, we introduce a highly efficient splitting-based method for global EPS that minimizes the objective function of ${l_2}$ data and prior terms (possibly non-smooth and non-convex) in linear time. Different from previous splitting-based methods that require solving a large linear system, our approach solves an equivalent constrained optimization problem, resulting in a sequence of 1D sub-problems. This enables linear time solvers for weighted-least squares and -total variation problems. Our solver converges quickly, and its runtime is even comparable to state-of-the-art local EPS approaches. We also propose a family of fast iteratively re-weighted algorithms using a non-convex prior term. Experimental results demonstrate the effectiveness and flexibility of our approach in a range of computer vision and image processing tasks.



### EgoSampling: Wide View Hyperlapse from Egocentric Videos
- **Arxiv ID**: http://arxiv.org/abs/1604.07741v2
- **DOI**: 10.1109/TCSVT.2017.2651051
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1604.07741v2)
- **Published**: 2016-04-26 16:25:24+00:00
- **Updated**: 2017-01-12 15:12:19+00:00
- **Authors**: Tavi Halperin, Yair Poleg, Chetan Arora, Shmuel Peleg
- **Comment**: Accepted for publication in IEEE Transactions on Circuits and Systems
  for Video Technology (TCSVT)
- **Journal**: None
- **Summary**: The possibility of sharing one's point of view makes use of wearable cameras compelling. These videos are often long, boring and coupled with extreme shake, as the camera is worn on a moving person. Fast forwarding (i.e. frame sampling) is a natural choice for quick video browsing. However, this accentuates the shake caused by natural head motion in an egocentric video, making the fast forwarded video useless. We propose EgoSampling, an adaptive frame sampling that gives stable, fast forwarded, hyperlapse videos. Adaptive frame sampling is formulated as an energy minimization problem, whose optimal solution can be found in polynomial time. We further turn the camera shake from a drawback into a feature, enabling the increase in field-of-view of the output video. This is obtained when each output frame is mosaiced from several input frames. The proposed technique also enables the generation of a single hyperlapse video from multiple egocentric videos, allowing even faster video consumption.



### Compressive phase-only filtering at extreme compression rates
- **Arxiv ID**: http://arxiv.org/abs/1604.07751v5
- **DOI**: 10.1016/j.optcom.2016.09.024
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1604.07751v5)
- **Published**: 2016-04-26 16:49:58+00:00
- **Updated**: 2016-09-29 06:43:39+00:00
- **Authors**: David Pastor-Calle, Anna Pastuszczak, Michal Mikolajczyk, Rafal Kotynski
- **Comment**: None
- **Journal**: Opt. Commun. vol. 383, pp. 446-452, (2017)
- **Summary**: We introduce an efficient method for the reconstruction of the correlation between a compressively measured image and a phase-only filter. The proposed method is based on two properties of phase-only filtering: such filtering is a unitary circulant transform, and the correlation plane it produces is usually sparse. Thanks to these properties, phase-only filters are perfectly compatible with the framework of compressive sensing. Moreover, the lasso-based recovery algorithm is very fast when phase-only filtering is used as the compression matrix. The proposed method can be seen as a generalisation of the correlation-based pattern recognition technique, which is hereby applied directly to non-adaptively acquired compressed data. At the time of measurement, any prior knowledge of the target object for which the data will be scanned is not required. We show that images measured at extremely high compression rates may still contain sufficient information for target classification and localization, even if the compression rate is high enough, that visual recognition of the target in the reconstructed image is no longer possible. The method has been applied by us to highly undersampled measurements obtained from a single-pixel camera, with sampling based on randomly chosen Walsh-Hadamard patterns.



### A Framework for Human Pose Estimation in Videos
- **Arxiv ID**: http://arxiv.org/abs/1604.07788v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.07788v1)
- **Published**: 2016-04-26 18:45:25+00:00
- **Updated**: 2016-04-26 18:45:25+00:00
- **Authors**: Dong Zhang, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a method to estimate a sequence of human poses in unconstrained videos. We aim to demonstrate that by using temporal information, the human pose estimation results can be improved over image based pose estimation methods. In contrast to the commonly employed graph optimization formulation, which is NP-hard and needs approximate solutions, we formulate this problem into a unified two stage tree-based optimization problem for which an efficient and exact solution exists. Although the proposed method finds an exact solution, it does not sacrifice the ability to model the spatial and temporal constraints between body parts in the frames; in fact it models the {\em symmetric} parts better than the existing methods. The proposed method is based on two main ideas: `Abstraction' and `Association' to enforce the intra- and inter-frame body part constraints without inducing extra computational complexity to the polynomial time solution. Using the idea of `Abstraction', a new concept of `abstract body part' is introduced to conceptually combine the symmetric body parts and model them in the tree based body part structure. Using the idea of `Association', the optimal tracklets are generated for each abstract body part, in order to enforce the spatiotemporal constraints between body parts in adjacent frames. A sequence of the best poses is inferred from the abstract body part tracklets through the tree-based optimization. Finally, the poses are refined by limb alignment and refinement schemes. We evaluated the proposed method on three publicly available video based human pose estimation datasets, and obtained dramatically improved performance compared to the state-of-the-art methods.



### An Enhanced Deep Feature Representation for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1604.07807v2
- **DOI**: 10.1109/WACV.2016.7477681
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.07807v2)
- **Published**: 2016-04-26 19:27:50+00:00
- **Updated**: 2016-04-28 08:17:35+00:00
- **Authors**: Shangxuan Wu, Ying-Cong Chen, Xiang Li, An-Cong Wu, Jin-Jie You, Wei-Shi Zheng
- **Comment**: Citation for this paper: Shangxuan Wu, Ying-Cong Chen, Xiang Li,
  An-Cong Wu, Jin-Jie You, and Wei-Shi Zheng. An Enhanced Deep Feature
  Representation for Person Re-identification. In IEEE WACV, 2016
- **Journal**: None
- **Summary**: Feature representation and metric learning are two critical components in person re-identification models. In this paper, we focus on the feature representation and claim that hand-crafted histogram features can be complementary to Convolutional Neural Network (CNN) features. We propose a novel feature extraction model called Feature Fusion Net (FFN) for pedestrian image representation. In FFN, back propagation makes CNN features constrained by the handcrafted features. Utilizing color histogram features (RGB, HSV, YCbCr, Lab and YIQ) and texture features (multi-scale and multi-orientation Gabor features), we get a new deep feature representation that is more discriminative and compact. Experiments on three challenging datasets (VIPeR, CUHK01, PRID450s) validates the effectiveness of our proposal.



### Learning by tracking: Siamese CNN for robust target association
- **Arxiv ID**: http://arxiv.org/abs/1604.07866v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1604.07866v3)
- **Published**: 2016-04-26 21:42:51+00:00
- **Updated**: 2016-08-04 15:01:36+00:00
- **Authors**: Laura Leal-Taixé, Cristian Canton Ferrer, Konrad Schindler
- **Comment**: None
- **Journal**: Computer Vision and Pattern Recognition Conference Workshops
  (CVPRW). DeepVision: Deep Learning for Computer Vision. 2016
- **Summary**: This paper introduces a novel approach to the task of data association within the context of pedestrian tracking, by introducing a two-stage learning scheme to match pairs of detections. First, a Siamese convolutional neural network (CNN) is trained to learn descriptors encoding local spatio-temporal structures between the two input image patches, aggregating pixel values and optical flow information. Second, a set of contextual features derived from the position and size of the compared input patches are combined with the CNN output by means of a gradient boosting classifier to generate the final matching probability. This learning approach is validated by using a linear programming based multi-person tracker showing that even a simple and efficient tracker may outperform much more complex models when fed with our learned matching probabilities. Results on publicly available sequences show that our method meets state-of-the-art standards in multiple people tracking.



### Are Face and Object Recognition Independent? A Neurocomputational Modeling Exploration
- **Arxiv ID**: http://arxiv.org/abs/1604.07872v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1604.07872v1)
- **Published**: 2016-04-26 22:04:22+00:00
- **Updated**: 2016-04-26 22:04:22+00:00
- **Authors**: Panqu Wang, Isabel Gauthier, Garrison Cottrell
- **Comment**: None
- **Journal**: Journal of Cognitive Neuroscience, 28(4):558-574. 2016
- **Summary**: Are face and object recognition abilities independent? Although it is commonly believed that they are, Gauthier et al.(2014) recently showed that these abilities become more correlated as experience with nonface categories increases. They argued that there is a single underlying visual ability, v, that is expressed in performance with both face and nonface categories as experience grows. Using the Cambridge Face Memory Test and the Vanderbilt Expertise Test, they showed that the shared variance between Cambridge Face Memory Test and Vanderbilt Expertise Test performance increases monotonically as experience increases. Here, we address why a shared resource across different visual domains does not lead to competition and to an inverse correlation in abilities? We explain this conundrum using our neurocomputational model of face and object processing (The Model, TM). Our results show that, as in the behavioral data, the correlation between subordinate level face and object recognition accuracy increases as experience grows. We suggest that different domains do not compete for resources because the relevant features are shared between faces and objects. The essential power of experience is to generate a "spreading transform" for faces that generalizes to objects that must be individuated. Interestingly, when the task of the network is basic level categorization, no increase in the correlation between domains is observed. Hence, our model predicts that it is the type of experience that matters and that the source of the correlation is in the fusiform face area, rather than in cortical areas that subserve basic level categorization. This result is consistent with our previous modeling elucidating why the FFA is recruited for novel domains of expertise (Tong et al., 2008).



