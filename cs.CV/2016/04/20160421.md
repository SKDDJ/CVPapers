# Arxiv Papers in cs.CV on 2016-04-21
### Deep Adaptive Network: An Efficient Deep Neural Network with Sparse Binary Connections
- **Arxiv ID**: http://arxiv.org/abs/1604.06154v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1604.06154v1)
- **Published**: 2016-04-21 01:47:33+00:00
- **Updated**: 2016-04-21 01:47:33+00:00
- **Authors**: Xichuan Zhou, Shengli Li, Kai Qin, Kunping Li, Fang Tang, Shengdong Hu, Shujun Liu, Zhi Lin
- **Comment**: 10 pages, extended and submitted to IEEE Transactions of Systems,
  Man, and Cybernetics
- **Journal**: None
- **Summary**: Deep neural networks are state-of-the-art models for understanding the content of images, video and raw input data. However, implementing a deep neural network in embedded systems is a challenging task, because a typical deep neural network, such as a Deep Belief Network using 128x128 images as input, could exhaust Giga bytes of memory and result in bandwidth and computing bottleneck. To address this challenge, this paper presents a hardware-oriented deep learning algorithm, named as the Deep Adaptive Network, which attempts to exploit the sparsity in the neural connections. The proposed method adaptively reduces the weights associated with negligible features to zero, leading to sparse feedforward network architecture. Furthermore, since the small proportion of important weights are significantly larger than zero, they can be robustly thresholded and represented using single-bit integers (-1 and +1), leading to implementations of deep neural networks with sparse and binary connections. Our experiments showed that, for the application of recognizing MNIST handwritten digits, the features extracted by a two-layer Deep Adaptive Network with about 25% reserved important connections achieved 97.2% classification accuracy, which was almost the same with the standard Deep Belief Network (97.3%). Furthermore, for efficient hardware implementations, the sparse-and-binary-weighted deep neural network could save about 99.3% memory and 99.9% computation units without significant loss of classification accuracy for pattern recognition applications.



### The THUMOS Challenge on Action Recognition for Videos "in the Wild"
- **Arxiv ID**: http://arxiv.org/abs/1604.06182v1
- **DOI**: 10.1016/j.cviu.2016.10.018
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.06182v1)
- **Published**: 2016-04-21 05:08:59+00:00
- **Updated**: 2016-04-21 05:08:59+00:00
- **Authors**: Haroon Idrees, Amir R. Zamir, Yu-Gang Jiang, Alex Gorban, Ivan Laptev, Rahul Sukthankar, Mubarak Shah
- **Comment**: Preprint submitted to Computer Vision and Image Understanding
- **Journal**: None
- **Summary**: Automatically recognizing and localizing wide ranges of human actions has crucial importance for video understanding. Towards this goal, the THUMOS challenge was introduced in 2013 to serve as a benchmark for action recognition. Until then, video action recognition, including THUMOS challenge, had focused primarily on the classification of pre-segmented (i.e., trimmed) videos, which is an artificial task. In THUMOS 2014, we elevated action recognition to a more practical level by introducing temporally untrimmed videos. These also include `background videos' which share similar scenes and backgrounds as action videos, but are devoid of the specific actions. The three editions of the challenge organized in 2013--2015 have made THUMOS a common benchmark for action classification and detection and the annual challenge is widely attended by teams from around the world.   In this paper we describe the THUMOS benchmark in detail and give an overview of data collection and annotation procedures. We present the evaluation protocols used to quantify results in the two THUMOS tasks of action classification and temporal detection. We also present results of submissions to the THUMOS 2015 challenge and review the participating approaches. Additionally, we include a comprehensive empirical study evaluating the differences in action recognition between trimmed and untrimmed videos, and how well methods trained on trimmed videos generalize to untrimmed videos. We conclude by proposing several directions and improvements for future THUMOS challenges.



### Articulated Hand Pose Estimation Review
- **Arxiv ID**: http://arxiv.org/abs/1604.06195v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.06195v1)
- **Published**: 2016-04-21 06:55:42+00:00
- **Updated**: 2016-04-21 06:55:42+00:00
- **Authors**: Emad Barsoum
- **Comment**: Review of state of the art articulated hand pose estimation since
  2007, this paper was written in May 2015
- **Journal**: None
- **Summary**: With the increase number of companies focusing on commercializing Augmented Reality (AR), Virtual Reality (VR) and wearable devices, the need for a hand based input mechanism is becoming essential in order to make the experience natural, seamless and immersive. Hand pose estimation has progressed drastically in recent years due to the introduction of commodity depth cameras.   Hand pose estimation based on vision is still a challenging problem due to its complexity from self-occlusion (between fingers), close similarity between fingers, dexterity of the hands, speed of the pose and the high dimension of the hand kinematic parameters. Articulated hand pose estimation is still an open problem and under intensive research from both academia and industry.   The 2 approaches used for hand pose estimation are: discriminative and generative. Generative approach is a model based that tries to fit a hand model to the observed data. Discriminative approach is appearance based, usually implemented with machine learning (ML) and require a large amount of training data. Recent hand pose estimation uses hybrid approach by combining both discriminative and generative methods into a single hand pipeline.   In this paper, we focus on reviewing recent progress of hand pose estimation from depth sensor. We will survey discriminative methods, generative methods and hybrid methods. This paper is not a comprehensive review of all hand pose estimation techniques, it is a subset of some of the recent state-of-the-art techniques.



### Incremental Reconstruction of Urban Environments by Edge-Points Delaunay Triangulation
- **Arxiv ID**: http://arxiv.org/abs/1604.06232v2
- **DOI**: 10.1109/IROS.2015.7354012
- **Categories**: **cs.CV**, cs.RO, I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/1604.06232v2)
- **Published**: 2016-04-21 09:59:42+00:00
- **Updated**: 2016-04-27 13:11:03+00:00
- **Authors**: Andrea Romanoni, Matteo Matteucci
- **Comment**: Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International
  Conference on (IROS) 2015. http://hdl.handle.net/11311/972021
- **Journal**: None
- **Summary**: Urban reconstruction from a video captured by a surveying vehicle constitutes a core module of automated mapping. When computational power represents a limited resource and, a detailed map is not the primary goal, the reconstruction can be performed incrementally, from a monocular video, carving a 3D Delaunay triangulation of sparse points; this allows online incremental mapping for tasks such as traversability analysis or obstacle avoidance. To exploit the sharp edges of urban landscape, we propose to use a Delaunay triangulation of Edge-Points, which are the 3D points corresponding to image edges. These points constrain the edges of the 3D Delaunay triangulation to real-world edges. Besides the use of the Edge-Points, a second contribution of this paper is the Inverse Cone Heuristic that preemptively avoids the creation of artifacts in the reconstructed manifold surface. We force the reconstruction of a manifold surface since it makes it possible to apply computer graphics or photometric refinement algorithms to the output mesh. We evaluated our approach on four real sequences of the public available KITTI dataset by comparing the incremental reconstruction against Velodyne measurements.



### Novelty Detection in MultiClass Scenarios with Incomplete Set of Class Labels
- **Arxiv ID**: http://arxiv.org/abs/1604.06242v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.06242v2)
- **Published**: 2016-04-21 10:18:26+00:00
- **Updated**: 2016-05-15 16:44:15+00:00
- **Authors**: Nomi Vinokurov, Daphna Weinshall
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: We address the problem of novelty detection in multiclass scenarios where some class labels are missing from the training set. Our method is based on the initial assignment of confidence values, which measure the affinity between a new test point and each known class. We first compare the values of the two top elements in this vector of confidence values. In the heart of our method lies the training of an ensemble of classifiers, each trained to discriminate known from novel classes based on some partition of the training data into presumed-known and presumednovel classes. Our final novelty score is derived from the output of this ensemble of classifiers. We evaluated our method on two datasets of images containing a relatively large number of classes - the Caltech-256 and Cifar-100 datasets. We compared our method to 3 alternative methods which represent commonly used approaches, including the one-class SVM, novelty based on k-NN, novelty based on maximal confidence, and the recent KNFST method. The results show a very clear and marked advantage for our method over all alternative methods, in an experimental setup where class labels are missing during training.



### Evaluation of the Effect of Improper Segmentation on Word Spotting
- **Arxiv ID**: http://arxiv.org/abs/1604.06243v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.06243v1)
- **Published**: 2016-04-21 10:20:12+00:00
- **Updated**: 2016-04-21 10:20:12+00:00
- **Authors**: Sounak Dey, Anguelos Nicolaou, Josep Llados, Umapada Pal
- **Comment**: None
- **Journal**: None
- **Summary**: Word spotting is an important recognition task in historical document analysis. In most cases methods are developed and evaluated assuming perfect word segmentations. In this paper we propose an experimental framework to quantify the effect of goodness of word segmentation has on the performance achieved by word spotting methods in identical unbiased conditions. The framework consists of generating systematic distortions on segmentation and retrieving the original queries from the distorted dataset. We apply the framework on the George Washington and Barcelona Marriage Dataset and on several established and state-of-the-art methods. The experiments allow for an estimate of the end-to-end performance of word spotting methods.



### Automatic 3D Reconstruction of Manifold Meshes via Delaunay Triangulation and Mesh Sweeping
- **Arxiv ID**: http://arxiv.org/abs/1604.06258v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.06258v1)
- **Published**: 2016-04-21 11:10:19+00:00
- **Updated**: 2016-04-21 11:10:19+00:00
- **Authors**: Andrea Romanoni, Amaël Delaunoy, Marc Pollefeys, Matteo Matteucci
- **Comment**: in IEEE Winter Conference on Applications of Computer Vision (WACV)
  2016
- **Journal**: None
- **Summary**: In this paper we propose a new approach to incrementally initialize a manifold surface for automatic 3D reconstruction from images. More precisely we focus on the automatic initialization of a 3D mesh as close as possible to the final solution; indeed many approaches require a good initial solution for further refinement via multi-view stereo techniques. Our novel algorithm automatically estimates an initial manifold mesh for surface evolving multi-view stereo algorithms, where the manifold property needs to be enforced. It bootstraps from 3D points extracted via Structure from Motion, then iterates between a state-of-the-art manifold reconstruction step and a novel mesh sweeping algorithm that looks for new 3D points in the neighborhood of the reconstructed manifold to be added in the manifold reconstruction. The experimental results show quantitatively that the mesh sweeping improves the resolution and the accuracy of the manifold reconstruction, allowing a better convergence of state-of-the-art surface evolution multi-view stereo algorithms.



### TI-POOLING: transformation-invariant pooling for feature learning in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1604.06318v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.06318v2)
- **Published**: 2016-04-21 14:17:05+00:00
- **Updated**: 2016-09-22 14:42:28+00:00
- **Authors**: Dmitry Laptev, Nikolay Savinov, Joachim M. Buhmann, Marc Pollefeys
- **Comment**: Accepted at CVPR 2016. The first two authors assert equal
  contribution and joint first authorship
- **Journal**: None
- **Summary**: In this paper we present a deep neural network topology that incorporates a simple to implement transformation invariant pooling operator (TI-POOLING). This operator is able to efficiently handle prior knowledge on nuisance variations in the data, such as rotation or scale changes. Most current methods usually make use of dataset augmentation to address this issue, but this requires larger number of model parameters and more training data, and results in significantly increased training time and larger chance of under- or overfitting. The main reason for these drawbacks is that the learned model needs to capture adequate features for all the possible transformations of the input. On the other hand, we formulate features in convolutional neural networks to be transformation-invariant. We achieve that using parallel siamese architectures for the considered transformation set and applying the TI-POOLING operator on their outputs before the fully-connected layers. We show that this topology internally finds the most optimal "canonical" instance of the input image for training and therefore limits the redundancy in learned features. This more efficient use of training data results in better performance on popular benchmark datasets with smaller number of parameters when comparing to standard convolutional neural networks with dataset augmentation and to other baselines.



### Improving Human Action Recognition by Non-action Classification
- **Arxiv ID**: http://arxiv.org/abs/1604.06397v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.06397v2)
- **Published**: 2016-04-21 17:46:25+00:00
- **Updated**: 2016-04-22 02:50:12+00:00
- **Authors**: Yang Wang, Minh Hoai
- **Comment**: appears in CVPR16
- **Journal**: None
- **Summary**: In this paper we consider the task of recognizing human actions in realistic video where human actions are dominated by irrelevant factors. We first study the benefits of removing non-action video segments, which are the ones that do not portray any human action. We then learn a non-action classifier and use it to down-weight irrelevant video segments. The non-action classifier is trained using ActionThread, a dataset with shot-level annotation for the occurrence or absence of a human action. The non-action classifier can be used to identify non-action shots with high precision and subsequently used to improve the performance of action recognition systems.



### Analysis of the Entropy-guided Switching Trimmed Mean Deviation-based Anisotropic Diffusion filter
- **Arxiv ID**: http://arxiv.org/abs/1604.06427v2
- **DOI**: 10.1117/1.JEI.25.4.043001
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.06427v2)
- **Published**: 2016-04-21 19:06:59+00:00
- **Updated**: 2016-11-14 16:15:50+00:00
- **Authors**: Uche A. Nnolim
- **Comment**: None
- **Journal**: None
- **Summary**: This report describes the experimental analysis of a proposed switching filter-anisotropic diffusion hybrid for the filtering of the fixed value (salt and pepper) impulse noise (FVIN). The filter works well at both low and high noise densities though it was specifically designed for high noise density levels. The filter combines the switching mechanism of decision-based filters and the partial differential equation-based formulation to yield a powerful system capable of recovering the image signals at very high noise levels. Experimental results indicate that the filter surpasses other filters, especially at very high noise levels. Additionally, its adaptive nature ensures that the performance is guided by the metrics obtained from the noisy input image. The filter algorithm is of both global and local nature, where the former is chosen to reduce computation time and complexity, while the latter is used for best results.



### Walk and Learn: Facial Attribute Representation Learning from Egocentric Video and Contextual Data
- **Arxiv ID**: http://arxiv.org/abs/1604.06433v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.06433v3)
- **Published**: 2016-04-21 19:21:55+00:00
- **Updated**: 2016-06-22 20:51:33+00:00
- **Authors**: Jing Wang, Yu Cheng, Rogerio Schmidt Feris
- **Comment**: Paper accepted by CVPR 2016
- **Journal**: None
- **Summary**: The way people look in terms of facial attributes (ethnicity, hair color, facial hair, etc.) and the clothes or accessories they wear (sunglasses, hat, hoodies, etc.) is highly dependent on geo-location and weather condition, respectively. This work explores, for the first time, the use of this contextual information, as people with wearable cameras walk across different neighborhoods of a city, in order to learn a rich feature representation for facial attribute classification, without the costly manual annotation required by previous methods. By tracking the faces of casual walkers on more than 40 hours of egocentric video, we are able to cover tens of thousands of different identities and automatically extract nearly 5 million pairs of images connected by or from different face tracks, along with their weather and location context, under pose and lighting variations. These image pairs are then fed into a deep network that preserves similarity of images connected by the same track, in order to capture identity-related attribute features, and optimizes for location and weather prediction to capture additional facial attribute features. Finally, the network is fine-tuned with manually annotated samples. We perform an extensive experimental analysis on wearable data and two standard benchmark datasets based on web images (LFWA and CelebA). Our method outperforms by a large margin a network trained from scratch. Moreover, even without using manually annotated identity labels for pre-training as in previous methods, our approach achieves results that are better than the state of the art.



### LOH and behold: Web-scale visual search, recommendation and clustering using Locally Optimized Hashing
- **Arxiv ID**: http://arxiv.org/abs/1604.06480v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1604.06480v2)
- **Published**: 2016-04-21 20:23:55+00:00
- **Updated**: 2016-07-30 02:34:52+00:00
- **Authors**: Yannis Kalantidis, Lyndon Kennedy, Huy Nguyen, Clayton Mellina, David A. Shamma
- **Comment**: Accepted for publication at the 4th Workshop on Web-scale Vision and
  Social Media (VSM), ECCV 2016
- **Journal**: None
- **Summary**: We propose a novel hashing-based matching scheme, called Locally Optimized Hashing (LOH), based on a state-of-the-art quantization algorithm that can be used for efficient, large-scale search, recommendation, clustering, and deduplication. We show that matching with LOH only requires set intersections and summations to compute and so is easily implemented in generic distributed computing systems. We further show application of LOH to: a) large-scale search tasks where performance is on par with other state-of-the-art hashing approaches; b) large-scale recommendation where queries consisting of thousands of images can be used to generate accurate recommendations from collections of hundreds of millions of images; and c) efficient clustering with a graph-based algorithm that can be scaled to massive collections in a distributed environment or can be used for deduplication for small collections, like search results, performing better than traditional hashing approaches while only requiring a few milliseconds to run. In this paper we experiment on datasets of up to 100 million images, but in practice our system can scale to larger collections and can be used for other types of data that have a vector representation in a Euclidean space.



### Visual Congruent Ads for Image Search
- **Arxiv ID**: http://arxiv.org/abs/1604.06481v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1604.06481v1)
- **Published**: 2016-04-21 20:23:58+00:00
- **Updated**: 2016-04-21 20:23:58+00:00
- **Authors**: Yannis Kalantidis, Ayman Farahat, Lyndon Kennedy, Ricardo Baeza-Yates, David A. Shamma
- **Comment**: None
- **Journal**: None
- **Summary**: The quality of user experience online is affected by the relevance and placement of advertisements. We propose a new system for selecting and displaying visual advertisements in image search result sets. Our method compares the visual similarity of candidate ads to the image search results and selects the most visually similar ad to be displayed. The method further selects an appropriate location in the displayed image grid to minimize the perceptual visual differences between the ad and its neighbors. We conduct an experiment with about 900 users and find that our proposed method provides significant improvement in the users' overall satisfaction with the image search experience, without diminishing the users' ability to see the ad or recall the advertised brand.



### Humans and deep networks largely agree on which kinds of variation make object recognition harder
- **Arxiv ID**: http://arxiv.org/abs/1604.06486v1
- **DOI**: 10.3389/fncom.2016.00092
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1604.06486v1)
- **Published**: 2016-04-21 20:53:00+00:00
- **Updated**: 2016-04-21 20:53:00+00:00
- **Authors**: Saeed Reza Kheradpisheh, Masoud Ghodrati, Mohammad Ganjtabesh, Timothée Masquelier
- **Comment**: None
- **Journal**: Frontiers in Computational Neuroscience (2016) 10:92
- **Summary**: View-invariant object recognition is a challenging problem, which has attracted much attention among the psychology, neuroscience, and computer vision communities. Humans are notoriously good at it, even if some variations are presumably more difficult to handle than others (e.g. 3D rotations). Humans are thought to solve the problem through hierarchical processing along the ventral stream, which progressively extracts more and more invariant visual features. This feed-forward architecture has inspired a new generation of bio-inspired computer vision systems called deep convolutional neural networks (DCNN), which are currently the best algorithms for object recognition in natural images. Here, for the first time, we systematically compared human feed-forward vision and DCNNs at view-invariant object recognition using the same images and controlling for both the kinds of transformation as well as their magnitude. We used four object categories and images were rendered from 3D computer models. In total, 89 human subjects participated in 10 experiments in which they had to discriminate between two or four categories after rapid presentation with backward masking. We also tested two recent DCNNs on the same tasks. We found that humans and DCNNs largely agreed on the relative difficulties of each kind of variation: rotation in depth is by far the hardest transformation to handle, followed by scale, then rotation in plane, and finally position. This suggests that humans recognize objects mainly through 2D template matching, rather than by constructing 3D object models, and that DCNNs are not too unreasonable models of human feed-forward vision. Also, our results show that the variation levels in rotation in depth and scale strongly modulate both humans' and DCNNs' recognition performances. We thus argue that these variations should be controlled in the image datasets used in vision research.



### Online Action Detection
- **Arxiv ID**: http://arxiv.org/abs/1604.06506v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.06506v2)
- **Published**: 2016-04-21 22:02:50+00:00
- **Updated**: 2016-08-30 09:29:39+00:00
- **Authors**: Roeland De Geest, Efstratios Gavves, Amir Ghodrati, Zhenyang Li, Cees Snoek, Tinne Tuytelaars
- **Comment**: Project page:
  http://homes.esat.kuleuven.be/~rdegeest/OnlineActionDetection.html
- **Journal**: None
- **Summary**: In online action detection, the goal is to detect the start of an action in a video stream as soon as it happens. For instance, if a child is chasing a ball, an autonomous car should recognize what is going on and respond immediately. This is a very challenging problem for four reasons. First, only partial actions are observed. Second, there is a large variability in negative data. Third, the start of the action is unknown, so it is unclear over what time window the information should be integrated. Finally, in real world data, large within-class variability exists. This problem has been addressed before, but only to some extent. Our contributions to online action detection are threefold. First, we introduce a realistic dataset composed of 27 episodes from 6 popular TV series. The dataset spans over 16 hours of footage annotated with 30 action classes, totaling 6,231 action instances. Second, we analyze and compare various baseline methods, showing this is a challenging problem for which none of the methods provides a good solution. Third, we analyze the change in performance when there is a variation in viewpoint, occlusion, truncation, etc. We introduce an evaluation protocol for fair comparison. The dataset, the baselines and the models will all be made publicly available to encourage (much needed) further research on online action detection on realistic data.



