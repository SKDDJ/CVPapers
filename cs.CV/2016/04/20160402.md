# Arxiv Papers in cs.CV on 2016-04-02
### 3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1604.00449v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1604.00449v1)
- **Published**: 2016-04-02 01:28:27+00:00
- **Updated**: 2016-04-02 01:28:27+00:00
- **Authors**: Christopher B. Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, Silvio Savarese
- **Comment**: Appendix can be found at
  http://cvgl.stanford.edu/papers/choy_16_appendix.pdf
- **Journal**: None
- **Summary**: Inspired by the recent success of methods that employ shape priors to achieve robust 3D reconstructions, we propose a novel recurrent neural network architecture that we call the 3D Recurrent Reconstruction Neural Network (3D-R2N2). The network learns a mapping from images of objects to their underlying 3D shapes from a large collection of synthetic data. Our network takes in one or more images of an object instance from arbitrary viewpoints and outputs a reconstruction of the object in the form of a 3D occupancy grid. Unlike most of the previous works, our network does not require any image annotations or object class labels for training or testing. Our extensive experimental analysis shows that our reconstruction framework i) outperforms the state-of-the-art methods for single view reconstruction, and ii) enables the 3D reconstruction of objects in situations when traditional SFM/SLAM methods fail (because of lack of texture and/or wide baseline).



### Automatic Annotation of Structured Facts in Images
- **Arxiv ID**: http://arxiv.org/abs/1604.00466v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1604.00466v3)
- **Published**: 2016-04-02 06:35:45+00:00
- **Updated**: 2016-04-08 00:04:22+00:00
- **Authors**: Mohamed Elhoseiny, Scott Cohen, Walter Chang, Brian Price, Ahmed Elgammal
- **Comment**: None
- **Journal**: None
- **Summary**: Motivated by the application of fact-level image understanding, we present an automatic method for data collection of structured visual facts from images with captions. Example structured facts include attributed objects (e.g., <flower, red>), actions (e.g., <baby, smile>), interactions (e.g., <man, walking, dog>), and positional information (e.g., <vase, on, table>). The collected annotations are in the form of fact-image pairs (e.g.,<man, walking, dog> and an image region containing this fact). With a language approach, the proposed method is able to collect hundreds of thousands of visual fact annotations with accuracy of 83% according to human judgment. Our method automatically collected more than 380,000 visual fact annotations and more than 110,000 unique visual facts from images with captions and localized them in images in less than one day of processing time on standard CPU platforms.



### Overlay Text Extraction From TV News Broadcast
- **Arxiv ID**: http://arxiv.org/abs/1604.00470v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.00470v1)
- **Published**: 2016-04-02 07:28:23+00:00
- **Updated**: 2016-04-02 07:28:23+00:00
- **Authors**: Raghvendra Kannao, Prithwijit Guha
- **Comment**: Published in INDICON 2015
- **Journal**: None
- **Summary**: The text data present in overlaid bands convey brief descriptions of news events in broadcast videos. The process of text extraction becomes challenging as overlay text is presented in widely varying formats and often with animation effects. We note that existing edge density based methods are well suited for our application on account of their simplicity and speed of operation. However, these methods are sensitive to thresholds and have high false positive rates. In this paper, we present a contrast enhancement based preprocessing stage for overlay text detection and a parameter free edge density based scheme for efficient text band detection. The second contribution of this paper is a novel approach for multiple text region tracking with a formal identification of all possible detection failure cases. The tracking stage enables us to establish the temporal presence of text bands and their linking over time. The third contribution is the adoption of Tesseract OCR for the specific task of overlay text recognition using web news articles. The proposed approach is tested and found superior on news videos acquired from three Indian English television news channels along with benchmark datasets.



### Robust video object tracking via Bayesian model averaging based feature fusion
- **Arxiv ID**: http://arxiv.org/abs/1604.00475v3
- **DOI**: 10.1117/1.OE.55.8.083102
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.00475v3)
- **Published**: 2016-04-02 08:44:20+00:00
- **Updated**: 2016-09-06 12:05:57+00:00
- **Authors**: Yi Dai, Bin Liu
- **Comment**: None
- **Journal**: Opt. Eng. 55(8), 083102 (2016)
- **Summary**: In this article, we are concerned with tracking an object of interest in video stream. We propose an algorithm that is robust against occlusion, the presence of confusing colors, abrupt changes in the object feature space and changes in object size. We develop the algorithm within a Bayesian modeling framework. The state space model is used for capturing the temporal correlation in the sequence of frame images by modeling the underlying dynamics of the tracking system. The Bayesian model averaging (BMA) strategy is proposed for fusing multi-clue information in the observations. Any number of object features are allowed to be involved in the proposed framework. Every feature represents one source of information to be fused and is associated with an observation model. The state inference is performed by employing the particle filter methods. In comparison with related approaches, the BMA based tracker is shown to have robustness, expressivity, and comprehensibility.



### A Fully Convolutional Neural Network for Cardiac Segmentation in Short-Axis MRI
- **Arxiv ID**: http://arxiv.org/abs/1604.00494v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.00494v3)
- **Published**: 2016-04-02 12:32:55+00:00
- **Updated**: 2017-04-27 03:04:26+00:00
- **Authors**: Phi Vu Tran
- **Comment**: Initial Technical Report; Include link to models and code
- **Journal**: None
- **Summary**: Automated cardiac segmentation from magnetic resonance imaging datasets is an essential step in the timely diagnosis and management of cardiac pathologies. We propose to tackle the problem of automated left and right ventricle segmentation through the application of a deep fully convolutional neural network architecture. Our model is efficiently trained end-to-end in a single learning stage from whole-image inputs and ground truths to make inference at every pixel. To our knowledge, this is the first application of a fully convolutional neural network architecture for pixel-wise labeling in cardiac magnetic resonance imaging. Numerical experiments demonstrate that our model is robust to outperform previous fully automated methods across multiple evaluation measures on a range of cardiac datasets. Moreover, our model is fast and can leverage commodity compute resources such as the graphics processing unit to enable state-of-the-art cardiac segmentation at massive scales. The models and code are available at https://github.com/vuptran/cardiac-segmentation



### Voronoi Region-Based Adaptive Unsupervised Color Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1604.00533v1
- **DOI**: None
- **Categories**: **cs.CV**, 05B45, 62H30, 54E05, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1604.00533v1)
- **Published**: 2016-04-02 17:27:24+00:00
- **Updated**: 2016-04-02 17:27:24+00:00
- **Authors**: R. Hettiarachchi, J. F. Peters
- **Comment**: 21 pages, 5 figures
- **Journal**: None
- **Summary**: Color image segmentation is a crucial step in many computer vision and pattern recognition applications. This article introduces an adaptive and unsupervised clustering approach based on Voronoi regions, which can be applied to solve the color image segmentation problem. The proposed method performs region splitting and merging within Voronoi regions of the Dirichlet Tessellated image (also called a Voronoi diagram) , which improves the efficiency and the accuracy of the number of clusters and cluster centroids estimation process. Furthermore, the proposed method uses cluster centroid proximity to merge proximal clusters in order to find the final number of clusters and cluster centroids. In contrast to the existing adaptive unsupervised cluster-based image segmentation algorithms, the proposed method uses K-means clustering algorithm in place of the Fuzzy C-means algorithm to find the final segmented image. The proposed method was evaluated on three different unsupervised image segmentation evaluation benchmarks and its results were compared with two other adaptive unsupervised cluster-based image segmentation algorithms. The experimental results reported in this article confirm that the proposed method outperforms the existing algorithms in terms of the quality of image segmentation results. Also, the proposed method results in the lowest average execution time per image compared to the existing methods reported in this article.



### Image Quality Assessment for Performance Evaluation of Focus Measure Operators
- **Arxiv ID**: http://arxiv.org/abs/1604.00546v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.00546v1)
- **Published**: 2016-04-02 19:28:01+00:00
- **Updated**: 2016-04-02 19:28:01+00:00
- **Authors**: Farida Memon, Mukhtiar Ali Unar, Sheeraz Memon
- **Comment**: 8 pages, Mehran University Research Journal of Engineering and
  Technology, Vol. 34, No. 4, 2015
- **Journal**: None
- **Summary**: This paper presents the performance evaluation of eight focus measure operators namely Image CURV (Curvature), GRAE (Gradient Energy), HISE (Histogram Entropy), LAPM (Modified Laplacian), LAPV (Variance of Laplacian), LAPD (Diagonal Laplacian), LAP3 (Laplacian in 3D Window) and WAVS (Sum of Wavelet Coefficients). Statistical matrics such as MSE (Mean Squared Error), PNSR (Peak Signal to Noise Ratio), SC (Structural Content), NCC (Normalized Cross Correlation), MD (Maximum Difference) and NAE (Normalized Absolute Error) are used to evaluate stated focus measures in this research. . FR (Full Reference) method of the image quality assessment is utilized in this paper. Results indicate that LAPD method is comparatively better than other seven focus operators at typical imaging conditions.



