# Arxiv Papers in cs.CV on 2016-04-08
### Infrared Colorization Using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1604.02245v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, 82C32 (Primary), 68T45 (Secondary), H.5.1; I.4.8; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/1604.02245v3)
- **Published**: 2016-04-08 07:10:47+00:00
- **Updated**: 2016-07-26 09:35:51+00:00
- **Authors**: Matthias Limmer, Hendrik P. A. Lensch
- **Comment**: 8 pages, 11 figures, 1 table, submitted to ICMLA2016
- **Journal**: None
- **Summary**: This paper proposes a method for transferring the RGB color spectrum to near-infrared (NIR) images using deep multi-scale convolutional neural networks. A direct and integrated transfer between NIR and RGB pixels is trained. The trained model does not require any user guidance or a reference image database in the recall phase to produce images with a natural appearance. To preserve the rich details of the NIR image, its high frequency features are transferred to the estimated RGB image. The presented approach is trained and evaluated on a real-world dataset containing a large amount of road scene images in summer. The dataset was captured by a multi-CCD NIR/RGB camera, which ensures a perfect pixel to pixel registration.



### Deep Structured Scene Parsing by Learning with Image Descriptions
- **Arxiv ID**: http://arxiv.org/abs/1604.02271v3
- **DOI**: None
- **Categories**: **cs.CV**, 68U10, I.4.8; I.5
- **Links**: [PDF](http://arxiv.org/pdf/1604.02271v3)
- **Published**: 2016-04-08 08:17:37+00:00
- **Updated**: 2018-02-28 02:38:51+00:00
- **Authors**: Liang Lin, Guangrun Wang, Rui Zhang, Ruimao Zhang, Xiaodan Liang, Wangmeng Zuo
- **Comment**: Discovering a semantic object hierarchy with object interaction
  relations (Publhised in Proceedings of IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR), 2016. (oral))
- **Journal**: None
- **Summary**: This paper addresses a fundamental problem of scene understanding: How to parse the scene image into a structured configuration (i.e., a semantic object hierarchy with object interaction relations) that finely accords with human perception. We propose a deep architecture consisting of two networks: i) a convolutional neural network (CNN) extracting the image representation for pixelwise object labeling and ii) a recursive neural network (RNN) discovering the hierarchical object structure and the inter-object relations. Rather than relying on elaborative user annotations (e.g., manually labeling semantic maps and relations), we train our deep model in a weakly-supervised manner by leveraging the descriptive sentences of the training images. Specifically, we decompose each sentence into a semantic tree consisting of nouns and verb phrases, and facilitate these trees discovering the configurations of the training images. Once these scene configurations are determined, then the parameters of both the CNN and RNN are updated accordingly by back propagation. The entire model training is accomplished through an Expectation-Maximization method. Extensive experiments suggest that our model is capable of producing meaningful and structured scene configurations and achieving more favorable scene labeling performance on PASCAL VOC 2012 over other state-of-the-art weakly-supervised methods.



### Online Open World Recognition
- **Arxiv ID**: http://arxiv.org/abs/1604.02275v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1604.02275v1)
- **Published**: 2016-04-08 08:43:15+00:00
- **Updated**: 2016-04-08 08:43:15+00:00
- **Authors**: Rocco De Rosa, Thomas Mensink, Barbara Caputo
- **Comment**: keywords{Open world recognition, Open set, Incremental Learning,
  Metric Learning, Nonparametric methods, Classification confidence}
- **Journal**: None
- **Summary**: As we enter into the big data age and an avalanche of images have become readily available, recognition systems face the need to move from close, lab settings where the number of classes and training data are fixed, to dynamic scenarios where the number of categories to be recognized grows continuously over time, as well as new data providing useful information to update the system. Recent attempts, like the open world recognition framework, tried to inject dynamics into the system by detecting new unknown classes and adding them incrementally, while at the same time continuously updating the models for the known classes. incrementally adding new classes and detecting instances from unknown classes, while at the same time continuously updating the models for the known classes. In this paper we argue that to properly capture the intrinsic dynamic of open world recognition, it is necessary to add to these aspects (a) the incremental learning of the underlying metric, (b) the incremental estimate of confidence thresholds for the unknown classes, and (c) the use of local learning to precisely describe the space of classes. We extend three existing metric learning algorithms towards these goals by using online metric learning. Experimentally we validate our approach on two large-scale datasets in different learning scenarios. For all these scenarios our proposed methods outperform their non-online counterparts. We conclude that local and online learning is important to capture the full dynamics of open world recognition.



### A method for locally approximating regularized iterative tomographic reconstruction methods
- **Arxiv ID**: http://arxiv.org/abs/1604.02292v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1604.02292v1)
- **Published**: 2016-04-08 10:19:20+00:00
- **Updated**: 2016-04-08 10:19:20+00:00
- **Authors**: D. M. Pelt, K. J. Batenburg
- **Comment**: 32 pages, 13 figures
- **Journal**: None
- **Summary**: In many applications of tomography, the acquired projections are either limited in number or contain a significant amount of noise. In these cases, standard reconstruction methods tend to produce artifacts that can make further analysis difficult. Advanced regularized iterative methods, such as total variation minimization, are often able to achieve a higher reconstruction quality by exploiting prior knowledge about the scanned object. In practice, however, these methods often have prohibitively long computation times or large memory requirements. Furthermore, since they are based on minimizing a global objective function, regularized iterative methods need to reconstruct the entire scanned object, even when one is only interested in a (small) region of the reconstructed image.   In this paper, we present a method to approximate regularized iterative reconstruction methods inside a (small) region of the scanned object. The method only performs computations inside the region of interest, ensuring low computational requirements. Reconstruction results for different phantom images and types of regularization are given, showing that reconstructions of the proposed local method are almost identical to those of the global regularized iterative methods that are approximated, even for relatively small regions of interest. Furthermore, we show that larger regions can be reconstructed efficiently by reconstructing several small regions in parallel and combining them into a single reconstruction afterwards.



### Free-Space Detection with Self-Supervised and Online Trained Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1604.02316v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.02316v2)
- **Published**: 2016-04-08 11:54:40+00:00
- **Updated**: 2017-01-05 13:59:30+00:00
- **Authors**: Willem P. Sanberg, Gijs Dubbelman, Peter H. N. de With
- **Comment**: version as accepted at IS&T Electronic Imaging - Autonomous Vehicles
  and Machines Conference (San Francisco USA, January 2017); updated with two
  additional robustness experiments and formatted in conference style; 8 pages,
  public data available
- **Journal**: None
- **Summary**: Recently, vision-based Advanced Driver Assist Systems have gained broad interest. In this work, we investigate free-space detection, for which we propose to employ a Fully Convolutional Network (FCN). We show that this FCN can be trained in a self-supervised manner and achieve similar results compared to training on manually annotated data, thereby reducing the need for large manually annotated training sets. To this end, our self-supervised training relies on a stereo-vision disparity system, to automatically generate (weak) training labels for the color-based FCN. Additionally, our self-supervised training facilitates online training of the FCN instead of offline. Consequently, given that the applied FCN is relatively small, the free-space analysis becomes highly adaptive to any traffic scene that the vehicle encounters. We have validated our algorithm using publicly available data and on a new challenging benchmark dataset that is released with this paper. Experiments show that the online training boosts performance with 5% when compared to offline training, both for Fmax and AP.



### Bayesian Neighbourhood Component Analysis
- **Arxiv ID**: http://arxiv.org/abs/1604.02354v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1604.02354v1)
- **Published**: 2016-04-08 13:35:03+00:00
- **Updated**: 2016-04-08 13:35:03+00:00
- **Authors**: Dong Wang, Xiaoyang Tan
- **Comment**: None
- **Journal**: None
- **Summary**: Learning a good distance metric in feature space potentially improves the performance of the KNN classifier and is useful in many real-world applications. Many metric learning algorithms are however based on the point estimation of a quadratic optimization problem, which is time-consuming, susceptible to overfitting, and lack a natural mechanism to reason with parameter uncertainty, an important property useful especially when the training set is small and/or noisy. To deal with these issues, we present a novel Bayesian metric learning method, called Bayesian NCA, based on the well-known Neighbourhood Component Analysis method, in which the metric posterior is characterized by the local label consistency constraints of observations, encoded with a similarity graph instead of independent pairwise constraints. For efficient Bayesian optimization, we explore the variational lower bound over the log-likelihood of the original NCA objective. Experiments on several publicly available datasets demonstrate that the proposed method is able to learn robust metric measures from small size dataset and/or from challenging training set with labels contaminated by errors. The proposed method is also shown to outperform a previous pairwise constrained Bayesian metric learning method.



### Finding Optimal Combination of Kernels using Genetic Programming
- **Arxiv ID**: http://arxiv.org/abs/1604.02376v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1604.02376v2)
- **Published**: 2016-04-08 15:33:30+00:00
- **Updated**: 2016-04-22 23:16:20+00:00
- **Authors**: Jyothi Korra
- **Comment**: None
- **Journal**: None
- **Summary**: In Computer Vision, problem of identifying or classifying the objects present in an image is called Object Categorization. It is a challenging problem, especially when the images have clutter background, occlusions or different lighting conditions. Many vision features have been proposed which aid object categorization even in such adverse conditions. Past research has shown that, employing multiple features rather than any single features leads to better recognition. Multiple Kernel Learning (MKL) framework has been developed for learning an optimal combination of features for object categorization. Existing MKL methods use linear combination of base kernels which may not be optimal for object categorization. Real-world object categorization may need to consider complex combination of kernels(non-linear) and not only linear combination. Evolving non-linear functions of base kernels using Genetic Programming is proposed in this report. Experiment results show that non-kernel generated using genetic programming gives good accuracy as compared to linear combination of kernels.



### STD2P: RGBD Semantic Segmentation Using Spatio-Temporal Data-Driven Pooling
- **Arxiv ID**: http://arxiv.org/abs/1604.02388v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.02388v3)
- **Published**: 2016-04-08 16:01:34+00:00
- **Updated**: 2017-04-26 13:13:02+00:00
- **Authors**: Yang He, Wei-Chen Chiu, Margret Keuper, Mario Fritz
- **Comment**: To appear in CVPR 2017
- **Journal**: None
- **Summary**: We propose a novel superpixel-based multi-view convolutional neural network for semantic image segmentation. The proposed network produces a high quality segmentation of a single image by leveraging information from additional views of the same scene. Particularly in indoor videos such as captured by robotic platforms or handheld and bodyworn RGBD cameras, nearby video frames provide diverse viewpoints and additional context of objects and scenes. To leverage such information, we first compute region correspondences by optical flow and image boundary-based superpixels. Given these region correspondences, we propose a novel spatio-temporal pooling layer to aggregate information over space and time. We evaluate our approach on the NYU--Depth--V2 and the SUN3D datasets and compare it to various state-of-the-art single-view and multi-view approaches. Besides a general improvement over the state-of-the-art, we also show the benefits of making use of unlabeled frames during training for multi-view as well as single-view prediction.



### CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples
- **Arxiv ID**: http://arxiv.org/abs/1604.02426v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.02426v3)
- **Published**: 2016-04-08 19:04:35+00:00
- **Updated**: 2016-09-07 16:46:58+00:00
- **Authors**: Filip Radenović, Giorgos Tolias, Ondřej Chum
- **Comment**: ECCV 2016
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) achieve state-of-the-art performance in many computer vision tasks. However, this achievement is preceded by extreme manual annotation in order to perform either training from scratch or fine-tuning for the target task. In this work, we propose to fine-tune CNN for image retrieval from a large collection of unordered images in a fully automated manner. We employ state-of-the-art retrieval and Structure-from-Motion (SfM) methods to obtain 3D models, which are used to guide the selection of the training data for CNN fine-tuning. We show that both hard positive and hard negative examples enhance the final performance in particular object retrieval with compact codes.



### Image segmentation of cross-country scenes captured in IR spectrum
- **Arxiv ID**: http://arxiv.org/abs/1604.02469v1
- **DOI**: 10.1007/978-3-642-23363-0_10
- **Categories**: **cs.CV**, 68T10, I.4.7; I.4.8; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/1604.02469v1)
- **Published**: 2016-04-08 20:14:46+00:00
- **Updated**: 2016-04-08 20:14:46+00:00
- **Authors**: Artem Lenskiy
- **Comment**: Corrected version of the chapter published in Advances in Robotics
  and Virtual Reality, Volume 26 of the series Intelligent Systems Reference
  Library pp 227-247
- **Journal**: None
- **Summary**: Computer vision has become a major source of information for autonomous navigation of robots of various types, self-driving cars, military robots and mars/lunar rovers are some examples. Nevertheless, the majority of methods focus on analysing images captured in visible spectrum. In this manuscript we elaborate on the problem of segmenting cross-country scenes captured in IR spectrum. For this purpose we proposed employing salient features. Salient features are robust to variations in scale, brightness and view angle. We suggest the Speeded-Up Robust Features as a basis for our salient features for a number of reasons discussed in the paper. We also provide a comparison of two SURF implementations. The SURF features are extracted from images of different terrain types. For every feature we estimate a terrain class membership function. The membership values are obtained by means of either the multi-layer perceptron or nearest neighbours. The features' class membership values and their spatial positions are then applied to estimate class membership values for all pixels in the image. To decrease the effect of segmentation blinking that is caused by rapid switching between different terrain types and to speed up segmentation, we are tracking camera position and predict features' positions. The comparison of the multi-layer perception and the nearest neighbour classifiers is presented in the paper. The error rate of the terrain segmentation using the nearest neighbours obtained on the testing set is 16.6+-9.17%.



### One-class classifiers based on entropic spanning graphs
- **Arxiv ID**: http://arxiv.org/abs/1604.02477v4
- **DOI**: 10.1109/TNNLS.2016.2608983
- **Categories**: **cs.LG**, cs.CV, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1604.02477v4)
- **Published**: 2016-04-08 20:41:54+00:00
- **Updated**: 2016-08-12 15:49:01+00:00
- **Authors**: Lorenzo Livi, Cesare Alippi
- **Comment**: Extended and revised version of the paper "One-Class Classification
  Through Mutual Information Minimization" presented at the 2016 IEEE IJCNN,
  Vancouver, Canada
- **Journal**: None
- **Summary**: One-class classifiers offer valuable tools to assess the presence of outliers in data. In this paper, we propose a design methodology for one-class classifiers based on entropic spanning graphs. Our approach takes into account the possibility to process also non-numeric data by means of an embedding procedure. The spanning graph is learned on the embedded input data and the outcoming partition of vertices defines the classifier. The final partition is derived by exploiting a criterion based on mutual information minimization. Here, we compute the mutual information by using a convenient formulation provided in terms of the $\alpha$-Jensen difference. Once training is completed, in order to associate a confidence level with the classifier decision, a graph-based fuzzy model is constructed. The fuzzification process is based only on topological information of the vertices of the entropic spanning graph. As such, the proposed one-class classifier is suitable also for data characterized by complex geometric structures. We provide experiments on well-known benchmarks containing both feature vectors and labeled graphs. In addition, we apply the method to the protein solubility recognition problem by considering several representations for the input samples. Experimental results demonstrate the effectiveness and versatility of the proposed method with respect to other state-of-the-art approaches.



### Machine Learning for Visual Navigation of Unmanned Ground Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1604.02485v1
- **DOI**: 10.4018/978-1-60960-818-7.ch418
- **Categories**: **cs.CV**, I.4.7, I.4.8, I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/1604.02485v1)
- **Published**: 2016-04-08 21:04:36+00:00
- **Updated**: 2016-04-08 21:04:36+00:00
- **Authors**: Artem A. Lenskiy, Jong-Soo Lee
- **Comment**: Preprint of the chapter in the Machine Learning for Visual Navigation
  of Unmanned Ground Vehicles 2012
- **Journal**: None
- **Summary**: The use of visual information for the navigation of unmanned ground vehicles in a cross-country environment recently received great attention. However, until now, the use of textural information has been somewhat less effective than color or laser range information. This manuscript reviews the recent achievements in cross-country scene segmentation and addresses their shortcomings. It then describes a problem related to classification of high dimensional texture features. Finally, it compares three machine learning algorithms aimed at resolving this problem. The experimental results for each machine learning algorithm with the discussion of comparisons are given at the end of the manuscript.



### Application of Multifractal Analysis to Segmentation of Water Bodies in Optical and Synthetic Aperture Radar Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/1604.02488v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/1604.02488v1)
- **Published**: 2016-04-08 21:24:15+00:00
- **Updated**: 2016-04-08 21:24:15+00:00
- **Authors**: Victor Manuel San Martin, Alejandra Figliola
- **Comment**: None
- **Journal**: None
- **Summary**: A method for segmenting water bodies in optical and synthetic aperture radar (SAR) satellite images is proposed. It makes use of the textural features of the different regions in the image for segmentation. The method consists in a multiscale analysis of the images, which allows us to study the images regularity both, locally and globally. As results of the analysis, coarse multifractal spectra of studied images and a group of images that associates each position (pixel) with its corresponding value of local regularity (or singularity) spectrum are obtained. Thresholds are then applied to the multifractal spectra of the images for the classification. These thresholds are selected after studying the characteristics of the spectra under the assumption that water bodies have larger local regularity than other soil types. Classifications obtained by the multifractal method are compared quantitatively with those obtained by neural networks trained to classify the pixels of the images in covered against uncovered by water. In optical images, the classifications are also compared with those derived using the so-called Normalized Differential Water Index (NDWI).



