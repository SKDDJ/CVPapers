# Arxiv Papers in cs.CV on 2016-04-22
### Opt: A Domain Specific Language for Non-linear Least Squares Optimization in Graphics and Imaging
- **Arxiv ID**: http://arxiv.org/abs/1604.06525v3
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.PL
- **Links**: [PDF](http://arxiv.org/pdf/1604.06525v3)
- **Published**: 2016-04-22 03:02:59+00:00
- **Updated**: 2017-09-09 13:23:33+00:00
- **Authors**: Zachary DeVito, Michael Mara, Michael Zollhöfer, Gilbert Bernstein, Jonathan Ragan-Kelley, Christian Theobalt, Pat Hanrahan, Matthew Fisher, Matthias Nießner
- **Comment**: None
- **Journal**: None
- **Summary**: Many graphics and vision problems can be expressed as non-linear least squares optimizations of objective functions over visual data, such as images and meshes. The mathematical descriptions of these functions are extremely concise, but their implementation in real code is tedious, especially when optimized for real-time performance on modern GPUs in interactive applications. In this work, we propose a new language, Opt (available under http://optlang.org), for writing these objective functions over image- or graph-structured unknowns concisely and at a high level. Our compiler automatically transforms these specifications into state-of-the-art GPU solvers based on Gauss-Newton or Levenberg-Marquardt methods. Opt can generate different variations of the solver, so users can easily explore tradeoffs in numerical precision, matrix-free methods, and solver approaches. In our results, we implement a variety of real-world graphics and vision applications. Their energy functions are expressible in tens of lines of code, and produce highly-optimized GPU solver implementations. These solver have performance competitive with the best published hand-tuned, application-specific GPU solvers, and orders of magnitude beyond a general-purpose auto-generated solver.



### A Classifier-guided Approach for Top-down Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1604.06570v1
- **DOI**: 10.1016/j.image.2016.04.001
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.06570v1)
- **Published**: 2016-04-22 08:43:34+00:00
- **Updated**: 2016-04-22 08:43:34+00:00
- **Authors**: Hisham Cholakkal, Jubin Johnson, Deepu Rajan
- **Comment**: To appear in Signal Processing: Image Communication, Elsevier.
  Available online from April 2016
- **Journal**: None
- **Summary**: We propose a framework for top-down salient object detection that incorporates a tightly coupled image classification module. The classifier is trained on novel category-aware sparse codes computed on object dictionaries used for saliency modeling. A misclassification indicates that the corresponding saliency model is inaccurate. Hence, the classifier selects images for which the saliency models need to be updated. The category-aware sparse coding produces better image classification accuracy as compared to conventional sparse coding with a reduced computational complexity. A saliency-weighted max-pooling is proposed to improve image classification, which is further used to refine the saliency maps. Experimental results on Graz-02 and PASCAL VOC-07 datasets demonstrate the effectiveness of salient object detection. Although the role of the classifier is to support salient object detection, we evaluate its performance in image classification and also illustrate the utility of thresholded saliency maps for image segmentation.



### Convolutional Two-Stream Network Fusion for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1604.06573v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.06573v2)
- **Published**: 2016-04-22 08:51:17+00:00
- **Updated**: 2016-09-26 10:47:21+00:00
- **Authors**: Christoph Feichtenhofer, Axel Pinz, Andrew Zisserman
- **Comment**: in Proc. CVPR 2016
- **Journal**: None
- **Summary**: Recent applications of Convolutional Neural Networks (ConvNets) for human action recognition in videos have proposed different solutions for incorporating the appearance and motion information. We study a number of ways of fusing ConvNet towers both spatially and temporally in order to best take advantage of this spatio-temporal information. We make the following findings: (i) that rather than fusing at the softmax layer, a spatial and temporal network can be fused at a convolution layer without loss of performance, but with a substantial saving in parameters; (ii) that it is better to fuse such networks spatially at the last convolutional layer than earlier, and that additionally fusing at the class prediction layer can boost accuracy; finally (iii) that pooling of abstract convolutional features over spatiotemporal neighbourhoods further boosts performance. Based on these studies we propose a new ConvNet architecture for spatiotemporal fusion of video snippets, and evaluate its performance on standard benchmarks where this architecture achieves state-of-the-art results.



### Kernelized Covariance for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1604.06582v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.06582v2)
- **Published**: 2016-04-22 09:16:22+00:00
- **Updated**: 2016-09-02 12:05:09+00:00
- **Authors**: Jacopo Cavazza, Andrea Zunino, Marco San Biagio, Vittorio Murino
- **Comment**: Accepted paper at the 23rd International Conference on Pattern
  Recognition (ICPR), Cancun, Mexico, 2016
- **Journal**: None
- **Summary**: In this paper we aim at increasing the descriptive power of the covariance matrix, limited in capturing linear mutual dependencies between variables only. We present a rigorous and principled mathematical pipeline to recover the kernel trick for computing the covariance matrix, enhancing it to model more complex, non-linear relationships conveyed by the raw data. To this end, we propose Kernelized-COV, which generalizes the original covariance representation without compromising the efficiency of the computation. In the experiments, we validate the proposed framework against many previous approaches in the literature, scoring on par or superior with respect to the state of the art on benchmark datasets for 3D action recognition.



### Optimizing Top Precision Performance Measure of Content-Based Image Retrieval by Learning Similarity Function
- **Arxiv ID**: http://arxiv.org/abs/1604.06620v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.06620v5)
- **Published**: 2016-04-22 12:15:56+00:00
- **Updated**: 2016-08-20 06:15:22+00:00
- **Authors**: Ru-Ze Liang, Lihui Shi, Haoxiang Wang, Jiandong Meng, Jim Jing-Yan Wang, Qingquan Sun, Yi Gu
- **Comment**: Pattern Recognition (ICPR), 2016 23st International Conference on
- **Journal**: None
- **Summary**: In this paper we study the problem of content-based image retrieval. In this problem, the most popular performance measure is the top precision measure, and the most important component of a retrieval system is the similarity function used to compare a query image against a database image. However, up to now, there is no existing similarity learning method proposed to optimize the top precision measure. To fill this gap, in this paper, we propose a novel similarity learning method to maximize the top precision measure. We model this problem as a minimization problem with an objective function as the combination of the losses of the relevant images ranked behind the top-ranked irrelevant image, and the squared Frobenius norm of the similarity function parameter. This minimization problem is solved as a quadratic programming problem. The experiments over two benchmark data sets show the advantages of the proposed method over other similarity learning methods when the top precision is used as the performance measure.



### The Mean Partition Theorem of Consensus Clustering
- **Arxiv ID**: http://arxiv.org/abs/1604.06626v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1604.06626v2)
- **Published**: 2016-04-22 12:32:37+00:00
- **Updated**: 2016-04-26 06:55:06+00:00
- **Authors**: Brijnesh J. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: To devise efficient solutions for approximating a mean partition in consensus clustering, Dimitriadou et al. [3] presented a necessary condition of optimality for a consensus function based on least square distances. We show that their result is pivotal for deriving interesting properties of consensus clustering beyond optimization. For this, we present the necessary condition of optimality in a slightly stronger form in terms of the Mean Partition Theorem and extend it to the Expected Partition Theorem. To underpin its versatility, we show three examples that apply the Mean Partition Theorem: (i) equivalence of the mean partition and optimal multiple alignment, (ii) construction of profiles and motifs, and (iii) relationship between consensus clustering and cluster stability.



### Synthetic Data for Text Localisation in Natural Images
- **Arxiv ID**: http://arxiv.org/abs/1604.06646v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.06646v1)
- **Published**: 2016-04-22 13:23:08+00:00
- **Updated**: 2016-04-22 13:23:08+00:00
- **Authors**: Ankush Gupta, Andrea Vedaldi, Andrew Zisserman
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we introduce a new method for text detection in natural images. The method comprises two contributions: First, a fast and scalable engine to generate synthetic images of text in clutter. This engine overlays synthetic text to existing background images in a natural way, accounting for the local 3D scene geometry. Second, we use the synthetic images to train a Fully-Convolutional Regression Network (FCRN) which efficiently performs text detection and bounding-box regression at all locations and multiple scales in an image. We discuss the relation of FCRN to the recently-introduced YOLO detector, as well as other end-to-end object detection systems based on deep learning. The resulting detection network significantly out performs current methods for text detection in natural images, achieving an F-measure of 84.2% on the standard ICDAR 2013 benchmark. Furthermore, it can process 15 images per second on a GPU.



### Multiscale Segmentation via Bregman Distances and Nonlinear Spectral Analysis
- **Arxiv ID**: http://arxiv.org/abs/1604.06665v2
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, math.SP
- **Links**: [PDF](http://arxiv.org/pdf/1604.06665v2)
- **Published**: 2016-04-22 14:06:42+00:00
- **Updated**: 2016-10-03 09:43:09+00:00
- **Authors**: Leonie Zeune, Guus van Dalum, Leon W. M. M. Terstappen, S. A. van Gils, Christoph Brune
- **Comment**: None
- **Journal**: None
- **Summary**: In biomedical imaging reliable segmentation of objects (e.g. from small cells up to large organs) is of fundamental importance for automated medical diagnosis. New approaches for multi-scale segmentation can considerably improve performance in case of natural variations in intensity, size and shape. This paper aims at segmenting objects of interest based on shape contours and automatically finding multiple objects with different scales. The overall strategy of this work is to combine nonlinear segmentation with scales spaces and spectral decompositions recently introduced in literature. For this we generalize a variational segmentation model based on total variation using Bregman distances to construct an inverse scale space. This offers the new model to be accomplished by a scale analysis approach based on a spectral decomposition of the total variation. As a result we obtain a very efficient, (nearly) parameter-free multiscale segmentation method that comes with an adaptive regularization parameter choice. The added benefit of our method is demonstrated by systematic synthetic tests and its usage in a new biomedical toolbox for identifying and classifying circulating tumor cells. Due to the nature of nonlinear diffusion underlying, the mathematical concepts in this work offer promising extensions to nonlocal classification problems.



### Learning rotation invariant convolutional filters for texture classification
- **Arxiv ID**: http://arxiv.org/abs/1604.06720v2
- **DOI**: 10.1109/ICPR.2016.7899932
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.06720v2)
- **Published**: 2016-04-22 15:55:37+00:00
- **Updated**: 2016-09-21 09:41:48+00:00
- **Authors**: Diego Marcos, Michele Volpi, Devis Tuia
- **Comment**: 6 pages, in ICPR 2016
- **Journal**: None
- **Summary**: We present a method for learning discriminative filters using a shallow Convolutional Neural Network (CNN). We encode rotation invariance directly in the model by tying the weights of groups of filters to several rotated versions of the canonical filter in the group. These filters can be used to extract rotation invariant features well-suited for image classification. We test this learning procedure on a texture classification benchmark, where the orientations of the training images differ from those of the test images. We obtain results comparable to the state-of-the-art. Compared to standard shallow CNNs, the proposed method obtains higher classification performance while reducing by an order of magnitude the number of parameters to be learned.



### Refining Architectures of Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1604.06832v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.06832v1)
- **Published**: 2016-04-22 22:39:55+00:00
- **Updated**: 2016-04-22 22:39:55+00:00
- **Authors**: Sukrit Shankar, Duncan Robertson, Yani Ioannou, Antonio Criminisi, Roberto Cipolla
- **Comment**: 9 pages, 6 figures, CVPR 2016
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (CNNs) have recently evinced immense success for various image recognition tasks. However, a question of paramount importance is somewhat unanswered in deep learning research - is the selected CNN optimal for the dataset in terms of accuracy and model size? In this paper, we intend to answer this question and introduce a novel strategy that alters the architecture of a given CNN for a specified dataset, to potentially enhance the original accuracy while possibly reducing the model size. We use two operations for architecture refinement, viz. stretching and symmetrical splitting. Our procedure starts with a pre-trained CNN for a given dataset, and optimally decides the stretch and split factors across the network to refine the architecture. We empirically demonstrate the necessity of the two operations. We evaluate our approach on two natural scenes attributes datasets, SUN Attributes and CAMIT-NSAD, with architectures of GoogleNet and VGG-11, that are quite contrasting in their construction. We justify our choice of datasets, and show that they are interestingly distinct from each other, and together pose a challenge to our architectural refinement algorithm. Our results substantiate the usefulness of the proposed method.



