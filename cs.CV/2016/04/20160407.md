# Arxiv Papers in cs.CV on 2016-04-07
### A Classification Leveraged Object Detector
- **Arxiv ID**: http://arxiv.org/abs/1604.01841v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01841v1)
- **Published**: 2016-04-07 01:11:50+00:00
- **Updated**: 2016-04-07 01:11:50+00:00
- **Authors**: Miao Sun, Tony X. Han, Zhihai He
- **Comment**: Work in 2013, which contained some detailed algorithms for PASCAL VOC
  2012 detection competition
- **Journal**: None
- **Summary**: Currently, the state-of-the-art image classification algorithms outperform the best available object detector by a big margin in terms of average precision. We, therefore, propose a simple yet principled approach that allows us to leverage object detection through image classification on supporting regions specified by a preliminary object detector. Using a simple bag-of- words model based image classification algorithm, we leveraged the performance of the deformable model objector from 35.9% to 39.5% in average precision over 20 categories on standard PASCAL VOC 2007 detection dataset.



### Joint Detection and Identification Feature Learning for Person Search
- **Arxiv ID**: http://arxiv.org/abs/1604.01850v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01850v3)
- **Published**: 2016-04-07 02:16:26+00:00
- **Updated**: 2017-04-06 01:31:08+00:00
- **Authors**: Tong Xiao, Shuang Li, Bochao Wang, Liang Lin, Xiaogang Wang
- **Comment**: CVPR 2017 camera-ready
- **Journal**: None
- **Summary**: Existing person re-identification benchmarks and methods mainly focus on matching cropped pedestrian images between queries and candidates. However, it is different from real-world scenarios where the annotations of pedestrian bounding boxes are unavailable and the target person needs to be searched from a gallery of whole scene images. To close the gap, we propose a new deep learning framework for person search. Instead of breaking it down into two separate tasks---pedestrian detection and person re-identification, we jointly handle both aspects in a single convolutional neural network. An Online Instance Matching (OIM) loss function is proposed to train the network effectively, which is scalable to datasets with numerous identities. To validate our approach, we collect and annotate a large-scale benchmark dataset for person search. It contains 18,184 images, 8,432 identities, and 96,143 pedestrian bounding boxes. Experiments show that our framework outperforms other separate approaches, and the proposed OIM loss function converges much faster and better than the conventional Softmax loss.



### GIFT: A Real-time and Scalable 3D Shape Search Engine
- **Arxiv ID**: http://arxiv.org/abs/1604.01879v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01879v2)
- **Published**: 2016-04-07 05:45:56+00:00
- **Updated**: 2017-03-31 07:30:06+00:00
- **Authors**: Song Bai, Xiang Bai, Zhichao Zhou, Zhaoxiang Zhang, Longin Jan Latecki
- **Comment**: accepted by CVPR16, achieved the first place in Shrec2016
  competition: Large-Scale 3D Shape Retrieval under the perturbed case
- **Journal**: None
- **Summary**: Projective analysis is an important solution for 3D shape retrieval, since human visual perceptions of 3D shapes rely on various 2D observations from different view points. Although multiple informative and discriminative views are utilized, most projection-based retrieval systems suffer from heavy computational cost, thus cannot satisfy the basic requirement of scalability for search engines. In this paper, we present a real-time 3D shape search engine based on the projective images of 3D shapes. The real-time property of our search engine results from the following aspects: (1) efficient projection and view feature extraction using GPU acceleration; (2) the first inverted file, referred as F-IF, is utilized to speed up the procedure of multi-view matching; (3) the second inverted file (S-IF), which captures a local distribution of 3D shapes in the feature manifold, is adopted for efficient context-based re-ranking. As a result, for each query the retrieval task can be finished within one second despite the necessary cost of IO overhead. We name the proposed 3D shape search engine, which combines GPU acceleration and Inverted File Twice, as GIFT. Besides its high efficiency, GIFT also outperforms the state-of-the-art methods significantly in retrieval accuracy on various shape benchmarks and competitions.



### Reinterpreting the Transformation Posterior in Probabilistic Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1604.01889v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01889v1)
- **Published**: 2016-04-07 06:51:21+00:00
- **Updated**: 2016-04-07 06:51:21+00:00
- **Authors**: Jie Luo, Karteek Popuri, Dana Cobzas, Hongyi Ding, Masashi Sugiyama
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: Probabilistic image registration methods estimate the posterior distribution of transformation. The conventional way of interpreting the transformation posterior is to use the mode as the most likely transformation and assign its corresponding intensity to the registered voxel. Meanwhile, summary statistics of the posterior are employed to evaluate the registration uncertainty, that is the trustworthiness of the registered image. Despite the wide acceptance, this convention has never been justified. In this paper, based on illustrative examples, we question the correctness and usefulness of conventional methods. In order to faithfully translate the transformation posterior, we propose to encode the variability of values into a novel data type called ensemble fields. Ensemble fields can serve as a complement to the registered image and a foundation for developing advanced methods to characterize the uncertainty in registration-based tasks. We demonstrate the potential of ensemble fields by pilot examples



### A CNN Based Scene Chinese Text Recognition Algorithm With Synthetic Data Engine
- **Arxiv ID**: http://arxiv.org/abs/1604.01891v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01891v1)
- **Published**: 2016-04-07 07:08:25+00:00
- **Updated**: 2016-04-07 07:08:25+00:00
- **Authors**: Xiaohang Ren, Kai Chen, Jun Sun
- **Comment**: 2 pages, DAS 2016 short paper
- **Journal**: None
- **Summary**: Scene text recognition plays an important role in many computer vision applications. The small size of available public available scene text datasets is the main challenge when training a text recognition CNN model. In this paper, we propose a CNN based Chinese text recognition algorithm. To enlarge the dataset for training the CNN model, we design a synthetic data engine for Chinese scene character generation, which generates representative character images according to the fonts use frequency of Chinese texts. As the Chinese text is more complex, the English text recognition CNN architecture is modified for Chinese text. To ensure the small size nature character dataset and the large size artificial character dataset are comparable in training, the CNN model are trained progressively. The proposed Chinese text recognition algorithm is evaluated with two Chinese text datasets. The algorithm achieves better recognize accuracy compared to the baseline methods.



### A Novel Scene Text Detection Algorithm Based On Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1604.01894v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01894v1)
- **Published**: 2016-04-07 07:16:35+00:00
- **Updated**: 2016-04-07 07:16:35+00:00
- **Authors**: Xiaohang Ren, Kai Chen, Jun Sun
- **Comment**: 5 pages, IWPR 2016
- **Journal**: None
- **Summary**: Candidate text region extraction plays a critical role in convolutional neural network (CNN) based text detection from natural images. In this paper, we propose a CNN based scene text detection algorithm with a new text region extractor. The so called candidate text region extractor I-MSER is based on Maximally Stable Extremal Region (MSER), which can improve the independency and completeness of the extracted candidate text regions. Design of I-MSER is motivated by the observation that text MSERs have high similarity and are close to each other. The independency of candidate text regions obtained by I-MSER is guaranteed by selecting the most representative regions from a MSER tree which is generated according to the spatial overlapping relationship among the MSERs. A multi-layer CNN model is trained to score the confidence value of the extracted regions extracted by the I-MSER for text detection. The new text detection algorithm based on I-MSER is evaluated with wide-used ICDAR 2011 and 2013 datasets and shows improved detection performance compared to the existing algorithms.



### Geometric Scene Parsing with Hierarchical LSTM
- **Arxiv ID**: http://arxiv.org/abs/1604.01931v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01931v2)
- **Published**: 2016-04-07 09:20:51+00:00
- **Updated**: 2016-04-08 06:21:57+00:00
- **Authors**: Zhanglin Peng, Ruimao Zhang, Xiaodan Liang, Xiaobai Liu, Liang Lin
- **Comment**: To be presented at IJCAI'16
- **Journal**: None
- **Summary**: This paper addresses the problem of geometric scene parsing, i.e. simultaneously labeling geometric surfaces (e.g. sky, ground and vertical plane) and determining the interaction relations (e.g. layering, supporting, siding and affinity) between main regions. This problem is more challenging than the traditional semantic scene labeling, as recovering geometric structures necessarily requires the rich and diverse contextual information. To achieve these goals, we propose a novel recurrent neural network model, named Hierarchical Long Short-Term Memory (H-LSTM). It contains two coupled sub-networks: the Pixel LSTM (P-LSTM) and the Multi-scale Super-pixel LSTM (MS-LSTM) for handling the surface labeling and relation prediction, respectively. The two sub-networks provide complementary information to each other to exploit hierarchical scene contexts, and they are jointly optimized for boosting the performance. Our extensive experiments show that our model is capable of parsing scene geometric structures and outperforming several state-of-the-art methods by large margins. In addition, we show promising 3D reconstruction results from the still images based on the geometric parsing.



### Automatic Content-aware Non-Photorealistic Rendering of Images
- **Arxiv ID**: http://arxiv.org/abs/1604.01962v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01962v4)
- **Published**: 2016-04-07 11:31:03+00:00
- **Updated**: 2016-04-19 12:19:59+00:00
- **Authors**: Akshay Gadi Patil, Shanmuganathan Raman
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Non-photorealistic rendering techniques work on image features and often manipulate a set of characteristics such as edges and texture to achieve a desired depiction of the scene. Most computational photography methods decompose an image using edge preserving filters and work on the resulting base and detail layers independently to achieve desired visual effects. We propose a new approach for content-aware non-photorealistic rendering of images where we manipulate the visually salient and the non-salient regions separately. We propose a novel content-aware framework in order to render an image for applications such as detail exaggeration, artificial blurring and image abstraction. The processed regions of the image are blended seamlessly for all these applications. We demonstrate that content awareness of the proposed method leads to automatic generation of non-photorealistic rendering of the same image for the different applications mentioned above.



### Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies
- **Arxiv ID**: http://arxiv.org/abs/1604.01980v2
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1604.01980v2)
- **Published**: 2016-04-07 12:43:07+00:00
- **Updated**: 2016-10-10 16:42:55+00:00
- **Authors**: Emanuel Laude, Thomas Möllenhoff, Michael Moeller, Jan Lellmann, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: Convex relaxations of nonconvex multilabel problems have been demonstrated to produce superior (provably optimal or near-optimal) solutions to a variety of classical computer vision problems. Yet, they are of limited practical use as they require a fine discretization of the label space, entailing a huge demand in memory and runtime. In this work, we propose the first sublabel accurate convex relaxation for vectorial multilabel problems. The key idea is that we approximate the dataterm of the vectorial labeling problem in a piecewise convex (rather than piecewise linear) manner. As a result we have a more faithful approximation of the original cost function that provides a meaningful interpretation for the fractional solutions of the relaxed convex problem. In numerous experiments on large-displacement optical flow estimation and on color image denoising we demonstrate that the computed solutions have superior quality while requiring much lower memory and runtime.



### Edge Detection Based Shape Identification
- **Arxiv ID**: http://arxiv.org/abs/1604.02030v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.02030v1)
- **Published**: 2016-04-07 15:09:08+00:00
- **Updated**: 2016-04-07 15:09:08+00:00
- **Authors**: Vivek Kumar, Sumit Pandey, Amrindra Pal, Sandeep Sharma
- **Comment**: Presented in the National Conference on Emerging Trends in
  Engineering Science & Technology
- **Journal**: None
- **Summary**: Image recognition is the need of the hour. In order to be able to recognize an image, it is of immense importance that the image should be distinguishable from the background. In the present work, an approach is presented for automatic detection and recognition of regular 2D shapes in low noise environments. The work has a large number of direct applications in the real world. The algorithm proposed is based on locating the edges and thus, in turn calculating the area of the object helps in identification of a specified shape. The results were simulated using MATLAB tool are encouraging and validate the proposed algorithm.   Index Terms: Edge Detection, Area Calculation, Shape Detection, Object Recognition



### 3-D Hand Pose Estimation from Kinect's Point Cloud Using Appearance Matching
- **Arxiv ID**: http://arxiv.org/abs/1604.02032v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.02032v1)
- **Published**: 2016-04-07 15:16:17+00:00
- **Updated**: 2016-04-07 15:16:17+00:00
- **Authors**: Pasquale Coscia, Francesco A. N. Palmieri, Francesco Castaldo, Alberto Cavallo
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel appearance-based approach for pose estimation of a human hand using the point clouds provided by the low-cost Microsoft Kinect sensor. Both the free-hand case, in which the hand is isolated from the surrounding environment, and the hand-object case, in which the different types of interactions are classified, have been considered. The hand-object case is clearly the most challenging task having to deal with multiple tracks. The approach proposed here belongs to the class of partial pose estimation where the estimated pose in a frame is used for the initialization of the next one. The pose estimation is obtained by applying a modified version of the Iterative Closest Point (ICP) algorithm to synthetic models to obtain the rigid transformation that aligns each model with respect to the input data. The proposed framework uses a "pure" point cloud as provided by the Kinect sensor without any other information such as RGB values or normal vector components. For this reason, the proposed method can also be applied to data obtained from other types of depth sensor, or RGB-D camera.



### A robust autoassociative memory with coupled networks of Kuramoto-type oscillators
- **Arxiv ID**: http://arxiv.org/abs/1604.02085v2
- **DOI**: 10.1103/PhysRevE.94.022309
- **Categories**: **nlin.AO**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1604.02085v2)
- **Published**: 2016-04-07 17:31:13+00:00
- **Updated**: 2016-06-28 06:10:29+00:00
- **Authors**: Daniel Heger, Katharina Krischer
- **Comment**: None
- **Journal**: None
- **Summary**: Uncertain recognition success, unfavorable scaling of connection complexity or dependence on complex external input impair the usefulness of current oscillatory neural networks for pattern recognition or restrict technical realizations to small networks. We propose a new network architecture of coupled oscillators for pattern recognition which shows none of the mentioned aws. Furthermore we illustrate the recognition process with simulation results and analyze the new dynamics analytically: Possible output patterns are isolated attractors of the system. Additionally, simple criteria for recognition success are derived from a lower bound on the basins of attraction.



### Trajectory Aligned Features For First Person Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1604.02115v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.02115v1)
- **Published**: 2016-04-07 19:09:07+00:00
- **Updated**: 2016-04-07 19:09:07+00:00
- **Authors**: Suriya Singh, Chetan Arora, C. V. Jawahar
- **Comment**: None
- **Journal**: None
- **Summary**: Egocentric videos are characterised by their ability to have the first person view. With the popularity of Google Glass and GoPro, use of egocentric videos is on the rise. Recognizing action of the wearer from egocentric videos is an important problem. Unstructured movement of the camera due to natural head motion of the wearer causes sharp changes in the visual field of the egocentric camera causing many standard third person action recognition techniques to perform poorly on such videos. Objects present in the scene and hand gestures of the wearer are the most important cues for first person action recognition but are difficult to segment and recognize in an egocentric video. We propose a novel representation of the first person actions derived from feature trajectories. The features are simple to compute using standard point tracking and does not assume segmentation of hand/objects or recognizing object or hand pose unlike in many previous approaches. We train a bag of words classifier with the proposed features and report a performance improvement of more than 11% on publicly available datasets. Although not designed for the particular case, we show that our technique can also recognize wearer's actions when hands or objects are not visible.



### Resolving Language and Vision Ambiguities Together: Joint Segmentation & Prepositional Attachment Resolution in Captioned Scenes
- **Arxiv ID**: http://arxiv.org/abs/1604.02125v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1604.02125v4)
- **Published**: 2016-04-07 19:26:56+00:00
- **Updated**: 2016-09-26 04:08:19+00:00
- **Authors**: Gordon Christie, Ankit Laddha, Aishwarya Agrawal, Stanislaw Antol, Yash Goyal, Kevin Kochersberger, Dhruv Batra
- **Comment**: *The first two authors contributed equally. Conference on Empirical
  Methods in Natural Language Processing (EMNLP) 2016
- **Journal**: None
- **Summary**: We present an approach to simultaneously perform semantic segmentation and prepositional phrase attachment resolution for captioned images. Some ambiguities in language cannot be resolved without simultaneously reasoning about an associated image. If we consider the sentence "I shot an elephant in my pajamas", looking at language alone (and not using common sense), it is unclear if it is the person or the elephant wearing the pajamas or both. Our approach produces a diverse set of plausible hypotheses for both semantic segmentation and prepositional phrase attachment resolution that are then jointly reranked to select the most consistent pair. We show that our semantic segmentation and prepositional phrase attachment resolution modules have complementary strengths, and that joint reasoning produces more accurate results than any module operating in isolation. Multiple hypotheses are also shown to be crucial to improved multiple-module reasoning. Our vision and language approach significantly outperforms the Stanford Parser (De Marneffe et al., 2006) by 17.91% (28.69% relative) and 12.83% (25.28% relative) in two different experiments. We also make small improvements over DeepLab-CRF (Chen et al., 2015).



### Horizon Lines in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1604.02129v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.02129v2)
- **Published**: 2016-04-07 19:38:24+00:00
- **Updated**: 2016-08-16 18:48:57+00:00
- **Authors**: Scott Workman, Menghua Zhai, Nathan Jacobs
- **Comment**: British Machine Vision Conference (BMVC) 2016
- **Journal**: None
- **Summary**: The horizon line is an important contextual attribute for a wide variety of image understanding tasks. As such, many methods have been proposed to estimate its location from a single image. These methods typically require the image to contain specific cues, such as vanishing points, coplanar circles, and regular textures, thus limiting their real-world applicability. We introduce a large, realistic evaluation dataset, Horizon Lines in the Wild (HLW), containing natural images with labeled horizon lines. Using this dataset, we investigate the application of convolutional neural networks for directly estimating the horizon line, without requiring any explicit geometric constraints or other special cues. An extensive evaluation shows that using our CNNs, either in isolation or in conjunction with a previous geometric approach, we achieve state-of-the-art results on the challenging HLW dataset and two existing benchmark datasets.



### A MultiPath Network for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1604.02135v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.02135v2)
- **Published**: 2016-04-07 19:43:47+00:00
- **Updated**: 2016-08-08 13:29:02+00:00
- **Authors**: Sergey Zagoruyko, Adam Lerer, Tsung-Yi Lin, Pedro O. Pinheiro, Sam Gross, Soumith Chintala, Piotr Dollár
- **Comment**: None
- **Journal**: None
- **Summary**: The recent COCO object detection dataset presents several new challenges for object detection. In particular, it contains objects at a broad range of scales, less prototypical images, and requires more precise localization. To address these challenges, we test three modifications to the standard Fast R-CNN object detector: (1) skip connections that give the detector access to features at multiple network layers, (2) a foveal structure to exploit object context at multiple object resolutions, and (3) an integral loss function and corresponding network adjustment that improve localization. The result of these modifications is that information can flow along multiple paths in our network, including through features from multiple network layers and from multiple object views. We refer to our modified classifier as a "MultiPath" network. We couple our MultiPath network with DeepMask object proposals, which are well suited for localization and small objects, and adapt our pipeline to predict segmentation masks in addition to bounding boxes. The combined system improves results over the baseline Fast R-CNN detector with Selective Search by 66% overall and by 4x on small objects. It placed second in both the COCO 2015 detection and segmentation challenges.



### A Semi-Lagrangian two-level preconditioned Newton-Krylov solver for constrained diffeomorphic image registration
- **Arxiv ID**: http://arxiv.org/abs/1604.02153v2
- **DOI**: 10.1137/16M1070475
- **Categories**: **math.OC**, cs.CV, 68U10, 49J20, 35Q93, 65K10, 65F08, 76D55
- **Links**: [PDF](http://arxiv.org/pdf/1604.02153v2)
- **Published**: 2016-04-07 20:00:11+00:00
- **Updated**: 2018-02-28 20:16:09+00:00
- **Authors**: Andreas Mang, George Biros
- **Comment**: None
- **Journal**: SIAM Journal on Scientific Computing, 39(6):B1064-B1101, 2017
- **Summary**: We propose an efficient numerical algorithm for the solution of diffeomorphic image registration problems. We use a variational formulation constrained by a partial differential equation (PDE), where the constraints are a scalar transport equation.   We use a pseudospectral discretization in space and second-order accurate semi-Lagrangian time stepping scheme for the transport equations. We solve for a stationary velocity field using a preconditioned, globalized, matrix-free Newton-Krylov scheme. We propose and test a two-level Hessian preconditioner. We consider two strategies for inverting the preconditioner on the coarse grid: a nested preconditioned conjugate gradient method (exact solve) and a nested Chebyshev iterative method (inexact solve) with a fixed number of iterations.   We test the performance of our solver in different synthetic and real-world two-dimensional application scenarios. We study grid convergence and computational efficiency of our new scheme. We compare the performance of our solver against our initial implementation that uses the same spatial discretization but a standard, explicit, second-order Runge-Kutta scheme for the numerical time integration of the transport equations and a single-level preconditioner. Our improved scheme delivers significant speedups over our original implementation. As a highlight, we observe a 20$\times$ speedup for a two dimensional, real world multi-subject medical image registration problem.



### Families in the Wild (FIW): Large-Scale Kinship Image Database and Benchmarks
- **Arxiv ID**: http://arxiv.org/abs/1604.02182v2
- **DOI**: 10.1145/2964284.2967219
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.02182v2)
- **Published**: 2016-04-07 21:45:53+00:00
- **Updated**: 2017-05-04 03:15:48+00:00
- **Authors**: Joseph P. Robinson, Ming Shao, Yue Wu, Yun Fu
- **Comment**: None
- **Journal**: ACM MM (2016) 242-246
- **Summary**: We present the largest kinship recognition dataset to date, Families in the Wild (FIW). Motivated by the lack of a single, unified dataset for kinship recognition, we aim to provide a dataset that captivates the interest of the research community. With only a small team, we were able to collect, organize, and label over 10,000 family photos of 1,000 families with our annotation tool designed to mark complex hierarchical relationships and local label information in a quick and efficient manner. We include several benchmarks for two image-based tasks, kinship verification and family recognition. For this, we incorporate several visual features and metric learning methods as baselines. Also, we demonstrate that a pre-trained Convolutional Neural Network (CNN) as an off-the-shelf feature extractor outperforms the other feature types. Then, results were further boosted by fine-tuning two deep CNNs on FIW data: (1) for kinship verification, a triplet loss function was learned on top of the network of pre-trained weights; (2) for family recognition, a family-specific softmax classifier was added to the network.



