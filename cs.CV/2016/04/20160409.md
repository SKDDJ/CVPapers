# Arxiv Papers in cs.CV on 2016-04-09
### Person Re-identification in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1604.02531v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.02531v2)
- **Published**: 2016-04-09 06:57:28+00:00
- **Updated**: 2017-04-06 15:02:40+00:00
- **Authors**: Liang Zheng, Hengheng Zhang, Shaoyan Sun, Manmohan Chandraker, Yi Yang, Qi Tian
- **Comment**: accepted as spotlight to CVPR 2017
- **Journal**: None
- **Summary**: We present a novel large-scale dataset and comprehensive baselines for end-to-end pedestrian detection and person recognition in raw video frames. Our baselines address three issues: the performance of various combinations of detectors and recognizers, mechanisms for pedestrian detection to help improve overall re-identification accuracy and assessing the effectiveness of different detectors for re-identification. We make three distinct contributions. First, a new dataset, PRW, is introduced to evaluate Person Re-identification in the Wild, using videos acquired through six synchronized cameras. It contains 932 identities and 11,816 frames in which pedestrians are annotated with their bounding box positions and identities. Extensive benchmarking results are presented on this dataset. Second, we show that pedestrian detection aids re-identification through two simple yet effective improvements: a discriminatively trained ID-discriminative Embedding (IDE) in the person subspace using convolutional neural network (CNN) features and a Confidence Weighted Similarity (CWS) metric that incorporates detection scores into similarity measurement. Third, we derive insights in evaluating detector performance for the particular scenario of accurate person re-identification.



### T-CNN: Tubelets with Convolutional Neural Networks for Object Detection from Videos
- **Arxiv ID**: http://arxiv.org/abs/1604.02532v4
- **DOI**: 10.1109/TCSVT.2017.2736553
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.02532v4)
- **Published**: 2016-04-09 07:07:47+00:00
- **Updated**: 2017-08-03 07:03:49+00:00
- **Authors**: Kai Kang, Hongsheng Li, Junjie Yan, Xingyu Zeng, Bin Yang, Tong Xiao, Cong Zhang, Zhe Wang, Ruohui Wang, Xiaogang Wang, Wanli Ouyang
- **Comment**: ImageNet 2015 VID challenge tech report. The first two authors share
  co-first authorship. Accepted as a Transaction paper by T-CSVT Special Issue
  on Large Scale and Nonlinear Similarity Learning for Intelligent Video
  Analysis
- **Journal**: None
- **Summary**: The state-of-the-art performance for object detection has been significantly improved over the past two years. Besides the introduction of powerful deep neural networks such as GoogleNet and VGG, novel object detection frameworks such as R-CNN and its successors, Fast R-CNN and Faster R-CNN, play an essential role in improving the state-of-the-art. Despite their effectiveness on still images, those frameworks are not specifically designed for object detection from videos. Temporal and contextual information of videos are not fully investigated and utilized. In this work, we propose a deep learning framework that incorporates temporal and contextual information from tubelets obtained in videos, which dramatically improves the baseline performance of existing still-image detection frameworks when they are applied to videos. It is called T-CNN, i.e. tubelets with convolutional neueral networks. The proposed framework won the recently introduced object-detection-from-video (VID) task with provided data in the ImageNet Large-Scale Visual Recognition Challenge 2015 (ILSVRC2015).



### Scene-driven Retrieval in Edited Videos using Aesthetic and Semantic Deep Features
- **Arxiv ID**: http://arxiv.org/abs/1604.02546v1
- **DOI**: 10.1145/2911996.2912012
- **Categories**: **cs.CV**, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1604.02546v1)
- **Published**: 2016-04-09 09:41:14+00:00
- **Updated**: 2016-04-09 09:41:14+00:00
- **Authors**: Lorenzo Baraldi, Costantino Grana, Rita Cucchiara
- **Comment**: ICMR 2016
- **Journal**: None
- **Summary**: This paper presents a novel retrieval pipeline for video collections, which aims to retrieve the most significant parts of an edited video for a given query, and represent them with thumbnails which are at the same time semantically meaningful and aesthetically remarkable. Videos are first segmented into coherent and story-telling scenes, then a retrieval algorithm based on deep learning is proposed to retrieve the most significant scenes for a textual query. A ranking strategy based on deep features is finally used to tackle the problem of visualizing the best thumbnail. Qualitative and quantitative experiments are conducted on a collection of edited videos to demonstrate the effectiveness of our approach.



