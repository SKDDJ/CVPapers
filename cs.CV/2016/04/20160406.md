# Arxiv Papers in cs.CV on 2016-04-06
### Learning A Deep $\ell_\infty$ Encoder for Hashing
- **Arxiv ID**: http://arxiv.org/abs/1604.01475v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1604.01475v1)
- **Published**: 2016-04-06 03:54:33+00:00
- **Updated**: 2016-04-06 03:54:33+00:00
- **Authors**: Zhangyang Wang, Yingzhen Yang, Shiyu Chang, Qing Ling, Thomas S. Huang
- **Comment**: To be presented at IJCAI'16
- **Journal**: None
- **Summary**: We investigate the $\ell_\infty$-constrained representation which demonstrates robustness to quantization errors, utilizing the tool of deep learning. Based on the Alternating Direction Method of Multipliers (ADMM), we formulate the original convex minimization problem as a feed-forward neural network, named \textit{Deep $\ell_\infty$ Encoder}, by introducing the novel Bounded Linear Unit (BLU) neuron and modeling the Lagrange multipliers as network biases. Such a structural prior acts as an effective network regularization, and facilitates the model initialization. We then investigate the effective use of the proposed model in the application of hashing, by coupling the proposed encoders under a supervised pairwise loss, to develop a \textit{Deep Siamese $\ell_\infty$ Network}, which can be optimized from end to end. Extensive experiments demonstrate the impressive performances of the proposed model. We also provide an in-depth analysis of its behaviors against the competitors.



### A Focused Dynamic Attention Model for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1604.01485v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1604.01485v1)
- **Published**: 2016-04-06 05:16:10+00:00
- **Updated**: 2016-04-06 05:16:10+00:00
- **Authors**: Ilija Ilievski, Shuicheng Yan, Jiashi Feng
- **Comment**: Submitted to ECCV 2016
- **Journal**: None
- **Summary**: Visual Question and Answering (VQA) problems are attracting increasing interest from multiple research disciplines. Solving VQA problems requires techniques from both computer vision for understanding the visual contents of a presented image or video, as well as the ones from natural language processing for understanding semantics of the question and generating the answers. Regarding visual content modeling, most of existing VQA methods adopt the strategy of extracting global features from the image or video, which inevitably fails in capturing fine-grained information such as spatial configuration of multiple objects. Extracting features from auto-generated regions -- as some region-based image recognition methods do -- cannot essentially address this problem and may introduce some overwhelming irrelevant features with the question. In this work, we propose a novel Focused Dynamic Attention (FDA) model to provide better aligned image content representation with proposed questions. Being aware of the key words in the question, FDA employs off-the-shelf object detector to identify important regions and fuse the information from the regions and global features via an LSTM unit. Such question-driven representations are then combined with question representation and fed into a reasoning unit for generating the answers. Extensive evaluation on a large-scale benchmark dataset, VQA, clearly demonstrate the superior performance of FDA over well-established baselines.



### Keyboard Based Control of Four Dimensional Rotations
- **Arxiv ID**: http://arxiv.org/abs/1604.02013v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1604.02013v1)
- **Published**: 2016-04-06 05:20:04+00:00
- **Updated**: 2016-04-06 05:20:04+00:00
- **Authors**: Akira Kageyama
- **Comment**: Accepted for publication in Journal of Visualization
- **Journal**: None
- **Summary**: Aiming at applications to the scientific visualization of three dimensional simulations with time evolution, a keyboard based control method to specify rotations in four dimensions is proposed. It is known that four dimensional rotations are generally so-called double rotations, and a double rotation is a combination of simultaneously applied two simple rotations. The proposed method can specify both the simple and double rotations by single key typings of the keyboard. The method is tested in visualizations of a regular pentachoron in four dimensional space by a hyperplane slicing.



### How Does the Low-Rank Matrix Decomposition Help Internal and External Learnings for Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1604.01497v3
- **DOI**: 10.1109/TIP.2017.2768185
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01497v3)
- **Published**: 2016-04-06 06:03:02+00:00
- **Updated**: 2017-06-30 04:17:10+00:00
- **Authors**: Shuang Wang, Bo Yue, Xuefeng Liang, Peiyuan Ji, Licheng Jiao
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing ( Volume: 27, Issue: 3,
  March 2018)
- **Summary**: Wisely utilizing the internal and external learning methods is a new challenge in super-resolution problem. To address this issue, we analyze the attributes of two methodologies and find two observations of their recovered details: 1) they are complementary in both feature space and image plane, 2) they distribute sparsely in the spatial space. These inspire us to propose a low-rank solution which effectively integrates two learning methods and then achieves a superior result. To fit this solution, the internal learning method and the external learning method are tailored to produce multiple preliminary results. Our theoretical analysis and experiment prove that the proposed low-rank solution does not require massive inputs to guarantee the performance, and thereby simplifying the design of two learning methods for the solution. Intensive experiments show the proposed solution improves the single learning method in both qualitative and quantitative assessments. Surprisingly, it shows more superior capability on noisy images and outperforms state-of-the-art methods.



### LOMo: Latent Ordinal Model for Facial Analysis in Videos
- **Arxiv ID**: http://arxiv.org/abs/1604.01500v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01500v1)
- **Published**: 2016-04-06 06:14:58+00:00
- **Updated**: 2016-04-06 06:14:58+00:00
- **Authors**: Karan Sikka, Gaurav Sharma, Marian Bartlett
- **Comment**: 2016 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)
- **Journal**: None
- **Summary**: We study the problem of facial analysis in videos. We propose a novel weakly supervised learning method that models the video event (expression, pain etc.) as a sequence of automatically mined, discriminative sub-events (eg. onset and offset phase for smile, brow lower and cheek raise for pain). The proposed model is inspired by the recent works on Multiple Instance Learning and latent SVM/HCRF- it extends such frameworks to model the ordinal or temporal aspect in the videos, approximately. We obtain consistent improvements over relevant competitive baselines on four challenging and publicly available video based facial analysis datasets for prediction of expression, clinical pain and intent in dyadic conversations. In combination with complimentary features, we report state-of-the-art results on these datasets.



### Training Constrained Deconvolutional Networks for Road Scene Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1604.01545v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01545v1)
- **Published**: 2016-04-06 09:02:50+00:00
- **Updated**: 2016-04-06 09:02:50+00:00
- **Authors**: German Ros, Simon Stent, Pablo F. Alcantarilla, Tomoki Watanabe
- **Comment**: submitted as a conference paper
- **Journal**: None
- **Summary**: In this work we investigate the problem of road scene semantic segmentation using Deconvolutional Networks (DNs). Several constraints limit the practical performance of DNs in this context: firstly, the paucity of existing pixel-wise labelled training data, and secondly, the memory constraints of embedded hardware, which rule out the practical use of state-of-the-art DN architectures such as fully convolutional networks (FCN). To address the first constraint, we introduce a Multi-Domain Road Scene Semantic Segmentation (MDRS3) dataset, aggregating data from six existing densely and sparsely labelled datasets for training our models, and two existing, separate datasets for testing their generalisation performance. We show that, while MDRS3 offers a greater volume and variety of data, end-to-end training of a memory efficient DN does not yield satisfactory performance. We propose a new training strategy to overcome this, based on (i) the creation of a best-possible source network (S-Net) from the aggregated data, ignoring time and memory constraints; and (ii) the transfer of knowledge from S-Net to the memory-efficient target network (T-Net). We evaluate different techniques for S-Net creation and T-Net transferral, and demonstrate that training a constrained deconvolutional network in this manner can unlock better performance than existing training approaches. Specifically, we show that a target network can be trained to achieve improved accuracy versus an FCN despite using less than 1\% of the memory. We believe that our approach can be useful beyond automotive scenarios where labelled data is similarly scarce or fragmented and where practical constraints exist on the desired model size. We make available our network models and aggregated multi-domain dataset for reproducibility.



### Fast $(1+ε)$-approximation of the Löwner extremal matrices of high-dimensional symmetric matrices
- **Arxiv ID**: http://arxiv.org/abs/1604.01592v1
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1604.01592v1)
- **Published**: 2016-04-06 12:39:20+00:00
- **Updated**: 2016-04-06 12:39:20+00:00
- **Authors**: Frank Nielsen, Richard Nock
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Matrix data sets are common nowadays like in biomedical imaging where the Diffusion Tensor Magnetic Resonance Imaging (DT-MRI) modality produces data sets of   3D symmetric positive definite matrices anchored at voxel positions capturing the anisotropic diffusion properties of water molecules in biological tissues. The space of symmetric matrices can be partially ordered using the L\"owner ordering, and computing extremal matrices dominating a given set of matrices is a basic primitive used in matrix-valued signal processing. In this letter, we design a fast and easy-to-implement iterative algorithm to approximate arbitrarily finely these extremal matrices. Finally, we discuss on extensions to matrix clustering.



### Correlated and Individual Multi-Modal Deep Learning for RGB-D Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1604.01655v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01655v3)
- **Published**: 2016-04-06 15:06:02+00:00
- **Updated**: 2016-12-09 13:56:02+00:00
- **Authors**: Ziyan Wang, Jiwen Lu, Ruogu Lin, Jianjiang Feng, Jie zhou
- **Comment**: 11 pages, 7 figures, submitted to a conference in 2016
- **Journal**: None
- **Summary**: In this paper, we propose a new correlated and individual multi-modal deep learning (CIMDL) method for RGB-D object recognition. Unlike most conventional RGB-D object recognition methods which extract features from the RGB and depth channels individually, our CIMDL jointly learns feature representations from raw RGB-D data with a pair of deep neural networks, so that the sharable and modal-specific information can be simultaneously exploited. Specifically, we construct a pair of deep convolutional neural networks (CNNs) for the RGB and depth data, and concatenate them at the top layer of the network with a loss function which learns a new feature space where both correlated part and the individual part of the RGB-D information are well modelled. The parameters of the whole networks are updated by using the back-propagation criterion. Experimental results on two widely used RGB-D object image benchmark datasets clearly show that our method outperforms state-of-the-arts.



### A Survey on Bayesian Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1604.01662v4
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1604.01662v4)
- **Published**: 2016-04-06 15:35:08+00:00
- **Updated**: 2021-01-06 03:14:13+00:00
- **Authors**: Hao Wang, Dit-Yan Yeung
- **Comment**: Published in ACM Computing Surveys (CSUR) 2020. Constantly updating
  project page at https://github.com/js05212/BayesianDeepLearning-Survey
- **Journal**: None
- **Summary**: A comprehensive artificial intelligence system needs to not only perceive the environment with different `senses' (e.g., seeing and hearing) but also infer the world's conditional (or even causal) relations and corresponding uncertainty. The past decade has seen major advances in many perception tasks such as visual object recognition and speech recognition using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. In recent years, Bayesian deep learning has emerged as a unified probabilistic framework to tightly integrate deep learning and Bayesian models. In this general framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in turn, the feedback from the inference process is able to enhance the perception of text or images. This survey provides a comprehensive introduction to Bayesian deep learning and reviews its recent applications on recommender systems, topic models, control, etc. Besides, we also discuss the relationship and differences between Bayesian deep learning and other related topics such as Bayesian treatment of neural networks. For a constantly updating project page, please refer to https://github.com/js05212/BayesianDeepLearning-Survey.



### The Cityscapes Dataset for Semantic Urban Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/1604.01685v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01685v2)
- **Published**: 2016-04-06 16:34:33+00:00
- **Updated**: 2016-04-07 15:39:22+00:00
- **Authors**: Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele
- **Comment**: Includes supplemental material
- **Journal**: None
- **Summary**: Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes.   To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.



### Reading Between the Pixels: Photographic Steganography for Camera Display Messaging
- **Arxiv ID**: http://arxiv.org/abs/1604.01720v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.MM, cs.NI, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1604.01720v1)
- **Published**: 2016-04-06 18:43:18+00:00
- **Updated**: 2016-04-06 18:43:18+00:00
- **Authors**: Eric Wengrowski, Kristin Dana, Marco Gruteser, Narayan Mandayam
- **Comment**: 16 pages with references 8 tables and figures
- **Journal**: None
- **Summary**: We exploit human color metamers to send light-modulated messages less visible to the human eye, but recoverable by cameras. These messages are a key component to camera-display messaging, such as handheld smartphones capturing information from electronic signage. Each color pixel in the display image is modified by a particular color gradient vector. The challenge is to find the color gradient that maximizes camera response, while minimizing human response. The mismatch in human spectral and camera sensitivity curves creates an opportunity for hidden messaging. Our approach does not require knowledge of these sensitivity curves, instead we employ a data-driven method. We learn an ellipsoidal partitioning of the six-dimensional space of colors and color gradients. This partitioning creates metamer sets defined by the base color at the display pixel and the color gradient direction for message encoding. We sample from the resulting metamer sets to find color steps for each base color to embed a binary message into an arbitrary image with reduced visible artifacts. Unlike previous methods that rely on visually obtrusive intensity modulation, we embed with color so that the message is more hidden. Ordinary displays and cameras are used without the need for expensive LEDs or high speed devices. The primary contribution of this work is a framework to map the pixels in an arbitrary image to a metamer pair for steganographic photo messaging.



### Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text
- **Arxiv ID**: http://arxiv.org/abs/1604.01729v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1604.01729v2)
- **Published**: 2016-04-06 19:01:28+00:00
- **Updated**: 2016-11-29 20:37:42+00:00
- **Authors**: Subhashini Venugopalan, Lisa Anne Hendricks, Raymond Mooney, Kate Saenko
- **Comment**: Accepted at EMNLP 2016. Project page:
  http://vsubhashini.github.io/language_fusion.html
- **Journal**: Proc.EMNLP (2016) pg.1961-1966
- **Summary**: This paper investigates how linguistic knowledge mined from large text corpora can aid the generation of natural language descriptions of videos. Specifically, we integrate both a neural language model and distributional semantics trained on large text corpora into a recent LSTM-based architecture for video description. We evaluate our approach on a collection of Youtube videos as well as two large movie description datasets showing significant improvements in grammaticality while modestly improving descriptive quality.



### Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding
- **Arxiv ID**: http://arxiv.org/abs/1604.01753v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01753v3)
- **Published**: 2016-04-06 19:56:04+00:00
- **Updated**: 2016-07-26 22:49:22+00:00
- **Authors**: Gunnar A. Sigurdsson, Gül Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, Abhinav Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision has a great potential to help our daily lives by searching for lost keys, watering flowers or reminding us to take a pill. To succeed with such tasks, computer vision methods need to be trained from real and diverse examples of our daily dynamic scenes. While most of such scenes are not particularly exciting, they typically do not appear on YouTube, in movies or TV broadcasts. So how do we collect sufficiently many diverse but boring samples representing our lives? We propose a novel Hollywood in Homes approach to collect such data. Instead of shooting videos in the lab, we ensure diversity by distributing and crowdsourcing the whole process of video creation from script writing to video recording and annotation. Following this procedure we collect a new dataset, Charades, with hundreds of people recording videos in their own homes, acting out casual everyday activities. The dataset is composed of 9,848 annotated videos with an average length of 30 seconds, showing activities of 267 people from three continents. Each video is annotated by multiple free-text descriptions, action labels, action intervals and classes of interacted objects. In total, Charades provides 27,847 video descriptions, 66,500 temporally localized intervals for 157 action classes and 41,104 labels for 46 object classes. Using this rich data, we evaluate and provide baseline results for several tasks including action recognition and automatic description generation. We believe that the realism, diversity, and casual nature of this dataset will present unique challenges and new opportunities for computer vision community.



### A Subpath Kernel for Learning Hierarchical Image Representations
- **Arxiv ID**: http://arxiv.org/abs/1604.01787v1
- **DOI**: 10.1007/978-3-319-18224-7_4
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01787v1)
- **Published**: 2016-04-06 20:04:17+00:00
- **Updated**: 2016-04-06 20:04:17+00:00
- **Authors**: Yanwei Cui, Laetitia Chapel, Sébastien Lefèvre
- **Comment**: 10th IAPR-TC-15 International Workshop, GbRPR 2015, Beijing, China,
  May 13-15, 2015. Proceedings
- **Journal**: None
- **Summary**: Tree kernels have demonstrated their ability to deal with hierarchical data, as the intrinsic tree structure often plays a discriminative role. While such kernels have been successfully applied to various domains such as nature language processing and bioinformatics, they mostly concentrate on ordered trees and whose nodes are described by symbolic data. Meanwhile, hierarchical representations have gained increasing interest to describe image content. This is particularly true in remote sensing, where such representations allow for revealing different objects of interest at various scales through a tree structure. However, the induced trees are unordered and the nodes are equipped with numerical features. In this paper, we propose a new structured kernel for hierarchical image representations which is built on the concept of subpath kernel. Experimental results on both artificial and remote sensing datasets show that the proposed kernel manages to deal with the hierarchical nature of the data, leading to better classification rates.



### Learning to Track at 100 FPS with Deep Regression Networks
- **Arxiv ID**: http://arxiv.org/abs/1604.01802v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1604.01802v2)
- **Published**: 2016-04-06 20:39:34+00:00
- **Updated**: 2016-08-16 02:34:48+00:00
- **Authors**: David Held, Sebastian Thrun, Silvio Savarese
- **Comment**: To appear in European Conference on Computer Vision (ECCV) 2016
- **Journal**: None
- **Summary**: Machine learning techniques are often used in computer vision due to their ability to leverage large amounts of training data to improve performance. Unfortunately, most generic object trackers are still trained from scratch online and do not benefit from the large number of videos that are readily available for offline training. We propose a method for offline training of neural networks that can track novel objects at test-time at 100 fps. Our tracker is significantly faster than previous methods that use neural networks for tracking, which are typically very slow to run and not practical for real-time applications. Our tracker uses a simple feed-forward network with no online training required. The tracker learns a generic relationship between object motion and appearance and can be used to track novel objects that do not appear in the training set. We test our network on a standard tracking benchmark to demonstrate our tracker's state-of-the-art performance. Further, our performance improves as we add more videos to our offline training set. To the best of our knowledge, our tracker is the first neural-network tracker that learns to track generic objects at 100 fps.



### R-FUSE: Robust Fast Fusion of Multi-Band Images Based on Solving a Sylvester Equation
- **Arxiv ID**: http://arxiv.org/abs/1604.01818v1
- **DOI**: 10.1109/LSP.2016.2608858
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01818v1)
- **Published**: 2016-04-06 21:41:16+00:00
- **Updated**: 2016-04-06 21:41:16+00:00
- **Authors**: Qi Wei, Nicolas Dobigeon, Jean-Yves Tourneret, Jose Bioucas-Dias, Simon Godsill
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1502.03121
- **Journal**: None
- **Summary**: This paper proposes a robust fast multi-band image fusion method to merge a high-spatial low-spectral resolution image and a low-spatial high-spectral resolution image. Following the method recently developed in [1], the generalized Sylvester matrix equation associated with the multi-band image fusion problem is solved in a more robust and efficient way by exploiting the Woodbury formula, avoiding any permutation operation in the frequency domain as well as the blurring kernel invertibility assumption required in [1]. Thanks to this improvement, the proposed algorithm requires fewer computational operations and is also more robust with respect to the blurring kernel compared with the one in [1]. The proposed new algorithm is tested with different priors considered in [1]. Our conclusion is that the proposed fusion algorithm is more robust than the one in [1] with a reduced computational cost.



### Exploiting Semantic Information and Deep Matching for Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1604.01827v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.01827v2)
- **Published**: 2016-04-06 22:58:53+00:00
- **Updated**: 2016-08-23 03:45:41+00:00
- **Authors**: Min Bai, Wenjie Luo, Kaustav Kundu, Raquel Urtasun
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle the problem of estimating optical flow from a monocular camera in the context of autonomous driving. We build on the observation that the scene is typically composed of a static background, as well as a relatively small number of traffic participants which move rigidly in 3D. We propose to estimate the traffic participants using instance-level segmentation. For each traffic participant, we use the epipolar constraints that govern each independent motion for faster and more accurate estimation. Our second contribution is a new convolutional net that learns to perform flow matching, and is able to estimate the uncertainty of its matches. This is a core element of our flow estimation pipeline. We demonstrate the effectiveness of our approach in the challenging KITTI 2015 flow benchmark, and show that our approach outperforms published approaches by a large margin.



