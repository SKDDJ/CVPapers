# Arxiv Papers in cs.CV on 2016-04-19
### Cognitive state classification using transformed fMRI data
- **Arxiv ID**: http://arxiv.org/abs/1604.05413v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.7; I.5.2; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1604.05413v1)
- **Published**: 2016-04-19 02:52:31+00:00
- **Updated**: 2016-04-19 02:52:31+00:00
- **Authors**: Hariharan Ramasangu, Neelam Sinha
- **Comment**: 5 pages, Conference-SPCOM14
- **Journal**: None
- **Summary**: One approach, for understanding human brain functioning, is to analyze the changes in the brain while performing cognitive tasks. Towards this, Functional Magnetic Resonance (fMR) images of subjects performing well-defined tasks are widely utilized for task-specific analyses. In this work, we propose a procedure to enable classification between two chosen cognitive tasks, using their respective fMR image sequences. The time series of expert-marked anatomically-mapped relevant voxels are processed and fed as input to the classical Naive Bayesian and SVM classifiers. The processing involves use of random sieve function, phase information in the data transformed using Fourier and Hilbert transformations. This processing results in improved classification, as against using the voxel intensities directly, as illustrated. The novelty of the proposed method lies in utilizing the phase information in the transformed domain, for classifying between the cognitive tasks along with random sieve function chosen with a particular probability distribution. The proposed classification procedure is applied on a publicly available dataset, StarPlus data, with 6 subjects performing the two distinct cognitive tasks of watching either a picture or a sentence. The classification accuracy stands at an average of 65.6%(using Naive Bayes classifier) and 76.4%(using SVM classifier) for raw data. The corresponding classification accuracy stands at 96.8% and 97.5% for Fourier transformed data. For Hilbert transformed data, it is 93.7% and 99%, for 6 subjects, on 2 cognitive tasks.



### Comparing Face Detection and Recognition Techniques
- **Arxiv ID**: http://arxiv.org/abs/1610.04575v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.04575v1)
- **Published**: 2016-04-19 03:16:14+00:00
- **Updated**: 2016-04-19 03:16:14+00:00
- **Authors**: Jyothi Korra
- **Comment**: None
- **Journal**: None
- **Summary**: This paper implements and compares different techniques for face detection and recognition. One is find where the face is located in the images that is face detection and second is face recognition that is identifying the person. We study three techniques in this paper: Face detection using self organizing map (SOM), Face recognition by projection and nearest neighbor and Face recognition using SVM.



### Triplet Probabilistic Embedding for Face Verification and Clustering
- **Arxiv ID**: http://arxiv.org/abs/1604.05417v3
- **DOI**: 10.1109/BTAS.2016.7791205
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1604.05417v3)
- **Published**: 2016-04-19 03:29:56+00:00
- **Updated**: 2017-01-18 03:10:44+00:00
- **Authors**: Swami Sankaranarayanan, Azadeh Alavi, Carlos Castillo, Rama Chellappa
- **Comment**: Oral Paper in BTAS 2016; NVIDIA Best paper Award
  (http://ieee-biometrics.org/btas2016/awards.html)
- **Journal**: None
- **Summary**: Despite significant progress made over the past twenty five years, unconstrained face verification remains a challenging problem. This paper proposes an approach that couples a deep CNN-based approach with a low-dimensional discriminative embedding learned using triplet probability constraints to solve the unconstrained face verification problem. Aside from yielding performance improvements, this embedding provides significant advantages in terms of memory and for post-processing operations like subject specific clustering. Experiments on the challenging IJB-A dataset show that the proposed algorithm performs comparably or better than the state of the art methods in verification and identification metrics, while requiring much less training data and training time. The superior performance of the proposed method on the CFP dataset shows that the representation learned by our deep CNN is robust to extreme pose variation. Furthermore, we demonstrate the robustness of the deep features to challenges including age, pose, blur and clutter by performing simple clustering experiments on both IJB-A and LFW datasets.



### Improving Raw Image Storage Efficiency by Exploiting Similarity
- **Arxiv ID**: http://arxiv.org/abs/1604.05442v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1604.05442v1)
- **Published**: 2016-04-19 06:29:36+00:00
- **Updated**: 2016-04-19 06:29:36+00:00
- **Authors**: Binqi Zhang, Chen Wang, Bing Bing Zhou, Albert Y. Zomaya
- **Comment**: None
- **Journal**: None
- **Summary**: To improve the temporal and spatial storage efficiency, researchers have intensively studied various techniques, including compression and deduplication. Through our evaluation, we find that methods such as photo tags or local features help to identify the content-based similar- ity between raw images. The images can then be com- pressed more efficiently to get better storage space sav- ings. Furthermore, storing similar raw images together enables rapid data sorting, searching and retrieval if the images are stored in a distributed and large-scale envi- ronment by reducing fragmentation. In this paper, we evaluated the compressibility by designing experiments and observing the results. We found that on a statistical basis the higher similarity photos have, the better com- pression results are. This research helps provide a clue for future large-scale storage system design.



### Parts for the Whole: The DCT Norm for Extreme Visual Recovery
- **Arxiv ID**: http://arxiv.org/abs/1604.05451v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.05451v1)
- **Published**: 2016-04-19 07:13:50+00:00
- **Updated**: 2016-04-19 07:13:50+00:00
- **Authors**: Yunhe Wang, Chang Xu, Shan You, Dacheng Tao, Chao Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Here we study the extreme visual recovery problem, in which over 90\% of pixel values in a given image are missing. Existing low rank-based algorithms are only effective for recovering data with at most 90\% missing values. Thus, we exploit visual data's smoothness property to help solve this challenging extreme visual recovery problem. Based on the Discrete Cosine Transformation (DCT), we propose a novel DCT norm that involves all pixels and produces smooth estimations in any view. Our theoretical analysis shows that the total variation (TV) norm, which only achieves local smoothness, is a special case of the proposed DCT norm. We also develop a new visual recovery algorithm by minimizing the DCT and nuclear norms to achieve a more visually pleasing estimation. Experimental results on a benchmark image dataset demonstrate that the proposed approach is superior to state-of-the-art methods in terms of peak signal-to-noise ratio and structural similarity.



### Deep Saliency with Encoded Low level Distance Map and High Level Features
- **Arxiv ID**: http://arxiv.org/abs/1604.05495v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.05495v1)
- **Published**: 2016-04-19 10:07:16+00:00
- **Updated**: 2016-04-19 10:07:16+00:00
- **Authors**: Gayoung Lee, Yu-Wing Tai, Junmo Kim
- **Comment**: Accepted by IEEE Conference on Computer Vision and Pattern
  Recognition(CVPR) 2016. Project page:
  https://github.com/gylee1103/SaliencyELD
- **Journal**: None
- **Summary**: Recent advances in saliency detection have utilized deep learning to obtain high level features to detect salient regions in a scene. These advances have demonstrated superior results over previous works that utilize hand-crafted low level features for saliency detection. In this paper, we demonstrate that hand-crafted features can provide complementary information to enhance performance of saliency detection that utilizes only high level features. Our method utilizes both high level and low level features for saliency detection under a unified deep learning framework. The high level features are extracted using the VGG-net, and the low level features are compared with other parts of an image to form a low level distance map. The low level distance map is then encoded using a convolutional neural network(CNN) with multiple 1X1 convolutional and ReLU layers. We concatenate the encoded low level distance map and the high level features, and connect them to a fully connected neural network classifier to evaluate the saliency of a query region. Our experiments show that our method can further improve the performance of state-of-the-art deep learning-based saliency detection methods.



### Using Apache Lucene to Search Vector of Locally Aggregated Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1604.05576v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, I.4; I.5.4; H.3.3
- **Links**: [PDF](http://arxiv.org/pdf/1604.05576v1)
- **Published**: 2016-04-19 14:08:34+00:00
- **Updated**: 2016-04-19 14:08:34+00:00
- **Authors**: Giuseppe Amato, Paolo Bolettieri, Fabrizio Falchi, Claudio Gennaro, Lucia Vadicamo
- **Comment**: In Proceedings of the 11th Joint Conference on Computer Vision,
  Imaging and Computer Graphics Theory and Applications (VISIGRAPP 2016) -
  Volume 4: VISAPP, p. 383-392
- **Journal**: None
- **Summary**: Surrogate Text Representation (STR) is a profitable solution to efficient similarity search on metric space using conventional text search engines, such as Apache Lucene. This technique is based on comparing the permutations of some reference objects in place of the original metric distance. However, the Achilles heel of STR approach is the need to reorder the result set of the search according to the metric distance. This forces to use a support database to store the original objects, which requires efficient random I/O on a fast secondary memory (such as flash-based storages). In this paper, we propose to extend the Surrogate Text Representation to specifically address a class of visual metric objects known as Vector of Locally Aggregated Descriptors (VLAD). This approach is based on representing the individual sub-vectors forming the VLAD vector with the STR, providing a finer representation of the vector and enabling us to get rid of the reordering phase. The experiments on a publicly available dataset show that the extended STR outperforms the baseline STR achieving satisfactory performance near to the one obtained with the original VLAD vectors.



### WarpNet: Weakly Supervised Matching for Single-view Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1604.05592v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.05592v2)
- **Published**: 2016-04-19 14:28:42+00:00
- **Updated**: 2016-06-20 09:40:46+00:00
- **Authors**: Angjoo Kanazawa, David W. Jacobs, Manmohan Chandraker
- **Comment**: to appear in IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR) 2016
- **Journal**: None
- **Summary**: We present an approach to matching images of objects in fine-grained datasets without using part annotations, with an application to the challenging problem of weakly supervised single-view reconstruction. This is in contrast to prior works that require part annotations, since matching objects across class and pose variations is challenging with appearance features alone. We overcome this challenge through a novel deep learning architecture, WarpNet, that aligns an object in one image with a different object in another. We exploit the structure of the fine-grained dataset to create artificial data for training this network in an unsupervised-discriminative learning approach. The output of the network acts as a spatial prior that allows generalization at test time to match real images across variations in appearance, viewpoint and articulation. On the CUB-200-2011 dataset of bird categories, we improve the AP over an appearance-only network by 13.6%. We further demonstrate that our WarpNet matches, together with the structure of fine-grained datasets, allow single-view reconstructions with quality comparable to using annotated point correspondences.



### Right whale recognition using convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1604.05605v1
- **DOI**: None
- **Categories**: **cs.CV**, 68
- **Links**: [PDF](http://arxiv.org/pdf/1604.05605v1)
- **Published**: 2016-04-19 14:46:30+00:00
- **Updated**: 2016-04-19 14:46:30+00:00
- **Authors**: Andrei Polzounov, Ilmira Terpugova, Deividas Skiparis, Andrei Mihai
- **Comment**: 10 pages + appendix, 15 figures
- **Journal**: None
- **Summary**: We studied the feasibility of recognizing individual right whales (Eubalaena glacialis) using convolutional neural networks. Prior studies have shown that CNNs can be used in wide range of classification and categorization tasks such as automated human face recognition. To test applicability of deep learning to whale recognition we have developed several models based on best practices from literature. Here, we describe the performance of the models. We conclude that machine recognition of whales is feasible and comment on the difficulty of the problem



### Online Human Action Detection using Joint Classification-Regression Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1604.05633v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.05633v2)
- **Published**: 2016-04-19 15:58:56+00:00
- **Updated**: 2016-07-26 15:54:07+00:00
- **Authors**: Yanghao Li, Cuiling Lan, Junliang Xing, Wenjun Zeng, Chunfeng Yuan, Jiaying Liu
- **Comment**: 2016 ECCV Conference
- **Journal**: None
- **Summary**: Human action recognition from well-segmented 3D skeleton data has been intensively studied and has been attracting an increasing attention. Online action detection goes one step further and is more challenging, which identifies the action type and localizes the action positions on the fly from the untrimmed stream data. In this paper, we study the problem of online action detection from streaming skeleton data. We propose a multi-task end-to-end Joint Classification-Regression Recurrent Neural Network to better explore the action type and temporal localization information. By employing a joint classification and regression optimization objective, this network is capable of automatically localizing the start and end points of actions more accurately. Specifically, by leveraging the merits of the deep Long Short-Term Memory (LSTM) subnetwork, the proposed model automatically captures the complex long-range temporal dynamics, which naturally avoids the typical sliding window design and thus ensures high computational efficiency. Furthermore, the subtask of regression optimization provides the ability to forecast the action prior to its occurrence. To evaluate our proposed model, we build a large streaming video dataset with annotations. Experimental results on our dataset and the public G3D dataset both demonstrate very promising performance of our scheme.



### Track and Transfer: Watching Videos to Simulate Strong Human Supervision for Weakly-Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1604.05766v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.05766v1)
- **Published**: 2016-04-19 22:23:29+00:00
- **Updated**: 2016-04-19 22:23:29+00:00
- **Authors**: Krishna Kumar Singh, Fanyi Xiao, Yong Jae Lee
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  2016
- **Journal**: None
- **Summary**: The status quo approach to training object detectors requires expensive bounding box annotations. Our framework takes a markedly different direction: we transfer tracked object boxes from weakly-labeled videos to weakly-labeled images to automatically generate pseudo ground-truth boxes, which replace manually annotated bounding boxes. We first mine discriminative regions in the weakly-labeled image collection that frequently/rarely appear in the positive/negative images. We then match those regions to videos and retrieve the corresponding tracked object boxes. Finally, we design a hough transform algorithm to vote for the best box to serve as the pseudo GT for each image, and use them to train an object detector. Together, these lead to state-of-the-art weakly-supervised detection results on the PASCAL 2007 and 2010 datasets.



