# Arxiv Papers in cs.CV on 2016-04-12
### Application of the Second-Order Statistics for Estimation of the Pure Spectra of Individual Components from the Visible Hyperspectral Images of Their Mixture
- **Arxiv ID**: http://arxiv.org/abs/1604.03193v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.chem-ph
- **Links**: [PDF](http://arxiv.org/pdf/1604.03193v1)
- **Published**: 2016-04-12 01:23:40+00:00
- **Updated**: 2016-04-12 01:23:40+00:00
- **Authors**: Sung-Ho Jong, Yong-U Ri, Kye-Ryong Sin
- **Comment**: 11 pages, 10 figures
- **Journal**: None
- **Summary**: The second-order statistics (SOS) can be applied in estimation of the pure spectra of chemical components from the spectrum of their mixture, when SOS seems to be good at estimation of spectral patterns, but their peak directions are opposite in some cases. In this paper, one method for judgment of the peak direction of the pure spectra was proposed, where the base line of the pure spectra was drawn by using their histograms and the peak directions were chosen so as to make all of the pure spectra located upwards over the base line. Results of the SOS analysis on the visible hyperspectral images of the mixture composed of two or three chemical components showed that the present method offered the reasonable shape and direction of the pure spectra of its components.



### Privacy-Preserving Human Activity Recognition from Extreme Low Resolution
- **Arxiv ID**: http://arxiv.org/abs/1604.03196v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.03196v3)
- **Published**: 2016-04-12 01:33:53+00:00
- **Updated**: 2016-12-26 11:03:46+00:00
- **Authors**: Michael S. Ryoo, Brandon Rothrock, Charles Fleming, Hyun Jong Yang
- **Comment**: None
- **Journal**: AAAI 2017
- **Summary**: Privacy protection from surreptitious video recordings is an important societal challenge. We desire a computer vision system (e.g., a robot) that can recognize human activities and assist our daily life, yet ensure that it is not recording video that may invade our privacy. This paper presents a fundamental approach to address such contradicting objectives: human activity recognition while only using extreme low-resolution (e.g., 16x12) anonymized videos. We introduce the paradigm of inverse super resolution (ISR), the concept of learning the optimal set of image transformations to generate multiple low-resolution (LR) training videos from a single video. Our ISR learns different types of sub-pixel transformations optimized for the activity classification, allowing the classifier to best take advantage of existing high-resolution videos (e.g., YouTube videos) by creating multiple LR training videos tailored for the problem. We experimentally confirm that the paradigm of inverse super resolution is able to benefit activity recognition from extreme low-resolution videos.



### Geometric Feature-Based Facial Expression Recognition in Image Sequences Using Multi-Class AdaBoost and Support Vector Machines
- **Arxiv ID**: http://arxiv.org/abs/1604.03225v1
- **DOI**: 10.3390/s130607714
- **Categories**: **cs.CV**, 68T01, I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/1604.03225v1)
- **Published**: 2016-04-12 03:00:13+00:00
- **Updated**: 2016-04-12 03:00:13+00:00
- **Authors**: Deepak Ghimire, Joonwhoan Lee
- **Comment**: 21 pages, Sensors Journal, facial expression recognition
- **Journal**: Sensors 2013, 13, 7714-7734
- **Summary**: Facial expressions are widely used in the behavioral interpretation of emotions, cognitive science, and social interactions. In this paper, we present a novel method for fully automatic facial expression recognition in facial image sequences. As the facial expression evolves over time facial landmarks are automatically tracked in consecutive video frames, using displacements based on elastic bunch graph matching displacement estimation. Feature vectors from individual landmarks, as well as pairs of landmarks tracking results are extracted, and normalized, with respect to the first frame in the sequence. The prototypical expression sequence for each class of facial expression is formed, by taking the median of the landmark tracking results from the training facial expression sequences. Multi-class AdaBoost with dynamic time warping similarity distance between the feature vector of input facial expression and prototypical facial expression, is used as a weak classifier to select the subset of discriminative feature vectors. Finally, two methods for facial expression recognition are presented, either by using multi-class AdaBoost with dynamic time warping, or by using support vector machine on the boosted feature vectors. The results on the Cohn-Kanade (CK+) facial expression database show a recognition accuracy of 95.17% and 97.35% using multi-class AdaBoost and support vector machines, respectively.



### Recurrent Attentional Networks for Saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/1604.03227v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1604.03227v1)
- **Published**: 2016-04-12 03:03:04+00:00
- **Updated**: 2016-04-12 03:03:04+00:00
- **Authors**: Jason Kuen, Zhenhua Wang, Gang Wang
- **Comment**: CVPR 2016
- **Journal**: None
- **Summary**: Convolutional-deconvolution networks can be adopted to perform end-to-end saliency detection. But, they do not work well with objects of multiple scales. To overcome such a limitation, in this work, we propose a recurrent attentional convolutional-deconvolution network (RACDNN). Using spatial transformer and recurrent network units, RACDNN is able to iteratively attend to selected image sub-regions to perform saliency refinement progressively. Besides tackling the scale problem, RACDNN can also learn context-aware features from past iterations to enhance saliency refinement in future iterations. Experiments on several challenging saliency detection datasets validate the effectiveness of RACDNN, and show that RACDNN outperforms state-of-the-art saliency detection methods.



### CRAFT Objects from Images
- **Arxiv ID**: http://arxiv.org/abs/1604.03239v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.03239v1)
- **Published**: 2016-04-12 03:57:48+00:00
- **Updated**: 2016-04-12 03:57:48+00:00
- **Authors**: Bin Yang, Junjie Yan, Zhen Lei, Stan Z. Li
- **Comment**: CVPR2016
- **Journal**: None
- **Summary**: Object detection is a fundamental problem in image understanding. One popular solution is the R-CNN framework and its fast versions. They decompose the object detection problem into two cascaded easier tasks: 1) generating object proposals from images, 2) classifying proposals into various object categories. Despite that we are handling with two relatively easier tasks, they are not solved perfectly and there's still room for improvement. In this paper, we push the "divide and conquer" solution even further by dividing each task into two sub-tasks. We call the proposed method "CRAFT" (Cascade Region-proposal-network And FasT-rcnn), which tackles each task with a carefully designed network cascade. We show that the cascade structure helps in both tasks: in proposal generation, it provides more compact and better localized object proposals; in object classification, it reduces false positives (mainly between ambiguous categories) by capturing both inter- and intra-category variances. CRAFT achieves consistent and considerable improvement over the state-of-the-art on object detection benchmarks like PASCAL VOC 07/12 and ILSVRC.



### Thesis: Multiple Kernel Learning for Object Categorization
- **Arxiv ID**: http://arxiv.org/abs/1604.03247v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1604.03247v1)
- **Published**: 2016-04-12 04:56:24+00:00
- **Updated**: 2016-04-12 04:56:24+00:00
- **Authors**: Dinesh Govindaraj
- **Comment**: None
- **Journal**: None
- **Summary**: Object Categorization is a challenging problem, especially when the images have clutter background, occlusions or different lighting conditions. In the past, many descriptors have been proposed which aid object categorization even in such adverse conditions. Each descriptor has its own merits and de-merits. Some descriptors are invariant to transformations while the others are more discriminative. Past research has shown that, employing multiple descriptors rather than any single descriptor leads to better recognition. The problem of learning the optimal combination of the available descriptors for a particular classification task is studied. Multiple Kernel Learning (MKL) framework has been developed for learning an optimal combination of descriptors for object categorization. Existing MKL formulations often employ block l-1 norm regularization which is equivalent to selecting a single kernel from a library of kernels. Since essentially a single descriptor is selected, the existing formulations maybe sub- optimal for object categorization. A MKL formulation based on block l-infinity norm regularization has been developed, which chooses an optimal combination of kernels as opposed to selecting a single kernel. A Composite Multiple Kernel Learning(CKL) formulation based on mixed l-infinity and l-1 norm regularization has been developed. These formulations end in Second Order Cone Programs(SOCP). Other efficient alter- native algorithms for these formulation have been implemented. Empirical results on benchmark datasets show significant improvement using these new MKL formulations.



### Attributes as Semantic Units between Natural Language and Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1604.03249v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1604.03249v1)
- **Published**: 2016-04-12 05:23:26+00:00
- **Updated**: 2016-04-12 05:23:26+00:00
- **Authors**: Marcus Rohrbach
- **Comment**: book chapter
- **Journal**: None
- **Summary**: Impressive progress has been made in the fields of computer vision and natural language processing. However, it remains a challenge to find the best point of interaction for these very different modalities. In this chapter we discuss how attributes allow us to exchange information between the two modalities and in this way lead to an interaction on a semantic level. Specifically we discuss how attributes allow using knowledge mined from language resources for recognizing novel visual categories, how we can generate sentence description about images and video, how we can ground natural language in visual content, and finally, how we can answer natural language questions about images.



### Volumetric and Multi-View CNNs for Object Classification on 3D Data
- **Arxiv ID**: http://arxiv.org/abs/1604.03265v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1604.03265v2)
- **Published**: 2016-04-12 07:10:43+00:00
- **Updated**: 2016-04-29 06:21:09+00:00
- **Authors**: Charles R. Qi, Hao Su, Matthias Niessner, Angela Dai, Mengyuan Yan, Leonidas J. Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: 3D shape models are becoming widely available and easier to capture, making available 3D information crucial for progress in object classification. Current state-of-the-art methods rely on CNNs to address this problem. Recently, we witness two types of CNNs being developed: CNNs based upon volumetric representations versus CNNs based upon multi-view representations. Empirical results from these two types of CNNs exhibit a large gap, indicating that existing volumetric CNN architectures and approaches are unable to fully exploit the power of 3D representations. In this paper, we aim to improve both volumetric CNNs and multi-view CNNs according to extensive analysis of existing approaches. To this end, we introduce two distinct network architectures of volumetric CNNs. In addition, we examine multi-view CNNs, where we introduce multi-resolution filtering in 3D. Overall, we are able to outperform current state-of-the-art methods for both volumetric CNNs and multi-view CNNs. We provide extensive experiments designed to evaluate underlying design choices, thus providing a better understanding of the space of methods available for object classification on 3D data.



### Scan, Attend and Read: End-to-End Handwritten Paragraph Recognition with MDLSTM Attention
- **Arxiv ID**: http://arxiv.org/abs/1604.03286v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.03286v3)
- **Published**: 2016-04-12 08:11:20+00:00
- **Updated**: 2016-08-23 08:47:49+00:00
- **Authors**: Théodore Bluche, Jérôme Louradour, Ronaldo Messina
- **Comment**: None
- **Journal**: None
- **Summary**: We present an attention-based model for end-to-end handwriting recognition. Our system does not require any segmentation of the input paragraph. The model is inspired by the differentiable attention models presented recently for speech recognition, image captioning or translation. The main difference is the covert and overt attention, implemented as a multi-dimensional LSTM network. Our principal contribution towards handwriting recognition lies in the automatic transcription without a prior segmentation into lines, which was crucial in previous approaches. To the best of our knowledge this is the first successful attempt of end-to-end multi-line handwriting recognition. We carried out experiments on the well-known IAM Database. The results are encouraging and bring hope to perform full paragraph transcription in the near future.



### Spatial Attention Deep Net with Partial PSO for Hierarchical Hybrid Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1604.03334v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.03334v2)
- **Published**: 2016-04-12 10:47:12+00:00
- **Updated**: 2016-10-20 10:51:42+00:00
- **Authors**: Qi Ye, Shanxin Yuan, Tae-Kyun Kim
- **Comment**: The work is accepted by ECCV2016, Demo video:
  https://youtu.be/2Hg0c88rHkk, Project Page:
  https://sites.google.com/site/qiyeincv/home/eccv2016
- **Journal**: None
- **Summary**: Discriminative methods often generate hand poses kinematically implausible, then generative methods are used to correct (or verify) these results in a hybrid method. Estimating 3D hand pose in a hierarchy, where the high-dimensional output space is decomposed into smaller ones, has been shown effective. Existing hierarchical methods mainly focus on the decomposition of the output space while the input space remains almost the same along the hierarchy. In this paper, a hybrid hand pose estimation method is proposed by applying the kinematic hierarchy strategy to the input space (as well as the output space) of the discriminative method by a spatial attention mechanism and to the optimization of the generative method by hierarchical Particle Swarm Optimization (PSO). The spatial attention mechanism integrates cascaded and hierarchical regression into a CNN framework by transforming both the input(and feature space) and the output space, which greatly reduces the viewpoint and articulation variations. Between the levels in the hierarchy, the hierarchical PSO forces the kinematic constraints to the results of the CNNs. The experimental results show that our method significantly outperforms four state-of-the-art methods and three baselines on three public benchmarks.



### An incremental linear-time learning algorithm for the Optimum-Path Forest classifier
- **Arxiv ID**: http://arxiv.org/abs/1604.03346v5
- **DOI**: 10.1016/j.ipl.2017.05.004
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1604.03346v5)
- **Published**: 2016-04-12 11:31:23+00:00
- **Updated**: 2016-11-23 12:08:23+00:00
- **Authors**: Moacir Ponti, Mateus Riva
- **Comment**: submitted to IPL Journal for consideration in Nov/2016
- **Journal**: None
- **Summary**: We present a classification method with incremental capabilities based on the Optimum-Path Forest classifier (OPF). The OPF considers instances as nodes of a fully-connected training graph, arc weights represent distances between two feature vectors. Our algorithm includes new instances in an OPF in linear-time, while keeping similar accuracies when compared with the original quadratic-time model.



### Orientation-boosted Voxel Nets for 3D Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1604.03351v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1604.03351v2)
- **Published**: 2016-04-12 11:43:14+00:00
- **Updated**: 2017-10-19 21:41:12+00:00
- **Authors**: Nima Sedaghat, Mohammadreza Zolfaghari, Ehsan Amiri, Thomas Brox
- **Comment**: BMVC'17 version. Added some experiments + auto-alignment of
  Modelnet40
- **Journal**: None
- **Summary**: Recent work has shown good recognition results in 3D object recognition using 3D convolutional networks. In this paper, we show that the object orientation plays an important role in 3D recognition. More specifically, we argue that objects induce different features in the network under rotation. Thus, we approach the category-level classification task as a multi-task problem, in which the network is trained to predict the pose of the object in addition to the class label as a parallel task. We show that this yields significant improvements in the classification results. We test our suggested architecture on several datasets representing various 3D data sources: LiDAR data, CAD models, and RGB-D images. We report state-of-the-art results on classification as well as significant improvements in precision and speed over the baseline on 3D detection.



### Video Description using Bidirectional Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1604.03390v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1604.03390v2)
- **Published**: 2016-04-12 13:09:01+00:00
- **Updated**: 2016-12-12 12:28:07+00:00
- **Authors**: Álvaro Peris, Marc Bolaños, Petia Radeva, Francisco Casacuberta
- **Comment**: 8 pages, 3 figures, 1 table, Submitted to International Conference on
  Artificial Neural Networks (ICANN)
- **Journal**: None
- **Summary**: Although traditionally used in the machine translation field, the encoder-decoder framework has been recently applied for the generation of video and image descriptions. The combination of Convolutional and Recurrent Neural Networks in these models has proven to outperform the previous state of the art, obtaining more accurate video descriptions. In this work we propose pushing further this model by introducing two contributions into the encoding stage. First, producing richer image representations by combining object and location information from Convolutional Neural Networks and second, introducing Bidirectional Recurrent Neural Networks for capturing both forward and backward temporal relationships in the input frames.



### Multi-modal Fusion for Diabetes Mellitus and Impaired Glucose Regulation Detection
- **Arxiv ID**: http://arxiv.org/abs/1604.03443v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.03443v1)
- **Published**: 2016-04-12 15:31:52+00:00
- **Updated**: 2016-04-12 15:31:52+00:00
- **Authors**: Jinxing Li, David Zhang, Yongcheng Li, Jian Wu
- **Comment**: 9 pages, 8 figures, 30 conference
- **Journal**: None
- **Summary**: Effective and accurate diagnosis of Diabetes Mellitus (DM), as well as its early stage Impaired Glucose Regulation (IGR), has attracted much attention recently. Traditional Chinese Medicine (TCM) [3], [5] etc. has proved that tongue, face and sublingual diagnosis as a noninvasive method is a reasonable way for disease detection. However, most previous works only focus on a single modality (tongue, face or sublingual) for diagnosis, although different modalities may provide complementary information for the diagnosis of DM and IGR. In this paper, we propose a novel multi-modal classification method to discriminate between DM (or IGR) and healthy controls. Specially, the tongue, facial and sublingual images are first collected by using a non-invasive capture device. The color, texture and geometry features of these three types of images are then extracted, respectively. Finally, our so-called multi-modal similar and specific learning (MMSSL) approach is proposed to combine features of tongue, face and sublingual, which not only exploits the correlation but also extracts individual components among them. Experimental results on a dataset consisting of 192 Healthy, 198 DM and 114 IGR samples (all samples were obtained from Guangdong Provincial Hospital of Traditional Chinese Medicine) substantiate the effectiveness and superiority of our proposed method for the diagnosis of DM and IGR, compared to the case of using a single modality.



### From Pixels to Sentiment: Fine-tuning CNNs for Visual Sentiment Prediction
- **Arxiv ID**: http://arxiv.org/abs/1604.03489v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1604.03489v2)
- **Published**: 2016-04-12 17:24:39+00:00
- **Updated**: 2017-01-27 18:02:16+00:00
- **Authors**: Victor Campos, Brendan Jou, Xavier Giro-i-Nieto
- **Comment**: Accepted for publication in Image and Vision Computing. Models and
  source code available at https://github.com/imatge-upc/sentiment-2016
- **Journal**: None
- **Summary**: Visual multimedia have become an inseparable part of our digital social lives, and they often capture moments tied with deep affections. Automated visual sentiment analysis tools can provide a means of extracting the rich feelings and latent dispositions embedded in these media. In this work, we explore how Convolutional Neural Networks (CNNs), a now de facto computational machine learning tool particularly in the area of Computer Vision, can be specifically applied to the task of visual sentiment prediction. We accomplish this through fine-tuning experiments using a state-of-the-art CNN and via rigorous architecture analysis, we present several modifications that lead to accuracy improvements over prior art on a dataset of images from a popular social media platform. We additionally present visualizations of local patterns that the network learned to associate with image sentiment for insight into how visual positivity (or negativity) is perceived by the model.



### GPU-FV: Realtime Fisher Vector and Its Applications in Video Monitoring
- **Arxiv ID**: http://arxiv.org/abs/1604.03498v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.03498v1)
- **Published**: 2016-04-12 18:22:08+00:00
- **Updated**: 2016-04-12 18:22:08+00:00
- **Authors**: Wenying Ma, Liangliang Cao, Lei Yu, Guoping Long, Yucheng Li
- **Comment**: accepted by ICMR 2016
- **Journal**: None
- **Summary**: Fisher vector has been widely used in many multimedia retrieval and visual recognition applications with good performance. However, the computation complexity prevents its usage in real-time video monitoring. In this work, we proposed and implemented GPU-FV, a fast Fisher vector extraction method with the help of modern GPUs. The challenge of implementing Fisher vector on GPUs lies in the data dependency in feature extraction and expensive memory access in Fisher vector computing. To handle these challenges, we carefully designed GPU-FV in a way that utilizes the computing power of GPU as much as possible, and applied optimizations such as loop tiling to boost the performance. GPU-FV is about 12 times faster than the CPU version, and 50\% faster than a non-optimized GPU implementation. For standard video input (320*240), GPU-FV can process each frame within 34ms on a model GPU. Our experiments show that GPU-FV obtains a similar recognition accuracy as traditional FV on VOC 2007 and Caltech 256 image sets. We also applied GPU-FV for realtime video monitoring tasks and found that GPU-FV outperforms a number of previous works. Especially, when the number of training examples are small, GPU-FV outperforms the recent popular deep CNN features borrowed from ImageNet. The code can be downloaded from the following link https://bitbucket.org/mawenjing/gpu-fv.



### Counting Everyday Objects in Everyday Scenes
- **Arxiv ID**: http://arxiv.org/abs/1604.03505v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.03505v3)
- **Published**: 2016-04-12 18:31:43+00:00
- **Updated**: 2017-05-09 03:24:40+00:00
- **Authors**: Prithvijit Chattopadhyay, Ramakrishna Vedantam, Ramprasaath R. Selvaraju, Dhruv Batra, Devi Parikh
- **Comment**: None
- **Journal**: None
- **Summary**: We are interested in counting the number of instances of object classes in natural, everyday images. Previous counting approaches tackle the problem in restricted domains such as counting pedestrians in surveillance videos. Counts can also be estimated from outputs of other vision tasks like object detection. In this work, we build dedicated models for counting designed to tackle the large variance in counts, appearances, and scales of objects found in natural scenes. Our approach is inspired by the phenomenon of subitizing - the ability of humans to make quick assessments of counts given a perceptual signal, for small count values. Given a natural scene, we employ a divide and conquer strategy while incorporating context across the scene to adapt the subitizing idea to counting. Our approach offers consistent improvements over numerous baseline approaches for counting on the PASCAL VOC 2007 and COCO datasets. Subsequently, we study how counting can be used to improve object detection. We then show a proof of concept application of our counting methods to the task of Visual Question Answering, by studying the `how many?' questions in the VQA and COCO-QA datasets.



### Full Flow: Optical Flow Estimation By Global Optimization over Regular Grids
- **Arxiv ID**: http://arxiv.org/abs/1604.03513v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.03513v1)
- **Published**: 2016-04-12 18:40:47+00:00
- **Updated**: 2016-04-12 18:40:47+00:00
- **Authors**: Qifeng Chen, Vladlen Koltun
- **Comment**: To be presented at CVPR 2016
- **Journal**: None
- **Summary**: We present a global optimization approach to optical flow estimation. The approach optimizes a classical optical flow objective over the full space of mappings between discrete grids. No descriptor matching is used. The highly regular structure of the space of mappings enables optimizations that reduce the computational complexity of the algorithm's inner loop from quadratic to linear and support efficient matching of tens of thousands of nodes to tens of thousands of displacements. We show that one-shot global optimization of a classical Horn-Schunck-type objective over regular grids at a single resolution is sufficient to initialize continuous interpolation and achieve state-of-the-art performance on challenging modern benchmarks.



### Fast Object Localization Using a CNN Feature Map Based Multi-Scale Search
- **Arxiv ID**: http://arxiv.org/abs/1604.03517v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.03517v1)
- **Published**: 2016-04-12 18:44:10+00:00
- **Updated**: 2016-04-12 18:44:10+00:00
- **Authors**: Hyungtae Lee, Heesung Kwon, Archith J. Bency, William D. Nothwang
- **Comment**: None
- **Journal**: None
- **Summary**: Object localization is an important task in computer vision but requires a large amount of computational power due mainly to an exhaustive multiscale search on the input image. In this paper, we describe a near real-time multiscale search on a deep CNN feature map that does not use region proposals. The proposed approach effectively exploits local semantic information preserved in the feature map of the outermost convolutional layer. A multi-scale search is performed on the feature map by processing all the sub-regions of different sizes using separate expert units of fully connected layers. Each expert unit receives as input local semantic features only from the corresponding sub-regions of a specific geometric shape. Therefore, it contains more nearly optimal parameters tailored to the corresponding shape. This multi-scale and multi-aspect ratio scanning strategy can effectively localize a potential object of an arbitrary size. The proposed approach is fast and able to localize objects of interest with a frame rate of 4 fps while providing improved detection performance over the state-of-the art on the PASCAL VOC 12 and MSCOCO data sets.



### DTM: Deformable Template Matching
- **Arxiv ID**: http://arxiv.org/abs/1604.03518v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.03518v1)
- **Published**: 2016-04-12 18:44:25+00:00
- **Updated**: 2016-04-12 18:44:25+00:00
- **Authors**: Hyungtae Lee, Heesung Kwon, Ryan M. Robinson, William D. Nothwang
- **Comment**: None
- **Journal**: None
- **Summary**: A novel template matching algorithm that can incorporate the concept of deformable parts, is presented in this paper. Unlike the deformable part model (DPM) employed in object recognition, the proposed template-matching approach called Deformable Template Matching (DTM) does not require a training step. Instead, deformation is achieved by a set of predefined basic rules (e.g. the left sub-patch cannot pass across the right patch). Experimental evaluation of this new method using the PASCAL VOC 07 dataset demonstrated substantial performance improvement over conventional template matching algorithms. Additionally, to confirm the applicability of DTM, the concept is applied to the generation of a rotation-invariant SIFT descriptor. Experimental evaluation employing deformable matching of SIFT features shows an increased number of matching features compared to a conventional SIFT matching.



### Going Deeper with Contextual CNN for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1604.03519v3
- **DOI**: 10.1109/TIP.2017.2725580
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1604.03519v3)
- **Published**: 2016-04-12 18:44:34+00:00
- **Updated**: 2017-05-09 14:21:21+00:00
- **Authors**: Hyungtae Lee, Heesung Kwon
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: In this paper, we describe a novel deep convolutional neural network (CNN) that is deeper and wider than other existing deep networks for hyperspectral image classification. Unlike current state-of-the-art approaches in CNN-based hyperspectral image classification, the proposed network, called contextual deep CNN, can optimally explore local contextual interactions by jointly exploiting local spatio-spectral relationships of neighboring individual pixel vectors. The joint exploitation of the spatio-spectral information is achieved by a multi-scale convolutional filter bank used as an initial component of the proposed CNN pipeline. The initial spatial and spectral feature maps obtained from the multi-scale filter bank are then combined together to form a joint spatio-spectral feature map. The joint feature map representing rich spectral and spatial properties of the hyperspectral image is then fed through a fully convolutional network that eventually predicts the corresponding label of each pixel vector. The proposed approach is tested on three benchmark datasets: the Indian Pines dataset, the Salinas dataset and the University of Pavia dataset. Performance comparison shows enhanced classification performance of the proposed approach over the current state-of-the-art on the three datasets.



### Cross-stitch Networks for Multi-task Learning
- **Arxiv ID**: http://arxiv.org/abs/1604.03539v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1604.03539v1)
- **Published**: 2016-04-12 19:43:25+00:00
- **Updated**: 2016-04-12 19:43:25+00:00
- **Authors**: Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, Martial Hebert
- **Comment**: To appear in CVPR 2016 (Spotlight)
- **Journal**: None
- **Summary**: Multi-task learning in Convolutional Networks has displayed remarkable success in the field of recognition. This success can be largely attributed to learning shared representations from multiple supervisory tasks. However, existing multi-task approaches rely on enumerating multiple network architectures specific to the tasks at hand, that do not generalize. In this paper, we propose a principled approach to learn shared representations in ConvNets using multi-task learning. Specifically, we propose a new sharing unit: "cross-stitch" unit. These units combine the activations from multiple networks and can be trained end-to-end. A network with cross-stitch units can learn an optimal combination of shared and task-specific representations. Our proposed method generalizes across multiple tasks and shows dramatically improved performance over baseline methods for categories with few training examples.



### Training Region-based Object Detectors with Online Hard Example Mining
- **Arxiv ID**: http://arxiv.org/abs/1604.03540v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1604.03540v1)
- **Published**: 2016-04-12 19:44:13+00:00
- **Updated**: 2016-04-12 19:44:13+00:00
- **Authors**: Abhinav Shrivastava, Abhinav Gupta, Ross Girshick
- **Comment**: To appear in Proceedings of IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR), 2016. (oral)
- **Journal**: None
- **Summary**: The field of object detection has made significant advances riding on the wave of region-based ConvNets, but their training procedure still includes many heuristics and hyperparameters that are costly to tune. We present a simple yet surprisingly effective online hard example mining (OHEM) algorithm for training region-based ConvNet detectors. Our motivation is the same as it has always been -- detection datasets contain an overwhelming number of easy examples and a small number of hard examples. Automatic selection of these hard examples can make training more effective and efficient. OHEM is a simple and intuitive algorithm that eliminates several heuristics and hyperparameters in common use. But more importantly, it yields consistent and significant boosts in detection performance on benchmarks like PASCAL VOC 2007 and 2012. Its effectiveness increases as datasets become larger and more difficult, as demonstrated by the results on the MS COCO dataset. Moreover, combined with complementary advances in the field, OHEM leads to state-of-the-art results of 78.9% and 76.3% mAP on PASCAL VOC 2007 and 2012 respectively.



### What do different evaluation metrics tell us about saliency models?
- **Arxiv ID**: http://arxiv.org/abs/1604.03605v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.03605v2)
- **Published**: 2016-04-12 22:16:20+00:00
- **Updated**: 2017-04-06 23:46:40+00:00
- **Authors**: Zoya Bylinskii, Tilke Judd, Aude Oliva, Antonio Torralba, Frédo Durand
- **Comment**: None
- **Journal**: None
- **Summary**: How best to evaluate a saliency model's ability to predict where humans look in images is an open research question. The choice of evaluation metric depends on how saliency is defined and how the ground truth is represented. Metrics differ in how they rank saliency models, and this results from how false positives and false negatives are treated, whether viewing biases are accounted for, whether spatial deviations are factored in, and how the saliency maps are pre-processed. In this paper, we provide an analysis of 8 different evaluation metrics and their properties. With the help of systematic experiments and visualizations of metric computations, we add interpretability to saliency scores and more transparency to the evaluation of saliency models. Building off the differences in metric properties and behaviors, we make recommendations for metric selections under specific assumptions and for specific applications.



