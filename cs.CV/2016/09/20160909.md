# Arxiv Papers in cs.CV on 2016-09-09
### Robust Structure from Motion in the Presence of Outliers and Missing Data
- **Arxiv ID**: http://arxiv.org/abs/1609.02638v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.02638v2)
- **Published**: 2016-09-09 02:00:20+00:00
- **Updated**: 2016-12-29 21:55:01+00:00
- **Authors**: Guanghui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Structure from motion is an import theme in computer vision. Although great progress has been made both in theory and applications, most of the algorithms only work for static scenes and rigid objects. In recent years, structure and motion recovery of non-rigid objects and dynamic scenes have received a lot of attention. In this paper, the state-of-the-art techniques for structure and motion factorization of non-rigid objects are reviewed and discussed. First, an introduction of the structure from motion problem is presented, followed by a general formulation of non-rigid structure from motion. Second, an augmented affined factorization framework, by using homogeneous representation, is presented to solve the registration issue in the presence of outlying and missing data. Third, based on the observation that the reprojection residuals of outliers are significantly larger than those of inliers, a robust factorization strategy with outlier rejection is proposed by means of the reprojection residuals, followed by some comparative experimental evaluations. Finally, some future research topics in non-rigid structure from motion are discussed.



### Automatic Selection of Stochastic Watershed Hierarchies
- **Arxiv ID**: http://arxiv.org/abs/1609.02715v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1609.02715v1)
- **Published**: 2016-09-09 09:26:22+00:00
- **Updated**: 2016-09-09 09:26:22+00:00
- **Authors**: Amin Fehri, Santiago Velasco-Forero, Fernand Meyer
- **Comment**: in European Conference of Signal Processing (EUSIPCO), 2016,
  Budapest, Hungary
- **Journal**: None
- **Summary**: The segmentation, seen as the association of a partition with an image, is a difficult task. It can be decomposed in two steps: at first, a family of contours associated with a series of nested partitions (or hierarchy) is created and organized, then pertinent contours are extracted. A coarser partition is obtained by merging adjacent regions of a finer partition. The strength of a contour is then measured by the level of the hierarchy for which its two adjacent regions merge. We present an automatic segmentation strategy using a wide range of stochastic watershed hierarchies. For a given set of homogeneous images, our approach selects automatically the best hierarchy and cut level to perform image simplification given an evaluation score. Experimental results illustrate the advantages of our approach on several real-life images datasets.



### An Interactive Segmentation Tool for Quantifying Fat in Lumbar Muscles using Axial Lumbar-Spine MRI
- **Arxiv ID**: http://arxiv.org/abs/1609.02744v1
- **DOI**: 10.1016/j.irbm.2015.10.004
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.02744v1)
- **Published**: 2016-09-09 11:13:58+00:00
- **Updated**: 2016-09-09 11:13:58+00:00
- **Authors**: Joseph Antony, Kevin McGuinness, Neil Welch, Joe Coyle, Andy Franklyn-Miller, Noel E. O'Connor, Kieran Moran
- **Comment**: None
- **Journal**: IRBM 37(1), 11-22 (2016)
- **Summary**: In this paper we present an interactive tool that can be used to quantify fat infiltration in lumbar muscles, which is useful in studying fat infiltration and lower back pain (LBP) in adults. Currently, a qualitative assessment by visual grading via a 5-point scale is used to study fat infiltration in lumbar muscles from an axial view of lumbar-spine MR Images. However, a quantitative approach (on a continuous scale of 0-100\%) may provide a greater insight. In this paper, we propose a method to precisely quantify the fat deposition / infiltration in a user-defined region of the lumbar muscles, which may aid better diagnosis and analysis. The key steps are interactively segmenting the region of interest (ROI) from the lumbar muscles using the well known livewire technique, identifying fatty regions in the segmented region based on variable-selection of threshold and softness levels, automatically detecting the center of the spinal column and fragmenting the lumbar muscles into smaller regions with reference to the center of the spinal column, computing key parameters [such as total and region-wise fat content percentage, total-cross sectional area (TCSA) and functional cross-sectional area (FCSA)] and exporting the computations and associated patient information from the MRI, into a database. A standalone application using MATLAB R2014a was developed to perform the required computations along with an intuitive graphical user interface (GUI).



### Image and Video Mining through Online Learning
- **Arxiv ID**: http://arxiv.org/abs/1609.02770v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.02770v2)
- **Published**: 2016-09-09 12:49:22+00:00
- **Updated**: 2016-12-07 12:26:30+00:00
- **Authors**: Andrew Gilbert, Richard Bowden
- **Comment**: None
- **Journal**: None
- **Summary**: Within the field of image and video recognition, the traditional approach is a dataset split into fixed training and test partitions. However, the labelling of the training set is time-consuming, especially as datasets grow in size and complexity. Furthermore, this approach is not applicable to the home user, who wants to intuitively group their media without tirelessly labelling the content. Our interactive approach is able to iteratively cluster classes of images and video. Our approach is based around the concept of an image signature which, unlike a standard bag of words model, can express co-occurrence statistics as well as symbol frequency. We efficiently compute metric distances between signatures despite their inherent high dimensionality and provide discriminative feature selection, to allow common and distinctive elements to be identified from a small set of user labelled examples. These elements are then accentuated in the image signature to increase similarity between examples and pull correct classes together. By repeating this process in an online learning framework, the accuracy of similarity increases dramatically despite labelling only a few training examples. To demonstrate that the approach is agnostic to media type and features used, we evaluate on three image datasets (15 scene, Caltech101 and FG-NET), a mixed text and image dataset (ImageTag), a dataset used in active learning (Iris) and on three action recognition datasets (UCF11, KTH and Hollywood2). On the UCF11 video dataset, the accuracy is 86.7% despite using only 90 labelled examples from a dataset of over 1200 videos, instead of the standard 1122 training videos. The approach is both scalable and efficient, with a single iteration over the full UCF11 dataset of around 1200 videos taking approximately 1 minute on a standard desktop machine.



### An empirical study on the effects of different types of noise in image classification tasks
- **Arxiv ID**: http://arxiv.org/abs/1609.02781v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.02781v1)
- **Published**: 2016-09-09 13:19:41+00:00
- **Updated**: 2016-09-09 13:19:41+00:00
- **Authors**: Gabriel B. Paranhos da Costa, Welinton A. Contato, Tiago S. Nazare, Jo√£o E. S. Batista Neto, Moacir Ponti
- **Comment**: None
- **Journal**: None
- **Summary**: Image classification is one of the main research problems in computer vision and machine learning. Since in most real-world image classification applications there is no control over how the images are captured, it is necessary to consider the possibility that these images might be affected by noise (e.g. sensor noise in a low-quality surveillance camera). In this paper we analyse the impact of three different types of noise on descriptors extracted by two widely used feature extraction methods (LBP and HOG) and how denoising the images can help to mitigate this problem. We carry out experiments on two different datasets and consider several types of noise, noise levels, and denoising methods. Our results show that noise can hinder classification performance considerably and make classes harder to separate. Although denoising methods were not able to reach the same performance of the noise-free scenario, they improved classification results for noisy data.



### Automated detection of smuggled high-risk security threats using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1609.02805v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.02805v1)
- **Published**: 2016-09-09 14:14:52+00:00
- **Updated**: 2016-09-09 14:14:52+00:00
- **Authors**: Nicolas Jaccard, Thomas W. Rogers, Edward J. Morton, Lewis D. Griffin
- **Comment**: Submission for Crime Detection and Prevention conference 2016
- **Journal**: None
- **Summary**: The security infrastructure is ill-equipped to detect and deter the smuggling of non-explosive devices that enable terror attacks such as those recently perpetrated in western Europe. The detection of so-called "small metallic threats" (SMTs) in cargo containers currently relies on statistical risk analysis, intelligence reports, and visual inspection of X-ray images by security officers. The latter is very slow and unreliable due to the difficulty of the task: objects potentially spanning less than 50 pixels have to be detected in images containing more than 2 million pixels against very complex and cluttered backgrounds. In this contribution, we demonstrate for the first time the use of Convolutional Neural Networks (CNNs), a type of Deep Learning, to automate the detection of SMTs in fullsize X-ray images of cargo containers. Novel approaches for dataset augmentation allowed to train CNNs from-scratch despite the scarcity of data available. We report fewer than 6% false alarms when detecting 90% SMTs synthetically concealed in stream-of-commerce images, which corresponds to an improvement of over an order of magnitude over conventional approaches such as Bag-of-Words (BoWs). The proposed scheme offers potentially super-human performance for a fraction of the time it would take for a security officers to carry out visual inspection (processing time is approximately 3.5s per container image).



### Track Facial Points in Unconstrained Videos
- **Arxiv ID**: http://arxiv.org/abs/1609.02825v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.02825v1)
- **Published**: 2016-09-09 15:02:08+00:00
- **Updated**: 2016-09-09 15:02:08+00:00
- **Authors**: Xi Peng, Qiong Hu, Junzhou Huang, Dimitris N. Metaxas
- **Comment**: British Machine Vision Conference (BMVC), 2016
- **Journal**: None
- **Summary**: Tracking Facial Points in unconstrained videos is challenging due to the non-rigid deformation that changes over time. In this paper, we propose to exploit incremental learning for person-specific alignment in wild conditions. Our approach takes advantage of part-based representation and cascade regression for robust and efficient alignment on each frame. Unlike existing methods that usually rely on models trained offline, we incrementally update the representation subspace and the cascade of regressors in a unified framework to achieve personalized modeling on the fly. To alleviate the drifting issue, the fitting results are evaluated using a deep neural network, where well-aligned faces are picked out to incrementally update the representation and fitting models. Both image and video datasets are employed to valid the proposed method. The results demonstrate the superior performance of our approach compared with existing approaches in terms of fitting accuracy and efficiency.



### Image Denoising Via Collaborative Support-Agnostic Recovery
- **Arxiv ID**: http://arxiv.org/abs/1609.02932v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.02932v1)
- **Published**: 2016-09-09 20:20:39+00:00
- **Updated**: 2016-09-09 20:20:39+00:00
- **Authors**: Muzammil Behzad, Mudassir Masood, Tarig Ballal, Maha Shadaydeh, Tareq Y. Al-Naffouri
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel image denoising algorithm using collaborative support-agnostic sparse reconstruction. An observed image is first divided into patches. Similarly structured patches are grouped together to be utilized for collaborative processing. In the proposed collaborative schemes, similar patches are assumed to share the same support taps. For sparse reconstruction, the likelihood of a tap being active in a patch is computed and refined through a collaboration process with other similar patches in the same group. This provides very good patch support estimation, hence enhancing the quality of image restoration. Performance comparisons with state-of-the-art algorithms, in terms of SSIM and PSNR, demonstrate the superiority of the proposed algorithm.



### The Role of Context Selection in Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1609.02948v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.02948v1)
- **Published**: 2016-09-09 21:30:14+00:00
- **Updated**: 2016-09-09 21:30:14+00:00
- **Authors**: Ruichi Yu, Xi Chen, Vlad I. Morariu, Larry S. Davis
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate the reasons why context in object detection has limited utility by isolating and evaluating the predictive power of different context cues under ideal conditions in which context provided by an oracle. Based on this study, we propose a region-based context re-scoring method with dynamic context selection to remove noise and emphasize informative context. We introduce latent indicator variables to select (or ignore) potential contextual regions, and learn the selection strategy with latent-SVM. We conduct experiments to evaluate the performance of the proposed context selection method on the SUN RGB-D dataset. The method achieves a significant improvement in terms of mean average precision (mAP), compared with both appearance based detectors and a conventional context model without the selection scheme.



### Learning-Based View Synthesis for Light Field Cameras
- **Arxiv ID**: http://arxiv.org/abs/1609.02974v1
- **DOI**: 10.1145/2980179.2980251
- **Categories**: **cs.CV**, cs.GR, I.4.1
- **Links**: [PDF](http://arxiv.org/pdf/1609.02974v1)
- **Published**: 2016-09-09 23:33:38+00:00
- **Updated**: 2016-09-09 23:33:38+00:00
- **Authors**: Nima Khademi Kalantari, Ting-Chun Wang, Ravi Ramamoorthi
- **Comment**: in ACM Transactions on Graphics 2016
- **Journal**: None
- **Summary**: With the introduction of consumer light field cameras, light field imaging has recently become widespread. However, there is an inherent trade-off between the angular and spatial resolution, and thus, these cameras often sparsely sample in either spatial or angular domain. In this paper, we use machine learning to mitigate this trade-off. Specifically, we propose a novel learning-based approach to synthesize new views from a sparse set of input views. We build upon existing view synthesis techniques and break down the process into disparity and color estimation components. We use two sequential convolutional neural networks to model these two components and train both networks simultaneously by minimizing the error between the synthesized and ground truth images. We show the performance of our approach using only four corner sub-aperture views from the light fields captured by the Lytro Illum camera. Experimental results show that our approach synthesizes high-quality images that are superior to the state-of-the-art techniques on a variety of challenging real-world scenes. We believe our method could potentially decrease the required angular resolution of consumer light field cameras, which allows their spatial resolution to increase.



