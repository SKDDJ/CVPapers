# Arxiv Papers in cs.CV on 2016-09-21
### Matrix Variate RBM Model with Gaussian Distributions
- **Arxiv ID**: http://arxiv.org/abs/1609.06417v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.06417v2)
- **Published**: 2016-09-21 03:42:52+00:00
- **Updated**: 2016-09-27 01:31:06+00:00
- **Authors**: Simeng Liu, Yanfeng Sun, Yongli Hu, Junbin Gao, Baocai Yin
- **Comment**: We think we need more mathematical derivation and experiments to
  support the proposed theory of the paper. In this period, it is not
  appropriate to publish it
- **Journal**: None
- **Summary**: Restricted Boltzmann Machine (RBM) is a particular type of random neural network models modeling vector data based on the assumption of Bernoulli distribution. For multi-dimensional and non-binary data, it is necessary to vectorize and discretize the information in order to apply the conventional RBM. It is well-known that vectorization would destroy internal structure of data, and the binary units will limit the applying performance due to fickle real data. To address the issue, this paper proposes a Matrix variate Gaussian Restricted Boltzmann Machine (MVGRBM) model for matrix data whose entries follow Gaussian distributions. Compared with some other RBM algorithm, MVGRBM can model real value data better and it has good performance in image classification.



### From Facial Expression Recognition to Interpersonal Relation Prediction
- **Arxiv ID**: http://arxiv.org/abs/1609.06426v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.06426v3)
- **Published**: 2016-09-21 06:14:17+00:00
- **Updated**: 2017-11-06 06:04:13+00:00
- **Authors**: Zhanpeng Zhang, Ping Luo, Chen Change Loy, Xiaoou Tang
- **Comment**: To appear in International Journal of Computer Vision. We release a
  large expression dataset (over 90,000 web images with manual annotation) and
  an interpersonal relation dataset. See
  http://mmlab.ie.cuhk.edu.hk/projects/socialrelation/
- **Journal**: None
- **Summary**: Interpersonal relation defines the association, e.g., warm, friendliness, and dominance, between two or more people. Motivated by psychological studies, we investigate if such fine-grained and high-level relation traits can be characterized and quantified from face images in the wild. We address this challenging problem by first studying a deep network architecture for robust recognition of facial expressions. Unlike existing models that typically learn from facial expression labels alone, we devise an effective multitask network that is capable of learning from rich auxiliary attributes such as gender, age, and head pose, beyond just facial expression data. While conventional supervised training requires datasets with complete labels (e.g., all samples must be labeled with gender, age, and expression), we show that this requirement can be relaxed via a novel attribute propagation method. The approach further allows us to leverage the inherent correspondences between heterogeneous attribute sources despite the disparate distributions of different datasets. With the network we demonstrate state-of-the-art results on existing facial expression recognition benchmarks. To predict inter-personal relation, we use the expression recognition network as branches for a Siamese model. Extensive experiments show that our model is capable of mining mutual context of faces for accurate fine-grained interpersonal prediction.



### Partial Least Squares Regression on Riemannian Manifolds and Its Application in Classifications
- **Arxiv ID**: http://arxiv.org/abs/1609.06434v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.06434v1)
- **Published**: 2016-09-21 06:48:07+00:00
- **Updated**: 2016-09-21 06:48:07+00:00
- **Authors**: Haoran Chen, Yanfeng Sun, Junbin Gao, Yongli Hu, Baocai Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Partial least squares regression (PLSR) has been a popular technique to explore the linear relationship between two datasets. However, most of algorithm implementations of PLSR may only achieve a suboptimal solution through an optimization on the Euclidean space. In this paper, we propose several novel PLSR models on Riemannian manifolds and develop optimization algorithms based on Riemannian geometry of manifolds. This algorithm can calculate all the factors of PLSR globally to avoid suboptimal solutions. In a number of experiments, we have demonstrated the benefits of applying the proposed model and algorithm to a variety of learning tasks in pattern recognition and object classification.



### Detecting facial landmarks in the video based on a hybrid framework
- **Arxiv ID**: http://arxiv.org/abs/1609.06441v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.06441v1)
- **Published**: 2016-09-21 07:29:49+00:00
- **Updated**: 2016-09-21 07:29:49+00:00
- **Authors**: Nian Cai, Zhineng Lin, Fu Zhang, Guandong Cen, Han Wang
- **Comment**: 8 pages, 5 figures, unpublished manuscript
- **Journal**: None
- **Summary**: To dynamically detect the facial landmarks in the video, we propose a novel hybrid framework termed as detection-tracking-detection (DTD). First, the face bounding box is achieved from the first frame of the video sequence based on a traditional face detection method. Then, a landmark detector detects the facial landmarks, which is based on a cascaded deep convolution neural network (DCNN). Next, the face bounding box in the current frame is estimated and validated after the facial landmarks in the previous frame are tracked based on the median flow. Finally, the facial landmarks in the current frame are exactly detected from the validated face bounding box via the landmark detector. Experimental results indicate that the proposed framework can detect the facial landmarks in the video sequence more effectively and with lower consuming time compared to the frame-by-frame method via the DCNN.



### Multi-View Constraint Propagation with Consensus Prior Knowledge
- **Arxiv ID**: http://arxiv.org/abs/1609.06456v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.06456v1)
- **Published**: 2016-09-21 08:13:06+00:00
- **Updated**: 2016-09-21 08:13:06+00:00
- **Authors**: Yaoyi Li, Hongtao Lu
- **Comment**: None
- **Journal**: None
- **Summary**: In many applications, the pairwise constraint is a kind of weaker supervisory information which can be collected easily. The constraint propagation has been proved to be a success of exploiting such side-information. In recent years, some methods of multi-view constraint propagation have been proposed. However, the problem of reasonably fusing different views remains unaddressed. In this paper, we present a method dubbed Consensus Prior Constraint Propagation (CPCP), which can provide the prior knowledge of the robustness of each data instance and its neighborhood. With the robustness generated from the consensus information of each view, we build a unified affinity matrix as a result of the propagation. Specifically, we fuse the affinity of different views at a data instance level instead of a view level. This paper also introduces an approach to deal with the imbalance between the positive and negative constraints. The proposed method has been tested in clustering tasks on two publicly available multi-view data sets to show the superior performance.



### Document Image Coding and Clustering for Script Discrimination
- **Arxiv ID**: http://arxiv.org/abs/1609.06492v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG, cs.NE, 97R40, 62H35, 68U15, 68T50
- **Links**: [PDF](http://arxiv.org/pdf/1609.06492v1)
- **Published**: 2016-09-21 10:52:03+00:00
- **Updated**: 2016-09-21 10:52:03+00:00
- **Authors**: Darko Brodic, Alessia Amelio, Zoran N. Milivojevic, Milena Jevtic
- **Comment**: 8 pages, 4 figures, 2 tables
- **Journal**: ICIC Express Letters Vol. 10 n. 7 July 2016 pp. 1561-1566
- **Summary**: The paper introduces a new method for discrimination of documents given in different scripts. The document is mapped into a uniformly coded text of numerical values. It is derived from the position of the letters in the text line, based on their typographical characteristics. Each code is considered as a gray level. Accordingly, the coded text determines a 1-D image, on which texture analysis by run-length statistics and local binary pattern is performed. It defines feature vectors representing the script content of the document. A modified clustering approach employed on document feature vector groups documents written in the same script. Experimentation performed on two custom oriented databases of historical documents in old Cyrillic, angular and round Glagolitic as well as Antiqua and Fraktur scripts demonstrates the superiority of the proposed method with respect to well-known methods in the state-of-the-art.



### Wavelet-Based Segmentation on the Sphere
- **Arxiv ID**: http://arxiv.org/abs/1609.06500v2
- **DOI**: 10.1016/j.patcog.2019.107081
- **Categories**: **cs.CV**, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1609.06500v2)
- **Published**: 2016-09-21 11:20:19+00:00
- **Updated**: 2019-11-10 18:16:30+00:00
- **Authors**: Xiaohao Cai, Christopher G. R. Wallis, Jennifer Y. H. Chan, Jason D. McEwen
- **Comment**: None
- **Journal**: None
- **Summary**: Segmentation, a useful/powerful technique in pattern recognition, is the process of identifying object outlines within images. There are a number of efficient algorithms for segmentation in Euclidean space that depend on the variational approach and partial differential equation modelling. Wavelets have been used successfully in various problems in image processing, including segmentation, inpainting, noise removal, super-resolution image restoration, and many others. Wavelets on the sphere have been developed to solve such problems for data defined on the sphere, which arise in numerous fields such as cosmology and geophysics. In this work, we propose a wavelet-based method to segment images on the sphere, accounting for the underlying geometry of spherical data. Our method is a direct extension of the tight-frame based segmentation method used to automatically identify tube-like structures such as blood vessels in medical imaging. It is compatible with any arbitrary type of wavelet frame defined on the sphere, such as axisymmetric wavelets, directional wavelets, curvelets, and hybrid wavelet constructions. Such an approach allows the desirable properties of wavelets to be naturally inherited in the segmentation process. In particular, directional wavelets and curvelets, which were designed to efficiently capture directional signal content, provide additional advantages in segmenting images containing prominent directional and curvilinear features. We present several numerical experiments, applying our wavelet-based segmentation method, as well as the common K-means method, on real-world spherical images. These experiments demonstrate the superiority of our method and show that it is capable of segmenting different kinds of spherical images, including those with prominent directional features. Moreover, our algorithm is efficient with convergence usually within a few iterations.



### Production-Level Facial Performance Capture Using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1609.06536v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1609.06536v2)
- **Published**: 2016-09-21 12:55:59+00:00
- **Updated**: 2017-06-02 13:54:51+00:00
- **Authors**: Samuli Laine, Tero Karras, Timo Aila, Antti Herva, Shunsuke Saito, Ronald Yu, Hao Li, Jaakko Lehtinen
- **Comment**: Final SCA 2017 version
- **Journal**: None
- **Summary**: We present a real-time deep learning framework for video-based facial performance capture -- the dense 3D tracking of an actor's face given a monocular video. Our pipeline begins with accurately capturing a subject using a high-end production facial capture pipeline based on multi-view stereo tracking and artist-enhanced animations. With 5-10 minutes of captured footage, we train a convolutional neural network to produce high-quality output, including self-occluded regions, from a monocular video sequence of that subject. Since this 3D facial performance capture is fully automated, our system can drastically reduce the amount of labor involved in the development of modern narrative-driven video games or films involving realistic digital doubles of actors and potentially hours of animated dialogue per character. We compare our results with several state-of-the-art monocular real-time facial capture techniques and demonstrate compelling animation inference in challenging areas such as eyes and lips.



### Revealing Structure in Large Graphs: Szemerédi's Regularity Lemma and its Use in Pattern Recognition
- **Arxiv ID**: http://arxiv.org/abs/1609.06583v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DM
- **Links**: [PDF](http://arxiv.org/pdf/1609.06583v1)
- **Published**: 2016-09-21 14:31:26+00:00
- **Updated**: 2016-09-21 14:31:26+00:00
- **Authors**: Marcello Pelillo, Ismail Elezi, Marco Fiorucci
- **Comment**: Accepted for publication in Pattern Recognition Letters
- **Journal**: None
- **Summary**: Introduced in the mid-1970's as an intermediate step in proving a long-standing conjecture on arithmetic progressions, Szemer\'edi's regularity lemma has emerged over time as a fundamental tool in different branches of graph theory, combinatorics and theoretical computer science. Roughly, it states that every graph can be approximated by the union of a small number of random-like bipartite graphs called regular pairs. In other words, the result provides us a way to obtain a good description of a large graph using a small amount of data, and can be regarded as a manifestation of the all-pervading dichotomy between structure and randomness. In this paper we will provide an overview of the regularity lemma and its algorithmic aspects, and will discuss its relevance in the context of pattern recognition research.



### Image Denoising via Multi-scale Nonlinear Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/1609.06585v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.06585v1)
- **Published**: 2016-09-21 14:42:47+00:00
- **Updated**: 2016-09-21 14:42:47+00:00
- **Authors**: Wensen Feng, Peng Qiao, Xuanyang Xi, Yunjin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Image denoising is a fundamental operation in image processing and holds considerable practical importance for various real-world applications. Arguably several thousands of papers are dedicated to image denoising. In the past decade, sate-of-the-art denoising algorithm have been clearly dominated by non-local patch-based methods, which explicitly exploit patch self-similarity within image. However, in recent two years, discriminatively trained local approaches have started to outperform previous non-local models and have been attracting increasing attentions due to the additional advantage of computational efficiency. Successful approaches include cascade of shrinkage fields (CSF) and trainable nonlinear reaction diffusion (TNRD). These two methods are built on filter response of linear filters of small size using feed forward architectures. Due to the locality inherent in local approaches, the CSF and TNRD model become less effective when noise level is high and consequently introduces some noise artifacts. In order to overcome this problem, in this paper we introduce a multi-scale strategy. To be specific, we build on our newly-developed TNRD model, adopting the multi-scale pyramid image representation to devise a multi-scale nonlinear diffusion process. As expected, all the parameters in the proposed multi-scale diffusion model, including the filters and the influence functions across scales, are learned from training data through a loss based approach. Numerical results on Gaussian and Poisson denoising substantiate that the exploited multi-scale strategy can successfully boost the performance of the original TNRD model with single scale. As a consequence, the resulting multi-scale diffusion models can significantly suppress the typical incorrect features for those noisy images with heavy noise.



### FaceNet2ExpNet: Regularizing a Deep Face Recognition Net for Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1609.06591v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.06591v2)
- **Published**: 2016-09-21 15:04:31+00:00
- **Updated**: 2016-09-22 00:59:45+00:00
- **Authors**: Hui Ding, Shaohua Kevin Zhou, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: Relatively small data sets available for expression recognition research make the training of deep networks for expression recognition very challenging. Although fine-tuning can partially alleviate the issue, the performance is still below acceptable levels as the deep features probably contain redun- dant information from the pre-trained domain. In this paper, we present FaceNet2ExpNet, a novel idea to train an expression recognition network based on static images. We first propose a new distribution function to model the high-level neurons of the expression network. Based on this, a two-stage training algorithm is carefully designed. In the pre-training stage, we train the convolutional layers of the expression net, regularized by the face net; In the refining stage, we append fully- connected layers to the pre-trained convolutional layers and train the whole network jointly. Visualization shows that the model trained with our method captures improved high-level expression semantics. Evaluations on four public expression databases, CK+, Oulu-CASIA, TFD, and SFEW demonstrate that our method achieves better results than state-of-the-art.



### Improving analytical tomographic reconstructions through consistency conditions
- **Arxiv ID**: http://arxiv.org/abs/1609.06604v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1609.06604v1)
- **Published**: 2016-09-21 15:34:39+00:00
- **Updated**: 2016-09-21 15:34:39+00:00
- **Authors**: Filippo Arcadu, Jakob Vogel, Marco Stampanoni, Federica Marone
- **Comment**: 16 pages, 12 figures
- **Journal**: None
- **Summary**: This work introduces and characterizes a fast parameterless filter based on the Helgason-Ludwig consistency conditions, used to improve the accuracy of analytical reconstructions of tomographic undersampled datasets. The filter, acting in the Radon domain, extrapolates intermediate projections between those existing. The resulting sinogram, doubled in views, is then reconstructed by a standard analytical method. Experiments with simulated data prove that the peak-signal-to-noise ratio of the results computed by filtered backprojection is improved up to 5-6 dB, if the filter is used prior to reconstruction.



### Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge
- **Arxiv ID**: http://arxiv.org/abs/1609.06647v1
- **DOI**: 10.1109/TPAMI.2016.2587640
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.06647v1)
- **Published**: 2016-09-21 17:40:57+00:00
- **Updated**: 2016-09-21 17:40:57+00:00
- **Authors**: Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1411.4555
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence (
  Volume: PP, Issue: 99 , July 2016 )
- **Summary**: Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. Finally, given the recent surge of interest in this task, a competition was organized in 2015 using the newly released COCO dataset. We describe and analyze the various improvements we applied to our own baseline and show the resulting performance in the competition, which we won ex-aequo with a team from Microsoft Research, and provide an open source implementation in TensorFlow.



### Land Use Classification using Convolutional Neural Networks Applied to Ground-Level Images
- **Arxiv ID**: http://arxiv.org/abs/1609.06653v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1609.06653v1)
- **Published**: 2016-09-21 18:01:24+00:00
- **Updated**: 2016-09-21 18:01:24+00:00
- **Authors**: Yi Zhu, Shawn Newsam
- **Comment**: ACM SIGSPATIAL 2015, Best Poster Award
- **Journal**: None
- **Summary**: Land use mapping is a fundamental yet challenging task in geographic science. In contrast to land cover mapping, it is generally not possible using overhead imagery. The recent, explosive growth of online geo-referenced photo collections suggests an alternate approach to geographic knowledge discovery. In this work, we present a general framework that uses ground-level images from Flickr for land use mapping. Our approach benefits from several novel aspects. First, we address the nosiness of the online photo collections, such as imprecise geolocation and uneven spatial distribution, by performing location and indoor/outdoor filtering, and semi- supervised dataset augmentation. Our indoor/outdoor classifier achieves state-of-the-art performance on several bench- mark datasets and approaches human-level accuracy. Second, we utilize high-level semantic image features extracted using deep learning, specifically convolutional neural net- works, which allow us to achieve upwards of 76% accuracy on a challenging eight class land use mapping problem.



### The Color of the Cat is Gray: 1 Million Full-Sentences Visual Question Answering (FSVQA)
- **Arxiv ID**: http://arxiv.org/abs/1609.06657v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1609.06657v1)
- **Published**: 2016-09-21 18:12:04+00:00
- **Updated**: 2016-09-21 18:12:04+00:00
- **Authors**: Andrew Shin, Yoshitaka Ushiku, Tatsuya Harada
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) task has showcased a new stage of interaction between language and vision, two of the most pivotal components of artificial intelligence. However, it has mostly focused on generating short and repetitive answers, mostly single words, which fall short of rich linguistic capabilities of humans. We introduce Full-Sentence Visual Question Answering (FSVQA) dataset, consisting of nearly 1 million pairs of questions and full-sentence answers for images, built by applying a number of rule-based natural language processing techniques to original VQA dataset and captions in the MS COCO dataset. This poses many additional complexities to conventional VQA task, and we provide a baseline for approaching and evaluating the task, on top of which we invite the research community to build further improvements.



### Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1609.06666v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1609.06666v2)
- **Published**: 2016-09-21 18:32:11+00:00
- **Updated**: 2017-03-05 15:29:45+00:00
- **Authors**: Martin Engelcke, Dushyant Rao, Dominic Zeng Wang, Chi Hay Tong, Ingmar Posner
- **Comment**: To be published at the IEEE International Conference on Robotics and
  Automation 2017
- **Journal**: None
- **Summary**: This paper proposes a computationally efficient approach to detecting objects natively in 3D point clouds using convolutional neural networks (CNNs). In particular, this is achieved by leveraging a feature-centric voting scheme to implement novel convolutional layers which explicitly exploit the sparsity encountered in the input. To this end, we examine the trade-off between accuracy and speed for different architectures and additionally propose to use an L1 penalty on the filter activations to further encourage sparsity in the intermediate representations. To the best of our knowledge, this is the first work to propose sparse convolutional layers and L1 regularisation for efficient large-scale processing of 3D data. We demonstrate the efficacy of our approach on the KITTI object detection benchmark and show that Vote3Deep models with as few as three layers outperform the previous state of the art in both laser and laser-vision based approaches by margins of up to 40% while remaining highly competitive in terms of processing time.



### Characterization of Lung Nodule Malignancy using Hybrid Shape and Appearance Features
- **Arxiv ID**: http://arxiv.org/abs/1609.06668v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.06668v1)
- **Published**: 2016-09-21 18:33:56+00:00
- **Updated**: 2016-09-21 18:33:56+00:00
- **Authors**: Mario Buty, Ziyue Xu, Mingchen Gao, Ulas Bagci, Aaron Wu, Daniel J. Mollura
- **Comment**: Accepted to MICCAI 2016
- **Journal**: None
- **Summary**: Computed tomography imaging is a standard modality for detecting and assessing lung cancer. In order to evaluate the malignancy of lung nodules, clinical practice often involves expert qualitative ratings on several criteria describing a nodule's appearance and shape. Translating these features for computer-aided diagnostics is challenging due to their subjective nature and the difficulties in gaining a complete description. In this paper, we propose a computerized approach to quantitatively evaluate both appearance distinctions and 3D surface variations. Nodule shape was modeled and parameterized using spherical harmonics, and appearance features were extracted using deep convolutional neural networks. Both sets of features were combined to estimate the nodule malignancy using a random forest classifier. The proposed algorithm was tested on the publicly available Lung Image Database Consortium dataset, achieving high accuracy. By providing lung nodule characterization, this method can provide a robust alternative reference opinion for lung cancer diagnosis.



### Fast and reliable stereopsis measurement at multiple distances with iPad
- **Arxiv ID**: http://arxiv.org/abs/1609.06669v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.06669v1)
- **Published**: 2016-09-21 18:34:25+00:00
- **Updated**: 2016-09-21 18:34:25+00:00
- **Authors**: Manuel Rodriguez-Vallejo, Clara Llorens-Quintana, Diego Montagud, Walter D. Furlan, Juan A. Monsoriu
- **Comment**: 14 pages, 3 figures, 4 tables
- **Journal**: None
- **Summary**: Purpose: To present a new fast and reliable application for iPad (ST) for screening stereopsis at multiple distances.   Methods: A new iPad application (app) based on a random dot stereogram was designed for screening stereopsis at multiple distances. Sixty-five subjects with no ocular diseases and wearing their habitual correction were tested at two different distances: 3 m and at 0.4 m. Results were compared with other commercial tests: TNO (at near) and Howard Dolman (at distance) Subjects were cited one week later in order to repeat the same procedures for assessing reproducibility of the tests.   Results: Stereopsis at near was better with ST (40 arcsec) than with TNO (60 arcsec), but not significantly (p = 0.36). The agreement was good (k = 0.604) and the reproducibility was better with ST (k = 0.801) than with TNO (k = 0.715), in fact median difference between days was significant only with TNO (p = 0.02). On the other hand, poor agreement was obtained between HD and ST at far distance (k=0.04), obtaining significant differences in medians (p = 0.001) and poorer reliability with HD (k = 0.374) than with ST (k = 0.502).   Conclusions: Screening stereopsis at near with a new iPad app demonstrated to be a fast and realiable. Results were in a good agreement with conventional tests as TNO, but it could not be compared at far vision with HD due to the limited resolution of the iPad.



### PixelNet: Towards a General Pixel-level Architecture
- **Arxiv ID**: http://arxiv.org/abs/1609.06694v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1609.06694v1)
- **Published**: 2016-09-21 19:32:46+00:00
- **Updated**: 2016-09-21 19:32:46+00:00
- **Authors**: Aayush Bansal, Xinlei Chen, Bryan Russell, Abhinav Gupta, Deva Ramanan
- **Comment**: None
- **Journal**: None
- **Summary**: We explore architectures for general pixel-level prediction problems, from low-level edge detection to mid-level surface normal estimation to high-level semantic segmentation. Convolutional predictors, such as the fully-convolutional network (FCN), have achieved remarkable success by exploiting the spatial redundancy of neighboring pixels through convolutional processing. Though computationally efficient, we point out that such approaches are not statistically efficient during learning precisely because spatial redundancy limits the information learned from neighboring pixels. We demonstrate that (1) stratified sampling allows us to add diversity during batch updates and (2) sampled multi-scale features allow us to explore more nonlinear predictors (multiple fully-connected layers followed by ReLU) that improve overall accuracy. Finally, our objective is to show how a architecture can get performance better than (or comparable to) the architectures designed for a particular task. Interestingly, our single architecture produces state-of-the-art results for semantic segmentation on PASCAL-Context, surface normal estimation on NYUDv2 dataset, and edge detection on BSDS without contextual post-processing.



### How should we evaluate supervised hashing?
- **Arxiv ID**: http://arxiv.org/abs/1609.06753v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.06753v3)
- **Published**: 2016-09-21 21:03:36+00:00
- **Updated**: 2017-08-10 17:00:50+00:00
- **Authors**: Alexandre Sablayrolles, Matthijs Douze, Hervé Jégou, Nicolas Usunier
- **Comment**: None
- **Journal**: None
- **Summary**: Hashing produces compact representations for documents, to perform tasks like classification or retrieval based on these short codes. When hashing is supervised, the codes are trained using labels on the training data. This paper first shows that the evaluation protocols used in the literature for supervised hashing are not satisfactory: we show that a trivial solution that encodes the output of a classifier significantly outperforms existing supervised or semi-supervised methods, while using much shorter codes. We then propose two alternative protocols for supervised hashing: one based on retrieval on a disjoint set of classes, and another based on transfer learning to new classes. We provide two baseline methods for image-related tasks to assess the performance of (semi-)supervised hashing: without coding and with unsupervised codes. These baselines give a lower- and upper-bound on the performance of a supervised hashing scheme.



### Spatio-Temporal Sentiment Hotspot Detection Using Geotagged Photos
- **Arxiv ID**: http://arxiv.org/abs/1609.06772v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1609.06772v1)
- **Published**: 2016-09-21 22:43:54+00:00
- **Updated**: 2016-09-21 22:43:54+00:00
- **Authors**: Yi Zhu, Shawn Newsam
- **Comment**: To appear in ACM SIGSPATIAL 2016
- **Journal**: None
- **Summary**: We perform spatio-temporal analysis of public sentiment using geotagged photo collections. We develop a deep learning-based classifier that predicts the emotion conveyed by an image. This allows us to associate sentiment with place. We perform spatial hotspot detection and show that different emotions have distinct spatial distributions that match expectations. We also perform temporal analysis using the capture time of the photos. Our spatio-temporal hotspot detection correctly identifies emerging concentrations of specific emotions and year-by-year analyses of select locations show there are strong temporal correlations between the predicted emotions and known events.



