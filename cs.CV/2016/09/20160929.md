# Arxiv Papers in cs.CV on 2016-09-29
### OPML: A One-Pass Closed-Form Solution for Online Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1609.09178v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1609.09178v1)
- **Published**: 2016-09-29 02:18:06+00:00
- **Updated**: 2016-09-29 02:18:06+00:00
- **Authors**: Wenbin Li, Yang Gao, Lei Wang, Luping Zhou, Jing Huo, Yinghuan Shi
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: To achieve a low computational cost when performing online metric learning for large-scale data, we present a one-pass closed-form solution namely OPML in this paper. Typically, the proposed OPML first adopts a one-pass triplet construction strategy, which aims to use only a very small number of triplets to approximate the representation ability of whole original triplets obtained by batch-manner methods. Then, OPML employs a closed-form solution to update the metric for new coming samples, which leads to a low space (i.e., $O(d)$) and time (i.e., $O(d^2)$) complexity, where $d$ is the feature dimensionality. In addition, an extension of OPML (namely COPML) is further proposed to enhance the robustness when in real case the first several samples come from the same class (i.e., cold start problem). In the experiments, we have systematically evaluated our methods (OPML and COPML) on three typical tasks, including UCI data classification, face verification, and abnormal event detection in videos, which aims to fully evaluate the proposed methods on different sample number, different feature dimensionalities and different feature extraction ways (i.e., hand-crafted and deeply-learned). The results show that OPML and COPML can obtain the promising performance with a very low computational cost. Also, the effectiveness of COPML under the cold start setting is experimentally verified.



### Structure-Aware Classification using Supervised Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/1609.09199v1
- **DOI**: 10.1109/ICASSP.2017.7952992
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1609.09199v1)
- **Published**: 2016-09-29 04:30:10+00:00
- **Updated**: 2016-09-29 04:30:10+00:00
- **Authors**: Yael Yankelevsky, Michael Elad
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a supervised dictionary learning algorithm that aims to preserve the local geometry in both dimensions of the data. A graph-based regularization explicitly takes into account the local manifold structure of the data points. A second graph regularization gives similar treatment to the feature domain and helps in learning a more robust dictionary. Both graphs can be constructed from the training data or learned and adapted along the dictionary learning process. The combination of these two terms promotes the discriminative power of the learned sparse representations and leads to improved classification accuracy. The proposed method was evaluated on several different datasets, representing both single-label and multi-label classification problems, and demonstrated better performance compared with other dictionary based approaches.



### CNN-aware Binary Map for General Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1609.09220v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.09220v1)
- **Published**: 2016-09-29 06:46:27+00:00
- **Updated**: 2016-09-29 06:46:27+00:00
- **Authors**: Mahdyar Ravanbakhsh, Hossein Mousavi, Moin Nabi, Mohammad Rastegari, Carlo Regazzoni
- **Comment**: ICIP 2016 Best Paper / Student Paper Finalist
- **Journal**: None
- **Summary**: In this paper we introduce a novel method for general semantic segmentation that can benefit from general semantics of Convolutional Neural Network (CNN). Our segmentation proposes visually and semantically coherent image segments. We use binary encoding of CNN features to overcome the difficulty of the clustering on the high-dimensional CNN feature space. These binary codes are very robust against noise and non-semantic changes in the image. These binary encoding can be embedded into the CNN as an extra layer at the end of the network. This results in real-time segmentation. To the best of our knowledge our method is the first attempt on general semantic image segmentation using CNN. All the previous papers were limited to few number of category of the images (e.g. PASCAL VOC). Experiments show that our segmentation algorithm outperform the state-of-the-art non-semantic segmentation methods by large margin.



### A comparative study of complexity of handwritten Bharati characters with that of major Indian scripts
- **Arxiv ID**: http://arxiv.org/abs/1609.09227v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.09227v1)
- **Published**: 2016-09-29 07:06:46+00:00
- **Updated**: 2016-09-29 07:06:46+00:00
- **Authors**: Manali Naik, V. Srinivasa Chakravarthy
- **Comment**: 22 pages
- **Journal**: None
- **Summary**: We present Bharati, a simple, novel script that can represent the characters of a majority of contemporary Indian scripts. The shapes/motifs of Bharati characters are drawn from some of the simplest characters of existing Indian scripts. Bharati characters are designed such that they strictly reflect the underlying phonetic organization, thereby attributing to the script qualities of simplicity, familiarity, ease of acquisition and use. Thus, employing Bharati script as a common script for a majority of Indian languages can ameliorate several existing communication bottlenecks in India. We perform a complexity analysis of handwritten Bharati script and compare its complexity with that of 9 major Indian scripts. The measures of complexity are derived from a theory of handwritten characters based on Catastrophe theory. Bharati script is shown to be simpler than the 9 major Indian scripts in most measures of complexity.



### Modelling depth for nonparametric foreground segmentation using RGBD devices
- **Arxiv ID**: http://arxiv.org/abs/1609.09240v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.09240v1)
- **Published**: 2016-09-29 07:54:31+00:00
- **Updated**: 2016-09-29 07:54:31+00:00
- **Authors**: Gabriel Moyà-Alcover, Ahmed Elgammal, Antoni Jaume-i-Capó, Javier Varona
- **Comment**: Accepted in Pattern Recognition Letters. Will update the info
- **Journal**: None
- **Summary**: The problem of detecting changes in a scene and segmenting the foreground from background is still challenging, despite previous work. Moreover, new RGBD capturing devices include depth cues, which could be incorporated to improve foreground segmentation. In this work, we present a new nonparametric approach where a unified model mixes the device multiple information cues. In order to unify all the device channel cues, a new probabilistic depth data model is also proposed where we show how handle the inaccurate data to improve foreground segmentation. A new RGBD video dataset is presented in order to introduce a new standard for comparison purposes of this kind of algorithms. Results show that the proposed approach can handle several practical situations and obtain good results in all cases.



### Kernel Methods on Approximate Infinite-Dimensional Covariance Operators for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1609.09251v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.09251v1)
- **Published**: 2016-09-29 08:26:28+00:00
- **Updated**: 2016-09-29 08:26:28+00:00
- **Authors**: Hà Quang Minh, Marco San Biagio, Loris Bazzani, Vittorio Murino
- **Comment**: 18 double-column pages
- **Journal**: None
- **Summary**: This paper presents a novel framework for visual object recognition using infinite-dimensional covariance operators of input features in the paradigm of kernel methods on infinite-dimensional Riemannian manifolds. Our formulation provides in particular a rich representation of image features by exploiting their non-linear correlations. Theoretically, we provide a finite-dimensional approximation of the Log-Hilbert-Schmidt (Log-HS) distance between covariance operators that is scalable to large datasets, while maintaining an effective discriminating capability. This allows us to efficiently approximate any continuous shift-invariant kernel defined using the Log-HS distance. At the same time, we prove that the Log-HS inner product between covariance operators is only approximable by its finite-dimensional counterpart in a very limited scenario. Consequently, kernels defined using the Log-HS inner product, such as polynomial kernels, are not scalable in the same way as shift-invariant kernels. Computationally, we apply the approximate Log-HS distance formulation to covariance operators of both handcrafted and convolutional features, exploiting both the expressiveness of these features and the power of the covariance representation. Empirically, we tested our framework on the task of image classification on twelve challenging datasets. In almost all cases, the results obtained outperform other state of the art methods, demonstrating the competitiveness and potential of our framework.



### Robust Moving Objects Detection in Lidar Data Exploiting Visual Cues
- **Arxiv ID**: http://arxiv.org/abs/1609.09267v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1609.09267v1)
- **Published**: 2016-09-29 09:29:46+00:00
- **Updated**: 2016-09-29 09:29:46+00:00
- **Authors**: Gheorghii Postica, Andrea Romanoni, Matteo Matteucci
- **Comment**: 6 pages, to appear in IROS 2016
- **Journal**: None
- **Summary**: Detecting moving objects in dynamic scenes from sequences of lidar scans is an important task in object tracking, mapping, localization, and navigation. Many works focus on changes detection in previously observed scenes, while a very limited amount of literature addresses moving objects detection. The state-of-the-art method exploits Dempster-Shafer Theory to evaluate the occupancy of a lidar scan and to discriminate points belonging to the static scene from moving ones. In this paper we improve both speed and accuracy of this method by discretizing the occupancy representation, and by removing false positives through visual cues. Many false positives lying on the ground plane are also removed thanks to a novel ground plane removal algorithm. Efficiency is improved through an octree indexing strategy. Experimental evaluation against the KITTI public dataset shows the effectiveness of our approach, both qualitatively and quantitatively with respect to the state- of-the-art.



### Pano2CAD: Room Layout From A Single Panorama Image
- **Arxiv ID**: http://arxiv.org/abs/1609.09270v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.09270v2)
- **Published**: 2016-09-29 09:35:29+00:00
- **Updated**: 2016-09-30 08:33:25+00:00
- **Authors**: Jiu Xu, Bjorn Stenger, Tommi Kerola, Tony Tung
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a method of estimating the geometry of a room and the 3D pose of objects from a single 360-degree panorama image. Assuming Manhattan World geometry, we formulate the task as a Bayesian inference problem in which we estimate positions and orientations of walls and objects. The method combines surface normal estimation, 2D object detection and 3D object pose estimation. Quantitative results are presented on a dataset of synthetically generated 3D rooms containing objects, as well as on a subset of hand-labeled images from the public SUN360 dataset.



### Comprehensive Evaluation of OpenCL-based Convolutional Neural Network Accelerators in Xilinx and Altera FPGAs
- **Arxiv ID**: http://arxiv.org/abs/1609.09296v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1609.09296v1)
- **Published**: 2016-09-29 11:03:21+00:00
- **Updated**: 2016-09-29 11:03:21+00:00
- **Authors**: R. Tapiador, A. Rios-Navarro, A. Linares-Barranco, Minkyu Kim, Deepak Kadetotad, Jae-sun Seo
- **Comment**: 6 pages, 6 figures. Robotic and Technology of Computers Lab report
- **Journal**: None
- **Summary**: Deep learning has significantly advanced the state of the art in artificial intelligence, gaining wide popularity from both industry and academia. Special interest is around Convolutional Neural Networks (CNN), which take inspiration from the hierarchical structure of the visual cortex, to form deep layers of convolutional operations, along with fully connected classifiers. Hardware implementations of these deep CNN architectures are challenged with memory bottlenecks that require many convolution and fully-connected layers demanding large amount of communication for parallel computation. Multi-core CPU based solutions have demonstrated their inadequacy for this problem due to the memory wall and low parallelism. Many-core GPU architectures show superior performance but they consume high power and also have memory constraints due to inconsistencies between cache and main memory. FPGA design solutions are also actively being explored, which allow implementing the memory hierarchy using embedded BlockRAM. This boosts the parallel use of shared memory elements between multiple processing units, avoiding data replicability and inconsistencies. This makes FPGAs potentially powerful solutions for real-time classification of CNNs. Both Altera and Xilinx have adopted OpenCL co-design framework from GPU for FPGA designs as a pseudo-automatic development solution. In this paper, a comprehensive evaluation and comparison of Altera and Xilinx OpenCL frameworks for a 5-layer deep CNN is presented. Hardware resources, temporal performance and the OpenCL architecture for CNNs are discussed. Xilinx demonstrates faster synthesis, better FPGA resource utilization and more compact boards. Altera provides multi-platforms tools, mature design community and better execution times.



### Deep Tracking on the Move: Learning to Track the World from a Moving Vehicle using Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1609.09365v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1609.09365v3)
- **Published**: 2016-09-29 14:39:10+00:00
- **Updated**: 2017-04-19 14:31:32+00:00
- **Authors**: Julie Dequaire, Dushyant Rao, Peter Ondruska, Dominic Wang, Ingmar Posner
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents an end-to-end approach for tracking static and dynamic objects for an autonomous vehicle driving through crowded urban environments. Unlike traditional approaches to tracking, this method is learned end-to-end, and is able to directly predict a full unoccluded occupancy grid map from raw laser input data. Inspired by the recently presented DeepTracking approach [Ondruska, 2016], we employ a recurrent neural network (RNN) to capture the temporal evolution of the state of the environment, and propose to use Spatial Transformer modules to exploit estimates of the egomotion of the vehicle. Our results demonstrate the ability to track a range of objects, including cars, buses, pedestrians, and cyclists through occlusion, from both moving and stationary platforms, using a single learned model. Experimental results demonstrate that the model can also predict the future states of objects from current inputs, with greater accuracy than previous work.



### Cooperative Training of Descriptor and Generator Networks
- **Arxiv ID**: http://arxiv.org/abs/1609.09408v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1609.09408v3)
- **Published**: 2016-09-29 16:14:45+00:00
- **Updated**: 2018-10-29 16:42:24+00:00
- **Authors**: Jianwen Xie, Yang Lu, Ruiqi Gao, Song-Chun Zhu, Ying Nian Wu
- **Comment**: 18 pages
- **Journal**: None
- **Summary**: This paper studies the cooperative training of two generative models for image modeling and synthesis. Both models are parametrized by convolutional neural networks (ConvNets). The first model is a deep energy-based model, whose energy function is defined by a bottom-up ConvNet, which maps the observed image to the energy. We call it the descriptor network. The second model is a generator network, which is a non-linear version of factor analysis. It is defined by a top-down ConvNet, which maps the latent factors to the observed image. The maximum likelihood learning algorithms of both models involve MCMC sampling such as Langevin dynamics. We observe that the two learning algorithms can be seamlessly interwoven into a cooperative learning algorithm that can train both models simultaneously. Specifically, within each iteration of the cooperative learning algorithm, the generator model generates initial synthesized examples to initialize a finite-step MCMC that samples and trains the energy-based descriptor model. After that, the generator model learns from how the MCMC changes its synthesized examples. That is, the descriptor model teaches the generator model by MCMC, so that the generator model accumulates the MCMC transitions and reproduces them by direct ancestral sampling. We call this scheme MCMC teaching. We show that the cooperative algorithm can learn highly realistic generative models.



### A Searchlight Factor Model Approach for Locating Shared Information in Multi-Subject fMRI Analysis
- **Arxiv ID**: http://arxiv.org/abs/1609.09432v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1609.09432v1)
- **Published**: 2016-09-29 17:20:23+00:00
- **Updated**: 2016-09-29 17:20:23+00:00
- **Authors**: Hejia Zhang, Po-Hsuan Chen, Janice Chen, Xia Zhu, Javier S. Turek, Theodore L. Willke, Uri Hasson, Peter J. Ramadge
- **Comment**: None
- **Journal**: None
- **Summary**: There is a growing interest in joint multi-subject fMRI analysis. The challenge of such analysis comes from inherent anatomical and functional variability across subjects. One approach to resolving this is a shared response factor model. This assumes a shared and time synchronized stimulus across subjects. Such a model can often identify shared information, but it may not be able to pinpoint with high resolution the spatial location of this information. In this work, we examine a searchlight based shared response model to identify shared information in small contiguous regions (searchlights) across the whole brain. Validation using classification tasks demonstrates that we can pinpoint informative local regions.



### Contextual RNN-GANs for Abstract Reasoning Diagram Generation
- **Arxiv ID**: http://arxiv.org/abs/1609.09444v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1609.09444v2)
- **Published**: 2016-09-29 17:56:32+00:00
- **Updated**: 2016-12-06 13:14:09+00:00
- **Authors**: Arnab Ghosh, Viveka Kulharia, Amitabha Mukerjee, Vinay Namboodiri, Mohit Bansal
- **Comment**: To Appear in AAAI-17 and NIPS Workshop on Adversarial Training
- **Journal**: None
- **Summary**: Understanding, predicting, and generating object motions and transformations is a core problem in artificial intelligence. Modeling sequences of evolving images may provide better representations and models of motion and may ultimately be used for forecasting, simulation, or video generation. Diagrammatic Abstract Reasoning is an avenue in which diagrams evolve in complex patterns and one needs to infer the underlying pattern sequence and generate the next image in the sequence. For this, we develop a novel Contextual Generative Adversarial Network based on Recurrent Neural Networks (Context-RNN-GANs), where both the generator and the discriminator modules are based on contextual history (modeled as RNNs) and the adversarial discriminator guides the generator to produce realistic images for the particular time step in the image sequence. We evaluate the Context-RNN-GAN model (and its variants) on a novel dataset of Diagrammatic Abstract Reasoning, where it performs competitively with 10th-grade human performance but there is still scope for interesting improvements as compared to college-grade human performance. We also evaluate our model on a standard video next-frame prediction task, achieving improved performance over comparable state-of-the-art.



### Redefining Binarization and the Visual Archetype
- **Arxiv ID**: http://arxiv.org/abs/1609.09451v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.09451v1)
- **Published**: 2016-09-29 18:15:16+00:00
- **Updated**: 2016-09-29 18:15:16+00:00
- **Authors**: Anguelos Nicolaou, Liwicki Marcus
- **Comment**: Short paper presented at the 12th IEEE workshop on Document Analysis
  Systems (DAS)
- **Journal**: None
- **Summary**: Although binarization is considered passe, it still remains a highly popular research topic. In this paper we propose a rethinking of what binarization is. We introduce the notion of the visual archetype as the ideal form of any one document. Binarization can be defined as the restoration of the visual archetype for a class of images. This definition broadens the scope of what binarization means but also suggests ground-truth should focus on the foreground.



### Reconstructing Vechicles from a Single Image: Shape Priors for Road Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/1609.09468v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1609.09468v1)
- **Published**: 2016-09-29 19:17:38+00:00
- **Updated**: 2016-09-29 19:17:38+00:00
- **Authors**: J. Krishna Murthy, G. V. Sai Krishna, Falak Chhaya, K. Madhava Krishna
- **Comment**: None
- **Journal**: None
- **Summary**: We present an approach for reconstructing vehicles from a single (RGB) image, in the context of autonomous driving. Though the problem appears to be ill-posed, we demonstrate that prior knowledge about how 3D shapes of vehicles project to an image can be used to reason about the reverse process, i.e., how shapes (back-)project from 2D to 3D. We encode this knowledge in \emph{shape priors}, which are learnt over a small keypoint-annotated dataset. We then formulate a shape-aware adjustment problem that uses the learnt shape priors to recover the 3D pose and shape of a query object from an image. For shape representation and inference, we leverage recent successes of Convolutional Neural Networks (CNNs) for the task of object and keypoint localization, and train a novel cascaded fully-convolutional architecture to localize vehicle \emph{keypoints} in images. The shape-aware adjustment then robustly recovers shape (3D locations of the detected keypoints) while simultaneously filling in occluded keypoints. To tackle estimation errors incurred due to erroneously detected keypoints, we use an Iteratively Re-weighted Least Squares (IRLS) scheme for robust optimization, and as a by-product characterize noise models for each predicted keypoint. We evaluate our approach on autonomous driving benchmarks, and present superior results to existing monocular, as well as stereo approaches.



### Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the Amazon Picking Challenge
- **Arxiv ID**: http://arxiv.org/abs/1609.09475v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1609.09475v3)
- **Published**: 2016-09-29 19:39:13+00:00
- **Updated**: 2017-05-07 20:12:55+00:00
- **Authors**: Andy Zeng, Kuan-Ting Yu, Shuran Song, Daniel Suo, Ed Walker Jr., Alberto Rodriguez, Jianxiong Xiao
- **Comment**: To appear at the International Conference on Robotics and Automation
  (ICRA) 2017. Project webpage: http://apc.cs.princeton.edu/
- **Journal**: None
- **Summary**: Robot warehouse automation has attracted significant interest in recent years, perhaps most visibly in the Amazon Picking Challenge (APC). A fully autonomous warehouse pick-and-place system requires robust vision that reliably recognizes and locates objects amid cluttered environments, self-occlusions, sensor noise, and a large variety of objects. In this paper we present an approach that leverages multi-view RGB-D data and self-supervised, data-driven learning to overcome those difficulties. The approach was part of the MIT-Princeton Team system that took 3rd- and 4th- place in the stowing and picking tasks, respectively at APC 2016. In the proposed approach, we segment and label multiple views of a scene with a fully convolutional neural network, and then fit pre-scanned 3D object models to the resulting segmentation to get the 6D object pose. Training a deep neural network for segmentation typically requires a large amount of training data. We propose a self-supervised method to generate a large labeled dataset without tedious manual segmentation. We demonstrate that our system can reliably estimate the 6D pose of objects under a variety of scenarios. All code, data, and benchmarks are available at http://apc.cs.princeton.edu/



### Multi-dimensional signal approximation with sparse structured priors using split Bregman iterations
- **Arxiv ID**: http://arxiv.org/abs/1609.09525v1
- **DOI**: None
- **Categories**: **cs.DS**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1609.09525v1)
- **Published**: 2016-09-29 20:50:16+00:00
- **Updated**: 2016-09-29 20:50:16+00:00
- **Authors**: Yoann Isaac, Quentin Barthélemy, Cédric Gouy-Pailler, Michèle Sebag, Jamal Atif
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the structurally-constrained sparse decomposition of multi-dimensional signals onto overcomplete families of vectors, called dictionaries. The contribution of the paper is threefold. Firstly, a generic spatio-temporal regularization term is designed and used together with the standard $\ell_1$ regularization term to enforce a sparse decomposition preserving the spatio-temporal structure of the signal. Secondly, an optimization algorithm based on the split Bregman approach is proposed to handle the associated optimization problem, and its convergence is analyzed. Our well-founded approach yields same accuracy as the other algorithms at the state-of-the-art, with significant gains in terms of convergence speed. Thirdly, the empirical validation of the approach on artificial and real-world problems demonstrates the generality and effectiveness of the method. On artificial problems, the proposed regularization subsumes the Total Variation minimization and recovers the expected decomposition. On the real-world problem of electro-encephalography brainwave decomposition, the approach outperforms similar approaches in terms of P300 evoked potentials detection, using structured spatial priors to guide the decomposition.



### Two-stage Convolutional Part Heatmap Regression for the 1st 3D Face Alignment in the Wild (3DFAW) Challenge
- **Arxiv ID**: http://arxiv.org/abs/1609.09545v1
- **DOI**: 10.1007/978-3-319-48881-3_43
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.09545v1)
- **Published**: 2016-09-29 23:07:01+00:00
- **Updated**: 2016-09-29 23:07:01+00:00
- **Authors**: Adrian Bulat, Georgios Tzimiropoulos
- **Comment**: Winner of 3D Face Alignment in the Wild (3DFAW) Challenge, ECCV 2016
- **Journal**: None
- **Summary**: This paper describes our submission to the 1st 3D Face Alignment in the Wild (3DFAW) Challenge. Our method builds upon the idea of convolutional part heatmap regression [1], extending it for 3D face alignment. Our method decomposes the problem into two parts: (a) X,Y (2D) estimation and (b) Z (depth) estimation. At the first stage, our method estimates the X,Y coordinates of the facial landmarks by producing a set of 2D heatmaps, one for each landmark, using convolutional part heatmap regression. Then, these heatmaps, alongside the input RGB image, are used as input to a very deep subnetwork trained via residual learning for regressing the Z coordinate. Our method ranked 1st in the 3DFAW Challenge, surpassing the second best result by more than 22%.



