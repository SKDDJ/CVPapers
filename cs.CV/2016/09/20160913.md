# Arxiv Papers in cs.CV on 2016-09-13
### DeepSkeleton: Learning Multi-task Scale-associated Deep Side Outputs for Object Skeleton Extraction in Natural Images
- **Arxiv ID**: http://arxiv.org/abs/1609.03659v3
- **DOI**: 10.1109/TIP.2017.2735182
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.03659v3)
- **Published**: 2016-09-13 02:23:39+00:00
- **Updated**: 2017-07-13 19:39:16+00:00
- **Authors**: Wei Shen, Kai Zhao, Yuan Jiang, Yan Wang, Xiang Bai, Alan Yuille
- **Comment**: submitted to TIP. arXiv admin note: substantial text overlap with
  arXiv:1603.09446
- **Journal**: None
- **Summary**: Object skeletons are useful for object representation and object detection. They are complementary to the object contour, and provide extra information, such as how object scale (thickness) varies among object parts. But object skeleton extraction from natural images is very challenging, because it requires the extractor to be able to capture both local and non-local image context in order to determine the scale of each skeleton pixel. In this paper, we present a novel fully convolutional network with multiple scale-associated side outputs to address this problem. By observing the relationship between the receptive field sizes of the different layers in the network and the skeleton scales they can capture, we introduce two scale-associated side outputs to each stage of the network. The network is trained by multi-task learning, where one task is skeleton localization to classify whether a pixel is a skeleton pixel or not, and the other is skeleton scale prediction to regress the scale of each skeleton pixel. Supervision is imposed at different stages by guiding the scale-associated side outputs toward the groundtruth skeletons at the appropriate scales. The responses of the multiple scale-associated side outputs are then fused in a scale-specific way to detect skeleton pixels using multiple scales effectively. Our method achieves promising results on two skeleton extraction datasets, and significantly outperforms other competitors. Additionally, the usefulness of the obtained skeletons and scales (thickness) are verified on two object detection applications: Foreground object segmentation and object proposal detection.



### Unsupervised Monocular Depth Estimation with Left-Right Consistency
- **Arxiv ID**: http://arxiv.org/abs/1609.03677v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1609.03677v3)
- **Published**: 2016-09-13 04:48:31+00:00
- **Updated**: 2017-04-12 14:40:50+00:00
- **Authors**: Clément Godard, Oisin Mac Aodha, Gabriel J. Brostow
- **Comment**: CVPR 2017 oral
- **Journal**: None
- **Summary**: Learning based methods have shown very promising results for the task of depth estimation in single images. However, most existing approaches treat depth prediction as a supervised regression problem and as a result, require vast quantities of corresponding ground truth depth data for training. Just recording quality depth data in a range of environments is a challenging problem. In this paper, we innovate beyond existing approaches, replacing the use of explicit depth data during training with easier-to-obtain binocular stereo footage.   We propose a novel training objective that enables our convolutional neural network to learn to perform single image depth estimation, despite the absence of ground truth depth data. Exploiting epipolar geometry constraints, we generate disparity images by training our network with an image reconstruction loss. We show that solving for image reconstruction alone results in poor quality depth images. To overcome this problem, we propose a novel training loss that enforces consistency between the disparities produced relative to both the left and right images, leading to improved performance and robustness compared to existing approaches. Our method produces state of the art results for monocular depth estimation on the KITTI driving dataset, even outperforming supervised methods that have been trained with ground truth depth.



### 3D Simulation for Robot Arm Control with Deep Q-Learning
- **Arxiv ID**: http://arxiv.org/abs/1609.03759v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1609.03759v2)
- **Published**: 2016-09-13 10:40:24+00:00
- **Updated**: 2016-12-13 16:09:17+00:00
- **Authors**: Stephen James, Edward Johns
- **Comment**: In NIPS 2016 Workshop: Deep Learning for Action and Interaction
  (https://sites.google.com/site/nips16interaction/)
- **Journal**: None
- **Summary**: Recent trends in robot arm control have seen a shift towards end-to-end solutions, using deep reinforcement learning to learn a controller directly from raw sensor data, rather than relying on a hand-crafted, modular pipeline. However, the high dimensionality of the state space often means that it is impractical to generate sufficient training data with real-world experiments. As an alternative solution, we propose to learn a robot controller in simulation, with the potential of then transferring this to a real robot. Building upon the recent success of deep Q-networks, we present an approach which uses 3D simulations to train a 7-DOF robotic arm in a control task without any prior knowledge. The controller accepts images of the environment as its only input, and outputs motor actions for the task of locating and grasping a cube, over a range of initial configurations. To encourage efficient learning, a structured reward function is designed with intermediate rewards. We also present preliminary results in direct transfer of policies over to a real robot, without any further training.



### Lie-X: Depth Image Based Articulated Object Pose Estimation, Tracking, and Action Recognition on Lie Groups
- **Arxiv ID**: http://arxiv.org/abs/1609.03773v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.03773v1)
- **Published**: 2016-09-13 11:36:26+00:00
- **Updated**: 2016-09-13 11:36:26+00:00
- **Authors**: Chi Xu, Lakshmi Narasimhan Govindarajan, Yu Zhang, Li Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Pose estimation, tracking, and action recognition of articulated objects from depth images are important and challenging problems, which are normally considered separately. In this paper, a unified paradigm based on Lie group theory is proposed, which enables us to collectively address these related problems. Our approach is also applicable to a wide range of articulated objects. Empirically it is evaluated on lab animals including mouse and fish, as well as on human hand. On these applications, it is shown to deliver competitive results compared to the state-of-the-arts, and non-trivial baselines including convolutional neural networks and regression forest methods.



### Towards Deep Compositional Networks
- **Arxiv ID**: http://arxiv.org/abs/1609.03795v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.03795v1)
- **Published**: 2016-09-13 12:31:29+00:00
- **Updated**: 2016-09-13 12:31:29+00:00
- **Authors**: Domen Tabernik, Matej Kristan, Jeremy L. Wyatt, Aleš Leonardis
- **Comment**: Published in proceedings of 23th International Conference on Pattern
  Recognition (ICPR 2016)
- **Journal**: None
- **Summary**: Hierarchical feature learning based on convolutional neural networks (CNN) has recently shown significant potential in various computer vision tasks. While allowing high-quality discriminative feature learning, the downside of CNNs is the lack of explicit structure in features, which often leads to overfitting, absence of reconstruction from partial observations and limited generative abilities. Explicit structure is inherent in hierarchical compositional models, however, these lack the ability to optimize a well-defined cost function. We propose a novel analytic model of a basic unit in a layered hierarchical model with both explicit compositional structure and a well-defined discriminative cost function. Our experiments on two datasets show that the proposed compositional model performs on a par with standard CNNs on discriminative tasks, while, due to explicit modeling of the structure in the feature units, affording a straight-forward visualization of parts and faster inference due to separability of the units. Actions



### A Unified Gender-Aware Age Estimation
- **Arxiv ID**: http://arxiv.org/abs/1609.03815v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.03815v1)
- **Published**: 2016-09-13 13:10:50+00:00
- **Updated**: 2016-09-13 13:10:50+00:00
- **Authors**: Qing Tian, Songcan Chen, Xiaoyang Tan
- **Comment**: None
- **Journal**: None
- **Summary**: Human age estimation has attracted increasing researches due to its wide applicability in such as security monitoring and advertisement recommendation. Although a variety of methods have been proposed, most of them focus only on the age-specific facial appearance. However, biological researches have shown that not only gender but also the aging difference between the male and the female inevitably affect the age estimation. To our knowledge, so far there have been two methods that have concerned the gender factor. The first is a sequential method which first classifies the gender and then performs age estimation respectively for classified male and female. Although it promotes age estimation performance because of its consideration on the gender semantic difference, an accumulation risk of estimation errors is unavoidable. To overcome drawbacks of the sequential strategy, the second is to regress the age appended with the gender by concatenating their labels as two dimensional output using Partial Least Squares (PLS). Although leading to promotion of age estimation performance, such a concatenation not only likely confuses the semantics between the gender and age, but also ignores the aging discrepancy between the male and the female. In order to overcome their shortcomings, in this paper we propose a unified framework to perform gender-aware age estimation. The proposed method considers and utilizes not only the semantic relationship between the gender and the age, but also the aging discrepancy between the male and the female. Finally, experimental results demonstrate not only the superiority of our method in performance, but also its good interpretability in revealing the aging discrepancy.



### Probabilistic Saliency Estimation
- **Arxiv ID**: http://arxiv.org/abs/1609.03868v2
- **DOI**: 10.1016/j.patcog.2017.09.023
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.03868v2)
- **Published**: 2016-09-13 14:42:46+00:00
- **Updated**: 2017-02-20 08:08:27+00:00
- **Authors**: Caglar Aytekin, Alexandros Iosifidis, Moncef Gabbouj
- **Comment**: Submitted to Pattern Recognition
- **Journal**: Pattern Recognition Volume 74, February 2018, Pages 359-372
- **Summary**: In this paper, we model the salient object detection problem under a probabilistic framework encoding the boundary connectivity saliency cue and smoothness constraints in an optimization problem. We show that this problem has a closed form global optimum which estimates the salient object. We further show that along with the probabilistic framework, the proposed method also enjoys a wide range of interpretations, i.e. graph cut, diffusion maps and one-class classification. With an analysis according to these interpretations, we also find that our proposed method provides approximations to the global optimum to another criterion that integrates local/global contrast and large area saliency cues. The proposed approach achieves mostly leading performance compared to the state-of-the-art algorithms over a large set of salient object detection datasets including around 17k images for several evaluation metrics. Furthermore, the computational complexity of the proposed method is favorable/comparable to many state-of-the-art techniques.



### Image Decomposition Using a Robust Regression Approach
- **Arxiv ID**: http://arxiv.org/abs/1609.03874v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.03874v2)
- **Published**: 2016-09-13 14:48:41+00:00
- **Updated**: 2017-12-04 14:19:44+00:00
- **Authors**: Shervin Minaee, Yao Wang
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1607.02547
- **Journal**: None
- **Summary**: This paper considers how to separate text and/or graphics from smooth background in screen content and mixed content images and proposes an algorithm to perform this segmentation task. The proposed methods make use of the fact that the background in each block is usually smoothly varying and can be modeled well by a linear combination of a few smoothly varying basis functions, while the foreground text and graphics create sharp discontinuity. This algorithm separates the background and foreground pixels by trying to fit pixel values in the block into a smooth function using a robust regression method. The inlier pixels that can be well represented with the smooth model will be considered as background, while remaining outlier pixels will be considered foreground. We have also created a dataset of screen content images extracted from HEVC standard test sequences for screen content coding with their ground truth segmentation result which can be used for this task. The proposed algorithm has been tested on the dataset mentioned above and is shown to have superior performance over other methods, such as the hierarchical k-means clustering algorithm, shape primitive extraction and coding, and the least absolute deviation fitting scheme for foreground segmentation.



### VIPLFaceNet: An Open Source Deep Face Recognition SDK
- **Arxiv ID**: http://arxiv.org/abs/1609.03892v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.03892v1)
- **Published**: 2016-09-13 15:13:55+00:00
- **Updated**: 2016-09-13 15:13:55+00:00
- **Authors**: Xin Liu, Meina Kan, Wanglong Wu, Shiguang Shan, Xilin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Robust face representation is imperative to highly accurate face recognition. In this work, we propose an open source face recognition method with deep representation named as VIPLFaceNet, which is a 10-layer deep convolutional neural network with 7 convolutional layers and 3 fully-connected layers. Compared with the well-known AlexNet, our VIPLFaceNet takes only 20% training time and 60% testing time, but achieves 40\% drop in error rate on the real-world face recognition benchmark LFW. Our VIPLFaceNet achieves 98.60% mean accuracy on LFW using one single network. An open-source C++ SDK based on VIPLFaceNet is released under BSD license. The SDK takes about 150ms to process one face image in a single thread on an i7 desktop CPU. VIPLFaceNet provides a state-of-the-art start point for both academic and industrial face recognition applications.



### Crafting a multi-task CNN for viewpoint estimation
- **Arxiv ID**: http://arxiv.org/abs/1609.03894v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1609.03894v1)
- **Published**: 2016-09-13 15:19:38+00:00
- **Updated**: 2016-09-13 15:19:38+00:00
- **Authors**: Francisco Massa, Renaud Marlet, Mathieu Aubry
- **Comment**: To appear in BMVC 2016
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) were recently shown to provide state-of-the-art results for object category viewpoint estimation. However different ways of formulating this problem have been proposed and the competing approaches have been explored with very different design choices. This paper presents a comparison of these approaches in a unified setting as well as a detailed analysis of the key factors that impact performance. Followingly, we present a new joint training method with the detection task and demonstrate its benefit. We also highlight the superiority of classification approaches over regression approaches, quantify the benefits of deeper architectures and extended training data, and demonstrate that synthetic data is beneficial even when using ImageNet training data. By combining all these elements, we demonstrate an improvement of approximately 5% mAVP over previous state-of-the-art results on the Pascal3D+ dataset. In particular for their most challenging 24 view classification task we improve the results from 31.1% to 36.1% mAVP.



### Associating Grasp Configurations with Hierarchical Features in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1609.03947v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1609.03947v5)
- **Published**: 2016-09-13 17:37:38+00:00
- **Updated**: 2017-07-26 00:41:01+00:00
- **Authors**: Li Yang Ku, Erik Learned-Miller, Rod Grupen
- **Comment**: 8 pages, 9 figures, IROS 2017
- **Journal**: None
- **Summary**: In this work, we provide a solution for posturing the anthropomorphic Robonaut-2 hand and arm for grasping based on visual information. A mapping from visual features extracted from a convolutional neural network (CNN) to grasp points is learned. We demonstrate that a CNN pre-trained for image classification can be applied to a grasping task based on a small set of grasping examples. Our approach takes advantage of the hierarchical nature of the CNN by identifying features that capture the hierarchical support relations between filters in different CNN layers and locating their 3D positions by tracing activations backwards in the CNN. When this backward trace terminates in the RGB-D image, important manipulable structures are thereby localized. These features that reside in different layers of the CNN are then associated with controllers that engage different kinematic subchains in the hand/arm system for grasping. A grasping dataset is collected using demonstrated hand/object relationships for Robonaut-2 to evaluate the proposed approach in terms of the precision of the resulting preshape postures. We demonstrate that this approach outperforms baseline approaches in cluttered scenarios on the grasping dataset and a point cloud based approach on a grasping task using Robonaut-2.



### Method to Assess the Temporal Persistence of Potential Biometric Features: Application to Oculomotor, and Gait-Related Databases
- **Arxiv ID**: http://arxiv.org/abs/1609.03948v1
- **DOI**: 10.1371/journal.pone.0178501
- **Categories**: **q-bio.QM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1609.03948v1)
- **Published**: 2016-09-13 17:38:39+00:00
- **Updated**: 2016-09-13 17:38:39+00:00
- **Authors**: Lee Friedman, Ioannis Rigas, Mark S. Nixon, Oleg V. Komogortsev
- **Comment**: 13 pages, 8 figures, 5 tables
- **Journal**: None
- **Summary**: Although temporal persistence, or permanence, is a well understood requirement for optimal biometric features, there is no general agreement on how to assess temporal persistence. We suggest that the best way to assess temporal persistence is to perform a test-retest study, and assess test-retest reliability. For ratio-scale features that are normally distributed, this is best done using the Intraclass Correlation Coefficient (ICC). For 10 distinct data sets (8 eye-movement related, and 2 gait related), we calculated the test-retest reliability ('Temporal persistence') of each feature, and compared biometric performance of high-ICC features to lower ICC features, and to the set of all features. We demonstrate that using a subset of only high-ICC features produced superior Rank-1-Identification Rate (Rank-1-IR) performance in 9 of 10 databases (p = 0.01, one-tailed). For Equal Error Rate (EER), using a subset of only high-ICC features produced superior performance in 8 of 10 databases (p = 0.055, one-tailed). In general, then, prescreening potential biometric features, and choosing only highly reliable features will yield better performance than lower ICC features or than the set of all features combined. We hypothesize that this would likely be the case for any biometric modality where the features can be expressed as quantitative values on an interval or ratio scale, assuming an adequate number of relatively independent features.



### The CUDA LATCH Binary Descriptor: Because Sometimes Faster Means Better
- **Arxiv ID**: http://arxiv.org/abs/1609.03986v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.03986v2)
- **Published**: 2016-09-13 19:24:02+00:00
- **Updated**: 2016-09-14 00:51:19+00:00
- **Authors**: Christopher Parker, Matthew Daiter, Kareem Omar, Gil Levi, Tal Hassner
- **Comment**: Accepted to ECCV'16 workshops
- **Journal**: None
- **Summary**: Accuracy, descriptor size, and the time required for extraction and matching are all important factors when selecting local image descriptors. To optimize over all these requirements, this paper presents a CUDA port for the recent Learned Arrangement of Three Patches (LATCH) binary descriptors to the GPU platform. The design of LATCH makes it well suited for GPU processing. Owing to its small size and binary nature, the GPU can further be used to efficiently match LATCH features. Taken together, this leads to breakneck descriptor extraction and matching speeds. We evaluate the trade off between these speeds and the quality of results in a feature matching intensive application. To this end, we use our proposed CUDA LATCH (CLATCH) to recover structure from motion (SfM), comparing 3D reconstructions and speed using different representations. Our results show that CLATCH provides high quality 3D reconstructions at fractions of the time required by other representations, with little, if any, loss of reconstruction quality.



