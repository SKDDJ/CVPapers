# Arxiv Papers in cs.CV on 2016-09-22
### Deep Learning for Video Classification and Captioning
- **Arxiv ID**: http://arxiv.org/abs/1609.06782v2
- **DOI**: 10.1145/3122865.3122867
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1609.06782v2)
- **Published**: 2016-09-22 00:08:59+00:00
- **Updated**: 2018-02-22 14:59:32+00:00
- **Authors**: Zuxuan Wu, Ting Yao, Yanwei Fu, Yu-Gang Jiang
- **Comment**: Book chapter in Frontiers of Multimedia Research
- **Journal**: None
- **Summary**: Accelerated by the tremendous increase in Internet bandwidth and storage space, video data has been generated, published and spread explosively, becoming an indispensable part of today's big data. In this paper, we focus on reviewing two lines of research aiming to stimulate the comprehension of videos with deep learning: video classification and video captioning. While video classification concentrates on automatically labeling video clips based on their semantic contents like human actions or complex events, video captioning attempts to generate a complete and natural sentence, enriching the single label as in video classification, to capture the most informative dynamics in videos. In addition, we also provide a review of popular benchmarks and competitions, which are critical for evaluating the technical progress of this vibrant field.



### Deep-Learned Collision Avoidance Policy for Distributed Multi-Agent Navigation
- **Arxiv ID**: http://arxiv.org/abs/1609.06838v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1609.06838v2)
- **Published**: 2016-09-22 07:05:56+00:00
- **Updated**: 2017-07-06 07:41:45+00:00
- **Authors**: Pinxin Long, Wenxi Liu, Jia Pan
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters 2(2): 656-663 (2017)
- **Summary**: High-speed, low-latency obstacle avoidance that is insensitive to sensor noise is essential for enabling multiple decentralized robots to function reliably in cluttered and dynamic environments. While other distributed multi-agent collision avoidance systems exist, these systems require online geometric optimization where tedious parameter tuning and perfect sensing are necessary.   We present a novel end-to-end framework to generate reactive collision avoidance policy for efficient distributed multi-agent navigation. Our method formulates an agent's navigation strategy as a deep neural network mapping from the observed noisy sensor measurements to the agent's steering commands in terms of movement velocity. We train the network on a large number of frames of collision avoidance data collected by repeatedly running a multi-agent simulator with different parameter settings. We validate the learned deep neural network policy in a set of simulated and real scenarios with noisy measurements and demonstrate that our method is able to generate a robust navigation strategy that is insensitive to imperfect sensing and works reliably in all situations. We also show that our method can be well generalized to scenarios that do not appear in our training data, including scenes with static obstacles and agents with different sizes. Videos are available at https://sites.google.com/view/deepmaca.



### On the usability of deep networks for object-based image analysis
- **Arxiv ID**: http://arxiv.org/abs/1609.06845v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1609.06845v1)
- **Published**: 2016-09-22 07:39:37+00:00
- **Updated**: 2016-09-22 07:39:37+00:00
- **Authors**: Nicolas Audebert, Bertrand Le Saux, Sébastien Lefèvre
- **Comment**: in International Conference on Geographic Object-Based Image Analysis
  (GEOBIA), Sep 2016, Enschede, Netherlands
- **Journal**: None
- **Summary**: As computer vision before, remote sensing has been radically changed by the introduction of Convolution Neural Networks. Land cover use, object detection and scene understanding in aerial images rely more and more on deep learning to achieve new state-of-the-art results. Recent architectures such as Fully Convolutional Networks (Long et al., 2015) can even produce pixel level annotations for semantic mapping. In this work, we show how to use such deep networks to detect, segment and classify different varieties of wheeled vehicles in aerial images from the ISPRS Potsdam dataset. This allows us to tackle object detection and classification on a complex dataset made up of visually similar classes, and to demonstrate the relevance of such a subclass modeling approach. Especially, we want to show that deep learning is also suitable for object-oriented analysis of Earth Observation data. First, we train a FCN variant on the ISPRS Potsdam dataset and show how the learnt semantic maps can be used to extract precise segmentation of vehicles, which allow us studying the repartition of vehicles in the city. Second, we train a CNN to perform vehicle classification on the VEDAI (Razakarivony and Jurie, 2016) dataset, and transfer its knowledge to classify candidate segmented vehicles on the Potsdam dataset.



### Semantic Segmentation of Earth Observation Data Using Multimodal and Multi-scale Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1609.06846v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1609.06846v1)
- **Published**: 2016-09-22 07:42:06+00:00
- **Updated**: 2016-09-22 07:42:06+00:00
- **Authors**: Nicolas Audebert, Bertrand Le Saux, Sébastien Lefèvre
- **Comment**: Asian Conference on Computer Vision (ACCV16), Nov 2016, Taipei,
  Taiwan
- **Journal**: None
- **Summary**: This work investigates the use of deep fully convolutional neural networks (DFCNN) for pixel-wise scene labeling of Earth Observation images. Especially, we train a variant of the SegNet architecture on remote sensing data over an urban area and study different strategies for performing accurate semantic segmentation. Our contributions are the following: 1) we transfer efficiently a DFCNN from generic everyday images to remote sensing images; 2) we introduce a multi-kernel convolutional layer for fast aggregation of predictions at multiple scales; 3) we perform data fusion from heterogeneous sensors (optical and laser) using residual correction. Our framework improves state-of-the-art accuracy on the ISPRS Vaihingen 2D Semantic Labeling dataset.



### How Useful is Region-based Classification of Remote Sensing Images in a Deep Learning Framework?
- **Arxiv ID**: http://arxiv.org/abs/1609.06861v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.06861v1)
- **Published**: 2016-09-22 08:21:20+00:00
- **Updated**: 2016-09-22 08:21:20+00:00
- **Authors**: Nicolas Audebert, Bertrand Le Saux, Sébastien Lefèvre
- **Comment**: IEEE International Geosciences and Remote Sensing Symposium (IGARSS),
  Jul 2016, Beijing, China
- **Journal**: None
- **Summary**: In this paper, we investigate the impact of segmentation algorithms as a preprocessing step for classification of remote sensing images in a deep learning framework. Especially, we address the issue of segmenting the image into regions to be classified using pre-trained deep neural networks as feature extractors for an SVM-based classifier. An efficient segmentation as a preprocessing step helps learning by adding a spatially-coherent structure to the data. Therefore, we compare algorithms producing superpixels with more traditional remote sensing segmentation algorithms and measure the variation in terms of classification accuracy. We establish that superpixel algorithms allow for a better classification accuracy as a homogenous and compact segmentation favors better generalization of the training samples.



### Distributed Training of Deep Neural Networks: Theoretical and Practical Limits of Parallel Scalability
- **Arxiv ID**: http://arxiv.org/abs/1609.06870v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.06870v4)
- **Published**: 2016-09-22 08:47:58+00:00
- **Updated**: 2016-12-05 08:19:11+00:00
- **Authors**: Janis Keuper, Franz-Josef Pfreundt
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a theoretical analysis and practical evaluation of the main bottlenecks towards a scalable distributed solution for the training of Deep Neuronal Networks (DNNs). The presented results show, that the current state of the art approach, using data-parallelized Stochastic Gradient Descent (SGD), is quickly turning into a vastly communication bound problem. In addition, we present simple but fixed theoretic constraints, preventing effective scaling of DNN training beyond only a few dozen nodes. This leads to poor scalability of DNN training in most practical scenarios.



### Realtime Hierarchical Clustering based on Boundary and Surface Statistics
- **Arxiv ID**: http://arxiv.org/abs/1609.06896v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.06896v1)
- **Published**: 2016-09-22 10:17:30+00:00
- **Updated**: 2016-09-22 10:17:30+00:00
- **Authors**: Dominik Alexander Klein, Dirk Schulz, Armin Bernd Cremers
- **Comment**: Asian Conf. on Computer Vision (ACCV) 2016
- **Journal**: None
- **Summary**: Visual grouping is a key mechanism in human scene perception. There, it belongs to the subconscious, early processing and is key prerequisite for other high level tasks such as recognition. In this paper, we introduce an efficient, realtime capable algorithm which likewise agglomerates a valuable hierarchical clustering of a scene, while using purely local appearance statistics. To speed up the processing, first we subdivide the image into meaningful, atomic segments using a fast Watershed transform. Starting from there, our rapid, agglomerative clustering algorithm prunes and maintains the connectivity graph between clusters to contain only such pairs, which directly touch in the image domain and are reciprocal nearest neighbors (RNN) wrt. a distance metric. The core of this approach is our novel cluster distance: it combines boundary and surface statistics both in terms of appearance as well as spatial linkage. This yields state-of-the-art performance, as we demonstrate in conclusive experiments conducted on BSDS500 and Pascal-Context datasets.



### A quantitative analysis of tilt in the Café Wall illusion: a bioplausible model for foveal and peripheral vision
- **Arxiv ID**: http://arxiv.org/abs/1609.06927v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.06927v1)
- **Published**: 2016-09-22 11:55:14+00:00
- **Updated**: 2016-09-22 11:55:14+00:00
- **Authors**: Nasim Nematzadeh, David M. W. Powers
- **Comment**: 8 pages, 9 figures, DICTA2016
- **Journal**: None
- **Summary**: The biological characteristics of human visual processing can be investigated through the study of optical illusions and their perception, giving rise to intuitions that may improve computer vision to match human performance. Geometric illusions are a specific subfamily in which orientations and angles are misperceived. This paper reports quantifiable predictions of the degree of tilt for a typical geometric illusion called Caf\'e Wall, in which the mortar between the tiles seems to tilt or bow. Our study employs a common bioplausible model of retinal processing and we further develop an analytic processing pipeline to quantify and thus predict the specific angle of tilt. We further study the effect of resolution and feature size in order to predict the different perceived tilts in different areas of the fovea and periphery, where resolution varies as the eye saccades to different parts of the image. In the experiments, several different minimal portions of the pattern, modeling monocular and binocular foveal views, are investigated across multiple scales, in order to quantify tilts with confidence intervals and explore the difference between local and global tilt.



### Walker-Independent Features for Gait Recognition from Motion Capture Data
- **Arxiv ID**: http://arxiv.org/abs/1609.06936v5
- **DOI**: 10.1007/978-3-319-49055-7_28
- **Categories**: **cs.CV**, 68T05, 68T10, I.5
- **Links**: [PDF](http://arxiv.org/pdf/1609.06936v5)
- **Published**: 2016-09-22 12:17:34+00:00
- **Updated**: 2022-12-07 22:15:27+00:00
- **Authors**: Michal Balazia, Petr Sojka
- **Comment**: Preprint. Full paper published at the Joint IAPR International
  Workshops on Structural and Syntactic Pattern Recognition and Statistical
  Techniques in Pattern Recognition (S+SSPR), Merida, Mexico, November 2016. 11
  pages. arXiv admin note: substantial text overlap with arXiv:1609.04392
- **Journal**: None
- **Summary**: MoCap-based human identification, as a pattern recognition discipline, can be optimized using a machine learning approach. Yet in some applications such as video surveillance new identities can appear on the fly and labeled data for all encountered people may not always be available. This work introduces the concept of learning walker-independent gait features directly from raw joint coordinates by a modification of the Fisher Linear Discriminant Analysis with Maximum Margin Criterion. Our new approach shows not only that these features can discriminate different people than who they are learned on, but also that the number of learning identities can be much smaller than the number of walkers encountered in the real operation.



### Symmetric Non-Rigid Structure from Motion for Category-Specific Object Structure Estimation
- **Arxiv ID**: http://arxiv.org/abs/1609.06988v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/1609.06988v1)
- **Published**: 2016-09-22 13:57:10+00:00
- **Updated**: 2016-09-22 13:57:10+00:00
- **Authors**: Yuan Gao, Alan Yuille
- **Comment**: Accepted to ECCV 2016
- **Journal**: None
- **Summary**: Many objects, especially these made by humans, are symmetric, e.g. cars and aeroplanes. This paper addresses the estimation of 3D structures of symmetric objects from multiple images of the same object category, e.g. different cars, seen from various viewpoints. We assume that the deformation between different instances from the same object category is non-rigid and symmetric. In this paper, we extend two leading non-rigid structure from motion (SfM) algorithms to exploit symmetry constraints. We model the both methods as energy minimization, in which we also recover the missing observations caused by occlusions. In particularly, we show that by rotating the coordinate system, the energy can be decoupled into two independent terms, which still exploit symmetry, to apply matrix factorization separately on each of them for initialization. The results on the Pascal3D+ dataset show that our methods significantly improve performance over baseline methods.



### Is the deconvolution layer the same as a convolutional layer?
- **Arxiv ID**: http://arxiv.org/abs/1609.07009v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.07009v1)
- **Published**: 2016-09-22 15:11:11+00:00
- **Updated**: 2016-09-22 15:11:11+00:00
- **Authors**: Wenzhe Shi, Jose Caballero, Lucas Theis, Ferenc Huszar, Andrew Aitken, Christian Ledig, Zehan Wang
- **Comment**: This is a note to share some additional insights for our the CVPR
  paper
- **Journal**: None
- **Summary**: In this note, we want to focus on aspects related to two questions most people asked us at CVPR about the network we presented. Firstly, What is the relationship between our proposed layer and the deconvolution layer? And secondly, why are convolutions in low-resolution (LR) space a better choice? These are key questions we tried to answer in the paper, but we were not able to go into as much depth and clarity as we would have liked in the space allowance. To better answer these questions in this note, we first discuss the relationships between the deconvolution layer in the forms of the transposed convolution layer, the sub-pixel convolutional layer and our efficient sub-pixel convolutional layer. We will refer to our efficient sub-pixel convolutional layer as a convolutional layer in LR space to distinguish it from the common sub-pixel convolutional layer. We will then show that for a fixed computational budget and complexity, a network with convolutions exclusively in LR space has more representation power at the same speed than a network that first upsamples the input in high resolution space.



### Image-embodied Knowledge Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1609.07028v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1609.07028v2)
- **Published**: 2016-09-22 15:37:45+00:00
- **Updated**: 2017-05-22 08:14:27+00:00
- **Authors**: Ruobing Xie, Zhiyuan Liu, Huanbo Luan, Maosong Sun
- **Comment**: 7 pages; Accepted by IJCAI-2017
- **Journal**: IJCAI-2017
- **Summary**: Entity images could provide significant visual information for knowledge representation learning. Most conventional methods learn knowledge representations merely from structured triples, ignoring rich visual information extracted from entity images. In this paper, we propose a novel Image-embodied Knowledge Representation Learning model (IKRL), where knowledge representations are learned with both triple facts and images. More specifically, we first construct representations for all images of an entity with a neural image encoder. These image representations are then integrated into an aggregated image-based representation via an attention-based method. We evaluate our IKRL models on knowledge graph completion and triple classification. Experimental results demonstrate that our models outperform all baselines on both tasks, which indicates the significance of visual information for knowledge representations and the capability of our models in learning knowledge representations with images.



### Pose-Selective Max Pooling for Measuring Similarity
- **Arxiv ID**: http://arxiv.org/abs/1609.07042v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1609.07042v4)
- **Published**: 2016-09-22 15:59:38+00:00
- **Updated**: 2016-11-14 04:10:09+00:00
- **Authors**: Xiang Xiang, Trac D. Tran
- **Comment**: The tutorial and program associated with this paper are available at
  https://github.com/eglxiang/ytf yet for non-commercial use
- **Journal**: None
- **Summary**: In this paper, we deal with two challenges for measuring the similarity of the subject identities in practical video-based face recognition - the variation of the head pose in uncontrolled environments and the computational expense of processing videos. Since the frame-wise feature mean is unable to characterize the pose diversity among frames, we define and preserve the overall pose diversity and closeness in a video. Then, identity will be the only source of variation across videos since the pose varies even within a single video. Instead of simply using all the frames, we select those faces whose pose point is closest to the centroid of the K-means cluster containing that pose point. Then, we represent a video as a bag of frame-wise deep face features while the number of features has been reduced from hundreds to K. Since the video representation can well represent the identity, now we measure the subject similarity between two videos as the max correlation among all possible pairs in the two bags of features. On the official 5,000 video-pairs of the YouTube Face dataset for face verification, our algorithm achieves a comparable performance with VGG-face that averages over deep features of all frames. Other vision tasks can also benefit from the generic idea of employing geometric cues to improve the descriptiveness of deep features.



### Customized Facial Constant Positive Air Pressure (CPAP) Masks
- **Arxiv ID**: http://arxiv.org/abs/1609.07049v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1609.07049v1)
- **Published**: 2016-09-22 16:11:57+00:00
- **Updated**: 2016-09-22 16:11:57+00:00
- **Authors**: Matan Sela, Nadav Toledo, Yaron Honen, Ron Kimmel
- **Comment**: None
- **Journal**: None
- **Summary**: Sleep apnea is a syndrome that is characterized by sudden breathing halts while sleeping. One of the common treatments involves wearing a mask that delivers continuous air flow into the nostrils so as to maintain a steady air pressure. These masks are designed for an average facial model and are often difficult to adjust due to poor fit to the actual patient. The incompatibility is characterized by gaps between the mask and the face, which deteriorates the impermeability of the mask and leads to air leakage. We suggest a fully automatic approach for designing a personalized nasal mask interface using a facial depth scan. The interfaces generated by the proposed method accurately fit the geometry of the scanned face, and are easy to manufacture. The proposed method utilizes cheap commodity depth sensors and 3D printing technologies to efficiently design and manufacture customized masks for patients suffering from sleep apnea.



### Large Margin Nearest Neighbor Classification using Curved Mahalanobis Distances
- **Arxiv ID**: http://arxiv.org/abs/1609.07082v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CG, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1609.07082v2)
- **Published**: 2016-09-22 17:41:03+00:00
- **Updated**: 2016-09-26 18:34:42+00:00
- **Authors**: Frank Nielsen, Boris Muzellec, Richard Nock
- **Comment**: 21 pages, 8 figures, 5 tables, extend ICIP 2016 paper entitled
  "classification With Mixtures of Curved Mahalanobis Metrics"
- **Journal**: None
- **Summary**: We consider the supervised classification problem of machine learning in Cayley-Klein projective geometries: We show how to learn a curved Mahalanobis metric distance corresponding to either the hyperbolic geometry or the elliptic geometry using the Large Margin Nearest Neighbor (LMNN) framework. We report on our experimental results, and further consider the case of learning a mixed curved Mahalanobis distance. Besides, we show that the Cayley-Klein Voronoi diagrams are affine, and can be built from an equivalent (clipped) power diagrams, and that Cayley-Klein balls have Mahalanobis shapes with displaced centers.



### Neural Photo Editing with Introspective Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1609.07093v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1609.07093v3)
- **Published**: 2016-09-22 18:07:56+00:00
- **Updated**: 2017-02-06 18:46:50+00:00
- **Authors**: Andrew Brock, Theodore Lim, J. M. Ritchie, Nick Weston
- **Comment**: 10 pages, 7 figures, 3 tables
- **Journal**: None
- **Summary**: The increasingly photorealistic sample quality of generative image models suggests their feasibility in applications beyond image generation. We present the Neural Photo Editor, an interface that leverages the power of generative neural networks to make large, semantically coherent changes to existing images. To tackle the challenge of achieving accurate reconstructions without loss of feature quality, we introduce the Introspective Adversarial Network, a novel hybridization of the VAE and GAN. Our model efficiently captures long-range dependencies through use of a computational block based on weight-shared dilated convolutions, and improves generalization performance with Orthogonal Regularization, a novel weight regularization method. We validate our contributions on CelebA, SVHN, and CIFAR-100, and produce samples and reconstructions with high visual fidelity.



### Deep Learning in Multi-Layer Architectures of Dense Nuclei
- **Arxiv ID**: http://arxiv.org/abs/1609.07160v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1609.07160v2)
- **Published**: 2016-09-22 20:55:16+00:00
- **Updated**: 2016-09-29 11:19:23+00:00
- **Authors**: Yonghua Yin, Erol Gelenbe
- **Comment**: 10 pages (a small edit to the abstract)
- **Journal**: None
- **Summary**: We assume that, within the dense clusters of neurons that can be found in nuclei, cells may interconnect via soma-to-soma interactions, in addition to conventional synaptic connections. We illustrate this idea with a multi-layer architecture (MLA) composed of multiple clusters of recurrent sub-networks of spiking Random Neural Networks (RNN) with dense soma-to-soma interactions, and use this RNN-MLA architecture for deep learning. The inputs to the clusters are first normalised by adjusting the external arrival rates of spikes to each cluster. Then we apply this architecture to learning from multi-channel datasets. Numerical results based on both images and sensor based data, show the value of this novel architecture for deep learning.



### Deep Quality: A Deep No-reference Quality Assessment System
- **Arxiv ID**: http://arxiv.org/abs/1609.07170v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1609.07170v1)
- **Published**: 2016-09-22 21:26:21+00:00
- **Updated**: 2016-09-22 21:26:21+00:00
- **Authors**: Prajna Paramita Dash, Akshaya Mishra, Alexander Wong
- **Comment**: 2 pages
- **Journal**: None
- **Summary**: Image quality assessment (IQA) continues to garner great interest in the research community, particularly given the tremendous rise in consumer video capture and streaming. Despite significant research effort in IQA in the past few decades, the area of no-reference image quality assessment remains a great challenge and is largely unsolved. In this paper, we propose a novel no-reference image quality assessment system called Deep Quality, which leverages the power of deep learning to model the complex relationship between visual content and the perceived quality. Deep Quality consists of a novel multi-scale deep convolutional neural network, trained to learn to assess image quality based on training samples consisting of different distortions and degradations such as blur, Gaussian noise, and compression artifacts. Preliminary results using the CSIQ benchmark image quality dataset showed that Deep Quality was able to achieve strong quality prediction performance (89% patch-level and 98% image-level prediction accuracy), being able to achieve similar performance as full-reference IQA methods.



