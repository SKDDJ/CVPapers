# Arxiv Papers in cs.CV on 2016-09-08
### Determination of Pedestrian Flow Performance Based on Video Tracking and Microscopic Simulations
- **Arxiv ID**: http://arxiv.org/abs/1609.02243v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/1609.02243v1)
- **Published**: 2016-09-08 01:58:10+00:00
- **Updated**: 2016-09-08 01:58:10+00:00
- **Authors**: Kardi Teknomo, Yasushi Takeyama, Hajime Inamura
- **Comment**: 4 pages, Teknomo, Kardi; Takeyama, Yasushi; Inamura, Hajime,
  Determination of Pedestrian Flow Performance Based on Video Tracking and
  Microscopic Simulations, Proceedings of Infrastructure Planning Conference
  Vol. 23 no 1, Ashikaga, Japan, pp. 639-642, Nov 2000
- **Journal**: None
- **Summary**: One of the objectives of understanding pedestrian behavior is to predict the effect of proposed changes in the design or evaluation of pedestrian facilities. We want to know the impact to the user of the facilities, as the design of the facilities change. That impact was traditionally evaluated by level of service standards. Another design criterion to measure the impact of design change is measured by the pedestrian flow performance index. This paper describes the determination of pedestrian flow performance based video tracking or any microscopic pedestrian simulation models. Most of pedestrian researches have been done on a macroscopic level, which is an aggregation of all pedestrian movement in pedestrian areas into flow, average speed and area module. Macroscopic level, however, does not consider the interaction between pedestrians. It is also not well suited for prediction of pedestrian flow performance in pedestrian areas or in buildings with some obstruction, that reduces the effective width of the walkways. On the other hand, the microscopic level has a more general usage and considers detail in the design. More efficient pedestrian flow can even be reached with less space. Those results have rejected the linearity assumption of space and flow in the macroscopic level.



### Ashwin: Plug-and-Play System for Machine-Human Image Annotation
- **Arxiv ID**: http://arxiv.org/abs/1609.02271v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1609.02271v2)
- **Published**: 2016-09-08 04:49:31+00:00
- **Updated**: 2016-09-09 18:37:03+00:00
- **Authors**: Anand Sriraman, Mandar Kulkarni, Rahul Kumar, Kanika Kalra, Purushotam Radadia, Shirish Karande
- **Comment**: HCOMP 2016 Demonstrations Track
- **Journal**: None
- **Summary**: We present an end-to-end machine-human image annotation system where each component can be attached in a plug-and-play fashion. These components include Feature Extraction, Machine Classifier, Task Sampling and Crowd Consensus.



### Learning Action Concept Trees and Semantic Alignment Networks from Image-Description Data
- **Arxiv ID**: http://arxiv.org/abs/1609.02284v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.02284v1)
- **Published**: 2016-09-08 05:53:31+00:00
- **Updated**: 2016-09-08 05:53:31+00:00
- **Authors**: Jiyang Gao, Ram Nevatia
- **Comment**: 16 pages, 5 figures
- **Journal**: None
- **Summary**: Action classification in still images has been a popular research topic in computer vision. Labelling large scale datasets for action classification requires tremendous manual work, which is hard to scale up. Besides, the action categories in such datasets are pre-defined and vocabularies are fixed. However humans may describe the same action with different phrases, which leads to the difficulty of vocabulary expansion for traditional fully-supervised methods. We observe that large amounts of images with sentence descriptions are readily available on the Internet. The sentence descriptions can be regarded as weak labels for the images, which contain rich information and could be used to learn flexible expressions of action categories. We propose a method to learn an Action Concept Tree (ACT) and an Action Semantic Alignment (ASA) model for classification from image-description data via a two-stage learning process. A new dataset for the task of learning actions from descriptions is built. Experimental results show that our method outperforms several baseline methods significantly.



### Adaptive Regularization in Convex Composite Optimization for Variational Imaging Problems
- **Arxiv ID**: http://arxiv.org/abs/1609.02356v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.02356v2)
- **Published**: 2016-09-08 09:45:14+00:00
- **Updated**: 2017-02-28 12:58:18+00:00
- **Authors**: Byung-Woo Hong, Ja-Keoung Koo, Hendrik Dirks, Martin Burger
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an adaptive regularization scheme in a variational framework where a convex composite energy functional is optimized. We consider a number of imaging problems including denoising, segmentation and motion estimation, which are considered as optimal solutions of the energy functionals that mainly consist of data fidelity, regularization and a control parameter for their trade-off. We presents an algorithm to determine the relative weight between data fidelity and regularization based on the residual that measures how well the observation fits the model. Our adaptive regularization scheme is designed to locally control the regularization at each pixel based on the assumption that the diversity of the residual of a given imaging model spatially varies. The energy optimization is presented in the alternating direction method of multipliers (ADMM) framework where the adaptive regularization is iteratively applied along with mathematical analysis of the proposed algorithm. We demonstrate the robustness and effectiveness of our adaptive regularization through experimental results presenting that the qualitative and quantitative evaluation results of each imaging task are superior to the results with a constant regularization scheme. The desired properties, robustness and effectiveness, of the regularization parameter selection in a variational framework for imaging problems are achieved by merely replacing the static regularization parameter with our adaptive one.



### Ear-to-ear Capture of Facial Intrinsics
- **Arxiv ID**: http://arxiv.org/abs/1609.02368v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.02368v1)
- **Published**: 2016-09-08 10:24:44+00:00
- **Updated**: 2016-09-08 10:24:44+00:00
- **Authors**: Alassane Seck, William A. P. Smith, Arnaud Dessein, Bernard Tiddeman, Hannah Dee, Abhishek Dutta
- **Comment**: None
- **Journal**: None
- **Summary**: We present a practical approach to capturing ear-to-ear face models comprising both 3D meshes and intrinsic textures (i.e. diffuse and specular albedo). Our approach is a hybrid of geometric and photometric methods and requires no geometric calibration. Photometric measurements made in a lightstage are used to estimate view dependent high resolution normal maps. We overcome the problem of having a single photometric viewpoint by capturing in multiple poses. We use uncalibrated multiview stereo to estimate a coarse base mesh to which the photometric views are registered. We propose a novel approach to robustly stitching surface normal and intrinsic texture data into a seamless, complete and highly detailed face model. The resulting relightable models provide photorealistic renderings in any view.



### Extraction of Skin Lesions from Non-Dermoscopic Images Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1609.02374v1
- **DOI**: 10.1007/s11548-017-1567-8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.02374v1)
- **Published**: 2016-09-08 11:05:27+00:00
- **Updated**: 2016-09-08 11:05:27+00:00
- **Authors**: Mohammad H. Jafari, Ebrahim Nasr-Esfahani, Nader Karimi, S. M. Reza Soroushmehr, Shadrokh Samavi, Kayvan Najarian
- **Comment**: None
- **Journal**: None
- **Summary**: Melanoma is amongst most aggressive types of cancer. However, it is highly curable if detected in its early stages. Prescreening of suspicious moles and lesions for malignancy is of great importance. Detection can be done by images captured by standard cameras, which are more preferable due to low cost and availability. One important step in computerized evaluation of skin lesions is accurate detection of lesion region, i.e. segmentation of an image into two regions as lesion and normal skin. Accurate segmentation can be challenging due to burdens such as illumination variation and low contrast between lesion and healthy skin. In this paper, a method based on deep neural networks is proposed for accurate extraction of a lesion region. The input image is preprocessed and then its patches are fed to a convolutional neural network (CNN). Local texture and global structure of the patches are processed in order to assign pixels to lesion or normal classes. A method for effective selection of training patches is used for more accurate detection of a lesion border. The output segmentation mask is refined by some post processing operations. The experimental results of qualitative and quantitative evaluations demonstrate that our method can outperform other state-of-the-art algorithms exist in the literature.



### End-to-End Eye Movement Detection Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1609.02452v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.02452v1)
- **Published**: 2016-09-08 14:58:15+00:00
- **Updated**: 2016-09-08 14:58:15+00:00
- **Authors**: Sabrina Hoppe, Andreas Bulling
- **Comment**: None
- **Journal**: None
- **Summary**: Common computational methods for automated eye movement detection - i.e. the task of detecting different types of eye movement in a continuous stream of gaze data - are limited in that they either involve thresholding on hand-crafted signal features, require individual detectors each only detecting a single movement, or require pre-segmented data. We propose a novel approach for eye movement detection that only involves learning a single detector end-to-end, i.e. directly from the continuous gaze data stream and simultaneously for different eye movements without any manual feature crafting or segmentation. Our method is based on convolutional neural networks (CNN) that recently demonstrated superior performance in a variety of tasks in computer vision, signal processing, and machine learning. We further introduce a novel multi-participant dataset that contains scripted and free-viewing sequences of ground-truth annotated saccades, fixations, and smooth pursuits. We show that our CNN-based method outperforms state-of-the-art baselines by a large margin on this challenging dataset, thereby underlining the significant potential of this approach for holistic, robust, and accurate eye movement protocol analysis.



### Quantifying Radiographic Knee Osteoarthritis Severity using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1609.02469v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.02469v1)
- **Published**: 2016-09-08 15:39:48+00:00
- **Updated**: 2016-09-08 15:39:48+00:00
- **Authors**: Joseph Antony, Kevin McGuinness, Noel E O Connor, Kieran Moran
- **Comment**: Included in ICPR 2016 proceedings
- **Journal**: None
- **Summary**: This paper proposes a new approach to automatically quantify the severity of knee osteoarthritis (OA) from radiographs using deep convolutional neural networks (CNN). Clinically, knee OA severity is assessed using Kellgren \& Lawrence (KL) grades, a five point scale. Previous work on automatically predicting KL grades from radiograph images were based on training shallow classifiers using a variety of hand engineered features. We demonstrate that classification accuracy can be significantly improved using deep convolutional neural network models pre-trained on ImageNet and fine-tuned on knee OA images. Furthermore, we argue that it is more appropriate to assess the accuracy of automatic knee OA severity predictions using a continuous distance-based evaluation metric like mean squared error than it is to use classification accuracy. This leads to the formulation of the prediction of KL grades as a regression problem and further improves accuracy. Results on a dataset of X-ray images and KL grades from the Osteoarthritis Initiative (OAI) show a sizable improvement over the current state-of-the-art.



### Reduced Memory Region Based Deep Convolutional Neural Network Detection
- **Arxiv ID**: http://arxiv.org/abs/1609.02500v1
- **DOI**: 10.1109/ICCE-Berlin.2016.7684706
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.02500v1)
- **Published**: 2016-09-08 17:16:38+00:00
- **Updated**: 2016-09-08 17:16:38+00:00
- **Authors**: Denis Tome', Luca Bondi, Emanuele Plebani, Luca Baroffio, Danilo Pau, Stefano Tubaro
- **Comment**: IEEE 2016 ICCE-Berlin
- **Journal**: 2016 IEEE 6th International Conference on Consumer Electronics -
  Berlin (ICCE-Berlin)
- **Summary**: Accurate pedestrian detection has a primary role in automotive safety: for example, by issuing warnings to the driver or acting actively on car's brakes, it helps decreasing the probability of injuries and human fatalities. In order to achieve very high accuracy, recent pedestrian detectors have been based on Convolutional Neural Networks (CNN). Unfortunately, such approaches require vast amounts of computational power and memory, preventing efficient implementations on embedded systems. This work proposes a CNN-based detector, adapting a general-purpose convolutional network to the task at hand. By thoroughly analyzing and optimizing each step of the detection pipeline, we develop an architecture that outperforms methods based on traditional image features and achieves an accuracy close to the state-of-the-art while having low computational complexity. Furthermore, the model is compressed in order to fit the tight constrains of low power devices with a limited amount of embedded memory available. This paper makes two main contributions: (1) it proves that a region based deep neural network can be finely tuned to achieve adequate accuracy for pedestrian detection (2) it achieves a very low memory usage without reducing detection accuracy on the Caltech Pedestrian dataset.



### Bottom-up Instance Segmentation using Deep Higher-Order CRFs
- **Arxiv ID**: http://arxiv.org/abs/1609.02583v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.02583v1)
- **Published**: 2016-09-08 20:37:39+00:00
- **Updated**: 2016-09-08 20:37:39+00:00
- **Authors**: Anurag Arnab, Philip H. S. Torr
- **Comment**: British Machine Vision Conference (BMVC) 2016
- **Journal**: None
- **Summary**: Traditional Scene Understanding problems such as Object Detection and Semantic Segmentation have made breakthroughs in recent years due to the adoption of deep learning. However, the former task is not able to localise objects at a pixel level, and the latter task has no notion of different instances of objects of the same class. We focus on the task of Instance Segmentation which recognises and localises objects down to a pixel level. Our model is based on a deep neural network trained for semantic segmentation. This network incorporates a Conditional Random Field with end-to-end trainable higher order potentials based on object detector outputs. This allows us to reason about instances from an initial, category-level semantic segmentation. Our simple method effectively leverages the great progress recently made in semantic segmentation and object detection. The accurate instance-level segmentations that our network produces is reflected by the considerable improvements obtained over previous work.



### Generating Videos with Scene Dynamics
- **Arxiv ID**: http://arxiv.org/abs/1609.02612v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1609.02612v3)
- **Published**: 2016-09-08 22:29:52+00:00
- **Updated**: 2016-10-26 13:58:10+00:00
- **Authors**: Carl Vondrick, Hamed Pirsiavash, Antonio Torralba
- **Comment**: NIPS 2016. See more at http://web.mit.edu/vondrick/tinyvideo/
- **Journal**: None
- **Summary**: We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation.



