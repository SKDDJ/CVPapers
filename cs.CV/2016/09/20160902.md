# Arxiv Papers in cs.CV on 2016-09-02
### Built-in Foreground/Background Prior for Weakly-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1609.00446v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.00446v1)
- **Published**: 2016-09-02 01:49:51+00:00
- **Updated**: 2016-09-02 01:49:51+00:00
- **Authors**: Fatemehsadat Saleh, Mohammad Sadegh Ali Akbarian, Mathieu Salzmann, Lars Petersson, Stephen Gould, Jose M. Alvarez
- **Comment**: Accepted in ECCV 2016
- **Journal**: None
- **Summary**: Pixel-level annotations are expensive and time consuming to obtain. Hence, weak supervision using only image tags could have a significant impact in semantic segmentation. Recently, CNN-based methods have proposed to fine-tune pre-trained networks using image tags. Without additional information, this leads to poor localization accuracy. This problem, however, was alleviated by making use of objectness priors to generate foreground/background masks. Unfortunately these priors either require training pixel-level annotations/bounding boxes, or still yield inaccurate object boundaries. Here, we propose a novel method to extract markedly more accurate masks from the pre-trained network itself, forgoing external objectness modules. This is accomplished using the activations of the higher-level convolutional layers, smoothed by a dense CRF. We demonstrate that our method, based on these masks and a weakly-supervised loss, outperforms the state-of-the-art tag-based weakly-supervised semantic segmentation techniques. Furthermore, we introduce a new form of inexpensive weak supervision yielding an additional accuracy boost.



### Label distribution based facial attractiveness computation by deep residual learning
- **Arxiv ID**: http://arxiv.org/abs/1609.00496v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.00496v2)
- **Published**: 2016-09-02 08:08:39+00:00
- **Updated**: 2016-09-07 09:06:31+00:00
- **Authors**: Shu Liu, Bo Li, Yangyu Fan, Zhe Guo, Ashok Samal
- **Comment**: 3 pages, 3 figures. The first two authors are parallel first author
- **Journal**: None
- **Summary**: Two challenges lie in the facial attractiveness computation research: the lack of true attractiveness labels (scores), and the lack of an accurate face representation. In order to address the first challenge, this paper recasts facial attractiveness computation as a label distribution learning (LDL) problem rather than a traditional single-label supervised learning task. In this way, the negative influence of the label incomplete problem can be reduced. Inspired by the recent promising work in face recognition using deep neural networks to learn effective features, the second challenge is expected to be solved from a deep learning point of view. A very deep residual network is utilized to enable automatic learning of hierarchical aesthetics representation. Integrating these two ideas, an end-to-end deep learning framework is established. Our approach achieves the best results on a standard benchmark SCUT-FBP dataset compared with other state-of-the-art work.



### SEBOOST - Boosting Stochastic Learning Using Subspace Optimization Techniques
- **Arxiv ID**: http://arxiv.org/abs/1609.00629v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1609.00629v1)
- **Published**: 2016-09-02 14:48:16+00:00
- **Updated**: 2016-09-02 14:48:16+00:00
- **Authors**: Elad Richardson, Rom Herskovitz, Boris Ginsburg, Michael Zibulevsky
- **Comment**: None
- **Journal**: None
- **Summary**: We present SEBOOST, a technique for boosting the performance of existing stochastic optimization methods. SEBOOST applies a secondary optimization process in the subspace spanned by the last steps and descent directions. The method was inspired by the SESOP optimization method for large-scale problems, and has been adapted for the stochastic learning framework. It can be applied on top of any existing optimization method with no need to tweak the internal algorithm. We show that the method is able to boost the performance of different algorithms, and make them more robust to changes in their hyper-parameters. As the boosting steps of SEBOOST are applied between large sets of descent steps, the additional subspace optimization hardly increases the overall computational burden. We introduce two hyper-parameters that control the balance between the baseline method and the secondary optimization process. The method was evaluated on several deep learning tasks, demonstrating promising results.



