# Arxiv Papers in cs.CV on 2016-09-20
### Reducing Drift in Visual Odometry by Inferring Sun Direction Using a Bayesian Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1609.05993v5
- **DOI**: 10.1109/ICRA.2017.7989235
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1609.05993v5)
- **Published**: 2016-09-20 02:02:51+00:00
- **Updated**: 2019-08-18 01:13:36+00:00
- **Authors**: Valentin Peretroukhin, Lee Clement, Jonathan Kelly
- **Comment**: In Proceedings of the IEEE International Conference on Robotics and
  Automation (ICRA'17), Singapore, May 29-Jun. 3, 2017
- **Journal**: None
- **Summary**: We present a method to incorporate global orientation information from the sun into a visual odometry pipeline using only the existing image stream, where the sun is typically not visible. We leverage recent advances in Bayesian Convolutional Neural Networks to train and implement a sun detection model that infers a three-dimensional sun direction vector from a single RGB image. Crucially, our method also computes a principled uncertainty associated with each prediction, using a Monte Carlo dropout scheme. We incorporate this uncertainty into a sliding window stereo visual odometry pipeline where accurate uncertainty estimates are critical for optimal data fusion. Our Bayesian sun detection model achieves a median error of approximately 12 degrees on the KITTI odometry benchmark training set, and yields improvements of up to 42% in translational ARMSE and 32% in rotational ARMSE compared to standard VO. An open source implementation of our Bayesian CNN sun estimator (Sun-BCNN) using Caffe is available at https://github. com/utiasSTARS/sun-bcnn-vo



### Deep CTR Prediction in Display Advertising
- **Arxiv ID**: http://arxiv.org/abs/1609.06018v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1609.06018v1)
- **Published**: 2016-09-20 04:50:03+00:00
- **Updated**: 2016-09-20 04:50:03+00:00
- **Authors**: Junxuan Chen, Baigui Sun, Hao Li, Hongtao Lu, Xian-Sheng Hua
- **Comment**: This manuscript is the accepted version for ACM Multimedia Conference
  2016
- **Journal**: None
- **Summary**: Click through rate (CTR) prediction of image ads is the core task of online display advertising systems, and logistic regression (LR) has been frequently applied as the prediction model. However, LR model lacks the ability of extracting complex and intrinsic nonlinear features from handcrafted high-dimensional image features, which limits its effectiveness. To solve this issue, in this paper, we introduce a novel deep neural network (DNN) based model that directly predicts the CTR of an image ad based on raw image pixels and other basic features in one step. The DNN model employs convolution layers to automatically extract representative visual features from images, and nonlinear CTR features are then learned from visual features and other contextual features by using fully-connected layers. Empirical evaluations on a real world dataset with over 50 million records demonstrate the effectiveness and efficiency of this method.



### Proposal of fault-tolerant tomographic image reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1609.06020v1
- **DOI**: 10.1117/12.2237107
- **Categories**: **physics.med-ph**, cs.CV, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1609.06020v1)
- **Published**: 2016-09-20 05:09:47+00:00
- **Updated**: 2016-09-20 05:09:47+00:00
- **Authors**: Hiroyuki Kudo, Keita Takaki, Fukashi Yamazaki, Takuya Nemoto
- **Comment**: 12 pages, 5 figures, SPIE Optics + Photonics 2016 Conference
  (Developments in X-Ray Tomography X) Paper No. 9967-55
- **Journal**: None
- **Summary**: This paper deals with tomographic image reconstruction under the situation where some of projection data bins are contaminated with abnormal data. Such situations occur in various instances of tomography. We propose a new reconstruction algorithm called the Fault-Tolerant reconstruction outlined as follows. The least-squares (L2-norm) error function ||Ax-b||_2^2 used in ordinary iterative reconstructions is sensitive to the existence of abnormal data. The proposed algorithm utilizes the L1-norm error function ||Ax-b||_1^1 instead of the L2-norm, and we develop a row-action-type iterative algorithm using the proximal splitting framework in convex optimization fields. We also propose an improved version of the L1-norm reconstruction called the L1-TV reconstruction, in which a weak Total Variation (TV) penalty is added to the cost function. Simulation results demonstrate that reconstructed images with the L2-norm were severely damaged by the effect of abnormal bins, whereas images with the L1-norm and L1-TV reconstructions were robust to the existence of abnormal bins.



### Contextual Relationship-based Activity Segmentation on an Event Stream in the IoT Environment with Multi-user Activities
- **Arxiv ID**: http://arxiv.org/abs/1609.06024v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.06024v1)
- **Published**: 2016-09-20 05:37:56+00:00
- **Updated**: 2016-09-20 05:37:56+00:00
- **Authors**: Minkyoung Cho, Younggi Kim, Younghee Lee
- **Comment**: 6 pages, 6 figures, 3 tables, M4IOT 2016
- **Journal**: None
- **Summary**: The human activity recognition in the IoT environment plays the central role in the ambient assisted living, where the human activities can be represented as a concatenated event stream generated from various smart objects. From the concatenated event stream, each activity should be distinguished separately for the human activity recognition to provide services that users may need. In this regard, accurately segmenting the entire stream at the precise boundary of each activity is indispensable high priority task to realize the activity recognition. Multiple human activities in an IoT environment generate varying event stream patterns, and the unpredictability of these patterns makes them include redundant or missing events. In dealing with this complex segmentation problem, we figured out that the dynamic and confusing patterns cause major problems due to: inclusive event stream, redundant events, and shared events. To address these problems, we exploited the contextual relationships associated with the activity status about either ongoing or terminated/started. To discover the intrinsic relationships between the events in a stream, we utilized the LSTM model by rendering it for the activity segmentation. Then, the inferred boundaries were revised by our validation algorithm for a bit shifted boundaries. Our experiments show the surprising result of high accuracy above 95%, on our own testbed with various smart objects. This is superior to the prior works that even do not assume the environment with multi-user activities, where their accuracies are slightly above 80% in their test environment. It proves that our work is feasible enough to be applied in the IoT environment.



### A very fast iterative algorithm for TV-regularized image reconstruction with applications to low-dose and few-view CT
- **Arxiv ID**: http://arxiv.org/abs/1609.06041v1
- **DOI**: 10.1117/12.2236788
- **Categories**: **physics.med-ph**, cs.CV, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1609.06041v1)
- **Published**: 2016-09-20 07:21:03+00:00
- **Updated**: 2016-09-20 07:21:03+00:00
- **Authors**: Hiroyuki Kudo, Fukashi Yamazaki, Takuya Nemoto, Keita Takaki
- **Comment**: 16 pages, 8 figures, SPIE Optics + Photonics 2016 Conference
  (Developments in X-Ray Tomography X) Paper No. 9967-37
- **Journal**: None
- **Summary**: This paper concerns iterative reconstruction for low-dose and few-view CT by minimizing a data-fidelity term regularized with the Total Variation (TV) penalty. We propose a very fast iterative algorithm to solve this problem. The algorithm derivation is outlined as follows. First, the original minimization problem is reformulated into the saddle point (primal-dual) problem by using the Lagrangian duality, to which we apply the first-order primal-dual iterative methods. Second, we precondition the iteration formula using the ramp flter of Filtered Backprojection (FBP) reconstruction algorithm in such a way that the problem solution is not altered. The resulting algorithm resembles the structure of so-called iterative FBP algorithm, and it converges to the exact minimizer of cost function very fast.



### Detecting Changes Between Optical Images of Different Spatial and Spectral Resolutions: a Fusion-Based Approach
- **Arxiv ID**: http://arxiv.org/abs/1609.06074v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/1609.06074v1)
- **Published**: 2016-09-20 09:58:35+00:00
- **Updated**: 2016-09-20 09:58:35+00:00
- **Authors**: Vinicius Ferraris, Nicolas Dobigeon, Qi Wei, Marie Chabert
- **Comment**: None
- **Journal**: None
- **Summary**: Change detection is one of the most challenging issues when analyzing remotely sensed images. Comparing several multi-date images acquired through the same kind of sensor is the most common scenario. Conversely, designing robust, flexible and scalable algorithms for change detection becomes even more challenging when the images have been acquired by two different kinds of sensors. This situation arises in case of emergency under critical constraints. This paper presents, to the best of authors' knowledge, the first strategy to deal with optical images characterized by dissimilar spatial and spectral resolutions. Typical considered scenarios include change detection between panchromatic or multispectral and hyperspectral images. The proposed strategy consists of a 3-step procedure: i) inferring a high spatial and spectral resolution image by fusion of the two observed images characterized one by a low spatial resolution and the other by a low spectral resolution, ii) predicting two images with respectively the same spatial and spectral resolutions as the observed images by degradation of the fused one and iii) implementing a decision rule to each pair of observed and predicted images characterized by the same spatial and spectral resolutions to identify changes. The performance of the proposed framework is evaluated on real images with simulated realistic changes.



### Robust Fusion of Multi-Band Images with Different Spatial and Spectral Resolutions for Change Detection
- **Arxiv ID**: http://arxiv.org/abs/1609.06076v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.data-an, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/1609.06076v1)
- **Published**: 2016-09-20 10:04:04+00:00
- **Updated**: 2016-09-20 10:04:04+00:00
- **Authors**: Vinicius Ferraris, Nicolas Dobigeon, Qi Wei, Marie Chabert
- **Comment**: None
- **Journal**: None
- **Summary**: Archetypal scenarios for change detection generally consider two images acquired through sensors of the same modality. However, in some specific cases such as emergency situations, the only images available may be those acquired through different kinds of sensors. More precisely, this paper addresses the problem of detecting changes between two multi-band optical images characterized by different spatial and spectral resolutions. This sensor dissimilarity introduces additional issues in the context of operational change detection. To alleviate these issues, classical change detection methods are applied after independent preprocessing steps (e.g., resampling) used to get the same spatial and spectral resolutions for the pair of observed images. Nevertheless, these preprocessing steps tend to throw away relevant information. Conversely, in this paper, we propose a method that more effectively uses the available information by modeling the two observed images as spatial and spectral versions of two (unobserved) latent images characterized by the same high spatial and high spectral resolutions. As they cover the same scene, these latent images are expected to be globally similar except for possible changes in sparse spatial locations. Thus, the change detection task is envisioned through a robust multi-band image fusion method which enforces the differences between the estimated latent images to be spatially sparse. This robust fusion problem is formulated as an inverse problem which is iteratively solved using an efficient block-coordinate descent algorithm. The proposed method is applied to real panchormatic/multispectral and hyperspectral images with simulated realistic changes. A comparison with state-of-the-art change detection methods evidences the accuracy of the proposed strategy.



### Adaptive Decontamination of the Training Set: A Unified Formulation for Discriminative Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1609.06118v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.06118v1)
- **Published**: 2016-09-20 11:46:17+00:00
- **Updated**: 2016-09-20 11:46:17+00:00
- **Authors**: Martin Danelljan, Gustav Häger, Fahad Shahbaz Khan, Michael Felsberg
- **Comment**: CVPR 2016
- **Journal**: None
- **Summary**: Tracking-by-detection methods have demonstrated competitive performance in recent years. In these approaches, the tracking model heavily relies on the quality of the training set. Due to the limited amount of labeled training data, additional samples need to be extracted and labeled by the tracker itself. This often leads to the inclusion of corrupted training samples, due to occlusions, misalignments and other perturbations. Existing tracking-by-detection methods either ignore this problem, or employ a separate component for managing the training set.   We propose a novel generic approach for alleviating the problem of corrupted training samples in tracking-by-detection frameworks. Our approach dynamically manages the training set by estimating the quality of the samples. Contrary to existing approaches, we propose a unified formulation by minimizing a single loss over both the target appearance model and the sample quality weights. The joint formulation enables corrupted samples to be down-weighted while increasing the impact of correct ones. Experiments are performed on three benchmarks: OTB-2015 with 100 videos, VOT-2015 with 60 videos, and Temple-Color with 128 videos. On the OTB-2015, our unified formulation significantly improves the baseline, with a gain of 3.8% in mean overlap precision. Finally, our method achieves state-of-the-art results on all three datasets. Code and supplementary material are available at http://www.cvl.isy.liu.se/research/objrec/visualtracking/decontrack/index.html .



### Discriminative Scale Space Tracking
- **Arxiv ID**: http://arxiv.org/abs/1609.06141v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.06141v1)
- **Published**: 2016-09-20 12:57:08+00:00
- **Updated**: 2016-09-20 12:57:08+00:00
- **Authors**: Martin Danelljan, Gustav Häger, Fahad Shahbaz Khan, Michael Felsberg
- **Comment**: To appear in TPAMI. This is the journal extension of the
  VOT2014-winning DSST tracking method
- **Journal**: None
- **Summary**: Accurate scale estimation of a target is a challenging research problem in visual object tracking. Most state-of-the-art methods employ an exhaustive scale search to estimate the target size. The exhaustive search strategy is computationally expensive and struggles when encountered with large scale variations. This paper investigates the problem of accurate and robust scale estimation in a tracking-by-detection framework. We propose a novel scale adaptive tracking approach by learning separate discriminative correlation filters for translation and scale estimation. The explicit scale filter is learned online using the target appearance sampled at a set of different scales. Contrary to standard approaches, our method directly learns the appearance change induced by variations in the target scale. Additionally, we investigate strategies to reduce the computational cost of our approach.   Extensive experiments are performed on the OTB and the VOT2014 datasets. Compared to the standard exhaustive scale search, our approach achieves a gain of 2.5% in average overlap precision on the OTB dataset. Additionally, our method is computationally efficient, operating at a 50% higher frame rate compared to the exhaustive scale search. Our method obtains the top rank in performance by outperforming 19 state-of-the-art trackers on OTB and 37 state-of-the-art trackers on VOT2014.



### Transfer Learning for Material Classification using Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1609.06188v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.06188v1)
- **Published**: 2016-09-20 14:15:41+00:00
- **Updated**: 2016-09-20 14:15:41+00:00
- **Authors**: Patrick Wieschollek, Hendrik P. A. Lensch
- **Comment**: None
- **Journal**: None
- **Summary**: Material classification in natural settings is a challenge due to complex interplay of geometry, reflectance properties, and illumination. Previous work on material classification relies strongly on hand-engineered features of visual samples. In this work we use a Convolutional Neural Network (convnet) that learns descriptive features for the specific task of material recognition. Specifically, transfer learning from the task of object recognition is exploited to more effectively train good features for material classification. The approach of transfer learning using convnets yields significantly higher recognition rates when compared to previous state-of-the-art approaches. We then analyze the relative contribution of reflectance and shading information by a decomposition of the image into its intrinsic components. The use of convnets for material classification was hindered by the strong demand for sufficient and diverse training data, even with transfer learning approaches. Therefore, we present a new data set containing approximately 10k images divided into 10 material categories.



### Hands-Free Segmentation of Medical Volumes via Binary Inputs
- **Arxiv ID**: http://arxiv.org/abs/1609.06192v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.06192v1)
- **Published**: 2016-09-20 14:18:40+00:00
- **Updated**: 2016-09-20 14:18:40+00:00
- **Authors**: Florian Dubost, Loic Peter, Christian Rupprecht, Benjamin Gutierrez-Becker, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel hands-free method to interactively segment 3D medical volumes. In our scenario, a human user progressively segments an organ by answering a series of questions of the form "Is this voxel inside the object to segment?". At each iteration, the chosen question is defined as the one halving a set of candidate segmentations given the answered questions. For a quick and efficient exploration, these segmentations are sampled according to the Metropolis-Hastings algorithm. Our sampling technique relies on a combination of relaxed shape prior, learnt probability map and consistency with previous answers. We demonstrate the potential of our strategy on a prostate segmentation MRI dataset. Through the study of failure cases with synthetic examples, we demonstrate the adaptation potential of our method. We also show that our method outperforms two intuitive baselines: one based on random questions, the other one being the thresholded probability map.



### GAdaBoost: Accelerating Adaboost Feature Selection with Genetic Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1609.06260v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.06260v1)
- **Published**: 2016-09-20 17:31:11+00:00
- **Updated**: 2016-09-20 17:31:11+00:00
- **Authors**: Mai Tolba, Mohamed Moustafa
- **Comment**: 8th International Conference on Evolutionary Computation Theory and
  Applications (ECTA 2016). Final paper will appear at the SCITEPRESS Digital
  Library
- **Journal**: None
- **Summary**: Boosted cascade of simple features, by Viola and Jones, is one of the most famous object detection frameworks. However, it suffers from a lengthy training process. This is due to the vast features space and the exhaustive search nature of Adaboost. In this paper we propose GAdaboost: a Genetic Algorithm to accelerate the training procedure through natural feature selection. Specifically, we propose to limit Adaboost search within a subset of the huge feature space, while evolving this subset following a Genetic Algorithm. Experiments demonstrate that our proposed GAdaboost is up to 3.7 times faster than Adaboost. We also demonstrate that the price of this speedup is a mere decrease (3%, 4%) in detection accuracy when tested on FDDB benchmark face detection set, and Caltech Web Faces respectively.



### Automated Visual Fin Identification of Individual Great White Sharks
- **Arxiv ID**: http://arxiv.org/abs/1609.06323v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.06323v2)
- **Published**: 2016-09-20 20:00:12+00:00
- **Updated**: 2016-10-01 16:32:53+00:00
- **Authors**: Benjamin Hughes, Tilo Burghardt
- **Comment**: 17 pages, 16 figures. To be published in IJCV. Article replaced to
  update first author contact details and to correct a Figure reference on page
  6
- **Journal**: None
- **Summary**: This paper discusses the automated visual identification of individual great white sharks from dorsal fin imagery. We propose a computer vision photo ID system and report recognition results over a database of thousands of unconstrained fin images. To the best of our knowledge this line of work establishes the first fully automated contour-based visual ID system in the field of animal biometrics. The approach put forward appreciates shark fins as textureless, flexible and partially occluded objects with an individually characteristic shape. In order to recover animal identities from an image we first introduce an open contour stroke model, which extends multi-scale region segmentation to achieve robust fin detection. Secondly, we show that combinatorial, scale-space selective fingerprinting can successfully encode fin individuality. We then measure the species-specific distribution of visual individuality along the fin contour via an embedding into a global `fin space'. Exploiting this domain, we finally propose a non-linear model for individual animal recognition and combine all approaches into a fine-grained multi-instance framework. We provide a system evaluation, compare results to prior work, and report performance and properties in detail.



### Markov Random Field Model-Based Salt and Pepper Noise Removal
- **Arxiv ID**: http://arxiv.org/abs/1609.06341v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.06341v1)
- **Published**: 2016-09-20 20:24:41+00:00
- **Updated**: 2016-09-20 20:24:41+00:00
- **Authors**: Ahmadreza Baghaie
- **Comment**: None
- **Journal**: None
- **Summary**: Problem of impulse noise reduction is a very well studied problem in image processing community and many different approaches have been proposed to tackle this problem. In the current work, the problem of fixed value impulse noise (salt and pepper) removal from images is investigated by use of a Markov Random Field (MRF) models with smoothness priors. After the formulation of the problem as an inpainting problem, graph cuts with $\alpha$-expansion moves are considered for minimization of the energy functional. As for comparisons, several other minimization techniques that are widely used for MRF models' optimization are considered and the results are compared using Peak-Signal-to-Noise-Ratio (PSNR) and Structural Similarity Index (SSIM) as metrics. The investigations show the superiority of graph cuts with $\alpha$-expansion moves over the other techniques both in terms of PSNR and also computational times.



### Robust Estimation of Multiple Inlier Structures
- **Arxiv ID**: http://arxiv.org/abs/1609.06371v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.06371v3)
- **Published**: 2016-09-20 22:06:58+00:00
- **Updated**: 2017-04-19 19:31:26+00:00
- **Authors**: Xiang Yang, Peter Meer
- **Comment**: None
- **Journal**: None
- **Summary**: The robust estimator presented in this paper processes each structure independently. The scales of the structures are estimated adaptively and no threshold is involved in spite of different objective functions. The user has to specify only the number of elemental subsets for random sampling. After classifying all the input data, the segmented structures are sorted by their strengths and the strongest inlier structures come out at the top. Like any robust estimators, this algorithm also has limitations which are described in detail. Several synthetic and real examples are presented to illustrate every aspect of the algorithm.



### Geometry-Based Next Frame Prediction from Monocular Video
- **Arxiv ID**: http://arxiv.org/abs/1609.06377v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, 68Txx, I.2.10; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1609.06377v2)
- **Published**: 2016-09-20 22:49:34+00:00
- **Updated**: 2017-06-12 21:52:06+00:00
- **Authors**: Reza Mahjourian, Martin Wicke, Anelia Angelova
- **Comment**: To appear in 2017 IEEE Intelligent Vehicles Symposium
- **Journal**: None
- **Summary**: We consider the problem of next frame prediction from video input. A recurrent convolutional neural network is trained to predict depth from monocular video input, which, along with the current video image and the camera trajectory, can then be used to compute the next frame. Unlike prior next-frame prediction approaches, we take advantage of the scene geometry and use the predicted depth for generating the next frame prediction. Our approach can produce rich next frame predictions which include depth information attached to each pixel. Another novel aspect of our approach is that it predicts depth from a sequence of images (e.g. in a video), rather than from a single still image. We evaluate the proposed approach on the KITTI dataset, a standard dataset for benchmarking tasks relevant to autonomous driving. The proposed method produces results which are visually and numerically superior to existing methods that directly predict the next frame. We show that the accuracy of depth prediction improves as more prior frames are considered.



