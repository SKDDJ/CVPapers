# Arxiv Papers in cs.CV on 2016-09-30
### Digitizing Municipal Street Inspections Using Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1609.09582v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1609.09582v1)
- **Published**: 2016-09-30 03:36:03+00:00
- **Updated**: 2016-09-30 03:36:03+00:00
- **Authors**: Varun Adibhatla, Shi Fan, Krystof Litomisky, Patrick Atwater
- **Comment**: Presented at the Data For Good Exchange 2016
- **Journal**: None
- **Summary**: "People want an authority to tell them how to value things. But they chose this authority not based on facts or results. They chose it because it seems authoritative and familiar." - The Big Short   The pavement condition index is one such a familiar measure used by many US cities to measure street quality and justify billions of dollars spent every year on street repair. These billion-dollar decisions are based on evaluation criteria that are subjective and not representative. In this paper, we build upon our initial submission to D4GX 2015 that approaches this problem of information asymmetry in municipal decision-making.   We describe a process to identify street-defects using computer vision techniques on data collected using the Street Quality Identification Device (SQUID). A User Interface to host a large quantity of image data towards digitizing the street inspection process and enabling actionable intelligence for a core public service is also described. This approach of combining device, data and decision-making around street repair enables cities make targeted decisions about street repair and could lead to an anticipatory response which can result in significant cost savings. Lastly, we share lessons learnt from the deployment of SQUID in the city of Syracuse, NY.



### A CNN Cascade for Landmark Guided Semantic Part Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1609.09642v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.09642v1)
- **Published**: 2016-09-30 09:11:13+00:00
- **Updated**: 2016-09-30 09:11:13+00:00
- **Authors**: Aaron Jackson, Michel Valstar, Georgios Tzimiropoulos
- **Comment**: accepted to Geometry Meets Deep Learning ECCV 2016 Workshop
- **Journal**: None
- **Summary**: This paper proposes a CNN cascade for semantic part segmentation guided by pose-specific information encoded in terms of a set of landmarks (or keypoints). There is large amount of prior work on each of these tasks separately, yet, to the best of our knowledge, this is the first time in literature that the interplay between pose estimation and semantic part segmentation is investigated. To address this limitation of prior work, in this paper, we propose a CNN cascade of tasks that firstly performs landmark localisation and then uses this information as input for guiding semantic part segmentation. We applied our architecture to the problem of facial part segmentation and report large performance improvement over the standard unguided network on the most challenging face datasets. Testing code and models will be published online at http://cs.nott.ac.uk/~psxasj/.



### Caffeinated FPGAs: FPGA Framework For Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1609.09671v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1609.09671v1)
- **Published**: 2016-09-30 11:13:21+00:00
- **Updated**: 2016-09-30 11:13:21+00:00
- **Authors**: Roberto DiCecco, Griffin Lacey, Jasmina Vasiljevic, Paul Chow, Graham Taylor, Shawki Areibi
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have gained significant traction in the field of machine learning, particularly due to their high accuracy in visual recognition. Recent works have pushed the performance of GPU implementations of CNNs to significantly improve their classification and training times. With these improvements, many frameworks have become available for implementing CNNs on both CPUs and GPUs, with no support for FPGA implementations. In this work we present a modified version of the popular CNN framework Caffe, with FPGA support. This allows for classification using CNN models and specialized FPGA implementations with the flexibility of reprogramming the device when necessary, seamless memory transactions between host and device, simple-to-use test benches, and the ability to create pipelined layer implementations. To validate the framework, we use the Xilinx SDAccel environment to implement an FPGA-based Winograd convolution engine and show that the FPGA layer can be used alongside other layers running on a host processor to run several popular CNNs (AlexNet, GoogleNet, VGG A, Overfeat). The results show that our framework achieves 50 GFLOPS across 3x3 convolutions in the benchmarks. This is achieved within a practical framework, which will aid in future development of FPGA-based CNNs.



### Training a Feedback Loop for Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1609.09698v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.09698v1)
- **Published**: 2016-09-30 12:35:26+00:00
- **Updated**: 2016-09-30 12:35:26+00:00
- **Authors**: Markus Oberweger, Paul Wohlhart, Vincent Lepetit
- **Comment**: Presented at ICCV 2015 (oral)
- **Journal**: None
- **Summary**: We propose an entirely data-driven approach to estimating the 3D pose of a hand given a depth image. We show that we can correct the mistakes made by a Convolutional Neural Network trained to predict an estimate of the 3D pose by using a feedback loop. The components of this feedback loop are also Deep Networks, optimized using training data. They remove the need for fitting a 3D model to the input data, which requires both a carefully designed fitting function and algorithm. We show that our approach outperforms state-of-the-art methods, and is efficient as our implementation runs at over 400 fps on a single GPU.



### A deep representation for depth images from synthetic data
- **Arxiv ID**: http://arxiv.org/abs/1609.09713v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.09713v1)
- **Published**: 2016-09-30 13:10:20+00:00
- **Updated**: 2016-09-30 13:10:20+00:00
- **Authors**: Fabio Maria Carlucci, Paolo Russo, Barbara Caputo
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) trained on large scale RGB databases have become the secret sauce in the majority of recent approaches for object categorization from RGB-D data. Thanks to colorization techniques, these methods exploit the filters learned from 2D images to extract meaningful representations in 2.5D. Still, the perceptual signature of these two kind of images is very different, with the first usually strongly characterized by textures, and the second mostly by silhouettes of objects. Ideally, one would like to have two CNNs, one for RGB and one for depth, each trained on a suitable data collection, able to capture the perceptual properties of each channel for the task at hand. This has not been possible so far, due to the lack of a suitable depth database. This paper addresses this issue, proposing to opt for synthetically generated images rather than collecting by hand a 2.5D large scale database. While being clearly a proxy for real data, synthetic images allow to trade quality for quantity, making it possible to generate a virtually infinite amount of data. We show that the filters learned from such data collection, using the very same architecture typically used on visual data, learns very different filters, resulting in depth features (a) able to better characterize the different facets of depth images, and (b) complementary with respect to those derived from CNNs pre-trained on 2D datasets. Experiments on two publicly available databases show the power of our approach.



### Latent fingerprint minutia extraction using fully convolutional network
- **Arxiv ID**: http://arxiv.org/abs/1609.09850v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.09850v2)
- **Published**: 2016-09-30 18:29:41+00:00
- **Updated**: 2017-09-07 12:15:54+00:00
- **Authors**: Yao Tang, Fei Gao, Jufu Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Minutiae play a major role in fingerprint identification. Extracting reliable minutiae is difficult for latent fingerprints which are usually of poor quality. As the limitation of traditional handcrafted features, a fully convolutional network (FCN) is utilized to learn features directly from data to overcome complex background noises. Raw fingerprints are mapped to a correspondingly-sized minutia-score map with a fixed stride. And thus a large number of minutiae will be extracted through a given threshold. Then small regions centering at these minutia points are entered into a convolutional neural network (CNN) to reclassify these minutiae and calculate their orientations. The CNN shares convolutional layers with the fully convolutional network to speed up. 0.45 second is used on average to detect one fingerprint on a GPU. On the NIST SD27 database, we achieve 53\% recall rate and 53\% precise rate that outperform many other algorithms. Our trained model is also visualized to show that we have successfully extracted features preserving ridge information of a latent fingerprint.



