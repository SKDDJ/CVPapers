# Arxiv Papers in cs.CV on 2016-09-16
### Image-to-Markup Generation with Coarse-to-Fine Attention
- **Arxiv ID**: http://arxiv.org/abs/1609.04938v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1609.04938v2)
- **Published**: 2016-09-16 08:14:50+00:00
- **Updated**: 2017-06-13 22:48:53+00:00
- **Authors**: Yuntian Deng, Anssi Kanervisto, Jeffrey Ling, Alexander M. Rush
- **Comment**: Accepted by ICML 2017
- **Journal**: None
- **Summary**: We present a neural encoder-decoder model to convert images into presentational markup based on a scalable coarse-to-fine attention mechanism. Our method is evaluated in the context of image-to-LaTeX generation, and we introduce a new dataset of real-world rendered mathematical expressions paired with LaTeX markup. We show that unlike neural OCR techniques using CTC-based models, attention-based approaches can tackle this non-standard OCR task. Our approach outperforms classical mathematical OCR systems by a large margin on in-domain rendered data, and, with pretraining, also performs well on out-of-domain handwritten data. To reduce the inference complexity associated with the attention-based approaches, we introduce a new coarse-to-fine attention layer that selects a support region before applying attention.



### Stamp processing with examplar features
- **Arxiv ID**: http://arxiv.org/abs/1609.05001v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.05001v1)
- **Published**: 2016-09-16 11:20:07+00:00
- **Updated**: 2016-09-16 11:20:07+00:00
- **Authors**: Yash Bhalgat, Mandar Kulkarni, Shirish Karande, Sachin Lodha
- **Comment**: None
- **Journal**: None
- **Summary**: Document digitization is becoming increasingly crucial. In this work, we propose a shape based approach for automatic stamp verification/detection in document images using an unsupervised feature learning. Given a small set of training images, our algorithm learns an appropriate shape representation using an unsupervised clustering. Experimental results demonstrate the effectiveness of our framework in challenging scenarios.



### Barcodes for Medical Image Retrieval Using Autoencoded Radon Transform
- **Arxiv ID**: http://arxiv.org/abs/1609.05112v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.05112v1)
- **Published**: 2016-09-16 15:51:24+00:00
- **Updated**: 2016-09-16 15:51:24+00:00
- **Authors**: Hamid R. Tizhoosh, Christopher Mitcheltree, Shujin Zhu, Shamak Dutta
- **Comment**: o appear in proceedings of the 23rd International Conference on
  Pattern Recognition (ICPR 2016), Cancun, Mexico, December 2016
- **Journal**: None
- **Summary**: Using content-based binary codes to tag digital images has emerged as a promising retrieval technology. Recently, Radon barcodes (RBCs) have been introduced as a new binary descriptor for image search. RBCs are generated by binarization of Radon projections and by assembling them into a vector, namely the barcode. A simple local thresholding has been suggested for binarization. In this paper, we put forward the idea of "autoencoded Radon barcodes". Using images in a training dataset, we autoencode Radon projections to perform binarization on outputs of hidden layers. We employed the mini-batch stochastic gradient descent approach for the training. Each hidden layer of the autoencoder can produce a barcode using a threshold determined based on the range of the logistic function used. The compressing capability of autoencoders apparently reduces the redundancies inherent in Radon projections leading to more accurate retrieval results. The IRMA dataset with 14,410 x-ray images is used to validate the performance of the proposed method. The experimental results, containing comparison with RBCs, SURF and BRISK, show that autoencoded Radon barcode (ARBC) has the capacity to capture important information and to learn richer representations resulting in lower retrieval errors for image retrieval measured with the accuracy of the first hit only.



### Dense Wide-Baseline Scene Flow From Two Handheld Video Cameras
- **Arxiv ID**: http://arxiv.org/abs/1609.05115v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.05115v1)
- **Published**: 2016-09-16 15:54:46+00:00
- **Updated**: 2016-09-16 15:54:46+00:00
- **Authors**: Christian Richardt, Hyeongwoo Kim, Levi Valgaerts, Christian Theobalt
- **Comment**: 11 pages, supplemental document included as appendix, 3DV 2016
- **Journal**: None
- **Summary**: We propose a new technique for computing dense scene flow from two handheld videos with wide camera baselines and different photometric properties due to different sensors or camera settings like exposure and white balance. Our technique innovates in two ways over existing methods: (1) it supports independently moving cameras, and (2) it computes dense scene flow for wide-baseline scenarios.We achieve this by combining state-of-the-art wide-baseline correspondence finding with a variational scene flow formulation. First, we compute dense, wide-baseline correspondences using DAISY descriptors for matching between cameras and over time. We then detect and replace occluded pixels in the correspondence fields using a novel edge-preserving Laplacian correspondence completion technique. We finally refine the computed correspondence fields in a variational scene flow formulation. We show dense scene flow results computed from challenging datasets with independently moving, handheld cameras of varying camera settings.



### Radon-Gabor Barcodes for Medical Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1609.05118v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.05118v1)
- **Published**: 2016-09-16 16:01:43+00:00
- **Updated**: 2016-09-16 16:01:43+00:00
- **Authors**: Mina Nouredanesh, H. R. Tizhoosh, Ershad Banijamali, James Tung
- **Comment**: To appear in proceedings of the 23rd International Conference on
  Pattern Recognition (ICPR 2016), Cancun, Mexico, December 2016
- **Journal**: None
- **Summary**: In recent years, with the explosion of digital images on the Web, content-based retrieval has emerged as a significant research area. Shapes, textures, edges and segments may play a key role in describing the content of an image. Radon and Gabor transforms are both powerful techniques that have been widely studied to extract shape-texture-based information. The combined Radon-Gabor features may be more robust against scale/rotation variations, presence of noise, and illumination changes. The objective of this paper is to harness the potentials of both Gabor and Radon transforms in order to introduce expressive binary features, called barcodes, for image annotation/tagging tasks. We propose two different techniques: Gabor-of-Radon-Image Barcodes (GRIBCs), and Guided-Radon-of-Gabor Barcodes (GRGBCs). For validation, we employ the IRMA x-ray dataset with 193 classes, containing 12,677 training images and 1,733 test images. A total error score as low as 322 and 330 were achieved for GRGBCs and GRIBCs, respectively. This corresponds to $\approx 81\%$ retrieval accuracy for the first hit.



### Deep Impression: Audiovisual Deep Residual Networks for Multimodal Apparent Personality Trait Recognition
- **Arxiv ID**: http://arxiv.org/abs/1609.05119v1
- **DOI**: 10.1007/978-3-319-49409-8_28
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.05119v1)
- **Published**: 2016-09-16 16:09:20+00:00
- **Updated**: 2016-09-16 16:09:20+00:00
- **Authors**: Yağmur Güçlütürk, Umut Güçlü, Marcel A. J. van Gerven, Rob van Lier
- **Comment**: None
- **Journal**: None
- **Summary**: Here, we develop an audiovisual deep residual network for multimodal apparent personality trait recognition. The network is trained end-to-end for predicting the Big Five personality traits of people from their videos. That is, the network does not require any feature engineering or visual analysis such as face detection, face landmark alignment or facial expression recognition. Recently, the network won the third place in the ChaLearn First Impressions Challenge with a test accuracy of 0.9109.



### SemanticFusion: Dense 3D Semantic Mapping with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1609.05130v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.05130v2)
- **Published**: 2016-09-16 16:46:21+00:00
- **Updated**: 2016-09-28 14:32:45+00:00
- **Authors**: John McCormac, Ankur Handa, Andrew Davison, Stefan Leutenegger
- **Comment**: None
- **Journal**: None
- **Summary**: Ever more robust, accurate and detailed mapping using visual sensing has proven to be an enabling factor for mobile robots across a wide variety of applications. For the next level of robot intelligence and intuitive user interaction, maps need extend beyond geometry and appearence - they need to contain semantics. We address this challenge by combining Convolutional Neural Networks (CNNs) and a state of the art dense Simultaneous Localisation and Mapping (SLAM) system, ElasticFusion, which provides long-term dense correspondence between frames of indoor RGB-D video even during loopy scanning trajectories. These correspondences allow the CNN's semantic predictions from multiple view points to be probabilistically fused into a map. This not only produces a useful semantic 3D map, but we also show on the NYUv2 dataset that fusing multiple predictions leads to an improvement even in the 2D semantic labelling over baseline single frame predictions. We also show that for a smaller reconstruction dataset with larger variation in prediction viewpoint, the improvement over single frame segmentation increases. Our system is efficient enough to allow real-time interactive use at frame-rates of approximately 25Hz.



### Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1609.05143v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.05143v1)
- **Published**: 2016-09-16 17:16:49+00:00
- **Updated**: 2016-09-16 17:16:49+00:00
- **Authors**: Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J. Lim, Abhinav Gupta, Li Fei-Fei, Ali Farhadi
- **Comment**: None
- **Journal**: None
- **Summary**: Two less addressed issues of deep reinforcement learning are (1) lack of generalization capability to new target goals, and (2) data inefficiency i.e., the model requires several (and often costly) episodes of trial and error to converge, which makes it impractical to be applied to real-world scenarios. In this paper, we address these two issues and apply our model to the task of target-driven visual navigation. To address the first issue, we propose an actor-critic model whose policy is a function of the goal as well as the current state, which allows to better generalize. To address the second issue, we propose AI2-THOR framework, which provides an environment with high-quality 3D scenes and physics engine. Our framework enables agents to take actions and interact with objects. Hence, we can collect a huge number of training samples efficiently.   We show that our proposed method (1) converges faster than the state-of-the-art deep reinforcement learning methods, (2) generalizes across targets and across scenes, (3) generalizes to a real robot scenario with a small amount of fine-tuning (although the model is trained in simulation), (4) is end-to-end trainable and does not need feature engineering, feature matching between frames or 3D reconstruction of the environment.   The supplementary video can be accessed at the following link: https://youtu.be/SmBxMDiOrvs.



### Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1609.05158v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1609.05158v2)
- **Published**: 2016-09-16 17:58:14+00:00
- **Updated**: 2016-09-23 17:16:37+00:00
- **Authors**: Wenzhe Shi, Jose Caballero, Ferenc Huszár, Johannes Totz, Andrew P. Aitken, Rob Bishop, Daniel Rueckert, Zehan Wang
- **Comment**: CVPR 2016 paper with updated affiliations and supplemental material,
  fixed typo in equation 4
- **Journal**: None
- **Summary**: Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.



### Rule Extraction Algorithm for Deep Neural Networks: A Review
- **Arxiv ID**: http://arxiv.org/abs/1610.05267v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.05267v1)
- **Published**: 2016-09-16 18:10:18+00:00
- **Updated**: 2016-09-16 18:10:18+00:00
- **Authors**: Tameru Hailesilassie
- **Comment**: 6 pages,2 figures,IEEE Publication format, Keywords- Artificial
  neural network; Deep neural network; Rule extraction; Decompositional;
  Pedagogical; Eclectic
- **Journal**: (IJCSIS) International Journal of Computer Science and Information
  Security,Vol. 14, No. 7, July 2016, page 371-381
- **Summary**: Despite the highest classification accuracy in wide varieties of application areas, artificial neural network has one disadvantage. The way this Network comes to a decision is not easily comprehensible. The lack of explanation ability reduces the acceptability of neural network in data mining and decision system. This drawback is the reason why researchers have proposed many rule extraction algorithms to solve the problem. Recently, Deep Neural Network (DNN) is achieving a profound result over the standard neural network for classification and recognition problems. It is a hot machine learning area proven both useful and innovative. This paper has thoroughly reviewed various rule extraction algorithms, considering the classification scheme: decompositional, pedagogical, and eclectics. It also presents the evaluation of these algorithms based on the neural network structure with which the algorithm is intended to work. The main contribution of this review is to show that there is a limited study of rule extraction algorithm from DNN.



