# Arxiv Papers in cs.CV on 2016-09-01
### Attentional Push: Augmenting Salience with Shared Attention Modeling
- **Arxiv ID**: http://arxiv.org/abs/1609.00072v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.00072v1)
- **Published**: 2016-09-01 00:43:11+00:00
- **Updated**: 2016-09-01 00:43:11+00:00
- **Authors**: Siavash Gorji, James J. Clark
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel visual attention tracking technique based on Shared Attention modeling. Our proposed method models the viewer as a participant in the activity occurring in the scene. We go beyond image salience and instead of only computing the power of an image region to pull attention to it, we also consider the strength with which other regions of the image push attention to the region in question. We use the term Attentional Push to refer to the power of image regions to direct and manipulate the attention allocation of the viewer. An attention model is presented that incorporates the Attentional Push cues with standard image salience-based attention modeling algorithms to improve the ability to predict where viewers will fixate. Experimental evaluation validates significant improvements in predicting viewers' fixations using the proposed methodology in both static and dynamic imagery.



### Image segmentation based on histogram of depth and an application in driver distraction detection
- **Arxiv ID**: http://arxiv.org/abs/1609.00096v1
- **DOI**: 10.1109/ICARCV.2014.7064437
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.00096v1)
- **Published**: 2016-09-01 03:19:43+00:00
- **Updated**: 2016-09-01 03:19:43+00:00
- **Authors**: Tran Hiep Dinh, Minh Trien Pham, Manh Duong Phung, Duc Manh Nguyen, Van Manh Hoang, Quang Vinh Tran
- **Comment**: 6 pages In 13th International Conference on Control Automation
  Robotics & Vision (ICARCV), 2014
- **Journal**: None
- **Summary**: This study proposes an approach to segment human object from a depth image based on histogram of depth values. The region of interest is first extracted based on a predefined threshold for histogram regions. A region growing process is then employed to separate multiple human bodies with the same depth interval. Our contribution is the identification of an adaptive growth threshold based on the detected histogram region. To demonstrate the effectiveness of the proposed method, an application in driver distraction detection was introduced. After successfully extracting the driver's position inside the car, we came up with a simple solution to track the driver motion. With the analysis of the difference between initial and current frame, a change of cluster position or depth value in the interested region, which cross the preset threshold, is considered as a distracted activity. The experiment results demonstrated the success of the algorithm in detecting typical distracted driving activities such as using phone for calling or texting, adjusting internal devices and drinking in real time.



### Grid Loss: Detecting Occluded Faces
- **Arxiv ID**: http://arxiv.org/abs/1609.00129v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.00129v1)
- **Published**: 2016-09-01 07:15:13+00:00
- **Updated**: 2016-09-01 07:15:13+00:00
- **Authors**: Michael Opitz, Georg Waltner, Georg Poier, Horst Possegger, Horst Bischof
- **Comment**: accepted to ECCV 2016
- **Journal**: None
- **Summary**: Detection of partially occluded objects is a challenging computer vision problem. Standard Convolutional Neural Network (CNN) detectors fail if parts of the detection window are occluded, since not every sub-part of the window is discriminative on its own. To address this issue, we propose a novel loss layer for CNNs, named grid loss, which minimizes the error rate on sub-blocks of a convolution layer independently rather than over the whole feature map. This results in parts being more discriminative on their own, enabling the detector to recover if the detection window is partially occluded. By mapping our loss layer back to a regular fully connected layer, no additional computational cost is incurred at runtime compared to standard CNNs. We demonstrate our method for face detection on several public face detection benchmarks and show that our method outperforms regular CNNs, is suitable for realtime applications and achieves state-of-the-art performance.



### Weakly Supervised PatchNets: Describing and Aggregating Local Patches for Scene Recognition
- **Arxiv ID**: http://arxiv.org/abs/1609.00153v2
- **DOI**: 10.1109/TIP.2017.2666739
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.00153v2)
- **Published**: 2016-09-01 09:15:41+00:00
- **Updated**: 2017-02-21 21:12:53+00:00
- **Authors**: Zhe Wang, Limin Wang, Yali Wang, Bowen Zhang, Yu Qiao
- **Comment**: To appear in IEEE Transactions on Image Processing. Code and model
  available at https://github.com/wangzheallen/vsad
- **Journal**: None
- **Summary**: Traditional feature encoding scheme (e.g., Fisher vector) with local descriptors (e.g., SIFT) and recent convolutional neural networks (CNNs) are two classes of successful methods for image recognition. In this paper, we propose a hybrid representation, which leverages the discriminative capacity of CNNs and the simplicity of descriptor encoding schema for image recognition, with a focus on scene recognition. To this end, we make three main contributions from the following aspects. First, we propose a patch-level and end-to-end architecture to model the appearance of local patches, called {\em PatchNet}. PatchNet is essentially a customized network trained in a weakly supervised manner, which uses the image-level supervision to guide the patch-level feature extraction. Second, we present a hybrid visual representation, called {\em VSAD}, by utilizing the robust feature representations of PatchNet to describe local patches and exploiting the semantic probabilities of PatchNet to aggregate these local patches into a global representation. Third, based on the proposed VSAD representation, we propose a new state-of-the-art scene recognition approach, which achieves an excellent performance on two standard benchmarks: MIT Indoor67 (86.2\%) and SUN397 (73.0\%).



### Transferring Object-Scene Convolutional Neural Networks for Event Recognition in Still Images
- **Arxiv ID**: http://arxiv.org/abs/1609.00162v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.00162v1)
- **Published**: 2016-09-01 09:47:22+00:00
- **Updated**: 2016-09-01 09:47:22+00:00
- **Authors**: Limin Wang, Zhe Wang, Yu Qiao, Luc Van Gool
- **Comment**: 20 pages
- **Journal**: None
- **Summary**: Event recognition in still images is an intriguing problem and has potential for real applications. This paper addresses the problem of event recognition by proposing a convolutional neural network that exploits knowledge of objects and scenes for event classification (OS2E-CNN). Intuitively, it stands to reason that there exists a correlation among the concepts of objects, scenes, and events. We empirically demonstrate that the recognition of objects and scenes substantially contributes to the recognition of events. Meanwhile, we propose an iterative selection method to identify a subset of object and scene classes, which help to more efficiently and effectively transfer their deep representations to event recognition. Specifically, we develop three types of transferring techniques: (1) initialization-based transferring, (2) knowledge-based transferring, and (3) data-based transferring. These newly designed transferring techniques exploit multi-task learning frameworks to incorporate extra knowledge from other networks and additional datasets into the training procedure of event CNNs. These multi-task learning frameworks turn out to be effective in reducing the effect of over-fitting and improving the generalization ability of the learned CNNs. With OS2E-CNN, we design a multi-ratio and multi-scale cropping strategy, and propose an end-to-end event recognition pipeline. We perform experiments on three event recognition benchmarks: the ChaLearn Cultural Event Recognition dataset, the Web Image Dataset for Event Recognition (WIDER), and the UIUC Sports Event dataset. The experimental results show that our proposed algorithm successfully adapts object and scene representations towards the event dataset and that it achieves the current state-of-the-art performance on these challenging datasets.



### Segmentation Free Object Discovery in Video
- **Arxiv ID**: http://arxiv.org/abs/1609.00221v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.00221v1)
- **Published**: 2016-09-01 13:08:39+00:00
- **Updated**: 2016-09-01 13:08:39+00:00
- **Authors**: Giovanni Cuffaro, Federico Becattini, Claudio Baecchi, Lorenzo Seidenari, Alberto Del Bimbo
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present a simple yet effective approach to extend without supervision any object proposal from static images to videos. Unlike previous methods, these spatio-temporal proposals, to which we refer as tracks, are generated relying on little or no visual content by only exploiting bounding boxes spatial correlations through time. The tracks that we obtain are likely to represent objects and are a general-purpose tool to represent meaningful video content for a wide variety of tasks. For unannotated videos, tracks can be used to discover content without any supervision. As further contribution we also propose a novel and dataset-independent method to evaluate a generic object proposal based on the entropy of a classifier output response. We experiment on two competitive datasets, namely YouTube Objects and ILSVRC-2015 VID.



### Semantic Image Based Geolocation Given a Map
- **Arxiv ID**: http://arxiv.org/abs/1609.00278v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1609.00278v1)
- **Published**: 2016-09-01 15:27:02+00:00
- **Updated**: 2016-09-01 15:27:02+00:00
- **Authors**: Arsalan Mousavian, Jana Kosecka
- **Comment**: None
- **Journal**: None
- **Summary**: The problem visual place recognition is commonly used strategy for localization. Most successful appearance based methods typically rely on a large database of views endowed with local or global image descriptors and strive to retrieve the views of the same location. The quality of the results is often affected by the density of the reference views and the robustness of the image representation with respect to viewpoint variations, clutter and seasonal changes. In this work we present an approach for geo-locating a novel view and determining camera location and orientation using a map and a sparse set of geo-tagged reference views. We propose a novel technique for detection and identification of building facades from geo-tagged reference view using the map and geometry of the building facades. We compute the likelihood of camera location and orientation of the query images using the detected landmark (building) identities from reference views, 2D map of the environment, and geometry of building facades. We evaluate our approach for building identification and geo-localization on a new challenging outdoors urban dataset exhibiting large variations in appearance and viewpoint.



### Deep Learning Human Mind for Automated Visual Classification
- **Arxiv ID**: http://arxiv.org/abs/1609.00344v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.00344v2)
- **Published**: 2016-09-01 18:47:05+00:00
- **Updated**: 2019-10-22 11:39:22+00:00
- **Authors**: Concetto Spampinato, Simone Palazzo, Isaak Kavasidis, Daniela Giordano, Mubarak Shah, Nasim Souly
- **Comment**: CVPR 2017 Accepted Paper
- **Journal**: CVPR 2017
- **Summary**: What if we could effectively read the mind and transfer human visual capabilities to computer vision methods? In this paper, we aim at addressing this question by developing the first visual object classifier driven by human brain signals. In particular, we employ EEG data evoked by visual object stimuli combined with Recurrent Neural Networks (RNN) to learn a discriminative brain activity manifold of visual categories. Afterwards, we train a Convolutional Neural Network (CNN)-based regressor to project images onto the learned manifold, thus effectively allowing machines to employ human brain-based features for automated visual classification. We use a 32-channel EEG to record brain activity of seven subjects while looking at images of 40 ImageNet object classes. The proposed RNN based approach for discriminating object classes using brain signals reaches an average accuracy of about 40%, which outperforms existing methods attempting to learn EEG visual object representations. As for automated object categorization, our human brain-driven approach obtains competitive performance, comparable to those achieved by powerful CNN models, both on ImageNet and CalTech 101, thus demonstrating its classification and generalization capabilities. This gives us a real hope that, indeed, human mind can be read and transferred to machines.



### Autonomous driving challenge: To Infer the property of a dynamic object based on its motion pattern using recurrent neural network
- **Arxiv ID**: http://arxiv.org/abs/1609.00361v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.00361v2)
- **Published**: 2016-09-01 19:39:08+00:00
- **Updated**: 2016-09-10 00:01:45+00:00
- **Authors**: Mona Fathollahi, Rangachar Kasturi
- **Comment**: None
- **Journal**: None
- **Summary**: In autonomous driving applications a critical challenge is to identify action to take to avoid an obstacle on collision course. For example, when a heavy object is suddenly encountered it is critical to stop the vehicle or change the lane even if it causes other traffic disruptions. However,there are situations when it is preferable to collide with the object rather than take an action that would result in a much more serious accident than collision with the object. For example, a heavy object which falls from a truck should be avoided whereas a bouncing ball or a soft target such as a foam box need not be.We present a novel method to discriminate between the motion characteristics of these types of objects based on their physical properties such as bounciness, elasticity, etc.In this preliminary work, we use recurrent neural net-work with LSTM cells to train a classifier to classify objects based on their motion trajectories. We test the algorithm on synthetic data, and, as a proof of concept, demonstrate its effectiveness on a limited set of real-world data.



### Defeating Image Obfuscation with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1609.00408v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1609.00408v2)
- **Published**: 2016-09-01 21:38:15+00:00
- **Updated**: 2016-09-06 21:47:12+00:00
- **Authors**: Richard McPherson, Reza Shokri, Vitaly Shmatikov
- **Comment**: None
- **Journal**: None
- **Summary**: We demonstrate that modern image recognition methods based on artificial neural networks can recover hidden information from images protected by various forms of obfuscation. The obfuscation techniques considered in this paper are mosaicing (also known as pixelation), blurring (as used by YouTube), and P3, a recently proposed system for privacy-preserving photo sharing that encrypts the significant JPEG coefficients to make images unrecognizable by humans. We empirically show how to train artificial neural networks to successfully identify faces and recognize objects and handwritten digits even if the images are protected using any of the above obfuscation techniques.



