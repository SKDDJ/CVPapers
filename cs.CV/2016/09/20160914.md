# Arxiv Papers in cs.CV on 2016-09-14
### Single-image RGB Photometric Stereo With Spatially-varying Albedo
- **Arxiv ID**: http://arxiv.org/abs/1609.04079v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.04079v1)
- **Published**: 2016-09-14 00:39:58+00:00
- **Updated**: 2016-09-14 00:39:58+00:00
- **Authors**: Ayan Chakrabarti, Kalyan Sunkavalli
- **Comment**: 3DV 2016. Project page at http://www.ttic.edu/chakrabarti/rgbps/
- **Journal**: None
- **Summary**: We present a single-shot system to recover surface geometry of objects with spatially-varying albedos, from images captured under a calibrated RGB photometric stereo setup---with three light directions multiplexed across different color channels in the observed RGB image. Since the problem is ill-posed point-wise, we assume that the albedo map can be modeled as piece-wise constant with a restricted number of distinct albedo values. We show that under ideal conditions, the shape of a non-degenerate local constant albedo surface patch can theoretically be recovered exactly. Moreover, we present a practical and efficient algorithm that uses this model to robustly recover shape from real images. Our method first reasons about shape locally in a dense set of patches in the observed image, producing shape distributions for every patch. These local distributions are then combined to produce a single consistent surface normal map. We demonstrate the efficacy of the approach through experiments on both synthetic renderings as well as real captured images.



### A Machine Learning Nowcasting Method based on Real-time Reanalysis Data
- **Arxiv ID**: http://arxiv.org/abs/1609.04103v2
- **DOI**: 10.1002/2016JD025783
- **Categories**: **physics.ao-ph**, cs.CV, physics.geo-ph, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1609.04103v2)
- **Published**: 2016-09-14 01:01:29+00:00
- **Updated**: 2017-04-08 06:39:31+00:00
- **Authors**: Lei Han, Juanzhen Sun, Wei Zhang, Yuanyuan Xiu, Hailei Feng, Yinjing Lin
- **Comment**: 22 pages, 11 figures, submitted to Journal of Geophysical Research:
  Atmospheres
- **Journal**: J. Geophys. Res. Atmos., 122, (2017)
- **Summary**: Despite marked progress over the past several decades, convective storm nowcasting remains a challenge because most nowcasting systems are based on linear extrapolation of radar reflectivity without much consideration for other meteorological fields. The variational Doppler radar analysis system (VDRAS) is an advanced convective-scale analysis system capable of providing analysis of 3-D wind, temperature, and humidity by assimilating Doppler radar observations. Although potentially useful, it is still an open question as to how to use these fields to improve nowcasting. In this study, we present results from our first attempt at developing a Support Vector Machine (SVM) Box-based nOWcasting (SBOW) method under the machine learning framework using VDRAS analysis data. The key design points of SBOW are as follows: 1) The study domain is divided into many position-fixed small boxes and the nowcasting problem is transformed into one question, i.e., will a radar echo > 35 dBZ appear in a box in 30 minutes? 2) Box-based temporal and spatial features, which include time trends and surrounding environmental information, are elaborately constructed, and 3) The box-based constructed features are used to first train the SVM classifier, and then the trained classifier is used to make predictions. Compared with complicated and expensive expert systems, the above design of SBOW allows the system to be small, compact, straightforward, and easy to maintain and expand at low cost. The experimental results show that, although no complicated tracking algorithm is used, SBOW can predict the storm movement trend and storm growth with reasonable skill.



### Tracking Tensor Subspaces with Informative Random Sampling for Real-Time MR Imaging
- **Arxiv ID**: http://arxiv.org/abs/1609.04104v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IT, math.IT, stat.CO
- **Links**: [PDF](http://arxiv.org/pdf/1609.04104v1)
- **Published**: 2016-09-14 01:23:05+00:00
- **Updated**: 2016-09-14 01:23:05+00:00
- **Authors**: Morteza Mardani, Georgios B. Giannakis, Kamil Ugurbil
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic resonance imaging (MRI) nowadays serves as an important modality for diagnostic and therapeutic guidance in clinics. However, the {\it slow acquisition} process, the dynamic deformation of organs, as well as the need for {\it real-time} reconstruction, pose major challenges toward obtaining artifact-free images. To cope with these challenges, the present paper advocates a novel subspace learning framework that permeates benefits from parallel factor (PARAFAC) decomposition of tensors (multiway data) to low-rank modeling of temporal sequence of images. Treating images as multiway data arrays, the novel method preserves spatial structures and unravels the latent correlations across various dimensions by means of the tensor subspace. Leveraging the spatio-temporal correlation of images, Tykhonov regularization is adopted as a rank surrogate for a least-squares optimization program. Alteranating majorization minimization is adopted to develop online algorithms that recursively procure the reconstruction upon arrival of a new undersampled $k$-space frame. The developed algorithms are {\it provably convergent} and highly {\it parallelizable} with lightweight FFT tasks per iteration. To further accelerate the acquisition process, randomized subsampling policies are devised that leverage intermediate estimates of the tensor subspace, offered by the online scheme, to {\it randomly} acquire {\it informative} $k$-space samples. In a nutshell, the novel approach enables tracking motion dynamics under low acquisition rates `on the fly.' GPU-based tests with real {\it in vivo} MRI datasets of cardiac cine images corroborate the merits of the novel approach relative to state-of-the-art alternatives.



### Understanding Convolutional Neural Networks with A Mathematical Model
- **Arxiv ID**: http://arxiv.org/abs/1609.04112v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.04112v2)
- **Published**: 2016-09-14 02:17:09+00:00
- **Updated**: 2016-11-02 21:55:26+00:00
- **Authors**: C. -C. Jay Kuo
- **Comment**: None
- **Journal**: None
- **Summary**: This work attempts to address two fundamental questions about the structure of the convolutional neural networks (CNN): 1) why a non-linear activation function is essential at the filter output of every convolutional layer? 2) what is the advantage of the two-layer cascade system over the one-layer system? A mathematical model called the "REctified-COrrelations on a Sphere" (RECOS) is proposed to answer these two questions. After the CNN training process, the converged filter weights define a set of anchor vectors in the RECOS model. Anchor vectors represent the frequently occurring patterns (or the spectral components). The necessity of rectification is explained using the RECOS model. Then, the behavior of a two-layer RECOS system is analyzed and compared with its one-layer counterpart. The LeNet-5 and the MNIST dataset are used to illustrate discussion points. Finally, the RECOS model is generalized to a multi-layer system with the AlexNet as an example.   Keywords: Convolutional Neural Network (CNN), Nonlinear Activation, RECOS Model, Rectified Linear Unit (ReLU), MNIST Dataset.



### Joint Gender Classification and Age Estimation by Nearly Orthogonalizing Their Semantic Spaces
- **Arxiv ID**: http://arxiv.org/abs/1609.04116v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.04116v1)
- **Published**: 2016-09-14 02:45:37+00:00
- **Updated**: 2016-09-14 02:45:37+00:00
- **Authors**: Qing Tian, Songcan Chen
- **Comment**: None
- **Journal**: None
- **Summary**: In human face-based biometrics, gender classification and age estimation are two typical learning tasks. Although a variety of approaches have been proposed to handle them, just a few of them are solved jointly, even so, these joint methods do not yet specifically concern the semantic difference between human gender and age, which is intuitively helpful for joint learning, consequently leaving us a room of further improving the performance. To this end, in this work we firstly propose a general learning framework for jointly estimating human gender and age by specially attempting to formulate such semantic relationships as a form of near-orthogonality regularization and then incorporate it into the objective of the joint learning framework. In order to evaluate the effectiveness of the proposed framework, we exemplify it by respectively taking the widely used binary-class SVM for gender classification, and two threshold-based ordinal regression methods (i.e., the discriminant learning for ordinal regression and support vector ordinal regression) for age estimation, and crucially coupling both through the proposed semantic formulation. Moreover, we develop its kernelized nonlinear counterpart by deriving a representer theorem for the joint learning strategy. Finally, through extensive experiments on three aging datasets FG-NET, Morph Album I and Morph Album II, we demonstrate the effectiveness and superiority of the proposed joint learning strategy.



### Proceedings of the third "international Traveling Workshop on Interactions between Sparse models and Technology" (iTWIST'16)
- **Arxiv ID**: http://arxiv.org/abs/1609.04167v1
- **DOI**: None
- **Categories**: **cs.NA**, cs.CV, cs.IT, cs.LG, math.IT, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1609.04167v1)
- **Published**: 2016-09-14 08:27:11+00:00
- **Updated**: 2016-09-14 08:27:11+00:00
- **Authors**: V. Abrol, O. Absil, P. -A. Absil, S. Anthoine, P. Antoine, T. Arildsen, N. Bertin, F. Bleichrodt, J. Bobin, A. Bol, A. Bonnefoy, F. Caltagirone, V. Cambareri, C. Chenot, V. Crnojević, M. Daňková, K. Degraux, J. Eisert, J. M. Fadili, M. Gabrié, N. Gac, D. Giacobello, A. Gonzalez, C. A. Gomez Gonzalez, A. González, P. -Y. Gousenbourger, M. Græsbøll Christensen, R. Gribonval, S. Guérit, S. Huang, P. Irofti, L. Jacques, U. S. Kamilov, S. Kiticć, M. Kliesch, F. Krzakala, J. A. Lee, W. Liao, T. Lindstrøm Jensen, A. Manoel, H. Mansour, A. Mohammad-Djafari, A. Moshtaghpour, F. Ngolè, B. Pairet, M. Panić, G. Peyré, A. Pižurica, P. Rajmic, M. Roblin, I. Roth, A. K. Sao, P. Sharma, J. -L. Starck, E. W. Tramel, T. van Waterschoot, D. Vukobratovic, L. Wang, B. Wirth, G. Wunder, H. Zhang
- **Comment**: 69 pages, 22 extended abstracts, iTWIST'16 website:
  http://www.itwist16.es.aau.dk
- **Journal**: None
- **Summary**: The third edition of the "international - Traveling Workshop on Interactions between Sparse models and Technology" (iTWIST) took place in Aalborg, the 4th largest city in Denmark situated beautifully in the northern part of the country, from the 24th to 26th of August 2016. The workshop venue was at the Aalborg University campus. One implicit objective of this biennial workshop is to foster collaboration between international scientific teams by disseminating ideas through both specific oral/poster presentations and free discussions. For this third edition, iTWIST'16 gathered about 50 international participants and features 8 invited talks, 12 oral presentations, and 12 posters on the following themes, all related to the theory, application and generalization of the "sparsity paradigm": Sparsity-driven data sensing and processing (e.g., optics, computer vision, genomics, biomedical, digital communication, channel estimation, astronomy); Application of sparse models in non-convex/non-linear inverse problems (e.g., phase retrieval, blind deconvolution, self calibration); Approximate probabilistic inference for sparse problems; Sparse machine learning and inference; "Blind" inverse problems and dictionary learning; Optimization for sparse modelling; Information theory, geometry and randomness; Sparsity? What's next? (Discrete-valued signals; Union of low-dimensional spaces, Cosparsity, mixed/group norm, model-based, low-complexity models, ...); Matrix/manifold sensing/processing (graph, low-rank approximation, ...); Complexity/accuracy tradeoffs in numerical methods/optimization; Electronic/optical compressive sensors (hardware).



### ContextLocNet: Context-Aware Deep Network Models for Weakly Supervised Localization
- **Arxiv ID**: http://arxiv.org/abs/1609.04331v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.04331v1)
- **Published**: 2016-09-14 16:17:03+00:00
- **Updated**: 2016-09-14 16:17:03+00:00
- **Authors**: Vadim Kantorov, Maxime Oquab, Minsu Cho, Ivan Laptev
- **Comment**: Accepted paper at ECCV2016. The website and code is at
  http://www.di.ens.fr/willow/research/contextlocnet
- **Journal**: None
- **Summary**: We aim to localize objects in images using image-level supervision only. Previous approaches to this problem mainly focus on discriminative object regions and often fail to locate precise object boundaries. We address this problem by introducing two types of context-aware guidance models, additive and contrastive models, that leverage their surrounding context regions to improve localization. The additive model encourages the predicted object region to be supported by its surrounding context region. The contrastive model encourages the predicted object region to be outstanding from its surrounding context region. Our approach benefits from the recent success of convolutional neural networks for object recognition and extends Fast R-CNN to weakly supervised object localization. Extensive experimental evaluation on the PASCAL VOC 2007 and 2012 benchmarks shows hat our context-aware approach significantly improves weakly supervised localization and detection.



### Quick and energy-efficient Bayesian computing of binocular disparity using stochastic digital signals
- **Arxiv ID**: http://arxiv.org/abs/1609.04337v2
- **DOI**: 10.1016/j.ijar.2016.11.004
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1609.04337v2)
- **Published**: 2016-09-14 16:41:31+00:00
- **Updated**: 2016-10-31 15:36:01+00:00
- **Authors**: Alexandre Coninx, Pierre Bessière, Jacques Droulez
- **Comment**: Preprint of article submitted for publication in International
  Journal of Approximate Reasoning and accepted pending minor revisions
- **Journal**: None
- **Summary**: Reconstruction of the tridimensional geometry of a visual scene using the binocular disparity information is an important issue in computer vision and mobile robotics, which can be formulated as a Bayesian inference problem. However, computation of the full disparity distribution with an advanced Bayesian model is usually an intractable problem, and proves computationally challenging even with a simple model. In this paper, we show how probabilistic hardware using distributed memory and alternate representation of data as stochastic bitstreams can solve that problem with high performance and energy efficiency. We put forward a way to express discrete probability distributions using stochastic data representations and perform Bayesian fusion using those representations, and show how that approach can be applied to diparity computation. We evaluate the system using a simulated stochastic implementation and discuss possible hardware implementations of such architectures and their potential for sensorimotor processing and robotics.



### Combining Texture and Shape Cues for Object Recognition With Minimal Supervision
- **Arxiv ID**: http://arxiv.org/abs/1609.04356v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.04356v1)
- **Published**: 2016-09-14 17:41:48+00:00
- **Updated**: 2016-09-14 17:41:48+00:00
- **Authors**: Xingchao Peng, Kate Saenko
- **Comment**: ACCV 2016
- **Journal**: None
- **Summary**: We present a novel approach to object classification and detection which requires minimal supervision and which combines visual texture cues and shape information learned from freely available unlabeled web search results. The explosion of visual data on the web can potentially make visual examples of almost any object easily accessible via web search. Previous unsupervised methods have utilized either large scale sources of texture cues from the web, or shape information from data such as crowdsourced CAD models. We propose a two-stream deep learning framework that combines these cues, with one stream learning visual texture cues from image search data, and the other stream learning rich shape information from 3D CAD models. To perform classification or detection for a novel image, the predictions of the two streams are combined using a late fusion scheme. We present experiments and visualizations for both tasks on the standard benchmark PASCAL VOC 2007 to demonstrate that texture and shape provide complementary information in our model. Our method outperforms previous web image based models, 3D CAD model based approaches, and weakly supervised models.



### Warped Convolutions: Efficient Invariance to Spatial Transformations
- **Arxiv ID**: http://arxiv.org/abs/1609.04382v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.04382v5)
- **Published**: 2016-09-14 19:10:14+00:00
- **Updated**: 2021-11-30 21:14:07+00:00
- **Authors**: João F. Henriques, Andrea Vedaldi
- **Comment**: Proceedings of the 34th International Conference on Machine Learning,
  Sydney, Australia, PMLR 70, 2017
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are extremely efficient, since they exploit the inherent translation-invariance of natural images. However, translation is just one of a myriad of useful spatial transformations. Can the same efficiency be attained when considering other spatial invariances? Such generalized convolutions have been considered in the past, but at a high computational cost. We present a construction that is simple and exact, yet has the same computational complexity that standard convolutions enjoy. It consists of a constant image warp followed by a simple convolution, which are standard blocks in deep learning toolboxes. With a carefully crafted warp, the resulting architecture can be made equivariant to a wide range of two-parameter spatial transformations. We show encouraging results in realistic scenarios, including the estimation of vehicle poses in the Google Earth dataset (rotation and scale), and face poses in Annotated Facial Landmarks in the Wild (3D rotations under perspective).



### 3D Face Reconstruction by Learning from Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/1609.04387v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.04387v2)
- **Published**: 2016-09-14 19:47:12+00:00
- **Updated**: 2016-09-26 12:12:34+00:00
- **Authors**: Elad Richardson, Matan Sela, Ron Kimmel
- **Comment**: The first two authors contributed equally to this work
- **Journal**: None
- **Summary**: Fast and robust three-dimensional reconstruction of facial geometric structure from a single image is a challenging task with numerous applications. Here, we introduce a learning-based approach for reconstructing a three-dimensional face from a single image. Recent face recovery methods rely on accurate localization of key characteristic points. In contrast, the proposed approach is based on a Convolutional-Neural-Network (CNN) which extracts the face geometry directly from its image. Although such deep architectures outperform other models in complex computer vision problems, training them properly requires a large dataset of annotated examples. In the case of three-dimensional faces, currently, there are no large volume data sets, while acquiring such big-data is a tedious task. As an alternative, we propose to generate random, yet nearly photo-realistic, facial images for which the geometric form is known. The suggested model successfully recovers facial shapes from real images, even for faces with extreme expressions and under various lighting conditions.



### Learning Robust Features for Gait Recognition by Maximum Margin Criterion
- **Arxiv ID**: http://arxiv.org/abs/1609.04392v6
- **DOI**: 10.1109/ICPR.2016.7899750
- **Categories**: **cs.CV**, 68T05, 68T10, I.5
- **Links**: [PDF](http://arxiv.org/pdf/1609.04392v6)
- **Published**: 2016-09-14 19:52:10+00:00
- **Updated**: 2022-12-07 22:14:54+00:00
- **Authors**: Michal Balazia, Petr Sojka
- **Comment**: Preprint. Full paper published at the 23rd IEEE/IAPR International
  Conference on Pattern Recognition (ICPR), Cancun, Mexico, December 2016. 6
  pages
- **Journal**: None
- **Summary**: In the field of gait recognition from motion capture data, designing human-interpretable gait features is a common practice of many fellow researchers. To refrain from ad-hoc schemes and to find maximally discriminative features we may need to explore beyond the limits of human interpretability. This paper contributes to the state-of-the-art with a machine learning approach for extracting robust gait features directly from raw joint coordinates. The features are learned by a modification of Linear Discriminant Analysis with Maximum Margin Criterion so that the identities are maximally separated and, in combination with an appropriate classifier, used for gait recognition. Experiments on the CMU MoCap database show that this method outperforms eight other relevant methods in terms of the distribution of biometric templates in respective feature spaces expressed in four class separability coefficients. Additional experiments indicate that this method is a leading concept for rank-based classifier systems.



### A Large Contextual Dataset for Classification, Detection and Counting of Cars with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1609.04453v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1609.04453v1)
- **Published**: 2016-09-14 21:44:58+00:00
- **Updated**: 2016-09-14 21:44:58+00:00
- **Authors**: T. Nathan Mundhenk, Goran Konjevod, Wesam A. Sakla, Kofi Boakye
- **Comment**: ECCV 2016 Pre-press revision
- **Journal**: None
- **Summary**: We have created a large diverse set of cars from overhead images, which are useful for training a deep learner to binary classify, detect and count them. The dataset and all related material will be made publically available. The set contains contextual matter to aid in identification of difficult targets. We demonstrate classification and detection on this dataset using a neural network we call ResCeption. This network combines residual learning with Inception-style layers and is used to count cars in one look. This is a new way to count objects rather than by localization or density estimation. It is fairly accurate, fast and easy to implement. Additionally, the counting method is not car or scene specific. It would be easy to train this method to count other kinds of objects and counting over new scenes requires no extra set up or assumptions about object locations.



