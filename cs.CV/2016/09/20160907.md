# Arxiv Papers in cs.CV on 2016-09-07
### A Boosting Method to Face Image Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/1609.01805v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.01805v3)
- **Published**: 2016-09-07 02:12:30+00:00
- **Updated**: 2018-05-04 09:12:13+00:00
- **Authors**: Shanjun Mao, Da Zhou, Yiping Zhang, Zhihong Zhang, Jingjing Cao
- **Comment**: 14 pages, 3 figure
- **Journal**: None
- **Summary**: Recently sparse representation has gained great success in face image super-resolution. The conventional sparsity-based methods enforce sparse coding on face image patches and the representation fidelity is measured by $\ell_{2}$-norm. Such a sparse coding model regularizes all facial patches equally, which however ignores distinct natures of different facial patches for image reconstruction. In this paper, we propose a new weighted-patch super-resolution method based on AdaBoost. Specifically, in each iteration of the AdaBoost operation, each facial patch is weighted automatically according to the performance of the model on it, so as to highlight those patches that are more critical for improving the reconstruction power in next step. In this way, through the AdaBoost training procedure, we can focus more on the patches (face regions) with richer information. Various experimental results on standard face database show that our proposed method outperforms state-of-the-art methods in terms of both objective metrics and visual quality.



### Tracking Algorithm for Microscopic Flow Data Collection
- **Arxiv ID**: http://arxiv.org/abs/1609.02137v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1609.02137v1)
- **Published**: 2016-09-07 02:27:52+00:00
- **Updated**: 2016-09-07 02:27:52+00:00
- **Authors**: Kardi Teknomo, Yasushi Takeyama, Hajime Inamura
- **Comment**: 2 pages, Teknomo, Kardi; Takeyama, Yasushi; Inamura, Hajime, Tracking
  Algorithm for Microscopic Flow Data Collection, Proceeding of JSCE
  Conference, Sendai, Japan Sept 2000
- **Journal**: None
- **Summary**: Various methods to automate traffic data collection have recently been developed by many researchers. A macroscopic data collection through image processing has been proposed. For microscopic traffic flow data, such as individual speed and time or distance headway, tracking of individual movement is needed. The tracking algorithms for pedestrian or vehicle have been developed to trace the movement of one or two pedestrians based on sign pattern, and feature detection. No research has been done to track many pedestrians or vehicles at once. This paper describes a new and fast algorithm to track the movement of many individual vehicles or pedestrians



### Tracking System to Automate Data Collection of Microscopic Pedestrian Traffic Flow
- **Arxiv ID**: http://arxiv.org/abs/1609.01810v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/1609.01810v1)
- **Published**: 2016-09-07 02:58:43+00:00
- **Updated**: 2016-09-07 02:58:43+00:00
- **Authors**: Kardi Teknomo, Yasushi Takeyama, Hajime Inamura
- **Comment**: 15 pages, Teknomo, Kardi; Takeyama, Yasushi; Inamura, Hajime,
  Tracking System to Automate Data Collection of Microscopic Pedestrian Traffic
  Flow, Proceeding of The 4th Eastern Asia Society For Transportation Studies,
  Hanoi, Vietnam, pp. 11-25, Oct. 2001
- **Journal**: None
- **Summary**: To deal with many pedestrian data, automatic data collection is needed. This paper describes how to automate the microscopic pedestrian flow data collection from video files. The study is restricted only to pedestrians without considering vehicular - pedestrian interaction. Pedestrian tracking system consists of three sub-systems, which calculates the image processing, object tracking and traffic flow variables. The system receives input of stacks of images and parameters. The first sub-system performs Image Processing analysis while the second sub-system carries out the tracking of pedestrians by matching the features and tracing the pedestrian numbers frame by frame. The last sub-system deals with a NTXY database to calculate the pedestrian traffic-flow characteristic such as flow rate, speed and area module. Comparison with manual data collection method confirmed that the procedures described have significant potential to automate the data collection of both microscopic and macroscopic pedestrian flow variables.



### Semantic Video Trailers
- **Arxiv ID**: http://arxiv.org/abs/1609.01819v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1609.01819v1)
- **Published**: 2016-09-07 03:35:54+00:00
- **Updated**: 2016-09-07 03:35:54+00:00
- **Authors**: Harrie Oosterhuis, Sujith Ravi, Michael Bendersky
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Query-based video summarization is the task of creating a brief visual trailer, which captures the parts of the video (or a collection of videos) that are most relevant to the user-issued query. In this paper, we propose an unsupervised label propagation approach for this task. Our approach effectively captures the multimodal semantics of queries and videos using state-of-the-art deep neural networks and creates a summary that is both semantically coherent and visually attractive. We describe the theoretical framework of our graph-based approach and empirically evaluate its effectiveness in creating relevant and attractive trailers. Finally, we showcase example video trailers generated by our system.



### Guided Filter based Edge-preserving Image Non-blind Deconvolution
- **Arxiv ID**: http://arxiv.org/abs/1609.01839v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.01839v1)
- **Published**: 2016-09-07 05:16:35+00:00
- **Updated**: 2016-09-07 05:16:35+00:00
- **Authors**: Hang Yang, Ming Zhu, Zhongbo Zhang, Heyan Huang
- **Comment**: 4 pages, 3 figures, ICIP 2013. arXiv admin note: text overlap with
  arXiv:1609.01380
- **Journal**: The 20th IEEE International Conference on Image Processing
  (ICIP),2013,4593--4596
- **Summary**: In this work, we propose a new approach for efficient edge-preserving image deconvolution. Our algorithm is based on a novel type of explicit image filter - guided filter. The guided filter can be used as an edge-preserving smoothing operator like the popular bilateral filter, but has better behaviors near edges. We propose an efficient iterative algorithm with the decouple of deblurring and denoising steps in the restoration process. In deblurring step, we proposed two cost function which could be computed with fast Fourier transform efficiently. The solution of the first one is used as the guidance image, and another solution will be filtered in next step. In the denoising step, the guided filter is used with the two obtained images for efficient edge-preserving filtering. Furthermore, we derive a simple and effective method to automatically adjust the regularization parameter at each iteration. We compare our deconvolution algorithm with many competitive deconvolution techniques in terms of ISNR and visual quality.



### Automatic Visual Theme Discovery from Joint Image and Text Corpora
- **Arxiv ID**: http://arxiv.org/abs/1609.01859v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.01859v1)
- **Published**: 2016-09-07 07:22:09+00:00
- **Updated**: 2016-09-07 07:22:09+00:00
- **Authors**: Ke Sun, Xianxu Hou, Qian Zhang, Guoping Qiu
- **Comment**: None
- **Journal**: None
- **Summary**: A popular approach to semantic image understanding is to manually tag images with keywords and then learn a mapping from vi- sual features to keywords. Manually tagging images is a subjective pro- cess and the same or very similar visual contents are often tagged with different keywords. Furthermore, not all tags have the same descriptive power for visual contents and large vocabulary available from natural language could result in a very diverse set of keywords. In this paper, we propose an unsupervised visual theme discovery framework as a better (more compact, efficient and effective) alternative to semantic represen- tation of visual contents. We first show that tag based annotation lacks consistency and compactness for describing visually similar contents. We then learn the visual similarity between tags based on the visual features of the images containing the tags. At the same time, we use a natural language processing technique (word embedding) to measure the seman- tic similarity between tags. Finally, we cluster tags into visual themes based on their visual similarity and semantic similarity measures using a spectral clustering algorithm. We conduct user studies to evaluate the effectiveness and rationality of the visual themes discovered by our unsu- pervised algorithm and obtains promising result. We then design three common computer vision tasks, example based image search, keyword based image search and image labelling to explore potential applica- tion of our visual themes discovery framework. In experiments, visual themes significantly outperforms tags on semantic image understand- ing and achieve state-of-art performance in all three tasks. This again demonstrate the effectiveness and versatility of proposed framework.



### Polysemous codes
- **Arxiv ID**: http://arxiv.org/abs/1609.01882v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.DB, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1609.01882v2)
- **Published**: 2016-09-07 08:45:19+00:00
- **Updated**: 2016-10-10 23:00:00+00:00
- **Authors**: Matthijs Douze, Hervé Jégou, Florent Perronnin
- **Comment**: The final (author) version of our ECCV'16 paper
- **Journal**: None
- **Summary**: This paper considers the problem of approximate nearest neighbor search in the compressed domain. We introduce polysemous codes, which offer both the distance estimation quality of product quantization and the efficient comparison of binary codes with Hamming distance. Their design is inspired by algorithms introduced in the 90's to construct channel-optimized vector quantizers. At search time, this dual interpretation accelerates the search. Most of the indexed vectors are filtered out with Hamming distance, letting only a fraction of the vectors to be ranked with an asymmetric distance estimator.   The method is complementary with a coarse partitioning of the feature space such as the inverted multi-index. This is shown by our experiments performed on several public benchmarks such as the BIGANN dataset comprising one billion vectors, for which we report state-of-the-art results for query times below 0.3\,millisecond per core. Last but not least, our approach allows the approximate computation of the k-NN graph associated with the Yahoo Flickr Creative Commons 100M, described by CNN image descriptors, in less than 8 hours on a single machine.



### DAiSEE: Towards User Engagement Recognition in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1609.01885v7
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1609.01885v7)
- **Published**: 2016-09-07 08:50:11+00:00
- **Updated**: 2022-07-07 12:16:48+00:00
- **Authors**: Abhay Gupta, Arjun D'Cunha, Kamal Awasthi, Vineeth Balasubramanian
- **Comment**: 12 pages, 14 figures, 5 tables
- **Journal**: None
- **Summary**: We introduce DAiSEE, the first multi-label video classification dataset comprising of 9068 video snippets captured from 112 users for recognizing the user affective states of boredom, confusion, engagement, and frustration in the wild. The dataset has four levels of labels namely - very low, low, high, and very high for each of the affective states, which are crowd annotated and correlated with a gold standard annotation created using a team of expert psychologists. We have also established benchmark results on this dataset using state-of-the-art video classification methods that are available today. We believe that DAiSEE will provide the research community with challenges in feature extraction, context-based inference, and development of suitable machine learning methods for related tasks, thus providing a springboard for further research. The dataset is available for download at https://people.iith.ac.in/vineethnb/resources/daisee/index.html.



### Polyp Detection and Segmentation from Video Capsule Endoscopy: A Review
- **Arxiv ID**: http://arxiv.org/abs/1609.01915v1
- **DOI**: 10.3390/jimaging3010001
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.01915v1)
- **Published**: 2016-09-07 10:09:06+00:00
- **Updated**: 2016-09-07 10:09:06+00:00
- **Authors**: V. B. Surya Prasath
- **Comment**: Project webpage: http://goo.gl/eAUWKJ - Complete Bibliography -
  Compiled and Continuously Updated
- **Journal**: Journal of Imaging, 3(1), 1, 2017
- **Summary**: Video capsule endoscopy (VCE) is used widely nowadays for visualizing the gastrointestinal (GI) tract. Capsule endoscopy exams are prescribed usually as an additional monitoring mechanism and can help in identifying polyps, bleeding, etc. To analyze the large scale video data produced by VCE exams automatic image processing, computer vision, and learning algorithms are required. Recently, automatic polyp detection algorithms have been proposed with various degrees of success. Though polyp detection in colonoscopy and other traditional endoscopy procedure based images is becoming a mature field, due to its unique imaging characteristics detecting polyps automatically in VCE is a hard problem. We review different polyp detection approaches for VCE imagery and provide systematic analysis with challenges faced by standard image processing and computer vision methods.



### A three-dimensional approach to Visual Speech Recognition using Discrete Cosine Transforms
- **Arxiv ID**: http://arxiv.org/abs/1609.01932v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.01932v1)
- **Published**: 2016-09-07 10:59:19+00:00
- **Updated**: 2016-09-07 10:59:19+00:00
- **Authors**: Toni Heidenreich, Michael W. Spratling
- **Comment**: Revised and shortened version of the Master's thesis of the author
  (King's College London, 2013)
- **Journal**: None
- **Summary**: Visual speech recognition aims to identify the sequence of phonemes from continuous speech. Unlike the traditional approach of using 2D image feature extraction methods to derive features of each video frame separately, this paper proposes a new approach using a 3D (spatio-temporal) Discrete Cosine Transform to extract features of each feasible sub-sequence of an input video which are subsequently classified individually using Support Vector Machines and combined to find the most likely phoneme sequence using a tailor-made Hidden Markov Model. The algorithm is trained and tested on the VidTimit database to recognise sequences of phonemes as well as visemes (visual speech units). Furthermore, the system is extended with the training on phoneme or viseme pairs (biphones) to counteract the human speech ambiguity of co-articulation. The test set accuracy for the recognition of phoneme sequences is 20%, and the accuracy of viseme sequences is 39%. Both results improve the best values reported in other papers by approximately 2%. The contribution of the result is three-fold: Firstly, this paper is the first to show that 3D feature extraction methods can be applied to continuous sequence recognition tasks despite the unknown start positions and durations of each phoneme. Secondly, the result confirms that 3D feature extraction methods improve the accuracy compared to 2D features extraction methods. Thirdly, the paper is the first to specifically compare an otherwise identical method with and without using biphones, verifying that the usage of biphones has a positive impact on the result.



### Object Tracking via Dynamic Feature Selection Processes
- **Arxiv ID**: http://arxiv.org/abs/1609.01958v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.01958v1)
- **Published**: 2016-09-07 12:27:11+00:00
- **Updated**: 2016-09-07 12:27:11+00:00
- **Authors**: Giorgio Roffo, Simone Melzi
- **Comment**: The paper will appear in the USB ECCV workshops proceedings and on
  the IEEE Xplore. The results will be presented at VOT2016 workshop which will
  take place on 10.12 at ECCV2016. In the days following the workshop, the raw
  results of the submitted trackers as well as the results paper will be made
  publicly available from the VOT homepage
- **Journal**: None
- **Summary**: DFST proposes an optimized visual tracking algorithm based on the real-time selection of locally and temporally discriminative features. A feature selection mechanism is embedded in the Adaptive colour Names (CN) tracking system that adaptively selects the top-ranked discriminative features for tracking. DFST provides a significant gain in accuracy and precision allowing the use of a dynamic set of features that results in an increased system flexibility. DFST is based on the unsupervised method "Infinite Feature Selection" (Inf-FS), which ranks features according with their "redundancy" without using class labels. By using a fast online algorithm for learning dictionaries the size of the box is adapted during the processing. At each update, we use multiple examples around the target (at different positions and scales). DFST also improved the CN by adding micro-shift at the predicted position and bounding box adaptation.



### Human Body Orientation Estimation using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1609.01984v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1609.01984v1)
- **Published**: 2016-09-07 13:53:26+00:00
- **Updated**: 2016-09-07 13:53:26+00:00
- **Authors**: Jinyoung Choi, Beom-Jin Lee, Byoung-Tak Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Personal robots are expected to interact with the user by recognizing the user's face. However, in most of the service robot applications, the user needs to move himself/herself to allow the robot to see him/her face to face. To overcome such limitations, a method for estimating human body orientation is required. Previous studies used various components such as feature extractors and classification models to classify the orientation which resulted in low performance. For a more robust and accurate approach, we propose the light weight convolutional neural networks, an end to end system, for estimating human body orientation. Our body orientation estimation model achieved 81.58% and 94% accuracy with the benchmark dataset and our own dataset respectively. The proposed method can be used in a wide range of service robot applications which depend on the ability to estimate human body orientation. To show its usefulness in service robot applications, we designed a simple robot application which allows the robot to move towards the user's frontal plane. With this, we demonstrated an improved face detection rate.



### Dense Motion Estimation for Smoke
- **Arxiv ID**: http://arxiv.org/abs/1609.02001v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.02001v2)
- **Published**: 2016-09-07 14:40:08+00:00
- **Updated**: 2016-09-08 14:19:43+00:00
- **Authors**: Da Chen, Wenbin Li, Peter Hall
- **Comment**: ACCV2016
- **Journal**: None
- **Summary**: Motion estimation for highly dynamic phenomena such as smoke is an open challenge for Computer Vision. Traditional dense motion estimation algorithms have difficulties with non-rigid and large motions, both of which are frequently observed in smoke motion. We propose an algorithm for dense motion estimation of smoke. Our algorithm is robust, fast, and has better performance over different types of smoke compared to other dense motion estimation algorithms, including state of the art and neural network approaches. The key to our contribution is to use skeletal flow, without explicit point matching, to provide a sparse flow. This sparse flow is upgraded to a dense flow. In this paper we describe our algorithm in greater detail, and provide experimental evidence to support our claims.



### Component-Based Distributed Framework for Coherent and Real-Time Video Dehazing
- **Arxiv ID**: http://arxiv.org/abs/1609.02035v2
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1609.02035v2)
- **Published**: 2016-09-07 15:56:09+00:00
- **Updated**: 2016-09-09 16:14:01+00:00
- **Authors**: Meihua Wang, Jiaming Mai, Yun Liang, Tom Z. J. Fu, Zhenjie Zhang, Ruichu Cai
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional dehazing techniques, as a well studied topic in image processing, are now widely used to eliminate the haze effects from individual images. However, even the state-of-the-art dehazing algorithms may not provide sufficient support to video analytics, as a crucial pre-processing step for video-based decision making systems (e.g., robot navigation), due to the limitations of these algorithms on poor result coherence and low processing efficiency. This paper presents a new framework, particularly designed for video dehazing, to output coherent results in real time, with two novel techniques. Firstly, we decompose the dehazing algorithms into three generic components, namely transmission map estimator, atmospheric light estimator and haze-free image generator. They can be simultaneously processed by multiple threads in the distributed system, such that the processing efficiency is optimized by automatic CPU resource allocation based on the workloads. Secondly, a cross-frame normalization scheme is proposed to enhance the coherence among consecutive frames, by sharing the parameters of atmospheric light from consecutive frames in the distributed computation platform. The combination of these techniques enables our framework to generate highly consistent and accurate dehazing results in real-time, by using only 3 PCs connected by Ethernet.



### Deep Markov Random Field for Image Modeling
- **Arxiv ID**: http://arxiv.org/abs/1609.02036v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1609.02036v1)
- **Published**: 2016-09-07 15:56:36+00:00
- **Updated**: 2016-09-07 15:56:36+00:00
- **Authors**: Zhirong Wu, Dahua Lin, Xiaoou Tang
- **Comment**: Accepted at ECCV 2016
- **Journal**: None
- **Summary**: Markov Random Fields (MRFs), a formulation widely used in generative image modeling, have long been plagued by the lack of expressive power. This issue is primarily due to the fact that conventional MRFs formulations tend to use simplistic factors to capture local patterns. In this paper, we move beyond such limitations, and propose a novel MRF model that uses fully-connected neurons to express the complex interactions among pixels. Through theoretical analysis, we reveal an inherent connection between this model and recurrent neural networks, and thereon derive an approximated feed-forward network that couples multiple RNNs along opposite directions. This formulation combines the expressive power of deep neural networks and the cyclic dependency structure of MRF in a unified model, bringing the modeling capability to a new level. The feed-forward approximation also allows it to be efficiently learned from data. Experimental results on a variety of low-level vision tasks show notable improvement over state-of-the-arts.



### Visual Saliency Detection Based on Multiscale Deep CNN Features
- **Arxiv ID**: http://arxiv.org/abs/1609.02077v1
- **DOI**: 10.1109/TIP.2016.2602079
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.02077v1)
- **Published**: 2016-09-07 17:13:16+00:00
- **Updated**: 2016-09-07 17:13:16+00:00
- **Authors**: Guanbin Li, Yizhou Yu
- **Comment**: Accepted for publication in IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Visual saliency is a fundamental problem in both cognitive and computational sciences, including computer vision. In this paper, we discover that a high-quality visual saliency model can be learned from multiscale features extracted using deep convolutional neural networks (CNNs), which have had many successes in visual recognition tasks. For learning such saliency models, we introduce a neural network architecture, which has fully connected layers on top of CNNs responsible for feature extraction at three different scales. The penultimate layer of our neural network has been confirmed to be a discriminative high-level feature vector for saliency detection, which we call deep contrast feature. To generate a more robust feature, we integrate handcrafted low-level features with our deep contrast feature. To promote further research and evaluation of visual saliency models, we also construct a new large database of 4447 challenging images and their pixelwise saliency annotations. Experimental results demonstrate that our proposed method is capable of achieving state-of-the-art performance on all public benchmarks, improving the F- measure by 6.12% and 10.0% respectively on the DUT-OMRON dataset and our new dataset (HKU-IS), and lowering the mean absolute error by 9% and 35.3% respectively on these two datasets.



### Clearing the Skies: A deep network architecture for single-image rain removal
- **Arxiv ID**: http://arxiv.org/abs/1609.02087v2
- **DOI**: 10.1109/TIP.2017.2691802
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.02087v2)
- **Published**: 2016-09-07 17:35:17+00:00
- **Updated**: 2017-02-06 19:53:29+00:00
- **Authors**: Xueyang Fu, Jiabin Huang, Xinghao Ding, Yinghao Liao, John Paisley
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a deep network architecture called DerainNet for removing rain streaks from an image. Based on the deep convolutional neural network (CNN), we directly learn the mapping relationship between rainy and clean image detail layers from data. Because we do not possess the ground truth corresponding to real-world rainy images, we synthesize images with rain for training. In contrast to other common strategies that increase depth or breadth of the network, we use image processing domain knowledge to modify the objective function and improve deraining with a modestly-sized CNN. Specifically, we train our DerainNet on the detail (high-pass) layer rather than in the image domain. Though DerainNet is trained on synthetic data, we find that the learned network translates very effectively to real-world images for testing. Moreover, we augment the CNN framework with image enhancement to improve the visual results. Compared with state-of-the-art single image de-raining methods, our method has improved rain removal and much faster computation time after network training.



### UberNet: Training a `Universal' Convolutional Neural Network for Low-, Mid-, and High-Level Vision using Diverse Datasets and Limited Memory
- **Arxiv ID**: http://arxiv.org/abs/1609.02132v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1609.02132v1)
- **Published**: 2016-09-07 19:35:30+00:00
- **Updated**: 2016-09-07 19:35:30+00:00
- **Authors**: Iasonas Kokkinos
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we introduce a convolutional neural network (CNN) that jointly handles low-, mid-, and high-level vision tasks in a unified architecture that is trained end-to-end. Such a universal network can act like a `swiss knife' for vision tasks; we call this architecture an UberNet to indicate its overarching nature.   We address two main technical challenges that emerge when broadening up the range of tasks handled by a single CNN: (i) training a deep architecture while relying on diverse training sets and (ii) training many (potentially unlimited) tasks with a limited memory budget. Properly addressing these two problems allows us to train accurate predictors for a host of tasks, without compromising accuracy.   Through these advances we train in an end-to-end manner a CNN that simultaneously addresses (a) boundary detection (b) normal estimation (c) saliency estimation (d) semantic segmentation (e) human part segmentation (f) semantic boundary detection, (g) region proposal generation and object detection. We obtain competitive performance while jointly addressing all of these tasks in 0.7 seconds per frame on a single GPU. A demonstration of this system can be found at http://cvn.ecp.fr/ubernet/.



### Optimizing Codes for Source Separation in Color Image Demosaicing and Compressive Video Recovery
- **Arxiv ID**: http://arxiv.org/abs/1609.02135v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.02135v2)
- **Published**: 2016-09-07 19:56:44+00:00
- **Updated**: 2017-07-11 17:47:04+00:00
- **Authors**: Alankar Kotwal, Ajit Rajwade
- **Comment**: None
- **Journal**: None
- **Summary**: There exist several applications in image processing (eg: video compressed sensing [Hitomi, Y. et al, "Video from a single coded exposure photograph using a learned overcomplete dictionary"] and color image demosaicing [Moghadam, A. A. et al, "Compressive Framework for Demosaicing of Natural Images"]) which require separation of constituent images given measurements in the form of a coded superposition of those images. Physically practical code patterns in these applications are non-negative, systematically structured, and do not always obey the nice incoherence properties of other patterns such as Gaussian codes, which can adversely affect reconstruction performance. The contribution of this paper is to design code patterns for video compressed sensing and demosaicing by minimizing the mutual coherence of the matrix $\boldsymbol{\Phi \Psi}$ where $\boldsymbol{\Phi}$ represents the sensing matrix created from the code, and $\boldsymbol{\Psi}$ is the signal representation matrix. Our main contribution is that we explicitly take into account the special structure of those code patterns as required by these applications: (1)~non-negativity, (2)~block-diagonal nature, and (3)~circular shifting. In particular, the last property enables for accurate and seamless patch-wise reconstruction for some important compressed sensing architectures.



### Automated Segmentation of Retinal Layers from Optical Coherent Tomography Images Using Geodesic Distance
- **Arxiv ID**: http://arxiv.org/abs/1609.02214v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.02214v1)
- **Published**: 2016-09-07 22:58:21+00:00
- **Updated**: 2016-09-07 22:58:21+00:00
- **Authors**: Jinming Duan, Christopher Tench, Irene Gottlob, Frank Proudlock, Li Bai
- **Comment**: 20 pages, 17 figures
- **Journal**: None
- **Summary**: Optical coherence tomography (OCT) is a non-invasive imaging technique that can produce images of the eye at the microscopic level. OCT image segmentation to localise retinal layer boundaries is a fundamental procedure for diagnosing and monitoring the progression of retinal and optical nerve disorders. In this paper, we introduce a novel and accurate geodesic distance method (GDM) for OCT segmentation of both healthy and pathological images in either two- or three-dimensional spaces. The method uses a weighted geodesic distance by an exponential function, taking into account both horizontal and vertical intensity variations. The weighted geodesic distance is efficiently calculated from an Eikonal equation via the fast sweeping method. The segmentation is then realised by solving an ordinary differential equation with the geodesic distance. The results of the GDM are compared with manually segmented retinal layer boundaries/surfaces. Extensive experiments demonstrate that the proposed GDM is robust to complex retinal structures with large curvatures and irregularities and it outperforms the parametric active contour algorithm as well as the graph theoretic based approaches for delineating the retinal layers in both healthy and pathological images.



