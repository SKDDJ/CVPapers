# Arxiv Papers in cs.CV on 2016-09-23
### EFANNA : An Extremely Fast Approximate Nearest Neighbor Search Algorithm Based on kNN Graph
- **Arxiv ID**: http://arxiv.org/abs/1609.07228v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.07228v3)
- **Published**: 2016-09-23 04:34:54+00:00
- **Updated**: 2016-12-03 06:31:31+00:00
- **Authors**: Cong Fu, Deng Cai
- **Comment**: None
- **Journal**: None
- **Summary**: Approximate nearest neighbor (ANN) search is a fundamental problem in many areas of data mining, machine learning and computer vision. The performance of traditional hierarchical structure (tree) based methods decreases as the dimensionality of data grows, while hashing based methods usually lack efficiency in practice. Recently, the graph based methods have drawn considerable attention. The main idea is that \emph{a neighbor of a neighbor is also likely to be a neighbor}, which we refer as \emph{NN-expansion}. These methods construct a $k$-nearest neighbor ($k$NN) graph offline. And at online search stage, these methods find candidate neighbors of a query point in some way (\eg, random selection), and then check the neighbors of these candidate neighbors for closer ones iteratively. Despite some promising results, there are mainly two problems with these approaches: 1) These approaches tend to converge to local optima. 2) Constructing a $k$NN graph is time consuming. We find that these two problems can be nicely solved when we provide a good initialization for NN-expansion. In this paper, we propose EFANNA, an extremely fast approximate nearest neighbor search algorithm based on $k$NN Graph. Efanna nicely combines the advantages of hierarchical structure based methods and nearest-neighbor-graph based methods. Extensive experiments have shown that EFANNA outperforms the state-of-art algorithms both on approximate nearest neighbor search and approximate nearest neighbor graph construction. To the best of our knowledge, EFANNA is the fastest algorithm so far both on approximate nearest neighbor graph construction and approximate nearest neighbor search. A library EFANNA based on this research is released on Github.



### Funnel-Structured Cascade for Multi-View Face Detection with Alignment-Awareness
- **Arxiv ID**: http://arxiv.org/abs/1609.07304v1
- **DOI**: 10.1016/j.neucom.2016.09.072
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.07304v1)
- **Published**: 2016-09-23 10:43:02+00:00
- **Updated**: 2016-09-23 10:43:02+00:00
- **Authors**: Shuzhe Wu, Meina Kan, Zhenliang He, Shiguang Shan, Xilin Chen
- **Comment**: Submitted to Neurocomputing (under review). An adapted open source
  implementation can be found at
  https://github.com/seetaface/SeetaFaceEngine/tree/master/FaceDetection
- **Journal**: None
- **Summary**: Multi-view face detection in open environment is a challenging task due to diverse variations of face appearances and shapes. Most multi-view face detectors depend on multiple models and organize them in parallel, pyramid or tree structure, which compromise between the accuracy and time-cost. Aiming at a more favorable multi-view face detector, we propose a novel funnel-structured cascade (FuSt) detection framework. In a coarse-to-fine flavor, our FuSt consists of, from top to bottom, 1) multiple view-specific fast LAB cascade for extremely quick face proposal, 2) multiple coarse MLP cascade for further candidate window verification, and 3) a unified fine MLP cascade with shape-indexed features for accurate face detection. Compared with other structures, on the one hand, the proposed one uses multiple computationally efficient distributed classifiers to propose a small number of candidate windows but with a high recall of multi-view faces. On the other hand, by using a unified MLP cascade to examine proposals of all views in a centralized style, it provides a favorable solution for multi-view face detection with high accuracy and low time-cost. Besides, the FuSt detector is alignment-aware and performs a coarse facial part prediction which is beneficial for subsequent face alignment. Extensive experiments on two challenging datasets, FDDB and AFW, demonstrate the effectiveness of our FuSt detector in both accuracy and speed.



### EgoCap: Egocentric Marker-less Motion Capture with Two Fisheye Cameras
- **Arxiv ID**: http://arxiv.org/abs/1609.07306v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.07306v1)
- **Published**: 2016-09-23 10:46:19+00:00
- **Updated**: 2016-09-23 10:46:19+00:00
- **Authors**: Helge Rhodin, Christian Richardt, Dan Casas, Eldar Insafutdinov, Mohammad Shafiei, Hans-Peter Seidel, Bernt Schiele, Christian Theobalt
- **Comment**: SIGGRAPH Asia 2016
- **Journal**: None
- **Summary**: Marker-based and marker-less optical skeletal motion-capture methods use an outside-in arrangement of cameras placed around a scene, with viewpoints converging on the center. They often create discomfort by possibly needed marker suits, and their recording volume is severely restricted and often constrained to indoor scenes with controlled backgrounds. Alternative suit-based systems use several inertial measurement units or an exoskeleton to capture motion. This makes capturing independent of a confined volume, but requires substantial, often constraining, and hard to set up body instrumentation. We therefore propose a new method for real-time, marker-less and egocentric motion capture which estimates the full-body skeleton pose from a lightweight stereo pair of fisheye cameras that are attached to a helmet or virtual reality headset. It combines the strength of a new generative pose estimation framework for fisheye views with a ConvNet-based body-part detector trained on a large new dataset. Our inside-in method captures full-body motion in general indoor and outdoor scenes, and also crowded scenes with many people in close vicinity. The captured user can freely move around, which enables reconstruction of larger-scale activities and is particularly useful in virtual reality to freely roam and interact, while seeing the fully motion-captured virtual body.



### Example-Based Image Synthesis via Randomized Patch-Matching
- **Arxiv ID**: http://arxiv.org/abs/1609.07370v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.07370v1)
- **Published**: 2016-09-23 14:08:30+00:00
- **Updated**: 2016-09-23 14:08:30+00:00
- **Authors**: Yi Ren, Yaniv Romano, Michael Elad
- **Comment**: None
- **Journal**: None
- **Summary**: Image and texture synthesis is a challenging task that has long been drawing attention in the fields of image processing, graphics, and machine learning. This problem consists of modelling the desired type of images, either through training examples or via a parametric modeling, and then generating images that belong to the same statistical origin.   This work addresses the image synthesis task, focusing on two specific families of images -- handwritten digits and face images. This paper offers two main contributions. First, we suggest a simple and intuitive algorithm capable of generating such images in a unified way. The proposed approach taken is pyramidal, consisting of upscaling and refining the estimated image several times. For each upscaling stage, the algorithm randomly draws small patches from a patch database, and merges these to form a coherent and novel image with high visual quality. The second contribution is a general framework for the evaluation of the generation performance, which combines three aspects: the likelihood, the originality and the spread of the synthesized images. We assess the proposed synthesis scheme and show that the results are similar in nature, and yet different from the ones found in the training set, suggesting that true synthesis effect has been obtained.



### The face-space duality hypothesis: a computational model
- **Arxiv ID**: http://arxiv.org/abs/1609.07371v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.07371v1)
- **Published**: 2016-09-23 14:10:52+00:00
- **Updated**: 2016-09-23 14:10:52+00:00
- **Authors**: Jonathan Vitale, Mary-Anne Williams, Benjamin Johnston
- **Comment**: in 38th Annual Meeting of the Cognitive Science Society, 2016
- **Journal**: None
- **Summary**: Valentine's face-space suggests that faces are represented in a psychological multidimensional space according to their perceived properties. However, the proposed framework was initially designed as an account of invariant facial features only, and explanations for dynamic features representation were neglected. In this paper we propose, develop and evaluate a computational model for a twofold structure of the face-space, able to unify both identity and expression representations in a single implemented model. To capture both invariant and dynamic facial features we introduce the face-space duality hypothesis and subsequently validate it through a mathematical presentation using a general approach to dimensionality reduction. Two experiments with real facial images show that the proposed face-space: (1) supports both identity and expression recognition, and (2) has a twofold structure anticipated by our formal argument.



### Real-time Human Pose Estimation from Video with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1609.07420v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.07420v1)
- **Published**: 2016-09-23 16:22:59+00:00
- **Updated**: 2016-09-23 16:22:59+00:00
- **Authors**: Marko Linna, Juho Kannala, Esa Rahtu
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: In this paper, we present a method for real-time multi-person human pose estimation from video by utilizing convolutional neural networks. Our method is aimed for use case specific applications, where good accuracy is essential and variation of the background and poses is limited. This enables us to use a generic network architecture, which is both accurate and fast. We divide the problem into two phases: (1) pre-training and (2) finetuning. In pre-training, the network is learned with highly diverse input data from publicly available datasets, while in finetuning we train with application specific data, which we record with Kinect. Our method differs from most of the state-of-the-art methods in that we consider the whole system, including person detector, pose estimator and an automatic way to record application specific training material for finetuning. Our method is considerably faster than many of the state-of-the-art methods. Our method can be thought of as a replacement for Kinect, and it can be used for higher level tasks, such as gesture control, games, person tracking, action recognition and action tracking. We achieved accuracy of 96.8\% (PCK@0.2) with application specific data.



### A Rotation Invariant Latent Factor Model for Moveme Discovery from Static Poses
- **Arxiv ID**: http://arxiv.org/abs/1609.07495v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1609.07495v1)
- **Published**: 2016-09-23 20:00:23+00:00
- **Updated**: 2016-09-23 20:00:23+00:00
- **Authors**: Matteo Ruggero Ronchi, Joon Sik Kim, Yisong Yue
- **Comment**: Long version of the paper accepted at the IEEE ICDM 2016 conference.
  10 pages, 9 figures, 1 table. Project page:
  http://www.vision.caltech.edu/~mronchi/projects/RotationInvariantMovemes/
- **Journal**: None
- **Summary**: We tackle the problem of learning a rotation invariant latent factor model when the training data is comprised of lower-dimensional projections of the original feature space. The main goal is the discovery of a set of 3-D bases poses that can characterize the manifold of primitive human motions, or movemes, from a training set of 2-D projected poses obtained from still images taken at various camera angles. The proposed technique for basis discovery is data-driven rather than hand-designed. The learned representation is rotation invariant, and can reconstruct any training instance from multiple viewing angles. We apply our method to modeling human poses in sports (via the Leeds Sports Dataset), and demonstrate the effectiveness of the learned bases in a range of applications such as activity classification, inference of dynamics from a single frame, and synthetic representation of movements.



