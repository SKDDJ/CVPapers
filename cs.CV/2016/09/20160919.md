# Arxiv Papers in cs.CV on 2016-09-19
### Color: A Crucial Factor for Aesthetic Quality Assessment in a Subjective Dataset of Paintings
- **Arxiv ID**: http://arxiv.org/abs/1609.05583v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.05583v1)
- **Published**: 2016-09-19 02:17:34+00:00
- **Updated**: 2016-09-19 02:17:34+00:00
- **Authors**: Seyed Ali Amirshahi, Gregor Uwe Hayn-Leichsenring, Joachim Denzler, Christoph Redies
- **Comment**: This paper was presented at the AIC 2013 Congress
- **Journal**: None
- **Summary**: Computational aesthetics is an emerging field of research which has attracted different research groups in the last few years. In this field, one of the main approaches to evaluate the aesthetic quality of paintings and photographs is a feature-based approach. Among the different features proposed to reach this goal, color plays an import role. In this paper, we introduce a novel dataset that consists of paintings of Western provenance from 36 well-known painters from the 15th to the 20th century. As a first step and to assess this dataset, using a classifier, we investigate the correlation between the subjective scores and two widely used features that are related to color perception and in different aesthetic quality assessment approaches. Results show a classification rate of up to 73% between the color features and the subjective scores.



### Fast Single Shot Detection and Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1609.05590v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.05590v1)
- **Published**: 2016-09-19 03:38:15+00:00
- **Updated**: 2016-09-19 03:38:15+00:00
- **Authors**: Patrick Poirson, Phil Ammirato, Cheng-Yang Fu, Wei Liu, Jana Kosecka, Alexander C. Berg
- **Comment**: None
- **Journal**: None
- **Summary**: For applications in navigation and robotics, estimating the 3D pose of objects is as important as detection. Many approaches to pose estimation rely on detecting or tracking parts or keypoints [11, 21]. In this paper we build on a recent state-of-the-art convolutional network for slidingwindow detection [10] to provide detection and rough pose estimation in a single shot, without intermediate stages of detecting parts or initial bounding boxes. While not the first system to treat pose estimation as a categorization problem, this is the first attempt to combine detection and pose estimation at the same level using a deep learning approach. The key to the architecture is a deep convolutional network where scores for the presence of an object category, the offset for its location, and the approximate pose are all estimated on a regular grid of locations in the image. The resulting system is as accurate as recent work on pose estimation (42.4% 8 View mAVP on Pascal 3D+ [21] ) and significantly faster (46 frames per second (FPS) on a TITAN X GPU). This approach to detection and rough pose estimation is fast and accurate enough to be widely applied as a pre-processing step for tasks including high-accuracy pose estimation, object tracking and localization, and vSLAM.



### Graph-Structured Representations for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1609.05600v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1609.05600v2)
- **Published**: 2016-09-19 05:21:36+00:00
- **Updated**: 2017-03-30 04:26:26+00:00
- **Authors**: Damien Teney, Lingqiao Liu, Anton van den Hengel
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes to improve visual question answering (VQA) with structured representations of both scene contents and questions. A key challenge in VQA is to require joint reasoning over the visual and text domains. The predominant CNN/LSTM-based approach to VQA is limited by monolithic vector representations that largely ignore structure in the scene and in the form of the question. CNN feature vectors cannot effectively capture situations as simple as multiple object instances, and LSTMs process questions as series of words, which does not reflect the true complexity of language structure. We instead propose to build graphs over the scene objects and over the question words, and we describe a deep neural network that exploits the structure in these representations. This shows significant benefit over the sequential processing of LSTMs. The overall efficacy of our approach is demonstrated by significant improvements over the state-of-the-art, from 71.2% to 74.4% in accuracy on the "abstract scenes" multiple-choice benchmark, and from 34.7% to 39.1% in accuracy over pairs of "balanced" scenes, i.e. images with fine-grained differences and opposite yes/no answers to a same question.



### Coarse-to-fine Surgical Instrument Detection for Cataract Surgery Monitoring
- **Arxiv ID**: http://arxiv.org/abs/1609.05619v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.05619v1)
- **Published**: 2016-09-19 07:40:41+00:00
- **Updated**: 2016-09-19 07:40:41+00:00
- **Authors**: Hassan Al Hajj, Gwenolé Quellec, Mathieu Lamard, Guy Cazuguel, Béatrice Cochener
- **Comment**: None
- **Journal**: None
- **Summary**: The amount of surgical data, recorded during video-monitored surgeries, has extremely increased. This paper aims at improving existing solutions for the automated analysis of cataract surgeries in real time. Through the analysis of a video recording the operating table, it is possible to know which instruments exit or enter the operating table, and therefore which ones are likely being used by the surgeon. Combining these observations with observations from the microscope video should enhance the overall performance of the systems. To this end, the proposed solution is divided into two main parts: one to detect the instruments at the beginning of the surgery and one to update the list of instruments every time a change is detected in the scene. In the first part, the goal is to separate the instruments from the background and from irrelevant objects. For the second, we are interested in detecting the instruments that appear and disappear whenever the surgeon interacts with the table. Experiments on a dataset of 36 cataract surgeries validate the good performance of the proposed solution.



### Multi-Residual Networks: Improving the Speed and Accuracy of Residual Networks
- **Arxiv ID**: http://arxiv.org/abs/1609.05672v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.05672v4)
- **Published**: 2016-09-19 11:34:24+00:00
- **Updated**: 2017-03-15 08:50:00+00:00
- **Authors**: Masoud Abdi, Saeid Nahavandi
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: In this article, we take one step toward understanding the learning behavior of deep residual networks, and supporting the observation that deep residual networks behave like ensembles. We propose a new convolutional neural network architecture which builds upon the success of residual networks by explicitly exploiting the interpretation of very deep networks as an ensemble. The proposed multi-residual network increases the number of residual functions in the residual blocks. Our architecture generates models that are wider, rather than deeper, which significantly improves accuracy. We show that our model achieves an error rate of 3.73% and 19.45% on CIFAR-10 and CIFAR-100 respectively, that outperforms almost all of the existing models. We also demonstrate that our model outperforms very deep residual networks by 0.22% (top-1 error) on the full ImageNet 2012 classification dataset. Additionally, inspired by the parallel structure of multi-residual networks, a model parallelism technique has been investigated. The model parallelism method distributes the computation of residual blocks among the processors, yielding up to 15% computational complexity improvement.



### A scalable convolutional neural network for task-specified scenarios via knowledge distillation
- **Arxiv ID**: http://arxiv.org/abs/1609.05695v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.05695v2)
- **Published**: 2016-09-19 12:43:32+00:00
- **Updated**: 2017-01-06 13:57:47+00:00
- **Authors**: Mengnan Shi, Fei Qin, Qixiang Ye, Zhenjun Han, Jianbin Jiao
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we explore the redundancy in convolutional neural network, which scales with the complexity of vision tasks. Considering that many front-end visual systems are interested in only a limited range of visual targets, the removing of task-specified network redundancy can promote a wide range of potential applications. We propose a task-specified knowledge distillation algorithm to derive a simplified model with pre-set computation cost and minimized accuracy loss, which suits the resource constraint front-end systems well. Experiments on the MNIST and CIFAR10 datasets demonstrate the feasibility of the proposed approach as well as the existence of task-specified redundancy.



### Poisson Noise Reduction with Higher-order Natural Image Prior Model
- **Arxiv ID**: http://arxiv.org/abs/1609.05722v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.05722v1)
- **Published**: 2016-09-19 13:35:14+00:00
- **Updated**: 2016-09-19 13:35:14+00:00
- **Authors**: Wensen Feng, Hong Qiao, Yunjin Chen
- **Comment**: 31 pages, 10 figures. To appear in SIAM Journal on Imaging Sciences
- **Journal**: None
- **Summary**: Poisson denoising is an essential issue for various imaging applications, such as night vision, medical imaging and microscopy. State-of-the-art approaches are clearly dominated by patch-based non-local methods in recent years. In this paper, we aim to propose a local Poisson denoising model with both structure simplicity and good performance. To this end, we consider a variational modeling to integrate the so-called Fields of Experts (FoE) image prior, that has proven an effective higher-order Markov Random Fields (MRF) model for many classic image restoration problems. We exploit several feasible variational variants for this task. We start with a direct modeling in the original image domain by taking into account the Poisson noise statistics, which performs generally well for the cases of high SNR. However, this strategy encounters problem in cases of low SNR. Then we turn to an alternative modeling strategy by using the Anscombe transform and Gaussian statistics derived data term. We retrain the FoE prior model directly in the transform domain. With the newly trained FoE model, we end up with a local variational model providing strongly competitive results against state-of-the-art non-local approaches, meanwhile bearing the property of simple structure. Furthermore, our proposed model comes along with an additional advantage, that the inference is very efficient as it is well-suited for parallel computation on GPUs. For images of size $512 \times 512$, our GPU implementation takes less than 1 second to produce state-of-the-art Poisson denoising performance.



### Random Forests versus Neural Networks - What's Best for Camera Localization?
- **Arxiv ID**: http://arxiv.org/abs/1609.05797v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1609.05797v3)
- **Published**: 2016-09-19 15:50:25+00:00
- **Updated**: 2017-07-13 08:52:13+00:00
- **Authors**: Daniela Massiceti, Alexander Krull, Eric Brachmann, Carsten Rother, Philip H. S. Torr
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: This work addresses the task of camera localization in a known 3D scene given a single input RGB image. State-of-the-art approaches accomplish this in two steps: firstly, regressing for every pixel in the image its 3D scene coordinate and subsequently, using these coordinates to estimate the final 6D camera pose via RANSAC. To solve the first step, Random Forests (RFs) are typically used. On the other hand, Neural Networks (NNs) reign in many dense regression tasks, but are not test-time efficient. We ask the question: which of the two is best for camera localization? To address this, we make two method contributions: (1) a test-time efficient NN architecture which we term a ForestNet that is derived and initialized from a RF, and (2) a new fully-differentiable robust averaging technique for regression ensembles which can be trained end-to-end with a NN. Our experimental findings show that for scene coordinate regression, traditional NN architectures are superior to test-time efficient RFs and ForestNets, however, this does not translate to final 6D camera pose accuracy where RFs and ForestNets perform slightly better. To summarize, our best method, a ForestNet with a robust average, which has an equivalent fast and lightweight RF, improves over the state-of-the-art for camera localization on the 7-Scenes dataset. While this work focuses on scene coordinate regression for camera localization, our innovations may also be applied to other continuous regression tasks.



### The Projected Power Method: An Efficient Algorithm for Joint Alignment from Pairwise Differences
- **Arxiv ID**: http://arxiv.org/abs/1609.05820v3
- **DOI**: None
- **Categories**: **cs.IT**, cs.CV, cs.LG, math.IT, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1609.05820v3)
- **Published**: 2016-09-19 16:29:46+00:00
- **Updated**: 2017-12-07 20:30:54+00:00
- **Authors**: Yuxin Chen, Emmanuel Candes
- **Comment**: Accepted to Communications on Pure and Applied Mathematics
- **Journal**: None
- **Summary**: Various applications involve assigning discrete label values to a collection of objects based on some pairwise noisy data. Due to the discrete---and hence nonconvex---structure of the problem, computing the optimal assignment (e.g.~maximum likelihood assignment) becomes intractable at first sight. This paper makes progress towards efficient computation by focusing on a concrete joint alignment problem---that is, the problem of recovering $n$ discrete variables $x_i \in \{1,\cdots, m\}$, $1\leq i\leq n$ given noisy observations of their modulo differences $\{x_i - x_j~\mathsf{mod}~m\}$. We propose a low-complexity and model-free procedure, which operates in a lifted space by representing distinct label values in orthogonal directions, and which attempts to optimize quadratic functions over hypercubes. Starting with a first guess computed via a spectral method, the algorithm successively refines the iterates via projected power iterations. We prove that for a broad class of statistical models, the proposed projected power method makes no error---and hence converges to the maximum likelihood estimate---in a suitable regime. Numerical experiments have been carried out on both synthetic and real data to demonstrate the practicality of our algorithm. We expect this algorithmic framework to be effective for a broad range of discrete assignment problems.



### On Support Relations and Semantic Scene Graphs
- **Arxiv ID**: http://arxiv.org/abs/1609.05834v4
- **DOI**: 10.1016/j.isprsjprs.2017.07.010
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1609.05834v4)
- **Published**: 2016-09-19 17:21:55+00:00
- **Updated**: 2017-11-16 11:07:23+00:00
- **Authors**: Michael Ying Yang, Wentong Liao, Hanno Ackermann, Bodo Rosenhahn
- **Comment**: Accepted in ISPRS Journal of Photogrammetry and Remote Sensing
- **Journal**: None
- **Summary**: Scene understanding is a popular and challenging topic in both computer vision and photogrammetry. Scene graph provides rich information for such scene understanding. This paper presents a novel approach to infer such relations and then to construct the scene graph. Support relations are estimated by considering important, previously ignored information: the physical stability and the prior support knowledge between object classes. In contrast to previous methods for extracting support relations, the proposed approach generates more accurate results, and does not require a pixel-wise semantic labeling of the scene. The semantic scene graph which describes all the contextual relations within the scene is constructed using this information. To evaluate the accuracy of these graphs, multiple different measures are formulated. The proposed algorithms are evaluated using the NYUv2 database. The results demonstrate that the inferred support relations are more precise than state-of-the-art. The scene graphs are compared against ground truth graphs.



### Deep Neural Ensemble for Retinal Vessel Segmentation in Fundus Images towards Achieving Label-free Angiography
- **Arxiv ID**: http://arxiv.org/abs/1609.05871v1
- **DOI**: 10.1109/EMBC.2016.7590955
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.05871v1)
- **Published**: 2016-09-19 19:11:05+00:00
- **Updated**: 2016-09-19 19:11:05+00:00
- **Authors**: Avisek Lahiri, Abhijit Guha Roy, Debdoot Sheet, Prabir Kumar Biswas
- **Comment**: Accepted as a conference paper at IEEE EMBC, 2016
- **Journal**: None
- **Summary**: Automated segmentation of retinal blood vessels in label-free fundus images entails a pivotal role in computed aided diagnosis of ophthalmic pathologies, viz., diabetic retinopathy, hypertensive disorders and cardiovascular diseases. The challenge remains active in medical image analysis research due to varied distribution of blood vessels, which manifest variations in their dimensions of physical appearance against a noisy background.   In this paper we formulate the segmentation challenge as a classification task. Specifically, we employ unsupervised hierarchical feature learning using ensemble of two level of sparsely trained denoised stacked autoencoder. First level training with bootstrap samples ensures decoupling and second level ensemble formed by different network architectures ensures architectural revision. We show that ensemble training of auto-encoders fosters diversity in learning dictionary of visual kernels for vessel segmentation. SoftMax classifier is used for fine tuning each member auto-encoder and multiple strategies are explored for 2-level fusion of ensemble members. On DRIVE dataset, we achieve maximum average accuracy of 95.33\% with an impressively low standard deviation of 0.003 and Kappa agreement coefficient of 0.708 . Comparison with other major algorithms substantiates the high efficacy of our model.



