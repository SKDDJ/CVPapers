# Arxiv Papers in cs.CV on 2016-09-26
### Multiview RGB-D Dataset for Object Instance Detection
- **Arxiv ID**: http://arxiv.org/abs/1609.07826v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1609.07826v1)
- **Published**: 2016-09-26 01:18:56+00:00
- **Updated**: 2016-09-26 01:18:56+00:00
- **Authors**: Georgios Georgakis, Md Alimoor Reza, Arsalan Mousavian, Phi-Hung Le, Jana Kosecka
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new multi-view RGB-D dataset of nine kitchen scenes, each containing several objects in realistic cluttered environments including a subset of objects from the BigBird dataset. The viewpoints of the scenes are densely sampled and objects in the scenes are annotated with bounding boxes and in the 3D point cloud. Also, an approach for detection and recognition is presented, which is comprised of two parts: i) a new multi-view 3D proposal generation method and ii) the development of several recognition baselines using AlexNet to score our proposals, which is trained either on crops of the dataset or on synthetically composited training images. Finally, we compare the performance of the object proposals and a detection baseline to the Washington RGB-D Scenes (WRGB-D) dataset and demonstrate that our Kitchen scenes dataset is more challenging for object detection and recognition. The dataset is available at: http://cs.gmu.edu/~robot/gmu-kitchens.html.



### From Monocular SLAM to Autonomous Drone Exploration
- **Arxiv ID**: http://arxiv.org/abs/1609.07835v3
- **DOI**: 10.1109/ECMR.2017.8098709
- **Categories**: **cs.RO**, cs.CV, 68T40, I.2.9
- **Links**: [PDF](http://arxiv.org/pdf/1609.07835v3)
- **Published**: 2016-09-26 03:02:35+00:00
- **Updated**: 2018-03-12 14:24:32+00:00
- **Authors**: Lukas von Stumberg, Vladyslav Usenko, Jakob Engel, Jörg Stückler, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: Micro aerial vehicles (MAVs) are strongly limited in their payload and power capacity. In order to implement autonomous navigation, algorithms are therefore desirable that use sensory equipment that is as small, low-weight, and low-power consuming as possible. In this paper, we propose a method for autonomous MAV navigation and exploration using a low-cost consumer-grade quadrocopter equipped with a monocular camera. Our vision-based navigation system builds on LSD-SLAM which estimates the MAV trajectory and a semi-dense reconstruction of the environment in real-time. Since LSD-SLAM only determines depth at high gradient pixels, texture-less areas are not directly observed so that previous exploration methods that assume dense map information cannot directly be applied. We propose an obstacle mapping and exploration approach that takes the properties of our semi-dense monocular SLAM system into account. In experiments, we demonstrate our vision-based autonomous navigation and exploration system with a Parrot Bebop MAV.



### Visual Fashion-Product Search at SK Planet
- **Arxiv ID**: http://arxiv.org/abs/1609.07859v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.07859v6)
- **Published**: 2016-09-26 06:53:36+00:00
- **Updated**: 2017-04-12 03:51:23+00:00
- **Authors**: Taewan Kim, Seyeong Kim, Sangil Na, Hayoon Kim, Moonki Kim, Byoung-Ki Jeon
- **Comment**: 13 pages, 11 figures, 3 tables
- **Journal**: None
- **Summary**: We build a large-scale visual search system which finds similar product images given a fashion item. Defining similarity among arbitrary fashion-products is still remains a challenging problem, even there is no exact ground-truth. To resolve this problem, we define more than 90 fashion-related attributes, and combination of these attributes can represent thousands of unique fashion-styles. The fashion-attributes are one of the ingredients to define semantic similarity among fashion-product images. To build our system at scale, these fashion-attributes are again used to build an inverted indexing scheme. In addition to these fashion-attributes for semantic similarity, we extract colour and appearance features in a region-of-interest (ROI) of a fashion item for visual similarity. By sharing our approach, we expect active discussion on that how to apply current computer vision research into the e-commerce industry.



### Lexicon-Free Fingerspelling Recognition from Video: Data, Models, and Signer Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1609.07876v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1609.07876v1)
- **Published**: 2016-09-26 07:34:24+00:00
- **Updated**: 2016-09-26 07:34:24+00:00
- **Authors**: Taehwan Kim, Jonathan Keane, Weiran Wang, Hao Tang, Jason Riggle, Gregory Shakhnarovich, Diane Brentari, Karen Livescu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1608.08339
- **Journal**: None
- **Summary**: We study the problem of recognizing video sequences of fingerspelled letters in American Sign Language (ASL). Fingerspelling comprises a significant but relatively understudied part of ASL. Recognizing fingerspelling is challenging for a number of reasons: It involves quick, small motions that are often highly coarticulated; it exhibits significant variation between signers; and there has been a dearth of continuous fingerspelling data collected. In this work we collect and annotate a new data set of continuous fingerspelling videos, compare several types of recognizers, and explore the problem of signer variation. Our best-performing models are segmental (semi-Markov) conditional random fields using deep neural network-based features. In the signer-dependent setting, our recognizers achieve up to about 92% letter accuracy. The multi-signer setting is much more challenging, but with neural network adaptation we achieve up to 83% letter accuracies in this setting.



### Linear Support Tensor Machine: Pedestrian Detection in Thermal Infrared Images
- **Arxiv ID**: http://arxiv.org/abs/1609.07878v1
- **DOI**: 10.1109/TIP.2017.2705426
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.07878v1)
- **Published**: 2016-09-26 07:54:00+00:00
- **Updated**: 2016-09-26 07:54:00+00:00
- **Authors**: Sujoy Kumar Biswas, Peyman Milanfar
- **Comment**: None
- **Journal**: None
- **Summary**: Pedestrian detection in thermal infrared images poses unique challenges because of the low resolution and noisy nature of the image. Here we propose a mid-level attribute in the form of multidimensional template, or tensor, using Local Steering Kernel (LSK) as low-level descriptors for detecting pedestrians in far infrared images. LSK is specifically designed to deal with intrinsic image noise and pixel level uncertainty by capturing local image geometry succinctly instead of collecting local orientation statistics (e.g., histograms in HOG). Our second contribution is the introduction of a new image similarity kernel in the popular maximum margin framework of support vector machines that results in a relatively short and simple training phase for building a rigid pedestrian detector. Our third contribution is to replace the sluggish but de facto sliding window based detection methodology with multichannel discrete Fourier transform, facilitating very fast and efficient pedestrian localization. The experimental studies on publicly available thermal infrared images justify our proposals and model assumptions. In addition, the proposed work also involves the release of our in-house annotations of pedestrians in more than 17000 frames of OSU Color Thermal database for the purpose of sharing with the research community.



### Deep Structured Features for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1609.07916v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1609.07916v3)
- **Published**: 2016-09-26 10:33:13+00:00
- **Updated**: 2017-06-16 15:12:49+00:00
- **Authors**: Michael Tschannen, Lukas Cavigelli, Fabian Mentzer, Thomas Wiatowski, Luca Benini
- **Comment**: EUSIPCO 2017, 5 pages, 2 figures
- **Journal**: None
- **Summary**: We propose a highly structured neural network architecture for semantic segmentation with an extremely small model size, suitable for low-power embedded and mobile platforms. Specifically, our architecture combines i) a Haar wavelet-based tree-like convolutional neural network (CNN), ii) a random layer realizing a radial basis function kernel approximation, and iii) a linear classifier. While stages i) and ii) are completely pre-specified, only the linear classifier is learned from data. We apply the proposed architecture to outdoor scene and aerial image semantic segmentation and show that the accuracy of our architecture is competitive with conventional pixel classification CNNs. Furthermore, we demonstrate that the proposed architecture is data efficient in the sense of matching the accuracy of pixel classification CNNs when trained on a much smaller data set.



### Optimistic and Pessimistic Neural Networks for Scene and Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1609.07982v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.07982v2)
- **Published**: 2016-09-26 14:24:08+00:00
- **Updated**: 2016-12-22 12:25:35+00:00
- **Authors**: Rene Grzeszick, Sebastian Sudholt, Gernot A. Fink
- **Comment**: This Version fixes an error in eq. 1,2 and adds some details to the
  significance testing
- **Journal**: None
- **Summary**: In this paper the application of uncertainty modeling to convolutional neural networks is evaluated. A novel method for adjusting the network's predictions based on uncertainty information is introduced. This allows the network to be either optimistic or pessimistic in its prediction scores. The proposed method builds on the idea of applying dropout at test time and sampling a predictive mean and variance from the network's output. Besides the methodological aspects, implementation details allowing for a fast evaluation are presented. Furthermore, a multilabel network architecture is introduced that strongly benefits from the presented approach. In the evaluation it will be shown that modeling uncertainty allows for improving the performance of a given model purely at test time without any further training steps. The evaluation considers several applications in the field of computer vision, including object classification and detection as well as scene attribute recognition.



### Super-resolving multiresolution images with band-independant geometry of multispectral pixels
- **Arxiv ID**: http://arxiv.org/abs/1609.07986v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.07986v3)
- **Published**: 2016-09-26 14:28:49+00:00
- **Updated**: 2017-04-04 10:29:55+00:00
- **Authors**: Nicolas Brodu
- **Comment**: Source code with a ready-to-use script for super-resolving Sentinel-2
  data is available at http://nicolas.brodu.net/recherche/superres/
- **Journal**: None
- **Summary**: A new resolution enhancement method is presented for multispectral and multi-resolution images, such as these provided by the Sentinel-2 satellites. Starting from the highest resolution bands, band-dependent information (reflectance) is separated from information that is common to all bands (geometry of scene elements). This model is then applied to unmix low-resolution bands, preserving their reflectance, while propagating band-independent information to preserve the sub-pixel details. A reference implementation is provided, with an application example for super-resolving Sentinel-2 data.



### BioLeaf: a professional mobile application to measure foliar damage caused by insect herbivory
- **Arxiv ID**: http://arxiv.org/abs/1609.08004v2
- **DOI**: 10.1016/j.compag.2016.09.007
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.08004v2)
- **Published**: 2016-09-26 14:59:50+00:00
- **Updated**: 2016-09-27 01:02:57+00:00
- **Authors**: Bruno Machado, Jonatan Orue, Mauro Arruda, Cleidimar Santos, Diogo Sarath, Wesley Goncalves, Gercina Silva, Hemerson Pistori, Antonia Roel, Jose Rodrigues-Jr
- **Comment**: None
- **Journal**: Computers and Electronics in Agriculture 129: 1. 44-55 (2016)
- **Summary**: Soybean is one of the ten greatest crops in the world, answering for billion-dollar businesses every year. This crop suffers from insect herbivory that costs millions from producers. Hence, constant monitoring of the crop foliar damage is necessary to guide the application of insecticides. However, current methods to measure foliar damage are expensive and dependent on laboratory facilities, in some cases, depending on complex devices. To cope with these shortcomings, we introduce an image processing methodology to measure the foliar damage in soybean leaves. We developed a non-destructive imaging method based on two techniques, Otsu segmentation and Bezier curves, to estimate the foliar loss in leaves with or without border damage. We instantiate our methodology in a mobile application named BioLeaf, which is freely distributed for smartphone users. We experimented with real-world leaves collected from a soybean crop in Brazil. Our results demonstrated that BioLeaf achieves foliar damage quantification with precision comparable to that of human specialists. With these results, our proposal might assist soybean producers, reducing the time to measure foliar damage, reducing analytical costs, and defining a commodity application that is applicable not only to soy, but also to different crops such as cotton, bean, potato, coffee, and vegetables.



### Connecting the dots across time: Reconstruction of single cell signaling trajectories using time-stamped data
- **Arxiv ID**: http://arxiv.org/abs/1609.08035v2
- **DOI**: None
- **Categories**: **q-bio.QM**, cond-mat.stat-mech, cs.CG, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1609.08035v2)
- **Published**: 2016-09-26 15:56:12+00:00
- **Updated**: 2017-07-20 14:33:02+00:00
- **Authors**: Sayak Mukherjee, David Stewart, William Stewart, Lewis L. Lanier, Jayajit Das
- **Comment**: revised version, accepted for publication in Royal Society Open
  Science
- **Journal**: None
- **Summary**: Single cell responses are shaped by the geometry of signaling kinetic trajectories carved in a multidimensional space spanned by signaling protein abundances. It is however challenging to assay large number (>3) of signaling species in live-cell imaging which makes it difficult to probe single cell signaling kinetic trajectories in large dimensions. Flow and mass cytometry techniques can measure a large number (4 - >40) of signaling species but are unable to track single cells. Thus cytometry experiments provide detailed time stamped snapshots of single cell signaling kinetics. Is it possible to use the time stamped cytometry data to reconstruct single cell signaling trajectories? Borrowing concepts of conserved and slow variables from non-equilibrium statistical physics we develop an approach to reconstruct signaling trajectories using snapshot data by creating new variables that remain invariant or vary slowly during the signaling kinetics. We apply this approach to reconstruct trajectories using snapshot data obtained from in silico simulations and live-cell imaging measurements. The use of invariants and slow variables to reconstruct trajectories provides a radically different way to track object using snapshot data. The approach is likely to have implications for solving matching problems in a wide range of disciplines.



### Robust Regression For Image Binarization Under Heavy Noises and Nonuniform Background
- **Arxiv ID**: http://arxiv.org/abs/1609.08078v4
- **DOI**: 10.1016/j.patcog.2018.04.005
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.08078v4)
- **Published**: 2016-09-26 17:07:49+00:00
- **Updated**: 2017-07-06 20:35:51+00:00
- **Authors**: Garret Vo, Chiwoo Park
- **Comment**: None
- **Journal**: Pattern Recognition Volume 81, September 2018, Pages 224-239
- **Summary**: This paper presents a robust regression approach for image binarization under significant background variations and observation noises. The work is motivated by the need of identifying foreground regions in noisy microscopic image or degraded document images, where significant background variation and severe noise make an image binarization challenging. The proposed method first estimates the background of an input image, subtracts the estimated background from the input image, and apply a global thresholding to the subtracted outcome for achieving a binary image of foregrounds. A robust regression approach was proposed to estimate the background intensity surface with minimal effects of foreground intensities and noises, and a global threshold selector was proposed on the basis of a model selection criterion in a sparse regression. The proposed approach was validated using 26 test images and the corresponding ground truths, and the outcomes of the proposed work were compared with those from nine existing image binarization methods. The approach was also combined with three state-of-the-art morphological segmentation methods to show how the proposed approach can improve their image segmentation outcomes.



### Swipe Mosaics from Video
- **Arxiv ID**: http://arxiv.org/abs/1609.08080v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.08080v1)
- **Published**: 2016-09-26 17:14:53+00:00
- **Updated**: 2016-09-26 17:14:53+00:00
- **Authors**: Malcolm Reynolds, Tom S. F. Haines, Gabriel J. Brostow
- **Comment**: None
- **Journal**: None
- **Summary**: A panoramic image mosaic is an attractive visualization for viewing many overlapping photos, but its images must be both captured and processed correctly to produce an acceptable composite. We propose Swipe Mosaics, an interactive visualization that places the individual video frames on a 2D planar map that represents the layout of the physical scene. Compared to traditional panoramic mosaics, our capture is easier because the user can both translate the camera center and film moving subjects. Processing and display degrade gracefully if the footage lacks distinct, overlapping, non-repeating texture. Our proposed visual odometry algorithm produces a distribution over (x,y) translations for image pairs. Inferring a distribution of possible camera motions allows us to better cope with parallax, lack of texture, dynamic scenes, and other phenomena that hurt deterministic reconstruction techniques. Robustness is obtained by training on synthetic scenes with known camera motions. We show that Swipe Mosaics are easy to generate, support a wide range of difficult scenes, and are useful for documenting a scene for closer inspection.



### Learning Language-Visual Embedding for Movie Understanding with Natural-Language
- **Arxiv ID**: http://arxiv.org/abs/1609.08124v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.08124v1)
- **Published**: 2016-09-26 19:14:12+00:00
- **Updated**: 2016-09-26 19:14:12+00:00
- **Authors**: Atousa Torabi, Niket Tandon, Leonid Sigal
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Learning a joint language-visual embedding has a number of very appealing properties and can result in variety of practical application, including natural language image/video annotation and search. In this work, we study three different joint language-visual neural network model architectures. We evaluate our models on large scale LSMDC16 movie dataset for two tasks: 1) Standard Ranking for video annotation and retrieval 2) Our proposed movie multiple-choice test. This test facilitate automatic evaluation of visual-language models for natural language video annotation based on human activities. In addition to original Audio Description (AD) captions, provided as part of LSMDC16, we collected and will make available a) manually generated re-phrasings of those captions obtained using Amazon MTurk b) automatically generated human activity elements in "Predicate + Object" (PO) phrases based on "Knowlywood", an activity knowledge mining model. Our best model archives Recall@10 of 19.2% on annotation and 18.9% on video retrieval tasks for subset of 1000 samples. For multiple-choice test, our best model achieve accuracy 58.11% over whole LSMDC16 public test-set.



### Automatic Construction of a Recurrent Neural Network based Classifier for Vehicle Passage Detection
- **Arxiv ID**: http://arxiv.org/abs/1609.08209v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1609.08209v1)
- **Published**: 2016-09-26 22:11:05+00:00
- **Updated**: 2016-09-26 22:11:05+00:00
- **Authors**: Evgeny Burnaev, Ivan Koptelov, German Novikov, Timur Khanipov
- **Comment**: 6 pages, 2 figures, 5 tables
- **Journal**: None
- **Summary**: Recurrent Neural Networks (RNNs) are extensively used for time-series modeling and prediction. We propose an approach for automatic construction of a binary classifier based on Long Short-Term Memory RNNs (LSTM-RNNs) for detection of a vehicle passage through a checkpoint. As an input to the classifier we use multidimensional signals of various sensors that are installed on the checkpoint. Obtained results demonstrate that the previous approach to handcrafting a classifier, consisting of a set of deterministic rules, can be successfully replaced by an automatic RNN training on an appropriately labelled data.



### Simultaneous Low-rank Component and Graph Estimation for High-dimensional Graph Signals: Application to Brain Imaging
- **Arxiv ID**: http://arxiv.org/abs/1609.08221v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1609.08221v2)
- **Published**: 2016-09-26 23:24:27+00:00
- **Updated**: 2017-01-09 03:23:43+00:00
- **Authors**: Rui Liu, Hossein Nejati, Seyed Hamid Safavi, Ngai-Man Cheung
- **Comment**: Accepted by ICASSP 2017
- **Journal**: None
- **Summary**: We propose an algorithm to uncover the intrinsic low-rank component of a high-dimensional, graph-smooth and grossly-corrupted dataset, under the situations that the underlying graph is unknown. Based on a model with a low-rank component plus a sparse perturbation, and an initial graph estimation, our proposed algorithm simultaneously learns the low-rank component and refines the graph. Our evaluations using synthetic and real brain imaging data in unsupervised and supervised classification tasks demonstrate encouraging performance.



