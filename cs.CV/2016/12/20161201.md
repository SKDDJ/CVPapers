# Arxiv Papers in cs.CV on 2016-12-01
### Texture Enhancement via High-Resolution Style Transfer for Single-Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1612.00085v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00085v1)
- **Published**: 2016-12-01 00:15:02+00:00
- **Updated**: 2016-12-01 00:15:02+00:00
- **Authors**: Il Jun Ahn, Woo Hyun Nam
- **Comment**: Il Jun Ahn and Woo Hyun Nam contributed equally to this work.
  Submitted to IEEE Transactions on Consumer Electronics
- **Journal**: None
- **Summary**: Recently, various deep-neural-network (DNN)-based approaches have been proposed for single-image super-resolution (SISR). Despite their promising results on major structure regions such as edges and lines, they still suffer from limited performance on texture regions that consist of very complex and fine patterns. This is because, during the acquisition of a low-resolution (LR) image via down-sampling, these regions lose most of the high frequency information necessary to represent the texture details. In this paper, we present a novel texture enhancement framework for SISR to effectively improve the spatial resolution in the texture regions as well as edges and lines. We call our method, high-resolution (HR) style transfer algorithm. Our framework consists of three steps: (i) generate an initial HR image from an interpolated LR image via an SISR algorithm, (ii) generate an HR style image from the initial HR image via down-scaling and tiling, and (iii) combine the HR style image with the initial HR image via a customized style transfer algorithm. Here, the HR style image is obtained by down-scaling the initial HR image and then repetitively tiling it into an image of the same size as the HR image. This down-scaling and tiling process comes from the idea that texture regions are often composed of small regions that similar in appearance albeit sometimes different in scale. This process creates an HR style image that is rich in details, which can be used to restore high-frequency texture details back into the initial HR image via the style transfer algorithm. Experimental results on a number of texture datasets show that our proposed HR style transfer algorithm provides more visually pleasing results compared with competitive methods.



### Beyond standard benchmarks: Parameterizing performance evaluation in visual object tracking
- **Arxiv ID**: http://arxiv.org/abs/1612.00089v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00089v2)
- **Published**: 2016-12-01 00:26:03+00:00
- **Updated**: 2017-03-25 18:13:40+00:00
- **Authors**: Luka Čehovin Zajc, Alan Lukežič, Aleš Leonardis, Matej Kristan
- **Comment**: None
- **Journal**: None
- **Summary**: Object-to-camera motion produces a variety of apparent motion patterns that significantly affect performance of short-term visual trackers. Despite being crucial for designing robust trackers, their influence is poorly explored in standard benchmarks due to weakly defined, biased and overlapping attribute annotations. In this paper we propose to go beyond pre-recorded benchmarks with post-hoc annotations by presenting an approach that utilizes omnidirectional videos to generate realistic, consistently annotated, short-term tracking scenarios with exactly parameterized motion patterns. We have created an evaluation system, constructed a fully annotated dataset of omnidirectional videos and the generators for typical motion patterns. We provide an in-depth analysis of major tracking paradigms which is complementary to the standard benchmarks and confirms the expressiveness of our evaluation approach.



### Shape Completion using 3D-Encoder-Predictor CNNs and Shape Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1612.00101v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00101v2)
- **Published**: 2016-12-01 01:10:41+00:00
- **Updated**: 2017-04-11 20:48:53+00:00
- **Authors**: Angela Dai, Charles Ruizhongtai Qi, Matthias Nießner
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a data-driven approach to complete partial 3D shapes through a combination of volumetric deep neural networks and 3D shape synthesis. From a partially-scanned input shape, our method first infers a low-resolution -- but complete -- output. To this end, we introduce a 3D-Encoder-Predictor Network (3D-EPN) which is composed of 3D convolutional layers. The network is trained to predict and fill in missing data, and operates on an implicit surface representation that encodes both known and unknown space. This allows us to predict global structure in unknown areas at high accuracy. We then correlate these intermediary results with 3D geometry from a shape database at test time. In a final pass, we propose a patch-based 3D shape synthesis method that imposes the 3D geometry from these retrieved shapes as constraints on the coarsely-completed mesh. This synthesis process enables us to reconstruct fine-scale detail and generate high-resolution output while respecting the global mesh structure obtained by the 3D-EPN. Although our 3D-EPN outperforms state-of-the-art completion method, the main contribution in our work lies in the combination of a data-driven shape predictor and analytic 3D shape synthesis. In our results, we show extensive evaluations on a newly-introduced shape completion benchmark for both real-world and synthetic data.



### Video Scene Parsing with Predictive Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/1612.00119v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00119v2)
- **Published**: 2016-12-01 02:48:48+00:00
- **Updated**: 2016-12-13 04:55:42+00:00
- **Authors**: Xiaojie Jin, Xin Li, Huaxin Xiao, Xiaohui Shen, Zhe Lin, Jimei Yang, Yunpeng Chen, Jian Dong, Luoqi Liu, Zequn Jie, Jiashi Feng, Shuicheng Yan
- **Comment**: 15 pages, 7 figures, 5 tables, currently v2
- **Journal**: None
- **Summary**: In this work, we address the challenging video scene parsing problem by developing effective representation learning methods given limited parsing annotations. In particular, we contribute two novel methods that constitute a unified parsing framework. (1) \textbf{Predictive feature learning}} from nearly unlimited unlabeled video data. Different from existing methods learning features from single frame parsing, we learn spatiotemporal discriminative features by enforcing a parsing network to predict future frames and their parsing maps (if available) given only historical frames. In this way, the network can effectively learn to capture video dynamics and temporal context, which are critical clues for video scene parsing, without requiring extra manual annotations. (2) \textbf{Prediction steering parsing}} architecture that effectively adapts the learned spatiotemporal features to scene parsing tasks and provides strong guidance for any off-the-shelf parsing model to achieve better video scene parsing performance. Extensive experiments over two challenging datasets, Cityscapes and Camvid, have demonstrated the effectiveness of our methods by showing significant improvement over well-established baselines.



### A Novel Artificial Fish Swarm Algorithm for Pattern Recognition with Convex Optimization
- **Arxiv ID**: http://arxiv.org/abs/1612.00125v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00125v2)
- **Published**: 2016-12-01 03:18:46+00:00
- **Updated**: 2016-12-07 15:05:48+00:00
- **Authors**: Lei Shi, Rui Guo, Yuchen Ma
- **Comment**: arXiv admin note: submission has been withdrawn by arXiv
  administrators due to inappropriate text reuse from external sources
- **Journal**: None
- **Summary**: Image pattern recognition is an important area in digital image processing. An efficient pattern recognition algorithm should be able to provide correct recognition at a reduced computational time. Off late amongst the machine learning pattern recognition algorithms, Artificial fish swarm algorithm is one of the swarm intelligence optimization algorithms that works based on population and stochastic search. In order to achieve acceptable result, there are many parameters needs to be adjusted in AFSA. Among these parameters, visual and step are very significant in view of the fact that artificial fish basically move based on these parameters. In standard AFSA, these two parameters remain constant until the algorithm termination. Large values of these parameters increase the capability of algorithm in global search, while small values improve the local search ability of the algorithm. In this paper, we empirically study the performance of the AFSA and different approaches to balance between local and global exploration have been tested based on the adaptive modification of visual and step during algorithm execution. The proposed approaches have been evaluated based on the four well-known benchmark functions. Experimental results show considerable positive impact on the performance of AFSA. A Convex optimization has been integrated into the proposed work to have an ideal segmentation of the input image which is a MR brain image.



### CDVAE: Co-embedding Deep Variational Auto Encoder for Conditional Variational Generation
- **Arxiv ID**: http://arxiv.org/abs/1612.00132v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1612.00132v2)
- **Published**: 2016-12-01 03:40:42+00:00
- **Updated**: 2017-03-28 03:21:34+00:00
- **Authors**: Jiajun Lu, Aditya Deshpande, David Forsyth
- **Comment**: None
- **Journal**: None
- **Summary**: Problems such as predicting a new shading field (Y) for an image (X) are ambiguous: many very distinct solutions are good. Representing this ambiguity requires building a conditional model P(Y|X) of the prediction, conditioned on the image. Such a model is difficult to train, because we do not usually have training data containing many different shadings for the same image. As a result, we need different training examples to share data to produce good models. This presents a danger we call "code space collapse" - the training procedure produces a model that has a very good loss score, but which represents the conditional distribution poorly. We demonstrate an improved method for building conditional models by exploiting a metric constraint on training data that prevents code space collapse. We demonstrate our model on two example tasks using real data: image saturation adjustment, image relighting. We describe quantitative metrics to evaluate ambiguous generation results. Our results quantitatively and qualitatively outperform different strong baselines.



### RMPE: Regional Multi-person Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1612.00137v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00137v5)
- **Published**: 2016-12-01 04:36:52+00:00
- **Updated**: 2018-02-04 04:27:56+00:00
- **Authors**: Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, Cewu Lu
- **Comment**: Models & Codes available at https://github.com/MVIG-SJTU/RMPE or
  https://github.com/Fang-Haoshu/RMPE
- **Journal**: None
- **Summary**: Multi-person pose estimation in the wild is challenging. Although state-of-the-art human detectors have demonstrated good performance, small errors in localization and recognition are inevitable. These errors can cause failures for a single-person pose estimator (SPPE), especially for methods that solely depend on human detection results. In this paper, we propose a novel regional multi-person pose estimation (RMPE) framework to facilitate pose estimation in the presence of inaccurate human bounding boxes. Our framework consists of three components: Symmetric Spatial Transformer Network (SSTN), Parametric Pose Non-Maximum-Suppression (NMS), and Pose-Guided Proposals Generator (PGPG). Our method is able to handle inaccurate bounding boxes and redundant detections, allowing it to achieve a 17% increase in mAP over the state-of-the-art methods on the MPII (multi person) dataset.Our model and source codes are publicly available.



### Towards Robust Deep Neural Networks with BANG
- **Arxiv ID**: http://arxiv.org/abs/1612.00138v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00138v3)
- **Published**: 2016-12-01 04:49:45+00:00
- **Updated**: 2018-01-30 20:59:56+00:00
- **Authors**: Andras Rozsa, Manuel Gunther, Terrance E. Boult
- **Comment**: Accepted to the IEEE Winter Conference on Applications of Computer
  Vision (WACV), 2018
- **Journal**: None
- **Summary**: Machine learning models, including state-of-the-art deep neural networks, are vulnerable to small perturbations that cause unexpected classification errors. This unexpected lack of robustness raises fundamental questions about their generalization properties and poses a serious concern for practical deployments. As such perturbations can remain imperceptible - the formed adversarial examples demonstrate an inherent inconsistency between vulnerable machine learning models and human perception - some prior work casts this problem as a security issue. Despite the significance of the discovered instabilities and ensuing research, their cause is not well understood and no effective method has been developed to address the problem. In this paper, we present a novel theory to explain why this unpleasant phenomenon exists in deep neural networks. Based on that theory, we introduce a simple, efficient, and effective training approach, Batch Adjusted Network Gradients (BANG), which significantly improves the robustness of machine learning models. While the BANG technique does not rely on any form of data augmentation or the utilization of adversarial images for training, the resultant classifiers are more resistant to adversarial perturbations while maintaining or even enhancing the overall classification performance.



### BASS Net: Band-Adaptive Spectral-Spatial Feature Learning Neural Network for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1612.00144v2
- **DOI**: 10.1109/TGRS.2017.2705073
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00144v2)
- **Published**: 2016-12-01 05:00:02+00:00
- **Updated**: 2016-12-02 09:02:51+00:00
- **Authors**: Anirban Santara, Kaustubh Mani, Pranoot Hatwar, Ankit Singh, Ankur Garg, Kirti Padia, Pabitra Mitra
- **Comment**: 8 pages, 10 figures, Submitted to IEEE TGRS, Code available at:
  https://github.com/kaustubh0mani/BASS-Net
- **Journal**: None
- **Summary**: Deep learning based landcover classification algorithms have recently been proposed in literature. In hyperspectral images (HSI) they face the challenges of large dimensionality, spatial variability of spectral signatures and scarcity of labeled data. In this article we propose an end-to-end deep learning architecture that extracts band specific spectral-spatial features and performs landcover classification. The architecture has fewer independent connection weights and thus requires lesser number of training data. The method is found to outperform the highest reported accuracies on popular hyperspectral image data sets.



### Perspective Transformer Nets: Learning Single-View 3D Object Reconstruction without 3D Supervision
- **Arxiv ID**: http://arxiv.org/abs/1612.00814v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.00814v3)
- **Published**: 2016-12-01 05:51:37+00:00
- **Updated**: 2017-08-13 02:40:50+00:00
- **Authors**: Xinchen Yan, Jimei Yang, Ersin Yumer, Yijie Guo, Honglak Lee
- **Comment**: published at NIPS 2016
- **Journal**: None
- **Summary**: Understanding the 3D world is a fundamental problem in computer vision. However, learning a good representation of 3D objects is still an open problem due to the high dimensionality of the data and many factors of variation involved. In this work, we investigate the task of single-view 3D object reconstruction from a learning agent's perspective. We formulate the learning process as an interaction between 3D and 2D representations and propose an encoder-decoder network with a novel projection loss defined by the perspective transformation. More importantly, the projection loss enables the unsupervised learning using 2D observation without explicit 3D supervision. We demonstrate the ability of the model in generating 3D volume from a single 2D image with three sets of experiments: (1) learning from single-class objects; (2) learning from multi-class objects and (3) testing on novel object classes. Results show superior performance and better generalization ability for 3D object reconstruction when the projection loss is involved.



### Adversarial Images for Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1612.00155v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.00155v1)
- **Published**: 2016-12-01 05:59:57+00:00
- **Updated**: 2016-12-01 05:59:57+00:00
- **Authors**: Pedro Tabacof, Julia Tavares, Eduardo Valle
- **Comment**: Workshop on Adversarial Training, NIPS 2016, Barcelona, Spain
- **Journal**: None
- **Summary**: We investigate adversarial attacks for autoencoders. We propose a procedure that distorts the input image to mislead the autoencoder in reconstructing a completely different target image. We attack the internal latent representations, attempting to make the adversarial input produce an internal representation as similar as possible as the target's. We find that autoencoders are much more robust to the attack than classifiers: while some examples have tolerably small input distortion, and reasonable similarity to the target image, there is a quasi-linear trade-off between those aims. We report results on MNIST and SVHN datasets, and also test regular deterministic autoencoders, reaching similar conclusions in all cases. Finally, we show that the usual adversarial attack for classifiers, while being much easier, also presents a direct proportion between distortion on the input, and misdirection on the output. That proportionality however is hidden by the normalization of the output, which maps a linear layer into non-linear probabilities.



### Monge's Optimal Transport Distance for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1612.00181v2
- **DOI**: None
- **Categories**: **cs.CV**, math.NA, 65N06
- **Links**: [PDF](http://arxiv.org/pdf/1612.00181v2)
- **Published**: 2016-12-01 09:05:30+00:00
- **Updated**: 2018-04-08 10:12:32+00:00
- **Authors**: Michael Snow, Jan Van lent
- **Comment**: 15 pages, 14 figure
- **Journal**: None
- **Summary**: This paper focuses on a similarity measure, known as the Wasserstein distance, with which to compare images. The Wasserstein distance results from a partial differential equation (PDE) formulation of Monge's optimal transport problem. We present an efficient numerical solution method for solving Monge's problem. To demonstrate the measure's discriminatory power when comparing images, we use a $1$-Nearest Neighbour ($1$-NN) machine learning algorithm to illustrate the measure's potential benefits over other more traditional distance metrics and also the Tangent Space distance, designed to perform excellently on the well-known MNIST dataset. To our knowledge, the PDE formulation of the Wasserstein metric has not been presented for dealing with image comparison, nor has the Wasserstein distance been used within the $1$-nearest neighbour architecture.



### Flight Dynamics-based Recovery of a UAV Trajectory using Ground Cameras
- **Arxiv ID**: http://arxiv.org/abs/1612.00192v2
- **DOI**: 10.1109/CVPR.2017.266
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1612.00192v2)
- **Published**: 2016-12-01 10:22:41+00:00
- **Updated**: 2017-11-21 11:25:44+00:00
- **Authors**: Artem Rozantsev, Sudipta N. Sinha, Debadeepta Dey, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new method to estimate the 6-dof trajectory of a flying object such as a quadrotor UAV within a 3D airspace monitored using multiple fixed ground cameras. It is based on a new structure from motion formulation for the 3D reconstruction of a single moving point with known motion dynamics. Our main contribution is a new bundle adjustment procedure which in addition to optimizing the camera poses, regularizes the point trajectory using a prior based on motion dynamics (or specifically flight dynamics). Furthermore, we can infer the underlying control input sent to the UAV's autopilot that determined its flight trajectory.   Our method requires neither perfect single-view tracking nor appearance matching across views. For robustness, we allow the tracker to generate multiple detections per frame in each video. The true detections and the data association across videos is estimated using robust multi-view triangulation and subsequently refined during our bundle adjustment procedure. Quantitative evaluation on simulated data and experiments on real videos from indoor and outdoor scenes demonstrates the effectiveness of our method.



### Learning in an Uncertain World: Representing Ambiguity Through Multiple Hypotheses
- **Arxiv ID**: http://arxiv.org/abs/1612.00197v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00197v3)
- **Published**: 2016-12-01 10:50:21+00:00
- **Updated**: 2017-08-22 13:55:17+00:00
- **Authors**: Christian Rupprecht, Iro Laina, Robert DiPietro, Maximilian Baust, Federico Tombari, Nassir Navab, Gregory D. Hager
- **Comment**: ICCV 2017
- **Journal**: None
- **Summary**: Many prediction tasks contain uncertainty. In some cases, uncertainty is inherent in the task itself. In future prediction, for example, many distinct outcomes are equally valid. In other cases, uncertainty arises from the way data is labeled. For example, in object detection, many objects of interest often go unlabeled, and in human pose estimation, occluded joints are often labeled with ambiguous values. In this work we focus on a principled approach for handling such scenarios. In particular, we propose a framework for reformulating existing single-prediction models as multiple hypothesis prediction (MHP) models and an associated meta loss and optimization procedure to train them. To demonstrate our approach, we consider four diverse applications: human pose estimation, future prediction, image classification and segmentation. We find that MHP models outperform their single-hypothesis counterparts in all cases, and that MHP models simultaneously expose valuable insights into the variability of predictions.



### Training Bit Fully Convolutional Network for Fast Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1612.00212v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.00212v1)
- **Published**: 2016-12-01 11:56:15+00:00
- **Updated**: 2016-12-01 11:56:15+00:00
- **Authors**: He Wen, Shuchang Zhou, Zhe Liang, Yuxiang Zhang, Dieqiao Feng, Xinyu Zhou, Cong Yao
- **Comment**: None
- **Journal**: None
- **Summary**: Fully convolutional neural networks give accurate, per-pixel prediction for input images and have applications like semantic segmentation. However, a typical FCN usually requires lots of floating point computation and large run-time memory, which effectively limits its usability. We propose a method to train Bit Fully Convolution Network (BFCN), a fully convolutional neural network that has low bit-width weights and activations. Because most of its computation-intensive convolutions are accomplished between low bit-width numbers, a BFCN can be accelerated by an efficient bit-convolution implementation. On CPU, the dot product operation between two bit vectors can be reduced to bitwise operations and popcounts, which can offer much higher throughput than 32-bit multiplications and additions.   To validate the effectiveness of BFCN, we conduct experiments on the PASCAL VOC 2012 semantic segmentation task and Cityscapes. Our BFCN with 1-bit weights and 2-bit activations, which runs 7.8x faster on CPU or requires less than 1\% resources on FPGA, can achieve comparable performance as the 32-bit counterpart.



### Learning to Generate Images of Outdoor Scenes from Attributes and Semantic Layouts
- **Arxiv ID**: http://arxiv.org/abs/1612.00215v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00215v1)
- **Published**: 2016-12-01 12:07:55+00:00
- **Updated**: 2016-12-01 12:07:55+00:00
- **Authors**: Levent Karacan, Zeynep Akata, Aykut Erdem, Erkut Erdem
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic image synthesis research has been rapidly growing with deep networks getting more and more expressive. In the last couple of years, we have observed images of digits, indoor scenes, birds, chairs, etc. being automatically generated. The expressive power of image generators have also been enhanced by introducing several forms of conditioning variables such as object names, sentences, bounding box and key-point locations. In this work, we propose a novel deep conditional generative adversarial network architecture that takes its strength from the semantic layout and scene attributes integrated as conditioning variables. We show that our architecture is able to generate realistic outdoor scene images under different conditions, e.g. day-night, sunny-foggy, with clear object boundaries.



### Fully Convolutional Crowd Counting On Highly Congested Scenes
- **Arxiv ID**: http://arxiv.org/abs/1612.00220v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00220v2)
- **Published**: 2016-12-01 12:24:35+00:00
- **Updated**: 2017-01-17 15:00:46+00:00
- **Authors**: Mark Marsden, Kevin McGuinness, Suzanne Little, Noel E. O'Connor
- **Comment**: 7 pages , VISAPP 2017
- **Journal**: None
- **Summary**: In this paper we advance the state-of-the-art for crowd counting in high density scenes by further exploring the idea of a fully convolutional crowd counting model introduced by (Zhang et al., 2016). Producing an accurate and robust crowd count estimator using computer vision techniques has attracted significant research interest in recent years. Applications for crowd counting systems exist in many diverse areas including city planning, retail, and of course general public safety. Developing a highly generalised counting model that can be deployed in any surveillance scenario with any camera perspective is the key objective for research in this area. Techniques developed in the past have generally performed poorly in highly congested scenes with several thousands of people in frame (Rodriguez et al., 2011). Our approach, influenced by the work of (Zhang et al., 2016), consists of the following contributions: (1) A training set augmentation scheme that minimises redundancy among training samples to improve model generalisation and overall counting performance; (2) a deep, single column, fully convolutional network (FCN) architecture; (3) a multi-scale averaging step during inference. The developed technique can analyse images of any resolution or aspect ratio and achieves state-of-the-art counting performance on the Shanghaitech Part B and UCF CC 50 datasets as well as competitive performance on Shanghaitech Part A.



### Video Captioning with Multi-Faceted Attention
- **Arxiv ID**: http://arxiv.org/abs/1612.00234v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00234v1)
- **Published**: 2016-12-01 13:11:29+00:00
- **Updated**: 2016-12-01 13:11:29+00:00
- **Authors**: Xiang Long, Chuang Gan, Gerard de Melo
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, video captioning has been attracting an increasing amount of interest, due to its potential for improving accessibility and information retrieval. While existing methods rely on different kinds of visual features and model structures, they do not fully exploit relevant semantic information. We present an extensible approach to jointly leverage several sorts of visual features and semantic attributes. Our novel architecture builds on LSTMs for sentence generation, with several attention layers and two multimodal layers. The attention mechanism learns to automatically select the most salient visual features or semantic attributes, and the multimodal layer yields overall representations for the input and outputs of the sentence generation component. Experimental results on the challenging MSVD and MSR-VTT datasets show that our framework outperforms the state-of-the-art approaches, while ground truth based semantic attributes are able to further elevate the output quality to a near-human level.



### A Theoretical Framework for Robustness of (Deep) Classifiers against Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1612.00334v12
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1612.00334v12)
- **Published**: 2016-12-01 16:20:39+00:00
- **Updated**: 2017-09-27 16:02:48+00:00
- **Authors**: Beilun Wang, Ji Gao, Yanjun Qi
- **Comment**: 38 pages , ICLR 2017 Workshop Track
- **Journal**: None
- **Summary**: Most machine learning classifiers, including deep neural networks, are vulnerable to adversarial examples. Such inputs are typically generated by adding small but purposeful modifications that lead to incorrect outputs while imperceptible to human eyes. The goal of this paper is not to introduce a single method, but to make theoretical steps towards fully understanding adversarial examples. By using concepts from topology, our theoretical analysis brings forth the key reasons why an adversarial example can fool a classifier ($f_1$) and adds its oracle ($f_2$, like human eyes) in such analysis. By investigating the topological relationship between two (pseudo)metric spaces corresponding to predictor $f_1$ and oracle $f_2$, we develop necessary and sufficient conditions that can determine if $f_1$ is always robust (strong-robust) against adversarial examples according to $f_2$. Interestingly our theorems indicate that just one unnecessary feature can make $f_1$ not strong-robust, and the right feature representation learning is the key to getting a classifier that is both accurate and strong-robust.



### Hippocampus Temporal Lobe Epilepsy Detection using a Combination of Shape-based Features and Spherical Harmonics Representation
- **Arxiv ID**: http://arxiv.org/abs/1612.00338v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00338v2)
- **Published**: 2016-12-01 16:27:59+00:00
- **Updated**: 2016-12-28 00:18:26+00:00
- **Authors**: Zohreh Kohan, Hamidreza Farhidzadeh, Reza Azmi, Behrouz Gholizadeh
- **Comment**: None
- **Journal**: None
- **Summary**: Most of the temporal lobe epilepsy detection approaches are based on hippocampus deformation and use complicated features, resulting, detection is done with complicated features extraction and pre-processing task. In this paper, a new detection method based on shape-based features and spherical harmonics is proposed which can analysis the hippocampus shape anomaly and detection asymmetry. This method consisted of two main parts; (1) shape feature extraction, and (2) image classification. For evaluation, HFH database is used which is publicly available in this field. Nine different geometry and 256 spherical harmonic features are introduced then selected Eighteen of them that detect the asymmetry in hippocampus significantly in a randomly selected subset of the dataset. Then a support vector machine (SVM) classifier was employed to classify the remaining images of the dataset to normal and epileptic images using our selected features. On a dataset of 25 images, 12 images were used for feature extraction and the rest 13 for classification. The results show that the proposed method has accuracy, specificity and sensitivity of, respectively, 84%, 100%, and 80%. Therefore, the proposed approach shows acceptable result and is straightforward also; complicated pre-processing steps were omitted compared to other methods.



### Global Minimum for a Finsler Elastica Minimal Path Approach
- **Arxiv ID**: http://arxiv.org/abs/1612.00343v3
- **DOI**: 10.1007/s11263-016-0975-5
- **Categories**: **cs.CG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1612.00343v3)
- **Published**: 2016-12-01 16:45:30+00:00
- **Updated**: 2018-05-21 11:44:20+00:00
- **Authors**: Da Chen, Jean-Marie Mirebeau, Laurent D. Cohen
- **Comment**: Improve the clarity of this manuscript
- **Journal**: International Journal of Computer Vision, Volume 122, Issue 3, pp
  458-483, 2017
- **Summary**: In this paper, we propose a novel curvature-penalized minimal path model via an orientation-lifted Finsler metric and the Euler elastica curve. The original minimal path model computes the globally minimal geodesic by solving an Eikonal partial differential equation (PDE). Essentially, this first-order model is unable to penalize curvature which is related to the path rigidity property in the classical active contour models. To solve this problem, we present an Eikonal PDE-based Finsler elastica minimal path approach to address the curvature-penalized geodesic energy minimization problem. We were successful at adding the curvature penalization to the classical geodesic energy. The basic idea of this work is to interpret the Euler elastica bending energy via a novel Finsler elastica metric that embeds a curvature penalty. This metric is non-Riemannian, anisotropic and asymmetric, and is defined over an orientation-lifted space by adding to the image domain the orientation as an extra space dimension. Based on this orientation lifting, the proposed minimal path model can benefit from both the curvature and orientation of the paths. Thanks to the fast marching method, the global minimum of the curvature-penalized geodesic energy can be computed efficiently. We introduce two anisotropic image data-driven speed functions that are computed by steerable filters. Based on these orientation-dependent speed functions, we can apply the proposed Finsler elastica minimal path model to the applications of closed contour detection, perceptual grouping and tubular structure extraction. Numerical experiments on both synthetic and real images show that these applications of the proposed model indeed obtain promising results.



### A Large Deformation Diffeomorphic Approach to Registration of CLARITY Images via Mutual Information
- **Arxiv ID**: http://arxiv.org/abs/1612.00356v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00356v3)
- **Published**: 2016-12-01 17:31:04+00:00
- **Updated**: 2017-08-11 15:52:28+00:00
- **Authors**: Kwame S. Kutten, Nicolas Charon, Michael I. Miller, J. T. Ratnanather, Jordan Matelsky, Alexander D. Baden, Kunal Lillaney, Karl Deisseroth, Li Ye, Joshua T. Vogelstein
- **Comment**: None
- **Journal**: None
- **Summary**: CLARITY is a method for converting biological tissues into translucent and porous hydrogel-tissue hybrids. This facilitates interrogation with light sheet microscopy and penetration of molecular probes while avoiding physical slicing. In this work, we develop a pipeline for registering CLARIfied mouse brains to an annotated brain atlas. Due to the novelty of this microscopy technique it is impractical to use absolute intensity values to align these images to existing standard atlases. Thus we adopt a large deformation diffeomorphic approach for registering images via mutual information matching. Furthermore we show how a cascaded multi-resolution approach can improve registration quality while reducing algorithm run time. As acquired image volumes were over a terabyte in size, they were far too large for work on personal computers. Therefore the NeuroData computational infrastructure was deployed for multi-resolution storage and visualization of these images and aligned annotations on the web.



### Improved Image Captioning via Policy Gradient optimization of SPIDEr
- **Arxiv ID**: http://arxiv.org/abs/1612.00370v4
- **DOI**: 10.1109/ICCV.2017.100
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1612.00370v4)
- **Published**: 2016-12-01 18:10:48+00:00
- **Updated**: 2018-03-12 18:53:06+00:00
- **Authors**: Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, Kevin Murphy
- **Comment**: Accepted at ICCV 2017
- **Journal**: None
- **Summary**: Current image captioning methods are usually trained via (penalized) maximum likelihood estimation. However, the log-likelihood score of a caption does not correlate well with human assessments of quality. Standard syntactic evaluation metrics, such as BLEU, METEOR and ROUGE, are also not well correlated. The newer SPICE and CIDEr metrics are better correlated, but have traditionally been hard to optimize for. In this paper, we show how to use a policy gradient (PG) method to directly optimize a linear combination of SPICE and CIDEr (a combination we call SPIDEr): the SPICE score ensures our captions are semantically faithful to the image, while CIDEr score ensures our captions are syntactically fluent. The PG method we propose improves on the prior MIXER approach, by using Monte Carlo rollouts instead of mixing MLE training with PG. We show empirically that our algorithm leads to easier optimization and improved results compared to MIXER. Finally, we show that using our PG method we can optimize any of the metrics, including the proposed SPIDEr metric which results in image captions that are strongly preferred by human raters compared to captions generated by the same model but trained to optimize MLE or the COCO metrics.



### Playing Doom with SLAM-Augmented Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1612.00380v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1612.00380v1)
- **Published**: 2016-12-01 18:54:51+00:00
- **Updated**: 2016-12-01 18:54:51+00:00
- **Authors**: Shehroze Bhatti, Alban Desmaison, Ondrej Miksik, Nantas Nardelli, N. Siddharth, Philip H. S. Torr
- **Comment**: None
- **Journal**: None
- **Summary**: A number of recent approaches to policy learning in 2D game domains have been successful going directly from raw input images to actions. However when employed in complex 3D environments, they typically suffer from challenges related to partial observability, combinatorial exploration spaces, path planning, and a scarcity of rewarding scenarios. Inspired from prior work in human cognition that indicates how humans employ a variety of semantic concepts and abstractions (object categories, localisation, etc.) to reason about the world, we build an agent-model that incorporates such abstractions into its policy-learning framework. We augment the raw image input to a Deep Q-Learning Network (DQN), by adding details of objects and structural elements encountered, along with the agent's localisation. The different components are automatically extracted and composed into a topological representation using on-the-fly object detection and 3D-scene reconstruction.We evaluate the efficacy of our approach in Doom, a 3D first-person combat game that exhibits a number of challenges discussed, and show that our augmented framework consistently learns better, more effective policies.



### Temporal Attention-Gated Model for Robust Sequence Classification
- **Arxiv ID**: http://arxiv.org/abs/1612.00385v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1612.00385v2)
- **Published**: 2016-12-01 19:11:24+00:00
- **Updated**: 2017-04-15 12:53:28+00:00
- **Authors**: Wenjie Pei, Tadas Baltrušaitis, David M. J. Tax, Louis-Philippe Morency
- **Comment**: Accepted by CVPR 2017
- **Journal**: None
- **Summary**: Typical techniques for sequence classification are designed for well-segmented sequences which have been edited to remove noisy or irrelevant parts. Therefore, such methods cannot be easily applied on noisy sequences expected in real-world applications. In this paper, we present the Temporal Attention-Gated Model (TAGM) which integrates ideas from attention models and gated recurrent networks to better deal with noisy or unsegmented sequences. Specifically, we extend the concept of attention model to measure the relevance of each observation (time step) of a sequence. We then use a novel gated recurrent network to learn the hidden representation for the final prediction. An important advantage of our approach is interpretability since the temporal attention weights provide a meaningful value for the salience of each time step in the sequence. We demonstrate the merits of our TAGM approach, both for prediction accuracy and interpretability, on three different tasks: spoken digit recognition, text-based sentiment analysis and visual event recognition.



### Anomaly Detection in Video Using Predictive Convolutional Long Short-Term Memory Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.00390v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00390v2)
- **Published**: 2016-12-01 19:28:59+00:00
- **Updated**: 2016-12-15 16:39:32+00:00
- **Authors**: Jefferson Ryan Medel, Andreas Savakis
- **Comment**: None
- **Journal**: None
- **Summary**: Automating the detection of anomalous events within long video sequences is challenging due to the ambiguity of how such events are defined. We approach the problem by learning generative models that can identify anomalies in videos using limited supervision. We propose end-to-end trainable composite Convolutional Long Short-Term Memory (Conv-LSTM) networks that are able to predict the evolution of a video sequence from a small number of input frames. Regularity scores are derived from the reconstruction errors of a set of predictions with abnormal video sequences yielding lower regularity scores as they diverge further from the actual sequence over time. The models utilize a composite structure and examine the effects of conditioning in learning more meaningful representations. The best model is chosen based on the reconstruction and prediction accuracy. The Conv-LSTM models are evaluated both qualitatively and quantitatively, demonstrating competitive results on anomaly detection datasets. Conv-LSTM units are shown to be an effective tool for modeling and predicting video sequences.



### Learning Shape Abstractions by Assembling Volumetric Primitives
- **Arxiv ID**: http://arxiv.org/abs/1612.00404v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00404v4)
- **Published**: 2016-12-01 20:05:27+00:00
- **Updated**: 2018-08-02 18:54:32+00:00
- **Authors**: Shubham Tulsiani, Hao Su, Leonidas J. Guibas, Alexei A. Efros, Jitendra Malik
- **Comment**: Project url: https://shubhtuls.github.io/volumetricPrimitives/
- **Journal**: None
- **Summary**: We present a learning framework for abstracting complex shapes by learning to assemble objects using 3D volumetric primitives. In addition to generating simple and geometrically interpretable explanations of 3D objects, our framework also allows us to automatically discover and exploit consistent structure in the data. We demonstrate that using our method allows predicting shape representations which can be leveraged for obtaining a consistent parsing across the instances of a shape collection and constructing an interpretable shape similarity measure. We also examine applications for image-based prediction as well as shape manipulation.



### Computerized Multiparametric MR image Analysis for Prostate Cancer Aggressiveness-Assessment
- **Arxiv ID**: http://arxiv.org/abs/1612.00408v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00408v1)
- **Published**: 2016-12-01 20:10:37+00:00
- **Updated**: 2016-12-01 20:10:37+00:00
- **Authors**: Imon Banerjee, Lewis Hahn, Geoffrey Sonn, Richard Fan, Daniel L. Rubin
- **Comment**: NIPS 2016 Workshop on Machine Learning for Health (NIPS ML4HC)
- **Journal**: None
- **Summary**: We propose an automated method for detecting aggressive prostate cancer(CaP) (Gleason score >=7) based on a comprehensive analysis of the lesion and the surrounding normal prostate tissue which has been simultaneously captured in T2-weighted MR images, diffusion-weighted images (DWI) and apparent diffusion coefficient maps (ADC). The proposed methodology was tested on a dataset of 79 patients (40 aggressive, 39 non-aggressive). We evaluated the performance of a wide range of popular quantitative imaging features on the characterization of aggressive versus non-aggressive CaP. We found that a group of 44 discriminative predictors among 1464 quantitative imaging features can be used to produce an area under the ROC curve of 0.73.



### TorontoCity: Seeing the World with a Million Eyes
- **Arxiv ID**: http://arxiv.org/abs/1612.00423v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00423v1)
- **Published**: 2016-12-01 20:39:49+00:00
- **Updated**: 2016-12-01 20:39:49+00:00
- **Authors**: Shenlong Wang, Min Bai, Gellert Mattyus, Hang Chu, Wenjie Luo, Bin Yang, Justin Liang, Joel Cheverie, Sanja Fidler, Raquel Urtasun
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we introduce the TorontoCity benchmark, which covers the full greater Toronto area (GTA) with 712.5 $km^2$ of land, 8439 $km$ of road and around 400,000 buildings. Our benchmark provides different perspectives of the world captured from airplanes, drones and cars driving around the city. Manually labeling such a large scale dataset is infeasible. Instead, we propose to utilize different sources of high-precision maps to create our ground truth. Towards this goal, we develop algorithms that allow us to align all data sources with the maps while requiring minimal human supervision. We have designed a wide variety of tasks including building height estimation (reconstruction), road centerline and curb extraction, building instance segmentation, building contour extraction (reorganization), semantic labeling and scene type classification (recognition). Our pilot study shows that most of these tasks are still difficult for modern convolutional neural networks.



### Efficient Pose and Cell Segmentation using Column Generation
- **Arxiv ID**: http://arxiv.org/abs/1612.00437v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00437v1)
- **Published**: 2016-12-01 20:56:17+00:00
- **Updated**: 2016-12-01 20:56:17+00:00
- **Authors**: Shaofei Wang, Chong Zhang, Miguel A. Gonzalez-Ballester, Julian Yarkony
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problems of multi-person pose segmentation in natural images and instance segmentation in biological images with crowded cells. We formulate these distinct tasks as integer programs where variables correspond to poses/cells. To optimize, we propose a generic relaxation scheme for solving these combinatorial problems using a column generation formulation where the program for generating a column is solved via exact optimization of very small scale integer programs. This results in efficient exploration of the spaces of poses and cells.



### Understanding image motion with group representations
- **Arxiv ID**: http://arxiv.org/abs/1612.00472v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1612.00472v2)
- **Published**: 2016-12-01 21:18:45+00:00
- **Updated**: 2018-02-26 17:33:45+00:00
- **Authors**: Andrew Jaegle, Stephen Phillips, Daphne Ippolito, Kostas Daniilidis
- **Comment**: Published as a conference paper at ICLR 2018; 14 pages, including
  references and supplement
- **Journal**: None
- **Summary**: Motion is an important signal for agents in dynamic environments, but learning to represent motion from unlabeled video is a difficult and underconstrained problem. We propose a model of motion based on elementary group properties of transformations and use it to train a representation of image motion. While most methods of estimating motion are based on pixel-level constraints, we use these group properties to constrain the abstract representation of motion itself. We demonstrate that a deep neural network trained using this method captures motion in both synthetic 2D sequences and real-world sequences of vehicle motion, without requiring any labels. Networks trained to respect these constraints implicitly identify the image characteristic of motion in different sequence types. In the context of vehicle motion, this method extracts information useful for localization, tracking, and odometry. Our results demonstrate that this representation is useful for learning motion in the general setting where explicit labels are difficult to obtain.



### In Teacher We Trust: Learning Compressed Models for Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/1612.00478v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00478v1)
- **Published**: 2016-12-01 21:37:19+00:00
- **Updated**: 2016-12-01 21:37:19+00:00
- **Authors**: Jonathan Shen, Noranart Vesdapunt, Vishnu N. Boddeti, Kris M. Kitani
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks continue to advance the state-of-the-art in many domains as they grow bigger and more complex. It has been observed that many of the parameters of a large network are redundant, allowing for the possibility of learning a smaller network that mimics the outputs of the large network through a process called Knowledge Distillation. We show, however, that standard Knowledge Distillation is not effective for learning small models for the task of pedestrian detection. To improve this process, we introduce a higher-dimensional hint layer to increase information flow. We also estimate the variance in the outputs of the large network and propose a loss function to incorporate this uncertainty. Finally, we attempt to boost the complexity of the small network without increasing its size by using as input hand-designed features that have been demonstrated to be effective for pedestrian detection. We succeed in training a model that contains $400\times$ fewer parameters than the large network while outperforming AlexNet on the Caltech Pedestrian Dataset.



### 3D Bounding Box Estimation Using Deep Learning and Geometry
- **Arxiv ID**: http://arxiv.org/abs/1612.00496v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00496v2)
- **Published**: 2016-12-01 22:16:48+00:00
- **Updated**: 2017-04-10 19:05:46+00:00
- **Authors**: Arsalan Mousavian, Dragomir Anguelov, John Flynn, Jana Kosecka
- **Comment**: To appear in IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR) 2017
- **Journal**: None
- **Summary**: We present a method for 3D object detection and pose estimation from a single image. In contrast to current techniques that only regress the 3D orientation of an object, our method first regresses relatively stable 3D object properties using a deep convolutional neural network and then combines these estimates with geometric constraints provided by a 2D object bounding box to produce a complete 3D bounding box. The first network output estimates the 3D object orientation using a novel hybrid discrete-continuous loss, which significantly outperforms the L2 loss. The second output regresses the 3D object dimensions, which have relatively little variance compared to alternatives and can often be predicted for many object types. These estimates, combined with the geometric constraints on translation imposed by the 2D bounding box, enable us to recover a stable and accurate 3D object pose. We evaluate our method on the challenging KITTI object detection benchmark both on the official metric of 3D orientation estimation and also on the accuracy of the obtained 3D bounding boxes. Although conceptually simple, our method outperforms more complex and computationally expensive approaches that leverage semantic segmentation, instance level segmentation and flat ground priors and sub-category detection. Our discrete-continuous loss also produces state of the art results for 3D viewpoint estimation on the Pascal 3D+ dataset.



### FINN: A Framework for Fast, Scalable Binarized Neural Network Inference
- **Arxiv ID**: http://arxiv.org/abs/1612.07119v1
- **DOI**: 10.1145/3020078.3021744
- **Categories**: **cs.CV**, cs.AR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.07119v1)
- **Published**: 2016-12-01 22:19:47+00:00
- **Updated**: 2016-12-01 22:19:47+00:00
- **Authors**: Yaman Umuroglu, Nicholas J. Fraser, Giulio Gambardella, Michaela Blott, Philip Leong, Magnus Jahre, Kees Vissers
- **Comment**: To appear in the 25th International Symposium on Field-Programmable
  Gate Arrays, February 2017
- **Journal**: None
- **Summary**: Research has shown that convolutional neural networks contain significant redundancy, and high classification accuracy can be obtained even when weights and activations are reduced from floating point to binary values. In this paper, we present FINN, a framework for building fast and flexible FPGA accelerators using a flexible heterogeneous streaming architecture. By utilizing a novel set of optimizations that enable efficient mapping of binarized neural networks to hardware, we implement fully connected, convolutional and pooling layers, with per-layer compute resources being tailored to user-provided throughput requirements. On a ZC706 embedded FPGA platform drawing less than 25 W total system power, we demonstrate up to 12.3 million image classifications per second with 0.31 {\mu}s latency on the MNIST dataset with 95.8% accuracy, and 21906 image classifications per second with 283 {\mu}s latency on the CIFAR-10 and SVHN datasets with respectively 80.1% and 94.9% accuracy. To the best of our knowledge, ours are the fastest classification rates reported to date on these benchmarks.



### Object-Centric Representation Learning from Unlabeled Videos
- **Arxiv ID**: http://arxiv.org/abs/1612.00500v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00500v1)
- **Published**: 2016-12-01 22:36:20+00:00
- **Updated**: 2016-12-01 22:36:20+00:00
- **Authors**: Ruohan Gao, Dinesh Jayaraman, Kristen Grauman
- **Comment**: In Proceedings of the Asian Conference on Computer Vision (ACCV),
  2016
- **Journal**: None
- **Summary**: Supervised (pre-)training currently yields state-of-the-art performance for representation learning for visual recognition, yet it comes at the cost of (1) intensive manual annotations and (2) an inherent restriction in the scope of data relevant for learning. In this work, we explore unsupervised feature learning from unlabeled video. We introduce a novel object-centric approach to temporal coherence that encourages similar representations to be learned for object-like regions segmented from nearby frames. Our framework relies on a Siamese-triplet network to train a deep convolutional neural network (CNN) representation. Compared to existing temporal coherence methods, our idea has the advantage of lightweight preprocessing of the unlabeled video (no tracking required) while still being able to extract object-level regions from which to learn invariances. Furthermore, as we show in results on several standard datasets, our method typically achieves substantial accuracy gains over competing unsupervised methods for image classification and retrieval tasks.



