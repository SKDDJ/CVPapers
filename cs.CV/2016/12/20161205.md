# Arxiv Papers in cs.CV on 2016-12-05
### Deep Metric Learning via Facility Location
- **Arxiv ID**: http://arxiv.org/abs/1612.01213v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.01213v2)
- **Published**: 2016-12-05 01:09:35+00:00
- **Updated**: 2017-04-11 17:24:42+00:00
- **Authors**: Hyun Oh Song, Stefanie Jegelka, Vivek Rathod, Kevin Murphy
- **Comment**: Submission accepted at CVPR 2017
- **Journal**: None
- **Summary**: Learning the representation and the similarity metric in an end-to-end fashion with deep networks have demonstrated outstanding results for clustering and retrieval. However, these recent approaches still suffer from the performance degradation stemming from the local metric training procedure which is unaware of the global structure of the embedding space.   We propose a global metric learning scheme for optimizing the deep metric embedding with the learnable clustering function and the clustering metric (NMI) in a novel structured prediction framework.   Our experiments on CUB200-2011, Cars196, and Stanford online products datasets show state of the art performance both on the clustering and retrieval tasks measured in the NMI and Recall@K evaluation metrics.



### Deep Multi-Modal Image Correspondence Learning
- **Arxiv ID**: http://arxiv.org/abs/1612.01225v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01225v1)
- **Published**: 2016-12-05 02:16:09+00:00
- **Updated**: 2016-12-05 02:16:09+00:00
- **Authors**: Chen Liu, Jiajun Wu, Pushmeet Kohli, Yasutaka Furukawa
- **Comment**: None
- **Journal**: None
- **Summary**: Inference of correspondences between images from different modalities is an extremely important perceptual ability that enables humans to understand and recognize cross-modal concepts. In this paper, we consider an instance of this problem that involves matching photographs of building interiors with their corresponding floorplan. This is a particularly challenging problem because a floorplan, as a stylized architectural drawing, is very different in appearance from a color photograph. Furthermore, individual photographs by themselves depict only a part of a floorplan (e.g., kitchen, bathroom, and living room). We propose the use of a number of different neural network architectures for this task, which are trained and evaluated on a novel large-scale dataset of 5 million floorplan images and 80 million associated photographs. Experimental evaluation reveals that our neural network architectures are able to identify visual cues that result in reliable matches across these two quite different modalities. In fact, the trained networks are able to even outperform human subjects in several challenging image matching problems. Our result implies that neural networks are effective at perceptual tasks that require long periods of reasoning even for humans to solve.



### Deep Blur Mapping: Exploiting High-Level Semantics by Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.01227v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01227v2)
- **Published**: 2016-12-05 02:21:39+00:00
- **Updated**: 2018-06-12 02:43:21+00:00
- **Authors**: Kede Ma, Huan Fu, Tongliang Liu, Zhou Wang, Dacheng Tao
- **Comment**: Tech report
- **Journal**: None
- **Summary**: The human visual system excels at detecting local blur of visual images, but the underlying mechanism is not well understood. Traditional views of blur such as reduction in energy at high frequencies and loss of phase coherence at localized features have fundamental limitations. For example, they cannot well discriminate flat regions from blurred ones. Here we propose that high-level semantic information is critical in successfully identifying local blur. Therefore, we resort to deep neural networks that are proficient at learning high-level features and propose the first end-to-end local blur mapping algorithm based on a fully convolutional network. By analyzing various architectures with different depths and design philosophies, we empirically show that high-level features of deeper layers play a more important role than low-level features of shallower layers in resolving challenging ambiguities for this task. We test the proposed method on a standard blur detection benchmark and demonstrate that it significantly advances the state-of-the-art (ODS F-score of 0.853). Furthermore, we explore the use of the generated blur maps in three applications, including blur region segmentation, blur degree estimation, and blur magnification.



### Deep Pyramidal Residual Networks with Separated Stochastic Depth
- **Arxiv ID**: http://arxiv.org/abs/1612.01230v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01230v1)
- **Published**: 2016-12-05 02:41:18+00:00
- **Updated**: 2016-12-05 02:41:18+00:00
- **Authors**: Yoshihiro Yamada, Masakazu Iwamura, Koichi Kise
- **Comment**: None
- **Journal**: None
- **Summary**: On general object recognition, Deep Convolutional Neural Networks (DCNNs) achieve high accuracy. In particular, ResNet and its improvements have broken the lowest error rate records. In this paper, we propose a method to successfully combine two ResNet improvements, ResDrop and PyramidNet. We confirmed that the proposed network outperformed the conventional methods; on CIFAR-100, the proposed network achieved an error rate of 16.18% in contrast to PiramidNet achieving that of 18.29% and ResNeXt 17.31%.



### Multi-way Particle Swarm Fusion
- **Arxiv ID**: http://arxiv.org/abs/1612.01234v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01234v1)
- **Published**: 2016-12-05 03:05:21+00:00
- **Updated**: 2016-12-05 03:05:21+00:00
- **Authors**: Chen Liu, Hang Yan, Pushmeet Kohli, Yasutaka Furukawa
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel MAP inference framework for Markov Random Field (MRF) in parallel computing environments. The inference framework, dubbed Swarm Fusion, is a natural generalization of the Fusion Move method. Every thread (in a case of multi-threading environments) maintains and updates a solution. At each iteration, a thread can generate arbitrary number of solution proposals and take arbitrary number of concurrent solutions from the other threads to perform multi-way fusion in updating its solution. The framework is general, making popular existing inference techniques such as alpha-expansion, fusion move, parallel alpha-expansion, and hierarchical fusion, its special cases. We have evaluated the effectiveness of our approach against competing methods on three problems of varying difficulties, in particular, the stereo, the optical flow, and the layered depthmap estimation problems.



### Turning an Urban Scene Video into a Cinemagraph
- **Arxiv ID**: http://arxiv.org/abs/1612.01235v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01235v1)
- **Published**: 2016-12-05 03:09:27+00:00
- **Updated**: 2016-12-05 03:09:27+00:00
- **Authors**: Hang Yan, Yebin Liu, Yasutaka Furukawa
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes an algorithm that turns a regular video capturing urban scenes into a high-quality endless animation, known as a Cinemagraph. The creation of a Cinemagraph usually requires a static camera in a carefully configured scene. The task becomes challenging for a regular video with a moving camera and objects. Our approach first warps an input video into the viewpoint of a reference camera. Based on the warped video, we propose effective temporal analysis algorithms to detect regions with static geometry and dynamic appearance, where geometric modeling is reliable and visually attractive animations can be created. Lastly, the algorithm applies a sequence of video processing techniques to produce a Cinemagraph movie. We have tested the proposed approach on numerous challenging real scenes. To our knowledge, this work is the first to automatically generate Cinemagraph animations from regular movies in the wild.



### Cancerous Nuclei Detection and Scoring in Breast Cancer Histopathological Images
- **Arxiv ID**: http://arxiv.org/abs/1612.01237v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01237v1)
- **Published**: 2016-12-05 03:27:21+00:00
- **Updated**: 2016-12-05 03:27:21+00:00
- **Authors**: Pegah Faridi, Habibollah Danyali, Mohammad Sadegh Helfroush, Mojgan Akbarzadeh Jahromi
- **Comment**: None
- **Journal**: None
- **Summary**: Early detection and prognosis of breast cancer are feasible by utilizing histopathological grading of biopsy specimens. This research is focused on detection and grading of nuclear pleomorphism in histopathological images of breast cancer. The proposed method consists of three internal steps. First, unmixing colors of H&E is used in the preprocessing step. Second, nuclei boundaries are extracted incorporating the center of cancerous nuclei which are detected by applying morphological operations and Difference of Gaussian filter on the preprocessed image. Finally, segmented nuclei are scored to accomplish one parameter of the Nottingham grading system for breast cancer. In this approach, the nuclei area, chromatin density, contour regularity, and nucleoli presence, are features for nuclear pleomorphism scoring. Experimental results showed that the proposed algorithm, with an accuracy of 86.6%, made significant advancement in detecting cancerous nuclei compared to existing methods in the related literature.



### Deep Image Category Discovery using a Transferred Similarity Function
- **Arxiv ID**: http://arxiv.org/abs/1612.01253v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.01253v1)
- **Published**: 2016-12-05 05:41:26+00:00
- **Updated**: 2016-12-05 05:41:26+00:00
- **Authors**: Yen-Chang Hsu, Zhaoyang Lv, Zsolt Kira
- **Comment**: 13 pages, 9 figures
- **Journal**: None
- **Summary**: Automatically discovering image categories in unlabeled natural images is one of the important goals of unsupervised learning. However, the task is challenging and even human beings define visual categories based on a large amount of prior knowledge. In this paper, we similarly utilize prior knowledge to facilitate the discovery of image categories. We present a novel end-to-end network to map unlabeled images to categories as a clustering network. We propose that this network can be learned with contrastive loss which is only based on weak binary pair-wise constraints. Such binary constraints can be learned from datasets in other domains as transferred similarity functions, which mimic a simple knowledge transfer. We first evaluate our experiments on the MNIST dataset as a proof of concept, based on predicted similarities trained on Omniglot, showing a 99\% accuracy which significantly outperforms clustering based approaches. Then we evaluate the discovery performance on Cifar-10, STL-10, and ImageNet, which achieves both state-of-the-art accuracy and shows it can be scalable to various large natural images.



### Panoramic Structure from Motion via Geometric Relationship Detection
- **Arxiv ID**: http://arxiv.org/abs/1612.01256v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01256v1)
- **Published**: 2016-12-05 06:24:10+00:00
- **Updated**: 2016-12-05 06:24:10+00:00
- **Authors**: Satoshi Ikehata, Ivaylo Boyadzhiev, Qi Shan, Yasutaka Furukawa
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of Structure from Motion (SfM) for indoor panoramic image streams, extremely challenging even for the state-of-the-art due to the lack of textures and minimal parallax. The key idea is the fusion of single-view and multi-view reconstruction techniques via geometric relationship detection (e.g., detecting 2D lines as coplanar in 3D). Rough geometry suffices to perform such detection, and our approach utilizes rough surface normal estimates from an image-to-normal deep network to discover geometric relationships among lines. The detected relationships provide exact geometric constraints in our line-based linear SfM formulation. A constrained linear least squares is used to reconstruct a 3D model and camera motions, followed by the bundle adjustment. We have validated our algorithm on challenging datasets, outperforming various state-of-the-art reconstruction techniques.



### Point Pair Feature based Object Detection for Random Bin Picking
- **Arxiv ID**: http://arxiv.org/abs/1612.01288v1
- **DOI**: 10.1109/CRV.2016.59
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01288v1)
- **Published**: 2016-12-05 09:57:45+00:00
- **Updated**: 2016-12-05 09:57:45+00:00
- **Authors**: Wim Abbeloos, Toon Goedemé
- **Comment**: None
- **Journal**: None
- **Summary**: Point pair features are a popular representation for free form 3D object detection and pose estimation. In this paper, their performance in an industrial random bin picking context is investigated. A new method to generate representative synthetic datasets is proposed. This allows to investigate the influence of a high degree of clutter and the presence of self similar features, which are typical to our application. We provide an overview of solutions proposed in literature and discuss their strengths and weaknesses. A simple heuristic method to drastically reduce the computational complexity is introduced, which results in improved robustness, speed and accuracy compared to the naive approach.



### Message Passing Multi-Agent GANs
- **Arxiv ID**: http://arxiv.org/abs/1612.01294v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1612.01294v1)
- **Published**: 2016-12-05 10:10:13+00:00
- **Updated**: 2016-12-05 10:10:13+00:00
- **Authors**: Arnab Ghosh, Viveka Kulharia, Vinay Namboodiri
- **Comment**: The first 2 authors contributed equally for this work
- **Journal**: None
- **Summary**: Communicating and sharing intelligence among agents is an important facet of achieving Artificial General Intelligence. As a first step towards this challenge, we introduce a novel framework for image generation: Message Passing Multi-Agent Generative Adversarial Networks (MPM GANs). While GANs have recently been shown to be very effective for image generation and other tasks, these networks have been limited to mostly single generator-discriminator networks. We show that we can obtain multi-agent GANs that communicate through message passing to achieve better image generation. The objectives of the individual agents in this framework are two fold: a co-operation objective and a competing objective. The co-operation objective ensures that the message sharing mechanism guides the other generator to generate better than itself while the competing objective encourages each generator to generate better than its counterpart. We analyze and visualize the messages that these GANs share among themselves in various scenarios. We quantitatively show that the message sharing formulation serves as a regularizer for the adversarial training. Qualitatively, we show that the different generators capture different traits of the underlying data distribution.



### Stereo image de-fencing using smartphones
- **Arxiv ID**: http://arxiv.org/abs/1612.01323v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01323v1)
- **Published**: 2016-12-05 11:56:56+00:00
- **Updated**: 2016-12-05 11:56:56+00:00
- **Authors**: Sankaraganesh Jonna, Sukla Satapathy, Rajiv R. Sahay
- **Comment**: Under review as a conference paper
- **Journal**: None
- **Summary**: Conventional approaches to image de-fencing have limited themselves to using only image data in adjacent frames of the captured video of an approximately static scene. In this work, we present a method to harness disparity using a stereo pair of fenced images in order to detect fence pixels. Tourists and amateur photographers commonly carry smartphones/phablets which can be used to capture a short video sequence of the fenced scene. We model the formation of the occluded frames in the captured video. Furthermore, we propose an optimization framework to estimate the de-fenced image using the total variation prior to regularize the ill-posed problem.



### Classification With an Edge: Improving Semantic Image Segmentation with Boundary Detection
- **Arxiv ID**: http://arxiv.org/abs/1612.01337v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01337v2)
- **Published**: 2016-12-05 13:12:24+00:00
- **Updated**: 2017-12-21 21:36:14+00:00
- **Authors**: Dimitrios Marmanis, Konrad Schindler, Jan Dirk Wegner, Silvano Galliani, Mihai Datcu, Uwe Stilla
- **Comment**: None
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing, Volume 135,
  January 2018, Pages 158-172, ISSN 0924-2716,
  https://doi.org/10.1016/j.isprsjprs.2017.11.009
- **Summary**: We present an end-to-end trainable deep convolutional neural network (DCNN) for semantic segmentation with built-in awareness of semantically meaningful boundaries. Semantic segmentation is a fundamental remote sensing task, and most state-of-the-art methods rely on DCNNs as their workhorse. A major reason for their success is that deep networks learn to accumulate contextual information over very large windows (receptive fields). However, this success comes at a cost, since the associated loss of effecive spatial resolution washes out high-frequency details and leads to blurry object boundaries. Here, we propose to counter this effect by combining semantic segmentation with semantically informed edge detection, thus making class-boundaries explicit in the model, First, we construct a comparatively simple, memory-efficient model by adding boundary detection to the Segnet encoder-decoder architecture. Second, we also include boundary detection in FCN-type models and set up a high-end classifier ensemble. We show that boundary detection significantly improves semantic segmentation with CNNs. Our high-end ensemble achieves > 90% overall accuracy on the ISPRS Vaihingen benchmark.



### Highly Efficient Regression for Scalable Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1612.01341v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01341v1)
- **Published**: 2016-12-05 13:23:42+00:00
- **Updated**: 2016-12-05 13:23:42+00:00
- **Authors**: Hanxiao Wang, Shaogang Gong, Tao Xiang
- **Comment**: None
- **Journal**: None
- **Summary**: Existing person re-identification models are poor for scaling up to large data required in real-world applications due to: (1) Complexity: They employ complex models for optimal performance resulting in high computational cost for training at a large scale; (2) Inadaptability: Once trained, they are unsuitable for incremental update to incorporate any new data available. This work proposes a truly scalable solution to re-id by addressing both problems. Specifically, a Highly Efficient Regression (HER) model is formulated by embedding the Fisher's criterion to a ridge regression model for very fast re-id model learning with scalable memory/storage usage. Importantly, this new HER model supports faster than real-time incremental model updates therefore making real-time active learning feasible in re-id with human-in-the-loop. Extensive experiments show that such a simple and fast model not only outperforms notably the state-of-the-art re-id methods, but also is more scalable to large data with additional benefits to active learning for reducing human labelling effort in re-id deployment.



### Human-In-The-Loop Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1612.01345v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01345v2)
- **Published**: 2016-12-05 13:29:47+00:00
- **Updated**: 2018-05-04 19:56:18+00:00
- **Authors**: Hanxiao Wang, Shaogang Gong, Xiatian Zhu, Tao Xiang
- **Comment**: None
- **Journal**: None
- **Summary**: Current person re-identification (re-id) methods assume that (1) pre-labelled training data are available for every camera pair, (2) the gallery size for re-identification is moderate. Both assumptions scale poorly to real-world applications when camera network size increases and gallery size becomes large. Human verification of automatic model ranked re-id results becomes inevitable. In this work, a novel human-in-the-loop re-id model based on Human Verification Incremental Learning (HVIL) is formulated which does not require any pre-labelled training data to learn a model, therefore readily scalable to new camera pairs. This HVIL model learns cumulatively from human feedback to provide instant improvement to re-id ranking of each probe on-the-fly enabling the model scalable to large gallery sizes. We further formulate a Regularised Metric Ensemble Learning (RMEL) model to combine a series of incrementally learned HVIL models into a single ensemble model to be used when human feedback becomes unavailable.



### On-Demand Learning for Deep Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1612.01380v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01380v3)
- **Published**: 2016-12-05 14:53:23+00:00
- **Updated**: 2017-08-02 14:48:27+00:00
- **Authors**: Ruohan Gao, Kristen Grauman
- **Comment**: International Conference on Computer Vision (ICCV), Venice, Italy,
  Oct. 2017
- **Journal**: None
- **Summary**: While machine learning approaches to image restoration offer great promise, current methods risk training models fixated on performing well only for image corruption of a particular level of difficulty---such as a certain level of noise or blur. First, we examine the weakness of conventional "fixated" models and demonstrate that training general models to handle arbitrary levels of corruption is indeed non-trivial. Then, we propose an on-demand learning algorithm for training image restoration models with deep convolutional neural networks. The main idea is to exploit a feedback mechanism to self-generate training instances where they are needed most, thereby learning models that can generalize across difficulty levels. On four restoration tasks---image inpainting, pixel interpolation, image deblurring, and image denoising---and three diverse datasets, our approach consistently outperforms both the status quo training procedure and curriculum learning alternatives.



### ImageNet pre-trained models with batch normalization
- **Arxiv ID**: http://arxiv.org/abs/1612.01452v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01452v2)
- **Published**: 2016-12-05 17:49:41+00:00
- **Updated**: 2016-12-06 12:20:00+00:00
- **Authors**: Marcel Simon, Erik Rodner, Joachim Denzler
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNN) pre-trained on ImageNet are the backbone of most state-of-the-art approaches. In this paper, we present a new set of pre-trained models with popular state-of-the-art architectures for the Caffe framework. The first release includes Residual Networks (ResNets) with generation script as well as the batch-normalization-variants of AlexNet and VGG19. All models outperform previous models with the same architecture. The models and training code are available at http://www.inf-cv.uni-jena.de/Research/CNN+Models.html and https://github.com/cvjena/cnn-models



### ArtTrack: Articulated Multi-person Tracking in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1612.01465v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01465v3)
- **Published**: 2016-12-05 18:38:56+00:00
- **Updated**: 2017-05-09 09:56:46+00:00
- **Authors**: Eldar Insafutdinov, Mykhaylo Andriluka, Leonid Pishchulin, Siyu Tang, Evgeny Levinkov, Bjoern Andres, Bernt Schiele
- **Comment**: Accepted to CVPR 2017
- **Journal**: None
- **Summary**: In this paper we propose an approach for articulated tracking of multiple people in unconstrained videos. Our starting point is a model that resembles existing architectures for single-frame pose estimation but is substantially faster. We achieve this in two ways: (1) by simplifying and sparsifying the body-part relationship graph and leveraging recent methods for faster inference, and (2) by offloading a substantial share of computation onto a feed-forward convolutional architecture that is able to detect and associate body joints of the same person even in clutter. We use this model to generate proposals for body joint locations and formulate articulated tracking as spatio-temporal grouping of such proposals. This allows to jointly solve the association problem for all people in the scene by propagating evidence from strong detections through time and enforcing constraints that each proposal can be assigned to one person only. We report results on a public MPII Human Pose benchmark and on a new MPII Video Pose dataset of image sequences with multiple people. We demonstrate that our model achieves state-of-the-art results while using only a fraction of time and is able to leverage temporal information to improve state-of-the-art for crowded scenes.



### Authoring image decompositions with generative models
- **Arxiv ID**: http://arxiv.org/abs/1612.01479v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01479v1)
- **Published**: 2016-12-05 18:59:53+00:00
- **Updated**: 2016-12-05 18:59:53+00:00
- **Authors**: Jason Rock, Theerasit Issaranon, Aditya Deshpande, David Forsyth
- **Comment**: None
- **Journal**: None
- **Summary**: We show how to extend traditional intrinsic image decompositions to incorporate further layers above albedo and shading. It is hard to obtain data to learn a multi-layer decomposition. Instead, we can learn to decompose an image into layers that are "like this" by authoring generative models for each layer using proxy examples that capture the Platonic ideal (Mondrian images for albedo; rendered 3D primitives for shading; material swatches for shading detail). Our method then generates image layers, one from each model, that explain the image. Our approach rests on innovation in generative models for images. We introduce a Convolutional Variational Auto Encoder (conv-VAE), a novel VAE architecture that can reconstruct high fidelity images. The approach is general, and does not require that layers admit a physical interpretation.



### ROAM: a Rich Object Appearance Model with Application to Rotoscoping
- **Arxiv ID**: http://arxiv.org/abs/1612.01495v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01495v1)
- **Published**: 2016-12-05 20:03:18+00:00
- **Updated**: 2016-12-05 20:03:18+00:00
- **Authors**: Ondrej Miksik, Juan-Manuel Pérez-Rúa, Philip H. S. Torr, Patrick Pérez
- **Comment**: None
- **Journal**: None
- **Summary**: Rotoscoping, the detailed delineation of scene elements through a video shot, is a painstaking task of tremendous importance in professional post-production pipelines. While pixel-wise segmentation techniques can help for this task, professional rotoscoping tools rely on parametric curves that offer the artists a much better interactive control on the definition, editing and manipulation of the segments of interest. Sticking to this prevalent rotoscoping paradigm, we propose a novel framework to capture and track the visual aspect of an arbitrary object in a scene, given a first closed outline of this object. This model combines a collection of local foreground/background appearance models spread along the outline, a global appearance model of the enclosed object and a set of distinctive foreground landmarks. The structure of this rich appearance model allows simple initialization, efficient iterative optimization with exact minimization at each step, and on-line adaptation in videos. We demonstrate qualitatively and quantitatively the merit of this framework through comparisons with tools based on either dynamic segmentation with a closed curve or pixel-wise binary labelling.



### Explaining Radiological Emphysema Subtypes with Unsupervised Texture Prototypes: MESA COPD Study
- **Arxiv ID**: http://arxiv.org/abs/1612.01820v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01820v1)
- **Published**: 2016-12-05 20:42:20+00:00
- **Updated**: 2016-12-05 20:42:20+00:00
- **Authors**: Jie Yang, Elsa D. Angelini, Benjamin M. Smith, John H. M. Austin, Eric A. Hoffman, David A. Bluemke, R. Graham Barr, Andrew F. Laine
- **Comment**: MICCAI workshop on Medical Computer Vision: Algorithms for Big Data
  (2016)
- **Journal**: None
- **Summary**: Pulmonary emphysema is traditionally subcategorized into three subtypes, which have distinct radiological appearances on computed tomography (CT) and can help with the diagnosis of chronic obstructive pulmonary disease (COPD). Automated texture-based quantification of emphysema subtypes has been successfully implemented via supervised learning of these three emphysema subtypes. In this work, we demonstrate that unsupervised learning on a large heterogeneous database of CT scans can generate texture prototypes that are visually homogeneous and distinct, reproducible across subjects, and capable of predicting accurately the three standard radiological subtypes. These texture prototypes enable automated labeling of lung volumes, and open the way to new interpretations of lung CT scans with finer subtyping of emphysema.



### Towards the Limit of Network Quantization
- **Arxiv ID**: http://arxiv.org/abs/1612.01543v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1612.01543v2)
- **Published**: 2016-12-05 21:04:17+00:00
- **Updated**: 2017-11-13 19:44:32+00:00
- **Authors**: Yoojin Choi, Mostafa El-Khamy, Jungwon Lee
- **Comment**: Published as a conference paper at ICLR 2017
- **Journal**: None
- **Summary**: Network quantization is one of network compression techniques to reduce the redundancy of deep neural networks. It reduces the number of distinct network parameter values by quantization in order to save the storage for them. In this paper, we design network quantization schemes that minimize the performance loss due to quantization given a compression ratio constraint. We analyze the quantitative relation of quantization errors to the neural network loss function and identify that the Hessian-weighted distortion measure is locally the right objective function for the optimization of network quantization. As a result, Hessian-weighted k-means clustering is proposed for clustering network parameters to quantize. When optimal variable-length binary codes, e.g., Huffman codes, are employed for further compression, we derive that the network quantization problem can be related to the entropy-constrained scalar quantization (ECSQ) problem in information theory and consequently propose two solutions of ECSQ for network quantization, i.e., uniform quantization and an iterative solution similar to Lloyd's algorithm. Finally, using the simple uniform quantization followed by Huffman coding, we show from our experiments that the compression ratios of 51.25, 22.17 and 40.65 are achievable for LeNet, 32-layer ResNet and AlexNet, respectively.



### Object Classification with Joint Projection and Low-rank Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/1612.01594v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01594v1)
- **Published**: 2016-12-05 23:49:26+00:00
- **Updated**: 2016-12-05 23:49:26+00:00
- **Authors**: Homa Foroughi, Nilanjan Ray, Hong Zhang
- **Comment**: arXiv admin note: text overlap with arXiv:1603.07697; text overlap
  with arXiv:1404.3606 by other authors
- **Journal**: None
- **Summary**: For an object classification system, the most critical obstacles towards real-world applications are often caused by large intra-class variability, arising from different lightings, occlusion and corruption, in limited sample sets. Most methods in the literature would fail when the training samples are heavily occluded, corrupted or have significant illumination or viewpoint variations. Besides, most of the existing methods and especially deep learning-based methods, need large training sets to achieve a satisfactory recognition performance. Although using the pre-trained network on a generic large-scale dataset and fine-tune it to the small-sized target dataset is a widely used technique, this would not help when the content of base and target datasets are very different. To address these issues, we propose a joint projection and low-rank dictionary learning method using dual graph constraints (JP-LRDL). The proposed joint learning method would enable us to learn the features on top of which dictionaries can be better learned, from the data with large intra-class variability. Specifically, a structured class-specific dictionary is learned and the discrimination is further improved by imposing a graph constraint on the coding coefficients, that maximizes the intra-class compactness and inter-class separability. We also enforce low-rank and structural incoherence constraints on sub-dictionaries to make them more compact and robust to variations and outliers and reduce the redundancy among them, respectively. To preserve the intrinsic structure of data and penalize unfavourable relationship among training samples simultaneously, we introduce a projection graph into the framework, which significantly enhances the discriminative ability of the projection matrix and makes the method robust to small-sized and high-dimensional datasets.



