# Arxiv Papers in cs.CV on 2016-12-13
### Deep Convolutional Poses for Human Interaction Recognition in Monocular Videos
- **Arxiv ID**: http://arxiv.org/abs/1612.03982v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.03982v1)
- **Published**: 2016-12-13 00:22:58+00:00
- **Updated**: 2016-12-13 00:22:58+00:00
- **Authors**: Marcel Sheeny de Moraes, Sankha Mukherjee, Neil M Robertson
- **Comment**: None
- **Journal**: None
- **Summary**: Human interaction recognition is a challenging problem in computer vision and has been researched over the years due to its important applications. With the development of deep models for the human pose estimation problem, this work aims to verify the effectiveness of using the human pose in order to recognize the human interaction in monocular videos. This paper developed a method based on 5 steps: detect each person in the scene, track them, retrieve the human pose, extract features based on the pose and finally recognize the interaction using a classifier. The Two-Person interaction dataset was used for the development of this methodology. Using a whole sequence evaluation approach it achieved 87.56% of average accuracy of all interaction. Yun, et at achieved 91.10% using the same dataset, however their methodology used the depth sensor to recognize the interaction. The methodology developed in this paper shows that an RGB camera can be as effective as depth cameras to recognize the interaction between two persons using the recent development of deep models to estimate the human pose.



### A Video-Based Method for Objectively Rating Ataxia
- **Arxiv ID**: http://arxiv.org/abs/1612.04007v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.04007v3)
- **Published**: 2016-12-13 03:21:57+00:00
- **Updated**: 2017-09-07 16:43:21+00:00
- **Authors**: Ronnachai Jaroensri, Amy Zhao, Guha Balakrishnan, Derek Lo, Jeremy Schmahmann, John Guttag, Fredo Durand
- **Comment**: MLHC 2017
- **Journal**: None
- **Summary**: For many movement disorders, such as Parkinson's disease and ataxia, disease progression is visually assessed by a clinician using a numerical disease rating scale. These tests are subjective, time-consuming, and must be administered by a professional. This can be problematic where specialists are not available, or when a patient is not consistently evaluated by the same clinician. We present an automated method for quantifying the severity of motion impairment in patients with ataxia, using only video recordings. We consider videos of the finger-to-nose test, a common movement task used as part of the assessment of ataxia progression during the course of routine clinical checkups.   Our method uses neural network-based pose estimation and optical flow techniques to track the motion of the patient's hand in a video recording. We extract features that describe qualities of the motion such as speed and variation in performance. Using labels provided by an expert clinician, we train a supervised learning model that predicts severity according to the Brief Ataxia Rating Scale (BARS). The performance of our system is comparable to that of a group of ataxia specialists in terms of mean error and correlation, and our system's predictions were consistently within the range of inter-rater variability. This work demonstrates the feasibility of using computer vision and machine learning to produce consistent and clinically useful measures of motor impairment.



### Theory and Tools for the Conversion of Analog to Spiking Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.04052v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1612.04052v1)
- **Published**: 2016-12-13 07:58:34+00:00
- **Updated**: 2016-12-13 07:58:34+00:00
- **Authors**: Bodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, Michael Pfeiffer
- **Comment**: 9 pages, 2 figures, presented at the workshop "Computing with Spikes"
  at NIPS 2016, Barcelona, Spain
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) have shown great potential for numerous real-world machine learning applications, but performing inference in large CNNs in real-time remains a challenge. We have previously demonstrated that traditional CNNs can be converted into deep spiking neural networks (SNNs), which exhibit similar accuracy while reducing both latency and computational load as a consequence of their data-driven, event-based style of computing. Here we provide a novel theory that explains why this conversion is successful, and derive from it several new tools to convert a larger and more powerful class of deep networks into SNNs. We identify the main sources of approximation errors in previous conversion methods, and propose simple mechanisms to fix these issues. Furthermore, we develop spiking implementations of common CNN operations such as max-pooling, softmax, and batch-normalization, which allow almost loss-less conversion of arbitrary CNN architectures into the spiking domain. Empirical evaluation of different network architectures on the MNIST and CIFAR10 benchmarks leads to the best SNN results reported to date.



### Learning to Hash-tag Videos with Tag2Vec
- **Arxiv ID**: http://arxiv.org/abs/1612.04061v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1612.04061v1)
- **Published**: 2016-12-13 08:32:02+00:00
- **Updated**: 2016-12-13 08:32:02+00:00
- **Authors**: Aditya Singh, Saurabh Saini, Rajvi Shah, PJ Narayanan
- **Comment**: None
- **Journal**: None
- **Summary**: User-given tags or labels are valuable resources for semantic understanding of visual media such as images and videos. Recently, a new type of labeling mechanism known as hash-tags have become increasingly popular on social media sites. In this paper, we study the problem of generating relevant and useful hash-tags for short video clips. Traditional data-driven approaches for tag enrichment and recommendation use direct visual similarity for label transfer and propagation. We attempt to learn a direct low-cost mapping from video to hash-tags using a two step training process. We first employ a natural language processing (NLP) technique, skip-gram models with neural network training to learn a low-dimensional vector representation of hash-tags (Tag2Vec) using a corpus of 10 million hash-tags. We then train an embedding function to map video features to the low-dimensional Tag2vec space. We learn this embedding for 29 categories of short video clips with hash-tags. A query video without any tag-information can then be directly mapped to the vector space of tags using the learned embedding and relevant tags can be found by performing a simple nearest-neighbor retrieval in the Tag2Vec space. We validate the relevance of the tags suggested by our system qualitatively and quantitatively with a user study.



### Spatial Pyramid Convolutional Neural Network for Social Event Detection in Static Image
- **Arxiv ID**: http://arxiv.org/abs/1612.04062v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.04062v1)
- **Published**: 2016-12-13 08:32:56+00:00
- **Updated**: 2016-12-13 08:32:56+00:00
- **Authors**: Reza Fuad Rachmadi, Keiichi Uchimura, Gou Koutaki
- **Comment**: in Proceeding of 11th International Student Conference on Advanced
  Science and Technology (ICAST) 2016
- **Journal**: None
- **Summary**: Social event detection in a static image is a very challenging problem and it's very useful for internet of things applications including automatic photo organization, ads recommender system, or image captioning. Several publications show that variety of objects, scene, and people can be very ambiguous for the system to decide the event that occurs in the image. We proposed the spatial pyramid configuration of convolutional neural network (CNN) classifier for social event detection in a static image. By applying the spatial pyramid configuration to the CNN classifier, the detail that occurs in the image can observe more accurately by the classifier. USED dataset provided by Ahmad et al. is used to evaluate our proposed method, which consists of two different image sets, EiMM, and SED dataset. As a result, the average accuracy of our system outperforms the baseline method by 15% and 2% respectively.



### Observation of dynamics inside an unlabeled live cell using bright-field photon microscopy: Evaluation of organelles' trajectories
- **Arxiv ID**: http://arxiv.org/abs/1612.04110v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, q-bio.CB, q-bio.SC
- **Links**: [PDF](http://arxiv.org/pdf/1612.04110v1)
- **Published**: 2016-12-13 11:59:57+00:00
- **Updated**: 2016-12-13 11:59:57+00:00
- **Authors**: Renata Rychtarikova, Dalibor Stys
- **Comment**: 12 pages, 5 figures, supplementary data
- **Journal**: None
- **Summary**: This article presents an algorithm for the evaluation of organelles' movements inside of an unmodified live cell. We used a time-lapse image series obtained using wide-field bright-field photon transmission microscopy as an algorithm input. The benefit of the algorithm is the application of the R\'enyi information entropy, namely a variable called a point information gain, which enables to highlight the borders of the intracellular organelles and to localize the organelles' centers of mass with the precision of one pixel.



### Compressive Image Recovery Using Recurrent Generative Model
- **Arxiv ID**: http://arxiv.org/abs/1612.04229v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.04229v2)
- **Published**: 2016-12-13 15:21:41+00:00
- **Updated**: 2017-05-03 19:48:30+00:00
- **Authors**: Akshat Dave, Anil Kumar Vadathya, Kaushik Mitra
- **Comment**: Submitted to ICIP 2017
- **Journal**: None
- **Summary**: Reconstruction of signals from compressively sensed measurements is an ill-posed problem. In this paper, we leverage the recurrent generative model, RIDE, as an image prior for compressive image reconstruction. Recurrent networks can model long-range dependencies in images and hence are suitable to handle global multiplexing in reconstruction from compressive imaging. We perform MAP inference with RIDE using back-propagation to the inputs and projected gradient method. We propose an entropy thresholding based approach for preserving texture in images well. Our approach shows superior reconstructions compared to recent global reconstruction approaches like D-AMP and TVAL3 on both simulated and real data.



### How do people explore virtual environments?
- **Arxiv ID**: http://arxiv.org/abs/1612.04335v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.04335v2)
- **Published**: 2016-12-13 20:01:18+00:00
- **Updated**: 2017-09-19 19:28:33+00:00
- **Authors**: Vincent Sitzmann, Ana Serrano, Amy Pavel, Maneesh Agrawala, Diego Gutierrez, Belen Masia, Gordon Wetzstein
- **Comment**: First two authors contributed equally
- **Journal**: None
- **Summary**: Understanding how people explore immersive virtual environments is crucial for many applications, such as designing virtual reality (VR) content, developing new compression algorithms, or learning computational models of saliency or visual attention. Whereas a body of recent work has focused on modeling saliency in desktop viewing conditions, VR is very different from these conditions in that viewing behavior is governed by stereoscopic vision and by the complex interaction of head orientation, gaze, and other kinematic constraints. To further our understanding of viewing behavior and saliency in VR, we capture and analyze gaze and head orientation data of 169 users exploring stereoscopic, static omni-directional panoramas, for a total of 1980 head and gaze trajectories for three different viewing conditions. We provide a thorough analysis of our data, which leads to several important insights, such as the existence of a particular fixation bias, which we then use to adapt existing saliency predictors to immersive VR conditions. In addition, we explore other applications of our data and analysis, including automatic alignment of VR video cuts, panorama thumbnails, panorama video synopsis, and saliency-based compression.



### Fast Patch-based Style Transfer of Arbitrary Style
- **Arxiv ID**: http://arxiv.org/abs/1612.04337v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.04337v1)
- **Published**: 2016-12-13 20:05:37+00:00
- **Updated**: 2016-12-13 20:05:37+00:00
- **Authors**: Tian Qi Chen, Mark Schmidt
- **Comment**: None
- **Journal**: None
- **Summary**: Artistic style transfer is an image synthesis problem where the content of an image is reproduced with the style of another. Recent works show that a visually appealing style transfer can be achieved by using the hidden activations of a pretrained convolutional neural network. However, existing methods either apply (i) an optimization procedure that works for any style image but is very expensive, or (ii) an efficient feedforward network that only allows a limited number of trained styles. In this work we propose a simpler optimization objective based on local matching that combines the content structure and style textures in a single layer of the pretrained network. We show that our objective has desirable properties such as a simpler optimization landscape, intuitive parameter tuning, and consistent frame-by-frame performance on video. Furthermore, we use 80,000 natural images and 80,000 paintings to train an inverse network that approximates the result of the optimization. This results in a procedure for artistic style transfer that is efficient but also allows arbitrary content and style images.



### Stacked Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.04357v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1612.04357v4)
- **Published**: 2016-12-13 20:48:58+00:00
- **Updated**: 2017-04-12 15:04:01+00:00
- **Authors**: Xun Huang, Yixuan Li, Omid Poursaeed, John Hopcroft, Serge Belongie
- **Comment**: CVPR 2017, camera-ready version
- **Journal**: None
- **Summary**: In this paper, we propose a novel generative model named Stacked Generative Adversarial Networks (SGAN), which is trained to invert the hierarchical representations of a bottom-up discriminative network. Our model consists of a top-down stack of GANs, each learned to generate lower-level representations conditioned on higher-level representations. A representation discriminator is introduced at each feature hierarchy to encourage the representation manifold of the generator to align with that of the bottom-up discriminative network, leveraging the powerful discriminative representations to guide the generative model. In addition, we introduce a conditional loss that encourages the use of conditional information from the layer above, and a novel entropy loss that maximizes a variational lower bound on the conditional entropy of generator outputs. We first train each stack independently, and then train the whole model end-to-end. Unlike the original GAN that uses a single noise vector to represent all the variations, our SGAN decomposes variations into multiple levels and gradually resolves uncertainties in the top-down generative process. Based on visual inspection, Inception scores and visual Turing test, we demonstrate that SGAN is able to generate images of much higher quality than GANs without stacking.



### Finding Tiny Faces
- **Arxiv ID**: http://arxiv.org/abs/1612.04402v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.04402v2)
- **Published**: 2016-12-13 21:28:02+00:00
- **Updated**: 2017-04-15 06:18:08+00:00
- **Authors**: Peiyun Hu, Deva Ramanan
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: Though tremendous strides have been made in object recognition, one of the remaining open challenges is detecting small objects. We explore three aspects of the problem in the context of finding small faces: the role of scale invariance, image resolution, and contextual reasoning. While most recognition approaches aim to be scale-invariant, the cues for recognizing a 3px tall face are fundamentally different than those for recognizing a 300px tall face. We take a different approach and train separate detectors for different scales. To maintain efficiency, detectors are trained in a multi-task fashion: they make use of features extracted from multiple layers of single (deep) feature hierarchy. While training detectors for large objects is straightforward, the crucial challenge remains training detectors for small objects. We show that context is crucial, and define templates that make use of massively-large receptive fields (where 99% of the template extends beyond the object of interest). Finally, we explore the role of scale in pre-trained deep networks, providing ways to extrapolate networks tuned for limited scales to rather extreme ranges. We demonstrate state-of-the-art results on massively-benchmarked face datasets (FDDB and WIDER FACE). In particular, when compared to prior art on WIDER FACE, our results reduce error by a factor of 2 (our models produce an AP of 82% while prior art ranges from 29-64%).



