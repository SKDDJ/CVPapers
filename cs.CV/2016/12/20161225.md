# Arxiv Papers in cs.CV on 2016-12-25
### A Fixed-Point Model for Pancreas Segmentation in Abdominal CT Scans
- **Arxiv ID**: http://arxiv.org/abs/1612.08230v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.08230v4)
- **Published**: 2016-12-25 02:15:50+00:00
- **Updated**: 2017-06-21 04:00:59+00:00
- **Authors**: Yuyin Zhou, Lingxi Xie, Wei Shen, Yan Wang, Elliot K. Fishman, Alan L. Yuille
- **Comment**: Accepted to MICCAI 2017 (8 pages, 3 figures)
- **Journal**: None
- **Summary**: Deep neural networks have been widely adopted for automatic organ segmentation from abdominal CT scans. However, the segmentation accuracy of some small organs (e.g., the pancreas) is sometimes below satisfaction, arguably because deep networks are easily disrupted by the complex and variable background regions which occupies a large fraction of the input volume. In this paper, we formulate this problem into a fixed-point model which uses a predicted segmentation mask to shrink the input region. This is motivated by the fact that a smaller input region often leads to more accurate segmentation. In the training process, we use the ground-truth annotation to generate accurate input regions and optimize network weights. On the testing stage, we fix the network parameters and update the segmentation results in an iterative manner. We evaluate our approach on the NIH pancreas segmentation dataset, and outperform the state-of-the-art by more than 4%, measured by the average Dice-S{\o}rensen Coefficient (DSC). In addition, we report 62.43% DSC in the worst case, which guarantees the reliability of our approach in clinical applications.



### YOLO9000: Better, Faster, Stronger
- **Arxiv ID**: http://arxiv.org/abs/1612.08242v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.08242v1)
- **Published**: 2016-12-25 07:21:38+00:00
- **Updated**: 2016-12-25 07:21:38+00:00
- **Authors**: Joseph Redmon, Ali Farhadi
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.



### Globally Optimal Object Tracking with Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.08274v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.08274v1)
- **Published**: 2016-12-25 16:00:40+00:00
- **Updated**: 2016-12-25 16:00:40+00:00
- **Authors**: Jinho Lee, Brian Kenji Iwana, Shouta Ide, Seiichi Uchida
- **Comment**: 6pages, 8figures
- **Journal**: None
- **Summary**: Tracking is one of the most important but still difficult tasks in computer vision and pattern recognition. The main difficulties in the tracking field are appearance variation and occlusion. Most traditional tracking methods set the parameters or templates to track target objects in advance and should be modified accordingly. Thus, we propose a new and robust tracking method using a Fully Convolutional Network (FCN) to obtain an object probability map and Dynamic Programming (DP) to seek the globally optimal path through all frames of video. Our proposed method solves the object appearance variation problem with the use of a FCN and deals with occlusion by DP. We show that our method is effective in tracking various single objects through video frames.



