# Arxiv Papers in cs.CV on 2016-12-29
### Meta-Unsupervised-Learning: A supervised approach to unsupervised learning
- **Arxiv ID**: http://arxiv.org/abs/1612.09030v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1612.09030v2)
- **Published**: 2016-12-29 03:20:33+00:00
- **Updated**: 2017-01-03 17:34:39+00:00
- **Authors**: Vikas K. Garg, Adam Tauman Kalai
- **Comment**: 22 pages
- **Journal**: None
- **Summary**: We introduce a new paradigm to investigate unsupervised learning, reducing unsupervised learning to supervised learning. Specifically, we mitigate the subjectivity in unsupervised decision-making by leveraging knowledge acquired from prior, possibly heterogeneous, supervised learning tasks. We demonstrate the versatility of our framework via comprehensive expositions and detailed experiments on several unsupervised problems such as (a) clustering, (b) outlier detection, and (c) similarity prediction under a common umbrella of meta-unsupervised-learning. We also provide rigorous PAC-agnostic bounds to establish the theoretical foundations of our framework, and show that our framing of meta-clustering circumvents Kleinberg's impossibility theorem for clustering.



### From Virtual to Real World Visual Perception using Domain Adaptation -- The DPM as Example
- **Arxiv ID**: http://arxiv.org/abs/1612.09134v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1612.09134v1)
- **Published**: 2016-12-29 13:16:22+00:00
- **Updated**: 2016-12-29 13:16:22+00:00
- **Authors**: Antonio M. Lopez, Jiaolong Xu, Jose L. Gomez, David Vazquez, German Ros
- **Comment**: Invited book chapter to appear in "Domain Adaptation in Computer
  Vision Applications", Springer Series: Advances in Computer Vision and
  Pattern Recognition, Edited by Gabriela Csurka
- **Journal**: None
- **Summary**: Supervised learning tends to produce more accurate classifiers than unsupervised learning in general. This implies that training data is preferred with annotations. When addressing visual perception challenges, such as localizing certain object classes within an image, the learning of the involved classifiers turns out to be a practical bottleneck. The reason is that, at least, we have to frame object examples with bounding boxes in thousands of images. A priori, the more complex the model is regarding its number of parameters, the more annotated examples are required. This annotation task is performed by human oracles, which ends up in inaccuracies and errors in the annotations (aka ground truth) since the task is inherently very cumbersome and sometimes ambiguous. As an alternative we have pioneered the use of virtual worlds for collecting such annotations automatically and with high precision. However, since the models learned with virtual data must operate in the real world, we still need to perform domain adaptation (DA). In this chapter we revisit the DA of a deformable part-based model (DPM) as an exemplifying case of virtual- to-real-world DA. As a use case, we address the challenge of vehicle detection for driver assistance, using different publicly available virtual-world data. While doing so, we investigate questions such as: how does the domain gap behave due to virtual-vs-real data with respect to dominant object appearance per domain, as well as the role of photo-realism in the virtual world.



### Learning Visual N-Grams from Web Data
- **Arxiv ID**: http://arxiv.org/abs/1612.09161v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.09161v2)
- **Published**: 2016-12-29 14:50:53+00:00
- **Updated**: 2017-08-06 01:59:22+00:00
- **Authors**: Ang Li, Allan Jabri, Armand Joulin, Laurens van der Maaten
- **Comment**: None
- **Journal**: None
- **Summary**: Real-world image recognition systems need to recognize tens of thousands of classes that constitute a plethora of visual concepts. The traditional approach of annotating thousands of images per class for training is infeasible in such a scenario, prompting the use of webly supervised data. This paper explores the training of image-recognition systems on large numbers of images and associated user comments. In particular, we develop visual n-gram models that can predict arbitrary phrases that are relevant to the content of an image. Our visual n-gram models are feed-forward convolutional networks trained using new loss functions that are inspired by n-gram models commonly used in language modeling. We demonstrate the merits of our models in phrase prediction, phrase-based image retrieval, relating images and captions, and zero-shot transfer.



### Quantum Clustering and Gaussian Mixtures
- **Arxiv ID**: http://arxiv.org/abs/1612.09199v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1612.09199v1)
- **Published**: 2016-12-29 16:55:49+00:00
- **Updated**: 2016-12-29 16:55:49+00:00
- **Authors**: Mahajabin Rahman, Davi Geiger
- **Comment**: 16 pages, 13 figures
- **Journal**: None
- **Summary**: The mixture of Gaussian distributions, a soft version of k-means , is considered a state-of-the-art clustering algorithm. It is widely used in computer vision for selecting classes, e.g., color, texture, and shapes. In this algorithm, each class is described by a Gaussian distribution, defined by its mean and covariance. The data is described by a weighted sum of these Gaussian distributions. We propose a new method, inspired by quantum interference in physics. Instead of modeling each class distribution directly, we model a class wave function such that its magnitude square is the class Gaussian distribution. We then mix the class wave functions to create the mixture wave function. The final mixture distribution is then the magnitude square of the mixture wave function. As a result, we observe the quantum class interference phenomena, not present in the Gaussian mixture model. We show that the quantum method outperforms the Gaussian mixture method in every aspect of the estimations. It provides more accurate estimations of all distribution parameters, with much less fluctuations, and it is also more robust to data deformations from the Gaussian assumptions. We illustrate our method for color segmentation as an example application.



### Deep Learning Logo Detection with Data Expansion by Synthesising Context
- **Arxiv ID**: http://arxiv.org/abs/1612.09322v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.09322v3)
- **Published**: 2016-12-29 21:48:22+00:00
- **Updated**: 2018-03-16 10:04:15+00:00
- **Authors**: Hang Su, Xiatian Zhu, Shaogang Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Logo detection in unconstrained images is challenging, particularly when only very sparse labelled training images are accessible due to high labelling costs. In this work, we describe a model training image synthesising method capable of improving significantly logo detection performance when only a handful of (e.g., 10) labelled training images captured in realistic context are available, avoiding extensive manual labelling costs. Specifically, we design a novel algorithm for generating Synthetic Context Logo (SCL) training images to increase model robustness against unknown background clutters, resulting in superior logo detection performance. For benchmarking model performance, we introduce a new logo detection dataset TopLogo-10 collected from top 10 most popular clothing/wearable brandname logos captured in rich visual context. Extensive comparisons show the advantages of our proposed SCL model over the state-of-the-art alternatives for logo detection using two real-world logo benchmark datasets: FlickrLogo-32 and our new TopLogo-10.



### Rotation equivariant vector field networks
- **Arxiv ID**: http://arxiv.org/abs/1612.09346v3
- **DOI**: 10.1109/ICCV.2017.540
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.09346v3)
- **Published**: 2016-12-29 23:42:49+00:00
- **Updated**: 2017-08-25 12:26:43+00:00
- **Authors**: Diego Marcos, Michele Volpi, Nikos Komodakis, Devis Tuia
- **Comment**: 10 pages, accepted at ICCV 2017
- **Journal**: None
- **Summary**: In many computer vision tasks, we expect a particular behavior of the output with respect to rotations of the input image. If this relationship is explicitly encoded, instead of treated as any other variation, the complexity of the problem is decreased, leading to a reduction in the size of the required model. In this paper, we propose the Rotation Equivariant Vector Field Networks (RotEqNet), a Convolutional Neural Network (CNN) architecture encoding rotation equivariance, invariance and covariance. Each convolutional filter is applied at multiple orientations and returns a vector field representing magnitude and angle of the highest scoring orientation at every spatial location. We develop a modified convolution operator relying on this representation to obtain deep architectures. We test RotEqNet on several problems requiring different responses with respect to the inputs' rotation: image classification, biomedical image segmentation, orientation estimation and patch matching. In all cases, we show that RotEqNet offers extremely compact models in terms of number of parameters and provides results in line to those of networks orders of magnitude larger.



