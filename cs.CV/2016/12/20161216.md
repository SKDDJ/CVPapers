# Arxiv Papers in cs.CV on 2016-12-16
### Towards a Deep Learning Framework for Unconstrained Face Detection
- **Arxiv ID**: http://arxiv.org/abs/1612.05322v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.05322v2)
- **Published**: 2016-12-16 00:34:06+00:00
- **Updated**: 2017-01-02 18:06:49+00:00
- **Authors**: Yutong Zheng, Chenchen Zhu, Khoa Luu, Chandrasekhar Bhagavatula, T. Hoang Ngan Le, Marios Savvides
- **Comment**: Accepted by BTAS 2016. arXiv admin note: substantial text overlap
  with arXiv:1606.05413
- **Journal**: None
- **Summary**: Robust face detection is one of the most important pre-processing steps to support facial expression analysis, facial landmarking, face recognition, pose estimation, building of 3D facial models, etc. Although this topic has been intensely studied for decades, it is still challenging due to numerous variants of face images in real-world scenarios. In this paper, we present a novel approach named Multiple Scale Faster Region-based Convolutional Neural Network (MS-FRCNN) to robustly detect human facial regions from images collected under various challenging conditions, e.g. large occlusions, extremely low resolutions, facial expressions, strong illumination variations, etc. The proposed approach is benchmarked on two challenging face detection databases, i.e. the Wider Face database and the Face Detection Dataset and Benchmark (FDDB), and compared against recent other face detection methods, e.g. Two-stage CNN, Multi-scale Cascade CNN, Faceness, Aggregate Chanel Features, HeadHunter, Multi-view Face Detection, Cascade CNN, etc. The experimental results show that our proposed approach consistently achieves highly competitive results with the state-of-the-art performance against other recent face detection methods.



### A Stochastic Large Deformation Model for Computational Anatomy
- **Arxiv ID**: http://arxiv.org/abs/1612.05323v1
- **DOI**: None
- **Categories**: **cs.CV**, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1612.05323v1)
- **Published**: 2016-12-16 00:45:46+00:00
- **Updated**: 2016-12-16 00:45:46+00:00
- **Authors**: Alexis Arnaudon, Darryl D. Holm, Akshay Pai, Stefan Sommer
- **Comment**: None
- **Journal**: None
- **Summary**: In the study of shapes of human organs using computational anatomy, variations are found to arise from inter-subject anatomical differences, disease-specific effects, and measurement noise. This paper introduces a stochastic model for incorporating random variations into the Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework. By accounting for randomness in a particular setup which is crafted to fit the geometrical properties of LDDMM, we formulate the template estimation problem for landmarks with noise and give two methods for efficiently estimating the parameters of the noise fields from a prescribed data set. One method directly approximates the time evolution of the variance of each landmark by a finite set of differential equations, and the other is based on an Expectation-Maximisation algorithm. In the second method, the evaluation of the data likelihood is achieved without registering the landmarks, by applying bridge sampling using a stochastically perturbed version of the large deformation gradient flow algorithm. The method and the estimation algorithms are experimentally validated on synthetic examples and shape data of human corpora callosa.



### Fast, Dense Feature SDM on an iPhone
- **Arxiv ID**: http://arxiv.org/abs/1612.05332v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.05332v1)
- **Published**: 2016-12-16 01:47:31+00:00
- **Updated**: 2016-12-16 01:47:31+00:00
- **Authors**: Ashton Fagg, Simon Lucey, Sridha Sridharan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present our method for enabling dense SDM to run at over 90 FPS on a mobile device. Our contributions are two-fold. Drawing inspiration from the FFT, we propose a Sparse Compositional Regression (SCR) framework, which enables a significant speed up over classical dense regressors. Second, we propose a binary approximation to SIFT features. Binary Approximated SIFT (BASIFT) features, which are a computationally efficient approximation to SIFT, a commonly used feature with SDM. We demonstrate the performance of our algorithm on an iPhone 7, and show that we achieve similar accuracy to SDM.



### Mirrored Light Field Video Camera Adapter
- **Arxiv ID**: http://arxiv.org/abs/1612.05335v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1612.05335v1)
- **Published**: 2016-12-16 02:05:59+00:00
- **Updated**: 2016-12-16 02:05:59+00:00
- **Authors**: Dorian Tsai, Donald G. Dansereau, Steve Martin, Peter Corke
- **Comment**: tech report, v0.5, 15 pages, 6 figures
- **Journal**: None
- **Summary**: This paper proposes the design of a custom mirror-based light field camera adapter that is cheap, simple in construction, and accessible. Mirrors of different shape and orientation reflect the scene into an upwards-facing camera to create an array of virtual cameras with overlapping field of view at specified depths, and deliver video frame rate light fields. We describe the design, construction, decoding and calibration processes of our mirror-based light field camera adapter in preparation for an open-source release to benefit the robotic vision community.



### FusionNet: A deep fully residual convolutional neural network for image segmentation in connectomics
- **Arxiv ID**: http://arxiv.org/abs/1612.05360v2
- **DOI**: 10.3389/fcomp.2021.613981
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.05360v2)
- **Published**: 2016-12-16 04:08:43+00:00
- **Updated**: 2016-12-26 11:42:56+00:00
- **Authors**: Tran Minh Quan, David G. C. Hildebrand, Won-Ki Jeong
- **Comment**: None
- **Journal**: None
- **Summary**: Electron microscopic connectomics is an ambitious research direction with the goal of studying comprehensive brain connectivity maps by using high-throughput, nano-scale microscopy. One of the main challenges in connectomics research is developing scalable image analysis algorithms that require minimal user intervention. Recently, deep learning has drawn much attention in computer vision because of its exceptional performance in image classification tasks. For this reason, its application to connectomic analyses holds great promise, as well. In this paper, we introduce a novel deep neural network architecture, FusionNet, for the automatic segmentation of neuronal structures in connectomics data. FusionNet leverages the latest advances in machine learning, such as semantic segmentation and residual neural networks, with the novel introduction of summation-based skip connections to allow a much deeper network architecture for a more accurate segmentation. We demonstrate the performance of the proposed method by comparing it with state-of-the-art electron microscopy (EM) segmentation methods from the ISBI EM segmentation challenge. We also show the segmentation results on two different tasks including cell membrane and cell body segmentation and a statistical analysis of cell morphology.



### Medical Image Synthesis with Context-Aware Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.05362v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.05362v1)
- **Published**: 2016-12-16 04:26:06+00:00
- **Updated**: 2016-12-16 04:26:06+00:00
- **Authors**: Dong Nie, Roger Trullo, Caroline Petitjean, Su Ruan, Dinggang Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Computed tomography (CT) is critical for various clinical applications, e.g., radiotherapy treatment planning and also PET attenuation correction. However, CT exposes radiation during acquisition, which may cause side effects to patients. Compared to CT, magnetic resonance imaging (MRI) is much safer and does not involve any radiations. Therefore, recently, researchers are greatly motivated to estimate CT image from its corresponding MR image of the same subject for the case of radiotherapy planning. In this paper, we propose a data-driven approach to address this challenging problem. Specifically, we train a fully convolutional network to generate CT given an MR image. To better model the nonlinear relationship from MRI to CT and to produce more realistic images, we propose to use the adversarial training strategy and an image gradient difference loss function. We further apply AutoContext Model to implement a context-aware generative adversarial network. Experimental results show that our method is accurate and robust for predicting CT images from MRI images, and also outperforms three state-of-the-art methods under comparison.



### Learning Residual Images for Face Attribute Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1612.05363v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.05363v2)
- **Published**: 2016-12-16 04:28:26+00:00
- **Updated**: 2017-04-12 06:00:19+00:00
- **Authors**: Wei Shen, Rujie Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Face attributes are interesting due to their detailed description of human faces. Unlike prior researches working on attribute prediction, we address an inverse and more challenging problem called face attribute manipulation which aims at modifying a face image according to a given attribute value. Instead of manipulating the whole image, we propose to learn the corresponding residual image defined as the difference between images before and after the manipulation. In this way, the manipulation can be operated efficiently with modest pixel modification. The framework of our approach is based on the Generative Adversarial Network. It consists of two image transformation networks and a discriminative network. The transformation networks are responsible for the attribute manipulation and its dual operation and the discriminative network is used to distinguish the generated images from real images. We also apply dual learning to allow transformation networks to learn from each other. Experiments show that residual images can be effectively learned and used for attribute manipulations. The generated images remain most of the details in attribute-irrelevant areas.



### Output Constraint Transfer for Kernelized Correlation Filter in Tracking
- **Arxiv ID**: http://arxiv.org/abs/1612.05365v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.05365v1)
- **Published**: 2016-12-16 04:42:30+00:00
- **Updated**: 2016-12-16 04:42:30+00:00
- **Authors**: Baochang Zhang, Zhigang Li, Xianbin Cao, Qixiang Ye, Chen Chen, Linlin Shen, Alessandro Perina, Rongrong Ji
- **Comment**: arXiv admin note: text overlap with arXiv:1404.7584 by other authors
- **Journal**: None
- **Summary**: Kernelized Correlation Filter (KCF) is one of the state-of-the-art object trackers. However, it does not reasonably model the distribution of correlation response during tracking process, which might cause the drifting problem, especially when targets undergo significant appearance changes due to occlusion, camera shaking, and/or deformation. In this paper, we propose an Output Constraint Transfer (OCT) method that by modeling the distribution of correlation response in a Bayesian optimization framework is able to mitigate the drifting problem. OCT builds upon the reasonable assumption that the correlation response to the target image follows a Gaussian distribution, which we exploit to select training samples and reduce model uncertainty. OCT is rooted in a new theory which transfers data distribution to a constraint of the optimized variable, leading to an efficient framework to calculate correlation filters. Extensive experiments on a commonly used tracking benchmark show that the proposed method significantly improves KCF, and achieves better performance than other state-of-the-art trackers. To encourage further developments, the source code is made available https://github.com/bczhangbczhang/OCT-KCF.



### The VQA-Machine: Learning How to Use Existing Vision Algorithms to Answer New Questions
- **Arxiv ID**: http://arxiv.org/abs/1612.05386v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.05386v1)
- **Published**: 2016-12-16 07:07:25+00:00
- **Updated**: 2016-12-16 07:07:25+00:00
- **Authors**: Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel
- **Comment**: None
- **Journal**: None
- **Summary**: One of the most intriguing features of the Visual Question Answering (VQA) challenge is the unpredictability of the questions. Extracting the information required to answer them demands a variety of image operations from detection and counting, to segmentation and reconstruction. To train a method to perform even one of these operations accurately from {image,question,answer} tuples would be challenging, but to aim to achieve them all with a limited set of such training data seems ambitious at best. We propose here instead a more general and scalable approach which exploits the fact that very good methods to achieve these operations already exist, and thus do not need to be trained. Our method thus learns how to exploit a set of external off-the-shelf algorithms to achieve its goal, an approach that has something in common with the Neural Turing Machine. The core of our proposed method is a new co-attention model. In addition, the proposed approach generates human-readable reasons for its decision, and can still be trained end-to-end without ground truth reasons being given. We demonstrate the effectiveness on two publicly available datasets, Visual Genome and VQA, and show that it produces the state-of-the-art results in both cases.



### Deep Residual Hashing
- **Arxiv ID**: http://arxiv.org/abs/1612.05400v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.05400v1)
- **Published**: 2016-12-16 09:31:17+00:00
- **Updated**: 2016-12-16 09:31:17+00:00
- **Authors**: Sailesh Conjeti, Abhijit Guha Roy, Amin Katouzian, Nassir Navab
- **Comment**: Submitted to Information Processing in Medical Imaging, 2017 (Under
  review)
- **Journal**: None
- **Summary**: Hashing aims at generating highly compact similarity preserving code words which are well suited for large-scale image retrieval tasks.   Most existing hashing methods first encode the images as a vector of hand-crafted features followed by a separate binarization step to generate hash codes. This two-stage process may produce sub-optimal encoding. In this paper, for the first time, we propose a deep architecture for supervised hashing through residual learning, termed Deep Residual Hashing (DRH), for an end-to-end simultaneous representation learning and hash coding. The DRH model constitutes four key elements: (1) a sub-network with multiple stacked residual blocks; (2) hashing layer for binarization; (3) supervised retrieval loss function based on neighbourhood component analysis for similarity preserving embedding; and (4) hashing related losses and regularisation to control the quantization error and improve the quality of hash coding. We present results of extensive experiments on a large public chest x-ray image database with co-morbidities and discuss the outcome showing substantial improvements over the latest state-of-the art methods.



### Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.05424v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.05424v2)
- **Published**: 2016-12-16 10:50:36+00:00
- **Updated**: 2017-08-23 12:35:56+00:00
- **Authors**: Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, Dilip Krishnan
- **Comment**: Final CVPR 2017 paper and supplementary material
- **Journal**: None
- **Summary**: Collecting well-annotated image datasets to train modern machine learning algorithms is prohibitively expensive for many tasks. One appealing alternative is rendering synthetic data where ground-truth annotations are generated automatically. Unfortunately, models trained purely on rendered images often fail to generalize to real images. To address this shortcoming, prior work introduced unsupervised domain adaptation algorithms that attempt to map representations between the two domains or learn to extract features that are domain-invariant. In this work, we present a new approach that learns, in an unsupervised manner, a transformation in the pixel space from one domain to the other. Our generative adversarial network (GAN)-based method adapts source-domain images to appear as if drawn from the target domain. Our approach not only produces plausible samples, but also outperforms the state-of-the-art on a number of unsupervised domain adaptation scenarios by large margins. Finally, we demonstrate that the adaptation process generalizes to object classes unseen during training.



### A Message Passing Algorithm for the Minimum Cost Multicut Problem
- **Arxiv ID**: http://arxiv.org/abs/1612.05441v2
- **DOI**: None
- **Categories**: **cs.DS**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1612.05441v2)
- **Published**: 2016-12-16 12:10:34+00:00
- **Updated**: 2017-01-12 11:55:04+00:00
- **Authors**: Paul Swoboda, Bjoern Andres
- **Comment**: Added acknowledgments
- **Journal**: None
- **Summary**: We propose a dual decomposition and linear program relaxation of the NP -hard minimum cost multicut problem. Unlike other polyhedral relaxations of the multicut polytope, it is amenable to efficient optimization by message passing. Like other polyhedral elaxations, it can be tightened efficiently by cutting planes. We define an algorithm that alternates between message passing and efficient separation of cycle- and odd-wheel inequalities. This algorithm is more efficient than state-of-the-art algorithms based on linear programming, including algorithms written in the framework of leading commercial software, as we show in experiments with large instances of the problem from applications in computer vision, biomedical image analysis and data mining.



### A Dual Ascent Framework for Lagrangean Decomposition of Combinatorial Problems
- **Arxiv ID**: http://arxiv.org/abs/1612.05460v2
- **DOI**: None
- **Categories**: **cs.DS**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1612.05460v2)
- **Published**: 2016-12-16 13:32:18+00:00
- **Updated**: 2017-01-12 11:53:40+00:00
- **Authors**: Paul Swoboda, Jan Kuske, Bogdan Savchynskyy
- **Comment**: Added acknowledgments
- **Journal**: None
- **Summary**: We propose a general dual ascent framework for Lagrangean decomposition of combinatorial problems. Although methods of this type have shown their efficiency for a number of problems, so far there was no general algorithm applicable to multiple problem types. In his work, we propose such a general algorithm. It depends on several parameters, which can be used to optimize its performance in each particular setting. We demonstrate efficacy of our method on graph matching and multicut problems, where it outperforms state-of-the-art solvers including those based on subgradient optimization and off-the-shelf linear programming solvers.



### A Study of Lagrangean Decompositions and Dual Ascent Solvers for Graph Matching
- **Arxiv ID**: http://arxiv.org/abs/1612.05476v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.05476v2)
- **Published**: 2016-12-16 14:14:42+00:00
- **Updated**: 2017-01-12 11:51:00+00:00
- **Authors**: Paul Swoboda, Carsten Rother, Hassan Abu Alhaija, Dagmar Kainmueller, Bogdan Savchynskyy
- **Comment**: Added acknowledgments
- **Journal**: None
- **Summary**: We study the quadratic assignment problem, in computer vision also known as graph matching. Two leading solvers for this problem optimize the Lagrange decomposition duals with sub-gradient and dual ascent (also known as message passing) updates. We explore s direction further and propose several additional Lagrangean relaxations of the graph matching problem along with corresponding algorithms, which are all based on a common dual ascent framework. Our extensive empirical evaluation gives several theoretical insights and suggests a new state-of-the-art any-time solver for the considered problem. Our improvement over state-of-the-art is particularly visible on a new dataset with large-scale sparse problem instances containing more than 500 graph nodes each.



### Video Propagation Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.05478v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.05478v3)
- **Published**: 2016-12-16 14:23:17+00:00
- **Updated**: 2017-04-11 09:58:35+00:00
- **Authors**: Varun Jampani, Raghudeep Gadde, Peter V. Gehler
- **Comment**: Appearing in Computer Vision and Pattern Recognition, 2017 (CVPR'17)
- **Journal**: None
- **Summary**: We propose a technique that propagates information forward through video data. The method is conceptually simple and can be applied to tasks that require the propagation of structured information, such as semantic labels, based on video content. We propose a 'Video Propagation Network' that processes video frames in an adaptive manner. The model is applied online: it propagates information forward without the need to access future frames. In particular we combine two components, a temporal bilateral network for dense and video adaptive filtering, followed by a spatial network to refine features and increased flexibility. We present experiments on video object segmentation and semantic video segmentation and show increased performance comparing to the best previous task-specific methods, while having favorable runtime. Additionally we demonstrate our approach on an example regression task of color propagation in a grayscale video.



### On the crucial impact of the coupling projector-backprojector in iterative tomographic reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1612.05515v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.05515v1)
- **Published**: 2016-12-16 15:48:11+00:00
- **Updated**: 2016-12-16 15:48:11+00:00
- **Authors**: Filippo Arcadu, Marco Stampanoni, Federica Marone
- **Comment**: None
- **Journal**: None
- **Summary**: The performance of an iterative reconstruction algorithm for X-ray tomography is strongly determined by the features of the used forward and backprojector. For this reason, a large number of studies has focused on the to design of projectors with increasingly higher accuracy and speed. To what extent the accuracy of an iterative algorithm is affected by the mathematical affinity and the similarity between the actual implementation of the forward and backprojection, referred here as "coupling projector-backprojector", has been an overlooked aspect so far. The experimental study presented here shows that the reconstruction quality and the convergence of an iterative algorithm greatly rely on a good matching between the implementation of the tomographic operators. In comparison, other aspects like the accuracy of the standalone operators, the usage of physical constraints or the choice of stopping criteria may even play a less relevant role.



### SonoNet: Real-Time Detection and Localisation of Fetal Standard Scan Planes in Freehand Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/1612.05601v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.05601v2)
- **Published**: 2016-12-16 19:20:20+00:00
- **Updated**: 2017-07-25 16:12:50+00:00
- **Authors**: Christian F. Baumgartner, Konstantinos Kamnitsas, Jacqueline Matthew, Tara P. Fletcher, Sandra Smith, Lisa M. Koch, Bernhard Kainz, Daniel Rueckert
- **Comment**: 12 pages, 8 figures, published in IEEE Transactions in Medical
  Imaging
- **Journal**: None
- **Summary**: Identifying and interpreting fetal standard scan planes during 2D ultrasound mid-pregnancy examinations are highly complex tasks which require years of training. Apart from guiding the probe to the correct location, it can be equally difficult for a non-expert to identify relevant structures within the image. Automatic image processing can provide tools to help experienced as well as inexperienced operators with these tasks. In this paper, we propose a novel method based on convolutional neural networks which can automatically detect 13 fetal standard views in freehand 2D ultrasound data as well as provide a localisation of the fetal structures via a bounding box. An important contribution is that the network learns to localise the target anatomy using weak supervision based on image-level labels only. The network architecture is designed to operate in real-time while providing optimal output for the localisation task. We present results for real-time annotation, retrospective frame retrieval from saved videos, and localisation on a very large and challenging dataset consisting of images and video recordings of full clinical anomaly screenings. We found that the proposed method achieved an average F1-score of 0.798 in a realistic classification experiment modelling real-time detection, and obtained a 90.09% accuracy for retrospective frame retrieval. Moreover, an accuracy of 77.8% was achieved on the localisation task.



