# Arxiv Papers in cs.CV on 2016-12-27
### An Automated CNN Recommendation System for Image Classification Tasks
- **Arxiv ID**: http://arxiv.org/abs/1612.08484v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.08484v1)
- **Published**: 2016-12-27 03:18:28+00:00
- **Updated**: 2016-12-27 03:18:28+00:00
- **Authors**: Song Wang, Li Sun, Wei Fan, Jun Sun, Satoshi Naoi, Koichi Shirahata, Takuya Fukagai, Yasumoto Tomita, Atsushi Ike
- **Comment**: Submitted to ICME 2017 and all the methods in this paper are patented
- **Journal**: None
- **Summary**: Nowadays the CNN is widely used in practical applications for image classification task. However the design of the CNN model is very professional work and which is very difficult for ordinary users. Besides, even for experts of CNN, to select an optimal model for specific task may still need a lot of time (to train many different models). In order to solve this problem, we proposed an automated CNN recommendation system for image classification task. Our system is able to evaluate the complexity of the classification task and the classification ability of the CNN model precisely. By using the evaluation results, the system can recommend the optimal CNN model and which can match the task perfectly. The recommendation process of the system is very fast since we don't need any model training. The experiment results proved that the evaluation methods are very accurate and reliable.



### End-to-End Data Visualization by Metric Learning and Coordinate Transformation
- **Arxiv ID**: http://arxiv.org/abs/1612.08499v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.08499v1)
- **Published**: 2016-12-27 05:03:09+00:00
- **Updated**: 2016-12-27 05:03:09+00:00
- **Authors**: Lilei Zheng, Ying Zhang, Stefan Duffner, Khalid Idrissi, Christophe Garcia, Atilla Baskurt
- **Comment**: 17 pages, 9 figures
- **Journal**: None
- **Summary**: This paper presents a deep nonlinear metric learning framework for data visualization on an image dataset. We propose the Triangular Similarity and prove its equivalence to the Cosine Similarity in measuring a data pair. Based on this novel similarity, a geometrically motivated loss function - the triangular loss - is then developed for optimizing a metric learning system comprising two identical CNNs. It is shown that this deep nonlinear system can be efficiently trained by a hybrid algorithm based on the conventional backpropagation algorithm. More interestingly, benefiting from classical manifold learning theories, the proposed system offers two different views to visualize the outputs, the second of which provides better classification results than the state-of-the-art methods in the visualizable spaces.



### Learning Non-Lambertian Object Intrinsics across ShapeNet Categories
- **Arxiv ID**: http://arxiv.org/abs/1612.08510v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.08510v1)
- **Published**: 2016-12-27 06:38:43+00:00
- **Updated**: 2016-12-27 06:38:43+00:00
- **Authors**: Jian Shi, Yue Dong, Hao Su, Stella X. Yu
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the non-Lambertian object intrinsic problem of recovering diffuse albedo, shading, and specular highlights from a single image of an object.   We build a large-scale object intrinsics database based on existing 3D models in the ShapeNet database. Rendered with realistic environment maps, millions of synthetic images of objects and their corresponding albedo, shading, and specular ground-truth images are used to train an encoder-decoder CNN. Once trained, the network can decompose an image into the product of albedo and shading components, along with an additive specular component.   Our CNN delivers accurate and sharp results in this classical inverse problem of computer vision, sharp details attributed to skip layer connections at corresponding resolutions from the encoder to the decoder. Benchmarked on our ShapeNet and MIT intrinsics datasets, our model consistently outperforms the state-of-the-art by a large margin.   We train and test our CNN on different object categories. Perhaps surprising especially from the CNN classification perspective, our intrinsics CNN generalizes very well across categories. Our analysis shows that feature learning at the encoder stage is more crucial for developing a universal representation across categories.   We apply our synthetic data trained model to images and videos downloaded from the internet, and observe robust and realistic intrinsics results. Quality non-Lambertian intrinsics could open up many interesting applications such as image-based albedo and specular editing.



### Robust LSTM-Autoencoders for Face De-Occlusion in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1612.08534v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.08534v1)
- **Published**: 2016-12-27 08:36:48+00:00
- **Updated**: 2016-12-27 08:36:48+00:00
- **Authors**: Fang Zhao, Jiashi Feng, Jian Zhao, Wenhan Yang, Shuicheng Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Face recognition techniques have been developed significantly in recent years. However, recognizing faces with partial occlusion is still challenging for existing face recognizers which is heavily desired in real-world applications concerning surveillance and security. Although much research effort has been devoted to developing face de-occlusion methods, most of them can only work well under constrained conditions, such as all the faces are from a pre-defined closed set. In this paper, we propose a robust LSTM-Autoencoders (RLA) model to effectively restore partially occluded faces even in the wild. The RLA model consists of two LSTM components, which aims at occlusion-robust face encoding and recurrent occlusion removal respectively. The first one, named multi-scale spatial LSTM encoder, reads facial patches of various scales sequentially to output a latent representation, and occlusion-robustness is achieved owing to the fact that the influence of occlusion is only upon some of the patches. Receiving the representation learned by the encoder, the LSTM decoder with a dual channel architecture reconstructs the overall face and detects occlusion simultaneously, and by feat of LSTM, the decoder breaks down the task of face de-occlusion into restoring the occluded part step by step. Moreover, to minimize identify information loss and guarantee face recognition accuracy over recovered faces, we introduce an identity-preserving adversarial training scheme to further improve RLA. Extensive experiments on both synthetic and real datasets of faces with occlusion clearly demonstrate the effectiveness of our proposed RLA in removing different types of facial occlusion at various locations. The proposed method also provides significantly larger performance gain than other de-occlusion methods in promoting recognition performance over partially-occluded faces.



### Bayesian Nonparametric Models for Synchronous Brain-Computer Interfaces
- **Arxiv ID**: http://arxiv.org/abs/1612.08642v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1612.08642v1)
- **Published**: 2016-12-27 14:17:20+00:00
- **Updated**: 2016-12-27 14:17:20+00:00
- **Authors**: Jaime Fernando Delgado Saa, Mujdat Cetin
- **Comment**: None
- **Journal**: None
- **Summary**: A brain-computer interface (BCI) is a system that aims for establishing a non-muscular communication path for subjects who had suffer from a neurodegenerative disease. Many BCI systems make use of the phenomena of event-related synchronization and de-synchronization of brain waves as a main feature for classification of different cognitive tasks. However, the temporal dynamics of the electroencephalographic (EEG) signals contain additional information that can be incorporated into the inference engine in order to improve the performance of the BCIs. This information about the dynamics of the signals have been exploited previously in BCIs by means of generative and discriminative methods. In particular, hidden Markov models (HMMs) have been used in previous works. These methods have the disadvantage that the model parameters such as the number of hidden states and the number of Gaussian mixtures need to be fix "a priori". In this work, we propose a Bayesian nonparametric model for brain signal classification that does not require "a priori" selection of the number of hidden states and the number of Gaussian mixtures of a HMM. The results show that the proposed model outperform other methods based on HMM as well as the winner algorithm of the BCI competition IV.



### Semantic Perceptual Image Compression using Deep Convolution Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.08712v2
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1612.08712v2)
- **Published**: 2016-12-27 19:21:18+00:00
- **Updated**: 2017-03-29 16:29:54+00:00
- **Authors**: Aaditya Prakash, Nick Moran, Solomon Garber, Antonella DiLillo, James Storer
- **Comment**: Accepted to Data Compression Conference, 11 pages, 5 figures
- **Journal**: None
- **Summary**: It has long been considered a significant problem to improve the visual quality of lossy image and video compression. Recent advances in computing power together with the availability of large training data sets has increased interest in the application of deep learning cnns to address image recognition and image processing tasks. Here, we present a powerful cnn tailored to the specific task of semantic image understanding to achieve higher visual quality in lossy compression. A modest increase in complexity is incorporated to the encoder which allows a standard, off-the-shelf jpeg decoder to be used. While jpeg encoding may be optimized for generic images, the process is ultimately unaware of the specific content of the image to be compressed. Our technique makes jpeg content-aware by designing and training a model to identify multiple semantic regions in a given image. Unlike object detection techniques, our model does not require labeling of object positions and is able to identify objects in a single pass. We present a new cnn architecture directed specifically to image compression, which generates a map that highlights semantically-salient regions so that they can be encoded at higher quality as compared to background regions. By adding a complete set of features for every class, and then taking a threshold over the sum of all feature activations, we generate a map that highlights semantically-salient regions so that they can be encoded at a better quality compared to background regions. Experiments are presented on the Kodak PhotoCD dataset and the MIT Saliency Benchmark dataset, in which our algorithm achieves higher visual quality for the same compressed size.



