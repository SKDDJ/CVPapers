# Arxiv Papers in cs.CV on 2016-12-04
### SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1612.01051v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01051v4)
- **Published**: 2016-12-04 02:12:22+00:00
- **Updated**: 2019-06-11 05:02:58+00:00
- **Authors**: Bichen Wu, Alvin Wan, Forrest Iandola, Peter H. Jin, Kurt Keutzer
- **Comment**: The supplementary material of this paper, which discusses the energy
  efficiency of SqueezeDet, is attached after the main paper. The source code
  of this work is open-source released at
  https://github.com/BichenWuUCB/squeezeDet
- **Journal**: None
- **Summary**: Object detection is a crucial task for autonomous driving. In addition to requiring high accuracy to ensure safety, object detection for autonomous driving also requires real-time inference speed to guarantee prompt vehicle control, as well as small model size and energy efficiency to enable embedded system deployment.   In this work, we propose SqueezeDet, a fully convolutional neural network for object detection that aims to simultaneously satisfy all of the above constraints. In our network, we use convolutional layers not only to extract feature maps but also as the output layer to compute bounding boxes and class probabilities. The detection pipeline of our model only contains a single forward pass of a neural network, thus it is extremely fast. Our model is fully-convolutional, which leads to a small model size and better energy efficiency. While achieving the same accuracy as previous baselines, our model is 30.4x smaller, 19.7x faster, and consumes 35.2x lower energy. The code is open-sourced at \url{https://github.com/BichenWuUCB/squeezeDet}.



### Learning to Segment Object Candidates via Recursive Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.01057v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01057v4)
- **Published**: 2016-12-04 03:35:35+00:00
- **Updated**: 2018-07-29 02:17:35+00:00
- **Authors**: Tianshui Chen, Liang Lin, Xian Wu, Nong Xiao, Xiaonan Luo
- **Comment**: Accepted at TIP
- **Journal**: None
- **Summary**: To avoid the exhaustive search over locations and scales, current state-of-the-art object detection systems usually involve a crucial component generating a batch of candidate object proposals from images. In this paper, we present a simple yet effective approach for segmenting object proposals via a deep architecture of recursive neural networks (ReNNs), which hierarchically groups regions for detecting object candidates over scales. Unlike traditional methods that mainly adopt fixed similarity measures for merging regions or finding object proposals, our approach adaptively learns the region merging similarity and the objectness measure during the process of hierarchical region grouping. Specifically, guided by a structured loss, the ReNN model jointly optimizes the cross-region similarity metric with the region merging process as well as the objectness prediction. During inference of the object proposal generation, we introduce randomness into the greedy search to cope with the ambiguity of grouping regions. Extensive experiments on standard benchmarks, e.g., PASCAL VOC and ImageNet, suggest that our approach is capable of producing object proposals with high recall while well preserving the object boundaries and outperforms other existing methods in both accuracy and efficiency.



### Word Recognition with Deep Conditional Random Fields
- **Arxiv ID**: http://arxiv.org/abs/1612.01072v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01072v1)
- **Published**: 2016-12-04 05:39:42+00:00
- **Updated**: 2016-12-04 05:39:42+00:00
- **Authors**: Gang Chen, Yawei Li, Sargur N. Srihari
- **Comment**: 5 pages, published in ICIP 2016. arXiv admin note: substantial text
  overlap with arXiv:1412.3397
- **Journal**: None
- **Summary**: Recognition of handwritten words continues to be an important problem in document analysis and recognition. Existing approaches extract hand-engineered features from word images--which can perform poorly with new data sets. Recently, deep learning has attracted great attention because of the ability to learn features from raw data. Moreover they have yielded state-of-the-art results in classification tasks including character recognition and scene recognition. On the other hand, word recognition is a sequential problem where we need to model the correlation between characters. In this paper, we propose using deep Conditional Random Fields (deep CRFs) for word recognition. Basically, we combine CRFs with deep learning, in which deep features are learned and sequences are labeled in a unified framework. We pre-train the deep structure with stacked restricted Boltzmann machines (RBMs) for feature learning and optimize the entire network with an online learning algorithm. The proposed model was evaluated on two datasets, and seen to perform significantly better than competitive baseline models. The source code is available at https://github.com/ganggit/deepCRFs.



### Skin Cancer Detection and Tracking using Data Synthesis and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1612.01074v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01074v1)
- **Published**: 2016-12-04 05:47:47+00:00
- **Updated**: 2016-12-04 05:47:47+00:00
- **Authors**: Yunzhu Li, Andre Esteva, Brett Kuprel, Rob Novoa, Justin Ko, Sebastian Thrun
- **Comment**: 4 pages, 5 figures, Yunzhu Li and Andre Esteva contributed equally to
  this work
- **Journal**: None
- **Summary**: Dense object detection and temporal tracking are needed across applications domains ranging from people-tracking to analysis of satellite imagery over time. The detection and tracking of malignant skin cancers and benign moles poses a particularly challenging problem due to the general uniformity of large skin patches, the fact that skin lesions vary little in their appearance, and the relatively small amount of data available. Here we introduce a novel data synthesis technique that merges images of individual skin lesions with full-body images and heavily augments them to generate significant amounts of data. We build a convolutional neural network (CNN) based system, trained on this synthetic data, and demonstrate superior performance to traditional detection and tracking techniques. Additionally, we compare our system to humans trained with simple criteria. Our system is intended for potential clinical use to augment the capabilities of healthcare providers. While domain-specific, we believe the methods invoked in this work will be useful in applying CNNs across domains that suffer from limited data availability.



### Joint Visual Denoising and Classification using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1612.01075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01075v1)
- **Published**: 2016-12-04 05:51:55+00:00
- **Updated**: 2016-12-04 05:51:55+00:00
- **Authors**: Gang Chen, Yawei Li, Sargur N. Srihari
- **Comment**: 5 pages, 7 figures, ICIP 2016
- **Journal**: None
- **Summary**: Visual restoration and recognition are traditionally addressed in pipeline fashion, i.e. denoising followed by classification. Instead, observing correlations between the two tasks, for example clearer image will lead to better categorization and vice visa, we propose a joint framework for visual restoration and recognition for handwritten images, inspired by advances in deep autoencoder and multi-modality learning. Our model is a 3-pathway deep architecture with a hidden-layer representation which is shared by multi-inputs and outputs, and each branch can be composed of a multi-layer deep model. Thus, visual restoration and classification can be unified using shared representation via non-linear mapping, and model parameters can be learnt via backpropagation. Using MNIST and USPS data corrupted with structured noise, the proposed framework performs at least 20\% better in classification than separate pipelines, as well as clearer recovered images. The noise model and the reproducible source code is available at {\url{https://github.com/ganggit/jointmodel}}.



### End-to-end Learning of Driving Models from Large-scale Video Datasets
- **Arxiv ID**: http://arxiv.org/abs/1612.01079v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01079v2)
- **Published**: 2016-12-04 07:02:52+00:00
- **Updated**: 2017-07-23 10:10:56+00:00
- **Authors**: Huazhe Xu, Yang Gao, Fisher Yu, Trevor Darrell
- **Comment**: camera ready for CVPR2017
- **Journal**: None
- **Summary**: Robust perception-action models should be learned from training data with diverse visual appearances and realistic behaviors, yet current approaches to deep visuomotor policy learning have been generally limited to in-situ models learned from a single vehicle or a simulation environment. We advocate learning a generic vehicle motion model from large scale crowd-sourced video data, and develop an end-to-end trainable architecture for learning to predict a distribution over future vehicle egomotion from instantaneous monocular camera observations and previous vehicle state. Our model incorporates a novel FCN-LSTM architecture, which can be learned from large-scale crowd-sourced vehicle action data, and leverages available scene segmentation side tasks to improve performance under a privileged learning paradigm.



### Multi-Label Image Classification with Regional Latent Semantic Dependencies
- **Arxiv ID**: http://arxiv.org/abs/1612.01082v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01082v3)
- **Published**: 2016-12-04 07:25:25+00:00
- **Updated**: 2017-03-12 23:41:23+00:00
- **Authors**: Junjie Zhang, Qi Wu, Chunhua Shen, Jian Zhang, Jianfeng Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolution neural networks (CNN) have demonstrated advanced performance on single-label image classification, and various progress also have been made to apply CNN methods on multi-label image classification, which requires to annotate objects, attributes, scene categories etc. in a single shot. Recent state-of-the-art approaches to multi-label image classification exploit the label dependencies in an image, at global level, largely improving the labeling capacity. However, predicting small objects and visual concepts is still challenging due to the limited discrimination of the global visual features. In this paper, we propose a Regional Latent Semantic Dependencies model (RLSD) to address this problem. The utilized model includes a fully convolutional localization architecture to localize the regions that may contain multiple highly-dependent labels. The localized regions are further sent to the recurrent neural networks (RNN) to characterize the latent semantic dependencies at the regional level. Experimental results on several benchmark datasets show that our proposed model achieves the best performance compared to the state-of-the-art models, especially for predicting small objects occurred in the images. In addition, we set up an upper bound model (RLSD+ft-RPN) using bounding box coordinates during training, the experimental results also show that our RLSD can approach the upper bound without using the bounding-box annotations, which is more realistic in the real world.



### Pyramid Scene Parsing Network
- **Arxiv ID**: http://arxiv.org/abs/1612.01105v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01105v2)
- **Published**: 2016-12-04 11:46:22+00:00
- **Updated**: 2017-04-27 12:15:17+00:00
- **Authors**: Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes.



### A method for the segmentation of images based on thresholding and applied to vesicular textures
- **Arxiv ID**: http://arxiv.org/abs/1612.01131v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01131v1)
- **Published**: 2016-12-04 15:47:27+00:00
- **Updated**: 2016-12-04 15:47:27+00:00
- **Authors**: Amelia Carolina Sparavigna
- **Comment**: Keywords: Segmentation, Edge Detection, Image Analysis, 2D Textures,
  Texture Functions
- **Journal**: None
- **Summary**: In image processing, a segmentation is a process of partitioning an image into multiple sets of pixels, that are defined as super-pixels. Each super-pixel is characterized by a label or parameter. Here, we are proposing a method for determining the super-pixels based on the thresholding of the image. This approach is quite useful for studying the images showing vesicular textures.



### General models for rational cameras and the case of two-slit projections
- **Arxiv ID**: http://arxiv.org/abs/1612.01160v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01160v4)
- **Published**: 2016-12-04 18:22:36+00:00
- **Updated**: 2017-04-11 11:55:04+00:00
- **Authors**: Matthew Trager, Bernd Sturmfels, John Canny, Martial Hebert, Jean Ponce
- **Comment**: 9 pages + supplementary material
- **Journal**: None
- **Summary**: The rational camera model recently introduced in [19] provides a general methodology for studying abstract nonlinear imaging systems and their multi-view geometry. This paper builds on this framework to study "physical realizations" of rational cameras. More precisely, we give an explicit account of the mapping between between physical visual rays and image points (missing in the original description), which allows us to give simple analytical expressions for direct and inverse projections. We also consider "primitive" camera models, that are orbits under the action of various projective transformations, and lead to a general notion of intrinsic parameters. The methodology is general, but it is illustrated concretely by an in-depth study of two-slit cameras, that we model using pairs of linear projections. This simple analytical form allows us to describe models for the corresponding primitive cameras, to introduce intrinsic parameters with a clear geometric meaning, and to define an epipolar tensor characterizing two-view correspondences. In turn, this leads to new algorithms for structure from motion and self-calibration.



### Who is Mistaken?
- **Arxiv ID**: http://arxiv.org/abs/1612.01175v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01175v2)
- **Published**: 2016-12-04 20:45:42+00:00
- **Updated**: 2017-03-31 16:36:53+00:00
- **Authors**: Benjamin Eysenbach, Carl Vondrick, Antonio Torralba
- **Comment**: See project website at: http://people.csail.mit.edu/bce/mistaken/ .
  (Edit: fixed typos and references)
- **Journal**: None
- **Summary**: Recognizing when people have false beliefs is crucial for understanding their actions. We introduce the novel problem of identifying when people in abstract scenes have incorrect beliefs. We present a dataset of scenes, each visually depicting an 8-frame story in which a character has a mistaken belief. We then create a representation of characters' beliefs for two tasks in human action understanding: predicting who is mistaken, and when they are mistaken. Experiments suggest that our method for identifying mistaken characters performs better on these tasks than simple baselines. Diagnostics on our model suggest it learns important cues for recognizing mistaken beliefs, such as gaze. We believe models of people's beliefs will have many applications in action understanding, robotics, and healthcare.



### Online Localization and Prediction of Actions and Interactions
- **Arxiv ID**: http://arxiv.org/abs/1612.01194v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01194v1)
- **Published**: 2016-12-04 22:16:55+00:00
- **Updated**: 2016-12-04 22:16:55+00:00
- **Authors**: Khurram Soomro, Haroon Idrees, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a person-centric and online approach to the challenging problem of localization and prediction of actions and interactions in videos. Typically, localization or recognition is performed in an offline manner where all the frames in the video are processed together. This prevents timely localization and prediction of actions and interactions - an important consideration for many tasks including surveillance and human-machine interaction.   In our approach, we estimate human poses at each frame and train discriminative appearance models using the superpixels inside the pose bounding boxes. Since the pose estimation per frame is inherently noisy, the conditional probability of pose hypotheses at current time-step (frame) is computed using pose estimations in the current frame and their consistency with poses in the previous frames. Next, both the superpixel and pose-based foreground likelihoods are used to infer the location of actors at each time through a Conditional Random. The issue of visual drift is handled by updating the appearance models, and refining poses using motion smoothness on joint locations, in an online manner. For online prediction of action (interaction) confidences, we propose an approach based on Structural SVM that operates on short video segments, and is trained with the objective that confidence of an action or interaction increases as time progresses. Lastly, we quantify the performance of both detection and prediction together, and analyze how the prediction accuracy varies as a time function of observed action (interaction) at different levels of detection performance. Our experiments on several datasets suggest that despite using only a few frames to localize actions (interactions) at each time instant, we are able to obtain competitive results to state-of-the-art offline methods.



### DenseReg: Fully Convolutional Dense Shape Regression In-the-Wild
- **Arxiv ID**: http://arxiv.org/abs/1612.01202v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01202v2)
- **Published**: 2016-12-04 23:08:06+00:00
- **Updated**: 2017-06-19 18:51:03+00:00
- **Authors**: Rıza Alp Güler, George Trigeorgis, Epameinondas Antonakos, Patrick Snape, Stefanos Zafeiriou, Iasonas Kokkinos
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: In this paper we propose to learn a mapping from image pixels into a dense template grid through a fully convolutional network. We formulate this task as a regression problem and train our network by leveraging upon manually annotated facial landmarks "in-the-wild". We use such landmarks to establish a dense correspondence field between a three-dimensional object template and the input image, which then serves as the ground-truth for training our regression system. We show that we can combine ideas from semantic segmentation with regression networks, yielding a highly-accurate "quantized regression" architecture.   Our system, called DenseReg, allows us to estimate dense image-to-template correspondences in a fully convolutional manner. As such our network can provide useful correspondence information as a stand-alone system, while when used as an initialization for Statistical Deformable Models we obtain landmark localization results that largely outperform the current state-of-the-art on the challenging 300W benchmark. We thoroughly evaluate our method on a host of facial analysis tasks and also provide qualitative results for dense human body correspondence. We make our code available at http://alpguler.com/DenseReg.html along with supplementary materials.



