# Arxiv Papers in cs.CV on 2016-12-10
### Co-localization with Category-Consistent Features and Geodesic Distance Propagation
- **Arxiv ID**: http://arxiv.org/abs/1612.03236v3
- **DOI**: 10.1109/ICCVW.2017.134
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.03236v3)
- **Published**: 2016-12-10 01:43:01+00:00
- **Updated**: 2019-05-19 08:47:42+00:00
- **Authors**: Hieu Le, Chen-Ping Yu, Gregory Zelinsky, Dimitris Samaras
- **Comment**: IEEE International Conference on Computer Vision Workshops (ICCVW)
- **Journal**: None
- **Summary**: Co-localization is the problem of localizing objects of the same class using only the set of images that contain them. This is a challenging task because the object detector must be built without negative examples that can lead to more informative supervision signals. The main idea of our method is to cluster the feature space of a generically pre-trained CNN, to find a set of CNN features that are consistently and highly activated for an object category, which we call category-consistent CNN features. Then, we propagate their combined activation map using superpixel geodesic distances for co-localization. In our first set of experiments, we show that the proposed method achieves state-of-the-art performance on three related benchmarks: PASCAL 2007, PASCAL-2012, and the Object Discovery dataset. We also show that our method is able to detect and localize truly unseen categories, on six held-out ImageNet categories with accuracy that is significantly higher than previous state-of-the-art. Our intuitive approach achieves this success without any region proposals or object detectors and can be based on a CNN that was pre-trained purely on image classification tasks without further fine-tuning.



### StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.03242v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1612.03242v2)
- **Published**: 2016-12-10 03:11:37+00:00
- **Updated**: 2017-08-05 02:18:21+00:00
- **Authors**: Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris Metaxas
- **Comment**: ICCV 2017 Oral Presentation
- **Journal**: None
- **Summary**: Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing text-to-image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256x256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.



### Generalized Deep Image to Image Regression
- **Arxiv ID**: http://arxiv.org/abs/1612.03268v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1612.03268v1)
- **Published**: 2016-12-10 08:22:27+00:00
- **Updated**: 2016-12-10 08:22:27+00:00
- **Authors**: Venkataraman Santhanam, Vlad I. Morariu, Larry S. Davis
- **Comment**: Submitted to CVPR on November 15th, 2016. Code will be made available
  soon
- **Journal**: None
- **Summary**: We present a Deep Convolutional Neural Network architecture which serves as a generic image-to-image regressor that can be trained end-to-end without any further machinery. Our proposed architecture: the Recursively Branched Deconvolutional Network (RBDN) develops a cheap multi-context image representation very early on using an efficient recursive branching scheme with extensive parameter sharing and learnable upsampling. This multi-context representation is subjected to a highly non-linear locality preserving transformation by the remainder of our network comprising of a series of convolutions/deconvolutions without any spatial downsampling. The RBDN architecture is fully convolutional and can handle variable sized images during inference. We provide qualitative/quantitative results on $3$ diverse tasks: relighting, denoising and colorization and show that our proposed RBDN architecture obtains comparable results to the state-of-the-art on each of these tasks when used off-the-shelf without any post processing or task-specific architectural modifications.



### Towards an Automated Image De-fencing Algorithm Using Sparsity
- **Arxiv ID**: http://arxiv.org/abs/1612.03273v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.03273v1)
- **Published**: 2016-12-10 10:07:24+00:00
- **Updated**: 2016-12-10 10:07:24+00:00
- **Authors**: Sankaraganesh Jonna, Krishna K. Nakka, Rajiv R. Sahay
- **Comment**: The paper was accepted in VISAPP-2015
- **Journal**: None
- **Summary**: Conventional approaches to image de-fencing suffer from non-robust fence detection and are limited to processing images of static scenes. In this position paper, we propose an automatic de-fencing algorithm for images of dynamic scenes. We divide the problem of image de-fencing into the tasks of automated fence detection, motion estimation and fusion of data from multiple frames of a captured video of the dynamic scene. Fences are detected automatically using two approaches, namely, employing Gabor filter and a machine learning method. We cast the fence removal problem in an optimization framework, by modeling the formation of the degraded observations. The inverse problem is solved using split Bregman technique assuming total variation of the de-fenced image as the regularization constraint.



### Salient Object Detection with Convex Hull Overlap
- **Arxiv ID**: http://arxiv.org/abs/1612.03284v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1612.03284v2)
- **Published**: 2016-12-10 12:42:10+00:00
- **Updated**: 2020-10-15 18:14:11+00:00
- **Authors**: Yongqing Liang
- **Comment**: Published in: 2018 IEEE International Conference on Big Data (Big
  Data)
- **Journal**: None
- **Summary**: Salient object detection plays an important part in a vision system to detect important regions. Convolutional neural network (CNN) based methods directly train their models with large-scale datasets, but what is the crucial feature for saliency is still a problem. In this paper, we establish a novel bottom-up feature named convex hull overlap (CHO), combining with appearance contrast features, to detect salient objects. CHO feature is a kind of enhanced Gestalt cue. Psychologists believe that surroundedness reflects objects overlap relationship. An object which is on the top of the others is attractive. Our method significantly differs from other earlier works in (1) We set up a hand-crafted feature to detect salient object that our model does not need to be trained by large-scale datasets; (2) Previous works only focus on appearance features, while our CHO feature makes up the gap between the spatial object covering and the object's saliency. Our experiments on a large number of public datasets have obtained very positive results.



