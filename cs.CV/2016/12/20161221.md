# Arxiv Papers in cs.CV on 2016-12-21
### Unsupervised Place Discovery for Visual Place Classification
- **Arxiv ID**: http://arxiv.org/abs/1612.06933v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06933v1)
- **Published**: 2016-12-21 00:53:18+00:00
- **Updated**: 2016-12-21 00:53:18+00:00
- **Authors**: Fei Xiaoxiao, Tanaka Kanji, Inamoto Kouya
- **Comment**: Technical Report, 5 pages, 4 figures
- **Journal**: None
- **Summary**: In this study, we explore the use of deep convolutional neural networks (DCNNs) in visual place classification for robotic mapping and localization. An open question is how to partition the robot's workspace into places to maximize the performance (e.g., accuracy, precision, recall) of potential DCNN classifiers. This is a chicken and egg problem: If we had a well-trained DCNN classifier, it is rather easy to partition the robot's workspace into places, but the training of a DCNN classifier requires a set of pre-defined place classes. In this study, we address this problem and present several strategies for unsupervised discovery of place classes ("time cue," "location cue," "time-appearance cue," and "location-appearance cue"). We also evaluate the efficacy of the proposed methods using the publicly available University of Michigan North Campus Long-Term (NCLT) Dataset.



### Temporal Tessellation: A Unified Approach for Video Analysis
- **Arxiv ID**: http://arxiv.org/abs/1612.06950v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06950v2)
- **Published**: 2016-12-21 02:29:53+00:00
- **Updated**: 2017-04-14 19:20:10+00:00
- **Authors**: Dotan Kaufman, Gil Levi, Tal Hassner, Lior Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: We present a general approach to video understanding, inspired by semantic transfer techniques that have been successfully used for 2D image analysis. Our method considers a video to be a 1D sequence of clips, each one associated with its own semantics. The nature of these semantics -- natural language captions or other labels -- depends on the task at hand. A test video is processed by forming correspondences between its clips and the clips of reference videos with known semantics, following which, reference semantics can be transferred to the test video. We describe two matching methods, both designed to ensure that (a) reference clips appear similar to test clips and (b), taken together, the semantics of the selected reference clips is consistent and maintains temporal coherence. We use our method for video captioning on the LSMDC'16 benchmark, video summarization on the SumMe and TVSum benchmarks, Temporal Action Detection on the Thumos2014 benchmark, and sound prediction on the Greatest Hits benchmark. Our method not only surpasses the state of the art, in four out of five benchmarks, but importantly, it is the only single method we know of that was successfully applied to such a diverse range of tasks.



### Image biomarker standardisation initiative
- **Arxiv ID**: http://arxiv.org/abs/1612.07003v11
- **DOI**: 10.1148/radiol.2020191145
- **Categories**: **cs.CV**, eess.IV, I.2.1, I.2.10, I.4.7, I.4.9, J.3, I.2.1; I.2.10; I.4.7; I.4.9; J.3
- **Links**: [PDF](http://arxiv.org/pdf/1612.07003v11)
- **Published**: 2016-12-21 08:04:31+00:00
- **Updated**: 2019-12-17 11:04:08+00:00
- **Authors**: Alex Zwanenburg, Stefan Leger, Martin Vallières, Steffen Löck
- **Comment**: Added figures 2.5, 2.6. Replaced figure 2.7. Added missing section
  header for the normalised dependence count non-uniformity feature. Fixed
  layout issues with small font sizes that appeared in the last half of the
  document
- **Journal**: Radiology (2020)
- **Summary**: The image biomarker standardisation initiative (IBSI) is an independent international collaboration which works towards standardising the extraction of image biomarkers from acquired imaging for the purpose of high-throughput quantitative image analysis (radiomics). Lack of reproducibility and validation of high-throughput quantitative image analysis studies is considered to be a major challenge for the field. Part of this challenge lies in the scantiness of consensus-based guidelines and definitions for the process of translating acquired imaging into high-throughput image biomarkers. The IBSI therefore seeks to provide image biomarker nomenclature and definitions, benchmark data sets, and benchmark values to verify image processing and image biomarker calculations, as well as reporting guidelines, for high-throughput image analysis.



### An Empirical Study of Language CNN for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1612.07086v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.07086v3)
- **Published**: 2016-12-21 13:04:18+00:00
- **Updated**: 2017-08-02 12:33:50+00:00
- **Authors**: Jiuxiang Gu, Gang Wang, Jianfei Cai, Tsuhan Chen
- **Comment**: Comments: 10 pages, In proceedings of ICCV 2017
- **Journal**: None
- **Summary**: Language Models based on recurrent neural networks have dominated recent image caption generation tasks. In this paper, we introduce a Language CNN model which is suitable for statistical language modeling tasks and shows competitive performance in image captioning. In contrast to previous models which predict next word based on one previous word and hidden state, our language CNN is fed with all the previous words and can model the long-range dependencies of history words, which are critical for image captioning. The effectiveness of our approach is validated on two datasets MS COCO and Flickr30K. Our extensive experimental results show that our method outperforms the vanilla recurrent neural network based language models and is competitive with the state-of-the-art methods.



### Stochastic Multidimensional Scaling
- **Arxiv ID**: http://arxiv.org/abs/1612.07089v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1612.07089v1)
- **Published**: 2016-12-21 13:08:35+00:00
- **Updated**: 2016-12-21 13:08:35+00:00
- **Authors**: Ketan Rajawat, Sandeep Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: Multidimensional scaling (MDS) is a popular dimensionality reduction techniques that has been widely used for network visualization and cooperative localization. However, the traditional stress minimization formulation of MDS necessitates the use of batch optimization algorithms that are not scalable to large-sized problems. This paper considers an alternative stochastic stress minimization framework that is amenable to incremental and distributed solutions. A novel linear-complexity stochastic optimization algorithm is proposed that is provably convergent and simple to implement. The applicability of the proposed algorithm to localization and visualization tasks is also expounded. Extensive tests on synthetic and real datasets demonstrate the efficacy of the proposed algorithm.



### Trilaminar Multiway Reconstruction Tree for Efficient Large Scale Structure from Motion
- **Arxiv ID**: http://arxiv.org/abs/1612.07153v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.07153v1)
- **Published**: 2016-12-21 14:50:36+00:00
- **Updated**: 2016-12-21 14:50:36+00:00
- **Authors**: Kun Sun, Wenbing Tao
- **Comment**: this manuscript has been submitted to cvpr 2017
- **Journal**: None
- **Summary**: Accuracy and efficiency are two key problems in large scale incremental Structure from Motion (SfM). In this paper, we propose a unified framework to divide the image set into clusters suitable for reconstruction as well as find multiple reliable and stable starting points. Image partitioning performs in two steps. First, some small image groups are selected at places with high image density, and then all the images are clustered according to their optimal reconstruction paths to these image groups. This promises that the scene is always reconstructed from dense places to sparse areas, which can reduce error accumulation when images have weak overlap. To enable faster speed, images outside the selected group in each cluster are further divided to achieve a greater degree of parallelism. Experiments show that our method achieves significant speedup, higher accuracy and better completeness.



### A Unified Framework for Tumor Proliferation Score Prediction in Breast Histopathology
- **Arxiv ID**: http://arxiv.org/abs/1612.07180v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.07180v2)
- **Published**: 2016-12-21 15:24:34+00:00
- **Updated**: 2017-08-11 06:59:20+00:00
- **Authors**: Kyunghyun Paeng, Sangheum Hwang, Sunggyun Park, Minsoo Kim
- **Comment**: Accepted to the 3rd Workshop on Deep Learning in Medical Image
  Analysis (DLMIA 2017), MICCAI 2017
- **Journal**: None
- **Summary**: We present a unified framework to predict tumor proliferation scores from breast histopathology whole slide images. Our system offers a fully automated solution to predicting both a molecular data-based, and a mitosis counting-based tumor proliferation score. The framework integrates three modules, each fine-tuned to maximize the overall performance: An image processing component for handling whole slide images, a deep learning based mitosis detection network, and a proliferation scores prediction module. We have achieved 0.567 quadratic weighted Cohen's kappa in mitosis counting-based score prediction and 0.652 F1-score in mitosis detection. On Spearman's correlation coefficient, which evaluates predictive accuracy on the molecular data based score, the system obtained 0.6171. Our approach won first place in all of the three tasks in Tumor Proliferation Assessment Challenge 2016 which is MICCAI grand challenge.



### Multi-Agent Cooperation and the Emergence of (Natural) Language
- **Arxiv ID**: http://arxiv.org/abs/1612.07182v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.GT, cs.LG, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/1612.07182v2)
- **Published**: 2016-12-21 15:27:06+00:00
- **Updated**: 2017-03-05 21:40:51+00:00
- **Authors**: Angeliki Lazaridou, Alexander Peysakhovich, Marco Baroni
- **Comment**: Accepted at ICLR 2017
- **Journal**: None
- **Summary**: The current mainstream approach to train natural language systems is to expose them to large amounts of text. This passive learning is problematic if we are interested in developing interactive machines, such as conversational agents. We propose a framework for language learning that relies on multi-agent communication. We study this learning in the context of referential games. In these games, a sender and a receiver see a pair of images. The sender is told one of them is the target and is allowed to send a message from a fixed, arbitrary vocabulary to the receiver. The receiver must rely on this message to identify the target. Thus, the agents develop their own language interactively out of the need to communicate. We show that two networks with simple configurations are able to learn to coordinate in the referential game. We further explore how to make changes to the game environment to cause the "word meanings" induced in the game to better reflect intuitive semantic properties of the images. In addition, we present a simple strategy for grounding the agents' code into natural language. Both of these are necessary steps towards developing machines that are able to communicate with humans productively.



### Learning Motion Patterns in Videos
- **Arxiv ID**: http://arxiv.org/abs/1612.07217v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.07217v2)
- **Published**: 2016-12-21 16:14:41+00:00
- **Updated**: 2017-04-10 11:33:59+00:00
- **Authors**: Pavel Tokmakov, Karteek Alahari, Cordelia Schmid
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of determining whether an object is in motion, irrespective of camera motion, is far from being solved. We address this challenging task by learning motion patterns in videos. The core of our approach is a fully convolutional network, which is learned entirely from synthetic video sequences, and their ground-truth optical flow and motion segmentation. This encoder-decoder style architecture first learns a coarse representation of the optical flow field features, and then refines it iteratively to produce motion labels at the original high-resolution. We further improve this labeling with an objectness map and a conditional random field, to account for errors in optical flow, and also to focus on moving "things" rather than "stuff". The output label of each pixel denotes whether it has undergone independent motion, i.e., irrespective of camera motion. We demonstrate the benefits of this learning framework on the moving object segmentation task, where the goal is to segment all objects in motion. Our approach outperforms the top method on the recently released DAVIS benchmark dataset, comprising real-world sequences, by 5.6%. We also evaluate on the Berkeley motion segmentation database, achieving state-of-the-art results.



### Top-down Visual Saliency Guided by Captions
- **Arxiv ID**: http://arxiv.org/abs/1612.07360v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.07360v2)
- **Published**: 2016-12-21 22:02:34+00:00
- **Updated**: 2017-04-12 22:49:47+00:00
- **Authors**: Vasili Ramanishka, Abir Das, Jianming Zhang, Kate Saenko
- **Comment**: CVPR 2017 camera ready version
- **Journal**: None
- **Summary**: Neural image/video captioning models can generate accurate descriptions, but their internal process of mapping regions to words is a black box and therefore difficult to explain. Top-down neural saliency methods can find important regions given a high-level semantic task such as object classification, but cannot use a natural language sentence as the top-down input for the task. In this paper, we propose Caption-Guided Visual Saliency to expose the region-to-word mapping in modern encoder-decoder networks and demonstrate that it is learned implicitly from caption training data, without any pixel-level annotations. Our approach can produce spatial or spatiotemporal heatmaps for both predicted captions, and for arbitrary query sentences. It recovers saliency without the overhead of introducing explicit attention layers, and can be used to analyze a variety of existing model architectures and improve their design. Evaluation on large-scale video and image datasets demonstrates that our approach achieves comparable captioning performance with existing methods while providing more accurate saliency heatmaps. Our code is available at visionlearninggroup.github.io/caption-guided-saliency/.



### Automatic Identification of Scenedesmus Polymorphic Microalgae from Microscopic Images
- **Arxiv ID**: http://arxiv.org/abs/1612.07379v2
- **DOI**: 10.1007/s10044-017-0662-3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.07379v2)
- **Published**: 2016-12-21 23:09:01+00:00
- **Updated**: 2017-10-23 16:13:18+00:00
- **Authors**: Jhony-Heriberto Giraldo-Zuluaga, Geman Diez, Alexander Gomez, Tatiana Martinez, Mariana Peñuela Vasquez, Jesus Francisco Vargas Bonilla, Augusto Salazar
- **Comment**: This is a pre-print of an article published in Pattern Analysis and
  Applications. The final authenticated version is available online at:
  https://doi.org/10.1007/s10044-017-0662-3, Pattern Anal Applic (2017)
- **Journal**: None
- **Summary**: Microalgae counting is used to measure biomass quantity. Usually, it is performed in a manual way using a Neubauer chamber and expert criterion, with the risk of a high error rate. This paper addresses the methodology for automatic identification of Scenedesmus microalgae (used in the methane production and food industry) and applies it to images captured by a digital microscope. The use of contrast adaptive histogram equalization for pre-processing, and active contours for segmentation are presented. The calculation of statistical features (Histogram of Oriented Gradients, Hu and Zernike moments) with texture features (Haralick and Local Binary Patterns descriptors) are proposed for algae characterization. Scenedesmus algae can build coenobia consisting of 1, 2, 4 and 8 cells. The amount of algae of each coenobium helps to determine the amount of lipids, proteins, and other substances in a given sample of a algae crop. The knowledge of the quantity of those elements improves the quality of bioprocess applications. Classification of coenobia achieves accuracies of 98.63% and 97.32% with Support Vector Machine (SVM) and Artificial Neural Network (ANN), respectively. According to the results it is possible to consider the proposed methodology as an alternative to the traditional technique for algae counting. The database used in this paper is publicly available for download.



