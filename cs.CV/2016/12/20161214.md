# Arxiv Papers in cs.CV on 2016-12-14
### Disentangling Space and Time in Video with Hierarchical Variational Auto-encoders
- **Arxiv ID**: http://arxiv.org/abs/1612.04440v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1612.04440v2)
- **Published**: 2016-12-14 00:20:46+00:00
- **Updated**: 2016-12-19 17:17:26+00:00
- **Authors**: Will Grathwohl, Aaron Wilson
- **Comment**: fixed typo in equation 16
- **Journal**: None
- **Summary**: There are many forms of feature information present in video data. Principle among them are object identity information which is largely static across multiple video frames, and object pose and style information which continuously transforms from frame to frame. Most existing models confound these two types of representation by mapping them to a shared feature space. In this paper we propose a probabilistic approach for learning separable representations of object identity and pose information using unsupervised video data. Our approach leverages a deep generative model with a factored prior distribution that encodes properties of temporal invariances in the hidden feature set. Learning is achieved via variational inference. We present results of learning identity and pose information on a dataset of moving characters as well as a dataset of rotating 3D objects. Our experimental results demonstrate our model's success in factoring its representation, and demonstrate that the model achieves improved performance in transfer learning tasks.



### Analysis of proposed PDE-based underwater image enhancement algorithms
- **Arxiv ID**: http://arxiv.org/abs/1612.04447v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.04447v1)
- **Published**: 2016-12-14 00:47:51+00:00
- **Updated**: 2016-12-14 00:47:51+00:00
- **Authors**: U. A. Nnolim
- **Comment**: 57 pages, 9 figures
- **Journal**: None
- **Summary**: This report describes the experimental analysis of proposed underwater image enhancement algorithms based on partial differential equations (PDEs). The algorithms perform simultaneous smoothing and enhancement due to the combination of both processes within the PDE-formulation. The framework enables the incorporation of suitable colour and contrast enhancement algorithms within one unified functional. Additional modification of the formulation includes the combination of the popular Contrast Limited Adaptive Histogram Equalization (CLAHE) with the proposed approach. This modification enables the hybrid algorithm to provide both local enhancement (due to the CLAHE) and global enhancement (due to the proposed contrast term). Additionally, the CLAHE clip limit parameter is computed dynamically in each iteration and used to gauge the amount of local enhancement performed by the CLAHE within the formulation. This enables the algorithm to reduce or prevent the enhancement of noisy artifacts, which if present, are also smoothed out by the anisotropic diffusion term within the PDE formulation. In other words, the modified algorithm combines the strength of the CLAHE, AD and the contrast term while minimizing their weaknesses. Ultimately, the system is optimized using image data metrics for automated enhancement and compromise between visual and quantitative results. Experiments indicate that the proposed algorithms perform a series of functions such as illumination correction, colour enhancement correction and restoration, contrast enhancement and noise suppression. Moreover, the proposed approaches surpass most other conventional algorithms found in the literature.



### Sparse Factorization Layers for Neural Networks with Limited Supervision
- **Arxiv ID**: http://arxiv.org/abs/1612.04468v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1612.04468v1)
- **Published**: 2016-12-14 03:13:29+00:00
- **Updated**: 2016-12-14 03:13:29+00:00
- **Authors**: Parker Koch, Jason J. Corso
- **Comment**: None
- **Journal**: None
- **Summary**: Whereas CNNs have demonstrated immense progress in many vision problems, they suffer from a dependence on monumental amounts of labeled training data. On the other hand, dictionary learning does not scale to the size of problems that CNNs can handle, despite being very effective at low-level vision tasks such as denoising and inpainting. Recently, interest has grown in adapting dictionary learning methods for supervised tasks such as classification and inverse problems. We propose two new network layers that are based on dictionary learning: a sparse factorization layer and a convolutional sparse factorization layer, analogous to fully-connected and convolutional layers, respectively. Using our derivations, these layers can be dropped in to existing CNNs, trained together in an end-to-end fashion with back-propagation, and leverage semisupervision in ways classical CNNs cannot. We experimentally compare networks with these two new layers against a baseline CNN. Our results demonstrate that networks with either of the sparse factorization layers are able to outperform classical CNNs when supervised data are few. They also show performance improvements in certain tasks when compared to the CNN with no sparse factorization layers with the same exact number of parameters.



### Single Image Action Recognition using Semantic Body Part Actions
- **Arxiv ID**: http://arxiv.org/abs/1612.04520v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.04520v1)
- **Published**: 2016-12-14 07:54:55+00:00
- **Updated**: 2016-12-14 07:54:55+00:00
- **Authors**: Zhichen Zhao, Huimin Ma, Shaodi You
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel single image action recognition algorithm which is based on the idea of semantic body part actions. Unlike existing bottom up methods, we argue that the human action is a combination of meaningful body part actions. In detail, we divide human body into five parts: head, torso, arms, hands and legs. And for each of the body parts, we define several semantic body part actions, e.g., hand holding, hand waving. These semantic body part actions are strongly related to the body actions, e.g., writing, and jogging. Based on the idea, we propose a deep neural network based system: first, body parts are localized by a Semi-FCN network. Second, for each body parts, a Part Action Res-Net is used to predict semantic body part actions. And finally, we use SVM to fuse the body part actions and predict the entire body action. Experiments on two dataset: PASCAL VOC 2012 and Stanford-40 report mAP improvement from the state-of-the-art by 3.8% and 2.6% respectively.



### Astronomical image reconstruction with convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1612.04526v2
- **DOI**: None
- **Categories**: **cs.CV**, astro-ph.IM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1612.04526v2)
- **Published**: 2016-12-14 08:17:07+00:00
- **Updated**: 2017-06-07 08:25:27+00:00
- **Authors**: RÃ©mi Flamary
- **Comment**: None
- **Journal**: None
- **Summary**: State of the art methods in astronomical image reconstruction rely on the resolution of a regularized or constrained optimization problem. Solving this problem can be computationally intensive and usually leads to a quadratic or at least superlinear complexity w.r.t. the number of pixels in the image. We investigate in this work the use of convolutional neural networks for image reconstruction in astronomy. With neural networks, the computationally intensive tasks is the training step, but the prediction step has a fixed complexity per pixel, i.e. a linear complexity. Numerical experiments show that our approach is both computationally efficient and competitive with other state of the art methods in addition to being interpretable.



### Permutation-equivariant neural networks applied to dynamics prediction
- **Arxiv ID**: http://arxiv.org/abs/1612.04530v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1612.04530v1)
- **Published**: 2016-12-14 08:31:53+00:00
- **Updated**: 2016-12-14 08:31:53+00:00
- **Authors**: Nicholas Guttenberg, Nathaniel Virgo, Olaf Witkowski, Hidetoshi Aoki, Ryota Kanai
- **Comment**: 7 pages, 4 figures
- **Journal**: None
- **Summary**: The introduction of convolutional layers greatly advanced the performance of neural networks on image tasks due to innately capturing a way of encoding and learning translation-invariant operations, matching one of the underlying symmetries of the image domain. In comparison, there are a number of problems in which there are a number of different inputs which are all 'of the same type' --- multiple particles, multiple agents, multiple stock prices, etc. The corresponding symmetry to this is permutation symmetry, in that the algorithm should not depend on the specific ordering of the input data. We discuss a permutation-invariant neural network layer in analogy to convolutional layers, and show the ability of this architecture to learn to predict the motion of a variable number of interacting hard discs in 2D. In the same way that convolutional layers can generalize to different image sizes, the permutation layer we describe generalizes to different numbers of objects.



### The Mehler-Fock Transform and some Applications in Texture Analysis and Color Processing
- **Arxiv ID**: http://arxiv.org/abs/1612.04573v1
- **DOI**: None
- **Categories**: **cs.CV**, 43A32, I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1612.04573v1)
- **Published**: 2016-12-14 11:04:04+00:00
- **Updated**: 2016-12-14 11:04:04+00:00
- **Authors**: Reiner Lenz
- **Comment**: None
- **Journal**: None
- **Summary**: Many stochastic processes are defined on special geometrical objects like spheres and cones. We describe how tools from harmonic analysis, i.e. Fourier analysis on groups, can be used to investigate probability density functions (pdfs) on groups and homogeneous spaces. We consider the special case of the Lorentz group SU(1,1) and the unit disk with its hyperbolic geometry, but the procedure can be generalized to a much wider class of Lie-groups. We mainly concentrate on the Mehler-Fock transform which is the radial part of the Fourier transform on the disk. Some of the characteristic features of this transform are the relation to group-convolutions, the isometry between signal and transform space, the relation to the Laplace-Beltrami operator and the relation to group representation theory. We will give an overview over these properties and their applications in signal processing. We will illustrate the theory with two examples from low-level vision and color image processing.



### Defining the Pose of any 3D Rigid Object and an Associated Distance
- **Arxiv ID**: http://arxiv.org/abs/1612.04631v3
- **DOI**: 10.1007/s11263-017-1052-4
- **Categories**: **cs.CV**, math.MG, physics.class-ph
- **Links**: [PDF](http://arxiv.org/pdf/1612.04631v3)
- **Published**: 2016-12-14 13:46:55+00:00
- **Updated**: 2017-11-29 14:10:04+00:00
- **Authors**: Romain BrÃ©gier, FrÃ©dÃ©ric Devernay, Laetitia Leyrit, James Crowley
- **Comment**: None
- **Journal**: International Journal of Computer Vision, Springer Verlag, 2017
- **Summary**: The pose of a rigid object is usually regarded as a rigid transformation, described by a translation and a rotation. However, equating the pose space with the space of rigid transformations is in general abusive, as it does not account for objects with proper symmetries -- which are common among man-made objects.In this article, we define pose as a distinguishable static state of an object, and equate a pose with a set of rigid transformations. Based solely on geometric considerations, we propose a frame-invariant metric on the space of possible poses, valid for any physical rigid object, and requiring no arbitrary tuning. This distance can be evaluated efficiently using a representation of poses within an Euclidean space of at most 12 dimensions depending on the object's symmetries. This makes it possible to efficiently perform neighborhood queries such as radius searches or k-nearest neighbor searches within a large set of poses using off-the-shelf methods. Pose averaging considering this metric can similarly be performed easily, using a projection function from the Euclidean space onto the pose space. The practical value of those theoretical developments is illustrated with an application of pose estimation of instances of a 3D rigid object given an input depth map, via a Mean Shift procedure.



### Harmonic Networks: Deep Translation and Rotation Equivariance
- **Arxiv ID**: http://arxiv.org/abs/1612.04642v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1612.04642v2)
- **Published**: 2016-12-14 14:01:11+00:00
- **Updated**: 2017-04-11 13:34:17+00:00
- **Authors**: Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov, Gabriel J. Brostow
- **Comment**: Submitted to CVPR 2017
- **Journal**: None
- **Summary**: Translating or rotating an input image should not affect the results of many computer vision tasks. Convolutional neural networks (CNNs) are already translation equivariant: input image translations produce proportionate feature map translations. This is not the case for rotations. Global rotation equivariance is typically sought through data augmentation, but patch-wise equivariance is more difficult. We present Harmonic Networks or H-Nets, a CNN exhibiting equivariance to patch-wise translation and 360-rotation. We achieve this by replacing regular CNN filters with circular harmonics, returning a maximal response and orientation for every receptive field patch.   H-Nets use a rich, parameter-efficient and low computational complexity representation, and we show that deep feature maps within the network encode complicated rotational invariants. We demonstrate that our layers are general enough to be used in conjunction with the latest architectures and techniques, such as deep supervision and batch normalization. We also achieve state-of-the-art classification on rotated-MNIST, and competitive results on other benchmark challenges.



### UnrealStereo: Controlling Hazardous Factors to Analyze Stereo Vision
- **Arxiv ID**: http://arxiv.org/abs/1612.04647v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.04647v2)
- **Published**: 2016-12-14 14:13:59+00:00
- **Updated**: 2018-09-06 12:08:22+00:00
- **Authors**: Yi Zhang, Weichao Qiu, Qi Chen, Xiaolin Hu, Alan Yuille
- **Comment**: 3DV 2018 (oral)
- **Journal**: None
- **Summary**: A reliable stereo algorithm is critical for many robotics applications. But textureless and specular regions can easily cause failure by making feature matching difficult. Understanding whether an algorithm is robust to these hazardous regions is important. Although many stereo benchmarks have been developed to evaluate performance, it is hard to quantify the effect of hazardous regions in real images because the location and severity of these regions are unknown. In this paper, we develop a synthetic image generation tool enabling to control hazardous factors, such as making objects more specular or transparent, to produce hazardous regions at different degrees. The densely controlled sampling strategy in virtual worlds enables to effectively stress test stereo algorithms by varying the types and degrees of the hazard. We generate a large synthetic image dataset with automatically computed hazardous regions and analyze algorithms on these regions. The observations from synthetic images are further validated by annotating hazardous regions in real-world datasets Middlebury and KITTI (which gives a sparse sampling of the hazards). Our synthetic image generation tool is based on a game engine Unreal Engine 4 and will be open-source along with the virtual scenes in our experiments. Many publicly available realistic game contents can be used by our tool to provide an enormous resource for development and evaluation of algorithms.



### Efficient phase retrieval based on dark fringe recognition with an ability of bypassing invalid fringes
- **Arxiv ID**: http://arxiv.org/abs/1612.04733v1
- **DOI**: 10.1016/j.optcom.2017.06.058
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1612.04733v1)
- **Published**: 2016-12-14 17:13:03+00:00
- **Updated**: 2016-12-14 17:13:03+00:00
- **Authors**: Wen-Kai Yu, An-Dong Xiong, Xu-Ri Yao, Guang-Jie Zhai, Qing Zhao
- **Comment**: 14 pages, 5 figures
- **Journal**: Optics Communications, 402, 413-421 (2017)
- **Summary**: This paper discusses the noisy phase retrieval problem: recovering a complex image signal with independent noise from quadratic measurements. Inspired by the dark fringes shown in the measured images of the array detector, a novel phase retrieval approach is proposed and demonstrated both theoretically and experimentally to recognize the dark fringes and bypass the invalid fringes. A more accurate relative phase ratio between arbitrary two pixels is achieved by calculating the multiplicative ratios (or the sum of phase difference) on the path between them. Then the object phase image can be reconstructed precisely. Our approach is a good choice for retrieving high-quality phase images from noisy signals and has many potential applications in the fields such as X-ray crystallography, diffractive imaging, and so on.



### Super-resolution Reconstruction of SAR Image based on Non-Local Means Denoising Combined with BP Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1612.04755v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.04755v1)
- **Published**: 2016-12-14 18:02:56+00:00
- **Updated**: 2016-12-14 18:02:56+00:00
- **Authors**: Zeling Wu, Haoxiang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this article, we propose a super-resolution method to resolve the problem of image low spatial because of the limitation of imaging devices. We make use of the strong non-linearity mapped ability of the back-propagation neural networks(BPNN). Training sample images are got by undersampled method. The elements chose as the inputs of the BPNN are pixels referred to Non-local means(NL-Means). Making use of the self-similarity of the images, those inputs are the pixels which are pixels gained from modified NL-means which is specific for super-resolution. Besides, small change on core function of NL-means has been applied in the method we use in this article so that we can have a clearer edge in the shrunk image. Experimental results gained from the Peak Signal to Noise Ratio(PSNR) and the Equivalent Number of Look(ENL), indicate that adding the similar pixels as inputs will increase the results than not taking them into consideration.



### Attentive Explanations: Justifying Decisions and Pointing to the Evidence
- **Arxiv ID**: http://arxiv.org/abs/1612.04757v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1612.04757v2)
- **Published**: 2016-12-14 18:12:47+00:00
- **Updated**: 2017-07-25 09:33:03+00:00
- **Authors**: Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Bernt Schiele, Trevor Darrell, Marcus Rohrbach
- **Comment**: None
- **Journal**: None
- **Summary**: Deep models are the defacto standard in visual decision models due to their impressive performance on a wide array of visual tasks. However, they are frequently seen as opaque and are unable to explain their decisions. In contrast, humans can justify their decisions with natural language and point to the evidence in the visual world which led to their decisions. We postulate that deep models can do this as well and propose our Pointing and Justification (PJ-X) model which can justify its decision with a sentence and point to the evidence by introspecting its decision and explanation process using an attention mechanism. Unfortunately there is no dataset available with reference explanations for visual decision making. We thus collect two datasets in two domains where it is interesting and challenging to explain decisions. First, we extend the visual question answering task to not only provide an answer but also a natural language explanation for the answer. Second, we focus on explaining human activities which is traditionally more challenging than object classification. We extensively evaluate our PJ-X model, both on the justification and pointing tasks, by comparing it to prior models and ablations using both automatic and human evaluations.



### Detect, Replace, Refine: Deep Structured Prediction For Pixel Wise Labeling
- **Arxiv ID**: http://arxiv.org/abs/1612.04770v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.04770v1)
- **Published**: 2016-12-14 18:54:33+00:00
- **Updated**: 2016-12-14 18:54:33+00:00
- **Authors**: Spyros Gidaris, Nikos Komodakis
- **Comment**: None
- **Journal**: None
- **Summary**: Pixel wise image labeling is an interesting and challenging problem with great significance in the computer vision community. In order for a dense labeling algorithm to be able to achieve accurate and precise results, it has to consider the dependencies that exist in the joint space of both the input and the output variables. An implicit approach for modeling those dependencies is by training a deep neural network that, given as input an initial estimate of the output labels and the input image, it will be able to predict a new refined estimate for the labels. In this context, our work is concerned with what is the optimal architecture for performing the label improvement task. We argue that the prior approaches of either directly predicting new label estimates or predicting residual corrections w.r.t. the initial labels with feed-forward deep network architectures are sub-optimal. Instead, we propose a generic architecture that decomposes the label improvement task to three steps: 1) detecting the initial label estimates that are incorrect, 2) replacing the incorrect labels with new ones, and finally 3) refining the renewed labels by predicting residual corrections w.r.t. them. Furthermore, we explore and compare various other alternative architectures that consist of the aforementioned Detection, Replace, and Refine components. We extensively evaluate the examined architectures in the challenging task of dense disparity estimation (stereo matching) and we report both quantitative and qualitative results on three different datasets. Finally, our dense disparity estimation network that implements the proposed generic architecture, achieves state-of-the-art results in the KITTI 2015 test surpassing prior approaches by a significant margin.



### Beam Search for Learning a Deep Convolutional Neural Network of 3D Shapes
- **Arxiv ID**: http://arxiv.org/abs/1612.04774v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/1612.04774v1)
- **Published**: 2016-12-14 19:06:05+00:00
- **Updated**: 2016-12-14 19:06:05+00:00
- **Authors**: Xu Xu, Sinisa Todorovic
- **Comment**: ICPR 2016
- **Journal**: None
- **Summary**: This paper addresses 3D shape recognition. Recent work typically represents a 3D shape as a set of binary variables corresponding to 3D voxels of a uniform 3D grid centered on the shape, and resorts to deep convolutional neural networks(CNNs) for modeling these binary variables. Robust learning of such CNNs is currently limited by the small datasets of 3D shapes available, an order of magnitude smaller than other common datasets in computer vision. Related work typically deals with the small training datasets using a number of ad hoc, hand-tuning strategies. To address this issue, we formulate CNN learning as a beam search aimed at identifying an optimal CNN architecture, namely, the number of layers, nodes, and their connectivity in the network, as well as estimating parameters of such an optimal CNN. Each state of the beam search corresponds to a candidate CNN. Two types of actions are defined to add new convolutional filters or new convolutional layers to a parent CNN, and thus transition to children states. The utility function of each action is efficiently computed by transferring parameter values of the parent CNN to its children, thereby enabling an efficient beam search. Our experimental evaluation on the 3D ModelNet dataset demonstrates that our model pursuit using the beam search yields a CNN with superior performance on 3D shape classification than the state of the art.



### Registering large volume serial-section electron microscopy image sets for neural circuit reconstruction using FFT signal whitening
- **Arxiv ID**: http://arxiv.org/abs/1612.04787v1
- **DOI**: 10.1109/AIPR.2016.8010595
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.04787v1)
- **Published**: 2016-12-14 20:03:05+00:00
- **Updated**: 2016-12-14 20:03:05+00:00
- **Authors**: Arthur W. Wetzel, Jennifer Bakal, Markus Dittrich, David G. C. Hildebrand, Josh L. Morgan, Jeff W. Lichtman
- **Comment**: 10 pages, 4 figures as submitted for the 2016 IEEE Applied Imagery
  and Pattern Recognition Workshop proceedings, Oct 18-20, 2016
- **Journal**: None
- **Summary**: The detailed reconstruction of neural anatomy for connectomics studies requires a combination of resolution and large three-dimensional data capture provided by serial section electron microscopy (ssEM). The convergence of high throughput ssEM imaging and improved tissue preparation methods now allows ssEM capture of complete specimen volumes up to cubic millimeter scale. The resulting multi-terabyte image sets span thousands of serial sections and must be precisely registered into coherent volumetric forms in which neural circuits can be traced and segmented. This paper introduces a Signal Whitening Fourier Transform Image Registration approach (SWiFT-IR) under development at the Pittsburgh Supercomputing Center and its use to align mouse and zebrafish brain datasets acquired using the wafer mapper ssEM imaging technology recently developed at Harvard University. Unlike other methods now used for ssEM registration, SWiFT-IR modifies its spatial frequency response during image matching to maximize a signal-to-noise measure used as its primary indicator of alignment quality. This alignment signal is more robust to rapid variations in biological content and unavoidable data distortions than either phase-only or standard Pearson correlation, thus allowing more precise alignment and statistical confidence. These improvements in turn enable an iterative registration procedure based on projections through multiple sections rather than more typical adjacent-pair matching methods. This projection approach, when coupled with known anatomical constraints and iteratively applied in a multi-resolution pyramid fashion, drives the alignment into a smooth form that properly represents complex and widely varying anatomical content such as the full cross-section zebrafish data.



### Deep Function Machines: Generalized Neural Networks for Topological Layer Expression
- **Arxiv ID**: http://arxiv.org/abs/1612.04799v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.04799v2)
- **Published**: 2016-12-14 20:39:23+00:00
- **Updated**: 2017-11-06 20:51:33+00:00
- **Authors**: William H. Guss
- **Comment**: 23 pages, 9 figures, with experiments
- **Journal**: None
- **Summary**: In this paper we propose a generalization of deep neural networks called deep function machines (DFMs). DFMs act on vector spaces of arbitrary (possibly infinite) dimension and we show that a family of DFMs are invariant to the dimension of input data; that is, the parameterization of the model does not directly hinge on the quality of the input (eg. high resolution images). Using this generalization we provide a new theory of universal approximation of bounded non-linear operators between function spaces. We then suggest that DFMs provide an expressive framework for designing new neural network layer types with topological considerations in mind. Finally, we introduce a novel architecture, RippLeNet, for resolution invariant computer vision, which empirically achieves state of the art invariance.



### Spectral video construction from RGB video: Application to Image Guided Neurosurgery
- **Arxiv ID**: http://arxiv.org/abs/1612.04809v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.04809v1)
- **Published**: 2016-12-14 20:55:16+00:00
- **Updated**: 2016-12-14 20:55:16+00:00
- **Authors**: Md. Abul Hasnat, Jussi Parkkinen, Markku Hauta-Kasari
- **Comment**: Experiments were conducted in 2011, Paper rewritten with recent
  review in 2015
- **Journal**: None
- **Summary**: Spectral imaging has received enormous interest in the field of medical imaging modalities. It provides a powerful tool for the analysis of different organs and non-invasive tissues. Therefore, significant amount of research has been conducted to explore the possibility of using spectral imaging in biomedical applications. To observe spectral image information in real time during surgery and monitor the temporal changes in the organs and tissues is a demanding task. Available spectral imaging devices are not sufficient to accomplish this task with an acceptable spatial and spectral resolution. A solution to this problem is to estimate the spectral video from RGB video and perform visualization with the most prominent spectral bands. In this research, we propose a framework to generate neurosurgery spectral video from RGB video. A spectral estimation technique is applied on each RGB video frames. The RGB video is captured using a digital camera connected with an operational microscope dedicated to neurosurgery. A database of neurosurgery spectral images is used to collect training data and evaluate the estimation accuracy. A searching technique is used to identify the best training set. Five different spectrum estimation techniques are experimented to indentify the best method. Although this framework is established for neurosurgery spectral video generation, however, the methodology outlined here would also be applicable to other similar research.



### Fast-AT: Fast Automatic Thumbnail Generation using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.04811v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.04811v2)
- **Published**: 2016-12-14 20:59:19+00:00
- **Updated**: 2017-04-10 23:17:19+00:00
- **Authors**: Seyed A. Esmaeili, Bharat Singh, Larry S. Davis
- **Comment**: None
- **Journal**: None
- **Summary**: Fast-AT is an automatic thumbnail generation system based on deep neural networks. It is a fully-convolutional deep neural network, which learns specific filters for thumbnails of different sizes and aspect ratios. During inference, the appropriate filter is selected depending on the dimensions of the target thumbnail. Unlike most previous work, Fast-AT does not utilize saliency but addresses the problem directly. In addition, it eliminates the need to conduct region search on the saliency map. The model generalizes to thumbnails of different sizes including those with extreme aspect ratios and can generate thumbnails in real time. A data set of more than 70,000 thumbnail annotations was collected to train Fast-AT. We show competitive results in comparison to existing techniques.



### The More You Know: Using Knowledge Graphs for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1612.04844v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.04844v2)
- **Published**: 2016-12-14 21:18:30+00:00
- **Updated**: 2017-04-22 00:43:18+00:00
- **Authors**: Kenneth Marino, Ruslan Salakhutdinov, Abhinav Gupta
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: One characteristic that sets humans apart from modern learning-based computer vision algorithms is the ability to acquire knowledge about the world and use that knowledge to reason about the visual world. Humans can learn about the characteristics of objects and the relationships that occur between them to learn a large variety of visual concepts, often with few examples. This paper investigates the use of structured prior knowledge in the form of knowledge graphs and shows that using this knowledge improves performance on image classification. We build on recent work on end-to-end learning on graphs, introducing the Graph Search Neural Network as a way of efficiently incorporating large knowledge graphs into a vision classification pipeline. We show in a number of experiments that our method outperforms standard neural network baselines for multi-label classification.



### Temporal-Needle: A view and appearance invariant video descriptor
- **Arxiv ID**: http://arxiv.org/abs/1612.04854v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.04854v1)
- **Published**: 2016-12-14 21:46:09+00:00
- **Updated**: 2016-12-14 21:46:09+00:00
- **Authors**: Michal Yarom, Michal Irani
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to detect similar actions across videos can be very useful for real-world applications in many fields. However, this task is still challenging for existing systems, since videos that present the same action, can be taken from significantly different viewing directions, performed by different actors and backgrounds and under various video qualities. Video descriptors play a significant role in these systems. In this work we propose the "temporal-needle" descriptor which captures the dynamic behavior, while being invariant to viewpoint and appearance. The descriptor is computed using multi temporal scales of the video and by computing self-similarity for every patch through time in every temporal scale. The descriptor is computed for every pixel in the video. However, to find similar actions across videos, we consider only a small subset of the descriptors - the statistical significant descriptors. This allow us to find good correspondences across videos more efficiently. Using the descriptor, we were able to detect the same behavior across videos in a variety of scenarios. We demonstrate the use of the descriptor in tasks such as temporal and spatial alignment, action detection and even show its potential in unsupervised video clustering into categories. In this work we handled only videos taken with stationary cameras, but the descriptor can be extended to handle moving camera as well.



### Border-Peeling Clustering
- **Arxiv ID**: http://arxiv.org/abs/1612.04869v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.04869v2)
- **Published**: 2016-12-14 22:22:41+00:00
- **Updated**: 2019-06-30 07:15:44+00:00
- **Authors**: Hadar Averbuch-Elor, Nadav Bar, Daniel Cohen-Or
- **Comment**: 9 pages, 9 figures, supplementary material added as ancillary file
- **Journal**: None
- **Summary**: In this paper, we present a novel non-parametric clustering technique. Our technique is based on the notion that each latent cluster is comprised of layers that surround its core, where the external layers, or border points, implicitly separate the clusters. Unlike previous techniques, such as DBSCAN, where the cores of the clusters are defined directly by their densities, here the latent cores are revealed by a progressive peeling of the border points. Analyzing the density of the local neighborhoods allows identifying the border points and associating them with points of inner layers. We show that the peeling process adapts to the local densities and characteristics to successfully separate adjacent clusters (of possibly different densities). We extensively tested our technique on large sets of labeled data, including high-dimensional datasets of deep features that were trained by a convolutional neural network. We show that our technique is competitive to other state-of-the-art non-parametric methods using a fixed set of parameters throughout the experiments.



### Scale Coding Bag of Deep Features for Human Attribute and Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1612.04884v2
- **DOI**: 10.1007/s00138-017-0871-1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.04884v2)
- **Published**: 2016-12-14 23:44:23+00:00
- **Updated**: 2018-03-26 12:11:49+00:00
- **Authors**: Fahad Shahbaz Khan, Joost van de Weijer, Rao Muhammad Anwer, Andrew D. Bagdanov, Michael Felsberg, Jorma Laaksonen
- **Comment**: To appear in Machine Vision and Applications
- **Journal**: None
- **Summary**: Most approaches to human attribute and action recognition in still images are based on image representation in which multi-scale local features are pooled across scale into a single, scale-invariant encoding. Both in bag-of-words and the recently popular representations based on convolutional neural networks, local features are computed at multiple scales. However, these multi-scale convolutional features are pooled into a single scale-invariant representation. We argue that entirely scale-invariant image representations are sub-optimal and investigate approaches to scale coding within a Bag of Deep Features framework.   Our approach encodes multi-scale information explicitly during the image encoding stage. We propose two strategies to encode multi-scale information explicitly in the final image representation. We validate our two scale coding techniques on five datasets: Willow, PASCAL VOC 2010, PASCAL VOC 2012, Stanford-40 and Human Attributes (HAT-27). On all datasets, the proposed scale coding approaches outperform both the scale-invariant method and the standard deep features of the same network. Further, combining our scale coding approaches with standard deep features leads to consistent improvement over the state-of-the-art.



