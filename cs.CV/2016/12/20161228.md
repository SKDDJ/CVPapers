# Arxiv Papers in cs.CV on 2016-12-28
### An FFT-based Synchronization Approach to Recognize Human Behaviors using STN-LFP Signal
- **Arxiv ID**: http://arxiv.org/abs/1612.08780v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1612.08780v1)
- **Published**: 2016-12-28 01:08:56+00:00
- **Updated**: 2016-12-28 01:08:56+00:00
- **Authors**: Hosein M. Golshan, Adam O. Hebb, Sara J. Hanrahan, Joshua Nedrud, Mohammad H. Mahoor
- **Comment**: IEEE Conf on ICASSP 2017
- **Journal**: None
- **Summary**: Classification of human behavior is key to developing closed-loop Deep Brain Stimulation (DBS) systems, which may be able to decrease the power consumption and side effects of the existing systems. Recent studies have shown that the Local Field Potential (LFP) signals from both Subthalamic Nuclei (STN) of the brain can be used to recognize human behavior. Since the DBS leads implanted in each STN can collect three bipolar signals, the selection of a suitable pair of LFPs that achieves optimal recognition performance is still an open problem to address. Considering the presence of synchronized aggregate activity in the basal ganglia, this paper presents an FFT-based synchronization approach to automatically select a relevant pair of LFPs and use the pair together with an SVM-based MKL classifier for behavior recognition purposes. Our experiments on five subjects show the superiority of the proposed approach compared to other methods used for behavior classification.



### Superpixel Segmentation Using Gaussian Mixture Model
- **Arxiv ID**: http://arxiv.org/abs/1612.08792v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.08792v2)
- **Published**: 2016-12-28 02:58:41+00:00
- **Updated**: 2017-02-21 02:28:15+00:00
- **Authors**: Zhihua Ban, Jianguo Liu, Li Cao
- **Comment**: None
- **Journal**: None
- **Summary**: Superpixel segmentation algorithms are to partition an image into perceptually coherence atomic regions by assigning every pixel a superpixel label. Those algorithms have been wildly used as a preprocessing step in computer vision works, as they can enormously reduce the number of entries of subsequent algorithms. In this work, we propose an alternative superpixel segmentation method based on Gaussian mixture model (GMM) by assuming that each superpixel corresponds to a Gaussian distribution, and assuming that each pixel is generated by first randomly choosing one distribution from several Gaussian distributions which are defined to be related to that pixel, and then the pixel is drawn from the selected distribution. Based on this assumption, each pixel is supposed to be drawn from a mixture of Gaussian distributions with unknown parameters (GMM). An algorithm based on expectation-maximization method is applied to estimate the unknown parameters. Once the unknown parameters are obtained, the superpixel label of a pixel is determined by a posterior probability. The success of applying GMM to superpixel segmentation depends on the two major differences between the traditional GMM-based clustering and the proposed one: data points in our model may be non-identically distributed, and we present an approach to control the shape of the estimated Gaussian functions by adjusting their covariance matrices. Our method is of linear complexity with respect to the number of pixels. The proposed algorithm is inherently parallel and can get faster speed by adding simple OpenMP directives to our implementation. According to our experiments, our algorithm outperforms the state-of-the-art superpixel algorithms in accuracy and presents a competitive performance in computational efficiency.



### Symbolic Representation and Classification of Logos
- **Arxiv ID**: http://arxiv.org/abs/1612.08796v1
- **DOI**: 10.1007/978-981-10-2104-6_50
- **Categories**: **cs.CV**, 68U10, I.4.9; I.4.9; I.5.0; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1612.08796v1)
- **Published**: 2016-12-28 04:06:10+00:00
- **Updated**: 2016-12-28 04:06:10+00:00
- **Authors**: D. S. Guru, N. Vinay Kumar
- **Comment**: 15 pages, 6 figures, 6 tables, Proceedings of International
  Conference on Computer Vision and Image Processing (CVIP 2016). arXiv admin
  note: text overlap with arXiv:1609.01414
- **Journal**: None
- **Summary**: In this paper, a model for classification of logos based on symbolic representation of features is presented. The proposed model makes use of global features of logo images such as color, texture, and shape features for classification. The logo images are broadly classified into three different classes, viz., logo image containing only text, an image with only symbol, and an image with both text and a symbol. In each class, the similar looking logo images are clustered using K-means clustering algorithm. The intra-cluster variations present in each cluster corresponding to each class are then preserved using symbolic interval data. Thus referenced logo images are represented in the form of interval data. A sample logo image is then classified using suitable symbolic classifier. For experimentation purpose, relatively large amount of color logo images is created consisting of 5044 logo images. The classification results are validated with the help of accuracy, precision, recall, F-measure, and time. To check the efficacy of the proposed model, the comparative analyses are given against the other models. The results show that the proposed model outperforms the other models with respect to time and F-measure.



### Multivariate mixture model for myocardium segmentation combining multi-source images
- **Arxiv ID**: http://arxiv.org/abs/1612.08820v1
- **DOI**: 10.1109/TPAMI.2018.2869576
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.08820v1)
- **Published**: 2016-12-28 07:44:17+00:00
- **Updated**: 2016-12-28 07:44:17+00:00
- **Authors**: Xiahai Zhuang
- **Comment**: None
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  2019
- **Summary**: This paper proposes a method for simultaneous segmentation of multi-source images, using the multivariate mixture model (MvMM) and maximum of log-likelihood (LL) framework. The segmentation is a procedure of texture classification, and the MvMM is used to model the joint intensity distribution of the images. Specifically, the method is applied to the myocardial segmentation combining the complementary texture information from multi-sequence (MS) cardiac magnetic resonance (CMR) images. Furthermore, there exist inter-image mis-registration and intra-image misalignment of slices in the MS CMR images. Hence, the MvMM is formulated with transformations, which are embedded into the LL framework and optimized simultaneously with the segmentation parameters. The proposed method is able to correct the inter- and intra-image misalignment by registering each slice of the MS CMR to a virtual common space, as well as to delineate the indistinguishable boundaries of myocardium consisting of pathologies. Results have shown statistically significant improvement in the segmentation performance of the proposed method with respect to the conventional approaches which can solely segment each image separately. The proposed method has also demonstrated better robustness in the incongruent data, where some images may not fully cover the region of interest and the full coverage can only be reconstructed combining the images from multiple sources.



### Accelerated Convolutions for Efficient Multi-Scale Time to Contact Computation in Julia
- **Arxiv ID**: http://arxiv.org/abs/1612.08825v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/1612.08825v1)
- **Published**: 2016-12-28 08:46:21+00:00
- **Updated**: 2016-12-28 08:46:21+00:00
- **Authors**: Alexander Amini, Berthold Horn, Alan Edelman
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutions have long been regarded as fundamental to applied mathematics, physics and engineering. Their mathematical elegance allows for common tasks such as numerical differentiation to be computed efficiently on large data sets. Efficient computation of convolutions is critical to artificial intelligence in real-time applications, like machine vision, where convolutions must be continuously and efficiently computed on tens to hundreds of kilobytes per second. In this paper, we explore how convolutions are used in fundamental machine vision applications. We present an accelerated n-dimensional convolution package in the high performance computing language, Julia, and demonstrate its efficacy in solving the time to contact problem for machine vision. Results are measured against synthetically generated videos and quantitatively assessed according to their mean squared error from the ground truth. We achieve over an order of magnitude decrease in compute time and allocated memory for comparable machine vision applications. All code is packaged and integrated into the official Julia Package Manager to be used in various other scenarios.



### FastMask: Segment Multi-scale Object Candidates in One Shot
- **Arxiv ID**: http://arxiv.org/abs/1612.08843v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1612.08843v4)
- **Published**: 2016-12-28 10:24:42+00:00
- **Updated**: 2017-04-11 21:20:57+00:00
- **Authors**: Hexiang Hu, Shiyi Lan, Yuning Jiang, Zhimin Cao, Fei Sha
- **Comment**: Accepted as CVPR 2017
- **Journal**: None
- **Summary**: Objects appear to scale differently in natural images. This fact requires methods dealing with object-centric tasks (e.g. object proposal) to have robust performance over variances in object scales. In the paper, we present a novel segment proposal framework, namely FastMask, which takes advantage of hierarchical features in deep convolutional neural networks to segment multi-scale objects in one shot. Innovatively, we adapt segment proposal network into three different functional components (body, neck and head). We further propose a weight-shared residual neck module as well as a scale-tolerant attentional head module for efficient one-shot inference. On MS COCO benchmark, the proposed FastMask outperforms all state-of-the-art segment proposal methods in average recall being 2~5 times faster. Moreover, with a slight trade-off in accuracy, FastMask can segment objects in near real time (~13 fps) with 800*600 resolution images, demonstrating its potential in practical applications. Our implementation is available on https://github.com/voidrank/FastMask.



### Semantic Video Segmentation by Gated Recurrent Flow Propagation
- **Arxiv ID**: http://arxiv.org/abs/1612.08871v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.08871v2)
- **Published**: 2016-12-28 12:50:39+00:00
- **Updated**: 2017-10-02 07:52:42+00:00
- **Authors**: David Nilsson, Cristian Sminchisescu
- **Comment**: The experiments section is extended compared to the previous version
- **Journal**: None
- **Summary**: Semantic video segmentation is challenging due to the sheer amount of data that needs to be processed and labeled in order to construct accurate models. In this paper we present a deep, end-to-end trainable methodology to video segmentation that is capable of leveraging information present in unlabeled data in order to improve semantic estimates. Our model combines a convolutional architecture and a spatio-temporal transformer recurrent layer that are able to temporally propagate labeling information by means of optical flow, adaptively gated based on its locally estimated uncertainty. The flow, the recognition and the gated temporal propagation modules can be trained jointly, end-to-end. The temporal, gated recurrent flow propagation component of our model can be plugged into any static semantic segmentation architecture and turn it into a weakly supervised video processing one. Our extensive experiments in the challenging CityScapes and Camvid datasets, and based on multiple deep architectures, indicate that the resulting model can leverage unlabeled temporal frames, next to a labeled one, in order to improve both the video segmentation accuracy and the consistency of its temporal labeling, at no additional annotation cost and with little extra computation.



### MARTA GANs: Unsupervised Representation Learning for Remote Sensing Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1612.08879v3
- **DOI**: 10.1109/LGRS.2017.2752750
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.08879v3)
- **Published**: 2016-12-28 13:24:10+00:00
- **Updated**: 2017-11-21 07:15:41+00:00
- **Authors**: Daoyu Lin, Kun Fu, Yang Wang, Guangluan Xu, Xian Sun
- **Comment**: IEEE GRSL
- **Journal**: IEEE Geoscience and Remote Sensing Letters ( Volume: 14, Issue:
  11, Nov. 2017 )
- **Summary**: With the development of deep learning, supervised learning has frequently been adopted to classify remotely sensed images using convolutional networks (CNNs). However, due to the limited amount of labeled data available, supervised learning is often difficult to carry out. Therefore, we proposed an unsupervised model called multiple-layer feature-matching generative adversarial networks (MARTA GANs) to learn a representation using only unlabeled data. MARTA GANs consists of both a generative model $G$ and a discriminative model $D$. We treat $D$ as a feature extractor. To fit the complex properties of remote sensing data, we use a fusion layer to merge the mid-level and global features. $G$ can produce numerous images that are similar to the training data; therefore, $D$ can learn better representations of remotely sensed images using the training data provided by $G$. The classification results on two widely used remote sensing image databases show that the proposed method significantly improves the classification performance compared with other state-of-the-art methods.



### Unsupervised domain adaptation in brain lesion segmentation with adversarial networks
- **Arxiv ID**: http://arxiv.org/abs/1612.08894v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.08894v1)
- **Published**: 2016-12-28 14:23:34+00:00
- **Updated**: 2016-12-28 14:23:34+00:00
- **Authors**: Konstantinos Kamnitsas, Christian Baumgartner, Christian Ledig, Virginia F. J. Newcombe, Joanna P. Simpson, Andrew D. Kane, David K. Menon, Aditya Nori, Antonio Criminisi, Daniel Rueckert, Ben Glocker
- **Comment**: None
- **Journal**: None
- **Summary**: Significant advances have been made towards building accurate automatic segmentation systems for a variety of biomedical applications using machine learning. However, the performance of these systems often degrades when they are applied on new data that differ from the training data, for example, due to variations in imaging protocols. Manually annotating new data for each test domain is not a feasible solution. In this work we investigate unsupervised domain adaptation using adversarial neural networks to train a segmentation method which is more invariant to differences in the input data, and which does not require any annotations on the test domain. Specifically, we learn domain-invariant features by learning to counter an adversarial network, which attempts to classify the domain of the input data by observing the activations of the segmentation network. Furthermore, we propose a multi-connected domain discriminator for improved adversarial training. Our system is evaluated using two MR databases of subjects with traumatic brain injuries, acquired using different scanners and imaging protocols. Using our unsupervised approach, we obtain segmentation accuracies which are close to the upper bound of supervised domain adaptation.



### Fast color transfer from multiple images
- **Arxiv ID**: http://arxiv.org/abs/1612.08927v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1612.08927v1)
- **Published**: 2016-12-28 16:50:55+00:00
- **Updated**: 2016-12-28 16:50:55+00:00
- **Authors**: Asad Khan, Luo Jiang, Wei Li, Ligang Liu
- **Comment**: arXiv admin note: text overlap with arXiv:1610.04861
- **Journal**: None
- **Summary**: Color transfer between images uses the statistics information of image effectively. We present a novel approach of local color transfer between images based on the simple statistics and locally linear embedding. A sketching interface is proposed for quickly and easily specifying the color correspondences between target and source image. The user can specify the correspondences of local region using scribes, which more accurately transfers the target color to the source image while smoothly preserving the boundaries, and exhibits more natural output results. Our algorithm is not restricted to one-to-one image color transfer and can make use of more than one target images to transfer the color in different regions in the source image. Moreover, our algorithm does not require to choose the same color style and image size between source and target images. We propose the sub-sampling to reduce the computational load. Comparing with other approaches, our algorithm is much better in color blending in the input data. Our approach preserves the other color details in the source image. Various experimental results show that our approach specifies the correspondences of local color region in source and target images. And it expresses the intention of users and generates more actual and natural results of visual effect.



### Partial Membership Latent Dirichlet Allocation
- **Arxiv ID**: http://arxiv.org/abs/1612.08936v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1612.08936v1)
- **Published**: 2016-12-28 17:32:52+00:00
- **Updated**: 2016-12-28 17:32:52+00:00
- **Authors**: Chao Chen, Alina Zare, Huy Trinh, Gbeng Omotara, J. Tory Cobb, Timotius Lagaunne
- **Comment**: Version 1, Sent for Review. arXiv admin note: substantial text
  overlap with arXiv:1511.02821
- **Journal**: None
- **Summary**: Topic models (e.g., pLSA, LDA, sLDA) have been widely used for segmenting imagery. However, these models are confined to crisp segmentation, forcing a visual word (i.e., an image patch) to belong to one and only one topic. Yet, there are many images in which some regions cannot be assigned a crisp categorical label (e.g., transition regions between a foggy sky and the ground or between sand and water at a beach). In these cases, a visual word is best represented with partial memberships across multiple topics. To address this, we present a partial membership latent Dirichlet allocation (PM-LDA) model and an associated parameter estimation algorithm. This model can be useful for imagery where a visual word may be a mixture of multiple topics. Experimental results on visual and sonar imagery show that PM-LDA can produce both crisp and soft semantic image segmentations; a capability previous topic modeling methods do not have.



