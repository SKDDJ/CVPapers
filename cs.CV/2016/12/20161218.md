# Arxiv Papers in cs.CV on 2016-12-18
### Joint Spatial-Angular Sparse Coding for dMRI with Separable Dictionaries
- **Arxiv ID**: http://arxiv.org/abs/1612.05846v3
- **DOI**: 10.1016/j.media.2018.05.002.
- **Categories**: **stat.ML**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1612.05846v3)
- **Published**: 2016-12-18 02:08:42+00:00
- **Updated**: 2018-05-29 15:05:26+00:00
- **Authors**: Evan Schwab, Ren√© Vidal, Nicolas Charon
- **Comment**: None
- **Journal**: Evan Schwab, Rene Vidal, Nicolas Charon, Joint spatial-angular
  sparse coding for dMRI with separable dictionaries, Medical Image Analysis,
  Volume 48, August 2018, Pages 25-42, ISSN 1361-8415
- **Summary**: Diffusion MRI (dMRI) provides the ability to reconstruct neuronal fibers in the brain, $\textit{in vivo}$, by measuring water diffusion along angular gradient directions in q-space. High angular resolution diffusion imaging (HARDI) can produce better estimates of fiber orientation than the popularly used diffusion tensor imaging, but the high number of samples needed to estimate diffusivity requires longer patient scan times. To accelerate dMRI, compressed sensing (CS) has been utilized by exploiting a sparse dictionary representation of the data, discovered through sparse coding. The sparser the representation, the fewer samples are needed to reconstruct a high resolution signal with limited information loss, and so an important area of research has focused on finding the sparsest possible representation of dMRI. Current reconstruction methods however, rely on an angular representation $\textit{per voxel}$ with added spatial regularization, and so, for non-zero signals, one is required to have at least one non-zero coefficient per voxel. This means that the global level of sparsity must be greater than the number of voxels. In contrast, we propose a joint spatial-angular representation of dMRI that will allow us to achieve levels of global sparsity that are below the number of voxels. A major challenge, however, is the computational complexity of solving a global sparse coding problem over large-scale dMRI. In this work, we present novel adaptations of popular sparse coding algorithms that become better suited for solving large-scale problems by exploiting spatial-angular separability. Our experiments show that our method achieves significantly sparser representations of HARDI than is possible by the state of the art.



### 3D Shape Induction from 2D Views of Multiple Objects
- **Arxiv ID**: http://arxiv.org/abs/1612.05872v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.05872v1)
- **Published**: 2016-12-18 08:44:00+00:00
- **Updated**: 2016-12-18 08:44:00+00:00
- **Authors**: Matheus Gadelha, Subhransu Maji, Rui Wang
- **Comment**: Submitted to CVPR 2017
- **Journal**: None
- **Summary**: In this paper we investigate the problem of inducing a distribution over three-dimensional structures given two-dimensional views of multiple objects taken from unknown viewpoints. Our approach called "projective generative adversarial networks" (PrGANs) trains a deep generative model of 3D shapes whose projections match the distributions of the input 2D views. The addition of a projection module allows us to infer the underlying 3D shape distribution without using any 3D, viewpoint information, or annotation during the learning phase. We show that our approach produces 3D shapes of comparable quality to GANs trained on 3D data for a number of shape categories including chairs, airplanes, and cars. Experiments also show that the disentangled representation of 2D shapes into geometry and viewpoint leads to a good generative model of 2D shapes. The key advantage is that our model allows us to predict 3D, viewpoint, and generate novel views from an input image in a completely unsupervised manner.



### Deep Learning on Lie Groups for Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1612.05877v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.05877v2)
- **Published**: 2016-12-18 09:08:29+00:00
- **Updated**: 2017-04-11 08:47:00+00:00
- **Authors**: Zhiwu Huang, Chengde Wan, Thomas Probst, Luc Van Gool
- **Comment**: Accepted to CVPR 2017
- **Journal**: None
- **Summary**: In recent years, skeleton-based action recognition has become a popular 3D classification problem. State-of-the-art methods typically first represent each motion sequence as a high-dimensional trajectory on a Lie group with an additional dynamic time warping, and then shallowly learn favorable Lie group features. In this paper we incorporate the Lie group structure into a deep network architecture to learn more appropriate Lie group features for 3D action recognition. Within the network structure, we design rotation mapping layers to transform the input Lie group features into desirable ones, which are aligned better in the temporal domain. To reduce the high feature dimensionality, the architecture is equipped with rotation pooling layers for the elements on the Lie group. Furthermore, we propose a logarithm mapping layer to map the resulting manifold data into a tangent space that facilitates the application of regular output layers for the final classification. Evaluations of the proposed network for standard 3D human action recognition datasets clearly demonstrate its superiority over existing shallow Lie group feature learning methods as well as most conventional deep learning methods.



### Learning a No-Reference Quality Metric for Single-Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1612.05890v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.05890v1)
- **Published**: 2016-12-18 10:30:58+00:00
- **Updated**: 2016-12-18 10:30:58+00:00
- **Authors**: Chao Ma, Chih-Yuan Yang, Xiaokang Yang, Ming-Hsuan Yang
- **Comment**: Accepted by Computer Vision and Image Understanding
- **Journal**: None
- **Summary**: Numerous single-image super-resolution algorithms have been proposed in the literature, but few studies address the problem of performance evaluation based on visual perception. While most super-resolution images are evaluated by fullreference metrics, the effectiveness is not clear and the required ground-truth images are not always available in practice. To address these problems, we conduct human subject studies using a large set of super-resolution images and propose a no-reference metric learned from visual perceptual scores. Specifically, we design three types of low-level statistical features in both spatial and frequency domains to quantify super-resolved artifacts, and learn a two-stage regression model to predict the quality scores of super-resolution images without referring to ground-truth images. Extensive experimental results show that the proposed metric is effective and efficient to assess the quality of super-resolution images based on human perception.



### Deep Multi-instance Networks with Sparse Label Assignment for Whole Mammogram Classification
- **Arxiv ID**: http://arxiv.org/abs/1612.05968v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.05968v1)
- **Published**: 2016-12-18 18:31:11+00:00
- **Updated**: 2016-12-18 18:31:11+00:00
- **Authors**: Wentao Zhu, Qi Lou, Yeeleng Scott Vang, Xiaohui Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Mammogram classification is directly related to computer-aided diagnosis of breast cancer. Traditional methods requires great effort to annotate the training data by costly manual labeling and specialized computational models to detect these annotations during test. Inspired by the success of using deep convolutional features for natural image analysis and multi-instance learning for labeling a set of instances/patches, we propose end-to-end trained deep multi-instance networks for mass classification based on whole mammogram without the aforementioned costly need to annotate the training data. We explore three different schemes to construct deep multi-instance networks for whole mammogram classification. Experimental results on the INbreast dataset demonstrate the robustness of proposed deep networks compared to previous work using segmentation and detection annotations in the training.



### Adversarial Deep Structural Networks for Mammographic Mass Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1612.05970v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.05970v2)
- **Published**: 2016-12-18 18:40:21+00:00
- **Updated**: 2017-06-09 21:32:38+00:00
- **Authors**: Wentao Zhu, Xiang Xiang, Trac D. Tran, Xiaohui Xie
- **Comment**: First version on arXiv 2016, MICCAI 2017 Deep Learning in Medical
  Image Analysis (DLMIA) workshop
- **Journal**: None
- **Summary**: Mass segmentation is an important task in mammogram analysis, providing effective morphological features and regions of interest (ROI) for mass detection and classification. Inspired by the success of using deep convolutional features for natural image analysis and conditional random fields (CRF) for structural learning, we propose an end-to-end network for mammographic mass segmentation. The network employs a fully convolutional network (FCN) to model potential function, followed by a CRF to perform structural learning. Because the mass distribution varies greatly with pixel position, the FCN is combined with position priori for the task. Due to the small size of mammogram datasets, we use adversarial training to control over-fitting. Four models with different convolutional kernels are further fused to improve the segmentation results. Experimental results on two public datasets, INbreast and DDSM-BCRP, show that our end-to-end network combined with adversarial training achieves the-state-of-the-art results.



