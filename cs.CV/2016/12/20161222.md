# Arxiv Papers in cs.CV on 2016-12-22
### Efficient Action Detection in Untrimmed Videos via Multi-Task Learning
- **Arxiv ID**: http://arxiv.org/abs/1612.07403v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1612.07403v2)
- **Published**: 2016-12-22 00:37:42+00:00
- **Updated**: 2017-04-04 17:49:19+00:00
- **Authors**: Yi Zhu, Shawn Newsam
- **Comment**: WACV 2017 camera ready, minor updates about test time efficiency
- **Journal**: None
- **Summary**: This paper studies the joint learning of action recognition and temporal localization in long, untrimmed videos. We employ a multi-task learning framework that performs the three highly related steps of action proposal, action recognition, and action localization refinement in parallel instead of the standard sequential pipeline that performs the steps in order. We develop a novel temporal actionness regression module that estimates what proportion of a clip contains action. We use it for temporal localization but it could have other applications like video retrieval, surveillance, summarization, etc. We also introduce random shear augmentation during training to simulate viewpoint change. We evaluate our framework on three popular video benchmarks. Results demonstrate that our joint model is efficient in terms of storage and computation in that we do not need to compute and cache dense trajectory features, and that it is several times faster than its sequential ConvNets counterpart. Yet, despite being more efficient, it outperforms state-of-the-art methods with respect to accuracy.



### Probabilistic graphical model based approach for water mapping using GaoFen-2 (GF-2) high resolution imagery and Landsat 8 time series
- **Arxiv ID**: http://arxiv.org/abs/1612.07801v1
- **DOI**: None
- **Categories**: **stat.AP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1612.07801v1)
- **Published**: 2016-12-22 02:59:54+00:00
- **Updated**: 2016-12-22 02:59:54+00:00
- **Authors**: Luyan Ji, Jie Wang, Xiurui Geng, Peng Gong
- **Comment**: 17 pages, 9 figures, 6 tables
- **Journal**: None
- **Summary**: The objective of this paper is to evaluate the potential of Gaofen-2 (GF-2) high resolution multispectral sensor (MS) and panchromatic (PAN) imagery on water mapping. Difficulties of water mapping on high resolution data includes: 1) misclassification between water and shadows or other low-reflectance ground objects, which is mostly caused by the spectral similarity within the given band range; 2) small water bodies with size smaller than the spatial resolution of MS image. To solve the confusion between water and low-reflectance objects, the Landsat 8 time series with two shortwave infrared (SWIR) bands is added because water has extremely strong absorption in SWIR. In order to integrate the three multi-sensor, multi-resolution data sets, the probabilistic graphical model (PGM) is utilized here with conditional probability distribution defined mainly based on the size of each object. For comparison, results from the SVM classifier on the PCA fused and MS data, thresholding method on the PAN image, and water index method on the Landsat data are computed. The confusion matrices are calculated for all the methods. The results demonstrate that the PGM method can achieve the best performance with the highest overall accuracy. Moreover, small rivers can also be extracted by adding weight on the PAN result in PGM. Finally, the post-classification procedure is applied on the PGM result to further exclude misclassification in shadow and water-land boundary regions. Accordingly, the producer's, user's and overall accuracy are all increased, indicating the effectiveness of our method.



### Physically-Based Rendering for Indoor Scene Understanding Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.07429v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.07429v3)
- **Published**: 2016-12-22 03:18:12+00:00
- **Updated**: 2017-07-02 01:25:36+00:00
- **Authors**: Yinda Zhang, Shuran Song, Ersin Yumer, Manolis Savva, Joon-Young Lee, Hailin Jin, Thomas Funkhouser
- **Comment**: Updates camera ready version. Accepted by CVPR 2017
- **Journal**: None
- **Summary**: Indoor scene understanding is central to applications such as robot navigation and human companion assistance. Over the last years, data-driven deep neural networks have outperformed many traditional approaches thanks to their representation learning capabilities. One of the bottlenecks in training for better representations is the amount of available per-pixel ground truth data that is required for core scene understanding tasks such as semantic segmentation, normal prediction, and object edge detection. To address this problem, a number of works proposed using synthetic data. However, a systematic study of how such synthetic data is generated is missing. In this work, we introduce a large-scale synthetic dataset with 400K physically-based rendered images from 45K realistic 3D indoor scenes. We study the effects of rendering methods and scene lighting on training for three computer vision tasks: surface normal prediction, semantic segmentation, and object boundary detection. This study provides insights into the best practices for training with synthetic data (more realistic rendering is worth it) and shows that pretraining with our new synthetic dataset can improve results beyond the current state of the art on all three tasks.



### Deep Blind Compressed Sensing
- **Arxiv ID**: http://arxiv.org/abs/1612.07453v1
- **DOI**: None
- **Categories**: **cs.CV**, G.1.6; I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/1612.07453v1)
- **Published**: 2016-12-22 06:12:43+00:00
- **Updated**: 2016-12-22 06:12:43+00:00
- **Authors**: Shikha Singh, Vanika Singhal, Angshul Majumdar
- **Comment**: DCC 2017 Poster
- **Journal**: None
- **Summary**: This work addresses the problem of extracting deeply learned features directly from compressive measurements. There has been no work in this area. Existing deep learning tools only give good results when applied on the full signal, that too usually after preprocessing. These techniques require the signal to be reconstructed first. In this work we show that by learning directly from the compressed domain, considerably better results can be obtained. This work extends the recently proposed framework of deep matrix factorization in combination with blind compressed sensing; hence the term deep blind compressed sensing. Simulation experiments have been carried out on imaging via single pixel camera, under-sampled biomedical signals, arising in wireless body area network and compressive hyperspectral imaging. In all cases, the superiority of our proposed deep blind compressed sensing can be envisaged.



### Handwriting recognition using Cohort of LSTM and lexicon verification with extremely large lexicon
- **Arxiv ID**: http://arxiv.org/abs/1612.07528v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.07528v4)
- **Published**: 2016-12-22 10:21:09+00:00
- **Updated**: 2017-09-25 08:57:50+00:00
- **Authors**: Bruno Stuner, Cl√©ment Chatelain, Thierry Paquet
- **Comment**: 31 pages, paper submitted to Pattern Recognition
- **Journal**: None
- **Summary**: State-of-the-art methods for handwriting recognition are based on Long Short Term Memory (LSTM) recurrent neural networks (RNN), which now provides very impressive character recognition performance. The character recognition is generally coupled with a lexicon driven decoding process which integrates dictionaries. Unfortunately these dictionaries are limited to hundred of thousands words for the best systems, which prevent from having a good language coverage, and therefore limit the global recognition performance. In this article, we propose an alternative to the lexicon driven decoding process based on a lexicon verification process, coupled with an original cascade architecture. The cascade is made of a large number of complementary networks extracted from a single training (called cohort), making the learning process very light. The proposed method achieves new state-of-the art word recognition performance on the Rimes and IAM databases. Dealing with gigantic lexicon of 3 millions words, the methods also demonstrates interesting performance with a fast decision stage.



### A Revisit of Hashing Algorithms for Approximate Nearest Neighbor Search
- **Arxiv ID**: http://arxiv.org/abs/1612.07545v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.07545v6)
- **Published**: 2016-12-22 11:17:55+00:00
- **Updated**: 2019-06-19 01:39:13+00:00
- **Authors**: Deng Cai
- **Comment**: None
- **Journal**: None
- **Summary**: Approximate Nearest Neighbor Search (ANNS) is a fundamental problem in many areas of machine learning and data mining. During the past decade, numerous hashing algorithms are proposed to solve this problem. Every proposed algorithm claims outperform other state-of-the-art hashing methods. However, the evaluation of these hashing papers was not thorough enough, and those claims should be re-examined. The ultimate goal of an ANNS method is returning the most accurate answers (nearest neighbors) in the shortest time. If implemented correctly, almost all the hashing methods will have their performance improved as the code length increases. However, many existing hashing papers only report the performance with the code length shorter than 128. In this paper, we carefully revisit the problem of search with a hash index, and analyze the pros and cons of two popular hash index search procedures. Then we proposed a very simple but effective two level index structures and make a thorough comparison of eleven popular hashing algorithms. Surprisingly, the random-projection-based Locality Sensitive Hashing (LSH) is the best performed algorithm, which is in contradiction to the claims in all the other ten hashing papers. Despite the extreme simplicity of random-projection-based LSH, our results show that the capability of this algorithm has been far underestimated. For the sake of reproducibility, all the codes used in the paper are released on GitHub, which can be used as a testing platform for a fair comparison between various hashing algorithms.



### Re-evaluating Automatic Metrics for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1612.07600v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1612.07600v1)
- **Published**: 2016-12-22 14:00:28+00:00
- **Updated**: 2016-12-22 14:00:28+00:00
- **Authors**: Mert Kilickaya, Aykut Erdem, Nazli Ikizler-Cinbis, Erkut Erdem
- **Comment**: None
- **Journal**: None
- **Summary**: The task of generating natural language descriptions from images has received a lot of attention in recent years. Consequently, it is becoming increasingly important to evaluate such image captioning approaches in an automatic manner. In this paper, we provide an in-depth evaluation of the existing image captioning metrics through a series of carefully designed experiments. Moreover, we explore the utilization of the recently proposed Word Mover's Distance (WMD) document metric for the purpose of image captioning. Our findings outline the differences and/or similarities between metrics and their relative robustness by means of extensive correlation, accuracy and distraction based evaluations. Our results also demonstrate that WMD provides strong advantages over other metrics.



### Hardware for Machine Learning: Challenges and Opportunities
- **Arxiv ID**: http://arxiv.org/abs/1612.07625v5
- **DOI**: 10.1109/CICC.2017.7993626
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.07625v5)
- **Published**: 2016-12-22 14:50:40+00:00
- **Updated**: 2017-10-17 02:50:38+00:00
- **Authors**: Vivienne Sze, Yu-Hsin Chen, Joel Emer, Amr Suleiman, Zhengdong Zhang
- **Comment**: Published as an invited conference paper at CICC 2017
- **Journal**: None
- **Summary**: Machine learning plays a critical role in extracting meaningful information out of the zetabytes of sensor data collected every day. For some applications, the goal is to analyze and understand the data to identify trends (e.g., surveillance, portable/wearable electronics); in other applications, the goal is to take immediate action based the data (e.g., robotics/drones, self-driving cars, smart Internet of Things). For many of these applications, local embedded processing near the sensor is preferred over the cloud due to privacy or latency concerns, or limitations in the communication bandwidth. However, at the sensor there are often stringent constraints on energy consumption and cost in addition to throughput and accuracy requirements. Furthermore, flexibility is often required such that the processing can be adapted for different applications or environments (e.g., update the weights and model in the classifier). In many applications, machine learning often involves transforming the input data into a higher dimensional space, which, along with programmable weights, increases data movement and consequently energy consumption. In this paper, we will discuss how these challenges can be addressed at various levels of hardware design ranging from architecture, hardware-friendly algorithms, mixed-signal circuits, and advanced technologies (including memories and sensors).



### MultiNet: Real-time Joint Semantic Reasoning for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1612.07695v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1612.07695v2)
- **Published**: 2016-12-22 16:55:02+00:00
- **Updated**: 2018-05-08 18:36:33+00:00
- **Authors**: Marvin Teichmann, Michael Weber, Marius Zoellner, Roberto Cipolla, Raquel Urtasun
- **Comment**: 9 pages, 7 tables and 9 figures; first place on Kitti Road
  Segmentation; Code on GitHub (https://github.com/MarvinTeichmann/MultiNet)
- **Journal**: None
- **Summary**: While most approaches to semantic reasoning have focused on improving performance, in this paper we argue that computational times are very important in order to enable real time applications such as autonomous driving. Towards this goal, we present an approach to joint classification, detection and semantic segmentation via a unified architecture where the encoder is shared amongst the three tasks. Our approach is very simple, can be trained end-to-end and performs extremely well in the challenging KITTI dataset, outperforming the state-of-the-art in the road segmentation task. Our approach is also very efficient, taking less than 100 ms to perform all tasks.



### Set2Model Networks: Learning Discriminatively To Learn Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1612.07697v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.07697v2)
- **Published**: 2016-12-22 17:01:01+00:00
- **Updated**: 2017-10-27 12:29:53+00:00
- **Authors**: A. Vakhitov, A. Kuzmin, V. Lempitsky
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new "learning-to-learn"-type approach that enables rapid learning of concepts from small-to-medium sized training sets and is primarily designed for web-initialized image retrieval. At the core of our approach is a deep architecture (a Set2Model network) that maps sets of examples to simple generative probabilistic models such as Gaussians or mixtures of Gaussians in the space of high-dimensional descriptors. The parameters of the embedding into the descriptor space are trained in the end-to-end fashion in the meta-learning stage using a set of training learning problems. The main technical novelty of our approach is the derivation of the backprop process through the mixture model fitting, which makes the likelihood of the resulting models differentiable with respect to the positions of the input descriptors.   While the meta-learning process for a Set2Model network is discriminative, a trained Set2Model network performs generative learning of generative models in the descriptor space, which facilitates learning in the cases when no negative examples are available, and whenever the concept being learned is polysemous or represented by noisy training sets. Among other experiments, we demonstrate that these properties allow Set2Model networks to pick visual concepts from the raw outputs of Internet image search engines better than a set of strong baselines.



### Adversarial Examples Detection in Deep Networks with Convolutional Filter Statistics
- **Arxiv ID**: http://arxiv.org/abs/1612.07767v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.07767v2)
- **Published**: 2016-12-22 19:45:31+00:00
- **Updated**: 2017-10-26 18:42:57+00:00
- **Authors**: Xin Li, Fuxin Li
- **Comment**: Published in ICCV 2017
- **Journal**: None
- **Summary**: Deep learning has greatly improved visual recognition in recent years. However, recent research has shown that there exist many adversarial examples that can negatively impact the performance of such an architecture. This paper focuses on detecting those adversarial examples by analyzing whether they come from the same distribution as the normal examples. Instead of directly training a deep neural network to detect adversarials, a much simpler approach was proposed based on statistics on outputs from convolutional layers. A cascade classifier was designed to efficiently detect adversarials. Furthermore, trained from one particular adversarial generating mechanism, the resulting classifier can successfully detect adversarials from a completely different mechanism as well. The resulting classifier is non-subdifferentiable, hence creates a difficulty for adversaries to attack by using the gradient of the classifier. After detecting adversarial examples, we show that many of them can be recovered by simply performing a small average filter on the image. Those findings should lead to more insights about the classification mechanisms in deep convolutional neural networks.



### First-Person Activity Forecasting with Online Inverse Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1612.07796v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.07796v3)
- **Published**: 2016-12-22 20:57:22+00:00
- **Updated**: 2017-08-06 22:06:18+00:00
- **Authors**: Nicholas Rhinehart, Kris M. Kitani
- **Comment**: To appear at ICCV 2017 (Oral)
- **Journal**: None
- **Summary**: We address the problem of incrementally modeling and forecasting long-term goals of a first-person camera wearer: what the user will do, where they will go, and what goal they seek. In contrast to prior work in trajectory forecasting, our algorithm, DARKO, goes further to reason about semantic states (will I pick up an object?), and future goal states that are far in terms of both space and time. DARKO learns and forecasts from first-person visual observations of the user's daily behaviors via an Online Inverse Reinforcement Learning (IRL) approach. Classical IRL discovers only the rewards in a batch setting, whereas DARKO discovers the states, transitions, rewards, and goals of a user from streaming data. Among other results, we show DARKO forecasts goals better than competing methods in both noisy and ideal settings, and our approach is theoretically and empirically no-regret.



### Learning from Simulated and Unsupervised Images through Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1612.07828v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1612.07828v2)
- **Published**: 2016-12-22 22:10:51+00:00
- **Updated**: 2017-07-19 21:24:52+00:00
- **Authors**: Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Josh Susskind, Wenda Wang, Russ Webb
- **Comment**: Accepted at CVPR 2017 for oral presentation
- **Journal**: None
- **Summary**: With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulator's output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a 'self-regularization' term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.



### Understanding Image and Text Simultaneously: a Dual Vision-Language Machine Comprehension Task
- **Arxiv ID**: http://arxiv.org/abs/1612.07833v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1612.07833v1)
- **Published**: 2016-12-22 22:44:17+00:00
- **Updated**: 2016-12-22 22:44:17+00:00
- **Authors**: Nan Ding, Sebastian Goodman, Fei Sha, Radu Soricut
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: We introduce a new multi-modal task for computer systems, posed as a combined vision-language comprehension challenge: identifying the most suitable text describing a scene, given several similar options. Accomplishing the task entails demonstrating comprehension beyond just recognizing "keywords" (or key-phrases) and their corresponding visual concepts. Instead, it requires an alignment between the representations of the two modalities that achieves a visually-grounded "understanding" of various linguistic elements and their dependencies. This new task also admits an easy-to-compute and well-studied metric: the accuracy in detecting the true target among the decoys.   The paper makes several contributions: an effective and extensible mechanism for generating decoys from (human-created) image captions; an instance of applying this mechanism, yielding a large-scale machine comprehension dataset (based on the COCO images and captions) that we make publicly available; human evaluation results on this dataset, informing a performance upper-bound; and several baseline and competitive learning approaches that illustrate the utility of the proposed task and dataset in advancing both image and language comprehension. We also show that, in a multi-task learning setting, the performance on the proposed task is positively correlated with the end-to-end task of image captioning.



