# Arxiv Papers in cs.CV on 2016-12-26
### Image-Text Multi-Modal Representation Learning by Adversarial Backpropagation
- **Arxiv ID**: http://arxiv.org/abs/1612.08354v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.08354v1)
- **Published**: 2016-12-26 09:51:18+00:00
- **Updated**: 2016-12-26 09:51:18+00:00
- **Authors**: Gwangbeen Park, Woobin Im
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: We present novel method for image-text multi-modal representation learning. In our knowledge, this work is the first approach of applying adversarial learning concept to multi-modal learning and not exploiting image-text pair information to learn multi-modal feature. We only use category information in contrast with most previous methods using image-text pair information for multi-modal embedding. In this paper, we show that multi-modal feature can be achieved without image-text pair information and our method makes more similar distribution with image and text in multi-modal feature space than other methods which use image-text pair information. And we show our multi-modal feature has universal semantic information, even though it was trained for category prediction. Our model is end-to-end backpropagation, intuitive and easily extended to other multi-modal learning work.



### Extracting Sub-Exposure Images from a Single Capture Through Fourier-based Optical Modulation
- **Arxiv ID**: http://arxiv.org/abs/1612.08359v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.08359v2)
- **Published**: 2016-12-26 10:30:05+00:00
- **Updated**: 2018-02-13 15:08:38+00:00
- **Authors**: Shah Rez Khan, Martin Feldman, Bahadir K. Gunturk
- **Comment**: None
- **Journal**: None
- **Summary**: Through pixel-wise optical coding of images during exposure time, it is possible to extract sub-exposure images from a single capture. Such a capability can be used for different purposes, including high-speed imaging, high-dynamic-range imaging and compressed sensing. In this paper, we demonstrate a sub-exposure image extraction method, where the exposure coding pattern is inspired from frequency division multiplexing idea of communication systems. The coding masks modulate sub-exposure images in such a way that they are placed in non-overlapping regions in Fourier domain. The sub-exposure image extraction process involves digital filtering of the captured signal with proper band-pass filters. The prototype imaging system incorporates a Liquid Crystal over Silicon (LCoS) based spatial light modulator synchronized with a camera for pixel-wise exposure coding.



### Signature of Geometric Centroids for 3D Local Shape Description and Partial Shape Matching
- **Arxiv ID**: http://arxiv.org/abs/1612.08408v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.08408v1)
- **Published**: 2016-12-26 15:44:17+00:00
- **Updated**: 2016-12-26 15:44:17+00:00
- **Authors**: Keke Tang, Peng Song, Xiaoping Chen
- **Comment**: to be published in the Proceedings of the Asian Conference on
  Computer Vision (ACCV'2016)
- **Journal**: None
- **Summary**: Depth scans acquired from different views may contain nuisances such as noise, occlusion, and varying point density. We propose a novel Signature of Geometric Centroids descriptor, supporting direct shape matching on the scans, without requiring any preprocessing such as scan denoising or converting into a mesh. First, we construct the descriptor by voxelizing the local shape within a uniquely defined local reference frame and concatenating geometric centroid and point density features extracted from each voxel. Second, we compare two descriptors by employing only corresponding voxels that are both non-empty, thus supporting matching incomplete local shape such as those close to scan boundary. Third, we propose a descriptor saliency measure and compute it from a descriptor-graph to improve shape matching performance. We demonstrate the descriptor's robustness and effectiveness for shape matching by comparing it with three state-of-the-art descriptors, and applying it to object/scene reconstruction and 3D object recognition.



