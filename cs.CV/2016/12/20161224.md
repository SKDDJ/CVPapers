# Arxiv Papers in cs.CV on 2016-12-24
### EgoReID: Cross-view Self-Identification and Human Re-identification in Egocentric and Surveillance Videos
- **Arxiv ID**: http://arxiv.org/abs/1612.08153v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/1612.08153v1)
- **Published**: 2016-12-24 09:00:37+00:00
- **Updated**: 2016-12-24 09:00:37+00:00
- **Authors**: Shervin Ardeshir, Sandesh Sharma, Ali Broji
- **Comment**: None
- **Journal**: None
- **Summary**: Human identification remains to be one of the challenging tasks in computer vision community due to drastic changes in visual features across different viewpoints, lighting conditions, occlusion, etc. Most of the literature has been focused on exploring human re-identification across viewpoints that are not too drastically different in nature. Cameras usually capture oblique or side views of humans, leaving room for a lot of geometric and visual reasoning. Given the recent popularity of egocentric and top-view vision, re-identification across these two drastically different views can now be explored. Having an egocentric and a top view video, our goal is to identify the cameraman in the content of the top-view video, and also re-identify the people visible in the egocentric video, by matching them to the identities present in the top-view video. We propose a CRF-based method to address the two problems. Our experimental results demonstrates the efficiency of the proposed approach over a variety of video recorded from two views.



### Unsupervised Video Segmentation via Spatio-Temporally Nonlocal Appearance Learning
- **Arxiv ID**: http://arxiv.org/abs/1612.08169v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.08169v1)
- **Published**: 2016-12-24 12:04:31+00:00
- **Updated**: 2016-12-24 12:04:31+00:00
- **Authors**: Kaihua Zhang, Xuejun Li, Qingshan Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Video object segmentation is challenging due to the factors like rapidly fast motion, cluttered backgrounds, arbitrary object appearance variation and shape deformation. Most existing methods only explore appearance information between two consecutive frames, which do not make full use of the usefully long-term nonlocal information that is helpful to make the learned appearance stable, and hence they tend to fail when the targets suffer from large viewpoint changes and significant non-rigid deformations. In this paper, we propose a simple yet effective approach to mine the long-term sptatio-temporally nonlocal appearance information for unsupervised video segmentation. The motivation of our algorithm comes from the spatio-temporal nonlocality of the region appearance reoccurrence in a video. Specifically, we first generate a set of superpixels to represent the foreground and background, and then update the appearance of each superpixel with its long-term sptatio-temporally nonlocal counterparts generated by the approximate nearest neighbor search method with the efficient KD-tree algorithm. Then, with the updated appearances, we formulate a spatio-temporal graphical model comprised of the superpixel label consistency potentials. Finally, we generate the segmentation by optimizing the graphical model via iteratively updating the appearance model and estimating the labels. Extensive evaluations on the SegTrack and Youtube-Objects datasets demonstrate the effectiveness of the proposed method, which performs favorably against some state-of-art methods.



### Joint denoising and distortion correction of atomic scale scanning transmission electron microscopy images
- **Arxiv ID**: http://arxiv.org/abs/1612.08170v1
- **DOI**: 10.1088/1361-6420/aa7b94
- **Categories**: **cs.CV**, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/1612.08170v1)
- **Published**: 2016-12-24 12:37:17+00:00
- **Updated**: 2016-12-24 12:37:17+00:00
- **Authors**: Benjamin Berkels, Benedikt Wirth
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, modern electron microscopes deliver images at atomic scale. The precise atomic structure encodes information about material properties. Thus, an important ingredient in the image analysis is to locate the centers of the atoms shown in micrographs as precisely as possible. Here, we consider scanning transmission electron microscopy (STEM), which acquires data in a rastering pattern, pixel by pixel. Due to this rastering combined with the magnification to atomic scale, movements of the specimen even at the nanometer scale lead to random image distortions that make precise atom localization difficult. Given a series of STEM images, we derive a Bayesian method that jointly estimates the distortion in each image and reconstructs the underlying atomic grid of the material by fitting the atom bumps with suitable bump functions. The resulting highly non-convex minimization problems are solved numerically with a trust region approach. Well-posedness of the reconstruction method and the model behavior for faster and faster rastering are investigated using variational techniques. The performance of the method is finally evaluated on both synthetic and real experimental data.



### PixelCNN Models with Auxiliary Variables for Natural Image Modeling
- **Arxiv ID**: http://arxiv.org/abs/1612.08185v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.08185v4)
- **Published**: 2016-12-24 14:20:05+00:00
- **Updated**: 2017-07-01 13:24:08+00:00
- **Authors**: Alexander Kolesnikov, Christoph H. Lampert
- **Comment**: ICML 2017
- **Journal**: None
- **Summary**: We study probabilistic models of natural images and extend the autoregressive family of PixelCNN architectures by incorporating auxiliary variables. Subsequently, we describe two new generative image models that exploit different image transformations as auxiliary variables: a quantized grayscale view of the image or a multi-resolution image pyramid. The proposed models tackle two known shortcomings of existing PixelCNN models: 1) their tendency to focus on low-level image details, while largely ignoring high-level image information, such as object shapes, and 2) their computationally costly procedure for image sampling. We experimentally demonstrate benefits of the proposed models, in particular showing that they produce much more realistically looking image samples than previous state-of-the-art probabilistic models.



