# Arxiv Papers in cs.CV on 2016-12-20
### Efficiently Computing Piecewise Flat Embeddings for Data Clustering and Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1612.06496v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06496v1)
- **Published**: 2016-12-20 03:06:58+00:00
- **Updated**: 2016-12-20 03:06:58+00:00
- **Authors**: Renee T. Meinhold, Tyler L. Hayes, Nathan D. Cahill
- **Comment**: Presented at the 2016 IEEE MIT Undergraduate Research Technology
  Conference (http://ieee.scripts.mit.edu/conference/index.php)
- **Journal**: None
- **Summary**: Image segmentation is a popular area of research in computer vision that has many applications in automated image processing. A recent technique called piecewise flat embeddings (PFE) has been proposed for use in image segmentation; PFE transforms image pixel data into a lower dimensional representation where similar pixels are pulled close together and dissimilar pixels are pushed apart. This technique has shown promising results, but its original formulation is not computationally feasible for large images. We propose two improvements to the algorithm for computing PFE: first, we reformulate portions of the algorithm to enable various linear algebra operations to be performed in parallel; second, we propose utilizing an iterative linear solver (preconditioned conjugate gradient) to quickly solve a linear least-squares problem that occurs in the inner loop of a nested iteration. With these two computational improvements, we show on a publicly available image database that PFE can be sped up by an order of magnitude without sacrificing segmentation performance. Our results make this technique more practical for use on large data sets, not only for image segmentation, but for general data clustering problems.



### Deeply Aggregated Alternating Minimization for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1612.06508v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06508v1)
- **Published**: 2016-12-20 04:56:56+00:00
- **Updated**: 2016-12-20 04:56:56+00:00
- **Authors**: Youngjung Kim, Hyungjoo Jung, Dongbo Min, Kwanghoon Sohn
- **Comment**: 9 PAGES
- **Journal**: None
- **Summary**: Regularization-based image restoration has remained an active research topic in computer vision and image processing. It often leverages a guidance signal captured in different fields as an additional cue. In this work, we present a general framework for image restoration, called deeply aggregated alternating minimization (DeepAM). We propose to train deep neural network to advance two of the steps in the conventional AM algorithm: proximal mapping and ?- continuation. Both steps are learned from a large dataset in an end-to-end manner. The proposed framework enables the convolutional neural networks (CNNs) to operate as a prior or regularizer in the AM algorithm. We show that our learned regularizer via deep aggregation outperforms the recent data-driven approaches as well as the nonlocalbased methods. The flexibility and effectiveness of our framework are demonstrated in several image restoration tasks, including single image denoising, RGB-NIR restoration, and depth super-resolution.



### Exploring the Design Space of Deep Convolutional Neural Networks at Large Scale
- **Arxiv ID**: http://arxiv.org/abs/1612.06519v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1612.06519v1)
- **Published**: 2016-12-20 06:20:43+00:00
- **Updated**: 2016-12-20 06:20:43+00:00
- **Authors**: Forrest Iandola
- **Comment**: thesis, UC Berkeley (2016)
- **Journal**: None
- **Summary**: In recent years, the research community has discovered that deep neural networks (DNNs) and convolutional neural networks (CNNs) can yield higher accuracy than all previous solutions to a broad array of machine learning problems. To our knowledge, there is no single CNN/DNN architecture that solves all problems optimally. Instead, the "right" CNN/DNN architecture varies depending on the application at hand. CNN/DNNs comprise an enormous design space. Quantitatively, we find that a small region of the CNN design space contains 30 billion different CNN architectures.   In this dissertation, we develop a methodology that enables systematic exploration of the design space of CNNs. Our methodology is comprised of the following four themes.   1. Judiciously choosing benchmarks and metrics.   2. Rapidly training CNN models.   3. Defining and describing the CNN design space.   4. Exploring the design space of CNN architectures.   Taken together, these four themes comprise an effective methodology for discovering the "right" CNN architectures to meet the needs of practical applications.



### 3D Human Pose Estimation = 2D Pose Estimation + Matching
- **Arxiv ID**: http://arxiv.org/abs/1612.06524v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06524v2)
- **Published**: 2016-12-20 06:45:49+00:00
- **Updated**: 2017-04-11 07:33:51+00:00
- **Authors**: Ching-Hang Chen, Deva Ramanan
- **Comment**: Demo code: https://github.com/flyawaychase/3DHumanPose
- **Journal**: None
- **Summary**: We explore 3D human pose estimation from a single RGB image. While many approaches try to directly predict 3D pose from image measurements, we explore a simple architecture that reasons through intermediate 2D pose predictions. Our approach is based on two key observations (1) Deep neural nets have revolutionized 2D pose estimation, producing accurate 2D predictions even for poses with self occlusions. (2) Big-data sets of 3D mocap data are now readily available, making it tempting to lift predicted 2D poses to 3D through simple memorization (e.g., nearest neighbors). The resulting architecture is trivial to implement with off-the-shelf 2D pose estimation systems and 3D mocap libraries. Importantly, we demonstrate that such methods outperform almost all state-of-the-art 3D pose estimation systems, most of which directly try to regress 3D pose from 2D measurements.



### Automatic Generation of Grounded Visual Questions
- **Arxiv ID**: http://arxiv.org/abs/1612.06530v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1612.06530v2)
- **Published**: 2016-12-20 07:20:16+00:00
- **Updated**: 2017-05-29 12:54:35+00:00
- **Authors**: Shijie Zhang, Lizhen Qu, Shaodi You, Zhenglu Yang, Jiawan Zhang
- **Comment**: VQA
- **Journal**: IJCAI 2017
- **Summary**: In this paper, we propose the first model to be able to generate visually grounded questions with diverse types for a single image. Visual question generation is an emerging topic which aims to ask questions in natural language based on visual input. To the best of our knowledge, it lacks automatic methods to generate meaningful questions with various types for the same visual input. To circumvent the problem, we propose a model that automatically generates visually grounded questions with varying types. Our model takes as input both images and the captions generated by a dense caption model, samples the most probable question types, and generates the questions in sequel. The experimental results on two real world datasets show that our model outperforms the strongest baseline in terms of both correctness and diversity with a wide margin.



### Wide-Slice Residual Networks for Food Recognition
- **Arxiv ID**: http://arxiv.org/abs/1612.06543v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06543v1)
- **Published**: 2016-12-20 08:19:52+00:00
- **Updated**: 2016-12-20 08:19:52+00:00
- **Authors**: Niki Martinel, Gian Luca Foresti, Christian Micheloni
- **Comment**: None
- **Journal**: None
- **Summary**: Food diary applications represent a tantalizing market. Such applications, based on image food recognition, opened to new challenges for computer vision and pattern recognition algorithms. Recent works in the field are focusing either on hand-crafted representations or on learning these by exploiting deep neural networks. Despite the success of such a last family of works, these generally exploit off-the shelf deep architectures to classify food dishes. Thus, the architectures are not cast to the specific problem. We believe that better results can be obtained if the deep architecture is defined with respect to an analysis of the food composition. Following such an intuition, this work introduces a new deep scheme that is designed to handle the food structure. Specifically, inspired by the recent success of residual deep network, we exploit such a learning scheme and introduce a slice convolution block to capture the vertical food layers. Outputs of the deep residual blocks are combined with the sliced convolution to produce the classification score for specific food categories. To evaluate our proposed architecture we have conducted experimental results on three benchmark datasets. Results demonstrate that our solution shows better performance with respect to existing approaches (e.g., a top-1 accuracy of 90.27% on the Food-101 challenging dataset).



### End-to-End Pedestrian Collision Warning System based on a Convolutional Neural Network with Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1612.06558v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06558v1)
- **Published**: 2016-12-20 09:10:30+00:00
- **Updated**: 2016-12-20 09:10:30+00:00
- **Authors**: Heechul Jung, Min-Kook Choi, Kwon Soon, Woo Young Jung
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: Traditional pedestrian collision warning systems sometimes raise alarms even when there is no danger (e.g., when all pedestrians are walking on the sidewalk). These false alarms can make it difficult for drivers to concentrate on their driving. In this paper, we propose a novel framework for an end-to-end pedestrian collision warning system based on a convolutional neural network. Semantic segmentation information is used to train the convolutional neural network and two loss functions, such as cross entropy and Euclidean losses, are minimized. Finally, we demonstrate the effectiveness of our method in reducing false alarms and increasing warning accuracy compared to a traditional histogram of oriented gradients (HoG)-based system.



### Detecting Unexpected Obstacles for Self-Driving Cars: Fusing Deep Learning and Geometric Modeling
- **Arxiv ID**: http://arxiv.org/abs/1612.06573v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1612.06573v1)
- **Published**: 2016-12-20 09:55:00+00:00
- **Updated**: 2016-12-20 09:55:00+00:00
- **Authors**: Sebastian Ramos, Stefan Gehrig, Peter Pinggera, Uwe Franke, Carsten Rother
- **Comment**: Submitted to the IEEE International Conference on Robotics and
  Automation (ICRA) 2017
- **Journal**: None
- **Summary**: The detection of small road hazards, such as lost cargo, is a vital capability for self-driving cars. We tackle this challenging and rarely addressed problem with a vision system that leverages appearance, contextual as well as geometric cues. To utilize the appearance and contextual cues, we propose a new deep learning-based obstacle detection framework. Here a variant of a fully convolutional network is used to predict a pixel-wise semantic labeling of (i) free-space, (ii) on-road unexpected obstacles, and (iii) background. The geometric cues are exploited using a state-of-the-art detection approach that predicts obstacles from stereo input images via model-based statistical hypothesis tests. We present a principled Bayesian framework to fuse the semantic and stereo-based detection results. The mid-level Stixel representation is used to describe obstacles in a flexible, compact and robust manner. We evaluate our new obstacle detection system on the Lost and Found dataset, which includes very challenging scenes with obstacles of only 5 cm height. Overall, we report a major improvement over the state-of-the-art, with relative performance gains of up to 50%. In particular, we achieve a detection rate of over 90% for distances of up to 50 m. Our system operates at 22 Hz on our self-driving platform.



### Deep Motion Features for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1612.06615v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06615v1)
- **Published**: 2016-12-20 11:33:31+00:00
- **Updated**: 2016-12-20 11:33:31+00:00
- **Authors**: Susanna Gladh, Martin Danelljan, Fahad Shahbaz Khan, Michael Felsberg
- **Comment**: ICPR 2016. Best paper award in the "Computer Vision and Robot Vision"
  track
- **Journal**: None
- **Summary**: Robust visual tracking is a challenging computer vision problem, with many real-world applications. Most existing approaches employ hand-crafted appearance features, such as HOG or Color Names. Recently, deep RGB features extracted from convolutional neural networks have been successfully applied for tracking. Despite their success, these features only capture appearance information. On the other hand, motion cues provide discriminative and complementary information that can improve tracking performance. Contrary to visual tracking, deep motion features have been successfully applied for action recognition and video classification tasks. Typically, the motion features are learned by training a CNN on optical flow images extracted from large amounts of labeled videos.   This paper presents an investigation of the impact of deep motion features in a tracking-by-detection framework. We further show that hand-crafted, deep RGB, and deep motion features contain complementary information. To the best of our knowledge, we are the first to propose fusing appearance information with deep motion features for visual tracking. Comprehensive experiments clearly suggest that our fusion approach with deep motion features outperforms standard methods relying on appearance information alone.



### Unsupervised Perceptual Rewards for Imitation Learning
- **Arxiv ID**: http://arxiv.org/abs/1612.06699v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1612.06699v3)
- **Published**: 2016-12-20 15:04:38+00:00
- **Updated**: 2017-06-12 21:38:17+00:00
- **Authors**: Pierre Sermanet, Kelvin Xu, Sergey Levine
- **Comment**: None
- **Journal**: None
- **Summary**: Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a reward function takes considerable hand engineering and often requires additional sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple implicit intermediate steps that must be executed in sequence. Even when the final outcome can be measured, it does not necessarily provide feedback on these intermediate steps. To address these issues, we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. We present a method that is able to identify key intermediate steps of a task from only a handful of demonstration sequences, and automatically identify the most discriminative features for identifying these steps. This method makes use of the features in a pre-trained deep model, but does not require any explicit specification of sub-goals. The resulting reward functions can then be used by an RL agent to learn to perform the task in real-world settings. To evaluate the learned reward, we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. We also show that our method can be used to learn a real-world door opening skill using a real robot, even when the demonstration used for reward learning is provided by a human using their own hand. To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task. Supplementary material and data are available at https://sermanet.github.io/rewards



### Dynamic Action Recognition: A convolutional neural network model for temporally organized joint location data
- **Arxiv ID**: http://arxiv.org/abs/1612.06703v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06703v1)
- **Published**: 2016-12-20 15:20:28+00:00
- **Updated**: 2016-12-20 15:20:28+00:00
- **Authors**: Adhavan Jayabalan, Harish Karunakaran, Shravan Murlidharan, Tesia Shizume
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Motivation: Recognizing human actions in a video is a challenging task which has applications in various fields. Previous works in this area have either used images from a 2D or 3D camera. Few have used the idea that human actions can be easily identified by the movement of the joints in the 3D space and instead used a Recurrent Neural Network (RNN) for modeling. Convolutional neural networks (CNN) have the ability to recognise even the complex patterns in data which makes it suitable for detecting human actions. Thus, we modeled a CNN which can predict the human activity using the joint data. Furthermore, using the joint data representation has the benefit of lower dimensionality than image or video representations. This makes our model simpler and faster than the RNN models. In this study, we have developed a six layer convolutional network, which reduces each input feature vector of the form 15x1961x4 to an one dimensional binary vector which gives us the predicted activity. Results: Our model is able to recognise an activity correctly upto 87% accuracy. Joint data is taken from the Cornell Activity Datasets which have day to day activities like talking, relaxing, eating, cooking etc.



### Action-Driven Object Detection with Top-Down Visual Attentions
- **Arxiv ID**: http://arxiv.org/abs/1612.06704v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.06704v1)
- **Published**: 2016-12-20 15:24:46+00:00
- **Updated**: 2016-12-20 15:24:46+00:00
- **Authors**: Donggeun Yoo, Sunggyun Park, Kyunghyun Paeng, Joon-Young Lee, In So Kweon
- **Comment**: None
- **Journal**: None
- **Summary**: A dominant paradigm for deep learning based object detection relies on a "bottom-up" approach using "passive" scoring of class agnostic proposals. These approaches are efficient but lack of holistic analysis of scene-level context. In this paper, we present an "action-driven" detection mechanism using our "top-down" visual attention model. We localize an object by taking sequential actions that the attention model provides. The attention model conditioned with an image region provides required actions to get closer toward a target object. An action at each time step is weak itself but an ensemble of the sequential actions makes a bounding-box accurately converge to a target object boundary. This attention model we call AttentionNet is composed of a convolutional neural network. During our whole detection procedure, we only utilize the actions from a single AttentionNet without any modules for object proposals nor post bounding-box regression. We evaluate our top-down detection mechanism over the PASCAL VOC series and ILSVRC CLS-LOC dataset, and achieve state-of-the-art performances compared to the major bottom-up detection methods. In particular, our detection mechanism shows a strong advantage in elaborate localization by outperforming Faster R-CNN with a margin of +7.1% over PASCAL VOC 2007 when we increase the IoU threshold for positive detection to 0.7.



### Local Sparse Approximation for Image Restoration with Adaptive Block Size Selection
- **Arxiv ID**: http://arxiv.org/abs/1612.06738v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, eess.IV, eess.SP, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1612.06738v1)
- **Published**: 2016-12-20 16:28:48+00:00
- **Updated**: 2016-12-20 16:28:48+00:00
- **Authors**: Sujit Kumar Sahoo
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper the problem of image restoration (denoising and inpainting) is approached using sparse approximation of local image blocks. The local image blocks are extracted by sliding square windows over the image. An adaptive block size selection procedure for local sparse approximation is proposed, which affects the global recovery of underlying image. Ideally the adaptive local block selection yields the minimum mean square error (MMSE) in recovered image. This framework gives us a clustered image based on the selected block size, then each cluster is restored separately using sparse approximation. The results obtained using the proposed framework are very much comparable with the recently proposed image restoration techniques.



### Two decades of local binary patterns: A survey
- **Arxiv ID**: http://arxiv.org/abs/1612.06795v2
- **DOI**: 10.1016/B978-0-12-802806-3.00009-9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06795v2)
- **Published**: 2016-12-20 18:33:28+00:00
- **Updated**: 2017-01-16 02:49:16+00:00
- **Authors**: Matti Pietikäinen, Guoying Zhao
- **Comment**: In Advances in Independent Component Analysis and Learning Machines,
  Academic Press, 2015, Pages 175-210
- **Journal**: None
- **Summary**: Texture is an important characteristic for many types of images. In recent years very discriminative and computationally efficient local texture descriptors based on local binary patterns (LBP) have been developed, which has led to significant progress in applying texture methods to different problems and applications. Due to this progress, the division between texture descriptors and more generic image or video descriptors has been disappearing. A large number of different variants of LBP have been developed to improve its robustness, and to increase its discriminative power and applicability to different types of problems. In this chapter, the most recent and important variants of LBP in 2D, spatiotemporal, 3D, and 4D domains are surveyed. Interesting new developments of LBP in 1D signal analysis are also considered. Finally, some future challenges for research are presented.



### Center-Focusing Multi-task CNN with Injected Features for Classification of Glioma Nuclear Images
- **Arxiv ID**: http://arxiv.org/abs/1612.06825v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06825v2)
- **Published**: 2016-12-20 19:54:37+00:00
- **Updated**: 2017-01-10 18:44:32+00:00
- **Authors**: Veda Murthy, Le Hou, Dimitris Samaras, Tahsin M. Kurc, Joel H. Saltz
- **Comment**: None
- **Journal**: None
- **Summary**: Classifying the various shapes and attributes of a glioma cell nucleus is crucial for diagnosis and understanding the disease. We investigate automated classification of glioma nuclear shapes and visual attributes using Convolutional Neural Networks (CNNs) on pathology images of automatically segmented nuclei. We propose three methods that improve the performance of a previously-developed semi-supervised CNN. First, we propose a method that allows the CNN to focus on the most important part of an image- the image's center containing the nucleus. Second, we inject (concatenate) pre-extracted VGG features into an intermediate layer of our Semi-Supervised CNN so that during training, the CNN can learn a set of complementary features. Third, we separate the losses of the two groups of target classes (nuclear shapes and attributes) into a single-label loss and a multi-label loss so that the prior knowledge of inter-label exclusiveness can be incorporated. On a dataset of 2078 images, the proposed methods combined reduce the error rate of attribute and shape classification by 21.54% and 15.07% respectively compared to the existing state-of-the-art method on the same dataset.



### From Images to 3D Shape Attributes
- **Arxiv ID**: http://arxiv.org/abs/1612.06836v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06836v2)
- **Published**: 2016-12-20 20:24:57+00:00
- **Updated**: 2017-12-03 22:58:22+00:00
- **Authors**: David F. Fouhey, Abhinav Gupta, Andrew Zisserman
- **Comment**: Updated based on TPAMI reviews: title changed, sections reordered,
  moderate modifications throughout text
- **Journal**: None
- **Summary**: Our goal in this paper is to investigate properties of 3D shape that can be determined from a single image. We define 3D shape attributes -- generic properties of the shape that capture curvature, contact and occupied space. Our first objective is to infer these 3D shape attributes from a single image. A second objective is to infer a 3D shape embedding -- a low dimensional vector representing the 3D shape.   We study how the 3D shape attributes and embedding can be obtained from a single image by training a Convolutional Neural Network (CNN) for this task. We start with synthetic images so that the contribution of various cues and nuisance parameters can be controlled. We then turn to real images and introduce a large scale image dataset of sculptures containing 143K images covering 2197 works from 242 artists.   For the CNN trained on the sculpture dataset we show the following: (i) which regions of the imaged sculpture are used by the CNN to infer the 3D shape attributes; (ii) that the shape embedding can be used to match previously unseen sculptures largely independent of viewpoint; and (iii) that the 3D attributes generalize to images of other (non-sculpture) object classes.



### Beyond Skip Connections: Top-Down Modulation for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1612.06851v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.06851v2)
- **Published**: 2016-12-20 20:57:59+00:00
- **Updated**: 2017-09-19 22:37:40+00:00
- **Authors**: Abhinav Shrivastava, Rahul Sukthankar, Jitendra Malik, Abhinav Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, we have seen tremendous progress in the field of object detection. Most of the recent improvements have been achieved by targeting deeper feedforward networks. However, many hard object categories such as bottle, remote, etc. require representation of fine details and not just coarse, semantic representations. But most of these fine details are lost in the early convolutional layers. What we need is a way to incorporate finer details from lower layers into the detection architecture. Skip connections have been proposed to combine high-level and low-level features, but we argue that selecting the right features from low-level requires top-down contextual information. Inspired by the human visual pathway, in this paper we propose top-down modulations as a way to incorporate fine details into the detection framework. Our approach supplements the standard bottom-up, feedforward ConvNet with a top-down modulation (TDM) network, connected using lateral connections. These connections are responsible for the modulation of lower layer filters, and the top-down network handles the selection and integration of contextual information and low-level features. The proposed TDM architecture provides a significant boost on the COCO testdev benchmark, achieving 28.6 AP for VGG16, 35.2 AP for ResNet101, and 37.3 for InceptionResNetv2 network, without any bells and whistles (e.g., multi-scale, iterative box refinement, etc.).



### CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1612.06890v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.06890v1)
- **Published**: 2016-12-20 21:40:40+00:00
- **Updated**: 2016-12-20 21:40:40+00:00
- **Authors**: Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, Ross Girshick
- **Comment**: None
- **Journal**: None
- **Summary**: When building artificial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress and discover shortcomings. Existing benchmarks for visual question answering can help, but have strong biases that models can exploit to correctly answer questions without reasoning. They also conflate multiple sources of error, making it hard to pinpoint model weaknesses. We present a diagnostic dataset that tests a range of visual reasoning abilities. It contains minimal biases and has detailed annotations describing the kind of reasoning each question requires. We use this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations.



### A Statistical Approach to Continuous Self-Calibrating Eye Gaze Tracking for Head-Mounted Virtual Reality Systems
- **Arxiv ID**: http://arxiv.org/abs/1612.06919v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06919v1)
- **Published**: 2016-12-20 23:25:27+00:00
- **Updated**: 2016-12-20 23:25:27+00:00
- **Authors**: Subarna Tripathi, Brian Guenter
- **Comment**: Accepted for publication in WACV 2017
- **Journal**: None
- **Summary**: We present a novel, automatic eye gaze tracking scheme inspired by smooth pursuit eye motion while playing mobile games or watching virtual reality contents. Our algorithm continuously calibrates an eye tracking system for a head mounted display. This eliminates the need for an explicit calibration step and automatically compensates for small movements of the headset with respect to the head. The algorithm finds correspondences between corneal motion and screen space motion, and uses these to generate Gaussian Process Regression models. A combination of those models provides a continuous mapping from corneal position to screen space position. Accuracy is nearly as good as achieved with an explicit calibration step.



