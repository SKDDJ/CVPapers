# Arxiv Papers in cs.CV on 2016-12-15
### Deep learning is effective for the classification of OCT images of normal versus Age-related Macular Degeneration
- **Arxiv ID**: http://arxiv.org/abs/1612.04891v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.04891v1)
- **Published**: 2016-12-15 00:23:43+00:00
- **Updated**: 2016-12-15 00:23:43+00:00
- **Authors**: Cecilia S. Lee, Doug M. Baughman, Aaron Y. Lee
- **Comment**: 4 Figures, 1 Table
- **Journal**: None
- **Summary**: Objective: The advent of Electronic Medical Records (EMR) with large electronic imaging databases along with advances in deep neural networks with machine learning has provided a unique opportunity to achieve milestones in automated image analysis. Optical coherence tomography (OCT) is the most commonly obtained imaging modality in ophthalmology and represents a dense and rich dataset when combined with labels derived from the EMR. We sought to determine if deep learning could be utilized to distinguish normal OCT images from images from patients with Age-related Macular Degeneration (AMD). Methods: Automated extraction of an OCT imaging database was performed and linked to clinical endpoints from the EMR. OCT macula scans were obtained by Heidelberg Spectralis, and each OCT scan was linked to EMR clinical endpoints extracted from EPIC. The central 11 images were selected from each OCT scan of two cohorts of patients: normal and AMD. Cross-validation was performed using a random subset of patients. Area under receiver operator curves (auROC) were constructed at an independent image level, macular OCT level, and patient level. Results: Of an extraction of 2.6 million OCT images linked to clinical datapoints from the EMR, 52,690 normal and 48,312 AMD macular OCT images were selected. A deep neural network was trained to categorize images as either normal or AMD. At the image level, we achieved an auROC of 92.78% with an accuracy of 87.63%. At the macula level, we achieved an auROC of 93.83% with an accuracy of 88.98%. At a patient level, we achieved an auROC of 97.45% with an accuracy of 93.45%. Peak sensitivity and specificity with optimal cutoffs were 92.64% and 93.69% respectively. Conclusions: Deep learning techniques are effective for classifying OCT images. These findings have important implications in utilizing OCT in automated screening and computer aided diagnosis tools.



### Tinkering Under the Hood: Interactive Zero-Shot Learning with Net Surgery
- **Arxiv ID**: http://arxiv.org/abs/1612.04901v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.04901v1)
- **Published**: 2016-12-15 01:27:10+00:00
- **Updated**: 2016-12-15 01:27:10+00:00
- **Authors**: Vivek Krishnan, Deva Ramanan
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the task of visual net surgery, in which a CNN can be reconfigured without extra data to recognize novel concepts that may be omitted from the training set. While most prior work make use of linguistic cues for such "zero-shot" learning, we do so by using a pictorial language representation of the training set, implicitly learned by a CNN, to generalize to new classes. To this end, we introduce a set of visualization techniques that better reveal the activation patterns and relations between groups of CNN filters. We next demonstrate that knowledge of pictorial languages can be used to rewire certain CNN neurons into a part model, which we call a pictorial language classifier. We demonstrate the robustness of simple PLCs by applying them in a weakly supervised manner: labeling unlabeled concepts for visual classes present in the training data. Specifically we show that a PLC built on top of a CNN trained for ImageNet classification can localize humans in Graz-02 and determine the pose of birds in PASCAL-VOC without extra labeled data or additional training. We then apply PLCs in an interactive zero-shot manner, demonstrating that pictorial languages are expressive enough to detect a set of visual classes in MS-COCO that never appear in the ImageNet training set.



### Regressing Robust and Discriminative 3D Morphable Models with a very Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1612.04904v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.04904v1)
- **Published**: 2016-12-15 02:02:28+00:00
- **Updated**: 2016-12-15 02:02:28+00:00
- **Authors**: Anh Tuan Tran, Tal Hassner, Iacopo Masi, Gerard Medioni
- **Comment**: None
- **Journal**: None
- **Summary**: The 3D shapes of faces are well known to be discriminative. Yet despite this, they are rarely used for face recognition and always under controlled viewing conditions. We claim that this is a symptom of a serious but often overlooked problem with existing methods for single view 3D face reconstruction: when applied "in the wild", their 3D estimates are either unstable and change for different photos of the same subject or they are over-regularized and generic. In response, we describe a robust method for regressing discriminative 3D morphable face models (3DMM). We use a convolutional neural network (CNN) to regress 3DMM shape and texture parameters directly from an input photo. We overcome the shortage of training data required for this purpose by offering a method for generating huge numbers of labeled examples. The 3D estimates produced by our CNN surpass state of the art accuracy on the MICC data set. Coupled with a 3D-3D face matching pipeline, we show the first competitive face recognition results on the LFW, YTF and IJB-A benchmarks using 3D face shapes as representations, rather than the opaque deep feature vectors used by other modern systems.



### Recurrent Image Captioner: Describing Images with Spatial-Invariant Transformation and Attention Filtering
- **Arxiv ID**: http://arxiv.org/abs/1612.04949v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1612.04949v1)
- **Published**: 2016-12-15 07:19:46+00:00
- **Updated**: 2016-12-15 07:19:46+00:00
- **Authors**: Hao Liu, Yang Yang, Fumin Shen, Lixin Duan, Heng Tao Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Along with the prosperity of recurrent neural network in modelling sequential data and the power of attention mechanism in automatically identify salient information, image captioning, a.k.a., image description, has been remarkably advanced in recent years. Nonetheless, most existing paradigms may suffer from the deficiency of invariance to images with different scaling, rotation, etc.; and effective integration of standalone attention to form a holistic end-to-end system. In this paper, we propose a novel image captioning architecture, termed Recurrent Image Captioner (\textbf{RIC}), which allows visual encoder and language decoder to coherently cooperate in a recurrent manner. Specifically, we first equip CNN-based visual encoder with a differentiable layer to enable spatially invariant transformation of visual signals. Moreover, we deploy an attention filter module (differentiable) between encoder and decoder to dynamically determine salient visual parts. We also employ bidirectional LSTM to preprocess sentences for generating better textual representations. Besides, we propose to exploit variational inference to optimize the whole architecture. Extensive experimental results on three benchmark datasets (i.e., Flickr8k, Flickr30k and MS COCO) demonstrate the superiority of our proposed architecture as compared to most of the state-of-the-art methods.



### Cloud Dictionary: Sparse Coding and Modeling for Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1612.04956v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1612.04956v2)
- **Published**: 2016-12-15 07:53:27+00:00
- **Updated**: 2017-03-20 19:45:44+00:00
- **Authors**: Or Litany, Tal Remez, Alex Bronstein
- **Comment**: Signal Processing with Adaptive Sparse Structured Representations
  (SPARS), 2017
- **Journal**: None
- **Summary**: With the development of range sensors such as LIDAR and time-of-flight cameras, 3D point cloud scans have become ubiquitous in computer vision applications, the most prominent ones being gesture recognition and autonomous driving. Parsimony-based algorithms have shown great success on images and videos where data points are sampled on a regular Cartesian grid. We propose an adaptation of these techniques to irregularly sampled signals by using continuous dictionaries. We present an example application in the form of point cloud denoising.



### Design of Image Matched Non-Separable Wavelet using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1612.04966v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.04966v1)
- **Published**: 2016-12-15 08:28:16+00:00
- **Updated**: 2016-12-15 08:28:16+00:00
- **Authors**: Naushad Ansari, Anubha Gupta, Rahul Duggal
- **Comment**: None
- **Journal**: None
- **Summary**: Image-matched nonseparable wavelets can find potential use in many applications including image classification, segmen- tation, compressive sensing, etc. This paper proposes a novel design methodology that utilizes convolutional neural net- work (CNN) to design two-channel non-separable wavelet matched to a given image. The design is proposed on quin- cunx lattice. The loss function of the convolutional neural network is setup with total squared error between the given input image to CNN and the reconstructed image at the output of CNN, leading to perfect reconstruction at the end of train- ing. Simulation results have been shown on some standard images.



### Development of a Real-time Colorectal Tumor Classification System for Narrow-band Imaging zoom-videoendoscopy
- **Arxiv ID**: http://arxiv.org/abs/1612.05000v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.05000v2)
- **Published**: 2016-12-15 09:59:23+00:00
- **Updated**: 2016-12-21 03:48:01+00:00
- **Authors**: Tsubasa Hirakawa, Toru Tamaki, Bisser Raytchev, Kazufumi Kaneda, Tetsushi Koide, Shigeto Yoshida, Hiroshi Mieno, Shinji Tanaka
- **Comment**: 9 pages, 8 figures
- **Journal**: None
- **Summary**: Colorectal endoscopy is important for the early detection and treatment of colorectal cancer and is used worldwide. A computer-aided diagnosis (CAD) system that provides an objective measure to endoscopists during colorectal endoscopic examinations would be of great value. In this study, we describe a newly developed CAD system that provides real-time objective measures. Our system captures the video stream from an endoscopic system and transfers it to a desktop computer. The captured video stream is then classified by a pretrained classifier and the results are displayed on a monitor. The experimental results show that our developed system works efficiently in actual endoscopic examinations and is medically significant.



### A Multilinear Tongue Model Derived from Speech Related MRI Data of the Human Vocal Tract
- **Arxiv ID**: http://arxiv.org/abs/1612.05005v5
- **DOI**: 10.1016/j.csl.2018.02.001
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.05005v5)
- **Published**: 2016-12-15 10:31:40+00:00
- **Updated**: 2018-04-17 08:16:54+00:00
- **Authors**: Alexander Hewer, Stefanie Wuhrer, Ingmar Steiner, Korin Richmond
- **Comment**: None
- **Journal**: Computer Speech & Language 51 (2018) 68-92
- **Summary**: We present a multilinear statistical model of the human tongue that captures anatomical and tongue pose related shape variations separately. The model is derived from 3D magnetic resonance imaging data of 11 speakers sustaining speech related vocal tract configurations. The extraction is performed by using a minimally supervised method that uses as basis an image segmentation approach and a template fitting technique. Furthermore, it uses image denoising to deal with possibly corrupt data, palate surface information reconstruction to handle palatal tongue contacts, and a bootstrap strategy to refine the obtained shapes. Our evaluation concludes that limiting the degrees of freedom for the anatomical and speech related variations to 5 and 4, respectively, produces a model that can reliably register unknown data while avoiding overfitting effects. Furthermore, we show that it can be used to generate a plausible tongue animation by tracking sparse motion capture data.



### Objective Micro-Facial Movement Detection Using FACS-Based Regions and Baseline Evaluation
- **Arxiv ID**: http://arxiv.org/abs/1612.05038v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.05038v1)
- **Published**: 2016-12-15 12:15:36+00:00
- **Updated**: 2016-12-15 12:15:36+00:00
- **Authors**: Adrian K. Davison, Cliff Lansley, Choon Ching Ng, Kevin Tan, Moi Hoon Yap
- **Comment**: None
- **Journal**: None
- **Summary**: Micro-facial expressions are regarded as an important human behavioural event that can highlight emotional deception. Spotting these movements is difficult for humans and machines, however research into using computer vision to detect subtle facial expressions is growing in popularity. This paper proposes an individualised baseline micro-movement detection method using 3D Histogram of Oriented Gradients (3D HOG) temporal difference method. We define a face template consisting of 26 regions based on the Facial Action Coding System (FACS). We extract the temporal features of each region using 3D HOG. Then, we use Chi-square distance to find subtle facial motion in the local regions. Finally, an automatic peak detector is used to detect micro-movements above the newly proposed adaptive baseline threshold. The performance is validated on two FACS coded datasets: SAMM and CASME II. This objective method focuses on the movement of the 26 face regions. When comparing with the ground truth, the best result was an AUC of 0.7512 and 0.7261 on SAMM and CASME II, respectively. The results show that 3D HOG outperformed for micro-movement detection, compared to state-of-the-art feature representations: Local Binary Patterns in Three Orthogonal Planes and Histograms of Oriented Optical Flow.



### Towards Score Following in Sheet Music Images
- **Arxiv ID**: http://arxiv.org/abs/1612.05050v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1612.05050v1)
- **Published**: 2016-12-15 13:10:13+00:00
- **Updated**: 2016-12-15 13:10:13+00:00
- **Authors**: Matthias Dorfer, Andreas Arzt, Gerhard Widmer
- **Comment**: Published In Proceedings of the 17th International Society for Music
  Information Retrieval Conference (2016)
- **Journal**: None
- **Summary**: This paper addresses the matching of short music audio snippets to the corresponding pixel location in images of sheet music. A system is presented that simultaneously learns to read notes, listens to music and matches the currently played music to its corresponding notes in the sheet. It consists of an end-to-end multi-modal convolutional neural network that takes as input images of sheet music and spectrograms of the respective audio snippets. It learns to predict, for a given unseen audio snippet (covering approximately one bar of music), the corresponding position in the respective score line. Our results suggest that with the use of (deep) neural networks -- which have proven to be powerful image processing models -- working with sheet music becomes feasible and a promising future research direction.



### Reflectance Adaptive Filtering Improves Intrinsic Image Estimation
- **Arxiv ID**: http://arxiv.org/abs/1612.05062v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.05062v2)
- **Published**: 2016-12-15 13:42:54+00:00
- **Updated**: 2017-06-12 12:39:49+00:00
- **Authors**: Thomas Nestmeyer, Peter V. Gehler
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: Separating an image into reflectance and shading layers poses a challenge for learning approaches because no large corpus of precise and realistic ground truth decompositions exists. The Intrinsic Images in the Wild~(IIW) dataset provides a sparse set of relative human reflectance judgments, which serves as a standard benchmark for intrinsic images. A number of methods use IIW to learn statistical dependencies between the images and their reflectance layer. Although learning plays an important role for high performance, we show that a standard signal processing technique achieves performance on par with current state-of-the-art. We propose a loss function for CNN learning of dense reflectance predictions. Our results show a simple pixel-wise decision, without any context or prior knowledge, is sufficient to provide a strong baseline on IIW. This sets a competitive baseline which only two other approaches surpass. We then develop a joint bilateral filtering method that implements strong prior knowledge about reflectance constancy. This filtering operation can be applied to any intrinsic image algorithm and we improve several previous results achieving a new state-of-the-art on IIW. Our findings suggest that the effect of learning-based approaches may have been over-estimated so far. Explicit prior knowledge is still at least as important to obtain high performance in intrinsic image decompositions.



### Beyond Holistic Object Recognition: Enriching Image Understanding with Part States
- **Arxiv ID**: http://arxiv.org/abs/1612.07310v1
- **DOI**: None
- **Categories**: **cs.CV**, F.2.2
- **Links**: [PDF](http://arxiv.org/pdf/1612.07310v1)
- **Published**: 2016-12-15 13:46:58+00:00
- **Updated**: 2016-12-15 13:46:58+00:00
- **Authors**: Cewu Lu, Hao Su, Yongyi Lu, Li Yi, Chikeung Tang, Leonidas Guibas
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Important high-level vision tasks such as human-object interaction, image captioning and robotic manipulation require rich semantic descriptions of objects at part level. Based upon previous work on part localization, in this paper, we address the problem of inferring rich semantics imparted by an object part in still images. We propose to tokenize the semantic space as a discrete set of part states. Our modeling of part state is spatially localized, therefore, we formulate the part state inference problem as a pixel-wise annotation problem. An iterative part-state inference neural network is specifically designed for this task, which is efficient in time and accurate in performance. Extensive experiments demonstrate that the proposed method can effectively predict the semantic states of parts and simultaneously correct localization errors, thus benefiting a few visual understanding applications. The other contribution of this paper is our part state dataset which contains rich part-level semantic annotations.



### SceneNet RGB-D: 5M Photorealistic Images of Synthetic Indoor Trajectories with Ground Truth
- **Arxiv ID**: http://arxiv.org/abs/1612.05079v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.05079v3)
- **Published**: 2016-12-15 14:22:38+00:00
- **Updated**: 2017-01-30 11:06:14+00:00
- **Authors**: John McCormac, Ankur Handa, Stefan Leutenegger, Andrew J. Davison
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce SceneNet RGB-D, expanding the previous work of SceneNet to enable large scale photorealistic rendering of indoor scene trajectories. It provides pixel-perfect ground truth for scene understanding problems such as semantic segmentation, instance segmentation, and object detection, and also for geometric computer vision problems such as optical flow, depth estimation, camera pose estimation, and 3D reconstruction. Random sampling permits virtually unlimited scene configurations, and here we provide a set of 5M rendered RGB-D images from over 15K trajectories in synthetic layouts with random but physically simulated object poses. Each layout also has random lighting, camera trajectories, and textures. The scale of this dataset is well suited for pre-training data-driven computer vision techniques from scratch with RGB-D inputs, which previously has been limited by relatively small labelled datasets in NYUv2 and SUN RGB-D. It also provides a basis for investigating 3D scene labelling tasks by providing perfect camera poses and depth data as proxy for a SLAM system. We host the dataset at http://robotvault.bitbucket.io/scenenet-rgbd.html



### Coupling Adaptive Batch Sizes with Learning Rates
- **Arxiv ID**: http://arxiv.org/abs/1612.05086v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1612.05086v2)
- **Published**: 2016-12-15 14:42:45+00:00
- **Updated**: 2017-06-28 12:07:02+00:00
- **Authors**: Lukas Balles, Javier Romero, Philipp Hennig
- **Comment**: Thirty-Third Conference on Uncertainty in Artificial Intelligence
  (UAI), 2017, (accepted)
- **Journal**: None
- **Summary**: Mini-batch stochastic gradient descent and variants thereof have become standard for large-scale empirical risk minimization like the training of neural networks. These methods are usually used with a constant batch size chosen by simple empirical inspection. The batch size significantly influences the behavior of the stochastic optimization algorithm, though, since it determines the variance of the gradient estimates. This variance also changes over the optimization process; when using a constant batch size, stability and convergence is thus often enforced by means of a (manually tuned) decreasing learning rate schedule.   We propose a practical method for dynamic batch size adaptation. It estimates the variance of the stochastic gradients and adapts the batch size to decrease the variance proportionally to the value of the objective function, removing the need for the aforementioned learning rate decrease. In contrast to recent related work, our algorithm couples the batch size to the learning rate, directly reflecting the known relationship between the two. On popular image classification benchmarks, our batch size adaptation yields faster optimization convergence, while simultaneously simplifying learning rate tuning. A TensorFlow implementation is available.



### CSVideoNet: A Real-time End-to-end Learning Framework for High-frame-rate Video Compressive Sensing
- **Arxiv ID**: http://arxiv.org/abs/1612.05203v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.05203v5)
- **Published**: 2016-12-15 19:28:38+00:00
- **Updated**: 2018-01-28 04:32:43+00:00
- **Authors**: Kai Xu, Fengbo Ren
- **Comment**: 9 pages, 6 figures, 7 tables. Accepted by WACV 2018
- **Journal**: None
- **Summary**: This paper addresses the real-time encoding-decoding problem for high-frame-rate video compressive sensing (CS). Unlike prior works that perform reconstruction using iterative optimization-based approaches, we propose a non-iterative model, named "CSVideoNet". CSVideoNet directly learns the inverse mapping of CS and reconstructs the original input in a single forward propagation. To overcome the limitations of existing CS cameras, we propose a multi-rate CNN and a synthesizing RNN to improve the trade-off between compression ratio (CR) and spatial-temporal resolution of the reconstructed videos. The experiment results demonstrate that CSVideoNet significantly outperforms the state-of-the-art approaches. With no pre/post-processing, we achieve 25dB PSNR recovery quality at 100x CR, with a frame rate of 125 fps on a Titan X GPU. Due to the feedforward and high-data-concurrency natures of CSVideoNet, it can take advantage of GPU acceleration to achieve three orders of magnitude speed-up over conventional iterative-based approaches. We share the source code at https://github.com/PSCLab-ASU/CSVideoNet.



### Visual Compiler: Synthesizing a Scene-Specific Pedestrian Detector and Pose Estimator
- **Arxiv ID**: http://arxiv.org/abs/1612.05234v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.05234v1)
- **Published**: 2016-12-15 20:43:06+00:00
- **Updated**: 2016-12-15 20:43:06+00:00
- **Authors**: Namhoon Lee, Xinshuo Weng, Vishnu Naresh Boddeti, Yu Zhang, Fares Beainy, Kris Kitani, Takeo Kanade
- **Comment**: submitted to CVPR 2017
- **Journal**: None
- **Summary**: We introduce the concept of a Visual Compiler that generates a scene specific pedestrian detector and pose estimator without any pedestrian observations. Given a single image and auxiliary scene information in the form of camera parameters and geometric layout of the scene, the Visual Compiler first infers geometrically and photometrically accurate images of humans in that scene through the use of computer graphics rendering. Using these renders we learn a scene-and-region specific spatially-varying fully convolutional neural network, for simultaneous detection, pose estimation and segmentation of pedestrians. We demonstrate that when real human annotated data is scarce or non-existent, our data generation strategy can provide an excellent solution for bootstrapping human detection and pose estimation. Experimental results show that our approach outperforms off-the-shelf state-of-the-art pedestrian detectors and pose estimators that are trained on real data.



