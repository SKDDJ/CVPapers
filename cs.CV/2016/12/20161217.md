# Arxiv Papers in cs.CV on 2016-12-17
### A Fusion Method Based on Decision Reliability Ratio for Finger Vein Verification
- **Arxiv ID**: http://arxiv.org/abs/1612.05712v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.05712v1)
- **Published**: 2016-12-17 07:03:38+00:00
- **Updated**: 2016-12-17 07:03:38+00:00
- **Authors**: Liao Ni, Yi Zhang, He Zheng, Shilei Liu, Houjun Huang, Wenxin Li
- **Comment**: 5 pages, 6 figures
- **Journal**: None
- **Summary**: Finger vein verification has developed a lot since its first proposal, but there is still not a perfect algorithm. It is proved that algorithms with the same overall accuracy may have different misclassified patterns. We could make use of this complementation to fuse individual algorithms together for more precise result. According to our observation, algorithm has different confidence on its decisions but it is seldom considered in fusion methods. Our work is first to define decision reliability ratio to quantify this confidence, and then propose the Maximum Decision Reliability Ratio (MDRR) fusion method incorporating Weighted Voting. Experiment conducted on a data set of 1000 fingers and 5 images per finger proves the effectiveness of the method. The classifier obtained by MDRR method gets an accuracy of 99.42% while the maximum accuracy of the original individual classifiers is 97.77%. The experiment results also show the MDRR outperforms the traditional fusion methods as Voting, Weighted Voting, Sum and Weighted Sum.



### Microscopic Muscle Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1612.05719v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.05719v1)
- **Published**: 2016-12-17 08:05:51+00:00
- **Updated**: 2016-12-17 08:05:51+00:00
- **Authors**: Xiangfei Kong, Lin Yang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a robust image enhancement algorithm dedicated for muscle fiber specimen images captured by optical microscopes. Blur or out of focus problems are prevalent in muscle images during the image acquisition stage. Traditional image deconvolution methods do not work since they assume the blur kernels are known and also produce ring artifacts. We provide a compact framework which involves a novel spatially non-uniform blind deblurring approach specialized to muscle images which automatically detects and alleviates degraded regions. Ring artifacts problems are addressed and a kernel propagation strategy is proposed to speedup the algorithm and deals with the high non-uniformity of the blur kernels on muscle images. Experiments show that the proposed framework performs well on muscle images taken with modern advanced optical microscopes. Our framework is free of laborious parameter settings and is computationally efficient.



### Learning to predict where to look in interactive environments using deep recurrent q-learning
- **Arxiv ID**: http://arxiv.org/abs/1612.05753v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.05753v2)
- **Published**: 2016-12-17 13:29:59+00:00
- **Updated**: 2017-02-18 21:23:14+00:00
- **Authors**: Sajad Mousavi, Michael Schukat, Enda Howley, Ali Borji, Nasser Mozayani
- **Comment**: None
- **Journal**: None
- **Summary**: Bottom-Up (BU) saliency models do not perform well in complex interactive environments where humans are actively engaged in tasks (e.g., sandwich making and playing the video games). In this paper, we leverage Reinforcement Learning (RL) to highlight task-relevant locations of input frames. We propose a soft attention mechanism combined with the Deep Q-Network (DQN) model to teach an RL agent how to play a game and where to look by focusing on the most pertinent parts of its visual input. Our evaluations on several Atari 2600 games show that the soft attention based model could predict fixation locations significantly better than bottom-up models such as Itti-Kochs saliency and Graph-Based Visual Saliency (GBVS) models.



### The Dem@Care Experiments and Datasets: a Technical Report
- **Arxiv ID**: http://arxiv.org/abs/1701.01142v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01142v1)
- **Published**: 2016-12-17 19:43:18+00:00
- **Updated**: 2016-12-17 19:43:18+00:00
- **Authors**: Anastasios Karakostas, Alexia Briassouli, Konstantinos Avgerinakis, Ioannis Kompatsiaris, Magda Tsolaki
- **Comment**: 4pages 2figures
- **Journal**: None
- **Summary**: The objective of Dem@Care is the development of a complete system providing personal health services to people with dementia, as well as medical professionals and caregivers, by using a multitude of sensors, for context-aware, multi-parametric monitoring of lifestyle, ambient environment, and health parameters. Multi-sensor data analysis, combined with intelligent decision making mechanisms, will allow an accurate representation of the person's current status and will provide the appropriate feedback, both to the person and the associated caregivers, enhancing the standard clinical workflow. Within the project framework, several data collection activities have taken place to assist technical development and evaluation tasks. In all these activities, particular attention has been paid to adhere to ethical guidelines and preserve the participants' privacy. This technical report describes shorty the (a) the main objectives of the project, (b) the main ethical principles and (c) the datasets that have been already created.



### EgoTransfer: Transferring Motion Across Egocentric and Exocentric Domains using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.05836v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1612.05836v1)
- **Published**: 2016-12-17 23:33:37+00:00
- **Updated**: 2016-12-17 23:33:37+00:00
- **Authors**: Shervin Ardeshir, Krishna Regmi, Ali Borji
- **Comment**: None
- **Journal**: None
- **Summary**: Mirror neurons have been observed in the primary motor cortex of primate species, in particular in humans and monkeys. A mirror neuron fires when a person performs a certain action, and also when he observes the same action being performed by another person. A crucial step towards building fully autonomous intelligent systems with human-like learning abilities is the capability in modeling the mirror neuron. On one hand, the abundance of egocentric cameras in the past few years has offered the opportunity to study a lot of vision problems from the first-person perspective. A great deal of interesting research has been done during the past few years, trying to explore various computer vision tasks from the perspective of the self. On the other hand, videos recorded by traditional static cameras, capture humans performing different actions from an exocentric third-person perspective. In this work, we take the first step towards relating motion information across these two perspectives. We train models that predict motion in an egocentric view, by observing it from an exocentric view, and vice versa. This allows models to predict how an egocentric motion would look like from outside. To do so, we train linear and nonlinear models and evaluate their performance in terms of retrieving the egocentric (exocentric) motion features, while having access to an exocentric (egocentric) motion feature. Our experimental results demonstrate that motion information can be successfully transferred across the two views.



