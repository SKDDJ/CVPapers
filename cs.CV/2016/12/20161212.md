# Arxiv Papers in cs.CV on 2016-12-12
### An Attention-Driven Approach of No-Reference Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/1612.03530v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.03530v3)
- **Published**: 2016-12-12 03:25:35+00:00
- **Updated**: 2017-05-29 02:42:28+00:00
- **Authors**: Diqi Chen, Yizhou Wang, Tianfu Wu, Wen Gao
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: In this paper, we present a novel method of no-reference image quality assessment (NR-IQA), which is to predict the perceptual quality score of a given image without using any reference image. The proposed method harnesses three functions (i) the visual attention mechanism, which affects many aspects of visual perception including image quality assessment, however, is overlooked in the NR-IQA literature. The method assumes that the fixation areas on an image contain key information to the process of IQA. (ii) the robust averaging strategy, which is a means \--- supported by psychology studies \--- to integrating multiple/step-wise evidence to make a final perceptual judgment. (iii) the multi-task learning, which is believed to be an effectual means to shape representation learning and could result in a more generalized model.   To exploit the synergy of the three, we consider the NR-IQA as a dynamic perception process, in which the model samples a sequence of "informative" areas and aggregates the information to learn a representation for the tasks of jointly predicting the image quality score and the distortion type.   The model learning is implemented by a reinforcement strategy, in which the rewards of both tasks guide the learning of the optimal sampling policy to acquire the "task-informative" image regions so that the predictions can be made accurately and efficiently (in terms of the sampling steps). The reinforcement learning is realized by a deep network with the policy gradient method and trained through back-propagation.   In experiments, the model is tested on the TID2008 dataset and it outperforms several state-of-the-art methods. Furthermore, the model is very efficient in the sense that a small number of fixations are used in NR-IQA.



### PIGMIL: Positive Instance Detection via Graph Updating for Multiple Instance Learning
- **Arxiv ID**: http://arxiv.org/abs/1612.03550v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.03550v1)
- **Published**: 2016-12-12 06:12:19+00:00
- **Updated**: 2016-12-12 06:12:19+00:00
- **Authors**: Dongkuan Xu, Jia Wu, Wei Zhang, Yingjie Tian
- **Comment**: 11 pages, 9 figures
- **Journal**: None
- **Summary**: Positive instance detection, especially for these in positive bags (true positive instances, TPIs), plays a key role for multiple instance learning (MIL) arising from a specific classification problem only provided with bag (a set of instances) label information. However, most previous MIL methods on this issue ignore the global similarity among positive instances and that negative instances are non-i.i.d., usually resulting in the detection of TPI not precise and sensitive to outliers. To the end, we propose a positive instance detection via graph updating for multiple instance learning, called PIGMIL, to detect TPI accurately. PIGMIL selects instances from working sets (WSs) of some working bags (WBs) as positive candidate pool (PCP). The global similarity among positive instances and the robust discrimination of instances of PCP from negative instances are measured to construct the consistent similarity and discrimination graph (CSDG). As a result, the primary goal (i.e. TPI detection) is transformed into PCP updating, which is approximated efficiently by updating CSDG with a random walk ranking algorithm and an instance updating strategy. At last bags are transformed into feature representation vector based on the identified TPIs to train a classifier. Extensive experiments demonstrate the high precision of PIGMIL's detection of TPIs and its excellent performance compared to classic baseline MIL methods.



### Text-guided Attention Model for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1612.03557v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.03557v1)
- **Published**: 2016-12-12 06:52:36+00:00
- **Updated**: 2016-12-12 06:52:36+00:00
- **Authors**: Jonghwan Mun, Minsu Cho, Bohyung Han
- **Comment**: None
- **Journal**: None
- **Summary**: Visual attention plays an important role to understand images and demonstrates its effectiveness in generating natural language descriptions of images. On the other hand, recent studies show that language associated with an image can steer visual attention in the scene during our cognitive process. Inspired by this, we introduce a text-guided attention model for image captioning, which learns to drive visual attention using associated captions. For this model, we propose an exemplar-based learning approach that retrieves from training data associated captions with each image, and use them to learn attention on visual features. Our attention model enables to describe a detailed state of scenes by distinguishing small or confusable objects effectively. We validate our model on MS-COCO Captioning benchmark and achieve the state-of-the-art performance in standard metrics.



### Statistics of Visual Responses to Object Stimuli from Primate AIT Neurons to DNN Neurons
- **Arxiv ID**: http://arxiv.org/abs/1612.03590v2
- **DOI**: 10.1162/NECO_a_01039
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1612.03590v2)
- **Published**: 2016-12-12 10:13:15+00:00
- **Updated**: 2017-01-20 02:23:42+00:00
- **Authors**: Qiulei Dong, Zhanyi Hu
- **Comment**: None
- **Journal**: Neural Computation, 30, 447-476 (2018)
- **Summary**: Cadieu et al. (Cadieu,2014) reported that deep neural networks(DNNs) could rival the representation of primate inferotemporal cortex for object recognition. Lehky et al. (Lehky,2011) provided a statistical analysis on neural responses to object stimuli in primate AIT cortex. They found the intrinsic dimensionality of object representations in AIT cortex is around 100 (Lehky,2014). Considering the outstanding performance of DNNs in object recognition, it is worthwhile investigating whether the responses of DNN neurons have similar response statistics to those of AIT neurons. Following Lehky et al.'s works, we analyze the response statistics to image stimuli and the intrinsic dimensionality of object representations of DNN neurons. Our findings show in terms of kurtosis and Pareto tail index, the response statistics on single-neuron selectivity and population sparseness of DNN neurons are fundamentally different from those of IT neurons except some special cases. By increasing the number of neurons and stimuli, the conclusions could alter substantially. In addition, with the ascendancy of the convolutional layers of DNNs, the single-neuron selectivity and population sparseness of DNN neurons increase, indicating the last convolutional layer is to learn features for object representations, while the following fully-connected layers are to learn categorization features. It is also found that a sufficiently large number of stimuli and neurons are necessary for obtaining a stable dimensionality. To our knowledge, this is the first work to analyze the response statistics of DNN neurons comparing with AIT neurons, and our results provide not only some insights into the discrepancy of DNN neurons with respect to IT neurons in object representation, but also shed some light on possible outcomes of IT neurons when the number of recorded neurons and stimuli is beyond the level in (Lehky,2011,2014).



### VIBIKNet: Visual Bidirectional Kernelized Network for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1612.03628v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1612.03628v1)
- **Published**: 2016-12-12 11:41:46+00:00
- **Updated**: 2016-12-12 11:41:46+00:00
- **Authors**: Marc Bolaños, Álvaro Peris, Francisco Casacuberta, Petia Radeva
- **Comment**: Submitted to IbPRIA'17, 8 pages, 3 figures, 1 table
- **Journal**: None
- **Summary**: In this paper, we address the problem of visual question answering by proposing a novel model, called VIBIKNet. Our model is based on integrating Kernelized Convolutional Neural Networks and Long-Short Term Memory units to generate an answer given a question about an image. We prove that VIBIKNet is an optimal trade-off between accuracy and computational load, in terms of memory and time consumption. We validate our method on the VQA challenge dataset and compare it to the top performing methods in order to illustrate its performance and speed.



### A Binary Convolutional Encoder-decoder Network for Real-time Natural Scene Text Processing
- **Arxiv ID**: http://arxiv.org/abs/1612.03630v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.03630v1)
- **Published**: 2016-12-12 11:48:00+00:00
- **Updated**: 2016-12-12 11:48:00+00:00
- **Authors**: Zichuan Liu, Yixing Li, Fengbo Ren, Hao Yu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we develop a binary convolutional encoder-decoder network (B-CEDNet) for natural scene text processing (NSTP). It converts a text image to a class-distinguished salience map that reveals the categorical, spatial and morphological information of characters. The existing solutions are either memory consuming or run-time consuming that cannot be applied to real-time applications on resource-constrained devices such as advanced driver assistance systems. The developed network can process multiple regions containing characters by one-off forward operation, and is trained to have binary weights and binary feature maps, which lead to both remarkable inference run-time speedup and memory usage reduction. By training with over 200, 000 synthesis scene text images (size of $32\times128$), it can achieve $90\%$ and $91\%$ pixel-wise accuracy on ICDAR-03 and ICDAR-13 datasets. It only consumes $4.59\ ms$ inference run-time realized on GPU with a small network size of 2.14 MB, which is up to $8\times$ faster and $96\%$ smaller than it full-precision version.



### Analysis and Optimization of Loss Functions for Multiclass, Top-k, and Multilabel Classification
- **Arxiv ID**: http://arxiv.org/abs/1612.03663v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1612.03663v1)
- **Published**: 2016-12-12 13:20:09+00:00
- **Updated**: 2016-12-12 13:20:09+00:00
- **Authors**: Maksim Lapin, Matthias Hein, Bernt Schiele
- **Comment**: None
- **Journal**: None
- **Summary**: Top-k error is currently a popular performance measure on large scale image classification benchmarks such as ImageNet and Places. Despite its wide acceptance, our understanding of this metric is limited as most of the previous research is focused on its special case, the top-1 error. In this work, we explore two directions that shed more light on the top-k error. First, we provide an in-depth analysis of established and recently proposed single-label multiclass methods along with a detailed account of efficient optimization algorithms for them. Our results indicate that the softmax loss and the smooth multiclass SVM are surprisingly competitive in top-k error uniformly across all k, which can be explained by our analysis of multiclass top-k calibration. Further improvements for a specific k are possible with a number of proposed top-k loss functions. Second, we use the top-k methods to explore the transition from multiclass to multilabel learning. In particular, we find that it is possible to obtain effective multilabel classifiers on Pascal VOC using a single label per image for training, while the gap between multiclass and multilabel methods on MS COCO is more significant. Finally, our contribution of efficient algorithms for training with the considered top-k and multilabel loss functions is of independent interest.



### Segmentation of large images based on super-pixels and community detection in graphs
- **Arxiv ID**: http://arxiv.org/abs/1612.03705v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1612.03705v1)
- **Published**: 2016-12-12 14:31:29+00:00
- **Updated**: 2016-12-12 14:31:29+00:00
- **Authors**: Oscar A. C. Linares, Glenda Michele Botelho, Francisco Aparecido Rodrigues, João Batista Neto
- **Comment**: 20 pages, 12 figures
- **Journal**: None
- **Summary**: Image segmentation has many applications which range from machine learning to medical diagnosis. In this paper, we propose a framework for the segmentation of images based on super-pixels and algorithms for community identification in graphs. The super-pixel pre-segmentation step reduces the number of nodes in the graph, rendering the method the ability to process large images. Moreover, community detection algorithms provide more accurate segmentation than traditional approaches, such as those based on spectral graph partition. We also compare our method with two algorithms: a) the graph-based approach by Felzenszwalb and Huttenlocher and b) the contour-based method by Arbelaez. Results have shown that our method provides more precise segmentation and is faster than both of them.



### COCO-Stuff: Thing and Stuff Classes in Context
- **Arxiv ID**: http://arxiv.org/abs/1612.03716v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.03716v4)
- **Published**: 2016-12-12 14:46:23+00:00
- **Updated**: 2018-03-28 10:15:59+00:00
- **Authors**: Holger Caesar, Jasper Uijlings, Vittorio Ferrari
- **Comment**: CVPR 2018 camera-ready
- **Journal**: None
- **Summary**: Semantic classes can be either things (objects with a well-defined shape, e.g. car, person) or stuff (amorphous background regions, e.g. grass, sky). While lots of classification and detection works focus on thing classes, less attention has been given to stuff classes. Nonetheless, stuff classes are important as they allow to explain important aspects of an image, including (1) scene type; (2) which thing classes are likely to be present and their location (through contextual reasoning); (3) physical attributes, material types and geometric properties of the scene. To understand stuff and things in context we introduce COCO-Stuff, which augments all 164K images of the COCO 2017 dataset with pixel-wise annotations for 91 stuff classes. We introduce an efficient stuff annotation protocol based on superpixels, which leverages the original thing annotations. We quantify the speed versus quality trade-off of our protocol and explore the relation between annotation time and boundary complexity. Furthermore, we use COCO-Stuff to analyze: (a) the importance of stuff and thing classes in terms of their surface cover and how frequently they are mentioned in image captions; (b) the spatial relations between stuff and things, highlighting the rich contextual relations that make our dataset unique; (c) the performance of a modern semantic segmentation method on stuff and thing classes, and whether stuff is easier to segment than things.



### Hybrid Learning of Optical Flow and Next Frame Prediction to Boost Optical Flow in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1612.03777v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.03777v2)
- **Published**: 2016-12-12 16:45:08+00:00
- **Updated**: 2017-04-07 13:01:12+00:00
- **Authors**: Nima Sedaghat, Mohammadreza Zolfaghari, Thomas Brox
- **Comment**: None
- **Journal**: None
- **Summary**: CNN-based optical flow estimation has attracted attention recently, mainly due to its impressively high frame rates. These networks perform well on synthetic datasets, but they are still far behind the classical methods in real-world videos. This is because there is no ground truth optical flow for training these networks on real data. In this paper, we boost CNN-based optical flow estimation in real scenes with the help of the freely available self-supervised task of next-frame prediction. To this end, we train the network in a hybrid way, providing it with a mixture of synthetic and real videos. With the help of a sample-variant multi-tasking architecture, the network is trained on different tasks depending on the availability of ground-truth. We also experiment with the prediction of "next-flow" instead of estimation of the current flow, which is intuitively closer to the task of next-frame prediction and yields favorable results. We demonstrate the improvement in optical flow estimation on the real-world KITTI benchmark. Additionally, we test the optical flow indirectly in an action classification scenario. As a side product of this work, we report significant improvements over state-of-the-art in the task of next-frame prediction.



### PoseAgent: Budget-Constrained 6D Object Pose Estimation via Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1612.03779v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.03779v2)
- **Published**: 2016-12-12 16:50:47+00:00
- **Updated**: 2017-04-11 08:24:22+00:00
- **Authors**: Alexander Krull, Eric Brachmann, Sebastian Nowozin, Frank Michel, Jamie Shotton, Carsten Rother
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art computer vision algorithms often achieve efficiency by making discrete choices about which hypotheses to explore next. This allows allocation of computational resources to promising candidates, however, such decisions are non-differentiable. As a result, these algorithms are hard to train in an end-to-end fashion. In this work we propose to learn an efficient algorithm for the task of 6D object pose estimation. Our system optimizes the parameters of an existing state-of-the art pose estimation system using reinforcement learning, where the pose estimation system now becomes the stochastic policy, parametrized by a CNN. Additionally, we present an efficient training algorithm that dramatically reduces computation time. We show empirically that our learned pose estimation procedure makes better use of limited resources and improves upon the state-of-the-art on a challenging dataset. Our approach enables differentiable end-to-end training of complex algorithmic pipelines and learns to make optimal use of a given computational budget.



### Generalizable Features From Unsupervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1612.03809v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.03809v1)
- **Published**: 2016-12-12 17:45:48+00:00
- **Updated**: 2016-12-12 17:45:48+00:00
- **Authors**: Mehdi Mirza, Aaron Courville, Yoshua Bengio
- **Comment**: None
- **Journal**: None
- **Summary**: Humans learn a predictive model of the world and use this model to reason about future events and the consequences of actions. In contrast to most machine predictors, we exhibit an impressive ability to generalize to unseen scenarios and reason intelligently in these settings. One important aspect of this ability is physical intuition(Lake et al., 2016). In this work, we explore the potential of unsupervised learning to find features that promote better generalization to settings outside the supervised training distribution. Our task is predicting the stability of towers of square blocks. We demonstrate that an unsupervised model, trained to predict future frames of a video sequence of stable and unstable block configurations, can yield features that support extrapolating stability prediction to blocks configurations outside the training set distribution



### Inverse Compositional Spatial Transformer Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.03897v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.03897v1)
- **Published**: 2016-12-12 20:53:05+00:00
- **Updated**: 2016-12-12 20:53:05+00:00
- **Authors**: Chen-Hsuan Lin, Simon Lucey
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we establish a theoretical connection between the classical Lucas & Kanade (LK) algorithm and the emerging topic of Spatial Transformer Networks (STNs). STNs are of interest to the vision and learning communities due to their natural ability to combine alignment and classification within the same theoretical framework. Inspired by the Inverse Compositional (IC) variant of the LK algorithm, we present Inverse Compositional Spatial Transformer Networks (IC-STNs). We demonstrate that IC-STNs can achieve better performance than conventional STNs with less model capacity; in particular, we show superior performance in pure image alignment tasks as well as joint alignment/classification problems on real-world problems.



### Deep Supervised Hashing with Triplet Labels
- **Arxiv ID**: http://arxiv.org/abs/1612.03900v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.03900v1)
- **Published**: 2016-12-12 20:56:38+00:00
- **Updated**: 2016-12-12 20:56:38+00:00
- **Authors**: Xiaofang Wang, Yi Shi, Kris M. Kitani
- **Comment**: Appear in ACCV 2016
- **Journal**: None
- **Summary**: Hashing is one of the most popular and powerful approximate nearest neighbor search techniques for large-scale image retrieval. Most traditional hashing methods first represent images as off-the-shelf visual features and then produce hashing codes in a separate stage. However, off-the-shelf visual features may not be optimally compatible with the hash code learning procedure, which may result in sub-optimal hash codes. Recently, deep hashing methods have been proposed to simultaneously learn image features and hash codes using deep neural networks and have shown superior performance over traditional hashing methods. Most deep hashing methods are given supervised information in the form of pairwise labels or triplet labels. The current state-of-the-art deep hashing method DPSH~\cite{li2015feature}, which is based on pairwise labels, performs image feature learning and hash code learning simultaneously by maximizing the likelihood of pairwise similarities. Inspired by DPSH~\cite{li2015feature}, we propose a triplet label based deep hashing method which aims to maximize the likelihood of the given triplet labels. Experimental results show that our method outperforms all the baselines on CIFAR-10 and NUS-WIDE datasets, including the state-of-the-art method DPSH~\cite{li2015feature} and all the previous triplet label based deep hashing methods.



### 3D fully convolutional networks for subcortical segmentation in MRI: A large-scale study
- **Arxiv ID**: http://arxiv.org/abs/1612.03925v2
- **DOI**: 10.1016/j.neuroimage.2017.04.039
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.03925v2)
- **Published**: 2016-12-12 21:09:06+00:00
- **Updated**: 2017-04-20 02:03:35+00:00
- **Authors**: J. Dolz, C. Desrosiers, I. Ben Ayed
- **Comment**: Accepted in the special issue of Neuroimage: "Brain Segmentation and
  Parcellation"
- **Journal**: None
- **Summary**: This study investigates a 3D and fully convolutional neural network (CNN) for subcortical brain structure segmentation in MRI. 3D CNN architectures have been generally avoided due to their computational and memory requirements during inference. We address the problem via small kernels, allowing deeper architectures. We further model both local and global context by embedding intermediate-layer outputs in the final prediction, which encourages consistency between features extracted at different scales and embeds fine-grained information directly in the segmentation process. Our model is efficiently trained end-to-end on a graphics processing unit (GPU), in a single stage, exploiting the dense inference capabilities of fully CNNs.   We performed comprehensive experiments over two publicly available datasets. First, we demonstrate a state-of-the-art performance on the ISBR dataset. Then, we report a {\em large-scale} multi-site evaluation over 1112 unregistered subject datasets acquired from 17 different sites (ABIDE dataset), with ages ranging from 7 to 64 years, showing that our method is robust to various acquisition protocols, demographics and clinical factors. Our method yielded segmentations that are highly consistent with a standard atlas-based approach, while running in a fraction of the time needed by atlas-based methods and avoiding registration/normalization steps. This makes it convenient for massive multi-site neuroanatomical imaging studies. To the best of our knowledge, our work is the first to study subcortical structure segmentation on such large-scale and heterogeneous data.



### Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer
- **Arxiv ID**: http://arxiv.org/abs/1612.03928v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.03928v3)
- **Published**: 2016-12-12 21:15:57+00:00
- **Updated**: 2017-02-12 22:05:47+00:00
- **Authors**: Sergey Zagoruyko, Nikos Komodakis
- **Comment**: None
- **Journal**: None
- **Summary**: Attention plays a critical role in human visual experience. Furthermore, it has recently been demonstrated that attention can also play an important role in the context of applying artificial neural networks to a variety of tasks from fields such as computer vision and NLP. In this work we show that, by properly defining attention for convolutional neural networks, we can actually use this type of information in order to significantly improve the performance of a student CNN network by forcing it to mimic the attention maps of a powerful teacher network. To that end, we propose several novel methods of transferring attention, showing consistent improvement across a variety of datasets and convolutional neural network architectures. Code and models for our experiments are available at https://github.com/szagoruyko/attention-transfer



### Autoencoder-based holographic image restoration
- **Arxiv ID**: http://arxiv.org/abs/1612.03959v1
- **DOI**: 10.1364/AO.56.000F27
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1612.03959v1)
- **Published**: 2016-12-12 22:49:03+00:00
- **Updated**: 2016-12-12 22:49:03+00:00
- **Authors**: Tomoyoshi Shimobaba, Yutaka Endo, Ryuji Hirayama, Yuki Nagahama, Takayuki Takahashi, Takashi Nishitsuji, Takashi Kakue, Atsushi Shiraki, Naoki Takada, Nobuyuki Masuda, Tomoyoshi Ito
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a holographic image restoration method using an autoencoder, which is an artificial neural network. Because holographic reconstructed images are often contaminated by direct light, conjugate light, and speckle noise, the discrimination of reconstructed images may be difficult. In this paper, we demonstrate the restoration of reconstructed images from holograms that record page data in holographic memory and QR codes by using the proposed method.



### Neural Networks with Manifold Learning for Diabetic Retinopathy Detection
- **Arxiv ID**: http://arxiv.org/abs/1612.03961v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.03961v1)
- **Published**: 2016-12-12 22:51:17+00:00
- **Updated**: 2016-12-12 22:51:17+00:00
- **Authors**: Arjun Raj Rajanna, Kamelia Aryafar, Rajeev Ramchandran, Christye Sisson, Ali Shokoufandeh, Raymond Ptucha
- **Comment**: Published in Proceedings of "IEEE Western NY Image & Signal
  Processing Workshop"
- **Journal**: None
- **Summary**: Widespread outreach programs using remote retinal imaging have proven to decrease the risk from diabetic retinopathy, the leading cause of blindness in the US. However, this process still requires manual verification of image quality and grading of images for level of disease by a trained human grader and will continue to be limited by the lack of such scarce resources. Computer-aided diagnosis of retinal images have recently gained increasing attention in the machine learning community. In this paper, we introduce a set of neural networks for diabetic retinopathy classification of fundus retinal images. We evaluate the efficiency of the proposed classifiers in combination with preprocessing and augmentation steps on a sample dataset. Our experimental results show that neural networks in combination with preprocessing on the images can boost the classification accuracy on this dataset. Moreover the proposed models are scalable and can be used in large scale datasets for diabetic retinopathy detection. The models introduced in this paper can be used to facilitate the diagnosis and speed up the detection process.



