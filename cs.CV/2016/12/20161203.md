# Arxiv Papers in cs.CV on 2016-12-03
### Commonly Uncommon: Semantic Sparsity in Situation Recognition
- **Arxiv ID**: http://arxiv.org/abs/1612.00901v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1612.00901v1)
- **Published**: 2016-12-03 00:31:52+00:00
- **Updated**: 2016-12-03 00:31:52+00:00
- **Authors**: Mark Yatskar, Vicente Ordonez, Luke Zettlemoyer, Ali Farhadi
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic sparsity is a common challenge in structured visual classification problems; when the output space is complex, the vast majority of the possible predictions are rarely, if ever, seen in the training set. This paper studies semantic sparsity in situation recognition, the task of producing structured summaries of what is happening in images, including activities, objects and the roles objects play within the activity. For this problem, we find empirically that most object-role combinations are rare, and current state-of-the-art models significantly underperform in this sparse data regime. We avoid many such errors by (1) introducing a novel tensor composition function that learns to share examples across role-noun combinations and (2) semantically augmenting our training data with automatically gathered examples of rarely observed outputs using web data. When integrated within a complete CRF-based structured prediction model, the tensor-based approach outperforms existing state of the art by a relative improvement of 2.11% and 4.40% on top-5 verb and noun-role accuracy, respectively. Adding 5 million images with our semantic augmentation techniques gives further relative improvements of 6.23% and 9.57% on top-5 verb and noun-role accuracy.



### End-to-end learning of brain tissue segmentation from imperfect labeling
- **Arxiv ID**: http://arxiv.org/abs/1612.00940v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00940v2)
- **Published**: 2016-12-03 08:43:33+00:00
- **Updated**: 2017-06-05 20:14:37+00:00
- **Authors**: Alex Fedorov, Jeremy Johnson, Eswar Damaraju, Alexei Ozerin, Vince Calhoun, Sergey Plis
- **Comment**: Published as a conference paper at IJCNN 2017 Preprint version
- **Journal**: None
- **Summary**: Segmenting a structural magnetic resonance imaging (MRI) scan is an important pre-processing step for analytic procedures and subsequent inferences about longitudinal tissue changes. Manual segmentation defines the current gold standard in quality but is prohibitively expensive. Automatic approaches are computationally intensive, incredibly slow at scale, and error prone due to usually involving many potentially faulty intermediate steps. In order to streamline the segmentation, we introduce a deep learning model that is based on volumetric dilated convolutions, subsequently reducing both processing time and errors. Compared to its competitors, the model has a reduced set of parameters and thus is easier to train and much faster to execute. The contrast in performance between the dilated network and its competitors becomes obvious when both are tested on a large dataset of unprocessed human brain volumes. The dilated network consistently outperforms not only another state-of-the-art deep learning approach, the up convolutional network, but also the ground truth on which it was trained. Not only can the incredible speed of our model make large scale analyses much easier but we also believe it has great potential in a clinical setting where, with little to no substantial delay, a patient and provider can go over test results.



### Semi-supervised learning of deep metrics for stereo reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1612.00979v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1612.00979v1)
- **Published**: 2016-12-03 16:09:32+00:00
- **Updated**: 2016-12-03 16:09:32+00:00
- **Authors**: Stepan Tulyakov, Anton Ivanov, Francois Fleuret
- **Comment**: 11 pages, 3 figures
- **Journal**: None
- **Summary**: Deep-learning metrics have recently demonstrated extremely good performance to match image patches for stereo reconstruction. However, training such metrics requires large amount of labeled stereo images, which can be difficult or costly to collect for certain applications. The main contribution of our work is a new semi-supervised method for learning deep metrics from unlabeled stereo images, given coarse information about the scenes and the optical system. Our method alternatively optimizes the metric with a standard stochastic gradient descent, and applies stereo constraints to regularize its prediction. Experiments on reference data-sets show that, for a given network architecture, training with this new method without ground-truth produces a metric with performance as good as state-of-the-art baselines trained with the said ground-truth. This work has three practical implications. Firstly, it helps to overcome limitations of training sets, in particular noisy ground truth. Secondly it allows to use much more training data during learning. Thirdly, it allows to tune deep metric for a particular stereo system, even if ground truth is not available.



### Food Image Recognition by Using Convolutional Neural Networks (CNNs)
- **Arxiv ID**: http://arxiv.org/abs/1612.00983v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00983v2)
- **Published**: 2016-12-03 16:22:59+00:00
- **Updated**: 2019-02-25 15:22:55+00:00
- **Authors**: Yuzhen Lu
- **Comment**: 6 pages, 5 figures, 3 tables
- **Journal**: None
- **Summary**: Food image recognition is one of the promising applications of visual object recognition in computer vision. In this study, a small-scale dataset consisting of 5822 images of ten categories and a five-layer CNN was constructed to recognize these images. The bag-of-features (BoF) model coupled with support vector machine (SVM) was first evaluated for image classification, resulting in an overall accuracy of 56%; while the CNN model performed much better with an overall accuracy of 74%. Data augmentation techniques based on geometric transformation were applied to increase the size of training images, which achieved a significantly improved accuracy of more than 90% while preventing the overfitting issue that occurred to the CNN based on raw training data. Further improvements can be expected by collecting more images and optimizing the network architecture and hyper-parameters.



### Deep Learning with Energy-efficient Binary Gradient Cameras
- **Arxiv ID**: http://arxiv.org/abs/1612.00986v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00986v1)
- **Published**: 2016-12-03 17:04:52+00:00
- **Updated**: 2016-12-03 17:04:52+00:00
- **Authors**: Suren Jayasuriya, Orazio Gallo, Jinwei Gu, Jan Kautz
- **Comment**: None
- **Journal**: None
- **Summary**: Power consumption is a critical factor for the deployment of embedded computer vision systems. We explore the use of computational cameras that directly output binary gradient images to reduce the portion of the power consumption allocated to image sensing. We survey the accuracy of binary gradient cameras on a number of computer vision tasks using deep learning. These include object recognition, head pose regression, face detection, and gesture recognition. We show that, for certain applications, accuracy can be on par or even better than what can be achieved on traditional images. We are also the first to recover intensity information from binary spatial gradient images--useful for applications with a human observer in the loop, such as surveillance. Our results, which we validate with a prototype binary gradient camera, point to the potential of gradient-based computer vision systems.



### Ensembles of Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.00991v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00991v1)
- **Published**: 2016-12-03 17:49:02+00:00
- **Updated**: 2016-12-03 17:49:02+00:00
- **Authors**: Yaxing Wang, Lichao Zhang, Joost van de Weijer
- **Comment**: accepted NIPS 2016 Workshop on Adversarial Training
- **Journal**: None
- **Summary**: Ensembles are a popular way to improve results of discriminative CNNs. The combination of several networks trained starting from different initializations improves results significantly. In this paper we investigate the usage of ensembles of GANs. The specific nature of GANs opens up several new ways to construct ensembles. The first one is based on the fact that in the minimax game which is played to optimize the GAN objective the generator network keeps on changing even after the network can be considered optimal. As such ensembles of GANs can be constructed based on the same network initialization but just taking models which have different amount of iterations. These so-called self ensembles are much faster to train than traditional ensembles. The second method, called cascade GANs, redirects part of the training data which is badly modeled by the first GAN to another GAN. In experiments on the CIFAR10 dataset we show that ensembles of GANs obtain model probability distributions which better model the data distribution. In addition, we show that these improved results can be obtained at little additional computational cost.



### Mining Spatio-temporal Data on Industrialization from Historical Registries
- **Arxiv ID**: http://arxiv.org/abs/1612.00992v1
- **DOI**: 10.3808/jei.201700381
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1612.00992v1)
- **Published**: 2016-12-03 17:54:03+00:00
- **Updated**: 2016-12-03 17:54:03+00:00
- **Authors**: David Berenbaum, Dwyer Deighan, Thomas Marlow, Ashley Lee, Scott Frickel, Mark Howison
- **Comment**: None
- **Journal**: Journal of Environmental Informatics 34(1): 28-34 (2019)
- **Summary**: Despite the growing availability of big data in many fields, historical data on socioevironmental phenomena are often not available due to a lack of automated and scalable approaches for collecting, digitizing, and assembling them. We have developed a data-mining method for extracting tabulated, geocoded data from printed directories. While scanning and optical character recognition (OCR) can digitize printed text, these methods alone do not capture the structure of the underlying data. Our pipeline integrates both page layout analysis and OCR to extract tabular, geocoded data from structured text. We demonstrate the utility of this method by applying it to scanned manufacturing registries from Rhode Island that record 41 years of industrial land use. The resulting spatio-temporal data can be used for socioenvironmental analyses of industrialization at a resolution that was not previously possible. In particular, we find strong evidence for the dispersion of manufacturing from the urban core of Providence, the state's capital, along the Interstate 95 corridor to the north and south.



### A Non-Local Means Approach for Gaussian Noise Removal from Images using a Modified Weighting Kernel
- **Arxiv ID**: http://arxiv.org/abs/1612.01006v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01006v1)
- **Published**: 2016-12-03 19:00:18+00:00
- **Updated**: 2016-12-03 19:00:18+00:00
- **Authors**: Mojtaba Kazemi, Ehsan Mohammadi. P, Parichehr shahidi sadeghi, Mohamad B. Menhaj
- **Comment**: 2017 25th Iranian Conference on Electrical Engineering (ICEE)
- **Journal**: None
- **Summary**: Gaussian noise removal is an interesting area in digital image processing not only to improve the visual quality, but for its impact on other post-processing algorithms like image registration or segmentation. Many presented state-of-the-art denoising methods are based on the self-similarity or patch-based image processing. Specifically, Non-Local Means (NLM) as a patch-based filter has gained increasing attention in recent years. Essentially, this filter tends to obtain the noise-less signal value by computing the Gaussian-weighted Euclidean distance between the patch under-processing and other patches inside the image. However, the NLM filter is sensitive to the outliers (pixels that their intensity values are far away from other pixels) inside the patch, meaning that the pixels with the symmetric locations in the patch are assigned the same weight. This can lead to sub-optimal denoising performance when the destructive nature of noise generates some outliers inside patches. In this paper, we propose a new weighting approach to modify the Gaussian kernel of the NLM filter. Our approach employs the geometric distance between image intensities to come up with new weights for each pixel of a patch, lowering the impact of outliers on the denoising performance. Experiments on a set of standard images and different noise levels show that our proposed method outperforms the other compared denoising filters.



### Short-term traffic flow forecasting with spatial-temporal correlation in a hybrid deep learning framework
- **Arxiv ID**: http://arxiv.org/abs/1612.01022v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01022v1)
- **Published**: 2016-12-03 21:30:26+00:00
- **Updated**: 2016-12-03 21:30:26+00:00
- **Authors**: Yuankai Wu, Huachun Tan
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Deep learning approaches have reached a celebrity status in artificial intelligence field, its success have mostly relied on Convolutional Networks (CNN) and Recurrent Networks. By exploiting fundamental spatial properties of images and videos, the CNN always achieves dominant performance on visual tasks. And the Recurrent Networks (RNN) especially long short-term memory methods (LSTM) can successfully characterize the temporal correlation, thus exhibits superior capability for time series tasks. Traffic flow data have plentiful characteristics on both time and space domain. However, applications of CNN and LSTM approaches on traffic flow are limited. In this paper, we propose a novel deep architecture combined CNN and LSTM to forecast future traffic flow (CLTFP). An 1-dimension CNN is exploited to capture spatial features of traffic flow, and two LSTMs are utilized to mine the short-term variability and periodicities of traffic flow. Given those meaningful features, the feature-level fusion is performed to achieve short-term forecasting. The proposed CLTFP is compared with other popular forecasting methods on an open datasets. Experimental results indicate that the CLTFP has considerable advantages in traffic flow forecasting. in additional, the proposed CLTFP is analyzed from the view of Granger Causality, and several interesting properties of CLTFP are discovered and discussed .



### Areas of Attention for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1612.01033v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01033v2)
- **Published**: 2016-12-03 23:01:36+00:00
- **Updated**: 2017-08-25 14:36:01+00:00
- **Authors**: Marco Pedersoli, Thomas Lucas, Cordelia Schmid, Jakob Verbeek
- **Comment**: Accepted in ICCV 2017
- **Journal**: None
- **Summary**: We propose "Areas of Attention", a novel attention-based model for automatic image captioning. Our approach models the dependencies between image regions, caption words, and the state of an RNN language model, using three pairwise interactions. In contrast to previous attention-based approaches that associate image regions only to the RNN state, our method allows a direct association between caption words and image regions. During training these associations are inferred from image-level captions, akin to weakly-supervised object detector training. These associations help to improve captioning by localizing the corresponding regions during testing. We also propose and compare different ways of generating attention areas: CNN activation grids, object proposals, and spatial transformers nets applied in a convolutional fashion. Spatial transformers give the best results. They allow for image specific attention areas, and can be trained jointly with the rest of the network. Our attention mechanism and spatial transformer attention areas together yield state-of-the-art results on the MSCOCO dataset.o meaningful latent semantic structure in the generated captions.



### Semi-Automated Annotation of Discrete States in Large Video Datasets
- **Arxiv ID**: http://arxiv.org/abs/1612.01035v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01035v1)
- **Published**: 2016-12-03 23:40:14+00:00
- **Updated**: 2016-12-03 23:40:14+00:00
- **Authors**: Lex Fridman, Bryan Reimer
- **Comment**: To be presented at AAAI 2017. arXiv admin note: text overlap with
  arXiv:1508.04028
- **Journal**: None
- **Summary**: We propose a framework for semi-automated annotation of video frames where the video is of an object that at any point in time can be labeled as being in one of a finite number of discrete states. A Hidden Markov Model (HMM) is used to model (1) the behavior of the underlying object and (2) the noisy observation of its state through an image processing algorithm. The key insight of this approach is that the annotation of frame-by-frame video can be reduced from a problem of labeling every single image to a problem of detecting a transition between states of the underlying objected being recording on video. The performance of the framework is evaluated on a driver gaze classification dataset composed of 16,000,000 images that were fully annotated over 6,000 hours of direct manual annotation labor. On this dataset, we achieve a 13x reduction in manual annotation for an average accuracy of 99.1% and a 84x reduction for an average accuracy of 91.2%.



