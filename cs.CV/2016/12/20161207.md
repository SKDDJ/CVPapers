# Arxiv Papers in cs.CV on 2016-12-07
### ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events
- **Arxiv ID**: http://arxiv.org/abs/1612.02095v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1612.02095v2)
- **Published**: 2016-12-07 01:46:09+00:00
- **Updated**: 2017-11-25 23:44:46+00:00
- **Authors**: Evan Racah, Christopher Beckham, Tegan Maharaj, Samira Ebrahimi Kahou, Prabhat, Christopher Pal
- **Comment**: None
- **Journal**: None
- **Summary**: Then detection and identification of extreme weather events in large-scale climate simulations is an important problem for risk management, informing governmental policy decisions and advancing our basic understanding of the climate system. Recent work has shown that fully supervised convolutional neural networks (CNNs) can yield acceptable accuracy for classifying well-known types of extreme weather events when large amounts of labeled data are available. However, many different types of spatially localized climate patterns are of interest including hurricanes, extra-tropical cyclones, weather fronts, and blocking events among others. Existing labeled data for these patterns can be incomplete in various ways, such as covering only certain years or geographic areas and having false negatives. This type of climate data therefore poses a number of interesting machine learning challenges. We present a multichannel spatiotemporal CNN architecture for semi-supervised bounding box prediction and exploratory data analysis. We demonstrate that our approach is able to leverage temporal information and unlabeled data to improve the localization of extreme weather events. Further, we explore the representations learned by our model in order to better understand this important data. We present a dataset, ExtremeWeather, to encourage machine learning research in this area and to help facilitate further work in understanding and mitigating the effects of climate change. The dataset is available at extremeweatherdataset.github.io and the code is available at https://github.com/eracah/hur-detect.



### Bottom-Up Top-Down Cues for Weakly-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1612.02101v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02101v3)
- **Published**: 2016-12-07 02:37:51+00:00
- **Updated**: 2017-04-09 08:32:03+00:00
- **Authors**: Qinbin Hou, Puneet Kumar Dokania, Daniela Massiceti, Yunchao Wei, Ming-Ming Cheng, Philip Torr
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the task of learning a classifier for semantic segmentation using weak supervision in the form of image labels which specify the object classes present in the image. Our method uses deep convolutional neural networks (CNNs) and adopts an Expectation-Maximization (EM) based approach. We focus on the following three aspects of EM: (i) initialization; (ii) latent posterior estimation (E-step) and (iii) the parameter update (M-step). We show that saliency and attention maps, our bottom-up and top-down cues respectively, of simple images provide very good cues to learn an initialization for the EM-based algorithm. Intuitively, we show that before trying to learn to segment complex images, it is much easier and highly effective to first learn to segment a set of simple images and then move towards the complex ones. Next, in order to update the parameters, we propose minimizing the combination of the standard softmax loss and the KL divergence between the true latent posterior and the likelihood given by the CNN. We argue that this combination is more robust to wrong predictions made by the expectation step of the EM method. We support this argument with empirical and visual results. Extensive experiments and discussions show that: (i) our method is very simple and intuitive; (ii) requires only image-level labels; and (iii) consistently outperforms other weakly-supervised state-of-the-art methods with a very high margin on the PASCAL VOC 2012 dataset.



### Richer Convolutional Features for Edge Detection
- **Arxiv ID**: http://arxiv.org/abs/1612.02103v3
- **DOI**: 10.1109/TPAMI.2018.2878849
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02103v3)
- **Published**: 2016-12-07 02:57:03+00:00
- **Updated**: 2019-07-03 04:44:45+00:00
- **Authors**: Yun Liu, Ming-Ming Cheng, Xiaowei Hu, Kai Wang, Xiang Bai
- **Comment**: None
- **Journal**: IEEE TPAMI, 41(8):1939-1946, 2019
- **Summary**: In this paper, we propose an accurate edge detector using richer convolutional features (RCF). Since objects in nature images have various scales and aspect ratios, the automatically learned rich hierarchical representations by CNNs are very critical and effective to detect edges and object boundaries. And the convolutional features gradually become coarser with receptive fields increasing. Based on these observations, our proposed network architecture makes full use of multiscale and multi-level information to perform the image-to-image edge prediction by combining all of the useful convolutional features into a holistic framework. It is the first attempt to adopt such rich convolutional features in computer vision tasks. Using VGG16 network, we achieve \sArt results on several available datasets. When evaluating on the well-known BSDS500 benchmark, we achieve ODS F-measure of \textbf{.811} while retaining a fast speed (\textbf{8} FPS). Besides, our fast version of RCF achieves ODS F-measure of \textbf{.806} with \textbf{30} FPS.



### Mode Regularized Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.02136v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1612.02136v5)
- **Published**: 2016-12-07 07:45:38+00:00
- **Updated**: 2017-03-02 06:28:13+00:00
- **Authors**: Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, Wenjie Li
- **Comment**: Published as a conference paper at ICLR 2017
- **Journal**: None
- **Summary**: Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem.



### Learning Localized Geometric Features Using 3D-CNN: An Application to Manufacturability Analysis of Drilled Holes
- **Arxiv ID**: http://arxiv.org/abs/1612.02141v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1612.02141v2)
- **Published**: 2016-12-07 08:07:05+00:00
- **Updated**: 2017-06-22 01:28:06+00:00
- **Authors**: Aditya Balu, Sambit Ghadai, Kin Gwn Lore, Gavin Young, Adarsh Krishnamurthy, Soumik Sarkar
- **Comment**: 9 Pages
- **Journal**: None
- **Summary**: 3D convolutional neural networks (3D-CNN) have been used for object recognition based on the voxelized shape of an object. In this paper, we present a 3D-CNN based method to learn distinct local geometric features of interest within an object. In this context, the voxelized representation may not be sufficient to capture the distinguishing information about such local features. To enable efficient learning, we augment the voxel data with surface normals of the object boundary. We then train a 3D-CNN with this augmented data and identify the local features critical for decision-making using 3D gradient-weighted class activation maps. An application of this feature identification framework is to recognize difficult-to-manufacture drilled hole features in a complex CAD geometry. The framework can be extended to identify difficult-to-manufacture features at multiple spatial scales leading to a real-time decision support system for design for manufacturability.



### Re-identification of Humans in Crowds using Personal, Social and Environmental Constraints
- **Arxiv ID**: http://arxiv.org/abs/1612.02155v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02155v1)
- **Published**: 2016-12-07 09:03:11+00:00
- **Updated**: 2016-12-07 09:03:11+00:00
- **Authors**: Shayan Modiri Assari, Haroon Idrees, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of human re-identification across non-overlapping cameras in crowds.Re-identification in crowded scenes is a challenging problem due to large number of people and frequent occlusions, coupled with changes in their appearance due to different properties and exposure of cameras. To solve this problem, we model multiple Personal, Social and Environmental (PSE) constraints on human motion across cameras. The personal constraints include appearance and preferred speed of each individual assumed to be similar across the non-overlapping cameras. The social influences (constraints) are quadratic in nature, i.e. occur between pairs of individuals, and modeled through grouping and collision avoidance. Finally, the environmental constraints capture the transition probabilities between gates (entrances / exits) in different cameras, defined as multi-modal distributions of transition time and destination between all pairs of gates. We incorporate these constraints into an energy minimization framework for solving human re-identification. Assigning $1-1$ correspondence while modeling PSE constraints is NP-hard. We present a stochastic local search algorithm to restrict the search space of hypotheses, and obtain $1-1$ solution in the presence of linear and quadratic PSE constraints. Moreover, we present an alternate optimization using Frank-Wolfe algorithm that solves the convex approximation of the objective function with linear relaxation on binary variables, and yields an order of magnitude speed up over stochastic local search with minor drop in performance. We evaluate our approach using Cumulative Matching Curves as well $1-1$ assignment on several thousand frames of Grand Central, PRID and DukeMTMC datasets, and obtain significantly better results compared to existing re-identification methods.



### Consensus Based Medical Image Segmentation Using Semi-Supervised Learning And Graph Cuts
- **Arxiv ID**: http://arxiv.org/abs/1612.02166v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02166v3)
- **Published**: 2016-12-07 09:38:11+00:00
- **Updated**: 2018-05-21 05:36:59+00:00
- **Authors**: Dwarikanath Mahapatra
- **Comment**: None
- **Journal**: None
- **Summary**: Medical image segmentation requires consensus ground truth segmentations to be derived from multiple expert annotations. A novel approach is proposed that obtains consensus segmentations from experts using graph cuts (GC) and semi supervised learning (SSL). Popular approaches use iterative Expectation Maximization (EM) to estimate the final annotation and quantify annotator's performance. Such techniques pose the risk of getting trapped in local minima. We propose a self consistency (SC) score to quantify annotator consistency using low level image features. SSL is used to predict missing annotations by considering global features and local image consistency. The SC score also serves as the penalty cost in a second order Markov random field (MRF) cost function optimized using graph cuts to derive the final consensus label. Graph cut obtains a global maximum without an iterative procedure. Experimental results on synthetic images, real data of Crohn's disease patients and retinal images show our final segmentation to be accurate and more consistent than competing methods.



### Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring
- **Arxiv ID**: http://arxiv.org/abs/1612.02177v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02177v2)
- **Published**: 2016-12-07 10:08:33+00:00
- **Updated**: 2018-05-07 06:15:57+00:00
- **Authors**: Seungjun Nah, Tae Hyun Kim, Kyoung Mu Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Non-uniform blind deblurring for general dynamic scenes is a challenging computer vision problem as blurs arise not only from multiple object motions but also from camera shake, scene depth variation. To remove these complicated motion blurs, conventional energy optimization based methods rely on simple assumptions such that blur kernel is partially uniform or locally linear. Moreover, recent machine learning based methods also depend on synthetic blur datasets generated under these assumptions. This makes conventional deblurring methods fail to remove blurs where blur kernel is difficult to approximate or parameterize (e.g. object motion boundaries). In this work, we propose a multi-scale convolutional neural network that restores sharp images in an end-to-end manner where blur is caused by various sources. Together, we present multi-scale loss function that mimics conventional coarse-to-fine approaches. Furthermore, we propose a new large-scale dataset that provides pairs of realistic blurry image and the corresponding ground truth sharp image that are obtained by a high-speed camera. With the proposed model trained on this dataset, we demonstrate empirically that our method achieves the state-of-the-art performance in dynamic scene deblurring not only qualitatively, but also quantitatively.



### Fusion of Range and Thermal Images for Person Detection
- **Arxiv ID**: http://arxiv.org/abs/1612.02183v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02183v1)
- **Published**: 2016-12-07 10:29:54+00:00
- **Updated**: 2016-12-07 10:29:54+00:00
- **Authors**: Wim Abbeloos, Toon Goedemé
- **Comment**: VII International Conference on Electrical Engineering FIE 2014,
  Santiago de Cuba
- **Journal**: Proceedings Conferencia Internacional de Ingenier\'ia El\'ectrica
  7 (2014) 1-4
- **Summary**: Detecting people in images is a challenging problem. Differences in pose, clothing and lighting, along with other factors, cause a lot of variation in their appearance. To overcome these issues, we propose a system based on fused range and thermal infrared images. These measurements show considerably less variation and provide more meaningful information. We provide a brief introduction to the sensor technology used and propose a calibration method. Several data fusion algorithms are compared and their performance is assessed on a simulated data set. The results of initial experiments on real data are analyzed and the measurement errors and the challenges they present are discussed. The resulting fused data are used to efficiently detect people in a fixed camera set-up. The system is extended to include person tracking.



### Saliency Driven Image Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1612.02184v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02184v3)
- **Published**: 2016-12-07 10:30:44+00:00
- **Updated**: 2018-01-17 07:34:12+00:00
- **Authors**: Roey Mechrez, Eli Shechtman, Lihi Zelnik-Manor
- **Comment**: to appear in WACV'18
- **Journal**: None
- **Summary**: Have you ever taken a picture only to find out that an unimportant background object ended up being overly salient? Or one of those team sports photos where your favorite player blends with the rest? Wouldn't it be nice if you could tweak these pictures just a little bit so that the distractor would be attenuated and your favorite player will stand-out among her peers? Manipulating images in order to control the saliency of objects is the goal of this paper. We propose an approach that considers the internal color and saliency properties of the image. It changes the saliency map via an optimization framework that relies on patch-based manipulation using only patches from within the same image to achieve realistic looking results. Applications include object enhancement, distractors attenuation and background decluttering. Comparing our method to previous ones shows significant improvement, both in the achieved saliency manipulation and in the realistic appearance of the resulting images.



### Template Matching with Deformable Diversity Similarity
- **Arxiv ID**: http://arxiv.org/abs/1612.02190v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02190v2)
- **Published**: 2016-12-07 10:43:55+00:00
- **Updated**: 2017-04-18 12:03:06+00:00
- **Authors**: Itamar Talmi, Roey Mechrez, Lihi Zelnik-Manor
- **Comment**: accepted to CVPR2017 (spotlight)
- **Journal**: None
- **Summary**: We propose a novel measure for template matching named Deformable Diversity Similarity -- based on the diversity of feature matches between a target image window and the template. We rely on both local appearance and geometric information that jointly lead to a powerful approach for matching. Our key contribution is a similarity measure, that is robust to complex deformations, significant background clutter, and occlusions. Empirical evaluation on the most up-to-date benchmark shows that our method outperforms the current state-of-the-art in its detection accuracy while improving computational complexity.



### A Functional Regression approach to Facial Landmark Tracking
- **Arxiv ID**: http://arxiv.org/abs/1612.02203v2
- **DOI**: 10.1109/TPAMI.2017.2745568
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02203v2)
- **Published**: 2016-12-07 11:34:36+00:00
- **Updated**: 2017-09-20 15:58:32+00:00
- **Authors**: Enrique Sánchez-Lozano, Georgios Tzimiropoulos, Brais Martinez, Fernando De la Torre, Michel Valstar
- **Comment**: Accepted at IEEE TPAMI. This is authors' version. 0162-8828
  \copyright 2017 IEEE. Personal use is permitted, but
  republication/redistribution requires IEEE permission. See
  http://www.ieee.org/publications_standards/publications/rights/index.html for
  more information
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2017
- **Summary**: Linear regression is a fundamental building block in many face detection and tracking algorithms, typically used to predict shape displacements from image features through a linear mapping. This paper presents a Functional Regression solution to the least squares problem, which we coin Continuous Regression, resulting in the first real-time incremental face tracker. Contrary to prior work in Functional Regression, in which B-splines or Fourier series were used, we propose to approximate the input space by its first-order Taylor expansion, yielding a closed-form solution for the continuous domain of displacements. We then extend the continuous least squares problem to correlated variables, and demonstrate the generalisation of our approach. We incorporate Continuous Regression into the cascaded regression framework, and show its computational benefits for both training and testing. We then present a fast approach for incremental learning within Cascaded Continuous Regression, coined iCCR, and show that its complexity allows real-time face tracking, being 20 times faster than the state of the art. To the best of our knowledge, this is the first incremental face tracker that is shown to operate in real-time. We show that iCCR achieves state-of-the-art performance on the 300-VW dataset, the most recent, large-scale benchmark for face tracking.



### Embedded Line Scan Image Sensors: The Low Cost Alternative for High Speed Imaging
- **Arxiv ID**: http://arxiv.org/abs/1612.02218v1
- **DOI**: 10.1109/IPTA.2015.7367207
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02218v1)
- **Published**: 2016-12-07 12:27:26+00:00
- **Updated**: 2016-12-07 12:27:26+00:00
- **Authors**: Stef Van Wolputte, Wim Abbeloos, Stijn Helsen, Abdellatif Bey-Temsamani, Toon Goedemé
- **Comment**: 2015 International Conference on Image Processing Theory, Tools and
  Applications (IPTA)
- **Journal**: Proceedings of the International Conference on Image Processing
  Theory, Tools and Applications (2015) 543-549
- **Summary**: In this paper we propose a low-cost high-speed imaging line scan system. We replace an expensive industrial line scan camera and illumination with a custom-built set-up of cheap off-the-shelf components, yielding a measurement system with comparative quality while costing about 20 times less. We use a low-cost linear (1D) image sensor, cheap optics including a LED-based or LASER-based lighting and an embedded platform to process the images. A step-by-step method to design such a custom high speed imaging system and select proper components is proposed. Simulations allowing to predict the final image quality to be obtained by the set-up has been developed. Finally, we applied our method in a lab, closely representing the real-life cases. Our results shows that our simulations are very accurate and that our low-cost line scan set-up acquired image quality compared to the high-end commercial vision system, for a fraction of the price.



### Process Monitoring of Extrusion Based 3D Printing via Laser Scanning
- **Arxiv ID**: http://arxiv.org/abs/1612.02219v1
- **DOI**: 10.13140/2.1.5175.0081
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02219v1)
- **Published**: 2016-12-07 12:29:46+00:00
- **Updated**: 2016-12-07 12:29:46+00:00
- **Authors**: Matthias Faes, Wim Abbeloos, Frederik Vogeler, Hans Valkenaers, Kurt Coppens, Toon Goedemé, Eleonora Ferraris
- **Comment**: International Conference on Polymers and Moulds Innovations(PMI) 2014
- **Journal**: Conference Proceedings PMI 6 (2014) 363-367
- **Summary**: Extrusion based 3D Printing (E3DP) is an Additive Manufacturing (AM) technique that extrudes thermoplastic polymer in order to build up components using a layerwise approach. Hereby, AM typically requires long production times in comparison to mass production processes such as Injection Molding. Failures during the AM process are often only noticed after build completion and frequently lead to part rejection because of dimensional inaccuracy or lack of mechanical performance, resulting in an important loss of time and material. A solution to improve the accuracy and robustness of a manufacturing technology is the integration of sensors to monitor and control process state-variables online. In this way, errors can be rapidly detected and possibly compensated at an early stage. To achieve this, we integrated a modular 2D laser triangulation scanner into an E3DP machine and analyzed feedback signals. A 2D laser triangulation scanner was selected here owing to the very compact size, achievable accuracy and the possibility of capturing geometrical 3D data. Thus, our implemented system is able to provide both quantitative and qualitative information. Also, in this work, first steps towards the development of a quality control loop for E3DP processes are presented and opportunities are discussed.



### Exploring the potential of combining time of flight and thermal infrared cameras for person detection
- **Arxiv ID**: http://arxiv.org/abs/1612.02223v1
- **DOI**: 10.5220/0004595704640470
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02223v1)
- **Published**: 2016-12-07 12:35:45+00:00
- **Updated**: 2016-12-07 12:35:45+00:00
- **Authors**: Wim Abbeloos, Toon Goedemé
- **Comment**: Proceedings of the 10th International Conference on Informatics in
  Control, Automation and Robotics
- **Journal**: Proceedings of the International Conference on Informatics in
  Control, Automation and Robotics (2013) 464-470
- **Summary**: Combining new, low-cost thermal infrared and time-of-flight range sensors provides new opportunities. In this position paper we explore the possibilities of combining these sensors and using their fused data for person detection. The proposed calibration approach for this sensor combination differs from the traditional stereo camera calibration in two fundamental ways. A first distinction is that the spectral sensitivity of the two sensors differs significantly. In fact, there is no sensitivity range overlap at all. A second distinction is that their resolution is typically very low, which requires special attention. We assume a situation in which the sensors' relative position is known, but their orientation is unknown. In addition, some of the typical measurement errors are discussed, and methods to compensate for them are proposed. We discuss how the fused data could allow increased accuracy and robustness without the need for complex algorithms requiring large amounts of computational power and training data.



### Global Hypothesis Generation for 6D Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1612.02287v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02287v3)
- **Published**: 2016-12-07 15:23:12+00:00
- **Updated**: 2017-01-02 09:09:03+00:00
- **Authors**: Frank Michel, Alexander Kirillov, Eric Brachmann, Alexander Krull, Stefan Gumhold, Bogdan Savchynskyy, Carsten Rother
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the task of estimating the 6D pose of a known 3D object from a single RGB-D image. Most modern approaches solve this task in three steps: i) Compute local features; ii) Generate a pool of pose-hypotheses; iii) Select and refine a pose from the pool. This work focuses on the second step. While all existing approaches generate the hypotheses pool via local reasoning, e.g. RANSAC or Hough-voting, we are the first to show that global reasoning is beneficial at this stage. In particular, we formulate a novel fully-connected Conditional Random Field (CRF) that outputs a very small number of pose-hypotheses. Despite the potential functions of the CRF being non-Gaussian, we give a new and efficient two-step optimization procedure, with some guarantees for optimality. We utilize our global hypotheses generation procedure to produce results that exceed state-of-the-art for the challenging "Occluded Object Dataset".



### Spatially Adaptive Computation Time for Residual Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.02297v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.02297v2)
- **Published**: 2016-12-07 15:37:57+00:00
- **Updated**: 2017-07-02 17:49:27+00:00
- **Authors**: Michael Figurnov, Maxwell D. Collins, Yukun Zhu, Li Zhang, Jonathan Huang, Dmitry Vetrov, Ruslan Salakhutdinov
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: This paper proposes a deep learning architecture based on Residual Network that dynamically adjusts the number of executed layers for the regions of the image. This architecture is end-to-end trainable, deterministic and problem-agnostic. It is therefore applicable without any modifications to a wide range of computer vision problems such as image classification, object detection and image segmentation. We present experimental results showing that this model improves the computational efficiency of Residual Networks on the challenging ImageNet classification and COCO object detection datasets. Additionally, we evaluate the computation time maps on the visual saliency dataset cat2000 and find that they correlate surprisingly well with human eye fixation positions.



### Pano2Vid: Automatic Cinematography for Watching 360$^{\circ}$ Videos
- **Arxiv ID**: http://arxiv.org/abs/1612.02335v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02335v1)
- **Published**: 2016-12-07 17:20:09+00:00
- **Updated**: 2016-12-07 17:20:09+00:00
- **Authors**: Yu-Chuan Su, Dinesh Jayaraman, Kristen Grauman
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce the novel task of Pano2Vid $-$ automatic cinematography in panoramic 360$^{\circ}$ videos. Given a 360$^{\circ}$ video, the goal is to direct an imaginary camera to virtually capture natural-looking normal field-of-view (NFOV) video. By selecting "where to look" within the panorama at each time step, Pano2Vid aims to free both the videographer and the end viewer from the task of determining what to watch. Towards this goal, we first compile a dataset of 360$^{\circ}$ videos downloaded from the web, together with human-edited NFOV camera trajectories to facilitate evaluation. Next, we propose AutoCam, a data-driven approach to solve the Pano2Vid task. AutoCam leverages NFOV web video to discriminatively identify space-time "glimpses" of interest at each time instant, and then uses dynamic programming to select optimal human-like camera trajectories. Through experimental evaluation on multiple newly defined Pano2Vid performance measures against several baselines, we show that our method successfully produces informative videos that could conceivably have been captured by human videographers.



### Differential Angular Imaging for Material Recognition
- **Arxiv ID**: http://arxiv.org/abs/1612.02372v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02372v2)
- **Published**: 2016-12-07 18:59:19+00:00
- **Updated**: 2017-07-14 00:43:10+00:00
- **Authors**: Jia Xue, Hang Zhang, Kristin Dana, Ko Nishino
- **Comment**: None
- **Journal**: None
- **Summary**: Material recognition for real-world outdoor surfaces has become increasingly important for computer vision to support its operation "in the wild." Computational surface modeling that underlies material recognition has transitioned from reflectance modeling using in-lab controlled radiometric measurements to image-based representations based on internet-mined images of materials captured in the scene. We propose to take a middle-ground approach for material recognition that takes advantage of both rich radiometric cues and flexible image capture. We realize this by developing a framework for differential angular imaging, where small angular variations in image capture provide an enhanced appearance representation and significant recognition improvement. We build a large-scale material database, Ground Terrain in Outdoor Scenes (GTOS) database, geared towards real use for autonomous agents. The database consists of over 30,000 images covering 40 classes of outdoor ground terrain under varying weather and lighting conditions. We develop a novel approach for material recognition called a Differential Angular Imaging Network (DAIN) to fully leverage this large dataset. With this novel network architecture, we extract characteristics of materials encoded in the angular and spatial gradients of their appearance. Our results show that DAIN achieves recognition performance that surpasses single view or coarsely quantized multiview images. These results demonstrate the effectiveness of differential angular imaging as a means for flexible, in-place material recognition.



### Automatic Detection of ADHD and ASD from Expressive Behaviour in RGBD Data
- **Arxiv ID**: http://arxiv.org/abs/1612.02374v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02374v1)
- **Published**: 2016-12-07 19:01:17+00:00
- **Updated**: 2016-12-07 19:01:17+00:00
- **Authors**: Shashank Jaiswal, Michel Valstar, Alinda Gillott, David Daley
- **Comment**: None
- **Journal**: None
- **Summary**: Attention Deficit Hyperactivity Disorder (ADHD) and Autism Spectrum Disorder (ASD) are neurodevelopmental conditions which impact on a significant number of children and adults. Currently, the diagnosis of such disorders is done by experts who employ standard questionnaires and look for certain behavioural markers through manual observation. Such methods for their diagnosis are not only subjective, difficult to repeat, and costly but also extremely time consuming. In this work, we present a novel methodology to aid diagnostic predictions about the presence/absence of ADHD and ASD by automatic visual analysis of a person's behaviour. To do so, we conduct the questionnaires in a computer-mediated way while recording participants with modern RGBD (Colour+Depth) sensors. In contrast to previous automatic approaches which have focussed only detecting certain behavioural markers, our approach provides a fully automatic end-to-end system for directly predicting ADHD and ASD in adults. Using state of the art facial expression analysis based on Dynamic Deep Learning and 3D analysis of behaviour, we attain classification rates of 96% for Controls vs Condition (ADHD/ASD) group and 94% for Comorbid (ADHD+ASD) vs ASD only group. We show that our system is a potentially useful time saving contribution to the diagnostic field of ADHD and ASD.



### DeMoN: Depth and Motion Network for Learning Monocular Stereo
- **Arxiv ID**: http://arxiv.org/abs/1612.02401v2
- **DOI**: 10.1109/CVPR.2017.596
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02401v2)
- **Published**: 2016-12-07 20:26:53+00:00
- **Updated**: 2017-04-11 09:14:10+00:00
- **Authors**: Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, Thomas Brox
- **Comment**: Camera ready version for CVPR 2017. Supplementary material included.
  Project page:
  http://lmb.informatik.uni-freiburg.de/people/ummenhof/depthmotionnet/
- **Journal**: None
- **Summary**: In this paper we formulate structure from motion as a learning problem. We train a convolutional network end-to-end to compute depth and camera motion from successive, unconstrained image pairs. The architecture is composed of multiple stacked encoder-decoder networks, the core part being an iterative network that is able to improve its own predictions. The network estimates not only depth and motion, but additionally surface normals, optical flow between the images and confidence of the matching. A crucial component of the approach is a training loss based on spatial relative differences. Compared to traditional two-frame structure from motion methods, results are more accurate and more robust. In contrast to the popular depth-from-single-image networks, DeMoN learns the concept of matching and, thus, better generalizes to structures not seen during training.



