# Arxiv Papers in cs.CV on 2016-12-09
### Fast Fourier single-pixel imaging using binary illumination
- **Arxiv ID**: http://arxiv.org/abs/1612.02880v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02880v1)
- **Published**: 2016-12-09 01:02:37+00:00
- **Updated**: 2016-12-09 01:02:37+00:00
- **Authors**: Zibang Zhang, Xueying Wang, Jingang Zhong
- **Comment**: None
- **Journal**: None
- **Summary**: Fourier single-pixel imaging (FSI) has proven capable of reconstructing high-quality two-dimensional and three-dimensional images. The utilization of the sparsity of natural images in Fourier domain allows high-resolution images to be reconstructed from far fewer measurements than effective image pixels. However, applying original FSI in digital micro-mirror device (DMD) based high-speed imaging system turns out to be challenging, because the original FSI uses grayscale Fourier basis patterns for illumination while DMDs generate grayscale patterns at a relatively low rate. DMDs are a binary device which can only generate a black-and-white pattern at each instance. In this paper, we adopt binary Fourier patterns for illumination to achieve DMD-based high-speed single-pixel imaging. Binary Fourier patterns are generated by upsampling and then applying error diffusion based dithering to the grayscale patterns. Experiments demonstrate the proposed technique able to achieve static imaging with high quality and dynamic imaging in real time. The proposed technique potentially allows high-quality and high-speed imaging over broad wavebands.



### Gesture-based Bootstrapping for Egocentric Hand Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1612.02889v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02889v2)
- **Published**: 2016-12-09 01:49:41+00:00
- **Updated**: 2018-06-11 18:15:06+00:00
- **Authors**: Yubo Zhang, Vishnu Naresh Boddeti, Kris M. Kitani
- **Comment**: None
- **Journal**: None
- **Summary**: Accurately identifying hands in images is a key sub-task for human activity understanding with wearable first-person point-of-view cameras. Traditional hand segmentation approaches rely on a large corpus of manually labeled data to generate robust hand detectors. However, these approaches still face challenges as the appearance of the hand varies greatly across users, tasks, environments or illumination conditions. A key observation in the case of many wearable applications and interfaces is that, it is only necessary to accurately detect the user's hands in a specific situational context. Based on this observation, we introduce an interactive approach to learn a person-specific hand segmentation model that does not require any manually labeled training data. Our approach proceeds in two steps, an interactive bootstrapping step for identifying moving hand regions, followed by learning a personalized user specific hand appearance model. Concretely, our approach uses two convolutional neural networks: (1) a gesture network that uses pre-defined motion information to detect the hand region; and (2) an appearance network that learns a person specific model of the hand region based on the output of the gesture network. During training, to make the appearance network robust to errors in the gesture network, the loss function of the former network incorporates the confidence of the gesture network while learning. Experiments demonstrate the robustness of our approach with an F1 score over 0.8 on all challenging datasets across a wide range of illumination and hand appearance variations, improving over a baseline approach by over 10%.



### Facial Expression Recognition using Convolutional Neural Networks: State of the Art
- **Arxiv ID**: http://arxiv.org/abs/1612.02903v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02903v1)
- **Published**: 2016-12-09 03:59:31+00:00
- **Updated**: 2016-12-09 03:59:31+00:00
- **Authors**: Christopher Pramerdorfer, Martin Kampel
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to recognize facial expressions automatically enables novel applications in human-computer interaction and other areas. Consequently, there has been active research in this field, with several recent works utilizing Convolutional Neural Networks (CNNs) for feature extraction and inference. These works differ significantly in terms of CNN architectures and other factors. Based on the reported results alone, the performance impact of these factors is unclear. In this paper, we review the state of the art in image-based facial expression recognition using CNNs and highlight algorithmic differences and their performance impact. On this basis, we identify existing bottlenecks and consequently directions for advancing this research field. Furthermore, we demonstrate that overcoming one of these bottlenecks - the comparatively basic architectures of the CNNs utilized in this field - leads to a substantial performance increase. By forming an ensemble of modern deep CNNs, we obtain a FER2013 test accuracy of 75.2%, outperforming previous works without requiring auxiliary training data or face registration.



### A series of maximum entropy upper bounds of the differential entropy
- **Arxiv ID**: http://arxiv.org/abs/1612.02954v1
- **DOI**: None
- **Categories**: **cs.IT**, cs.CV, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1612.02954v1)
- **Published**: 2016-12-09 09:36:14+00:00
- **Updated**: 2016-12-09 09:36:14+00:00
- **Authors**: Frank Nielsen, Richard Nock
- **Comment**: 18 pages
- **Journal**: None
- **Summary**: We present a series of closed-form maximum entropy upper bounds for the differential entropy of a continuous univariate random variable and study the properties of that series. We then show how to use those generic bounds for upper bounding the differential entropy of Gaussian mixture models. This requires to calculate the raw moments and raw absolute moments of Gaussian mixtures in closed-form that may also be handy in statistical machine learning and information theory. We report on our experiments and discuss on the tightness of those bounds.



### Automatic Model Based Dataset Generation for Fast and Accurate Crop and Weeds Detection
- **Arxiv ID**: http://arxiv.org/abs/1612.03019v3
- **DOI**: 10.1109/IROS.2017.8206408
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1612.03019v3)
- **Published**: 2016-12-09 13:43:19+00:00
- **Updated**: 2017-08-06 17:33:20+00:00
- **Authors**: Maurilio Di Cicco, Ciro Potena, Giorgio Grisetti, Alberto Pretto
- **Comment**: To appear in IEEE/RSJ IROS 2017
- **Journal**: None
- **Summary**: Selective weeding is one of the key challenges in the field of agriculture robotics. To accomplish this task, a farm robot should be able to accurately detect plants and to distinguish them between crop and weeds. Most of the promising state-of-the-art approaches make use of appearance-based models trained on large annotated datasets. Unfortunately, creating large agricultural datasets with pixel-level annotations is an extremely time consuming task, actually penalizing the usage of data-driven techniques. In this paper, we face this problem by proposing a novel and effective approach that aims to dramatically minimize the human intervention needed to train the detection and classification algorithms. The idea is to procedurally generate large synthetic training datasets randomizing the key features of the target environment (i.e., crop and weed species, type of soil, light conditions). More specifically, by tuning these model parameters, and exploiting a few real-world textures, it is possible to render a large amount of realistic views of an artificial agricultural scenario with no effort. The generated data can be directly used to train the model or to supplement real-world images. We validate the proposed methodology by using as testbed a modern deep learning based image segmentation architecture. We compare the classification results obtained using both real and synthetic images as training data. The reported results confirm the effectiveness and the potentiality of our approach.



### ActionFlowNet: Learning Motion Representation for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1612.03052v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.03052v3)
- **Published**: 2016-12-09 15:20:23+00:00
- **Updated**: 2018-02-16 22:15:25+00:00
- **Authors**: Joe Yue-Hei Ng, Jonghyun Choi, Jan Neumann, Larry S. Davis
- **Comment**: WACV 2018
- **Journal**: None
- **Summary**: Even with the recent advances in convolutional neural networks (CNN) in various visual recognition tasks, the state-of-the-art action recognition system still relies on hand crafted motion feature such as optical flow to achieve the best performance. We propose a multitask learning model ActionFlowNet to train a single stream network directly from raw pixels to jointly estimate optical flow while recognizing actions with convolutional neural networks, capturing both appearance and motion in a single model. We additionally provide insights to how the quality of the learned optical flow affects the action recognition. Our model significantly improves action recognition accuracy by a large margin 31% compared to state-of-the-art CNN-based action recognition models trained without external large scale data and additional optical flow input. Without pretraining on large external labeled datasets, our model, by well exploiting the motion information, achieves competitive recognition accuracy to the models trained with large labeled datasets such as ImageNet and Sport-1M.



### Following Gaze Across Views
- **Arxiv ID**: http://arxiv.org/abs/1612.03094v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.03094v1)
- **Published**: 2016-12-09 17:20:17+00:00
- **Updated**: 2016-12-09 17:20:17+00:00
- **Authors**: Adrià Recasens, Carl Vondrick, Aditya Khosla, Antonio Torralba
- **Comment**: 9 pages, 8 figures
- **Journal**: None
- **Summary**: Following the gaze of people inside videos is an important signal for understanding people and their actions. In this paper, we present an approach for following gaze across views by predicting where a particular person is looking throughout a scene. We collect VideoGaze, a new dataset which we use as a benchmark to both train and evaluate models. Given one view with a person in it and a second view of the scene, our model estimates a density for gaze location in the second view. A key aspect of our approach is an end-to-end model that solves the following sub-problems: saliency, gaze pose, and geometric relationships between views. Although our model is supervised only with gaze, we show that the model learns to solve these subproblems automatically without supervision. Experiments suggest that our approach follows gaze better than standard baselines and produces plausible results for everyday situations.



### Boundary-aware Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1612.03129v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.03129v2)
- **Published**: 2016-12-09 18:57:33+00:00
- **Updated**: 2017-04-07 01:43:57+00:00
- **Authors**: Zeeshan Hayder, Xuming He, Mathieu Salzmann
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of instance-level semantic segmentation, which aims at jointly detecting, segmenting and classifying every individual object in an image. In this context, existing methods typically propose candidate objects, usually as bounding boxes, and directly predict a binary mask within each such proposal. As a consequence, they cannot recover from errors in the object candidate generation process, such as too small or shifted boxes.   In this paper, we introduce a novel object segment representation based on the distance transform of the object masks. We then design an object mask network (OMN) with a new residual-deconvolution architecture that infers such a representation and decodes it into the final binary object mask. This allows us to predict masks that go beyond the scope of the bounding boxes and are thus robust to inaccurate object candidates. We integrate our OMN into a Multitask Network Cascade framework, and learn the resulting boundary-aware instance segmentation (BAIS) network in an end-to-end manner. Our experiments on the PASCAL VOC 2012 and the Cityscapes datasets demonstrate the benefits of our approach, which outperforms the state-of-the-art in both object proposal generation and instance segmentation.



### Understanding and Mapping Natural Beauty
- **Arxiv ID**: http://arxiv.org/abs/1612.03142v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.03142v2)
- **Published**: 2016-12-09 19:48:15+00:00
- **Updated**: 2017-08-09 17:37:29+00:00
- **Authors**: Scott Workman, Richard Souvenir, Nathan Jacobs
- **Comment**: International Conference on Computer Vision (ICCV) 2017
- **Journal**: None
- **Summary**: While natural beauty is often considered a subjective property of images, in this paper, we take an objective approach and provide methods for quantifying and predicting the scenicness of an image. Using a dataset containing hundreds of thousands of outdoor images captured throughout Great Britain with crowdsourced ratings of natural beauty, we propose an approach to predict scenicness which explicitly accounts for the variance of human ratings. We demonstrate that quantitative measures of scenicness can benefit semantic image understanding, content-aware image processing, and a novel application of cross-view mapping, where the sparsity of ground-level images can be addressed by incorporating unlabeled overhead images in the training and prediction steps. For each application, our methods for scenicness prediction result in quantitative and qualitative improvements over baseline approaches.



### Feature Pyramid Networks for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1612.03144v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.03144v2)
- **Published**: 2016-12-09 19:55:54+00:00
- **Updated**: 2017-04-19 22:46:32+00:00
- **Authors**: Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie
- **Comment**: None
- **Journal**: None
- **Summary**: Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.



### Panoptic Studio: A Massively Multiview System for Social Interaction Capture
- **Arxiv ID**: http://arxiv.org/abs/1612.03153v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.03153v1)
- **Published**: 2016-12-09 20:25:04+00:00
- **Updated**: 2016-12-09 20:25:04+00:00
- **Authors**: Hanbyul Joo, Tomas Simon, Xulong Li, Hao Liu, Lei Tan, Lin Gui, Sean Banerjee, Timothy Godisart, Bart Nabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara, Yaser Sheikh
- **Comment**: Submitted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence
- **Journal**: None
- **Summary**: We present an approach to capture the 3D motion of a group of people engaged in a social interaction. The core challenges in capturing social interactions are: (1) occlusion is functional and frequent; (2) subtle motion needs to be measured over a space large enough to host a social group; (3) human appearance and configuration variation is immense; and (4) attaching markers to the body may prime the nature of interactions. The Panoptic Studio is a system organized around the thesis that social interactions should be measured through the integration of perceptual analyses over a large variety of view points. We present a modularized system designed around this principle, consisting of integrated structural, hardware, and software innovations. The system takes, as input, 480 synchronized video streams of multiple people engaged in social activities, and produces, as output, the labeled time-varying 3D structure of anatomical landmarks on individuals in the space. Our algorithm is designed to fuse the "weak" perceptual processes in the large number of views by progressively generating skeletal proposals from low-level appearance cues, and a framework for temporal refinement is also presented by associating body parts to reconstructed dense 3D trajectory stream. Our system and method are the first in reconstructing full body motion of more than five people engaged in social interactions without using markers. We also empirically demonstrate the impact of the number of views in achieving this goal.



### Automatic Lymphocyte Detection in H&E Images with Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.03217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.03217v1)
- **Published**: 2016-12-09 23:31:35+00:00
- **Updated**: 2016-12-09 23:31:35+00:00
- **Authors**: Jianxu Chen, Chukka Srinivas
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic detection of lymphocyte in H&E images is a necessary first step in lots of tissue image analysis algorithms. An accurate and robust automated lymphocyte detection approach is of great importance in both computer science and clinical studies. Most of the existing approaches for lymphocyte detection are based on traditional image processing algorithms and/or classic machine learning methods. In the recent years, deep learning techniques have fundamentally transformed the way that a computer interprets images and have become a matchless solution in various pattern recognition problems. In this work, we design a new deep neural network model which extends the fully convolutional network by combining the ideas in several recent techniques, such as shortcut links. Also, we design a new training scheme taking the prior knowledge about lymphocytes into consideration. The training scheme not only efficiently exploits the limited amount of free-form annotations from pathologists, but also naturally supports efficient fine-tuning. As a consequence, our model has the potential of self-improvement by leveraging the errors collected during real applications. Our experiments show that our deep neural network model achieves good performance in the images of different staining conditions or different types of tissues.



