# Arxiv Papers in cs.CV on 2016-12-06
### Superpixels: An Evaluation of the State-of-the-Art
- **Arxiv ID**: http://arxiv.org/abs/1612.01601v3
- **DOI**: 10.1016/j.cviu.2017.03.007
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01601v3)
- **Published**: 2016-12-06 00:26:54+00:00
- **Updated**: 2017-04-19 20:40:48+00:00
- **Authors**: David Stutz, Alexander Hermans, Bastian Leibe
- **Comment**: None
- **Journal**: None
- **Summary**: Superpixels group perceptually similar pixels to create visually meaningful entities while heavily reducing the number of primitives for subsequent processing steps. As of these properties, superpixel algorithms have received much attention since their naming in 2003. By today, publicly available superpixel algorithms have turned into standard tools in low-level vision. As such, and due to their quick adoption in a wide range of applications, appropriate benchmarks are crucial for algorithm selection and comparison. Until now, the rapidly growing number of algorithms as well as varying experimental setups hindered the development of a unifying benchmark. We present a comprehensive evaluation of 28 state-of-the-art superpixel algorithms utilizing a benchmark focussing on fair comparison and designed to provide new insights relevant for applications. To this end, we explicitly discuss parameter optimization and the importance of strictly enforcing connectivity. Furthermore, by extending well-known metrics, we are able to summarize algorithm performance independent of the number of generated superpixels, thereby overcoming a major limitation of available benchmarks. Furthermore, we discuss runtime, robustness against noise, blur and affine transformations, implementation details as well as aspects of visual quality. Finally, we present an overall ranking of superpixel algorithms which redefines the state-of-the-art and enables researchers to easily select appropriate algorithms and the corresponding implementations which themselves are made publicly available as part of our benchmark at davidstutz.de/projects/superpixel-benchmark/.



### Automatic Event Detection for Signal-based Surveillance
- **Arxiv ID**: http://arxiv.org/abs/1612.01611v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01611v1)
- **Published**: 2016-12-06 00:54:45+00:00
- **Updated**: 2016-12-06 00:54:45+00:00
- **Authors**: Jingxin Xu, Clinton Fookes, Sridha Sridharan
- **Comment**: None
- **Journal**: None
- **Summary**: Signal-based Surveillance systems such as Closed Circuits Televisions (CCTV) have been widely installed in public places. Those systems are normally used to find the events with security interest, and play a significant role in public safety. Though such systems are still heavily reliant on human labour to monitor the captured information, there have been a number of automatic techniques proposed to analysing the data. This article provides an overview of automatic surveillance event detection techniques . Despite it's popularity in research, it is still too challenging a problem to be realised in a real world deployment. The challenges come from not only the detection techniques such as signal processing and machine learning, but also the experimental design with factors such as data collection, evaluation protocols, and ground-truth annotation. Finally, this article propose that multi-disciplinary research is the path towards a solution to this problem.



### Learning to Detect Multiple Photographic Defects
- **Arxiv ID**: http://arxiv.org/abs/1612.01635v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01635v5)
- **Published**: 2016-12-06 02:32:22+00:00
- **Updated**: 2018-03-08 07:05:30+00:00
- **Authors**: Ning Yu, Xiaohui Shen, Zhe Lin, Radomir Mech, Connelly Barnes
- **Comment**: Accepted to WACV'18
- **Journal**: None
- **Summary**: In this paper, we introduce the problem of simultaneously detecting multiple photographic defects. We aim at detecting the existence, severity, and potential locations of common photographic defects related to color, noise, blur and composition. The automatic detection of such defects could be used to provide users with suggestions for how to improve photos without the need to laboriously try various correction methods. Defect detection could also help users select photos of higher quality while filtering out those with severe defects in photo curation and summarization.   To investigate this problem, we collected a large-scale dataset of user annotations on seven common photographic defects, which allows us to evaluate algorithms by measuring their consistency with human judgments. Our new dataset enables us to formulate the problem as a multi-task learning problem and train a multi-column deep convolutional neural network (CNN) to simultaneously predict the severity of all the defects. Unlike some existing single-defect estimation methods that rely on low-level statistics and may fail in many cases on natural photographs, our model is able to understand image contents and quality at a higher level. As a result, in our experiments, we show that our model has predictions with much higher consistency with human judgments than low-level methods as well as several baseline CNN models. Our model also performs better than an average human from our user study.



### Fine-grained Recurrent Neural Networks for Automatic Prostate Segmentation in Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/1612.01655v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01655v1)
- **Published**: 2016-12-06 03:56:07+00:00
- **Updated**: 2016-12-06 03:56:07+00:00
- **Authors**: Xin Yang, Lequan Yu, Lingyun Wu, Yi Wang, Dong Ni, Jing Qin, Pheng-Ann Heng
- **Comment**: To appear in AAAI Conference 2017
- **Journal**: None
- **Summary**: Boundary incompleteness raises great challenges to automatic prostate segmentation in ultrasound images. Shape prior can provide strong guidance in estimating the missing boundary, but traditional shape models often suffer from hand-crafted descriptors and local information loss in the fitting procedure. In this paper, we attempt to address those issues with a novel framework. The proposed framework can seamlessly integrate feature extraction and shape prior exploring, and estimate the complete boundary with a sequential manner. Our framework is composed of three key modules. Firstly, we serialize the static 2D prostate ultrasound images into dynamic sequences and then predict prostate shapes by sequentially exploring shape priors. Intuitively, we propose to learn the shape prior with the biologically plausible Recurrent Neural Networks (RNNs). This module is corroborated to be effective in dealing with the boundary incompleteness. Secondly, to alleviate the bias caused by different serialization manners, we propose a multi-view fusion strategy to merge shape predictions obtained from different perspectives. Thirdly, we further implant the RNN core into a multiscale Auto-Context scheme to successively refine the details of the shape prediction map. With extensive validation on challenging prostate ultrasound images, our framework bridges severe boundary incompleteness and achieves the best performance in prostate boundary delineation when compared with several advanced methods. Additionally, our approach is general and can be extended to other medical image segmentation tasks, where boundary incompleteness is one of the main challenges.



### Binary Subspace Coding for Query-by-Image Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1612.01657v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1612.01657v1)
- **Published**: 2016-12-06 04:01:17+00:00
- **Updated**: 2016-12-06 04:01:17+00:00
- **Authors**: Ruicong Xu, Yang Yang, Yadan Luo, Fumin Shen, Zi Huang, Heng Tao Shen
- **Comment**: None
- **Journal**: None
- **Summary**: The query-by-image video retrieval (QBIVR) task has been attracting considerable research attention recently. However, most existing methods represent a video by either aggregating or projecting all its frames into a single datum point, which may easily cause severe information loss. In this paper, we propose an efficient QBIVR framework to enable an effective and efficient video search with image query. We first define a similarity-preserving distance metric between an image and its orthogonal projection in the subspace of the video, which can be equivalently transformed to a Maximum Inner Product Search (MIPS) problem.   Besides, to boost the efficiency of solving the MIPS problem, we propose two asymmetric hashing schemes, which bridge the domain gap of images and videos. The first approach, termed Inner-product Binary Coding (IBC), preserves the inner relationships of images and videos in a common Hamming space. To further improve the retrieval efficiency, we devise a Bilinear Binary Coding (BBC) approach, which employs compact bilinear projections instead of a single large projection matrix. Extensive experiments have been conducted on four real-world video datasets to verify the effectiveness of our proposed approaches as compared to the state-of-the-arts.



### MarioQA: Answering Questions by Watching Gameplay Videos
- **Arxiv ID**: http://arxiv.org/abs/1612.01669v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01669v2)
- **Published**: 2016-12-06 05:23:52+00:00
- **Updated**: 2017-08-13 07:49:55+00:00
- **Authors**: Jonghwan Mun, Paul Hongsuck Seo, Ilchae Jung, Bohyung Han
- **Comment**: None
- **Journal**: None
- **Summary**: We present a framework to analyze various aspects of models for video question answering (VideoQA) using customizable synthetic datasets, which are constructed automatically from gameplay videos. Our work is motivated by the fact that existing models are often tested only on datasets that require excessively high-level reasoning or mostly contain instances accessible through single frame inferences. Hence, it is difficult to measure capacity and flexibility of trained models, and existing techniques often rely on ad-hoc implementations of deep neural networks without clear insight into datasets and models. We are particularly interested in understanding temporal relationships between video events to solve VideoQA problems; this is because reasoning temporal dependency is one of the most distinct components in videos from images. To address this objective, we automatically generate a customized synthetic VideoQA dataset using {\em Super Mario Bros.} gameplay videos so that it contains events with different levels of reasoning complexity. Using the dataset, we show that properly constructed datasets with events in various complexity levels are critical to learn effective models and improve overall performance.



### Cluster-Wise Ratio Tests for Fast Camera Localization
- **Arxiv ID**: http://arxiv.org/abs/1612.01689v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01689v2)
- **Published**: 2016-12-06 07:35:24+00:00
- **Updated**: 2017-05-20 18:02:46+00:00
- **Authors**: Raúl Díaz, Charless C. Fowlkes
- **Comment**: None
- **Journal**: None
- **Summary**: Feature point matching for camera localization suffers from scalability problems. Even when feature descriptors associated with 3D scene points are locally unique, as coverage grows, similar or repeated features become increasingly common. As a result, the standard distance ratio-test used to identify reliable image feature points is overly restrictive and rejects many good candidate matches. We propose a simple coarse-to-fine strategy that uses conservative approximations to robust local ratio-tests that can be computed efficiently using global approximate k-nearest neighbor search. We treat these forward matches as votes in camera pose space and use them to prioritize back-matching within candidate camera pose clusters, exploiting feature co-visibility captured by clustering the 3D model camera pose graph. This approach achieves state-of-the-art camera localization results on a variety of popular benchmarks, outperforming several methods that use more complicated data structures and that make more restrictive assumptions on camera pose. We also carry out diagnostic analyses on a difficult test dataset containing globally repetitive structure that suggest our approach successfully adapts to the challenges of large-scale image localization.



### Deep Neural Networks for No-Reference and Full-Reference Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/1612.01697v2
- **DOI**: 10.1109/TIP.2017.2760518
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01697v2)
- **Published**: 2016-12-06 08:15:03+00:00
- **Updated**: 2017-12-07 20:36:50+00:00
- **Authors**: Sebastian Bosse, Dominique Maniry, Klaus-Robert Müller, Thomas Wiegand, Wojciech Samek
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing, 27(1):206-219, 2018
- **Summary**: We present a deep neural network-based approach to image quality assessment (IQA). The network is trained end-to-end and comprises ten convolutional layers and five pooling layers for feature extraction, and two fully connected layers for regression, which makes it significantly deeper than related IQA models. Unique features of the proposed architecture are that: 1) with slight adaptations it can be used in a no-reference (NR) as well as in a full-reference (FR) IQA setting and 2) it allows for joint learning of local quality and local weights, i.e., relative importance of local quality to the global quality estimate, in an unified framework. Our approach is purely data-driven and does not rely on hand-crafted features or other types of prior domain knowledge about the human visual system or image statistics. We evaluate the proposed approach on the LIVE, CISQ, and TID2013 databases as well as the LIVE In the wild image quality challenge database and show superior performance to state-of-the-art NR and FR IQA methods. Finally, cross-database evaluation shows a high ability to generalize between different databases, indicating a high robustness of the learned features.



### Deep Stereo Matching with Dense CRF Priors
- **Arxiv ID**: http://arxiv.org/abs/1612.01725v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01725v2)
- **Published**: 2016-12-06 09:51:21+00:00
- **Updated**: 2017-01-24 20:08:28+00:00
- **Authors**: Ron Slossberg, Aaron Wetzler, Ron Kimmel
- **Comment**: None
- **Journal**: None
- **Summary**: Stereo reconstruction from rectified images has recently been revisited within the context of deep learning. Using a deep Convolutional Neural Network to obtain patch-wise matching cost volumes has resulted in state of the art stereo reconstruction on classic datasets like Middlebury and Kitti. By introducing this cost into a classical stereo pipeline, the final results are improved dramatically over non-learning based cost models. However these pipelines typically include hand engineered post processing steps to effectively regularize and clean the result. Here, we show that it is possible to take a more holistic approach by training a fully end-to-end network which directly includes regularization in the form of a densely connected Conditional Random Field (CRF) that acts as a prior on inter-pixel interactions. We demonstrate that our approach on both synthetic and real world datasets outperforms an alternative end-to-end network and compares favorably to more hand engineered approaches.



### Video Ladder Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.01756v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1612.01756v3)
- **Published**: 2016-12-06 11:15:28+00:00
- **Updated**: 2016-12-30 09:01:02+00:00
- **Authors**: Francesco Cricri, Xingyang Ni, Mikko Honkala, Emre Aksu, Moncef Gabbouj
- **Comment**: This version extends the paper accepted at the NIPS 2016 workshop on
  ML for Spatiotemporal Forecasting, with more details and more experimental
  results
- **Journal**: None
- **Summary**: We present the Video Ladder Network (VLN) for efficiently generating future video frames. VLN is a neural encoder-decoder model augmented at all layers by both recurrent and feedforward lateral connections. At each layer, these connections form a lateral recurrent residual block, where the feedforward connection represents a skip connection and the recurrent connection represents the residual. Thanks to the recurrent connections, the decoder can exploit temporal summaries generated from all layers of the encoder. This way, the top layer is relieved from the pressure of modeling lower-level spatial and temporal details. Furthermore, we extend the basic version of VLN to incorporate ResNet-style residual blocks in the encoder and decoder, which help improving the prediction results. VLN is trained in self-supervised regime on the Moving MNIST dataset, achieving competitive results while having very simple structure and providing fast inference.



### FLIC: Fast Linear Iterative Clustering with Active Search
- **Arxiv ID**: http://arxiv.org/abs/1612.01810v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01810v3)
- **Published**: 2016-12-06 14:16:19+00:00
- **Updated**: 2018-10-05 13:42:55+00:00
- **Authors**: Jiaxing Zhao, Ren Bo, Qibin Hou, Ming-Ming Cheng, Paul L. Rosin
- **Comment**: AAAI 2018
- **Journal**: None
- **Summary**: Benefiting from its high efficiency and simplicity, Simple Linear Iterative Clustering (SLIC) remains one of the most popular over-segmentation tools. However, due to explicit enforcement of spatial similarity for region continuity, the boundary adaptation of SLIC is sub-optimal. It also has drawbacks on convergence rate as a result of both the fixed search region and separately doing the assignment step and the update step. In this paper, we propose an alternative approach to fix the inherent limitations of SLIC. In our approach, each pixel actively searches its corresponding segment under the help of its neighboring pixels, which naturally enables region coherence without being harmful to boundary adaptation. We also jointly perform the assignment and update steps, allowing high convergence rate. Extensive evaluations on Berkeley segmentation benchmark verify that our method outperforms competitive methods under various evaluation metrics. It also has the lowest time cost among existing methods (approximately 30fps for a 481x321 image on a single CPU core).



### Revisiting Winner Take All (WTA) Hashing for Sparse Datasets
- **Arxiv ID**: http://arxiv.org/abs/1612.01834v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1612.01834v2)
- **Published**: 2016-12-06 14:51:37+00:00
- **Updated**: 2016-12-07 08:50:26+00:00
- **Authors**: Beidi Chen, Anshumali Shrivastava
- **Comment**: None
- **Journal**: None
- **Summary**: WTA (Winner Take All) hashing has been successfully applied in many large scale vision applications. This hashing scheme was tailored to take advantage of the comparative reasoning (or order based information), which showed significant accuracy improvements. In this paper, we identify a subtle issue with WTA, which grows with the sparsity of the datasets. This issue limits the discriminative power of WTA. We then propose a solution for this problem based on the idea of Densification which provably fixes the issue. Our experiments show that Densified WTA Hashing outperforms Vanilla WTA both in image classification and retrieval tasks consistently and significantly.



### Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1612.01887v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1612.01887v2)
- **Published**: 2016-12-06 16:03:50+00:00
- **Updated**: 2017-06-06 06:59:15+00:00
- **Authors**: Jiasen Lu, Caiming Xiong, Devi Parikh, Richard Socher
- **Comment**: 12 pages, 11 figures, CVPR2017 camera ready
- **Journal**: None
- **Summary**: Attention-based neural encoder-decoder frameworks have been widely adopted for image captioning. Most methods force visual attention to be active for every generated word. However, the decoder likely requires little to no visual information from the image to predict non-visual words such as "the" and "of". Other words that may seem visual can often be predicted reliably just from the language model e.g., "sign" after "behind a red stop" or "phone" following "talking on a cell". In this paper, we propose a novel adaptive attention model with a visual sentinel. At each time step, our model decides whether to attend to the image (and if so, to which regions) or to the visual sentinel. The model decides whether to attend to the image and where, in order to extract meaningful information for sequential word generation. We test our method on the COCO image captioning 2015 challenge dataset and Flickr30K. Our approach sets the new state-of-the-art by a significant margin.



### Tag Prediction at Flickr: a View from the Darkroom
- **Arxiv ID**: http://arxiv.org/abs/1612.01922v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01922v3)
- **Published**: 2016-12-06 17:39:49+00:00
- **Updated**: 2017-12-19 23:37:04+00:00
- **Authors**: Kofi Boakye, Sachin Farfade, Hamid Izadinia, Yannis Kalantidis, Pierre Garrigues
- **Comment**: Presented at the ACM Multimedia Thematic Workshops, 2017
- **Journal**: None
- **Summary**: Automated photo tagging has established itself as one of the most compelling applications of deep learning. While deep convolutional neural networks have repeatedly demonstrated top performance on standard datasets for classification, there are a number of often overlooked but important considerations when deploying this technology in a real-world scenario. In this paper, we present our efforts in developing a large-scale photo tagging system for Flickr photo search. We discuss topics including how to 1) select the tags that matter most to our users; 2) develop lightweight, high-performance models for tag prediction; and 3) leverage the power of large amounts of noisy data for training. Our results demonstrate that, for real-world datasets, training exclusively with this noisy data yields performance on par with the standard paradigm of first pre-training on clean data and then fine-tuning. In addition, we observe that the models trained with user-generated data can yield better fine-tuning results when a small amount of clean data is available. As such, we advocate for the approach of harnessing user-generated data in large-scale systems.



### FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.01925v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01925v1)
- **Published**: 2016-12-06 17:52:47+00:00
- **Updated**: 2016-12-06 17:52:47+00:00
- **Authors**: Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, Thomas Brox
- **Comment**: Including supplementary material. For the video see:
  http://lmb.informatik.uni-freiburg.de/Publications/2016/IMKDB16/
- **Journal**: None
- **Summary**: The FlowNet demonstrated that optical flow estimation can be cast as a learning problem. However, the state of the art with regard to the quality of the flow has still been defined by traditional methods. Particularly on small displacements and real-world data, FlowNet cannot compete with variational methods. In this paper, we advance the concept of end-to-end learning of optical flow and make it work really well. The large improvements in quality and speed are caused by three major contributions: first, we focus on the training data and show that the schedule of presenting data during training is very important. Second, we develop a stacked architecture that includes warping of the second image with intermediate optical flow. Third, we elaborate on small displacements by introducing a sub-network specializing on small motions. FlowNet 2.0 is only marginally slower than the original FlowNet but decreases the estimation error by more than 50%. It performs on par with state-of-the-art methods, while running at interactive frame rates. Moreover, we present faster variants that allow optical flow computation at up to 140fps with accuracy matching the original FlowNet.



### Correlation Alignment for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1612.01939v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1612.01939v1)
- **Published**: 2016-12-06 18:31:57+00:00
- **Updated**: 2016-12-06 18:31:57+00:00
- **Authors**: Baochen Sun, Jiashi Feng, Kate Saenko
- **Comment**: Introduction to CORAL, CORAL-LDA, and Deep CORAL. arXiv admin note:
  text overlap with arXiv:1511.05547
- **Journal**: None
- **Summary**: In this chapter, we present CORrelation ALignment (CORAL), a simple yet effective method for unsupervised domain adaptation. CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. In contrast to subspace manifold methods, it aligns the original feature distributions of the source and target domains, rather than the bases of lower-dimensional subspaces. It is also much simpler than other distribution matching methods. CORAL performs remarkably well in extensive evaluations on standard benchmark datasets. We first describe a solution that applies a linear transformation to source features to align them with target features before classifier training. For linear classifiers, we propose to equivalently apply CORAL to the classifier weights, leading to added efficiency when the number of classifiers is small but the number and dimensionality of target examples are very high. The resulting CORAL Linear Discriminant Analysis (CORAL-LDA) outperforms LDA by a large margin on standard domain adaptation benchmarks. Finally, we extend CORAL to learn a nonlinear transformation that aligns correlations of layer activations in deep neural networks (DNNs). The resulting Deep CORAL approach works seamlessly with DNNs and achieves state-of-the-art performance on standard benchmark datasets. Our code is available at:~\url{https://github.com/VisionLearningGroup/CORAL}



### Memory Efficient Multi-Scale Line Detector Architecture for Retinal Blood Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1612.09524v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR
- **Links**: [PDF](http://arxiv.org/pdf/1612.09524v1)
- **Published**: 2016-12-06 19:06:28+00:00
- **Updated**: 2016-12-06 19:06:28+00:00
- **Authors**: Hamza Bendaoudi, Farida Cheriet, J. M. Pierre Langlois
- **Comment**: This paper was accepted and presented at Conference on Design and
  Architectures for Signal and Image Processing - DASIP 2016
- **Journal**: None
- **Summary**: This paper presents a memory efficient architecture that implements the Multi-Scale Line Detector (MSLD) algorithm for real-time retinal blood vessel detection in fundus images on a Zynq FPGA. This implementation benefits from the FPGA parallelism to drastically reduce the memory requirements of the MSLD from two images to a few values. The architecture is optimized in terms of resource utilization by reusing the computations and optimizing the bit-width. The throughput is increased by designing fully pipelined functional units. The architecture is capable of achieving a comparable accuracy to its software implementation but 70x faster for low resolution images. For high resolution images, it achieves an acceleration by a factor of 323x.



### Learning Diverse Image Colorization
- **Arxiv ID**: http://arxiv.org/abs/1612.01958v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01958v2)
- **Published**: 2016-12-06 19:28:22+00:00
- **Updated**: 2017-04-27 14:34:28+00:00
- **Authors**: Aditya Deshpande, Jiajun Lu, Mao-Chuang Yeh, Min Jin Chong, David Forsyth
- **Comment**: This revision to appear in CVPR17
- **Journal**: None
- **Summary**: Colorization is an ambiguous problem, with multiple viable colorizations for a single grey-level image. However, previous methods only produce the single most probable colorization. Our goal is to model the diversity intrinsic to the problem of colorization and produce multiple colorizations that display long-scale spatial co-ordination. We learn a low dimensional embedding of color fields using a variational autoencoder (VAE). We construct loss terms for the VAE decoder that avoid blurry outputs and take into account the uneven distribution of pixel colors. Finally, we build a conditional model for the multi-modal distribution between grey-level image and the color field embeddings. Samples from this conditional model result in diverse colorization. We demonstrate that our method obtains better diverse colorizations than a standard conditional variational autoencoder (CVAE) model, as well as a recently proposed conditional generative adversarial network (cGAN).



### Core Sampling Framework for Pixel Classification
- **Arxiv ID**: http://arxiv.org/abs/1612.01981v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.01981v1)
- **Published**: 2016-12-06 20:28:44+00:00
- **Updated**: 2016-12-06 20:28:44+00:00
- **Authors**: Manohar Karki, Robert DiBiano, Saikat Basu, Supratik Mukhopadhyay
- **Comment**: None
- **Journal**: None
- **Summary**: The intermediate map responses of a Convolutional Neural Network (CNN) contain information about an image that can be used to extract contextual knowledge about it. In this paper, we present a core sampling framework that is able to use these activation maps from several layers as features to another neural network using transfer learning to provide an understanding of an input image. Our framework creates a representation that combines features from the test data and the contextual knowledge gained from the responses of a pretrained network, processes it and feeds it to a separate Deep Belief Network. We use this representation to extract more information from an image at the pixel level, hence gaining understanding of the whole image. We experimentally demonstrate the usefulness of our framework using a pretrained VGG-16 model to perform segmentation on the BAERI dataset of Synthetic Aperture Radar(SAR) imagery and the CAMVID dataset.



### Diverse Sampling for Self-Supervised Learning of Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1612.01991v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.01991v1)
- **Published**: 2016-12-06 20:54:18+00:00
- **Updated**: 2016-12-06 20:54:18+00:00
- **Authors**: Mohammadreza Mostajabi, Nicholas Kolkin, Gregory Shakhnarovich
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an approach for learning category-level semantic segmentation purely from image-level classification tags indicating presence of categories. It exploits localization cues that emerge from training classification-tasked convolutional networks, to drive a "self-supervision" process that automatically labels a sparse, diverse training set of points likely to belong to classes of interest. Our approach has almost no hyperparameters, is modular, and allows for very fast training of segmentation in less than 3 minutes. It obtains competitive results on the VOC 2012 segmentation benchmark. More, significantly the modularity and fast training of our framework allows new classes to efficiently added for inference.



