# Arxiv Papers in cs.CV on 2009-07-02
### An Iterative Fingerprint Enhancement Algorithm Based on Accurate Determination of Orientation Flow
- **Arxiv ID**: http://arxiv.org/abs/0907.0288v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/0907.0288v1)
- **Published**: 2009-07-02 04:57:32+00:00
- **Updated**: 2009-07-02 04:57:32+00:00
- **Authors**: Simant Dube
- **Comment**: 10 pages, 4 figures. Ongoing work. To be submitted to appropriate
  conference/journal
- **Journal**: None
- **Summary**: We describe an algorithm to enhance and binarize a fingerprint image. The algorithm is based on accurate determination of orientation flow of the ridges of the fingerprint image by computing variance of the neighborhood pixels around a pixel in different directions. We show that an iterative algorithm which captures the mutual interdependence of orientation flow computation, enhancement and binarization gives very good results on poor quality images.



### Bounding the Probability of Error for High Precision Recognition
- **Arxiv ID**: http://arxiv.org/abs/0907.0418v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/0907.0418v1)
- **Published**: 2009-07-02 16:09:47+00:00
- **Updated**: 2009-07-02 16:09:47+00:00
- **Authors**: Andrew Kae, Gary B. Huang, Erik Learned-Miller
- **Comment**: None
- **Journal**: None
- **Summary**: We consider models for which it is important, early in processing, to estimate some variables with high precision, but perhaps at relatively low rates of recall. If some variables can be identified with near certainty, then they can be conditioned upon, allowing further inference to be done efficiently. Specifically, we consider optical character recognition (OCR) systems that can be bootstrapped by identifying a subset of correctly translated document words with very high precision. This "clean set" is subsequently used as document-specific training data. While many current OCR systems produce measures of confidence for the identity of each letter or word, thresholding these confidence values, even at very high values, still produces some errors.   We introduce a novel technique for identifying a set of correct words with very high precision. Rather than estimating posterior probabilities, we bound the probability that any given word is incorrect under very general assumptions, using an approximate worst case analysis. As a result, the parameters of the model are nearly irrelevant, and we are able to identify a subset of words, even in noisy documents, of which we are highly confident. On our set of 10 documents, we are able to identify about 6% of the words on average without making a single error. This ability to produce word lists with very high precision allows us to use a family of models which depends upon such clean word lists.



