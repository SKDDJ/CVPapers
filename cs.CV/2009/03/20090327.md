# Arxiv Papers in cs.CV on 2009-03-27
### An Exponential Lower Bound on the Complexity of Regularization Paths
- **Arxiv ID**: http://arxiv.org/abs/0903.4817v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CG, cs.CV, math.OC, stat.ML, 90C20, F.2.2; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/0903.4817v3)
- **Published**: 2009-03-27 17:23:31+00:00
- **Updated**: 2012-10-25 23:47:12+00:00
- **Authors**: Bernd Gärtner, Martin Jaggi, Clément Maria
- **Comment**: Journal version, 28 Pages, 5 Figures
- **Journal**: Journal of Computational Geometry (JoCG) 3(1), 168-195, 2012
- **Summary**: For a variety of regularized optimization problems in machine learning, algorithms computing the entire solution path have been developed recently. Most of these methods are quadratic programs that are parameterized by a single parameter, as for example the Support Vector Machine (SVM). Solution path algorithms do not only compute the solution for one particular value of the regularization parameter but the entire path of solutions, making the selection of an optimal parameter much easier.   It has been assumed that these piecewise linear solution paths have only linear complexity, i.e. linearly many bends. We prove that for the support vector machine this complexity can be exponential in the number of training points in the worst case. More strongly, we construct a single instance of n input points in d dimensions for an SVM such that at least \Theta(2^{n/2}) = \Theta(2^d) many distinct subsets of support vectors occur as the regularization parameter changes.



### A Combinatorial Algorithm to Compute Regularization Paths
- **Arxiv ID**: http://arxiv.org/abs/0903.4856v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, F.2.2; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/0903.4856v1)
- **Published**: 2009-03-27 18:16:04+00:00
- **Updated**: 2009-03-27 18:16:04+00:00
- **Authors**: Bernd Gärtner, Joachim Giesen, Martin Jaggi, Torsten Welsch
- **Comment**: 7 Pages, 1 Figure
- **Journal**: None
- **Summary**: For a wide variety of regularization methods, algorithms computing the entire solution path have been developed recently. Solution path algorithms do not only compute the solution for one particular value of the regularization parameter but the entire path of solutions, making the selection of an optimal parameter much easier. Most of the currently used algorithms are not robust in the sense that they cannot deal with general or degenerate input. Here we present a new robust, generic method for parametric quadratic programming. Our algorithm directly applies to nearly all machine learning applications, where so far every application required its own different algorithm.   We illustrate the usefulness of our method by applying it to a very low rank problem which could not be solved by existing path tracking methods, namely to compute part-worth values in choice based conjoint analysis, a popular technique from market research to estimate consumers preferences on a class of parameterized options.



