# Arxiv Papers in cs.CV on 2009-04-14
### Boosting through Optimization of Margin Distributions
- **Arxiv ID**: http://arxiv.org/abs/0904.2037v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/0904.2037v3)
- **Published**: 2009-04-14 01:57:12+00:00
- **Updated**: 2010-01-06 09:00:26+00:00
- **Authors**: Chunhua Shen, Hanxi Li
- **Comment**: 9 pages. To publish/Published in IEEE Transactions on Neural
  Networks, 21(7), July 2010
- **Journal**: None
- **Summary**: Boosting has attracted much research attention in the past decade. The success of boosting algorithms may be interpreted in terms of the margin theory. Recently it has been shown that generalization error of classifiers can be obtained by explicitly taking the margin distribution of the training data into account. Most of the current boosting algorithms in practice usually optimizes a convex loss function and do not make use of the margin distribution. In this work we design a new boosting algorithm, termed margin-distribution boosting (MDBoost), which directly maximizes the average margin and minimizes the margin variance simultaneously. This way the margin distribution is optimized. A totally-corrective optimization algorithm based on column generation is proposed to implement MDBoost. Experiments on UCI datasets show that MDBoost outperforms AdaBoost and LPBoost in most cases.



