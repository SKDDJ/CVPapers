# Arxiv Papers in cs.CV on 2002-12-12
### The Management of Context-Sensitive Features: A Review of Strategies
- **Arxiv ID**: http://arxiv.org/abs/cs/0212037v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, I.2.6; I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/cs/0212037v1)
- **Published**: 2002-12-12 18:14:38+00:00
- **Updated**: 2002-12-12 18:14:38+00:00
- **Authors**: Peter D. Turney
- **Comment**: 7 pages
- **Journal**: 13th International Conference on Machine Learning, Workshop on
  Learning in Context-Sensitive Domains, Bari, Italy, (1996), 60-66
- **Summary**: In this paper, we review five heuristic strategies for handling context-sensitive features in supervised machine learning from examples. We discuss two methods for recovering lost (implicit) contextual information. We mention some evidence that hybrid strategies can have a synergetic effect. We then show how the work of several machine learning researchers fits into this framework. While we do not claim that these strategies exhaust the possibilities, it appears that the framework includes all of the techniques that can be found in the published literature on contextsensitive learning.



### The Identification of Context-Sensitive Features: A Formal Definition of Context for Concept Learning
- **Arxiv ID**: http://arxiv.org/abs/cs/0212038v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, I.2.6; I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/cs/0212038v1)
- **Published**: 2002-12-12 18:29:02+00:00
- **Updated**: 2002-12-12 18:29:02+00:00
- **Authors**: Peter D. Turney
- **Comment**: 7 pages
- **Journal**: 13th International Conference on Machine Learning, Workshop on
  Learning in Context-Sensitive Domains, Bari, Italy, (1996), 53-59
- **Summary**: A large body of research in machine learning is concerned with supervised learning from examples. The examples are typically represented as vectors in a multi-dimensional feature space (also known as attribute-value descriptions). A teacher partitions a set of training examples into a finite number of classes. The task of the learning algorithm is to induce a concept from the training examples. In this paper, we formally distinguish three types of features: primary, contextual, and irrelevant features. We also formally define what it means for one feature to be context-sensitive to another feature. Context-sensitive features complicate the task of the learner and potentially impair the learner's performance. Our formal definitions make it possible for a learner to automatically identify context-sensitive features. After context-sensitive features have been identified, there are several strategies that the learner can employ for managing the features; however, a discussion of these strategies is outside of the scope of this paper. The formal definitions presented here correct a flaw in previously proposed definitions. We discuss the relationship between our work and a formal definition of relevance.



### Data Engineering for the Analysis of Semiconductor Manufacturing Data
- **Arxiv ID**: http://arxiv.org/abs/cs/0212040v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CE, cs.CV, I.2.6; I.5.2; I.5.4; J.2
- **Links**: [PDF](http://arxiv.org/pdf/cs/0212040v1)
- **Published**: 2002-12-12 19:11:11+00:00
- **Updated**: 2002-12-12 19:11:11+00:00
- **Authors**: Peter D. Turney
- **Comment**: 10 pages
- **Journal**: Proceedings of the IJCAI-95 Workshop on Data Engineering for
  Inductive Learning, Montreal, Quebec, (1995), 50-59
- **Summary**: We have analyzed manufacturing data from several different semiconductor manufacturing plants, using decision tree induction software called Q-YIELD. The software generates rules for predicting when a given product should be rejected. The rules are intended to help the process engineers improve the yield of the product, by helping them to discover the causes of rejection. Experience with Q-YIELD has taught us the importance of data engineering -- preprocessing the data to enable or facilitate decision tree induction. This paper discusses some of the data engineering problems we have encountered with semiconductor manufacturing data. The paper deals with two broad classes of problems: engineering the features in a feature vector representation and engineering the definition of the target concept (the classes). Manufacturing process data present special problems for feature engineering, since the data have multiple levels of granularity (detail, resolution). Engineering the target concept is important, due to our focus on understanding the past, as opposed to the more common focus in machine learning on predicting the future.



### Robust Classification with Context-Sensitive Features
- **Arxiv ID**: http://arxiv.org/abs/cs/0212041v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, I.2.6; I.5.2; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/cs/0212041v1)
- **Published**: 2002-12-12 19:26:52+00:00
- **Updated**: 2002-12-12 19:26:52+00:00
- **Authors**: Peter D. Turney
- **Comment**: 9 pages
- **Journal**: Proceedings of the Sixth International Conference on Industrial
  and Engineering Applications of Artificial Intelligence and Expert Systems,
  Edinburgh, Scotland, (1993), 268-276
- **Summary**: This paper addresses the problem of classifying observations when features are context-sensitive, especially when the testing set involves a context that is different from the training set. The paper begins with a precise definition of the problem, then general strategies are presented for enhancing the performance of classification algorithms on this type of problem. These strategies are tested on three domains. The first domain is the diagnosis of gas turbine engines. The problem is to diagnose a faulty engine in one context, such as warm weather, when the fault has previously been seen only in another context, such as cold weather. The second domain is speech recognition. The context is given by the identity of the speaker. The problem is to recognize words spoken by a new speaker, not represented in the training set. The third domain is medical prognosis. The problem is to predict whether a patient with hepatitis will live or die. The context is the age of the patient. For all three domains, exploiting context results in substantially more accurate classification.



### Exploiting Context When Learning to Classify
- **Arxiv ID**: http://arxiv.org/abs/cs/0212035v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, I.2.6; I.5.2; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/cs/0212035v1)
- **Published**: 2002-12-12 19:40:50+00:00
- **Updated**: 2002-12-12 19:40:50+00:00
- **Authors**: Peter D. Turney
- **Comment**: 6 pages
- **Journal**: Proceedings of the European Conference on Machine Learning,
  Vienna, Austria, (1993), 402-407
- **Summary**: This paper addresses the problem of classifying observations when features are context-sensitive, specifically when the testing set involves a context that is different from the training set. The paper begins with a precise definition of the problem, then general strategies are presented for enhancing the performance of classification algorithms on this type of problem. These strategies are tested on two domains. The first domain is the diagnosis of gas turbine engines. The problem is to diagnose a faulty engine in one context, such as warm weather, when the fault has previously been seen only in another context, such as cold weather. The second domain is speech recognition. The problem is to recognize words spoken by a new speaker, not represented in the training set. For both domains, exploiting context results in substantially more accurate classification.



