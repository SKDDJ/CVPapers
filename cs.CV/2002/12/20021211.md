# Arxiv Papers in cs.CV on 2002-12-11
### Technical Note: Bias and the Quantification of Stability
- **Arxiv ID**: http://arxiv.org/abs/cs/0212028v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, I.2.6; I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/cs/0212028v1)
- **Published**: 2002-12-11 15:50:41+00:00
- **Updated**: 2002-12-11 15:50:41+00:00
- **Authors**: Peter D. Turney
- **Comment**: 14 pages
- **Journal**: Machine Learning, (1995), 20, 23-33
- **Summary**: Research on bias in machine learning algorithms has generally been concerned with the impact of bias on predictive accuracy. We believe that there are other factors that should also play a role in the evaluation of bias. One such factor is the stability of the algorithm; in other words, the repeatability of the results. If we obtain two sets of data from the same phenomenon, with the same underlying probability distribution, then we would like our learning algorithm to induce approximately the same concepts from both sets of data. This paper introduces a method for quantifying stability, based on a measure of the agreement between concepts. We also discuss the relationships among stability, predictive accuracy, and bias.



### A Theory of Cross-Validation Error
- **Arxiv ID**: http://arxiv.org/abs/cs/0212029v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, I.2.6; I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/cs/0212029v1)
- **Published**: 2002-12-11 16:08:36+00:00
- **Updated**: 2002-12-11 16:08:36+00:00
- **Authors**: Peter D. Turney
- **Comment**: 48 pages
- **Journal**: Journal of Experimental and Theoretical Artificial Intelligence,
  (1994), 6, 361-391
- **Summary**: This paper presents a theory of error in cross-validation testing of algorithms for predicting real-valued attributes. The theory justifies the claim that predicting real-valued attributes requires balancing the conflicting demands of simplicity and accuracy. Furthermore, the theory indicates precisely how these conflicting demands must be balanced, in order to minimize cross-validation error. A general theory is presented, then it is developed in detail for linear regression and instance-based learning.



### Theoretical Analyses of Cross-Validation Error and Voting in Instance-Based Learning
- **Arxiv ID**: http://arxiv.org/abs/cs/0212030v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, I.2.6; I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/cs/0212030v1)
- **Published**: 2002-12-11 17:36:00+00:00
- **Updated**: 2002-12-11 17:36:00+00:00
- **Authors**: Peter D. Turney
- **Comment**: 48 pages
- **Journal**: Journal of Experimental and Theoretical Artificial Intelligence,
  (1994), 6, 331-360
- **Summary**: This paper begins with a general theory of error in cross-validation testing of algorithms for supervised learning from examples. It is assumed that the examples are described by attribute-value pairs, where the values are symbolic. Cross-validation requires a set of training examples and a set of testing examples. The value of the attribute that is to be predicted is known to the learner in the training set, but unknown in the testing set. The theory demonstrates that cross-validation error has two components: error on the training set (inaccuracy) and sensitivity to noise (instability). This general theory is then applied to voting in instance-based learning. Given an example in the testing set, a typical instance-based learning algorithm predicts the designated attribute by voting among the k nearest neighbors (the k most similar examples) to the testing example in the training set. Voting is intended to increase the stability (resistance to noise) of instance-based learning, but a theoretical analysis shows that there are circumstances in which voting can be destabilizing. The theory suggests ways to minimize cross-validation error, by insuring that voting is stable and does not adversely affect accuracy.



### Contextual Normalization Applied to Aircraft Gas Turbine Engine Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/cs/0212031v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CE, cs.CV, I.2.6; I.5.4; J.2
- **Links**: [PDF](http://arxiv.org/pdf/cs/0212031v1)
- **Published**: 2002-12-11 18:30:59+00:00
- **Updated**: 2002-12-11 18:30:59+00:00
- **Authors**: Peter D. Turney, Michael Halasz
- **Comment**: 45 pages
- **Journal**: Journal of Applied Intelligence, (1993), 3, 109-129
- **Summary**: Diagnosing faults in aircraft gas turbine engines is a complex problem. It involves several tasks, including rapid and accurate interpretation of patterns in engine sensor data. We have investigated contextual normalization for the development of a software tool to help engine repair technicians with interpretation of sensor data. Contextual normalization is a new strategy for employing machine learning. It handles variation in data that is due to contextual factors, rather than the health of the engine. It does this by normalizing the data in a context-sensitive manner. This learning strategy was developed and tested using 242 observations of an aircraft gas turbine engine in a test cell, where each observation consists of roughly 12,000 numbers, gathered over a 12 second interval. There were eight classes of observations: seven deliberately implanted classes of faults and a healthy class. We compared two approaches to implementing our learning strategy: linear regression and instance-based learning. We have three main results. (1) For the given problem, instance-based learning works better than linear regression. (2) For this problem, contextual normalization works better than other common forms of normalization. (3) The algorithms described here can be the basis for a useful software tool for assisting technicians with the interpretation of sensor data.



### Types of Cost in Inductive Concept Learning
- **Arxiv ID**: http://arxiv.org/abs/cs/0212034v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, I.2.6; I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/cs/0212034v1)
- **Published**: 2002-12-11 19:42:14+00:00
- **Updated**: 2002-12-11 19:42:14+00:00
- **Authors**: Peter D. Turney
- **Comment**: 7 pages
- **Journal**: Workshop on Cost-Sensitive Learning at the Seventeenth
  International Conference on Machine Learning, (2000), Stanford University,
  California, 15-21
- **Summary**: Inductive concept learning is the task of learning to assign cases to a discrete set of classes. In real-world applications of concept learning, there are many different types of cost involved. The majority of the machine learning literature ignores all types of cost (unless accuracy is interpreted as a type of cost measure). A few papers have investigated the cost of misclassification errors. Very few papers have examined the many other types of cost. In this paper, we attempt to create a taxonomy of the different types of cost that are involved in inductive concept learning. This taxonomy may help to organize the literature on cost-sensitive learning. We hope that it will inspire researchers to investigate all types of cost in inductive concept learning in more depth.



