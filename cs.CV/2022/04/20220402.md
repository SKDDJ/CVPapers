# Arxiv Papers in cs.CV on 2022-04-02
### SkeleVision: Towards Adversarial Resiliency of Person Tracking with Multi-Task Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.00734v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.00734v1)
- **Published**: 2022-04-02 01:21:09+00:00
- **Updated**: 2022-04-02 01:21:09+00:00
- **Authors**: Nilaksh Das, Sheng-Yun Peng, Duen Horng Chau
- **Comment**: None
- **Journal**: None
- **Summary**: Person tracking using computer vision techniques has wide ranging applications such as autonomous driving, home security and sports analytics. However, the growing threat of adversarial attacks raises serious concerns regarding the security and reliability of such techniques. In this work, we study the impact of multi-task learning (MTL) on the adversarial robustness of the widely used SiamRPN tracker, in the context of person tracking. Specifically, we investigate the effect of jointly learning with semantically analogous tasks of person tracking and human keypoint detection. We conduct extensive experiments with more powerful adversarial attacks that can be physically realizable, demonstrating the practical value of our approach. Our empirical study with simulated as well as real-world datasets reveals that training with MTL consistently makes it harder to attack the SiamRPN tracker, compared to typically training only on the single task of person tracking.



### What to look at and where: Semantic and Spatial Refined Transformer for detecting human-object interactions
- **Arxiv ID**: http://arxiv.org/abs/2204.00746v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.00746v2)
- **Published**: 2022-04-02 02:41:31+00:00
- **Updated**: 2022-05-25 19:32:58+00:00
- **Authors**: A S M Iftekhar, Hao Chen, Kaustav Kundu, Xinyu Li, Joseph Tighe, Davide Modolo
- **Comment**: CVPR 2022 Oral
- **Journal**: None
- **Summary**: We propose a novel one-stage Transformer-based semantic and spatial refined transformer (SSRT) to solve the Human-Object Interaction detection task, which requires to localize humans and objects, and predicts their interactions. Differently from previous Transformer-based HOI approaches, which mostly focus at improving the design of the decoder outputs for the final detection, SSRT introduces two new modules to help select the most relevant object-action pairs within an image and refine the queries' representation using rich semantic and spatial features. These enhancements lead to state-of-the-art results on the two most popular HOI benchmarks: V-COCO and HICO-DET.



### Homography Loss for Monocular 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.00754v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.00754v1)
- **Published**: 2022-04-02 03:48:03+00:00
- **Updated**: 2022-04-02 03:48:03+00:00
- **Authors**: Jiaqi Gu, Bojian Wu, Lubin Fan, Jianqiang Huang, Shen Cao, Zhiyu Xiang, Xian-Sheng Hua
- **Comment**: 8 pages, 5 figures. Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Monocular 3D object detection is an essential task in autonomous driving. However, most current methods consider each 3D object in the scene as an independent training sample, while ignoring their inherent geometric relations, thus inevitably resulting in a lack of leveraging spatial constraints. In this paper, we propose a novel method that takes all the objects into consideration and explores their mutual relationships to help better estimate the 3D boxes. Moreover, since 2D detection is more reliable currently, we also investigate how to use the detected 2D boxes as guidance to globally constrain the optimization of the corresponding predicted 3D boxes. To this end, a differentiable loss function, termed as Homography Loss, is proposed to achieve the goal, which exploits both 2D and 3D information, aiming at balancing the positional relationships between different objects by global constraints, so as to obtain more accurately predicted 3D boxes. Thanks to the concise design, our loss function is universal and can be plugged into any mature monocular 3D detector, while significantly boosting the performance over their baseline. Experiments demonstrate that our method yields the best performance (Nov. 2021) compared with the other state-of-the-arts by a large margin on KITTI 3D datasets.



### Do learned representations respect causal relationships?
- **Arxiv ID**: http://arxiv.org/abs/2204.00762v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.00762v2)
- **Published**: 2022-04-02 04:53:10+00:00
- **Updated**: 2022-04-07 13:07:41+00:00
- **Authors**: Lan Wang, Vishnu Naresh Boddeti
- **Comment**: None
- **Journal**: None
- **Summary**: Data often has many semantic attributes that are causally associated with each other. But do attribute-specific learned representations of data also respect the same causal relations? We answer this question in three steps. First, we introduce NCINet, an approach for observational causal discovery from high-dimensional data. It is trained purely on synthetically generated representations and can be applied to real representations, and is specifically designed to mitigate the domain gap between the two. Second, we apply NCINet to identify the causal relations between image representations of different pairs of attributes with known and unknown causal relations between the labels. For this purpose, we consider image representations learned for predicting attributes on the 3D Shapes, CelebA, and the CASIA-WebFace datasets, which we annotate with multiple multi-class attributes. Third, we analyze the effect on the underlying causal relation between learned representations induced by various design choices in representation learning. Our experiments indicate that (1) NCINet significantly outperforms existing observational causal discovery approaches for estimating the causal relation between pairs of random samples, both in the presence and absence of an unobserved confounder, (2) under controlled scenarios, learned representations can indeed satisfy the underlying causal relations between their respective labels, and (3) the causal relations are positively correlated with the predictive capability of the representations.



### Ad Creative Discontinuation Prediction with Multi-Modal Multi-Task Neural Survival Networks
- **Arxiv ID**: http://arxiv.org/abs/2204.11588v1
- **DOI**: 10.3390/app12073594
- **Categories**: **cs.IR**, cs.AI, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.11588v1)
- **Published**: 2022-04-02 04:57:23+00:00
- **Updated**: 2022-04-02 04:57:23+00:00
- **Authors**: Shunsuke Kitada, Hitoshi Iyatomi, Yoshifumi Seki
- **Comment**: 23 pages, 5 figures. Accepted by Appl. Sci. on March 29th, 2022
- **Journal**: Appl. Sci. 2022, 12(7), 3594
- **Summary**: Discontinuing ad creatives at an appropriate time is one of the most important ad operations that can have a significant impact on sales. Such operational support for ineffective ads has been less explored than that for effective ads. After pre-analyzing 1,000,000 real-world ad creatives, we found that there are two types of discontinuation: short-term (i.e., cut-out) and long-term (i.e., wear-out). In this paper, we propose a practical prediction framework for the discontinuation of ad creatives with a hazard function-based loss function inspired by survival analysis. Our framework predicts the discontinuations with a multi-modal deep neural network that takes as input the ad creative (e.g., text, categorical, image, numerical features). To improve the prediction performance for the two different types of discontinuations and for the ad creatives that contribute to sales, we introduce two new techniques: (1) a two-term estimation technique with multi-task learning and (2) a click-through rate-weighting technique for the loss function. We evaluated our framework using the large-scale ad creative dataset, including 10 billion scale impressions. In terms of the concordance index (short: 0.896, long: 0.939, and overall: 0.792), our framework achieved significantly better performance than the conventional method (0.531). Additionally, we confirmed that our framework (i) demonstrated the same degree of discontinuation effect as manual operations for short-term cases, and (ii) accurately predicted the ad discontinuation order, which is important for long-running ad creatives for long-term cases.



### SAD: A Large-scale Dataset towards Airport Detection in Synthetic Aperture Radar Images
- **Arxiv ID**: http://arxiv.org/abs/2204.00790v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.00790v2)
- **Published**: 2022-04-02 07:29:10+00:00
- **Updated**: 2022-04-07 12:25:15+00:00
- **Authors**: Daochang Wang, Fan Zhang, Fei Ma, Wei Hu, Yu Tang, Yongsheng Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Airports have an important role in both military and civilian domains. The synthetic aperture radar (SAR) based airport detection has received increasing attention in recent years. However, due to the high cost of SAR imaging and annotation process, there is no publicly available SAR dataset for airport detection. As a result, deep learning methods have not been fully used in airport detection tasks. To provide a benchmark for airport detection research in SAR images, this paper introduces a large-scale SAR Airport Dataset (SAD). In order to adequately reflect the demands of real world applications, it contains 624 SAR images from Sentinel 1B and covers 104 airfield instances with different scales, orientations and shapes. The experiments of multiple deep learning approach on this dataset proves its effectiveness. It developing state-of-the-art airport area detection algorithms or other relevant tasks.



### IR-GAN: Image Manipulation with Linguistic Instruction by Increment Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2204.00792v1
- **DOI**: 10.1145/3394171.3413777
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.00792v1)
- **Published**: 2022-04-02 07:48:39+00:00
- **Updated**: 2022-04-02 07:48:39+00:00
- **Authors**: Zhenhuan Liu, Jincan Deng, Liang Li, Shaofei Cai, Qianqian Xu, Shuhui Wang, Qingming Huang
- **Comment**: None
- **Journal**: Proceedings of the 28th ACM International Conference on
  Multimedia,2020
- **Summary**: Conditional image generation is an active research topic including text2image and image translation.   Recently image manipulation with linguistic instruction brings new challenges of multimodal conditional generation.   However, traditional conditional image generation models mainly focus on generating high-quality and visually realistic images, and lack resolving the partial consistency between image and instruction.   To address this issue, we propose an Increment Reasoning Generative Adversarial Network (IR-GAN), which aims to reason the consistency between visual increment in images and semantic increment in instructions.   First, we introduce the word-level and instruction-level instruction encoders to learn user's intention from history-correlated instructions as semantic increment.   Second, we embed the representation of semantic increment into that of source image for generating target image, where source image plays the role of referring auxiliary.   Finally, we propose a reasoning discriminator to measure the consistency between visual increment and semantic increment, which purifies user's intention and guarantees the good logic of generated target image.   Extensive experiments and visualization conducted on two datasets show the effectiveness of IR-GAN.



### R(Det)^2: Randomized Decision Routing for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.00794v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.00794v1)
- **Published**: 2022-04-02 07:54:58+00:00
- **Updated**: 2022-04-02 07:54:58+00:00
- **Authors**: Ya-Li Li, Shengjin Wang
- **Comment**: 10 pages, 5 figures; Accepted by CVPR2022
- **Journal**: None
- **Summary**: In the paradigm of object detection, the decision head is an important part, which affects detection performance significantly. Yet how to design a high-performance decision head remains to be an open issue. In this paper, we propose a novel approach to combine decision trees and deep neural networks in an end-to-end learning manner for object detection. First, we disentangle the decision choices and prediction values by plugging soft decision trees into neural networks. To facilitate effective learning, we propose randomized decision routing with node selective and associative losses, which can boost the feature representative learning and network decision simultaneously. Second, we develop the decision head for object detection with narrow branches to generate the routing probabilities and masks, for the purpose of obtaining divergent decisions from different nodes. We name this approach as the randomized decision routing for object detection, abbreviated as R(Det)$^2$. Experiments on MS-COCO dataset demonstrate that R(Det)$^2$ is effective to improve the detection performance. Equipped with existing detectors, it achieves $1.4\sim 3.6$\% AP improvement.



### Unsupervised Coherent Video Cartoonization with Perceptual Motion Consistency
- **Arxiv ID**: http://arxiv.org/abs/2204.00795v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.00795v1)
- **Published**: 2022-04-02 07:59:02+00:00
- **Updated**: 2022-04-02 07:59:02+00:00
- **Authors**: Zhenhuan Liu, Liang Li, Huajie Jiang, Xin Jin, Dandan Tu, Shuhui Wang, Zheng-Jun Zha
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, creative content generations like style transfer and neural photo editing have attracted more and more attention. Among these, cartoonization of real-world scenes has promising applications in entertainment and industry. Different from image translations focusing on improving the style effect of generated images, video cartoonization has additional requirements on the temporal consistency. In this paper, we propose a spatially-adaptive semantic alignment framework with perceptual motion consistency for coherent video cartoonization in an unsupervised manner. The semantic alignment module is designed to restore deformation of semantic structure caused by spatial information lost in the encoder-decoder architecture. Furthermore, we devise the spatio-temporal correlative map as a style-independent, global-aware regularization on the perceptual motion consistency. Deriving from similarity measurement of high-level features in photo and cartoon frames, it captures global semantic information beyond raw pixel-value in optical flow. Besides, the similarity measurement disentangles temporal relationships from domain-specific style properties, which helps regularize the temporal consistency without hurting style effects of cartoon images. Qualitative and quantitative experiments demonstrate our method is able to generate highly stylistic and temporal consistent cartoon videos.



### RFVTM: A Recovery and Filtering Vertex Trichotomy Matching for Remote Sensing Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2204.00818v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.00818v1)
- **Published**: 2022-04-02 09:03:55+00:00
- **Updated**: 2022-04-02 09:03:55+00:00
- **Authors**: Ming Zhao, Bowen An, Yongpeng Wu, Huynh Van Luong, André Kaup
- **Comment**: None
- **Journal**: None
- **Summary**: Reliable feature point matching is a vital yet challenging process in feature-based image registration. In this paper,a robust feature point matching algorithm called Recovery and Filtering Vertex Trichotomy Matching (RFVTM) is proposed to remove outliers and retain sufficient inliers for remote sensing images. A novel affine invariant descriptor called vertex trichotomy descriptor is proposed on the basis of that geometrical relations between any of vertices and lines are preserved after affine transformations, which is constructed by mapping each vertex into trichotomy sets. The outlier removals in Vertex Trichotomy Matching (VTM) are implemented by iteratively comparing the disparity of corresponding vertex trichotomy descriptors. Some inliers mistakenly validated by a large amount of outliers are removed in VTM iterations, and several residual outliers close to correct locations cannot be excluded with the same graph structures. Therefore, a recovery and filtering strategy is designed to recover some inliers based on identical vertex trichotomy descriptors and restricted transformation errors. Assisted with the additional recovered inliers, residual outliers can also be filtered out during the process of reaching identical graph for the expanded vertex sets. Experimental results demonstrate the superior performance on precision and stability of this algorithm under various conditions, such as remote sensing images with large transformations, duplicated patterns, or inconsistent spectral content.



### Semantic-Aware Domain Generalized Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.00822v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.2.10; I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2204.00822v1)
- **Published**: 2022-04-02 09:09:59+00:00
- **Updated**: 2022-04-02 09:09:59+00:00
- **Authors**: Duo Peng, Yinjie Lei, Munawar Hayat, Yulan Guo, Wen Li
- **Comment**: 16 pages, 7 figures, accepted at CVPR 2022 (Oral Presentation)
- **Journal**: None
- **Summary**: Deep models trained on source domain lack generalization when evaluated on unseen target domains with different data distributions. The problem becomes even more pronounced when we have no access to target domain samples for adaptation. In this paper, we address domain generalized semantic segmentation, where a segmentation model is trained to be domain-invariant without using any target domain data. Existing approaches to tackle this problem standardize data into a unified distribution. We argue that while such a standardization promotes global normalization, the resulting features are not discriminative enough to get clear segmentation boundaries. To enhance separation between categories while simultaneously promoting domain invariance, we propose a framework including two novel modules: Semantic-Aware Normalization (SAN) and Semantic-Aware Whitening (SAW). Specifically, SAN focuses on category-level center alignment between features from different image styles, while SAW enforces distributed alignment for the already center-aligned features. With the help of SAN and SAW, we encourage both intra-category compactness and inter-category separability. We validate our approach through extensive experiments on widely-used datasets (i.e. GTAV, SYNTHIA, Cityscapes, Mapillary and BDDS). Our approach shows significant improvements over existing state-of-the-art on various backbone networks. Code is available at https://github.com/leolyj/SAN-SAW



### MRI-based Multi-task Decoupling Learning for Alzheimer's Disease Detection and MMSE Score Prediction: A Multi-site Validation
- **Arxiv ID**: http://arxiv.org/abs/2204.01708v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.01708v3)
- **Published**: 2022-04-02 09:19:18+00:00
- **Updated**: 2023-07-07 07:53:05+00:00
- **Authors**: Xu Tian, Jin Liu, Hulin Kuang, Yu Sheng, Jianxin Wang, The Alzheimer's Disease Neuroimaging Initiative
- **Comment**: There are some misstatements in the related work section of the
  paper. In the methods section, there are also errors in the description of
  some modules
- **Journal**: None
- **Summary**: Accurately detecting Alzheimer's disease (AD) and predicting mini-mental state examination (MMSE) score are important tasks in elderly health by magnetic resonance imaging (MRI). Most of the previous methods on these two tasks are based on single-task learning and rarely consider the correlation between them. Since the MMSE score, which is an important basis for AD diagnosis, can also reflect the progress of cognitive impairment, some studies have begun to apply multi-task learning methods to these two tasks. However, how to exploit feature correlation remains a challenging problem for these methods. To comprehensively address this challenge, we propose a MRI-based multi-task decoupled learning method for AD detection and MMSE score prediction. First, a multi-task learning network is proposed to implement AD detection and MMSE score prediction, which exploits feature correlation by adding three multi-task interaction layers between the backbones of the two tasks. Each multi-task interaction layer contains two feature decoupling modules and one feature interaction module. Furthermore, to enhance the generalization between tasks of the features selected by the feature decoupling module, we propose the feature consistency loss constrained feature decoupling module. Finally, in order to exploit the specific distribution information of MMSE score in different groups, a distribution loss is proposed to further enhance the model performance. We evaluate our proposed method on multi-site datasets. Experimental results show that our proposed multi-task decoupled representation learning method achieves good performance, outperforming single-task learning and other existing state-of-the-art methods.



### Online Convolutional Re-parameterization
- **Arxiv ID**: http://arxiv.org/abs/2204.00826v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.00826v1)
- **Published**: 2022-04-02 09:50:19+00:00
- **Updated**: 2022-04-02 09:50:19+00:00
- **Authors**: Mu Hu, Junyi Feng, Jiashen Hua, Baisheng Lai, Jianqiang Huang, Xiaojin Gong, Xiansheng Hua
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Structural re-parameterization has drawn increasing attention in various computer vision tasks. It aims at improving the performance of deep models without introducing any inference-time cost. Though efficient during inference, such models rely heavily on the complicated training-time blocks to achieve high accuracy, leading to large extra training cost. In this paper, we present online convolutional re-parameterization (OREPA), a two-stage pipeline, aiming to reduce the huge training overhead by squeezing the complex training-time block into a single convolution. To achieve this goal, we introduce a linear scaling layer for better optimizing the online blocks. Assisted with the reduced training cost, we also explore some more effective re-param components. Compared with the state-of-the-art re-param models, OREPA is able to save the training-time memory cost by about 70% and accelerate the training speed by around 2x. Meanwhile, equipped with OREPA, the models outperform previous methods on ImageNet by up to +0.6%.We also conduct experiments on object detection and semantic segmentation and show consistent improvements on the downstream tasks. Codes are available at https://github.com/JUGGHM/OREPA_CVPR2022 .



### Automatic Registration of Images with Inconsistent Content Through Line-Support Region Segmentation and Geometrical Outlier Removal
- **Arxiv ID**: http://arxiv.org/abs/2204.00832v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.00832v1)
- **Published**: 2022-04-02 10:47:16+00:00
- **Updated**: 2022-04-02 10:47:16+00:00
- **Authors**: Ming Zhao, Yongpeng Wu, Shengda Pan, Fan Zhou, Bowen An, André Kaup
- **Comment**: None
- **Journal**: None
- **Summary**: The implementation of automatic image registration is still difficult in various applications. In this paper, an automatic image registration approach through line-support region segmentation and geometrical outlier removal (ALRS-GOR) is proposed. This new approach is designed to address the problems associated with the registration of images with affine deformations and inconsistent content, such as remote sensing images with different spectral content or noise interference, or map images with inconsistent annotations. To begin with, line-support regions, namely a straight region whose points share roughly the same image gradient angle, are extracted to address the issues of inconsistent content existing in images. To alleviate the incompleteness of line segments, an iterative strategy with multi-resolution is employed to preserve global structures that are masked at full resolution by image details or noise. Then, Geometrical Outlier Removal (GOR) is developed to provide reliable feature point matching, which is based on affineinvariant geometrical classifications for corresponding matches initialized by SIFT. The candidate outliers are selected by comparing the disparity of accumulated classifications among all matches, instead of conventional methods which only rely on local geometrical relations. Various image sets have been considered in this paper for the evaluation of the proposed approach, including aerial images with simulated affine deformations, remote sensing optical and synthetic aperture radar images taken at different situations (multispectral, multisensor, and multitemporal), and map images with inconsistent annotations. Experimental results demonstrate the superior performance of the proposed method over the existing approaches for the whole data set.



### PixelFolder: An Efficient Progressive Pixel Synthesis Network for Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2204.00833v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.00833v4)
- **Published**: 2022-04-02 10:55:11+00:00
- **Updated**: 2022-07-27 06:40:18+00:00
- **Authors**: Jing He, Yiyi Zhou, Qi Zhang, Jun Peng, Yunhang Shen, Xiaoshuai Sun, Chao Chen, Rongrong Ji
- **Comment**: Accepted by ECCV2022. The code is available at
  https://github.com/BlingHe/PixelFolder
- **Journal**: None
- **Summary**: Pixel synthesis is a promising research paradigm for image generation, which can well exploit pixel-wise prior knowledge for generation. However, existing methods still suffer from excessive memory footprint and computation overhead. In this paper, we propose a progressive pixel synthesis network towards efficient image generation, coined as PixelFolder. Specifically, PixelFolder formulates image generation as a progressive pixel regression problem and synthesizes images via a multi-stage structure, which can greatly reduce the overhead caused by large tensor transformations. In addition, we introduce novel pixel folding operations to further improve model efficiency while maintaining pixel-wise prior knowledge for end-to-end regression. With these innovative designs, we greatly reduce the expenditure of pixel synthesis, e.g., reducing 89% computation and 53% parameters compared with the latest pixel synthesis method CIPS. To validate our approach, we conduct extensive experiments on two benchmark datasets, namely FFHQ and LSUN Church. The experimental results show that with much less expenditure, PixelFolder obtains new state-of-the-art (SOTA) performance on two benchmark datasets, i.e., 3.77 FID and 2.45 FID on FFHQ and LSUN Church, respectively.Meanwhile, PixelFolder is also more efficient than the SOTA methods like StyleGAN2, reducing about 72% computation and 31% parameters, respectively. These results greatly validate the effectiveness of the proposed PixelFolder.



### Rotated Object Detection via Scale-invariant Mahalanobis Distance in Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2204.00840v2
- **DOI**: 10.1109/LGRS.2022.3197617
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.00840v2)
- **Published**: 2022-04-02 11:21:39+00:00
- **Updated**: 2022-04-07 10:07:41+00:00
- **Authors**: Siyang Wen, Wei Guo, Yi Liu, Ruijie Wu
- **Comment**: 5 pages, 7 figures
- **Journal**: None
- **Summary**: Rotated object detection in aerial images is a meaningful yet challenging task as objects are densely arranged and have arbitrary orientations. The eight-parameter (coordinates of box vectors) methods in rotated object detection usually use ln-norm losses (L1 loss, L2 loss, and smooth L1 loss) as loss functions. As ln-norm losses are mainly based on non-scale-invariant Minkowski distance, using ln-norm losses will lead to inconsistency with the detection metric rotational Intersection-over-Union (IoU) and training instability. To address the problems, we use Mahalanobis distance to calculate loss between the predicted and the target box vertices' vectors, proposing a new loss function called Mahalanobis Distance Loss (MDL) for eight-parameter rotated object detection. As Mahalanobis distance is scale-invariant, MDL is more consistent with detection metric and more stable during training than ln-norm losses. To alleviate the problem of boundary discontinuity like all other eight-parameter methods, we further take the minimum loss value to make MDL continuous at boundary cases. We achieve state-of-art performance on DOTA-v1.0 with the proposed method MDL. Furthermore, compared to the experiment that uses smooth L1 loss, we find that MDL performs better in rotated object detection.



### Adversarial Neon Beam: A Light-based Physical Attack to DNNs
- **Arxiv ID**: http://arxiv.org/abs/2204.00853v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.00853v3)
- **Published**: 2022-04-02 12:57:00+00:00
- **Updated**: 2023-05-23 07:42:50+00:00
- **Authors**: Chengyin Hu, Weiwen Shi, Wen Li
- **Comment**: None
- **Journal**: None
- **Summary**: In the physical world, deep neural networks (DNNs) are impacted by light and shadow, which can have a significant effect on their performance. While stickers have traditionally been used as perturbations in most physical attacks, their perturbations can often be easily detected. To address this, some studies have explored the use of light-based perturbations, such as lasers or projectors, to generate more subtle perturbations, which are artificial rather than natural. In this study, we introduce a novel light-based attack called the adversarial neon beam (AdvNB), which utilizes common neon beams to create a natural black-box physical attack. Our approach is evaluated on three key criteria: effectiveness, stealthiness, and robustness. Quantitative results obtained in simulated environments demonstrate the effectiveness of the proposed method, and in physical scenarios, we achieve an attack success rate of 81.82%, surpassing the baseline. By using common neon beams as perturbations, we enhance the stealthiness of the proposed attack, enabling physical samples to appear more natural. Moreover, we validate the robustness of our approach by successfully attacking advanced DNNs with a success rate of over 75% in all cases. We also discuss defense strategies against the AdvNB attack and put forward other light-based physical attacks.



### Forestry digital twin with machine learning in Landsat 7 data
- **Arxiv ID**: http://arxiv.org/abs/2204.01709v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.01709v1)
- **Published**: 2022-04-02 14:14:28+00:00
- **Updated**: 2022-04-02 14:14:28+00:00
- **Authors**: Xuetao Jiang, Meiyu Jiang, YuChun Gou, Qian Li, Qingguo Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Modeling forests using historical data allows for more accurately evolution analysis, thus providing an important basis for other studies. As a recognized and effective tool, remote sensing plays an important role in forestry analysis. We can use it to derive information about the forest, including tree type, coverage and canopy density. There are many forest time series modeling studies using statistic values, but few using remote sensing images. Image prediction digital twin is an implementation of digital twin, which aims to predict future images bases on historical data. In this paper, we propose an LSTM-based digital twin approach for forest modeling, using Landsat 7 remote sensing image within 20 years. The experimental results show that the prediction twin method in this paper can effectively predict the future images of study area.



### Acoustic-to-articulatory Inversion based on Speech Decomposition and Auxiliary Feature
- **Arxiv ID**: http://arxiv.org/abs/2204.00873v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2204.00873v1)
- **Published**: 2022-04-02 14:47:19+00:00
- **Updated**: 2022-04-02 14:47:19+00:00
- **Authors**: Jianrong Wang, Jinyu Liu, Longxuan Zhao, Shanyu Wang, Ruiguo Yu, Li Liu
- **Comment**: None
- **Journal**: ICASSP 2022
- **Summary**: Acoustic-to-articulatory inversion (AAI) is to obtain the movement of articulators from speech signals. Until now, achieving a speaker-independent AAI remains a challenge given the limited data. Besides, most current works only use audio speech as input, causing an inevitable performance bottleneck. To solve these problems, firstly, we pre-train a speech decomposition network to decompose audio speech into speaker embedding and content embedding as the new personalized speech features to adapt to the speaker-independent case. Secondly, to further improve the AAI, we propose a novel auxiliary feature network to estimate the lip auxiliary features from the above personalized speech features. Experimental results on three public datasets show that, compared with the state-of-the-art only using the audio speech feature, the proposed method reduces the average RMSE by 0.25 and increases the average correlation coefficient by 2.0% in the speaker-dependent case. More importantly, the average RMSE decreases by 0.29 and the average correlation coefficient increases by 5.0% in the speaker-independent case.



### Convolutional Neural Networks for Image Spam Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.01710v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.01710v1)
- **Published**: 2022-04-02 15:10:44+00:00
- **Updated**: 2022-04-02 15:10:44+00:00
- **Authors**: Tazmina Sharmin, Fabio Di Troia, Katerina Potika, Mark Stamp
- **Comment**: None
- **Journal**: Information Security Journal: A Global Perspective 29(3):103-117,
  January 2020
- **Summary**: Spam can be defined as unsolicited bulk email. In an effort to evade text-based filters, spammers sometimes embed spam text in an image, which is referred to as image spam. In this research, we consider the problem of image spam detection, based on image analysis. We apply convolutional neural networks (CNN) to this problem, we compare the results obtained using CNNs to other machine learning techniques, and we compare our results to previous related work. We consider both real-world image spam and challenging image spam-like datasets. Our results improve on previous work by employing CNNs based on a novel feature set consisting of a combination of the raw image and Canny edges.



### Moment-based Adversarial Training for Embodied Language Comprehension
- **Arxiv ID**: http://arxiv.org/abs/2204.00889v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.00889v1)
- **Published**: 2022-04-02 16:07:24+00:00
- **Updated**: 2022-04-02 16:07:24+00:00
- **Authors**: Shintaro Ishikawa, Komei Sugiura
- **Comment**: Accepted for presentation at ICPR2022
- **Journal**: None
- **Summary**: In this paper, we focus on a vision-and-language task in which a robot is instructed to execute household tasks. Given an instruction such as "Rinse off a mug and place it in the coffee maker," the robot is required to locate the mug, wash it, and put it in the coffee maker. This is challenging because the robot needs to break down the instruction sentences into subgoals and execute them in the correct order. On the ALFRED benchmark, the performance of state-of-the-art methods is still far lower than that of humans. This is partially because existing methods sometimes fail to infer subgoals that are not explicitly specified in the instruction sentences. We propose Moment-based Adversarial Training (MAT), which uses two types of moments for perturbation updates in adversarial training. We introduce MAT to the embedding spaces of the instruction, subgoals, and state representations to handle their varieties. We validated our method on the ALFRED benchmark, and the results demonstrated that our method outperformed the baseline method for all the metrics on the benchmark.



### A Free Lunch to Person Re-identification: Learning from Automatically Generated Noisy Tracklets
- **Arxiv ID**: http://arxiv.org/abs/2204.00891v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.00891v1)
- **Published**: 2022-04-02 16:18:13+00:00
- **Updated**: 2022-04-02 16:18:13+00:00
- **Authors**: Hehan Teng, Tao He, Yuchen Guo, Zhenhua Guo, Guiguang Ding
- **Comment**: None
- **Journal**: None
- **Summary**: A series of unsupervised video-based re-identification (re-ID) methods have been proposed to solve the problem of high labor cost required to annotate re-ID datasets. But their performance is still far lower than the supervised counterparts. In the mean time, clean datasets without noise are used in these methods, which is not realistic. In this paper, we propose to tackle this problem by learning re-ID models from automatically generated person tracklets by multiple objects tracking (MOT) algorithm. To this end, we design a tracklet-based multi-level clustering (TMC) framework to effectively learn the re-ID model from the noisy person tracklets. First, intra-tracklet isolation to reduce ID switch noise within tracklets; second, alternates between using inter-tracklet association to eliminate ID fragmentation noise and network training using the pseudo label. Extensive experiments on MARS with various manually generated noises show the effectiveness of the proposed framework. Specifically, the proposed framework achieved mAP 53.4% and rank-1 63.7% on the simulated tracklets with strongest noise, even outperforming the best existing method on clean tracklets. Based on the results, we believe that building re-ID models from automatically generated noisy tracklets is a reasonable approach and will also be an important way to make re-ID models feasible in real-world applications.



### Mix-up Self-Supervised Learning for Contrast-agnostic Applications
- **Arxiv ID**: http://arxiv.org/abs/2204.00901v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.00901v1)
- **Published**: 2022-04-02 16:58:36+00:00
- **Updated**: 2022-04-02 16:58:36+00:00
- **Authors**: Yichen Zhang, Yifang Yin, Ying Zhang, Roger Zimmermann
- **Comment**: Accepted by ICME 2021
- **Journal**: None
- **Summary**: Contrastive self-supervised learning has attracted significant research attention recently. It learns effective visual representations from unlabeled data by embedding augmented views of the same image close to each other while pushing away embeddings of different images. Despite its great success on ImageNet classification, COCO object detection, etc., its performance degrades on contrast-agnostic applications, e.g., medical image classification, where all images are visually similar to each other. This creates difficulties in optimizing the embedding space as the distance between images is rather small. To solve this issue, we present the first mix-up self-supervised learning framework for contrast-agnostic applications. We address the low variance across images based on cross-domain mix-up and build the pretext task based on two synergistic objectives: image reconstruction and transparency prediction. Experimental results on two benchmark datasets validate the effectiveness of our method, where an improvement of 2.5% ~ 7.4% in top-1 accuracy was obtained compared to existing self-supervised learning methods.



### Deep Algebraic Fitting for Multiple Circle Primitives Extraction from Raw Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2204.00920v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.00920v1)
- **Published**: 2022-04-02 18:27:04+00:00
- **Updated**: 2022-04-02 18:27:04+00:00
- **Authors**: Zeyong Wei, Honghua Chen, Hao Tang, Qian Xie, Mingqiang Wei, Jun Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The shape of circle is one of fundamental geometric primitives of man-made engineering objects. Thus, extraction of circles from scanned point clouds is a quite important task in 3D geometry data processing. However, existing circle extraction methods either are sensitive to the quality of raw point clouds when classifying circle-boundary points, or require well-designed fitting functions when regressing circle parameters. To relieve the challenges, we propose an end-to-end Point Cloud Circle Algebraic Fitting Network (Circle-Net) based on a synergy of deep circle-boundary point feature learning and weighted algebraic fitting. First, we design a circle-boundary learning module, which considers local and global neighboring contexts of each point, to detect all potential circle-boundary points. Second, we develop a deep feature based circle parameter learning module for weighted algebraic fitting, without designing any weight metric, to avoid the influence of outliers during fitting. Unlike most of the cutting-edge circle extraction wisdoms, the proposed classification-and-fitting modules are originally co-trained with a comprehensive loss to enhance the quality of extracted circles.Comparisons on the established dataset and real-scanned point clouds exhibit clear improvements of Circle-Net over SOTAs in terms of both noise-robustness and extraction accuracy. We will release our code, model, and data for both training and evaluation on GitHub upon publication.



### Word separation in continuous sign language using isolated signs and post-processing
- **Arxiv ID**: http://arxiv.org/abs/2204.00923v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.00923v4)
- **Published**: 2022-04-02 18:34:33+00:00
- **Updated**: 2023-06-01 07:43:13+00:00
- **Authors**: Razieh Rastgoo, Kourosh Kiani, Sergio Escalera
- **Comment**: None
- **Journal**: None
- **Summary**: . Continuous Sign Language Recognition (CSLR) is a long challenging task in Computer Vision due to the difficulties in detecting the explicit boundaries between the words in a sign sentence. To deal with this challenge, we propose a two-stage model. In the first stage, the predictor model, which includes a combination of CNN, SVD, and LSTM, is trained with the isolated signs. In the second stage, we apply a post-processing algorithm to the Softmax outputs obtained from the first part of the model in order to separate the isolated signs in the continuous signs. While the proposed model is trained on the isolated sign classes with similar frame numbers, it is evaluated on the continuous sign videos with a different frame length per each isolated sign class. Due to the lack of a large dataset, including both the sign sequences and the corresponding isolated signs, two public datasets in Isolated Sign Language Recognition (ISLR), RKS-PERSIANSIGN and ASLLVD, are used for evaluation. Results of the continuous sign videos confirm the efficiency of the proposed model to deal with isolated sign boundaries detection.



### Single Image Internal Distribution Measurement Using Non-Local Variational Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2204.01711v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.01711v1)
- **Published**: 2022-04-02 18:43:55+00:00
- **Updated**: 2022-04-02 18:43:55+00:00
- **Authors**: Yeahia Sarker, Abdullah-Al-Zubaer Imran, Md Hafiz Ahamed, Ripon K. Chakrabortty, Michael J. Ryan, Sajal K. Das
- **Comment**: A Preprint Version
- **Journal**: None
- **Summary**: Deep learning-based super-resolution methods have shown great promise, especially for single image super-resolution (SISR) tasks. Despite the performance gain, these methods are limited due to their reliance on copious data for model training. In addition, supervised SISR solutions rely on local neighbourhood information focusing only on the feature learning processes for the reconstruction of low-dimensional images. Moreover, they fail to capitalize on global context due to their constrained receptive field. To combat these challenges, this paper proposes a novel image-specific solution, namely non-local variational autoencoder (\texttt{NLVAE}), to reconstruct a high-resolution (HR) image from a single low-resolution (LR) image without the need for any prior training. To harvest maximum details for various receptive regions and high-quality synthetic images, \texttt{NLVAE} is introduced as a self-supervised strategy that reconstructs high-resolution images using disentangled information from the non-local neighbourhood. Experimental results from seven benchmark datasets demonstrate the effectiveness of the \texttt{NLVAE} model. Moreover, our proposed model outperforms a number of baseline and state-of-the-art methods as confirmed through extensive qualitative and quantitative evaluations.



### SinNeRF: Training Neural Radiance Fields on Complex Scenes from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2204.00928v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.00928v2)
- **Published**: 2022-04-02 19:32:42+00:00
- **Updated**: 2022-08-12 00:58:58+00:00
- **Authors**: Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey Shi, Zhangyang Wang
- **Comment**: Project page: https://vita-group.github.io/SinNeRF/
- **Journal**: None
- **Summary**: Despite the rapid development of Neural Radiance Field (NeRF), the necessity of dense covers largely prohibits its wider applications. While several recent works have attempted to address this issue, they either operate with sparse views (yet still, a few of them) or on simple objects/scenes. In this work, we consider a more ambitious task: training neural radiance field, over realistically complex visual scenes, by "looking only once", i.e., using only a single view. To attain this goal, we present a Single View NeRF (SinNeRF) framework consisting of thoughtfully designed semantic and geometry regularizations. Specifically, SinNeRF constructs a semi-supervised learning process, where we introduce and propagate geometry pseudo labels and semantic pseudo labels to guide the progressive training process. Extensive experiments are conducted on complex scene benchmarks, including NeRF synthetic dataset, Local Light Field Fusion dataset, and DTU dataset. We show that even without pre-training on multi-view datasets, SinNeRF can yield photo-realistic novel-view synthesis results. Under the single image setting, SinNeRF significantly outperforms the current state-of-the-art NeRF baselines in all cases. Project page: https://vita-group.github.io/SinNeRF/



### A-ACT: Action Anticipation through Cycle Transformations
- **Arxiv ID**: http://arxiv.org/abs/2204.00942v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.00942v1)
- **Published**: 2022-04-02 21:50:45+00:00
- **Updated**: 2022-04-02 21:50:45+00:00
- **Authors**: Akash Gupta, Jingen Liu, Liefeng Bo, Amit K. Roy-Chowdhury, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: While action anticipation has garnered a lot of research interest recently, most of the works focus on anticipating future action directly through observed visual cues only. In this work, we take a step back to analyze how the human capability to anticipate the future can be transferred to machine learning algorithms. To incorporate this ability in intelligent systems a question worth pondering upon is how exactly do we anticipate? Is it by anticipating future actions from past experiences? Or is it by simulating possible scenarios based on cues from the present? A recent study on human psychology explains that, in anticipating an occurrence, the human brain counts on both systems. In this work, we study the impact of each system for the task of action anticipation and introduce a paradigm to integrate them in a learning framework. We believe that intelligent systems designed by leveraging the psychological anticipation models will do a more nuanced job at the task of human action prediction. Furthermore, we introduce cyclic transformation in the temporal dimension in feature and semantic label space to instill the human ability of reasoning of past actions based on the predicted future. Experiments on Epic-Kitchen, Breakfast, and 50Salads dataset demonstrate that the action anticipation model learned using a combination of the two systems along with the cycle transformation performs favorably against various state-of-the-art approaches.



### Efficient Convolutional Neural Networks on Raspberry Pi for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2204.00943v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.00943v4)
- **Published**: 2022-04-02 21:55:00+00:00
- **Updated**: 2022-11-19 05:44:11+00:00
- **Authors**: Rui-Yang Ju, Ting-Yu Lin, Jia-Hao Jian, Jen-Shiun Chiang
- **Comment**: None
- **Journal**: None
- **Summary**: With the good performance of deep learning algorithms in the field of computer vision (CV), the convolutional neural network (CNN) architecture has become a main backbone of the computer vision task. With the widespread use of mobile devices, neural network models based on platforms with low computing power are gradually being paid attention. However, due to the limitation of computing power, deep learning algorithms are usually not available on mobile devices. This paper proposes a lightweight convolutional neural network, TripleNet, which can operate easily on Raspberry Pi. Adopted from the concept of block connections in ThreshNet, the newly proposed network model compresses and accelerates the network model, reduces the amount of parameters of the network, and shortens the inference time of each image while ensuring the accuracy. Our proposed TripleNet and other state-of-the-art (SOTA) neural networks perform image classification experiments with the CIFAR-10 and SVHN datasets on Raspberry Pi. The experimental results show that, compared with GhostNet, MobileNet, ThreshNet, EfficientNet, and HarDNet, the inference time of TripleNet per image is shortened by 15%, 16%, 17%, 24%, and 30%, respectively. The detail codes of this work are available at https://github.com/RuiyangJu/TripleNet.



### Progressive Minimal Path Method with Embedded CNN
- **Arxiv ID**: http://arxiv.org/abs/2204.00944v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.00944v2)
- **Published**: 2022-04-02 22:00:27+00:00
- **Updated**: 2022-04-05 07:48:38+00:00
- **Authors**: Wei Liao
- **Comment**: Accepted to IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR), New Orleans, 2022
- **Journal**: None
- **Summary**: We propose Path-CNN, a method for the segmentation of centerlines of tubular structures by embedding convolutional neural networks (CNNs) into the progressive minimal path method. Minimal path methods are widely used for topology-aware centerline segmentation, but usually these methods rely on weak, hand-tuned image features. In contrast, CNNs use strong image features which are learned automatically from images. But CNNs usually do not take the topology of the results into account, and often require a large amount of annotations for training. We integrate CNNs into the minimal path method, so that both techniques benefit from each other: CNNs employ learned image features to improve the determination of minimal paths, while the minimal path method ensures the correct topology of the segmented centerlines, provides strong geometric priors to increase the performance of CNNs, and reduces the amount of annotations for the training of CNNs significantly. Our method has lower hardware requirements than many recent methods. Qualitative and quantitative comparison with other methods shows that Path-CNN achieves better performance, especially when dealing with tubular structures with complex shapes in challenging environments.



### Matching Feature Sets for Few-Shot Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2204.00949v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.00949v1)
- **Published**: 2022-04-02 22:42:54+00:00
- **Updated**: 2022-04-02 22:42:54+00:00
- **Authors**: Arman Afrasiyabi, Hugo Larochelle, Jean-François Lalonde, Christian Gagné
- **Comment**: International Conference on Computer Vision and Pattern Recognition
  (CVPR), 2022
- **Journal**: None
- **Summary**: In image classification, it is common practice to train deep networks to extract a single feature vector per input image. Few-shot classification methods also mostly follow this trend. In this work, we depart from this established direction and instead propose to extract sets of feature vectors for each image. We argue that a set-based representation intrinsically builds a richer representation of images from the base classes, which can subsequently better transfer to the few-shot classes. To do so, we propose to adapt existing feature extractors to instead produce sets of feature vectors from images. Our approach, dubbed SetFeat, embeds shallow self-attention mechanisms inside existing encoder architectures. The attention modules are lightweight, and as such our method results in encoders that have approximately the same number of parameters as their original versions. During training and inference, a set-to-set matching metric is used to perform image classification. The effectiveness of our proposed architecture and metrics is demonstrated via thorough experiments on standard few-shot datasets -- namely miniImageNet, tieredImageNet, and CUB -- in both the 1- and 5-shot scenarios. In all cases but one, our method outperforms the state-of-the-art.



### A Sentinel-2 multi-year, multi-country benchmark dataset for crop classification and segmentation with deep learning
- **Arxiv ID**: http://arxiv.org/abs/2204.00951v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.00951v2)
- **Published**: 2022-04-02 23:14:46+00:00
- **Updated**: 2022-04-27 18:04:35+00:00
- **Authors**: Dimitrios Sykas, Maria Sdraka, Dimitrios Zografakis, Ioannis Papoutsis
- **Comment**: This work has been accepted for publication in IEEE Journal of
  Selected Topics in Applied Earth Observations and Remote Sensing. Copyright
  may be transferred without notice, after which this version may no longer be
  accessible
- **Journal**: None
- **Summary**: In this work we introduce Sen4AgriNet, a Sentinel-2 based time series multi country benchmark dataset, tailored for agricultural monitoring applications with Machine and Deep Learning. Sen4AgriNet dataset is annotated from farmer declarations collected via the Land Parcel Identification System (LPIS) for harmonizing country wide labels. These declarations have only recently been made available as open data, allowing for the first time the labeling of satellite imagery from ground truth data. We proceed to propose and standardise a new crop type taxonomy across Europe that address Common Agriculture Policy (CAP) needs, based on the Food and Agriculture Organization (FAO) Indicative Crop Classification scheme. Sen4AgriNet is the only multi-country, multi-year dataset that includes all spectral information. It is constructed to cover the period 2016-2020 for Catalonia and France, while it can be extended to include additional countries. Currently, it contains 42.5 million parcels, which makes it significantly larger than other available archives. We extract two sub-datasets to highlight its value for diverse Deep Learning applications; the Object Aggregated Dataset (OAD) and the Patches Assembled Dataset (PAD). OAD capitalizes zonal statistics of each parcel, thus creating a powerful label-to-features instance for classification algorithms. On the other hand, PAD structure generalizes the classification problem to parcel extraction and semantic segmentation and labeling. The PAD and OAD are examined under three different scenarios to showcase and model the effects of spatial and temporal variability across different years and different countries.



### Histogram of Oriented Gradients Meet Deep Learning: A Novel Multi-task Deep Network for Medical Image Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.01712v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.01712v1)
- **Published**: 2022-04-02 23:50:29+00:00
- **Updated**: 2022-04-02 23:50:29+00:00
- **Authors**: Binod Bhattarai, Ronast Subedi, Rebati Raman Gaire, Eduard Vazquez, Danail Stoyanov
- **Comment**: None
- **Journal**: None
- **Summary**: We present our novel deep multi-task learning method for medical image segmentation. Existing multi-task methods demand ground truth annotations for both the primary and auxiliary tasks. Contrary to it, we propose to generate the pseudo-labels of an auxiliary task in an unsupervised manner. To generate the pseudo-labels, we leverage Histogram of Oriented Gradients (HOGs), one of the most widely used and powerful hand-crafted features for detection. Together with the ground truth semantic segmentation masks for the primary task and pseudo-labels for the auxiliary task, we learn the parameters of the deep network to minimise the loss of both the primary task and the auxiliary task jointly. We employed our method on two powerful and widely used semantic segmentation networks: UNet and U2Net to train in a multi-task setup. To validate our hypothesis, we performed experiments on two different medical image segmentation data sets. From the extensive quantitative and qualitative results, we observe that our method consistently improves the performance compared to the counter-part method. Moreover, our method is the winner of FetReg Endovis Sub-challenge on Semantic Segmentation organised in conjunction with MICCAI 2021.



